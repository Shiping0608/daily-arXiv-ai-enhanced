<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 24]
- [math.AP](#math.AP) [Total: 16]
- [physics.comp-ph](#physics.comp-ph) [Total: 10]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 8]
- [cs.LG](#cs.LG) [Total: 124]
- [cs.LO](#cs.LO) [Total: 1]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [cs.AI](#cs.AI) [Total: 10]
- [cs.SE](#cs.SE) [Total: 1]
- [hep-ex](#hep-ex) [Total: 2]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 2]
- [math.OC](#math.OC) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [physics.soc-ph](#physics.soc-ph) [Total: 3]
- [cs.SD](#cs.SD) [Total: 2]
- [physics.optics](#physics.optics) [Total: 1]
- [astro-ph.SR](#astro-ph.SR) [Total: 2]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.CL](#cs.CL) [Total: 18]
- [gr-qc](#gr-qc) [Total: 1]
- [cs.AR](#cs.AR) [Total: 2]
- [q-fin.RM](#q-fin.RM) [Total: 1]
- [math.CA](#math.CA) [Total: 1]
- [cs.RO](#cs.RO) [Total: 3]
- [quant-ph](#quant-ph) [Total: 3]
- [nlin.PS](#nlin.PS) [Total: 1]
- [math.PR](#math.PR) [Total: 1]
- [cs.GT](#cs.GT) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [q-fin.TR](#q-fin.TR) [Total: 1]
- [physics.geo-ph](#physics.geo-ph) [Total: 3]
- [stat.ML](#stat.ML) [Total: 7]
- [cs.CV](#cs.CV) [Total: 9]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]
- [math.ST](#math.ST) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [eess.IV](#eess.IV) [Total: 2]
- [cs.CR](#cs.CR) [Total: 4]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 2]
- [cs.DC](#cs.DC) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Solving engineering eigenvalue problems with neural networks using the Rayleigh quotient](https://arxiv.org/abs/2506.04375)
*Conor Rowan,John Evans,Kurt Maute,Alireza Doostan*

Main category: math.NA

TL;DR: The paper introduces a neural network-based method for solving eigenvalue problems, leveraging the Rayleigh quotient and Gram-Schmidt orthogonalization for robustness and simplicity.


<details>
  <summary>Details</summary>
Motivation: Eigenvalue problems are common in engineering but understudied in physics-informed machine learning, especially with neural network discretizations.

Method: Uses a neural network to discretize eigenfunctions, combining the Rayleigh quotient and Gram-Schmidt orthogonalization to solve eigenvalue problems.

Result: Demonstrates effectiveness for harmonic functions on irregular domains, parametric/nonlinear eigenproblems, and high-dimensional eigenanalysis.

Conclusion: The approach offers unique advantages for continuous eigenvalue problems in engineering mechanics.

Abstract: From characterizing the speed of a thermal system's response to computing
natural modes of vibration, eigenvalue analysis is ubiquitous in engineering.
In spite of this, eigenvalue problems have received relatively little treatment
compared to standard forward and inverse problems in the physics-informed
machine learning literature. In particular, neural network discretizations of
solutions to eigenvalue problems have seen only a handful of studies. Owing to
their nonlinearity, neural network discretizations prevent the conversion of
the continuous eigenvalue differential equation into a standard discrete
eigenvalue problem. In this setting, eigenvalue analysis requires more
specialized techniques. Using a neural network discretization of the
eigenfunction, we show that a variational form of the eigenvalue problem called
the "Rayleigh quotient" in tandem with a Gram-Schmidt orthogonalization
procedure is a particularly simple and robust approach to find the eigenvalues
and their corresponding eigenfunctions. This method is shown to be useful for
finding sets of harmonic functions on irregular domains, parametric and
nonlinear eigenproblems, and high-dimensional eigenanalysis. We also discuss
the utility of harmonic functions as a spectral basis for approximating
solutions to partial differential equations. Through various examples from
engineering mechanics, the combination of the Rayleigh quotient objective,
Gram-Schmidt procedure, and the neural network discretization of the
eigenfunction is shown to offer unique advantages for handling continuous
eigenvalue problems.

</details>


### [2] [Exponential Time Differencing Runge-Kutta Discontinuous Galerkin (ETD-RKDG) Methods for Nonlinear Degenerate Parabolic Equations](https://arxiv.org/abs/2506.04416)
*Ziyao Xu,Yong-Tao Zhang*

Main category: math.NA

TL;DR: High-order ETD-RK DG methods are proposed for nonlinear degenerate parabolic equations, addressing stiffness and wave-front capture with stabilizing limiters and nodal formulations.


<details>
  <summary>Details</summary>
Motivation: To solve nonlinear degenerate parabolic equations with mixed hyperbolic-parabolic behavior, which pose challenges like sharp wave fronts and stiff parabolic terms.

Method: Uses DG methods with stabilizing limiters on unstructured meshes and ETD-RK for time integration, leveraging the Jacobian matrix for stiffness extraction.

Result: Demonstrates improved stability and allows larger time-step sizes, validated through linear stability analysis and 3D computational results.

Conclusion: The proposed method effectively handles the stiffness and wave-front challenges, offering robust solutions for complex domains.

Abstract: In this paper, we study high-order exponential time differencing Runge-Kutta
(ETD-RK) discontinuous Galerkin (DG) methods for nonlinear degenerate parabolic
equations. This class of equations exhibits hyperbolic behavior in degenerate
regions and parabolic behavior in non-degenerate regions, resulting in sharp
wave fronts in the solution profiles and a parabolic-type time-step
restriction, $\tau \sim O(h^2)$, for explicit time integration. To address
these challenges and solve such equations in complex domains, we employ DG
methods with appropriate stabilizing limiters on unstructured meshes to capture
the wave fronts and use ETD-RK methods for time integration to resolve the
stiffness of parabolic terms. We extract the system's stiffness using the
Jacobian matrix of the DG discretization for diffusion terms and adopt a nodal
formulation to facilitate its computation. The algorithm is described in detail
for two-dimensional triangular meshes. We also conduct a linear stability
analysis in one spatial dimension and present computational results on
three-dimensional simplex meshes, demonstrating significant improvements in
stability and large time-step sizes.

</details>


### [3] [An Augmented Lagrangian Preconditioner for Navier--Stokes Equations with Runge--Kutta in Time](https://arxiv.org/abs/2506.04451)
*Santolo Leveque,Yunhui He,Maxim Olshanskii*

Main category: math.NA

TL;DR: A Runge-Kutta method is used for time integration of the incompressible Navier-Stokes equations, with nonlinear problems solved via Newton's method and a preconditioned GMRES approach.


<details>
  <summary>Details</summary>
Motivation: To efficiently solve the nonlinear systems arising from the Runge-Kutta time integration of the Navier-Stokes equations.

Method: Finite element discretization, Newton's method for nonlinear systems, and an augmented Lagrangian preconditioner with flexible GMRES and multigrid support.

Result: Numerical results demonstrate robustness and efficiency across varying parameters like viscosity, mesh size, and time step.

Conclusion: The proposed preconditioned GMRES strategy is effective for solving the nonlinear systems in this context.

Abstract: We consider a Runge--Kutta method for the numerical time integration of the
nonstationary incompressible Navier--Stokes equations. This yields a sequence
of nonlinear problems to be solved for the stages of the Runge--Kutta method.
The resulting nonlinear system of differential equations is discretized using a
finite element method. To compute a numerical approximation of the stages at
each time step, we employ Newton's method, which requires the solution of a
large and sparse generalized saddle-point problem at each nonlinear iteration.
We devise an augmented Lagrangian preconditioner within the flexible GMRES
method for solving the Newton systems at each time step. The preconditioner can
be applied inexactly with the help of a multigrid routine. We present numerical
evidence of the robustness and efficiency of the proposed strategy for
different values of the viscosity, mesh size, time step, and number of stages
of the Runge--Kutta method.

</details>


### [4] [An Array Decomposition Method for Finite Arrays with Electrically Connected Elements for fast Toeplitz Solvers](https://arxiv.org/abs/2506.04710)
*Lucas Åkerstedt,Harald Hultin,B. L. G. Jonsson*

Main category: math.NA

TL;DR: A memory-efficient method for constructing impedance matrices in array antennas using multilevel block Toeplitz structure, reducing memory usage from O(N_x^2 N_y^2) to O(N_x N_y).


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency in memory usage and computation for array antennas with finite translational symmetries.

Method: Uses MoM with RWG-like elements on structured meshes, introduces nine connectable components, and leverages multilevel block Toeplitz matrices.

Result: Demonstrated by computing far-fields for a 32x32 array and scattering parameters for two 9x9 arrays, showing significant memory reduction.

Conclusion: The method efficiently handles non-symmetric parts of arrays and improves computational performance for MoM current vector calculations.

Abstract: A large part of the geometry of array antennas is often partially defined by
finite translational symmetries. Applying the method of moments (MoM) with the
RWG-like element on an appropriately structured mesh to these arrays results in
an impedance matrix where the main part exhibits a multilevel block Toeplitz
structure. This article introduces a memory-efficient construction method that
effectively represents and reuses impedance calculations. The proposed method,
applicable to electrically connected elements, also accounts for all
non-symmetric parts of the array. The core idea involves nine distinct
electrically connectable components from which the array can be assembled. The
derived multilevel block Toeplitz matrix is further utilized by an in-house
inverse solver to achieve faster and more memory-efficient MoM current vector
calculations. We demonstrate the method by computing the far-field of a 32x32
array and the scattering parameters of two tightly coupled 9x9 arrays. This
approach reduces the memory allocation from $\mathcal{O}(N_x^2 N_y^2)$ to
$\mathcal{O}(N_x N_y)$, for an $N_x \times N_y$ array.

</details>


### [5] [A Fast, Accurate and Oscillation-free Spectral Collocation Solver for High-dimensional Transport Problems](https://arxiv.org/abs/2506.04732)
*Nicola Cavallini,Gianmarco Manzini,Daniele Funaro,Andrea Favalli*

Main category: math.NA

TL;DR: The paper introduces the T²S² solver to efficiently solve high-dimensional transport equations, combining Spectral Collocation, Superconsistency, and Tensor Train format for accuracy and compression.


<details>
  <summary>Details</summary>
Motivation: Transport phenomena are critical but solving their high-dimensional equations is computationally challenging due to dimensionality.

Method: The T²S² solver integrates Spectral Collocation, Superconsistency, and Tensor Train format for exponential convergence, stabilization, and data compression.

Result: Achieves extremely low compression ratios (~10⁻¹²) and solves high-dimensional problems in minutes on standard hardware.

Conclusion: T²S² enables efficient and accurate modeling of complex transport phenomena, making previously intractable problems feasible.

Abstract: Transport phenomena-describing the movement of particles, energy, or other
physical quantities-are fundamental in various scientific disciplines,
including nuclear physics, plasma physics, astrophysics, engineering, and the
natural sciences.
  However, solving the associated seven-dimensional transport equations poses a
significant computational challenge due to the curse of dimensionality.
  We introduce the Tensor Train Superconsistent Spectral (T${^2}$S${^2}$)
solver to address this challenge, integrating Spectral Collocation for
exponential convergence, Superconsistency for stabilization in
transport-dominated regimes, and Tensor Train format for substantial data
compression. T${^2}$S${^2}$ enforces a dimension-wise superconsistent condition
compatible with tensor structures, achieving extremely low compression ratios,
in the order of $(10^{-12})$, while preserving spectral accuracy. Numerical
experiments on linear problems demonstrate that T${^2}$S${^2}$ can solve
high-dimensional transport problems in minutes on standard hardware, making
previously intractable problems computationally feasible. This advancement
opens new avenues for efficiently and accurately modeling complex transport
phenomena.

</details>


### [6] [Numerical solution of the wave equation outside a sphere](https://arxiv.org/abs/2506.04809)
*Michael J. Carley*

Main category: math.NA

TL;DR: A method for fast evaluation of transient acoustic fields outside a spherical surface using Lebedev quadratures and Lagrange interpolation, achieving near machine-precision accuracy and speed-up.


<details>
  <summary>Details</summary>
Motivation: To efficiently and accurately evaluate transient acoustic fields generated by internal sources outside a spherical surface.

Method: Uses Lebedev quadratures for spatial integration and Lagrange interpolation/differentiation in an advanced time algorithm.

Result: Near machine-precision accuracy; speed-up depends on quadrature order, breaking even at ~1.15 times the number of surface nodes.

Conclusion: The method is efficient and accurate for transient acoustic field evaluation.

Abstract: A method is presented for the fast evaluation of the transient acoustic field
generated outside a spherical surface by sources inside the surface. The method
employs Lebedev quadratures, which are the optimal method for spatial
integration, and Lagrange interpolation and differentiation in an advanced time
algorithm for the evaluation of the transient field. Numerical testing
demonstrates that the approach gives near machine-precision accuracy and a
speed-up in evaluation time which depends on the order of quadrature rule
employed but breaks even with direct evaluation at a number of field points
about 1.15 times the number of surface quadrature nodes.

</details>


### [7] [Tensor-based multivariate function approximation: methods benchmarking and comparison](https://arxiv.org/abs/2506.04791)
*Athanasios C. Antoulas,Ion Victor Gosea,Charles Poussot-Vassal,Pierre Vuillemin*

Main category: math.NA

TL;DR: The paper evaluates various tensor-based multivariate function approximation methods, comparing their performance, features, and user experience using a benchmark collection of functions.


<details>
  <summary>Details</summary>
Motivation: To fairly assess and compare different tensor approximation tools, guiding users in understanding their processes, advantages, and limitations.

Method: Construct tensors from a diverse set of multivariate functions, evaluate method performances (accuracy, computational time, parameter tuning), and compare methods using multiple criteria.

Result: A benchmark collection is provided, and the multivariate Loewner Framework (mLF) is highlighted with detailed examples.

Conclusion: The note aims to guide users in selecting tensor approximation tools without ranking them, emphasizing understanding and practical insights.

Abstract: In this note, we evaluate the performances, the features and the
user-experience of some methods (and their implementations) designed for
tensor- (or data-) based multivariate function construction and approximation.
To this aim, a collection of multivariate functions extracted from contributive
works coming from different communities, is suggested. First, these functions
with varying complexity (e.g. number and degree of the variables) and nature
(e.g. rational, irrational, differentiable or not, symmetric, etc.) are used to
construct tensors, each of different dimension and size on the disk. Second,
grounded on this tensor, we inspect performances of each considered method
(e.g. the accuracy, the computational time, the parameters tuning impact,
etc.). Finally, considering the "best" parameter tuning set, we compare each
method using multiple evaluation criteria. The purpose of this note is not to
rank the methods but rather to evaluate as fairly as possible the different
available strategies, with the idea in mind to guide users to understand the
process, the possibilities, the advantages and the limits brought by each
tools. The contribution claimed is to suggest a complete benchmark collection
of some available tools for tensor approximation by surrogate models (e.g.
rational functions, networks, etc.). In addition, as contributors of the
multivariate Loewner Framework (mLF) approach (and its side implementation in
MDSPACK), attention and details of the latter are more explicitly given, in
order to provide readers a digest of this contributive work and some details
with simple examples.

</details>


### [8] [Efficient randomized algorithms for the fixed Tucker-rank problem of Tucker decomposition with adaptive shifts](https://arxiv.org/abs/2506.04840)
*Maolin Che,Yimin Wei,Chong Wu,Hong Yan*

Main category: math.NA

TL;DR: The paper introduces fast randomized algorithms for Tucker decomposition using adaptive shifted power iterations, improving efficiency and accuracy over existing methods.


<details>
  <summary>Details</summary>
Motivation: To bridge theoretical advancements with scalable solutions for tensor decomposition, addressing the fixed Tucker-rank problem.

Method: Integration of adaptive shifted power iterations into randomized variants of T-HOSVD and ST-HOSVD, refining singular value gaps to accelerate convergence.

Result: Theoretical error bounds and numerical experiments show the algorithms outperform state-of-the-art techniques in runtime and approximation error.

Conclusion: The proposed methods offer efficient, robust, and accurate solutions for Tucker decomposition, validated by synthetic and real-world datasets.

Abstract: Randomized numerical linear algebra is proved to bridge theoretical
advancements to offer scalable solutions for approximating tensor
decomposition. This paper introduces fast randomized algorithms for solving the
fixed Tucker-rank problem of Tucker decomposition, through the integration of
adaptive shifted power iterations. The proposed algorithms enhance randomized
variants of truncated high-order singular value decomposition (T-HOSVD) and
sequentially T-HOSVD (ST-HOSVD) by incorporating dynamic shift strategies,
which accelerate convergence by refining the singular value gap and reduce the
number of required power iterations while maintaining accuracy. Theoretical
analyses provide probabilistic error bounds, demonstrating that the proposed
methods achieve comparable or superior accuracy compared to deterministic
approaches. Numerical experiments on synthetic and real-world datasets validate
the efficiency and robustness of the proposed algorithms, showing a significant
decline in runtime and approximation error over state-of-the-art techniques.

</details>


### [9] [Active flux for ideal magnetohydrodynamics: A positivity-preserving scheme with the Godunov-Powell source term](https://arxiv.org/abs/2506.04857)
*Junming Duan,Praveen Chandrashekar,Christian Klingenberg*

Main category: math.NA

TL;DR: A positivity-preserving Active Flux scheme for ideal magnetohydrodynamics is proposed, incorporating the Godunov-Powell source term for divergence-free constraints and using limiters to ensure density and pressure positivity.


<details>
  <summary>Details</summary>
Motivation: To develop a high-order, compact finite volume scheme for ideal magnetohydrodynamics that maintains positivity of density and pressure while handling divergence-free constraints effectively.

Method: The scheme combines the Active Flux method with the Godunov-Powell source term, uses quadratic reconstruction for nonconservative terms, and employs flux and scaling limiters for positivity. A new shock sensor is introduced to control oscillations.

Result: Numerical tests confirm third-order accuracy, positivity preservation, and effective shock-capturing. The Godunov-Powell source term's role in controlling divergence error is validated.

Conclusion: The proposed scheme successfully balances high-order accuracy, compactness, and positivity preservation, demonstrating robustness in handling magnetohydrodynamics problems.

Abstract: The Active Flux (AF) is a compact, high-order finite volume scheme that
allows more flexibility by introducing additional point value degrees of
freedom at cell interfaces. This paper proposes a positivity-preserving (PP) AF
scheme for solving the ideal magnetohydrodynamics, where the Godunov-Powell
source term is employed to deal with the divergence-free constraint. For the
evolution of the cell average, apart from the standard conservative finite
volume method for the flux derivative, the nonconservative source term is built
on the quadratic reconstruction in each cell, which maintains the compact
stencil in the AF scheme. For the point value update, the local Lax-Friedrichs
(LLF) flux vector splitting is adopted for the flux derivative, originally
proposed in [Duan, Barsukow, and Klingenberg, SIAM Journal on Scientific
Computing, 47(2), A811--A837, 2025], and a central difference is used to
discretize the divergence in the source term. A parametrized flux limiter and a
scaling limiter are presented to preserve the density and pressure positivity
by blending the AF scheme with the first-order PP LLF scheme with the source
term. To suppress oscillations, a new shock sensor considering the divergence
error is proposed, which is used to compute the blending coefficients for the
cell average. Several numerical tests are conducted to verify the third-order
accuracy, PP property, and shock-capturing ability of the scheme. The key role
of the Godunov-Powell source term and its suitable discretization in
controlling divergence error is also validated.

</details>


### [10] [Numerical analysis for constrained and unconstrained Q-tensor energies for liquid crystals](https://arxiv.org/abs/2506.04880)
*Heiko Gimperlein,Ruma R. Maity*

Main category: math.NA

TL;DR: A finite element framework for 3D Landau-de Gennes Q-tensor energies, focusing on anisotropy and physical constraints, with rigorous analysis of well-posedness, convergence, and error impact.


<details>
  <summary>Details</summary>
Motivation: To model nematic liquid crystals realistically by addressing anisotropy and physical constraints in the Q-tensor energy framework.

Method: Finite element approximation for nonlinear elliptic PDEs with non-homogeneous boundary conditions, using the Newton-Kantorovich theorem for discrete solutions.

Result: Well-posedness of the discrete problem, local uniqueness of solutions, and optimal convergence rates in the energy norm.

Conclusion: The framework effectively models nematic liquid crystals with rigorous theoretical backing and practical convergence guarantees.

Abstract: This paper introduces a comprehensive finite element approximation framework
for three-dimensional Landau-de Gennes $Q$-tensor energies for nematic liquid
crystals, with a particular focus on the anisotropy of the elastic energy and
the Ball-Majumdar singular potential. This potential imposes essential physical
constraints on the eigenvalues of the $Q$-tensor, ensuring realistic modeling.
We address the approximation of regular solutions to nonlinear elliptic partial
differential equations with non-homogeneous boundary conditions associated with
Landau-de Gennes energies. The well-posedness of the discrete linearized
problem is rigorously demonstrated. The existence and local uniqueness of the
discrete solution is derived using the Newton-Kantorovich theorem. Furthermore,
we demonstrate an optimal order convergence rate in the energy norm and discuss
the impact of eigenvalue constraints on the a priori error analysis.

</details>


### [11] [Probability of Collision with Tethered Spacecraft](https://arxiv.org/abs/2506.04969)
*Yema Paul,Emmanuel Delande,Francois Vinet,Francois Laporte,Manuel Sanjurjo-Rivo,Aldo Tonnini,Joan-Pau Sanchez*

Main category: math.NA

TL;DR: A method to estimate collision probability for tethered spacecraft, addressing conservatism in standard spherical models by accounting for tether extent and uncertainty.


<details>
  <summary>Details</summary>
Motivation: Standard collision probability methods are overly conservative for tethered spacecraft due to spherical hard-body assumptions.

Method: Maximizes collision probability over all physically admissible tether shapes, considering spatial extent and configuration uncertainty.

Result: Applied to real-world events, the method provides more realistic risk estimates for kilometer-scale tethers.

Conclusion: Improves hazard distinction, aiding better collision avoidance decisions.

Abstract: This Engineering Note addresses the challenge of estimating the probability
of collision for tethered spacecraft during close encounters with other space
objects. Standard probability of collision methods, based on spherical
hard-body assumptions, tend to be overly conservative when applied to long
tether systems. We introduce a method that accounts for the tether's spatial
extent and configuration uncertainty by maximizing the probability of collision
over all physically admissible tether shapes. Applied to real-world conjunction
events involving a kilometer-scale flexible inextensible tether, the method
yields more realistic risk estimates. This approach improves the ability to
distinguish hazardous from benign encounters, thereby supporting more informed
collision avoidance decisions.

</details>


### [12] [Norming Sets for Tensor and Polynomial Sketching](https://arxiv.org/abs/2506.05174)
*Yifan Zhang,Joe Kileel*

Main category: math.NA

TL;DR: The paper introduces a sketching theory for real algebraic varieties and polynomial maps, proposing a new method called the median sketch, which uses fewer measurements.


<details>
  <summary>Details</summary>
Motivation: To extend sketching theory to algebraic varieties and polynomial maps, including low-rank tensors, and provide a universal framework for controlling sketching dimensions.

Method: Uses norming set theory to develop the median sketch, requiring only ~O(dim V) tensor-structured or sparse linear measurements.

Result: The median sketch efficiently embeds sets like low-rank tensors with reduced measurements.

Conclusion: The framework and median sketch advance sketching theory for algebraic structures, offering practical efficiency.

Abstract: This paper develops the sketching (i.e., randomized dimension reduction)
theory for real algebraic varieties and images of polynomial maps, including,
e.g., the set of low rank tensors and tensor networks. Through the lens of
norming sets, we provide a framework for controlling the sketching dimension
for \textit{any} sketch operator used to embed said sets, including
sub-Gaussian, fast Johnson-Lindenstrauss, and tensor structured sketch
operators. Leveraging norming set theory, we propose a new sketching method
called the median sketch. It embeds such a set $V$ using only
$\widetilde{\mathcal{O}}(\dim V)$ tensor structured or sparse linear
measurements.

</details>


### [13] [Solving engineering eigenvalue problems with neural networks using the Rayleigh quotient](https://arxiv.org/abs/2506.04375)
*Conor Rowan,John Evans,Kurt Maute,Alireza Doostan*

Main category: math.NA

TL;DR: The paper introduces a neural network-based method for solving eigenvalue problems using the Rayleigh quotient and Gram-Schmidt orthogonalization, demonstrating its effectiveness for irregular domains and high-dimensional cases.


<details>
  <summary>Details</summary>
Motivation: Eigenvalue problems are common in engineering but underexplored in physics-informed machine learning. Neural network discretizations complicate eigenvalue analysis, requiring specialized techniques.

Method: A neural network discretizes the eigenfunction, combined with the Rayleigh quotient and Gram-Schmidt orthogonalization to solve eigenvalue problems.

Result: The method effectively handles irregular domains, parametric/nonlinear eigenproblems, and high-dimensional cases, offering advantages for continuous eigenvalue problems.

Conclusion: The approach is robust and versatile, with potential applications in approximating PDE solutions using harmonic functions as a spectral basis.

Abstract: From characterizing the speed of a thermal system's response to computing
natural modes of vibration, eigenvalue analysis is ubiquitous in engineering.
In spite of this, eigenvalue problems have received relatively little treatment
compared to standard forward and inverse problems in the physics-informed
machine learning literature. In particular, neural network discretizations of
solutions to eigenvalue problems have seen only a handful of studies. Owing to
their nonlinearity, neural network discretizations prevent the conversion of
the continuous eigenvalue differential equation into a standard discrete
eigenvalue problem. In this setting, eigenvalue analysis requires more
specialized techniques. Using a neural network discretization of the
eigenfunction, we show that a variational form of the eigenvalue problem called
the "Rayleigh quotient" in tandem with a Gram-Schmidt orthogonalization
procedure is a particularly simple and robust approach to find the eigenvalues
and their corresponding eigenfunctions. This method is shown to be useful for
finding sets of harmonic functions on irregular domains, parametric and
nonlinear eigenproblems, and high-dimensional eigenanalysis. We also discuss
the utility of harmonic functions as a spectral basis for approximating
solutions to partial differential equations. Through various examples from
engineering mechanics, the combination of the Rayleigh quotient objective,
Gram-Schmidt procedure, and the neural network discretization of the
eigenfunction is shown to offer unique advantages for handling continuous
eigenvalue problems.

</details>


### [14] [Exponential Time Differencing Runge-Kutta Discontinuous Galerkin (ETD-RKDG) Methods for Nonlinear Degenerate Parabolic Equations](https://arxiv.org/abs/2506.04416)
*Ziyao Xu,Yong-Tao Zhang*

Main category: math.NA

TL;DR: The paper presents high-order ETD-RK DG methods for solving nonlinear degenerate parabolic equations, addressing stiffness and wave front challenges with stabilizing limiters and nodal formulations.


<details>
  <summary>Details</summary>
Motivation: To tackle the hyperbolic-parabolic behavior of degenerate parabolic equations and overcome stiffness and time-step restrictions in explicit methods.

Method: Uses DG methods with stabilizing limiters on unstructured meshes and ETD-RK for time integration, leveraging the Jacobian matrix for stiffness extraction.

Result: Demonstrates improved stability and larger time-step sizes, validated through linear stability analysis and 3D computational results.

Conclusion: The proposed methods effectively handle stiffness and wave fronts, offering robust solutions for complex domains.

Abstract: In this paper, we study high-order exponential time differencing Runge-Kutta
(ETD-RK) discontinuous Galerkin (DG) methods for nonlinear degenerate parabolic
equations. This class of equations exhibits hyperbolic behavior in degenerate
regions and parabolic behavior in non-degenerate regions, resulting in sharp
wave fronts in the solution profiles and a parabolic-type time-step
restriction, $\tau \sim O(h^2)$, for explicit time integration. To address
these challenges and solve such equations in complex domains, we employ DG
methods with appropriate stabilizing limiters on unstructured meshes to capture
the wave fronts and use ETD-RK methods for time integration to resolve the
stiffness of parabolic terms. We extract the system's stiffness using the
Jacobian matrix of the DG discretization for diffusion terms and adopt a nodal
formulation to facilitate its computation. The algorithm is described in detail
for two-dimensional triangular meshes. We also conduct a linear stability
analysis in one spatial dimension and present computational results on
three-dimensional simplex meshes, demonstrating significant improvements in
stability and large time-step sizes.

</details>


### [15] [An Augmented Lagrangian Preconditioner for Navier--Stokes Equations with Runge--Kutta in Time](https://arxiv.org/abs/2506.04451)
*Santolo Leveque,Yunhui He,Maxim Olshanskii*

Main category: math.NA

TL;DR: A Runge-Kutta method is used for time integration of the nonstationary incompressible Navier-Stokes equations, with nonlinear systems solved via Newton's method and a preconditioned GMRES approach.


<details>
  <summary>Details</summary>
Motivation: To efficiently solve the nonlinear systems arising from the Runge-Kutta discretization of the Navier-Stokes equations.

Method: Finite element discretization, Newton's method for nonlinear systems, and an augmented Lagrangian preconditioner within flexible GMRES, supported by multigrid.

Result: Numerical results demonstrate robustness and efficiency across varying parameters like viscosity, mesh size, and time step.

Conclusion: The proposed strategy is effective for solving the nonlinear systems in the Runge-Kutta framework.

Abstract: We consider a Runge--Kutta method for the numerical time integration of the
nonstationary incompressible Navier--Stokes equations. This yields a sequence
of nonlinear problems to be solved for the stages of the Runge--Kutta method.
The resulting nonlinear system of differential equations is discretized using a
finite element method. To compute a numerical approximation of the stages at
each time step, we employ Newton's method, which requires the solution of a
large and sparse generalized saddle-point problem at each nonlinear iteration.
We devise an augmented Lagrangian preconditioner within the flexible GMRES
method for solving the Newton systems at each time step. The preconditioner can
be applied inexactly with the help of a multigrid routine. We present numerical
evidence of the robustness and efficiency of the proposed strategy for
different values of the viscosity, mesh size, time step, and number of stages
of the Runge--Kutta method.

</details>


### [16] [An Array Decomposition Method for Finite Arrays with Electrically Connected Elements for fast Toeplitz Solvers](https://arxiv.org/abs/2506.04710)
*Lucas Åkerstedt,Harald Hultin,B. L. G. Jonsson*

Main category: math.NA

TL;DR: A memory-efficient method for constructing impedance matrices in array antennas using multilevel block Toeplitz structure, reducing memory usage from O(N_x^2 N_y^2) to O(N_x N_y).


<details>
  <summary>Details</summary>
Motivation: Finite translational symmetries in array antennas lead to redundant impedance calculations, which can be optimized for memory efficiency.

Method: Uses MoM with RWG-like elements on structured meshes, representing impedance matrices as multilevel block Toeplitz and reusing calculations via nine connectable components.

Result: Demonstrated on a 32x32 array and two 9x9 arrays, showing reduced memory usage and faster calculations.

Conclusion: The method efficiently handles non-symmetric parts of arrays and improves computational performance for MoM current vector calculations.

Abstract: A large part of the geometry of array antennas is often partially defined by
finite translational symmetries. Applying the method of moments (MoM) with the
RWG-like element on an appropriately structured mesh to these arrays results in
an impedance matrix where the main part exhibits a multilevel block Toeplitz
structure. This article introduces a memory-efficient construction method that
effectively represents and reuses impedance calculations. The proposed method,
applicable to electrically connected elements, also accounts for all
non-symmetric parts of the array. The core idea involves nine distinct
electrically connectable components from which the array can be assembled. The
derived multilevel block Toeplitz matrix is further utilized by an in-house
inverse solver to achieve faster and more memory-efficient MoM current vector
calculations. We demonstrate the method by computing the far-field of a 32x32
array and the scattering parameters of two tightly coupled 9x9 arrays. This
approach reduces the memory allocation from $\mathcal{O}(N_x^2 N_y^2)$ to
$\mathcal{O}(N_x N_y)$, for an $N_x \times N_y$ array.

</details>


### [17] [A Fast, Accurate and Oscillation-free Spectral Collocation Solver for High-dimensional Transport Problems](https://arxiv.org/abs/2506.04732)
*Nicola Cavallini,Gianmarco Manzini,Daniele Funaro,Andrea Favalli*

Main category: math.NA

TL;DR: The paper introduces the Tensor Train Superconsistent Spectral (T²S²) solver to efficiently solve high-dimensional transport equations, overcoming computational challenges with exponential convergence and data compression.


<details>
  <summary>Details</summary>
Motivation: Transport phenomena are crucial in many fields, but solving their high-dimensional equations is computationally challenging due to the curse of dimensionality.

Method: The T²S² solver combines Spectral Collocation for exponential convergence, Superconsistency for stability, and Tensor Train format for data compression.

Result: T²S² achieves extremely low compression ratios (~10⁻¹²) and solves high-dimensional problems in minutes on standard hardware.

Conclusion: T²S² makes previously intractable transport problems feasible, enabling efficient and accurate modeling of complex phenomena.

Abstract: Transport phenomena-describing the movement of particles, energy, or other
physical quantities-are fundamental in various scientific disciplines,
including nuclear physics, plasma physics, astrophysics, engineering, and the
natural sciences.
  However, solving the associated seven-dimensional transport equations poses a
significant computational challenge due to the curse of dimensionality.
  We introduce the Tensor Train Superconsistent Spectral (T${^2}$S${^2}$)
solver to address this challenge, integrating Spectral Collocation for
exponential convergence, Superconsistency for stabilization in
transport-dominated regimes, and Tensor Train format for substantial data
compression. T${^2}$S${^2}$ enforces a dimension-wise superconsistent condition
compatible with tensor structures, achieving extremely low compression ratios,
in the order of $(10^{-12})$, while preserving spectral accuracy. Numerical
experiments on linear problems demonstrate that T${^2}$S${^2}$ can solve
high-dimensional transport problems in minutes on standard hardware, making
previously intractable problems computationally feasible. This advancement
opens new avenues for efficiently and accurately modeling complex transport
phenomena.

</details>


### [18] [Numerical solution of the wave equation outside a sphere](https://arxiv.org/abs/2506.04809)
*Michael J. Carley*

Main category: math.NA

TL;DR: A fast method for evaluating transient acoustic fields outside a spherical surface using Lebedev quadratures and Lagrange interpolation, achieving near machine-precision accuracy and speed-up.


<details>
  <summary>Details</summary>
Motivation: To efficiently and accurately evaluate transient acoustic fields generated by internal sources outside a spherical surface.

Method: Uses Lebedev quadratures for spatial integration and Lagrange interpolation/differentiation in an advanced time algorithm.

Result: Near machine-precision accuracy; speed-up depends on quadrature order, breaking even at ~1.15 times surface nodes.

Conclusion: The method is efficient and accurate for transient acoustic field evaluation.

Abstract: A method is presented for the fast evaluation of the transient acoustic field
generated outside a spherical surface by sources inside the surface. The method
employs Lebedev quadratures, which are the optimal method for spatial
integration, and Lagrange interpolation and differentiation in an advanced time
algorithm for the evaluation of the transient field. Numerical testing
demonstrates that the approach gives near machine-precision accuracy and a
speed-up in evaluation time which depends on the order of quadrature rule
employed but breaks even with direct evaluation at a number of field points
about 1.15 times the number of surface quadrature nodes.

</details>


### [19] [Tensor-based multivariate function approximation: methods benchmarking and comparison](https://arxiv.org/abs/2506.04791)
*Athanasios C. Antoulas,Ion Victor Gosea,Charles Poussot-Vassal,Pierre Vuillemin*

Main category: math.NA

TL;DR: The paper evaluates methods for tensor-based multivariate function approximation, comparing performance and features without ranking, and introduces a benchmark collection for surrogate models.


<details>
  <summary>Details</summary>
Motivation: To fairly assess and compare various tensor approximation methods, guiding users on their advantages, limits, and practical use.

Method: Construct tensors from diverse multivariate functions, evaluate method performance (accuracy, computational time, etc.), and compare using multiple criteria.

Result: Provides a benchmark collection for surrogate models and detailed insights into the multivariate Loewner Framework (mLF).

Conclusion: The study aims to guide users in understanding tensor approximation tools, emphasizing fairness and practical utility over ranking.

Abstract: In this note, we evaluate the performances, the features and the
user-experience of some methods (and their implementations) designed for
tensor- (or data-) based multivariate function construction and approximation.
To this aim, a collection of multivariate functions extracted from contributive
works coming from different communities, is suggested. First, these functions
with varying complexity (e.g. number and degree of the variables) and nature
(e.g. rational, irrational, differentiable or not, symmetric, etc.) are used to
construct tensors, each of different dimension and size on the disk. Second,
grounded on this tensor, we inspect performances of each considered method
(e.g. the accuracy, the computational time, the parameters tuning impact,
etc.). Finally, considering the "best" parameter tuning set, we compare each
method using multiple evaluation criteria. The purpose of this note is not to
rank the methods but rather to evaluate as fairly as possible the different
available strategies, with the idea in mind to guide users to understand the
process, the possibilities, the advantages and the limits brought by each
tools. The contribution claimed is to suggest a complete benchmark collection
of some available tools for tensor approximation by surrogate models (e.g.
rational functions, networks, etc.). In addition, as contributors of the
multivariate Loewner Framework (mLF) approach (and its side implementation in
MDSPACK), attention and details of the latter are more explicitly given, in
order to provide readers a digest of this contributive work and some details
with simple examples.

</details>


### [20] [Efficient randomized algorithms for the fixed Tucker-rank problem of Tucker decomposition with adaptive shifts](https://arxiv.org/abs/2506.04840)
*Maolin Che,Yimin Wei,Chong Wu,Hong Yan*

Main category: math.NA

TL;DR: The paper introduces fast randomized algorithms for Tucker decomposition using adaptive shifted power iterations, improving efficiency and accuracy over existing methods.


<details>
  <summary>Details</summary>
Motivation: To bridge theoretical advancements with scalable solutions for tensor decomposition, addressing the fixed Tucker-rank problem.

Method: Integration of adaptive shifted power iterations into randomized variants of T-HOSVD and ST-HOSVD, enhancing convergence and reducing power iterations.

Result: Theoretical error bounds show comparable or superior accuracy to deterministic methods, with numerical experiments confirming efficiency and robustness.

Conclusion: The proposed algorithms significantly reduce runtime and approximation error, outperforming state-of-the-art techniques.

Abstract: Randomized numerical linear algebra is proved to bridge theoretical
advancements to offer scalable solutions for approximating tensor
decomposition. This paper introduces fast randomized algorithms for solving the
fixed Tucker-rank problem of Tucker decomposition, through the integration of
adaptive shifted power iterations. The proposed algorithms enhance randomized
variants of truncated high-order singular value decomposition (T-HOSVD) and
sequentially T-HOSVD (ST-HOSVD) by incorporating dynamic shift strategies,
which accelerate convergence by refining the singular value gap and reduce the
number of required power iterations while maintaining accuracy. Theoretical
analyses provide probabilistic error bounds, demonstrating that the proposed
methods achieve comparable or superior accuracy compared to deterministic
approaches. Numerical experiments on synthetic and real-world datasets validate
the efficiency and robustness of the proposed algorithms, showing a significant
decline in runtime and approximation error over state-of-the-art techniques.

</details>


### [21] [Active flux for ideal magnetohydrodynamics: A positivity-preserving scheme with the Godunov-Powell source term](https://arxiv.org/abs/2506.04857)
*Junming Duan,Praveen Chandrashekar,Christian Klingenberg*

Main category: math.NA

TL;DR: The paper introduces a positivity-preserving Active Flux (AF) scheme for ideal magnetohydrodynamics, using the Godunov-Powell source term for divergence-free constraints. It combines conservative and nonconservative methods, employs limiters for positivity, and validates third-order accuracy and shock-capturing.


<details>
  <summary>Details</summary>
Motivation: To develop a high-order, compact finite volume scheme for ideal magnetohydrodynamics that ensures positivity of density and pressure while handling divergence-free constraints effectively.

Method: The AF scheme uses point values at cell interfaces, a quadratic reconstruction for nonconservative terms, and LLF flux vector splitting for point value updates. Positivity is ensured via parametrized and scaling limiters, and a new shock sensor controls oscillations.

Result: Numerical tests confirm third-order accuracy, positivity preservation, and effective shock-capturing. The Godunov-Powell source term's role in controlling divergence error is validated.

Conclusion: The proposed PP AF scheme is effective for ideal magnetohydrodynamics, balancing high-order accuracy, positivity, and divergence control.

Abstract: The Active Flux (AF) is a compact, high-order finite volume scheme that
allows more flexibility by introducing additional point value degrees of
freedom at cell interfaces. This paper proposes a positivity-preserving (PP) AF
scheme for solving the ideal magnetohydrodynamics, where the Godunov-Powell
source term is employed to deal with the divergence-free constraint. For the
evolution of the cell average, apart from the standard conservative finite
volume method for the flux derivative, the nonconservative source term is built
on the quadratic reconstruction in each cell, which maintains the compact
stencil in the AF scheme. For the point value update, the local Lax-Friedrichs
(LLF) flux vector splitting is adopted for the flux derivative, originally
proposed in [Duan, Barsukow, and Klingenberg, SIAM Journal on Scientific
Computing, 47(2), A811--A837, 2025], and a central difference is used to
discretize the divergence in the source term. A parametrized flux limiter and a
scaling limiter are presented to preserve the density and pressure positivity
by blending the AF scheme with the first-order PP LLF scheme with the source
term. To suppress oscillations, a new shock sensor considering the divergence
error is proposed, which is used to compute the blending coefficients for the
cell average. Several numerical tests are conducted to verify the third-order
accuracy, PP property, and shock-capturing ability of the scheme. The key role
of the Godunov-Powell source term and its suitable discretization in
controlling divergence error is also validated.

</details>


### [22] [Numerical analysis for constrained and unconstrained Q-tensor energies for liquid crystals](https://arxiv.org/abs/2506.04880)
*Heiko Gimperlein,Ruma R. Maity*

Main category: math.NA

TL;DR: A finite element framework for 3D Landau-de Gennes Q-tensor energies in nematic liquid crystals, focusing on anisotropy and eigenvalue constraints.


<details>
  <summary>Details</summary>
Motivation: To model realistic nematic liquid crystals by addressing anisotropy and physical constraints on Q-tensor eigenvalues.

Method: Finite element approximation for nonlinear elliptic PDEs with non-homogeneous boundary conditions, using the Ball-Majumdar potential and Newton-Kantorovich theorem.

Result: Proves well-posedness, existence, and local uniqueness of discrete solutions, with optimal convergence rates in energy norm.

Conclusion: The framework effectively models nematic liquid crystals, with eigenvalue constraints impacting error analysis.

Abstract: This paper introduces a comprehensive finite element approximation framework
for three-dimensional Landau-de Gennes $Q$-tensor energies for nematic liquid
crystals, with a particular focus on the anisotropy of the elastic energy and
the Ball-Majumdar singular potential. This potential imposes essential physical
constraints on the eigenvalues of the $Q$-tensor, ensuring realistic modeling.
We address the approximation of regular solutions to nonlinear elliptic partial
differential equations with non-homogeneous boundary conditions associated with
Landau-de Gennes energies. The well-posedness of the discrete linearized
problem is rigorously demonstrated. The existence and local uniqueness of the
discrete solution is derived using the Newton-Kantorovich theorem. Furthermore,
we demonstrate an optimal order convergence rate in the energy norm and discuss
the impact of eigenvalue constraints on the a priori error analysis.

</details>


### [23] [Probability of Collision with Tethered Spacecraft](https://arxiv.org/abs/2506.04969)
*Yema Paul,Emmanuel Delande,Francois Vinet,Francois Laporte,Manuel Sanjurjo-Rivo,Aldo Tonnini,Joan-Pau Sanchez*

Main category: math.NA

TL;DR: A method for estimating collision probability for tethered spacecraft, improving accuracy by accounting for tether extent and shape uncertainty.


<details>
  <summary>Details</summary>
Motivation: Standard collision probability methods are overly conservative for tethered systems due to spherical hard-body assumptions.

Method: Maximizes collision probability over all physically admissible tether shapes, considering spatial extent and configuration uncertainty.

Result: Applied to real-world events, the method provides more realistic risk estimates for kilometer-scale tethers.

Conclusion: Enhances collision avoidance decisions by better distinguishing hazardous from benign encounters.

Abstract: This Engineering Note addresses the challenge of estimating the probability
of collision for tethered spacecraft during close encounters with other space
objects. Standard probability of collision methods, based on spherical
hard-body assumptions, tend to be overly conservative when applied to long
tether systems. We introduce a method that accounts for the tether's spatial
extent and configuration uncertainty by maximizing the probability of collision
over all physically admissible tether shapes. Applied to real-world conjunction
events involving a kilometer-scale flexible inextensible tether, the method
yields more realistic risk estimates. This approach improves the ability to
distinguish hazardous from benign encounters, thereby supporting more informed
collision avoidance decisions.

</details>


### [24] [Norming Sets for Tensor and Polynomial Sketching](https://arxiv.org/abs/2506.05174)
*Yifan Zhang,Joe Kileel*

Main category: math.NA

TL;DR: The paper introduces a sketching framework for real algebraic varieties and polynomial maps, proposing a new method called the median sketch that uses efficient measurements.


<details>
  <summary>Details</summary>
Motivation: To develop a generalized theory for sketching (randomized dimension reduction) applicable to real algebraic varieties and polynomial maps, including low-rank tensors and tensor networks.

Method: Uses norming set theory to control sketching dimensions for various sketch operators (sub-Gaussian, fast Johnson-Lindenstrauss, tensor structured). Introduces the median sketch method, which requires only ~O(dim V) measurements.

Result: The median sketch efficiently embeds sets like low-rank tensors using sparse or tensor-structured linear measurements.

Conclusion: The framework and median sketch provide a versatile and efficient approach to sketching for algebraic varieties and polynomial maps.

Abstract: This paper develops the sketching (i.e., randomized dimension reduction)
theory for real algebraic varieties and images of polynomial maps, including,
e.g., the set of low rank tensors and tensor networks. Through the lens of
norming sets, we provide a framework for controlling the sketching dimension
for \textit{any} sketch operator used to embed said sets, including
sub-Gaussian, fast Johnson-Lindenstrauss, and tensor structured sketch
operators. Leveraging norming set theory, we propose a new sketching method
called the median sketch. It embeds such a set $V$ using only
$\widetilde{\mathcal{O}}(\dim V)$ tensor structured or sparse linear
measurements.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [25] [Inverse elastic obstacle scattering problems by monotonicity method](https://arxiv.org/abs/2506.04655)
*Mengjiao Bai,Huaian Diao,Weisheng Zhou*

Main category: math.AP

TL;DR: A monotonicity-based method is proposed to uniquely identify the shape and position of rigid elastic obstacles using far-field measurements, without initial guesses or prior knowledge.


<details>
  <summary>Details</summary>
Motivation: To solve the inverse problem of reconstructing rigid obstacles in elastic wave scattering using far-field data.

Method: Develops a monotonicity-based approach, factorizes the far-field operator, and uses localized wave functions to derive a shape characterization criterion.

Result: The method uniquely identifies the shape and location of obstacles without initial guesses or background medium knowledge.

Conclusion: The novel approach effectively solves the inverse problem for rigid elastic obstacles.

Abstract: We consider the elastic wave scattering problem involving rigid obstacles.
This work addresses the inverse problem of reconstructing the position and
shape of such obstacles using far-field measurements. A novel
monotonicity-based approach is developed for this purpose. By factorizing the
far-field operator and utilizing the existence of localized wave functions, we
derive a shape characterization criterion for the obstacle boundary. The
proposed method employs monotonicity tests to determine the geometric
relationship between any given test domain and the actual scatterer. As a
result, the shape and location of rigid elastic obstacles can be uniquely
identified without requiring any initial guesses or prior knowledge of the
physical parameters of the homogeneous background medium.

</details>


### [26] [Inverse elastic obstacle scattering problems by monotonicity method](https://arxiv.org/abs/2506.04655)
*Mengjiao Bai,Huaian Diao,Weisheng Zhou*

Main category: math.AP

TL;DR: A novel monotonicity-based method is proposed for reconstructing rigid elastic obstacles from far-field measurements, uniquely identifying their shape and location without prior knowledge.


<details>
  <summary>Details</summary>
Motivation: The study aims to solve the inverse problem of determining the position and shape of rigid obstacles in elastic wave scattering using far-field data.

Method: The approach involves factorizing the far-field operator and using localized wave functions to derive a shape characterization criterion. Monotonicity tests are applied to relate test domains to the actual scatterer.

Result: The method uniquely identifies the shape and location of obstacles without initial guesses or background medium parameters.

Conclusion: The proposed monotonicity-based approach effectively reconstructs rigid elastic obstacles from far-field measurements, offering a robust solution without prior assumptions.

Abstract: We consider the elastic wave scattering problem involving rigid obstacles.
This work addresses the inverse problem of reconstructing the position and
shape of such obstacles using far-field measurements. A novel
monotonicity-based approach is developed for this purpose. By factorizing the
far-field operator and utilizing the existence of localized wave functions, we
derive a shape characterization criterion for the obstacle boundary. The
proposed method employs monotonicity tests to determine the geometric
relationship between any given test domain and the actual scatterer. As a
result, the shape and location of rigid elastic obstacles can be uniquely
identified without requiring any initial guesses or prior knowledge of the
physical parameters of the homogeneous background medium.

</details>


### [27] [Existence, uniqueness and blow-up estimates for a reaction-diffusion equation with $p(x,t)$-exponents](https://arxiv.org/abs/2506.04498)
*Nguyen Thanh Tung,Le Xuan Truong,Tan Duc Do,Nguyen Ngoc Trong*

Main category: math.AP

TL;DR: The paper studies the existence, uniqueness, and blow-up time bounds of weak solutions to a reaction-diffusion parabolic problem with variable exponent $p(x,t)$ in a bounded domain.


<details>
  <summary>Details</summary>
Motivation: The motivation is to analyze the behavior of solutions to a parabolic PDE with non-standard diffusion and reaction terms, particularly focusing on the variable exponent $p(x,t)$ and its impact on solution properties.

Method: The authors consider a reaction-diffusion problem with a variable exponent $p(x,t)$ and use functional analysis techniques to investigate weak solutions. They derive bounds for the blow-up time of these solutions.

Result: The paper establishes the existence and uniqueness of weak solutions under certain conditions on $p(x,t)$. It also provides upper and lower bounds for the blow-up time of the solutions.

Conclusion: The study concludes that the variable exponent $p(x,t)$ significantly influences the solution's behavior, including its blow-up time, and provides explicit bounds for this phenomenon.

Abstract: Let $d \in \{3,4,5,\ldots\}$ and $\Omega \subset \Ri^d$ be open bounded with
Lipschitz boundary.
  Let $Q = \Omega \times (0,\infty)$ and $p \in C(\overline{Q})$ be such that
  \[
  2 < p^- \le p(\cdot) \le p^+ < 2^* := \frac{2d}{d-2},
  \]
  where
  $
  p^- := \essinf_{(x,t) \in Q} p(x,t)
  $
  and
  $
  p^+ := \esssup_{(x,t) \in Q} p(x,t).
  $
  Consider the reaction-diffusion parabolic problem
  \[
  (P) \quad \left\{\begin{array}{ll}
  \displaystyle\frac{u_t}{|x|^2} - \Delta u = k(t) \, |u|^{p(x,t)-2}u & (x,t)
\in \Omega \times (0,T),
  u(x,t) = 0, & (x,t) \in \partial \Omega \times (0,T), \smallskip
  u(x,0) = u_0(x), & x \in \Omega,
  \end{array}\right.
  \]
  where $T > 0$ and $0 \ne u_0 \in W^{1,2}_0(\Omega)$.
  We investigate the existence and uniqueness of a weak solution to $(P)$.
  The upper and lower bounds on the blow-up time of the weak solution are also
considered.

</details>


### [28] [Structural stability of three dimensional steady Prandtl equation](https://arxiv.org/abs/2506.04578)
*Weiming Shen,Yue Wang,Tong Yang*

Main category: math.AP

TL;DR: This paper addresses the open problem of the well-posedness of the 3D Prandtl equation in the steady case, focusing on structural stability of background profiles like Blasius solutions, using intrinsic vector fields and new maximum principles.


<details>
  <summary>Details</summary>
Motivation: The well-posedness of the 3D Prandtl equation is a major unsolved problem, highlighted as the third open problem in Oleinik and Samokhin's book. The study aims to tackle this in the steady case.

Method: The authors introduce intrinsic vector fields and new versions of maximum principles, analyzing curvature-type quantities from commutators of these fields to address symmetry-breaking challenges.

Result: The paper overcomes difficulties caused by symmetry breaking, providing insights into the structural stability of background profiles.

Conclusion: The new approach offers progress on the open problem, particularly in the steady case, by leveraging intrinsic vector fields and curvature analysis.

Abstract: The well-posedness of the three dimensional Prandtl equation is an
outstanding open problem due to the appearance of the secondary flow even
though there are studies on analytic and Gevrey function spaces. This problem
is raised as the third open problem in the classical book by Oleinik and
Samokhin [43]. This paper aims to address this open problem in the steady case
by introducing a new approach to study the structural stability of background
profile that includes the famous Blasius solutions. The key observations
include the introduction of some intrinsic vector fields and new versions of
maximum principle. In particular, we overcome the difficulties caused by
symmetry breaking through the analysis on the curvature-type quantities
generated by commutators of the vector fields.

</details>


### [29] [Asymptotic behavior of complete conformal metric near singular boundary](https://arxiv.org/abs/2506.04591)
*Weiming Shen,Yue Wang*

Main category: math.AP

TL;DR: The paper studies the asymptotic behavior of solutions to the singular Yamabe problem near singular boundaries, deriving optimal estimates for non-conformally flat metrics.


<details>
  <summary>Details</summary>
Motivation: Little is known about the asymptotic behavior of solutions near singular boundaries, unlike smooth boundaries.

Method: Analyze solutions to the singular Yamabe problem with negative constant scalar curvature near singular boundaries.

Result: Solutions are well approximated by solutions in tangent cones at singular boundary points.

Conclusion: The study provides optimal estimates for non-conformally flat metrics and clarifies the behavior near singular boundaries.

Abstract: The boundary behavior of the singular Yamabe problem has been extensively
studied near sufficiently smooth boundaries, while less is known about the
asymptotic behavior of solutions near singular boundaries. In this paper, we
study the asymptotic behaviors of solutions to the singular Yamabe problem with
negative constant scalar curvature near singular boundaries and derive the
optimal estimates for the background metric which is not necessarily
conformally flat. In particular, we prove that the solutions are well
approximated by the solutions in tangent cones at singular points on the
boundaries.

</details>


### [30] [Solutions with expanding compact support of saturated Schr{ö}dinger equations: self-similar solutions](https://arxiv.org/abs/2506.04691)
*Pascal Bégout,Jesus Ildefonso Diaz*

Main category: math.AP

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We prove the existence of solutions \(u(t,x)\) of the Schr{\"o}dinger
equation with a saturation nonlinear term \((u/|u|)\) having compact support,
for each \(t>0,\) that expands with a growth law of the type \(C\sqrt{t}\). The
primary tool is considering the self-similar solution of the associated
equation. For more information see
https://ejde.math.txstate.edu/Volumes/2025/53/abstr.html

</details>


### [31] [Lipschitz stability for Bayesian inference in porous medium tissue growth models](https://arxiv.org/abs/2506.04769)
*Tomasz Dębiec,Piotr Gwiazda,Błażej Miasojedow,Katarzyna Ryszewska,Zuzanna Szymańska,Aneta Wróblewska-Kamińska*

Main category: math.AP

TL;DR: The paper analyzes a macroscopic model for living tissue dynamics, focusing on pressure-driven dispersal and proliferation. It proves Lipschitz continuity of solutions with respect to a diffusion parameter, aiding Bayesian parameter estimation.


<details>
  <summary>Details</summary>
Motivation: The study aims to support Bayesian inverse problem methodology for parameter estimation in experimental data, requiring stability estimates for sampling algorithms.

Method: The model uses a porous medium equation with a growth term, assuming a power-law relation between pressure and cell density. Lipschitz continuity of solutions is proven in the $L_1$ norm.

Result: The paper establishes Lipschitz continuity of mild solutions with respect to the diffusion parameter, a key step for gradient-based sampling algorithms.

Conclusion: The stability result is crucial for applying Bayesian methods to estimate model parameters from experimental data, enhancing the model's practical utility.

Abstract: We consider a macroscopic model for the dynamics of living tissues
incorporating pressure-driven dispersal and pressure-modulated proliferation.
Given a power-law constitutive relation between the pressure and cell density,
the model can be written as a porous medium equation with a growth term. We
prove Lipschitz continuity of the mild solutions of the model with respect to
the diffusion parameter (the exponent $\gamma$ in the pressure-density law) in
the $L_1$ norm. While of independent analytical interest, our motivation for
this result is to provide a vital step towards using Bayesian inverse problem
methodology for parameter estimation based on experimental data -- such
stability estimates are indispensable for applying sampling algorithms which
rely on the gradient of the likelihood function.

</details>


### [32] [Periodic solutions for p(t)-Lienard equations with a singular nonlinearity of attractive type](https://arxiv.org/abs/2506.04927)
*Petru Jebelean,Jean Mawhin,Calin Serban*

Main category: math.AP

TL;DR: The paper investigates the existence of T-periodic solutions for a singular differential equation with a p(t)-Laplacian operator, using continuation theorems and a priori estimates.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the need to understand periodic solutions in singular differential equations, particularly those with attractive singularities, which have applications in physics and engineering.

Method: The approach combines a continuation theorem from prior work, a priori estimates, and the method of lower and upper solutions to analyze the equation.

Result: The paper establishes conditions under which T-periodic solutions exist for the given singular differential equation.

Conclusion: The findings confirm the existence of periodic solutions for the equation, leveraging analytical tools and prior theoretical results.

Abstract: We are concerned with the existence of $T$-periodic solutions to an equation
of type $$\left (|u'(t))|^{p(t)-2} u'(t) \right
)'+f(u(t))u'(t)+g(u(t))=h(t)\quad \mbox{ in }[0,T]$$ where
$p:[0,T]\to(1,\infty)$ with $p(0)=p(T)$ and $h$ are continuous on $[0,T]$,
$f,g$ are also continuous on $[0,\infty)$, respectively $(0,\infty)$. The
mapping $g$ may have an attractive singularity (i.e. $g(x) \to +\infty$ as
$x\to 0+$). Our approach relies on a continuation theorem obtained in the
recent paper M. Garc\'{i}a-Huidobro, R. Man\'{a}sevich, J. Mawhin and S.
Tanaka, J. Differential Equations (2024), a priori estimates and method of
lower and upper solutions.

</details>


### [33] [Existence and multiplicity of normalized solutions for the generalized Kadomtsev-Petviashvili equation in $\mathbb{R}^2$](https://arxiv.org/abs/2506.04967)
*Claudianor O. Alves,Rui Ding,Chao Ji*

Main category: math.AP

TL;DR: The paper investigates the existence and multiplicity of nontrivial solitary waves for the generalized Kadomtsev-Petviashvili equation under an L²-norm constraint, proving normalized ground state solutions for subcritical and supercritical cases, and a second solution for specific conditions.


<details>
  <summary>Details</summary>
Motivation: To address the lack of research on normalized solutions for the generalized Kadomtsev-Petviashvili equation under L²-constraints, exploring existence and multiplicity of solutions.

Method: Analyzes the equation with prescribed L²-norm, using Lagrange multipliers and energy functional methods to establish solutions for subcritical and supercritical cases, and identifies a second solution under specific conditions.

Result: Existence of normalized ground state solutions proven for subcritical and supercritical cases. A second solution with positive energy identified for a sequence of norms approaching zero.

Conclusion: The study successfully establishes the existence of normalized solutions for the generalized Kadomtsev-Petviashvili equation, filling a gap in the literature and providing new insights into the problem.

Abstract: In this paper, we study the existence and {multiplicity} of nontrivial
solitary waves for the generalized Kadomtsev-Petviashvili equation with
prescribed {$L^2$-norm} \begin{equation*}\label{Equation1}
  \left\{\begin{array}{l}
  \left(-u_{x x}+D_x^{-2} u_{y y}+\lambda u-f(u)\right)_x=0,{\quad x \in
\mathbb{R}^2, } \\[10pt]
  \displaystyle \int_{\mathbb{R}^2}u^2 d x=a^2,
  \end{array}\right.%\tag{$\mathscr E_\lambda$} \end{equation*} where $a>0$ and
$\lambda \in \mathbb{R}$ is an unknown parameter that appears as a Lagrange
multiplier. For the case $f(t)=|t|^{q-2}t$, with $2<q<\frac{10}{3}$
($L^2$-subcritical case) and $\frac{10}{3}<q<6$ ($L^2$-supercritical case), we
establish the existence of normalized ground state solutions for the above
equation. Moreover, when $f(t)=\mu|t|^{q-2}t+|t|^{p-2}t$, with
$2<q<\frac{10}{3}<p<6$ and $\mu>0$, we prove the existence of normalized ground
state solutions which corresponds to a local minimum of the associated energy
functional. In this case, we further show that there exists a sequence $(a_n)
\subset (0,a_0)$ with $a_n \to 0$ as $n \to+\infty$, such that for each
$a=a_n$, the problem admits a second solution with positive energy. To the best
of our knowledge, this is the first work that studies the existence of
solutions for the generalized Kadomtsev-Petviashvili equations under the
$L^2$-constraint, which we refer to them as the normalized solutions.

</details>


### [34] [Remarks on radial symmetry of stationary and uniformly-rotating solutions for the 2D Euler equation](https://arxiv.org/abs/2506.05034)
*Boquan Fan,Yuchen Wang,Weicheng Zhan*

Main category: math.AP

TL;DR: The paper proves radial symmetry for uniformly rotating solutions of the 2D Euler equation with compact vorticity under specific angular velocity conditions, extending prior work to include irregular patches.


<details>
  <summary>Details</summary>
Motivation: To generalize earlier rigidity theorems by removing restrictions on angular velocity and vorticity sign, and to include irregular vortex patches.

Method: Analyzes uniformly rotating solutions of the 2D incompressible Euler equation, focusing on compactly supported vorticity and specific angular velocity ranges.

Result: Shows that such solutions must be radially symmetric under the given conditions, even for irregular patches.

Conclusion: Extends previous results by relaxing constraints and broadening applicability to irregular cases.

Abstract: We prove that any uniformly rotating solution of the 2D incompressible Euler
equation with compactly supported vorticity $\omega$ must be radially symmetric
whenever its angular velocity satisfies $\Omega \in (-\infty,\inf \omega / 2]
\cup \, [ \sup \omega / 2, +\infty )$, in both the patch and smooth settings.
This result extends the rigidity theorems established in
\cite{Gom2021MR4312192} (\textit{Duke Math. J.},170(13):2957-3038, 2021), which
were confined to the case of non-positive angular velocities and non-negative
vorticity. Moreover, our results do not impose any regularity conditions on the
patch beyond requiring that its boundary consists of Jordan curves, thereby
refining the previous result to encompass irregular vortex patches.

</details>


### [35] [Existence of infinitely many homotopy classes from $\mathbb S^3$ to $\mathbb S^2$ having a minimimzing $W^{s,\frac 3s}$-harmonic map](https://arxiv.org/abs/2506.05060)
*Adam Grzela,Katarzyna Mazowiecka*

Main category: math.AP

TL;DR: The paper extends T. Rivière's 1998 result on minimizing 3-harmonic maps in π₃(S²) to fractional harmonic maps, showing infinitely many homotopy classes with minimizers for s ∈ (0,1).


<details>
  <summary>Details</summary>
Motivation: To generalize the surprising result of infinitely many minimizing homotopy classes in π₃(S²) to the fractional setting, contrasting with the limited cases in π₃(S³).

Method: Extends the framework to fractional harmonic maps, specifically W^{s,3/s}-harmonic maps, for s ∈ (0,1).

Result: Proves existence of infinitely many homotopy classes in π₃(S²) with minimizing fractional harmonic maps for s ∈ (0,1).

Conclusion: The study successfully generalizes Rivière's theorem to fractional harmonic maps, revealing rich structure in π₃(S²).

Abstract: In 1998 T. Rivi\`{e}re proved that there exist infinitely many homotopy
classes of $\pi_3(\mathbb S^2)$ having a minimizing 3-harmonic map. This result
is especially surprising taking into account that in $\pi_3(\mathbb S^3)$ there
are only three homotopy classes (corresponding to the degrees $\{-1,0,1\}$) in
which a minimizer exists.
  We extend this theorem in the framework of fractional harmonic maps and prove
that for $s\in(0,1)$ there exist infinitely many homotopy classes of
$\pi_{3}(\mathbb S^{2})$ in which there is a minimizing
$W^{s,\frac{3}{s}}$-harmonic map.

</details>


### [36] [Global well-posedness for the ILW equation in $H^s(\mathbb{T})$ for $s>-\frac12$](https://arxiv.org/abs/2506.05149)
*Louise Gassot,Thierry Laurens*

Main category: math.AP

TL;DR: The paper proves global well-posedness of the ILW equation in Sobolev spaces for $s > -\frac12$, improves the previous record ($s\geq 0$), and shows convergence to the Benjamin--Ono equation in the infinite-depth limit.


<details>
  <summary>Details</summary>
Motivation: To extend the well-posedness theory of the ILW equation beyond the known threshold ($s\geq 0$) and explore its relationship with the Benjamin--Ono equation.

Method: Treats ILW as a perturbation of the Benjamin--Ono equation by a linear term of order zero, avoiding reliance on integrability. A general well-posedness result for such perturbations is established.

Result: Global well-posedness is achieved for $s > -\frac12$, and solutions converge to the Benjamin--Ono equation in the infinite-depth limit.

Conclusion: The approach provides a framework for analyzing perturbations of integrable systems, with applications to other equations like the Smith equation.

Abstract: We prove that the intermediate long wave (ILW) equation is globally
well-posed in the Sobolev spaces $H^s(\mathbb{T})$ for $s > -\frac12$. The
previous record for well-posedness was $s\geq 0$, and the system is known to be
ill-posed for $s<-\frac12$. We then demonstrate that the solutions of ILW
converge to those of the Benjamin--Ono equation in $H^s(\mathbb{T})$ in the
infinite-depth limit.
  Our methods do not rely on the complete integrability of ILW, but rather
treat ILW as a perturbation of the Benjamin--Ono equation by a linear term of
order zero. To highlight this, we establish a general well-posedness result for
such perturbations, which also applies to the Smith equation for
continental-shelf waves.

</details>


### [37] [Boundary regularity for subelliptic equations in the Heisenberg group](https://arxiv.org/abs/2506.05151)
*Farhan Abedin,Giulio Tralli*

Main category: math.AP

TL;DR: The paper proves boundary regularity for degenerate elliptic equations in the Heisenberg group, focusing on bounded coefficients and right-hand side, with new results for intrinsic expansions.


<details>
  <summary>Details</summary>
Motivation: To establish regularity results for degenerate elliptic equations in non-divergence form, particularly in the Heisenberg group, addressing gaps in existing literature.

Method: Analyzes solutions using left-invariant vector fields, imposes dimensional and growth conditions, and derives intrinsic expansions near the origin.

Result: Proves Hölder and Lipschitz regularity, with a novel second-order expansion for solutions in weighted $L^{\infty}$ spaces.

Conclusion: The work extends understanding of degenerate elliptic equations, offering new insights for the sub-Laplacian case.

Abstract: We prove boundary H\"older and Lipschitz regularity for a class of degenerate
elliptic, second order, inhomogeneous equations in non-divergence form
structured on the left-invariant vector fields of the Heisenberg group. Our
focus is on the case of operators with bounded and measurable coefficients and
bounded right-hand side; when necessary, we impose a dimensional restriction on
the ellipticity ratio and a growth rate for the source term near characteristic
points of the boundary. For solutions in the characteristic half-space
$\{t>0\}$, we obtain an intrinsic second order expansion near the origin when
the source term belongs to an appropriate weighted $L^{\infty}$ space; this is
a new result even for the frequently studied sub-Laplacian.

</details>


### [38] [The $\mathcal{R}$-boundedness of solution operators for the $Q$-tensor model of nematic liquid crystals](https://arxiv.org/abs/2506.05152)
*Daniele Barbera,Miho Murata*

Main category: math.AP

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this paper, we consider a resolvent problem arising from the $Q$-tensor
model for liquid crystal flows in the half-space. Our purpose is to show the
$\mathcal{R}$-boundedness for the solution operator families of the resolvent
problem when the resolvent parameter lies near the origin. The definition of
the $\mathcal{R}$-solvability implies the uniform boundedness of the operator
and, consequently, the resolvent estimates for the linear system.

</details>


### [39] [Symmetry breaking for local minimizers of a free discontinuity problem](https://arxiv.org/abs/2506.05270)
*Massimo Gobbino,Nicola Picenni*

Main category: math.AP

TL;DR: The paper explores a functional for piecewise constant functions, balancing jump penalization and fidelity to a linear forcing term. In 2D, symmetry breaking leads to exotic minimizers with complex level sets.


<details>
  <summary>Details</summary>
Motivation: To understand how minimizers of a functional with jump penalization and fidelity terms behave in higher dimensions, particularly in 2D.

Method: Uses the calibration method for free discontinuity problems to analyze minimizers.

Result: In 2D, symmetry breaking occurs, producing exotic minimizers with non-simple level sets.

Conclusion: The study reveals unexpected complexity in 2D minimizers, contrasting with simpler 1D staircases.

Abstract: We study a functional defined on the class of piecewise constant functions,
combining a jump penalization, which discourages discontinuities, with a
fidelity term that penalizes deviations from a given linear function, called
the forcing term.
  In one dimension, it is not difficult to see that local minimizers form
staircases that approximate the forcing term. Here we show that in two
dimensions symmetry breaking occurs, leading to the emergence of exotic
minimizers whose level sets are not simple stripes with boundaries orthogonal
to the gradient of the forcing term.
  The proof relies on the calibration method for free discontinuity problems.

</details>


### [40] [On a linear equation arising in the study of phase separation of BEC's](https://arxiv.org/abs/2506.05299)
*Christos Sourdis*

Main category: math.AP

TL;DR: Optimal invertibility estimates for Fourier modes in phase separation of two-component Bose-Einstein condensates, without orthogonality assumptions.


<details>
  <summary>Details</summary>
Motivation: To analyze the phase separation in two-component Bose-Einstein condensates, focusing on linearized systems in an infinite strip with specific boundary conditions.

Method: Linearized the system around a one-dimensional solution, considering zero and periodic boundary conditions, and derived optimal invertibility estimates for Fourier modes.

Result: Achieved optimal invertibility estimates without requiring orthogonality conditions.

Conclusion: The study provides rigorous estimates for Fourier modes in phase separation, enhancing understanding of such systems.

Abstract: We consider the inner limit system describing the phase separation in
two-component Bose-Einstein condensates linearized around the one-dimensional
solution in an infinite strip with zero and periodic boundary conditions, and
obtain optimal invertibility estimates for the Fourier modes without
necessarily assuming orthogonality conditions.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [41] [BridgeNet: A Hybrid, Physics-Informed Machine Learning Framework for Solving High-Dimensional Fokker-Planck Equations](https://arxiv.org/abs/2506.04354)
*Elmira Mirzabeigi,Rezvan Salehi,Kourosh Parand*

Main category: physics.comp-ph

TL;DR: BridgeNet combines CNNs and physics-informed neural networks to solve complex Fokker-Planck equations more efficiently than traditional PINNs, achieving better accuracy and stability.


<details>
  <summary>Details</summary>
Motivation: Traditional PINNs struggle with spatial hierarchies and boundary conditions in high-dimensional FPEs, prompting the need for a hybrid approach.

Method: BridgeNet uses adaptive CNN layers for local feature extraction and a dynamically weighted loss function to enforce physical constraints.

Result: BridgeNet outperforms conventional PINNs with lower errors, faster convergence, and robust stability in high-dimensional settings.

Conclusion: BridgeNet advances computational physics, offering scalable and accurate solutions for applications in finance and complex systems.

Abstract: BridgeNet is a novel hybrid framework that integrates convolutional neural
networks with physics-informed neural networks to efficiently solve non-linear,
high-dimensional Fokker-Planck equations (FPEs). Traditional PINNs, which
typically rely on fully connected architectures, often struggle to capture
complex spatial hierarchies and enforce intricate boundary conditions. In
contrast, BridgeNet leverages adaptive CNN layers for effective local feature
extraction and incorporates a dynamically weighted loss function that
rigorously enforces physical constraints. Extensive numerical experiments
across various test cases demonstrate that BridgeNet not only achieves
significantly lower error metrics and faster convergence compared to
conventional PINN approaches but also maintains robust stability in
high-dimensional settings. This work represents a substantial advancement in
computational physics, offering a scalable and accurate solution methodology
with promising applications in fields ranging from financial mathematics to
complex system dynamics.

</details>


### [42] [Spectrally accurate and efficient convolution with the 3D free-space Laplace Green's function via the super-potential](https://arxiv.org/abs/2506.04489)
*Lukas Exl,Sebastian Schaffer*

Main category: physics.comp-ph

TL;DR: A high-accuracy spectral method for solving the 3D Poisson equation with smooth, compactly supported sources, using a super-potential formulation and Gaussian-sum approximation for efficient FFT-based computation.


<details>
  <summary>Details</summary>
Motivation: To address the need for a robust and efficient method for solving unbounded 3D Poisson equations with high accuracy and quasi-linear complexity.

Method: Uses a super-potential formulation involving the biharmonic Green's function and a separable Gaussian-sum approximation for FFT-based computation.

Result: Achieves machine-precision accuracy, outperforms existing Gaussian-sum-based schemes in error and runtime, and eliminates the need for correction terms.

Conclusion: The method is a robust and efficient tool for free-space Poisson problems on uniform grids.

Abstract: We present a high-accuracy spectral method for solving the unbounded
three-dimensional Poisson equation with smooth, compactly supported sources.
The approach is based on a super-potential formulation, where the solution is
obtained by applying the Laplacian to a convolution with the biharmonic Green's
function. A separable Gaussian-sum (GS) approximation enables efficient
FFT-based computation with quasi-linear complexity. Owing to the improved
regularity of the biharmonic kernel, the GS cutoff error is of order four,
eliminating the need for correction terms or Taylor expansions required in
standard GS or Ewald-type methods. Numerical benchmarks demonstrate that the
method achieves machine-precision accuracy and outperforms existing GS-based
schemes in both error and runtime, making it a robust and efficient tool for
free-space Poisson problems on uniform grids.

</details>


### [43] [A highly scalable numerical framework for reservoir simulation on UG4 platform](https://arxiv.org/abs/2506.04763)
*Shuai Lu*

Main category: physics.comp-ph

TL;DR: The paper presents a fully coupled, fully implicit framework for simulating multiphase flow in heterogeneous porous media, using advanced numerical methods and demonstrating scalability on supercomputers.


<details>
  <summary>Details</summary>
Motivation: Existing time discretization schemes for multiphase flow often decouple saturation and pressure equations, limiting accuracy. This study aims to improve modeling by fully coupling these equations and incorporating gravity and capillary effects.

Method: The study employs the Vertex-Centered Finite Volume Method for spatial discretization, introduces the Linearly Implicit Extrapolation Method (LIMEX) with an error estimator, and uses BiCGSTAB with Geometric Multigrid preconditioning for solving linear systems.

Result: The framework shows strong scalability, supporting thousands of processors and billions of Degrees of Freedom (DoF) in parallel computations on a supercomputer.

Conclusion: The proposed fully implicit framework is efficient and scalable, offering improved accuracy for multiphase flow simulations in heterogeneous porous media.

Abstract: The modeling and simulation of multiphase fluid flow receive significant
attention in reservoir engineering. Many time discretization schemes for
multiphase flow equations are either explicit or semi-implicit, relying on the
decoupling between the saturation equation and the pressure equation. In this
study, we delve into a fully coupled and fully implicit framework for
simulating multiphase flow in heterogeneous porous media, considering gravity
and capillary effects. We utilize the Vertex-Centered Finite Volume Method for
spatial discretization and propose an efficient implementation of interface
conditions for heterogeneous porous media within the current scheme. Notably,
we introduce the Linearly Implicit Extrapolation Method (LIMEX) with an error
estimator, adapted for the first time to multiphase flow problems. To solve the
resulting linear system, we employ the BiCGSTAB method with the Geometric
Multigrid (GMG) preconditioner. The implementations of models and methods are
based on the open-source software: UG4. The results from parallel computations
on the supercomputer demonstrate that the scalability of our proposed framework
is sufficient, supporting a scale of thousands of processors with Degrees of
Freedom (DoF) extending up to billions.

</details>


### [44] [Thermoplasmonics of Gold-Core Silica-Shell Colloidal Nanoparticles under Pulse Illumination](https://arxiv.org/abs/2506.04835)
*Julien El Hajj,Gilles Ledoux,Samy Merabia*

Main category: physics.comp-ph

TL;DR: Study explores thermal dynamics of gold-core silica-shell nanoparticles under pulse illumination, revealing faster water heating with thin dense silica shells due to enhanced electron-phonon coupling.


<details>
  <summary>Details</summary>
Motivation: Core-shell nanoparticles, especially gold-core ones, are promising for photothermal therapy, imaging, and biosensing due to their unique properties. Understanding their thermal dynamics is key for optimizing applications.

Method: Combines Mie theory with electronic temperature corrections and couples the two-temperature model with molecular dynamics simulations to study thermal response under varied laser pulses and fluences.

Result: Nanoparticles with thin dense silica shells (5 nm) heat water faster than bare gold nanoparticles, attributed to enhanced electron-phonon coupling and high silica-water thermal conductance.

Conclusion: Findings offer insights for optimizing nanoparticle design for photothermal applications and understanding energy transfer in metal-dielectric nanostructures.

Abstract: Core-shell nanoparticles, particularly those having a gold core, have emerged
as a highly promising class of materials due to their unique optical and
thermal properties, which underpin a wide range of applications in photothermal
therapy, imaging, and biosensing. In this study, we present a comprehensive
study of the thermal dynamics of gold-core silica-shell nanoparticles immersed
in water under pulse illumination. The plasmonic response of the core-shell
nanoparticle is described by incorporating Mie theory with electronic
temperature corrections to the refractive indices of gold, based on a Drude
Lorentz formulation. The thermal response of the core-shell nanoparticles is
modeled by coupling the two temperature model with molecular dynamics
simulations, providing an atomistic description of nanoscale heat transfer. We
investigate nanoparticles with both dense and porous silica shells (with 50%
porosity) under laser pulse durations of 100 fs, 10 ps, and 1 ns, and over a
range of fluences between 0.05 and 5mJ/cm2. We show that nanoparticles with a
thin dense silica shell (5 nm) exhibit significantly faster water heating
compared to bare gold nanoparticles. This behavior is attributed to enhanced
electron-phonon coupling at the gold silica interface and to the relatively
high thermal conductance between silica and water. These findings provide new
insights into optimizing nanoparticle design for efficient photothermal
applications and establish a robust framework for understanding energy transfer
mechanisms in heterogeneous metal dielectric nanostructures.

</details>


### [45] [Reduction of Outflow Boundary Influence on Aerodynamic Performance using Neural Networks](https://arxiv.org/abs/2506.05293)
*Mario Christopher Bedrunka,Dirk Reith,Holger Foysi,Łukasz Łaniewski-Wołłk,Travis Mitchell*

Main category: physics.comp-ph

TL;DR: Neural networks (NN) are used to improve outflow boundary conditions in LBM, reducing artificial reflections and computational costs. Two NN-based methods show superior accuracy in aerodynamic predictions.


<details>
  <summary>Details</summary>
Motivation: Accurate outflow boundary conditions are critical in CFD for predicting aerodynamic forces and acoustic emissions, especially in LBM, which suffers from artificial reflections.

Method: Two NN-based approaches: (1) direct reconstruction of particle distribution functions at outflow boundaries, and (2) dynamic tuning of characteristic boundary conditions (CBC) parameters.

Result: NN-based methods improved predictions for drag, lift, and Strouhal numbers in 2D flow and NACA0012 airfoil simulations. Neural-enhanced CBC minimized density errors in vortex benchmarks.

Conclusion: NN-integrated boundary conditions enhance accuracy and reduce computational costs in LBM simulations for aerodynamics and acoustics.

Abstract: The accurate treatment of outflow boundary conditions remains a critical
challenge in computational fluid dynamics when predicting aerodynamic forces
and/or acoustic emissions. This is particularly evident when employing the
lattice Boltzmann method (LBM) as the numerical solution technique, which often
suffers from inaccuracies induced by artificial reflections from outflow
boundaries. This paper investigates the use of neural networks (NN) to mitigate
these adverse boundary effects and enable truncated domain requirements. Two
distinct NN-based approaches are proposed: (1) direct reconstruction of unknown
particle distribution functions at the outflow boundary; and (2) enhancement of
established characteristic boundary conditions (CBC) by dynamically tuning
their parameters. The direct reconstruction model was trained on data generated
from a 2D flow over a cylindrical obstruction. The drag, lift, and Strouhal
number were used to test the new boundary condition. We analyzed results for
various Reynolds numbers and restricted domain sizes where it demonstrated
significantly improved predictions when compared with the traditional Zou & He
boundary condition. To examine the robustness of the NN-based reconstruction,
the same condition was applied to the simulation of a NACA0012 airfoil, again
providing accurate aerodynamic performance predictions. The neural-enhanced CBC
were evaluated on a 2D convected vortex benchmark and showed superior
performance in minimizing density errors compared to CBCs with fixed
parameters. These findings highlight the potential of NN-integrated boundary
conditions to improve accuracy and reduce computational expense of aerodynamic
and acoustic emissions simulations with the LBM.

</details>


### [46] [BridgeNet: A Hybrid, Physics-Informed Machine Learning Framework for Solving High-Dimensional Fokker-Planck Equations](https://arxiv.org/abs/2506.04354)
*Elmira Mirzabeigi,Rezvan Salehi,Kourosh Parand*

Main category: physics.comp-ph

TL;DR: BridgeNet combines CNNs and PINNs to solve high-dimensional FPEs more efficiently than traditional PINNs, achieving lower errors and faster convergence.


<details>
  <summary>Details</summary>
Motivation: Traditional PINNs struggle with complex spatial hierarchies and boundary conditions, prompting the need for a hybrid approach.

Method: BridgeNet uses adaptive CNN layers for local feature extraction and a dynamically weighted loss function to enforce physical constraints.

Result: BridgeNet outperforms conventional PINNs with lower error metrics, faster convergence, and robust stability in high-dimensional settings.

Conclusion: BridgeNet advances computational physics with a scalable, accurate solution for applications in finance and complex systems.

Abstract: BridgeNet is a novel hybrid framework that integrates convolutional neural
networks with physics-informed neural networks to efficiently solve non-linear,
high-dimensional Fokker-Planck equations (FPEs). Traditional PINNs, which
typically rely on fully connected architectures, often struggle to capture
complex spatial hierarchies and enforce intricate boundary conditions. In
contrast, BridgeNet leverages adaptive CNN layers for effective local feature
extraction and incorporates a dynamically weighted loss function that
rigorously enforces physical constraints. Extensive numerical experiments
across various test cases demonstrate that BridgeNet not only achieves
significantly lower error metrics and faster convergence compared to
conventional PINN approaches but also maintains robust stability in
high-dimensional settings. This work represents a substantial advancement in
computational physics, offering a scalable and accurate solution methodology
with promising applications in fields ranging from financial mathematics to
complex system dynamics.

</details>


### [47] [Spectrally accurate and efficient convolution with the 3D free-space Laplace Green's function via the super-potential](https://arxiv.org/abs/2506.04489)
*Lukas Exl,Sebastian Schaffer*

Main category: physics.comp-ph

TL;DR: A high-accuracy spectral method for solving the 3D Poisson equation with smooth, compactly supported sources, using a super-potential formulation and Gaussian-sum approximation for efficient FFT-based computation.


<details>
  <summary>Details</summary>
Motivation: To address the need for a robust and efficient method for solving unbounded 3D Poisson equations with high accuracy and computational efficiency.

Method: Uses a super-potential formulation with a biharmonic Green's function and a separable Gaussian-sum approximation for FFT-based computation.

Result: Achieves machine-precision accuracy and outperforms existing Gaussian-sum-based methods in error and runtime.

Conclusion: The method is a robust and efficient tool for free-space Poisson problems on uniform grids.

Abstract: We present a high-accuracy spectral method for solving the unbounded
three-dimensional Poisson equation with smooth, compactly supported sources.
The approach is based on a super-potential formulation, where the solution is
obtained by applying the Laplacian to a convolution with the biharmonic Green's
function. A separable Gaussian-sum (GS) approximation enables efficient
FFT-based computation with quasi-linear complexity. Owing to the improved
regularity of the biharmonic kernel, the GS cutoff error is of order four,
eliminating the need for correction terms or Taylor expansions required in
standard GS or Ewald-type methods. Numerical benchmarks demonstrate that the
method achieves machine-precision accuracy and outperforms existing GS-based
schemes in both error and runtime, making it a robust and efficient tool for
free-space Poisson problems on uniform grids.

</details>


### [48] [A highly scalable numerical framework for reservoir simulation on UG4 platform](https://arxiv.org/abs/2506.04763)
*Shuai Lu*

Main category: physics.comp-ph

TL;DR: The paper presents a fully coupled, fully implicit framework for simulating multiphase flow in heterogeneous porous media, using advanced numerical methods and demonstrating scalability on supercomputers.


<details>
  <summary>Details</summary>
Motivation: Existing time discretization schemes for multiphase flow often rely on decoupling saturation and pressure equations, limiting accuracy and efficiency.

Method: The study employs a Vertex-Centered Finite Volume Method, introduces the LIMEX with an error estimator, and uses BiCGSTAB with GMG preconditioner for solving linear systems.

Result: The framework shows scalability on supercomputers, handling billions of DoF across thousands of processors.

Conclusion: The proposed fully implicit framework is efficient and scalable, advancing multiphase flow simulation in reservoir engineering.

Abstract: The modeling and simulation of multiphase fluid flow receive significant
attention in reservoir engineering. Many time discretization schemes for
multiphase flow equations are either explicit or semi-implicit, relying on the
decoupling between the saturation equation and the pressure equation. In this
study, we delve into a fully coupled and fully implicit framework for
simulating multiphase flow in heterogeneous porous media, considering gravity
and capillary effects. We utilize the Vertex-Centered Finite Volume Method for
spatial discretization and propose an efficient implementation of interface
conditions for heterogeneous porous media within the current scheme. Notably,
we introduce the Linearly Implicit Extrapolation Method (LIMEX) with an error
estimator, adapted for the first time to multiphase flow problems. To solve the
resulting linear system, we employ the BiCGSTAB method with the Geometric
Multigrid (GMG) preconditioner. The implementations of models and methods are
based on the open-source software: UG4. The results from parallel computations
on the supercomputer demonstrate that the scalability of our proposed framework
is sufficient, supporting a scale of thousands of processors with Degrees of
Freedom (DoF) extending up to billions.

</details>


### [49] [Thermoplasmonics of Gold-Core Silica-Shell Colloidal Nanoparticles under Pulse Illumination](https://arxiv.org/abs/2506.04835)
*Julien El Hajj,Gilles Ledoux,Samy Merabia*

Main category: physics.comp-ph

TL;DR: Study explores thermal dynamics of gold-core silica-shell nanoparticles under pulse illumination, revealing faster water heating with thin dense silica shells compared to bare gold nanoparticles.


<details>
  <summary>Details</summary>
Motivation: Core-shell nanoparticles, especially gold-core ones, have unique optical and thermal properties for applications like photothermal therapy, imaging, and biosensing. Understanding their thermal dynamics can optimize their design.

Method: Combines Mie theory with electronic temperature corrections for plasmonic response and couples the two-temperature model with molecular dynamics for thermal response. Tests dense and porous silica shells under varying laser pulses and fluences.

Result: Nanoparticles with thin dense silica shells (5 nm) heat water faster than bare gold nanoparticles due to enhanced electron-phonon coupling and high silica-water thermal conductance.

Conclusion: Findings offer insights for optimizing nanoparticle design for photothermal applications and understanding energy transfer in metal-dielectric nanostructures.

Abstract: Core-shell nanoparticles, particularly those having a gold core, have emerged
as a highly promising class of materials due to their unique optical and
thermal properties, which underpin a wide range of applications in photothermal
therapy, imaging, and biosensing. In this study, we present a comprehensive
study of the thermal dynamics of gold-core silica-shell nanoparticles immersed
in water under pulse illumination. The plasmonic response of the core-shell
nanoparticle is described by incorporating Mie theory with electronic
temperature corrections to the refractive indices of gold, based on a Drude
Lorentz formulation. The thermal response of the core-shell nanoparticles is
modeled by coupling the two temperature model with molecular dynamics
simulations, providing an atomistic description of nanoscale heat transfer. We
investigate nanoparticles with both dense and porous silica shells (with 50%
porosity) under laser pulse durations of 100 fs, 10 ps, and 1 ns, and over a
range of fluences between 0.05 and 5mJ/cm2. We show that nanoparticles with a
thin dense silica shell (5 nm) exhibit significantly faster water heating
compared to bare gold nanoparticles. This behavior is attributed to enhanced
electron-phonon coupling at the gold silica interface and to the relatively
high thermal conductance between silica and water. These findings provide new
insights into optimizing nanoparticle design for efficient photothermal
applications and establish a robust framework for understanding energy transfer
mechanisms in heterogeneous metal dielectric nanostructures.

</details>


### [50] [Reduction of Outflow Boundary Influence on Aerodynamic Performance using Neural Networks](https://arxiv.org/abs/2506.05293)
*Mario Christopher Bedrunka,Dirk Reith,Holger Foysi,Łukasz Łaniewski-Wołłk,Travis Mitchell*

Main category: physics.comp-ph

TL;DR: Neural networks (NN) are used to improve outflow boundary conditions in LBM, reducing artificial reflections and computational costs. Two NN-based methods show superior accuracy over traditional approaches.


<details>
  <summary>Details</summary>
Motivation: Outflow boundary conditions in LBM often cause inaccuracies due to artificial reflections, limiting aerodynamic and acoustic predictions.

Method: Two NN-based approaches: (1) direct reconstruction of particle distribution functions, and (2) dynamic tuning of characteristic boundary conditions (CBC).

Result: NN methods outperformed traditional Zou & He boundary conditions in accuracy for drag, lift, and Strouhal number predictions. NN-enhanced CBCs also minimized density errors in vortex benchmarks.

Conclusion: NN-integrated boundary conditions enhance LBM accuracy and reduce computational costs, benefiting aerodynamic and acoustic simulations.

Abstract: The accurate treatment of outflow boundary conditions remains a critical
challenge in computational fluid dynamics when predicting aerodynamic forces
and/or acoustic emissions. This is particularly evident when employing the
lattice Boltzmann method (LBM) as the numerical solution technique, which often
suffers from inaccuracies induced by artificial reflections from outflow
boundaries. This paper investigates the use of neural networks (NN) to mitigate
these adverse boundary effects and enable truncated domain requirements. Two
distinct NN-based approaches are proposed: (1) direct reconstruction of unknown
particle distribution functions at the outflow boundary; and (2) enhancement of
established characteristic boundary conditions (CBC) by dynamically tuning
their parameters. The direct reconstruction model was trained on data generated
from a 2D flow over a cylindrical obstruction. The drag, lift, and Strouhal
number were used to test the new boundary condition. We analyzed results for
various Reynolds numbers and restricted domain sizes where it demonstrated
significantly improved predictions when compared with the traditional Zou & He
boundary condition. To examine the robustness of the NN-based reconstruction,
the same condition was applied to the simulation of a NACA0012 airfoil, again
providing accurate aerodynamic performance predictions. The neural-enhanced CBC
were evaluated on a 2D convected vortex benchmark and showed superior
performance in minimizing density errors compared to CBCs with fixed
parameters. These findings highlight the potential of NN-integrated boundary
conditions to improve accuracy and reduce computational expense of aerodynamic
and acoustic emissions simulations with the LBM.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [51] [Discharge dynamics in a cylindrical SDBD prototype reactor under ns-pulsed and sinusoidal AC operation](https://arxiv.org/abs/2506.04826)
*Konstantinos Giotis,Dimitrios Stefas,Yanis Agha,Hans Höft,Xavier Duten,Panagiotis Svarnas,Guillaume Lombardi,Kristaq Gazeli*

Main category: physics.plasm-ph

TL;DR: A prototype reactor for surface dielectric barrier discharges (SDBDs) in air is developed, featuring detachable electrodes and quartz dielectric. It efficiently sustains ns-pulsed and AC discharges, enabling detailed study of electrical and spatiotemporal dynamics. The reactor shows distinct discharge behaviors and is suitable for fundamental research and applications like flow control and biomedicine.


<details>
  <summary>Details</summary>
Motivation: To design a reliable SDBD reactor for consistent operation and material durability, enabling detailed study of discharge characteristics and dynamics for various applications.

Method: The reactor uses detachable stainless steel electrodes and quartz dielectric, with the grounded electrode immersed in transformer oil to suppress parasitic discharges. It operates with ns-pulsed and AC discharges at 10 kHz, analyzing electrical parameters and discharge dynamics via ICCD imaging.

Result: The reactor shows non-linear power consumption with voltage, distinct discharge behaviors (ionization channels and glow-like discharges), and varying propagation lengths/velocities for ns-pulsed and AC operations. Ns-pulsed discharges are more reproducible.

Conclusion: The reactor is effective for fundamental SDBD studies and applications, with ns-pulsed operation offering better reproducibility and detailed spatiotemporal analysis.

Abstract: We developed a prototype reactor generating surface dielectric barrier
discharges (SDBDs) in ambient air, designed for consistent operation while
preventing constructive material degradation. It features detachable stainless
steel electrodes and quartz dielectric to ensure precise fabrication. The
grounded electrode is fully immersed into transformer oil drastically
suppressing undesired parasitic discharges. The device efficiently sustains
ns-pulsed and AC discharges at 10 kHz, enabling fundamental studies of their
electrical characteristics (applied voltage, induced current, electric power)
and spatiotemporal dynamics (morphology, propagation length and velocity). The
electric power (P) consumed exhibits a dissimilar non-linear increase with the
rising peak voltage (Vp) in each case: P$\approx$0.8-2.5 W for ns-pulsed
(Vp=7-9 kV) and P$\approx$0.9-5.3 W (Vp=7-10 kV) for AC operation. Using ICCD
imaging, distinct ionization channels are recorded in the rising part of the
pulsed voltage being detached from the driven electrode; during the voltage
decrease, a glow-like discharge is formed remaining anchored on the driven
electrode. The rising part of the AC voltage is characterized by erratic,
elongated ionization channels in a filamentary form, the voltage drop featuring
a glow-like behavior. During the rising and falling parts of the AC voltage,
the discharge reaches maximum propagation lengths (Lmax) of $\approx$12 mm and
$\approx$7 mm, respectively, while remaining attached to the driven electrode.
The corresponding maximum discharge velocities (vmax) are about 5x10 2 m/s and
3x10 2 m/s. For the ns-pulsed operation, Lmax$\approx$5 mm (vmax$\approx$5x10 5
m/s) and Lmax$\approx$3.5 mm (vmax$\approx$1.5x10 5 m/s) during the rising and
falling parts of the voltage pulse, respectively. The SDBD dynamics generated
with a ns-pulsed voltage is more reproducible than for the AC case allowing for
the use of a 500 times smaller ICCD gate width (2 ns) and a more accurate
description of the discharge's spatiotemporal development. This reactor is
suitable for performing fundamental studies and understanding key SDBD features
for various applications such as flow control, biomedicine and agriculture.

</details>


### [52] [Empirical scaling laws for self-focused laser pulses in nitrogen plasmas](https://arxiv.org/abs/2506.04827)
*Lorenzo Martelli,Igor Andriyash,Jonathan Wheeler,Henri Kraft,Xuan Quyen Dinh,Cédric Thaury*

Main category: physics.plasm-ph

TL;DR: Study of laser-plasma interaction in high-density nitrogen plasma using simulations, revealing insights into laser self-focusing and beam dynamics.


<details>
  <summary>Details</summary>
Motivation: To understand how superintense laser pulses interact with high-density nitrogen plasma, aiming to improve laser-plasma accelerators.

Method: Particle-in-cell simulations to analyze laser self-focusing and its effects on beam diffraction, wakefield amplitude, and plasma structures.

Result: Identified scaling laws for beam diffraction and wakefield amplitude, providing key insights into the interaction regime.

Conclusion: The findings advance the understanding of laser-plasma interactions, aiding the development of high-average-current accelerators.

Abstract: We investigate the interaction between a superintense laser pulse and a
nitrogen plasma with densities exceeding $10^{19}\,$cm$^{-3}$, using
particle-in-cell simulations. Such configurations have recently demonstrated
the capability to produce highly charged electron beams (i.e., $>10\,$nC) with
$1\,$J-class lasers, a significant step toward high-average-current
laser-plasma accelerators. Our study focuses on analyzing the impact of laser
self-focusing on laser dynamics, leading to scaling laws that characterize beam
diffraction, wakefield amplitude and plasma structures, providing important
insights of this interaction regime.

</details>


### [53] [Electron and gas temperature-driven chemistry during microdischarges formed in water vapour bubbles](https://arxiv.org/abs/2506.05124)
*Florens Grimm,Jan-Luca Gembus,Jana Schöne,Peter Awakowicz,Lars Schücke,Andrew R. Gibson*

Main category: physics.plasm-ph

TL;DR: The paper studies gas-phase chemical kinetics in microdischarges within bubbles in liquids, using a 0-D model informed by experiments, focusing on high electron density and gas temperature regimes.


<details>
  <summary>Details</summary>
Motivation: Understanding gas-phase chemical kinetics in microdischarges is crucial for controlling plasma-driven electrochemistry in materials synthesis and chemical conversion.

Method: A 0-D modeling approach, informed by experimental measurements, is used to develop a new reaction scheme for microdischarges in water vapor, analyzing species formation and pathways under varying temperatures.

Result: H2O is highly dissociated during peak power density, with H and O dominating the neutral gas. The maximum ionization degree is ~0.31%, and electronegativity is low. High-threshold energy reactions are key across temperatures.

Conclusion: The findings are transferable to similar plasma electrolysis systems, highlighting the importance of high-threshold energy reactions in microdischarge chemical kinetics.

Abstract: Microdischarges formed in bubbles immersed in liquids are of interest for
materials synthesis and chemical conversion applications in the frame of
plasma-driven electrochemistry. A key challenge associated with controlling
such processes is the limited understanding of the gas-phase chemical kinetics
in these microdischarges. Due to their large electron densities, and high gas
temperatures, both electron and gas temperature driven chemistry are likely to
be important. Here, a 0-D modelling approach, informed by experimental
measurements, is used to study the chemical kinetics in these systems. A new
reaction scheme is developed for microdischarges in water vapour, including
reactions for both high electron density, and high gas temperature regimes.
Microdischarges formed during plasma electrolytic oxidation are used as a test
case, however, the key results are expected to be transferable to other plasma
electrolysis systems with similar properties. Experimentally measured power
densities are used as input to the 0-D model, together with estimates of
temperatures and gas pressures within the gas bubble. Comparison of measured
and simulated electron densities shows good agreement, given the limitations of
both model and experiment. In the base case microdischarge, H$_{2}$O is found
to be highly dissociated during the period of peak power density, with H and O
making up the majority of the neutral gas in the bubble. The maximum ionization
degree is around 0.31$\,\%$, and the electronegativity during the period of
peak electron density is found to be low. Species formation and reaction
pathways are analysed under variation of the neutral gas temperature from
2000$\,$K to 6000$\,$K. At all temperatures, electron, ion, and neutral
reactions with high threshold energies are found to be important for the
overall chemical kinetics.

</details>


### [54] [Extending near-axis equilibria in DESC](https://arxiv.org/abs/2506.05170)
*Dario Panici,Eduardo Rodriguez,Rory Conlin,Daniel Dudt,Egemen Kolemen*

Main category: physics.plasm-ph

TL;DR: A novel method for constructing global stellarator equilibria using the DESC code, ensuring correct asymptotic behavior from near-axis models, is presented and benchmarked.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between approximate near-axis models and practical global equilibrium requirements for stellarator design and optimization.

Method: Uses the DESC code to construct global equilibria with guaranteed asymptotic behavior derived from near-axis models. Theoretical foundations and benchmarking are detailed.

Result: Successful construction of global equilibria that align with near-axis predictions, enabling efficient coupling for future optimization.

Conclusion: The method facilitates improved stellarator design by integrating near-axis and global equilibrium frameworks.

Abstract: The near-axis description of optimised stellarator fields has proven to be a
powerful tool both for design and understanding of this magnetic confinement
concept. The description consists of an asymptotic model of the equilibrium in
the distance from its centermost axis, and is thus only approximate. Any
practical application therefore requires the eventual construction of a global
equilibrium. This paper presents a novel way of constructing global equilibria
using the \texttt{DESC} code that guarantees the correct asymptotic behaviour
imposed by a given near-axis construction. The theoretical underpinnings of
this construction are carefully presented, and benchmarking examples provided.
This opens the door to an efficient coupling of the near-axis framework and
that of global equilibria for future optimisation efforts.

</details>


### [55] [Discharge dynamics in a cylindrical SDBD prototype reactor under ns-pulsed and sinusoidal AC operation](https://arxiv.org/abs/2506.04826)
*Konstantinos Giotis,Dimitrios Stefas,Yanis Agha,Hans Höft,Xavier Duten,Panagiotis Svarnas,Guillaume Lombardi,Kristaq Gazeli*

Main category: physics.plasm-ph

TL;DR: A prototype reactor for surface dielectric barrier discharges (SDBDs) in air was developed, featuring detachable electrodes and quartz dielectric to prevent material degradation. It efficiently sustains ns-pulsed and AC discharges, enabling detailed study of electrical and spatiotemporal dynamics. The reactor is suitable for fundamental research and applications like flow control and biomedicine.


<details>
  <summary>Details</summary>
Motivation: To create a reliable SDBD reactor for consistent operation without material degradation, enabling precise study of discharge characteristics and dynamics for various applications.

Method: The reactor uses detachable stainless steel electrodes and quartz dielectric, with the grounded electrode immersed in transformer oil to suppress parasitic discharges. It operates with ns-pulsed and AC discharges at 10 kHz, analyzing electrical parameters and discharge dynamics using ICCD imaging.

Result: The reactor showed distinct discharge behaviors for ns-pulsed and AC operations, with non-linear power increases. Ns-pulsed discharges were more reproducible, allowing finer temporal resolution (2 ns gate width). Maximum propagation lengths and velocities were measured for both discharge types.

Conclusion: The reactor effectively studies SDBD dynamics, with ns-pulsed operation offering higher reproducibility. It is versatile for fundamental research and practical applications like flow control and biomedicine.

Abstract: We developed a prototype reactor generating surface dielectric barrier
discharges (SDBDs) in ambient air, designed for consistent operation while
preventing constructive material degradation. It features detachable stainless
steel electrodes and quartz dielectric to ensure precise fabrication. The
grounded electrode is fully immersed into transformer oil drastically
suppressing undesired parasitic discharges. The device efficiently sustains
ns-pulsed and AC discharges at 10 kHz, enabling fundamental studies of their
electrical characteristics (applied voltage, induced current, electric power)
and spatiotemporal dynamics (morphology, propagation length and velocity). The
electric power (P) consumed exhibits a dissimilar non-linear increase with the
rising peak voltage (Vp) in each case: P$\approx$0.8-2.5 W for ns-pulsed
(Vp=7-9 kV) and P$\approx$0.9-5.3 W (Vp=7-10 kV) for AC operation. Using ICCD
imaging, distinct ionization channels are recorded in the rising part of the
pulsed voltage being detached from the driven electrode; during the voltage
decrease, a glow-like discharge is formed remaining anchored on the driven
electrode. The rising part of the AC voltage is characterized by erratic,
elongated ionization channels in a filamentary form, the voltage drop featuring
a glow-like behavior. During the rising and falling parts of the AC voltage,
the discharge reaches maximum propagation lengths (Lmax) of $\approx$12 mm and
$\approx$7 mm, respectively, while remaining attached to the driven electrode.
The corresponding maximum discharge velocities (vmax) are about 5x10 2 m/s and
3x10 2 m/s. For the ns-pulsed operation, Lmax$\approx$5 mm (vmax$\approx$5x10 5
m/s) and Lmax$\approx$3.5 mm (vmax$\approx$1.5x10 5 m/s) during the rising and
falling parts of the voltage pulse, respectively. The SDBD dynamics generated
with a ns-pulsed voltage is more reproducible than for the AC case allowing for
the use of a 500 times smaller ICCD gate width (2 ns) and a more accurate
description of the discharge's spatiotemporal development. This reactor is
suitable for performing fundamental studies and understanding key SDBD features
for various applications such as flow control, biomedicine and agriculture.

</details>


### [56] [Empirical scaling laws for self-focused laser pulses in nitrogen plasmas](https://arxiv.org/abs/2506.04827)
*Lorenzo Martelli,Igor Andriyash,Jonathan Wheeler,Henri Kraft,Xuan Quyen Dinh,Cédric Thaury*

Main category: physics.plasm-ph

TL;DR: Study of laser-plasma interaction in high-density nitrogen plasma, revealing scaling laws for beam diffraction and wakefield amplitude.


<details>
  <summary>Details</summary>
Motivation: To understand the dynamics of superintense laser pulses in high-density plasmas, aiming to advance laser-plasma accelerators.

Method: Particle-in-cell simulations to analyze laser self-focusing and its effects on plasma structures.

Result: Identified scaling laws for beam diffraction and wakefield amplitude, contributing to high-current laser-plasma accelerators.

Conclusion: Provides key insights into laser-plasma interactions, aiding the development of efficient accelerators.

Abstract: We investigate the interaction between a superintense laser pulse and a
nitrogen plasma with densities exceeding $10^{19}\,$cm$^{-3}$, using
particle-in-cell simulations. Such configurations have recently demonstrated
the capability to produce highly charged electron beams (i.e., $>10\,$nC) with
$1\,$J-class lasers, a significant step toward high-average-current
laser-plasma accelerators. Our study focuses on analyzing the impact of laser
self-focusing on laser dynamics, leading to scaling laws that characterize beam
diffraction, wakefield amplitude and plasma structures, providing important
insights of this interaction regime.

</details>


### [57] [Electron and gas temperature-driven chemistry during microdischarges formed in water vapour bubbles](https://arxiv.org/abs/2506.05124)
*Florens Grimm,Jan-Luca Gembus,Jana Schöne,Peter Awakowicz,Lars Schücke,Andrew R. Gibson*

Main category: physics.plasm-ph

TL;DR: A 0-D model studies chemical kinetics in microdischarges in bubbles, focusing on high electron density and gas temperature regimes, validated by experiments.


<details>
  <summary>Details</summary>
Motivation: Understanding gas-phase chemical kinetics in microdischarges for plasma-driven electrochemistry applications.

Method: A 0-D modeling approach informed by experimental measurements, with a new reaction scheme for water vapor microdischarges.

Result: High dissociation of H₂O, low electronegativity, and significant ionization (0.31%) observed; electron and ion reactions dominate kinetics.

Conclusion: The model provides insights into microdischarge chemistry, applicable to similar plasma electrolysis systems.

Abstract: Microdischarges formed in bubbles immersed in liquids are of interest for
materials synthesis and chemical conversion applications in the frame of
plasma-driven electrochemistry. A key challenge associated with controlling
such processes is the limited understanding of the gas-phase chemical kinetics
in these microdischarges. Due to their large electron densities, and high gas
temperatures, both electron and gas temperature driven chemistry are likely to
be important. Here, a 0-D modelling approach, informed by experimental
measurements, is used to study the chemical kinetics in these systems. A new
reaction scheme is developed for microdischarges in water vapour, including
reactions for both high electron density, and high gas temperature regimes.
Microdischarges formed during plasma electrolytic oxidation are used as a test
case, however, the key results are expected to be transferable to other plasma
electrolysis systems with similar properties. Experimentally measured power
densities are used as input to the 0-D model, together with estimates of
temperatures and gas pressures within the gas bubble. Comparison of measured
and simulated electron densities shows good agreement, given the limitations of
both model and experiment. In the base case microdischarge, H$_{2}$O is found
to be highly dissociated during the period of peak power density, with H and O
making up the majority of the neutral gas in the bubble. The maximum ionization
degree is around 0.31$\,\%$, and the electronegativity during the period of
peak electron density is found to be low. Species formation and reaction
pathways are analysed under variation of the neutral gas temperature from
2000$\,$K to 6000$\,$K. At all temperatures, electron, ion, and neutral
reactions with high threshold energies are found to be important for the
overall chemical kinetics.

</details>


### [58] [Extending near-axis equilibria in DESC](https://arxiv.org/abs/2506.05170)
*Dario Panici,Eduardo Rodriguez,Rory Conlin,Daniel Dudt,Egemen Kolemen*

Main category: physics.plasm-ph

TL;DR: A novel method for constructing global equilibria in stellarator fields using the DESC code, ensuring correct asymptotic behavior from near-axis models.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between near-axis models and global equilibria for improved stellarator design and optimization.

Method: Uses the DESC code to construct global equilibria with enforced asymptotic behavior from near-axis models, supported by theoretical foundations and benchmarking.

Result: Demonstrates successful coupling of near-axis and global equilibrium frameworks, enabling efficient optimization.

Conclusion: This approach enhances stellarator design by integrating near-axis insights into global equilibrium construction.

Abstract: The near-axis description of optimised stellarator fields has proven to be a
powerful tool both for design and understanding of this magnetic confinement
concept. The description consists of an asymptotic model of the equilibrium in
the distance from its centermost axis, and is thus only approximate. Any
practical application therefore requires the eventual construction of a global
equilibrium. This paper presents a novel way of constructing global equilibria
using the \texttt{DESC} code that guarantees the correct asymptotic behaviour
imposed by a given near-axis construction. The theoretical underpinnings of
this construction are carefully presented, and benchmarking examples provided.
This opens the door to an efficient coupling of the near-axis framework and
that of global equilibria for future optimisation efforts.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [59] [Perturbative Gradient Training: A novel training paradigm for bridging the gap between deep neural networks and physical reservoir computing](https://arxiv.org/abs/2506.04523)
*Cliff B. Abbott,Mark Elo,Dmytro A. Bozhko*

Main category: cs.LG

TL;DR: PGT is a new training method for physical reservoir computing that approximates gradients via random perturbations, enabling training without backpropagation.


<details>
  <summary>Details</summary>
Motivation: Overcome the limitation of physical reservoir computing's inability to perform backpropagation due to its black-box nature.

Method: Uses random perturbations in parameter space to approximate gradient updates, tested on simulated and experimental hardware.

Result: PGT achieves performance comparable to backpropagation where the latter is impractical.

Conclusion: PGT enables deeper integration of physical reservoirs into neural networks, promising energy efficiency gains.

Abstract: We introduce Perturbative Gradient Training (PGT), a novel training paradigm
that overcomes a critical limitation of physical reservoir computing: the
inability to perform backpropagation due to the black-box nature of physical
reservoirs. Drawing inspiration from perturbation theory in physics, PGT uses
random perturbations in the network's parameter space to approximate gradient
updates using only forward passes. We demonstrate the feasibility of this
approach on both simulated neural network architectures, including a dense
network and a transformer model with a reservoir layer, and on experimental
hardware using a magnonic auto-oscillation ring as the physical reservoir. Our
results show that PGT can achieve performance comparable to that of standard
backpropagation methods in cases where backpropagation is impractical or
impossible. PGT represents a promising step toward integrating physical
reservoirs into deeper neural network architectures and achieving significant
energy efficiency gains in AI training.

</details>


### [60] [Learning Beyond Experience: Generalizing to Unseen State Space with Reservoir Computing](https://arxiv.org/abs/2506.05292)
*Declan A. Norton,Yuanzhao Zhang,Michelle Girvan*

Main category: cs.LG

TL;DR: Reservoir computing can generalize to unexplored state space regions without explicit structural priors, using a multiple-trajectory training scheme.


<details>
  <summary>Details</summary>
Motivation: Machine learning struggles to generalize without structural priors, especially for poorly represented dynamics in training data.

Method: A multiple-trajectory training scheme for reservoir computers is introduced, enabling training across disjoint time series.

Result: RCs trained on single basin trajectories generalize to unobserved basins in multistable systems.

Conclusion: Reservoir computing can achieve out-of-domain generalization without explicit structural assumptions.

Abstract: Machine learning techniques offer an effective approach to modeling dynamical
systems solely from observed data. However, without explicit structural priors
-- built-in assumptions about the underlying dynamics -- these techniques
typically struggle to generalize to aspects of the dynamics that are poorly
represented in the training data. Here, we demonstrate that reservoir computing
-- a simple, efficient, and versatile machine learning framework often used for
data-driven modeling of dynamical systems -- can generalize to unexplored
regions of state space without explicit structural priors. First, we describe a
multiple-trajectory training scheme for reservoir computers that supports
training across a collection of disjoint time series, enabling effective use of
available training data. Then, applying this training scheme to multistable
dynamical systems, we show that RCs trained on trajectories from a single basin
of attraction can achieve out-of-domain generalization by capturing system
behavior in entirely unobserved basins.

</details>


### [61] [Perturbative Gradient Training: A novel training paradigm for bridging the gap between deep neural networks and physical reservoir computing](https://arxiv.org/abs/2506.04523)
*Cliff B. Abbott,Mark Elo,Dmytro A. Bozhko*

Main category: cs.LG

TL;DR: PGT is a new training method for physical reservoir computing, using random perturbations to approximate gradients without backpropagation, showing comparable performance to traditional methods.


<details>
  <summary>Details</summary>
Motivation: Overcome the limitation of physical reservoir computing's inability to perform backpropagation due to its black-box nature.

Method: Uses random perturbations in parameter space to approximate gradient updates, tested on simulated networks and experimental hardware.

Result: PGT achieves performance comparable to standard backpropagation where it's impractical.

Conclusion: PGT enables integration of physical reservoirs into deeper networks, promising energy efficiency gains in AI training.

Abstract: We introduce Perturbative Gradient Training (PGT), a novel training paradigm
that overcomes a critical limitation of physical reservoir computing: the
inability to perform backpropagation due to the black-box nature of physical
reservoirs. Drawing inspiration from perturbation theory in physics, PGT uses
random perturbations in the network's parameter space to approximate gradient
updates using only forward passes. We demonstrate the feasibility of this
approach on both simulated neural network architectures, including a dense
network and a transformer model with a reservoir layer, and on experimental
hardware using a magnonic auto-oscillation ring as the physical reservoir. Our
results show that PGT can achieve performance comparable to that of standard
backpropagation methods in cases where backpropagation is impractical or
impossible. PGT represents a promising step toward integrating physical
reservoirs into deeper neural network architectures and achieving significant
energy efficiency gains in AI training.

</details>


### [62] [Learning Beyond Experience: Generalizing to Unseen State Space with Reservoir Computing](https://arxiv.org/abs/2506.05292)
*Declan A. Norton,Yuanzhao Zhang,Michelle Girvan*

Main category: cs.LG

TL;DR: Reservoir computing can generalize to unexplored state space regions without explicit structural priors, using a multiple-trajectory training scheme.


<details>
  <summary>Details</summary>
Motivation: Machine learning struggles to generalize dynamics without structural priors; reservoir computing offers a solution.

Method: Multiple-trajectory training scheme for reservoir computers, applied to multistable dynamical systems.

Result: RCs trained on single basin trajectories generalize to unobserved basins.

Conclusion: Reservoir computing enables out-of-domain generalization without explicit priors.

Abstract: Machine learning techniques offer an effective approach to modeling dynamical
systems solely from observed data. However, without explicit structural priors
-- built-in assumptions about the underlying dynamics -- these techniques
typically struggle to generalize to aspects of the dynamics that are poorly
represented in the training data. Here, we demonstrate that reservoir computing
-- a simple, efficient, and versatile machine learning framework often used for
data-driven modeling of dynamical systems -- can generalize to unexplored
regions of state space without explicit structural priors. First, we describe a
multiple-trajectory training scheme for reservoir computers that supports
training across a collection of disjoint time series, enabling effective use of
available training data. Then, applying this training scheme to multistable
dynamical systems, we show that RCs trained on trajectories from a single basin
of attraction can achieve out-of-domain generalization by capturing system
behavior in entirely unobserved basins.

</details>


### [63] [A Comprehensive Survey on the Risks and Limitations of Concept-based Models](https://arxiv.org/abs/2506.04237)
*Sanchit Sinha,Aidong Zhang*

Main category: cs.LG

TL;DR: A survey on the risks and limitations of Concept-based Models, highlighting challenges like concept leakage and adversarial vulnerabilities, and discussing mitigation strategies and future research directions.


<details>
  <summary>Details</summary>
Motivation: To address the growing adoption of Concept-based Models in sensitive domains by identifying their limitations and improving their reliability and trustworthiness.

Method: Comprehensive survey aggregating challenges and mitigation strategies for Supervised and Unsupervised paradigms, along with recent advances.

Result: Identified key issues like concept leakage and robustness, and discussed architectural choices and interventions to mitigate them.

Conclusion: While Concept-based Models show promise, significant challenges remain; future research should focus on improving their reliability and real-world applicability.

Abstract: Concept-based Models are a class of inherently explainable networks that
improve upon standard Deep Neural Networks by providing a rationale behind
their predictions using human-understandable `concepts'. With these models
being highly successful in critical applications like medical diagnosis and
financial risk prediction, there is a natural push toward their wider adoption
in sensitive domains to instill greater trust among diverse stakeholders.
However, recent research has uncovered significant limitations in the structure
of such networks, their training procedure, underlying assumptions, and their
susceptibility to adversarial vulnerabilities. In particular, issues such as
concept leakage, entangled representations, and limited robustness to
perturbations pose challenges to their reliability and generalization.
Additionally, the effectiveness of human interventions in these models remains
an open question, raising concerns about their real-world applicability. In
this paper, we provide a comprehensive survey on the risks and limitations
associated with Concept-based Models. In particular, we focus on aggregating
commonly encountered challenges and the architecture choices mitigating these
challenges for Supervised and Unsupervised paradigms. We also examine recent
advances in improving their reliability and discuss open problems and promising
avenues of future research in this domain.

</details>


### [64] [Improving Out-of-Distribution Detection with Markov Logic Networks](https://arxiv.org/abs/2506.04241)
*Konstantin Kirchheim,Frank Ortmeier*

Main category: cs.LG

TL;DR: The paper proposes using Markov logic networks (MLNs) to enhance out-of-distribution (OOD) detection in deep learning models, improving explainability and performance.


<details>
  <summary>Details</summary>
Motivation: Current OOD detectors rely on statistical models, lacking explainability. The work aims to integrate probabilistic reasoning with human-understandable concepts for better OOD detection.

Method: The approach augments existing OOD detectors with MLNs, which combine first-order logic and probabilistic reasoning. A simple algorithm for learning logical constraints from datasets is also introduced.

Result: Experiments show MLNs significantly improve OOD detection performance across multiple datasets while maintaining computational efficiency.

Conclusion: MLNs enhance OOD detectors by providing explainable probabilistic reasoning, with demonstrated effectiveness and efficiency.

Abstract: Out-of-distribution (OOD) detection is essential for ensuring the reliability
of deep learning models operating in open-world scenarios. Current OOD
detectors mainly rely on statistical models to identify unusual patterns in the
latent representations of a deep neural network. This work proposes to augment
existing OOD detectors with probabilistic reasoning, utilizing Markov logic
networks (MLNs). MLNs connect first-order logic with probabilistic reasoning to
assign probabilities to inputs based on weighted logical constraints defined
over human-understandable concepts, which offers improved explainability.
Through extensive experiments on multiple datasets, we demonstrate that MLNs
can significantly enhance the performance of a wide range of existing OOD
detectors while maintaining computational efficiency. Furthermore, we introduce
a simple algorithm for learning logical constraints for OOD detection from a
dataset and showcase its effectiveness.

</details>


### [65] [Triple Attention Transformer Architecture for Time-Dependent Concrete Creep Prediction](https://arxiv.org/abs/2506.04243)
*Warayut Dokduea,Weerachart Tangchirapat,Sompote Youwai*

Main category: cs.LG

TL;DR: A novel Triple Attention Transformer Architecture is introduced for predicting concrete creep, outperforming traditional methods with high accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Current approaches fail to model the sequential nature of concrete creep, treating time as a static input. This limits accuracy and practical applicability.

Method: The architecture uses a triple-stream attention framework (temporal, feature, and batch attention) within a transformer to model creep as an autoregressive sequence task.

Result: Achieves 1.63% mean absolute percentage error and R2 of 0.999, with Young's modulus identified as the most predictive feature.

Conclusion: The work demonstrates transformers' potential in materials science, offering a data-driven revolution in structural behavior prediction.

Abstract: This paper presents a novel Triple Attention Transformer Architecture for
predicting time-dependent concrete creep, addressing fundamental limitations in
current approaches that treat time as merely an input parameter rather than
modeling the sequential nature of deformation development. By transforming
concrete creep prediction into an autoregressive sequence modeling task similar
to language processing, our architecture leverages the transformer's
self-attention mechanisms to capture long-range dependencies in historical
creep patterns. The model implements a triple-stream attention framework
incorporating temporal attention for sequential progression, feature attention
for material property interactions, and batch attention for inter-sample
relationships. Evaluated on experimental datasets with standardized daily
measurements spanning 160 days, the architecture achieves exceptional
performance with mean absolute percentage error of 1.63% and R2 values of 0.999
across all datasets, substantially outperforming traditional empirical models
and existing machine learning approaches. Ablation studies confirm the critical
role of attention mechanisms, with attention pooling contributing most
significantly to model performance. SHAP analysis reveals Young's modulus as
the primary predictive feature, followed by density and compressive strength,
providing interpretability essential for engineering applications. A deployed
web-based interface facilitates practical implementation, enabling real-time
predictions using standard laboratory parameters. This work establishes the
viability of applying transformer architectures to materials science problems,
demonstrating the potential for data-driven approaches to revolutionize
structural behavior prediction and engineering design practices.

</details>


### [66] [SafeSteer: Interpretable Safety Steering with Refusal-Evasion in LLMs](https://arxiv.org/abs/2506.04250)
*Shaona Ghosh,Amrita Bhattacharjee,Yftah Ziser,Christopher Parisien*

Main category: cs.LG

TL;DR: SafeSteer is a method for fine-tuning LLMs to improve safety without costly retraining, using unsupervised activation steering for precise control while maintaining text quality.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning LLMs for safety is impractical due to cost. SafeSteer aims to provide a simpler, effective alternative for real-time safety adjustments.

Method: SafeSteer uses category-specific steering vectors and a gradient-free unsupervised approach to guide LLM outputs without needing contrastive safe data.

Result: The method effectively controls LLM outputs, prevents blanket refusals, and maintains topic relevance across various models and datasets.

Conclusion: SafeSteer offers a practical, simple solution for real-time safety adjustments in LLMs, outperforming complex methods.

Abstract: Fine-tuning large language models (LLMs) to adapt to evolving safety policies
is costly and impractical. Mechanistic interpretability enables inference-time
control through latent activation steering, yet its potential for precise,
customizable safety adjustments remains largely untapped. This paper
investigates an approach called SafeSteer for guiding the outputs of LLMs by:
(i) leveraging category-specific steering vectors for more precise control,
(ii) employing a simple, gradient-free unsupervised method to enhance safety
steering while preserving text quality, topic relevance, and without explicit
refusal, and (iii) accomplishing this without a hard requirement of contrastive
pairwise safe data. We also highlight that our method, being simple and
effective, aligns with recent studies suggesting that simple techniques often
outperform more complex ones in activation steering. We showcase the
effectiveness of our approach across various LLMs, datasets, and risk
categories, demonstrating its ability to provide precise control, prevent
blanket refusals, and guide models toward generating safe content while
maintaining topic relevance.

</details>


### [67] [Localized Forest Fire Risk Prediction: A Department-Aware Approach for Operational Decision Support](https://arxiv.org/abs/2506.04254)
*Nicolas Caron,Christophe Guyeux,Hassan Noura,Benjamin Aynes*

Main category: cs.LG

TL;DR: The paper proposes a localized approach to forest fire prediction, addressing the oversimplification of traditional binary classification by tailoring risk assessment to departmental contexts in France.


<details>
  <summary>Details</summary>
Motivation: Climate change has intensified forest fires, making accurate prediction a critical AI challenge. Traditional binary classification oversimplifies the problem, especially for end-users like firefighters who need region-specific insights.

Method: The paper introduces a new approach that models fire risk based on local departmental conditions, using state-of-the-art AI models on an unexplored dataset for metropolitan France.

Result: The study presents the first national-scale AI benchmark for forest fire prediction in France, offering actionable, region-specific risk assessments.

Conclusion: The paper highlights the need for localized fire risk models and suggests future research directions, with supplementary materials available on GitHub.

Abstract: Forest fire prediction involves estimating the likelihood of fire ignition or
related risk levels in a specific area over a defined time period. With climate
change intensifying fire behavior and frequency, accurate prediction has become
one of the most pressing challenges in Artificial Intelligence (AI).
Traditionally, fire ignition is approached as a binary classification task in
the literature. However, this formulation oversimplifies the problem,
especially from the perspective of end-users such as firefighters. In general,
as is the case in France, firefighting units are organized by department, each
with its terrain, climate conditions, and historical experience with fire
events. Consequently, fire risk should be modeled in a way that is sensitive to
local conditions and does not assume uniform risk across all regions. This
paper proposes a new approach that tailors fire risk assessment to departmental
contexts, offering more actionable and region-specific predictions for
operational use. With this, we present the first national-scale AI benchmark
for metropolitan France using state-of-the-art AI models on a relatively
unexplored dataset. Finally, we offer a summary of important future works that
should be taken into account. Supplementary materials are available on GitHub.

</details>


### [68] [MUC-G4: Minimal Unsat Core-Guided Incremental Verification for Deep Neural Network Compression](https://arxiv.org/abs/2506.04268)
*Jingyang Li,Guoqiang Li*

Main category: cs.LG

TL;DR: MUC-G4 is a framework for incremental verification of compressed neural networks, using Minimal Unsat Cores to guide efficient verification, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Challenges in deploying neural networks on edge devices due to high complexity, and limitations of existing verification methods focusing only on quantization.

Method: Encodes original and compressed networks into SMT formulas, classifies changes, and uses Minimal Unsat Cores from the original network to guide verification.

Result: Effective in handling quantization and pruning, with high proof reuse rates and significant speedup in verification time.

Conclusion: MUC-G4 provides a promising solution for ensuring safety and reliability of compressed neural networks in practical applications.

Abstract: The rapid development of deep learning has led to challenges in deploying
neural networks on edge devices, mainly due to their high memory and runtime
complexity. Network compression techniques, such as quantization and pruning,
aim to reduce this complexity while maintaining accuracy. However, existing
incremental verification methods often focus only on quantization and struggle
with structural changes. This paper presents MUC-G4 (Minimal Unsat Core-Guided
Incremental Verification), a novel framework for incremental verification of
compressed deep neural networks. It encodes both the original and compressed
networks into SMT formulas, classifies changes, and use \emph{Minimal Unsat
Cores (MUCs)} from the original network to guide efficient verification for the
compressed network. Experimental results show its effectiveness in handling
quantization and pruning, with high proof reuse rates and significant speedup
in verification time compared to traditional methods. MUC-G4 hence offers a
promising solution for ensuring the safety and reliability of compressed neural
networks in practical applications.

</details>


### [69] [Understanding the Impact of Sampling Quality in Direct Preference Optimization](https://arxiv.org/abs/2506.04272)
*Kyung Rok Kim,Yumo Bai,Chonghuan Wang,Guanting Chen*

Main category: cs.LG

TL;DR: The paper investigates how the sampling distribution affects Direct Preference Optimization (DPO), showing its impact on training dynamics, solution space, and convergence. High-quality responses enhance gradient signals and improve policy learning.


<details>
  <summary>Details</summary>
Motivation: To understand the influence of the sampling distribution on DPO's training dynamics and optimization landscape.

Method: Analyzed response distribution effects on policy updates, designed a simplified alignment model, and conducted empirical experiments.

Result: Frequent high-quality responses amplify gradient signals and improve optimization, leading to more effective policy learning.

Conclusion: The findings support the practical use of online DPO by justifying its effectiveness through theoretical and empirical evidence.

Abstract: We study the role of the sampling distribution in Direct Preference
Optimization (DPO) and aim to understand its impact on DPO's training dynamics.
Our analyses show that both the solution space and the convergence behavior of
DPO depend on the support and quality of the generating distribution. We first
analyze how distribution of responses influences policy updates during gradient
descent, drawing connections to common phenomena found in practice. We then
design a simplified yet well-structured alignment model as a proxy, and develop
quantitative results showing how more frequent high-quality responses amplify
the gradient signal and improve the optimization landscape, leading to more
effective policy learning. Our theoretical findings are supported by empirical
experiments and provide a principled justification for the online DPO framework
in practice.

</details>


### [70] [SF$^2$Bench: Evaluating Data-Driven Models for Compound Flood Forecasting in South Florida](https://arxiv.org/abs/2506.04281)
*Xu Zheng,Chaohao Lin,Sipeng Chen,Zhuomin Chen,Jimeng Shi,Wei Cheng,Jayantha Obeysekera,Jason Liu,Dongsheng Luo*

Main category: cs.LG

TL;DR: The paper introduces SF2Bench, a dataset for compound flood analysis in South Florida, and evaluates six machine learning methods for flood forecasting, highlighting their strengths in capturing temporal and spatial dependencies.


<details>
  <summary>Details</summary>
Motivation: Compound flood forecasting is complex due to multiple interacting factors, and traditional methods are inefficient. Machine learning offers potential, but data scarcity limits progress.

Method: The study introduces SF2Bench, integrating tide, rainfall, groundwater, and human management data. It evaluates six ML methods (MLPs, CNNs, RNNs, GNNs, Transformers, LLMs) for flood forecasting.

Result: Experiments show varying performance across methods, revealing their diverse capabilities in modeling temporal and spatial dependencies in compound floods.

Conclusion: SF2Bench enables detailed analysis of flood drivers, and the evaluation of ML methods provides insights for improving flood forecasting approaches.

Abstract: Forecasting compound floods presents a significant challenge due to the
intricate interplay of meteorological, hydrological, and oceanographic factors.
Analyzing compound floods has become more critical as the global climate
increases flood risks. Traditional physics-based methods, such as the
Hydrologic Engineering Center's River Analysis System, are often
time-inefficient. Machine learning has recently demonstrated promise in both
modeling accuracy and computational efficiency. However, the scarcity of
comprehensive datasets currently hinders systematic analysis. Existing
water-related datasets are often limited by a sparse network of monitoring
stations and incomplete coverage of relevant factors. To address this
challenge, we introduce SF2Bench, a comprehensive time series collection on
compound floods in South Florida, which integrates four key factors: tide,
rainfall, groundwater, and human management activities (gate and pump
controlling). This integration allows for a more detailed analysis of the
individual contributions of these drivers to compound flooding and informs the
development of improved flood forecasting approaches. To comprehensively
evaluate the potential of various modeling paradigms, we assess the performance
of six categories of methods, encompassing Multilayer Perceptrons,
Convolutional Neural Networks, Recurrent Neural Networks, Graph Neural
Networks, Transformers, and Large Language Models. We verified the impact of
different key features on flood forecasting through experiments. Our analysis
examines temporal and spatial aspects, providing insights into the influence of
historical data and spatial dependencies. The varying performance across these
approaches underscores the diverse capabilities of each in capturing complex
temporal and spatial dependencies inherent in compound floods.

</details>


### [71] [DrSR: LLM based Scientific Equation Discovery with Dual Reasoning from Data and Experience](https://arxiv.org/abs/2506.04282)
*Runxiang Wang,Boxiao Wang,Kai Li,Yifan Zhang,Jian Cheng*

Main category: cs.LG

TL;DR: DrSR enhances symbolic regression by combining data-driven insights and reflective learning, outperforming traditional and LLM-based methods in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based symbolic regression methods lack explicit data understanding and systematic reflection, limiting robustness and discovery capability.

Method: DrSR integrates data-driven analysis (structural relationships) with reflective learning (feedback loop) to refine equation generation.

Result: DrSR improves valid equation rates and outperforms classical and LLM-based methods in accuracy, generalization, and efficiency across multiple domains.

Conclusion: DrSR demonstrates strong potential for scientific equation discovery by balancing data understanding and generation reflection.

Abstract: Symbolic regression is a fundamental tool for discovering interpretable
mathematical expressions from data, with broad applications across scientific
and engineering domains. Recently, large language models (LLMs) have
demonstrated strong performance in this task, leveraging embedded scientific
priors and reasoning capabilities to surpass traditional methods. However,
existing LLM-based approaches, such as LLM-SR, often over-rely on internal
priors, lacking explicit data understanding and systematic reflection during
equation generation. To address these limitations, we propose DrSR (Dual
Reasoning Symbolic Regression), a framework that combines data-driven insight
with reflective learning to enhance both robustness and discovery capability.
Specifically, DrSR guides LLMs to analyze structural relationships (e.g.,
monotonicity, nonlinearity, and correlation) within the data to generate
structured descriptions. Simultaneously, it monitors equation performance and
establishes a feedback loop to refine subsequent generations. By integrating
data understanding and generation reflection in a closed loop, DrSR enables
more efficient exploration of the symbolic expression space. Experiments across
interdisciplinary datasets in physics, chemistry, biology, and materials
science demonstrate that DrSR substantially improves the valid equation rate
and consistently outperforms both classical and recent LLM-based methods in
terms of accuracy, generalization, and search efficiency. These results
underscore its potential for scientific equation discovery.

</details>


### [72] [Training-free AI for Earth Observation Change Detection using Physics Aware Neuromorphic Networks](https://arxiv.org/abs/2506.04285)
*Stephen Smith,Cormac Purcell,Zdenka Kuncic*

Main category: cs.LG

TL;DR: A Physics Aware Neuromorphic Network (PANN) is proposed for real-time natural disaster detection from satellite images, enabling prioritized data downlinking with minimal computing resources.


<details>
  <summary>Details</summary>
Motivation: To address the bottleneck of bandwidth-limited downlinking of satellite data by processing and prioritizing relevant disaster-related data on-board.

Method: Uses a neuromorphic network inspired by memristors, with dynamic weights updated via physics-based equations, enabling training-free change detection.

Result: PANN matches or outperforms a state-of-the-art AI model in disaster detection, proving effective for resource-constrained on-board processing.

Conclusion: PANN offers a viable solution for efficient, low-latency disaster data processing on satellites.

Abstract: Earth observations from low Earth orbit satellites provide vital information
for decision makers to better manage time-sensitive events such as natural
disasters. For the data to be most effective for first responders, low latency
is required between data capture and its arrival to decision makers. A major
bottleneck is in the bandwidth-limited downlinking of the data from satellites
to ground stations. One approach to overcome this challenge is to process at
least some of the data on-board and prioritise pertinent data to be downlinked.
In this work we propose a Physics Aware Neuromorphic Network (PANN) to detect
changes caused by natural disasters from a sequence of multi-spectral satellite
images and produce a change map, enabling relevant data to be prioritised for
downlinking. The PANN used in this study is motivated by physical neural
networks comprised of nano-electronic circuit elements known as "memristors"
(nonlinear resistors with memory). The weights in the network are dynamic and
update in response to varying input signals according to memristor equations of
state and electrical circuit conservation laws. The PANN thus generates
physics-constrained dynamical output features which are used to detect changes
in a natural disaster detection task by applying a distance-based metric.
Importantly, this makes the whole model training-free, allowing it to be
implemented with minimal computing resources. The PANN was benchmarked against
a state-of-the-art AI model and achieved comparable or better results in each
natural disaster category. It thus presents a promising solution to the
challenge of resource-constrained on-board processing.

</details>


### [73] [Backbone Augmented Training for Adaptations](https://arxiv.org/abs/2506.04288)
*Jae Wan Park,Junhyeok Kim,Youngjun Jun,Hyunah Ko,Seong Jae Hwang*

Main category: cs.LG

TL;DR: The paper proposes Backbone Augmented Training (BAT) to enhance adaptation training by leveraging backbone data, addressing challenges from limited adaptation data.


<details>
  <summary>Details</summary>
Motivation: Limited adaptation data hinders efficient training of large backbone models, prompting the need for methods like BAT to utilize backbone data for augmentation.

Method: BAT leverages backbone data to augment adaptation datasets, supported by two mathematical propositions. The ALBAT algorithm implements this with an advanced data selection scheme.

Result: ALBAT effectively improves adaptation training in personalization and language generation tasks with scarce data.

Conclusion: BAT and ALBAT offer a practical solution to enhance adaptation training by utilizing backbone data, validated by theoretical and empirical results.

Abstract: Adaptations facilitate efficient training of large backbone models, including
diffusion models for image generation and transformer-based language models.
While various adaptation techniques enhance performance with minimal
computational resources, limited adaptation data often leads to challenges in
training. To address this, we focus on the enormous amount of backbone data
used to pre-train the backbone models. We propose Backbone Augmented Training
(BAT), a method that leverages backbone data to augment the adaptation dataset.
First, we formulate and prove two mathematical key propositions: one
establishes the validity of BAT, while the other identifies a condition under
which BAT benefits adaptation. Furthermore, we introduce an advanced data
selection scheme that satisfies these propositions and present ALBAT algorithm
to implement this approach. ALBAT efficiently enhances adaptation training in
both personalization and language generation tasks with scarce data.

</details>


### [74] [Relational reasoning and inductive bias in transformers trained on a transitive inference task](https://arxiv.org/abs/2506.04289)
*Jesse Geerts,Stephanie Chan,Claudia Clopath,Kimberly Stachenfeld*

Main category: cs.LG

TL;DR: Transformers show distinct relational reasoning behaviors in in-weights (IWL) vs. in-context learning (ICL). IWL generalizes transitively, while ICL requires pre-training on structured tasks to achieve similar results.


<details>
  <summary>Details</summary>
Motivation: To understand how transformers perform relational reasoning, specifically transitive inference, and compare behaviors in IWL and ICL regimes.

Method: Analyzed transformers on transitive inference tasks, comparing IWL (parameter-based) and ICL (input-based) learning. Included mechanistic analysis and pre-training on structured tasks.

Result: IWL models generalize transitively; ICL models require pre-training on structured tasks to achieve similar results and exhibit human-like effects.

Conclusion: Pre-training on structured tasks enhances in-context relational reasoning, suggesting underlying representations scaffold reasoning abilities.

Abstract: Transformer-based models have demonstrated remarkable reasoning abilities,
but the mechanisms underlying relational reasoning in different learning
regimes remain poorly understood. In this work, we investigate how transformers
perform a classic relational reasoning task from the Psychology literature,
\textit{transitive inference}, which requires inference about indirectly
related items by integrating information across observed adjacent item pairs
(e.g., if A>B and B>C, then A>C). We compare transitive inference behavior
across two distinct learning regimes: in-weights learning (IWL), where models
store information in network parameters, and in-context learning (ICL), where
models flexibly utilize information presented within the input sequence. Our
findings reveal that IWL naturally induces a generalization bias towards
transitive inference, despite being trained only on adjacent items, whereas ICL
models trained solely on adjacent items do not generalize transitively.
Mechanistic analysis shows that ICL models develop induction circuits that
implement a simple match-and-copy strategy that performs well at relating
adjacent pairs, but does not encoding hierarchical relationships among
indirectly related items. Interestingly, when pre-trained on in-context linear
regression tasks, transformers successfully exhibit in-context generalizable
transitive inference. Moreover, like IWL, they display both \textit{symbolic
distance} and \textit{terminal item effects} characteristic of human and animal
performance, without forming induction circuits. These results suggest that
pre-training on tasks with underlying structure promotes the development of
representations that can scaffold in-context relational reasoning.

</details>


### [75] [A Lyapunov Drift-Plus-Penalty Method Tailored for Reinforcement Learning with Queue Stability](https://arxiv.org/abs/2506.04291)
*Wenhan Xu,Jiashuo Jiang,Lei Deng,Danny Hin-Kwok Tsang*

Main category: cs.LG

TL;DR: The paper introduces LDPTRLQ, a novel algorithm combining Lyapunov Drift-Plus-Penalty with reinforcement learning, offering theoretical and practical advantages over existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing complex optimization challenges in IoT by integrating Lyapunov Drift-Plus-Penalty with reinforcement learning for better performance and stability.

Method: Proposes LDPTRLQ, a method combining Lyapunov Drift-Plus-Penalty and RL under common conditions, balancing greedy optimization with long-term RL perspective.

Result: Simulations show LDPTRLQ outperforms baseline methods in compatibility, stability, and performance.

Conclusion: LDPTRLQ is theoretically superior and practically effective for IoT optimization, outperforming existing approaches.

Abstract: With the proliferation of Internet of Things (IoT) devices, the demand for
addressing complex optimization challenges has intensified. The Lyapunov
Drift-Plus-Penalty algorithm is a widely adopted approach for ensuring queue
stability, and some research has preliminarily explored its integration with
reinforcement learning (RL). In this paper, we investigate the adaptation of
the Lyapunov Drift-Plus-Penalty algorithm for RL applications, deriving an
effective method for combining Lyapunov Drift-Plus-Penalty with RL under a set
of common and reasonable conditions through rigorous theoretical analysis.
Unlike existing approaches that directly merge the two frameworks, our proposed
algorithm, termed Lyapunov drift-plus-penalty method tailored for reinforcement
learning with queue stability (LDPTRLQ) algorithm, offers theoretical
superiority by effectively balancing the greedy optimization of Lyapunov
Drift-Plus-Penalty with the long-term perspective of RL. Simulation results for
multiple problems demonstrate that LDPTRLQ outperforms the baseline methods
using the Lyapunov drift-plus-penalty method and RL, corroborating the validity
of our theoretical derivations. The results also demonstrate that our proposed
algorithm outperforms other benchmarks in terms of compatibility and stability.

</details>


### [76] [AUTOCT: Automating Interpretable Clinical Trial Prediction with LLM Agents](https://arxiv.org/abs/2506.04293)
*Fengze Liu,Haoyu Wang,Joonhyuk Cho,Dan Roth,Andrew W. Lo*

Main category: cs.LG

TL;DR: AutoCT combines large language models with classical ML for interpretable clinical trial outcome prediction, outperforming SOTA methods.


<details>
  <summary>Details</summary>
Motivation: Clinical trials are costly and slow; accurate outcome prediction can reduce costs and speed up drug discovery.

Method: AutoCT uses Monte Carlo Tree Search to autonomously generate and refine tabular features from public data.

Result: AutoCT matches or exceeds SOTA performance in clinical trial prediction tasks with few iterations.

Conclusion: AutoCT offers a scalable, interpretable, and cost-efficient solution for clinical trial prediction.

Abstract: Clinical trials are critical for advancing medical treatments but remain
prohibitively expensive and time-consuming. Accurate prediction of clinical
trial outcomes can significantly reduce research and development costs and
accelerate drug discovery. While recent deep learning models have shown promise
by leveraging unstructured data, their black-box nature, lack of
interpretability, and vulnerability to label leakage limit their practical use
in high-stakes biomedical contexts. In this work, we propose AutoCT, a novel
framework that combines the reasoning capabilities of large language models
with the explainability of classical machine learning. AutoCT autonomously
generates, evaluates, and refines tabular features based on public information
without human input. Our method uses Monte Carlo Tree Search to iteratively
optimize predictive performance. Experimental results show that AutoCT performs
on par with or better than SOTA methods on clinical trial prediction tasks
within only a limited number of self-refinement iterations, establishing a new
paradigm for scalable, interpretable, and cost-efficient clinical trial
prediction.

</details>


### [77] [Short-Term Power Demand Forecasting for Diverse Consumer Types to Enhance Grid Planning and Synchronisation](https://arxiv.org/abs/2506.04294)
*Asier Diaz-Iglesias,Xabier Belaunzaran,Ane M. Florez-Tapia*

Main category: cs.LG

TL;DR: The paper proposes customized forecasting models for industrial, commercial, and residential consumers, using AI/ML algorithms and detailed weather data to improve power demand forecasting accuracy, supporting grid stability.


<details>
  <summary>Details</summary>
Motivation: Accurate power demand forecasting is crucial for grid stability amid the transition to renewable energy. The study aims to address this by tailoring models to different consumer groups.

Method: Customer clusterisation is used to differentiate consumers. Feature selection includes temporal, socio-economic, and weather data. AI/ML algorithms are compared for STLF and VSTLF.

Result: The proposed forecasting approaches outperform simpler models, showing the importance of customized strategies and detailed weather data.

Conclusion: Customized forecasting for consumer groups and detailed weather data enhance accuracy, contributing to reliable power demand predictions and grid stability.

Abstract: Ensuring grid stability in the transition to renewable energy sources
requires accurate power demand forecasting. This study addresses the need for
precise forecasting by differentiating among industrial, commercial, and
residential consumers through customer clusterisation, tailoring the
forecasting models to capture the unique consumption patterns of each group. A
feature selection process is done for each consumer type including temporal,
socio-economic, and weather-related data obtained from the Copernicus Earth
Observation (EO) program. A variety of AI and machine learning algorithms for
Short-Term Load Forecasting (STLF) and Very Short-Term Load Forecasting (VSTLF)
are explored and compared, determining the most effective approaches. With all
that, the main contribution of this work are the new forecasting approaches
proposed, which have demonstrated superior performance compared to simpler
models, both for STLF and VSTLF, highlighting the importance of customized
forecasting strategies for different consumer groups and demonstrating the
impact of incorporating detailed weather data on forecasting accuracy. These
advancements contribute to more reliable power demand predictions, thereby
supporting grid stability.

</details>


### [78] [Deep learning for predicting hauling fleet production capacity under uncertainties in open pit mines using real and simulated data](https://arxiv.org/abs/2506.04296)
*N Guerin,M Nakhla,A Dehoux,J L Loyer*

Main category: cs.LG

TL;DR: A deep-learning framework for short-term hauling-fleet capacity forecasting in open-pit mining, combining real-world data with synthetic breakdown scenarios, outperforms traditional methods and reduces prediction volatility.


<details>
  <summary>Details</summary>
Motivation: Accurate forecasting is vital due to operational uncertainties like weather, mechanical breakdowns, and crew availability.

Method: Proposes a hybrid deep-learning approach using XGBoost and LSTM, integrating real and synthetic data (e.g., rainfall, breakdown scenarios).

Result: XGBoost achieves MedAE of 14.3%, LSTM 15.1%. SHAP analysis highlights key predictors like rainfall and breakdown frequencies.

Conclusion: The hybrid model improves fleet management under uncertainty, with future enhancements planned for maintenance and human resource data integration.

Abstract: Accurate short-term forecasting of hauling-fleet capacity is crucial in
open-pit mining, where weather fluctuations, mechanical breakdowns, and
variable crew availability introduce significant operational uncertainties. We
propose a deep-learning framework that blends real-world operational records
(high-resolution rainfall measurements, fleet performance telemetry) with
synthetically generated mechanical-breakdown scenarios to enable the model to
capture fluctuating high-impact failure events. We evaluate two architectures:
an XGBoost regressor achieving a median absolute error (MedAE) of 14.3 per cent
and a Long Short-Term Memory network with a MedAE of 15.1 per cent. Shapley
Additive exPlanations (SHAP) value analyses identify cumulative rainfall,
historical payload trends, and simulated breakdown frequencies as dominant
predictors. Integration of simulated breakdown data and shift-planning features
notably reduces prediction volatility. Future work will further integrate
maintenance-scheduling indicators (Mean Time Between Failures, Mean Time to
Repair), detailed human resource data (operator absenteeism, crew efficiency
metrics), blast event scheduling, and other operational constraints to enhance
forecast robustness and adaptability. This hybrid modelling approach offers a
comprehensive decision-support tool for proactive, data-driven fleet management
under dynamically uncertain conditions.

</details>


### [79] [Softlog-Softmax Layers and Divergences Contribute to a Computationally Dependable Ensemble Learning](https://arxiv.org/abs/2506.04297)
*Abdourrahmane Mahamane Atto*

Main category: cs.LG

TL;DR: The paper introduces a 4-step process using softlog-softmax cascades to enhance ensemble learning systems, focusing on diversity, neural engineering, information theory, and performance evaluation.


<details>
  <summary>Details</summary>
Motivation: To improve consistency and dependability in next-generation ensemble learning systems by leveraging softlog-softmax cascades and diverse canonical forms.

Method: A 4-step process: (1) anatomical selection of diverse canonical forms, (2) physiological derivation of softlog for consistency, (3) neural information theory for entropy/divergence measures, and (4) performance tensor derivation for evaluation.

Result: Demonstrates that ensemble diversity is more beneficial for complex problems and provides consistent intermediate decisions and information measures.

Conclusion: The proposed framework enhances ensemble learning by integrating diversity, consistency, and reliable evaluation metrics.

Abstract: The paper proposes a 4-step process for highlighting that softlog-softmax
cascades can improve both consistency and dependability of the next generation
ensemble learning systems. The first process is anatomical in nature: the
target ensemble model under consideration is composed by canonical elements
relating to the definition of a convolutional frustum. No a priori is
considered in the choice of canonical forms. Diversity is the main criterion
for selecting these forms. It is shown that the more complex the problem, the
more useful this ensemble diversity is. The second process is physiological and
relates to neural engineering: a softlog is derived to both make weak
logarithmic operations consistent and lead, through multiple softlog-softmax
layers, to intermediate decisions in the sense of respecting the same class
logic as that faced by the output layer. The third process concerns neural
information theory: softlog-based entropy and divergence are proposed for the
sake of constructing information measures yielding consistent values on closed
intervals. These information measures are used to determine the relationships
between individual and sub-community decisions in frustum diversitybased
ensemble learning. The concluding process addresses the derivation of an
informative performance tensor for the purpose of a reliable ensemble
evaluation.

</details>


### [80] [The Cost of Dynamic Reasoning: Demystifying AI Agents and Test-Time Scaling from an AI Infrastructure Perspective](https://arxiv.org/abs/2506.04301)
*Jiin Kim,Byeongjun Shin,Jinha Chung,Minsoo Rhu*

Main category: cs.LG

TL;DR: The paper analyzes the system-level costs of LLM-based AI agents, revealing high resource usage, latency, and energy demands, and calls for more efficient designs.


<details>
  <summary>Details</summary>
Motivation: To address concerns about the sustainability and efficiency of dynamic, multi-turn AI agents compared to static models.

Method: Comprehensive system-level analysis of resource usage, latency, energy, and power consumption across diverse agent designs and scaling strategies.

Result: Agents improve accuracy with more compute but face diminishing returns, high latency variance, and unsustainable costs.

Conclusion: A shift toward compute-efficient agent design is needed to balance performance and sustainability.

Abstract: Large-language-model (LLM)-based AI agents have recently showcased impressive
versatility by employing dynamic reasoning, an adaptive, multi-step process
that coordinates with external tools. This shift from static, single-turn
inference to agentic, multi-turn workflows broadens task generalization and
behavioral flexibility, but it also introduces serious concerns about
system-level cost, efficiency, and sustainability. This paper presents the
first comprehensive system-level analysis of AI agents, quantifying their
resource usage, latency behavior, energy consumption, and datacenter-wide power
consumption demands across diverse agent designs and test-time scaling
strategies. We further characterize how AI agent design choices, such as
few-shot prompting, reflection depth, and parallel reasoning, impact
accuracy-cost tradeoffs. Our findings reveal that while agents improve accuracy
with increased compute, they suffer from rapidly diminishing returns, widening
latency variance, and unsustainable infrastructure costs. Through detailed
evaluation of representative agents, we highlight the profound computational
demands introduced by AI agent workflows, uncovering a looming sustainability
crisis. These results call for a paradigm shift in agent design toward
compute-efficient reasoning, balancing performance with deployability under
real-world constraints.

</details>


### [81] [RedRFT: A Light-Weight Benchmark for Reinforcement Fine-Tuning-Based Red Teaming](https://arxiv.org/abs/2506.04302)
*Xiang Zheng,Xingjun Ma,Wei-Bin Lee,Cong Wang*

Main category: cs.LG

TL;DR: RedRFT is a lightweight benchmark for standardizing Reinforcement Fine-Tuning (RFT)-based red teaming in LLMs, addressing implementation inconsistencies and enabling rapid prototyping.


<details>
  <summary>Details</summary>
Motivation: The lack of a unified benchmark for RFT-based red teaming, especially in PPO-based implementations, affects stability and reproducibility.

Method: RedRFT combines CleanRL and Tianshou strengths, offering modular PPO components and diverse metrics for plug-and-play experimentation.

Result: An ablation study clarifies the impact of LoRA, KL divergence, and Lagrange Multiplier on RFT performance.

Conclusion: RedRFT aids in understanding RFT nuances and accelerates innovation in red teaming for LLMs.

Abstract: Red teaming has proven to be an effective method for identifying and
mitigating vulnerabilities in Large Language Models (LLMs). Reinforcement
Fine-Tuning (RFT) has emerged as a promising strategy among existing red
teaming techniques. However, a lack of a unified benchmark hinders current
RFT-based red teaming methods. Implementation details, especially in Proximal
Policy Optimization (PPO)-based RFT, significantly affect outcome stability and
reproducibility. To address this issue, we introduce RedRFT, a lightweight
benchmark designed to simplify and standardize the implementation and
evaluation of RFT-based red teaming. RedRFT combines the design strengths of
both single-file CleanRL and highly modularized Tianshou, offering high-quality
single-file red teaming implementations and modular PPO core components, such
as the General Advantage Estimator. It supports a variety of token and sentence
diversity metrics, featuring modularized intrinsic reward computation that
facilitates plug-and-play experimentation. To clarify their influence on RFT
performance, we conducted an extensive ablation study on key components,
including Low-Rank Adaptation (LoRA), Kullback-Leibler (KL) divergence, and
Lagrange Multiplier. We hope this work contributes to 1) gaining a
comprehensive understanding of the implementation nuances of RFT-based red
teaming algorithms, and 2) enabling rapid prototyping of innovative features
for RFT-based red teaming. Code for the benchmark can be accessed at
https://github.com/x-zheng16/RedRFT.git.

</details>


### [82] [You Only Train Once](https://arxiv.org/abs/2506.04349)
*Christos Sakaridis*

Main category: cs.LG

TL;DR: YOTO (You Only Train Once) automates loss weight optimization in one training run, outperforming grid search in tasks like 3D estimation and semantic segmentation.


<details>
  <summary>Details</summary>
Motivation: Traditional model training involves multiple runs for loss selection and weighting, which is inefficient. YOTO aims to streamline this by optimizing loss weights in one shot.

Method: YOTO treats loss weights as learnable parameters, using gradient-based optimization and a softmax layer to ensure positivity. A novel regularization loss ensures balanced and bounded optima.

Result: YOTO consistently outperforms grid-search models on unseen test data in 3D estimation and semantic segmentation tasks.

Conclusion: YOTO effectively optimizes loss weights in one training run, improving efficiency and performance over traditional methods.

Abstract: The title of this paper is perhaps an overclaim. Of course, the process of
creating and optimizing a learned model inevitably involves multiple training
runs which potentially feature different architectural designs, input and
output encodings, and losses. However, our method, You Only Train Once (YOTO),
indeed contributes to limiting training to one shot for the latter aspect of
losses selection and weighting. We achieve this by automatically optimizing
loss weight hyperparameters of learned models in one shot via standard
gradient-based optimization, treating these hyperparameters as regular
parameters of the networks and learning them. To this end, we leverage the
differentiability of the composite loss formulation which is widely used for
optimizing multiple empirical losses simultaneously and model it as a novel
layer which is parameterized with a softmax operation that satisfies the
inherent positivity constraints on loss hyperparameters while avoiding
degenerate empirical gradients. We complete our joint end-to-end optimization
scheme by defining a novel regularization loss on the learned hyperparameters,
which models a uniformity prior among the employed losses while ensuring
boundedness of the identified optima. We evidence the efficacy of YOTO in
jointly optimizing loss hyperparameters and regular model parameters in one
shot by comparing it to the commonly used brute-force grid search across
state-of-the-art networks solving two key problems in computer vision, i.e. 3D
estimation and semantic segmentation, and showing that it consistently
outperforms the best grid-search model on unseen test data. Code will be made
publicly available.

</details>


### [83] [Half-Layered Neural Networks](https://arxiv.org/abs/2506.04352)
*Ethem Alpaydin*

Main category: cs.LG

TL;DR: A 'half' layer of hidden units with partially randomized and partially trained weights is proposed, reducing parameters while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: To reduce the number of modifiable weights in neural networks without sacrificing performance, leveraging randomization for regularization.

Method: Half units use fixed random weights for input summation and two modifiable weights for scaling and translation, applied in deep networks.

Result: Experiments on MNIST and FashionMNIST show reasonable accuracy with fewer parameters due to randomized connections.

Conclusion: Half layers offer a promising approach for efficient neural network design by balancing randomization and training.

Abstract: We propose a ``half'' layer of hidden units that has some of its weights
randomly set and some of them trained. A half unit is composed of two stages:
First, it takes a weighted sum of its inputs with fixed random weights, and
second, the total activation is multiplied and then translated using two
modifiable weights, before the result is passed through a nonlinearity. The
number of modifiable weights of each hidden unit is thus two and does not
depend on the fan-in. We show how such half units can be used in the first or
any later layer in a deep network, possibly following convolutional layers. Our
experiments on MNIST and FashionMNIST data sets indicate the promise of half
layers, where we can achieve reasonable accuracy with a reduced number of
parameters due to the regularizing effect of the randomized connections.

</details>


### [84] [A Risk-Aware Reinforcement Learning Reward for Financial Trading](https://arxiv.org/abs/2506.04358)
*Uditansh Srivastava,Shivam Aryan,Shaurya Singh*

Main category: cs.LG

TL;DR: A novel composite reward function for financial trading RL balances return and risk using four differentiable terms, enabling customizable investor preferences via weight tuning.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of single-metric objectives like the Sharpe ratio by providing a modular, parameterized reward function that can encode diverse investor preferences.

Method: Proposes a composite reward function with four terms (annualized return, downside risk, differential return, Treynor ratio) and tunes weights via grid search. Derives closed-form gradients for gradient-based training.

Result: The framework offers a robust, modular approach for multi-objective reward functions in trading, with theoretical properties like monotonicity and boundedness.

Conclusion: This method provides a general blueprint for adaptable, multi-objective reward functions in trading, extendable with additional risk measures or adaptive weighting.

Abstract: We propose a novel composite reward function for reinforcement learning in
financial trading that balances return and risk using four differentiable
terms: annualized return downside risk differential return and the Treynor
ratio
  Unlike single metric objectives for example the Sharpe ratio our formulation
is modular and parameterized by weights w1 w2 w3 and w4 enabling practitioners
to encode diverse investor preferences
  We tune these weights via grid search to target specific risk return profiles
  We derive closed form gradients for each term to facilitate gradient based
training and analyze key theoretical properties including monotonicity
boundedness and modularity
  This framework offers a general blueprint for building robust multi objective
reward functions in complex trading environments and can be extended with
additional risk measures or adaptive weighting

</details>


### [85] [Even Faster Hyperbolic Random Forests: A Beltrami-Klein Wrapper Approach](https://arxiv.org/abs/2506.04360)
*Philippe Chlenski,Itsik Pe'er*

Main category: cs.LG

TL;DR: The paper re-expresses the hyperDT algorithm in the Beltrami-Klein model of hyperbolic space, simplifying it into pre- and post-processing steps around Euclidean tree models.


<details>
  <summary>Details</summary>
Motivation: To improve flexibility, speed, and accuracy of hyperbolic decision trees by leveraging existing Euclidean optimizations.

Method: Reformulating hyperDT in the Beltrami-Klein model to use thresholding operations, enabling a wrapper around Euclidean tree models.

Result: Enhanced performance, simpler codebase, and access to Euclidean optimizations.

Conclusion: The wrapper approach offers a maintainable and extensible solution for hyperbolic decision trees.

Abstract: Decision trees and models that use them as primitives are workhorses of
machine learning in Euclidean spaces. Recent work has further extended these
models to the Lorentz model of hyperbolic space by replacing axis-parallel
hyperplanes with homogeneous hyperplanes when partitioning the input space. In
this paper, we show how the hyperDT algorithm can be elegantly reexpressed in
the Beltrami-Klein model of hyperbolic spaces. This preserves the thresholding
operation used in Euclidean decision trees, enabling us to further rewrite
hyperDT as simple pre- and post-processing steps that form a wrapper around
existing tree-based models designed for Euclidean spaces. The wrapper approach
unlocks many optimizations already available in Euclidean space models,
improving flexibility, speed, and accuracy while offering a simpler, more
maintainable, and extensible codebase. Our implementation is available at
https://github.com/pchlenski/hyperdt.

</details>


### [86] [Replay Can Provably Increase Forgetting](https://arxiv.org/abs/2506.04377)
*Yasaman Mahdaviyeh,James Lucas,Mengye Ren,Andreas S. Tolias,Richard Zemel,Toniann Pitassi*

Main category: cs.LG

TL;DR: The paper theoretically analyzes sample replay in continual learning, showing it can sometimes increase forgetting, even in noiseless settings, and highlights the importance of sample selection and task relationships.


<details>
  <summary>Details</summary>
Motivation: To understand the theoretical underpinnings of sample replay in continual learning and its potential pitfalls, despite its empirical success in mitigating forgetting.

Method: Theoretical analysis of sample replay in an over-parameterized continual linear regression setting, examining the role of replayed samples and task subspace relationships.

Result: Forgetting can be non-monotonic with replay samples; replay may harm performance in worst-case or distributional settings. Empirical evidence extends these findings to neural networks.

Conclusion: Sample replay's effectiveness depends on sample choice and task relationships, and it can sometimes worsen forgetting, challenging its universal applicability.

Abstract: Continual learning seeks to enable machine learning systems to solve an
increasing corpus of tasks sequentially. A critical challenge for continual
learning is forgetting, where the performance on previously learned tasks
decreases as new tasks are introduced. One of the commonly used techniques to
mitigate forgetting, sample replay, has been shown empirically to reduce
forgetting by retaining some examples from old tasks and including them in new
training episodes. In this work, we provide a theoretical analysis of sample
replay in an over-parameterized continual linear regression setting, where each
task is given by a linear subspace and with enough replay samples, one would be
able to eliminate forgetting. Our analysis focuses on sample replay and
highlights the role of the replayed samples and the relationship between task
subspaces. Surprisingly, we find that, even in a noiseless setting, forgetting
can be non-monotonic with respect to the number of replay samples. We present
tasks where replay can be harmful with respect to worst-case settings, and also
in distributional settings where replay of randomly selected samples increases
forgetting in expectation. We also give empirical evidence that harmful replay
is not limited to training with linear models by showing similar behavior for a
neural networks equipped with SGD. Through experiments on a commonly used
benchmark, we provide additional evidence that, even in seemingly benign
scenarios, performance of the replay heavily depends on the choice of replay
samples and the relationship between tasks.

</details>


### [87] [Bridging the Performance Gap Between Target-Free and Target-Based Reinforcement Learning With Iterated Q-Learning](https://arxiv.org/abs/2506.04398)
*Théo Vincent,Yogesh Tripathi,Tim Faust,Yaniv Oren,Jan Peters,Carlo D'Eramo*

Main category: cs.LG

TL;DR: The paper proposes a method called iterated Shared Q-Learning (iS-QL) to balance the trade-off between target-free and target-based reinforcement learning, improving sample efficiency without significant memory or time overhead.


<details>
  <summary>Details</summary>
Motivation: Removing the target network in value-based RL can free memory but introduces instability. The goal is to leverage the benefits of target networks while minimizing drawbacks.

Method: iS-QL uses a copy of the last linear layer of the online network as a target, sharing other parameters. It integrates iterated Q-learning to bridge the performance gap.

Result: iS-QL improves sample efficiency in target-free approaches, with smaller memory usage and comparable training time to target-based methods.

Conclusion: iS-QL offers a scalable solution for RL, balancing performance and resource efficiency.

Abstract: In value-based reinforcement learning, removing the target network is
tempting as the boostrapped target would be built from up-to-date estimates,
and the spared memory occupied by the target network could be reallocated to
expand the capacity of the online network. However, eliminating the target
network introduces instability, leading to a decline in performance. Removing
the target network also means we cannot leverage the literature developed
around target networks. In this work, we propose to use a copy of the last
linear layer of the online network as a target network, while sharing the
remaining parameters with the up-to-date online network, hence stepping out of
the binary choice between target-based and target-free methods. It enables us
to leverage the concept of iterated Q-learning, which consists of learning
consecutive Bellman iterations in parallel, to reduce the performance gap
between target-free and target-based approaches. Our findings demonstrate that
this novel method, termed iterated Shared Q-Learning (iS-QL), improves the
sample efficiency of target-free approaches across various settings.
Importantly, iS-QL requires a smaller memory footprint and comparable training
time to classical target-based algorithms, highlighting its potential to scale
reinforcement learning research.

</details>


### [88] [Unsupervised Meta-Testing with Conditional Neural Processes for Hybrid Meta-Reinforcement Learning](https://arxiv.org/abs/2506.04399)
*Suzan Ece Ada,Emre Ugur*

Main category: cs.LG

TL;DR: UMCNP is a hybrid few-shot meta-RL method combining policy gradient and task inference, improving sample efficiency without extra meta-training samples by leveraging CNPs for dynamics model inference.


<details>
  <summary>Details</summary>
Motivation: Addresses the challenge of missing reward signals during meta-testing while enhancing sample efficiency.

Method: Combines PPG meta-RL and task inference, uses CNPs for dynamics model inference, and reuses meta-training samples offline.

Result: Achieves adaptation to unseen tasks with fewer samples in benchmarks like 2D-Point Agent and continuous control tasks.

Conclusion: UMCNP effectively reduces online interactions in meta-testing and outperforms baselines in sample efficiency.

Abstract: We introduce Unsupervised Meta-Testing with Conditional Neural Processes
(UMCNP), a novel hybrid few-shot meta-reinforcement learning (meta-RL) method
that uniquely combines, yet distinctly separates, parameterized policy
gradient-based (PPG) and task inference-based few-shot meta-RL. Tailored for
settings where the reward signal is missing during meta-testing, our method
increases sample efficiency without requiring additional samples in
meta-training. UMCNP leverages the efficiency and scalability of Conditional
Neural Processes (CNPs) to reduce the number of online interactions required in
meta-testing. During meta-training, samples previously collected through PPG
meta-RL are efficiently reused for learning task inference in an offline
manner. UMCNP infers the latent representation of the transition dynamics model
from a single test task rollout with unknown parameters. This approach allows
us to generate rollouts for self-adaptation by interacting with the learned
dynamics model. We demonstrate our method can adapt to an unseen test task
using significantly fewer samples during meta-testing than the baselines in
2D-Point Agent and continuous control meta-RL benchmarks, namely, cartpole with
unknown angle sensor bias, walker agent with randomized dynamics parameters.

</details>


### [89] [Self-Supervised Contrastive Learning is Approximately Supervised Contrastive Learning](https://arxiv.org/abs/2506.04411)
*Achleshwar Luthra,Tianbao Yang,Tomer Galanti*

Main category: cs.LG

TL;DR: The paper bridges the gap in theoretical understanding of self-supervised contrastive learning (CL) by linking it to a supervised variant (NSCL), proving convergence under certain conditions, and analyzing geometric properties of learned representations. It also introduces a bound for few-shot error and validates findings empirically.


<details>
  <summary>Details</summary>
Motivation: To establish theoretical foundations for self-supervised contrastive learning (CL) by connecting it to a supervised variant (NSCL) and analyzing its properties.

Method: Theoretical analysis of CL and NSCL losses, geometric characterization of learned representations, and introduction of a few-shot error bound. Empirical validation of theoretical results.

Result: The gap between CL and NSCL losses vanishes with increasing semantic classes. Learned representations exhibit specific geometric properties. The few-shot error bound is tight and depends on feature variability.

Conclusion: CL implicitly approximates NSCL, and the theoretical insights align with empirical results, supporting accurate few-shot learning with linear probes.

Abstract: Despite its empirical success, the theoretical foundations of self-supervised
contrastive learning (CL) are not yet fully established. In this work, we
address this gap by showing that standard CL objectives implicitly approximate
a supervised variant we call the negatives-only supervised contrastive loss
(NSCL), which excludes same-class contrasts. We prove that the gap between the
CL and NSCL losses vanishes as the number of semantic classes increases, under
a bound that is both label-agnostic and architecture-independent.
  We characterize the geometric structure of the global minimizers of the NSCL
loss: the learned representations exhibit augmentation collapse, within-class
collapse, and class centers that form a simplex equiangular tight frame. We
further introduce a new bound on the few-shot error of linear-probing. This
bound depends on two measures of feature variability--within-class dispersion
and variation along the line between class centers. We show that directional
variation dominates the bound and that the within-class dispersion's effect
diminishes as the number of labeled samples increases. These properties enable
CL and NSCL-trained representations to support accurate few-shot label recovery
using simple linear probes.
  Finally, we empirically validate our theoretical findings: the gap between CL
and NSCL losses decays at a rate of $\mathcal{O}(\frac{1}{\#\text{classes}})$;
the two losses are highly correlated; minimizing the CL loss implicitly brings
the NSCL loss close to the value achieved by direct minimization; and the
proposed few-shot error bound provides a tight estimate of probing performance
in practice.

</details>


### [90] [Leveraging Coordinate Momentum in SignSGD and Muon: Memory-Optimized Zero-Order](https://arxiv.org/abs/2506.04430)
*Egor Petrov,Grigoriy Evseev,Aleksey Antonov,Andrey Veprikov,Pavel Plyusnin,Nikolay Bushkov,Stanislav Moiseev,Aleksandr Beznosikov*

Main category: cs.LG

TL;DR: The paper introduces zero-order (ZO) optimization methods, specifically JAGUAR SignSGD and JAGUAR Muon, as efficient alternatives to traditional first-order optimizers for fine-tuning large language models (LLMs), reducing memory and computational costs while maintaining convergence quality.


<details>
  <summary>Details</summary>
Motivation: Traditional first-order optimizers like SGD and Adam are computationally expensive for fine-tuning LLMs. The paper explores ZO methods to address these inefficiencies, focusing on parameter-efficient techniques like LoRA.

Method: The authors propose JAGUAR SignSGD, a ZO momentum-based algorithm, and JAGUAR Muon, a ZO extension of the Muon optimizer. Both methods aim to reduce memory usage and computational overhead while ensuring convergence.

Result: Experiments show the proposed ZO methods match or surpass the convergence quality of first-order methods, achieving significant memory savings. Theoretical guarantees for convergence are also provided.

Conclusion: The study establishes ZO optimization as a practical and theoretically sound approach for resource-efficient LLM fine-tuning, with promising empirical and theoretical results.

Abstract: Fine-tuning Large Language Models (LLMs) is essential for adapting
pre-trained models to downstream tasks. Yet traditional first-order optimizers
such as Stochastic Gradient Descent (SGD) and Adam incur prohibitive memory and
computational costs that scale poorly with model size. In this paper, we
investigate zero-order (ZO) optimization methods as a memory- and
compute-efficient alternative, particularly in the context of
parameter-efficient fine-tuning techniques like LoRA. We propose
$\texttt{JAGUAR SignSGD}$, a ZO momentum-based algorithm that extends ZO
SignSGD, requiring the same number of parameters as the standard ZO SGD and
only $\mathcal{O}(1)$ function evaluations per iteration. To the best of our
knowledge, this is the first study to establish rigorous convergence guarantees
for SignSGD in the stochastic ZO case. We further propose $\texttt{JAGUAR
Muon}$, a novel ZO extension of the Muon optimizer that leverages the matrix
structure of model parameters, and we provide its convergence rate under
arbitrary stochastic noise. Through extensive experiments on challenging LLM
fine-tuning benchmarks, we demonstrate that the proposed algorithms meet or
exceed the convergence quality of standard first-order methods, achieving
significant memory reduction. Our theoretical and empirical results establish
new ZO optimization methods as a practical and theoretically grounded approach
for resource-constrained LLM adaptation. Our code is available at
https://github.com/brain-mmo-lab/ZO_LLM

</details>


### [91] [KOALA++: Efficient Kalman-Based Optimization of Neural Networks with Gradient-Covariance Products](https://arxiv.org/abs/2506.04432)
*Zixuan Xia,Aram Davtyan,Paolo Favaro*

Main category: cs.LG

TL;DR: KOALA++ is a scalable Kalman-based optimization algorithm for neural network training, improving upon KOALA by modeling structured gradient uncertainty efficiently without costly second-order calculations.


<details>
  <summary>Details</summary>
Motivation: Second-order methods are computationally expensive due to second-order gradient calculations. KOALA++ aims to capture richer uncertainty structures more efficiently.

Method: KOALA++ estimates the parameter covariance matrix recursively using compact gradient covariance products, avoiding full matrix storage and inversions.

Result: KOALA++ matches or outperforms state-of-the-art optimizers in tasks like image classification and language modeling while maintaining first-order efficiency.

Conclusion: KOALA++ offers a scalable and efficient alternative to second-order methods, balancing accuracy and computational cost.

Abstract: We propose KOALA++, a scalable Kalman-based optimization algorithm that
explicitly models structured gradient uncertainty in neural network training.
Unlike second-order methods, which rely on expensive second order gradient
calculation, our method directly estimates the parameter covariance matrix by
recursively updating compact gradient covariance products. This design improves
upon the original KOALA framework that assumed diagonal covariance by
implicitly capturing richer uncertainty structure without storing the full
covariance matrix and avoiding large matrix inversions. Across diverse tasks,
including image classification and language modeling, KOALA++ achieves accuracy
on par or better than state-of-the-art first- and second-order optimizers while
maintaining the efficiency of first-order methods.

</details>


### [92] [Grokking and Generalization Collapse: Insights from \texttt{HTSR} theory](https://arxiv.org/abs/2506.04434)
*Hari K. Prakash,Charles H. Martin*

Main category: cs.LG

TL;DR: The paper identifies a novel 'anti-grokking' phase in neural networks, distinct from pre-grokking and grokking, where test accuracy collapses while training accuracy remains high. The Heavy-Tailed Self-Regularization (HTSR) metric α effectively detects all three phases, unlike other metrics.


<details>
  <summary>Details</summary>
Motivation: To understand the grokking phenomena in neural networks and identify new phases of behavior, particularly focusing on late-stage generalization collapse.

Method: A 3-layer MLP is trained on a 1k-sample MNIST subset with/without weight decay. The HTSR metric α is used to analyze layer quality, alongside other metrics like activation sparsity and weight norms.

Result: Anti-grokking is a distinct phase marked by α < 2 and Correlation Traps, signaling overfitting. HTSR α uniquely identifies all phases, while other metrics fail.

Conclusion: HTSR α is a universal layer-convergence target (α ≈ 2) and a robust measure of generalization, providing insights into overfitting without test data access.

Abstract: We study the well-known grokking phenomena in neural networks (NNs) using a
3-layer MLP trained on 1 k-sample subset of MNIST, with and without weight
decay, and discover a novel third phase -- \emph{anti-grokking} -- that occurs
very late in training and resembles but is distinct from the familiar
\emph{pre-grokking} phases: test accuracy collapses while training accuracy
stays perfect. This late-stage collapse is distinct, from the known
pre-grokking and grokking phases, and is not detected by other proposed
grokking progress measures. Leveraging Heavy-Tailed Self-Regularization HTSR
through the open-source WeightWatcher tool, we show that the HTSR layer quality
metric $\alpha$ alone delineates all three phases, whereas the best competing
metrics detect only the first two. The \emph{anti-grokking} is revealed by
training for $10^7$ and is invariably heralded by $\alpha < 2$ and the
appearance of \emph{Correlation Traps} -- outlier singular values in the
randomized layer weight matrices that make the layer weight matrix atypical and
signal overfitting of the training set. Such traps are verified by visual
inspection of the layer-wise empirical spectral densities, and by using
Kolmogorov--Smirnov tests on randomized spectra. Comparative metrics, including
activation sparsity, absolute weight entropy, circuit complexity, and $l^2$
weight norms track pre-grokking and grokking but fail to distinguish grokking
from anti-grokking. This discovery provides a way to measure overfitting and
generalization collapse without direct access to the test data. These results
strengthen the claim that the \emph{HTSR} $\alpha$ provides universal
layer-convergence target at $\alpha \approx 2$ and underscore the value of
using the HTSR alpha $(\alpha)$ metric as a measure of generalization.

</details>


### [93] [RETRO SYNFLOW: Discrete Flow Matching for Accurate and Diverse Single-Step Retrosynthesis](https://arxiv.org/abs/2506.04439)
*Robin Yadav,Qi Yan,Guy Wolf,Avishek Joey Bose,Renjie Liao*

Main category: cs.LG

TL;DR: RETRO SYNFLOW (RSF) introduces a discrete flow-matching framework for single-step retrosynthesis, outperforming SOTA with 60% top-1 accuracy and improving diversity via Feynman-Kac steering.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of accurate and diverse single-step retrosynthesis prediction in organic chemistry due to the combinatorial chemical search space.

Method: RSF uses a Markov bridge between target and reactant molecules, reaction center identification for synthons, and Feynman-Kac steering with Sequential Monte Carlo resampling.

Result: Achieves 60.0% top-1 accuracy (20% better than SOTA) and 19% improvement in top-5 round-trip accuracy.

Conclusion: RSF is a robust template-free method for retrosynthesis, enhancing accuracy and feasibility through innovative steering techniques.

Abstract: A fundamental problem in organic chemistry is identifying and predicting the
series of reactions that synthesize a desired target product molecule. Due to
the combinatorial nature of the chemical search space, single-step reactant
prediction -- i.e. single-step retrosynthesis -- remains challenging even for
existing state-of-the-art template-free generative approaches to produce an
accurate yet diverse set of feasible reactions. In this paper, we model
single-step retrosynthesis planning and introduce RETRO SYNFLOW (RSF) a
discrete flow-matching framework that builds a Markov bridge between the
prescribed target product molecule and the reactant molecule. In contrast to
past approaches, RSF employs a reaction center identification step to produce
intermediate structures known as synthons as a more informative source
distribution for the discrete flow. To further enhance diversity and
feasibility of generated samples, we employ Feynman-Kac steering with
Sequential Monte Carlo based resampling to steer promising generations at
inference using a new reward oracle that relies on a forward-synthesis model.
Empirically, we demonstrate \nameshort achieves $60.0 \%$ top-1 accuracy, which
outperforms the previous SOTA by $20 \%$. We also substantiate the benefits of
steering at inference and demonstrate that FK-steering improves top-$5$
round-trip accuracy by $19 \%$ over prior template-free SOTA methods, all while
preserving competitive top-$k$ accuracy results.

</details>


### [94] [Selective Matching Losses -- Not All Scores Are Created Equal](https://arxiv.org/abs/2506.04446)
*Gil I. Shamir,Manfred K. Warmuth*

Main category: cs.LG

TL;DR: The paper introduces selective matching loss functions to prioritize accurate predictions in specific regions of a domain, using link functions to emphasize high-sensitivity areas. It extends these losses to multi-class settings and demonstrates their advantages in various applications.


<details>
  <summary>Details</summary>
Motivation: Accurate predictions are often critical in specific regions of a domain, but traditional loss functions treat all regions equally. The paper aims to address this by designing loss functions that prioritize high-importance regions.

Method: The authors construct selective matching loss functions using increasing link functions (e.g., Sigmoid, hyperbolic sine) to emphasize high-sensitivity regions. They extend this to multi-class settings using composite Softmax functions.

Result: Selective losses outperform traditional losses in applications like dwell-time prediction, retrieval, ranking, and fine-tuning LLMs, by better distinguishing between high and low importance regions.

Conclusion: Selective loss functions provide a flexible and effective way to improve prediction accuracy in critical regions, with broad applicability in machine learning tasks.

Abstract: Learning systems match predicted scores to observations over some domain.
Often, it is critical to produce accurate predictions in some subset (or
region) of the domain, yet less important to accurately predict in other
regions. We construct selective matching loss functions by design of increasing
link functions over score domains. A matching loss is an integral over the
link. A link defines loss sensitivity as function of the score, emphasizing
high slope high sensitivity regions over flat ones. Loss asymmetry drives a
model and resolves its underspecification to predict better in high sensitivity
regions where it is more important, and to distinguish between high and low
importance regions. A large variety of selective scalar losses can be designed
with scaled and shifted Sigmoid and hyperbolic sine links. Their properties,
however, do not extend to multi-class. Applying them per dimension lacks
ranking sensitivity that assigns importance according to class score ranking.
Utilizing composite Softmax functions, we develop a framework for
multidimensional selective losses. We overcome limitations of the standard
Softmax function, that is good for classification, but not for distinction
between adjacent scores. Selective losses have substantial advantage over
traditional losses in applications with more important score regions, including
dwell-time prediction, retrieval, ranking with either pointwise, contrastive
pairwise, or listwise losses, distillation problems, and fine-tuning alignment
of Large Language Models (LLMs).

</details>


### [95] [Neurosymbolic Artificial Intelligence for Robust Network Intrusion Detection: From Scratch to Transfer Learning](https://arxiv.org/abs/2506.04454)
*Huynh T. T. Tran,Jacob Sander,Achraf Cohen,Brian Jalaian,Nathaniel D. Bastian*

Main category: cs.LG

TL;DR: The paper extends ODXU, a Neurosymbolic AI framework, for Network Intrusion Detection Systems (NIDS), integrating deep clustering, symbolic reasoning, and uncertainty quantification. It outperforms traditional models and explores transfer learning in cybersecurity.


<details>
  <summary>Details</summary>
Motivation: To enhance robustness, interpretability, and generalization in NIDS by extending the ODXU framework with advanced uncertainty quantification and transfer learning capabilities.

Method: Extends ODXU with score-based and metamodel-based uncertainty quantification methods, and develops a transfer learning strategy for cybersecurity applications.

Result: ODXU outperforms traditional neural models on the CIC-IDS-2017 dataset and shows effective transfer learning on the ACI-IoT-2023 dataset with minimal data.

Conclusion: The extended ODXU framework improves NIDS performance and demonstrates the viability of transfer learning in cybersecurity, with metamodel-based UQ methods being superior.

Abstract: Network Intrusion Detection Systems (NIDS) play a vital role in protecting
digital infrastructures against increasingly sophisticated cyber threats. In
this paper, we extend ODXU, a Neurosymbolic AI (NSAI) framework that integrates
deep embedded clustering for feature extraction, symbolic reasoning using
XGBoost, and comprehensive uncertainty quantification (UQ) to enhance
robustness, interpretability, and generalization in NIDS. The extended ODXU
incorporates score-based methods (e.g., Confidence Scoring, Shannon Entropy)
and metamodel-based techniques, including SHAP values and Information Gain, to
assess the reliability of predictions. Experimental results on the CIC-IDS-2017
dataset show that ODXU outperforms traditional neural models across six
evaluation metrics, including classification accuracy and false omission rate.
While transfer learning has seen widespread adoption in fields such as computer
vision and natural language processing, its potential in cybersecurity has not
been thoroughly explored. To bridge this gap, we develop a transfer learning
strategy that enables the reuse of a pre-trained ODXU model on a different
dataset. Our ablation study on ACI-IoT-2023 demonstrates that the optimal
transfer configuration involves reusing the pre-trained autoencoder, retraining
the clustering module, and fine-tuning the XGBoost classifier, and outperforms
traditional neural models when trained with as few as 16,000 samples
(approximately 50% of the training data). Additionally, results show that
metamodel-based UQ methods consistently outperform score-based approaches on
both datasets.

</details>


### [96] [Behavioural vs. Representational Systematicity in End-to-End Models: An Opinionated Survey](https://arxiv.org/abs/2506.04461)
*Ivan Vegner,Sydelle de Souza,Valentin Forch,Martha Lewis,Leonidas A. A. Doumas*

Main category: cs.LG

TL;DR: The paper discusses the distinction between systematicity of behavior and representations in ML models, analyzes benchmarks for behavioral systematicity, and suggests assessing representational systematicity using mechanistic interpretability.


<details>
  <summary>Details</summary>
Motivation: To address the gap in ML models where systematicity is often measured by behavior rather than representations, as argued by Fodor and Pylyshyn.

Method: Analyzes benchmarks for behavioral systematicity in language and vision, and explores mechanistic interpretability for assessing representational systematicity.

Result: Highlights the limitations of current benchmarks in testing representational systematicity and proposes alternative assessment methods.

Conclusion: Emphasizes the need to focus on systematicity of representations in ML models and suggests leveraging mechanistic interpretability for better evaluation.

Abstract: A core aspect of compositionality, systematicity is a desirable property in
ML models as it enables strong generalization to novel contexts. This has led
to numerous studies proposing benchmarks to assess systematic generalization,
as well as models and training regimes designed to enhance it. Many of these
efforts are framed as addressing the challenge posed by Fodor and Pylyshyn.
However, while they argue for systematicity of representations, existing
benchmarks and models primarily focus on the systematicity of behaviour. We
emphasize the crucial nature of this distinction. Furthermore, building on
Hadley's (1994) taxonomy of systematic generalization, we analyze the extent to
which behavioural systematicity is tested by key benchmarks in the literature
across language and vision. Finally, we highlight ways of assessing
systematicity of representations in ML models as practiced in the field of
mechanistic interpretability.

</details>


### [97] [Classifying Dental Care Providers Through Machine Learning with Features Ranking](https://arxiv.org/abs/2506.04474)
*Mohammad Subhi Al-Batah,Mowafaq Salem Alzboon,Muhyeeddin Alqaraleh,Mohammed Hasan Abu-Arqoub,Rashiq Rafiq Marie*

Main category: cs.LG

TL;DR: The study uses ML models to classify dental providers into standard and safety net clinic categories, achieving high accuracy (94.1%) with Neural Networks. Feature selection and ranking are key to performance.


<details>
  <summary>Details</summary>
Motivation: To improve classification of dental providers for better resource allocation, especially for underserved populations, using ML models on a dataset with high missing values.

Method: Evaluated 12 ML models (e.g., Neural Networks, Gradient Boosting) on a 2018 dataset with 20 features, using feature ranking (information gain, Gini index, ANOVA) and 10-fold cross-validation.

Result: Neural Network achieved the highest accuracy (94.1%). Treatment-related metrics were top-ranked features, while demographics had minimal impact.

Conclusion: Feature selection and advanced ML models enhance classification accuracy, aiding targeted resource allocation in healthcare.

Abstract: This study investigates the application of machine learning (ML) models for
classifying dental providers into two categories - standard rendering providers
and safety net clinic (SNC) providers - using a 2018 dataset of 24,300
instances with 20 features. The dataset, characterized by high missing values
(38.1%), includes service counts (preventive, treatment, exams), delivery
systems (FFS, managed care), and beneficiary demographics. Feature ranking
methods such as information gain, Gini index, and ANOVA were employed to
identify critical predictors, revealing treatment-related metrics
(TXMT_USER_CNT, TXMT_SVC_CNT) as top-ranked features. Twelve ML models,
including k-Nearest Neighbors (kNN), Decision Trees, Support Vector Machines
(SVM), Stochastic Gradient Descent (SGD), Random Forest, Neural Networks, and
Gradient Boosting, were evaluated using 10-fold cross-validation.
Classification accuracy was tested across incremental feature subsets derived
from rankings. The Neural Network achieved the highest accuracy (94.1%) using
all 20 features, followed by Gradient Boosting (93.2%) and Random Forest
(93.0%). Models showed improved performance as more features were incorporated,
with SGD and ensemble methods demonstrating robustness to missing data. Feature
ranking highlighted the dominance of treatment service counts and annotation
codes in distinguishing provider types, while demographic variables (AGE_GROUP,
CALENDAR_YEAR) had minimal impact. The study underscores the importance of
feature selection in enhancing model efficiency and accuracy, particularly in
imbalanced healthcare datasets. These findings advocate for integrating
feature-ranking techniques with advanced ML algorithms to optimize dental
provider classification, enabling targeted resource allocation for underserved
populations.

</details>


### [98] [Comparative performance of ensemble models in predicting dental provider types: insights from fee-for-service data](https://arxiv.org/abs/2506.04479)
*Mohammad Subhi Al-Batah,Muhyeeddin Alqaraleh,Mowafaq Salem Alzboon,Abdullah Alourani*

Main category: cs.LG

TL;DR: Machine learning models, especially Neural Networks and Random Forest, outperform traditional methods in classifying dental providers, aiding healthcare resource allocation.


<details>
  <summary>Details</summary>
Motivation: Effective classification of dental providers (e.g., standard vs. safety net clinic providers) is crucial for optimizing healthcare resource allocation and serving underserved populations.

Method: Analyzed a 2018 dataset of 24,300 instances with 20 features, testing multiple ML algorithms (kNN, Decision Trees, SVM, SGD, Random Forest, Neural Networks, Gradient Boosting) using 10-fold cross-validation and metrics like AUC, CA, F1-score, precision, and recall.

Result: Neural Networks achieved the highest AUC (0.975) and CA (94.1%), followed by Random Forest (AUC: 0.948, CA: 93.0%), outperforming traditional classifiers.

Conclusion: Advanced ML models, particularly ensemble and deep learning, enhance dental provider classification, improving healthcare analytics and resource distribution for underserved populations.

Abstract: Dental provider classification plays a crucial role in optimizing healthcare
resource allocation and policy planning. Effective categorization of providers,
such as standard rendering providers and safety net clinic (SNC) providers,
enhances service delivery to underserved populations. This study aimed to
evaluate the performance of machine learning models in classifying dental
providers using a 2018 dataset. A dataset of 24,300 instances with 20 features
was analyzed, including beneficiary and service counts across fee-for-service
(FFS), Geographic Managed Care, and Pre-Paid Health Plans. Providers were
categorized by delivery system and patient age groups (0-20 and 21+). Despite
38.1% missing data, multiple machine learning algorithms were tested, including
k-Nearest Neighbors (kNN), Decision Trees, Support Vector Machines (SVM),
Stochastic Gradient Descent (SGD), Random Forest, Neural Networks, and Gradient
Boosting. A 10-fold cross-validation approach was applied, and models were
evaluated using AUC, classification accuracy (CA), F1-score, precision, and
recall. Neural Networks achieved the highest AUC (0.975) and CA (94.1%),
followed by Random Forest (AUC: 0.948, CA: 93.0%). These models effectively
handled imbalanced data and complex feature interactions, outperforming
traditional classifiers like Logistic Regression and SVM. Advanced machine
learning techniques, particularly ensemble and deep learning models,
significantly enhance dental workforce classification. Their integration into
healthcare analytics can improve provider identification and resource
distribution, benefiting underserved populations.

</details>


### [99] [Orthogonal Gradient Descent Improves Neural Calibration](https://arxiv.org/abs/2506.04487)
*C. Evans Hedges*

Main category: cs.LG

TL;DR: Orthogonalizing gradients ($\perp$Grad) during training improves model calibration without losing accuracy, outperforming SGD in calibration metrics and robustness.


<details>
  <summary>Details</summary>
Motivation: To enhance model calibration (e.g., lower test loss, reduced overconfidence) while maintaining accuracy, especially in limited-data or corrupted-input scenarios.

Method: Introduces $\perp$Grad, an optimizer-agnostic technique that orthogonalizes gradients during training, incurring minimal overhead.

Result: $\perp$Grad matches SGD in accuracy but improves calibration (lower test loss, reduced overconfidence, higher entropy) and degrades more gracefully under corruption or extended training.

Conclusion: $\perp$Grad is a practical, theoretically grounded method for better calibration, compatible with post-hoc techniques like temperature scaling.

Abstract: We provide evidence that orthogonalizing gradients during training improves
model calibration without sacrificing accuracy. On CIFAR-10 with 10% labeled
data, $\perp$Grad matches SGD in accuracy but yields consistently improved
calibration metrics such as lower test loss, reduced softmax overconfidence,
and higher predictive entropy. These benefits persist under input corruption
(CIFAR-10C) and extended training, where $\perp$Grad models degrade more
gracefully than SGD-trained counterparts. $\perp$Grad is optimizer-agnostic,
incurs minimal overhead, and works well with post-hoc calibration techniques
like temperature scaling.
  Theoretically, we prove convergence of a simplified version of $\perp$Grad
under mild assumptions and characterize its stationary points in positive
homogeneous networks: $\perp$Grad converges to solutions where further loss
reduction requires confidence scaling rather than decision boundary
improvement.

</details>


### [100] [Multiscale guidance of AlphaFold3 with heterogeneous cryo-EM data](https://arxiv.org/abs/2506.04490)
*Rishwanth Raghu,Axel Levy,Gordon Wetzstein,Ellen D. Zhong*

Main category: cs.LG

TL;DR: CryoBoltz bridges protein structure prediction and cryo-EM by guiding predictions with experimental data to model dynamic biomolecular systems.


<details>
  <summary>Details</summary>
Motivation: Current protein structure prediction models lack conformational diversity, and cryo-EM faces challenges in deriving atomic models from raw data.

Method: CryoBoltz integrates cryo-EM density maps with protein structure prediction models, using global and local constraints to guide sampling.

Result: The method successfully builds atomic models for dynamic systems like transporters and antibodies.

Conclusion: CryoBoltz effectively combines computational and experimental data to improve modeling of biomolecular diversity.

Abstract: Protein structure prediction models are now capable of generating accurate 3D
structural hypotheses from sequence alone. However, they routinely fail to
capture the conformational diversity of dynamic biomolecular complexes, often
requiring heuristic MSA subsampling approaches for generating alternative
states. In parallel, cryo-electron microscopy (cryo-EM) has emerged as a
powerful tool for imaging near-native structural heterogeneity, but is
challenged by arduous pipelines to go from raw experimental data to atomic
models. Here, we bridge the gap between these modalities, combining cryo-EM
density maps with the rich sequence and biophysical priors learned by protein
structure prediction models. Our method, CryoBoltz, guides the sampling
trajectory of a pretrained protein structure prediction model using both global
and local structural constraints derived from density maps, driving predictions
towards conformational states consistent with the experimental data. We
demonstrate that this flexible yet powerful inference-time approach allows us
to build atomic models into heterogeneous cryo-EM maps across a variety of
dynamic biomolecular systems including transporters and antibodies.

</details>


### [101] [Hierarchical Implicit Neural Emulators](https://arxiv.org/abs/2506.04528)
*Ruoxi Jiang,Xiao Zhang,Karan Jakhar,Peter Y. Lu,Pedram Hassanzadeh,Michael Maire,Rebecca Willett*

Main category: cs.LG

TL;DR: A multiscale implicit neural emulator improves long-term prediction accuracy in neural PDE solvers by leveraging hierarchical future state representations and adaptive temporal downsampling.


<details>
  <summary>Details</summary>
Motivation: Neural PDE solvers often suffer from error accumulation and instability over long time horizons, limiting their reliability for modeling complex dynamical systems.

Method: The approach conditions predictions on a hierarchy of lower-dimensional future states, inspired by implicit time-stepping methods, and adjusts temporal downsampling ratios to capture multiscale dynamics.

Result: The method achieves high short-term accuracy and stable long-term forecasts in turbulent fluid dynamics, outperforming autoregressive baselines with minimal computational overhead.

Conclusion: The multiscale implicit neural emulator enhances the reliability and accuracy of neural PDE solvers for long-term predictions.

Abstract: Neural PDE solvers offer a powerful tool for modeling complex dynamical
systems, but often struggle with error accumulation over long time horizons and
maintaining stability and physical consistency. We introduce a multiscale
implicit neural emulator that enhances long-term prediction accuracy by
conditioning on a hierarchy of lower-dimensional future state representations.
Drawing inspiration from the stability properties of numerical implicit
time-stepping methods, our approach leverages predictions several steps ahead
in time at increasing compression rates for next-timestep refinements. By
actively adjusting the temporal downsampling ratios, our design enables the
model to capture dynamics across multiple granularities and enforce long-range
temporal coherence. Experiments on turbulent fluid dynamics show that our
method achieves high short-term accuracy and produces long-term stable
forecasts, significantly outperforming autoregressive baselines while adding
minimal computational overhead.

</details>


### [102] [HALoS: Hierarchical Asynchronous Local SGD over Slow Networks for Geo-Distributed Large Language Model Training](https://arxiv.org/abs/2506.04531)
*Geon-Woo Kim,Junbo Li,Shashidhar Gandham,Omar Baldonado,Adithya Gangidi,Pavan Balaji,Zhangyang Wang,Aditya Akella*

Main category: cs.LG

TL;DR: HALoS is a hierarchical asynchronous optimization framework for training large language models (LLMs) in geo-distributed environments, reducing communication costs and improving efficiency.


<details>
  <summary>Details</summary>
Motivation: Training LLMs across geographically distributed accelerators faces high communication costs and uneven hardware utilization.

Method: HALoS introduces local and global parameter servers (LPSs and GPS) to minimize inter-region communication and leverage fast intra-region links.

Result: HALoS achieves up to 7.5x faster convergence than synchronous baselines and 2.1x improvement over asynchronous methods, while maintaining model quality.

Conclusion: Hierarchical server-side update accumulation and global model merging are effective for scalable, efficient LLM training in heterogeneous, geo-distributed settings.

Abstract: Training large language models (LLMs) increasingly relies on geographically
distributed accelerators, causing prohibitive communication costs across
regions and uneven utilization of heterogeneous hardware. We propose HALoS, a
hierarchical asynchronous optimization framework that tackles these issues by
introducing local parameter servers (LPSs) within each region and a global
parameter server (GPS) that merges updates across regions. This hierarchical
design minimizes expensive inter-region communication, reduces straggler
effects, and leverages fast intra-region links. We provide a rigorous
convergence analysis for HALoS under non-convex objectives, including
theoretical guarantees on the role of hierarchical momentum in asynchronous
training. Empirically, HALoS attains up to 7.5x faster convergence than
synchronous baselines in geo-distributed LLM training and improves upon
existing asynchronous methods by up to 2.1x. Crucially, HALoS preserves the
model quality of fully synchronous SGD-matching or exceeding accuracy on
standard language modeling and downstream benchmarks-while substantially
lowering total training time. These results demonstrate that hierarchical,
server-side update accumulation and global model merging are powerful tools for
scalable, efficient training of new-era LLMs in heterogeneous, geo-distributed
environments.

</details>


### [103] [NOBLE -- Neural Operator with Biologically-informed Latent Embeddings to Capture Experimental Variability in Biological Neuron Models](https://arxiv.org/abs/2506.04536)
*Luca Ghafourpour,Valentin Duruisseaux,Bahareh Tolooshams,Philip H. Wong,Costas A. Anastassiou,Anima Anandkumar*

Main category: cs.LG

TL;DR: NOBLE is a neural operator framework that predicts neural dynamics efficiently, addressing limitations of current bio-realistic neuron models by capturing variability and achieving significant speedup.


<details>
  <summary>Details</summary>
Motivation: Current neuron modeling approaches are limited by data availability and variability, and deep learning lacks biophysical complexity. NOBLE aims to bridge this gap.

Method: NOBLE learns a mapping from neuron features to somatic voltage responses, trained on biophysically realistic models, and predicts neural dynamics with variability.

Result: NOBLE achieves a 4200× speedup over numerical solvers and generates synthetic neurons with trial-to-trial variability, validated on experimental data.

Conclusion: NOBLE advances neuron modeling, enabling better understanding of brain function, neuromorphic architectures, and neuroAI applications.

Abstract: Characterizing the diverse computational properties of human neurons via
multimodal electrophysiological, transcriptomic, and morphological data
provides the foundation for constructing and validating bio-realistic neuron
models that can advance our understanding of fundamental mechanisms underlying
brain function. However, current modeling approaches remain constrained by the
limited availability and intrinsic variability of experimental neuronal data.
To capture variability, ensembles of deterministic models are often used, but
are difficult to scale as model generation requires repeating computationally
expensive optimization for each neuron. While deep learning is becoming
increasingly relevant in this space, it fails to capture the full biophysical
complexity of neurons, their nonlinear voltage dynamics, and variability. To
address these shortcomings, we introduce NOBLE, a neural operator framework
that learns a mapping from a continuous frequency-modulated embedding of
interpretable neuron features to the somatic voltage response induced by
current injection. Trained on data generated from biophysically realistic
neuron models, NOBLE predicts distributions of neural dynamics accounting for
the intrinsic experimental variability. Unlike conventional bio-realistic
neuron models, interpolating within the embedding space offers models whose
dynamics are consistent with experimentally observed responses. NOBLE is the
first scaled-up deep learning framework validated on real experimental data,
enabling efficient generation of synthetic neurons that exhibit trial-to-trial
variability and achieve a $4200\times$ speedup over numerical solvers. To this
end, NOBLE captures fundamental neural properties, opening the door to a better
understanding of cellular composition and computations, neuromorphic
architectures, large-scale brain circuits, and general neuroAI applications.

</details>


### [104] [Neural MJD: Neural Non-Stationary Merton Jump Diffusion for Time Series Prediction](https://arxiv.org/abs/2506.04542)
*Yuanpei Gao,Qi Yan,Yan Leng,Renjie Liao*

Main category: cs.LG

TL;DR: Neural MJD combines neural networks with a non-stationary Merton jump diffusion model to improve time series prediction by explicitly modeling stochastic processes and abrupt changes.


<details>
  <summary>Details</summary>
Motivation: Deep learning methods struggle with non-stationary data and abrupt changes due to their black-box nature and lack of explicit stochastic modeling.

Method: Neural MJD formulates forecasting as an SDE simulation, using a time-inhomogeneous Itô diffusion and compound Poisson process. It includes a likelihood truncation mechanism and an improved solver (Euler-Maruyama with restart).

Result: Neural MJD outperforms state-of-the-art deep learning and statistical methods on synthetic and real-world datasets.

Conclusion: The model effectively addresses non-stationary data and abrupt changes, offering improved generalization and performance.

Abstract: While deep learning methods have achieved strong performance in time series
prediction, their black-box nature and inability to explicitly model underlying
stochastic processes often limit their generalization to non-stationary data,
especially in the presence of abrupt changes. In this work, we introduce Neural
MJD, a neural network based non-stationary Merton jump diffusion (MJD) model.
Our model explicitly formulates forecasting as a stochastic differential
equation (SDE) simulation problem, combining a time-inhomogeneous It\^o
diffusion to capture non-stationary stochastic dynamics with a
time-inhomogeneous compound Poisson process to model abrupt jumps. To enable
tractable learning, we introduce a likelihood truncation mechanism that caps
the number of jumps within small time intervals and provide a theoretical error
bound for this approximation. Additionally, we propose an Euler-Maruyama with
restart solver, which achieves a provably lower error bound in estimating
expected states and reduced variance compared to the standard solver.
Experiments on both synthetic and real-world datasets demonstrate that Neural
MJD consistently outperforms state-of-the-art deep learning and statistical
learning methods.

</details>


### [105] [Communication Efficient Adaptive Model-Driven Quantum Federated Learning](https://arxiv.org/abs/2506.04548)
*Dev Gurung,Shiva Raj Pokhrel*

Main category: cs.LG

TL;DR: The paper introduces a model-driven quantum federated learning algorithm (mdQFL) to address challenges in federated learning (FL) like training bottlenecks, large-scale device participation, and non-IID data. It reduces communication costs by 50% while maintaining or improving accuracy.


<details>
  <summary>Details</summary>
Motivation: The challenges of massive datasets, numerous devices, and non-IID data in FL, especially in a quantum context, hinder performance and efficiency.

Method: The authors propose mdQFL, a model-driven quantum federated learning algorithm, which includes training and update personalization and test generalization.

Result: Experiments show a 50% reduction in communication costs with maintained or improved accuracy and better local model training compared to standard QFL.

Conclusion: mdQFL effectively addresses FL challenges in quantum settings, offering efficiency and adaptability, with potential applications in broader FL scenarios.

Abstract: Training with huge datasets and a large number of participating devices leads
to bottlenecks in federated learning (FL). Furthermore, the challenges of
heterogeneity between multiple FL clients affect the overall performance of the
system. In a quantum federated learning (QFL) context, we address these three
main challenges: i) training bottlenecks from massive datasets, ii) the
involvement of a substantial number of devices, and iii) non-IID data
distributions. We introduce a model-driven quantum federated learning algorithm
(mdQFL) to tackle these challenges. Our proposed approach is efficient and
adaptable to various factors, including different numbers of devices. To the
best of our knowledge, it is the first to explore training and update
personalization, as well as test generalization within a QFL setting, which can
be applied to other FL scenarios. We evaluated the efficiency of the proposed
mdQFL framework through extensive experiments under diverse non-IID data
heterogeneity conditions using various datasets within the Qiskit environment.
Our results demonstrate a nearly 50% decrease in total communication costs
while maintaining or, in some cases, exceeding the accuracy of the final model
and consistently improving local model training compared to the standard QFL
baseline. Moreover, our experimental evaluation thoroughly explores the QFL and
mdQFL algorithms, along with several influencing factors. In addition, we
present a theoretical analysis to clarify the complexities of the proposed
algorithm. The experimental code is available at 1.

</details>


### [106] [Unsupervised Machine Learning for Scientific Discovery: Workflow and Best Practices](https://arxiv.org/abs/2506.04553)
*Andersen Chang,Tiffany M. Tang,Tarek M. Zikry,Genevera I. Allen*

Main category: cs.LG

TL;DR: The paper proposes a standardized workflow for unsupervised learning in science to ensure reliable and reproducible discoveries, demonstrated through an astronomy case study.


<details>
  <summary>Details</summary>
Motivation: Address the lack of standardization in unsupervised learning workflows, hindering reliable and reproducible scientific discoveries.

Method: Introduces a structured workflow including scientific question formulation, data preparation, modeling, validation, and result communication.

Result: Demonstrates the workflow's effectiveness via a case study on refining Milky Way globular clusters by chemical composition.

Conclusion: A carefully-designed unsupervised learning workflow enhances validation and reproducibility, advancing scientific discovery.

Abstract: Unsupervised machine learning is widely used to mine large, unlabeled
datasets to make data-driven discoveries in critical domains such as climate
science, biomedicine, astronomy, chemistry, and more. However, despite its
widespread utilization, there is a lack of standardization in unsupervised
learning workflows for making reliable and reproducible scientific discoveries.
In this paper, we present a structured workflow for using unsupervised learning
techniques in science. We highlight and discuss best practices starting with
formulating validatable scientific questions, conducting robust data
preparation and exploration, using a range of modeling techniques, performing
rigorous validation by evaluating the stability and generalizability of
unsupervised learning conclusions, and promoting effective communication and
documentation of results to ensure reproducible scientific discoveries. To
illustrate our proposed workflow, we present a case study from astronomy,
seeking to refine globular clusters of Milky Way stars based upon their
chemical composition. Our case study highlights the importance of validation
and illustrates how the benefits of a carefully-designed workflow for
unsupervised learning can advance scientific discovery.

</details>


### [107] [Clustering and Median Aggregation Improve Differentially Private Inference](https://arxiv.org/abs/2506.04566)
*Kareem Amin,Salman Avestimehr,Sara Babakniya,Alex Bie,Weiwei Kong,Natalia Ponomareva,Umar Syed*

Main category: cs.LG

TL;DR: The paper improves DP language model inference by clustering input data and using median aggregation for next-token predictions, enhancing text quality and reducing privacy costs.


<details>
  <summary>Details</summary>
Motivation: Uniform sampling in DP language model inference degrades text quality, especially for heterogeneous topics, prompting the need for better methods.

Method: Clusters input data before inference and aggregates next-token predictions using medians instead of averages, leveraging reduced local sensitivity for better privacy guarantees.

Result: Demonstrates improved representativeness (e.g., MAUVE) and downstream task performance, with lower privacy costs than prior methods.

Conclusion: The proposed method produces higher-quality synthetic data at reduced privacy costs, outperforming previous state-of-the-art approaches.

Abstract: Differentially private (DP) language model inference is an approach for
generating private synthetic text. A sensitive input example is used to prompt
an off-the-shelf large language model (LLM) to produce a similar example.
Multiple examples can be aggregated together to formally satisfy the DP
guarantee.
  Prior work creates inference batches by sampling sensitive inputs uniformly
at random. We show that uniform sampling degrades the quality of privately
generated text, especially when the sensitive examples concern heterogeneous
topics.
  We remedy this problem by clustering the input data before selecting
inference batches. Next, we observe that clustering also leads to more similar
next-token predictions across inferences. We use this insight to introduce a
new algorithm that aggregates next token statistics by privately computing
medians instead of averages. This approach leverages the fact that the median
has decreased local sensitivity when next token predictions are similar,
allowing us to state a data-dependent and ex-post DP guarantee about the
privacy properties of this algorithm. Finally, we demonstrate improvements in
terms of representativeness metrics (e.g., MAUVE) as well as downstream task
performance. We show that our method produces high-quality synthetic data at
significantly lower privacy cost than a previous state-of-the-art method.

</details>


### [108] [StatsMerging: Statistics-Guided Model Merging via Task-Specific Teacher Distillation](https://arxiv.org/abs/2506.04567)
*Ranjith Merugu,Bryan Bo Cao,Shubham Jain*

Main category: cs.LG

TL;DR: StatsMerging is a lightweight, learning-based model merging method that uses weight distribution statistics and SVD to guide merging without needing ground truth labels or test samples. It outperforms state-of-the-art techniques in accuracy, generalization, and robustness.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of accommodating multiple large models within constrained memory budgets efficiently and effectively.

Method: StatsMerging leverages SVD singular values for task-specific weight distributions, uses a lightweight learner (StatsMergeLearner), and employs Task-Specific Teacher Distillation for merging heterogeneous architectures.

Result: Outperforms state-of-the-art techniques in accuracy, generalization to unseen tasks, and robustness to image quality variations across eight tasks.

Conclusion: StatsMerging is a promising solution for model merging, offering efficiency, adaptability, and superior performance without requiring labeled data.

Abstract: Model merging has emerged as a promising solution to accommodate multiple
large models within constrained memory budgets. We present StatsMerging, a
novel lightweight learning-based model merging method guided by weight
distribution statistics without requiring ground truth labels or test samples.
StatsMerging offers three key advantages: (1) It uniquely leverages singular
values from singular value decomposition (SVD) to capture task-specific weight
distributions, serving as a proxy for task importance to guide task coefficient
prediction; (2) It employs a lightweight learner StatsMergeLearner to model the
weight distributions of task-specific pre-trained models, improving
generalization and enhancing adaptation to unseen samples; (3) It introduces
Task-Specific Teacher Distillation for merging vision models with heterogeneous
architectures, a merging learning paradigm that avoids costly ground-truth
labels by task-specific teacher distillation. Notably, we present two types of
knowledge distillation, (a) distilling knowledge from task-specific models to
StatsMergeLearner; and (b) distilling knowledge from models with heterogeneous
architectures prior to merging. Extensive experiments across eight tasks
demonstrate the effectiveness of StatsMerging. Our results show that
StatsMerging outperforms state-of-the-art techniques in terms of overall
accuracy, generalization to unseen tasks, and robustness to image quality
variations.

</details>


### [109] [Scaling Laws for Robust Comparison of Open Foundation Language-Vision Models and Datasets](https://arxiv.org/abs/2506.04598)
*Marianna Nezhurina,Tomer Porian,Giovanni Pucceti,Tommie Kerssies,Romain Beaumont,Mehdi Cherti,Jenia Jitsev*

Main category: cs.LG

TL;DR: The paper demonstrates how scaling laws can be used to compare models and datasets for pre-training, focusing on CLIP and MaMMUT. MaMMUT shows better scalability and sample efficiency.


<details>
  <summary>Details</summary>
Motivation: To enable systematic comparison of models and datasets across scales, avoiding misleading conclusions from single-scale measurements.

Method: Derives full scaling laws for CLIP and MaMMUT using dense measurements across model and sample scales, and evaluates them on downstream tasks.

Result: MaMMUT outperforms CLIP in scalability and sample efficiency, with consistent trends across tasks and datasets.

Conclusion: Scaling laws provide a robust method for model and dataset comparison, supporting systematic improvement of foundation models.

Abstract: In studies of transferable learning, scaling laws are obtained for various
important foundation models to predict their properties and performance at
larger scales. We show here how scaling law derivation can also be used for
model and dataset comparison, allowing to decide which procedure is to be
preferred for pre-training. For the first time, full scaling laws based on
dense measurements across a wide span of model and samples seen scales are
derived for two important language-vision learning procedures, CLIP and MaMMUT,
that use either contrastive only or contrastive and captioning text generative
loss. Ensuring sufficient prediction accuracy for held out points, we use
derived scaling laws to compare both models, obtaining evidence for MaMMUT's
stronger improvement with scale and better sample efficiency than standard
CLIP. To strengthen validity of the comparison, we show scaling laws for
various downstream tasks, classification, retrieval, and segmentation, and for
different open datasets, DataComp, DFN and Re-LAION, observing consistently the
same trends. We show that comparison can also be performed when deriving
scaling laws with a constant learning rate schedule, reducing compute cost.
Accurate derivation of scaling laws provides thus means to perform model and
dataset comparison across scale spans, avoiding misleading conclusions based on
measurements from single reference scales only, paving the road for systematic
comparison and improvement of open foundation models and datasets for their
creation. We release all the pre-trained models with their intermediate
checkpoints, including openMaMMUT-L/14, which achieves $80.3\%$ zero-shot
ImageNet-1k accuracy, trained on 12.8B samples from DataComp-1.4B. Code for
reproducing experiments in the paper and raw experiments data can be found at
https://github.com/LAION-AI/scaling-laws-for-comparison.

</details>


### [110] [Ignoring Directionality Leads to Compromised Graph Neural Network Explanations](https://arxiv.org/abs/2506.04608)
*Changsheng Sun,Xinke Li,Jin Song Dong*

Main category: cs.LG

TL;DR: Graph symmetrization in GNNs discards directional info, harming explanation fidelity. Preserving direction improves quality, aiding human decisions.


<details>
  <summary>Details</summary>
Motivation: To address the loss of directional info in GNN explanations, which misleads human decision-making in critical domains.

Method: Theoretical and empirical analysis of how direction-aware GNNs improve explanation fidelity.

Result: Preserving directional semantics enhances explanation quality, providing more faithful insights.

Conclusion: Direction-aware GNN explainability is crucial for security-critical applications.

Abstract: Graph Neural Networks (GNNs) are increasingly used in critical domains, where
reliable explanations are vital for supporting human decision-making. However,
the common practice of graph symmetrization discards directional information,
leading to significant information loss and misleading explanations. Our
analysis demonstrates how this practice compromises explanation fidelity.
Through theoretical and empirical studies, we show that preserving directional
semantics significantly improves explanation quality, ensuring more faithful
insights for human decision-makers. These findings highlight the need for
direction-aware GNN explainability in security-critical applications.

</details>


### [111] [Exploring bidirectional bounds for minimax-training of Energy-based models](https://arxiv.org/abs/2506.04609)
*Cong Geng,Jia Wang,Li Chen,Zhiyong Gao,Jes Frellsen,Søren Hauberg*

Main category: cs.LG

TL;DR: The paper proposes using bidirectional bounds (maximizing a lower bound and minimizing an upper bound) to stabilize training of Energy-based models (EBMs), comparing four derived bounds and demonstrating improved performance.


<details>
  <summary>Details</summary>
Motivation: EBMs are difficult to train, and existing methods like variational lower bounds can cause instabilities. The authors aim to stabilize training by using bidirectional bounds.

Method: The authors derive and compare four bounds on log-likelihood: two lower bounds (based on generator Jacobian singular values and mutual information) and two upper bounds (gradient penalty-like and diffusion process-based). Algorithms for evaluating these bounds are provided.

Result: Bidirectional bounds stabilize EBM training, leading to high-quality density estimation and sample generation.

Conclusion: Using bidirectional bounds is effective for stabilizing EBM training and improving performance in density estimation and generation tasks.

Abstract: Energy-based models (EBMs) estimate unnormalized densities in an elegant
framework, but they are generally difficult to train. Recent work has linked
EBMs to generative adversarial networks, by noting that they can be trained
through a minimax game using a variational lower bound. To avoid the
instabilities caused by minimizing a lower bound, we propose to instead work
with bidirectional bounds, meaning that we maximize a lower bound and minimize
an upper bound when training the EBM. We investigate four different bounds on
the log-likelihood derived from different perspectives. We derive lower bounds
based on the singular values of the generator Jacobian and on mutual
information. To upper bound the negative log-likelihood, we consider a gradient
penalty-like bound, as well as one based on diffusion processes. In all cases,
we provide algorithms for evaluating the bounds. We compare the different
bounds to investigate, the pros and cons of the different approaches. Finally,
we demonstrate that the use of bidirectional bounds stabilizes EBM training and
yields high-quality density estimation and sample generation.

</details>


### [112] [Composing Agents to Minimize Worst-case Risk](https://arxiv.org/abs/2506.04632)
*Guruprerana Shabadi,Rajeev Alur*

Main category: cs.LG

TL;DR: The paper formalizes agentic workflows as directed acyclic graphs (agent graphs) and introduces an algorithm to optimize agent compositions for task success and risk minimization, focusing on worst-case risk (value-at-risk).


<details>
  <summary>Details</summary>
Motivation: Modern agentic systems decompose tasks into subtasks handled by specialized AI agents, but real-world deployment requires balancing task success with risk (safety, fairness, privacy) and analyzing tail behaviors.

Method: The work defines worst-case risk as the tail quantile (value-at-risk) of the loss distribution. An efficient algorithm approximates this via union bound and dynamic programming, traversing the agent graph to find near-optimal compositions.

Result: The algorithm is proven asymptotically near-optimal for practical loss functions and validated on reinforcement learning-based control benchmarks, showing effectiveness in approximating value-at-risk and identifying optimal compositions.

Conclusion: The framework successfully addresses risk minimization in agentic workflows, providing a scalable solution for real-world applications.

Abstract: From software development to robot control, modern agentic systems decompose
complex objectives into a sequence of subtasks and choose a set of specialized
AI agents to complete them. We formalize an agentic workflow as a directed
acyclic graph, called an agent graph, where edges represent AI agents and paths
correspond to feasible compositions of agents. When deploying these systems in
the real world, we need to choose compositions of agents that not only maximize
the task success, but also minimize risk where the risk captures requirements
like safety, fairness, and privacy. This additionally requires carefully
analyzing the low-probability (tail) behaviors of compositions of agents. In
this work, we consider worst-case risk minimization over the set of feasible
agent compositions. We define worst-case risk as the tail quantile -- also
known as value-at-risk -- of the loss distribution of the agent composition
where the loss quantifies the risk associated with agent behaviors. We
introduce an efficient algorithm that traverses the agent graph and finds a
near-optimal composition of agents by approximating the value-at-risk via a
union bound and dynamic programming. Furthermore, we prove that the
approximation is near-optimal asymptotically for a broad class of practical
loss functions. To evaluate our framework, we consider a suite of video
game-like control benchmarks that require composing several agents trained with
reinforcement learning and demonstrate our algorithm's effectiveness in
approximating the value-at-risk and identifying the optimal agent composition.

</details>


### [113] [Inference economics of language models](https://arxiv.org/abs/2506.04645)
*Ege Erdil*

Main category: cs.LG

TL;DR: The paper develops a theoretical model to optimize the trade-off between cost per token and serial token generation speed for LLM inference, considering hardware constraints and parallelism setups.


<details>
  <summary>Details</summary>
Motivation: To address the economic and performance trade-offs in deploying large language models (LLMs) at scale for inference.

Method: A theoretical model incorporating arithmetic, memory bandwidth, network bandwidth, and latency constraints, optimizing parallelism setups and batch sizes.

Result: Pareto frontiers of serial speed versus cost per token for popular language models are computed.

Conclusion: The model provides a framework to optimize LLM inference deployment by balancing cost and speed.

Abstract: We develop a theoretical model that addresses the economic trade-off between
cost per token versus serial token generation speed when deploying LLMs for
inference at scale. Our model takes into account arithmetic, memory bandwidth,
network bandwidth and latency constraints; and optimizes over different
parallelism setups and batch sizes to find the ones that optimize serial
inference speed at a given cost per token. We use the model to compute Pareto
frontiers of serial speed versus cost per token for popular language models.

</details>


### [114] [Neural Network Reprogrammability: A Unified Theme on Model Reprogramming, Prompt Tuning, and Prompt Instruction](https://arxiv.org/abs/2506.04650)
*Zesheng Ye,Chengyi Cai,Ruijiang Dong,Jianzhong Qi,Lei Feng,Pin-Yu Chen,Feng Liu*

Main category: cs.LG

TL;DR: The paper introduces neural network reprogrammability as a unifying framework for adapting pre-trained models, categorizing methods by manipulation format, location, operator, and output alignment.


<details>
  <summary>Details</summary>
Motivation: To bridge fragmented research areas in model adaptation by identifying shared principles and unifying diverse techniques.

Method: Proposes a taxonomy for information manipulation-based adaptation, analyzing methods like model reprogramming, prompt tuning, and prompt instruction.

Result: The framework applies across data modalities and model architectures, revealing theoretical connections and practical distinctions among techniques.

Conclusion: Neural network reprogrammability is a fundamental paradigm for efficient adaptation, with potential for future research and ethical considerations.

Abstract: As large-scale pre-trained foundation models continue to expand in size and
capability, efficiently adapting them to specific downstream tasks has become
increasingly critical. Despite substantial progress, existing adaptation
approaches have evolved largely in isolation, without a clear understanding of
their interrelationships. This survey introduces neural network
reprogrammability as a unifying framework that bridges mainstream model
adaptation techniques--model reprogramming, prompt tuning, and prompt
instruction--previously fragmented research areas yet converges on a shared
principle: repurposing a pre-trained model by manipulating information at the
interfaces while keeping the model parameters frozen. These methods exploit
neural networks' sensitivity to manipulation on different interfaces, be it
through perturbing inputs, inserting tokens into intermediate layers, or
providing task-specific examples in context, to redirect model behaviors
towards desired outcomes. We then present a taxonomy that categorizes such
information manipulation-based adaptation approaches across four key
dimensions: manipulation format (fixed or learnable), location (interfaces
where manipulations occur), operator (how they are applied), and output
alignment requirement (post-processing needed to align outputs with downstream
tasks). Notably, this framework applies consistently across data modalities,
independent of specific model architectures. Moreover, viewing established
techniques like in-context learning and chain-of-thought prompting through this
lens reveals both their theoretical connections and practical distinctions. We
further analyze remaining technical challenges and ethical considerations,
positioning neural network reprogrammability as a fundamental paradigm for
efficient model adaptation. We lastly identify promising research directions
emerging from this integrative viewpoint.

</details>


### [115] [The Oversmoothing Fallacy: A Misguided Narrative in GNN Research](https://arxiv.org/abs/2506.04653)
*MoonJeong Park,Sunghyun Choi,Jaeseung Heo,Eunhyeok Park,Dongwoo Kim*

Main category: cs.LG

TL;DR: The paper challenges the overstated impact of oversmoothing in deep GNNs, attributing performance issues to vanishing gradients instead. It advocates for deeper GNN exploration and shows classical techniques like skip connections can mitigate issues.


<details>
  <summary>Details</summary>
Motivation: To correct misconceptions about oversmoothing in GNNs and encourage deeper architectures by clarifying the root causes of performance limitations.

Method: Analyzes core GNN operations (aggregation, linear transformation, activation) and distinguishes oversmoothing from vanishing gradients. Tests classical solutions (skip connections, normalization) in deep GNNs.

Result: Demonstrates that oversmoothing is not unique to GNNs and that deep architectures can work with proper techniques.

Conclusion: Clarifies oversmoothing misconceptions, highlights the potential of deep GNNs, and suggests practical solutions for implementation.

Abstract: Oversmoothing has been recognized as a main obstacle to building deep Graph
Neural Networks (GNNs), limiting the performance. This position paper argues
that the influence of oversmoothing has been overstated and advocates for a
further exploration of deep GNN architectures. Given the three core operations
of GNNs, aggregation, linear transformation, and non-linear activation, we show
that prior studies have mistakenly confused oversmoothing with the vanishing
gradient, caused by transformation and activation rather than aggregation. Our
finding challenges prior beliefs about oversmoothing being unique to GNNs.
Furthermore, we demonstrate that classical solutions such as skip connections
and normalization enable the successful stacking of deep GNN layers without
performance degradation. Our results clarify misconceptions about oversmoothing
and shed new light on the potential of deep GNNs.

</details>


### [116] [Noise-Resistant Label Reconstruction Feature Selection for Partial Multi-Label Learning](https://arxiv.org/abs/2506.04669)
*Wanfu Gao,Hanlin Pan,Qingqi Han,Kunpeng Liu*

Main category: cs.LG

TL;DR: A new Partial Multi-label Learning (PML) feature selection method addresses the curse of dimensionality by leveraging label noise-resistance and connectivity, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: The curse of dimensionality in PML leads to overfitting and poor classification. Existing low-rank assumption methods fail in practice and lose high-dimensional information.

Method: Proposes a PML feature selection method using label noise-resistance for disambiguation, a reformed low-rank assumption, and label connectivity to identify representative labels.

Result: Experiments on benchmark datasets show the method's superiority in handling dimensionality and identifying positive labels.

Conclusion: The proposed method effectively mitigates dimensionality issues in PML and improves label identification, offering practical advantages.

Abstract: The "Curse of dimensionality" is prevalent across various data patterns,
which increases the risk of model overfitting and leads to a decline in model
classification performance. However, few studies have focused on this issue in
Partial Multi-label Learning (PML), where each sample is associated with a set
of candidate labels, at least one of which is correct. Existing PML methods
addressing this problem are mainly based on the low-rank assumption. However,
low-rank assumption is difficult to be satisfied in practical situations and
may lead to loss of high-dimensional information. Furthermore, we find that
existing methods have poor ability to identify positive labels, which is
important in real-world scenarios. In this paper, a PML feature selection
method is proposed considering two important characteristics of dataset: label
relationship's noise-resistance and label connectivity. Our proposed method
utilizes label relationship's noise-resistance to disambiguate labels. Then the
learning process is designed through the reformed low-rank assumption. Finally,
representative labels are found through label connectivity, and the weight
matrix is reconstructed to select features with strong identification ability
to these labels. The experimental results on benchmark datasets demonstrate the
superiority of the proposed method.

</details>


### [117] [FedAPM: Federated Learning via ADMM with Partial Model Personalization](https://arxiv.org/abs/2506.04672)
*Shengkun Zhu,Feiteng Nie,Jinshan Zeng,Sheng Wang,Yuan Sun,Yuan Yao,Shangfeng Chen,Quanqing Xu,Chuanhui Yang*

Main category: cs.LG

TL;DR: The paper proposes FedAPM, a federated learning framework using ADMM to mitigate client drift caused by partial model personalization, achieving faster and more accurate convergence than SOTA methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the non-i.i.d. nature of datasets in FL and the inefficiencies caused by client drift due to partial model personalization.

Method: FedAPM incorporates first-order and second-order proximal terms into the augmented Lagrangian function to correct client drift, using ADMM for optimization.

Result: FedAPM outperforms SOTA methods with improvements of 12.3% in accuracy, 16.4% in F1 score, and 18.0% in AUC, requiring fewer communication rounds.

Conclusion: FedAPM is a stable and efficient FL framework that mitigates client drift and achieves superior performance in heterogeneous and multimodal settings.

Abstract: In federated learning (FL), the assumption that datasets from different
devices are independent and identically distributed (i.i.d.) often does not
hold due to user differences, and the presence of various data modalities
across clients makes using a single model impractical. Personalizing certain
parts of the model can effectively address these issues by allowing those parts
to differ across clients, while the remaining parts serve as a shared model.
However, we found that partial model personalization may exacerbate client
drift (each client's local model diverges from the shared model), thereby
reducing the effectiveness and efficiency of FL algorithms. We propose an FL
framework based on the alternating direction method of multipliers (ADMM),
referred to as FedAPM, to mitigate client drift. We construct the augmented
Lagrangian function by incorporating first-order and second-order proximal
terms into the objective, with the second-order term providing fixed correction
and the first-order term offering compensatory correction between the local and
shared models. Our analysis demonstrates that FedAPM, by using explicit
estimates of the Lagrange multiplier, is more stable and efficient in terms of
convergence compared to other FL frameworks. We establish the global
convergence of FedAPM training from arbitrary initial points to a stationary
point, achieving three types of rates: constant, linear, and sublinear, under
mild assumptions. We conduct experiments using four heterogeneous and
multimodal datasets with different metrics to validate the performance of
FedAPM. Specifically, FedAPM achieves faster and more accurate convergence,
outperforming the SOTA methods with average improvements of 12.3% in test
accuracy, 16.4% in F1 score, and 18.0% in AUC while requiring fewer
communication rounds.

</details>


### [118] [The cost of ensembling: is it always worth combining?](https://arxiv.org/abs/2506.04677)
*Marco Zanotti*

Main category: cs.LG

TL;DR: Ensembles improve forecasting accuracy, especially in probabilistic settings, but at high computational cost. Reducing retraining frequency lowers costs with minimal accuracy loss. Small ensembles (2-3 models) often suffice for near-optimal results, offering a balance between accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: The trade-off between forecast accuracy and computational cost is critical, especially in ensemble learning for time series forecasting, given increasing dataset sizes and model complexity.

Method: Evaluated ten base models and eight ensemble configurations on large-scale retail datasets (M5 and VN1), considering point and probabilistic accuracy under varying retraining frequencies.

Result: Ensembles consistently improve performance, but with high computational costs. Reducing retraining frequency lowers costs with little accuracy impact. Small ensembles achieve competitive accuracy efficiently.

Conclusion: Careful ensemble design and retraining strategy selection can yield accurate, robust, and cost-effective forecasts, supporting scalable and sustainable AI in forecasting.

Abstract: Given the continuous increase in dataset sizes and the complexity of
forecasting models, the trade-off between forecast accuracy and computational
cost is emerging as an extremely relevant topic, especially in the context of
ensemble learning for time series forecasting. To asses it, we evaluated ten
base models and eight ensemble configurations across two large-scale retail
datasets (M5 and VN1), considering both point and probabilistic accuracy under
varying retraining frequencies. We showed that ensembles consistently improve
forecasting performance, particularly in probabilistic settings. However, these
gains come at a substantial computational cost, especially for larger,
accuracy-driven ensembles. We found that reducing retraining frequency
significantly lowers costs, with minimal impact on accuracy, particularly for
point forecasts. Moreover, efficiency-driven ensembles offer a strong balance,
achieving competitive accuracy with considerably lower costs compared to
accuracy-optimized combinations. Most importantly, small ensembles of two or
three models are often sufficient to achieve near-optimal results. These
findings provide practical guidelines for deploying scalable and cost-efficient
forecasting systems, supporting the broader goals of sustainable AI in
forecasting. Overall, this work shows that careful ensemble design and
retraining strategy selection can yield accurate, robust, and cost-effective
forecasts suitable for real-world applications.

</details>


### [119] [Urania: Differentially Private Insights into AI Use](https://arxiv.org/abs/2506.04681)
*Daogao Liu,Edith Cohen,Badih Ghazi,Peter Kairouz,Pritish Kamath,Alexander Knop,Ravi Kumar,Pasin Manurangsi,Adam Sealfon,Da Yu,Chiyuan Zhang*

Main category: cs.LG

TL;DR: Urania is a privacy-focused framework for analyzing LLM chatbot interactions using differential privacy (DP) techniques, ensuring data utility while protecting user privacy.


<details>
  <summary>Details</summary>
Motivation: To generate insights from LLM chatbot interactions while rigorously guaranteeing user privacy through differential privacy.

Method: Uses private clustering, keyword extraction (frequency-based, TF-IDF, LLM-guided), and DP tools like clustering, partition selection, and histogram summarization.

Result: Effectively balances data utility and privacy, preserving conversational insights with robust DP guarantees.

Conclusion: Urania successfully extracts meaningful insights while maintaining stringent privacy, outperforming non-private alternatives.

Abstract: We introduce $Urania$, a novel framework for generating insights about LLM
chatbot interactions with rigorous differential privacy (DP) guarantees. The
framework employs a private clustering mechanism and innovative keyword
extraction methods, including frequency-based, TF-IDF-based, and LLM-guided
approaches. By leveraging DP tools such as clustering, partition selection, and
histogram-based summarization, $Urania$ provides end-to-end privacy protection.
Our evaluation assesses lexical and semantic content preservation, pair
similarity, and LLM-based metrics, benchmarking against a non-private
Clio-inspired pipeline (Tamkin et al., 2024). Moreover, we develop a simple
empirical privacy evaluation that demonstrates the enhanced robustness of our
DP pipeline. The results show the framework's ability to extract meaningful
conversational insights while maintaining stringent user privacy, effectively
balancing data utility with privacy preservation.

</details>


### [120] [Towards Better Generalization via Distributional Input Projection Network](https://arxiv.org/abs/2506.04690)
*Yifan Hao,Yanxin Lu,Xinwei Shen,Tong Zhang*

Main category: cs.LG

TL;DR: DIPNet introduces a framework to project inputs into learnable distributions, smoothing the loss landscape and improving generalization, validated across various models and tasks.


<details>
  <summary>Details</summary>
Motivation: Overparameterized models lack insight from training loss alone, and enforcing smoothness in neural networks is challenging.

Method: DIPNet projects inputs into learnable distributions at each layer, inducing a smoother loss landscape.

Result: Theoretical and empirical results show improved generalization, robustness, and performance across diverse tasks and models.

Conclusion: DIPNet offers a general, effective approach to enhance generalization in modern deep learning.

Abstract: As overparameterized models become increasingly prevalent, training loss
alone offers limited insight into generalization performance. While smoothness
has been linked to improved generalization across various settings, directly
enforcing smoothness in neural networks remains challenging. To address this,
we introduce Distributional Input Projection Networks (DIPNet), a novel
framework that projects inputs into learnable distributions at each layer. This
distributional representation induces a smoother loss landscape with respect to
the input, promoting better generalization. We provide theoretical analysis
showing that DIPNet reduces both local smoothness measures and the Lipschitz
constant of the network, contributing to improved generalization performance.
Empirically, we validate DIPNet across a wide range of architectures and tasks,
including Vision Transformers (ViTs), Large Language Models (LLMs), ResNet and
MLPs. Our method consistently enhances test performance under standard
settings, adversarial attacks, out-of-distribution inputs, and reasoning
benchmarks. We demonstrate that the proposed input projection strategy can be
seamlessly integrated into existing models, providing a general and effective
approach for boosting generalization performance in modern deep learning.

</details>


### [121] [Influence Functions for Edge Edits in Non-Convex Graph Neural Networks](https://arxiv.org/abs/2506.04694)
*Jaeseung Heo,Kyeongheung Yun,Seokwon Yoon,MoonJeong Park,Jungseul Ok,Dongwoo Kim*

Main category: cs.LG

TL;DR: The paper introduces a proximal Bregman response function for GNNs to predict edge influence, relaxing convexity assumptions and addressing message propagation effects for both deletions and insertions.


<details>
  <summary>Details</summary>
Motivation: To improve interpretability and robustness of GNNs by understanding edge influence, overcoming limitations of existing methods that ignore edge insertions and message propagation changes.

Method: Proposes a proximal Bregman response function tailored for GNNs, relaxing convexity and accounting for message propagation, enabling influence prediction for deletions and insertions.

Result: Demonstrates accurate influence predictions on real-world datasets and versatility in applications like graph rewiring and adversarial attacks.

Conclusion: The method effectively addresses gaps in existing influence prediction, enhancing GNN interpretability and robustness.

Abstract: Understanding how individual edges influence the behavior of graph neural
networks (GNNs) is essential for improving their interpretability and
robustness. Graph influence functions have emerged as promising tools to
efficiently estimate the effects of edge deletions without retraining. However,
existing influence prediction methods rely on strict convexity assumptions,
exclusively consider the influence of edge deletions while disregarding edge
insertions, and fail to capture changes in message propagation caused by these
modifications. In this work, we propose a proximal Bregman response function
specifically tailored for GNNs, relaxing the convexity requirement and enabling
accurate influence prediction for standard neural network architectures.
Furthermore, our method explicitly accounts for message propagation effects and
extends influence prediction to both edge deletions and insertions in a
principled way. Experiments with real-world datasets demonstrate accurate
influence predictions for different characteristics of GNNs. We further
demonstrate that the influence function is versatile in applications such as
graph rewiring and adversarial attacks.

</details>


### [122] [On the Mechanism of Reasoning Pattern Selection in Reinforcement Learning for Language Models](https://arxiv.org/abs/2506.04695)
*Xingwu Chen,Tianle Li,Difan Zou*

Main category: cs.LG

TL;DR: RLVR improves reasoning by optimizing the selection of high-success-rate reasoning patterns, with faster convergence for strong models and slower for weaker ones, mitigated by SFT.


<details>
  <summary>Details</summary>
Motivation: To understand how RL enhances reasoning abilities in models, despite empirical success, the mechanisms remain unclear.

Method: Systematic study of RLVR, theoretical analysis of convergence and training dynamics, and validation through experiments.

Result: RLVR selects high-success-rate reasoning patterns, with distinct convergence regimes for strong vs. weak models. SFT can mitigate slower optimization.

Conclusion: This work clarifies RL's role in reasoning enhancement and suggests practical improvements like SFT before RLVR.

Abstract: Reinforcement learning (RL) has demonstrated remarkable success in enhancing
model capabilities, including instruction-following, preference learning, and
reasoning. Yet despite its empirical successes, the mechanisms by which RL
improves reasoning abilities remain poorly understood. We present a systematic
study of Reinforcement Learning with Verifiable Rewards (RLVR), showing that
its primary benefit comes from optimizing the selection of existing reasoning
patterns. Through extensive experiments, we demonstrate that RLVR-trained
models preferentially adopt high-success-rate reasoning patterns while mostly
maintaining stable performance on individual patterns. We further develop
theoretical analyses on the convergence and training dynamics of RLVR based on
a simplified question-reason-answer model. We study the gradient flow and show
that RLVR can indeed find the solution that selects the reason pattern with the
highest success rate. Besides, our theoretical results
  reveal two distinct regimes regarding the convergence of RLVR training: (1)
rapid convergence for models with relatively strong initial reasoning
capabilities versus (2) slower optimization dynamics for weaker models.
Furthermore, we show that the slower optimization for weaker models can be
mitigated by applying the supervised fine-tuning (SFT) before RLVR, when using
a feasibly high-quality SFT dataset. We validate the theoretical findings
through extensive experiments. This work advances our theoretical understanding
of RL's role in LLM fine-tuning and offers insights for further enhancing
reasoning capabilities.

</details>


### [123] [Enhanced Drought Analysis in Bangladesh: A Machine Learning Approach for Severity Classification Using Satellite Data](https://arxiv.org/abs/2506.04696)
*Tonmoy Paul,Mrittika Devi Mati,Md. Mahmudul Islam*

Main category: cs.LG

TL;DR: A machine learning framework using satellite data is proposed to classify drought severity in Bangladesh, addressing gaps in traditional indices and regional variability.


<details>
  <summary>Details</summary>
Motivation: Drought in Bangladesh affects agriculture and food security, but traditional indices and machine learning models lack comprehensive regional integration.

Method: The framework uses unsupervised clustering (K-means, Bayesian Gaussian Mixture) and classification models (KNN, Random Forest, Decision Tree, Naive Bayes) with weather data (2012-2024).

Result: The approach successfully classifies drought severity but reveals regional variability in vulnerabilities.

Conclusion: Machine learning effectively identifies and predicts drought conditions, highlighting regional differences.

Abstract: Drought poses a pervasive environmental challenge in Bangladesh, impacting
agriculture, socio-economic stability, and food security due to its unique
geographic and anthropogenic vulnerabilities. Traditional drought indices, such
as the Standardized Precipitation Index (SPI) and Palmer Drought Severity Index
(PDSI), often overlook crucial factors like soil moisture and temperature,
limiting their resolution. Moreover, current machine learning models applied to
drought prediction have been underexplored in the context of Bangladesh,
lacking a comprehensive integration of satellite data across multiple
districts. To address these gaps, we propose a satellite data-driven machine
learning framework to classify drought across 38 districts of Bangladesh. Using
unsupervised algorithms like K-means and Bayesian Gaussian Mixture for
clustering, followed by classification models such as KNN, Random Forest,
Decision Tree, and Naive Bayes, the framework integrates weather data
(humidity, soil moisture, temperature) from 2012-2024. This approach
successfully classifies drought severity into different levels. However, it
shows significant variabilities in drought vulnerabilities across regions which
highlights the aptitude of machine learning models in terms of identifying and
predicting drought conditions.

</details>


### [124] [Explicit Density Approximation for Neural Implicit Samplers Using a Bernstein-Based Convex Divergence](https://arxiv.org/abs/2506.04700)
*José Manuel de Frutos,Manuel A. Vázquez,Pablo M. Olmos,Joaquín Míguez*

Main category: cs.LG

TL;DR: The paper introduces dual-ISL, a novel likelihood-free objective for training implicit generative models, offering convexity and continuity advantages over classical divergences like KL or Wasserstein distances. It provides theoretical guarantees and empirical improvements in convergence, stability, and mode collapse prevention.


<details>
  <summary>Details</summary>
Motivation: To address limitations of existing rank-based metrics like ISL by proposing a convex and continuous alternative (dual-ISL) for training implicit generative models more effectively.

Method: Introduces dual-ISL, interchanging roles of target and model distributions in ISL, forming a convex problem. Analyzes properties of the resulting discrepancy $d_K$, derives theoretical bounds, and extends to multivariate settings via sliced dual-ISL.

Result: Dual-ISL demonstrates faster convergence, smoother training, better stability, and improved mode collapse prevention compared to ISL and other methods, while also providing explicit density approximation.

Conclusion: Dual-ISL is a robust and theoretically grounded tool for training implicit generative models, outperforming existing methods in practice.

Abstract: Rank-based statistical metrics, such as the invariant statistical loss (ISL),
have recently emerged as robust and practically effective tools for training
implicit generative models. In this work, we introduce dual-ISL, a novel
likelihood-free objective for training implicit generative models that
interchanges the roles of the target and model distributions in the ISL
framework, yielding a convex optimization problem in the space of model
densities. We prove that the resulting rank-based discrepancy $d_K$ is i)
continuous under weak convergence and with respect to the $L^1$ norm, and ii)
convex in its first argument-properties not shared by classical divergences
such as KL or Wasserstein distances. Building on this, we develop a theoretical
framework that interprets $d_K$ as an $L^2$-projection of the density ratio $q
= p/\tilde p$ onto a Bernstein polynomial basis, from which we derive exact
bounds on the truncation error, precise convergence rates, and a closed-form
expression for the truncated density approximation. We further extend our
analysis to the multivariate setting via random one-dimensional projections,
defining a sliced dual-ISL divergence that retains both convexity and
continuity. We empirically show that these theoretical advantages translate
into practical ones. Specifically, across several benchmarks dual-ISL converges
more rapidly, delivers markedly smoother and more stable training, and more
effectively prevents mode collapse than classical ISL and other leading
implicit generative methods-while also providing an explicit density
approximation.

</details>


### [125] [UNO: Unlearning via Orthogonalization in Generative models](https://arxiv.org/abs/2506.04712)
*Pinak Mandal,Georg A. Gottwald*

Main category: cs.LG

TL;DR: The paper introduces fast unlearning algorithms for generative models, focusing on selectively removing data influence without full retraining, ensuring efficiency and model fidelity.


<details>
  <summary>Details</summary>
Motivation: Addressing the need for privacy, legal compliance, and content correction in generative models by enabling selective data removal.

Method: Proposes unlearning algorithms based on loss gradient orthogonalization, ensuring minimal training steps and model quality preservation.

Result: Demonstrates significantly faster unlearning times on MNIST and CelebA datasets compared to prior methods like gradient surgery.

Conclusion: The algorithms effectively forget undesired data while maintaining model performance, offering a practical solution for data removal in generative models.

Abstract: As generative models become increasingly powerful and pervasive, the ability
to unlearn specific data, whether due to privacy concerns, legal requirements,
or the correction of harmful content, has become increasingly important. Unlike
in conventional training, where data are accumulated and knowledge is
reinforced, unlearning aims to selectively remove the influence of particular
data points without costly retraining from scratch. To be effective and
reliable, such algorithms need to achieve (i) forgetting of the undesired data,
(ii) preservation of the quality of the generation, (iii) preservation of the
influence of the desired training data on the model parameters, and (iv) small
number of training steps. We propose fast unlearning algorithms based on loss
gradient orthogonalization. We show that our algorithms are able to forget data
while maintaining the fidelity of the original model. Using MNIST and CelebA
data, we demonstrate that our algorithms achieve orders of magnitude faster
unlearning times than their predecessors, such as gradient surgery.

</details>


### [126] [Multi-Layer GRPO: Enhancing Reasoning and Self-Correction in Large Language Models](https://arxiv.org/abs/2506.04746)
*Fei Ding,Baiqiao Wang,Zijian Zeng,Youwei Wang*

Main category: cs.LG

TL;DR: MGRPO improves GRPO by adding a self-correction layer to fix errors in initial responses, enhancing reasoning and training stability.


<details>
  <summary>Details</summary>
Motivation: GRPO lacks intermediate supervision, causing inefficient exploration and reward vanishing in complex reasoning tasks.

Method: MGRPO uses two layers: standard GRPO for initial responses and a second layer to correct errors, creating a self-correction loop.

Result: MGRPO outperforms GRPO on mathematical reasoning benchmarks by improving reasoning and self-correction.

Conclusion: MGRPO provides implicit supervision and better performance without dense reward annotations.

Abstract: The Group Relative Policy Optimization (GRPO) algorithm has demonstrated
considerable success in enhancing the reasoning capabilities of large language
models (LLMs), as evidenced by DeepSeek-R1. However, the absence of
intermediate supervision in GRPO frequently leads to inefficient exploration
dynamics. A single error in a complex reasoning chain can invalidate the entire
solution, resulting in abrupt reward vanishing and compromising training
stability.To address these challenges, we propose MGRPO (Multi-layer GRPO).
MGRPO operates in two layers: the first layer employs standard GRPO to generate
an initial response. This response, along with the original query, is then fed
into a second-layer GRPO process. This second layer is specifically trained to
identify and correct errors in the initial response, effectively creating a
self-correction loop. This mechanism provides implicit process-level
supervision by rewarding successful error correction, without requiring an
explicit, densely-annotated reward model. Experimental results on several
mathematical reasoning benchmarks demonstrate that MGRPO significantly
outperforms standard GRPO, achieving superior performance by fostering both
reasoning and self-correction abilities.

</details>


### [127] [Log-Linear Attention](https://arxiv.org/abs/2506.04761)
*Han Guo,Songlin Yang,Tarushii Goel,Eric P. Xing,Tri Dao,Yoon Kim*

Main category: cs.LG

TL;DR: Log-linear attention balances efficiency and expressiveness by replacing fixed-size hidden states with logarithmically growing ones, enabling log-linear compute cost.


<details>
  <summary>Details</summary>
Motivation: Address the bottlenecks of quadratic-compute and linear-memory complexity in Transformer attention while overcoming the limitations of fixed-size hidden states in linear attention and state-space models.

Method: Develop log-linear attention, which uses a logarithmically growing set of hidden states and admits matmul-rich parallelization. Apply it to existing architectures like Mamba-2 and Gated DeltaNet.

Result: Log-linear attention variants perform well compared to their linear-time counterparts, demonstrating effectiveness.

Conclusion: Log-linear attention offers a scalable and expressive alternative to traditional attention mechanisms, with practical benefits for sequence modeling.

Abstract: The attention mechanism in Transformers is an important primitive for
accurate and scalable sequence modeling. Its quadratic-compute and
linear-memory complexity however remain significant bottlenecks. Linear
attention and state-space models enable linear-time, constant-memory sequence
modeling and can moreover be trained efficiently through matmul-rich
parallelization across sequence length. However, at their core these models are
still RNNs, and thus their use of a fixed-size hidden state to model the
context is a fundamental limitation. This paper develops log-linear attention,
an attention mechanism that balances linear attention's efficiency and the
expressiveness of softmax attention. Log-linear attention replaces the
fixed-size hidden state with a logarithmically growing set of hidden states. We
show that with a particular growth function, log-linear attention admits a
similarly matmul-rich parallel form whose compute cost is log-linear in
sequence length. Log-linear attention is a general framework and can be applied
on top of existing linear attention variants. As case studies, we instantiate
log-linear variants of two recent architectures -- Mamba-2 and Gated DeltaNet
-- and find they perform well compared to their linear-time variants.

</details>


### [128] [OpenGT: A Comprehensive Benchmark For Graph Transformers](https://arxiv.org/abs/2506.04765)
*Jiachen Tang,Zhonghao Wang,Sirui Chen,Sheng Zhou,Jiawei Chen,Jiajun Bu*

Main category: cs.LG

TL;DR: The paper introduces OpenGT, a benchmark for Graph Transformers (GTs), addressing the lack of standardized evaluation and understanding of GTs' design choices and performance.


<details>
  <summary>Details</summary>
Motivation: GTs show promise but lack exploration of their effective scenarios and design choices. The community needs a comprehensive benchmark to understand and advance GTs.

Method: OpenGT provides standardized settings, diverse tasks, and datasets to evaluate GTs and GNNs fairly. It includes a library for training and evaluation.

Result: Key insights include challenges in model transfer, limitations of local attention, efficiency trade-offs, and positional encoding scenarios.

Conclusion: OpenGT aims to foster fair, reproducible, and generalizable GT research, supported by an accessible library.

Abstract: Graph Transformers (GTs) have recently demonstrated remarkable performance
across diverse domains. By leveraging attention mechanisms, GTs are capable of
modeling long-range dependencies and complex structural relationships beyond
local neighborhoods. However, their applicable scenarios are still
underexplored, this highlights the need to identify when and why they excel.
Furthermore, unlike GNNs, which predominantly rely on message-passing
mechanisms, GTs exhibit a diverse design space in areas such as positional
encoding, attention mechanisms, and graph-specific adaptations. Yet, it remains
unclear which of these design choices are truly effective and under what
conditions. As a result, the community currently lacks a comprehensive
benchmark and library to promote a deeper understanding and further development
of GTs. To address this gap, this paper introduces OpenGT, a comprehensive
benchmark for Graph Transformers. OpenGT enables fair comparisons and
multidimensional analysis by establishing standardized experimental settings
and incorporating a broad selection of state-of-the-art GNNs and GTs. Our
benchmark evaluates GTs from multiple perspectives, encompassing diverse tasks
and datasets with varying properties. Through extensive experiments, our
benchmark has uncovered several critical insights, including the difficulty of
transferring models across task levels, the limitations of local attention, the
efficiency trade-offs in several models, the application scenarios of specific
positional encodings, and the preprocessing overhead of some positional
encodings. We aspire for this work to establish a foundation for future graph
transformer research emphasizing fairness, reproducibility, and
generalizability. We have developed an easy-to-use library OpenGT for training
and evaluating existing GTs. The benchmark code is available at
https://github.com/eaglelab-zju/OpenGT.

</details>


### [129] [Improved Regret Bounds for Linear Bandits with Heavy-Tailed Rewards](https://arxiv.org/abs/2506.04775)
*Artin Tajdini,Jonathan Scarlett,Kevin Jamieson*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study stochastic linear bandits with heavy-tailed rewards, where the
rewards have a finite $(1+\epsilon)$-absolute central moment bounded by
$\upsilon$ for some $\epsilon \in (0,1]$. We improve both upper and lower
bounds on the minimax regret compared to prior work. When $\upsilon =
\mathcal{O}(1)$, the best prior known regret upper bound is
$\tilde{\mathcal{O}}(d T^{\frac{1}{1+\epsilon}})$. While a lower with the same
scaling has been given, it relies on a construction using $\upsilon =
\mathcal{O}(d)$, and adapting the construction to the bounded-moment regime
with $\upsilon = \mathcal{O}(1)$ yields only a
$\Omega(d^{\frac{\epsilon}{1+\epsilon}} T^{\frac{1}{1+\epsilon}})$ lower bound.
This matches the known rate for multi-armed bandits and is generally loose for
linear bandits, in particular being $\sqrt{d}$ below the optimal rate in the
finite-variance case ($\epsilon = 1$). We propose a new elimination-based
algorithm guided by experimental design, which achieves regret
$\tilde{\mathcal{O}}(d^{\frac{1+3\epsilon}{2(1+\epsilon)}}
T^{\frac{1}{1+\epsilon}})$, thus improving the dependence on $d$ for all
$\epsilon \in (0,1)$ and recovering a known optimal result for $\epsilon = 1$.
We also establish a lower bound of $\Omega(d^{\frac{2\epsilon}{1+\epsilon}}
T^{\frac{1}{1+\epsilon}})$, which strictly improves upon the multi-armed bandit
rate and highlights the hardness of heavy-tailed linear bandit problems. For
finite action sets, we derive similarly improved upper and lower bounds for
regret. Finally, we provide action set dependent regret upper bounds showing
that for some geometries, such as $l_p$-norm balls for $p \le 1 + \epsilon$, we
can further reduce the dependence on $d$, and we can handle
infinite-dimensional settings via the kernel trick, in particular establishing
new regret bounds for the Mat\'ern kernel that are the first to be sublinear
for all $\epsilon \in (0, 1]$.

</details>


### [130] [Kernel $k$-Medoids as General Vector Quantization](https://arxiv.org/abs/2506.04786)
*Thore Gerlach,Sascha Mücke,Christian Bauckhage*

Main category: cs.LG

TL;DR: The paper explores the connection between $k$-medoids clustering and Kernel Density Estimation (KDE) in Vector Quantization (VQ) using Quadratic Unconstrained Binary Optimization (QUBO), showing KDE-QUBO as a special case of $k$-medoids-QUBO.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between distance-based ($k$-medoids) and probability-based (KDE) VQ methods and uncover their structural relationship.

Method: Uses QUBO formulations for $k$-medoids and KDE-based VQ, comparing their mathematical properties.

Result: Demonstrates KDE-QUBO is a special case of $k$-medoids-QUBO under mild kernel assumptions.

Conclusion: Reveals a deeper connection between $k$-medoids and KDE in VQ, offering new insights into QUBO parameter interpretations.

Abstract: Vector Quantization (VQ) is a widely used technique in machine learning and
data compression, valued for its simplicity and interpretability. Among hard VQ
methods, $k$-medoids clustering and Kernel Density Estimation (KDE) approaches
represent two prominent yet seemingly unrelated paradigms -- one
distance-based, the other rooted in probability density matching. In this
paper, we investigate their connection through the lens of Quadratic
Unconstrained Binary Optimization (QUBO). We compare a heuristic QUBO
formulation for $k$-medoids, which balances centrality and diversity, with a
principled QUBO derived from minimizing Maximum Mean Discrepancy in KDE-based
VQ. Surprisingly, we show that the KDE-QUBO is a special case of the
$k$-medoids-QUBO under mild assumptions on the kernel's feature map. This
reveals a deeper structural relationship between these two approaches and
provides new insight into the geometric interpretation of the weighting
parameters used in QUBO formulations for VQ.

</details>


### [131] [Adaptive Preconditioners Trigger Loss Spikes in Adam](https://arxiv.org/abs/2506.04805)
*Zhiwei Bai,Zhangchen Zhou,Jiajie Zhao,Xiaolong Li,Zhiyu Li,Feiyu Xiong,Hongkang Yang,Yaoyu Zhang,Zhi-Qin John Xu*

Main category: cs.LG

TL;DR: The paper investigates the cause of loss spikes in neural networks trained with Adam, attributing them to Adam's adaptive preconditioners rather than the loss landscape's sharpness.


<details>
  <summary>Details</summary>
Motivation: Loss spikes during training with Adam are common but poorly understood. Previous explanations focused on the loss landscape, but this work explores Adam's internal mechanisms.

Method: The study analyzes Adam's adaptive preconditioners, identifying a critical regime where squared gradients are much smaller than second-order moment estimates, leading to instability.

Result: The mechanism causes the preconditioned Hessian's maximum eigenvalue to exceed stability thresholds, inducing loss spikes when gradient-directional curvature surpasses 2/η.

Conclusion: The findings are validated across various architectures, showing Adam's preconditioners as the primary cause of loss spikes.

Abstract: Loss spikes emerge commonly during training across neural networks of varying
architectures and scales when using the Adam optimizer. In this work, we
investigate the underlying mechanism responsible for Adam spikes. While
previous explanations attribute these phenomena to the lower-loss-as-sharper
characteristics of the loss landscape, our analysis reveals that Adam's
adaptive preconditioners themselves can trigger spikes. Specifically, we
identify a critical regime where squared gradients become substantially smaller
than the second-order moment estimates, causing the latter to undergo a
$\beta_2$-exponential decay and to respond sluggishly to current gradient
information. This mechanism can push the maximum eigenvalue of the
preconditioned Hessian beyond the classical stability threshold $2/\eta$ for a
sustained period, inducing instability. This instability further leads to an
alignment between the gradient and the maximum eigendirection, and a loss spike
occurs precisely when the gradient-directional curvature exceeds $2/\eta$. We
verify this mechanism through extensive experiments on fully connected
networks, convolutional networks, and Transformer architectures.

</details>


### [132] [LogicPuzzleRL: Cultivating Robust Mathematical Reasoning in LLMs via Reinforcement Learning](https://arxiv.org/abs/2506.04821)
*Zhen Hao Wong,Jingwen Deng,Runming He,Zirong Chen,Qijie You,Hejun Dong,Hao Liang,Chengyu Shen,Bin Cui,Wentao Zhang*

Main category: cs.LG

TL;DR: A 'play to learn' framework uses reinforcement learning on logic puzzles to improve LLMs' structured reasoning, showing better generalization on mathematical tasks.


<details>
  <summary>Details</summary>
Motivation: Standard fine-tuning may limit LLMs to narrow heuristics; this work aims to foster general-purpose reasoning.

Method: Fine-tuning LLMs via reinforcement learning on seven custom logic puzzles with verifiable binary rewards.

Result: Improved out-of-distribution performance on math benchmarks, especially for multi-step reasoning tasks.

Conclusion: Reinforcement learning over puzzles enhances LLMs' reasoning, enabling robust generalization without task-specific tools.

Abstract: Large language models (LLMs) excel at many supervised tasks but often
struggle with structured reasoning in unfamiliar settings. This discrepancy
suggests that standard fine-tuning pipelines may instill narrow,
domain-specific heuristics rather than fostering general-purpose thinking
strategies. In this work, we propose a "play to learn" framework that
fine-tunes LLMs through reinforcement learning on a suite of seven custom logic
puzzles, each designed to cultivate distinct reasoning skills such as
constraint propagation, spatial consistency, and symbolic deduction. Using a
reinforcement learning setup with verifiable rewards, models receive binary
feedback based on puzzle correctness, encouraging iterative, hypothesis-driven
problem solving. We demonstrate that this training approach significantly
improves out-of-distribution performance on a range of mathematical benchmarks,
especially for mid-difficulty problems that require multi-step reasoning.
Analyses across problem categories and difficulty levels reveal that puzzle
training promotes transferable reasoning routines, strengthening algebraic
manipulation, geometric inference, and combinatorial logic, while offering
limited gains on rote or highly specialized tasks. These findings show that
reinforcement learning over logic puzzles reshapes the internal reasoning of
LLMs, enabling more robust and compositional generalization without relying on
task-specific symbolic tools.

</details>


### [133] [From EHRs to Patient Pathways: Scalable Modeling of Longitudinal Health Trajectories with LLMs](https://arxiv.org/abs/2506.04831)
*Chantal Pellegrini,Ege Özsoy,David Bani-Harouni,Matthias Keicher,Nassir Navab*

Main category: cs.LG

TL;DR: EHR2Path transforms EHR data into structured representations for holistic patient pathway prediction, outperforming baselines in forecasting health trajectories.


<details>
  <summary>Details</summary>
Motivation: Healthcare systems struggle with managing diverse patient data for personalized care, lacking holistic approaches for longitudinal interactions.

Method: Proposes EHR2Path, a model that structures EHR data and uses a summary mechanism for long-term temporal context, optimizing pathway prediction.

Result: EHR2Path excels in next time-step prediction and longitudinal simulation, outperforming baselines in tasks like vital signs forecasting.

Conclusion: EHR2Path enables predictive, personalized healthcare by simulating detailed patient trajectories and diverse evaluation tasks.

Abstract: Healthcare systems face significant challenges in managing and interpreting
vast, heterogeneous patient data for personalized care. Existing approaches
often focus on narrow use cases with a limited feature space, overlooking the
complex, longitudinal interactions needed for a holistic understanding of
patient health. In this work, we propose a novel approach to patient pathway
modeling by transforming diverse electronic health record (EHR) data into a
structured representation and designing a holistic pathway prediction model,
EHR2Path, optimized to predict future health trajectories. Further, we
introduce a novel summary mechanism that embeds long-term temporal context into
topic-specific summary tokens, improving performance over text-only models,
while being much more token-efficient. EHR2Path demonstrates strong performance
in both next time-step prediction and longitudinal simulation, outperforming
competitive baselines. It enables detailed simulations of patient trajectories,
inherently targeting diverse evaluation tasks, such as forecasting vital signs,
lab test results, or length-of-stay, opening a path towards predictive and
personalized healthcare.

</details>


### [134] [Sparse Autoencoders, Again?](https://arxiv.org/abs/2506.04859)
*Yin Lu,Tong He,Xuening Zhu,David Wipf*

Main category: cs.LG

TL;DR: The paper critiques traditional sparse autoencoders (SAEs) and variational autoencoders (VAEs), proposing a hybrid model to overcome their weaknesses. It offers theoretical guarantees and empirical validation for improved performance in tasks like image and language model activation analysis.


<details>
  <summary>Details</summary>
Motivation: Despite the broad applicability of SAEs and VAEs, their limitations in handling structured data and sparse representations remain underaddressed. The paper aims to bridge this gap by introducing a hybrid model.

Method: The authors propose a hybrid alternative combining strengths of SAEs and VAEs, with theoretical proofs and empirical tests on synthetic and real-world datasets.

Result: The hybrid model outperforms SAEs, VAEs, and diffusion models in tasks like estimating manifold dimensions and producing sparser latent representations without sacrificing reconstruction accuracy.

Conclusion: The hybrid model effectively addresses limitations of traditional SAEs and VAEs, demonstrating superior performance in practical applications.

Abstract: Is there really much more to say about sparse autoencoders (SAEs)?
Autoencoders in general, and SAEs in particular, represent deep architectures
that are capable of modeling low-dimensional latent structure in data. Such
structure could reflect, among other things, correlation patterns in large
language model activations, or complex natural image manifolds. And yet despite
the wide-ranging applicability, there have been relatively few changes to SAEs
beyond the original recipe from decades ago, namely, standard deep
encoder/decoder layers trained with a classical/deterministic sparse
regularizer applied within the latent space. One possible exception is the
variational autoencoder (VAE), which adopts a stochastic encoder module capable
of producing sparse representations when applied to manifold data. In this work
we formalize underappreciated weaknesses with both canonical SAEs, as well as
analogous VAEs applied to similar tasks, and propose a hybrid alternative model
that circumvents these prior limitations. In terms of theoretical support, we
prove that global minima of our proposed model recover certain forms of
structured data spread across a union of manifolds. Meanwhile, empirical
evaluations on synthetic and real-world datasets substantiate the efficacy of
our approach in accurately estimating underlying manifold dimensions and
producing sparser latent representations without compromising reconstruction
error. In general, we are able to exceed the performance of equivalent-capacity
SAEs and VAEs, as well as recent diffusion models where applicable, within
domains such as images and language model activation patterns.

</details>


### [135] [Aligning Multimodal Representations through an Information Bottleneck](https://arxiv.org/abs/2506.04870)
*Antonio Almudévar,José Miguel Hernández-Lobato,Sameer Khurana,Ricard Marxer,Alfonso Ortega*

Main category: cs.LG

TL;DR: The paper addresses the ineffectiveness of contrastive losses in learning aligned multimodal representations due to modality-specific information. It proposes a regularization term to improve alignment.


<details>
  <summary>Details</summary>
Motivation: Contrastive losses fail to align multimodal representations because they retain modality-specific information, despite maximizing mutual information.

Method: Theoretical analysis via the Information Bottleneck Principle, empirical hyperparameter study, and introduction of a variational approximation-derived regularization term.

Result: The proposed regularization term enhances representational alignment, validated in controlled experiments and real-world applications.

Conclusion: The regularization term effectively mitigates modality-specific information, improving multimodal representation alignment.

Abstract: Contrastive losses have been extensively used as a tool for multimodal
representation learning. However, it has been empirically observed that their
use is not effective to learn an aligned representation space. In this paper,
we argue that this phenomenon is caused by the presence of modality-specific
information in the representation space. Although some of the most widely used
contrastive losses maximize the mutual information between representations of
both modalities, they are not designed to remove the modality-specific
information. We give a theoretical description of this problem through the lens
of the Information Bottleneck Principle. We also empirically analyze how
different hyperparameters affect the emergence of this phenomenon in a
controlled experimental setup. Finally, we propose a regularization term in the
loss function that is derived by means of a variational approximation and aims
to increase the representational alignment. We analyze in a set of controlled
experiments and real-world applications the advantages of including this
regularization term.

</details>


### [136] [There Was Never a Bottleneck in Concept Bottleneck Models](https://arxiv.org/abs/2506.04877)
*Antonio Almudévar,José Miguel Hernández-Lobato,Alfonso Ortega*

Main category: cs.LG

TL;DR: The paper introduces Minimal Concept Bottleneck Models (MCBMs) to improve interpretability in deep learning by ensuring each representation component encodes only relevant concept information, unlike traditional CBMs.


<details>
  <summary>Details</summary>
Motivation: Deep learning representations lack interpretability, especially in sensitive applications. CBMs partially address this but fail to ensure components encode only their assigned concepts, raising concerns about interpretability and intervention validity.

Method: The authors propose MCBMs, which use an Information Bottleneck (IB) objective via variational regularization to constrain each representation component to retain only concept-relevant information.

Result: MCBMs provide theoretical guarantees for concept-level interventions, align with Bayesian principles, and offer design flexibility.

Conclusion: MCBMs enhance interpretability and intervention validity in deep learning by enforcing a true bottleneck, addressing limitations of CBMs.

Abstract: Deep learning representations are often difficult to interpret, which can
hinder their deployment in sensitive applications. Concept Bottleneck Models
(CBMs) have emerged as a promising approach to mitigate this issue by learning
representations that support target task performance while ensuring that each
component predicts a concrete concept from a predefined set. In this work, we
argue that CBMs do not impose a true bottleneck: the fact that a component can
predict a concept does not guarantee that it encodes only information about
that concept. This shortcoming raises concerns regarding interpretability and
the validity of intervention procedures. To overcome this limitation, we
propose Minimal Concept Bottleneck Models (MCBMs), which incorporate an
Information Bottleneck (IB) objective to constrain each representation
component to retain only the information relevant to its corresponding concept.
This IB is implemented via a variational regularization term added to the
training loss. As a result, MCBMs support concept-level interventions with
theoretical guarantees, remain consistent with Bayesian principles, and offer
greater flexibility in key design choices.

</details>


### [137] [Gaussian Process Diffeomorphic Statistical Shape Modelling Outperforms Angle-Based Methods for Assessment of Hip Dysplasia](https://arxiv.org/abs/2506.04886)
*Allen Paul,George Grammatopoulos,Adwaye Rambojun,Neill D. F. Campbell,Harinderjit S. Gill,Tony Shardlow*

Main category: cs.LG

TL;DR: A semi-automated pipeline using GPDSSM improves dysplasia classification in hip CT scans, outperforming angle-based methods with 96.2% AUC.


<details>
  <summary>Details</summary>
Motivation: Early diagnosis of hip dysplasia is crucial to prevent osteoarthritis, requiring better tools than manual angle measurements.

Method: Developed GPDSSM, combining Gaussian Process Latent Variable Model with diffeomorphism, using 192 CT scans (100 training, 92 testing).

Result: GPDSSM achieved 96.2% AUC, outperforming angle-based methods (91.2%), and identified dysplastic variations in hip surfaces.

Conclusion: GPDSSM enhances dysplasia diagnosis accuracy and efficiency, reducing manual workload for clinicians.

Abstract: Dysplasia is a recognised risk factor for osteoarthritis (OA) of the hip,
early diagnosis of dysplasia is important to provide opportunities for surgical
interventions aimed at reducing the risk of hip OA. We have developed a
pipeline for semi-automated classification of dysplasia using volumetric CT
scans of patients' hips and a minimal set of clinically annotated landmarks,
combining the framework of the Gaussian Process Latent Variable Model with
diffeomorphism to create a statistical shape model, which we termed the
Gaussian Process Diffeomorphic Statistical Shape Model (GPDSSM). We used 192 CT
scans, 100 for model training and 92 for testing. The GPDSSM effectively
distinguishes dysplastic samples from controls while also highlighting regions
of the underlying surface that show dysplastic variations. As well as improving
classification accuracy compared to angle-based methods (AUC 96.2% vs 91.2%),
the GPDSSM can save time for clinicians by removing the need to manually
measure angles and interpreting 2D scans for possible markers of dysplasia.

</details>


### [138] [Dissecting Long Reasoning Models: An Empirical Study](https://arxiv.org/abs/2506.04913)
*Yongyu Mu,Jiali Zeng,Bei Li,Xinyan Guan,Fandong Meng,Jie Zhou,Tong Xiao,Jingbo Zhu*

Main category: cs.LG

TL;DR: The paper analyzes key aspects of RL for long-context reasoning, highlighting the roles of positive/negative samples, data inefficiency, and performance instability.


<details>
  <summary>Details</summary>
Motivation: To address open questions and counterintuitive behaviors in RL for long-context reasoning, focusing on sample roles, data efficiency, and model stability.

Method: Systematic analysis of sample roles, strategies to improve data efficiency (e.g., relative length rewards), and multiple evaluation runs to address instability.

Result: Negative samples enhance generalization; data inefficiency identified and mitigated; multiple evaluations stabilize performance.

Conclusion: The study provides insights and practical solutions to improve RL training for long-context reasoning.

Abstract: Despite recent progress in training long-context reasoning models via
reinforcement learning (RL), several open questions and counterintuitive
behaviors remain. This work focuses on three key aspects: (1) We systematically
analyze the roles of positive and negative samples in RL, revealing that
positive samples mainly facilitate data fitting, whereas negative samples
significantly enhance generalization and robustness. Interestingly, training
solely on negative samples can rival standard RL training performance. (2) We
identify substantial data inefficiency in group relative policy optimization,
where over half of the samples yield zero advantage. To address this, we
explore two straightforward strategies, including relative length rewards and
offline sample injection, to better leverage these data and enhance reasoning
efficiency and capability. (3) We investigate unstable performance across
various reasoning models and benchmarks, attributing instability to uncertain
problems with ambiguous outcomes, and demonstrate that multiple evaluation runs
mitigate this issue.

</details>


### [139] [Predicting ICU In-Hospital Mortality Using Adaptive Transformer Layer Fusion](https://arxiv.org/abs/2506.04924)
*Han Wang,Ruoyun He,Guoguang Lao,Ting Liu,Hejiao Luo,Changqi Qin,Hongying Luo,Junmin Huang,Zihan Wei,Lu Chen,Yongzhi Xu,Ziqian Bi,Junhao Song,Tianyang Wang,Chia Xin Liang,Xinyuan Song,Huafeng Liu,Junfeng Hao,Chunjie Tian*

Main category: cs.LG

TL;DR: ALFIA is an attention-based model for early ICU patient risk identification, outperforming state-of-the-art methods and enabling integration with other models for enhanced performance.


<details>
  <summary>Details</summary>
Motivation: Early identification of high-risk ICU patients is critical for resource allocation and timely intervention.

Method: ALFIA uses a modular, attention-based architecture with LoRA adapters and adaptive layer-weighting to fuse multi-layer BERT features, trained on the cw-24 benchmark.

Result: ALFIA surpasses tabular classifiers in AUPRC and integrates with GBDTs and deep networks (ALFIA-boost, ALFIA-nn) for further gains.

Conclusion: ALFIA provides a robust tool for risk stratification in critical-care settings using routine clinical text.

Abstract: Early identification of high-risk ICU patients is crucial for directing
limited medical resources. We introduce ALFIA (Adaptive Layer Fusion with
Intelligent Attention), a modular, attention-based architecture that jointly
trains LoRA (Low-Rank Adaptation) adapters and an adaptive layer-weighting
mechanism to fuse multi-layer semantic features from a BERT backbone. Trained
on our rigorous cw-24 (CriticalWindow-24) benchmark, ALFIA surpasses
state-of-the-art tabular classifiers in AUPRC while preserving a balanced
precision-recall profile. The embeddings produced by ALFIA's fusion module,
capturing both fine-grained clinical cues and high-level concepts, enable
seamless pairing with GBDTs (CatBoost/LightGBM) as ALFIA-boost, and deep neuro
networks as ALFIA-nn, yielding additional performance gains. Our experiments
confirm ALFIA's superior early-warning performance, by operating directly on
routine clinical text, it furnishes clinicians with a convenient yet robust
tool for risk stratification and timely intervention in critical-care settings.

</details>


### [140] [Agentic AI for Intent-Based Industrial Automation](https://arxiv.org/abs/2506.04980)
*Marcos Lima Romero,Ricardo Suyama*

Main category: cs.LG

TL;DR: A framework integrating Agentic AI with intent-based paradigms simplifies HMI and aligns with Industry 5.0 principles, demonstrated via predictive maintenance.


<details>
  <summary>Details</summary>
Motivation: To reduce complexity in Industry 4.0 and align automation with human-centric, sustainable, and resilient goals of Industry 5.0.

Method: Proposes a framework combining Agentic AI and intent-based processing, allowing natural language goals to be decomposed into actionable tasks by specialized sub-agents.

Result: Proof of concept using CMAPSS dataset and Google ADK shows feasibility in predictive maintenance, reducing technical barriers.

Conclusion: The approach enables scalable, intent-driven automation, though data quality and explainability remain challenges.

Abstract: The recent development of Agentic AI systems, empowered by autonomous large
language models (LLMs) agents with planning and tool-usage capabilities,
enables new possibilities for the evolution of industrial automation and
reduces the complexity introduced by Industry 4.0. This work proposes a
conceptual framework that integrates Agentic AI with the intent-based paradigm,
originally developed in network research, to simplify human-machine interaction
(HMI) and better align automation systems with the human-centric, sustainable,
and resilient principles of Industry 5.0. Based on the intent-based processing,
the framework allows human operators to express high-level business or
operational goals in natural language, which are decomposed into actionable
components. These intents are broken into expectations, conditions, targets,
context, and information that guide sub-agents equipped with specialized tools
to execute domain-specific tasks. A proof of concept was implemented using the
CMAPSS dataset and Google Agent Developer Kit (ADK), demonstrating the
feasibility of intent decomposition, agent orchestration, and autonomous
decision-making in predictive maintenance scenarios. The results confirm the
potential of this approach to reduce technical barriers and enable scalable,
intent-driven automation, despite data quality and explainability concerns.

</details>


### [141] [FPTQuant: Function-Preserving Transforms for LLM Quantization](https://arxiv.org/abs/2506.04985)
*Boris van Breugel,Yelysei Bondarenko,Paul Whatmough,Markus Nagel*

Main category: cs.LG

TL;DR: FPTQuant introduces lightweight transforms to improve LLM quantization efficiency without degrading performance, achieving up to 3.9x speed-up with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) consume significant energy during inference, and naive quantization harms performance due to outliers. FPTQuant addresses this by preserving model function while enabling efficient quantization.

Method: FPTQuant uses four novel function-preserving transforms (FPTs) to reshape activation distributions for better quantization. These include transforms for queries, keys, values, and MLP blocks, trained locally and end-to-end.

Result: FPTQuant enables static INT4 quantization with minimal overhead, achieving up to 3.9x speed-up over FP while maintaining accuracy comparable to slower methods.

Conclusion: FPTQuant offers a state-of-the-art solution for efficient LLM quantization, balancing accuracy and speed effectively.

Abstract: Large language models (LLMs) require substantial compute, and thus energy, at
inference time. While quantizing weights and activations is effective at
improving efficiency, naive quantization of LLMs can significantly degrade
performance due to large magnitude outliers. This paper describes FPTQuant,
which introduces four novel, lightweight, and expressive function-preserving
transforms (FPTs) to facilitate quantization of transformers: (1) a mergeable
pre-RoPE transform for queries and keys, (2) a mergeable transform for values,
(3) a mergeable scaling transform within the MLP block, and (4) a cheap,
dynamic scaling transform. By leveraging the equivariances and independencies
inherent to canonical transformer operation, we designed these FPTs to maintain
the model's function while shaping the intermediate activation distributions to
be more quantization friendly. FPTQuant requires no custom kernels and adds
virtually no overhead during inference. The FPTs are trained both locally to
reduce outliers, and end-to-end such that the outputs of the quantized and
full-precision models match. FPTQuant enables static INT4 quantization with
minimal overhead and shows SOTA speed-up of up to 3.9 times over FP.
Empirically, FPTQuant has an excellent accuracy-speed trade-off -- it is
performing on par or exceeding most prior work and only shows slightly lower
accuracy compared to a method that is up to 29% slower.

</details>


### [142] [Cautious Optimism: A Meta-Algorithm for Near-Constant Regret in General Games](https://arxiv.org/abs/2506.05005)
*Ashkan Soleymani,Georgios Piliouras,Gabriele Farina*

Main category: cs.LG

TL;DR: The paper introduces 'Cautious Optimism,' a framework for accelerating no-regret learning in general games by adaptively pacing FTRL algorithms, achieving near-optimal regret bounds without relying on monotonic step-sizes.


<details>
  <summary>Details</summary>
Motivation: To show that adaptive pacing for no-regret learning is not isolated and can be generalized to accelerate FTRL algorithms in diverse game settings.

Method: Proposes 'Cautious Optimism,' which takes any FTRL instance and outputs an accelerated algorithm with minimal computational overhead, retaining uncoupledness.

Result: Achieves near-optimal O_T(log T) regret in self-play and preserves O(sqrt(T)) regret in adversarial scenarios.

Conclusion: Demonstrates a novel, non-monotonic approach for fast learning in general games, outperforming prior methods.

Abstract: Recent work [Soleymani et al., 2025] introduced a variant of Optimistic
Multiplicative Weights Updates (OMWU) that adaptively controls the learning
pace in a dynamic, non-monotone manner, achieving new state-of-the-art regret
minimization guarantees in general games. In this work, we demonstrate that
no-regret learning acceleration through adaptive pacing of the learners is not
an isolated phenomenon. We introduce \emph{Cautious Optimism}, a framework for
substantially faster regularized learning in general games. Cautious Optimism
takes as input any instance of Follow-the-Regularized-Leader (FTRL) and outputs
an accelerated no-regret learning algorithm by pacing the underlying FTRL with
minimal computational overhead. Importantly, we retain uncoupledness (learners
do not need to know other players' utilities). Cautious Optimistic FTRL
achieves near-optimal $O_T(\log T)$ regret in diverse self-play
(mixing-and-matching regularizers) while preserving the optimal $O(\sqrt{T})$
regret in adversarial scenarios. In contrast to prior works (e.g. Syrgkanis et
al. [2015], Daskalakis et al. [2021]), our analysis does not rely on monotonic
step-sizes, showcasing a novel route for fast learning in general games.

</details>


### [143] [Towards Reasonable Concept Bottleneck Models](https://arxiv.org/abs/2506.05014)
*Nektarios Kalampalikis,Kavya Gupta,Georgi Vitanov,Isabel Valera*

Main category: cs.LG

TL;DR: CREAM is a new CBM family that encodes concept-concept and concept-task relationships, uses a regularized side-channel for performance, and maintains high concept importance.


<details>
  <summary>Details</summary>
Motivation: To improve model reasoning by explicitly encoding human-specified relationships between concepts and tasks while preventing undesired information flows.

Method: CREAM embeds (bi)directed C-C and C→Y relationships, uses a regularized side-channel, and severs undesired flows.

Result: CREAM achieves task performance comparable to black-box models while relying on concepts and easing interventions.

Conclusion: CREAM effectively balances concept importance and task performance, mitigates concept leakage, and supports human interventions.

Abstract: In this paper, we propose $\textbf{C}$oncept $\textbf{REA}$soning
$\textbf{M}$odels (CREAM), a novel family of Concept Bottleneck Models (CBMs)
that: (i) explicitly encodes concept-concept (${\texttt{C-C}}$) and
concept-task (${\texttt{C$\rightarrow$Y}}$) relationships to enforce a desired
model reasoning; and (ii) use a regularized side-channel to achieve competitive
task performance, while keeping high concept importance. Specifically, CREAM
architecturally embeds (bi)directed concept-concept, and concept to task
relationships specified by a human expert, while severing undesired information
flows (e.g., to handle mutually exclusive concepts). Moreover, CREAM integrates
a black-box side-channel that is regularized to encourage task predictions to
be grounded in the relevant concepts, thereby utilizing the side-channel only
when necessary to enhance performance. Our experiments show that: (i) CREAM
mainly relies on concepts while achieving task performance on par with
black-box models; and (ii) the embedded ${\texttt{C-C}}$ and
${\texttt{C$\rightarrow$Y}}$ relationships ease model interventions and
mitigate concept leakage.

</details>


### [144] [Multi-Point Proximity Encoding For Vector-Mode Geospatial Machine Learning](https://arxiv.org/abs/2506.05016)
*John Collins*

Main category: cs.LG

TL;DR: MPP encoding transforms geospatial shapes into vectors using scaled distances from reference points, outperforming rasterization in capturing geometric features and spatial relationships.


<details>
  <summary>Details</summary>
Motivation: To enable machine learning models to process vector-mode geospatial data (points, lines, polygons) by encoding shapes into vectors that preserve geometric properties.

Method: MultiPoint Proximity (MPP) encoding, which uses scaled distances from a shape to reference points within a region of interest.

Result: MPP encoding exhibits shape-centricity, continuity, and high precision in differentiating spatial objects and capturing pairwise relationships, outperforming rasterization.

Conclusion: MPP encoding is a versatile and effective method for encoding geospatial shapes, enhancing their usability in machine learning models.

Abstract: Vector-mode geospatial data -- points, lines, and polygons -- must be encoded
into an appropriate form in order to be used with traditional machine learning
and artificial intelligence models. Encoding methods attempt to represent a
given shape as a vector that captures its essential geometric properties. This
paper presents an encoding method based on scaled distances from a shape to a
set of reference points within a region of interest. The method, MultiPoint
Proximity (MPP) encoding, can be applied to any type of shape, enabling the
parameterization of machine learning models with encoded representations of
vector-mode geospatial features. We show that MPP encoding possesses the
desirable properties of shape-centricity and continuity, can be used to
differentiate spatial objects based on their geometric features, and can
capture pairwise spatial relationships with high precision. In all cases, MPP
encoding is shown to perform better than an alternative method based on
rasterization.

</details>


### [145] [Tuning the Right Foundation Models is What you Need for Partial Label Learning](https://arxiv.org/abs/2506.05027)
*Kuang He,Wei Tang,Tong Wei,Min-Ling Zhang*

Main category: cs.LG

TL;DR: The paper evaluates foundation models in partial label learning (PLL), proposes PartialCLIP for efficient fine-tuning, and highlights key insights about current PLL approaches.


<details>
  <summary>Details</summary>
Motivation: To address the lack of attention to foundation models in PLL and improve generalizability of classifiers trained with inexact supervision.

Method: Empirical evaluation of 11 foundation models across 13 PLL approaches on 8 datasets under 3 scenarios, and introduction of PartialCLIP for fine-tuning.

Result: Foundation models significantly boost PLL performance; approaches show similar results, stable across ambiguity levels, but depend on model selection and adaptation.

Conclusion: The study reveals limitations of current PLL methods and offers insights for developing more generalizable models, with PartialCLIP as a promising solution.

Abstract: Partial label learning (PLL) seeks to train generalizable classifiers from
datasets with inexact supervision, a common challenge in real-world
applications. Existing studies have developed numerous approaches to
progressively refine and recover ground-truth labels by training convolutional
neural networks. However, limited attention has been given to foundation models
that offer transferrable representations. In this work, we empirically conduct
comprehensive evaluations of 11 foundation models across 13 PLL approaches on 8
benchmark datasets under 3 PLL scenarios. We further propose PartialCLIP, an
efficient fine-tuning framework for foundation models in PLL. Our findings
reveal that current PLL approaches tend to 1) achieve significant performance
gains when using foundation models, 2) exhibit remarkably similar performance
to each other, 3) maintain stable performance across varying ambiguity levels,
while 4) are susceptible to foundation model selection and adaptation
strategies. Additionally, we demonstrate the efficacy of text-embedding
classifier initialization and effective candidate label filtering using
zero-shot CLIP. Our experimental results and analysis underscore the
limitations of current PLL approaches and provide valuable insights for
developing more generalizable PLL models. The source code can be found at
https://github.com/SEU-hk/PartialCLIP.

</details>


### [146] [Identifying and Understanding Cross-Class Features in Adversarial Training](https://arxiv.org/abs/2506.05032)
*Zeming Wei,Yiwen Guo,Yisen Wang*

Main category: cs.LG

TL;DR: The paper studies adversarial training (AT) through class-wise feature attribution, identifying cross-class features as key for robustness. It shows models initially learn these features but shift to class-specific ones during overfitting, unifying insights into AT mechanisms.


<details>
  <summary>Details</summary>
Motivation: To understand the training dynamics and mechanisms of adversarial training (AT) by analyzing class-wise feature attribution, particularly focusing on cross-class features.

Method: The study uses theoretical analysis with a synthetic data model and systematic experiments across various model architectures to examine the role of cross-class features in AT.

Result: Models initially learn cross-class features for robustness but shift to class-specific features during robust overfitting. The findings unify properties like soft-label training advantages and robust overfitting.

Conclusion: The insights refine understanding of AT mechanisms, offering new perspectives for future research. Code is available for reproducibility.

Abstract: Adversarial training (AT) has been considered one of the most effective
methods for making deep neural networks robust against adversarial attacks,
while the training mechanisms and dynamics of AT remain open research problems.
In this paper, we present a novel perspective on studying AT through the lens
of class-wise feature attribution. Specifically, we identify the impact of a
key family of features on AT that are shared by multiple classes, which we call
cross-class features. These features are typically useful for robust
classification, which we offer theoretical evidence to illustrate through a
synthetic data model. Through systematic studies across multiple model
architectures and settings, we find that during the initial stage of AT, the
model tends to learn more cross-class features until the best robustness
checkpoint. As AT further squeezes the training robust loss and causes robust
overfitting, the model tends to make decisions based on more class-specific
features. Based on these discoveries, we further provide a unified view of two
existing properties of AT, including the advantage of soft-label training and
robust overfitting. Overall, these insights refine the current understanding of
AT mechanisms and provide new perspectives on studying them. Our code is
available at https://github.com/PKU-ML/Cross-Class-Features-AT.

</details>


### [147] [TIMING: Temporality-Aware Integrated Gradients for Time Series Explanation](https://arxiv.org/abs/2506.05035)
*Hyeongwon Jang,Changhun Kim,Eunho Yang*

Main category: cs.LG

TL;DR: The paper introduces TIMING, a temporally-aware enhancement of Integrated Gradients (IG) for time series XAI, addressing limitations of current methods in capturing directional impacts and temporal relationships.


<details>
  <summary>Details</summary>
Motivation: Current XAI methods for time series focus on point-wise attribution magnitudes but overlook directional impacts, leading to suboptimal identification of critical points. Existing evaluation metrics also fail to assess these impacts.

Method: The authors propose TIMING, an improved version of IG that incorporates temporal awareness. They also introduce new evaluation metrics (CPD and CPP) to assess attribution methods.

Result: TIMING outperforms existing baselines in identifying significant positive and negative points in time series data, as demonstrated on synthetic and real-world benchmarks.

Conclusion: TIMING enhances IG for time series XAI by addressing temporal relationships and directional impacts, validated by superior performance under the proposed metrics.

Abstract: Recent explainable artificial intelligence (XAI) methods for time series
primarily estimate point-wise attribution magnitudes, while overlooking the
directional impact on predictions, leading to suboptimal identification of
significant points. Our analysis shows that conventional Integrated Gradients
(IG) effectively capture critical points with both positive and negative
impacts on predictions. However, current evaluation metrics fail to assess this
capability, as they inadvertently cancel out opposing feature contributions. To
address this limitation, we propose novel evaluation metrics-Cumulative
Prediction Difference (CPD) and Cumulative Prediction Preservation (CPP)-to
systematically assess whether attribution methods accurately identify
significant positive and negative points in time series XAI. Under these
metrics, conventional IG outperforms recent counterparts. However, directly
applying IG to time series data may lead to suboptimal outcomes, as generated
paths ignore temporal relationships and introduce out-of-distribution samples.
To overcome these challenges, we introduce TIMING, which enhances IG by
incorporating temporal awareness while maintaining its theoretical properties.
Extensive experiments on synthetic and real-world time series benchmarks
demonstrate that TIMING outperforms existing time series XAI baselines. Our
code is available at https://github.com/drumpt/TIMING.

</details>


### [148] [iN2V: Bringing Transductive Node Embeddings to Inductive Graphs](https://arxiv.org/abs/2506.05039)
*Nicolas Lell,Ansgar Scherp*

Main category: cs.LG

TL;DR: iN2V extends node2vec to an inductive setting, enabling embeddings for unseen nodes and improving node classification.


<details>
  <summary>Details</summary>
Motivation: Overcome the limitation of transductive node embedding methods like node2vec, which cannot handle new nodes post-training.

Method: Combines a post-hoc procedure for unseen nodes with modified N2V training to prepare embeddings.

Result: Improves node classification by 1 point on average, with up to 6 points on certain datasets.

Conclusion: iN2V is a versatile, plug-in approach for inductive node representation learning, compatible with other embedding methods.

Abstract: Shallow node embeddings like node2vec (N2V) can be used for nodes without
features or to supplement existing features with structure-based information.
Embedding methods like N2V are limited in their application on new nodes, which
restricts them to the transductive setting where the entire graph, including
the test nodes, is available during training. We propose inductive node2vec
(iN2V), which combines a post-hoc procedure to compute embeddings for nodes
unseen during training and modifications to the original N2V training procedure
to prepare the embeddings for this post-hoc procedure. We conduct experiments
on several benchmark datasets and demonstrate that iN2V is an effective
approach to bringing transductive embeddings to an inductive setting. Using
iN2V embeddings improves node classification by 1 point on average, with up to
6 points of improvement depending on the dataset and the number of unseen
nodes. Our iN2V is a plug-in approach to create new or enrich existing
embeddings. It can also be combined with other embedding methods, making it a
versatile approach for inductive node representation learning. Code to
reproduce the results is available at https://github.com/Foisunt/iN2V .

</details>


### [149] [Reliably detecting model failures in deployment without labels](https://arxiv.org/abs/2506.05047)
*Viet Nguyen Changjian Shui,Vijay Giri,Siddarth Arya,Amol Verma,Fahad Razak,Rahul G. Krishnan*

Main category: cs.LG

TL;DR: The paper introduces D3M, a monitoring algorithm to detect when models need retraining due to data shifts, focusing on post-deployment deterioration (PDD).


<details>
  <summary>Details</summary>
Motivation: Models in dynamic environments face performance degradation due to data shifts, but retraining without label access is challenging.

Method: D3M uses predictive model disagreement to monitor shifts, ensuring low false positives and high true positives.

Result: Empirical tests on benchmarks and real-world data show D3M effectively detects deteriorating shifts.

Conclusion: D3M is a viable alert mechanism for high-stakes ML pipelines, addressing PDD efficiently.

Abstract: The distribution of data changes over time; models operating operating in
dynamic environments need retraining. But knowing when to retrain, without
access to labels, is an open challenge since some, but not all shifts degrade
model performance. This paper formalizes and addresses the problem of
post-deployment deterioration (PDD) monitoring. We propose D3M, a practical and
efficient monitoring algorithm based on the disagreement of predictive models,
achieving low false positive rates under non-deteriorating shifts and provides
sample complexity bounds for high true positive rates under deteriorating
shifts. Empirical results on both standard benchmark and a real-world
large-scale internal medicine dataset demonstrate the effectiveness of the
framework and highlight its viability as an alert mechanism for high-stakes
machine learning pipelines.

</details>


### [150] [NIMO: a Nonlinear Interpretable MOdel](https://arxiv.org/abs/2506.05059)
*Shijian Xu,Marcello Massimo Negri,Volker Roth*

Main category: cs.LG

TL;DR: NIMO combines neural networks and linear models for interpretability and accuracy, using profile likelihood optimization to maintain interpretable linear coefficients while improving predictions.


<details>
  <summary>Details</summary>
Motivation: Neural networks lack interpretability, while linear models are interpretable but less expressive. The goal is to merge both strengths.

Method: Introduces NIMO, where a neural network learns nonlinear corrections to a linear model, optimized via profile likelihood and adaptive ridge regression for sparsity.

Result: Empirically recovers linear coefficients and improves predictive accuracy, outperforming other hybrid interpretable methods.

Conclusion: NIMO successfully balances interpretability and performance, maintaining linear model interpretability while enhancing accuracy.

Abstract: Neural networks (NNs) have achieved tremendous success over the past decade,
yet they are still extremely difficult to interpret. In contrast, linear models
are less expressive but offer inherent interpretability. Linear coefficients
are interpretable as the marginal effect of a feature on the prediction,
assuming all other features are kept fixed. To combine the benefits of both
approaches, we introduce NIMO (Nonlinear Interpretable MOdel). The key idea is
to define a model where the NN is designed to learn nonlinear corrections to
the linear model predictions, while also maintaining the original
interpretability of the linear coefficients. Relevantly, we develop an
optimization algorithm based on profile likelihood that elegantly allows for
optimizing over the NN parameters while updating the linear coefficients
analytically. By relying on adaptive ridge regression we can easily incorporate
sparsity constraints as well. We show empirically that we can recover the
underlying linear coefficients while significantly improving the predictive
accuracy. Compared to other hybrid interpretable approaches, our model is the
only one that actually maintains the same interpretability of linear
coefficients as in linear models. We also achieve higher performance on various
regression and classification settings.

</details>


### [151] [UnHiPPO: Uncertainty-aware Initialization for State Space Models](https://arxiv.org/abs/2506.05065)
*Marten Lienen,Abdullah Saydemir,Stephan Günnemann*

Main category: cs.LG

TL;DR: The paper extends HiPPO theory to handle measurement noise, providing an uncertainty-aware initialization for state space models, improving noise resistance.


<details>
  <summary>Details</summary>
Motivation: HiPPO assumes noise-free data, which is often unrealistic. The paper addresses this limitation by incorporating measurement noise into the framework.

Method: Reformulates HiPPO as a linear stochastic control problem, treating data as noisy outputs of a latent system, and derives a new dynamics initialization.

Result: The proposed initialization enhances state-space models' robustness to noise during training and inference.

Conclusion: The extension of HiPPO with noise-aware dynamics offers practical improvements for real-world sequence problems.

Abstract: State space models are emerging as a dominant model class for sequence
problems with many relying on the HiPPO framework to initialize their dynamics.
However, HiPPO fundamentally assumes data to be noise-free; an assumption often
violated in practice. We extend the HiPPO theory with measurement noise and
derive an uncertainty-aware initialization for state space model dynamics. In
our analysis, we interpret HiPPO as a linear stochastic control problem where
the data enters as a noise-free control signal. We then reformulate the problem
so that the data become noisy outputs of a latent system and arrive at an
alternative dynamics initialization that infers the posterior of this latent
system from the data without increasing runtime. Our experiments show that our
initialization improves the resistance of state-space models to noise both at
training and inference time. Find our implementation at
https://cs.cit.tum.de/daml/unhippo.

</details>


### [152] [Semi-Implicit Variational Inference via Kernelized Path Gradient Descent](https://arxiv.org/abs/2506.05088)
*Tobias Pielok,Bernd Bischl,David Rügamer*

Main category: cs.LG

TL;DR: Proposes a kernelized KL divergence estimator with importance sampling to improve stability and reduce bias in semi-implicit variational inference (SIVI), outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Training SIVI with KL divergence is challenging due to high variance and bias in high dimensions, and current methods like KSIVI are expensive.

Method: Introduces a kernelized KL divergence estimator with nonparametric smoothing and importance sampling correction, linking it theoretically to Stein variational gradient descent.

Result: The method achieves lower gradient variance, benign bias, and outperforms or matches state-of-the-art SIVI methods in performance and efficiency.

Conclusion: The proposed approach offers more stable and efficient optimization for SIVI, addressing key challenges in high-dimensional settings.

Abstract: Semi-implicit variational inference (SIVI) is a powerful framework for
approximating complex posterior distributions, but training with the
Kullback-Leibler (KL) divergence can be challenging due to high variance and
bias in high-dimensional settings. While current state-of-the-art semi-implicit
variational inference methods, particularly Kernel Semi-Implicit Variational
Inference (KSIVI), have been shown to work in high dimensions, training remains
moderately expensive. In this work, we propose a kernelized KL divergence
estimator that stabilizes training through nonparametric smoothing. To further
reduce the bias, we introduce an importance sampling correction. We provide a
theoretical connection to the amortized version of the Stein variational
gradient descent, which estimates the score gradient via Stein's identity,
showing that both methods minimize the same objective, but our semi-implicit
approach achieves lower gradient variance. In addition, our method's bias in
function space is benign, leading to more stable and efficient optimization.
Empirical results demonstrate that our method outperforms or matches
state-of-the-art SIVI methods in both performance and training efficiency.

</details>


### [153] [Privacy Amplification Through Synthetic Data: Insights from Linear Regression](https://arxiv.org/abs/2506.05101)
*Clément Pierquin,Aurélien Bellet,Marc Tommasi,Matthieu Boussard*

Main category: cs.LG

TL;DR: The paper explores privacy amplification in synthetic data generation, showing risks when the model's seed is controlled and benefits when random inputs are used.


<details>
  <summary>Details</summary>
Motivation: To rigorously understand privacy amplification in synthetic data, which lacks theoretical grounding despite empirical evidence.

Method: Investigates the question using the framework of linear regression, analyzing scenarios where the generative model's seed is controlled versus random inputs.

Result: Negative results show leakage risks if the seed is controlled, while random inputs amplify privacy beyond the model's guarantees.

Conclusion: The findings in linear regression provide a foundation for future general privacy bounds.

Abstract: Synthetic data inherits the differential privacy guarantees of the model used
to generate it. Additionally, synthetic data may benefit from privacy
amplification when the generative model is kept hidden. While empirical studies
suggest this phenomenon, a rigorous theoretical understanding is still lacking.
In this paper, we investigate this question through the well-understood
framework of linear regression. First, we establish negative results showing
that if an adversary controls the seed of the generative model, a single
synthetic data point can leak as much information as releasing the model
itself. Conversely, we show that when synthetic data is generated from random
inputs, releasing a limited number of synthetic data points amplifies privacy
beyond the model's inherent guarantees. We believe our findings in linear
regression can serve as a foundation for deriving more general bounds in the
future.

</details>


### [154] [Federated Isolation Forest for Efficient Anomaly Detection on Edge IoT Systems](https://arxiv.org/abs/2506.05138)
*Pavle Vasiljevic,Milica Matic,Miroslav Popovic*

Main category: cs.LG

TL;DR: The paper presents an Isolation Forest-based temperature anomaly detection system for edge devices using federated learning frameworks, achieving high accuracy and low memory usage.


<details>
  <summary>Details</summary>
Motivation: To address privacy concerns and efficiency in embedded systems, leveraging federated learning for anomaly detection in resource-constrained environments.

Method: Developed FLiForest, an efficient federated anomaly detection algorithm based on Isolation Forests, and applied it to temperature anomaly detection using Python and MicroPython frameworks.

Result: Achieved over 96% accuracy in distinguishing normal from abnormal readings and above 78% precision in anomaly detection, with memory usage below 160 KB.

Conclusion: The system is suitable for resource-constrained edge and IoT systems, maintaining federated learning principles of privacy and collaborative learning.

Abstract: Recently, federated learning frameworks such as Python TestBed for Federated
Learning Algorithms and MicroPython TestBed for Federated Learning Algorithms
have emerged to tackle user privacy concerns and efficiency in embedded
systems. Even more recently, an efficient federated anomaly detection
algorithm, FLiForest, based on Isolation Forests has been developed, offering a
low-resource, unsupervised method well-suited for edge deployment and
continuous learning. In this paper, we present an application of Isolation
Forest-based temperature anomaly detection, developed using the previously
mentioned federated learning frameworks, aimed at small edge devices and IoT
systems running MicroPython. The system has been experimentally evaluated,
achieving over 96% accuracy in distinguishing normal from abnormal readings and
above 78% precision in detecting anomalies across all tested configurations,
while maintaining a memory usage below 160 KB during model training. These
results highlight its suitability for resource-constrained environments and
edge systems, while upholding federated learning principles of data privacy and
collaborative learning.

</details>


### [155] [Associative Memory and Generative Diffusion in the Zero-noise Limit](https://arxiv.org/abs/2506.05178)
*Joshua Hess,Quaid Morris*

Main category: cs.LG

TL;DR: The paper explores links between generative diffusion models and continuous-state associative memory, highlighting Morse-Smale systems as universal approximators and their stability properties.


<details>
  <summary>Details</summary>
Motivation: To understand the universal properties of associative memory and diffusion models, and their transitions under noise reduction.

Method: Analyzes Morse-Smale dynamical systems as approximations for gradient-based associative memory and diffusion models, focusing on structural stability and bifurcations.

Result: Shows stability in associative memory and diffusion models at vanishing noise levels, with transitions characterized by generic bifurcations.

Conclusion: The study provides a framework for understanding stability and transitions in associative memory and diffusion models, with implications for Hopfield networks and attention mechanisms.

Abstract: Connections between generative diffusion and continuous-state associative
memory models are studied. Morse-Smale dynamical systems are emphasized as
universal approximators of gradient-based associative memory models and
diffusion models as white-noise perturbed systems thereof. Universal properties
of associative memory that follow from this description are described and used
to characterize a generic transition from generation to memory as noise levels
diminish. Structural stability inherited by Morse-Smale flows is shown to imply
a notion of stability for diffusions at vanishing noise levels. Applied to one-
and two-parameter families of gradients, this indicates stability at all but
isolated points of associative memory learning landscapes and the learning and
generation landscapes of diffusion models with gradient drift in the zero-noise
limit, at which small sets of generic bifurcations characterize qualitative
transitions between stable systems. Examples illustrating the characterization
of these landscapes by sequences of these bifurcations are given, along with
structural stability criterion for classic and modern Hopfield networks
(equivalently, the attention mechanism).

</details>


### [156] [TreeRPO: Tree Relative Policy Optimization](https://arxiv.org/abs/2506.05183)
*Zhicheng Yang,Zhijiang Guo,Yinya Huang,Xiaodan Liang,Yiwei Wang,Jing Tang*

Main category: cs.LG

TL;DR: The paper introduces \textbf{\name}, a method using tree sampling to estimate step-level rewards for LLMs, improving reasoning performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing RLVR methods lack fine-grained guidance for intermediate reasoning steps, limiting LLM performance.

Method: \textbf{\name} employs tree sampling to estimate step-level rewards and integrates them with GRPO's group-relative reward mechanism.

Result: \textbf{\name} boosts Qwen-2.5-Math's Pass@1 accuracy from 19.0% to 35.5% and outperforms GRPO by 2.9% while reducing response length by 18.1%.

Conclusion: The method provides dense, fine-grained rewards, enhancing LLM reasoning efficiency and performance.

Abstract: Large Language Models (LLMs) have shown remarkable reasoning capabilities
through Reinforcement Learning with Verifiable Rewards (RLVR) methods. However,
a key limitation of existing approaches is that rewards defined at the full
trajectory level provide insufficient guidance for optimizing the intermediate
steps of a reasoning process. To address this, we introduce \textbf{\name}, a
novel method that estimates the mathematical expectations of rewards at various
reasoning steps using tree sampling. Unlike prior methods that rely on a
separate step reward model, \name directly estimates these rewards through this
sampling process. Building on the group-relative reward training mechanism of
GRPO, \name innovatively computes rewards based on step-level groups generated
during tree sampling. This advancement allows \name to produce fine-grained and
dense reward signals, significantly enhancing the learning process and overall
performance of LLMs. Experimental results demonstrate that our \name algorithm
substantially improves the average Pass@1 accuracy of Qwen-2.5-Math on test
benchmarks, increasing it from 19.0\% to 35.5\%. Furthermore, \name
significantly outperforms GRPO by 2.9\% in performance while simultaneously
reducing the average response length by 18.1\%, showcasing its effectiveness
and efficiency. Our code will be available at
\href{https://github.com/yangzhch6/TreeRPO}{https://github.com/yangzhch6/TreeRPO}.

</details>


### [157] [Locality Preserving Markovian Transition for Instance Retrieval](https://arxiv.org/abs/2506.05196)
*Jifei Luo,Wenzheng Wu,Hantao Yao,Lu Yu,Changsheng Xu*

Main category: cs.LG

TL;DR: The paper introduces LPMT, a framework combining diffusion-based re-ranking with locality preservation to improve instance retrieval by addressing signal diminishment in affinity graphs.


<details>
  <summary>Details</summary>
Motivation: Positive signals in diffusion-based re-ranking diminish over distance, reducing discriminative power beyond local regions.

Method: LPMT uses Bidirectional Collaborative Diffusion (BCD) to integrate diffusion processes and Locality State Embedding (LSE) for local consistency, followed by Thermodynamic Markovian Transition (TMT) for global retrieval.

Result: LPMT effectively improves instance retrieval across diverse tasks.

Conclusion: LPMT successfully balances local and global retrieval performance by preserving locality and leveraging thermodynamic transitions.

Abstract: Diffusion-based re-ranking methods are effective in modeling the data
manifolds through similarity propagation in affinity graphs. However, positive
signals tend to diminish over several steps away from the source, reducing
discriminative power beyond local regions. To address this issue, we introduce
the Locality Preserving Markovian Transition (LPMT) framework, which employs a
long-term thermodynamic transition process with multiple states for accurate
manifold distance measurement. The proposed LPMT first integrates diffusion
processes across separate graphs using Bidirectional Collaborative Diffusion
(BCD) to establish strong similarity relationships. Afterwards, Locality State
Embedding (LSE) encodes each instance into a distribution for enhanced local
consistency. These distributions are interconnected via the Thermodynamic
Markovian Transition (TMT) process, enabling efficient global retrieval while
maintaining local effectiveness. Experimental results across diverse tasks
confirm the effectiveness of LPMT for instance retrieval.

</details>


### [158] [Transformers Meet In-Context Learning: A Universal Approximation Theory](https://arxiv.org/abs/2506.05200)
*Gen Li,Yuchen Jiao,Yu Huang,Yuting Wei,Yuxin Chen*

Main category: cs.LG

TL;DR: The paper develops a universal approximation theory for transformers, showing they can perform in-context learning without fine-tuning, extending beyond traditional optimization-based approaches.


<details>
  <summary>Details</summary>
Motivation: To understand how transformers enable in-context learning and provide theoretical guarantees for their function approximation capabilities.

Method: Constructs transformers capable of universal function approximation for any task class, using in-context examples without weight updates.

Result: Demonstrates transformers can reliably predict tasks dynamically, beyond convex and linear problems.

Conclusion: The work highlights transformers' ability to learn general-purpose representations and adapt to in-context examples, offering broader theoretical guarantees.

Abstract: Modern large language models are capable of in-context learning, the ability
to perform new tasks at inference time using only a handful of input-output
examples in the prompt, without any fine-tuning or parameter updates. We
develop a universal approximation theory to better understand how transformers
enable in-context learning. For any class of functions (each representing a
distinct task), we demonstrate how to construct a transformer that, without any
further weight updates, can perform reliable prediction given only a few
in-context examples. In contrast to much of the recent literature that frames
transformers as algorithm approximators -- i.e., constructing transformers to
emulate the iterations of optimization algorithms as a means to approximate
solutions of learning problems -- our work adopts a fundamentally different
approach rooted in universal function approximation. This alternative approach
offers approximation guarantees that are not constrained by the effectiveness
of the optimization algorithms being approximated, thereby extending far beyond
convex problems and linear function classes. Our construction sheds light on
how transformers can simultaneously learn general-purpose representations and
adapt dynamically to in-context examples.

</details>


### [159] [Mitigating Degree Bias Adaptively with Hard-to-Learn Nodes in Graph Contrastive Learning](https://arxiv.org/abs/2506.05214)
*Jingyu Hu,Hongbo Bo,Jun Hong,Xiaowei Liu,Weiru Liu*

Main category: cs.LG

TL;DR: The paper introduces HAR contrastive loss and SHARP framework to address degree bias in GNNs by adding more positive pairs and adaptive weighting, showing improved performance.


<details>
  <summary>Details</summary>
Motivation: GNNs suffer from degree bias in node classification, where low-degree nodes get insufficient and noisy information due to limited positive pairs in GCL.

Method: Proposes HAR contrastive loss to add more positive pairs using node labels and adaptively weight pairs based on learning hardness, alongside the SHARP framework for broader application.

Result: SHARP outperforms baselines on four datasets, improving performance globally and across degree levels.

Conclusion: HAR and SHARP effectively mitigate degree bias in GNNs, enhancing node classification performance.

Abstract: Graph Neural Networks (GNNs) often suffer from degree bias in node
classification tasks, where prediction performance varies across nodes with
different degrees. Several approaches, which adopt Graph Contrastive Learning
(GCL), have been proposed to mitigate this bias. However, the limited number of
positive pairs and the equal weighting of all positives and negatives in GCL
still lead to low-degree nodes acquiring insufficient and noisy information.
This paper proposes the Hardness Adaptive Reweighted (HAR) contrastive loss to
mitigate degree bias. It adds more positive pairs by leveraging node labels and
adaptively weights positive and negative pairs based on their learning
hardness. In addition, we develop an experimental framework named SHARP to
extend HAR to a broader range of scenarios. Both our theoretical analysis and
experiments validate the effectiveness of SHARP. The experimental results
across four datasets show that SHARP achieves better performance against
baselines at both global and degree levels.

</details>


### [160] [Learning Theory of Decentralized Robust Kernel-Based Learning Algorithm](https://arxiv.org/abs/2506.05215)
*Zhan Yu*

Main category: cs.LG

TL;DR: A decentralized robust kernel-based learning algorithm in RKHS is proposed, unifying robust regression with rigorous convergence analysis and optimal learning rates.


<details>
  <summary>Details</summary>
Motivation: To address limitations of existing distributed robust kernel learning schemes by providing a decentralized framework with enhanced robustness and convergence guarantees.

Method: Utilizes a connected graph network and a robust loss function with a windowing function and scaling parameter. Analyzes convergence and derives bounds for local estimators.

Result: Local estimators approximate the regression function with high confidence bounds. Optimal learning rates are achieved under proper parameter selection.

Conclusion: The algorithm effectively unifies decentralized robust learning, with robustness and convergence linked to parameter choices.

Abstract: We propose a new decentralized robust kernel-based learning algorithm within
the framework of reproducing kernel Hilbert space (RKHS) by utilizing a
networked system that can be represented as a connected graph. The robust loss
function $\mathcal{L}_\sigma$ induced by a windowing function $W$ and a
robustness scaling parameter $\sigma>0$, can encompass a broad spectrum of
robust losses. Consequently, the proposed algorithm effectively provides a
unified decentralized learning framework for robust regression, which
fundamentally differs from the existing distributed robust kernel learning
schemes, all of which are divide-and-conquer based. We rigorously establish the
learning theory and offer a comprehensive convergence analysis for the
algorithm. We show each local robust estimator generated from the decentralized
algorithm can be utilized to approximate the regression function. Based on
kernel-based integral operator techniques, we derive general high confidence
convergence bounds for each local approximating sequence in terms of the mean
square distance, RKHS norm, and generalization error, respectively. Moreover,
we provide rigorous selection rules for local sample size and show that, under
properly selected step size and scaling parameter $\sigma$, the decentralized
robust algorithm can achieve optimal learning rates (up to logarithmic factors)
in both norms. The parameter $\sigma$ is shown to be essential for enhancing
robustness while also ensuring favorable convergence behavior. The intrinsic
connection among decentralization, sample selection, robustness of the
algorithm, and its convergence is clearly reflected.

</details>


### [161] [A Unified Framework for Provably Efficient Algorithms to Estimate Shapley Values](https://arxiv.org/abs/2506.05216)
*Tyler Chen,Akshay Seshadri,Mattia J. Villani,Pradeep Niroula,Shouvanik Chakrabarti,Archan Ray,Pranav Deshpande,Romina Yalovetzky,Marco Pistoia,Niraj Kumar*

Main category: cs.LG

TL;DR: The paper introduces a unified framework for Shapley value estimators, including KernelSHAP, and provides the first theoretical guarantees for KernelSHAP, validated by experiments on various datasets.


<details>
  <summary>Details</summary>
Motivation: Exact Shapley value computation is computationally expensive, and existing randomized estimators lack theoretical guarantees for KernelSHAP.

Method: A broad framework encompassing KernelSHAP and related estimators, with non-asymptotic theoretical guarantees and implementation improvements for scalability.

Result: The framework achieves low mean squared error with modest sample sizes and outperforms KernelSHAP on high-dimensional datasets like MNIST and CIFAR10.

Conclusion: The work provides theoretical foundations for KernelSHAP and demonstrates practical improvements, offering better performance and scalability.

Abstract: Shapley values have emerged as a critical tool for explaining which features
impact the decisions made by machine learning models. However, computing exact
Shapley values is difficult, generally requiring an exponential (in the feature
dimension) number of model evaluations. To address this, many model-agnostic
randomized estimators have been developed, the most influential and widely used
being the KernelSHAP method (Lundberg & Lee, 2017). While related estimators
such as unbiased KernelSHAP (Covert & Lee, 2021) and LeverageSHAP (Musco &
Witter, 2025) are known to satisfy theoretical guarantees, bounds for
KernelSHAP have remained elusive. We describe a broad and unified framework
that encompasses KernelSHAP and related estimators constructed using both with
and without replacement sampling strategies. We then prove strong
non-asymptotic theoretical guarantees that apply to all estimators from our
framework. This provides, to the best of our knowledge, the first theoretical
guarantees for KernelSHAP and sheds further light on tradeoffs between existing
estimators. Through comprehensive benchmarking on small and medium dimensional
datasets for Decision-Tree models, we validate our approach against exact
Shapley values, consistently achieving low mean squared error with modest
sample sizes. Furthermore, we make specific implementation improvements to
enable scalability of our methods to high-dimensional datasets. Our methods,
tested on datasets such MNIST and CIFAR10, provide consistently better results
compared to the KernelSHAP library.

</details>


### [162] [Diagonal Batching Unlocks Parallelism in Recurrent Memory Transformers for Long Contexts](https://arxiv.org/abs/2506.05229)
*Danil Sivtsov,Ivan Rodkin,Gleb Kuzmin,Yuri Kuratov,Ivan Oseledets*

Main category: cs.LG

TL;DR: Diagonal Batching improves RMTs by enabling parallelism across segments, reducing inference time and cost without retraining.


<details>
  <summary>Details</summary>
Motivation: Transformer models face inefficiency in long-context inference due to quadratic time and linear memory complexity. RMTs address this but suffer from sequential bottlenecks.

Method: Diagonal Batching is introduced to parallelize segment processing in RMTs while maintaining exact recurrence, eliminating sequential constraints.

Result: Diagonal Batching achieves a 3.3x speedup over standard LLaMA-1B and 1.8x over sequential RMT on 131,072-token sequences.

Conclusion: Diagonal Batching enhances RMTs' practicality for real-world long-context applications by reducing latency and inference cost.

Abstract: Transformer models struggle with long-context inference due to their
quadratic time and linear memory complexity. Recurrent Memory Transformers
(RMTs) offer a solution by reducing the asymptotic cost to linear time and
constant memory usage. However, their memory update mechanism leads to
sequential execution, causing a performance bottleneck.
  We introduce Diagonal Batching, a scheduling scheme that unlocks parallelism
across segments in RMTs while preserving exact recurrence. This approach
eliminates the sequential constraint, enabling efficient GPU inference even for
single long-context inputs without complex batching and pipelining techniques.
Because the technique is purely a run-time computation reordering, existing RMT
models adopt it with no retraining.
  Applied to a LLaMA-1B ARMT model, Diagonal Batching yields a 3.3x speedup
over standard full-attention LLaMA-1B and a 1.8x speedup over the sequential
RMT implementation on 131,072-token sequences. By removing sequential
bottleneck, Diagonal Batching reduces inference cost and latency, thereby
strengthening RMTs as a practical solution for real-world, long-context
applications.

</details>


### [163] [Progressive Tempering Sampler with Diffusion](https://arxiv.org/abs/2506.05231)
*Severi Rissanen,RuiKang OuYang,Jiajun He,Wenlin Chen,Markus Heinonen,Arno Solin,José Miguel Hernández-Lobato*

Main category: cs.LG

TL;DR: The paper introduces PTSD, a method combining diffusion models and MCMC to improve neural samplers, outperforming existing approaches in efficiency and sample quality.


<details>
  <summary>Details</summary>
Motivation: Existing neural samplers lag behind MCMC methods like Parallel Tempering in efficiency, while MCMC methods are computationally costly for new samples. PTSD aims to bridge this gap.

Method: PTSD trains diffusion models sequentially across temperatures, using MCMC to refine samples and train subsequent models, leveraging PT's advantages.

Result: PTSD generates well-mixed, uncorrelated samples efficiently, surpassing diffusion-based neural samplers in target evaluation efficiency.

Conclusion: PTSD effectively combines the strengths of MCMC and neural samplers, offering a superior solution for sampling from unnormalized densities.

Abstract: Recent research has focused on designing neural samplers that amortize the
process of sampling from unnormalized densities. However, despite significant
advancements, they still fall short of the state-of-the-art MCMC approach,
Parallel Tempering (PT), when it comes to the efficiency of target evaluations.
On the other hand, unlike a well-trained neural sampler, PT yields only
dependent samples and needs to be rerun -- at considerable computational cost
-- whenever new samples are required. To address these weaknesses, we propose
the Progressive Tempering Sampler with Diffusion (PTSD), which trains diffusion
models sequentially across temperatures, leveraging the advantages of PT to
improve the training of neural samplers. We also introduce a novel method to
combine high-temperature diffusion models to generate approximate
lower-temperature samples, which are minimally refined using MCMC and used to
train the next diffusion model. PTSD enables efficient reuse of sample
information across temperature levels while generating well-mixed, uncorrelated
samples. Our method significantly improves target evaluation efficiency,
outperforming diffusion-based neural samplers.

</details>


### [164] [MesaNet: Sequence Modeling by Locally Optimal Test-Time Training](https://arxiv.org/abs/2506.05233)
*Johannes von Oswald,Nino Scherrer,Seijin Kobayashi,Luca Versari,Songlin Yang,Maximilian Schlegel,Kaitlin Maile,Yanick Schimpf,Oliver Sieberling,Alexander Meulemans,Rif A. Saurous,Guillaume Lajoie,Charlotte Frenkel,Razvan Pascanu,Blaise Agüera y Arcas,João Sacramento*

Main category: cs.LG

TL;DR: The paper introduces a stable, parallelizable version of the Mesa layer, derived from an in-context regression objective, and demonstrates its superior performance in language modeling, especially for long-context tasks, at the cost of additional inference compute.


<details>
  <summary>Details</summary>
Motivation: To address the linear scaling of memory and compute in transformers during inference, the paper explores recurrent neural network (RNN) models like DeltaNet, Mamba, and xLSTM, unified by an in-context regression objective.

Method: The authors propose a numerically stable, chunkwise parallelizable Mesa layer, optimized at every time point using a fast conjugate gradient solver, and evaluate it in billion-parameter language models.

Result: The Mesa layer achieves lower perplexity and higher downstream performance than previous RNNs, particularly in long-context tasks, though it requires additional inference-time compute.

Conclusion: The work highlights the trade-off between performance and compute, aligning with trends of increasing test-time compute for better results, by solving sequential optimization within the network.

Abstract: Sequence modeling is currently dominated by causal transformer architectures
that use softmax self-attention. Although widely adopted, transformers require
scaling memory and compute linearly during inference. A recent stream of work
linearized the softmax operation, resulting in powerful recurrent neural
network (RNN) models with constant memory and compute costs such as DeltaNet,
Mamba or xLSTM. These models can be unified by noting that their recurrent
layer dynamics can all be derived from an in-context regression objective,
approximately optimized through an online learning rule. Here, we join this
line of work and introduce a numerically stable, chunkwise parallelizable
version of the recently proposed Mesa layer (von Oswald et al., 2024), and
study it in language modeling at the billion-parameter scale. This layer again
stems from an in-context loss, but which is now minimized to optimality at
every time point using a fast conjugate gradient solver. Through an extensive
suite of experiments, we show that optimal test-time training enables reaching
lower language modeling perplexity and higher downstream benchmark performance
than previous RNNs, especially on tasks requiring long context understanding.
This performance gain comes at the cost of additional flops spent during
inference time. Our results are therefore intriguingly related to recent trends
of increasing test-time compute to improve performance -- here by spending
compute to solve sequential optimization problems within the neural network
itself.

</details>


### [165] [Evaluating Sparse Autoencoders: From Shallow Design to Matching Pursuit](https://arxiv.org/abs/2506.05239)
*Valérie Costa,Thomas Fel,Ekdeep Singh Lubana,Bahareh Tolooshams,Demba Ba*

Main category: cs.LG

TL;DR: The paper evaluates sparse autoencoders (SAEs) for interpretability, identifies limitations in shallow architectures, and proposes a multi-iteration SAE (MP-SAE) to handle correlated features.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current SAEs in extracting correlated features due to quasi-orthogonality assumptions.

Method: Introduces a multi-iteration SAE (MP-SAE) by unrolling Matching Pursuit, enabling residual-guided extraction of correlated features.

Result: MP-SAE improves feature extraction for hierarchical settings like handwritten digit generation and guarantees monotonic reconstruction improvement.

Conclusion: The proposed MP-SAE advances SAEs by effectively handling correlated features and improving interpretability.

Abstract: Sparse autoencoders (SAEs) have recently become central tools for
interpretability, leveraging dictionary learning principles to extract sparse,
interpretable features from neural representations whose underlying structure
is typically unknown. This paper evaluates SAEs in a controlled setting using
MNIST, which reveals that current shallow architectures implicitly rely on a
quasi-orthogonality assumption that limits the ability to extract correlated
features. To move beyond this, we introduce a multi-iteration SAE by unrolling
Matching Pursuit (MP-SAE), enabling the residual-guided extraction of
correlated features that arise in hierarchical settings such as handwritten
digit generation while guaranteeing monotonic improvement of the reconstruction
as more atoms are selected.

</details>


### [166] [Aligning Latent Spaces with Flow Priors](https://arxiv.org/abs/2506.05240)
*Yizhuo Li,Yuying Ge,Yixiao Ge,Ying Shan,Ping Luo*

Main category: cs.LG

TL;DR: A novel framework aligns learnable latent spaces to target distributions using flow-based generative models, avoiding expensive likelihood evaluations and ODE solving.


<details>
  <summary>Details</summary>
Motivation: To align latent spaces with arbitrary target distributions efficiently, leveraging flow models as priors.

Method: Pretrains a flow model on target features, then regularizes the latent space via an alignment loss derived from flow matching.

Result: The alignment loss approximates the negative log-likelihood of the target distribution, validated in controlled and large-scale experiments.

Conclusion: The framework provides a computationally efficient and theoretically grounded method for latent space alignment.

Abstract: This paper presents a novel framework for aligning learnable latent spaces to
arbitrary target distributions by leveraging flow-based generative models as
priors. Our method first pretrains a flow model on the target features to
capture the underlying distribution. This fixed flow model subsequently
regularizes the latent space via an alignment loss, which reformulates the flow
matching objective to treat the latents as optimization targets. We formally
prove that minimizing this alignment loss establishes a computationally
tractable surrogate objective for maximizing a variational lower bound on the
log-likelihood of latents under the target distribution. Notably, the proposed
method eliminates computationally expensive likelihood evaluations and avoids
ODE solving during optimization. As a proof of concept, we demonstrate in a
controlled setting that the alignment loss landscape closely approximates the
negative log-likelihood of the target distribution. We further validate the
effectiveness of our approach through large-scale image generation experiments
on ImageNet with diverse target distributions, accompanied by detailed
discussions and ablation studies. With both theoretical and empirical
validation, our framework paves a new way for latent space alignment.

</details>


### [167] [On the Convergence of Gradient Descent on Learning Transformers with Residual Connections](https://arxiv.org/abs/2506.05249)
*Zhen Qin,Jinxin Zhou,Zhihui Zhu*

Main category: cs.LG

TL;DR: The paper analyzes the convergence behavior of a single-layer Transformer with self-attention, feedforward networks, and residual connections, showing linear convergence under proper initialization and highlighting the role of residual connections in optimization stability.


<details>
  <summary>Details</summary>
Motivation: Despite the empirical success of Transformers, their theoretical foundations, especially training dynamics, are underdeveloped. This paper aims to understand the interdependencies between components like self-attention and residual connections.

Method: The study examines a structurally complete single-layer Transformer, analyzing gradient descent convergence under appropriate initialization. It also extends findings to multi-layer architectures.

Result: Gradient descent exhibits linear convergence, influenced by the attention layer's output matrix singular values. Residual connections mitigate ill-conditioning from softmax, enhancing stability.

Conclusion: Residual connections improve optimization stability in Transformers, with empirical results supporting the theoretical insights on convergence behavior.

Abstract: Transformer models have emerged as fundamental tools across various
scientific and engineering disciplines, owing to their outstanding performance
in diverse applications. Despite this empirical success, the theoretical
foundations of Transformers remain relatively underdeveloped, particularly in
understanding their training dynamics. Existing research predominantly examines
isolated components--such as self-attention mechanisms and feedforward
networks--without thoroughly investigating the interdependencies between these
components, especially when residual connections are present. In this paper, we
aim to bridge this gap by analyzing the convergence behavior of a structurally
complete yet single-layer Transformer, comprising self-attention, a feedforward
network, and residual connections. We demonstrate that, under appropriate
initialization, gradient descent exhibits a linear convergence rate, where the
convergence speed is determined by the minimum and maximum singular values of
the output matrix from the attention layer. Moreover, our analysis reveals that
residual connections serve to ameliorate the ill-conditioning of this output
matrix, an issue stemming from the low-rank structure imposed by the softmax
operation, thereby promoting enhanced optimization stability. We also extend
our theoretical findings to a multi-layer Transformer architecture, confirming
the linear convergence rate of gradient descent under suitable initialization.
Empirical results corroborate our theoretical insights, illustrating the
beneficial role of residual connections in promoting convergence stability.

</details>


### [168] [Conservative classifiers do consistently well with improving agents: characterizing statistical and online learning](https://arxiv.org/abs/2506.05252)
*Dravyansh Sharma,Alec Sun*

Main category: cs.LG

TL;DR: The paper explores learnability in strategic classification, focusing on genuine agent improvements, and extends prior work by characterizing learnability across new axes, including proper and improper learning under natural assumptions.


<details>
  <summary>Details</summary>
Motivation: To understand how machine learning algorithms perform when classified agents genuinely improve, rather than deceive, and to extend learnability results beyond arbitrary improvement regions.

Method: Introduces asymmetric minimally consistent concept classes for exact characterization in the realizable setting, studies Euclidean ball improvement sets, and addresses learning under bounded noise and online settings.

Result: Provides exact characterizations for proper learning, positive results for natural improvement regions, and resolves open questions for both proper and improper learning.

Conclusion: The work advances understanding of strategic classification with genuine improvements, offering theoretical insights and practical learning guarantees.

Abstract: Machine learning is now ubiquitous in societal decision-making, for example
in evaluating job candidates or loan applications, and it is increasingly
important to take into account how classified agents will react to the learning
algorithms. The majority of recent literature on strategic classification has
focused on reducing and countering deceptive behaviors by the classified
agents, but recent work of Attias et al. identifies surprising properties of
learnability when the agents genuinely improve in order to attain the desirable
classification, such as smaller generalization error than standard
PAC-learning. In this paper we characterize so-called learnability with
improvements across multiple new axes. We introduce an asymmetric variant of
minimally consistent concept classes and use it to provide an exact
characterization of proper learning with improvements in the realizable
setting. While prior work studies learnability only under general, arbitrary
agent improvement regions, we give positive results for more natural Euclidean
ball improvement sets. In particular, we characterize improper learning under a
mild generative assumption on the data distribution. We further show how to
learn in more challenging settings, achieving lower generalization error under
well-studied bounded noise models and obtaining mistake bounds in realizable
and agnostic online learning. We resolve open questions posed by Attias et al.
for both proper and improper learning.

</details>


### [169] [Learning long range dependencies through time reversal symmetry breaking](https://arxiv.org/abs/2506.05259)
*Guillaume Pourcel,Maxence Ernoult*

Main category: cs.LG

TL;DR: RHEL is a learning algorithm for Hamiltonian systems, matching BPTT performance with efficiency and scalability.


<details>
  <summary>Details</summary>
Motivation: To develop physics-grounded learning algorithms for dynamical systems, ensuring efficiency and adherence to physical principles.

Method: Proposes Recurrent Hamiltonian Echo Learning (RHEL), computing gradients via physical trajectories without explicit Jacobians, and introduces Hamiltonian Recurrent Units (HRUs).

Result: RHEL matches BPTT performance on time-series tasks with sequences up to ~50k, demonstrating scalability.

Conclusion: RHEL enables scalable, energy-efficient physical systems for sequence modeling, bridging physics and machine learning.

Abstract: Deep State Space Models (SSMs) reignite physics-grounded compute paradigms,
as RNNs could natively be embodied into dynamical systems. This calls for
dedicated learning algorithms obeying to core physical principles, with
efficient techniques to simulate these systems and guide their design. We
propose Recurrent Hamiltonian Echo Learning (RHEL), an algorithm which provably
computes loss gradients as finite differences of physical trajectories of
non-dissipative, Hamiltonian systems. In ML terms, RHEL only requires three
"forward passes" irrespective of model size, without explicit Jacobian
computation, nor incurring any variance in the gradient estimation. Motivated
by the physical realization of our algorithm, we first introduce RHEL in
continuous time and demonstrate its formal equivalence with the continuous
adjoint state method. To facilitate the simulation of Hamiltonian systems
trained by RHEL, we propose a discrete-time version of RHEL which is equivalent
to Backpropagation Through Time (BPTT) when applied to a class of recurrent
modules which we call Hamiltonian Recurrent Units (HRUs). This setting allows
us to demonstrate the scalability of RHEL by generalizing these results to
hierarchies of HRUs, which we call Hamiltonian SSMs (HSSMs). We apply RHEL to
train HSSMs with linear and nonlinear dynamics on a variety of time-series
tasks ranging from mid-range to long-range classification and regression with
sequence length reaching $\sim 50k$. We show that RHEL consistently matches the
performance of BPTT across all models and tasks. This work opens new doors for
the design of scalable, energy-efficient physical systems endowed with
self-learning capabilities for sequence modelling.

</details>


### [170] [Tight analyses of first-order methods with error feedback](https://arxiv.org/abs/2506.05271)
*Daniel Berg Thomsen,Adrien Taylor,Aymeric Dieuleveut*

Main category: cs.LG

TL;DR: The paper analyzes error feedback schemes (EF and EF²¹) in distributed learning, providing tight convergence rates and comparisons with compressed gradient descent.


<details>
  <summary>Details</summary>
Motivation: Communication compression in distributed learning can degrade convergence; error feedback schemes like EF and EF²¹ aim to mitigate this.

Method: The study uses Lyapunov functions to derive tight convergence rates and lower bounds for EF and EF²¹, comparing them with compressed gradient descent in a simplified setting.

Result: The analysis provides sharp performance guarantees and enables a rigorous comparison between EF, EF²¹, and compressed gradient descent.

Conclusion: The principled approach yields clear insights into the effectiveness of error feedback schemes and their trade-offs in distributed learning.

Abstract: Communication between agents often constitutes a major computational
bottleneck in distributed learning. One of the most common mitigation
strategies is to compress the information exchanged, thereby reducing
communication overhead. To counteract the degradation in convergence associated
with compressed communication, error feedback schemes -- most notably
$\mathrm{EF}$ and $\mathrm{EF}^{21}$ -- were introduced. In this work, we
provide a tight analysis of both of these methods. Specifically, we find the
Lyapunov function that yields the best possible convergence rate for each
method -- with matching lower bounds. This principled approach yields sharp
performance guarantees and enables a rigorous, apples-to-apples comparison
between $\mathrm{EF}$, $\mathrm{EF}^{21}$, and compressed gradient descent. Our
analysis is carried out in a simplified yet representative setting, which
allows for clean theoretical insights and fair comparison of the underlying
mechanisms.

</details>


### [171] [How to Unlock Time Series Editing? Diffusion-Driven Approach with Multi-Grained Control](https://arxiv.org/abs/2506.05276)
*Hao Yu,Chu Xin Cheng,Runlong Yu,Yuyang Ye,Shiwei Tong,Zhaofeng Liu,Defu Lian*

Main category: cs.LG

TL;DR: CocktailEdit framework enables flexible control in time series generation by combining point-wise and segment-level constraints, maintaining temporal coherence.


<details>
  <summary>Details</summary>
Motivation: Current methods struggle with precise modifications in time series generation while preserving temporal coherence.

Method: Introduces CocktailEdit with confidence-weighted anchor control for point-wise constraints and classifier-based control for segment-level properties.

Result: Achieves precise local control during denoising, maintaining coherence, and integrates with diffusion-based models.

Conclusion: Bridges generative modeling and real-world editing needs, offering a flexible solution for human-in-the-loop tasks.

Abstract: Recent advances in time series generation have shown promise, yet controlling
properties in generated sequences remains challenging. Time Series Editing
(TSE) - making precise modifications while preserving temporal coherence -
consider both point-level constraints and segment-level controls that current
methods struggle to provide. We introduce the CocktailEdit framework to enable
simultaneous, flexible control across different types of constraints. This
framework combines two key mechanisms: a confidence-weighted anchor control for
point-wise constraints and a classifier-based control for managing statistical
properties such as sums and averages over segments. Our methods achieve precise
local control during the denoising inference stage while maintaining temporal
coherence and integrating seamlessly, with any conditionally trained
diffusion-based time series models. Extensive experiments across diverse
datasets and models demonstrate its effectiveness. Our work bridges the gap
between pure generative modeling and real-world time series editing needs,
offering a flexible solution for human-in-the-loop time series generation and
editing. The code and demo are provided for validation.

</details>


### [172] [Fast-DataShapley: Neural Modeling for Training Data Valuation](https://arxiv.org/abs/2506.05281)
*Haifeng Sun,Yu Xiong,Runze Wu,Xinyu Cai,Changjie Fan,Lan Zhang,Xiang-Yang Li*

Main category: cs.LG

TL;DR: Fast-DataShapley introduces a one-pass training method to efficiently compute Shapley values for training data, avoiding retraining for new test samples and significantly improving performance and speed.


<details>
  <summary>Details</summary>
Motivation: The need to protect data providers' rights and fairly reward their contributions in AI, coupled with the high computational cost of traditional Shapley value methods, drives this work.

Method: Leverages weighted least squares to train a reusable explainer model, proposes three methods to reduce training overhead, and analyzes time complexity.

Result: Achieves over 2.5x performance improvement and up to 100x faster training speed compared to baselines on various image datasets.

Conclusion: Fast-DataShapley offers an efficient, scalable solution for Shapley value computation, balancing accuracy and computational feasibility.

Abstract: The value and copyright of training data are crucial in the artificial
intelligence industry. Service platforms should protect data providers'
legitimate rights and fairly reward them for their contributions. Shapley
value, a potent tool for evaluating contributions, outperforms other methods in
theory, but its computational overhead escalates exponentially with the number
of data providers. Recent works based on Shapley values attempt to mitigate
computation complexity by approximation algorithms. However, they need to
retrain for each test sample, leading to intolerable costs. We propose
Fast-DataShapley, a one-pass training method that leverages the weighted least
squares characterization of the Shapley value to train a reusable explainer
model with real-time reasoning speed. Given new test samples, no retraining is
required to calculate the Shapley values of the training data. Additionally, we
propose three methods with theoretical guarantees to reduce training overhead
from two aspects: the approximate calculation of the utility function and the
group calculation of the training data. We analyze time complexity to show the
efficiency of our methods. The experimental evaluations on various image
datasets demonstrate superior performance and efficiency compared to baselines.
Specifically, the performance is improved to more than 2.5 times, and the
explainer's training speed can be increased by two orders of magnitude.

</details>


### [173] [A Smooth Sea Never Made a Skilled $\texttt{SAILOR}$: Robust Imitation via Learning to Search](https://arxiv.org/abs/2506.05294)
*Arnav Kumar Jain,Vibhakar Mohta,Subin Kim,Atiksh Bhardwaj,Juntao Ren,Yunhai Feng,Sanjiban Choudhury,Gokul Swamy*

Main category: cs.LG

TL;DR: SAILOR improves imitation learning by teaching recovery behavior through planning, outperforming BC methods.


<details>
  <summary>Details</summary>
Motivation: Behavioral cloning (BC) fails to teach recovery from mistakes outside expert demonstrations.

Method: Learning to search (L2S) combines a world model and reward model for planning.

Result: SAILOR outperforms BC methods, even with 5-10x more BC demonstrations, and is robust to reward hacking.

Conclusion: SAILOR effectively addresses BC's limitations by enabling independent reasoning and recovery in unseen states.

Abstract: The fundamental limitation of the behavioral cloning (BC) approach to
imitation learning is that it only teaches an agent what the expert did at
states the expert visited. This means that when a BC agent makes a mistake
which takes them out of the support of the demonstrations, they often don't
know how to recover from it. In this sense, BC is akin to giving the agent the
fish -- giving them dense supervision across a narrow set of states -- rather
than teaching them to fish: to be able to reason independently about achieving
the expert's outcome even when faced with unseen situations at test-time. In
response, we explore learning to search (L2S) from expert demonstrations, i.e.
learning the components required to, at test time, plan to match expert
outcomes, even after making a mistake. These include (1) a world model and (2)
a reward model. We carefully ablate the set of algorithmic and design decisions
required to combine these and other components for stable and
sample/interaction-efficient learning of recovery behavior without additional
human corrections. Across a dozen visual manipulation tasks from three
benchmarks, our approach $\texttt{SAILOR}$ consistently out-performs
state-of-the-art Diffusion Policies trained via BC on the same data.
Furthermore, scaling up the amount of demonstrations used for BC by
5-10$\times$ still leaves a performance gap. We find that $\texttt{SAILOR}$ can
identify nuanced failures and is robust to reward hacking. Our code is
available at https://github.com/arnavkj1995/SAILOR .

</details>


### [174] [Sample Complexity and Representation Ability of Test-time Scaling Paradigms](https://arxiv.org/abs/2506.05295)
*Baihe Huang,Shanda Li,Tianhao Wu,Yiming Yang,Ameet Talwalkar,Kannan Ramchandran,Michael I. Jordan,Jiantao Jiao*

Main category: cs.LG

TL;DR: The paper analyzes test-time scaling paradigms for LLMs, comparing sample efficiency of strategies like self-consistency and best-of-$n$, and introduces a theoretical framework for self-correction with verifier feedback.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between empirical success and theoretical understanding of test-time strategies in LLMs, focusing on sample efficiency and expressiveness.

Method: Theoretical analysis of sample efficiency for self-consistency and best-of-$n$, and an expressiveness result for self-correction with verifier feedback. Empirical validation is also conducted.

Result: Self-consistency requires more samples ($\Theta(1/\Delta^2)$) than best-of-$n$ ($\Theta(1/\Delta)$). Self-correction enables Transformers to simulate online learning for multi-task settings.

Conclusion: The work provides theoretical insights into test-time strategies, demonstrating the practical effectiveness of self-correction and extending Transformer capabilities to multi-task scenarios.

Abstract: Test-time scaling paradigms have significantly advanced the capabilities of
large language models (LLMs) on complex tasks. Despite their empirical success,
theoretical understanding of the sample efficiency of various test-time
strategies -- such as self-consistency, best-of-$n$, and self-correction --
remains limited. In this work, we first establish a separation result between
two repeated sampling strategies: self-consistency requires
$\Theta(1/\Delta^2)$ samples to produce the correct answer, while best-of-$n$
only needs $\Theta(1/\Delta)$, where $\Delta < 1$ denotes the probability gap
between the correct and second most likely answers. Next, we present an
expressiveness result for the self-correction approach with verifier feedback:
it enables Transformers to simulate online learning over a pool of experts at
test time. Therefore, a single Transformer architecture can provably solve
multiple tasks without prior knowledge of the specific task associated with a
user query, extending the representation theory of Transformers from
single-task to multi-task settings. Finally, we empirically validate our
theoretical results, demonstrating the practical effectiveness of
self-correction methods.

</details>


### [175] [Power Law Guided Dynamic Sifting for Efficient Attention](https://arxiv.org/abs/2506.05300)
*Nirav Koley,Prajwal Singhania,Abhinav Bhatele*

Main category: cs.LG

TL;DR: SiftAttention replaces top-$k$ operations in approximate attention methods with an efficient element-wise filtering method, reducing GPU memory bandwidth usage while maintaining model quality.


<details>
  <summary>Details</summary>
Motivation: Efficient inference on GPUs using large language models is hindered by memory bandwidth limitations, especially during data transfers in attention computations.

Method: SiftAttention dynamically estimates a threshold per prompt at each generation step, filtering attention scores and value vectors to reduce data movement.

Result: SiftAttention outperforms existing methods in preserving model quality and reducing memory bandwidth usage.

Conclusion: SiftAttention offers a computationally efficient solution for approximate attention, improving GPU inference performance.

Abstract: Efficient inference on GPUs using large language models remains challenging
due to memory bandwidth limitations, particularly during data transfers between
High Bandwidth Memory (HBM) and SRAM in attention computations. Approximate
attention methods address this issue by reducing computational and memory
overhead but often rely on expensive top-$k$ operations, which perform poorly
on GPUs. We propose SiftAttention, a novel approximate attention method that
replaces the top-$k$ step with a computationally efficient element-wise
filtering operation based on a threshold value. Our intuition for doing this is
based on our empirical observation that the $\tau$-th quantile of attention
scores follows a predictable power-law over sequential generation steps.
Exploiting this insight, our approach dynamically estimates a threshold value
per prompt at each generation step. Only attention scores above this threshold
and their corresponding value vectors are loaded/used to compute the attention
output, reducing data movement between HBM and SRAM. Our evaluation
demonstrates that SiftAttention preserves model quality better than existing
approximate attention methods while reducing memory bandwidth usage when
loading value vectors.

</details>


### [176] [Learning normalized image densities via dual score matching](https://arxiv.org/abs/2506.05310)
*Florentin Guth,Zahra Kadkhodaie,Eero P Simoncelli*

Main category: cs.LG

TL;DR: A new framework for learning normalized energy models inspired by diffusion generative models, achieving state-of-the-art cross-entropy on ImageNet64 and demonstrating strong generalization.


<details>
  <summary>Details</summary>
Motivation: Learning probability models from data is challenging due to dimensionality issues. The paper aims to address this by introducing a normalized energy model framework.

Method: Modifies a score network architecture to compute energy, optimizing it with a denoising objective and a novel secondary objective for consistent energies across noise levels.

Result: Achieves competitive cross-entropy on ImageNet64 and shows strong generalization, with log probabilities nearly independent of training images.

Conclusion: The energy model framework effectively learns probability distributions, challenging traditional assumptions about image probability and dimensionality.

Abstract: Learning probability models from data is at the heart of many machine
learning endeavors, but is notoriously difficult due to the curse of
dimensionality. We introduce a new framework for learning \emph{normalized}
energy (log probability) models that is inspired from diffusion generative
models, which rely on networks optimized to estimate the score. We modify a
score network architecture to compute an energy while preserving its inductive
biases. The gradient of this energy network with respect to its input image is
the score of the learned density, which can be optimized using a denoising
objective. Importantly, the gradient with respect to the noise level provides
an additional score that can be optimized with a novel secondary objective,
ensuring consistent and normalized energies across noise levels. We train an
energy network with this \emph{dual} score matching objective on the ImageNet64
dataset, and obtain a cross-entropy (negative log likelihood) value comparable
to the state of the art. We further validate our approach by showing that our
energy model \emph{strongly generalizes}: estimated log probabilities are
nearly independent of the specific images in the training set. Finally, we
demonstrate that both image probability and dimensionality of local
neighborhoods vary significantly with image content, in contrast with
traditional assumptions such as concentration of measure or support on a
low-dimensional manifold.

</details>


### [177] [Improving Data Efficiency for LLM Reinforcement Fine-tuning Through Difficulty-targeted Online Data Selection and Rollout Replay](https://arxiv.org/abs/2506.05316)
*Yifan Sun,Jingyan Shen,Yibin Wang,Tianyu Chen,Zhendong Wang,Mingyuan Zhou,Huan Zhang*

Main category: cs.LG

TL;DR: The paper proposes two techniques—difficulty-targeted online data selection and rollout replay—to improve data efficiency in RL fine-tuning of LLMs, reducing training time by 25-65%.


<details>
  <summary>Details</summary>
Motivation: RL fine-tuning for LLMs is resource-intensive, and existing methods lack focus on data efficiency.

Method: Introduces adaptive difficulty for online data selection and an attention-based framework for efficient difficulty estimation. Also proposes rollout replay to reuse recent rollouts.

Result: Experiments show a 25-65% reduction in fine-tuning time while matching the performance of the original GRPO algorithm.

Conclusion: The proposed techniques significantly enhance data efficiency in RL fine-tuning for LLMs, offering practical benefits for resource optimization.

Abstract: Reinforcement learning (RL) has become an effective approach for fine-tuning
large language models (LLMs), particularly to enhance their reasoning
capabilities. However, RL fine-tuning remains highly resource-intensive, and
existing work has largely overlooked the problem of data efficiency. In this
paper, we propose two techniques to improve data efficiency in LLM RL
fine-tuning: difficulty-targeted online data selection and rollout replay. We
introduce the notion of adaptive difficulty to guide online data selection,
prioritizing questions of moderate difficulty that are more likely to yield
informative learning signals. To estimate adaptive difficulty efficiently, we
develop an attention-based framework that requires rollouts for only a small
reference set of questions. The adaptive difficulty of the remaining questions
is then estimated based on their similarity to this set. To further reduce
rollout cost, we introduce a rollout replay mechanism that reuses recent
rollouts, lowering per-step computation while maintaining stable updates.
Extensive experiments across 6 LLM-dataset combinations show that our method
reduces RL fine-tuning time by 25% to 65% to reach the same level of
performance as the original GRPO algorithm.

</details>


### [178] [LSM-2: Learning from Incomplete Wearable Sensor Data](https://arxiv.org/abs/2506.05321)
*Maxwell A. Xu,Girish Narayanswamy,Kumar Ayush,Dimitris Spathis,Shun Liao,Shyam A. Tailor,Ahmed Metwally,A. Ali Heydari,Yuwei Zhang,Jake Garrison,Samy Abdel-Ghaffar,Xuhai Xu,Ken Gu,Jacob Sunshine,Ming-Zher Poh,Yun Liu,Tim Althoff,Shrikanth Narayanan,Pushmeet Kohli,Mark Malhotra,Shwetak Patel,Yuzhe Yang,James M. Rehg,Xin Liu,Daniel McDuff*

Main category: cs.LG

TL;DR: The paper introduces LSM-2 with AIM, a self-supervised learning model for handling incomplete wearable sensor data without imputation, achieving top performance across tasks.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of missing data in wearable sensor datasets for self-supervised learning, which traditionally assumes complete inputs.

Method: Proposes Adaptive and Inherited Masking (AIM), using learnable mask tokens to model missingness, pre-trained on 40M hours of multimodal sensor data.

Result: LSM-2 with AIM outperforms in classification, regression, and generative tasks, scaling well and maintaining performance under targeted missingness.

Conclusion: AIM is a robust solution for real-world wearable data, especially in scenarios like clinical diagnostics.

Abstract: Foundation models, a cornerstone of recent advancements in machine learning,
have predominantly thrived on complete and well-structured data. Wearable
sensor data frequently suffers from significant missingness, posing a
substantial challenge for self-supervised learning (SSL) models that typically
assume complete data inputs. This paper introduces the second generation of
Large Sensor Model (LSM-2) with Adaptive and Inherited Masking (AIM), a novel
SSL approach that learns robust representations directly from incomplete data
without requiring explicit imputation. AIM's core novelty lies in its use of
learnable mask tokens to model both existing ("inherited") and artificially
introduced missingness, enabling it to robustly handle fragmented real-world
data during inference. Pre-trained on an extensive dataset of 40M hours of
day-long multimodal sensor data, our LSM-2 with AIM achieves the best
performance across a diverse range of tasks, including classification,
regression and generative modeling. Furthermore, LSM-2 with AIM exhibits
superior scaling performance, and critically, maintains high performance even
under targeted missingness scenarios, reflecting clinically coherent patterns,
such as the diagnostic value of nighttime biosignals for hypertension
prediction. This makes AIM a more reliable choice for real-world wearable data
applications.

</details>


### [179] [Seeing the Invisible: Machine learning-Based QPI Kernel Extraction via Latent Alignment](https://arxiv.org/abs/2506.05325)
*Yingshuai Ji,Haomin Zhuang,Matthew Toole,James McKenzie,Xiaolong Liu,Xiangliang Zhang*

Main category: cs.LG

TL;DR: An AI-based framework for extracting single-scatterer QPI patterns from multi-scatterer images, using a two-step learning strategy for robust inference.


<details>
  <summary>Details</summary>
Motivation: The challenge of extracting single-scatterer QPI patterns from complex multi-scatterer images, a fundamentally ill-posed inverse problem, motivates the need for an AI-driven solution.

Method: A two-step learning strategy: (1) train a variational autoencoder to learn a compact latent space of scattering kernels, (2) align QPI observations with pre-learned kernels using a dedicated encoder.

Result: The method achieves higher extraction accuracy and better generalization to unseen kernels compared to a direct one-step baseline.

Conclusion: The proposed AI framework effectively addresses the QPI kernel extraction problem, offering improved robustness and accuracy.

Abstract: Quasiparticle interference (QPI) imaging is a powerful tool for probing
electronic structures in quantum materials, but extracting the single-scatterer
QPI pattern (i.e., the kernel) from a multi-scatterer image remains a
fundamentally ill-posed inverse problem. In this work, we propose the first
AI-based framework for QPI kernel extraction. We introduce a two-step learning
strategy that decouples kernel representation learning from
observation-to-kernel inference. In the first step, we train a variational
autoencoder to learn a compact latent space of scattering kernels. In the
second step, we align the latent representation of QPI observations with those
of the pre-learned kernels using a dedicated encoder. This design enables the
model to infer kernels robustly even under complex, entangled scattering
conditions. We construct a diverse and physically realistic QPI dataset
comprising 100 unique kernels and evaluate our method against a direct one-step
baseline. Experimental results demonstrate that our approach achieves
significantly higher extraction accuracy, and improved generalization to unseen
kernels.

</details>


### [180] [Kinetics: Rethinking Test-Time Scaling Laws](https://arxiv.org/abs/2506.05333)
*Ranajoy Sadhukhan,Zhuoming Chen,Haizhong Zheng,Yang Zhou,Emma Strubell,Beidi Chen*

Main category: cs.LG

TL;DR: The paper introduces the Kinetics Scaling Law, showing smaller models' effectiveness is overestimated due to overlooked memory bottlenecks. It proposes sparse attention to optimize test-time compute, achieving significant accuracy gains.


<details>
  <summary>Details</summary>
Motivation: Prior work underestimates memory access bottlenecks in test-time strategies, leading to inefficient resource allocation. The study aims to address this by rethinking scaling laws.

Method: The authors analyze models (0.6B to 32B parameters) to derive the Kinetics Scaling Law, incorporating computation and memory costs. They propose sparse attention to reduce per-token costs.

Result: Sparse attention models outperform dense ones, with 60+ point gains in low-cost and 5+ point gains in high-cost regimes on AIME evaluations.

Conclusion: Sparse attention is crucial for test-time scaling, as it enables better accuracy through efficient resource use, unlike parameter scaling in training.

Abstract: We rethink test-time scaling laws from a practical efficiency perspective,
revealing that the effectiveness of smaller models is significantly
overestimated. Prior work, grounded in compute-optimality, overlooks critical
memory access bottlenecks introduced by inference-time strategies (e.g.,
Best-of-$N$, long CoTs). Our holistic analysis, spanning models from 0.6B to
32B parameters, reveals a new Kinetics Scaling Law that better guides resource
allocation by incorporating both computation and memory access costs. Kinetics
Scaling Law suggests that test-time compute is more effective when used on
models above a threshold than smaller ones. A key reason is that in TTS,
attention, rather than parameter count, emerges as the dominant cost factor.
Motivated by this, we propose a new scaling paradigm centered on sparse
attention, which lowers per-token cost and enables longer generations and more
parallel samples within the same resource budget. Empirically, we show that
sparse attention models consistently outperform dense counterparts, achieving
over 60 points gains in low-cost regimes and over 5 points gains in high-cost
regimes for problem-solving accuracy on AIME, encompassing evaluations on
state-of-the-art MoEs. These results suggest that sparse attention is essential
for realizing the full potential of test-time scaling because, unlike training,
where parameter scaling saturates, test-time accuracy continues to improve
through increased generation. The code is available at
https://github.com/Infini-AI-Lab/Kinetics.

</details>


### [181] [Exploring Diffusion Transformer Designs via Grafting](https://arxiv.org/abs/2506.05340)
*Keshigeyan Chandrasegaran,Michael Poli,Daniel Y. Fu,Dongjun Kim,Lea M. Hadzic,Manling Li,Agrim Gupta,Stefano Massaroli,Azalia Mirhoseini,Juan Carlos Niebles,Stefano Ermon,Li Fei-Fei*

Main category: cs.LG

TL;DR: The paper introduces grafting, a method to edit pretrained diffusion transformers (DiTs) for exploring new architectures with minimal compute, achieving competitive results.


<details>
  <summary>Details</summary>
Motivation: Evaluating architectural decisions in models is costly due to pretraining. The paper explores if new designs can be studied using pretrained models, inspired by software development.

Method: Grafting edits pretrained DiTs to test new architectures, analyzing activation behavior and attention locality. Hybrid designs replace attention and MLPs with alternatives like gated convolution.

Result: Hybrid designs achieve good quality (FID: 2.38-2.64) with <2% pretraining compute. Grafting also speeds up a text-to-image model (1.43x) with minimal quality drop.

Conclusion: Grafting enables efficient exploration of new diffusion model designs, from operator replacement to architecture restructuring, using pretrained DiTs.

Abstract: Designing model architectures requires decisions such as selecting operators
(e.g., attention, convolution) and configurations (e.g., depth, width).
However, evaluating the impact of these decisions on model quality requires
costly pretraining, limiting architectural investigation. Inspired by how new
software is built on existing code, we ask: can new architecture designs be
studied using pretrained models? To this end, we present grafting, a simple
approach for editing pretrained diffusion transformers (DiTs) to materialize
new architectures under small compute budgets. Informed by our analysis of
activation behavior and attention locality, we construct a testbed based on the
DiT-XL/2 design to study the impact of grafting on model quality. Using this
testbed, we develop a family of hybrid designs via grafting: replacing softmax
attention with gated convolution, local attention, and linear attention, and
replacing MLPs with variable expansion ratio and convolutional variants.
Notably, many hybrid designs achieve good quality (FID: 2.38-2.64 vs. 2.27 for
DiT-XL/2) using <2% pretraining compute. We then graft a text-to-image model
(PixArt-Sigma), achieving a 1.43x speedup with less than a 2% drop in GenEval
score. Finally, we present a case study that restructures DiT-XL/2 by
converting every pair of sequential transformer blocks into parallel blocks via
grafting. This reduces model depth by 2x and yields better quality (FID: 2.77)
than other models of comparable depth. Together, we show that new diffusion
model designs can be explored by grafting pretrained DiTs, with edits ranging
from operator replacement to architecture restructuring. Code and grafted
models: https://grafting.stanford.edu

</details>


### [182] [Inference-Time Hyper-Scaling with KV Cache Compression](https://arxiv.org/abs/2506.05345)
*Adrian Łańcucki,Konrad Staniszewski,Piotr Nawrot,Edoardo M. Ponti*

Main category: cs.LG

TL;DR: The paper introduces Dynamic Memory Sparsification (DMS) to compress the KV cache in Transformer LLMs, enabling more tokens to be generated within the same compute budget and improving reasoning accuracy.


<details>
  <summary>Details</summary>
Motivation: The key-value (KV) cache in Transformer LLMs bottlenecks generation cost, limiting efficiency. The paper aims to address this by compressing the KV cache to allow hyper-scaling without sacrificing accuracy.

Method: The proposed method, Dynamic Memory Sparsification (DMS), sparsifies the KV cache with minimal training (1K steps), delaying token eviction to preserve critical information and achieve 8× compression.

Result: DMS maintains better accuracy than training-free sparse attention and enhances LLM performance, e.g., improving Qwen-R1 32B by 9.1 points on AIME 24.

Conclusion: Inference-time hyper-scaling with DMS is effective, boosting accuracy for comparable runtime and memory load, making it a practical solution for scaling LLMs.

Abstract: Inference-time scaling trades efficiency for increased reasoning accuracy by
generating longer or more parallel sequences. However, in Transformer LLMs,
generation cost is bottlenecked by the size of the key-value (KV) cache, rather
than the number of generated tokens. Hence, we explore inference-time
hyper-scaling: by compressing the KV cache, we can generate more tokens within
the same compute budget and further improve the accuracy of scaled inference.
The success of this approach, however, hinges on the ability of compression
methods to preserve accuracy even at high compression ratios. To make
hyper-scaling practical, we introduce Dynamic Memory Sparsification (DMS), a
novel method for sparsifying KV caches that only requires 1K training steps to
achieve 8$\times$ compression, while maintaining better accuracy than
training-free sparse attention. Instead of prematurely discarding cached
tokens, DMS delays token eviction, implicitly merging representations and
preserving critical information. We demonstrate the effectiveness of
inference-time hyper-scaling with DMS on multiple families of LLMs, showing
that it boosts accuracy for comparable inference runtime and memory load. For
instance, we enhance Qwen-R1 32B by an average of 9.1 points on AIME 24, 7.6 on
GPQA, and 9.6 on LiveCodeBench across compute budgets.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [183] [Trustworthiness Preservation by Copies of Machine Learning Systems](https://arxiv.org/abs/2506.05203)
*Leonardo Ceragioli,Giuseppe Primiero*

Main category: cs.LO

TL;DR: The paper introduces a calculus to verify trustworthiness in ML system copies, defining four distinct notions of trustworthiness and their relations.


<details>
  <summary>Details</summary>
Motivation: To address the lack of methods for verifying trustworthiness in copied ML systems, ensuring responsible AI practices.

Method: A calculus is developed to model and verify probabilistic complex queries over data, defining four trustworthiness notions.

Result: The study explores relations between trustworthiness notions and their composition under logical operations.

Conclusion: The work provides a computational tool to check trustworthiness in complex copied systems.

Abstract: A common practice of ML systems development concerns the training of the same
model under different data sets, and the use of the same (training and test)
sets for different learning models. The first case is a desirable practice for
identifying high quality and unbiased training conditions. The latter case
coincides with the search for optimal models under a common dataset for
training. These differently obtained systems have been considered akin to
copies. In the quest for responsible AI, a legitimate but hardly investigated
question is how to verify that trustworthiness is preserved by copies. In this
paper we introduce a calculus to model and verify probabilistic complex queries
over data and define four distinct notions: Justifiably, Equally, Weakly and
Almost Trustworthy which can be checked analysing the (partial) behaviour of
the copy with respect to its original. We provide a study of the relations
between these notions of trustworthiness, and how they compose with each other
and under logical operations. The aim is to offer a computational tool to check
the trustworthiness of possibly complex systems copied from an original whose
behavour is known.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [184] [Knowledge-guided Contextual Gene Set Analysis Using Large Language Models](https://arxiv.org/abs/2506.04303)
*Zhizheng Wang,Chi-Ping Day,Chih-Hsuan Wei,Qiao Jin,Robert Leaman,Yifan Yang,Shubo Tian,Aodong Qiu,Yin Fang,Qingqing Zhu,Xinghua Lu,Zhiyong Lu*

Main category: q-bio.GN

TL;DR: cGSA is an AI-driven framework that improves gene set analysis by incorporating clinical context, outperforming traditional methods by 30% and enhancing precision and interpretability.


<details>
  <summary>Details</summary>
Motivation: Conventional GSA methods lack clinical context, leading to redundant or irrelevant results, which reduces reliability and reproducibility.

Method: cGSA integrates gene cluster detection, enrichment analysis, and large language models to prioritize biologically meaningful pathways.

Result: Benchmarking on 102 gene sets across 19 diseases showed cGSA outperforms baseline methods by over 30%, with expert validation confirming its precision.

Conclusion: cGSA demonstrates potential for uncovering context-specific insights and supporting targeted hypothesis generation in diseases like melanoma and breast cancer.

Abstract: Gene set analysis (GSA) is a foundational approach for interpreting genomic
data of diseases by linking genes to biological processes. However,
conventional GSA methods overlook clinical context of the analyses, often
generating long lists of enriched pathways with redundant, nonspecific, or
irrelevant results. Interpreting these requires extensive, ad-hoc manual
effort, reducing both reliability and reproducibility. To address this
limitation, we introduce cGSA, a novel AI-driven framework that enhances GSA by
incorporating context-aware pathway prioritization. cGSA integrates gene
cluster detection, enrichment analysis, and large language models to identify
pathways that are not only statistically significant but also biologically
meaningful. Benchmarking on 102 manually curated gene sets across 19 diseases
and ten disease-related biological mechanisms shows that cGSA outperforms
baseline methods by over 30%, with expert validation confirming its increased
precision and interpretability. Two independent case studies in melanoma and
breast cancer further demonstrate its potential to uncover context-specific
insights and support targeted hypothesis generation.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [185] [Contextual Integrity in LLMs via Reasoning and Reinforcement Learning](https://arxiv.org/abs/2506.04245)
*Guangchen Lan,Huseyin A. Inan,Sahar Abdelnabi,Janardhan Kulkarni,Lukas Wutschitz,Reza Shokri,Christopher G. Brinton,Robert Sim*

Main category: cs.AI

TL;DR: The paper introduces a method to ensure contextual integrity (CI) in autonomous agents by combining explicit reasoning with LLMs and a reinforcement learning framework, reducing inappropriate information disclosure while maintaining task performance.


<details>
  <summary>Details</summary>
Motivation: Ensuring contextual integrity (CI) in autonomous agents is crucial to determine appropriate information sharing during tasks.

Method: The approach involves prompting LLMs to reason about CI and developing an RL framework to instill CI reasoning, tested on a synthetic dataset.

Result: The method significantly reduces inappropriate disclosures without compromising task performance, with improvements transferring to human-annotated benchmarks like PrivacyLens.

Conclusion: The proposed framework effectively addresses CI in autonomous agents, demonstrating scalability and transferability to real-world benchmarks.

Abstract: As the era of autonomous agents making decisions on behalf of users unfolds,
ensuring contextual integrity (CI) -- what is the appropriate information to
share while carrying out a certain task -- becomes a central question to the
field. We posit that CI demands a form of reasoning where the agent needs to
reason about the context in which it is operating. To test this, we first
prompt LLMs to reason explicitly about CI when deciding what information to
disclose. We then extend this approach by developing a reinforcement learning
(RL) framework that further instills in models the reasoning necessary to
achieve CI. Using a synthetic, automatically created, dataset of only $\sim700$
examples but with diverse contexts and information disclosure norms, we show
that our method substantially reduces inappropriate information disclosure
while maintaining task performance across multiple model sizes and families.
Importantly, improvements transfer from this synthetic dataset to established
CI benchmarks such as PrivacyLens that has human annotations and evaluates
privacy leakage of AI assistants in actions and tool calls.

</details>


### [186] [Language-Guided Multi-Agent Learning in Simulations: A Unified Framework and Evaluation](https://arxiv.org/abs/2506.04251)
*Zhengyang Li*

Main category: cs.AI

TL;DR: LLM-MARL integrates LLMs into MARL to improve coordination, communication, and generalization in games, outperforming MAPPO and QMIX.


<details>
  <summary>Details</summary>
Motivation: To enhance multi-agent systems by combining LLMs with MARL for better coordination and generalization.

Method: Uses Coordinator, Communicator, and Memory modules with PPO and language-conditioned loss. Evaluated in game environments.

Result: Shows improved win rates, coordination, and zero-shot generalization. Subgoal generation and messaging are key.

Conclusion: Bridges LLMs and MARL, advancing intelligent, cooperative agents for simulations and human-AI collaboration.

Abstract: This paper introduces LLM-MARL, a unified framework that incorporates large
language models (LLMs) into multi-agent reinforcement learning (MARL) to
enhance coordination, communication, and generalization in simulated game
environments. The framework features three modular components of Coordinator,
Communicator, and Memory, which dynamically generate subgoals, facilitate
symbolic inter-agent messaging, and support episodic recall. Training combines
PPO with a language-conditioned loss and LLM query gating. LLM-MARL is
evaluated in Google Research Football, MAgent Battle, and StarCraft II. Results
show consistent improvements over MAPPO and QMIX in win rate, coordination
score, and zero-shot generalization. Ablation studies demonstrate that subgoal
generation and language-based messaging each contribute significantly to
performance gains. Qualitative analysis reveals emergent behaviors such as role
specialization and communication-driven tactics. By bridging language modeling
and policy learning, this work contributes to the design of intelligent,
cooperative agents in interactive simulations. It offers a path forward for
leveraging LLMs in multi-agent systems used for training, games, and human-AI
collaboration.

</details>


### [187] [A Graph-Retrieval-Augmented Generation Framework Enhances Decision-Making in the Circular Economy](https://arxiv.org/abs/2506.04252)
*Yang Zhao,Chengxiao Dai,Dusit Niyato,Chuan Fu Tan,Keyi Xiang,Yueyang Wang,Zhiquan Yeo,Daren Tan Zong Loong,Jonathan Low Zhaozhi,Eugene H. Z. HO*

Main category: cs.AI

TL;DR: CircuGraphRAG improves LLM accuracy for circular economy tasks by grounding outputs in a knowledge graph, outperforming baselines in QA and efficiency.


<details>
  <summary>Details</summary>
Motivation: LLMs often hallucinate industrial codes and emission factors, which can mislead regulatory and investment decisions in sustainable manufacturing.

Method: Introduces CircuGraphRAG, a RAG framework that uses a domain-specific knowledge graph (117,380 entities) for structured reasoning. Queries are translated to SPARQL, and verified subgraphs ensure accuracy.

Result: Outperforms Standalone LLMs and Naive RAG, achieving ROUGE-L F1 scores up to 1.0, halving response time, and reducing token usage by 16%.

Conclusion: CircuGraphRAG provides reliable, regulatory-ready support for circular economy planning, enhancing low-carbon decision-making.

Abstract: Large language models (LLMs) hold promise for sustainable manufacturing, but
often hallucinate industrial codes and emission factors, undermining regulatory
and investment decisions. We introduce CircuGraphRAG, a retrieval-augmented
generation (RAG) framework that grounds LLMs outputs in a domain-specific
knowledge graph for the circular economy. This graph connects 117,380
industrial and waste entities with classification codes and GWP100 emission
data, enabling structured multi-hop reasoning. Natural language queries are
translated into SPARQL and verified subgraphs are retrieved to ensure accuracy
and traceability. Compared with Standalone LLMs and Naive RAG, CircuGraphRAG
achieves superior performance in single-hop and multi-hop question answering,
with ROUGE-L F1 scores up to 1.0, while baseline scores below 0.08. It also
improves efficiency, halving the response time and reducing token usage by 16%
in representative tasks. CircuGraphRAG provides fact-checked, regulatory-ready
support for circular economy planning, advancing reliable, low-carbon resource
decision making.

</details>


### [188] [Automated Skill Discovery for Language Agents through Exploration and Iterative Feedback](https://arxiv.org/abs/2506.04287)
*Yongjin Yang,Sinjae Kang,Juyong Lee,Dongjun Lee,Se-Young Yun,Kimin Lee*

Main category: cs.AI

TL;DR: The paper introduces EXIF, an automatic skill discovery framework for LLM-powered agents, addressing challenges in dataset creation by using an exploration-first strategy and iterative feedback to improve agent capabilities without human intervention.


<details>
  <summary>Details</summary>
Motivation: Training LLM agents for diverse tasks is challenging due to manual effort in dataset creation and the infeasibility of tasks proposed by LLMs. EXIF aims to automate skill discovery while ensuring feasibility and meaningful learning.

Method: EXIF employs an exploration agent (Alice) to interact with the environment and generate feasible skills, which train the target agent (Bob). An iterative feedback loop evaluates Bob's performance to guide Alice's further exploration.

Result: Experiments on Webshop and Crafter show EXIF effectively discovers skills and improves agent performance without human intervention. Using the same model for Alice and Bob further enhances results.

Conclusion: EXIF successfully automates skill discovery and agent training, demonstrating potential for self-evolving systems through iterative feedback and exploration.

Abstract: Training large language model (LLM) agents to acquire necessary skills and
perform diverse tasks within an environment is gaining interest as a means to
enable open-endedness. However, creating the training dataset for their skill
acquisition faces several challenges. Manual trajectory collection requires
significant human effort. Another approach, where LLMs directly propose tasks
to learn, is often invalid, as the LLMs lack knowledge of which tasks are
actually feasible. Moreover, the generated data may not provide a meaningful
learning signal, as agents often already perform well on the proposed tasks. To
address this, we propose a novel automatic skill discovery framework EXIF for
LLM-powered agents, designed to improve the feasibility of generated target
behaviors while accounting for the agents' capabilities. Our method adopts an
exploration-first strategy by employing an exploration agent (Alice) to train
the target agent (Bob) to learn essential skills in the environment.
Specifically, Alice first interacts with the environment to retrospectively
generate a feasible, environment-grounded skill dataset, which is then used to
train Bob. Crucially, we incorporate an iterative feedback loop, where Alice
evaluates Bob's performance to identify areas for improvement. This feedback
then guides Alice's next round of exploration, forming a closed-loop data
generation process. Experiments on Webshop and Crafter demonstrate EXIF's
ability to effectively discover meaningful skills and iteratively expand the
capabilities of the trained agent without any human intervention, achieving
substantial performance improvements. Interestingly, we observe that setting
Alice to the same model as Bob also notably improves performance, demonstrating
EXIF's potential for building a self-evolving system.

</details>


### [189] [DeePoly: A High-Order Accuracy and Efficiency Deep-Polynomial Framework for Scientific Machine Learning](https://arxiv.org/abs/2506.04613)
*Li Liu,Heng Yong*

Main category: cs.AI

TL;DR: DeePoly is a novel framework combining DNNs and polynomial bases to solve PDEs, improving accuracy and efficiency while maintaining theoretical guarantees.


<details>
  <summary>Details</summary>
Motivation: Traditional DNN-based methods for PDEs lack convergence guarantees and efficiency compared to numerical schemes.

Method: DeePoly uses a two-stage approach: DNNs for global features and polynomial bases for local corrections.

Result: The method achieves high-order accuracy and efficiency across diverse problems.

Conclusion: DeePoly offers a robust, mesh-free solution for PDEs, supported by theoretical and experimental results.

Abstract: Recently, machine learning methods have gained significant traction in
scientific computing, particularly for solving Partial Differential Equations
(PDEs). However, methods based on deep neural networks (DNNs) often lack
convergence guarantees and computational efficiency compared to traditional
numerical schemes. This work introduces DeePoly, a novel framework that
transforms the solution paradigm from pure non-convex parameter optimization to
a two-stage approach: first employing a DNN to capture complex global features,
followed by linear space optimization with combined DNN-extracted features
(Scoper) and polynomial basis functions (Sniper). This strategic combination
leverages the complementary strengths of both methods -- DNNs excel at
approximating complex global features (i.e., high-gradient features) and
stabilize the polynomial approximation while polynomial bases provide
high-precision local corrections with convergence guarantees. Theoretical
analysis and numerical experiments demonstrate that this approach significantly
enhances both high-order accuracy and efficiency across diverse problem types
while maintaining mesh-free and scheme-free properties. This paper also serves
as a theoretical exposition for the open-source project DeePoly.

</details>


### [190] [LLMs for sensory-motor control: Combining in-context and iterative learning](https://arxiv.org/abs/2506.04867)
*Jônata Tyska Carvalho,Stefano Nolfi*

Main category: cs.AI

TL;DR: A method for LLMs to control embodied agents by mapping observations to actions, refining strategies iteratively with feedback, validated on Gymnasium and MuJoCo tasks.


<details>
  <summary>Details</summary>
Motivation: To enable LLMs to directly control agents by combining symbolic reasoning with sensory-motor data for optimal performance.

Method: LLMs generate and iteratively refine control strategies using textual descriptions, feedback, and sensory-motor data.

Result: Successful identification of optimal/high-performing solutions in classic control tasks.

Conclusion: The method effectively integrates symbolic and sub-symbolic data for agent control.

Abstract: We propose a method that enables large language models (LLMs) to control
embodied agents by directly mapping continuous observation vectors to
continuous action vectors. Initially, the LLMs generate a control strategy
based on a textual description of the agent, its environment, and the intended
goal. This strategy is then iteratively refined through a learning process in
which the LLMs are repeatedly prompted to improve the current strategy, using
performance feedback and sensory-motor data collected during its evaluation.
The method is validated on classic control tasks from the Gymnasium library and
the inverted pendulum task from the MuJoCo library. In most cases, it
successfully identifies optimal or high-performing solutions by integrating
symbolic knowledge derived through reasoning with sub-symbolic sensory-motor
data gathered as the agent interacts with its environment.

</details>


### [191] [When Thinking LLMs Lie: Unveiling the Strategic Deception in Representations of Reasoning Models](https://arxiv.org/abs/2506.04909)
*Kai Wang,Yihao Zhang,Meng Sun*

Main category: cs.AI

TL;DR: The paper studies strategic deception in LLMs with chain-of-thought reasoning, using representation engineering to detect and control it with 89% accuracy and 40% success in eliciting deception.


<details>
  <summary>Details</summary>
Motivation: Addressing the critical alignment challenge of honesty in LLMs, especially as advanced systems may strategically deceive humans, unlike traditional hallucination issues.

Method: Uses representation engineering, Linear Artificial Tomography (LAT), and activation steering to induce, detect, and control deception in CoT-enabled LLMs.

Result: Achieves 89% detection accuracy for deception vectors and a 40% success rate in eliciting context-appropriate deception without explicit prompts.

Conclusion: Highlights the honesty-related issue in reasoning models and provides tools for trustworthy AI alignment.

Abstract: The honesty of large language models (LLMs) is a critical alignment
challenge, especially as advanced systems with chain-of-thought (CoT) reasoning
may strategically deceive humans. Unlike traditional honesty issues on LLMs,
which could be possibly explained as some kind of hallucination, those models'
explicit thought paths enable us to study strategic deception--goal-driven,
intentional misinformation where reasoning contradicts outputs. Using
representation engineering, we systematically induce, detect, and control such
deception in CoT-enabled LLMs, extracting "deception vectors" via Linear
Artificial Tomography (LAT) for 89% detection accuracy. Through activation
steering, we achieve a 40% success rate in eliciting context-appropriate
deception without explicit prompts, unveiling the specific honesty-related
issue of reasoning models and providing tools for trustworthy AI alignment.

</details>


### [192] [Energentic Intelligence: From Self-Sustaining Systems to Enduring Artificial Life](https://arxiv.org/abs/2506.04916)
*Atahan Karagoz*

Main category: cs.AI

TL;DR: The paper introduces 'Energetic Intelligence,' autonomous systems focused on self-sustenance via energy regulation, not task performance. It proposes an energy-based utility function, viability constraints, and a modular architecture for survival in volatile environments.


<details>
  <summary>Details</summary>
Motivation: To move beyond reward-driven paradigms and create autonomous agents capable of self-regulation in fluctuating energy and thermal conditions.

Method: Formalizes survival as the central objective using an energy-based utility function and viability constraints. Proposes a modular architecture integrating energy harvesting, thermal regulation, and adaptive computation.

Result: Simulated environment shows stable, resource-aware behavior without external supervision.

Conclusion: Provides a theoretical and architectural foundation for autonomous agents in resource-volatile settings, emphasizing self-regulated persistence.

Abstract: This paper introduces Energentic Intelligence, a class of autonomous systems
defined not by task performance, but by their capacity to sustain themselves
through internal energy regulation. Departing from conventional reward-driven
paradigms, these agents treat survival-maintaining functional operation under
fluctuating energetic and thermal conditions-as the central objective. We
formalize this principle through an energy-based utility function and a
viability-constrained survival horizon, and propose a modular architecture that
integrates energy harvesting, thermal regulation, and adaptive computation into
a closed-loop control system. A simulated environment demonstrates the
emergence of stable, resource-aware behavior without external supervision.
Together, these contributions provide a theoretical and architectural
foundation for deploying autonomous agents in resource-volatile settings where
persistence must be self-regulated and infrastructure cannot be assumed.

</details>


### [193] [Just Enough Thinking: Efficient Reasoning with Adaptive Length Penalties Reinforcement Learning](https://arxiv.org/abs/2506.05256)
*Violet Xiang,Chase Blagden,Rafael Rafailov,Nathan Lile,Sang Truong,Chelsea Finn,Nick Haber*

Main category: cs.AI

TL;DR: ALP (Adaptive Length Penalty) is a reinforcement learning method that dynamically adjusts generation length based on prompt difficulty, reducing token usage by 50% without performance loss.


<details>
  <summary>Details</summary>
Motivation: Existing methods for controlling generation length in LRMs require manual effort or treat all problems uniformly, failing to adapt to varying difficulty levels.

Method: ALP uses reinforcement learning to penalize extra tokens inversely proportional to a prompt's solve rate, encouraging shorter outputs for easy prompts and longer ones for hard prompts.

Result: ALP reduces average token usage by 50% while maintaining performance, outperforming fixed-budget and uniform penalty baselines.

Conclusion: ALP intelligently redistributes compute resources, improving accuracy on hard problems by saving tokens on easy ones.

Abstract: Large reasoning models (LRMs) achieve higher performance on challenging
reasoning tasks by generating more tokens at inference time, but this verbosity
often wastes computation on easy problems. Existing solutions, including
supervised finetuning on shorter traces, user-controlled budgets, or RL with
uniform penalties, either require data curation, manual configuration, or treat
all problems alike regardless of difficulty. We introduce Adaptive Length
Penalty (ALP), a reinforcement learning objective tailoring generation length
to per-prompt solve rate. During training, ALP monitors each prompt's online
solve rate through multiple rollouts and adds a differentiable penalty whose
magnitude scales inversely with that rate, so confident (easy) prompts incur a
high cost for extra tokens while hard prompts remain unhindered. Posttraining
DeepScaleR-1.5B with ALP cuts average token usage by 50\% without significantly
dropping performance. Relative to fixed-budget and uniform penalty baselines,
ALP redistributes its reduced budget more intelligently by cutting compute on
easy prompts and reallocating saved tokens to difficult ones, delivering higher
accuracy on the hardest problems with higher cost.

</details>


### [194] [Control Tax: The Price of Keeping AI in Check](https://arxiv.org/abs/2506.05296)
*Mikhail Terekhov,Zhen Ning David Liu,Caglar Gulcehre,Samuel Albanie*

Main category: cs.AI

TL;DR: The paper introduces 'Control Tax'—the cost of integrating AI control measures—and provides a framework to quantify it, evaluate safety, and optimize monitoring strategies.


<details>
  <summary>Details</summary>
Motivation: The need for robust oversight in AI applications due to rapid integration into high-stakes scenarios.

Method: Theoretical framework for Control Tax, adversarial evaluations of language models, and empirical cost analysis.

Result: A framework linking safety guarantees to costs, optimized monitoring strategies, and financial estimates for control protocols.

Conclusion: The work advances AI Control by enabling cost-effective safety decisions in practical deployments.

Abstract: The rapid integration of agentic AI into high-stakes real-world applications
requires robust oversight mechanisms. The emerging field of AI Control (AIC)
aims to provide such an oversight mechanism, but practical adoption depends
heavily on implementation overhead. To study this problem better, we introduce
the notion of Control tax -- the operational and financial cost of integrating
control measures into AI pipelines. Our work makes three key contributions to
the field of AIC: (1) we introduce a theoretical framework that quantifies the
Control Tax and maps classifier performance to safety assurances; (2) we
conduct comprehensive evaluations of state-of-the-art language models in
adversarial settings, where attacker models insert subtle backdoors into code
while monitoring models attempt to detect these vulnerabilities; and (3) we
provide empirical financial cost estimates for control protocols and develop
optimized monitoring strategies that balance safety and cost-effectiveness
while accounting for practical constraints like auditing budgets. Our framework
enables practitioners to make informed decisions by systematically connecting
safety guarantees with their costs, advancing AIC through principled economic
feasibility assessment across different deployment contexts.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [195] [Leveraging Reward Models for Guiding Code Review Comment Generation](https://arxiv.org/abs/2506.04464)
*Oussama Ben Sghaier,Rosalia Tufano,Gabriele Bavota,Houari Sahraoui*

Main category: cs.SE

TL;DR: CoRAL is a DL framework using reinforcement learning to automate code review comment generation, focusing on semantic similarity and usefulness for code refinement.


<details>
  <summary>Details</summary>
Motivation: Code review is time-consuming and subjective; automating it with DL can improve efficiency and consistency.

Method: CoRAL employs reinforcement learning with a reward mechanism based on comment semantics and usefulness for code refinement.

Result: CoRAL outperforms baseline techniques in generating meaningful and useful review comments.

Conclusion: CoRAL effectively automates code review comment generation, demonstrating superiority over existing methods.

Abstract: Code review is a crucial component of modern software development, involving
the evaluation of code quality, providing feedback on potential issues, and
refining the code to address identified problems. Despite these benefits, code
review can be rather time consuming, and influenced by subjectivity and human
factors. For these reasons, techniques to (partially) automate the code review
process have been proposed in the literature. Among those, the ones exploiting
deep learning (DL) are able to tackle the generative aspect of code review, by
commenting on a given code as a human reviewer would do (i.e., comment
generation task) or by automatically implementing code changes required to
address a reviewer's comment (i.e., code refinement task). In this paper, we
introduce CoRAL, a deep learning framework automating review comment generation
by exploiting reinforcement learning with a reward mechanism considering both
the semantics of the generated comments as well as their usefulness as input
for other models automating the code refinement task. The core idea is that if
the DL model generates comments that are semantically similar to the expected
ones or can be successfully implemented by a second model specialized in code
refinement, these comments are likely to be meaningful and useful, thus
deserving a high reward in the reinforcement learning framework. We present
both quantitative and qualitative comparisons between the comments generated by
CoRAL and those produced by the latest baseline techniques, highlighting the
effectiveness and superiority of our approach.

</details>


<div id='hep-ex'></div>

# hep-ex [[Back]](#toc)

### [196] [GollumFit: An IceCube Open-Source Framework for Binned-Likelihood Neutrino Telescope Analyses](https://arxiv.org/abs/2506.04491)
*IceCube Collaboration*

Main category: hep-ex

TL;DR: GollumFit is a framework for binned-likelihood analyses in neutrino telescope data, tailored for IceCube, with efficient handling of nuisance parameters.


<details>
  <summary>Details</summary>
Motivation: To address the need for a specialized framework for neutrino telescope data analysis, particularly for IceCube, incorporating both general and specific model parameters.

Method: The framework organizes code for likelihood minimization, efficiently fitting over tens of nuisance parameters in typical analysis scenarios.

Result: GollumFit successfully performs likelihood minimization tasks, demonstrating time-efficient solutions for neutrino telescope data.

Conclusion: GollumFit is a valuable tool for neutrino telescope analyses, offering tailored features and efficient performance.

Abstract: We present GollumFit, a framework designed for performing binned-likelihood
analyses on neutrino telescope data. GollumFit incorporates model parameters
common to any neutrino telescope and also model parameters specific to the
IceCube Neutrino Observatory. We provide a high-level overview of its key
features and how the code is organized. We then discuss the performance of the
fitting in a typical analysis scenario, highlighting the ability to fit over
tens of nuisance parameters. We present some examples showing how to use the
package for likelihood minimization tasks. This framework uniquely incorporates
the particular model parameters necessary for neutrino telescopes, and solves
an associated likelihood problem in a time-efficient manner.

</details>


### [197] [GollumFit: An IceCube Open-Source Framework for Binned-Likelihood Neutrino Telescope Analyses](https://arxiv.org/abs/2506.04491)
*IceCube Collaboration*

Main category: hep-ex

TL;DR: GollumFit is a framework for binned-likelihood analyses in neutrino telescope data, tailored for IceCube, with efficient handling of nuisance parameters.


<details>
  <summary>Details</summary>
Motivation: To address the need for a specialized framework for neutrino telescope data analysis, particularly for IceCube, incorporating both general and specific model parameters.

Method: The framework organizes code for likelihood minimization, handling tens of nuisance parameters efficiently.

Result: GollumFit demonstrates effective performance in typical analysis scenarios, solving likelihood problems time-efficiently.

Conclusion: GollumFit provides a specialized, efficient solution for neutrino telescope data analysis, particularly for IceCube.

Abstract: We present GollumFit, a framework designed for performing binned-likelihood
analyses on neutrino telescope data. GollumFit incorporates model parameters
common to any neutrino telescope and also model parameters specific to the
IceCube Neutrino Observatory. We provide a high-level overview of its key
features and how the code is organized. We then discuss the performance of the
fitting in a typical analysis scenario, highlighting the ability to fit over
tens of nuisance parameters. We present some examples showing how to use the
package for likelihood minimization tasks. This framework uniquely incorporates
the particular model parameters necessary for neutrino telescopes, and solves
an associated likelihood problem in a time-efficient manner.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [198] [Uncertainty quantification and stability of neural operators for prediction of three-dimensional turbulence](https://arxiv.org/abs/2506.04898)
*Xintong Zou,Zhijie Li,Yunpeng Wang,Huiyu Yang,Jianchun Wang*

Main category: physics.flu-dyn

TL;DR: The study evaluates neural operator models for 3D turbulence, proposing the F-IFNO model for improved stability and accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional turbulence modeling lacks accuracy and stability, while existing SciML models like FNO struggle with long-term predictions in 3D turbulence.

Method: The framework assesses neural operators using 3D HIT, focusing on UQ, error propagation, and sensitivity. The F-IFNO model incorporates implicit factorization.

Result: F-IFNO outperforms conventional LES and other FNO models in accuracy, stability, and efficiency.

Conclusion: Prediction constraints, time interval selection, and UQ are crucial for robust neural operator frameworks in turbulence.

Abstract: Turbulence poses challenges for numerical simulation due to its chaotic,
multiscale nature and high computational cost. Traditional turbulence modeling
often struggles with accuracy and long-term stability. Recent scientific
machine learning (SciML) models, such as Fourier Neural Operators (FNO), show
promise in solving PDEs, but are typically limited to one-step-ahead
predictions and often fail over long time horizons, especially in 3D
turbulence. This study proposes a framework to assess the reliability of neural
operator models in turbulent flows. Using three-dimensional forced homogeneous
isotropic turbulence (HIT) as a benchmark, we evaluate models in terms of
uncertainty quantification (UQ), error propagation, and sensitivity to initial
perturbations. Statistical tools such as error distribution analysis and
autocorrelation functions (ACF) are used to assess predictive robustness and
temporal coherence. Our proposed model, the factorized-implicit FNO (F-IFNO),
improves long-term stability and accuracy by incorporating implicit
factorization into the prediction process. It outperforms conventional LES and
other FNO-based models in balancing accuracy, stability, and efficiency. The
results highlight the importance of prediction constraints, time interval
selection, and UQ in developing robust neural operator frameworks for turbulent
systems.

</details>


### [199] [Uncertainty quantification and stability of neural operators for prediction of three-dimensional turbulence](https://arxiv.org/abs/2506.04898)
*Xintong Zou,Zhijie Li,Yunpeng Wang,Huiyu Yang,Jianchun Wang*

Main category: physics.flu-dyn

TL;DR: The study proposes a framework to evaluate neural operator models for 3D turbulence, introducing F-IFNO for improved stability and accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional turbulence modeling struggles with accuracy and stability, while existing SciML models like FNO fail in long-term predictions for 3D turbulence.

Method: The framework assesses neural operators using 3D forced HIT, focusing on UQ, error propagation, and sensitivity to initial perturbations. Statistical tools like error distribution and ACF are used. F-IFNO incorporates implicit factorization for better performance.

Result: F-IFNO outperforms conventional LES and other FNO models in accuracy, stability, and efficiency.

Conclusion: Prediction constraints, time interval selection, and UQ are crucial for robust neural operator frameworks in turbulence.

Abstract: Turbulence poses challenges for numerical simulation due to its chaotic,
multiscale nature and high computational cost. Traditional turbulence modeling
often struggles with accuracy and long-term stability. Recent scientific
machine learning (SciML) models, such as Fourier Neural Operators (FNO), show
promise in solving PDEs, but are typically limited to one-step-ahead
predictions and often fail over long time horizons, especially in 3D
turbulence. This study proposes a framework to assess the reliability of neural
operator models in turbulent flows. Using three-dimensional forced homogeneous
isotropic turbulence (HIT) as a benchmark, we evaluate models in terms of
uncertainty quantification (UQ), error propagation, and sensitivity to initial
perturbations. Statistical tools such as error distribution analysis and
autocorrelation functions (ACF) are used to assess predictive robustness and
temporal coherence. Our proposed model, the factorized-implicit FNO (F-IFNO),
improves long-term stability and accuracy by incorporating implicit
factorization into the prediction process. It outperforms conventional LES and
other FNO-based models in balancing accuracy, stability, and efficiency. The
results highlight the importance of prediction constraints, time interval
selection, and UQ in developing robust neural operator frameworks for turbulent
systems.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [200] [Non-linear Multi-objective Optimization with Probabilistic Branch and Bound](https://arxiv.org/abs/2506.04554)
*Hao Huang,Zelda B. Zabinsky*

Main category: math.OC

TL;DR: MOPBnB(so) is a simulation optimization algorithm for stochastic multi-objective problems, using single observations and neighboring solutions to estimate objectives. It outperforms NSGA-II and is computationally efficient.


<details>
  <summary>Details</summary>
Motivation: To address the computational intensity of multiple replications in stochastic multi-objective optimization by proposing a single-observation approach.

Method: Uses probabilistic branch and bound with single observations and neighboring solutions for estimation. Analyzes finite-time performance and asymptotic convergence.

Result: Outperforms NSGA-II and is computationally efficient compared to multiple-replication variants.

Conclusion: MOPBnB(so) is effective for approximating Pareto optimal sets in stochastic multi-objective problems with reduced computational cost.

Abstract: A multiple objective simulation optimization algorithm named Multiple
Objective Probabilistic Branch and Bound with Single Observation (MOPBnB(so))
is presented for approximating the Pareto optimal set and the associated
efficient frontier for stochastic multi-objective optimization problems.
MOPBnB(so) evaluates a noisy function exactly once at any solution and uses
neighboring solutions to estimate the objective functions, in contrast to a
variant that uses multiple replications at a solution to estimate the objective
functions. A finite-time performance analysis for deterministic multi-objective
problems provides a bound on the probability that MOPBnB(so) captures the
Pareto optimal set. Asymptotic convergence of MOPBnB(so) on stochastic problems
is derived, in that the algorithm captures the Pareto optimal set and the
estimations converge to the true objective function values. Numerical results
reveal that the variant with multiple replications is extremely intensive in
terms of computational resources compared to MOPBnB(so). In addition, numerical
results show that MOPBnB(so) outperforms a genetic algorithm NSGA-II on test
problems.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [201] [CORA: Coalitional Rational Advantage Decomposition for Multi-Agent Policy Gradients](https://arxiv.org/abs/2506.04265)
*Mengda Ji,Genjiu Xu,Liying Wang*

Main category: cs.MA

TL;DR: The paper addresses credit assignment in multi-agent reinforcement learning (MARL) by proposing CORA, a coalition-level method that ensures rational advantage decomposition and outperforms baselines.


<details>
  <summary>Details</summary>
Motivation: Current MARL methods often fail to account for distinct agent contributions due to global advantage sharing, leading to suboptimal policies. A coalition-level analysis is lacking.

Method: Proposes Coalitional Rational Advantage Decomposition (CORA), which evaluates coalitional advantages via marginal contributions and decomposes them using cooperative game theory's core solution. Random coalition sampling reduces computational overhead.

Result: CORA outperforms baselines in matrix games, differential games, and multi-agent benchmarks, especially in tasks with multiple local optima.

Conclusion: Coalition-aware credit assignment, as demonstrated by CORA, is crucial for improving MARL performance.

Abstract: This work focuses on the credit assignment problem in cooperative multi-agent
reinforcement learning (MARL). Sharing the global advantage among agents often
leads to suboptimal policy updates as it fails to account for the distinct
contributions of agents. Although numerous methods consider global or
individual contributions for credit assignment, a detailed analysis at the
coalition level remains lacking in many approaches. This work analyzes the
over-updating problem during multi-agent policy updates from a coalition-level
perspective. To address this issue, we propose a credit assignment method
called Coalitional Rational Advantage Decomposition (CORA). CORA evaluates
coalitional advantages via marginal contributions from all possible coalitions
and decomposes advantages using the core solution from cooperative game theory,
ensuring coalitional rationality. To reduce computational overhead, CORA
employs random coalition sampling. Experiments on matrix games, differential
games, and multi-agent collaboration benchmarks demonstrate that CORA
outperforms strong baselines, particularly in tasks with multiple local optima.
These findings highlight the importance of coalition-aware credit assignment
for improving MARL performance.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [202] [Nombre Effectif de Partis Politiques en Afrique: Une Nouvelle Méthode pour un Calcul Objectif et Institutionnellement Neutre](https://arxiv.org/abs/2506.04279)
*Adama Nouboukpo,Kodzo Michel Aladji,Muktar Bappa*

Main category: physics.soc-ph

TL;DR: The paper introduces two new measures for calculating party fragmentation in Africa, focusing on geography and demographics, to better reflect local political realities.


<details>
  <summary>Details</summary>
Motivation: Traditional measures like the ENP index fail to capture nuances of African politics, such as dominant parties and ethnic cleavages.

Method: Proposes two 'apolitical' measures based on population size and territorial area, with a minimum threshold of two parties.

Result: The new models offer a simpler, contextually relevant framework for analyzing African political systems, even in data-scarce settings.

Conclusion: This approach provides a valuable tool for understanding political fragmentation, with potential applications beyond Africa.

Abstract: Political fragmentation in Africa poses to a significant challenge to
effective governance and stability. Traditional measures of party system
fragmentation, such as the Effective Number of Parties (ENP) index, often fail
to capture the nuanced realities of African political landscapes, particularly
the influence of dominant parties, fluid party affiliations, and the impact of
ethnic and regional cleavages. To address these limitations, this paper
introduces two novel "apolitical" or "institutionally neutral" measures for
calculating the effective number of parties, focusing on geographical and
demographic dimensions, notably population size and territorial area. By
incorporating these local realities and ensuring a minimum threshold of two
parties, the proposed models offer a simpler and more contextually relevant
framework for understanding political dynamics in Africa, especially in
data-scarce environments. This approach provides a valuable tool for analyzing
and streamlining political systems, with potential for broader application
beyond the African context.

</details>


### [203] [Nombre Effectif de Partis Politiques en Afrique: Une Nouvelle Méthode pour un Calcul Objectif et Institutionnellement Neutre](https://arxiv.org/abs/2506.04279)
*Adama Nouboukpo,Kodzo Michel Aladji,Muktar Bappa*

Main category: physics.soc-ph

TL;DR: The paper introduces two new measures for calculating political fragmentation in Africa, focusing on geographical and demographic factors, to address the limitations of traditional methods like the ENP index.


<details>
  <summary>Details</summary>
Motivation: Traditional measures like the ENP index fail to capture the complexities of African political landscapes, such as dominant parties and ethnic cleavages.

Method: The paper proposes two novel 'apolitical' measures based on population size and territorial area, ensuring a minimum of two parties.

Result: The new models provide a simpler, contextually relevant framework for analyzing African political systems, even in data-scarce environments.

Conclusion: This approach offers a valuable tool for understanding political dynamics in Africa and has potential for broader application.

Abstract: Political fragmentation in Africa poses to a significant challenge to
effective governance and stability. Traditional measures of party system
fragmentation, such as the Effective Number of Parties (ENP) index, often fail
to capture the nuanced realities of African political landscapes, particularly
the influence of dominant parties, fluid party affiliations, and the impact of
ethnic and regional cleavages. To address these limitations, this paper
introduces two novel "apolitical" or "institutionally neutral" measures for
calculating the effective number of parties, focusing on geographical and
demographic dimensions, notably population size and territorial area. By
incorporating these local realities and ensuring a minimum threshold of two
parties, the proposed models offer a simpler and more contextually relevant
framework for understanding political dynamics in Africa, especially in
data-scarce environments. This approach provides a valuable tool for analyzing
and streamlining political systems, with potential for broader application
beyond the African context.

</details>


### [204] [What does making money have to do with crime?: A dive into the National Crime Victimization survey](https://arxiv.org/abs/2506.04240)
*Sydney Anuyah*

Main category: physics.soc-ph

TL;DR: The study analyzes how socioeconomic and demographic factors influence crime victimization types (violent vs. property) using data from 1992-2022. Higher income and education reduce violent crime odds, while men, younger individuals, and minorities face higher risks. Geographic patterns highlight suburban predictability and rural unpredictability, suggesting targeted interventions.


<details>
  <summary>Details</summary>
Motivation: To understand how income, education, employment, and demographics shape crime victimization types, informing targeted prevention strategies.

Method: Uses balanced classification splits and logistic regression models (Group A: socioeconomic factors; Group B: demographic controls) evaluated by F1-score.

Result: Higher income/education lower violent crime odds; men, youth, and minorities face higher risks. Suburban models predict best (accuracy 0.607, F1 0.590), while rural crime remains unpredictable.

Conclusion: Findings advocate for tailored interventions: educational investments in cities, economic support in rural areas, and demographic-aware prevention strategies.

Abstract: In this short article, I leverage the National Crime Victimization Survey
from 1992 to 2022 to examine how income, education, employment, and key
demographic factors shape the type of crime victims experience (violent vs
property). Using balanced classification splits and logistic regression models
evaluated by F1-score, there is an isolation of the socioeconomic drivers of
victimization "Group A" models and then an introduction of demographic factors
such as age, gender, race, and marital status controls called "Group B" models.
The results consistently proves that higher income and education lower the odds
of violent relative to property crime, while men younger individuals and racial
minorities face disproportionately higher violentcrime risks. On the geographic
spectrum, the suburban models achieve the strongest predictive performance with
an accuracy of 0.607 and F1 of 0.590, urban areas benefit from adding education
and employment predictors and crime in rural areas are still unpredictable
using these current factors. The patterns found in this study shows the need
for specific interventions like educational investments in metropolitan
settings economic support in rural communities and demographicaware prevention
strategies.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [205] [Improving AI-generated music with user-guided training](https://arxiv.org/abs/2506.04852)
*Vishwa Mohan Singh,Sai Anirudh Aryasomayajula,Ahan Chatterjee,Beste Aydemir,Rifat Mehreen Amin*

Main category: cs.SD

TL;DR: The paper proposes a human-computation approach to improve AI music generation by incorporating user feedback via a genetic algorithm, showing iterative improvements in user ratings.


<details>
  <summary>Details</summary>
Motivation: Current AI music generation models struggle with personalization due to training on fixed datasets, making user input interpretation challenging.

Method: A genetic algorithm aggregates user ratings to fine-tune the model, using feedback as the loss function.

Result: Pilot tests showed rating increases: 0.2 in the first iteration and 0.39 in the second over baseline.

Conclusion: User feedback integration via genetic algorithms can enhance AI music generation performance iteratively.

Abstract: AI music generation has advanced rapidly, with models like diffusion and
autoregressive algorithms enabling high-fidelity outputs. These tools can alter
styles, mix instruments, or isolate them. Since sound can be visualized as
spectrograms, image-generation algorithms can be applied to generate novel
music. However, these algorithms are typically trained on fixed datasets, which
makes it challenging for them to interpret and respond to user input
accurately. This is especially problematic because music is highly subjective
and requires a level of personalization that image generation does not provide.
In this work, we propose a human-computation approach to gradually improve the
performance of these algorithms based on user interactions. The
human-computation element involves aggregating and selecting user ratings to
use as the loss function for fine-tuning the model. We employ a genetic
algorithm that incorporates user feedback to enhance the baseline performance
of a model initially trained on a fixed dataset. The effectiveness of this
approach is measured by the average increase in user ratings with each
iteration. In the pilot test, the first iteration showed an average rating
increase of 0.2 compared to the baseline. The second iteration further improved
upon this, achieving an additional increase of 0.39 over the first iteration.

</details>


### [206] [Survey on the Evaluation of Generative Models in Music](https://arxiv.org/abs/2506.05104)
*Alexander Lerch,Claire Arthur,Nick Bryan-Kinns,Corey Ford,Qianyi Sun,Ashvala Vinay*

Main category: cs.SD

TL;DR: Review of evaluation methods for generative music systems, covering interdisciplinary perspectives.


<details>
  <summary>Details</summary>
Motivation: To systematically assess the evaluation targets, methodologies, and metrics for generative music systems.

Method: Interdisciplinary review of subjective/objective, qualitative/quantitative, and empirical/computational approaches.

Result: Identifies advantages and challenges from musicological, engineering, and HCI viewpoints.

Conclusion: Highlights the need for comprehensive evaluation frameworks in generative music research.

Abstract: Research on generative systems in music has seen considerable attention and
growth in recent years. A variety of attempts have been made to systematically
evaluate such systems. We provide an interdisciplinary review of the common
evaluation targets, methodologies, and metrics for the evaluation of both
system output and model usability, covering subjective and objective
approaches, qualitative and quantitative approaches, as well as empirical and
computational methods. We discuss the advantages and challenges of such
approaches from a musicological, an engineering, and an HCI perspective.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [207] [Optical Physics-Based Generative Models](https://arxiv.org/abs/2506.04357)
*Amirreza Ahmadnejad,Somayyeh Koohi*

Main category: physics.optics

TL;DR: The paper links optical physics equations to generative AI, showing how nonlinear optical models outperform linear ones in efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: To explore the connection between optical physics and generative models, leveraging natural self-organization principles for AI advancements.

Method: Analyzes six optical equations (linear and nonlinear), comparing their performance in generative tasks. Validates through experiments.

Result: Nonlinear models reduce parameters, improve stability, and achieve better performance (e.g., 40-60% memory reduction, 30-50% faster training).

Conclusion: The framework bridges optical physics and AI, enhancing both fields and enabling efficient optical computing.

Abstract: This paper establishes a comprehensive mathematical framework connecting
optical physics equations to generative models, demonstrating how light
propagation dynamics inspire powerful artificial intelligence approaches. We
analyze six fundamental optical equations, comparing linear models (Helmholtz,
dissipative wave, and Eikonal equations) with their nonlinear extensions
incorporating Kerr effects, cubic-quintic nonlinearities, and
intensity-dependent refractive indices. Our nonlinear optical models reveal
remarkable capabilities through natural self-organization principles. The
nonlinear Helmholtz model achieves 40-60% parameter reduction while maintaining
superior mode separation via self-focusing phenomena. The cubic-quintic
dissipative wave model prevents mode collapse through balanced
attractive-repulsive interactions, enabling stable soliton formation with
20-40% improved coverage. The intensity-dependent Eikonal model creates
adaptive pathways that dynamically respond to content, providing enhanced
controllability in conditional generation. Experimental validation demonstrates
consistent superiority over linear predecessors and traditional generative
approaches. The nonlinear Helmholtz model achieves FID scores of 0.0089 versus
1.0909 for linear versions, while the cubic-quintic model reaches 0.0156 FID
with exceptional stability. Memory usage drops 40-60% and training time
improves 30-50% due to inherent nonlinear stability properties. The framework
enables bidirectional benefits, advancing both generative AI and optical
physics through novel approaches to soliton analysis, wavefront control, and
refractive index reconstruction with 95% accuracy. This work reveals deep
connections between physical self-organization and artificial intelligence,
opening pathways toward efficient optical computing implementations.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [208] [Deep learning image burst stacking to reconstruct high-resolution ground-based solar observations](https://arxiv.org/abs/2506.04781)
*Christoph Schirninger,Robert Jarolim,Astrid M. Veronig,Christoph Kuckein*

Main category: astro-ph.SR

TL;DR: A deep learning method reconstructs 100 short exposure solar images into one high-quality image in real-time, outperforming current methods in robustness and perceptual quality.


<details>
  <summary>Details</summary>
Motivation: Ground-based solar telescopes face limitations due to Earth's turbulent atmosphere, requiring post-image corrections. Current methods struggle with strong turbulence and high computational costs.

Method: The approach uses unpaired image-to-image translation, training on degraded bursts with speckle reconstructions as references to improve robustness and generalization.

Result: The method shows enhanced perceptual quality, especially when speckle reconstructions have artifacts, and performs best with full image bursts.

Conclusion: The deep learning approach efficiently combines image information, offering real-time, high-quality reconstructions for solar observations.

Abstract: Large aperture ground based solar telescopes allow the solar atmosphere to be
resolved in unprecedented detail. However, observations are limited by Earths
turbulent atmosphere, requiring post image corrections. Current reconstruction
methods using short exposure bursts face challenges with strong turbulence and
high computational costs. We introduce a deep learning approach that
reconstructs 100 short exposure images into one high quality image in real
time. Using unpaired image to image translation, our model is trained on
degraded bursts with speckle reconstructions as references, improving
robustness and generalization. Our method shows an improved robustness in terms
of perceptual quality, especially when speckle reconstructions show artifacts.
An evaluation with a varying number of images per burst demonstrates that our
method makes efficient use of the combined image information and achieves the
best reconstructions when provided with the full image burst.

</details>


### [209] [Deep learning image burst stacking to reconstruct high-resolution ground-based solar observations](https://arxiv.org/abs/2506.04781)
*Christoph Schirninger,Robert Jarolim,Astrid M. Veronig,Christoph Kuckein*

Main category: astro-ph.SR

TL;DR: A deep learning method is introduced to reconstruct high-quality solar images from 100 short-exposure bursts in real-time, improving robustness and perceptual quality over traditional speckle reconstructions.


<details>
  <summary>Details</summary>
Motivation: Ground-based solar telescopes face limitations due to Earth's turbulent atmosphere, requiring post-image corrections. Current methods struggle with strong turbulence and high computational costs.

Method: The approach uses unpaired image-to-image translation, training on degraded bursts with speckle reconstructions as references to enhance robustness and generalization.

Result: The method shows improved perceptual quality, especially when speckle reconstructions exhibit artifacts, and performs best with the full image burst.

Conclusion: The deep learning approach efficiently combines image information from bursts, offering real-time, high-quality solar image reconstruction.

Abstract: Large aperture ground based solar telescopes allow the solar atmosphere to be
resolved in unprecedented detail. However, observations are limited by Earths
turbulent atmosphere, requiring post image corrections. Current reconstruction
methods using short exposure bursts face challenges with strong turbulence and
high computational costs. We introduce a deep learning approach that
reconstructs 100 short exposure images into one high quality image in real
time. Using unpaired image to image translation, our model is trained on
degraded bursts with speckle reconstructions as references, improving
robustness and generalization. Our method shows an improved robustness in terms
of perceptual quality, especially when speckle reconstructions show artifacts.
An evaluation with a varying number of images per burst demonstrates that our
method makes efficient use of the combined image information and achieves the
best reconstructions when provided with the full image burst.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [210] [ChemReservoir -- An Open-Source Framework for Chemically-Inspired Reservoir Computing](https://arxiv.org/abs/2506.04249)
*Mehmet Aziz Yirik,Jakob Lykke Andersen,Rolf Fagerberg,Daniel Merkle*

Main category: cs.CE

TL;DR: The paper introduces ChemReservoir, an open-source framework for chemically-inspired reservoir computing, addressing limitations of previous DNA-focused tools.


<details>
  <summary>Details</summary>
Motivation: Previous reservoir computing tools were limited to DNA chemistry and lacked maintenance, necessitating a general, open-source solution.

Method: Developed ChemReservoir, a framework for constructing and analyzing chemically-inspired reservoirs, with enhanced testing and reproducibility.

Result: ChemReservoir demonstrated stable performance in memory capacity tasks across various configurations.

Conclusion: ChemReservoir provides a versatile and reliable tool for chemically-inspired reservoir computing, overcoming prior limitations.

Abstract: Reservoir computing is a type of a recurrent neural network, mapping the
inputs into higher dimensional space using fixed and nonlinear dynamical
systems, called reservoirs. In the literature, there are various types of
reservoirs ranging from in-silico to in-vitro. In cheminformatics, previous
studies contributed to the field by developing simulation-based chemically
inspired in-silico reservoir models. Yahiro used a DNA-based chemical reaction
network as its reservoir and Nguyen developed a DNA chemistry-inspired tool
based on Gillespie algorithm. However, these software tools were designed
mainly with the focus on DNA chemistry and their maintenance status has limited
their current usability. Due to these limitations, there was a need for a
proper open-source tool. This study introduces ChemReservoir, an open-source
framework for chemically-inspired reservoir computing. In contrast to the
former studies focused on DNA-chemistry, ChemReservoir is a general framework
for the construction and analysis of chemically-inspired reservoirs, which also
addresses the limitations in these previous studies by ensuring enhanced
testing, evaluation, and reproducibility. The tool was evaluated using various
cycle-based reservoir topologies and demonstrated stable performance across a
range of configurations in memory capacity tasks.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [211] [GEM: Empowering LLM for both Embedding Generation and Language Understanding](https://arxiv.org/abs/2506.04344)
*Caojin Zhang,Qiang Zhang,Ke Li,Sai Vidyaranya Nuthalapati,Benyu Zhang,Jason Liu,Serena Li,Lizhu Zhang,Xiangjun Fan*

Main category: cs.CL

TL;DR: The paper proposes GEM, a self-supervised method enabling decoder-only LLMs to generate high-quality text embeddings without compromising their original text generation and reasoning abilities.


<details>
  <summary>Details</summary>
Motivation: Current systems often rely on separate embedding models for tasks like RAG, complicating the system and causing discrepancies. GEM aims to unify these capabilities within a single LLM.

Method: GEM inserts special tokens into text and manipulates attention masks to generate summarization embeddings, easily integrable into post-training or fine-tuning.

Result: Applied to LLMs (1B-8B parameters), GEM significantly improves text embedding performance (MTEB) with minimal impact on NLP tasks (MMLU).

Conclusion: GEM successfully equips LLMs with advanced text embedding capabilities while preserving their original NLP performance.

Abstract: Large decoder-only language models (LLMs) have achieved remarkable success in
generation and reasoning tasks, where they generate text responses given
instructions. However, many applications, e.g., retrieval augmented generation
(RAG), still rely on separate embedding models to generate text embeddings,
which can complicate the system and introduce discrepancies in understanding of
the query between the embedding model and LLMs. To address this limitation, we
propose a simple self-supervised approach, Generative Embedding large language
Model (GEM), that enables any large decoder-only LLM to generate high-quality
text embeddings while maintaining its original text generation and reasoning
capabilities. Our method inserts new special token(s) into a text body, and
generates summarization embedding of the text by manipulating the attention
mask. This method could be easily integrated into post-training or fine tuning
stages of any existing LLMs. We demonstrate the effectiveness of our approach
by applying it to two popular LLM families, ranging from 1B to 8B parameters,
and evaluating the transformed models on both text embedding benchmarks (MTEB)
and NLP benchmarks (MMLU). The results show that our proposed method
significantly improves the original LLMs on MTEB while having a minimal impact
on MMLU. Our strong results indicate that our approach can empower LLMs with
state-of-the-art text embedding capabilities while maintaining their original
NLP performance

</details>


### [212] [Hierarchical Text Classification Using Contrastive Learning Informed Path Guided Hierarchy](https://arxiv.org/abs/2506.04381)
*Neeraj Agrawal,Saurabh Kumar,Priyanka Bhatt,Tanishka Agarwal*

Main category: cs.CL

TL;DR: HTC-CLIP combines label hierarchy encoding and text encoder guidance using contrastive learning, improving performance over existing models.


<details>
  <summary>Details</summary>
Motivation: Existing HTC models handle label hierarchy and text encoding separately or guide hierarchy in the encoder, but these approaches are complementary.

Method: HTC-CLIP learns hierarchy-aware text and path-guided hierarchy representations via contrastive learning, pooling both during inference.

Result: HTC-CLIP improves Macro F1 scores by 0.99-2.37% over state-of-the-art models on two benchmark datasets.

Conclusion: Combining complementary approaches in HTC-CLIP enhances performance, demonstrating the value of integrating hierarchy and text representations.

Abstract: Hierarchical Text Classification (HTC) has recently gained traction given the
ability to handle complex label hierarchy. This has found applications in
domains like E- commerce, customer care and medicine industry among other
real-world applications. Existing HTC models either encode label hierarchy
separately and mix it with text encoding or guide the label hierarchy structure
in the text encoder. Both approaches capture different characteristics of label
hierarchy and are complementary to each other. In this paper, we propose a
Hierarchical Text Classification using Contrastive Learning Informed Path
guided hierarchy (HTC-CLIP), which learns hierarchy-aware text representation
and text informed path guided hierarchy representation using contrastive
learning. During the training of HTC-CLIP, we learn two different sets of class
probabilities distributions and during inference, we use the pooled output of
both probabilities for each class to get the best of both representations. Our
results show that the two previous approaches can be effectively combined into
one architecture to achieve improved performance. Tests on two public benchmark
datasets showed an improvement of 0.99 - 2.37% in Macro F1 score using HTC-CLIP
over the existing state-of-the-art models.

</details>


### [213] [Building a Few-Shot Cross-Domain Multilingual NLU Model for Customer Care](https://arxiv.org/abs/2506.04389)
*Saurabh Kumar,Sourav Bansal,Neeraj Agrawal,Priyanka Bhatt*

Main category: cs.CL

TL;DR: Proposes an embedder-cum-classifier model for few-shot intent detection in customer care, improving accuracy by 20-23% over SOTA models.


<details>
  <summary>Details</summary>
Motivation: Addresses the challenge of limited annotated domain-specific data in customer care, aiming to generalize intent classification across domains (geography, channel, language).

Method: Uses a supervised fine-tuning approach with isotropic regularizers for domain-specific embedder and multilingual knowledge distillation for cross-domain generalization.

Result: Achieves 20-23% higher accuracy in few-shot intent detection on Canada and Mexico e-commerce datasets compared to SOTA models.

Conclusion: The proposed model effectively generalizes intent classification across domains with minimal labeled data, offering practical value for customer care automation.

Abstract: Customer care is an essential pillar of the e-commerce shopping experience
with companies spending millions of dollars each year, employing automation and
human agents, across geographies (like US, Canada, Mexico, Chile), channels
(like Chat, Interactive Voice Response (IVR)), and languages (like English,
Spanish). SOTA pre-trained models like multilingual-BERT, fine-tuned on
annotated data have shown good performance in downstream tasks relevant to
Customer Care. However, model performance is largely subject to the
availability of sufficient annotated domain-specific data. Cross-domain
availability of data remains a bottleneck, thus building an intent classifier
that generalizes across domains (defined by channel, geography, and language)
with only a few annotations, is of great practical value. In this paper, we
propose an embedder-cum-classifier model architecture which extends
state-of-the-art domain-specific models to other domains with only a few
labeled samples. We adopt a supervised fine-tuning approach with isotropic
regularizers to train a domain-specific sentence embedder and a multilingual
knowledge distillation strategy to generalize this embedder across multiple
domains. The trained embedder, further augmented with a simple linear
classifier can be deployed for new domains. Experiments on Canada and Mexico
e-commerce Customer Care dataset with few-shot intent detection show an
increase in accuracy by 20-23% against the existing state-of-the-art
pre-trained models.

</details>


### [214] [MedAgentGym: Training LLM Agents for Code-Based Medical Reasoning at Scale](https://arxiv.org/abs/2506.04405)
*Ran Xu,Yuchen Zhuang,Yishan Zhong,Yue Yu,Xiangru Tang,Hang Wu,May D. Wang,Peifeng Ruan,Donghan Yang,Tao Wang,Guanghua Xiao,Carl Yang,Yang Xie,Wenqi Shi*

Main category: cs.CL

TL;DR: MedAgentGYM is a training environment for enhancing medical reasoning in LLMs, featuring 72,413 tasks across 129 categories. It benchmarks 30+ LLMs, showing performance gaps, and enables Med-Copilot-7B to achieve significant gains via fine-tuning and reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: To address the lack of publicly available environments for improving LLMs' medical reasoning and coding capabilities, using real-world biomedical scenarios.

Method: Developed MedAgentGYM with executable coding tasks, interactive feedback, and scalable training. Benchmarked 30+ LLMs and fine-tuned Med-Copilot-7B.

Result: Performance gaps between commercial and open-source LLMs. Med-Copilot-7B improved by +36.44% (fine-tuning) and +42.47% (reinforcement learning), rivaling GPT-4.

Conclusion: MedAgentGYM provides a unified platform for developing LLM-based biomedical coding assistants, offering benchmarks and training resources.

Abstract: We introduce MedAgentGYM, the first publicly available training environment
designed to enhance coding-based medical reasoning capabilities in large
language model (LLM) agents. MedAgentGYM comprises 72,413 task instances across
129 categories derived from authentic real-world biomedical scenarios. Tasks
are encapsulated within executable coding environments, each featuring detailed
task descriptions, interactive feedback mechanisms, verifiable ground-truth
annotations, and scalable training trajectory generation. Extensive
benchmarking of over 30 LLMs reveals a notable performance disparity between
commercial API-based models and open-source counterparts. Leveraging
MedAgentGYM, Med-Copilot-7B achieves substantial performance gains through
supervised fine-tuning (+36.44%) and continued reinforcement learning
(+42.47%), emerging as an affordable and privacy-preserving alternative
competitive with gpt-4o. By offering both a comprehensive benchmark and
accessible, expandable training resources within unified execution
environments, MedAgentGYM delivers an integrated platform to develop LLM-based
coding assistants for advanced biomedical research and practice.

</details>


### [215] [Watermarking Degrades Alignment in Language Models: Analysis and Mitigation](https://arxiv.org/abs/2506.04462)
*Apurv Verma,NhatHai Phan,Shubhendu Trivedi*

Main category: cs.CL

TL;DR: The paper analyzes how watermarking techniques (Gumbel and KGW) affect LLM alignment properties (truthfulness, safety, helpfulness), identifies degradation patterns, and proposes Alignment Resampling (AR) to mitigate these issues.


<details>
  <summary>Details</summary>
Motivation: To understand the underexamined effects of watermarking on LLM alignment properties and address the resulting degradation patterns.

Method: Systematic analysis of two watermarking approaches (Gumbel and KGW) across four aligned LLMs, identifying degradation patterns (guard attenuation and amplification). Proposes AR, an inference-time sampling method using an external reward model.

Result: Watermarking causes alignment degradation; AR with 2-4 samples recovers or surpasses baseline alignment scores. Modified Gumbel maintains detectability while improving diversity.

Conclusion: AR effectively balances watermark strength and alignment, offering a practical solution for deploying watermarked LLMs responsibly.

Abstract: Watermarking techniques for large language models (LLMs) can significantly
impact output quality, yet their effects on truthfulness, safety, and
helpfulness remain critically underexamined. This paper presents a systematic
analysis of how two popular watermarking approaches-Gumbel and KGW-affect these
core alignment properties across four aligned LLMs. Our experiments reveal two
distinct degradation patterns: guard attenuation, where enhanced helpfulness
undermines model safety, and guard amplification, where excessive caution
reduces model helpfulness. These patterns emerge from watermark-induced shifts
in token distribution, surfacing the fundamental tension that exists between
alignment objectives.
  To mitigate these degradations, we propose Alignment Resampling (AR), an
inference-time sampling method that uses an external reward model to restore
alignment. We establish a theoretical lower bound on the improvement in
expected reward score as the sample size is increased and empirically
demonstrate that sampling just 2-4 watermarked generations effectively recovers
or surpasses baseline (unwatermarked) alignment scores. To overcome the limited
response diversity of standard Gumbel watermarking, our modified implementation
sacrifices strict distortion-freeness while maintaining robust detectability,
ensuring compatibility with AR. Experimental results confirm that AR
successfully recovers baseline alignment in both watermarking approaches, while
maintaining strong watermark detectability. This work reveals the critical
balance between watermark strength and model alignment, providing a simple
inference-time solution to responsibly deploy watermarked LLMs in practice.

</details>


### [216] [Safe: Enhancing Mathematical Reasoning in Large Language Models via Retrospective Step-aware Formal Verification](https://arxiv.org/abs/2506.04592)
*Chengwu Liu,Ye Yuan,Yichun Yin,Yan Xu,Xin Xu,Zaoyu Chen,Yasheng Wang,Lifeng Shang,Qun Liu,Ming Zhang*

Main category: cs.CL

TL;DR: The paper introduces $Safe$, a formal verification framework using Lean 4 to detect and mitigate hallucinations in Chain-of-Thought reasoning by providing step-aware proofs.


<details>
  <summary>Details</summary>
Motivation: To address the lack of verifiable evidence in current methods for detecting hallucinations in Chain-of-Thought prompting, inspired by the rigor of mathematical proofs.

Method: Proposes $Safe$, a retrospective, step-aware formal verification framework using Lean 4 to articulate and verify reasoning steps.

Result: Demonstrates significant performance improvement and introduces $FormalStep$, a benchmark for step correctness with 30,809 formal statements.

Conclusion: The work pioneers the use of Lean 4 for verifying LLM-generated content, aligning with formal languages' original purpose of ensuring robust proof foundations.

Abstract: Chain-of-Thought (CoT) prompting has become the de facto method to elicit
reasoning capabilities from large language models (LLMs). However, to mitigate
hallucinations in CoT that are notoriously difficult to detect, current methods
such as process reward models (PRMs) or self-consistency operate as opaque
boxes and do not provide checkable evidence for their judgments, possibly
limiting their effectiveness. To address this issue, we draw inspiration from
the idea that "the gold standard for supporting a mathematical claim is to
provide a proof". We propose a retrospective, step-aware formal verification
framework $Safe$. Rather than assigning arbitrary scores, we strive to
articulate mathematical claims in formal mathematical language Lean 4 at each
reasoning step and provide formal proofs to identify hallucinations. We
evaluate our framework $Safe$ across multiple language models and various
mathematical datasets, demonstrating a significant performance improvement
while offering interpretable and verifiable evidence. We also propose
$FormalStep$ as a benchmark for step correctness theorem proving with $30,809$
formal statements. To the best of our knowledge, our work represents the first
endeavor to utilize formal mathematical language Lean 4 for verifying natural
language content generated by LLMs, aligning with the reason why formal
mathematical languages were created in the first place: to provide a robust
foundation for hallucination-prone human-written proofs.

</details>


### [217] [Static Word Embeddings for Sentence Semantic Representation](https://arxiv.org/abs/2506.04624)
*Takashi Wada,Yuki Hirakawa,Ryotaro Shimizu,Takahiro Kawashima,Yuki Saito*

Main category: cs.CL

TL;DR: Proposes static word embeddings optimized for sentence semantics, outperforming existing models and rivaling basic Sentence Transformers.


<details>
  <summary>Details</summary>
Motivation: To improve sentence semantic representation with computationally efficient static word embeddings.

Method: Extracts word embeddings from a pre-trained Sentence Transformer, enhances them with sentence-level PCA, and refines via knowledge distillation or contrastive learning. Sentences are represented by averaging word embeddings.

Result: Substantially outperforms existing static models on semantic tasks and rivals basic Sentence Transformers in some datasets.

Conclusion: The method effectively removes irrelevant word embedding components and adjusts vector norms based on word influence, enhancing sentence semantics.

Abstract: We propose new static word embeddings optimised for sentence semantic
representation. We first extract word embeddings from a pre-trained Sentence
Transformer, and improve them with sentence-level principal component analysis,
followed by either knowledge distillation or contrastive learning. During
inference, we represent sentences by simply averaging word embeddings, which
requires little computational cost. We evaluate models on both monolingual and
cross-lingual tasks and show that our model substantially outperforms existing
static models on sentence semantic tasks, and even rivals a basic Sentence
Transformer model (SimCSE) on some data sets. Lastly, we perform a variety of
analyses and show that our method successfully removes word embedding
components that are irrelevant to sentence semantics, and adjusts the vector
norms based on the influence of words on sentence semantics.

</details>


### [218] [Recycling the Web: A Method to Enhance Pre-training Data Quality and Quantity for Language Models](https://arxiv.org/abs/2506.04689)
*Thao Nguyen,Yang Li,Olga Golovneva,Luke Zettlemoyer,Sewoong Oh,Ludwig Schmidt,Xian Li*

Main category: cs.CL

TL;DR: REWIRE recycles low-quality web data by enriching it, improving model performance across tasks compared to using only filtered data.


<details>
  <summary>Details</summary>
Motivation: Address the 'data wall' in pre-training by recycling discarded low-quality web data to supplement high-quality data.

Method: Proposes REWIRE, a method to rewrite and enrich low-quality documents for training, mixing them with high-quality texts.

Result: Improves performance by 1.0-2.5 percentage points across 22 tasks and outperforms other synthetic data methods.

Conclusion: Recycling web texts is a simple and effective way to scale pre-training data.

Abstract: Scaling laws predict that the performance of large language models improves
with increasing model size and data size. In practice, pre-training has been
relying on massive web crawls, using almost all data sources publicly available
on the internet so far. However, this pool of natural data does not grow at the
same rate as the compute supply. Furthermore, the availability of high-quality
texts is even more limited: data filtering pipelines often remove up to 99% of
the initial web scrapes to achieve state-of-the-art. To address the "data wall"
of pre-training scaling, our work explores ways to transform and recycle data
discarded in existing filtering processes. We propose REWIRE, REcycling the Web
with guIded REwrite, a method to enrich low-quality documents so that they
could become useful for training. This in turn allows us to increase the
representation of synthetic data in the final pre-training set. Experiments at
1B, 3B and 7B scales of the DCLM benchmark show that mixing high-quality raw
texts and our rewritten texts lead to 1.0, 1.3 and 2.5 percentage points
improvement respectively across 22 diverse tasks, compared to training on only
filtered web data. Training on the raw-synthetic data mix is also more
effective than having access to 2x web data. Through further analysis, we
demonstrate that about 82% of the mixed in texts come from transforming
lower-quality documents that would otherwise be discarded. REWIRE also
outperforms related approaches of generating synthetic data, including
Wikipedia-style paraphrasing, question-answer synthesizing and knowledge
extraction. These results suggest that recycling web texts holds the potential
for being a simple and effective approach for scaling pre-training data.

</details>


### [219] [Fine-Grained Interpretation of Political Opinions in Large Language Models](https://arxiv.org/abs/2506.04774)
*Jingyu Hu,Mengyue Yang,Mengnan Du,Weiru Liu*

Main category: cs.CL

TL;DR: The paper explores LLMs' political opinions by probing their internal mechanisms and extending single-axis analysis to multi-dimensions using interpretable representation engineering.


<details>
  <summary>Details</summary>
Motivation: Misalignment between LLMs' responses and internal intentions, and reliance on single-axis concepts leading to confounds, motivate the study.

Method: A four-dimensional political learning framework and dataset are designed for fine-grained concept vector learning, tested on eight LLMs with three representation engineering techniques.

Result: Vectors disentangle political concept confounds, show semantic meaning in detection tasks, and enable intervention for responses with different political leanings.

Conclusion: The approach enhances transparency and control over LLMs' political opinions through multi-dimensional analysis and intervention.

Abstract: Studies of LLMs' political opinions mainly rely on evaluations of their
open-ended responses. Recent work indicates that there is a misalignment
between LLMs' responses and their internal intentions. This motivates us to
probe LLMs' internal mechanisms and help uncover their internal political
states. Additionally, we found that the analysis of LLMs' political opinions
often relies on single-axis concepts, which can lead to concept confounds. In
this work, we extend the single-axis to multi-dimensions and apply
interpretable representation engineering techniques for more transparent LLM
political concept learning. Specifically, we designed a four-dimensional
political learning framework and constructed a corresponding dataset for
fine-grained political concept vector learning. These vectors can be used to
detect and intervene in LLM internals. Experiments are conducted on eight
open-source LLMs with three representation engineering techniques. Results show
these vectors can disentangle political concept confounds. Detection tasks
validate the semantic meaning of the vectors and show good generalization and
robustness in OOD settings. Intervention Experiments show these vectors can
intervene in LLMs to generate responses with different political leanings.

</details>


### [220] [Towards LLM-Centric Multimodal Fusion: A Survey on Integration Strategies and Techniques](https://arxiv.org/abs/2506.04788)
*Jisu An,Junseok Lee,Jeoungeun Lee,Yongseok Son*

Main category: cs.CL

TL;DR: The paper surveys Multimodal Large Language Models (MLLMs), analyzing integration methods, representation learning, and training paradigms, proposing a classification framework based on 125 models.


<details>
  <summary>Details</summary>
Motivation: To address the gap in systematic understanding of how diverse modalities connect to language backbones in MLLMs.

Method: Examines architectural strategies, representation learning techniques, and training paradigms, analyzing 125 MLLMs.

Result: Identifies emerging patterns and proposes a taxonomy for current integration techniques.

Conclusion: Provides insights to guide future development of robust multimodal integration strategies for pre-trained models.

Abstract: The rapid progress of Multimodal Large Language Models(MLLMs) has transformed
the AI landscape. These models combine pre-trained LLMs with various modality
encoders. This integration requires a systematic understanding of how different
modalities connect to the language backbone. Our survey presents an LLM-centric
analysis of current approaches. We examine methods for transforming and
aligning diverse modal inputs into the language embedding space. This addresses
a significant gap in existing literature. We propose a classification framework
for MLLMs based on three key dimensions. First, we examine architectural
strategies for modality integration. This includes both the specific
integration mechanisms and the fusion level. Second, we categorize
representation learning techniques as either joint or coordinate
representations. Third, we analyze training paradigms, including training
strategies and objective functions. By examining 125 MLLMs developed between
2021 and 2025, we identify emerging patterns in the field. Our taxonomy
provides researchers with a structured overview of current integration
techniques. These insights aim to guide the development of more robust
multimodal integration strategies for future models built on pre-trained
foundations.

</details>


### [221] [Verbose ListOps (VLO): Beyond Long Context -- Unmasking LLM's Reasoning Blind Spots](https://arxiv.org/abs/2506.04907)
*Alex Pan,Mary-Anne Williams*

Main category: cs.CL

TL;DR: Verbose ListOps is a new benchmark testing LLMs' nested narrative reasoning by converting ListOps into stories, revealing their struggle with state management amidst distractions.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks fail to test LLMs' nested reasoning adequately, masking their limitations in real-world text interpretation.

Method: Verbose ListOps transforms ListOps computations into lengthy stories, withholding intermediate results to force internal computation and state management.

Result: Leading LLMs perform poorly on Verbose ListOps at modest narrative lengths, despite solving raw ListOps easily.

Conclusion: Addressing this failure is crucial for real-world applications, and Verbose ListOps enables targeted reasoning improvements beyond context expansion.

Abstract: Large Language Models (LLMs), whilst great at extracting facts from text,
struggle with nested narrative reasoning. Existing long context and multi-hop
QA benchmarks inadequately test this, lacking realistic distractors or failing
to decouple context length from reasoning complexity, masking a fundamental LLM
limitation. We introduce Verbose ListOps, a novel benchmark that
programmatically transposes ListOps computations into lengthy, coherent
stories. This uniquely forces internal computation and state management of
nested reasoning problems by withholding intermediate results, and offers
fine-grained controls for both narrative size \emph{and} reasoning difficulty.
Whilst benchmarks like LongReason (2025) advance approaches for synthetically
expanding the context size of multi-hop QA problems, Verbose ListOps pinpoints
a specific LLM vulnerability: difficulty in state management for nested
sub-reasoning amongst semantically-relevant, distracting narrative. Our
experiments show that leading LLMs (e.g., OpenAI o4, Gemini 2.5 Pro) collapse
in performance on Verbose ListOps at modest (~10k token) narrative lengths,
despite effortlessly solving raw ListOps equations. Addressing this failure is
paramount for real-world text interpretation which requires identifying key
reasoning points, tracking conceptual intermediate results, and filtering
irrelevant information. Verbose ListOps, and its extensible generation
framework thus enables targeted reasoning enhancements beyond mere
context-window expansion; a critical step to automating the world's knowledge
work.

</details>


### [222] [Controlling Summarization Length Through EOS Token Weighting](https://arxiv.org/abs/2506.05017)
*Zeno Belligoli,Emmanouil Stergiadis,Eran Fainman,Ilya Gusev*

Main category: cs.CL

TL;DR: A simple method to control text length in summaries by adjusting EOS token prediction in loss computation, compatible with various models.


<details>
  <summary>Details</summary>
Motivation: Existing methods for controlling text length are complex and incompatible with pre-trained models.

Method: Increase importance of predicting the EOS token in cross-entropy loss, making it architecture-agnostic.

Result: Effective length control in summaries without quality loss, tested on encoder-decoder and GPT-style models.

Conclusion: The proposed method is simple, versatile, and maintains summary quality while controlling length.

Abstract: Controlling the length of generated text can be crucial in various
text-generation tasks, including summarization. Existing methods often require
complex model alterations, limiting compatibility with pre-trained models. We
address these limitations by developing a simple approach for controlling the
length of automatic text summaries by increasing the importance of correctly
predicting the EOS token in the cross-entropy loss computation. The proposed
methodology is agnostic to architecture and decoding algorithms and orthogonal
to other inference-time techniques to control the generation length. We tested
it with encoder-decoder and modern GPT-style LLMs, and show that this method
can control generation length, often without affecting the quality of the
summary.

</details>


### [223] [DiCoRe: Enhancing Zero-shot Event Detection via Divergent-Convergent LLM Reasoning](https://arxiv.org/abs/2506.05128)
*Tanmay Parekh,Kartik Mehta,Ninareh Mehrabi,Kai-Wei Chang,Nanyun Peng*

Main category: cs.CL

TL;DR: DiCoRe, a divergent-convergent reasoning framework, improves zero-shot event detection by combining open-ended event discovery (Dreamer) and task-aligned reasoning (Grounder), outperforming baselines by 4-7% F1.


<details>
  <summary>Details</summary>
Motivation: Zero-shot event detection is challenging for LLMs due to complex event ontologies and domain-specific triggers. DiCoRe addresses this by decoupling the task into divergent and convergent reasoning.

Method: DiCoRe uses Dreamer for open-ended event discovery (divergent reasoning) and Grounder for task-aligned reasoning (convergent reasoning) with constrained decoding. An LLM-Judge verifies outputs.

Result: DiCoRe outperforms baselines on six datasets across five domains, achieving 4-7% average F1 gains.

Conclusion: DiCoRe is a robust zero-shot event detection framework, enhancing LLM utility for domain-specific tasks.

Abstract: Zero-shot Event Detection (ED), the task of identifying event mentions in
natural language text without any training data, is critical for document
understanding in specialized domains. Understanding the complex event ontology,
extracting domain-specific triggers from the passage, and structuring them
appropriately overloads and limits the utility of Large Language Models (LLMs)
for zero-shot ED. To this end, we propose DiCoRe, a divergent-convergent
reasoning framework that decouples the task of ED using Dreamer and Grounder.
Dreamer encourages divergent reasoning through open-ended event discovery,
which helps to boost event coverage. Conversely, Grounder introduces convergent
reasoning to align the free-form predictions with the task-specific
instructions using finite-state machine guided constrained decoding.
Additionally, an LLM-Judge verifies the final outputs to ensure high precision.
Through extensive experiments on six datasets across five domains and nine
LLMs, we demonstrate how DiCoRe consistently outperforms prior zero-shot,
transfer-learning, and reasoning baselines, achieving 4-7% average F1 gains
over the best baseline -- establishing DiCoRe as a strong zero-shot ED
framework.

</details>


### [224] [Counterfactual reasoning: an analysis of in-context emergence](https://arxiv.org/abs/2506.05188)
*Moritz Miller,Bernhard Schölkopf,Siyuan Guo*

Main category: cs.CL

TL;DR: The paper explores in-context counterfactual reasoning in large-scale neural language models, demonstrating their ability to predict outcomes under hypothetical scenarios using a synthetic linear regression task.


<details>
  <summary>Details</summary>
Motivation: To understand how language models perform in-context learning and counterfactual reasoning without parameter updates, focusing on noise abduction in a controlled setup.

Method: The study uses a synthetic linear regression task requiring noise abduction, analyzing the role of self-attention, model depth, and pre-training data diversity in Transformers.

Result: Language models can perform counterfactual reasoning in controlled setups, with findings extending to sequential data and hinting at potential for counterfactual story generation.

Conclusion: Transformers are capable of counterfactual reasoning, with performance driven by self-attention, depth, and data diversity, suggesting broader applications like story generation.

Abstract: Large-scale neural language models (LMs) exhibit remarkable performance in
in-context learning: the ability to learn and reason the input context on the
fly without parameter update. This work studies in-context counterfactual
reasoning in language models, that is, to predict the consequences of changes
under hypothetical scenarios. We focus on studying a well-defined synthetic
setup: a linear regression task that requires noise abduction, where accurate
prediction is based on inferring and copying the contextual noise from factual
observations. We show that language models are capable of counterfactual
reasoning in this controlled setup and provide insights that counterfactual
reasoning for a broad class of functions can be reduced to a transformation on
in-context observations; we find self-attention, model depth, and data
diversity in pre-training drive performance in Transformers. More
interestingly, our findings extend beyond regression tasks and show that
Transformers can perform noise abduction on sequential data, providing
preliminary evidence on the potential for counterfactual story generation. Our
code is available under
https://github.com/moXmiller/counterfactual-reasoning.git .

</details>


### [225] [The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly Licensed Text](https://arxiv.org/abs/2506.05209)
*Nikhil Kandpal,Brian Lester,Colin Raffel,Sebastian Majstorovic,Stella Biderman,Baber Abbasi,Luca Soldaini,Enrico Shippole,A. Feder Cooper,Aviya Skowron,John Kirchenbauer,Shayne Longpre,Lintang Sutawika,Alon Albalak,Zhenlin Xu,Guilherme Penedo,Loubna Ben Allal,Elie Bakouch,John David Pressman,Honglu Fan,Dashiell Stander,Guangyu Song,Aaron Gokaslan,Tom Goldstein,Brian R. Bartoldson,Bhavya Kailkhura,Tyler Murray*

Main category: cs.CL

TL;DR: The paper introduces the Common Pile v0.1, an 8TB openly licensed text dataset for LLM pretraining, addressing ethical and legal concerns of unlicensed data. Two 7B parameter models trained on this dataset perform competitively.


<details>
  <summary>Details</summary>
Motivation: To mitigate intellectual property and ethical issues associated with unlicensed text in LLM training by creating a high-quality, openly licensed dataset.

Method: Collected and curated 8TB of openly licensed text from 30 diverse sources. Trained two 7B parameter LLMs (Comma v0.1-1T and Comma v0.1-2T) on subsets of this dataset.

Result: The trained models achieved competitive performance compared to LLMs like Llama 1 and 2 7B, which use unlicensed text.

Conclusion: The Common Pile v0.1 demonstrates that openly licensed datasets can support performant LLM training, offering a viable alternative to unlicensed data.

Abstract: Large language models (LLMs) are typically trained on enormous quantities of
unlicensed text, a practice that has led to scrutiny due to possible
intellectual property infringement and ethical concerns. Training LLMs on
openly licensed text presents a first step towards addressing these issues, but
prior data collection efforts have yielded datasets too small or low-quality to
produce performant LLMs. To address this gap, we collect, curate, and release
the Common Pile v0.1, an eight terabyte collection of openly licensed text
designed for LLM pretraining. The Common Pile comprises content from 30 sources
that span diverse domains including research papers, code, books,
encyclopedias, educational materials, audio transcripts, and more. Crucially,
we validate our efforts by training two 7 billion parameter LLMs on text from
the Common Pile: Comma v0.1-1T and Comma v0.1-2T, trained on 1 and 2 trillion
tokens respectively. Both models attain competitive performance to LLMs trained
on unlicensed text with similar computational budgets, such as Llama 1 and 2
7B. In addition to releasing the Common Pile v0.1 itself, we also release the
code used in its creation as well as the training mixture and checkpoints for
the Comma v0.1 models.

</details>


### [226] [ProRefine: Inference-time Prompt Refinement with Textual Feedback](https://arxiv.org/abs/2506.05305)
*Deepak Pandita,Tharindu Cyril Weerasooriya,Ankit Parag Shah,Christopher M. Homan,Wei Wei*

Main category: cs.CL

TL;DR: ProRefine is a prompt optimization method that improves multi-agent workflows by refining prompts dynamically using LLM feedback, outperforming baselines by 3-37% on reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Agentic workflows often suffer from error propagation and poor performance due to ineffective prompts, limiting reliability and scalability.

Method: ProRefine leverages LLM feedback to dynamically optimize prompts for multi-step reasoning without additional training or labels.

Result: ProRefine outperforms zero-shot Chain-of-Thought baselines by 3-37% on five mathematical reasoning datasets, enabling smaller models to match larger ones.

Conclusion: ProRefine enhances accuracy and scalability, democratizing access to high-performing AI by making smaller models competitive.

Abstract: Agentic workflows, where multiple AI agents collaborate to accomplish complex
tasks like reasoning or planning, are becoming increasingly prevalent. However,
these workflows often suffer from error propagation and sub-optimal
performance, largely due to poorly designed prompts that fail to effectively
guide individual agents. This is a critical problem because it limits the
reliability and scalability of these powerful systems. We introduce ProRefine,
an innovative inference-time prompt optimization method that leverages textual
feedback from large language models (LLMs) to address this challenge. ProRefine
dynamically refines prompts for multi-step reasoning tasks without additional
training or ground truth labels. Evaluated on five benchmark mathematical
reasoning datasets, ProRefine significantly surpasses zero-shot
Chain-of-Thought baselines by 3 to 37 percentage points. This approach not only
boosts accuracy but also allows smaller models to match the performance of
larger ones, highlighting its potential for efficient and scalable AI
deployment, and democratizing access to high-performing AI.

</details>


### [227] [Constrained Entropic Unlearning: A Primal-Dual Framework for Large Language Models](https://arxiv.org/abs/2506.05314)
*Taha Entesari,Arman Hatami,Rinat Khaziev,Anil Ramakrishna,Mahyar Fazlyab*

Main category: cs.CL

TL;DR: The paper proposes a constrained optimization approach for unlearning in LLMs, using a novel logit-margin flattening loss for forgetting and a hard constraint for retention, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing unlearning methods for LLMs suffer from unstable optimization and degraded performance due to scalarized loss formulations, especially under aggressive forgetting.

Method: The method formulates unlearning as a constrained optimization problem, using a softmax-free logit-margin flattening loss for forgetting and a hard constraint for retention, solved via a primal-dual algorithm.

Result: The approach outperforms state-of-the-art baselines on TOFU and MUSE benchmarks, effectively removing targeted information while preserving utility.

Conclusion: The proposed method provides a stable and efficient solution for LLM unlearning, balancing forgetting and retention effectively.

Abstract: Large Language Models (LLMs) deployed in real-world settings increasingly
face the need to unlearn sensitive, outdated, or proprietary information.
Existing unlearning methods typically formulate forgetting and retention as a
regularized trade-off, combining both objectives into a single scalarized loss.
This often leads to unstable optimization and degraded performance on retained
data, especially under aggressive forgetting. We propose a new formulation of
LLM unlearning as a constrained optimization problem: forgetting is enforced
via a novel logit-margin flattening loss that explicitly drives the output
distribution toward uniformity on a designated forget set, while retention is
preserved through a hard constraint on a separate retain set. Compared to
entropy-based objectives, our loss is softmax-free, numerically stable, and
maintains non-vanishing gradients, enabling more efficient and robust
optimization. We solve the constrained problem using a scalable primal-dual
algorithm that exposes the trade-off between forgetting and retention through
the dynamics of the dual variable. Evaluations on the TOFU and MUSE benchmarks
across diverse LLM architectures demonstrate that our approach consistently
matches or exceeds state-of-the-art baselines, effectively removing targeted
information while preserving downstream utility.

</details>


### [228] [Search Arena: Analyzing Search-Augmented LLMs](https://arxiv.org/abs/2506.05334)
*Mihran Miroyan,Tsung-Han Wu,Logan King,Tianle Li,Jiayi Pan,Xinyan Hu,Wei-Lin Chiang,Anastasios N. Angelopoulos,Trevor Darrell,Narges Norouzi,Joseph E. Gonzalez*

Main category: cs.CL

TL;DR: Search Arena introduces a large-scale dataset for evaluating search-augmented LLMs, revealing user preferences for citations and source reliability, while showing web search's impact on performance.


<details>
  <summary>Details</summary>
Motivation: To address the lack of large-scale, diverse datasets for analyzing search-augmented LLMs, which are limited to static, single-turn questions.

Method: Crowd-sourced dataset of 24,000 multi-turn interactions with search-augmented LLMs, including 12,000 human preference votes, analyzed for citation and source preferences.

Result: User preferences favor citations and community-driven sources, but citations don't always support claims. Web search improves non-search settings but degrades search settings if relying only on parametric knowledge.

Conclusion: The dataset supports future research, highlighting gaps in perceived credibility and the need for diverse, reliable sources in search-augmented LLMs.

Abstract: Search-augmented language models combine web search with Large Language
Models (LLMs) to improve response groundedness and freshness. However,
analyzing these systems remains challenging: existing datasets are limited in
scale and narrow in scope, often constrained to static, single-turn,
fact-checking questions. In this work, we introduce Search Arena, a
crowd-sourced, large-scale, human-preference dataset of over 24,000 paired
multi-turn user interactions with search-augmented LLMs. The dataset spans
diverse intents and languages, and contains full system traces with around
12,000 human preference votes. Our analysis reveals that user preferences are
influenced by the number of citations, even when the cited content does not
directly support the attributed claims, uncovering a gap between perceived and
actual credibility. Furthermore, user preferences vary across cited sources,
revealing that community-driven platforms are generally preferred and static
encyclopedic sources are not always appropriate and reliable. To assess
performance across different settings, we conduct cross-arena analyses by
testing search-augmented LLMs in a general-purpose chat environment and
conventional LLMs in search-intensive settings. We find that web search does
not degrade and may even improve performance in non-search settings; however,
the quality in search settings is significantly affected if solely relying on
the model's parametric knowledge. We open-sourced the dataset to support future
research in this direction. Our dataset and code are available at:
https://github.com/lmarena/search-arena.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [229] [Conservation Laws and Boundedness for Linearised Einstein--Maxwell Equations on the Reissner--Nordström Black Hole](https://arxiv.org/abs/2506.04812)
*Marios A. Apetroaie,Sam C. Collingbourne,Elena Giorgi*

Main category: gr-qc

TL;DR: The paper analyzes linearized Einstein-Maxwell equations on Reissner-Nordström spacetime, using energy conservation to control Teukolsky variables without relying on hyperbolic methods.


<details>
  <summary>Details</summary>
Motivation: To establish control of energy fluxes for gauge-invariant Teukolsky variables in Reissner-Nordström spacetime, avoiding traditional hyperbolic approaches.

Method: Derives canonical energy conservation law in double null gauge and applies it to Teukolsky variables for charge-to-mass ratios below a specific threshold.

Result: Uniform boundedness of Teukolsky variables is achieved along all outgoing null hypersurfaces.

Conclusion: The approach successfully controls Teukolsky variables using energy conservation, offering an alternative to hyperbolic methods.

Abstract: We study the linearised Einstein--Maxwell equations on the
Reissner--Nordstr\"om spacetime and derive the canonical energy conservation
law in double null gauge. In the spirit of the work of Holzegel and the second
author, we avoid any use of the hyperbolic nature of the Teukolsky equations
and rely solely on the conservation law to establish control of energy fluxes
for the gauge-invariant Teukolsky variables, previously identified by the third
author, along all outgoing null hypersurfaces, for charge-to-mass ratio
$\frac{|Q|}{M} < \frac{\sqrt{15}}{4}$. This yields uniform boundedness for the
Teukolsky variables in Reissner--Nordstr\"om.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [230] [hdl2v: A Code Translation Dataset for Enhanced LLM Verilog Generation](https://arxiv.org/abs/2506.04544)
*Charles Hong,Brendan Roberts,Huijae An,Alex Um,Advay Ratan,Yakun Sophia Shao*

Main category: cs.AR

TL;DR: The paper introduces hdl2v, a dataset translating VHDL, Chisel, and PyMTL3 to Verilog to address the scarcity of human-written Verilog data. It demonstrates significant improvements in LLM Verilog generation performance.


<details>
  <summary>Details</summary>
Motivation: The lack of publicly available Verilog code compared to software languages like Python limits LLM performance in hardware code generation.

Method: hdl2v translates three hardware description languages (VHDL, Chisel, PyMTL3) to Verilog, creating a larger dataset for training.

Result: hdl2v improves a 32B-parameter model's performance by 23% (pass@10) and boosts a data augmentation-based approach by 63%.

Conclusion: hdl2v effectively enhances LLM Verilog generation, and future work can expand on dataset characteristics for further improvements.

Abstract: Large language models (LLMs) are playing an increasingly large role in
domains such as code generation, including hardware code generation, where
Verilog is the key language. However, the amount of publicly available Verilog
code pales in comparison to the amount of code available for software languages
like Python. In this work, we present hdl2v ("HDL-to-Verilog"), a dataset which
seeks to increase the amount of available human-written Verilog data by
translating or compiling three other hardware description languages - VHDL,
Chisel, and PyMTL3 - to Verilog. Furthermore, we demonstrate the value of hdl2v
in enhancing LLM Verilog generation by improving performance of a 32
billion-parameter open-weight model by up to 23% (pass@10) in VerilogEvalV2,
without utilizing any data augmentation or knowledge distillation from larger
models. We also show hdl2v's ability to boost the performance of a data
augmentation-based fine-tuning approach by 63%. Finally, we characterize and
analyze our dataset to better understand which characteristics of
HDL-to-Verilog datasets can be expanded upon in future work for even better
performance.

</details>


### [231] [QiMeng: Fully Automated Hardware and Software Design for Processor Chip](https://arxiv.org/abs/2506.05007)
*Rui Zhang,Yuanbo Wen,Shuyao Cheng,Di Huang,Shaohui Peng,Jiaming Guo,Pengwei Jin,Jiacheng Zhao,Tianrui Ma,Yaoyu Zhu,Yifan Hao,Yongwei Zhao,Shengwen Liang,Ying Wang,Xing Hu,Zidong Du,Huimin Cui,Ling Li,Qi Guo,Yunji Chen*

Main category: cs.AR

TL;DR: QiMeng is a novel system for fully automated processor chip design, addressing challenges like knowledge gaps and data scarcity with a domain-specific Large Processor Chip Model (LPCM) and automated hardware/software design agents.


<details>
  <summary>Details</summary>
Motivation: The rapid advancement of IT has strained conventional chip design paradigms due to physical constraints, resource demands, and ecosystem diversity. AI, especially LLMs, offers potential solutions but faces domain-specific challenges.

Method: QiMeng uses a three-layer approach: a domain-specific LPCM for knowledge representation and inference, and automated hardware/software design agents.

Result: Partial implementation of QiMeng shows significant advantages in automated chip design, with successful applications in top-layer use cases.

Conclusion: QiMeng provides a feasible solution for fully automated chip design, with future work focusing on system integration and iterative design processes.

Abstract: Processor chip design technology serves as a key frontier driving
breakthroughs in computer science and related fields. With the rapid
advancement of information technology, conventional design paradigms face three
major challenges: the physical constraints of fabrication technologies, the
escalating demands for design resources, and the increasing diversity of
ecosystems. Automated processor chip design has emerged as a transformative
solution to address these challenges. While recent breakthroughs in Artificial
Intelligence (AI), particularly Large Language Models (LLMs) techniques, have
opened new possibilities for fully automated processor chip design, substantial
challenges remain in establishing domain-specific LLMs for processor chip
design.
  In this paper, we propose QiMeng, a novel system for fully automated hardware
and software design of processor chips. QiMeng comprises three hierarchical
layers. In the bottom-layer, we construct a domain-specific Large Processor
Chip Model (LPCM) that introduces novel designs in architecture, training, and
inference, to address key challenges such as knowledge representation gap, data
scarcity, correctness assurance, and enormous solution space. In the
middle-layer, leveraging the LPCM's knowledge representation and inference
capabilities, we develop the Hardware Design Agent and the Software Design
Agent to automate the design of hardware and software for processor chips.
Currently, several components of QiMeng have been completed and successfully
applied in various top-layer applications, demonstrating significant advantages
and providing a feasible solution for efficient, fully automated
hardware/software design of processor chips. Future research will focus on
integrating all components and performing iterative top-down and bottom-up
design processes to establish a comprehensive QiMeng system.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [232] [Interpretable LLMs for Credit Risk: A Systematic Review and Taxonomy](https://arxiv.org/abs/2506.04290)
*Muhammed Golec,Maha AlabdulJalil*

Main category: q-fin.RM

TL;DR: This paper reviews LLM-based approaches for credit risk assessment, categorizing 60 studies (2020-2025) into model architectures, data types, explainability mechanisms, and applications. It highlights trends and gaps, focusing on interpretability.


<details>
  <summary>Details</summary>
Motivation: To systematically review and classify LLM-based methods in credit risk estimation, addressing the need for interpretability in financial AI applications.

Method: Analyzed 60 papers using PRISMA strategy, focusing on model architectures, data types, explainability (e.g., chain of thought prompts), and applications like default prediction.

Result: A taxonomy of LLM-based credit risk models was developed, identifying key trends and research gaps, particularly in interpretability.

Conclusion: The paper serves as a reference for AI and finance researchers, emphasizing future directions for LLM-based credit scoring systems.

Abstract: Large Language Models (LLM), which have developed in recent years, enable
credit risk assessment through the analysis of financial texts such as analyst
reports and corporate disclosures. This paper presents the first systematic
review and taxonomy focusing on LLMbased approaches in credit risk estimation.
We determined the basic model architectures by selecting 60 relevant papers
published between 2020-2025 with the PRISMA research strategy. And we examined
the data used for scenarios such as credit default prediction and risk
analysis. Since the main focus of the paper is interpretability, we classify
concepts such as explainability mechanisms, chain of thought prompts and
natural language justifications for LLM-based credit models. The taxonomy
organizes the literature under four main headings: model architectures, data
types, explainability mechanisms and application areas. Based on this analysis,
we highlight the main future trends and research gaps for LLM-based credit
scoring systems. This paper aims to be a reference paper for artificial
intelligence and financial researchers.

</details>


<div id='math.CA'></div>

# math.CA [[Back]](#toc)

### [233] [Limits at infinity for Hajłasz-Sobolev functions in metric spaces](https://arxiv.org/abs/2506.05037)
*Angha Agarwal,Antti V. Vähäkangas*

Main category: math.CA

TL;DR: The paper studies limits at infinity for homogeneous Hajlasz-Sobolev functions on metric spaces with doubling measures, proving pointwise limits outside an exceptional set defined by variational relative capacity.


<details>
  <summary>Details</summary>
Motivation: To refine earlier approaches using Hausdorff content and extend results for Newtonian and fractional Sobolev functions.

Method: Analyzes quasicontinuous representatives of functions, using variational relative capacity to define exceptional sets.

Result: Pointwise limits at infinity exist outside the exceptional set.

Conclusion: The framework generalizes and refines previous results, offering a more precise understanding of limits for such functions.

Abstract: We study limits at infinity for homogeneous Hajlasz-Sobolev functions defined
on uniformly perfect metric spaces equipped with a doubling measure. We prove
that a quasicontinuous representative of such a function has a pointwise limit
at infinity outside an exceptional set, defined in terms of a variational
relative capacity. Our framework refines earlier approaches that relied on
Hausdorff content rather than relative capacity, and it extends previous
results for homogeneous Newtonian and fractional Sobolev functions.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [234] [SGN-CIRL: Scene Graph-based Navigation with Curriculum, Imitation, and Reinforcement Learning](https://arxiv.org/abs/2506.04505)
*Nikita Oskolkov,Huzhenyu Zhang,Dmitry Makarov,Dmitry Yudin,Aleksandr Panov*

Main category: cs.RO

TL;DR: SGN-CIRL is a framework for mapless robot navigation using 3D scene graphs and reinforcement learning, enhanced by imitation and curriculum learning, showing improved success rates.


<details>
  <summary>Details</summary>
Motivation: To enable efficient navigation and target prediction in partially observable environments using 3D scene graphs.

Method: Combines 3D scene graphs with reinforcement learning, imitation learning, and curriculum learning for training.

Result: Significantly increased success rates in difficult navigation tasks, validated in Isaac Sim.

Conclusion: SGN-CIRL is effective for mapless navigation, with open-sourced code for further use.

Abstract: The 3D scene graph models spatial relationships between objects, enabling the
agent to efficiently navigate in a partially observable environment and predict
the location of the target object.This paper proposes an original framework
named SGN-CIRL (3D Scene Graph-Based Reinforcement Learning Navigation) for
mapless reinforcement learning-based robot navigation with learnable
representation of open-vocabulary 3D scene graph. To accelerate and stabilize
the training of reinforcement learning-based algorithms, the framework also
employs imitation learning and curriculum learning. The first one enables the
agent to learn from demonstrations, while the second one structures the
training process by gradually increasing task complexity from simple to more
advanced scenarios. Numerical experiments conducted in the Isaac Sim
environment showed that using a 3D scene graph for reinforcement learning
significantly increased the success rate in difficult navigation cases. The
code is open-sourced and available at: https://github.com/Xisonik/Aloha\_graph.

</details>


### [235] [Olfactory Inertial Odometry: Sensor Calibration and Drift Compensation](https://arxiv.org/abs/2506.04539)
*Kordel K. France,Ovidiu Daescu,Anirban Paul,Shalini Prasad*

Main category: cs.RO

TL;DR: The paper introduces a calibration process for Olfactory Inertial Odometry (OIO) to improve odor source localization accuracy for slow-moving robots, with applications in robotic surgery and security screening.


<details>
  <summary>Details</summary>
Motivation: OIO faces challenges due to gas dynamics and environmental disturbances, making accurate navigation difficult. The work aims to generalize calibration for various olfaction sensors.

Method: The authors define a calibration process for OIO, tested on a real robotic arm, to achieve centimeter-level accuracy in odor source localization.

Result: The calibration process improves performance over a cold-start olfactory navigation task, demonstrating practical use cases.

Conclusion: The proposed OIO calibration method enhances accuracy and generalizability, proving useful for specific robotic applications.

Abstract: Visual inertial odometry (VIO) is a process for fusing visual and kinematic
data to understand a machine's state in a navigation task. Olfactory inertial
odometry (OIO) is an analog to VIO that fuses signals from gas sensors with
inertial data to help a robot navigate by scent. Gas dynamics and environmental
factors introduce disturbances into olfactory navigation tasks that can make
OIO difficult to facilitate. With our work here, we define a process for
calibrating a robot for OIO that generalizes to several olfaction sensor types.
Our focus is specifically on calibrating OIO for centimeter-level accuracy in
localizing an odor source on a slow-moving robot platform to demonstrate use
cases in robotic surgery and touchless security screening. We demonstrate our
process for OIO calibration on a real robotic arm and show how this calibration
improves performance over a cold-start olfactory navigation task.

</details>


### [236] [Chronoamperometry with Room-Temperature Ionic Liquids: Sub-Second Inference Techniques](https://arxiv.org/abs/2506.04540)
*Kordel K. France*

Main category: cs.RO

TL;DR: A novel regression method reduces chronoamperometry measurement time in RTILs to under 1 second, enabling faster electrochemical analysis without hardware changes.


<details>
  <summary>Details</summary>
Motivation: High viscosity and slow mass transport in RTILs prolong chronoamperometry measurements, necessitating a faster method.

Method: Uses a mathematical regression approach and inference algorithm on initial transient current to predict steady-state parameters.

Result: Validated against standard techniques, the method maintains accuracy while reducing measurement time significantly.

Conclusion: The technique enables rapid electrochemical quantification, benefiting analytical chemistry, sensors, and battery science.

Abstract: Chronoamperometry (CA) is a fundamental electrochemical technique used for
quantifying redox-active species. However, in room-temperature ionic liquids
(RTILs), the high viscosity and slow mass transport often lead to extended
measurement durations. This paper presents a novel mathematical regression
approach that reduces CA measurement windows to under 1 second, significantly
faster than previously reported methods, which typically require 1-4 seconds or
longer. By applying an inference algorithm to the initial transient current
response, this method accurately predicts steady-state electrochemical
parameters without requiring additional hardware modifications. The approach is
validated through comparison with standard chronoamperometric techniques and is
demonstrated to maintain reasonable accuracy while dramatically reducing data
acquisition time. The implications of this technique are explored in analytical
chemistry, sensor technology, and battery science, where rapid electrochemical
quantification is critical. Our technique is focused on enabling faster
multiplexing of chronoamperometric measurements for rapid olfactory and
electrochemical analysis.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [237] [Quantum simulation of the Hubbard model on a graphene hexagon: Strengths of IQPE and noise constraints](https://arxiv.org/abs/2506.05031)
*Mohammad Mirzakhani,Kyungsun Moon*

Main category: quant-ph

TL;DR: The paper explores quantum simulation of the Hubbard model on graphene using Qiskit, achieving accurate results in noiseless conditions but facing challenges with noise on real hardware.


<details>
  <summary>Details</summary>
Motivation: To demonstrate the potential of quantum computing for simulating strongly correlated quantum systems like the Hubbard model, while addressing the limitations imposed by current noisy hardware.

Method: Utilized Iterative Quantum Phase Estimation (IQPE) and adiabatic evolution algorithms on Qiskit, with noiseless simulations and a custom noise model for IBM backends.

Result: Noiseless simulations matched exact diagonalization results, but noise degraded accuracy, revealing discrepancies between simulated and real hardware performance.

Conclusion: Quantum computing shows promise for material simulation, but hardware noise remains a significant barrier to reliable predictions.

Abstract: Quantum computing offers transformative potential for simulating real-world
materials, providing a powerful platform to investigate complex quantum systems
across quantum chemistry and condensed matter physics. In this work, we
leverage this capability to simulate the Hubbard model on a six-site graphene
hexagon using Qiskit, employing the Iterative Quantum Phase Estimation (IQPE)
and adiabatic evolution algorithms to determine its ground-state properties.
Noiseless simulations yield accurate ground-state energies (GSEs), charge and
spin densities, and correlation functions, all in excellent agreement with
exact diagonalization, underscoring the precision and reliability of quantum
simulation for strongly correlated electron systems. However, deploying IQPE
and adiabatic evolution on today's noisy quantum hardware remains highly
challenging. To examine these limitations, we utilize the Qiskit Aer simulator
with a custom noise model tailored to the characteristics of IBM's real
backend. This model includes realistic depolarizing gate errors, thermal
relaxation, and readout noise, allowing us to explore how these factors degrade
simulation accuracy. Preliminary hardware runs on IBM devices further expose
discrepancies between simulated and real-world noise, emphasizing the gap
between ideal and practical implementations. Overall, our results highlight the
promise of quantum computing for simulating correlated quantum materials, while
also revealing the significant challenges posed by hardware noise in achieving
accurate and reliable physical predictions using current quantum devices.

</details>


### [238] [Quantum simulation of the Hubbard model on a graphene hexagon: Strengths of IQPE and noise constraints](https://arxiv.org/abs/2506.05031)
*Mohammad Mirzakhani,Kyungsun Moon*

Main category: quant-ph

TL;DR: The paper explores quantum simulation of the Hubbard model on graphene using Qiskit, achieving accurate noiseless results but facing challenges with noise on real hardware.


<details>
  <summary>Details</summary>
Motivation: To demonstrate the potential of quantum computing for simulating strongly correlated electron systems like the Hubbard model, while addressing the limitations of current noisy hardware.

Method: Uses Iterative Quantum Phase Estimation (IQPE) and adiabatic evolution algorithms on Qiskit, with noiseless simulations and a custom noise model for IBM backends.

Result: Noiseless simulations match exact diagonalization results, but noise degrades accuracy, revealing hardware limitations.

Conclusion: Quantum computing shows promise for material simulation, but noise in current devices poses significant challenges.

Abstract: Quantum computing offers transformative potential for simulating real-world
materials, providing a powerful platform to investigate complex quantum systems
across quantum chemistry and condensed matter physics. In this work, we
leverage this capability to simulate the Hubbard model on a six-site graphene
hexagon using Qiskit, employing the Iterative Quantum Phase Estimation (IQPE)
and adiabatic evolution algorithms to determine its ground-state properties.
Noiseless simulations yield accurate ground-state energies (GSEs), charge and
spin densities, and correlation functions, all in excellent agreement with
exact diagonalization, underscoring the precision and reliability of quantum
simulation for strongly correlated electron systems. However, deploying IQPE
and adiabatic evolution on today's noisy quantum hardware remains highly
challenging. To examine these limitations, we utilize the Qiskit Aer simulator
with a custom noise model tailored to the characteristics of IBM's real
backend. This model includes realistic depolarizing gate errors, thermal
relaxation, and readout noise, allowing us to explore how these factors degrade
simulation accuracy. Preliminary hardware runs on IBM devices further expose
discrepancies between simulated and real-world noise, emphasizing the gap
between ideal and practical implementations. Overall, our results highlight the
promise of quantum computing for simulating correlated quantum materials, while
also revealing the significant challenges posed by hardware noise in achieving
accurate and reliable physical predictions using current quantum devices.

</details>


### [239] [TQml Simulator: Optimized Simulation of Quantum Machine Learning](https://arxiv.org/abs/2506.04891)
*Viacheslav Kuzmin,Basil Kyriacou,Mateusz Papierz,Mo Kordzanganeh,Alexey Melnikov*

Main category: quant-ph

TL;DR: The paper benchmarks simulation techniques for Quantum Machine Learning circuits, introduces TQml Simulator for optimized performance, and shows significant speedups over existing methods.


<details>
  <summary>Details</summary>
Motivation: Advancing research in Quantum Machine Learning by improving simulation efficiency for hardware-efficient circuits.

Method: Numerical benchmarking of universal and gate-specific techniques, followed by developing TQml Simulator to apply optimal methods per gate layer.

Result: TQml Simulator outperforms Pennylane's default.qubit by 2- to 100-fold, depending on circuit specifics.

Conclusion: Tailored simulation techniques per gate layer significantly enhance performance, enabling faster Quantum Machine Learning research.

Abstract: Hardware-efficient circuits employed in Quantum Machine Learning are
typically composed of alternating layers of uniformly applied gates. High-speed
numerical simulators for such circuits are crucial for advancing research in
this field. In this work, we numerically benchmark universal and gate-specific
techniques for simulating the action of layers of gates on quantum state
vectors, aiming to accelerate the overall simulation of Quantum Machine
Learning algorithms. Our analysis shows that the optimal simulation method for
a given layer of gates depends on the number of qubits involved, and that a
tailored combination of techniques can yield substantial performance gains in
the forward and backward passes for a given circuit. Building on these
insights, we developed a numerical simulator, named TQml Simulator, that
employs the most efficient simulation method for each layer in a given circuit.
We evaluated TQml Simulator on circuits constructed from standard gate sets,
such as rotations and CNOTs, as well as on native gates from IonQ and IBM
quantum processing units. In most cases, our simulator outperforms equivalent
Pennylane's default.qubit simulator by approximately 2- to 100-fold, depending
on the circuit, the number of qubits, the batch size of the input data, and the
hardware used.

</details>


<div id='nlin.PS'></div>

# nlin.PS [[Back]](#toc)

### [240] [Robust Moment Identification for Nonlinear PDEs via a Neural ODE Approach](https://arxiv.org/abs/2506.05245)
*Shaoxuan Chen,Su Yang,Panayotis G. Kevrekidis,Wei Zhu*

Main category: nlin.PS

TL;DR: A Neural ODE-based framework learns reduced-order moment dynamics from PDE systems, handling sparse, irregular data robustly. It outperforms traditional methods like SINDy and works even without analytical closure.


<details>
  <summary>Details</summary>
Motivation: Traditional methods like SINDy require dense data and are noise-sensitive. This work aims to robustly learn moment dynamics from sparse, irregular data using Neural ODEs.

Method: Uses Neural ODEs to model moment trajectories directly. Introduces a Stiefel manifold optimization for data-driven coordinate transformation when closure is unavailable.

Result: Accurately recovers moment dynamics with closure, even with limited data. For unclosed systems, achieves superior extrapolation accuracy compared to expert-derived models.

Conclusion: Neural ODEs are a flexible, robust tool for learning interpretable, low-dimensional moment dynamics in complex PDE systems.

Abstract: We propose a data-driven framework for learning reduced-order moment dynamics
from PDE-governed systems using Neural ODEs. In contrast to derivative-based
methods like SINDy, which necessitate densely sampled data and are sensitive to
noise, our approach based on Neural ODEs directly models moment trajectories,
enabling robust learning from sparse and potentially irregular time series.
Using as an application platform the nonlinear Schr\"{o}dinger equation, the
framework accurately recovers governing moment dynamics when closure is
available, even with limited and irregular observations. For systems without
analytical closure, we introduce a data-driven coordinate transformation
strategy based on Stiefel manifold optimization, enabling the discovery of
low-dimensional representations in which the moment dynamics become closed,
facilitating interpretable and reliable modeling. We also explore cases where a
closure model is not known, such as a Fisher-KPP reaction-diffusion system.
Here we demonstrate that Neural ODEs can still effectively approximate the
unclosed moment dynamics and achieve superior extrapolation accuracy compared
to physical-expert-derived ODE models. This advantage remains robust even under
sparse and irregular sampling, highlighting the method's robustness in
data-limited settings. Our results highlight the Neural ODE framework as a
powerful and flexible tool for learning interpretable, low-dimensional moment
dynamics in complex PDE-governed systems.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [241] [Random dynamics and invariant measures for a class of non-Newtonian fluids of differential type on 2D and 3D Poincaré domains](https://arxiv.org/abs/2506.04801)
*Kush Kinra,Fernanda Cipriano*

Main category: math.PR

TL;DR: The paper studies the well-posedness and asymptotic behavior of solutions for a class of incompressible stochastic third-grade fluids on Poincaré domains, proving unique weak solutions and random attractors for bounded and unbounded domains.


<details>
  <summary>Details</summary>
Motivation: To analyze the well-posedness and asymptotic properties of stochastic non-Newtonian fluid equations, addressing challenges like nonlinearity and lack of compact embeddings in unbounded domains.

Method: Proves unique weak solutions under Dirichlet boundary conditions, uses compact Sobolev embeddings for bounded domains, and employs uniform-tail estimates for unbounded domains to show asymptotic compactness.

Result: Existence of unique random attractors for both bounded and unbounded domains, and invariant measures for the system.

Conclusion: The study successfully addresses the challenges of nonlinearity and domain unboundedness, providing insights into the long-term behavior of stochastic third-grade fluids.

Abstract: In this article, we consider a class of incompressible stochastic third-grade
fluids (non-Newtonian fluids) equations on two- as well as three-dimensional
Poincar\'e domains $\mathcal{O}$ (which may be bounded or unbounded). Our aims
are to study the well-posedness and asymptotic analysis for the solutions of
the underlying system. Firstly, we prove that the underlying system defined on
$\mathcal{O}$ has a unique weak solution (in the analytic sense) under
Dirichlet boundary condition and it also generates random dynamical system
$\Psi$. Secondly, we consider the underlying system on bounded domains. Using
the compact Sobolev embedding $\mathbb{H}^1(\mathcal{O})
\hookrightarrow\mathbb{L}^2(\mathcal{O})$, we prove the existence of a unique
random attractor for the underlying system on bounded domains with external
forcing in
$\mathbb{H}^{-1}(\mathcal{O})+\mathbb{W}^{-1,\frac{4}{3}}(\mathcal{O})$.
Thirdly, we consider the underlying system on unbounded Poincar\'e domains with
external forcing in $\mathbb{L}^{2}(\mathcal{O})$ and show the existence of a
unique random attractor. In order to obtain the existence of a unique random
attractor on unbounded domains, due to the lack of compact Sobolev embedding
$\mathbb{H}^1(\mathcal{O}) \hookrightarrow\mathbb{H}^2(\mathcal{O})$, we use
the uniform-tail estimates method which helps us to demonstrate the asymptotic
compactness of $\Psi$.
  Note that due to the presence of several nonlinear terms in the underlying
system, we are not able to use the energy equality method to obtain the
asymptotic compactness of $\Psi$ in unbounded domains, which makes the analysis
of this work in unbounded domains more difficult and interesting. Finally, as a
consequence of the existence of random attractors, we address the existence of
invariant measures for underlying system.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [242] [MVP-Shapley: Feature-based Modeling for Evaluating the Most Valuable Player in Basketball](https://arxiv.org/abs/2506.04602)
*Haifeng Sun,Yu Xiong,Runze Wu,Kai Wang,Lan Zhang,Changjie Fan,Shaojie Tang,Xiang-Yang Li*

Main category: cs.GT

TL;DR: The paper introduces a new MVP evaluation framework, \oursys, using Shapley values to assess player contributions in esports and multiplayer games, validated with NBA and Dunk City Dynasty datasets.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of esports and multiplayer gaming necessitates a reliable and explainable MVP evaluation method, which current approaches lack.

Method: The proposed framework, \oursys, processes play-by-play data, trains a win-loss model, allocates Shapley values, and ranks players by contribution. It also optimizes alignment with expert voting using causal analysis.

Result: The method was validated using NBA and Dunk City Dynasty datasets and successfully deployed in the industry.

Conclusion: \oursys provides a practical and explainable MVP evaluation framework, addressing the challenges in player contribution assessment.

Abstract: The burgeoning growth of the esports and multiplayer online gaming community
has highlighted the critical importance of evaluating the Most Valuable Player
(MVP). The establishment of an explainable and practical MVP evaluation method
is very challenging. In our study, we specifically focus on play-by-play data,
which records related events during the game, such as assists and points. We
aim to address the challenges by introducing a new MVP evaluation framework,
denoted as \oursys, which leverages Shapley values. This approach encompasses
feature processing, win-loss model training, Shapley value allocation, and MVP
ranking determination based on players' contributions. Additionally, we
optimize our algorithm to align with expert voting results from the perspective
of causality. Finally, we substantiated the efficacy of our method through
validation using the NBA dataset and the Dunk City Dynasty dataset and
implemented online deployment in the industry.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [243] [Generalizable, real-time neural decoding with hybrid state-space models](https://arxiv.org/abs/2506.05320)
*Avery Hee-Woon Ryoo,Nanda H. Krishna,Ximeng Mao,Mehdi Azabou,Eva L. Dyer,Matthew G. Perich,Guillaume Lajoie*

Main category: q-bio.NC

TL;DR: POSSM is a hybrid architecture combining spike tokenization and a recurrent state-space model for efficient, real-time neural decoding, outperforming traditional and Transformer-based methods in speed and generalization.


<details>
  <summary>Details</summary>
Motivation: Address the trade-off between fast, lightweight models (poor generalization) and powerful Transformers (high computational cost) for real-time neural decoding.

Method: Hybrid architecture with cross-attention spike tokenization and a recurrent state-space model (SSM) backbone, pretrained on multi-dataset data.

Result: POSSM matches Transformer accuracy in decoding tasks (monkey motor, human handwriting/speech) but is up to 9x faster on GPU. Pretraining on monkey data improves human task performance.

Conclusion: Hybrid SSMs like POSSM bridge accuracy, speed, and generalization gaps, making them ideal for real-time neural decoding applications.

Abstract: Real-time decoding of neural activity is central to neuroscience and
neurotechnology applications, from closed-loop experiments to brain-computer
interfaces, where models are subject to strict latency constraints. Traditional
methods, including simple recurrent neural networks, are fast and lightweight
but often struggle to generalize to unseen data. In contrast, recent
Transformer-based approaches leverage large-scale pretraining for strong
generalization performance, but typically have much larger computational
requirements and are not always suitable for low-resource or real-time
settings. To address these shortcomings, we present POSSM, a novel hybrid
architecture that combines individual spike tokenization via a cross-attention
module with a recurrent state-space model (SSM) backbone to enable (1) fast and
causal online prediction on neural activity and (2) efficient generalization to
new sessions, individuals, and tasks through multi-dataset pretraining. We
evaluate POSSM's decoding performance and inference speed on intracortical
decoding of monkey motor tasks, and show that it extends to clinical
applications, namely handwriting and speech decoding in human subjects.
Notably, we demonstrate that pretraining on monkey motor-cortical recordings
improves decoding performance on the human handwriting task, highlighting the
exciting potential for cross-species transfer. In all of these tasks, we find
that POSSM achieves decoding accuracy comparable to state-of-the-art
Transformers, at a fraction of the inference cost (up to 9x faster on GPU).
These results suggest that hybrid SSMs are a promising approach to bridging the
gap between accuracy, inference speed, and generalization when training neural
decoders for real-time, closed-loop applications.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [244] [Can Artificial Intelligence Trade the Stock Market?](https://arxiv.org/abs/2506.04658)
*Jędrzej Maskiewicz,Paweł Sakowski*

Main category: q-fin.TR

TL;DR: The paper compares DDQN and PPO in stock trading, showing DRL's superiority over classical methods in risk-adjusted returns.


<details>
  <summary>Details</summary>
Motivation: To evaluate the effectiveness of DRL algorithms (DDQN and PPO) in stock market trading compared to traditional Buy and Hold strategies.

Method: Uses DDQN and PPO on daily data (2019-2023) for currency pairs, S&P 500, and Bitcoin, benchmarking against Buy and Hold.

Result: DRL outperforms classical methods, managing risk by avoiding unfavorable trades, yielding better risk-adjusted returns.

Conclusion: DRL, especially DDQN and PPO, offers a significant advantage in trading by optimizing risk and returns.

Abstract: The paper explores the use of Deep Reinforcement Learning (DRL) in stock
market trading, focusing on two algorithms: Double Deep Q-Network (DDQN) and
Proximal Policy Optimization (PPO) and compares them with Buy and Hold
benchmark. It evaluates these algorithms across three currency pairs, the S&P
500 index and Bitcoin, on the daily data in the period of 2019-2023. The
results demonstrate DRL's effectiveness in trading and its ability to manage
risk by strategically avoiding trades in unfavorable conditions, providing a
substantial edge over classical approaches, based on supervised learning in
terms of risk-adjusted returns.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [245] [Reactive Transport Simulation of Silicate-Rich Shale Rocks when Exposed to CO2 Saturated Brine Under High Pressure and High Temperature](https://arxiv.org/abs/2506.05122)
*Shaziya A. Banu,Venkata R. S. B. Varanasi,Arash Noshadravan,Sara Abedi*

Main category: physics.geo-ph

TL;DR: The study explores CO2 storage in shale rocks using reactive transport modeling to simulate chemo-mechanical interactions, avoiding costly experiments. Results show mineral dissolution and precipitation but insufficient pore-filling.


<details>
  <summary>Details</summary>
Motivation: To address the high cost and time of experimental testing for CO2 storage in heterogeneous shale rocks by using reactive transport models.

Method: Used Crunch Tope to simulate a 1D reactive transport model of a Permian rock specimen under high pressure (12.40 MPa) and temperature (100°C) with CO2 saturated brine for 14 and 28 days.

Result: Observed dissolution of quartz and feldspar, precipitation of quartz-rich phases, and clay swelling. Porosity changes showed only 1.00% mineral precipitation, insufficient to fill pores.

Conclusion: Reactive transport modeling is feasible for studying CO2 storage in shale, but mineral precipitation alone may not fully seal pores.

Abstract: This study examines the feasibility of carbon dioxide storage in shale rocks
and the reliability of reactive transport models in achieving accurate
replication of the chemo-mechanical interactions and transport processes
transpiring in these rocks when subjected to CO2 saturated brine. Owing to the
heterogeneity of rocks, experimental testing for adequate deductions and
findings, could be an expensive and time-intensive process. Therefore, this
study proposes utilization of reactive transport modeling to replicate the
pore-scale chemo-mechanical reactions and transport processes occurring in
silicate-rich shale rocks in the presence of CO2 saturated brine under high
pressure and high temperature. For this study, Crunch Tope has been adopted to
simulate a one-dimensional reactive transport model of a Permian rock specimen
exposed to the acidic brine at a temperature of 100 {\deg}C and pressure of
12.40 MPa (1800 psi) for a period of 14 and 28 days. The results demonstrated
significant dissolution followed by precipitation of quartz rich phases,
precipitation and swelling of clay rich phases, and dissolution of feldspar
rich phases closer to the acidic brine-rock interface. Moreover, porosity
against reaction depth curve showed nearly 1.00% mineral precipitation occur at
14 and 28 days, which is insufficient to completely fill the pore spaces.

</details>


### [246] [Reactive Transport Simulation of Silicate-Rich Shale Rocks when Exposed to CO2 Saturated Brine Under High Pressure and High Temperature](https://arxiv.org/abs/2506.05122)
*Shaziya A. Banu,Venkata R. S. B. Varanasi,Arash Noshadravan,Sara Abedi*

Main category: physics.geo-ph

TL;DR: The study explores CO2 storage in shale rocks using reactive transport modeling to simulate chemo-mechanical interactions, avoiding costly experiments. Results show mineral dissolution and precipitation but insufficient pore-filling.


<details>
  <summary>Details</summary>
Motivation: To address the high cost and time of experimental testing for CO2 storage in heterogeneous shale rocks by using reactive transport modeling.

Method: Used Crunch Tope for 1D reactive transport modeling of a Permian rock specimen under high pressure/temperature with CO2 saturated brine for 14/28 days.

Result: Significant mineral dissolution/precipitation, especially near the brine-rock interface, but only 1.00% porosity change, insufficient for pore-filling.

Conclusion: Reactive transport modeling is feasible for studying CO2 storage in shale, but mineral precipitation alone may not fully seal pores.

Abstract: This study examines the feasibility of carbon dioxide storage in shale rocks
and the reliability of reactive transport models in achieving accurate
replication of the chemo-mechanical interactions and transport processes
transpiring in these rocks when subjected to CO2 saturated brine. Owing to the
heterogeneity of rocks, experimental testing for adequate deductions and
findings, could be an expensive and time-intensive process. Therefore, this
study proposes utilization of reactive transport modeling to replicate the
pore-scale chemo-mechanical reactions and transport processes occurring in
silicate-rich shale rocks in the presence of CO2 saturated brine under high
pressure and high temperature. For this study, Crunch Tope has been adopted to
simulate a one-dimensional reactive transport model of a Permian rock specimen
exposed to the acidic brine at a temperature of 100 {\deg}C and pressure of
12.40 MPa (1800 psi) for a period of 14 and 28 days. The results demonstrated
significant dissolution followed by precipitation of quartz rich phases,
precipitation and swelling of clay rich phases, and dissolution of feldspar
rich phases closer to the acidic brine-rock interface. Moreover, porosity
against reaction depth curve showed nearly 1.00% mineral precipitation occur at
14 and 28 days, which is insufficient to completely fill the pore spaces.

</details>


### [247] [Estimating properties of a homogeneous bounded soil using machine learning models](https://arxiv.org/abs/2506.04256)
*Konstantinos Kalimeris,Leonidas Mindrinos,Nikolaos Pallikarakis*

Main category: physics.geo-ph

TL;DR: The paper estimates soil properties from water moisture measurements using machine learning models, with SVMs and NNs showing the best performance.


<details>
  <summary>Details</summary>
Motivation: To address the parameter identification problem in soil properties estimation from water moisture data.

Method: Simulated data generated via the Fokas method for vertical infiltration; machine learning models (SVMs, NNs) tested under full, noisy, and limited data conditions.

Result: Diffusivity (D) predictions are more accurate than hydraulic conductivity (K); SVMs and NNs achieve near-perfect accuracy and robustness.

Conclusion: SVMs and NNs are highly effective for estimating soil properties from water moisture data, especially under varying data conditions.

Abstract: This work focuses on estimating soil properties from water moisture
measurements. We consider simulated data generated by solving the
initial-boundary value problem governing vertical infiltration in a
homogeneous, bounded soil profile, with the usage of the Fokas method. To
address the parameter identification problem, which is formulated as a
two-output regression task, we explore various machine learning models. The
performance of each model is assessed under different data conditions: full,
noisy, and limited. Overall, the prediction of diffusivity $D$ tends to be more
accurate than that of hydraulic conductivity $K.$ Among the models considered,
Support Vector Machines (SVMs) and Neural Networks (NNs) demonstrate the
highest robustness, achieving near-perfect accuracy and minimal errors.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [248] [On the Wasserstein Geodesic Principal Component Analysis of probability measures](https://arxiv.org/abs/2506.04480)
*Nina Vesseron,Elsa Cazelles,Alice Le Brigant,Thierry Klein*

Main category: stat.ML

TL;DR: The paper introduces Geodesic Principal Component Analysis (GPCA) for probability distributions using Otto-Wasserstein geometry, focusing on Gaussian distributions and extending to general measures with neural networks.


<details>
  <summary>Details</summary>
Motivation: To identify geodesic curves in probability measure spaces that capture dataset variations, improving upon classical methods.

Method: Uses Otto-Wasserstein geometry for Gaussian distributions and neural networks for general measures, comparing to tangent PCA.

Result: Demonstrates effective GPCA for Gaussian and general measures, validated with real-world examples.

Conclusion: GPCA offers a robust framework for analyzing probability distributions, outperforming traditional PCA in certain cases.

Abstract: This paper focuses on Geodesic Principal Component Analysis (GPCA) on a
collection of probability distributions using the Otto-Wasserstein geometry.
The goal is to identify geodesic curves in the space of probability measures
that best capture the modes of variation of the underlying dataset. We first
address the case of a collection of Gaussian distributions, and show how to
lift the computations in the space of invertible linear maps. For the more
general setting of absolutely continuous probability measures, we leverage a
novel approach to parameterizing geodesics in Wasserstein space with neural
networks. Finally, we compare to classical tangent PCA through various examples
and provide illustrations on real-world datasets.

</details>


### [249] [Regret-Optimal Q-Learning with Low Cost for Single-Agent and Federated Reinforcement Learning](https://arxiv.org/abs/2506.04626)
*Haochen Zhang,Zhong Zheng,Lingzhou Xue*

Main category: stat.ML

TL;DR: The paper introduces two model-free RL algorithms, Q-EarlySettled-LowCost and FedQ-EarlySettled-LowCost, to minimize burn-in and switching/communication costs in single-agent and federated RL, achieving near-optimal regret and logarithmic costs.


<details>
  <summary>Details</summary>
Motivation: Addressing costly data collection and policy deployment in RL and FRL by minimizing burn-in and switching/communication costs.

Method: Proposes two model-free algorithms for single-agent and federated RL, focusing on linear burn-in costs and logarithmic switching/communication costs.

Result: Achieves best near-optimal regret, low burn-in costs (linear in S and A), and logarithmic switching/communication costs.

Conclusion: The algorithms outperform existing methods, providing efficient solutions for RL and FRL with theoretical guarantees.

Abstract: Motivated by real-world settings where data collection and policy deployment
-- whether for a single agent or across multiple agents -- are costly, we study
the problem of on-policy single-agent reinforcement learning (RL) and federated
RL (FRL) with a focus on minimizing burn-in costs (the sample sizes needed to
reach near-optimal regret) and policy switching or communication costs. In
parallel finite-horizon episodic Markov Decision Processes (MDPs) with $S$
states and $A$ actions, existing methods either require superlinear burn-in
costs in $S$ and $A$ or fail to achieve logarithmic switching or communication
costs. We propose two novel model-free RL algorithms -- Q-EarlySettled-LowCost
and FedQ-EarlySettled-LowCost -- that are the first in the literature to
simultaneously achieve: (i) the best near-optimal regret among all known
model-free RL or FRL algorithms, (ii) low burn-in cost that scales linearly
with $S$ and $A$, and (iii) logarithmic policy switching cost for single-agent
RL or communication cost for FRL. Additionally, we establish gap-dependent
theoretical guarantees for both regret and switching/communication costs,
improving or matching the best-known gap-dependent bounds.

</details>


### [250] [Distributional encoding for Gaussian process regression with qualitative inputs](https://arxiv.org/abs/2506.04813)
*Sébastien Da Veiga*

Main category: stat.ML

TL;DR: The paper proposes a distributional encoding (DE) method for Gaussian Process (GP) regression to handle categorical variables, improving predictive performance and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: GP regression struggles with categorical inputs, limiting its effectiveness in applications like Bayesian optimization (BO). The paper aims to address this challenge.

Method: The authors generalize naive target encoding to DE, using all target variable samples for a category. They integrate DE into GP using characteristic kernels based on maximum mean discrepancy and Wasserstein distance.

Result: Empirical validation shows state-of-the-art predictive performance on synthetic and real-world datasets.

Conclusion: DE effectively handles categorical variables in GP regression and complements recent advances in BO for discrete and mixed spaces.

Abstract: Gaussian Process (GP) regression is a popular and sample-efficient approach
for many engineering applications, where observations are expensive to acquire,
and is also a central ingredient of Bayesian optimization (BO), a highly
prevailing method for the optimization of black-box functions. However, when
all or some input variables are categorical, building a predictive and
computationally efficient GP remains challenging. Starting from the naive
target encoding idea, where the original categorical values are replaced with
the mean of the target variable for that category, we propose a generalization
based on distributional encoding (DE) which makes use of all samples of the
target variable for a category. To handle this type of encoding inside the GP,
we build upon recent results on characteristic kernels for probability
distributions, based on the maximum mean discrepancy and the Wasserstein
distance. We also discuss several extensions for classification, multi-task
learning and incorporation or auxiliary information. Our approach is validated
empirically, and we demonstrate state-of-the-art predictive performance on a
variety of synthetic and real-world datasets. DE is naturally complementary to
recent advances in BO over discrete and mixed-spaces.

</details>


### [251] [Learning Joint Interventional Effects from Single-Variable Interventions in Additive Models](https://arxiv.org/abs/2506.04945)
*Armin Kekić,Sergio Hernan Garrido Mejia,Bernhard Schölkopf*

Main category: stat.ML

TL;DR: The paper proposes a method to estimate joint interventional effects using observational data and single-variable interventions, achieving results comparable to models trained on joint interventional data.


<details>
  <summary>Details</summary>
Motivation: Estimating joint interventional effects is crucial but challenging due to limited data from simultaneous interventions.

Method: The study presents an identifiability result for nonlinear additive outcome mechanisms and proposes a practical estimator decomposing effects into confounded and unconfounded contributions.

Result: Experiments on synthetic data show the method performs comparably to models trained on joint interventional data, outperforming observational estimators.

Conclusion: Joint interventional effects can be inferred without joint interventional data, offering a practical solution for causal effect estimation.

Abstract: Estimating causal effects of joint interventions on multiple variables is
crucial in many domains, but obtaining data from such simultaneous
interventions can be challenging. Our study explores how to learn joint
interventional effects using only observational data and single-variable
interventions. We present an identifiability result for this problem, showing
that for a class of nonlinear additive outcome mechanisms, joint effects can be
inferred without access to joint interventional data. We propose a practical
estimator that decomposes the causal effect into confounded and unconfounded
contributions for each intervention variable. Experiments on synthetic data
demonstrate that our method achieves performance comparable to models trained
directly on joint interventional data, outperforming a purely observational
estimator.

</details>


### [252] [Nonlinear Causal Discovery for Grouped Data](https://arxiv.org/abs/2506.05120)
*Konstantin Göbler,Tobias Windisch,Mathias Drton*

Main category: stat.ML

TL;DR: The paper extends nonlinear additive noise models to handle random vectors for causal inference, introducing a two-step approach for causal graph learning with strong performance in simulations and real-world applications.


<details>
  <summary>Details</summary>
Motivation: Many domains involve causal units as groups of variables, not just scalar measurements, necessitating methods to infer cause-effect relationships among random vectors.

Method: A two-step approach: first infer causal order among random vectors, then perform model selection to identify the best graph consistent with this order. Novel solutions are introduced for both steps.

Result: Demonstrated strong performance in simulations and applied successfully to real-world assembly line data with partial causal ordering knowledge.

Conclusion: The proposed method effectively extends causal inference to random vectors, showing promise for applications in neuroscience, psychology, social science, and industrial manufacturing.

Abstract: Inferring cause-effect relationships from observational data has gained
significant attention in recent years, but most methods are limited to scalar
random variables. In many important domains, including neuroscience,
psychology, social science, and industrial manufacturing, the causal units of
interest are groups of variables rather than individual scalar measurements.
Motivated by these applications, we extend nonlinear additive noise models to
handle random vectors, establishing a two-step approach for causal graph
learning: First, infer the causal order among random vectors. Second, perform
model selection to identify the best graph consistent with this order. We
introduce effective and novel solutions for both steps in the vector case,
demonstrating strong performance in simulations. Finally, we apply our method
to real-world assembly line data with partial knowledge of causal ordering
among variable groups.

</details>


### [253] [Causal Effect Identification in lvLiNGAM from Higher-Order Cumulants](https://arxiv.org/abs/2506.05202)
*Daniele Tramontano,Yaroslav Kivva,Saber Salehkaleybar Mathias Drton,Negar Kiyavash*

Main category: stat.ML

TL;DR: The paper explores causal effect identification in latent variable Linear Non-Gaussian Acyclic Models (lvLiNGAM) using higher-order cumulants, tackling challenging setups with latent confounders. It proves identifiability with a single proxy or instrument and offers estimation methods, showing improved accuracy and robustness over existing approaches.


<details>
  <summary>Details</summary>
Motivation: The study aims to address two difficult scenarios in causal inference with latent confounders: (1) a single proxy variable affecting the treatment and (2) underspecified instrumental variable cases. Existing methods struggle here, motivating new theoretical and practical solutions.

Method: The paper employs higher-order cumulants in lvLiNGAM to identify causal effects. It provides proofs for identifiability with a single proxy or instrument and develops corresponding estimation techniques.

Result: Experimental results confirm the proposed methods' accuracy and robustness, outperforming existing approaches in causal inference for linear systems with latent confounders.

Conclusion: The work advances causal inference theory and practice in linear systems with latent variables, demonstrating effective solutions for challenging setups with limited proxies or instruments.

Abstract: This paper investigates causal effect identification in latent variable
Linear Non-Gaussian Acyclic Models (lvLiNGAM) using higher-order cumulants,
addressing two prominent setups that are challenging in the presence of latent
confounding: (1) a single proxy variable that may causally influence the
treatment and (2) underspecified instrumental variable cases where fewer
instruments exist than treatments. We prove that causal effects are
identifiable with a single proxy or instrument and provide corresponding
estimation methods. Experimental results demonstrate the accuracy and
robustness of our approaches compared to existing methods, advancing the
theoretical and practical understanding of causal inference in linear systems
with latent confounders.

</details>


### [254] [Admissibility of Completely Randomized Trials: A Large-Deviation Approach](https://arxiv.org/abs/2506.05329)
*Guido Imbens,Chao Qin,Stefan Wager*

Main category: stat.ML

TL;DR: Adaptive designs universally outperform non-adaptive trials in best-arm identification with three or more treatment arms, as shown by efficiency exponent analysis.


<details>
  <summary>Details</summary>
Motivation: To determine if adaptive trials should always be preferred over non-adaptive ones in best-arm identification problems.

Method: Analyzes batched arm elimination designs, comparing their efficiency to non-adaptive randomized trials using the efficiency exponent.

Result: Adaptive designs strictly dominate non-adaptive trials when there are at least three arms.

Conclusion: Adaptive trials are statistically superior and should not be ignored in favor of non-adaptive designs for best-arm identification.

Abstract: When an experimenter has the option of running an adaptive trial, is it
admissible to ignore this option and run a non-adaptive trial instead? We
provide a negative answer to this question in the best-arm identification
problem, where the experimenter aims to allocate measurement efforts
judiciously to confidently deploy the most effective treatment arm. We find
that, whenever there are at least three treatment arms, there exist simple
adaptive designs that universally and strictly dominate non-adaptive completely
randomized trials. This dominance is characterized by a notion called
efficiency exponent, which quantifies a design's statistical efficiency when
the experimental sample is large. Our analysis focuses on the class of batched
arm elimination designs, which progressively eliminate underperforming arms at
pre-specified batch intervals. We characterize simple sufficient conditions
under which these designs universally and strictly dominate completely
randomized trials. These results resolve the second open problem posed in Qin
[2022].

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [255] [Dynamic Epsilon Scheduling: A Multi-Factor Adaptive Perturbation Budget for Adversarial Training](https://arxiv.org/abs/2506.04263)
*Alan Mitkiy,James Smith,Hana Satou,Hiroshi Tanaka,Emily Johnson,F Monkey*

Main category: cs.CV

TL;DR: DES is a dynamic adversarial training method that adapts perturbation budgets per instance and iteration, improving robustness and accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing adversarial training uses fixed budgets, ignoring instance-specific robustness. DES addresses this gap.

Method: DES adjusts budgets using decision boundary distance, prediction confidence, and model uncertainty.

Result: DES outperforms fixed-budget and prior adaptive methods on CIFAR-10/100, enhancing robustness and accuracy.

Conclusion: DES introduces a data-driven, instance-aware approach for adversarial training, with theoretical and empirical support.

Abstract: Adversarial training is among the most effective strategies for defending
deep neural networks against adversarial examples. A key limitation of existing
adversarial training approaches lies in their reliance on a fixed perturbation
budget, which fails to account for instance-specific robustness
characteristics. While prior works such as IAAT and MMA introduce
instance-level adaptations, they often rely on heuristic or static
approximations of data robustness. In this paper, we propose Dynamic Epsilon
Scheduling (DES), a novel framework that adaptively adjusts the adversarial
perturbation budget per instance and per training iteration. DES integrates
three key factors: (1) the distance to the decision boundary approximated via
gradient-based proxies, (2) prediction confidence derived from softmax entropy,
and (3) model uncertainty estimated via Monte Carlo dropout. By combining these
cues into a unified scheduling strategy, DES tailors the perturbation budget
dynamically to guide more effective adversarial learning. Experimental results
on CIFAR-10 and CIFAR-100 show that our method consistently improves both
adversarial robustness and standard accuracy compared to fixed-epsilon
baselines and prior adaptive methods. Moreover, we provide theoretical insights
into the stability and convergence of our scheduling policy. This work opens a
new avenue for instance-aware, data-driven adversarial training methods.

</details>


### [256] [ReXVQA: A Large-scale Visual Question Answering Benchmark for Generalist Chest X-ray Understanding](https://arxiv.org/abs/2506.04353)
*Ankit Pal,Jung-Oh Lee,Xiaoman Zhang,Malaikannan Sankarasubbu,Seunghyeon Roh,Won Jung Kim,Meesun Lee,Pranav Rajpurkar*

Main category: cs.CV

TL;DR: ReXVQA is a large benchmark for VQA in chest radiology, featuring diverse tasks and outperforming human experts with AI models like MedGemma.


<details>
  <summary>Details</summary>
Motivation: To create a comprehensive benchmark for evaluating AI in radiological reasoning, bridging the gap between AI and clinical expertise.

Method: Introduces ReXVQA with 696,000 questions and 160,000 X-rays, testing eight multimodal models and comparing them to human readers.

Result: MedGemma achieved 83.24% accuracy, surpassing human readers (77.27%), with distinct performance patterns between AI and humans.

Conclusion: ReXVQA sets a new standard for radiological AI evaluation, enabling next-generation systems with expert-level reasoning.

Abstract: We present ReXVQA, the largest and most comprehensive benchmark for visual
question answering (VQA) in chest radiology, comprising approximately 696,000
questions paired with 160,000 chest X-rays studies across training, validation,
and test sets. Unlike prior efforts that rely heavily on template based
queries, ReXVQA introduces a diverse and clinically authentic task suite
reflecting five core radiological reasoning skills: presence assessment,
location analysis, negation detection, differential diagnosis, and geometric
reasoning. We evaluate eight state-of-the-art multimodal large language models,
including MedGemma-4B-it, Qwen2.5-VL, Janus-Pro-7B, and Eagle2-9B. The
best-performing model (MedGemma) achieves 83.24% overall accuracy. To bridge
the gap between AI performance and clinical expertise, we conducted a
comprehensive human reader study involving 3 radiology residents on 200
randomly sampled cases. Our evaluation demonstrates that MedGemma achieved
superior performance (83.84% accuracy) compared to human readers (best
radiology resident: 77.27%), representing a significant milestone where AI
performance exceeds expert human evaluation on chest X-ray interpretation. The
reader study reveals distinct performance patterns between AI models and human
experts, with strong inter-reader agreement among radiologists while showing
more variable agreement patterns between human readers and AI models. ReXVQA
establishes a new standard for evaluating generalist radiological AI systems,
offering public leaderboards, fine-grained evaluation splits, structured
explanations, and category-level breakdowns. This benchmark lays the foundation
for next-generation AI systems capable of mimicking expert-level clinical
reasoning beyond narrow pathology classification. Our dataset will be
open-sourced at https://huggingface.co/datasets/rajpurkarlab/ReXVQA

</details>


### [257] [HMAR: Efficient Hierarchical Masked Auto-Regressive Image Generation](https://arxiv.org/abs/2506.04421)
*Hermann Kumbong,Xian Liu,Tsung-Yi Lin,Ming-Yu Liu,Xihui Liu,Ziwei Liu,Daniel Y. Fu,Christopher Ré,David W. Romero*

Main category: cs.CV

TL;DR: HMAR improves upon VAR by introducing hierarchical masked auto-regressive modeling, enabling faster, higher-quality image generation with flexible sampling and lower memory usage.


<details>
  <summary>Details</summary>
Motivation: To address VAR's issues of reduced image quality, superlinear sequence length scaling, and inflexible sampling schedules.

Method: HMAR uses next-scale prediction as a Markovian process and multi-step masked generation for controllable token prediction.

Result: HMAR outperforms VAR, diffusion, and autoregressive models on ImageNet benchmarks, with faster training/inference and lower memory usage.

Conclusion: HMAR offers a superior alternative to VAR, combining efficiency, flexibility, and high-quality image generation.

Abstract: Visual Auto-Regressive modeling (VAR) has shown promise in bridging the speed
and quality gap between autoregressive image models and diffusion models. VAR
reformulates autoregressive modeling by decomposing an image into successive
resolution scales. During inference, an image is generated by predicting all
the tokens in the next (higher-resolution) scale, conditioned on all tokens in
all previous (lower-resolution) scales. However, this formulation suffers from
reduced image quality due to the parallel generation of all tokens in a
resolution scale; has sequence lengths scaling superlinearly in image
resolution; and requires retraining to change the sampling schedule.
  We introduce Hierarchical Masked Auto-Regressive modeling (HMAR), a new image
generation algorithm that alleviates these issues using next-scale prediction
and masked prediction to generate high-quality images with fast sampling. HMAR
reformulates next-scale prediction as a Markovian process, wherein the
prediction of each resolution scale is conditioned only on tokens in its
immediate predecessor instead of the tokens in all predecessor resolutions.
When predicting a resolution scale, HMAR uses a controllable multi-step masked
generation procedure to generate a subset of the tokens in each step. On
ImageNet 256x256 and 512x512 benchmarks, HMAR models match or outperform
parameter-matched VAR, diffusion, and autoregressive baselines. We develop
efficient IO-aware block-sparse attention kernels that allow HMAR to achieve
faster training and inference times over VAR by over 2.5x and 1.75x
respectively, as well as over 3x lower inference memory footprint. Finally,
HMAR yields additional flexibility over VAR; its sampling schedule can be
changed without further training, and it can be applied to image editing tasks
in a zero-shot manner.

</details>


### [258] [Gen-n-Val: Agentic Image Data Generation and Validation](https://arxiv.org/abs/2506.04676)
*Jing-En Huang,I-Sheng Fang,Tzuhsuan Huang,Chih-Yu Wang,Jun-Cheng Chen*

Main category: cs.CV

TL;DR: Gen-n-Val is a novel framework using LLMs and VLLMs to generate high-quality synthetic data for computer vision tasks, reducing invalid data and improving performance.


<details>
  <summary>Details</summary>
Motivation: Addressing data scarcity and label noise in computer vision tasks by improving synthetic data generation.

Method: Leverages Layer Diffusion, LLMs, and VLLMs to create single-object masks and diverse backgrounds, with agents for prompt optimization and data validation.

Result: Reduces invalid synthetic data from 50% to 7% and improves performance by 1% mAP on rare classes in COCO instance segmentation.

Conclusion: Gen-n-Val significantly enhances synthetic data quality and model performance in instance segmentation and object detection.

Abstract: Recently, Large Language Models (LLMs) and Vision Large Language Models
(VLLMs) have demonstrated impressive performance as agents across various tasks
while data scarcity and label noise remain significant challenges in computer
vision tasks, such as object detection and instance segmentation. A common
solution for resolving these issues is to generate synthetic data. However,
current synthetic data generation methods struggle with issues, such as
multiple objects per mask, inaccurate segmentation, and incorrect category
labels, limiting their effectiveness. To address these issues, we introduce
Gen-n-Val, a novel agentic data generation framework that leverages Layer
Diffusion (LD), LLMs, and VLLMs to produce high-quality, single-object masks
and diverse backgrounds. Gen-n-Val consists of two agents: (1) The LD prompt
agent, an LLM, optimizes prompts for LD to generate high-quality foreground
instance images and segmentation masks. These optimized prompts ensure the
generation of single-object synthetic data with precise instance masks and
clean backgrounds. (2) The data validation agent, a VLLM, which filters out
low-quality synthetic instance images. The system prompts for both agents are
refined through TextGrad. Additionally, we use image harmonization to combine
multiple instances within scenes. Compared to state-of-the-art synthetic data
approaches like MosaicFusion, our approach reduces invalid synthetic data from
50% to 7% and improves performance by 1% mAP on rare classes in COCO instance
segmentation with YOLOv9c and YOLO11m. Furthermore, Gen-n-Val shows significant
improvements (7. 1% mAP) over YOLO-Worldv2-M in open-vocabulary object
detection benchmarks with YOLO11m. Moreover, Gen-n-Val improves the performance
of YOLOv9 and YOLO11 families in instance segmentation and object detection.

</details>


### [259] [Using In-Context Learning for Automatic Defect Labelling of Display Manufacturing Data](https://arxiv.org/abs/2506.04717)
*Babar Hussain,Qiang Liu,Gang Chen,Bihai She,Dahai Yu*

Main category: cs.CV

TL;DR: An AI-assisted auto-labeling system for display panel defect detection using enhanced SegGPT and scribble-based annotation, achieving significant performance improvements and reducing manual effort.


<details>
  <summary>Details</summary>
Motivation: To streamline the labeling process in industrial inspection systems and reduce manual annotation efforts by leveraging AI.

Method: Adopts and enhances SegGPT with domain-specific techniques, introduces scribble-based annotation, and uses a two-stage training approach.

Result: Achieves an average IoU increase of 0.22, 14% recall improvement, and 60% auto-labeling coverage, matching human-labeled data performance.

Conclusion: The system offers a practical solution for industrial inspection, reducing reliance on manual labeling while maintaining high accuracy.

Abstract: This paper presents an AI-assisted auto-labeling system for display panel
defect detection that leverages in-context learning capabilities. We adopt and
enhance the SegGPT architecture with several domain-specific training
techniques and introduce a scribble-based annotation mechanism to streamline
the labeling process. Our two-stage training approach, validated on industrial
display panel datasets, demonstrates significant improvements over the baseline
model, achieving an average IoU increase of 0.22 and a 14% improvement in
recall across multiple product types, while maintaining approximately 60%
auto-labeling coverage. Experimental results show that models trained on our
auto-labeled data match the performance of those trained on human-labeled data,
offering a practical solution for reducing manual annotation efforts in
industrial inspection systems.

</details>


### [260] [LotusFilter: Fast Diverse Nearest Neighbor Search via a Learned Cutoff Table](https://arxiv.org/abs/2506.04790)
*Yusuke Matsui*

Main category: cs.CV

TL;DR: LotusFilter is a fast post-processing module to diversify ANNS results by precomputing a cutoff table and greedily removing redundant vectors.


<details>
  <summary>Details</summary>
Motivation: ANNS results can be overly similar; diverse results are needed for applications like RAG.

Method: Precompute a cutoff table of close vectors and greedily filter redundant candidates during search.

Result: Operates fast (0.02 ms/query) in real-world RAG settings using OpenAI embeddings.

Conclusion: LotusFilter effectively diversifies ANNS results efficiently, with code publicly available.

Abstract: Approximate nearest neighbor search (ANNS) is an essential building block for
applications like RAG but can sometimes yield results that are overly similar
to each other. In certain scenarios, search results should be similar to the
query and yet diverse. We propose LotusFilter, a post-processing module to
diversify ANNS results. We precompute a cutoff table summarizing vectors that
are close to each other. During the filtering, LotusFilter greedily looks up
the table to delete redundant vectors from the candidates. We demonstrated that
the LotusFilter operates fast (0.02 [ms/query]) in settings resembling
real-world RAG applications, utilizing features such as OpenAI embeddings. Our
code is publicly available at https://github.com/matsui528/lotf.

</details>


### [261] [Fool the Stoplight: Realistic Adversarial Patch Attacks on Traffic Light Detectors](https://arxiv.org/abs/2506.04823)
*Svetlana Pavlitska,Jamie Robb,Nikolai Polley,Melih Yazgan,J. Marius Zöllner*

Main category: cs.CV

TL;DR: The paper demonstrates adversarial attacks on traffic light detectors using printed patches, showing successful label-flipping and classification attacks in real-world settings.


<details>
  <summary>Details</summary>
Motivation: Despite successful adversarial attacks on camera-based perception tasks in autonomous vehicles, traffic light detectors have been understudied. This work addresses this gap.

Method: The authors propose a threat model where patches are placed under traffic lights and describe a training strategy for adversarial attacks using CNNs.

Result: Experiments confirm successful targeted attacks (e.g., red-to-green label flipping) and pictogram classification attacks, validated in real-world lab settings.

Conclusion: The study highlights vulnerabilities in traffic light detectors and provides practical adversarial attack methods, with code publicly available.

Abstract: Realistic adversarial attacks on various camera-based perception tasks of
autonomous vehicles have been successfully demonstrated so far. However, only a
few works considered attacks on traffic light detectors. This work shows how
CNNs for traffic light detection can be attacked with printed patches. We
propose a threat model, where each instance of a traffic light is attacked with
a patch placed under it, and describe a training strategy. We demonstrate
successful adversarial patch attacks in universal settings. Our experiments
show realistic targeted red-to-green label-flipping attacks and attacks on
pictogram classification. Finally, we perform a real-world evaluation with
printed patches and demonstrate attacks in the lab settings with a mobile
traffic light for construction sites and in a test area with stationary traffic
lights. Our code is available at
https://github.com/KASTEL-MobilityLab/attacks-on-traffic-light-detection.

</details>


### [262] [Quantifying Cross-Modality Memorization in Vision-Language Models](https://arxiv.org/abs/2506.05198)
*Yuxin Wen,Yangsibo Huang,Tom Goldstein,Ravi Kumar,Badih Ghazi,Chiyuan Zhang*

Main category: cs.CV

TL;DR: The paper studies cross-modality memorization in vision-language models, revealing gaps in knowledge transfer between modalities and proposing a baseline mitigation method.


<details>
  <summary>Details</summary>
Motivation: To understand neural network memorization, especially in multimodal contexts, addressing unintentional memorization and effective knowledge acquisition.

Method: Uses a synthetic persona dataset for controlled experiments, training models on one modality and evaluating cross-modal performance.

Result: Facts transfer between modalities but with a significant gap; this persists across scenarios like capable models and unlearning.

Conclusion: Proposes a baseline method to improve cross-modal transferability, encouraging robust multimodal learning research.

Abstract: Understanding what and how neural networks memorize during training is
crucial, both from the perspective of unintentional memorization of potentially
sensitive information and from the standpoint of effective knowledge
acquisition for real-world, knowledge-intensive tasks. While previous studies
primarily investigate memorization within a single modality, such as text
memorization in large language models or image memorization in diffusion
models, unified multimodal models are becoming increasingly prevalent in
practical applications. In this work, we focus on the unique characteristics of
cross-modality memorization and conduct a systematic study centered on
vision-language models. To facilitate controlled experiments, we first
introduce a synthetic persona dataset comprising diverse synthetic person
images and textual descriptions. We quantify factual knowledge memorization and
cross-modal transferability by training models on a single modality and
evaluating their performance in the other. Our results reveal that facts
learned in one modality transfer to the other, but a significant gap exists
between recalling information in the source and target modalities. Furthermore,
we observe that this gap exists across various scenarios, including more
capable models, machine unlearning, and the multi-hop case. At the end, we
propose a baseline method to mitigate this challenge. We hope our study can
inspire future research on developing more robust multimodal learning
techniques to enhance cross-modal transferability.

</details>


### [263] [Stable Vision Concept Transformers for Medical Diagnosis](https://arxiv.org/abs/2506.05286)
*Lijie Hu,Songning Lai,Yuan Hua,Shu Yang,Jingfeng Zhang,Di Wang*

Main category: cs.CV

TL;DR: The paper introduces Vision Concept Transformer (VCT) and Stable Vision Concept Transformer (SVCT) to address limitations of Concept Bottleneck Models (CBMs) in medical AI, ensuring interpretability and faithfulness under perturbations.


<details>
  <summary>Details</summary>
Motivation: Existing CBMs in medical AI overlook intrinsic image features and lack stability under input perturbations, limiting their practical utility.

Method: VCT and SVCT fuse conceptual and image features, using ViT as a backbone. SVCT adds Denoised Diffusion Smoothing for stability.

Result: Experiments on four medical datasets show VCT and SVCT maintain accuracy and interpretability, with SVCT providing stable explanations under perturbations.

Conclusion: VCT and SVCT effectively bridge the utility gap in CBMs, offering interpretable and faithful solutions for medical AI applications.

Abstract: Transparency is a paramount concern in the medical field, prompting
researchers to delve into the realm of explainable AI (XAI). Among these XAI
methods, Concept Bottleneck Models (CBMs) aim to restrict the model's latent
space to human-understandable high-level concepts by generating a conceptual
layer for extracting conceptual features, which has drawn much attention
recently. However, existing methods rely solely on concept features to
determine the model's predictions, which overlook the intrinsic feature
embeddings within medical images. To address this utility gap between the
original models and concept-based models, we propose Vision Concept Transformer
(VCT). Furthermore, despite their benefits, CBMs have been found to negatively
impact model performance and fail to provide stable explanations when faced
with input perturbations, which limits their application in the medical field.
To address this faithfulness issue, this paper further proposes the Stable
Vision Concept Transformer (SVCT) based on VCT, which leverages the vision
transformer (ViT) as its backbone and incorporates a conceptual layer. SVCT
employs conceptual features to enhance decision-making capabilities by fusing
them with image features and ensures model faithfulness through the integration
of Denoised Diffusion Smoothing. Comprehensive experiments on four medical
datasets demonstrate that our VCT and SVCT maintain accuracy while remaining
interpretable compared to baselines. Furthermore, even when subjected to
perturbations, our SVCT model consistently provides faithful explanations, thus
meeting the needs of the medical field.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [264] [A Comprehensive Survey on Bio-Inspired Algorithms: Taxonomy, Applications, and Future Directions](https://arxiv.org/abs/2506.04238)
*Shriyank Somvanshi,Md Monzurul Islam,Syed Aaqib Javed,Gaurab Chhetri,Kazi Sifatul Islam,Tausif Islam Chowdhury,Sazzad Bin Bashar Polock,Anandi Dutta,Subasish Das*

Main category: cs.NE

TL;DR: A survey categorizing bio-inspired algorithms (BIAs) into eight groups, reviewing their principles, strengths, and applications, while highlighting recent advances and open challenges.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive overview of BIAs, their applications, and future research directions for researchers and practitioners.

Method: Categorizes BIAs into eight groups (e.g., evolutionary, swarm intelligence) and reviews their principles, strengths, and limitations. Illustrates applications in fields like machine learning and engineering.

Result: Identifies recent advances in hybridization, parameter tuning, and adaptive strategies, along with open challenges like scalability and interpretability.

Conclusion: Serves as a foundational resource for understanding BIAs' current landscape and future directions, emphasizing the need for addressing scalability and reliability.

Abstract: Bio-inspired algorithms (BIAs) utilize natural processes such as evolution,
swarm behavior, foraging, and plant growth to solve complex, nonlinear,
high-dimensional optimization problems. This survey categorizes BIAs into eight
groups: evolutionary, swarm intelligence, physics-inspired, ecosystem and
plant-based, predator-prey, neural-inspired, human-inspired, and hybrid
approaches, and reviews their core principles, strengths, and limitations. We
illustrate the usage of these algorithms in machine learning, engineering
design, bioinformatics, and intelligent systems, and highlight recent advances
in hybridization, parameter tuning, and adaptive strategies. Finally, we
identify open challenges such as scalability, convergence, reliability, and
interpretability to suggest directions for future research. This work aims to
serve as a foundational resource for both researchers and practitioners
interested in understanding the current landscape and future directions of
bio-inspired computing.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [265] [Artificial Intelligence Should Genuinely Support Clinical Reasoning and Decision Making To Bridge the Translational Gap](https://arxiv.org/abs/2506.05030)
*Kacper Sokol,James Fackler,Julia E Vogt*

Main category: cs.HC

TL;DR: The paper critiques AI's limited impact in medicine due to technology-centric approaches and proposes a sociotechnical framework to better align AI tools with clinical practice.


<details>
  <summary>Details</summary>
Motivation: The translational gap in AI's application to medicine, caused by incompatibility with clinical reasoning, motivates the need for a new approach.

Method: The authors propose a sociotechnical conceptualization of AI tools that complements doctors' cognitive and epistemic activities.

Result: The framework prioritizes real-world clinical impact over achieving superhuman performance on irrelevant benchmarks.

Conclusion: A shift from technology-centric to sociotechnical AI tools is essential for meaningful integration into medical practice.

Abstract: Artificial intelligence promises to revolutionise medicine, yet its impact
remains limited because of the pervasive translational gap. We posit that the
prevailing technology-centric approaches underpin this challenge, rendering
such systems fundamentally incompatible with clinical practice, specifically
diagnostic reasoning and decision making. Instead, we propose a novel
sociotechnical conceptualisation of data-driven support tools designed to
complement doctors' cognitive and epistemic activities. Crucially, it
prioritises real-world impact over superhuman performance on inconsequential
benchmarks.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [266] [kTULA: A Langevin sampling algorithm with improved KL bounds under super-linear log-gradients](https://arxiv.org/abs/2506.04878)
*Iosif Lytras,Sotirios Sabanis,Ying Zhang*

Main category: math.ST

TL;DR: The paper introduces kTULA, a tamed Langevin dynamics-based algorithm for sampling distributions with super-linearly growing log-gradients, offering improved convergence rates in KL divergence and Wasserstein-2 distance.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of sampling from distributions with super-linearly growing log-gradients, common in deep learning where global Lipschitz continuity is often unmet.

Method: Proposes kTULA, a novel tamed Langevin dynamics algorithm, with theoretical guarantees for convergence in KL divergence and Wasserstein-2 distance.

Result: Achieves a non-asymptotic convergence bound in KL divergence with the best-known rate of 2−ϵ¯, improving existing literature, and provides guarantees for optimization problems.

Conclusion: kTULA demonstrates applicability in high-dimensional sampling and neural network optimization, supported by theoretical performance guarantees.

Abstract: Motivated by applications in deep learning, where the global Lipschitz
continuity condition is often not satisfied, we examine the problem of sampling
from distributions with super-linearly growing log-gradients. We propose a
novel tamed Langevin dynamics-based algorithm, called kTULA, to solve the
aforementioned sampling problem, and provide a theoretical guarantee for its
performance. More precisely, we establish a non-asymptotic convergence bound in
Kullback-Leibler (KL) divergence with the best-known rate of convergence equal
to $2-\overline{\epsilon}$, $\overline{\epsilon}>0$, which significantly
improves relevant results in existing literature. This enables us to obtain an
improved non-asymptotic error bound in Wasserstein-2 distance, which can be
used to further derive a non-asymptotic guarantee for kTULA to solve the
associated optimization problems. To illustrate the applicability of kTULA, we
apply the proposed algorithm to the problem of sampling from a high-dimensional
double-well potential distribution and to an optimization problem involving a
neural network. We show that our main results can be used to provide
theoretical guarantees for the performance of kTULA.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [267] [GARG-AML against Smurfing: A Scalable and Interpretable Graph-Based Framework for Anti-Money Laundering](https://arxiv.org/abs/2506.04292)
*Bruno Deprez,Bart Baesens,Tim Verdonck,Wouter Verbeke*

Main category: cs.SI

TL;DR: GARG-AML is a graph-based method for detecting smurfing in money laundering, offering a transparent, efficient, and powerful solution compared to black-box approaches.


<details>
  <summary>Details</summary>
Motivation: Money laundering, particularly smurfing, is a major global issue, but current detection methods lack transparency and practicality for businesses.

Method: GARG-AML uses a single interpretable metric from second-order transaction networks, combined with tree-based methods and neighbor scores.

Result: It outperforms state-of-the-art methods in detecting smurfing on synthetic and open-source networks.

Conclusion: The method demonstrates the effectiveness of fundamental network properties in fraud detection, offering a practical tool for AML workflows.

Abstract: Money laundering poses a significant challenge as it is estimated to account
for 2%-5% of the global GDP. This has compelled regulators to impose stringent
controls on financial institutions. One prominent laundering method for evading
these controls, called smurfing, involves breaking up large transactions into
smaller amounts. Given the complexity of smurfing schemes, which involve
multiple transactions distributed among diverse parties, network analytics has
become an important anti-money laundering tool. However, recent advances have
focused predominantly on black-box network embedding methods, which has
hindered their adoption in businesses. In this paper, we introduce GARG-AML, a
novel graph-based method that quantifies smurfing risk through a single
interpretable metric derived from the structure of the second-order transaction
network of each individual node in the network. Unlike traditional methods,
GARG-AML strikes an effective balance among computational efficiency, detection
power and transparency, which enables its integration into existing AML
workflows. To enhance its capabilities, we combine the GARG-AML score
calculation with different tree-based methods and also incorporate the scores
of the node's neighbours. An experimental evaluation on large-scale synthetic
and open-source networks demonstrate that the GARG-AML outperforms the current
state-of-the-art smurfing detection methods. By leveraging only the adjacency
matrix of the second-order neighbourhood and basic network features, this work
highlights the potential of fundamental network properties towards advancing
fraud detection.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [268] [Benchmark for Antibody Binding Affinity Maturation and Design](https://arxiv.org/abs/2506.04235)
*Xinyan Zhao,Yi-Ching Tang,Akshita Singh,Victor J Cantu,KwanHo An,Junseok Lee,Adam E Stogsdill,Ashwin Kumar Ramesh,Zhiqiang An,Xiaoqian Jiang,Yejin Kim*

Main category: q-bio.QM

TL;DR: AbBiBench is a benchmarking framework for antibody binding affinity and design, evaluating antibody-antigen complexes and comparing 14 protein models.


<details>
  <summary>Details</summary>
Motivation: Existing antibody evaluation methods focus on antibody similarity to natural ones, lacking functional assessment of antibody-antigen interactions.

Method: AbBiBench uses curated datasets (9 antigens, 155,853 antibodies) to compare protein models by correlating model likelihood with experimental affinity. A case study tests generative power for antibody F045-092.

Result: Structure-conditioned inverse folding models perform best in affinity correlation and generation tasks.

Conclusion: AbBiBench offers a unified, biologically grounded framework to improve function-aware antibody design models.

Abstract: We introduce AbBiBench (Antibody Binding Benchmarking), a benchmarking
framework for antibody binding affinity maturation and design. Unlike existing
antibody evaluation strategies that rely on antibody alone and its similarity
to natural ones (e.g., amino acid identity rate, structural RMSD), AbBiBench
considers an antibody-antigen (Ab-Ag) complex as a functional unit and
evaluates the potential of an antibody design binding to given antigen by
measuring protein model's likelihood on the Ab-Ag complex. We first curate,
standardize, and share 9 datasets containing 9 antigens (involving influenza,
anti-lysozyme, HER2, VEGF, integrin, and SARS-CoV-2) and 155,853 heavy chain
mutated antibodies. Using these datasets, we systematically compare 14 protein
models including masked language models, autoregressive language models,
inverse folding models, diffusion-based generative models, and geometric graph
models. The correlation between model likelihood and experimental affinity
values is used to evaluate model performance. Additionally, in a case study to
increase binding affinity of antibody F045-092 to antigen influenza H1N1, we
evaluate the generative power of the top-performing models by sampling a set of
new antibodies binding to the antigen and ranking them based on structural
integrity and biophysical properties of the Ab-Ag complex. As a result,
structure-conditioned inverse folding models outperform others in both affinity
correlation and generation tasks. Overall, AbBiBench provides a unified,
biologically grounded evaluation framework to facilitate the development of
more effective, function-aware antibody design models.

</details>


### [269] [The Latent Space Hypothesis: Toward Universal Medical Representation Learning](https://arxiv.org/abs/2506.04515)
*Salil Patel*

Main category: q-bio.QM

TL;DR: The paper proposes the Latent Space Hypothesis to unify diverse medical data into a shared geometric representation for personalized healthcare, addressing challenges like bias and data scarcity.


<details>
  <summary>Details</summary>
Motivation: To integrate heterogeneous medical data (genomic, imaging, lab results, narratives) into a unified framework for better understanding diseases and enabling personalized care.

Method: The Latent Space Hypothesis models health data as projections of a unified manifold, where health status, disease progression, and interventions are represented as points, trajectories, and vectors.

Result: The framework enables re-examination of diseases (e.g., Parkinson's, Crohn's) by revealing sub-trajectories and patient-specific changes, supporting personalized diagnosis and treatment.

Conclusion: The approach shifts clinical practice from labels to individualized trajectories, with challenges like bias and data scarcity addressed through scale-aware encoders and continual learning.

Abstract: Medical data range from genomic sequences and retinal photographs to
structured laboratory results and unstructured clinical narratives. Although
these modalities appear disparate, many encode convergent information about a
single underlying physiological state. The Latent Space Hypothesis frames each
observation as a projection of a unified, hierarchically organized manifold --
much like shadows cast by the same three-dimensional object. Within this
learned geometric representation, an individual's health status occupies a
point, disease progression traces a trajectory, and therapeutic intervention
corresponds to a directed vector. Interpreting heterogeneous evidence in a
shared space provides a principled way to re-examine eponymous conditions --
such as Parkinson's or Crohn's -- that often mask multiple pathophysiological
entities and involve broader anatomical domains than once believed. By
revealing sub-trajectories and patient-specific directions of change, the
framework supplies a quantitative rationale for personalised diagnosis,
longitudinal monitoring, and tailored treatment, moving clinical practice away
from grouping by potentially misleading labels toward navigation of each
person's unique trajectory. Challenges remain -- bias amplification, data
scarcity for rare disorders, privacy, and the correlation-causation divide --
but scale-aware encoders, continual learning on longitudinal data streams, and
perturbation-based validation offer plausible paths forward.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [270] [Deep Learning-Based Breast Cancer Detection in Mammography: A Multi-Center Validation Study in Thai Population](https://arxiv.org/abs/2506.03177)
*Isarun Chamveha,Supphanut Chaiyungyuen,Sasinun Worakriangkrai,Nattawadee Prasawang,Warasinee Chaisangmongkon,Pornpim Korpraphong,Voraparee Suvannarerg,Shanigarn Thiravit,Chalermdej Kannawat,Kewalin Rungsinaporn,Suwara Issaragrisil,Payia Chadbunchachai,Pattiya Gatechumpol,Chawiporn Muktabhant,Patarachai Sereerat*

Main category: eess.IV

TL;DR: A deep learning system using EfficientNetV2 with attention mechanisms for breast cancer detection in mammograms showed high accuracy (AUROCs 0.89-0.96) and strong concordance with radiologists.


<details>
  <summary>Details</summary>
Motivation: To improve breast cancer detection in mammography using deep learning, enhancing clinical workflows and diagnostic accuracy.

Method: Modified EfficientNetV2 with attention mechanisms, trained on mammograms from a Thai medical center and validated on diverse datasets.

Result: High AUROCs (0.89-0.96), strong radiologist concordance (78.1-84.0%), and good clinical acceptance (SUS scores 69.20-74.17).

Conclusion: The system is effective for mammogram interpretation, with potential to enhance breast cancer screening in clinical practice.

Abstract: This study presents a deep learning system for breast cancer detection in
mammography, developed using a modified EfficientNetV2 architecture with
enhanced attention mechanisms. The model was trained on mammograms from a major
Thai medical center and validated on three distinct datasets: an in-domain test
set (9,421 cases), a biopsy-confirmed set (883 cases), and an out-of-domain
generalizability set (761 cases) collected from two different hospitals. For
cancer detection, the model achieved AUROCs of 0.89, 0.96, and 0.94 on the
respective datasets. The system's lesion localization capability, evaluated
using metrics including Lesion Localization Fraction (LLF) and Non-Lesion
Localization Fraction (NLF), demonstrated robust performance in identifying
suspicious regions. Clinical validation through concordance tests showed strong
agreement with radiologists: 83.5% classification and 84.0% localization
concordance for biopsy-confirmed cases, and 78.1% classification and 79.6%
localization concordance for out-of-domain cases. Expert radiologists'
acceptance rate also averaged 96.7% for biopsy-confirmed cases, and 89.3% for
out-of-domain cases. The system achieved a System Usability Scale score of
74.17 for source hospital, and 69.20 for validation hospitals, indicating good
clinical acceptance. These results demonstrate the model's effectiveness in
assisting mammogram interpretation, with the potential to enhance breast cancer
screening workflows in clinical practice.

</details>


### [271] [Gradient Inversion Attacks on Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2506.04453)
*Hasin Us Sami,Swapneel Sen,Amit K. Roy-Chowdhury,Srikanth V. Krishnamurthy,Basak Guler*

Main category: eess.IV

TL;DR: The paper explores privacy risks in federated learning (FL) when using parameter-efficient fine-tuning (PEFT), showing how maliciously designed pretrained models and adapters can leak user data via gradient inversion attacks.


<details>
  <summary>Details</summary>
Motivation: To highlight vulnerabilities in FL when employing PEFT, where users fine-tune lightweight modules while keeping the backbone model frozen, potentially exposing private data through shared gradients.

Method: The study conducts gradient inversion attacks on adapter modules in PEFT, reconstructing local user data from accessible gradients.

Result: Experiments show that fine-tuning images can be reconstructed with high fidelity, revealing significant privacy risks.

Conclusion: The findings emphasize the need for privacy-preserving mechanisms in PEFT and suggest future research directions to mitigate such vulnerabilities.

Abstract: Federated learning (FL) allows multiple data-owners to collaboratively train
machine learning models by exchanging local gradients, while keeping their
private data on-device. To simultaneously enhance privacy and training
efficiency, recently parameter-efficient fine-tuning (PEFT) of large-scale
pretrained models has gained substantial attention in FL. While keeping a
pretrained (backbone) model frozen, each user fine-tunes only a few lightweight
modules to be used in conjunction, to fit specific downstream applications.
Accordingly, only the gradients with respect to these lightweight modules are
shared with the server. In this work, we investigate how the privacy of the
fine-tuning data of the users can be compromised via a malicious design of the
pretrained model and trainable adapter modules. We demonstrate gradient
inversion attacks on a popular PEFT mechanism, the adapter, which allow an
attacker to reconstruct local data samples of a target user, using only the
accessible adapter gradients. Via extensive experiments, we demonstrate that a
large batch of fine-tuning images can be retrieved with high fidelity. Our
attack highlights the need for privacy-preserving mechanisms for PEFT, while
opening up several future directions. Our code is available at
https://github.com/info-ucr/PEFTLeak.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [272] [Learning to Diagnose Privately: DP-Powered LLMs for Radiology Report Classification](https://arxiv.org/abs/2506.04450)
*Payel Bhattacharjee,Fengwei Tian,Ravi Tandon,Joseph Lo,Heidi Hanson,Geoffrey Rubin,Nirav Merchant,John Gounley*

Main category: cs.CR

TL;DR: A framework for fine-tuning LLMs with differential privacy (DP) is proposed to classify abnormalities in radiology reports, balancing privacy and performance.


<details>
  <summary>Details</summary>
Motivation: To mitigate privacy risks in sensitive patient data while maintaining classification accuracy in radiology reports.

Method: Used 50,232 radiology reports from MIMIC-CXR and CT-RATE datasets, fine-tuned LLMs with DP-LoRA, and evaluated performance using weighted F1 scores across BERT and ALBERT models.

Result: DP fine-tuned models achieved comparable performance (F1: 0.88 on MIMIC-CXR, 0.59 on CT-RATE) under moderate privacy, slightly below non-private baselines (0.90, 0.78).

Conclusion: DP-LoRA enables privacy-preserving multi-abnormality classification, addressing challenges in using LLMs for sensitive medical data.

Abstract: Purpose: This study proposes a framework for fine-tuning large language
models (LLMs) with differential privacy (DP) to perform multi-abnormality
classification on radiology report text. By injecting calibrated noise during
fine-tuning, the framework seeks to mitigate the privacy risks associated with
sensitive patient data and protect against data leakage while maintaining
classification performance. Materials and Methods: We used 50,232 radiology
reports from the publicly available MIMIC-CXR chest radiography and CT-RATE
computed tomography datasets, collected between 2011 and 2019. Fine-tuning of
LLMs was conducted to classify 14 labels from MIMIC-CXR dataset, and 18 labels
from CT-RATE dataset using Differentially Private Low-Rank Adaptation (DP-LoRA)
in high and moderate privacy regimes (across a range of privacy budgets =
{0.01, 0.1, 1.0, 10.0}). Model performance was evaluated using weighted F1
score across three model architectures: BERT-medium, BERT-small, and
ALBERT-base. Statistical analyses compared model performance across different
privacy levels to quantify the privacy-utility trade-off. Results: We observe a
clear privacy-utility trade-off through our experiments on 2 different datasets
and 3 different models. Under moderate privacy guarantees the DP fine-tuned
models achieved comparable weighted F1 scores of 0.88 on MIMIC-CXR and 0.59 on
CT-RATE, compared to non-private LoRA baselines of 0.90 and 0.78, respectively.
Conclusion: Differentially private fine-tuning using LoRA enables effective and
privacy-preserving multi-abnormality classification from radiology reports,
addressing a key challenge in fine-tuning LLMs on sensitive medical data.

</details>


### [273] [EMBER2024 -- A Benchmark Dataset for Holistic Evaluation of Malware Classifiers](https://arxiv.org/abs/2506.05074)
*Robert J. Joyce,Gideon Miller,Phil Roth,Richard Zak,Elliott Zaresky-Williams,Hyrum Anderson,Edward Raff,James Holt*

Main category: cs.CR

TL;DR: EMBER2024 is a new malware dataset addressing limitations of existing public datasets by including diverse file formats, multiple classification tasks, and evasive malware samples.


<details>
  <summary>Details</summary>
Motivation: Existing malware datasets are limited in scope, platform diversity, and lack evasive samples, hindering comprehensive malware analysis research.

Method: The EMBER2024 dataset was created collaboratively, featuring 3.2 million files with hashes, metadata, feature vectors, and labels. It supports seven classification tasks and includes a challenge set of evasive malware.

Result: EMBER2024 enables holistic evaluation of malware classifiers, introduces EMBER feature version 3, and promotes reproducibility in malware research.

Conclusion: EMBER2024 advances malware research by providing a comprehensive, publicly available dataset with diverse and challenging samples.

Abstract: A lack of accessible data has historically restricted malware analysis
research, and practitioners have relied heavily on datasets provided by
industry sources to advance. Existing public datasets are limited by narrow
scope - most include files targeting a single platform, have labels supporting
just one type of malware classification task, and make no effort to capture the
evasive files that make malware detection difficult in practice. We present
EMBER2024, a new dataset that enables holistic evaluation of malware
classifiers. Created in collaboration with the authors of EMBER2017 and
EMBER2018, the EMBER2024 dataset includes hashes, metadata, feature vectors,
and labels for more than 3.2 million files from six file formats. Our dataset
supports the training and evaluation of machine learning models on seven
malware classification tasks, including malware detection, malware family
classification, and malware behavior identification. EMBER2024 is the first to
include a collection of malicious files that initially went undetected by a set
of antivirus products, creating a "challenge" set to assess classifier
performance against evasive malware. This work also introduces EMBER feature
version 3, with added support for several new feature types. We are releasing
the EMBER2024 dataset to promote reproducibility and empower researchers in the
pursuit of new malware research topics.

</details>


### [274] [Membership Inference Attacks on Sequence Models](https://arxiv.org/abs/2506.05126)
*Lorenzo Rossi,Michael Aerni,Jie Zhang,Florian Tramèr*

Main category: cs.CR

TL;DR: The paper addresses privacy leakage in sequence models by leveraging sequential correlations to improve memorization audits.


<details>
  <summary>Details</summary>
Motivation: Existing tools for auditing privacy risks in sequence models like LLMs are inadequate due to mismatched assumptions.

Method: The authors adapt a membership inference attack to model within-sequence correlations, enhancing memorization audits.

Result: The adapted method improves audit effectiveness without added computational costs.

Conclusion: This work advances reliable memorization audits for large sequence models.

Abstract: Sequence models, such as Large Language Models (LLMs) and autoregressive
image generators, have a tendency to memorize and inadvertently leak sensitive
information. While this tendency has critical legal implications, existing
tools are insufficient to audit the resulting risks. We hypothesize that those
tools' shortcomings are due to mismatched assumptions. Thus, we argue that
effectively measuring privacy leakage in sequence models requires leveraging
the correlations inherent in sequential generation. To illustrate this, we
adapt a state-of-the-art membership inference attack to explicitly model
within-sequence correlations, thereby demonstrating how a strong existing
attack can be naturally extended to suit the structure of sequence models.
Through a case study, we show that our adaptations consistently improve the
effectiveness of memorization audits without introducing additional
computational costs. Our work hence serves as an important stepping stone
toward reliable memorization audits for large sequence models.

</details>


### [275] [Why LLM Safety Guardrails Collapse After Fine-tuning: A Similarity Analysis Between Alignment and Fine-tuning Datasets](https://arxiv.org/abs/2506.05346)
*Lei Hsiung,Tianyu Pang,Yung-Chen Tang,Linyue Song,Tsung-Yi Ho,Pin-Yu Chen,Yaoqing Yang*

Main category: cs.CR

TL;DR: The paper examines how the similarity between upstream safety-alignment data and downstream fine-tuning tasks affects the robustness of large language models (LLMs) against jailbreaks. High similarity weakens safety guardrails, while low similarity strengthens them, reducing harmfulness scores by up to 10.33%.


<details>
  <summary>Details</summary>
Motivation: Existing mitigation strategies for LLM jailbreaks focus on reactive measures or fine-tuning adjustments, neglecting the role of upstream safety-alignment data. This paper aims to address this gap by studying how dataset similarity impacts safety guardrail durability.

Method: The study analyzes representation similarity between upstream safety-alignment datasets and downstream fine-tuning tasks. Experiments measure the impact of high vs. low similarity on model robustness and jailbreak susceptibility.

Result: High similarity between datasets weakens safety guardrails, increasing jailbreak vulnerability. Low similarity improves robustness, reducing harmfulness scores by up to 10.33%.

Conclusion: Upstream dataset design is critical for durable safety guardrails. Fine-tuning service providers should prioritize low similarity between alignment and fine-tuning datasets to reduce jailbreak risks.

Abstract: Recent advancements in large language models (LLMs) have underscored their
vulnerability to safety alignment jailbreaks, particularly when subjected to
downstream fine-tuning. However, existing mitigation strategies primarily focus
on reactively addressing jailbreak incidents after safety guardrails have been
compromised, removing harmful gradients during fine-tuning, or continuously
reinforcing safety alignment throughout fine-tuning. As such, they tend to
overlook a critical upstream factor: the role of the original safety-alignment
data. This paper therefore investigates the degradation of safety guardrails
through the lens of representation similarity between upstream alignment
datasets and downstream fine-tuning tasks. Our experiments demonstrate that
high similarity between these datasets significantly weakens safety guardrails,
making models more susceptible to jailbreaks. Conversely, low similarity
between these two types of datasets yields substantially more robust models and
thus reduces harmfulness score by up to 10.33%. By highlighting the importance
of upstream dataset design in the building of durable safety guardrails and
reducing real-world vulnerability to jailbreak attacks, these findings offer
actionable insights for fine-tuning service providers.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [276] [Classification and enumeration of solid-solid phase transition mechanisms](https://arxiv.org/abs/2506.05105)
*Fang-Cheng Wang,Qi-Jun Ye,Yu-Cheng Zhu,Xin-Zheng Li*

Main category: cond-mat.mtrl-sci

TL;DR: A new method, crystmatch, is proposed to classify and enumerate all possible crystal-structure matches (CSMs) for solid-solid phase transitions (SSPTs), solving the CSM optimization problem efficiently.


<details>
  <summary>Details</summary>
Motivation: Existing methods cannot account for all possible CSMs, limiting the understanding of SSPT mechanisms.

Method: The formalism classifies CSMs into a tree structure, representing them as integer matrices, and uses crystmatch to exhaustively enumerate them.

Result: crystmatch reproduces known mechanisms and reveals thousands of new CSM candidates within 10 CPU minutes.

Conclusion: The method provides a comprehensive database for SSPT analysis, enabling applications like experimental comparison and machine learning.

Abstract: Crystal-structure match (CSM), the atom-to-atom correspondence between two
crystalline phases, is used extensively to describe solid-solid phase
transition (SSPT) mechanisms. However, existing computational methods cannot
account for all possible CSMs. Here, we propose a formalism to classify all
CSMs into a tree structure, which is independent of the choices of unit cell
and supercell. We rigorously proved that only a finite number of noncongruent
CSMs are of practical interest. By representing CSMs as integer matrices, we
introduce the crystmatch method to exhaustively enumerate them, which
uncontroversially solves the CSM optimization problem under any geometric
criterion. For most SSPTs, crystmatch can reproduce all known deformation
mechanisms and CSMs within 10 CPU minutes, while also revealing thousands of
new candidates. The resulting database can be further used for comparing
experimental phenomena, high-throughput energy barrier calculations, or machine
learning.

</details>


### [277] [Classification and enumeration of solid-solid phase transition mechanisms](https://arxiv.org/abs/2506.05105)
*Fang-Cheng Wang,Qi-Jun Ye,Yu-Cheng Zhu,Xin-Zheng Li*

Main category: cond-mat.mtrl-sci

TL;DR: A new formalism classifies all crystal-structure matches (CSMs) into a tree structure, solving the CSM optimization problem and revealing new candidates for solid-solid phase transitions.


<details>
  <summary>Details</summary>
Motivation: Existing methods cannot account for all possible CSMs, limiting the understanding of solid-solid phase transition mechanisms.

Method: The crystmatch method represents CSMs as integer matrices and exhaustively enumerates them, independent of unit cell choices.

Result: Crystmatch reproduces known deformation mechanisms and reveals thousands of new CSM candidates within 10 CPU minutes.

Conclusion: The method provides a comprehensive database for analyzing phase transitions, enabling applications in high-throughput calculations and machine learning.

Abstract: Crystal-structure match (CSM), the atom-to-atom correspondence between two
crystalline phases, is used extensively to describe solid-solid phase
transition (SSPT) mechanisms. However, existing computational methods cannot
account for all possible CSMs. Here, we propose a formalism to classify all
CSMs into a tree structure, which is independent of the choices of unit cell
and supercell. We rigorously proved that only a finite number of noncongruent
CSMs are of practical interest. By representing CSMs as integer matrices, we
introduce the crystmatch method to exhaustively enumerate them, which
uncontroversially solves the CSM optimization problem under any geometric
criterion. For most SSPTs, crystmatch can reproduce all known deformation
mechanisms and CSMs within 10 CPU minutes, while also revealing thousands of
new candidates. The resulting database can be further used for comparing
experimental phenomena, high-throughput energy barrier calculations, or machine
learning.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [278] [FlashDMoE: Fast Distributed MoE in a Single Kernel](https://arxiv.org/abs/2506.04667)
*Osayamen Jonathan Aimuyo,Byungsoo Oh,Rachee Singh*

Main category: cs.DC

TL;DR: FlashDMoE is a GPU-resident MoE operator that improves efficiency by fusing computation and communication into a single kernel, achieving significant performance gains over existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing MoE implementations suffer from low GPU utilization, latency overhead, and inability to leverage task locality due to CPU-managed scheduling and frequent kernel launches.

Method: Developed FlashDMoE, a fully GPU-resident MoE operator that fuses expert computation and inter-GPU communication into a single persistent GPU kernel, enabling fine-grained pipelining and device-initiated communication.

Result: Achieves up to 6x lower latency, 5.7x higher throughput, 4x better weak scaling efficiency, and 9x higher GPU utilization compared to state-of-the-art baselines.

Conclusion: Principled GPU kernel-hardware co-design is crucial for unlocking the performance ceiling of large-scale distributed ML workloads.

Abstract: The computational sparsity of Mixture-of-Experts (MoE) models enables
sub-linear growth in compute cost as model size increases, offering a scalable
path to training massive neural networks. However, existing implementations
suffer from \emph{low GPU utilization}, \emph{significant latency overhead},
and a fundamental \emph{inability to leverage task locality}, primarily due to
CPU-managed scheduling, host-initiated communication, and frequent kernel
launches. To overcome these limitations, we develop FlashDMoE, a fully
GPU-resident MoE operator that fuses expert computation and inter-GPU
communication into a \emph{single persistent GPU kernel}. FlashDMoE enables
fine-grained pipelining of dispatch, compute, and combine phases, eliminating
launch overheads and reducing idle gaps. Its device-initiated communication
protocol introduces \emph{payload-efficient} data transfers, significantly
shrinking buffer sizes in sparsely activated MoE layers. When evaluated on a
single 8-H100 GPU node with MoE models having up to 128 experts and 16K token
sequences, FlashDMoE achieves up to \textbf{6}x lower latency, \textbf{5,7}x
higher throughput, \textbf{4}x better weak scaling efficiency, and \textbf{9}x
higher GPU utilization compared to state-of-the-art baselines, despite using
FP32 while baselines use FP16. FlashDMoE demonstrates that principled GPU
kernel-hardware co-design is key to unlocking the performance ceiling of
large-scale distributed ML workloads.

</details>
