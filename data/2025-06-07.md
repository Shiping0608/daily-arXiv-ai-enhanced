<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 12]
- [physics.comp-ph](#physics.comp-ph) [Total: 5]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 4]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [hep-ex](#hep-ex) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [astro-ph.SR](#astro-ph.SR) [Total: 1]
- [math.AP](#math.AP) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Solving engineering eigenvalue problems with neural networks using the Rayleigh quotient](https://arxiv.org/abs/2506.04375)
*Conor Rowan,John Evans,Kurt Maute,Alireza Doostan*

Main category: math.NA

TL;DR: The paper introduces a neural network-based method for solving eigenvalue problems, leveraging the Rayleigh quotient and Gram-Schmidt orthogonalization for robustness and simplicity.


<details>
  <summary>Details</summary>
Motivation: Eigenvalue problems are common in engineering but understudied in physics-informed machine learning, especially with neural network discretizations.

Method: Uses a neural network to discretize eigenfunctions, combining the Rayleigh quotient and Gram-Schmidt orthogonalization to solve eigenvalue problems.

Result: Demonstrates effectiveness for harmonic functions on irregular domains, parametric/nonlinear eigenproblems, and high-dimensional eigenanalysis.

Conclusion: The approach offers unique advantages for continuous eigenvalue problems in engineering mechanics.

Abstract: From characterizing the speed of a thermal system's response to computing
natural modes of vibration, eigenvalue analysis is ubiquitous in engineering.
In spite of this, eigenvalue problems have received relatively little treatment
compared to standard forward and inverse problems in the physics-informed
machine learning literature. In particular, neural network discretizations of
solutions to eigenvalue problems have seen only a handful of studies. Owing to
their nonlinearity, neural network discretizations prevent the conversion of
the continuous eigenvalue differential equation into a standard discrete
eigenvalue problem. In this setting, eigenvalue analysis requires more
specialized techniques. Using a neural network discretization of the
eigenfunction, we show that a variational form of the eigenvalue problem called
the "Rayleigh quotient" in tandem with a Gram-Schmidt orthogonalization
procedure is a particularly simple and robust approach to find the eigenvalues
and their corresponding eigenfunctions. This method is shown to be useful for
finding sets of harmonic functions on irregular domains, parametric and
nonlinear eigenproblems, and high-dimensional eigenanalysis. We also discuss
the utility of harmonic functions as a spectral basis for approximating
solutions to partial differential equations. Through various examples from
engineering mechanics, the combination of the Rayleigh quotient objective,
Gram-Schmidt procedure, and the neural network discretization of the
eigenfunction is shown to offer unique advantages for handling continuous
eigenvalue problems.

</details>


### [2] [Exponential Time Differencing Runge-Kutta Discontinuous Galerkin (ETD-RKDG) Methods for Nonlinear Degenerate Parabolic Equations](https://arxiv.org/abs/2506.04416)
*Ziyao Xu,Yong-Tao Zhang*

Main category: math.NA

TL;DR: High-order ETD-RK DG methods are proposed for nonlinear degenerate parabolic equations, addressing stiffness and wave-front capture with stabilizing limiters and nodal formulations.


<details>
  <summary>Details</summary>
Motivation: To solve nonlinear degenerate parabolic equations with mixed hyperbolic-parabolic behavior, which pose challenges like sharp wave fronts and stiff parabolic terms.

Method: Uses DG methods with stabilizing limiters on unstructured meshes and ETD-RK for time integration, leveraging the Jacobian matrix for stiffness extraction.

Result: Demonstrates improved stability and allows larger time-step sizes, validated through linear stability analysis and 3D computational results.

Conclusion: The proposed method effectively handles the stiffness and wave-front challenges, offering robust solutions for complex domains.

Abstract: In this paper, we study high-order exponential time differencing Runge-Kutta
(ETD-RK) discontinuous Galerkin (DG) methods for nonlinear degenerate parabolic
equations. This class of equations exhibits hyperbolic behavior in degenerate
regions and parabolic behavior in non-degenerate regions, resulting in sharp
wave fronts in the solution profiles and a parabolic-type time-step
restriction, $\tau \sim O(h^2)$, for explicit time integration. To address
these challenges and solve such equations in complex domains, we employ DG
methods with appropriate stabilizing limiters on unstructured meshes to capture
the wave fronts and use ETD-RK methods for time integration to resolve the
stiffness of parabolic terms. We extract the system's stiffness using the
Jacobian matrix of the DG discretization for diffusion terms and adopt a nodal
formulation to facilitate its computation. The algorithm is described in detail
for two-dimensional triangular meshes. We also conduct a linear stability
analysis in one spatial dimension and present computational results on
three-dimensional simplex meshes, demonstrating significant improvements in
stability and large time-step sizes.

</details>


### [3] [An Augmented Lagrangian Preconditioner for Navier--Stokes Equations with Runge--Kutta in Time](https://arxiv.org/abs/2506.04451)
*Santolo Leveque,Yunhui He,Maxim Olshanskii*

Main category: math.NA

TL;DR: A Runge-Kutta method is used for time integration of the incompressible Navier-Stokes equations, with nonlinear problems solved via Newton's method and a preconditioned GMRES approach.


<details>
  <summary>Details</summary>
Motivation: To efficiently solve the nonlinear systems arising from the Runge-Kutta time integration of the Navier-Stokes equations.

Method: Finite element discretization, Newton's method for nonlinear systems, and an augmented Lagrangian preconditioner with flexible GMRES and multigrid support.

Result: Numerical results demonstrate robustness and efficiency across varying parameters like viscosity, mesh size, and time step.

Conclusion: The proposed preconditioned GMRES strategy is effective for solving the nonlinear systems in this context.

Abstract: We consider a Runge--Kutta method for the numerical time integration of the
nonstationary incompressible Navier--Stokes equations. This yields a sequence
of nonlinear problems to be solved for the stages of the Runge--Kutta method.
The resulting nonlinear system of differential equations is discretized using a
finite element method. To compute a numerical approximation of the stages at
each time step, we employ Newton's method, which requires the solution of a
large and sparse generalized saddle-point problem at each nonlinear iteration.
We devise an augmented Lagrangian preconditioner within the flexible GMRES
method for solving the Newton systems at each time step. The preconditioner can
be applied inexactly with the help of a multigrid routine. We present numerical
evidence of the robustness and efficiency of the proposed strategy for
different values of the viscosity, mesh size, time step, and number of stages
of the Runge--Kutta method.

</details>


### [4] [An Array Decomposition Method for Finite Arrays with Electrically Connected Elements for fast Toeplitz Solvers](https://arxiv.org/abs/2506.04710)
*Lucas Åkerstedt,Harald Hultin,B. L. G. Jonsson*

Main category: math.NA

TL;DR: A memory-efficient method for constructing impedance matrices in array antennas using multilevel block Toeplitz structure, reducing memory usage from O(N_x^2 N_y^2) to O(N_x N_y).


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency in memory usage and computation for array antennas with finite translational symmetries.

Method: Uses MoM with RWG-like elements on structured meshes, introduces nine connectable components, and leverages multilevel block Toeplitz matrices.

Result: Demonstrated by computing far-fields for a 32x32 array and scattering parameters for two 9x9 arrays, showing significant memory reduction.

Conclusion: The method efficiently handles non-symmetric parts of arrays and improves computational performance for MoM current vector calculations.

Abstract: A large part of the geometry of array antennas is often partially defined by
finite translational symmetries. Applying the method of moments (MoM) with the
RWG-like element on an appropriately structured mesh to these arrays results in
an impedance matrix where the main part exhibits a multilevel block Toeplitz
structure. This article introduces a memory-efficient construction method that
effectively represents and reuses impedance calculations. The proposed method,
applicable to electrically connected elements, also accounts for all
non-symmetric parts of the array. The core idea involves nine distinct
electrically connectable components from which the array can be assembled. The
derived multilevel block Toeplitz matrix is further utilized by an in-house
inverse solver to achieve faster and more memory-efficient MoM current vector
calculations. We demonstrate the method by computing the far-field of a 32x32
array and the scattering parameters of two tightly coupled 9x9 arrays. This
approach reduces the memory allocation from $\mathcal{O}(N_x^2 N_y^2)$ to
$\mathcal{O}(N_x N_y)$, for an $N_x \times N_y$ array.

</details>


### [5] [A Fast, Accurate and Oscillation-free Spectral Collocation Solver for High-dimensional Transport Problems](https://arxiv.org/abs/2506.04732)
*Nicola Cavallini,Gianmarco Manzini,Daniele Funaro,Andrea Favalli*

Main category: math.NA

TL;DR: The paper introduces the T²S² solver to efficiently solve high-dimensional transport equations, combining Spectral Collocation, Superconsistency, and Tensor Train format for accuracy and compression.


<details>
  <summary>Details</summary>
Motivation: Transport phenomena are critical but solving their high-dimensional equations is computationally challenging due to dimensionality.

Method: The T²S² solver integrates Spectral Collocation, Superconsistency, and Tensor Train format for exponential convergence, stabilization, and data compression.

Result: Achieves extremely low compression ratios (~10⁻¹²) and solves high-dimensional problems in minutes on standard hardware.

Conclusion: T²S² enables efficient and accurate modeling of complex transport phenomena, making previously intractable problems feasible.

Abstract: Transport phenomena-describing the movement of particles, energy, or other
physical quantities-are fundamental in various scientific disciplines,
including nuclear physics, plasma physics, astrophysics, engineering, and the
natural sciences.
  However, solving the associated seven-dimensional transport equations poses a
significant computational challenge due to the curse of dimensionality.
  We introduce the Tensor Train Superconsistent Spectral (T${^2}$S${^2}$)
solver to address this challenge, integrating Spectral Collocation for
exponential convergence, Superconsistency for stabilization in
transport-dominated regimes, and Tensor Train format for substantial data
compression. T${^2}$S${^2}$ enforces a dimension-wise superconsistent condition
compatible with tensor structures, achieving extremely low compression ratios,
in the order of $(10^{-12})$, while preserving spectral accuracy. Numerical
experiments on linear problems demonstrate that T${^2}$S${^2}$ can solve
high-dimensional transport problems in minutes on standard hardware, making
previously intractable problems computationally feasible. This advancement
opens new avenues for efficiently and accurately modeling complex transport
phenomena.

</details>


### [6] [Numerical solution of the wave equation outside a sphere](https://arxiv.org/abs/2506.04809)
*Michael J. Carley*

Main category: math.NA

TL;DR: A method for fast evaluation of transient acoustic fields outside a spherical surface using Lebedev quadratures and Lagrange interpolation, achieving near machine-precision accuracy and speed-up.


<details>
  <summary>Details</summary>
Motivation: To efficiently and accurately evaluate transient acoustic fields generated by internal sources outside a spherical surface.

Method: Uses Lebedev quadratures for spatial integration and Lagrange interpolation/differentiation in an advanced time algorithm.

Result: Near machine-precision accuracy; speed-up depends on quadrature order, breaking even at ~1.15 times the number of surface nodes.

Conclusion: The method is efficient and accurate for transient acoustic field evaluation.

Abstract: A method is presented for the fast evaluation of the transient acoustic field
generated outside a spherical surface by sources inside the surface. The method
employs Lebedev quadratures, which are the optimal method for spatial
integration, and Lagrange interpolation and differentiation in an advanced time
algorithm for the evaluation of the transient field. Numerical testing
demonstrates that the approach gives near machine-precision accuracy and a
speed-up in evaluation time which depends on the order of quadrature rule
employed but breaks even with direct evaluation at a number of field points
about 1.15 times the number of surface quadrature nodes.

</details>


### [7] [Tensor-based multivariate function approximation: methods benchmarking and comparison](https://arxiv.org/abs/2506.04791)
*Athanasios C. Antoulas,Ion Victor Gosea,Charles Poussot-Vassal,Pierre Vuillemin*

Main category: math.NA

TL;DR: The paper evaluates various tensor-based multivariate function approximation methods, comparing their performance, features, and user experience using a benchmark collection of functions.


<details>
  <summary>Details</summary>
Motivation: To fairly assess and compare different tensor approximation tools, guiding users in understanding their processes, advantages, and limitations.

Method: Construct tensors from a diverse set of multivariate functions, evaluate method performances (accuracy, computational time, parameter tuning), and compare methods using multiple criteria.

Result: A benchmark collection is provided, and the multivariate Loewner Framework (mLF) is highlighted with detailed examples.

Conclusion: The note aims to guide users in selecting tensor approximation tools without ranking them, emphasizing understanding and practical insights.

Abstract: In this note, we evaluate the performances, the features and the
user-experience of some methods (and their implementations) designed for
tensor- (or data-) based multivariate function construction and approximation.
To this aim, a collection of multivariate functions extracted from contributive
works coming from different communities, is suggested. First, these functions
with varying complexity (e.g. number and degree of the variables) and nature
(e.g. rational, irrational, differentiable or not, symmetric, etc.) are used to
construct tensors, each of different dimension and size on the disk. Second,
grounded on this tensor, we inspect performances of each considered method
(e.g. the accuracy, the computational time, the parameters tuning impact,
etc.). Finally, considering the "best" parameter tuning set, we compare each
method using multiple evaluation criteria. The purpose of this note is not to
rank the methods but rather to evaluate as fairly as possible the different
available strategies, with the idea in mind to guide users to understand the
process, the possibilities, the advantages and the limits brought by each
tools. The contribution claimed is to suggest a complete benchmark collection
of some available tools for tensor approximation by surrogate models (e.g.
rational functions, networks, etc.). In addition, as contributors of the
multivariate Loewner Framework (mLF) approach (and its side implementation in
MDSPACK), attention and details of the latter are more explicitly given, in
order to provide readers a digest of this contributive work and some details
with simple examples.

</details>


### [8] [Efficient randomized algorithms for the fixed Tucker-rank problem of Tucker decomposition with adaptive shifts](https://arxiv.org/abs/2506.04840)
*Maolin Che,Yimin Wei,Chong Wu,Hong Yan*

Main category: math.NA

TL;DR: The paper introduces fast randomized algorithms for Tucker decomposition using adaptive shifted power iterations, improving efficiency and accuracy over existing methods.


<details>
  <summary>Details</summary>
Motivation: To bridge theoretical advancements with scalable solutions for tensor decomposition, addressing the fixed Tucker-rank problem.

Method: Integration of adaptive shifted power iterations into randomized variants of T-HOSVD and ST-HOSVD, refining singular value gaps to accelerate convergence.

Result: Theoretical error bounds and numerical experiments show the algorithms outperform state-of-the-art techniques in runtime and approximation error.

Conclusion: The proposed methods offer efficient, robust, and accurate solutions for Tucker decomposition, validated by synthetic and real-world datasets.

Abstract: Randomized numerical linear algebra is proved to bridge theoretical
advancements to offer scalable solutions for approximating tensor
decomposition. This paper introduces fast randomized algorithms for solving the
fixed Tucker-rank problem of Tucker decomposition, through the integration of
adaptive shifted power iterations. The proposed algorithms enhance randomized
variants of truncated high-order singular value decomposition (T-HOSVD) and
sequentially T-HOSVD (ST-HOSVD) by incorporating dynamic shift strategies,
which accelerate convergence by refining the singular value gap and reduce the
number of required power iterations while maintaining accuracy. Theoretical
analyses provide probabilistic error bounds, demonstrating that the proposed
methods achieve comparable or superior accuracy compared to deterministic
approaches. Numerical experiments on synthetic and real-world datasets validate
the efficiency and robustness of the proposed algorithms, showing a significant
decline in runtime and approximation error over state-of-the-art techniques.

</details>


### [9] [Active flux for ideal magnetohydrodynamics: A positivity-preserving scheme with the Godunov-Powell source term](https://arxiv.org/abs/2506.04857)
*Junming Duan,Praveen Chandrashekar,Christian Klingenberg*

Main category: math.NA

TL;DR: A positivity-preserving Active Flux scheme for ideal magnetohydrodynamics is proposed, incorporating the Godunov-Powell source term for divergence-free constraints and using limiters to ensure density and pressure positivity.


<details>
  <summary>Details</summary>
Motivation: To develop a high-order, compact finite volume scheme for ideal magnetohydrodynamics that maintains positivity of density and pressure while handling divergence-free constraints effectively.

Method: The scheme combines the Active Flux method with the Godunov-Powell source term, uses quadratic reconstruction for nonconservative terms, and employs flux and scaling limiters for positivity. A new shock sensor is introduced to control oscillations.

Result: Numerical tests confirm third-order accuracy, positivity preservation, and effective shock-capturing. The Godunov-Powell source term's role in controlling divergence error is validated.

Conclusion: The proposed scheme successfully balances high-order accuracy, compactness, and positivity preservation, demonstrating robustness in handling magnetohydrodynamics problems.

Abstract: The Active Flux (AF) is a compact, high-order finite volume scheme that
allows more flexibility by introducing additional point value degrees of
freedom at cell interfaces. This paper proposes a positivity-preserving (PP) AF
scheme for solving the ideal magnetohydrodynamics, where the Godunov-Powell
source term is employed to deal with the divergence-free constraint. For the
evolution of the cell average, apart from the standard conservative finite
volume method for the flux derivative, the nonconservative source term is built
on the quadratic reconstruction in each cell, which maintains the compact
stencil in the AF scheme. For the point value update, the local Lax-Friedrichs
(LLF) flux vector splitting is adopted for the flux derivative, originally
proposed in [Duan, Barsukow, and Klingenberg, SIAM Journal on Scientific
Computing, 47(2), A811--A837, 2025], and a central difference is used to
discretize the divergence in the source term. A parametrized flux limiter and a
scaling limiter are presented to preserve the density and pressure positivity
by blending the AF scheme with the first-order PP LLF scheme with the source
term. To suppress oscillations, a new shock sensor considering the divergence
error is proposed, which is used to compute the blending coefficients for the
cell average. Several numerical tests are conducted to verify the third-order
accuracy, PP property, and shock-capturing ability of the scheme. The key role
of the Godunov-Powell source term and its suitable discretization in
controlling divergence error is also validated.

</details>


### [10] [Numerical analysis for constrained and unconstrained Q-tensor energies for liquid crystals](https://arxiv.org/abs/2506.04880)
*Heiko Gimperlein,Ruma R. Maity*

Main category: math.NA

TL;DR: A finite element framework for 3D Landau-de Gennes Q-tensor energies, focusing on anisotropy and physical constraints, with rigorous analysis of well-posedness, convergence, and error impact.


<details>
  <summary>Details</summary>
Motivation: To model nematic liquid crystals realistically by addressing anisotropy and physical constraints in the Q-tensor energy framework.

Method: Finite element approximation for nonlinear elliptic PDEs with non-homogeneous boundary conditions, using the Newton-Kantorovich theorem for discrete solutions.

Result: Well-posedness of the discrete problem, local uniqueness of solutions, and optimal convergence rates in the energy norm.

Conclusion: The framework effectively models nematic liquid crystals with rigorous theoretical backing and practical convergence guarantees.

Abstract: This paper introduces a comprehensive finite element approximation framework
for three-dimensional Landau-de Gennes $Q$-tensor energies for nematic liquid
crystals, with a particular focus on the anisotropy of the elastic energy and
the Ball-Majumdar singular potential. This potential imposes essential physical
constraints on the eigenvalues of the $Q$-tensor, ensuring realistic modeling.
We address the approximation of regular solutions to nonlinear elliptic partial
differential equations with non-homogeneous boundary conditions associated with
Landau-de Gennes energies. The well-posedness of the discrete linearized
problem is rigorously demonstrated. The existence and local uniqueness of the
discrete solution is derived using the Newton-Kantorovich theorem. Furthermore,
we demonstrate an optimal order convergence rate in the energy norm and discuss
the impact of eigenvalue constraints on the a priori error analysis.

</details>


### [11] [Probability of Collision with Tethered Spacecraft](https://arxiv.org/abs/2506.04969)
*Yema Paul,Emmanuel Delande,Francois Vinet,Francois Laporte,Manuel Sanjurjo-Rivo,Aldo Tonnini,Joan-Pau Sanchez*

Main category: math.NA

TL;DR: A method to estimate collision probability for tethered spacecraft, addressing conservatism in standard spherical models by accounting for tether extent and uncertainty.


<details>
  <summary>Details</summary>
Motivation: Standard collision probability methods are overly conservative for tethered spacecraft due to spherical hard-body assumptions.

Method: Maximizes collision probability over all physically admissible tether shapes, considering spatial extent and configuration uncertainty.

Result: Applied to real-world events, the method provides more realistic risk estimates for kilometer-scale tethers.

Conclusion: Improves hazard distinction, aiding better collision avoidance decisions.

Abstract: This Engineering Note addresses the challenge of estimating the probability
of collision for tethered spacecraft during close encounters with other space
objects. Standard probability of collision methods, based on spherical
hard-body assumptions, tend to be overly conservative when applied to long
tether systems. We introduce a method that accounts for the tether's spatial
extent and configuration uncertainty by maximizing the probability of collision
over all physically admissible tether shapes. Applied to real-world conjunction
events involving a kilometer-scale flexible inextensible tether, the method
yields more realistic risk estimates. This approach improves the ability to
distinguish hazardous from benign encounters, thereby supporting more informed
collision avoidance decisions.

</details>


### [12] [Norming Sets for Tensor and Polynomial Sketching](https://arxiv.org/abs/2506.05174)
*Yifan Zhang,Joe Kileel*

Main category: math.NA

TL;DR: The paper introduces a sketching theory for real algebraic varieties and polynomial maps, proposing a new method called the median sketch, which uses fewer measurements.


<details>
  <summary>Details</summary>
Motivation: To extend sketching theory to algebraic varieties and polynomial maps, including low-rank tensors, and provide a universal framework for controlling sketching dimensions.

Method: Uses norming set theory to develop the median sketch, requiring only ~O(dim V) tensor-structured or sparse linear measurements.

Result: The median sketch efficiently embeds sets like low-rank tensors with reduced measurements.

Conclusion: The framework and median sketch advance sketching theory for algebraic structures, offering practical efficiency.

Abstract: This paper develops the sketching (i.e., randomized dimension reduction)
theory for real algebraic varieties and images of polynomial maps, including,
e.g., the set of low rank tensors and tensor networks. Through the lens of
norming sets, we provide a framework for controlling the sketching dimension
for \textit{any} sketch operator used to embed said sets, including
sub-Gaussian, fast Johnson-Lindenstrauss, and tensor structured sketch
operators. Leveraging norming set theory, we propose a new sketching method
called the median sketch. It embeds such a set $V$ using only
$\widetilde{\mathcal{O}}(\dim V)$ tensor structured or sparse linear
measurements.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [13] [BridgeNet: A Hybrid, Physics-Informed Machine Learning Framework for Solving High-Dimensional Fokker-Planck Equations](https://arxiv.org/abs/2506.04354)
*Elmira Mirzabeigi,Rezvan Salehi,Kourosh Parand*

Main category: physics.comp-ph

TL;DR: BridgeNet combines CNNs and physics-informed neural networks to solve complex Fokker-Planck equations more efficiently than traditional PINNs, achieving better accuracy and stability.


<details>
  <summary>Details</summary>
Motivation: Traditional PINNs struggle with spatial hierarchies and boundary conditions in high-dimensional FPEs, prompting the need for a hybrid approach.

Method: BridgeNet uses adaptive CNN layers for local feature extraction and a dynamically weighted loss function to enforce physical constraints.

Result: BridgeNet outperforms conventional PINNs with lower errors, faster convergence, and robust stability in high-dimensional settings.

Conclusion: BridgeNet advances computational physics, offering scalable and accurate solutions for applications in finance and complex systems.

Abstract: BridgeNet is a novel hybrid framework that integrates convolutional neural
networks with physics-informed neural networks to efficiently solve non-linear,
high-dimensional Fokker-Planck equations (FPEs). Traditional PINNs, which
typically rely on fully connected architectures, often struggle to capture
complex spatial hierarchies and enforce intricate boundary conditions. In
contrast, BridgeNet leverages adaptive CNN layers for effective local feature
extraction and incorporates a dynamically weighted loss function that
rigorously enforces physical constraints. Extensive numerical experiments
across various test cases demonstrate that BridgeNet not only achieves
significantly lower error metrics and faster convergence compared to
conventional PINN approaches but also maintains robust stability in
high-dimensional settings. This work represents a substantial advancement in
computational physics, offering a scalable and accurate solution methodology
with promising applications in fields ranging from financial mathematics to
complex system dynamics.

</details>


### [14] [Spectrally accurate and efficient convolution with the 3D free-space Laplace Green's function via the super-potential](https://arxiv.org/abs/2506.04489)
*Lukas Exl,Sebastian Schaffer*

Main category: physics.comp-ph

TL;DR: A high-accuracy spectral method for solving the 3D Poisson equation with smooth, compactly supported sources, using a super-potential formulation and Gaussian-sum approximation for efficient FFT-based computation.


<details>
  <summary>Details</summary>
Motivation: To address the need for a robust and efficient method for solving unbounded 3D Poisson equations with high accuracy and quasi-linear complexity.

Method: Uses a super-potential formulation involving the biharmonic Green's function and a separable Gaussian-sum approximation for FFT-based computation.

Result: Achieves machine-precision accuracy, outperforms existing Gaussian-sum-based schemes in error and runtime, and eliminates the need for correction terms.

Conclusion: The method is a robust and efficient tool for free-space Poisson problems on uniform grids.

Abstract: We present a high-accuracy spectral method for solving the unbounded
three-dimensional Poisson equation with smooth, compactly supported sources.
The approach is based on a super-potential formulation, where the solution is
obtained by applying the Laplacian to a convolution with the biharmonic Green's
function. A separable Gaussian-sum (GS) approximation enables efficient
FFT-based computation with quasi-linear complexity. Owing to the improved
regularity of the biharmonic kernel, the GS cutoff error is of order four,
eliminating the need for correction terms or Taylor expansions required in
standard GS or Ewald-type methods. Numerical benchmarks demonstrate that the
method achieves machine-precision accuracy and outperforms existing GS-based
schemes in both error and runtime, making it a robust and efficient tool for
free-space Poisson problems on uniform grids.

</details>


### [15] [A highly scalable numerical framework for reservoir simulation on UG4 platform](https://arxiv.org/abs/2506.04763)
*Shuai Lu*

Main category: physics.comp-ph

TL;DR: The paper presents a fully coupled, fully implicit framework for simulating multiphase flow in heterogeneous porous media, using advanced numerical methods and demonstrating scalability on supercomputers.


<details>
  <summary>Details</summary>
Motivation: Existing time discretization schemes for multiphase flow often decouple saturation and pressure equations, limiting accuracy. This study aims to improve modeling by fully coupling these equations and incorporating gravity and capillary effects.

Method: The study employs the Vertex-Centered Finite Volume Method for spatial discretization, introduces the Linearly Implicit Extrapolation Method (LIMEX) with an error estimator, and uses BiCGSTAB with Geometric Multigrid preconditioning for solving linear systems.

Result: The framework shows strong scalability, supporting thousands of processors and billions of Degrees of Freedom (DoF) in parallel computations on a supercomputer.

Conclusion: The proposed fully implicit framework is efficient and scalable, offering improved accuracy for multiphase flow simulations in heterogeneous porous media.

Abstract: The modeling and simulation of multiphase fluid flow receive significant
attention in reservoir engineering. Many time discretization schemes for
multiphase flow equations are either explicit or semi-implicit, relying on the
decoupling between the saturation equation and the pressure equation. In this
study, we delve into a fully coupled and fully implicit framework for
simulating multiphase flow in heterogeneous porous media, considering gravity
and capillary effects. We utilize the Vertex-Centered Finite Volume Method for
spatial discretization and propose an efficient implementation of interface
conditions for heterogeneous porous media within the current scheme. Notably,
we introduce the Linearly Implicit Extrapolation Method (LIMEX) with an error
estimator, adapted for the first time to multiphase flow problems. To solve the
resulting linear system, we employ the BiCGSTAB method with the Geometric
Multigrid (GMG) preconditioner. The implementations of models and methods are
based on the open-source software: UG4. The results from parallel computations
on the supercomputer demonstrate that the scalability of our proposed framework
is sufficient, supporting a scale of thousands of processors with Degrees of
Freedom (DoF) extending up to billions.

</details>


### [16] [Thermoplasmonics of Gold-Core Silica-Shell Colloidal Nanoparticles under Pulse Illumination](https://arxiv.org/abs/2506.04835)
*Julien El Hajj,Gilles Ledoux,Samy Merabia*

Main category: physics.comp-ph

TL;DR: Study explores thermal dynamics of gold-core silica-shell nanoparticles under pulse illumination, revealing faster water heating with thin dense silica shells due to enhanced electron-phonon coupling.


<details>
  <summary>Details</summary>
Motivation: Core-shell nanoparticles, especially gold-core ones, are promising for photothermal therapy, imaging, and biosensing due to their unique properties. Understanding their thermal dynamics is key for optimizing applications.

Method: Combines Mie theory with electronic temperature corrections and couples the two-temperature model with molecular dynamics simulations to study thermal response under varied laser pulses and fluences.

Result: Nanoparticles with thin dense silica shells (5 nm) heat water faster than bare gold nanoparticles, attributed to enhanced electron-phonon coupling and high silica-water thermal conductance.

Conclusion: Findings offer insights for optimizing nanoparticle design for photothermal applications and understanding energy transfer in metal-dielectric nanostructures.

Abstract: Core-shell nanoparticles, particularly those having a gold core, have emerged
as a highly promising class of materials due to their unique optical and
thermal properties, which underpin a wide range of applications in photothermal
therapy, imaging, and biosensing. In this study, we present a comprehensive
study of the thermal dynamics of gold-core silica-shell nanoparticles immersed
in water under pulse illumination. The plasmonic response of the core-shell
nanoparticle is described by incorporating Mie theory with electronic
temperature corrections to the refractive indices of gold, based on a Drude
Lorentz formulation. The thermal response of the core-shell nanoparticles is
modeled by coupling the two temperature model with molecular dynamics
simulations, providing an atomistic description of nanoscale heat transfer. We
investigate nanoparticles with both dense and porous silica shells (with 50%
porosity) under laser pulse durations of 100 fs, 10 ps, and 1 ns, and over a
range of fluences between 0.05 and 5mJ/cm2. We show that nanoparticles with a
thin dense silica shell (5 nm) exhibit significantly faster water heating
compared to bare gold nanoparticles. This behavior is attributed to enhanced
electron-phonon coupling at the gold silica interface and to the relatively
high thermal conductance between silica and water. These findings provide new
insights into optimizing nanoparticle design for efficient photothermal
applications and establish a robust framework for understanding energy transfer
mechanisms in heterogeneous metal dielectric nanostructures.

</details>


### [17] [Reduction of Outflow Boundary Influence on Aerodynamic Performance using Neural Networks](https://arxiv.org/abs/2506.05293)
*Mario Christopher Bedrunka,Dirk Reith,Holger Foysi,Łukasz Łaniewski-Wołłk,Travis Mitchell*

Main category: physics.comp-ph

TL;DR: Neural networks (NN) are used to improve outflow boundary conditions in LBM, reducing artificial reflections and computational costs. Two NN-based methods show superior accuracy in aerodynamic predictions.


<details>
  <summary>Details</summary>
Motivation: Accurate outflow boundary conditions are critical in CFD for predicting aerodynamic forces and acoustic emissions, especially in LBM, which suffers from artificial reflections.

Method: Two NN-based approaches: (1) direct reconstruction of particle distribution functions at outflow boundaries, and (2) dynamic tuning of characteristic boundary conditions (CBC) parameters.

Result: NN-based methods improved predictions for drag, lift, and Strouhal numbers in 2D flow and NACA0012 airfoil simulations. Neural-enhanced CBC minimized density errors in vortex benchmarks.

Conclusion: NN-integrated boundary conditions enhance accuracy and reduce computational costs in LBM simulations for aerodynamics and acoustics.

Abstract: The accurate treatment of outflow boundary conditions remains a critical
challenge in computational fluid dynamics when predicting aerodynamic forces
and/or acoustic emissions. This is particularly evident when employing the
lattice Boltzmann method (LBM) as the numerical solution technique, which often
suffers from inaccuracies induced by artificial reflections from outflow
boundaries. This paper investigates the use of neural networks (NN) to mitigate
these adverse boundary effects and enable truncated domain requirements. Two
distinct NN-based approaches are proposed: (1) direct reconstruction of unknown
particle distribution functions at the outflow boundary; and (2) enhancement of
established characteristic boundary conditions (CBC) by dynamically tuning
their parameters. The direct reconstruction model was trained on data generated
from a 2D flow over a cylindrical obstruction. The drag, lift, and Strouhal
number were used to test the new boundary condition. We analyzed results for
various Reynolds numbers and restricted domain sizes where it demonstrated
significantly improved predictions when compared with the traditional Zou & He
boundary condition. To examine the robustness of the NN-based reconstruction,
the same condition was applied to the simulation of a NACA0012 airfoil, again
providing accurate aerodynamic performance predictions. The neural-enhanced CBC
were evaluated on a 2D convected vortex benchmark and showed superior
performance in minimizing density errors compared to CBCs with fixed
parameters. These findings highlight the potential of NN-integrated boundary
conditions to improve accuracy and reduce computational expense of aerodynamic
and acoustic emissions simulations with the LBM.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [18] [Discharge dynamics in a cylindrical SDBD prototype reactor under ns-pulsed and sinusoidal AC operation](https://arxiv.org/abs/2506.04826)
*Konstantinos Giotis,Dimitrios Stefas,Yanis Agha,Hans Höft,Xavier Duten,Panagiotis Svarnas,Guillaume Lombardi,Kristaq Gazeli*

Main category: physics.plasm-ph

TL;DR: A prototype reactor for surface dielectric barrier discharges (SDBDs) in air is developed, featuring detachable electrodes and quartz dielectric. It efficiently sustains ns-pulsed and AC discharges, enabling detailed study of electrical and spatiotemporal dynamics. The reactor shows distinct discharge behaviors and is suitable for fundamental research and applications like flow control and biomedicine.


<details>
  <summary>Details</summary>
Motivation: To design a reliable SDBD reactor for consistent operation and material durability, enabling detailed study of discharge characteristics and dynamics for various applications.

Method: The reactor uses detachable stainless steel electrodes and quartz dielectric, with the grounded electrode immersed in transformer oil to suppress parasitic discharges. It operates with ns-pulsed and AC discharges at 10 kHz, analyzing electrical parameters and discharge dynamics via ICCD imaging.

Result: The reactor shows non-linear power consumption with voltage, distinct discharge behaviors (ionization channels and glow-like discharges), and varying propagation lengths/velocities for ns-pulsed and AC operations. Ns-pulsed discharges are more reproducible.

Conclusion: The reactor is effective for fundamental SDBD studies and applications, with ns-pulsed operation offering better reproducibility and detailed spatiotemporal analysis.

Abstract: We developed a prototype reactor generating surface dielectric barrier
discharges (SDBDs) in ambient air, designed for consistent operation while
preventing constructive material degradation. It features detachable stainless
steel electrodes and quartz dielectric to ensure precise fabrication. The
grounded electrode is fully immersed into transformer oil drastically
suppressing undesired parasitic discharges. The device efficiently sustains
ns-pulsed and AC discharges at 10 kHz, enabling fundamental studies of their
electrical characteristics (applied voltage, induced current, electric power)
and spatiotemporal dynamics (morphology, propagation length and velocity). The
electric power (P) consumed exhibits a dissimilar non-linear increase with the
rising peak voltage (Vp) in each case: P$\approx$0.8-2.5 W for ns-pulsed
(Vp=7-9 kV) and P$\approx$0.9-5.3 W (Vp=7-10 kV) for AC operation. Using ICCD
imaging, distinct ionization channels are recorded in the rising part of the
pulsed voltage being detached from the driven electrode; during the voltage
decrease, a glow-like discharge is formed remaining anchored on the driven
electrode. The rising part of the AC voltage is characterized by erratic,
elongated ionization channels in a filamentary form, the voltage drop featuring
a glow-like behavior. During the rising and falling parts of the AC voltage,
the discharge reaches maximum propagation lengths (Lmax) of $\approx$12 mm and
$\approx$7 mm, respectively, while remaining attached to the driven electrode.
The corresponding maximum discharge velocities (vmax) are about 5x10 2 m/s and
3x10 2 m/s. For the ns-pulsed operation, Lmax$\approx$5 mm (vmax$\approx$5x10 5
m/s) and Lmax$\approx$3.5 mm (vmax$\approx$1.5x10 5 m/s) during the rising and
falling parts of the voltage pulse, respectively. The SDBD dynamics generated
with a ns-pulsed voltage is more reproducible than for the AC case allowing for
the use of a 500 times smaller ICCD gate width (2 ns) and a more accurate
description of the discharge's spatiotemporal development. This reactor is
suitable for performing fundamental studies and understanding key SDBD features
for various applications such as flow control, biomedicine and agriculture.

</details>


### [19] [Empirical scaling laws for self-focused laser pulses in nitrogen plasmas](https://arxiv.org/abs/2506.04827)
*Lorenzo Martelli,Igor Andriyash,Jonathan Wheeler,Henri Kraft,Xuan Quyen Dinh,Cédric Thaury*

Main category: physics.plasm-ph

TL;DR: Study of laser-plasma interaction in high-density nitrogen plasma using simulations, revealing insights into laser self-focusing and beam dynamics.


<details>
  <summary>Details</summary>
Motivation: To understand how superintense laser pulses interact with high-density nitrogen plasma, aiming to improve laser-plasma accelerators.

Method: Particle-in-cell simulations to analyze laser self-focusing and its effects on beam diffraction, wakefield amplitude, and plasma structures.

Result: Identified scaling laws for beam diffraction and wakefield amplitude, providing key insights into the interaction regime.

Conclusion: The findings advance the understanding of laser-plasma interactions, aiding the development of high-average-current accelerators.

Abstract: We investigate the interaction between a superintense laser pulse and a
nitrogen plasma with densities exceeding $10^{19}\,$cm$^{-3}$, using
particle-in-cell simulations. Such configurations have recently demonstrated
the capability to produce highly charged electron beams (i.e., $>10\,$nC) with
$1\,$J-class lasers, a significant step toward high-average-current
laser-plasma accelerators. Our study focuses on analyzing the impact of laser
self-focusing on laser dynamics, leading to scaling laws that characterize beam
diffraction, wakefield amplitude and plasma structures, providing important
insights of this interaction regime.

</details>


### [20] [Electron and gas temperature-driven chemistry during microdischarges formed in water vapour bubbles](https://arxiv.org/abs/2506.05124)
*Florens Grimm,Jan-Luca Gembus,Jana Schöne,Peter Awakowicz,Lars Schücke,Andrew R. Gibson*

Main category: physics.plasm-ph

TL;DR: The paper studies gas-phase chemical kinetics in microdischarges within bubbles in liquids, using a 0-D model informed by experiments, focusing on high electron density and gas temperature regimes.


<details>
  <summary>Details</summary>
Motivation: Understanding gas-phase chemical kinetics in microdischarges is crucial for controlling plasma-driven electrochemistry in materials synthesis and chemical conversion.

Method: A 0-D modeling approach, informed by experimental measurements, is used to develop a new reaction scheme for microdischarges in water vapor, analyzing species formation and pathways under varying temperatures.

Result: H2O is highly dissociated during peak power density, with H and O dominating the neutral gas. The maximum ionization degree is ~0.31%, and electronegativity is low. High-threshold energy reactions are key across temperatures.

Conclusion: The findings are transferable to similar plasma electrolysis systems, highlighting the importance of high-threshold energy reactions in microdischarge chemical kinetics.

Abstract: Microdischarges formed in bubbles immersed in liquids are of interest for
materials synthesis and chemical conversion applications in the frame of
plasma-driven electrochemistry. A key challenge associated with controlling
such processes is the limited understanding of the gas-phase chemical kinetics
in these microdischarges. Due to their large electron densities, and high gas
temperatures, both electron and gas temperature driven chemistry are likely to
be important. Here, a 0-D modelling approach, informed by experimental
measurements, is used to study the chemical kinetics in these systems. A new
reaction scheme is developed for microdischarges in water vapour, including
reactions for both high electron density, and high gas temperature regimes.
Microdischarges formed during plasma electrolytic oxidation are used as a test
case, however, the key results are expected to be transferable to other plasma
electrolysis systems with similar properties. Experimentally measured power
densities are used as input to the 0-D model, together with estimates of
temperatures and gas pressures within the gas bubble. Comparison of measured
and simulated electron densities shows good agreement, given the limitations of
both model and experiment. In the base case microdischarge, H$_{2}$O is found
to be highly dissociated during the period of peak power density, with H and O
making up the majority of the neutral gas in the bubble. The maximum ionization
degree is around 0.31$\,\%$, and the electronegativity during the period of
peak electron density is found to be low. Species formation and reaction
pathways are analysed under variation of the neutral gas temperature from
2000$\,$K to 6000$\,$K. At all temperatures, electron, ion, and neutral
reactions with high threshold energies are found to be important for the
overall chemical kinetics.

</details>


### [21] [Extending near-axis equilibria in DESC](https://arxiv.org/abs/2506.05170)
*Dario Panici,Eduardo Rodriguez,Rory Conlin,Daniel Dudt,Egemen Kolemen*

Main category: physics.plasm-ph

TL;DR: A novel method for constructing global stellarator equilibria using the DESC code, ensuring correct asymptotic behavior from near-axis models, is presented and benchmarked.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between approximate near-axis models and practical global equilibrium requirements for stellarator design and optimization.

Method: Uses the DESC code to construct global equilibria with guaranteed asymptotic behavior derived from near-axis models. Theoretical foundations and benchmarking are detailed.

Result: Successful construction of global equilibria that align with near-axis predictions, enabling efficient coupling for future optimization.

Conclusion: The method facilitates improved stellarator design by integrating near-axis and global equilibrium frameworks.

Abstract: The near-axis description of optimised stellarator fields has proven to be a
powerful tool both for design and understanding of this magnetic confinement
concept. The description consists of an asymptotic model of the equilibrium in
the distance from its centermost axis, and is thus only approximate. Any
practical application therefore requires the eventual construction of a global
equilibrium. This paper presents a novel way of constructing global equilibria
using the \texttt{DESC} code that guarantees the correct asymptotic behaviour
imposed by a given near-axis construction. The theoretical underpinnings of
this construction are carefully presented, and benchmarking examples provided.
This opens the door to an efficient coupling of the near-axis framework and
that of global equilibria for future optimisation efforts.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [22] [Classification and enumeration of solid-solid phase transition mechanisms](https://arxiv.org/abs/2506.05105)
*Fang-Cheng Wang,Qi-Jun Ye,Yu-Cheng Zhu,Xin-Zheng Li*

Main category: cond-mat.mtrl-sci

TL;DR: A new method, crystmatch, is proposed to classify and enumerate all possible crystal-structure matches (CSMs) for solid-solid phase transitions (SSPTs), solving the CSM optimization problem efficiently.


<details>
  <summary>Details</summary>
Motivation: Existing methods cannot account for all possible CSMs, limiting the understanding of SSPT mechanisms.

Method: The formalism classifies CSMs into a tree structure, representing them as integer matrices, and uses crystmatch to exhaustively enumerate them.

Result: crystmatch reproduces known mechanisms and reveals thousands of new CSM candidates within 10 CPU minutes.

Conclusion: The method provides a comprehensive database for SSPT analysis, enabling applications like experimental comparison and machine learning.

Abstract: Crystal-structure match (CSM), the atom-to-atom correspondence between two
crystalline phases, is used extensively to describe solid-solid phase
transition (SSPT) mechanisms. However, existing computational methods cannot
account for all possible CSMs. Here, we propose a formalism to classify all
CSMs into a tree structure, which is independent of the choices of unit cell
and supercell. We rigorously proved that only a finite number of noncongruent
CSMs are of practical interest. By representing CSMs as integer matrices, we
introduce the crystmatch method to exhaustively enumerate them, which
uncontroversially solves the CSM optimization problem under any geometric
criterion. For most SSPTs, crystmatch can reproduce all known deformation
mechanisms and CSMs within 10 CPU minutes, while also revealing thousands of
new candidates. The resulting database can be further used for comparing
experimental phenomena, high-throughput energy barrier calculations, or machine
learning.

</details>


<div id='hep-ex'></div>

# hep-ex [[Back]](#toc)

### [23] [GollumFit: An IceCube Open-Source Framework for Binned-Likelihood Neutrino Telescope Analyses](https://arxiv.org/abs/2506.04491)
*IceCube Collaboration*

Main category: hep-ex

TL;DR: GollumFit is a framework for binned-likelihood analyses in neutrino telescope data, tailored for IceCube, with efficient handling of nuisance parameters.


<details>
  <summary>Details</summary>
Motivation: To address the need for a specialized framework for neutrino telescope data analysis, particularly for IceCube, incorporating both general and specific model parameters.

Method: The framework organizes code for likelihood minimization, efficiently fitting over tens of nuisance parameters in typical analysis scenarios.

Result: GollumFit successfully performs likelihood minimization tasks, demonstrating time-efficient solutions for neutrino telescope data.

Conclusion: GollumFit is a valuable tool for neutrino telescope analyses, offering tailored features and efficient performance.

Abstract: We present GollumFit, a framework designed for performing binned-likelihood
analyses on neutrino telescope data. GollumFit incorporates model parameters
common to any neutrino telescope and also model parameters specific to the
IceCube Neutrino Observatory. We provide a high-level overview of its key
features and how the code is organized. We then discuss the performance of the
fitting in a typical analysis scenario, highlighting the ability to fit over
tens of nuisance parameters. We present some examples showing how to use the
package for likelihood minimization tasks. This framework uniquely incorporates
the particular model parameters necessary for neutrino telescopes, and solves
an associated likelihood problem in a time-efficient manner.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [24] [Uncertainty quantification and stability of neural operators for prediction of three-dimensional turbulence](https://arxiv.org/abs/2506.04898)
*Xintong Zou,Zhijie Li,Yunpeng Wang,Huiyu Yang,Jianchun Wang*

Main category: physics.flu-dyn

TL;DR: The study evaluates neural operator models for 3D turbulence, proposing the F-IFNO model for improved stability and accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional turbulence modeling lacks accuracy and stability, while existing SciML models like FNO struggle with long-term predictions in 3D turbulence.

Method: The framework assesses neural operators using 3D HIT, focusing on UQ, error propagation, and sensitivity. The F-IFNO model incorporates implicit factorization.

Result: F-IFNO outperforms conventional LES and other FNO models in accuracy, stability, and efficiency.

Conclusion: Prediction constraints, time interval selection, and UQ are crucial for robust neural operator frameworks in turbulence.

Abstract: Turbulence poses challenges for numerical simulation due to its chaotic,
multiscale nature and high computational cost. Traditional turbulence modeling
often struggles with accuracy and long-term stability. Recent scientific
machine learning (SciML) models, such as Fourier Neural Operators (FNO), show
promise in solving PDEs, but are typically limited to one-step-ahead
predictions and often fail over long time horizons, especially in 3D
turbulence. This study proposes a framework to assess the reliability of neural
operator models in turbulent flows. Using three-dimensional forced homogeneous
isotropic turbulence (HIT) as a benchmark, we evaluate models in terms of
uncertainty quantification (UQ), error propagation, and sensitivity to initial
perturbations. Statistical tools such as error distribution analysis and
autocorrelation functions (ACF) are used to assess predictive robustness and
temporal coherence. Our proposed model, the factorized-implicit FNO (F-IFNO),
improves long-term stability and accuracy by incorporating implicit
factorization into the prediction process. It outperforms conventional LES and
other FNO-based models in balancing accuracy, stability, and efficiency. The
results highlight the importance of prediction constraints, time interval
selection, and UQ in developing robust neural operator frameworks for turbulent
systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [25] [Perturbative Gradient Training: A novel training paradigm for bridging the gap between deep neural networks and physical reservoir computing](https://arxiv.org/abs/2506.04523)
*Cliff B. Abbott,Mark Elo,Dmytro A. Bozhko*

Main category: cs.LG

TL;DR: PGT is a new training method for physical reservoir computing that approximates gradients via random perturbations, enabling training without backpropagation.


<details>
  <summary>Details</summary>
Motivation: Overcome the limitation of physical reservoir computing's inability to perform backpropagation due to its black-box nature.

Method: Uses random perturbations in parameter space to approximate gradient updates, tested on simulated and experimental hardware.

Result: PGT achieves performance comparable to backpropagation where the latter is impractical.

Conclusion: PGT enables deeper integration of physical reservoirs into neural networks, promising energy efficiency gains.

Abstract: We introduce Perturbative Gradient Training (PGT), a novel training paradigm
that overcomes a critical limitation of physical reservoir computing: the
inability to perform backpropagation due to the black-box nature of physical
reservoirs. Drawing inspiration from perturbation theory in physics, PGT uses
random perturbations in the network's parameter space to approximate gradient
updates using only forward passes. We demonstrate the feasibility of this
approach on both simulated neural network architectures, including a dense
network and a transformer model with a reservoir layer, and on experimental
hardware using a magnonic auto-oscillation ring as the physical reservoir. Our
results show that PGT can achieve performance comparable to that of standard
backpropagation methods in cases where backpropagation is impractical or
impossible. PGT represents a promising step toward integrating physical
reservoirs into deeper neural network architectures and achieving significant
energy efficiency gains in AI training.

</details>


### [26] [Learning Beyond Experience: Generalizing to Unseen State Space with Reservoir Computing](https://arxiv.org/abs/2506.05292)
*Declan A. Norton,Yuanzhao Zhang,Michelle Girvan*

Main category: cs.LG

TL;DR: Reservoir computing can generalize to unexplored state space regions without explicit structural priors, using a multiple-trajectory training scheme.


<details>
  <summary>Details</summary>
Motivation: Machine learning struggles to generalize without structural priors, especially for poorly represented dynamics in training data.

Method: A multiple-trajectory training scheme for reservoir computers is introduced, enabling training across disjoint time series.

Result: RCs trained on single basin trajectories generalize to unobserved basins in multistable systems.

Conclusion: Reservoir computing can achieve out-of-domain generalization without explicit structural assumptions.

Abstract: Machine learning techniques offer an effective approach to modeling dynamical
systems solely from observed data. However, without explicit structural priors
-- built-in assumptions about the underlying dynamics -- these techniques
typically struggle to generalize to aspects of the dynamics that are poorly
represented in the training data. Here, we demonstrate that reservoir computing
-- a simple, efficient, and versatile machine learning framework often used for
data-driven modeling of dynamical systems -- can generalize to unexplored
regions of state space without explicit structural priors. First, we describe a
multiple-trajectory training scheme for reservoir computers that supports
training across a collection of disjoint time series, enabling effective use of
available training data. Then, applying this training scheme to multistable
dynamical systems, we show that RCs trained on trajectories from a single basin
of attraction can achieve out-of-domain generalization by capturing system
behavior in entirely unobserved basins.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [27] [Reactive Transport Simulation of Silicate-Rich Shale Rocks when Exposed to CO2 Saturated Brine Under High Pressure and High Temperature](https://arxiv.org/abs/2506.05122)
*Shaziya A. Banu,Venkata R. S. B. Varanasi,Arash Noshadravan,Sara Abedi*

Main category: physics.geo-ph

TL;DR: The study explores CO2 storage in shale rocks using reactive transport modeling to simulate chemo-mechanical interactions, avoiding costly experiments. Results show mineral dissolution and precipitation but insufficient pore-filling.


<details>
  <summary>Details</summary>
Motivation: To address the high cost and time of experimental testing for CO2 storage in heterogeneous shale rocks by using reactive transport models.

Method: Used Crunch Tope to simulate a 1D reactive transport model of a Permian rock specimen under high pressure (12.40 MPa) and temperature (100°C) with CO2 saturated brine for 14 and 28 days.

Result: Observed dissolution of quartz and feldspar, precipitation of quartz-rich phases, and clay swelling. Porosity changes showed only 1.00% mineral precipitation, insufficient to fill pores.

Conclusion: Reactive transport modeling is feasible for studying CO2 storage in shale, but mineral precipitation alone may not fully seal pores.

Abstract: This study examines the feasibility of carbon dioxide storage in shale rocks
and the reliability of reactive transport models in achieving accurate
replication of the chemo-mechanical interactions and transport processes
transpiring in these rocks when subjected to CO2 saturated brine. Owing to the
heterogeneity of rocks, experimental testing for adequate deductions and
findings, could be an expensive and time-intensive process. Therefore, this
study proposes utilization of reactive transport modeling to replicate the
pore-scale chemo-mechanical reactions and transport processes occurring in
silicate-rich shale rocks in the presence of CO2 saturated brine under high
pressure and high temperature. For this study, Crunch Tope has been adopted to
simulate a one-dimensional reactive transport model of a Permian rock specimen
exposed to the acidic brine at a temperature of 100 {\deg}C and pressure of
12.40 MPa (1800 psi) for a period of 14 and 28 days. The results demonstrated
significant dissolution followed by precipitation of quartz rich phases,
precipitation and swelling of clay rich phases, and dissolution of feldspar
rich phases closer to the acidic brine-rock interface. Moreover, porosity
against reaction depth curve showed nearly 1.00% mineral precipitation occur at
14 and 28 days, which is insufficient to completely fill the pore spaces.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [28] [Nombre Effectif de Partis Politiques en Afrique: Une Nouvelle Méthode pour un Calcul Objectif et Institutionnellement Neutre](https://arxiv.org/abs/2506.04279)
*Adama Nouboukpo,Kodzo Michel Aladji,Muktar Bappa*

Main category: physics.soc-ph

TL;DR: The paper introduces two new measures for calculating party fragmentation in Africa, focusing on geography and demographics, to better reflect local political realities.


<details>
  <summary>Details</summary>
Motivation: Traditional measures like the ENP index fail to capture nuances of African politics, such as dominant parties and ethnic cleavages.

Method: Proposes two 'apolitical' measures based on population size and territorial area, with a minimum threshold of two parties.

Result: The new models offer a simpler, contextually relevant framework for analyzing African political systems, even in data-scarce settings.

Conclusion: This approach provides a valuable tool for understanding political fragmentation, with potential applications beyond Africa.

Abstract: Political fragmentation in Africa poses to a significant challenge to
effective governance and stability. Traditional measures of party system
fragmentation, such as the Effective Number of Parties (ENP) index, often fail
to capture the nuanced realities of African political landscapes, particularly
the influence of dominant parties, fluid party affiliations, and the impact of
ethnic and regional cleavages. To address these limitations, this paper
introduces two novel "apolitical" or "institutionally neutral" measures for
calculating the effective number of parties, focusing on geographical and
demographic dimensions, notably population size and territorial area. By
incorporating these local realities and ensuring a minimum threshold of two
parties, the proposed models offer a simpler and more contextually relevant
framework for understanding political dynamics in Africa, especially in
data-scarce environments. This approach provides a valuable tool for analyzing
and streamlining political systems, with potential for broader application
beyond the African context.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [29] [Deep learning image burst stacking to reconstruct high-resolution ground-based solar observations](https://arxiv.org/abs/2506.04781)
*Christoph Schirninger,Robert Jarolim,Astrid M. Veronig,Christoph Kuckein*

Main category: astro-ph.SR

TL;DR: A deep learning method reconstructs 100 short exposure solar images into one high-quality image in real-time, outperforming current methods in robustness and perceptual quality.


<details>
  <summary>Details</summary>
Motivation: Ground-based solar telescopes face limitations due to Earth's turbulent atmosphere, requiring post-image corrections. Current methods struggle with strong turbulence and high computational costs.

Method: The approach uses unpaired image-to-image translation, training on degraded bursts with speckle reconstructions as references to improve robustness and generalization.

Result: The method shows enhanced perceptual quality, especially when speckle reconstructions have artifacts, and performs best with full image bursts.

Conclusion: The deep learning approach efficiently combines image information, offering real-time, high-quality reconstructions for solar observations.

Abstract: Large aperture ground based solar telescopes allow the solar atmosphere to be
resolved in unprecedented detail. However, observations are limited by Earths
turbulent atmosphere, requiring post image corrections. Current reconstruction
methods using short exposure bursts face challenges with strong turbulence and
high computational costs. We introduce a deep learning approach that
reconstructs 100 short exposure images into one high quality image in real
time. Using unpaired image to image translation, our model is trained on
degraded bursts with speckle reconstructions as references, improving
robustness and generalization. Our method shows an improved robustness in terms
of perceptual quality, especially when speckle reconstructions show artifacts.
An evaluation with a varying number of images per burst demonstrates that our
method makes efficient use of the combined image information and achieves the
best reconstructions when provided with the full image burst.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [30] [Inverse elastic obstacle scattering problems by monotonicity method](https://arxiv.org/abs/2506.04655)
*Mengjiao Bai,Huaian Diao,Weisheng Zhou*

Main category: math.AP

TL;DR: A monotonicity-based method is proposed to uniquely identify the shape and position of rigid elastic obstacles using far-field measurements, without initial guesses or prior knowledge.


<details>
  <summary>Details</summary>
Motivation: To solve the inverse problem of reconstructing rigid obstacles in elastic wave scattering using far-field data.

Method: Develops a monotonicity-based approach, factorizes the far-field operator, and uses localized wave functions to derive a shape characterization criterion.

Result: The method uniquely identifies the shape and location of obstacles without initial guesses or background medium knowledge.

Conclusion: The novel approach effectively solves the inverse problem for rigid elastic obstacles.

Abstract: We consider the elastic wave scattering problem involving rigid obstacles.
This work addresses the inverse problem of reconstructing the position and
shape of such obstacles using far-field measurements. A novel
monotonicity-based approach is developed for this purpose. By factorizing the
far-field operator and utilizing the existence of localized wave functions, we
derive a shape characterization criterion for the obstacle boundary. The
proposed method employs monotonicity tests to determine the geometric
relationship between any given test domain and the actual scatterer. As a
result, the shape and location of rigid elastic obstacles can be uniquely
identified without requiring any initial guesses or prior knowledge of the
physical parameters of the homogeneous background medium.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [31] [Quantum simulation of the Hubbard model on a graphene hexagon: Strengths of IQPE and noise constraints](https://arxiv.org/abs/2506.05031)
*Mohammad Mirzakhani,Kyungsun Moon*

Main category: quant-ph

TL;DR: The paper explores quantum simulation of the Hubbard model on graphene using Qiskit, achieving accurate results in noiseless conditions but facing challenges with noise on real hardware.


<details>
  <summary>Details</summary>
Motivation: To demonstrate the potential of quantum computing for simulating strongly correlated quantum systems like the Hubbard model, while addressing the limitations imposed by current noisy hardware.

Method: Utilized Iterative Quantum Phase Estimation (IQPE) and adiabatic evolution algorithms on Qiskit, with noiseless simulations and a custom noise model for IBM backends.

Result: Noiseless simulations matched exact diagonalization results, but noise degraded accuracy, revealing discrepancies between simulated and real hardware performance.

Conclusion: Quantum computing shows promise for material simulation, but hardware noise remains a significant barrier to reliable predictions.

Abstract: Quantum computing offers transformative potential for simulating real-world
materials, providing a powerful platform to investigate complex quantum systems
across quantum chemistry and condensed matter physics. In this work, we
leverage this capability to simulate the Hubbard model on a six-site graphene
hexagon using Qiskit, employing the Iterative Quantum Phase Estimation (IQPE)
and adiabatic evolution algorithms to determine its ground-state properties.
Noiseless simulations yield accurate ground-state energies (GSEs), charge and
spin densities, and correlation functions, all in excellent agreement with
exact diagonalization, underscoring the precision and reliability of quantum
simulation for strongly correlated electron systems. However, deploying IQPE
and adiabatic evolution on today's noisy quantum hardware remains highly
challenging. To examine these limitations, we utilize the Qiskit Aer simulator
with a custom noise model tailored to the characteristics of IBM's real
backend. This model includes realistic depolarizing gate errors, thermal
relaxation, and readout noise, allowing us to explore how these factors degrade
simulation accuracy. Preliminary hardware runs on IBM devices further expose
discrepancies between simulated and real-world noise, emphasizing the gap
between ideal and practical implementations. Overall, our results highlight the
promise of quantum computing for simulating correlated quantum materials, while
also revealing the significant challenges posed by hardware noise in achieving
accurate and reliable physical predictions using current quantum devices.

</details>
