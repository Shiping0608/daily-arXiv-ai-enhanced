{"id": "2506.04354", "pdf": "https://arxiv.org/pdf/2506.04354", "abs": "https://arxiv.org/abs/2506.04354", "authors": ["Elmira Mirzabeigi", "Rezvan Salehi", "Kourosh Parand"], "title": "BridgeNet: A Hybrid, Physics-Informed Machine Learning Framework for Solving High-Dimensional Fokker-Planck Equations", "categories": ["physics.comp-ph", "cs.LG", "math-ph", "math.AP", "math.MP"], "comment": null, "summary": "BridgeNet is a novel hybrid framework that integrates convolutional neural\nnetworks with physics-informed neural networks to efficiently solve non-linear,\nhigh-dimensional Fokker-Planck equations (FPEs). Traditional PINNs, which\ntypically rely on fully connected architectures, often struggle to capture\ncomplex spatial hierarchies and enforce intricate boundary conditions. In\ncontrast, BridgeNet leverages adaptive CNN layers for effective local feature\nextraction and incorporates a dynamically weighted loss function that\nrigorously enforces physical constraints. Extensive numerical experiments\nacross various test cases demonstrate that BridgeNet not only achieves\nsignificantly lower error metrics and faster convergence compared to\nconventional PINN approaches but also maintains robust stability in\nhigh-dimensional settings. This work represents a substantial advancement in\ncomputational physics, offering a scalable and accurate solution methodology\nwith promising applications in fields ranging from financial mathematics to\ncomplex system dynamics.", "AI": {"tldr": "BridgeNet combines CNNs and physics-informed neural networks to solve complex Fokker-Planck equations more efficiently than traditional PINNs, achieving better accuracy and stability.", "motivation": "Traditional PINNs struggle with spatial hierarchies and boundary conditions in high-dimensional FPEs, prompting the need for a hybrid approach.", "method": "BridgeNet uses adaptive CNN layers for local feature extraction and a dynamically weighted loss function to enforce physical constraints.", "result": "BridgeNet outperforms conventional PINNs with lower errors, faster convergence, and robust stability in high-dimensional settings.", "conclusion": "BridgeNet advances computational physics, offering scalable and accurate solutions for applications in finance and complex systems."}}
{"id": "2506.04489", "pdf": "https://arxiv.org/pdf/2506.04489", "abs": "https://arxiv.org/abs/2506.04489", "authors": ["Lukas Exl", "Sebastian Schaffer"], "title": "Spectrally accurate and efficient convolution with the 3D free-space Laplace Green's function via the super-potential", "categories": ["physics.comp-ph", "cs.NA", "math.NA", "65N35, 65N80"], "comment": "11 pages, 1 figure", "summary": "We present a high-accuracy spectral method for solving the unbounded\nthree-dimensional Poisson equation with smooth, compactly supported sources.\nThe approach is based on a super-potential formulation, where the solution is\nobtained by applying the Laplacian to a convolution with the biharmonic Green's\nfunction. A separable Gaussian-sum (GS) approximation enables efficient\nFFT-based computation with quasi-linear complexity. Owing to the improved\nregularity of the biharmonic kernel, the GS cutoff error is of order four,\neliminating the need for correction terms or Taylor expansions required in\nstandard GS or Ewald-type methods. Numerical benchmarks demonstrate that the\nmethod achieves machine-precision accuracy and outperforms existing GS-based\nschemes in both error and runtime, making it a robust and efficient tool for\nfree-space Poisson problems on uniform grids.", "AI": {"tldr": "A high-accuracy spectral method for solving the 3D Poisson equation with smooth, compactly supported sources, using a super-potential formulation and Gaussian-sum approximation for efficient FFT-based computation.", "motivation": "To address the need for a robust and efficient method for solving unbounded 3D Poisson equations with high accuracy and quasi-linear complexity.", "method": "Uses a super-potential formulation involving the biharmonic Green's function and a separable Gaussian-sum approximation for FFT-based computation.", "result": "Achieves machine-precision accuracy, outperforms existing Gaussian-sum-based schemes in error and runtime, and eliminates the need for correction terms.", "conclusion": "The method is a robust and efficient tool for free-space Poisson problems on uniform grids."}}
{"id": "2506.04763", "pdf": "https://arxiv.org/pdf/2506.04763", "abs": "https://arxiv.org/abs/2506.04763", "authors": ["Shuai Lu"], "title": "A highly scalable numerical framework for reservoir simulation on UG4 platform", "categories": ["physics.comp-ph", "cs.DC"], "comment": null, "summary": "The modeling and simulation of multiphase fluid flow receive significant\nattention in reservoir engineering. Many time discretization schemes for\nmultiphase flow equations are either explicit or semi-implicit, relying on the\ndecoupling between the saturation equation and the pressure equation. In this\nstudy, we delve into a fully coupled and fully implicit framework for\nsimulating multiphase flow in heterogeneous porous media, considering gravity\nand capillary effects. We utilize the Vertex-Centered Finite Volume Method for\nspatial discretization and propose an efficient implementation of interface\nconditions for heterogeneous porous media within the current scheme. Notably,\nwe introduce the Linearly Implicit Extrapolation Method (LIMEX) with an error\nestimator, adapted for the first time to multiphase flow problems. To solve the\nresulting linear system, we employ the BiCGSTAB method with the Geometric\nMultigrid (GMG) preconditioner. The implementations of models and methods are\nbased on the open-source software: UG4. The results from parallel computations\non the supercomputer demonstrate that the scalability of our proposed framework\nis sufficient, supporting a scale of thousands of processors with Degrees of\nFreedom (DoF) extending up to billions.", "AI": {"tldr": "The paper presents a fully coupled, fully implicit framework for simulating multiphase flow in heterogeneous porous media, using advanced numerical methods and demonstrating scalability on supercomputers.", "motivation": "Existing time discretization schemes for multiphase flow often decouple saturation and pressure equations, limiting accuracy. This study aims to improve modeling by fully coupling these equations and incorporating gravity and capillary effects.", "method": "The study employs the Vertex-Centered Finite Volume Method for spatial discretization, introduces the Linearly Implicit Extrapolation Method (LIMEX) with an error estimator, and uses BiCGSTAB with Geometric Multigrid preconditioning for solving linear systems.", "result": "The framework shows strong scalability, supporting thousands of processors and billions of Degrees of Freedom (DoF) in parallel computations on a supercomputer.", "conclusion": "The proposed fully implicit framework is efficient and scalable, offering improved accuracy for multiphase flow simulations in heterogeneous porous media."}}
{"id": "2506.04835", "pdf": "https://arxiv.org/pdf/2506.04835", "abs": "https://arxiv.org/abs/2506.04835", "authors": ["Julien El Hajj", "Gilles Ledoux", "Samy Merabia"], "title": "Thermoplasmonics of Gold-Core Silica-Shell Colloidal Nanoparticles under Pulse Illumination", "categories": ["physics.comp-ph", "cond-mat.mtrl-sci"], "comment": null, "summary": "Core-shell nanoparticles, particularly those having a gold core, have emerged\nas a highly promising class of materials due to their unique optical and\nthermal properties, which underpin a wide range of applications in photothermal\ntherapy, imaging, and biosensing. In this study, we present a comprehensive\nstudy of the thermal dynamics of gold-core silica-shell nanoparticles immersed\nin water under pulse illumination. The plasmonic response of the core-shell\nnanoparticle is described by incorporating Mie theory with electronic\ntemperature corrections to the refractive indices of gold, based on a Drude\nLorentz formulation. The thermal response of the core-shell nanoparticles is\nmodeled by coupling the two temperature model with molecular dynamics\nsimulations, providing an atomistic description of nanoscale heat transfer. We\ninvestigate nanoparticles with both dense and porous silica shells (with 50%\nporosity) under laser pulse durations of 100 fs, 10 ps, and 1 ns, and over a\nrange of fluences between 0.05 and 5mJ/cm2. We show that nanoparticles with a\nthin dense silica shell (5 nm) exhibit significantly faster water heating\ncompared to bare gold nanoparticles. This behavior is attributed to enhanced\nelectron-phonon coupling at the gold silica interface and to the relatively\nhigh thermal conductance between silica and water. These findings provide new\ninsights into optimizing nanoparticle design for efficient photothermal\napplications and establish a robust framework for understanding energy transfer\nmechanisms in heterogeneous metal dielectric nanostructures.", "AI": {"tldr": "Study explores thermal dynamics of gold-core silica-shell nanoparticles under pulse illumination, revealing faster water heating with thin dense silica shells due to enhanced electron-phonon coupling.", "motivation": "Core-shell nanoparticles, especially gold-core ones, are promising for photothermal therapy, imaging, and biosensing due to their unique properties. Understanding their thermal dynamics is key for optimizing applications.", "method": "Combines Mie theory with electronic temperature corrections and couples the two-temperature model with molecular dynamics simulations to study thermal response under varied laser pulses and fluences.", "result": "Nanoparticles with thin dense silica shells (5 nm) heat water faster than bare gold nanoparticles, attributed to enhanced electron-phonon coupling and high silica-water thermal conductance.", "conclusion": "Findings offer insights for optimizing nanoparticle design for photothermal applications and understanding energy transfer in metal-dielectric nanostructures."}}
{"id": "2506.04375", "pdf": "https://arxiv.org/pdf/2506.04375", "abs": "https://arxiv.org/abs/2506.04375", "authors": ["Conor Rowan", "John Evans", "Kurt Maute", "Alireza Doostan"], "title": "Solving engineering eigenvalue problems with neural networks using the Rayleigh quotient", "categories": ["math.NA", "cs.LG", "cs.NA"], "comment": null, "summary": "From characterizing the speed of a thermal system's response to computing\nnatural modes of vibration, eigenvalue analysis is ubiquitous in engineering.\nIn spite of this, eigenvalue problems have received relatively little treatment\ncompared to standard forward and inverse problems in the physics-informed\nmachine learning literature. In particular, neural network discretizations of\nsolutions to eigenvalue problems have seen only a handful of studies. Owing to\ntheir nonlinearity, neural network discretizations prevent the conversion of\nthe continuous eigenvalue differential equation into a standard discrete\neigenvalue problem. In this setting, eigenvalue analysis requires more\nspecialized techniques. Using a neural network discretization of the\neigenfunction, we show that a variational form of the eigenvalue problem called\nthe \"Rayleigh quotient\" in tandem with a Gram-Schmidt orthogonalization\nprocedure is a particularly simple and robust approach to find the eigenvalues\nand their corresponding eigenfunctions. This method is shown to be useful for\nfinding sets of harmonic functions on irregular domains, parametric and\nnonlinear eigenproblems, and high-dimensional eigenanalysis. We also discuss\nthe utility of harmonic functions as a spectral basis for approximating\nsolutions to partial differential equations. Through various examples from\nengineering mechanics, the combination of the Rayleigh quotient objective,\nGram-Schmidt procedure, and the neural network discretization of the\neigenfunction is shown to offer unique advantages for handling continuous\neigenvalue problems.", "AI": {"tldr": "The paper introduces a neural network-based method for solving eigenvalue problems, leveraging the Rayleigh quotient and Gram-Schmidt orthogonalization for robustness and simplicity.", "motivation": "Eigenvalue problems are common in engineering but understudied in physics-informed machine learning, especially with neural network discretizations.", "method": "Uses a neural network to discretize eigenfunctions, combining the Rayleigh quotient and Gram-Schmidt orthogonalization to solve eigenvalue problems.", "result": "Demonstrates effectiveness for harmonic functions on irregular domains, parametric/nonlinear eigenproblems, and high-dimensional eigenanalysis.", "conclusion": "The approach offers unique advantages for continuous eigenvalue problems in engineering mechanics."}}
{"id": "2506.04826", "pdf": "https://arxiv.org/pdf/2506.04826", "abs": "https://arxiv.org/abs/2506.04826", "authors": ["Konstantinos Giotis", "Dimitrios Stefas", "Yanis Agha", "Hans H\u00f6ft", "Xavier Duten", "Panagiotis Svarnas", "Guillaume Lombardi", "Kristaq Gazeli"], "title": "Discharge dynamics in a cylindrical SDBD prototype reactor under ns-pulsed and sinusoidal AC operation", "categories": ["physics.plasm-ph"], "comment": null, "summary": "We developed a prototype reactor generating surface dielectric barrier\ndischarges (SDBDs) in ambient air, designed for consistent operation while\npreventing constructive material degradation. It features detachable stainless\nsteel electrodes and quartz dielectric to ensure precise fabrication. The\ngrounded electrode is fully immersed into transformer oil drastically\nsuppressing undesired parasitic discharges. The device efficiently sustains\nns-pulsed and AC discharges at 10 kHz, enabling fundamental studies of their\nelectrical characteristics (applied voltage, induced current, electric power)\nand spatiotemporal dynamics (morphology, propagation length and velocity). The\nelectric power (P) consumed exhibits a dissimilar non-linear increase with the\nrising peak voltage (Vp) in each case: P$\\approx$0.8-2.5 W for ns-pulsed\n(Vp=7-9 kV) and P$\\approx$0.9-5.3 W (Vp=7-10 kV) for AC operation. Using ICCD\nimaging, distinct ionization channels are recorded in the rising part of the\npulsed voltage being detached from the driven electrode; during the voltage\ndecrease, a glow-like discharge is formed remaining anchored on the driven\nelectrode. The rising part of the AC voltage is characterized by erratic,\nelongated ionization channels in a filamentary form, the voltage drop featuring\na glow-like behavior. During the rising and falling parts of the AC voltage,\nthe discharge reaches maximum propagation lengths (Lmax) of $\\approx$12 mm and\n$\\approx$7 mm, respectively, while remaining attached to the driven electrode.\nThe corresponding maximum discharge velocities (vmax) are about 5x10 2 m/s and\n3x10 2 m/s. For the ns-pulsed operation, Lmax$\\approx$5 mm (vmax$\\approx$5x10 5\nm/s) and Lmax$\\approx$3.5 mm (vmax$\\approx$1.5x10 5 m/s) during the rising and\nfalling parts of the voltage pulse, respectively. The SDBD dynamics generated\nwith a ns-pulsed voltage is more reproducible than for the AC case allowing for\nthe use of a 500 times smaller ICCD gate width (2 ns) and a more accurate\ndescription of the discharge's spatiotemporal development. This reactor is\nsuitable for performing fundamental studies and understanding key SDBD features\nfor various applications such as flow control, biomedicine and agriculture.", "AI": {"tldr": "A prototype reactor for surface dielectric barrier discharges (SDBDs) in air is developed, featuring detachable electrodes and quartz dielectric. It efficiently sustains ns-pulsed and AC discharges, enabling detailed study of electrical and spatiotemporal dynamics. The reactor shows distinct discharge behaviors and is suitable for fundamental research and applications like flow control and biomedicine.", "motivation": "To design a reliable SDBD reactor for consistent operation and material durability, enabling detailed study of discharge characteristics and dynamics for various applications.", "method": "The reactor uses detachable stainless steel electrodes and quartz dielectric, with the grounded electrode immersed in transformer oil to suppress parasitic discharges. It operates with ns-pulsed and AC discharges at 10 kHz, analyzing electrical parameters and discharge dynamics via ICCD imaging.", "result": "The reactor shows non-linear power consumption with voltage, distinct discharge behaviors (ionization channels and glow-like discharges), and varying propagation lengths/velocities for ns-pulsed and AC operations. Ns-pulsed discharges are more reproducible.", "conclusion": "The reactor is effective for fundamental SDBD studies and applications, with ns-pulsed operation offering better reproducibility and detailed spatiotemporal analysis."}}
{"id": "2506.05293", "pdf": "https://arxiv.org/pdf/2506.05293", "abs": "https://arxiv.org/abs/2506.05293", "authors": ["Mario Christopher Bedrunka", "Dirk Reith", "Holger Foysi", "\u0141ukasz \u0141aniewski-Wo\u0142\u0142k", "Travis Mitchell"], "title": "Reduction of Outflow Boundary Influence on Aerodynamic Performance using Neural Networks", "categories": ["physics.comp-ph", "physics.flu-dyn"], "comment": null, "summary": "The accurate treatment of outflow boundary conditions remains a critical\nchallenge in computational fluid dynamics when predicting aerodynamic forces\nand/or acoustic emissions. This is particularly evident when employing the\nlattice Boltzmann method (LBM) as the numerical solution technique, which often\nsuffers from inaccuracies induced by artificial reflections from outflow\nboundaries. This paper investigates the use of neural networks (NN) to mitigate\nthese adverse boundary effects and enable truncated domain requirements. Two\ndistinct NN-based approaches are proposed: (1) direct reconstruction of unknown\nparticle distribution functions at the outflow boundary; and (2) enhancement of\nestablished characteristic boundary conditions (CBC) by dynamically tuning\ntheir parameters. The direct reconstruction model was trained on data generated\nfrom a 2D flow over a cylindrical obstruction. The drag, lift, and Strouhal\nnumber were used to test the new boundary condition. We analyzed results for\nvarious Reynolds numbers and restricted domain sizes where it demonstrated\nsignificantly improved predictions when compared with the traditional Zou & He\nboundary condition. To examine the robustness of the NN-based reconstruction,\nthe same condition was applied to the simulation of a NACA0012 airfoil, again\nproviding accurate aerodynamic performance predictions. The neural-enhanced CBC\nwere evaluated on a 2D convected vortex benchmark and showed superior\nperformance in minimizing density errors compared to CBCs with fixed\nparameters. These findings highlight the potential of NN-integrated boundary\nconditions to improve accuracy and reduce computational expense of aerodynamic\nand acoustic emissions simulations with the LBM.", "AI": {"tldr": "Neural networks (NN) are used to improve outflow boundary conditions in LBM, reducing artificial reflections and computational costs. Two NN-based methods show superior accuracy in aerodynamic predictions.", "motivation": "Accurate outflow boundary conditions are critical in CFD for predicting aerodynamic forces and acoustic emissions, especially in LBM, which suffers from artificial reflections.", "method": "Two NN-based approaches: (1) direct reconstruction of particle distribution functions at outflow boundaries, and (2) dynamic tuning of characteristic boundary conditions (CBC) parameters.", "result": "NN-based methods improved predictions for drag, lift, and Strouhal numbers in 2D flow and NACA0012 airfoil simulations. Neural-enhanced CBC minimized density errors in vortex benchmarks.", "conclusion": "NN-integrated boundary conditions enhance accuracy and reduce computational costs in LBM simulations for aerodynamics and acoustics."}}
{"id": "2506.04416", "pdf": "https://arxiv.org/pdf/2506.04416", "abs": "https://arxiv.org/abs/2506.04416", "authors": ["Ziyao Xu", "Yong-Tao Zhang"], "title": "Exponential Time Differencing Runge-Kutta Discontinuous Galerkin (ETD-RKDG) Methods for Nonlinear Degenerate Parabolic Equations", "categories": ["math.NA", "cs.NA"], "comment": "34 pages", "summary": "In this paper, we study high-order exponential time differencing Runge-Kutta\n(ETD-RK) discontinuous Galerkin (DG) methods for nonlinear degenerate parabolic\nequations. This class of equations exhibits hyperbolic behavior in degenerate\nregions and parabolic behavior in non-degenerate regions, resulting in sharp\nwave fronts in the solution profiles and a parabolic-type time-step\nrestriction, $\\tau \\sim O(h^2)$, for explicit time integration. To address\nthese challenges and solve such equations in complex domains, we employ DG\nmethods with appropriate stabilizing limiters on unstructured meshes to capture\nthe wave fronts and use ETD-RK methods for time integration to resolve the\nstiffness of parabolic terms. We extract the system's stiffness using the\nJacobian matrix of the DG discretization for diffusion terms and adopt a nodal\nformulation to facilitate its computation. The algorithm is described in detail\nfor two-dimensional triangular meshes. We also conduct a linear stability\nanalysis in one spatial dimension and present computational results on\nthree-dimensional simplex meshes, demonstrating significant improvements in\nstability and large time-step sizes.", "AI": {"tldr": "High-order ETD-RK DG methods are proposed for nonlinear degenerate parabolic equations, addressing stiffness and wave-front capture with stabilizing limiters and nodal formulations.", "motivation": "To solve nonlinear degenerate parabolic equations with mixed hyperbolic-parabolic behavior, which pose challenges like sharp wave fronts and stiff parabolic terms.", "method": "Uses DG methods with stabilizing limiters on unstructured meshes and ETD-RK for time integration, leveraging the Jacobian matrix for stiffness extraction.", "result": "Demonstrates improved stability and allows larger time-step sizes, validated through linear stability analysis and 3D computational results.", "conclusion": "The proposed method effectively handles the stiffness and wave-front challenges, offering robust solutions for complex domains."}}
{"id": "2506.04827", "pdf": "https://arxiv.org/pdf/2506.04827", "abs": "https://arxiv.org/abs/2506.04827", "authors": ["Lorenzo Martelli", "Igor Andriyash", "Jonathan Wheeler", "Henri Kraft", "Xuan Quyen Dinh", "C\u00e9dric Thaury"], "title": "Empirical scaling laws for self-focused laser pulses in nitrogen plasmas", "categories": ["physics.plasm-ph"], "comment": "6 figures, 9 pages", "summary": "We investigate the interaction between a superintense laser pulse and a\nnitrogen plasma with densities exceeding $10^{19}\\,$cm$^{-3}$, using\nparticle-in-cell simulations. Such configurations have recently demonstrated\nthe capability to produce highly charged electron beams (i.e., $>10\\,$nC) with\n$1\\,$J-class lasers, a significant step toward high-average-current\nlaser-plasma accelerators. Our study focuses on analyzing the impact of laser\nself-focusing on laser dynamics, leading to scaling laws that characterize beam\ndiffraction, wakefield amplitude and plasma structures, providing important\ninsights of this interaction regime.", "AI": {"tldr": "Study of laser-plasma interaction in high-density nitrogen plasma using simulations, revealing insights into laser self-focusing and beam dynamics.", "motivation": "To understand how superintense laser pulses interact with high-density nitrogen plasma, aiming to improve laser-plasma accelerators.", "method": "Particle-in-cell simulations to analyze laser self-focusing and its effects on beam diffraction, wakefield amplitude, and plasma structures.", "result": "Identified scaling laws for beam diffraction and wakefield amplitude, providing key insights into the interaction regime.", "conclusion": "The findings advance the understanding of laser-plasma interactions, aiding the development of high-average-current accelerators."}}
{"id": "2506.04491", "pdf": "https://arxiv.org/pdf/2506.04491", "abs": "https://arxiv.org/abs/2506.04491", "authors": ["IceCube Collaboration"], "title": "GollumFit: An IceCube Open-Source Framework for Binned-Likelihood Neutrino Telescope Analyses", "categories": ["hep-ex", "astro-ph.HE", "physics.comp-ph", "physics.data-an"], "comment": null, "summary": "We present GollumFit, a framework designed for performing binned-likelihood\nanalyses on neutrino telescope data. GollumFit incorporates model parameters\ncommon to any neutrino telescope and also model parameters specific to the\nIceCube Neutrino Observatory. We provide a high-level overview of its key\nfeatures and how the code is organized. We then discuss the performance of the\nfitting in a typical analysis scenario, highlighting the ability to fit over\ntens of nuisance parameters. We present some examples showing how to use the\npackage for likelihood minimization tasks. This framework uniquely incorporates\nthe particular model parameters necessary for neutrino telescopes, and solves\nan associated likelihood problem in a time-efficient manner.", "AI": {"tldr": "GollumFit is a framework for binned-likelihood analyses in neutrino telescope data, tailored for IceCube, with efficient handling of nuisance parameters.", "motivation": "To address the need for a specialized framework for neutrino telescope data analysis, particularly for IceCube, incorporating both general and specific model parameters.", "method": "The framework organizes code for likelihood minimization, efficiently fitting over tens of nuisance parameters in typical analysis scenarios.", "result": "GollumFit successfully performs likelihood minimization tasks, demonstrating time-efficient solutions for neutrino telescope data.", "conclusion": "GollumFit is a valuable tool for neutrino telescope analyses, offering tailored features and efficient performance."}}
{"id": "2506.04451", "pdf": "https://arxiv.org/pdf/2506.04451", "abs": "https://arxiv.org/abs/2506.04451", "authors": ["Santolo Leveque", "Yunhui He", "Maxim Olshanskii"], "title": "An Augmented Lagrangian Preconditioner for Navier--Stokes Equations with Runge--Kutta in Time", "categories": ["math.NA", "cs.NA"], "comment": null, "summary": "We consider a Runge--Kutta method for the numerical time integration of the\nnonstationary incompressible Navier--Stokes equations. This yields a sequence\nof nonlinear problems to be solved for the stages of the Runge--Kutta method.\nThe resulting nonlinear system of differential equations is discretized using a\nfinite element method. To compute a numerical approximation of the stages at\neach time step, we employ Newton's method, which requires the solution of a\nlarge and sparse generalized saddle-point problem at each nonlinear iteration.\nWe devise an augmented Lagrangian preconditioner within the flexible GMRES\nmethod for solving the Newton systems at each time step. The preconditioner can\nbe applied inexactly with the help of a multigrid routine. We present numerical\nevidence of the robustness and efficiency of the proposed strategy for\ndifferent values of the viscosity, mesh size, time step, and number of stages\nof the Runge--Kutta method.", "AI": {"tldr": "A Runge-Kutta method is used for time integration of the incompressible Navier-Stokes equations, with nonlinear problems solved via Newton's method and a preconditioned GMRES approach.", "motivation": "To efficiently solve the nonlinear systems arising from the Runge-Kutta time integration of the Navier-Stokes equations.", "method": "Finite element discretization, Newton's method for nonlinear systems, and an augmented Lagrangian preconditioner with flexible GMRES and multigrid support.", "result": "Numerical results demonstrate robustness and efficiency across varying parameters like viscosity, mesh size, and time step.", "conclusion": "The proposed preconditioned GMRES strategy is effective for solving the nonlinear systems in this context."}}
{"id": "2506.05124", "pdf": "https://arxiv.org/pdf/2506.05124", "abs": "https://arxiv.org/abs/2506.05124", "authors": ["Florens Grimm", "Jan-Luca Gembus", "Jana Sch\u00f6ne", "Peter Awakowicz", "Lars Sch\u00fccke", "Andrew R. Gibson"], "title": "Electron and gas temperature-driven chemistry during microdischarges formed in water vapour bubbles", "categories": ["physics.plasm-ph"], "comment": null, "summary": "Microdischarges formed in bubbles immersed in liquids are of interest for\nmaterials synthesis and chemical conversion applications in the frame of\nplasma-driven electrochemistry. A key challenge associated with controlling\nsuch processes is the limited understanding of the gas-phase chemical kinetics\nin these microdischarges. Due to their large electron densities, and high gas\ntemperatures, both electron and gas temperature driven chemistry are likely to\nbe important. Here, a 0-D modelling approach, informed by experimental\nmeasurements, is used to study the chemical kinetics in these systems. A new\nreaction scheme is developed for microdischarges in water vapour, including\nreactions for both high electron density, and high gas temperature regimes.\nMicrodischarges formed during plasma electrolytic oxidation are used as a test\ncase, however, the key results are expected to be transferable to other plasma\nelectrolysis systems with similar properties. Experimentally measured power\ndensities are used as input to the 0-D model, together with estimates of\ntemperatures and gas pressures within the gas bubble. Comparison of measured\nand simulated electron densities shows good agreement, given the limitations of\nboth model and experiment. In the base case microdischarge, H$_{2}$O is found\nto be highly dissociated during the period of peak power density, with H and O\nmaking up the majority of the neutral gas in the bubble. The maximum ionization\ndegree is around 0.31$\\,\\%$, and the electronegativity during the period of\npeak electron density is found to be low. Species formation and reaction\npathways are analysed under variation of the neutral gas temperature from\n2000$\\,$K to 6000$\\,$K. At all temperatures, electron, ion, and neutral\nreactions with high threshold energies are found to be important for the\noverall chemical kinetics.", "AI": {"tldr": "The paper studies gas-phase chemical kinetics in microdischarges within bubbles in liquids, using a 0-D model informed by experiments, focusing on high electron density and gas temperature regimes.", "motivation": "Understanding gas-phase chemical kinetics in microdischarges is crucial for controlling plasma-driven electrochemistry in materials synthesis and chemical conversion.", "method": "A 0-D modeling approach, informed by experimental measurements, is used to develop a new reaction scheme for microdischarges in water vapor, analyzing species formation and pathways under varying temperatures.", "result": "H2O is highly dissociated during peak power density, with H and O dominating the neutral gas. The maximum ionization degree is ~0.31%, and electronegativity is low. High-threshold energy reactions are key across temperatures.", "conclusion": "The findings are transferable to similar plasma electrolysis systems, highlighting the importance of high-threshold energy reactions in microdischarge chemical kinetics."}}
{"id": "2506.04523", "pdf": "https://arxiv.org/pdf/2506.04523", "abs": "https://arxiv.org/abs/2506.04523", "authors": ["Cliff B. Abbott", "Mark Elo", "Dmytro A. Bozhko"], "title": "Perturbative Gradient Training: A novel training paradigm for bridging the gap between deep neural networks and physical reservoir computing", "categories": ["cs.LG", "cond-mat.mes-hall", "cs.ET", "cs.NE", "physics.comp-ph"], "comment": "7 pages, 8 figures, submitted to IEEE Transactions on Neural Netowrks\n  and Learning Systems", "summary": "We introduce Perturbative Gradient Training (PGT), a novel training paradigm\nthat overcomes a critical limitation of physical reservoir computing: the\ninability to perform backpropagation due to the black-box nature of physical\nreservoirs. Drawing inspiration from perturbation theory in physics, PGT uses\nrandom perturbations in the network's parameter space to approximate gradient\nupdates using only forward passes. We demonstrate the feasibility of this\napproach on both simulated neural network architectures, including a dense\nnetwork and a transformer model with a reservoir layer, and on experimental\nhardware using a magnonic auto-oscillation ring as the physical reservoir. Our\nresults show that PGT can achieve performance comparable to that of standard\nbackpropagation methods in cases where backpropagation is impractical or\nimpossible. PGT represents a promising step toward integrating physical\nreservoirs into deeper neural network architectures and achieving significant\nenergy efficiency gains in AI training.", "AI": {"tldr": "PGT is a new training method for physical reservoir computing that approximates gradients via random perturbations, enabling training without backpropagation.", "motivation": "Overcome the limitation of physical reservoir computing's inability to perform backpropagation due to its black-box nature.", "method": "Uses random perturbations in parameter space to approximate gradient updates, tested on simulated and experimental hardware.", "result": "PGT achieves performance comparable to backpropagation where the latter is impractical.", "conclusion": "PGT enables deeper integration of physical reservoirs into neural networks, promising energy efficiency gains."}}
{"id": "2506.04710", "pdf": "https://arxiv.org/pdf/2506.04710", "abs": "https://arxiv.org/abs/2506.04710", "authors": ["Lucas \u00c5kerstedt", "Harald Hultin", "B. L. G. Jonsson"], "title": "An Array Decomposition Method for Finite Arrays with Electrically Connected Elements for fast Toeplitz Solvers", "categories": ["math.NA", "cs.NA", "eess.SP"], "comment": "12 pages, 17 figures", "summary": "A large part of the geometry of array antennas is often partially defined by\nfinite translational symmetries. Applying the method of moments (MoM) with the\nRWG-like element on an appropriately structured mesh to these arrays results in\nan impedance matrix where the main part exhibits a multilevel block Toeplitz\nstructure. This article introduces a memory-efficient construction method that\neffectively represents and reuses impedance calculations. The proposed method,\napplicable to electrically connected elements, also accounts for all\nnon-symmetric parts of the array. The core idea involves nine distinct\nelectrically connectable components from which the array can be assembled. The\nderived multilevel block Toeplitz matrix is further utilized by an in-house\ninverse solver to achieve faster and more memory-efficient MoM current vector\ncalculations. We demonstrate the method by computing the far-field of a 32x32\narray and the scattering parameters of two tightly coupled 9x9 arrays. This\napproach reduces the memory allocation from $\\mathcal{O}(N_x^2 N_y^2)$ to\n$\\mathcal{O}(N_x N_y)$, for an $N_x \\times N_y$ array.", "AI": {"tldr": "A memory-efficient method for constructing impedance matrices in array antennas using multilevel block Toeplitz structure, reducing memory usage from O(N_x^2 N_y^2) to O(N_x N_y).", "motivation": "To address the inefficiency in memory usage and computation for array antennas with finite translational symmetries.", "method": "Uses MoM with RWG-like elements on structured meshes, introduces nine connectable components, and leverages multilevel block Toeplitz matrices.", "result": "Demonstrated by computing far-fields for a 32x32 array and scattering parameters for two 9x9 arrays, showing significant memory reduction.", "conclusion": "The method efficiently handles non-symmetric parts of arrays and improves computational performance for MoM current vector calculations."}}
{"id": "2506.05170", "pdf": "https://arxiv.org/pdf/2506.05170", "abs": "https://arxiv.org/abs/2506.05170", "authors": ["Dario Panici", "Eduardo Rodriguez", "Rory Conlin", "Daniel Dudt", "Egemen Kolemen"], "title": "Extending near-axis equilibria in DESC", "categories": ["physics.plasm-ph"], "comment": "30 pages, 7 figures", "summary": "The near-axis description of optimised stellarator fields has proven to be a\npowerful tool both for design and understanding of this magnetic confinement\nconcept. The description consists of an asymptotic model of the equilibrium in\nthe distance from its centermost axis, and is thus only approximate. Any\npractical application therefore requires the eventual construction of a global\nequilibrium. This paper presents a novel way of constructing global equilibria\nusing the \\texttt{DESC} code that guarantees the correct asymptotic behaviour\nimposed by a given near-axis construction. The theoretical underpinnings of\nthis construction are carefully presented, and benchmarking examples provided.\nThis opens the door to an efficient coupling of the near-axis framework and\nthat of global equilibria for future optimisation efforts.", "AI": {"tldr": "A novel method for constructing global stellarator equilibria using the DESC code, ensuring correct asymptotic behavior from near-axis models, is presented and benchmarked.", "motivation": "To bridge the gap between approximate near-axis models and practical global equilibrium requirements for stellarator design and optimization.", "method": "Uses the DESC code to construct global equilibria with guaranteed asymptotic behavior derived from near-axis models. Theoretical foundations and benchmarking are detailed.", "result": "Successful construction of global equilibria that align with near-axis predictions, enabling efficient coupling for future optimization.", "conclusion": "The method facilitates improved stellarator design by integrating near-axis and global equilibrium frameworks."}}
{"id": "2506.04781", "pdf": "https://arxiv.org/pdf/2506.04781", "abs": "https://arxiv.org/abs/2506.04781", "authors": ["Christoph Schirninger", "Robert Jarolim", "Astrid M. Veronig", "Christoph Kuckein"], "title": "Deep learning image burst stacking to reconstruct high-resolution ground-based solar observations", "categories": ["astro-ph.SR", "astro-ph.IM", "cs.CV", "physics.comp-ph"], "comment": null, "summary": "Large aperture ground based solar telescopes allow the solar atmosphere to be\nresolved in unprecedented detail. However, observations are limited by Earths\nturbulent atmosphere, requiring post image corrections. Current reconstruction\nmethods using short exposure bursts face challenges with strong turbulence and\nhigh computational costs. We introduce a deep learning approach that\nreconstructs 100 short exposure images into one high quality image in real\ntime. Using unpaired image to image translation, our model is trained on\ndegraded bursts with speckle reconstructions as references, improving\nrobustness and generalization. Our method shows an improved robustness in terms\nof perceptual quality, especially when speckle reconstructions show artifacts.\nAn evaluation with a varying number of images per burst demonstrates that our\nmethod makes efficient use of the combined image information and achieves the\nbest reconstructions when provided with the full image burst.", "AI": {"tldr": "A deep learning method reconstructs 100 short exposure solar images into one high-quality image in real-time, outperforming current methods in robustness and perceptual quality.", "motivation": "Ground-based solar telescopes face limitations due to Earth's turbulent atmosphere, requiring post-image corrections. Current methods struggle with strong turbulence and high computational costs.", "method": "The approach uses unpaired image-to-image translation, training on degraded bursts with speckle reconstructions as references to improve robustness and generalization.", "result": "The method shows enhanced perceptual quality, especially when speckle reconstructions have artifacts, and performs best with full image bursts.", "conclusion": "The deep learning approach efficiently combines image information, offering real-time, high-quality reconstructions for solar observations."}}
{"id": "2506.04732", "pdf": "https://arxiv.org/pdf/2506.04732", "abs": "https://arxiv.org/abs/2506.04732", "authors": ["Nicola Cavallini", "Gianmarco Manzini", "Daniele Funaro", "Andrea Favalli"], "title": "A Fast, Accurate and Oscillation-free Spectral Collocation Solver for High-dimensional Transport Problems", "categories": ["math.NA", "cs.NA"], "comment": null, "summary": "Transport phenomena-describing the movement of particles, energy, or other\nphysical quantities-are fundamental in various scientific disciplines,\nincluding nuclear physics, plasma physics, astrophysics, engineering, and the\nnatural sciences.\n  However, solving the associated seven-dimensional transport equations poses a\nsignificant computational challenge due to the curse of dimensionality.\n  We introduce the Tensor Train Superconsistent Spectral (T${^2}$S${^2}$)\nsolver to address this challenge, integrating Spectral Collocation for\nexponential convergence, Superconsistency for stabilization in\ntransport-dominated regimes, and Tensor Train format for substantial data\ncompression. T${^2}$S${^2}$ enforces a dimension-wise superconsistent condition\ncompatible with tensor structures, achieving extremely low compression ratios,\nin the order of $(10^{-12})$, while preserving spectral accuracy. Numerical\nexperiments on linear problems demonstrate that T${^2}$S${^2}$ can solve\nhigh-dimensional transport problems in minutes on standard hardware, making\npreviously intractable problems computationally feasible. This advancement\nopens new avenues for efficiently and accurately modeling complex transport\nphenomena.", "AI": {"tldr": "The paper introduces the T\u00b2S\u00b2 solver to efficiently solve high-dimensional transport equations, combining Spectral Collocation, Superconsistency, and Tensor Train format for accuracy and compression.", "motivation": "Transport phenomena are critical but solving their high-dimensional equations is computationally challenging due to dimensionality.", "method": "The T\u00b2S\u00b2 solver integrates Spectral Collocation, Superconsistency, and Tensor Train format for exponential convergence, stabilization, and data compression.", "result": "Achieves extremely low compression ratios (~10\u207b\u00b9\u00b2) and solves high-dimensional problems in minutes on standard hardware.", "conclusion": "T\u00b2S\u00b2 enables efficient and accurate modeling of complex transport phenomena, making previously intractable problems feasible."}}
{"id": "2506.04809", "pdf": "https://arxiv.org/pdf/2506.04809", "abs": "https://arxiv.org/abs/2506.04809", "authors": ["Michael J. Carley"], "title": "Numerical solution of the wave equation outside a sphere", "categories": ["math.NA", "cs.NA", "math.AP", "physics.class-ph", "physics.comp-ph"], "comment": null, "summary": "A method is presented for the fast evaluation of the transient acoustic field\ngenerated outside a spherical surface by sources inside the surface. The method\nemploys Lebedev quadratures, which are the optimal method for spatial\nintegration, and Lagrange interpolation and differentiation in an advanced time\nalgorithm for the evaluation of the transient field. Numerical testing\ndemonstrates that the approach gives near machine-precision accuracy and a\nspeed-up in evaluation time which depends on the order of quadrature rule\nemployed but breaks even with direct evaluation at a number of field points\nabout 1.15 times the number of surface quadrature nodes.", "AI": {"tldr": "A method for fast evaluation of transient acoustic fields outside a spherical surface using Lebedev quadratures and Lagrange interpolation, achieving near machine-precision accuracy and speed-up.", "motivation": "To efficiently and accurately evaluate transient acoustic fields generated by internal sources outside a spherical surface.", "method": "Uses Lebedev quadratures for spatial integration and Lagrange interpolation/differentiation in an advanced time algorithm.", "result": "Near machine-precision accuracy; speed-up depends on quadrature order, breaking even at ~1.15 times the number of surface nodes.", "conclusion": "The method is efficient and accurate for transient acoustic field evaluation."}}
{"id": "2506.04791", "pdf": "https://arxiv.org/pdf/2506.04791", "abs": "https://arxiv.org/abs/2506.04791", "authors": ["Athanasios C. Antoulas", "Ion Victor Gosea", "Charles Poussot-Vassal", "Pierre Vuillemin"], "title": "Tensor-based multivariate function approximation: methods benchmarking and comparison", "categories": ["math.NA", "cs.CE", "cs.NA", "cs.SE", "93A15, 93A30, 93B11, 93B15, 93C05, 93C80"], "comment": "Report with a collection of examples, aimed at being regularly\n  updated. Associated GIT: https://github.com/cpoussot/mLF", "summary": "In this note, we evaluate the performances, the features and the\nuser-experience of some methods (and their implementations) designed for\ntensor- (or data-) based multivariate function construction and approximation.\nTo this aim, a collection of multivariate functions extracted from contributive\nworks coming from different communities, is suggested. First, these functions\nwith varying complexity (e.g. number and degree of the variables) and nature\n(e.g. rational, irrational, differentiable or not, symmetric, etc.) are used to\nconstruct tensors, each of different dimension and size on the disk. Second,\ngrounded on this tensor, we inspect performances of each considered method\n(e.g. the accuracy, the computational time, the parameters tuning impact,\netc.). Finally, considering the \"best\" parameter tuning set, we compare each\nmethod using multiple evaluation criteria. The purpose of this note is not to\nrank the methods but rather to evaluate as fairly as possible the different\navailable strategies, with the idea in mind to guide users to understand the\nprocess, the possibilities, the advantages and the limits brought by each\ntools. The contribution claimed is to suggest a complete benchmark collection\nof some available tools for tensor approximation by surrogate models (e.g.\nrational functions, networks, etc.). In addition, as contributors of the\nmultivariate Loewner Framework (mLF) approach (and its side implementation in\nMDSPACK), attention and details of the latter are more explicitly given, in\norder to provide readers a digest of this contributive work and some details\nwith simple examples.", "AI": {"tldr": "The paper evaluates various tensor-based multivariate function approximation methods, comparing their performance, features, and user experience using a benchmark collection of functions.", "motivation": "To fairly assess and compare different tensor approximation tools, guiding users in understanding their processes, advantages, and limitations.", "method": "Construct tensors from a diverse set of multivariate functions, evaluate method performances (accuracy, computational time, parameter tuning), and compare methods using multiple criteria.", "result": "A benchmark collection is provided, and the multivariate Loewner Framework (mLF) is highlighted with detailed examples.", "conclusion": "The note aims to guide users in selecting tensor approximation tools without ranking them, emphasizing understanding and practical insights."}}
{"id": "2506.04898", "pdf": "https://arxiv.org/pdf/2506.04898", "abs": "https://arxiv.org/abs/2506.04898", "authors": ["Xintong Zou", "Zhijie Li", "Yunpeng Wang", "Huiyu Yang", "Jianchun Wang"], "title": "Uncertainty quantification and stability of neural operators for prediction of three-dimensional turbulence", "categories": ["physics.flu-dyn", "physics.comp-ph"], "comment": null, "summary": "Turbulence poses challenges for numerical simulation due to its chaotic,\nmultiscale nature and high computational cost. Traditional turbulence modeling\noften struggles with accuracy and long-term stability. Recent scientific\nmachine learning (SciML) models, such as Fourier Neural Operators (FNO), show\npromise in solving PDEs, but are typically limited to one-step-ahead\npredictions and often fail over long time horizons, especially in 3D\nturbulence. This study proposes a framework to assess the reliability of neural\noperator models in turbulent flows. Using three-dimensional forced homogeneous\nisotropic turbulence (HIT) as a benchmark, we evaluate models in terms of\nuncertainty quantification (UQ), error propagation, and sensitivity to initial\nperturbations. Statistical tools such as error distribution analysis and\nautocorrelation functions (ACF) are used to assess predictive robustness and\ntemporal coherence. Our proposed model, the factorized-implicit FNO (F-IFNO),\nimproves long-term stability and accuracy by incorporating implicit\nfactorization into the prediction process. It outperforms conventional LES and\nother FNO-based models in balancing accuracy, stability, and efficiency. The\nresults highlight the importance of prediction constraints, time interval\nselection, and UQ in developing robust neural operator frameworks for turbulent\nsystems.", "AI": {"tldr": "The study evaluates neural operator models for 3D turbulence, proposing the F-IFNO model for improved stability and accuracy.", "motivation": "Traditional turbulence modeling lacks accuracy and stability, while existing SciML models like FNO struggle with long-term predictions in 3D turbulence.", "method": "The framework assesses neural operators using 3D HIT, focusing on UQ, error propagation, and sensitivity. The F-IFNO model incorporates implicit factorization.", "result": "F-IFNO outperforms conventional LES and other FNO models in accuracy, stability, and efficiency.", "conclusion": "Prediction constraints, time interval selection, and UQ are crucial for robust neural operator frameworks in turbulence."}}
{"id": "2506.05031", "pdf": "https://arxiv.org/pdf/2506.05031", "abs": "https://arxiv.org/abs/2506.05031", "authors": ["Mohammad Mirzakhani", "Kyungsun Moon"], "title": "Quantum simulation of the Hubbard model on a graphene hexagon: Strengths of IQPE and noise constraints", "categories": ["quant-ph", "cond-mat.mes-hall", "cond-mat.str-el", "physics.comp-ph"], "comment": "14 pages, 10 figures", "summary": "Quantum computing offers transformative potential for simulating real-world\nmaterials, providing a powerful platform to investigate complex quantum systems\nacross quantum chemistry and condensed matter physics. In this work, we\nleverage this capability to simulate the Hubbard model on a six-site graphene\nhexagon using Qiskit, employing the Iterative Quantum Phase Estimation (IQPE)\nand adiabatic evolution algorithms to determine its ground-state properties.\nNoiseless simulations yield accurate ground-state energies (GSEs), charge and\nspin densities, and correlation functions, all in excellent agreement with\nexact diagonalization, underscoring the precision and reliability of quantum\nsimulation for strongly correlated electron systems. However, deploying IQPE\nand adiabatic evolution on today's noisy quantum hardware remains highly\nchallenging. To examine these limitations, we utilize the Qiskit Aer simulator\nwith a custom noise model tailored to the characteristics of IBM's real\nbackend. This model includes realistic depolarizing gate errors, thermal\nrelaxation, and readout noise, allowing us to explore how these factors degrade\nsimulation accuracy. Preliminary hardware runs on IBM devices further expose\ndiscrepancies between simulated and real-world noise, emphasizing the gap\nbetween ideal and practical implementations. Overall, our results highlight the\npromise of quantum computing for simulating correlated quantum materials, while\nalso revealing the significant challenges posed by hardware noise in achieving\naccurate and reliable physical predictions using current quantum devices.", "AI": {"tldr": "The paper explores quantum simulation of the Hubbard model on graphene using Qiskit, achieving accurate results in noiseless conditions but facing challenges with noise on real hardware.", "motivation": "To demonstrate the potential of quantum computing for simulating strongly correlated quantum systems like the Hubbard model, while addressing the limitations imposed by current noisy hardware.", "method": "Utilized Iterative Quantum Phase Estimation (IQPE) and adiabatic evolution algorithms on Qiskit, with noiseless simulations and a custom noise model for IBM backends.", "result": "Noiseless simulations matched exact diagonalization results, but noise degraded accuracy, revealing discrepancies between simulated and real hardware performance.", "conclusion": "Quantum computing shows promise for material simulation, but hardware noise remains a significant barrier to reliable predictions."}}
{"id": "2506.04840", "pdf": "https://arxiv.org/pdf/2506.04840", "abs": "https://arxiv.org/abs/2506.04840", "authors": ["Maolin Che", "Yimin Wei", "Chong Wu", "Hong Yan"], "title": "Efficient randomized algorithms for the fixed Tucker-rank problem of Tucker decomposition with adaptive shifts", "categories": ["math.NA", "cs.NA", "65F55, 68W20, 15A18, 15A69"], "comment": "41 pages, 43 figures", "summary": "Randomized numerical linear algebra is proved to bridge theoretical\nadvancements to offer scalable solutions for approximating tensor\ndecomposition. This paper introduces fast randomized algorithms for solving the\nfixed Tucker-rank problem of Tucker decomposition, through the integration of\nadaptive shifted power iterations. The proposed algorithms enhance randomized\nvariants of truncated high-order singular value decomposition (T-HOSVD) and\nsequentially T-HOSVD (ST-HOSVD) by incorporating dynamic shift strategies,\nwhich accelerate convergence by refining the singular value gap and reduce the\nnumber of required power iterations while maintaining accuracy. Theoretical\nanalyses provide probabilistic error bounds, demonstrating that the proposed\nmethods achieve comparable or superior accuracy compared to deterministic\napproaches. Numerical experiments on synthetic and real-world datasets validate\nthe efficiency and robustness of the proposed algorithms, showing a significant\ndecline in runtime and approximation error over state-of-the-art techniques.", "AI": {"tldr": "The paper introduces fast randomized algorithms for Tucker decomposition using adaptive shifted power iterations, improving efficiency and accuracy over existing methods.", "motivation": "To bridge theoretical advancements with scalable solutions for tensor decomposition, addressing the fixed Tucker-rank problem.", "method": "Integration of adaptive shifted power iterations into randomized variants of T-HOSVD and ST-HOSVD, refining singular value gaps to accelerate convergence.", "result": "Theoretical error bounds and numerical experiments show the algorithms outperform state-of-the-art techniques in runtime and approximation error.", "conclusion": "The proposed methods offer efficient, robust, and accurate solutions for Tucker decomposition, validated by synthetic and real-world datasets."}}
{"id": "2506.05105", "pdf": "https://arxiv.org/pdf/2506.05105", "abs": "https://arxiv.org/abs/2506.05105", "authors": ["Fang-Cheng Wang", "Qi-Jun Ye", "Yu-Cheng Zhu", "Xin-Zheng Li"], "title": "Classification and enumeration of solid-solid phase transition mechanisms", "categories": ["cond-mat.mtrl-sci", "physics.comp-ph"], "comment": "22 pages, 14 figures", "summary": "Crystal-structure match (CSM), the atom-to-atom correspondence between two\ncrystalline phases, is used extensively to describe solid-solid phase\ntransition (SSPT) mechanisms. However, existing computational methods cannot\naccount for all possible CSMs. Here, we propose a formalism to classify all\nCSMs into a tree structure, which is independent of the choices of unit cell\nand supercell. We rigorously proved that only a finite number of noncongruent\nCSMs are of practical interest. By representing CSMs as integer matrices, we\nintroduce the crystmatch method to exhaustively enumerate them, which\nuncontroversially solves the CSM optimization problem under any geometric\ncriterion. For most SSPTs, crystmatch can reproduce all known deformation\nmechanisms and CSMs within 10 CPU minutes, while also revealing thousands of\nnew candidates. The resulting database can be further used for comparing\nexperimental phenomena, high-throughput energy barrier calculations, or machine\nlearning.", "AI": {"tldr": "A new method, crystmatch, is proposed to classify and enumerate all possible crystal-structure matches (CSMs) for solid-solid phase transitions (SSPTs), solving the CSM optimization problem efficiently.", "motivation": "Existing methods cannot account for all possible CSMs, limiting the understanding of SSPT mechanisms.", "method": "The formalism classifies CSMs into a tree structure, representing them as integer matrices, and uses crystmatch to exhaustively enumerate them.", "result": "crystmatch reproduces known mechanisms and reveals thousands of new CSM candidates within 10 CPU minutes.", "conclusion": "The method provides a comprehensive database for SSPT analysis, enabling applications like experimental comparison and machine learning."}}
{"id": "2506.04857", "pdf": "https://arxiv.org/pdf/2506.04857", "abs": "https://arxiv.org/abs/2506.04857", "authors": ["Junming Duan", "Praveen Chandrashekar", "Christian Klingenberg"], "title": "Active flux for ideal magnetohydrodynamics: A positivity-preserving scheme with the Godunov-Powell source term", "categories": ["math.NA", "cs.NA"], "comment": "27 pages, 12 figures", "summary": "The Active Flux (AF) is a compact, high-order finite volume scheme that\nallows more flexibility by introducing additional point value degrees of\nfreedom at cell interfaces. This paper proposes a positivity-preserving (PP) AF\nscheme for solving the ideal magnetohydrodynamics, where the Godunov-Powell\nsource term is employed to deal with the divergence-free constraint. For the\nevolution of the cell average, apart from the standard conservative finite\nvolume method for the flux derivative, the nonconservative source term is built\non the quadratic reconstruction in each cell, which maintains the compact\nstencil in the AF scheme. For the point value update, the local Lax-Friedrichs\n(LLF) flux vector splitting is adopted for the flux derivative, originally\nproposed in [Duan, Barsukow, and Klingenberg, SIAM Journal on Scientific\nComputing, 47(2), A811--A837, 2025], and a central difference is used to\ndiscretize the divergence in the source term. A parametrized flux limiter and a\nscaling limiter are presented to preserve the density and pressure positivity\nby blending the AF scheme with the first-order PP LLF scheme with the source\nterm. To suppress oscillations, a new shock sensor considering the divergence\nerror is proposed, which is used to compute the blending coefficients for the\ncell average. Several numerical tests are conducted to verify the third-order\naccuracy, PP property, and shock-capturing ability of the scheme. The key role\nof the Godunov-Powell source term and its suitable discretization in\ncontrolling divergence error is also validated.", "AI": {"tldr": "A positivity-preserving Active Flux scheme for ideal magnetohydrodynamics is proposed, incorporating the Godunov-Powell source term for divergence-free constraints and using limiters to ensure density and pressure positivity.", "motivation": "To develop a high-order, compact finite volume scheme for ideal magnetohydrodynamics that maintains positivity of density and pressure while handling divergence-free constraints effectively.", "method": "The scheme combines the Active Flux method with the Godunov-Powell source term, uses quadratic reconstruction for nonconservative terms, and employs flux and scaling limiters for positivity. A new shock sensor is introduced to control oscillations.", "result": "Numerical tests confirm third-order accuracy, positivity preservation, and effective shock-capturing. The Godunov-Powell source term's role in controlling divergence error is validated.", "conclusion": "The proposed scheme successfully balances high-order accuracy, compactness, and positivity preservation, demonstrating robustness in handling magnetohydrodynamics problems."}}
{"id": "2506.05122", "pdf": "https://arxiv.org/pdf/2506.05122", "abs": "https://arxiv.org/abs/2506.05122", "authors": ["Shaziya A. Banu", "Venkata R. S. B. Varanasi", "Arash Noshadravan", "Sara Abedi"], "title": "Reactive Transport Simulation of Silicate-Rich Shale Rocks when Exposed to CO2 Saturated Brine Under High Pressure and High Temperature", "categories": ["physics.geo-ph", "physics.comp-ph"], "comment": null, "summary": "This study examines the feasibility of carbon dioxide storage in shale rocks\nand the reliability of reactive transport models in achieving accurate\nreplication of the chemo-mechanical interactions and transport processes\ntranspiring in these rocks when subjected to CO2 saturated brine. Owing to the\nheterogeneity of rocks, experimental testing for adequate deductions and\nfindings, could be an expensive and time-intensive process. Therefore, this\nstudy proposes utilization of reactive transport modeling to replicate the\npore-scale chemo-mechanical reactions and transport processes occurring in\nsilicate-rich shale rocks in the presence of CO2 saturated brine under high\npressure and high temperature. For this study, Crunch Tope has been adopted to\nsimulate a one-dimensional reactive transport model of a Permian rock specimen\nexposed to the acidic brine at a temperature of 100 {\\deg}C and pressure of\n12.40 MPa (1800 psi) for a period of 14 and 28 days. The results demonstrated\nsignificant dissolution followed by precipitation of quartz rich phases,\nprecipitation and swelling of clay rich phases, and dissolution of feldspar\nrich phases closer to the acidic brine-rock interface. Moreover, porosity\nagainst reaction depth curve showed nearly 1.00% mineral precipitation occur at\n14 and 28 days, which is insufficient to completely fill the pore spaces.", "AI": {"tldr": "The study explores CO2 storage in shale rocks using reactive transport modeling to simulate chemo-mechanical interactions, avoiding costly experiments. Results show mineral dissolution and precipitation but insufficient pore-filling.", "motivation": "To address the high cost and time of experimental testing for CO2 storage in heterogeneous shale rocks by using reactive transport models.", "method": "Used Crunch Tope to simulate a 1D reactive transport model of a Permian rock specimen under high pressure (12.40 MPa) and temperature (100\u00b0C) with CO2 saturated brine for 14 and 28 days.", "result": "Observed dissolution of quartz and feldspar, precipitation of quartz-rich phases, and clay swelling. Porosity changes showed only 1.00% mineral precipitation, insufficient to fill pores.", "conclusion": "Reactive transport modeling is feasible for studying CO2 storage in shale, but mineral precipitation alone may not fully seal pores."}}
{"id": "2506.04880", "pdf": "https://arxiv.org/pdf/2506.04880", "abs": "https://arxiv.org/abs/2506.04880", "authors": ["Heiko Gimperlein", "Ruma R. Maity"], "title": "Numerical analysis for constrained and unconstrained Q-tensor energies for liquid crystals", "categories": ["math.NA", "cs.NA", "65N12, 35J47, 65N30, 76A15"], "comment": null, "summary": "This paper introduces a comprehensive finite element approximation framework\nfor three-dimensional Landau-de Gennes $Q$-tensor energies for nematic liquid\ncrystals, with a particular focus on the anisotropy of the elastic energy and\nthe Ball-Majumdar singular potential. This potential imposes essential physical\nconstraints on the eigenvalues of the $Q$-tensor, ensuring realistic modeling.\nWe address the approximation of regular solutions to nonlinear elliptic partial\ndifferential equations with non-homogeneous boundary conditions associated with\nLandau-de Gennes energies. The well-posedness of the discrete linearized\nproblem is rigorously demonstrated. The existence and local uniqueness of the\ndiscrete solution is derived using the Newton-Kantorovich theorem. Furthermore,\nwe demonstrate an optimal order convergence rate in the energy norm and discuss\nthe impact of eigenvalue constraints on the a priori error analysis.", "AI": {"tldr": "A finite element framework for 3D Landau-de Gennes Q-tensor energies, focusing on anisotropy and physical constraints, with rigorous analysis of well-posedness, convergence, and error impact.", "motivation": "To model nematic liquid crystals realistically by addressing anisotropy and physical constraints in the Q-tensor energy framework.", "method": "Finite element approximation for nonlinear elliptic PDEs with non-homogeneous boundary conditions, using the Newton-Kantorovich theorem for discrete solutions.", "result": "Well-posedness of the discrete problem, local uniqueness of solutions, and optimal convergence rates in the energy norm.", "conclusion": "The framework effectively models nematic liquid crystals with rigorous theoretical backing and practical convergence guarantees."}}
{"id": "2506.05292", "pdf": "https://arxiv.org/pdf/2506.05292", "abs": "https://arxiv.org/abs/2506.05292", "authors": ["Declan A. Norton", "Yuanzhao Zhang", "Michelle Girvan"], "title": "Learning Beyond Experience: Generalizing to Unseen State Space with Reservoir Computing", "categories": ["cs.LG", "math.DS", "nlin.CD", "physics.comp-ph"], "comment": "15 pages, 9 figures", "summary": "Machine learning techniques offer an effective approach to modeling dynamical\nsystems solely from observed data. However, without explicit structural priors\n-- built-in assumptions about the underlying dynamics -- these techniques\ntypically struggle to generalize to aspects of the dynamics that are poorly\nrepresented in the training data. Here, we demonstrate that reservoir computing\n-- a simple, efficient, and versatile machine learning framework often used for\ndata-driven modeling of dynamical systems -- can generalize to unexplored\nregions of state space without explicit structural priors. First, we describe a\nmultiple-trajectory training scheme for reservoir computers that supports\ntraining across a collection of disjoint time series, enabling effective use of\navailable training data. Then, applying this training scheme to multistable\ndynamical systems, we show that RCs trained on trajectories from a single basin\nof attraction can achieve out-of-domain generalization by capturing system\nbehavior in entirely unobserved basins.", "AI": {"tldr": "Reservoir computing can generalize to unexplored state space regions without explicit structural priors, using a multiple-trajectory training scheme.", "motivation": "Machine learning struggles to generalize without structural priors, especially for poorly represented dynamics in training data.", "method": "A multiple-trajectory training scheme for reservoir computers is introduced, enabling training across disjoint time series.", "result": "RCs trained on single basin trajectories generalize to unobserved basins in multistable systems.", "conclusion": "Reservoir computing can achieve out-of-domain generalization without explicit structural assumptions."}}
{"id": "2506.04969", "pdf": "https://arxiv.org/pdf/2506.04969", "abs": "https://arxiv.org/abs/2506.04969", "authors": ["Yema Paul", "Emmanuel Delande", "Francois Vinet", "Francois Laporte", "Manuel Sanjurjo-Rivo", "Aldo Tonnini", "Joan-Pau Sanchez"], "title": "Probability of Collision with Tethered Spacecraft", "categories": ["math.NA", "cs.NA"], "comment": "13 pages, 2 figures, Engineering Note", "summary": "This Engineering Note addresses the challenge of estimating the probability\nof collision for tethered spacecraft during close encounters with other space\nobjects. Standard probability of collision methods, based on spherical\nhard-body assumptions, tend to be overly conservative when applied to long\ntether systems. We introduce a method that accounts for the tether's spatial\nextent and configuration uncertainty by maximizing the probability of collision\nover all physically admissible tether shapes. Applied to real-world conjunction\nevents involving a kilometer-scale flexible inextensible tether, the method\nyields more realistic risk estimates. This approach improves the ability to\ndistinguish hazardous from benign encounters, thereby supporting more informed\ncollision avoidance decisions.", "AI": {"tldr": "A method to estimate collision probability for tethered spacecraft, addressing conservatism in standard spherical models by accounting for tether extent and uncertainty.", "motivation": "Standard collision probability methods are overly conservative for tethered spacecraft due to spherical hard-body assumptions.", "method": "Maximizes collision probability over all physically admissible tether shapes, considering spatial extent and configuration uncertainty.", "result": "Applied to real-world events, the method provides more realistic risk estimates for kilometer-scale tethers.", "conclusion": "Improves hazard distinction, aiding better collision avoidance decisions."}}
{"id": "2506.05174", "pdf": "https://arxiv.org/pdf/2506.05174", "abs": "https://arxiv.org/abs/2506.05174", "authors": ["Yifan Zhang", "Joe Kileel"], "title": "Norming Sets for Tensor and Polynomial Sketching", "categories": ["math.NA", "cs.IT", "cs.NA", "math.AG", "math.IT"], "comment": "16 pages", "summary": "This paper develops the sketching (i.e., randomized dimension reduction)\ntheory for real algebraic varieties and images of polynomial maps, including,\ne.g., the set of low rank tensors and tensor networks. Through the lens of\nnorming sets, we provide a framework for controlling the sketching dimension\nfor \\textit{any} sketch operator used to embed said sets, including\nsub-Gaussian, fast Johnson-Lindenstrauss, and tensor structured sketch\noperators. Leveraging norming set theory, we propose a new sketching method\ncalled the median sketch. It embeds such a set $V$ using only\n$\\widetilde{\\mathcal{O}}(\\dim V)$ tensor structured or sparse linear\nmeasurements.", "AI": {"tldr": "The paper introduces a sketching theory for real algebraic varieties and polynomial maps, proposing a new method called the median sketch, which uses fewer measurements.", "motivation": "To extend sketching theory to algebraic varieties and polynomial maps, including low-rank tensors, and provide a universal framework for controlling sketching dimensions.", "method": "Uses norming set theory to develop the median sketch, requiring only ~O(dim V) tensor-structured or sparse linear measurements.", "result": "The median sketch efficiently embeds sets like low-rank tensors with reduced measurements.", "conclusion": "The framework and median sketch advance sketching theory for algebraic structures, offering practical efficiency."}}
{"id": "2506.04279", "pdf": "https://arxiv.org/pdf/2506.04279", "abs": "https://arxiv.org/abs/2506.04279", "authors": ["Adama Nouboukpo", "Kodzo Michel Aladji", "Muktar Bappa"], "title": "Nombre Effectif de Partis Politiques en Afrique: Une Nouvelle M\u00e9thode pour un Calcul Objectif et Institutionnellement Neutre", "categories": ["physics.soc-ph", "cs.NA", "math.NA"], "comment": "in French language", "summary": "Political fragmentation in Africa poses to a significant challenge to\neffective governance and stability. Traditional measures of party system\nfragmentation, such as the Effective Number of Parties (ENP) index, often fail\nto capture the nuanced realities of African political landscapes, particularly\nthe influence of dominant parties, fluid party affiliations, and the impact of\nethnic and regional cleavages. To address these limitations, this paper\nintroduces two novel \"apolitical\" or \"institutionally neutral\" measures for\ncalculating the effective number of parties, focusing on geographical and\ndemographic dimensions, notably population size and territorial area. By\nincorporating these local realities and ensuring a minimum threshold of two\nparties, the proposed models offer a simpler and more contextually relevant\nframework for understanding political dynamics in Africa, especially in\ndata-scarce environments. This approach provides a valuable tool for analyzing\nand streamlining political systems, with potential for broader application\nbeyond the African context.", "AI": {"tldr": "The paper introduces two new measures for calculating party fragmentation in Africa, focusing on geography and demographics, to better reflect local political realities.", "motivation": "Traditional measures like the ENP index fail to capture nuances of African politics, such as dominant parties and ethnic cleavages.", "method": "Proposes two 'apolitical' measures based on population size and territorial area, with a minimum threshold of two parties.", "result": "The new models offer a simpler, contextually relevant framework for analyzing African political systems, even in data-scarce settings.", "conclusion": "This approach provides a valuable tool for understanding political fragmentation, with potential applications beyond Africa."}}
{"id": "2506.04655", "pdf": "https://arxiv.org/pdf/2506.04655", "abs": "https://arxiv.org/abs/2506.04655", "authors": ["Mengjiao Bai", "Huaian Diao", "Weisheng Zhou"], "title": "Inverse elastic obstacle scattering problems by monotonicity method", "categories": ["math.AP", "cs.NA", "math.NA"], "comment": null, "summary": "We consider the elastic wave scattering problem involving rigid obstacles.\nThis work addresses the inverse problem of reconstructing the position and\nshape of such obstacles using far-field measurements. A novel\nmonotonicity-based approach is developed for this purpose. By factorizing the\nfar-field operator and utilizing the existence of localized wave functions, we\nderive a shape characterization criterion for the obstacle boundary. The\nproposed method employs monotonicity tests to determine the geometric\nrelationship between any given test domain and the actual scatterer. As a\nresult, the shape and location of rigid elastic obstacles can be uniquely\nidentified without requiring any initial guesses or prior knowledge of the\nphysical parameters of the homogeneous background medium.", "AI": {"tldr": "A monotonicity-based method is proposed to uniquely identify the shape and position of rigid elastic obstacles using far-field measurements, without initial guesses or prior knowledge.", "motivation": "To solve the inverse problem of reconstructing rigid obstacles in elastic wave scattering using far-field data.", "method": "Develops a monotonicity-based approach, factorizes the far-field operator, and uses localized wave functions to derive a shape characterization criterion.", "result": "The method uniquely identifies the shape and location of obstacles without initial guesses or background medium knowledge.", "conclusion": "The novel approach effectively solves the inverse problem for rigid elastic obstacles."}}
