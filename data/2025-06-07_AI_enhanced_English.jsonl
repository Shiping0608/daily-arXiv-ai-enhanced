{"id": "2506.04354", "pdf": "https://arxiv.org/pdf/2506.04354", "abs": "https://arxiv.org/abs/2506.04354", "authors": ["Elmira Mirzabeigi", "Rezvan Salehi", "Kourosh Parand"], "title": "BridgeNet: A Hybrid, Physics-Informed Machine Learning Framework for Solving High-Dimensional Fokker-Planck Equations", "categories": ["physics.comp-ph", "cs.LG", "math-ph", "math.AP", "math.MP"], "comment": null, "summary": "BridgeNet is a novel hybrid framework that integrates convolutional neural\nnetworks with physics-informed neural networks to efficiently solve non-linear,\nhigh-dimensional Fokker-Planck equations (FPEs). Traditional PINNs, which\ntypically rely on fully connected architectures, often struggle to capture\ncomplex spatial hierarchies and enforce intricate boundary conditions. In\ncontrast, BridgeNet leverages adaptive CNN layers for effective local feature\nextraction and incorporates a dynamically weighted loss function that\nrigorously enforces physical constraints. Extensive numerical experiments\nacross various test cases demonstrate that BridgeNet not only achieves\nsignificantly lower error metrics and faster convergence compared to\nconventional PINN approaches but also maintains robust stability in\nhigh-dimensional settings. This work represents a substantial advancement in\ncomputational physics, offering a scalable and accurate solution methodology\nwith promising applications in fields ranging from financial mathematics to\ncomplex system dynamics.", "AI": {"tldr": "BridgeNet combines CNNs and physics-informed neural networks to solve complex Fokker-Planck equations more efficiently than traditional PINNs, achieving better accuracy and stability.", "motivation": "Traditional PINNs struggle with spatial hierarchies and boundary conditions in high-dimensional FPEs, prompting the need for a hybrid approach.", "method": "BridgeNet uses adaptive CNN layers for local feature extraction and a dynamically weighted loss function to enforce physical constraints.", "result": "BridgeNet outperforms conventional PINNs with lower errors, faster convergence, and robust stability in high-dimensional settings.", "conclusion": "BridgeNet advances computational physics, offering scalable and accurate solutions for applications in finance and complex systems."}}
{"id": "2506.04489", "pdf": "https://arxiv.org/pdf/2506.04489", "abs": "https://arxiv.org/abs/2506.04489", "authors": ["Lukas Exl", "Sebastian Schaffer"], "title": "Spectrally accurate and efficient convolution with the 3D free-space Laplace Green's function via the super-potential", "categories": ["physics.comp-ph", "cs.NA", "math.NA", "65N35, 65N80"], "comment": "11 pages, 1 figure", "summary": "We present a high-accuracy spectral method for solving the unbounded\nthree-dimensional Poisson equation with smooth, compactly supported sources.\nThe approach is based on a super-potential formulation, where the solution is\nobtained by applying the Laplacian to a convolution with the biharmonic Green's\nfunction. A separable Gaussian-sum (GS) approximation enables efficient\nFFT-based computation with quasi-linear complexity. Owing to the improved\nregularity of the biharmonic kernel, the GS cutoff error is of order four,\neliminating the need for correction terms or Taylor expansions required in\nstandard GS or Ewald-type methods. Numerical benchmarks demonstrate that the\nmethod achieves machine-precision accuracy and outperforms existing GS-based\nschemes in both error and runtime, making it a robust and efficient tool for\nfree-space Poisson problems on uniform grids.", "AI": {"tldr": "A high-accuracy spectral method for solving the 3D Poisson equation with smooth, compactly supported sources, using a super-potential formulation and Gaussian-sum approximation for efficient FFT-based computation.", "motivation": "To address the need for a robust and efficient method for solving unbounded 3D Poisson equations with high accuracy and quasi-linear complexity.", "method": "Uses a super-potential formulation involving the biharmonic Green's function and a separable Gaussian-sum approximation for FFT-based computation.", "result": "Achieves machine-precision accuracy, outperforms existing Gaussian-sum-based schemes in error and runtime, and eliminates the need for correction terms.", "conclusion": "The method is a robust and efficient tool for free-space Poisson problems on uniform grids."}}
{"id": "2506.04763", "pdf": "https://arxiv.org/pdf/2506.04763", "abs": "https://arxiv.org/abs/2506.04763", "authors": ["Shuai Lu"], "title": "A highly scalable numerical framework for reservoir simulation on UG4 platform", "categories": ["physics.comp-ph", "cs.DC"], "comment": null, "summary": "The modeling and simulation of multiphase fluid flow receive significant\nattention in reservoir engineering. Many time discretization schemes for\nmultiphase flow equations are either explicit or semi-implicit, relying on the\ndecoupling between the saturation equation and the pressure equation. In this\nstudy, we delve into a fully coupled and fully implicit framework for\nsimulating multiphase flow in heterogeneous porous media, considering gravity\nand capillary effects. We utilize the Vertex-Centered Finite Volume Method for\nspatial discretization and propose an efficient implementation of interface\nconditions for heterogeneous porous media within the current scheme. Notably,\nwe introduce the Linearly Implicit Extrapolation Method (LIMEX) with an error\nestimator, adapted for the first time to multiphase flow problems. To solve the\nresulting linear system, we employ the BiCGSTAB method with the Geometric\nMultigrid (GMG) preconditioner. The implementations of models and methods are\nbased on the open-source software: UG4. The results from parallel computations\non the supercomputer demonstrate that the scalability of our proposed framework\nis sufficient, supporting a scale of thousands of processors with Degrees of\nFreedom (DoF) extending up to billions.", "AI": {"tldr": "The paper presents a fully coupled, fully implicit framework for simulating multiphase flow in heterogeneous porous media, using advanced numerical methods and demonstrating scalability on supercomputers.", "motivation": "Existing time discretization schemes for multiphase flow often decouple saturation and pressure equations, limiting accuracy. This study aims to improve modeling by fully coupling these equations and incorporating gravity and capillary effects.", "method": "The study employs the Vertex-Centered Finite Volume Method for spatial discretization, introduces the Linearly Implicit Extrapolation Method (LIMEX) with an error estimator, and uses BiCGSTAB with Geometric Multigrid preconditioning for solving linear systems.", "result": "The framework shows strong scalability, supporting thousands of processors and billions of Degrees of Freedom (DoF) in parallel computations on a supercomputer.", "conclusion": "The proposed fully implicit framework is efficient and scalable, offering improved accuracy for multiphase flow simulations in heterogeneous porous media."}}
{"id": "2506.04835", "pdf": "https://arxiv.org/pdf/2506.04835", "abs": "https://arxiv.org/abs/2506.04835", "authors": ["Julien El Hajj", "Gilles Ledoux", "Samy Merabia"], "title": "Thermoplasmonics of Gold-Core Silica-Shell Colloidal Nanoparticles under Pulse Illumination", "categories": ["physics.comp-ph", "cond-mat.mtrl-sci"], "comment": null, "summary": "Core-shell nanoparticles, particularly those having a gold core, have emerged\nas a highly promising class of materials due to their unique optical and\nthermal properties, which underpin a wide range of applications in photothermal\ntherapy, imaging, and biosensing. In this study, we present a comprehensive\nstudy of the thermal dynamics of gold-core silica-shell nanoparticles immersed\nin water under pulse illumination. The plasmonic response of the core-shell\nnanoparticle is described by incorporating Mie theory with electronic\ntemperature corrections to the refractive indices of gold, based on a Drude\nLorentz formulation. The thermal response of the core-shell nanoparticles is\nmodeled by coupling the two temperature model with molecular dynamics\nsimulations, providing an atomistic description of nanoscale heat transfer. We\ninvestigate nanoparticles with both dense and porous silica shells (with 50%\nporosity) under laser pulse durations of 100 fs, 10 ps, and 1 ns, and over a\nrange of fluences between 0.05 and 5mJ/cm2. We show that nanoparticles with a\nthin dense silica shell (5 nm) exhibit significantly faster water heating\ncompared to bare gold nanoparticles. This behavior is attributed to enhanced\nelectron-phonon coupling at the gold silica interface and to the relatively\nhigh thermal conductance between silica and water. These findings provide new\ninsights into optimizing nanoparticle design for efficient photothermal\napplications and establish a robust framework for understanding energy transfer\nmechanisms in heterogeneous metal dielectric nanostructures.", "AI": {"tldr": "Study explores thermal dynamics of gold-core silica-shell nanoparticles under pulse illumination, revealing faster water heating with thin dense silica shells due to enhanced electron-phonon coupling.", "motivation": "Core-shell nanoparticles, especially gold-core ones, are promising for photothermal therapy, imaging, and biosensing due to their unique properties. Understanding their thermal dynamics is key for optimizing applications.", "method": "Combines Mie theory with electronic temperature corrections and couples the two-temperature model with molecular dynamics simulations to study thermal response under varied laser pulses and fluences.", "result": "Nanoparticles with thin dense silica shells (5 nm) heat water faster than bare gold nanoparticles, attributed to enhanced electron-phonon coupling and high silica-water thermal conductance.", "conclusion": "Findings offer insights for optimizing nanoparticle design for photothermal applications and understanding energy transfer in metal-dielectric nanostructures."}}
{"id": "2506.04375", "pdf": "https://arxiv.org/pdf/2506.04375", "abs": "https://arxiv.org/abs/2506.04375", "authors": ["Conor Rowan", "John Evans", "Kurt Maute", "Alireza Doostan"], "title": "Solving engineering eigenvalue problems with neural networks using the Rayleigh quotient", "categories": ["math.NA", "cs.LG", "cs.NA"], "comment": null, "summary": "From characterizing the speed of a thermal system's response to computing\nnatural modes of vibration, eigenvalue analysis is ubiquitous in engineering.\nIn spite of this, eigenvalue problems have received relatively little treatment\ncompared to standard forward and inverse problems in the physics-informed\nmachine learning literature. In particular, neural network discretizations of\nsolutions to eigenvalue problems have seen only a handful of studies. Owing to\ntheir nonlinearity, neural network discretizations prevent the conversion of\nthe continuous eigenvalue differential equation into a standard discrete\neigenvalue problem. In this setting, eigenvalue analysis requires more\nspecialized techniques. Using a neural network discretization of the\neigenfunction, we show that a variational form of the eigenvalue problem called\nthe \"Rayleigh quotient\" in tandem with a Gram-Schmidt orthogonalization\nprocedure is a particularly simple and robust approach to find the eigenvalues\nand their corresponding eigenfunctions. This method is shown to be useful for\nfinding sets of harmonic functions on irregular domains, parametric and\nnonlinear eigenproblems, and high-dimensional eigenanalysis. We also discuss\nthe utility of harmonic functions as a spectral basis for approximating\nsolutions to partial differential equations. Through various examples from\nengineering mechanics, the combination of the Rayleigh quotient objective,\nGram-Schmidt procedure, and the neural network discretization of the\neigenfunction is shown to offer unique advantages for handling continuous\neigenvalue problems.", "AI": {"tldr": "The paper introduces a neural network-based method for solving eigenvalue problems, leveraging the Rayleigh quotient and Gram-Schmidt orthogonalization for robustness and simplicity.", "motivation": "Eigenvalue problems are common in engineering but understudied in physics-informed machine learning, especially with neural network discretizations.", "method": "Uses a neural network to discretize eigenfunctions, combining the Rayleigh quotient and Gram-Schmidt orthogonalization to solve eigenvalue problems.", "result": "Demonstrates effectiveness for harmonic functions on irregular domains, parametric/nonlinear eigenproblems, and high-dimensional eigenanalysis.", "conclusion": "The approach offers unique advantages for continuous eigenvalue problems in engineering mechanics."}}
{"id": "2506.04826", "pdf": "https://arxiv.org/pdf/2506.04826", "abs": "https://arxiv.org/abs/2506.04826", "authors": ["Konstantinos Giotis", "Dimitrios Stefas", "Yanis Agha", "Hans H\u00f6ft", "Xavier Duten", "Panagiotis Svarnas", "Guillaume Lombardi", "Kristaq Gazeli"], "title": "Discharge dynamics in a cylindrical SDBD prototype reactor under ns-pulsed and sinusoidal AC operation", "categories": ["physics.plasm-ph"], "comment": null, "summary": "We developed a prototype reactor generating surface dielectric barrier\ndischarges (SDBDs) in ambient air, designed for consistent operation while\npreventing constructive material degradation. It features detachable stainless\nsteel electrodes and quartz dielectric to ensure precise fabrication. The\ngrounded electrode is fully immersed into transformer oil drastically\nsuppressing undesired parasitic discharges. The device efficiently sustains\nns-pulsed and AC discharges at 10 kHz, enabling fundamental studies of their\nelectrical characteristics (applied voltage, induced current, electric power)\nand spatiotemporal dynamics (morphology, propagation length and velocity). The\nelectric power (P) consumed exhibits a dissimilar non-linear increase with the\nrising peak voltage (Vp) in each case: P$\\approx$0.8-2.5 W for ns-pulsed\n(Vp=7-9 kV) and P$\\approx$0.9-5.3 W (Vp=7-10 kV) for AC operation. Using ICCD\nimaging, distinct ionization channels are recorded in the rising part of the\npulsed voltage being detached from the driven electrode; during the voltage\ndecrease, a glow-like discharge is formed remaining anchored on the driven\nelectrode. The rising part of the AC voltage is characterized by erratic,\nelongated ionization channels in a filamentary form, the voltage drop featuring\na glow-like behavior. During the rising and falling parts of the AC voltage,\nthe discharge reaches maximum propagation lengths (Lmax) of $\\approx$12 mm and\n$\\approx$7 mm, respectively, while remaining attached to the driven electrode.\nThe corresponding maximum discharge velocities (vmax) are about 5x10 2 m/s and\n3x10 2 m/s. For the ns-pulsed operation, Lmax$\\approx$5 mm (vmax$\\approx$5x10 5\nm/s) and Lmax$\\approx$3.5 mm (vmax$\\approx$1.5x10 5 m/s) during the rising and\nfalling parts of the voltage pulse, respectively. The SDBD dynamics generated\nwith a ns-pulsed voltage is more reproducible than for the AC case allowing for\nthe use of a 500 times smaller ICCD gate width (2 ns) and a more accurate\ndescription of the discharge's spatiotemporal development. This reactor is\nsuitable for performing fundamental studies and understanding key SDBD features\nfor various applications such as flow control, biomedicine and agriculture.", "AI": {"tldr": "A prototype reactor for surface dielectric barrier discharges (SDBDs) in air is developed, featuring detachable electrodes and quartz dielectric. It efficiently sustains ns-pulsed and AC discharges, enabling detailed study of electrical and spatiotemporal dynamics. The reactor shows distinct discharge behaviors and is suitable for fundamental research and applications like flow control and biomedicine.", "motivation": "To design a reliable SDBD reactor for consistent operation and material durability, enabling detailed study of discharge characteristics and dynamics for various applications.", "method": "The reactor uses detachable stainless steel electrodes and quartz dielectric, with the grounded electrode immersed in transformer oil to suppress parasitic discharges. It operates with ns-pulsed and AC discharges at 10 kHz, analyzing electrical parameters and discharge dynamics via ICCD imaging.", "result": "The reactor shows non-linear power consumption with voltage, distinct discharge behaviors (ionization channels and glow-like discharges), and varying propagation lengths/velocities for ns-pulsed and AC operations. Ns-pulsed discharges are more reproducible.", "conclusion": "The reactor is effective for fundamental SDBD studies and applications, with ns-pulsed operation offering better reproducibility and detailed spatiotemporal analysis."}}
{"id": "2506.05293", "pdf": "https://arxiv.org/pdf/2506.05293", "abs": "https://arxiv.org/abs/2506.05293", "authors": ["Mario Christopher Bedrunka", "Dirk Reith", "Holger Foysi", "\u0141ukasz \u0141aniewski-Wo\u0142\u0142k", "Travis Mitchell"], "title": "Reduction of Outflow Boundary Influence on Aerodynamic Performance using Neural Networks", "categories": ["physics.comp-ph", "physics.flu-dyn"], "comment": null, "summary": "The accurate treatment of outflow boundary conditions remains a critical\nchallenge in computational fluid dynamics when predicting aerodynamic forces\nand/or acoustic emissions. This is particularly evident when employing the\nlattice Boltzmann method (LBM) as the numerical solution technique, which often\nsuffers from inaccuracies induced by artificial reflections from outflow\nboundaries. This paper investigates the use of neural networks (NN) to mitigate\nthese adverse boundary effects and enable truncated domain requirements. Two\ndistinct NN-based approaches are proposed: (1) direct reconstruction of unknown\nparticle distribution functions at the outflow boundary; and (2) enhancement of\nestablished characteristic boundary conditions (CBC) by dynamically tuning\ntheir parameters. The direct reconstruction model was trained on data generated\nfrom a 2D flow over a cylindrical obstruction. The drag, lift, and Strouhal\nnumber were used to test the new boundary condition. We analyzed results for\nvarious Reynolds numbers and restricted domain sizes where it demonstrated\nsignificantly improved predictions when compared with the traditional Zou & He\nboundary condition. To examine the robustness of the NN-based reconstruction,\nthe same condition was applied to the simulation of a NACA0012 airfoil, again\nproviding accurate aerodynamic performance predictions. The neural-enhanced CBC\nwere evaluated on a 2D convected vortex benchmark and showed superior\nperformance in minimizing density errors compared to CBCs with fixed\nparameters. These findings highlight the potential of NN-integrated boundary\nconditions to improve accuracy and reduce computational expense of aerodynamic\nand acoustic emissions simulations with the LBM.", "AI": {"tldr": "Neural networks (NN) are used to improve outflow boundary conditions in LBM, reducing artificial reflections and computational costs. Two NN-based methods show superior accuracy in aerodynamic predictions.", "motivation": "Accurate outflow boundary conditions are critical in CFD for predicting aerodynamic forces and acoustic emissions, especially in LBM, which suffers from artificial reflections.", "method": "Two NN-based approaches: (1) direct reconstruction of particle distribution functions at outflow boundaries, and (2) dynamic tuning of characteristic boundary conditions (CBC) parameters.", "result": "NN-based methods improved predictions for drag, lift, and Strouhal numbers in 2D flow and NACA0012 airfoil simulations. Neural-enhanced CBC minimized density errors in vortex benchmarks.", "conclusion": "NN-integrated boundary conditions enhance accuracy and reduce computational costs in LBM simulations for aerodynamics and acoustics."}}
{"id": "2506.04416", "pdf": "https://arxiv.org/pdf/2506.04416", "abs": "https://arxiv.org/abs/2506.04416", "authors": ["Ziyao Xu", "Yong-Tao Zhang"], "title": "Exponential Time Differencing Runge-Kutta Discontinuous Galerkin (ETD-RKDG) Methods for Nonlinear Degenerate Parabolic Equations", "categories": ["math.NA", "cs.NA"], "comment": "34 pages", "summary": "In this paper, we study high-order exponential time differencing Runge-Kutta\n(ETD-RK) discontinuous Galerkin (DG) methods for nonlinear degenerate parabolic\nequations. This class of equations exhibits hyperbolic behavior in degenerate\nregions and parabolic behavior in non-degenerate regions, resulting in sharp\nwave fronts in the solution profiles and a parabolic-type time-step\nrestriction, $\\tau \\sim O(h^2)$, for explicit time integration. To address\nthese challenges and solve such equations in complex domains, we employ DG\nmethods with appropriate stabilizing limiters on unstructured meshes to capture\nthe wave fronts and use ETD-RK methods for time integration to resolve the\nstiffness of parabolic terms. We extract the system's stiffness using the\nJacobian matrix of the DG discretization for diffusion terms and adopt a nodal\nformulation to facilitate its computation. The algorithm is described in detail\nfor two-dimensional triangular meshes. We also conduct a linear stability\nanalysis in one spatial dimension and present computational results on\nthree-dimensional simplex meshes, demonstrating significant improvements in\nstability and large time-step sizes.", "AI": {"tldr": "High-order ETD-RK DG methods are proposed for nonlinear degenerate parabolic equations, addressing stiffness and wave-front capture with stabilizing limiters and nodal formulations.", "motivation": "To solve nonlinear degenerate parabolic equations with mixed hyperbolic-parabolic behavior, which pose challenges like sharp wave fronts and stiff parabolic terms.", "method": "Uses DG methods with stabilizing limiters on unstructured meshes and ETD-RK for time integration, leveraging the Jacobian matrix for stiffness extraction.", "result": "Demonstrates improved stability and allows larger time-step sizes, validated through linear stability analysis and 3D computational results.", "conclusion": "The proposed method effectively handles the stiffness and wave-front challenges, offering robust solutions for complex domains."}}
{"id": "2506.04827", "pdf": "https://arxiv.org/pdf/2506.04827", "abs": "https://arxiv.org/abs/2506.04827", "authors": ["Lorenzo Martelli", "Igor Andriyash", "Jonathan Wheeler", "Henri Kraft", "Xuan Quyen Dinh", "C\u00e9dric Thaury"], "title": "Empirical scaling laws for self-focused laser pulses in nitrogen plasmas", "categories": ["physics.plasm-ph"], "comment": "6 figures, 9 pages", "summary": "We investigate the interaction between a superintense laser pulse and a\nnitrogen plasma with densities exceeding $10^{19}\\,$cm$^{-3}$, using\nparticle-in-cell simulations. Such configurations have recently demonstrated\nthe capability to produce highly charged electron beams (i.e., $>10\\,$nC) with\n$1\\,$J-class lasers, a significant step toward high-average-current\nlaser-plasma accelerators. Our study focuses on analyzing the impact of laser\nself-focusing on laser dynamics, leading to scaling laws that characterize beam\ndiffraction, wakefield amplitude and plasma structures, providing important\ninsights of this interaction regime.", "AI": {"tldr": "Study of laser-plasma interaction in high-density nitrogen plasma using simulations, revealing insights into laser self-focusing and beam dynamics.", "motivation": "To understand how superintense laser pulses interact with high-density nitrogen plasma, aiming to improve laser-plasma accelerators.", "method": "Particle-in-cell simulations to analyze laser self-focusing and its effects on beam diffraction, wakefield amplitude, and plasma structures.", "result": "Identified scaling laws for beam diffraction and wakefield amplitude, providing key insights into the interaction regime.", "conclusion": "The findings advance the understanding of laser-plasma interactions, aiding the development of high-average-current accelerators."}}
{"id": "2506.04491", "pdf": "https://arxiv.org/pdf/2506.04491", "abs": "https://arxiv.org/abs/2506.04491", "authors": ["IceCube Collaboration"], "title": "GollumFit: An IceCube Open-Source Framework for Binned-Likelihood Neutrino Telescope Analyses", "categories": ["hep-ex", "astro-ph.HE", "physics.comp-ph", "physics.data-an"], "comment": null, "summary": "We present GollumFit, a framework designed for performing binned-likelihood\nanalyses on neutrino telescope data. GollumFit incorporates model parameters\ncommon to any neutrino telescope and also model parameters specific to the\nIceCube Neutrino Observatory. We provide a high-level overview of its key\nfeatures and how the code is organized. We then discuss the performance of the\nfitting in a typical analysis scenario, highlighting the ability to fit over\ntens of nuisance parameters. We present some examples showing how to use the\npackage for likelihood minimization tasks. This framework uniquely incorporates\nthe particular model parameters necessary for neutrino telescopes, and solves\nan associated likelihood problem in a time-efficient manner.", "AI": {"tldr": "GollumFit is a framework for binned-likelihood analyses in neutrino telescope data, tailored for IceCube, with efficient handling of nuisance parameters.", "motivation": "To address the need for a specialized framework for neutrino telescope data analysis, particularly for IceCube, incorporating both general and specific model parameters.", "method": "The framework organizes code for likelihood minimization, efficiently fitting over tens of nuisance parameters in typical analysis scenarios.", "result": "GollumFit successfully performs likelihood minimization tasks, demonstrating time-efficient solutions for neutrino telescope data.", "conclusion": "GollumFit is a valuable tool for neutrino telescope analyses, offering tailored features and efficient performance."}}
{"id": "2506.04451", "pdf": "https://arxiv.org/pdf/2506.04451", "abs": "https://arxiv.org/abs/2506.04451", "authors": ["Santolo Leveque", "Yunhui He", "Maxim Olshanskii"], "title": "An Augmented Lagrangian Preconditioner for Navier--Stokes Equations with Runge--Kutta in Time", "categories": ["math.NA", "cs.NA"], "comment": null, "summary": "We consider a Runge--Kutta method for the numerical time integration of the\nnonstationary incompressible Navier--Stokes equations. This yields a sequence\nof nonlinear problems to be solved for the stages of the Runge--Kutta method.\nThe resulting nonlinear system of differential equations is discretized using a\nfinite element method. To compute a numerical approximation of the stages at\neach time step, we employ Newton's method, which requires the solution of a\nlarge and sparse generalized saddle-point problem at each nonlinear iteration.\nWe devise an augmented Lagrangian preconditioner within the flexible GMRES\nmethod for solving the Newton systems at each time step. The preconditioner can\nbe applied inexactly with the help of a multigrid routine. We present numerical\nevidence of the robustness and efficiency of the proposed strategy for\ndifferent values of the viscosity, mesh size, time step, and number of stages\nof the Runge--Kutta method.", "AI": {"tldr": "A Runge-Kutta method is used for time integration of the incompressible Navier-Stokes equations, with nonlinear problems solved via Newton's method and a preconditioned GMRES approach.", "motivation": "To efficiently solve the nonlinear systems arising from the Runge-Kutta time integration of the Navier-Stokes equations.", "method": "Finite element discretization, Newton's method for nonlinear systems, and an augmented Lagrangian preconditioner with flexible GMRES and multigrid support.", "result": "Numerical results demonstrate robustness and efficiency across varying parameters like viscosity, mesh size, and time step.", "conclusion": "The proposed preconditioned GMRES strategy is effective for solving the nonlinear systems in this context."}}
{"id": "2506.05124", "pdf": "https://arxiv.org/pdf/2506.05124", "abs": "https://arxiv.org/abs/2506.05124", "authors": ["Florens Grimm", "Jan-Luca Gembus", "Jana Sch\u00f6ne", "Peter Awakowicz", "Lars Sch\u00fccke", "Andrew R. Gibson"], "title": "Electron and gas temperature-driven chemistry during microdischarges formed in water vapour bubbles", "categories": ["physics.plasm-ph"], "comment": null, "summary": "Microdischarges formed in bubbles immersed in liquids are of interest for\nmaterials synthesis and chemical conversion applications in the frame of\nplasma-driven electrochemistry. A key challenge associated with controlling\nsuch processes is the limited understanding of the gas-phase chemical kinetics\nin these microdischarges. Due to their large electron densities, and high gas\ntemperatures, both electron and gas temperature driven chemistry are likely to\nbe important. Here, a 0-D modelling approach, informed by experimental\nmeasurements, is used to study the chemical kinetics in these systems. A new\nreaction scheme is developed for microdischarges in water vapour, including\nreactions for both high electron density, and high gas temperature regimes.\nMicrodischarges formed during plasma electrolytic oxidation are used as a test\ncase, however, the key results are expected to be transferable to other plasma\nelectrolysis systems with similar properties. Experimentally measured power\ndensities are used as input to the 0-D model, together with estimates of\ntemperatures and gas pressures within the gas bubble. Comparison of measured\nand simulated electron densities shows good agreement, given the limitations of\nboth model and experiment. In the base case microdischarge, H$_{2}$O is found\nto be highly dissociated during the period of peak power density, with H and O\nmaking up the majority of the neutral gas in the bubble. The maximum ionization\ndegree is around 0.31$\\,\\%$, and the electronegativity during the period of\npeak electron density is found to be low. Species formation and reaction\npathways are analysed under variation of the neutral gas temperature from\n2000$\\,$K to 6000$\\,$K. At all temperatures, electron, ion, and neutral\nreactions with high threshold energies are found to be important for the\noverall chemical kinetics.", "AI": {"tldr": "The paper studies gas-phase chemical kinetics in microdischarges within bubbles in liquids, using a 0-D model informed by experiments, focusing on high electron density and gas temperature regimes.", "motivation": "Understanding gas-phase chemical kinetics in microdischarges is crucial for controlling plasma-driven electrochemistry in materials synthesis and chemical conversion.", "method": "A 0-D modeling approach, informed by experimental measurements, is used to develop a new reaction scheme for microdischarges in water vapor, analyzing species formation and pathways under varying temperatures.", "result": "H2O is highly dissociated during peak power density, with H and O dominating the neutral gas. The maximum ionization degree is ~0.31%, and electronegativity is low. High-threshold energy reactions are key across temperatures.", "conclusion": "The findings are transferable to similar plasma electrolysis systems, highlighting the importance of high-threshold energy reactions in microdischarge chemical kinetics."}}
{"id": "2506.04523", "pdf": "https://arxiv.org/pdf/2506.04523", "abs": "https://arxiv.org/abs/2506.04523", "authors": ["Cliff B. Abbott", "Mark Elo", "Dmytro A. Bozhko"], "title": "Perturbative Gradient Training: A novel training paradigm for bridging the gap between deep neural networks and physical reservoir computing", "categories": ["cs.LG", "cond-mat.mes-hall", "cs.ET", "cs.NE", "physics.comp-ph"], "comment": "7 pages, 8 figures, submitted to IEEE Transactions on Neural Netowrks\n  and Learning Systems", "summary": "We introduce Perturbative Gradient Training (PGT), a novel training paradigm\nthat overcomes a critical limitation of physical reservoir computing: the\ninability to perform backpropagation due to the black-box nature of physical\nreservoirs. Drawing inspiration from perturbation theory in physics, PGT uses\nrandom perturbations in the network's parameter space to approximate gradient\nupdates using only forward passes. We demonstrate the feasibility of this\napproach on both simulated neural network architectures, including a dense\nnetwork and a transformer model with a reservoir layer, and on experimental\nhardware using a magnonic auto-oscillation ring as the physical reservoir. Our\nresults show that PGT can achieve performance comparable to that of standard\nbackpropagation methods in cases where backpropagation is impractical or\nimpossible. PGT represents a promising step toward integrating physical\nreservoirs into deeper neural network architectures and achieving significant\nenergy efficiency gains in AI training.", "AI": {"tldr": "PGT is a new training method for physical reservoir computing that approximates gradients via random perturbations, enabling training without backpropagation.", "motivation": "Overcome the limitation of physical reservoir computing's inability to perform backpropagation due to its black-box nature.", "method": "Uses random perturbations in parameter space to approximate gradient updates, tested on simulated and experimental hardware.", "result": "PGT achieves performance comparable to backpropagation where the latter is impractical.", "conclusion": "PGT enables deeper integration of physical reservoirs into neural networks, promising energy efficiency gains."}}
{"id": "2506.04710", "pdf": "https://arxiv.org/pdf/2506.04710", "abs": "https://arxiv.org/abs/2506.04710", "authors": ["Lucas \u00c5kerstedt", "Harald Hultin", "B. L. G. Jonsson"], "title": "An Array Decomposition Method for Finite Arrays with Electrically Connected Elements for fast Toeplitz Solvers", "categories": ["math.NA", "cs.NA", "eess.SP"], "comment": "12 pages, 17 figures", "summary": "A large part of the geometry of array antennas is often partially defined by\nfinite translational symmetries. Applying the method of moments (MoM) with the\nRWG-like element on an appropriately structured mesh to these arrays results in\nan impedance matrix where the main part exhibits a multilevel block Toeplitz\nstructure. This article introduces a memory-efficient construction method that\neffectively represents and reuses impedance calculations. The proposed method,\napplicable to electrically connected elements, also accounts for all\nnon-symmetric parts of the array. The core idea involves nine distinct\nelectrically connectable components from which the array can be assembled. The\nderived multilevel block Toeplitz matrix is further utilized by an in-house\ninverse solver to achieve faster and more memory-efficient MoM current vector\ncalculations. We demonstrate the method by computing the far-field of a 32x32\narray and the scattering parameters of two tightly coupled 9x9 arrays. This\napproach reduces the memory allocation from $\\mathcal{O}(N_x^2 N_y^2)$ to\n$\\mathcal{O}(N_x N_y)$, for an $N_x \\times N_y$ array.", "AI": {"tldr": "A memory-efficient method for constructing impedance matrices in array antennas using multilevel block Toeplitz structure, reducing memory usage from O(N_x^2 N_y^2) to O(N_x N_y).", "motivation": "To address the inefficiency in memory usage and computation for array antennas with finite translational symmetries.", "method": "Uses MoM with RWG-like elements on structured meshes, introduces nine connectable components, and leverages multilevel block Toeplitz matrices.", "result": "Demonstrated by computing far-fields for a 32x32 array and scattering parameters for two 9x9 arrays, showing significant memory reduction.", "conclusion": "The method efficiently handles non-symmetric parts of arrays and improves computational performance for MoM current vector calculations."}}
{"id": "2506.05170", "pdf": "https://arxiv.org/pdf/2506.05170", "abs": "https://arxiv.org/abs/2506.05170", "authors": ["Dario Panici", "Eduardo Rodriguez", "Rory Conlin", "Daniel Dudt", "Egemen Kolemen"], "title": "Extending near-axis equilibria in DESC", "categories": ["physics.plasm-ph"], "comment": "30 pages, 7 figures", "summary": "The near-axis description of optimised stellarator fields has proven to be a\npowerful tool both for design and understanding of this magnetic confinement\nconcept. The description consists of an asymptotic model of the equilibrium in\nthe distance from its centermost axis, and is thus only approximate. Any\npractical application therefore requires the eventual construction of a global\nequilibrium. This paper presents a novel way of constructing global equilibria\nusing the \\texttt{DESC} code that guarantees the correct asymptotic behaviour\nimposed by a given near-axis construction. The theoretical underpinnings of\nthis construction are carefully presented, and benchmarking examples provided.\nThis opens the door to an efficient coupling of the near-axis framework and\nthat of global equilibria for future optimisation efforts.", "AI": {"tldr": "A novel method for constructing global stellarator equilibria using the DESC code, ensuring correct asymptotic behavior from near-axis models, is presented and benchmarked.", "motivation": "To bridge the gap between approximate near-axis models and practical global equilibrium requirements for stellarator design and optimization.", "method": "Uses the DESC code to construct global equilibria with guaranteed asymptotic behavior derived from near-axis models. Theoretical foundations and benchmarking are detailed.", "result": "Successful construction of global equilibria that align with near-axis predictions, enabling efficient coupling for future optimization.", "conclusion": "The method facilitates improved stellarator design by integrating near-axis and global equilibrium frameworks."}}
{"id": "2506.04781", "pdf": "https://arxiv.org/pdf/2506.04781", "abs": "https://arxiv.org/abs/2506.04781", "authors": ["Christoph Schirninger", "Robert Jarolim", "Astrid M. Veronig", "Christoph Kuckein"], "title": "Deep learning image burst stacking to reconstruct high-resolution ground-based solar observations", "categories": ["astro-ph.SR", "astro-ph.IM", "cs.CV", "physics.comp-ph"], "comment": null, "summary": "Large aperture ground based solar telescopes allow the solar atmosphere to be\nresolved in unprecedented detail. However, observations are limited by Earths\nturbulent atmosphere, requiring post image corrections. Current reconstruction\nmethods using short exposure bursts face challenges with strong turbulence and\nhigh computational costs. We introduce a deep learning approach that\nreconstructs 100 short exposure images into one high quality image in real\ntime. Using unpaired image to image translation, our model is trained on\ndegraded bursts with speckle reconstructions as references, improving\nrobustness and generalization. Our method shows an improved robustness in terms\nof perceptual quality, especially when speckle reconstructions show artifacts.\nAn evaluation with a varying number of images per burst demonstrates that our\nmethod makes efficient use of the combined image information and achieves the\nbest reconstructions when provided with the full image burst.", "AI": {"tldr": "A deep learning method reconstructs 100 short exposure solar images into one high-quality image in real-time, outperforming current methods in robustness and perceptual quality.", "motivation": "Ground-based solar telescopes face limitations due to Earth's turbulent atmosphere, requiring post-image corrections. Current methods struggle with strong turbulence and high computational costs.", "method": "The approach uses unpaired image-to-image translation, training on degraded bursts with speckle reconstructions as references to improve robustness and generalization.", "result": "The method shows enhanced perceptual quality, especially when speckle reconstructions have artifacts, and performs best with full image bursts.", "conclusion": "The deep learning approach efficiently combines image information, offering real-time, high-quality reconstructions for solar observations."}}
{"id": "2506.04732", "pdf": "https://arxiv.org/pdf/2506.04732", "abs": "https://arxiv.org/abs/2506.04732", "authors": ["Nicola Cavallini", "Gianmarco Manzini", "Daniele Funaro", "Andrea Favalli"], "title": "A Fast, Accurate and Oscillation-free Spectral Collocation Solver for High-dimensional Transport Problems", "categories": ["math.NA", "cs.NA"], "comment": null, "summary": "Transport phenomena-describing the movement of particles, energy, or other\nphysical quantities-are fundamental in various scientific disciplines,\nincluding nuclear physics, plasma physics, astrophysics, engineering, and the\nnatural sciences.\n  However, solving the associated seven-dimensional transport equations poses a\nsignificant computational challenge due to the curse of dimensionality.\n  We introduce the Tensor Train Superconsistent Spectral (T${^2}$S${^2}$)\nsolver to address this challenge, integrating Spectral Collocation for\nexponential convergence, Superconsistency for stabilization in\ntransport-dominated regimes, and Tensor Train format for substantial data\ncompression. T${^2}$S${^2}$ enforces a dimension-wise superconsistent condition\ncompatible with tensor structures, achieving extremely low compression ratios,\nin the order of $(10^{-12})$, while preserving spectral accuracy. Numerical\nexperiments on linear problems demonstrate that T${^2}$S${^2}$ can solve\nhigh-dimensional transport problems in minutes on standard hardware, making\npreviously intractable problems computationally feasible. This advancement\nopens new avenues for efficiently and accurately modeling complex transport\nphenomena.", "AI": {"tldr": "The paper introduces the T\u00b2S\u00b2 solver to efficiently solve high-dimensional transport equations, combining Spectral Collocation, Superconsistency, and Tensor Train format for accuracy and compression.", "motivation": "Transport phenomena are critical but solving their high-dimensional equations is computationally challenging due to dimensionality.", "method": "The T\u00b2S\u00b2 solver integrates Spectral Collocation, Superconsistency, and Tensor Train format for exponential convergence, stabilization, and data compression.", "result": "Achieves extremely low compression ratios (~10\u207b\u00b9\u00b2) and solves high-dimensional problems in minutes on standard hardware.", "conclusion": "T\u00b2S\u00b2 enables efficient and accurate modeling of complex transport phenomena, making previously intractable problems feasible."}}
{"id": "2506.04809", "pdf": "https://arxiv.org/pdf/2506.04809", "abs": "https://arxiv.org/abs/2506.04809", "authors": ["Michael J. Carley"], "title": "Numerical solution of the wave equation outside a sphere", "categories": ["math.NA", "cs.NA", "math.AP", "physics.class-ph", "physics.comp-ph"], "comment": null, "summary": "A method is presented for the fast evaluation of the transient acoustic field\ngenerated outside a spherical surface by sources inside the surface. The method\nemploys Lebedev quadratures, which are the optimal method for spatial\nintegration, and Lagrange interpolation and differentiation in an advanced time\nalgorithm for the evaluation of the transient field. Numerical testing\ndemonstrates that the approach gives near machine-precision accuracy and a\nspeed-up in evaluation time which depends on the order of quadrature rule\nemployed but breaks even with direct evaluation at a number of field points\nabout 1.15 times the number of surface quadrature nodes.", "AI": {"tldr": "A method for fast evaluation of transient acoustic fields outside a spherical surface using Lebedev quadratures and Lagrange interpolation, achieving near machine-precision accuracy and speed-up.", "motivation": "To efficiently and accurately evaluate transient acoustic fields generated by internal sources outside a spherical surface.", "method": "Uses Lebedev quadratures for spatial integration and Lagrange interpolation/differentiation in an advanced time algorithm.", "result": "Near machine-precision accuracy; speed-up depends on quadrature order, breaking even at ~1.15 times the number of surface nodes.", "conclusion": "The method is efficient and accurate for transient acoustic field evaluation."}}
{"id": "2506.04791", "pdf": "https://arxiv.org/pdf/2506.04791", "abs": "https://arxiv.org/abs/2506.04791", "authors": ["Athanasios C. Antoulas", "Ion Victor Gosea", "Charles Poussot-Vassal", "Pierre Vuillemin"], "title": "Tensor-based multivariate function approximation: methods benchmarking and comparison", "categories": ["math.NA", "cs.CE", "cs.NA", "cs.SE", "93A15, 93A30, 93B11, 93B15, 93C05, 93C80"], "comment": "Report with a collection of examples, aimed at being regularly\n  updated. Associated GIT: https://github.com/cpoussot/mLF", "summary": "In this note, we evaluate the performances, the features and the\nuser-experience of some methods (and their implementations) designed for\ntensor- (or data-) based multivariate function construction and approximation.\nTo this aim, a collection of multivariate functions extracted from contributive\nworks coming from different communities, is suggested. First, these functions\nwith varying complexity (e.g. number and degree of the variables) and nature\n(e.g. rational, irrational, differentiable or not, symmetric, etc.) are used to\nconstruct tensors, each of different dimension and size on the disk. Second,\ngrounded on this tensor, we inspect performances of each considered method\n(e.g. the accuracy, the computational time, the parameters tuning impact,\netc.). Finally, considering the \"best\" parameter tuning set, we compare each\nmethod using multiple evaluation criteria. The purpose of this note is not to\nrank the methods but rather to evaluate as fairly as possible the different\navailable strategies, with the idea in mind to guide users to understand the\nprocess, the possibilities, the advantages and the limits brought by each\ntools. The contribution claimed is to suggest a complete benchmark collection\nof some available tools for tensor approximation by surrogate models (e.g.\nrational functions, networks, etc.). In addition, as contributors of the\nmultivariate Loewner Framework (mLF) approach (and its side implementation in\nMDSPACK), attention and details of the latter are more explicitly given, in\norder to provide readers a digest of this contributive work and some details\nwith simple examples.", "AI": {"tldr": "The paper evaluates various tensor-based multivariate function approximation methods, comparing their performance, features, and user experience using a benchmark collection of functions.", "motivation": "To fairly assess and compare different tensor approximation tools, guiding users in understanding their processes, advantages, and limitations.", "method": "Construct tensors from a diverse set of multivariate functions, evaluate method performances (accuracy, computational time, parameter tuning), and compare methods using multiple criteria.", "result": "A benchmark collection is provided, and the multivariate Loewner Framework (mLF) is highlighted with detailed examples.", "conclusion": "The note aims to guide users in selecting tensor approximation tools without ranking them, emphasizing understanding and practical insights."}}
{"id": "2506.04898", "pdf": "https://arxiv.org/pdf/2506.04898", "abs": "https://arxiv.org/abs/2506.04898", "authors": ["Xintong Zou", "Zhijie Li", "Yunpeng Wang", "Huiyu Yang", "Jianchun Wang"], "title": "Uncertainty quantification and stability of neural operators for prediction of three-dimensional turbulence", "categories": ["physics.flu-dyn", "physics.comp-ph"], "comment": null, "summary": "Turbulence poses challenges for numerical simulation due to its chaotic,\nmultiscale nature and high computational cost. Traditional turbulence modeling\noften struggles with accuracy and long-term stability. Recent scientific\nmachine learning (SciML) models, such as Fourier Neural Operators (FNO), show\npromise in solving PDEs, but are typically limited to one-step-ahead\npredictions and often fail over long time horizons, especially in 3D\nturbulence. This study proposes a framework to assess the reliability of neural\noperator models in turbulent flows. Using three-dimensional forced homogeneous\nisotropic turbulence (HIT) as a benchmark, we evaluate models in terms of\nuncertainty quantification (UQ), error propagation, and sensitivity to initial\nperturbations. Statistical tools such as error distribution analysis and\nautocorrelation functions (ACF) are used to assess predictive robustness and\ntemporal coherence. Our proposed model, the factorized-implicit FNO (F-IFNO),\nimproves long-term stability and accuracy by incorporating implicit\nfactorization into the prediction process. It outperforms conventional LES and\nother FNO-based models in balancing accuracy, stability, and efficiency. The\nresults highlight the importance of prediction constraints, time interval\nselection, and UQ in developing robust neural operator frameworks for turbulent\nsystems.", "AI": {"tldr": "The study evaluates neural operator models for 3D turbulence, proposing the F-IFNO model for improved stability and accuracy.", "motivation": "Traditional turbulence modeling lacks accuracy and stability, while existing SciML models like FNO struggle with long-term predictions in 3D turbulence.", "method": "The framework assesses neural operators using 3D HIT, focusing on UQ, error propagation, and sensitivity. The F-IFNO model incorporates implicit factorization.", "result": "F-IFNO outperforms conventional LES and other FNO models in accuracy, stability, and efficiency.", "conclusion": "Prediction constraints, time interval selection, and UQ are crucial for robust neural operator frameworks in turbulence."}}
{"id": "2506.05031", "pdf": "https://arxiv.org/pdf/2506.05031", "abs": "https://arxiv.org/abs/2506.05031", "authors": ["Mohammad Mirzakhani", "Kyungsun Moon"], "title": "Quantum simulation of the Hubbard model on a graphene hexagon: Strengths of IQPE and noise constraints", "categories": ["quant-ph", "cond-mat.mes-hall", "cond-mat.str-el", "physics.comp-ph"], "comment": "14 pages, 10 figures", "summary": "Quantum computing offers transformative potential for simulating real-world\nmaterials, providing a powerful platform to investigate complex quantum systems\nacross quantum chemistry and condensed matter physics. In this work, we\nleverage this capability to simulate the Hubbard model on a six-site graphene\nhexagon using Qiskit, employing the Iterative Quantum Phase Estimation (IQPE)\nand adiabatic evolution algorithms to determine its ground-state properties.\nNoiseless simulations yield accurate ground-state energies (GSEs), charge and\nspin densities, and correlation functions, all in excellent agreement with\nexact diagonalization, underscoring the precision and reliability of quantum\nsimulation for strongly correlated electron systems. However, deploying IQPE\nand adiabatic evolution on today's noisy quantum hardware remains highly\nchallenging. To examine these limitations, we utilize the Qiskit Aer simulator\nwith a custom noise model tailored to the characteristics of IBM's real\nbackend. This model includes realistic depolarizing gate errors, thermal\nrelaxation, and readout noise, allowing us to explore how these factors degrade\nsimulation accuracy. Preliminary hardware runs on IBM devices further expose\ndiscrepancies between simulated and real-world noise, emphasizing the gap\nbetween ideal and practical implementations. Overall, our results highlight the\npromise of quantum computing for simulating correlated quantum materials, while\nalso revealing the significant challenges posed by hardware noise in achieving\naccurate and reliable physical predictions using current quantum devices.", "AI": {"tldr": "The paper explores quantum simulation of the Hubbard model on graphene using Qiskit, achieving accurate results in noiseless conditions but facing challenges with noise on real hardware.", "motivation": "To demonstrate the potential of quantum computing for simulating strongly correlated quantum systems like the Hubbard model, while addressing the limitations imposed by current noisy hardware.", "method": "Utilized Iterative Quantum Phase Estimation (IQPE) and adiabatic evolution algorithms on Qiskit, with noiseless simulations and a custom noise model for IBM backends.", "result": "Noiseless simulations matched exact diagonalization results, but noise degraded accuracy, revealing discrepancies between simulated and real hardware performance.", "conclusion": "Quantum computing shows promise for material simulation, but hardware noise remains a significant barrier to reliable predictions."}}
{"id": "2506.04840", "pdf": "https://arxiv.org/pdf/2506.04840", "abs": "https://arxiv.org/abs/2506.04840", "authors": ["Maolin Che", "Yimin Wei", "Chong Wu", "Hong Yan"], "title": "Efficient randomized algorithms for the fixed Tucker-rank problem of Tucker decomposition with adaptive shifts", "categories": ["math.NA", "cs.NA", "65F55, 68W20, 15A18, 15A69"], "comment": "41 pages, 43 figures", "summary": "Randomized numerical linear algebra is proved to bridge theoretical\nadvancements to offer scalable solutions for approximating tensor\ndecomposition. This paper introduces fast randomized algorithms for solving the\nfixed Tucker-rank problem of Tucker decomposition, through the integration of\nadaptive shifted power iterations. The proposed algorithms enhance randomized\nvariants of truncated high-order singular value decomposition (T-HOSVD) and\nsequentially T-HOSVD (ST-HOSVD) by incorporating dynamic shift strategies,\nwhich accelerate convergence by refining the singular value gap and reduce the\nnumber of required power iterations while maintaining accuracy. Theoretical\nanalyses provide probabilistic error bounds, demonstrating that the proposed\nmethods achieve comparable or superior accuracy compared to deterministic\napproaches. Numerical experiments on synthetic and real-world datasets validate\nthe efficiency and robustness of the proposed algorithms, showing a significant\ndecline in runtime and approximation error over state-of-the-art techniques.", "AI": {"tldr": "The paper introduces fast randomized algorithms for Tucker decomposition using adaptive shifted power iterations, improving efficiency and accuracy over existing methods.", "motivation": "To bridge theoretical advancements with scalable solutions for tensor decomposition, addressing the fixed Tucker-rank problem.", "method": "Integration of adaptive shifted power iterations into randomized variants of T-HOSVD and ST-HOSVD, refining singular value gaps to accelerate convergence.", "result": "Theoretical error bounds and numerical experiments show the algorithms outperform state-of-the-art techniques in runtime and approximation error.", "conclusion": "The proposed methods offer efficient, robust, and accurate solutions for Tucker decomposition, validated by synthetic and real-world datasets."}}
{"id": "2506.05105", "pdf": "https://arxiv.org/pdf/2506.05105", "abs": "https://arxiv.org/abs/2506.05105", "authors": ["Fang-Cheng Wang", "Qi-Jun Ye", "Yu-Cheng Zhu", "Xin-Zheng Li"], "title": "Classification and enumeration of solid-solid phase transition mechanisms", "categories": ["cond-mat.mtrl-sci", "physics.comp-ph"], "comment": "22 pages, 14 figures", "summary": "Crystal-structure match (CSM), the atom-to-atom correspondence between two\ncrystalline phases, is used extensively to describe solid-solid phase\ntransition (SSPT) mechanisms. However, existing computational methods cannot\naccount for all possible CSMs. Here, we propose a formalism to classify all\nCSMs into a tree structure, which is independent of the choices of unit cell\nand supercell. We rigorously proved that only a finite number of noncongruent\nCSMs are of practical interest. By representing CSMs as integer matrices, we\nintroduce the crystmatch method to exhaustively enumerate them, which\nuncontroversially solves the CSM optimization problem under any geometric\ncriterion. For most SSPTs, crystmatch can reproduce all known deformation\nmechanisms and CSMs within 10 CPU minutes, while also revealing thousands of\nnew candidates. The resulting database can be further used for comparing\nexperimental phenomena, high-throughput energy barrier calculations, or machine\nlearning.", "AI": {"tldr": "A new method, crystmatch, is proposed to classify and enumerate all possible crystal-structure matches (CSMs) for solid-solid phase transitions (SSPTs), solving the CSM optimization problem efficiently.", "motivation": "Existing methods cannot account for all possible CSMs, limiting the understanding of SSPT mechanisms.", "method": "The formalism classifies CSMs into a tree structure, representing them as integer matrices, and uses crystmatch to exhaustively enumerate them.", "result": "crystmatch reproduces known mechanisms and reveals thousands of new CSM candidates within 10 CPU minutes.", "conclusion": "The method provides a comprehensive database for SSPT analysis, enabling applications like experimental comparison and machine learning."}}
{"id": "2506.04857", "pdf": "https://arxiv.org/pdf/2506.04857", "abs": "https://arxiv.org/abs/2506.04857", "authors": ["Junming Duan", "Praveen Chandrashekar", "Christian Klingenberg"], "title": "Active flux for ideal magnetohydrodynamics: A positivity-preserving scheme with the Godunov-Powell source term", "categories": ["math.NA", "cs.NA"], "comment": "27 pages, 12 figures", "summary": "The Active Flux (AF) is a compact, high-order finite volume scheme that\nallows more flexibility by introducing additional point value degrees of\nfreedom at cell interfaces. This paper proposes a positivity-preserving (PP) AF\nscheme for solving the ideal magnetohydrodynamics, where the Godunov-Powell\nsource term is employed to deal with the divergence-free constraint. For the\nevolution of the cell average, apart from the standard conservative finite\nvolume method for the flux derivative, the nonconservative source term is built\non the quadratic reconstruction in each cell, which maintains the compact\nstencil in the AF scheme. For the point value update, the local Lax-Friedrichs\n(LLF) flux vector splitting is adopted for the flux derivative, originally\nproposed in [Duan, Barsukow, and Klingenberg, SIAM Journal on Scientific\nComputing, 47(2), A811--A837, 2025], and a central difference is used to\ndiscretize the divergence in the source term. A parametrized flux limiter and a\nscaling limiter are presented to preserve the density and pressure positivity\nby blending the AF scheme with the first-order PP LLF scheme with the source\nterm. To suppress oscillations, a new shock sensor considering the divergence\nerror is proposed, which is used to compute the blending coefficients for the\ncell average. Several numerical tests are conducted to verify the third-order\naccuracy, PP property, and shock-capturing ability of the scheme. The key role\nof the Godunov-Powell source term and its suitable discretization in\ncontrolling divergence error is also validated.", "AI": {"tldr": "A positivity-preserving Active Flux scheme for ideal magnetohydrodynamics is proposed, incorporating the Godunov-Powell source term for divergence-free constraints and using limiters to ensure density and pressure positivity.", "motivation": "To develop a high-order, compact finite volume scheme for ideal magnetohydrodynamics that maintains positivity of density and pressure while handling divergence-free constraints effectively.", "method": "The scheme combines the Active Flux method with the Godunov-Powell source term, uses quadratic reconstruction for nonconservative terms, and employs flux and scaling limiters for positivity. A new shock sensor is introduced to control oscillations.", "result": "Numerical tests confirm third-order accuracy, positivity preservation, and effective shock-capturing. The Godunov-Powell source term's role in controlling divergence error is validated.", "conclusion": "The proposed scheme successfully balances high-order accuracy, compactness, and positivity preservation, demonstrating robustness in handling magnetohydrodynamics problems."}}
{"id": "2506.05122", "pdf": "https://arxiv.org/pdf/2506.05122", "abs": "https://arxiv.org/abs/2506.05122", "authors": ["Shaziya A. Banu", "Venkata R. S. B. Varanasi", "Arash Noshadravan", "Sara Abedi"], "title": "Reactive Transport Simulation of Silicate-Rich Shale Rocks when Exposed to CO2 Saturated Brine Under High Pressure and High Temperature", "categories": ["physics.geo-ph", "physics.comp-ph"], "comment": null, "summary": "This study examines the feasibility of carbon dioxide storage in shale rocks\nand the reliability of reactive transport models in achieving accurate\nreplication of the chemo-mechanical interactions and transport processes\ntranspiring in these rocks when subjected to CO2 saturated brine. Owing to the\nheterogeneity of rocks, experimental testing for adequate deductions and\nfindings, could be an expensive and time-intensive process. Therefore, this\nstudy proposes utilization of reactive transport modeling to replicate the\npore-scale chemo-mechanical reactions and transport processes occurring in\nsilicate-rich shale rocks in the presence of CO2 saturated brine under high\npressure and high temperature. For this study, Crunch Tope has been adopted to\nsimulate a one-dimensional reactive transport model of a Permian rock specimen\nexposed to the acidic brine at a temperature of 100 {\\deg}C and pressure of\n12.40 MPa (1800 psi) for a period of 14 and 28 days. The results demonstrated\nsignificant dissolution followed by precipitation of quartz rich phases,\nprecipitation and swelling of clay rich phases, and dissolution of feldspar\nrich phases closer to the acidic brine-rock interface. Moreover, porosity\nagainst reaction depth curve showed nearly 1.00% mineral precipitation occur at\n14 and 28 days, which is insufficient to completely fill the pore spaces.", "AI": {"tldr": "The study explores CO2 storage in shale rocks using reactive transport modeling to simulate chemo-mechanical interactions, avoiding costly experiments. Results show mineral dissolution and precipitation but insufficient pore-filling.", "motivation": "To address the high cost and time of experimental testing for CO2 storage in heterogeneous shale rocks by using reactive transport models.", "method": "Used Crunch Tope to simulate a 1D reactive transport model of a Permian rock specimen under high pressure (12.40 MPa) and temperature (100\u00b0C) with CO2 saturated brine for 14 and 28 days.", "result": "Observed dissolution of quartz and feldspar, precipitation of quartz-rich phases, and clay swelling. Porosity changes showed only 1.00% mineral precipitation, insufficient to fill pores.", "conclusion": "Reactive transport modeling is feasible for studying CO2 storage in shale, but mineral precipitation alone may not fully seal pores."}}
{"id": "2506.04880", "pdf": "https://arxiv.org/pdf/2506.04880", "abs": "https://arxiv.org/abs/2506.04880", "authors": ["Heiko Gimperlein", "Ruma R. Maity"], "title": "Numerical analysis for constrained and unconstrained Q-tensor energies for liquid crystals", "categories": ["math.NA", "cs.NA", "65N12, 35J47, 65N30, 76A15"], "comment": null, "summary": "This paper introduces a comprehensive finite element approximation framework\nfor three-dimensional Landau-de Gennes $Q$-tensor energies for nematic liquid\ncrystals, with a particular focus on the anisotropy of the elastic energy and\nthe Ball-Majumdar singular potential. This potential imposes essential physical\nconstraints on the eigenvalues of the $Q$-tensor, ensuring realistic modeling.\nWe address the approximation of regular solutions to nonlinear elliptic partial\ndifferential equations with non-homogeneous boundary conditions associated with\nLandau-de Gennes energies. The well-posedness of the discrete linearized\nproblem is rigorously demonstrated. The existence and local uniqueness of the\ndiscrete solution is derived using the Newton-Kantorovich theorem. Furthermore,\nwe demonstrate an optimal order convergence rate in the energy norm and discuss\nthe impact of eigenvalue constraints on the a priori error analysis.", "AI": {"tldr": "A finite element framework for 3D Landau-de Gennes Q-tensor energies, focusing on anisotropy and physical constraints, with rigorous analysis of well-posedness, convergence, and error impact.", "motivation": "To model nematic liquid crystals realistically by addressing anisotropy and physical constraints in the Q-tensor energy framework.", "method": "Finite element approximation for nonlinear elliptic PDEs with non-homogeneous boundary conditions, using the Newton-Kantorovich theorem for discrete solutions.", "result": "Well-posedness of the discrete problem, local uniqueness of solutions, and optimal convergence rates in the energy norm.", "conclusion": "The framework effectively models nematic liquid crystals with rigorous theoretical backing and practical convergence guarantees."}}
{"id": "2506.05292", "pdf": "https://arxiv.org/pdf/2506.05292", "abs": "https://arxiv.org/abs/2506.05292", "authors": ["Declan A. Norton", "Yuanzhao Zhang", "Michelle Girvan"], "title": "Learning Beyond Experience: Generalizing to Unseen State Space with Reservoir Computing", "categories": ["cs.LG", "math.DS", "nlin.CD", "physics.comp-ph"], "comment": "15 pages, 9 figures", "summary": "Machine learning techniques offer an effective approach to modeling dynamical\nsystems solely from observed data. However, without explicit structural priors\n-- built-in assumptions about the underlying dynamics -- these techniques\ntypically struggle to generalize to aspects of the dynamics that are poorly\nrepresented in the training data. Here, we demonstrate that reservoir computing\n-- a simple, efficient, and versatile machine learning framework often used for\ndata-driven modeling of dynamical systems -- can generalize to unexplored\nregions of state space without explicit structural priors. First, we describe a\nmultiple-trajectory training scheme for reservoir computers that supports\ntraining across a collection of disjoint time series, enabling effective use of\navailable training data. Then, applying this training scheme to multistable\ndynamical systems, we show that RCs trained on trajectories from a single basin\nof attraction can achieve out-of-domain generalization by capturing system\nbehavior in entirely unobserved basins.", "AI": {"tldr": "Reservoir computing can generalize to unexplored state space regions without explicit structural priors, using a multiple-trajectory training scheme.", "motivation": "Machine learning struggles to generalize without structural priors, especially for poorly represented dynamics in training data.", "method": "A multiple-trajectory training scheme for reservoir computers is introduced, enabling training across disjoint time series.", "result": "RCs trained on single basin trajectories generalize to unobserved basins in multistable systems.", "conclusion": "Reservoir computing can achieve out-of-domain generalization without explicit structural assumptions."}}
{"id": "2506.04969", "pdf": "https://arxiv.org/pdf/2506.04969", "abs": "https://arxiv.org/abs/2506.04969", "authors": ["Yema Paul", "Emmanuel Delande", "Francois Vinet", "Francois Laporte", "Manuel Sanjurjo-Rivo", "Aldo Tonnini", "Joan-Pau Sanchez"], "title": "Probability of Collision with Tethered Spacecraft", "categories": ["math.NA", "cs.NA"], "comment": "13 pages, 2 figures, Engineering Note", "summary": "This Engineering Note addresses the challenge of estimating the probability\nof collision for tethered spacecraft during close encounters with other space\nobjects. Standard probability of collision methods, based on spherical\nhard-body assumptions, tend to be overly conservative when applied to long\ntether systems. We introduce a method that accounts for the tether's spatial\nextent and configuration uncertainty by maximizing the probability of collision\nover all physically admissible tether shapes. Applied to real-world conjunction\nevents involving a kilometer-scale flexible inextensible tether, the method\nyields more realistic risk estimates. This approach improves the ability to\ndistinguish hazardous from benign encounters, thereby supporting more informed\ncollision avoidance decisions.", "AI": {"tldr": "A method to estimate collision probability for tethered spacecraft, addressing conservatism in standard spherical models by accounting for tether extent and uncertainty.", "motivation": "Standard collision probability methods are overly conservative for tethered spacecraft due to spherical hard-body assumptions.", "method": "Maximizes collision probability over all physically admissible tether shapes, considering spatial extent and configuration uncertainty.", "result": "Applied to real-world events, the method provides more realistic risk estimates for kilometer-scale tethers.", "conclusion": "Improves hazard distinction, aiding better collision avoidance decisions."}}
{"id": "2506.05174", "pdf": "https://arxiv.org/pdf/2506.05174", "abs": "https://arxiv.org/abs/2506.05174", "authors": ["Yifan Zhang", "Joe Kileel"], "title": "Norming Sets for Tensor and Polynomial Sketching", "categories": ["math.NA", "cs.IT", "cs.NA", "math.AG", "math.IT"], "comment": "16 pages", "summary": "This paper develops the sketching (i.e., randomized dimension reduction)\ntheory for real algebraic varieties and images of polynomial maps, including,\ne.g., the set of low rank tensors and tensor networks. Through the lens of\nnorming sets, we provide a framework for controlling the sketching dimension\nfor \\textit{any} sketch operator used to embed said sets, including\nsub-Gaussian, fast Johnson-Lindenstrauss, and tensor structured sketch\noperators. Leveraging norming set theory, we propose a new sketching method\ncalled the median sketch. It embeds such a set $V$ using only\n$\\widetilde{\\mathcal{O}}(\\dim V)$ tensor structured or sparse linear\nmeasurements.", "AI": {"tldr": "The paper introduces a sketching theory for real algebraic varieties and polynomial maps, proposing a new method called the median sketch, which uses fewer measurements.", "motivation": "To extend sketching theory to algebraic varieties and polynomial maps, including low-rank tensors, and provide a universal framework for controlling sketching dimensions.", "method": "Uses norming set theory to develop the median sketch, requiring only ~O(dim V) tensor-structured or sparse linear measurements.", "result": "The median sketch efficiently embeds sets like low-rank tensors with reduced measurements.", "conclusion": "The framework and median sketch advance sketching theory for algebraic structures, offering practical efficiency."}}
{"id": "2506.04279", "pdf": "https://arxiv.org/pdf/2506.04279", "abs": "https://arxiv.org/abs/2506.04279", "authors": ["Adama Nouboukpo", "Kodzo Michel Aladji", "Muktar Bappa"], "title": "Nombre Effectif de Partis Politiques en Afrique: Une Nouvelle M\u00e9thode pour un Calcul Objectif et Institutionnellement Neutre", "categories": ["physics.soc-ph", "cs.NA", "math.NA"], "comment": "in French language", "summary": "Political fragmentation in Africa poses to a significant challenge to\neffective governance and stability. Traditional measures of party system\nfragmentation, such as the Effective Number of Parties (ENP) index, often fail\nto capture the nuanced realities of African political landscapes, particularly\nthe influence of dominant parties, fluid party affiliations, and the impact of\nethnic and regional cleavages. To address these limitations, this paper\nintroduces two novel \"apolitical\" or \"institutionally neutral\" measures for\ncalculating the effective number of parties, focusing on geographical and\ndemographic dimensions, notably population size and territorial area. By\nincorporating these local realities and ensuring a minimum threshold of two\nparties, the proposed models offer a simpler and more contextually relevant\nframework for understanding political dynamics in Africa, especially in\ndata-scarce environments. This approach provides a valuable tool for analyzing\nand streamlining political systems, with potential for broader application\nbeyond the African context.", "AI": {"tldr": "The paper introduces two new measures for calculating party fragmentation in Africa, focusing on geography and demographics, to better reflect local political realities.", "motivation": "Traditional measures like the ENP index fail to capture nuances of African politics, such as dominant parties and ethnic cleavages.", "method": "Proposes two 'apolitical' measures based on population size and territorial area, with a minimum threshold of two parties.", "result": "The new models offer a simpler, contextually relevant framework for analyzing African political systems, even in data-scarce settings.", "conclusion": "This approach provides a valuable tool for understanding political fragmentation, with potential applications beyond Africa."}}
{"id": "2506.04655", "pdf": "https://arxiv.org/pdf/2506.04655", "abs": "https://arxiv.org/abs/2506.04655", "authors": ["Mengjiao Bai", "Huaian Diao", "Weisheng Zhou"], "title": "Inverse elastic obstacle scattering problems by monotonicity method", "categories": ["math.AP", "cs.NA", "math.NA"], "comment": null, "summary": "We consider the elastic wave scattering problem involving rigid obstacles.\nThis work addresses the inverse problem of reconstructing the position and\nshape of such obstacles using far-field measurements. A novel\nmonotonicity-based approach is developed for this purpose. By factorizing the\nfar-field operator and utilizing the existence of localized wave functions, we\nderive a shape characterization criterion for the obstacle boundary. The\nproposed method employs monotonicity tests to determine the geometric\nrelationship between any given test domain and the actual scatterer. As a\nresult, the shape and location of rigid elastic obstacles can be uniquely\nidentified without requiring any initial guesses or prior knowledge of the\nphysical parameters of the homogeneous background medium.", "AI": {"tldr": "A monotonicity-based method is proposed to uniquely identify the shape and position of rigid elastic obstacles using far-field measurements, without initial guesses or prior knowledge.", "motivation": "To solve the inverse problem of reconstructing rigid obstacles in elastic wave scattering using far-field data.", "method": "Develops a monotonicity-based approach, factorizes the far-field operator, and uses localized wave functions to derive a shape characterization criterion.", "result": "The method uniquely identifies the shape and location of obstacles without initial guesses or background medium knowledge.", "conclusion": "The novel approach effectively solves the inverse problem for rigid elastic obstacles."}}
{"id": "2506.04354", "pdf": "https://arxiv.org/pdf/2506.04354", "abs": "https://arxiv.org/abs/2506.04354", "authors": ["Elmira Mirzabeigi", "Rezvan Salehi", "Kourosh Parand"], "title": "BridgeNet: A Hybrid, Physics-Informed Machine Learning Framework for Solving High-Dimensional Fokker-Planck Equations", "categories": ["physics.comp-ph", "cs.LG", "math-ph", "math.AP", "math.MP"], "comment": null, "summary": "BridgeNet is a novel hybrid framework that integrates convolutional neural\nnetworks with physics-informed neural networks to efficiently solve non-linear,\nhigh-dimensional Fokker-Planck equations (FPEs). Traditional PINNs, which\ntypically rely on fully connected architectures, often struggle to capture\ncomplex spatial hierarchies and enforce intricate boundary conditions. In\ncontrast, BridgeNet leverages adaptive CNN layers for effective local feature\nextraction and incorporates a dynamically weighted loss function that\nrigorously enforces physical constraints. Extensive numerical experiments\nacross various test cases demonstrate that BridgeNet not only achieves\nsignificantly lower error metrics and faster convergence compared to\nconventional PINN approaches but also maintains robust stability in\nhigh-dimensional settings. This work represents a substantial advancement in\ncomputational physics, offering a scalable and accurate solution methodology\nwith promising applications in fields ranging from financial mathematics to\ncomplex system dynamics.", "AI": {"tldr": "BridgeNet combines CNNs and PINNs to solve high-dimensional FPEs more efficiently than traditional PINNs, achieving lower errors and faster convergence.", "motivation": "Traditional PINNs struggle with complex spatial hierarchies and boundary conditions, prompting the need for a hybrid approach.", "method": "BridgeNet uses adaptive CNN layers for local feature extraction and a dynamically weighted loss function to enforce physical constraints.", "result": "BridgeNet outperforms conventional PINNs with lower error metrics, faster convergence, and robust stability in high-dimensional settings.", "conclusion": "BridgeNet advances computational physics with a scalable, accurate solution for applications in finance and complex systems."}}
{"id": "2506.04489", "pdf": "https://arxiv.org/pdf/2506.04489", "abs": "https://arxiv.org/abs/2506.04489", "authors": ["Lukas Exl", "Sebastian Schaffer"], "title": "Spectrally accurate and efficient convolution with the 3D free-space Laplace Green's function via the super-potential", "categories": ["physics.comp-ph", "cs.NA", "math.NA", "65N35, 65N80"], "comment": "11 pages, 1 figure", "summary": "We present a high-accuracy spectral method for solving the unbounded\nthree-dimensional Poisson equation with smooth, compactly supported sources.\nThe approach is based on a super-potential formulation, where the solution is\nobtained by applying the Laplacian to a convolution with the biharmonic Green's\nfunction. A separable Gaussian-sum (GS) approximation enables efficient\nFFT-based computation with quasi-linear complexity. Owing to the improved\nregularity of the biharmonic kernel, the GS cutoff error is of order four,\neliminating the need for correction terms or Taylor expansions required in\nstandard GS or Ewald-type methods. Numerical benchmarks demonstrate that the\nmethod achieves machine-precision accuracy and outperforms existing GS-based\nschemes in both error and runtime, making it a robust and efficient tool for\nfree-space Poisson problems on uniform grids.", "AI": {"tldr": "A high-accuracy spectral method for solving the 3D Poisson equation with smooth, compactly supported sources, using a super-potential formulation and Gaussian-sum approximation for efficient FFT-based computation.", "motivation": "To address the need for a robust and efficient method for solving unbounded 3D Poisson equations with high accuracy and computational efficiency.", "method": "Uses a super-potential formulation with a biharmonic Green's function and a separable Gaussian-sum approximation for FFT-based computation.", "result": "Achieves machine-precision accuracy and outperforms existing Gaussian-sum-based methods in error and runtime.", "conclusion": "The method is a robust and efficient tool for free-space Poisson problems on uniform grids."}}
{"id": "2506.04763", "pdf": "https://arxiv.org/pdf/2506.04763", "abs": "https://arxiv.org/abs/2506.04763", "authors": ["Shuai Lu"], "title": "A highly scalable numerical framework for reservoir simulation on UG4 platform", "categories": ["physics.comp-ph", "cs.DC"], "comment": null, "summary": "The modeling and simulation of multiphase fluid flow receive significant\nattention in reservoir engineering. Many time discretization schemes for\nmultiphase flow equations are either explicit or semi-implicit, relying on the\ndecoupling between the saturation equation and the pressure equation. In this\nstudy, we delve into a fully coupled and fully implicit framework for\nsimulating multiphase flow in heterogeneous porous media, considering gravity\nand capillary effects. We utilize the Vertex-Centered Finite Volume Method for\nspatial discretization and propose an efficient implementation of interface\nconditions for heterogeneous porous media within the current scheme. Notably,\nwe introduce the Linearly Implicit Extrapolation Method (LIMEX) with an error\nestimator, adapted for the first time to multiphase flow problems. To solve the\nresulting linear system, we employ the BiCGSTAB method with the Geometric\nMultigrid (GMG) preconditioner. The implementations of models and methods are\nbased on the open-source software: UG4. The results from parallel computations\non the supercomputer demonstrate that the scalability of our proposed framework\nis sufficient, supporting a scale of thousands of processors with Degrees of\nFreedom (DoF) extending up to billions.", "AI": {"tldr": "The paper presents a fully coupled, fully implicit framework for simulating multiphase flow in heterogeneous porous media, using advanced numerical methods and demonstrating scalability on supercomputers.", "motivation": "Existing time discretization schemes for multiphase flow often rely on decoupling saturation and pressure equations, limiting accuracy and efficiency.", "method": "The study employs a Vertex-Centered Finite Volume Method, introduces the LIMEX with an error estimator, and uses BiCGSTAB with GMG preconditioner for solving linear systems.", "result": "The framework shows scalability on supercomputers, handling billions of DoF across thousands of processors.", "conclusion": "The proposed fully implicit framework is efficient and scalable, advancing multiphase flow simulation in reservoir engineering."}}
{"id": "2506.04835", "pdf": "https://arxiv.org/pdf/2506.04835", "abs": "https://arxiv.org/abs/2506.04835", "authors": ["Julien El Hajj", "Gilles Ledoux", "Samy Merabia"], "title": "Thermoplasmonics of Gold-Core Silica-Shell Colloidal Nanoparticles under Pulse Illumination", "categories": ["physics.comp-ph", "cond-mat.mtrl-sci"], "comment": null, "summary": "Core-shell nanoparticles, particularly those having a gold core, have emerged\nas a highly promising class of materials due to their unique optical and\nthermal properties, which underpin a wide range of applications in photothermal\ntherapy, imaging, and biosensing. In this study, we present a comprehensive\nstudy of the thermal dynamics of gold-core silica-shell nanoparticles immersed\nin water under pulse illumination. The plasmonic response of the core-shell\nnanoparticle is described by incorporating Mie theory with electronic\ntemperature corrections to the refractive indices of gold, based on a Drude\nLorentz formulation. The thermal response of the core-shell nanoparticles is\nmodeled by coupling the two temperature model with molecular dynamics\nsimulations, providing an atomistic description of nanoscale heat transfer. We\ninvestigate nanoparticles with both dense and porous silica shells (with 50%\nporosity) under laser pulse durations of 100 fs, 10 ps, and 1 ns, and over a\nrange of fluences between 0.05 and 5mJ/cm2. We show that nanoparticles with a\nthin dense silica shell (5 nm) exhibit significantly faster water heating\ncompared to bare gold nanoparticles. This behavior is attributed to enhanced\nelectron-phonon coupling at the gold silica interface and to the relatively\nhigh thermal conductance between silica and water. These findings provide new\ninsights into optimizing nanoparticle design for efficient photothermal\napplications and establish a robust framework for understanding energy transfer\nmechanisms in heterogeneous metal dielectric nanostructures.", "AI": {"tldr": "Study explores thermal dynamics of gold-core silica-shell nanoparticles under pulse illumination, revealing faster water heating with thin dense silica shells compared to bare gold nanoparticles.", "motivation": "Core-shell nanoparticles, especially gold-core ones, have unique optical and thermal properties for applications like photothermal therapy, imaging, and biosensing. Understanding their thermal dynamics can optimize their design.", "method": "Combines Mie theory with electronic temperature corrections for plasmonic response and couples the two-temperature model with molecular dynamics for thermal response. Tests dense and porous silica shells under varying laser pulses and fluences.", "result": "Nanoparticles with thin dense silica shells (5 nm) heat water faster than bare gold nanoparticles due to enhanced electron-phonon coupling and high silica-water thermal conductance.", "conclusion": "Findings offer insights for optimizing nanoparticle design for photothermal applications and understanding energy transfer in metal-dielectric nanostructures."}}
{"id": "2506.04375", "pdf": "https://arxiv.org/pdf/2506.04375", "abs": "https://arxiv.org/abs/2506.04375", "authors": ["Conor Rowan", "John Evans", "Kurt Maute", "Alireza Doostan"], "title": "Solving engineering eigenvalue problems with neural networks using the Rayleigh quotient", "categories": ["math.NA", "cs.LG", "cs.NA"], "comment": null, "summary": "From characterizing the speed of a thermal system's response to computing\nnatural modes of vibration, eigenvalue analysis is ubiquitous in engineering.\nIn spite of this, eigenvalue problems have received relatively little treatment\ncompared to standard forward and inverse problems in the physics-informed\nmachine learning literature. In particular, neural network discretizations of\nsolutions to eigenvalue problems have seen only a handful of studies. Owing to\ntheir nonlinearity, neural network discretizations prevent the conversion of\nthe continuous eigenvalue differential equation into a standard discrete\neigenvalue problem. In this setting, eigenvalue analysis requires more\nspecialized techniques. Using a neural network discretization of the\neigenfunction, we show that a variational form of the eigenvalue problem called\nthe \"Rayleigh quotient\" in tandem with a Gram-Schmidt orthogonalization\nprocedure is a particularly simple and robust approach to find the eigenvalues\nand their corresponding eigenfunctions. This method is shown to be useful for\nfinding sets of harmonic functions on irregular domains, parametric and\nnonlinear eigenproblems, and high-dimensional eigenanalysis. We also discuss\nthe utility of harmonic functions as a spectral basis for approximating\nsolutions to partial differential equations. Through various examples from\nengineering mechanics, the combination of the Rayleigh quotient objective,\nGram-Schmidt procedure, and the neural network discretization of the\neigenfunction is shown to offer unique advantages for handling continuous\neigenvalue problems.", "AI": {"tldr": "The paper introduces a neural network-based method for solving eigenvalue problems using the Rayleigh quotient and Gram-Schmidt orthogonalization, demonstrating its effectiveness for irregular domains and high-dimensional cases.", "motivation": "Eigenvalue problems are common in engineering but underexplored in physics-informed machine learning. Neural network discretizations complicate eigenvalue analysis, requiring specialized techniques.", "method": "A neural network discretizes the eigenfunction, combined with the Rayleigh quotient and Gram-Schmidt orthogonalization to solve eigenvalue problems.", "result": "The method effectively handles irregular domains, parametric/nonlinear eigenproblems, and high-dimensional cases, offering advantages for continuous eigenvalue problems.", "conclusion": "The approach is robust and versatile, with potential applications in approximating PDE solutions using harmonic functions as a spectral basis."}}
{"id": "2506.04826", "pdf": "https://arxiv.org/pdf/2506.04826", "abs": "https://arxiv.org/abs/2506.04826", "authors": ["Konstantinos Giotis", "Dimitrios Stefas", "Yanis Agha", "Hans H\u00f6ft", "Xavier Duten", "Panagiotis Svarnas", "Guillaume Lombardi", "Kristaq Gazeli"], "title": "Discharge dynamics in a cylindrical SDBD prototype reactor under ns-pulsed and sinusoidal AC operation", "categories": ["physics.plasm-ph"], "comment": null, "summary": "We developed a prototype reactor generating surface dielectric barrier\ndischarges (SDBDs) in ambient air, designed for consistent operation while\npreventing constructive material degradation. It features detachable stainless\nsteel electrodes and quartz dielectric to ensure precise fabrication. The\ngrounded electrode is fully immersed into transformer oil drastically\nsuppressing undesired parasitic discharges. The device efficiently sustains\nns-pulsed and AC discharges at 10 kHz, enabling fundamental studies of their\nelectrical characteristics (applied voltage, induced current, electric power)\nand spatiotemporal dynamics (morphology, propagation length and velocity). The\nelectric power (P) consumed exhibits a dissimilar non-linear increase with the\nrising peak voltage (Vp) in each case: P$\\approx$0.8-2.5 W for ns-pulsed\n(Vp=7-9 kV) and P$\\approx$0.9-5.3 W (Vp=7-10 kV) for AC operation. Using ICCD\nimaging, distinct ionization channels are recorded in the rising part of the\npulsed voltage being detached from the driven electrode; during the voltage\ndecrease, a glow-like discharge is formed remaining anchored on the driven\nelectrode. The rising part of the AC voltage is characterized by erratic,\nelongated ionization channels in a filamentary form, the voltage drop featuring\na glow-like behavior. During the rising and falling parts of the AC voltage,\nthe discharge reaches maximum propagation lengths (Lmax) of $\\approx$12 mm and\n$\\approx$7 mm, respectively, while remaining attached to the driven electrode.\nThe corresponding maximum discharge velocities (vmax) are about 5x10 2 m/s and\n3x10 2 m/s. For the ns-pulsed operation, Lmax$\\approx$5 mm (vmax$\\approx$5x10 5\nm/s) and Lmax$\\approx$3.5 mm (vmax$\\approx$1.5x10 5 m/s) during the rising and\nfalling parts of the voltage pulse, respectively. The SDBD dynamics generated\nwith a ns-pulsed voltage is more reproducible than for the AC case allowing for\nthe use of a 500 times smaller ICCD gate width (2 ns) and a more accurate\ndescription of the discharge's spatiotemporal development. This reactor is\nsuitable for performing fundamental studies and understanding key SDBD features\nfor various applications such as flow control, biomedicine and agriculture.", "AI": {"tldr": "A prototype reactor for surface dielectric barrier discharges (SDBDs) in air was developed, featuring detachable electrodes and quartz dielectric to prevent material degradation. It efficiently sustains ns-pulsed and AC discharges, enabling detailed study of electrical and spatiotemporal dynamics. The reactor is suitable for fundamental research and applications like flow control and biomedicine.", "motivation": "To create a reliable SDBD reactor for consistent operation without material degradation, enabling precise study of discharge characteristics and dynamics for various applications.", "method": "The reactor uses detachable stainless steel electrodes and quartz dielectric, with the grounded electrode immersed in transformer oil to suppress parasitic discharges. It operates with ns-pulsed and AC discharges at 10 kHz, analyzing electrical parameters and discharge dynamics using ICCD imaging.", "result": "The reactor showed distinct discharge behaviors for ns-pulsed and AC operations, with non-linear power increases. Ns-pulsed discharges were more reproducible, allowing finer temporal resolution (2 ns gate width). Maximum propagation lengths and velocities were measured for both discharge types.", "conclusion": "The reactor effectively studies SDBD dynamics, with ns-pulsed operation offering higher reproducibility. It is versatile for fundamental research and practical applications like flow control and biomedicine."}}
{"id": "2506.05293", "pdf": "https://arxiv.org/pdf/2506.05293", "abs": "https://arxiv.org/abs/2506.05293", "authors": ["Mario Christopher Bedrunka", "Dirk Reith", "Holger Foysi", "\u0141ukasz \u0141aniewski-Wo\u0142\u0142k", "Travis Mitchell"], "title": "Reduction of Outflow Boundary Influence on Aerodynamic Performance using Neural Networks", "categories": ["physics.comp-ph", "physics.flu-dyn"], "comment": null, "summary": "The accurate treatment of outflow boundary conditions remains a critical\nchallenge in computational fluid dynamics when predicting aerodynamic forces\nand/or acoustic emissions. This is particularly evident when employing the\nlattice Boltzmann method (LBM) as the numerical solution technique, which often\nsuffers from inaccuracies induced by artificial reflections from outflow\nboundaries. This paper investigates the use of neural networks (NN) to mitigate\nthese adverse boundary effects and enable truncated domain requirements. Two\ndistinct NN-based approaches are proposed: (1) direct reconstruction of unknown\nparticle distribution functions at the outflow boundary; and (2) enhancement of\nestablished characteristic boundary conditions (CBC) by dynamically tuning\ntheir parameters. The direct reconstruction model was trained on data generated\nfrom a 2D flow over a cylindrical obstruction. The drag, lift, and Strouhal\nnumber were used to test the new boundary condition. We analyzed results for\nvarious Reynolds numbers and restricted domain sizes where it demonstrated\nsignificantly improved predictions when compared with the traditional Zou & He\nboundary condition. To examine the robustness of the NN-based reconstruction,\nthe same condition was applied to the simulation of a NACA0012 airfoil, again\nproviding accurate aerodynamic performance predictions. The neural-enhanced CBC\nwere evaluated on a 2D convected vortex benchmark and showed superior\nperformance in minimizing density errors compared to CBCs with fixed\nparameters. These findings highlight the potential of NN-integrated boundary\nconditions to improve accuracy and reduce computational expense of aerodynamic\nand acoustic emissions simulations with the LBM.", "AI": {"tldr": "Neural networks (NN) are used to improve outflow boundary conditions in LBM, reducing artificial reflections and computational costs. Two NN-based methods show superior accuracy over traditional approaches.", "motivation": "Outflow boundary conditions in LBM often cause inaccuracies due to artificial reflections, limiting aerodynamic and acoustic predictions.", "method": "Two NN-based approaches: (1) direct reconstruction of particle distribution functions, and (2) dynamic tuning of characteristic boundary conditions (CBC).", "result": "NN methods outperformed traditional Zou & He boundary conditions in accuracy for drag, lift, and Strouhal number predictions. NN-enhanced CBCs also minimized density errors in vortex benchmarks.", "conclusion": "NN-integrated boundary conditions enhance LBM accuracy and reduce computational costs, benefiting aerodynamic and acoustic simulations."}}
{"id": "2506.04416", "pdf": "https://arxiv.org/pdf/2506.04416", "abs": "https://arxiv.org/abs/2506.04416", "authors": ["Ziyao Xu", "Yong-Tao Zhang"], "title": "Exponential Time Differencing Runge-Kutta Discontinuous Galerkin (ETD-RKDG) Methods for Nonlinear Degenerate Parabolic Equations", "categories": ["math.NA", "cs.NA"], "comment": "34 pages", "summary": "In this paper, we study high-order exponential time differencing Runge-Kutta\n(ETD-RK) discontinuous Galerkin (DG) methods for nonlinear degenerate parabolic\nequations. This class of equations exhibits hyperbolic behavior in degenerate\nregions and parabolic behavior in non-degenerate regions, resulting in sharp\nwave fronts in the solution profiles and a parabolic-type time-step\nrestriction, $\\tau \\sim O(h^2)$, for explicit time integration. To address\nthese challenges and solve such equations in complex domains, we employ DG\nmethods with appropriate stabilizing limiters on unstructured meshes to capture\nthe wave fronts and use ETD-RK methods for time integration to resolve the\nstiffness of parabolic terms. We extract the system's stiffness using the\nJacobian matrix of the DG discretization for diffusion terms and adopt a nodal\nformulation to facilitate its computation. The algorithm is described in detail\nfor two-dimensional triangular meshes. We also conduct a linear stability\nanalysis in one spatial dimension and present computational results on\nthree-dimensional simplex meshes, demonstrating significant improvements in\nstability and large time-step sizes.", "AI": {"tldr": "The paper presents high-order ETD-RK DG methods for solving nonlinear degenerate parabolic equations, addressing stiffness and wave front challenges with stabilizing limiters and nodal formulations.", "motivation": "To tackle the hyperbolic-parabolic behavior of degenerate parabolic equations and overcome stiffness and time-step restrictions in explicit methods.", "method": "Uses DG methods with stabilizing limiters on unstructured meshes and ETD-RK for time integration, leveraging the Jacobian matrix for stiffness extraction.", "result": "Demonstrates improved stability and larger time-step sizes, validated through linear stability analysis and 3D computational results.", "conclusion": "The proposed methods effectively handle stiffness and wave fronts, offering robust solutions for complex domains."}}
{"id": "2506.04827", "pdf": "https://arxiv.org/pdf/2506.04827", "abs": "https://arxiv.org/abs/2506.04827", "authors": ["Lorenzo Martelli", "Igor Andriyash", "Jonathan Wheeler", "Henri Kraft", "Xuan Quyen Dinh", "C\u00e9dric Thaury"], "title": "Empirical scaling laws for self-focused laser pulses in nitrogen plasmas", "categories": ["physics.plasm-ph"], "comment": "6 figures, 9 pages", "summary": "We investigate the interaction between a superintense laser pulse and a\nnitrogen plasma with densities exceeding $10^{19}\\,$cm$^{-3}$, using\nparticle-in-cell simulations. Such configurations have recently demonstrated\nthe capability to produce highly charged electron beams (i.e., $>10\\,$nC) with\n$1\\,$J-class lasers, a significant step toward high-average-current\nlaser-plasma accelerators. Our study focuses on analyzing the impact of laser\nself-focusing on laser dynamics, leading to scaling laws that characterize beam\ndiffraction, wakefield amplitude and plasma structures, providing important\ninsights of this interaction regime.", "AI": {"tldr": "Study of laser-plasma interaction in high-density nitrogen plasma, revealing scaling laws for beam diffraction and wakefield amplitude.", "motivation": "To understand the dynamics of superintense laser pulses in high-density plasmas, aiming to advance laser-plasma accelerators.", "method": "Particle-in-cell simulations to analyze laser self-focusing and its effects on plasma structures.", "result": "Identified scaling laws for beam diffraction and wakefield amplitude, contributing to high-current laser-plasma accelerators.", "conclusion": "Provides key insights into laser-plasma interactions, aiding the development of efficient accelerators."}}
{"id": "2506.04491", "pdf": "https://arxiv.org/pdf/2506.04491", "abs": "https://arxiv.org/abs/2506.04491", "authors": ["IceCube Collaboration"], "title": "GollumFit: An IceCube Open-Source Framework for Binned-Likelihood Neutrino Telescope Analyses", "categories": ["hep-ex", "astro-ph.HE", "physics.comp-ph", "physics.data-an"], "comment": null, "summary": "We present GollumFit, a framework designed for performing binned-likelihood\nanalyses on neutrino telescope data. GollumFit incorporates model parameters\ncommon to any neutrino telescope and also model parameters specific to the\nIceCube Neutrino Observatory. We provide a high-level overview of its key\nfeatures and how the code is organized. We then discuss the performance of the\nfitting in a typical analysis scenario, highlighting the ability to fit over\ntens of nuisance parameters. We present some examples showing how to use the\npackage for likelihood minimization tasks. This framework uniquely incorporates\nthe particular model parameters necessary for neutrino telescopes, and solves\nan associated likelihood problem in a time-efficient manner.", "AI": {"tldr": "GollumFit is a framework for binned-likelihood analyses in neutrino telescope data, tailored for IceCube, with efficient handling of nuisance parameters.", "motivation": "To address the need for a specialized framework for neutrino telescope data analysis, particularly for IceCube, incorporating both general and specific model parameters.", "method": "The framework organizes code for likelihood minimization, handling tens of nuisance parameters efficiently.", "result": "GollumFit demonstrates effective performance in typical analysis scenarios, solving likelihood problems time-efficiently.", "conclusion": "GollumFit provides a specialized, efficient solution for neutrino telescope data analysis, particularly for IceCube."}}
{"id": "2506.04451", "pdf": "https://arxiv.org/pdf/2506.04451", "abs": "https://arxiv.org/abs/2506.04451", "authors": ["Santolo Leveque", "Yunhui He", "Maxim Olshanskii"], "title": "An Augmented Lagrangian Preconditioner for Navier--Stokes Equations with Runge--Kutta in Time", "categories": ["math.NA", "cs.NA"], "comment": null, "summary": "We consider a Runge--Kutta method for the numerical time integration of the\nnonstationary incompressible Navier--Stokes equations. This yields a sequence\nof nonlinear problems to be solved for the stages of the Runge--Kutta method.\nThe resulting nonlinear system of differential equations is discretized using a\nfinite element method. To compute a numerical approximation of the stages at\neach time step, we employ Newton's method, which requires the solution of a\nlarge and sparse generalized saddle-point problem at each nonlinear iteration.\nWe devise an augmented Lagrangian preconditioner within the flexible GMRES\nmethod for solving the Newton systems at each time step. The preconditioner can\nbe applied inexactly with the help of a multigrid routine. We present numerical\nevidence of the robustness and efficiency of the proposed strategy for\ndifferent values of the viscosity, mesh size, time step, and number of stages\nof the Runge--Kutta method.", "AI": {"tldr": "A Runge-Kutta method is used for time integration of the nonstationary incompressible Navier-Stokes equations, with nonlinear systems solved via Newton's method and a preconditioned GMRES approach.", "motivation": "To efficiently solve the nonlinear systems arising from the Runge-Kutta discretization of the Navier-Stokes equations.", "method": "Finite element discretization, Newton's method for nonlinear systems, and an augmented Lagrangian preconditioner within flexible GMRES, supported by multigrid.", "result": "Numerical results demonstrate robustness and efficiency across varying parameters like viscosity, mesh size, and time step.", "conclusion": "The proposed strategy is effective for solving the nonlinear systems in the Runge-Kutta framework."}}
{"id": "2506.05124", "pdf": "https://arxiv.org/pdf/2506.05124", "abs": "https://arxiv.org/abs/2506.05124", "authors": ["Florens Grimm", "Jan-Luca Gembus", "Jana Sch\u00f6ne", "Peter Awakowicz", "Lars Sch\u00fccke", "Andrew R. Gibson"], "title": "Electron and gas temperature-driven chemistry during microdischarges formed in water vapour bubbles", "categories": ["physics.plasm-ph"], "comment": null, "summary": "Microdischarges formed in bubbles immersed in liquids are of interest for\nmaterials synthesis and chemical conversion applications in the frame of\nplasma-driven electrochemistry. A key challenge associated with controlling\nsuch processes is the limited understanding of the gas-phase chemical kinetics\nin these microdischarges. Due to their large electron densities, and high gas\ntemperatures, both electron and gas temperature driven chemistry are likely to\nbe important. Here, a 0-D modelling approach, informed by experimental\nmeasurements, is used to study the chemical kinetics in these systems. A new\nreaction scheme is developed for microdischarges in water vapour, including\nreactions for both high electron density, and high gas temperature regimes.\nMicrodischarges formed during plasma electrolytic oxidation are used as a test\ncase, however, the key results are expected to be transferable to other plasma\nelectrolysis systems with similar properties. Experimentally measured power\ndensities are used as input to the 0-D model, together with estimates of\ntemperatures and gas pressures within the gas bubble. Comparison of measured\nand simulated electron densities shows good agreement, given the limitations of\nboth model and experiment. In the base case microdischarge, H$_{2}$O is found\nto be highly dissociated during the period of peak power density, with H and O\nmaking up the majority of the neutral gas in the bubble. The maximum ionization\ndegree is around 0.31$\\,\\%$, and the electronegativity during the period of\npeak electron density is found to be low. Species formation and reaction\npathways are analysed under variation of the neutral gas temperature from\n2000$\\,$K to 6000$\\,$K. At all temperatures, electron, ion, and neutral\nreactions with high threshold energies are found to be important for the\noverall chemical kinetics.", "AI": {"tldr": "A 0-D model studies chemical kinetics in microdischarges in bubbles, focusing on high electron density and gas temperature regimes, validated by experiments.", "motivation": "Understanding gas-phase chemical kinetics in microdischarges for plasma-driven electrochemistry applications.", "method": "A 0-D modeling approach informed by experimental measurements, with a new reaction scheme for water vapor microdischarges.", "result": "High dissociation of H\u2082O, low electronegativity, and significant ionization (0.31%) observed; electron and ion reactions dominate kinetics.", "conclusion": "The model provides insights into microdischarge chemistry, applicable to similar plasma electrolysis systems."}}
{"id": "2506.04523", "pdf": "https://arxiv.org/pdf/2506.04523", "abs": "https://arxiv.org/abs/2506.04523", "authors": ["Cliff B. Abbott", "Mark Elo", "Dmytro A. Bozhko"], "title": "Perturbative Gradient Training: A novel training paradigm for bridging the gap between deep neural networks and physical reservoir computing", "categories": ["cs.LG", "cond-mat.mes-hall", "cs.ET", "cs.NE", "physics.comp-ph"], "comment": "7 pages, 8 figures, submitted to IEEE Transactions on Neural Netowrks\n  and Learning Systems", "summary": "We introduce Perturbative Gradient Training (PGT), a novel training paradigm\nthat overcomes a critical limitation of physical reservoir computing: the\ninability to perform backpropagation due to the black-box nature of physical\nreservoirs. Drawing inspiration from perturbation theory in physics, PGT uses\nrandom perturbations in the network's parameter space to approximate gradient\nupdates using only forward passes. We demonstrate the feasibility of this\napproach on both simulated neural network architectures, including a dense\nnetwork and a transformer model with a reservoir layer, and on experimental\nhardware using a magnonic auto-oscillation ring as the physical reservoir. Our\nresults show that PGT can achieve performance comparable to that of standard\nbackpropagation methods in cases where backpropagation is impractical or\nimpossible. PGT represents a promising step toward integrating physical\nreservoirs into deeper neural network architectures and achieving significant\nenergy efficiency gains in AI training.", "AI": {"tldr": "PGT is a new training method for physical reservoir computing, using random perturbations to approximate gradients without backpropagation, showing comparable performance to traditional methods.", "motivation": "Overcome the limitation of physical reservoir computing's inability to perform backpropagation due to its black-box nature.", "method": "Uses random perturbations in parameter space to approximate gradient updates, tested on simulated networks and experimental hardware.", "result": "PGT achieves performance comparable to standard backpropagation where it's impractical.", "conclusion": "PGT enables integration of physical reservoirs into deeper networks, promising energy efficiency gains in AI training."}}
{"id": "2506.04710", "pdf": "https://arxiv.org/pdf/2506.04710", "abs": "https://arxiv.org/abs/2506.04710", "authors": ["Lucas \u00c5kerstedt", "Harald Hultin", "B. L. G. Jonsson"], "title": "An Array Decomposition Method for Finite Arrays with Electrically Connected Elements for fast Toeplitz Solvers", "categories": ["math.NA", "cs.NA", "eess.SP"], "comment": "12 pages, 17 figures", "summary": "A large part of the geometry of array antennas is often partially defined by\nfinite translational symmetries. Applying the method of moments (MoM) with the\nRWG-like element on an appropriately structured mesh to these arrays results in\nan impedance matrix where the main part exhibits a multilevel block Toeplitz\nstructure. This article introduces a memory-efficient construction method that\neffectively represents and reuses impedance calculations. The proposed method,\napplicable to electrically connected elements, also accounts for all\nnon-symmetric parts of the array. The core idea involves nine distinct\nelectrically connectable components from which the array can be assembled. The\nderived multilevel block Toeplitz matrix is further utilized by an in-house\ninverse solver to achieve faster and more memory-efficient MoM current vector\ncalculations. We demonstrate the method by computing the far-field of a 32x32\narray and the scattering parameters of two tightly coupled 9x9 arrays. This\napproach reduces the memory allocation from $\\mathcal{O}(N_x^2 N_y^2)$ to\n$\\mathcal{O}(N_x N_y)$, for an $N_x \\times N_y$ array.", "AI": {"tldr": "A memory-efficient method for constructing impedance matrices in array antennas using multilevel block Toeplitz structure, reducing memory usage from O(N_x^2 N_y^2) to O(N_x N_y).", "motivation": "Finite translational symmetries in array antennas lead to redundant impedance calculations, which can be optimized for memory efficiency.", "method": "Uses MoM with RWG-like elements on structured meshes, representing impedance matrices as multilevel block Toeplitz and reusing calculations via nine connectable components.", "result": "Demonstrated on a 32x32 array and two 9x9 arrays, showing reduced memory usage and faster calculations.", "conclusion": "The method efficiently handles non-symmetric parts of arrays and improves computational performance for MoM current vector calculations."}}
{"id": "2506.05170", "pdf": "https://arxiv.org/pdf/2506.05170", "abs": "https://arxiv.org/abs/2506.05170", "authors": ["Dario Panici", "Eduardo Rodriguez", "Rory Conlin", "Daniel Dudt", "Egemen Kolemen"], "title": "Extending near-axis equilibria in DESC", "categories": ["physics.plasm-ph"], "comment": "30 pages, 7 figures", "summary": "The near-axis description of optimised stellarator fields has proven to be a\npowerful tool both for design and understanding of this magnetic confinement\nconcept. The description consists of an asymptotic model of the equilibrium in\nthe distance from its centermost axis, and is thus only approximate. Any\npractical application therefore requires the eventual construction of a global\nequilibrium. This paper presents a novel way of constructing global equilibria\nusing the \\texttt{DESC} code that guarantees the correct asymptotic behaviour\nimposed by a given near-axis construction. The theoretical underpinnings of\nthis construction are carefully presented, and benchmarking examples provided.\nThis opens the door to an efficient coupling of the near-axis framework and\nthat of global equilibria for future optimisation efforts.", "AI": {"tldr": "A novel method for constructing global equilibria in stellarator fields using the DESC code, ensuring correct asymptotic behavior from near-axis models.", "motivation": "To bridge the gap between near-axis models and global equilibria for improved stellarator design and optimization.", "method": "Uses the DESC code to construct global equilibria with enforced asymptotic behavior from near-axis models, supported by theoretical foundations and benchmarking.", "result": "Demonstrates successful coupling of near-axis and global equilibrium frameworks, enabling efficient optimization.", "conclusion": "This approach enhances stellarator design by integrating near-axis insights into global equilibrium construction."}}
{"id": "2506.04781", "pdf": "https://arxiv.org/pdf/2506.04781", "abs": "https://arxiv.org/abs/2506.04781", "authors": ["Christoph Schirninger", "Robert Jarolim", "Astrid M. Veronig", "Christoph Kuckein"], "title": "Deep learning image burst stacking to reconstruct high-resolution ground-based solar observations", "categories": ["astro-ph.SR", "astro-ph.IM", "cs.CV", "physics.comp-ph"], "comment": null, "summary": "Large aperture ground based solar telescopes allow the solar atmosphere to be\nresolved in unprecedented detail. However, observations are limited by Earths\nturbulent atmosphere, requiring post image corrections. Current reconstruction\nmethods using short exposure bursts face challenges with strong turbulence and\nhigh computational costs. We introduce a deep learning approach that\nreconstructs 100 short exposure images into one high quality image in real\ntime. Using unpaired image to image translation, our model is trained on\ndegraded bursts with speckle reconstructions as references, improving\nrobustness and generalization. Our method shows an improved robustness in terms\nof perceptual quality, especially when speckle reconstructions show artifacts.\nAn evaluation with a varying number of images per burst demonstrates that our\nmethod makes efficient use of the combined image information and achieves the\nbest reconstructions when provided with the full image burst.", "AI": {"tldr": "A deep learning method is introduced to reconstruct high-quality solar images from 100 short-exposure bursts in real-time, improving robustness and perceptual quality over traditional speckle reconstructions.", "motivation": "Ground-based solar telescopes face limitations due to Earth's turbulent atmosphere, requiring post-image corrections. Current methods struggle with strong turbulence and high computational costs.", "method": "The approach uses unpaired image-to-image translation, training on degraded bursts with speckle reconstructions as references to enhance robustness and generalization.", "result": "The method shows improved perceptual quality, especially when speckle reconstructions exhibit artifacts, and performs best with the full image burst.", "conclusion": "The deep learning approach efficiently combines image information from bursts, offering real-time, high-quality solar image reconstruction."}}
{"id": "2506.04732", "pdf": "https://arxiv.org/pdf/2506.04732", "abs": "https://arxiv.org/abs/2506.04732", "authors": ["Nicola Cavallini", "Gianmarco Manzini", "Daniele Funaro", "Andrea Favalli"], "title": "A Fast, Accurate and Oscillation-free Spectral Collocation Solver for High-dimensional Transport Problems", "categories": ["math.NA", "cs.NA"], "comment": null, "summary": "Transport phenomena-describing the movement of particles, energy, or other\nphysical quantities-are fundamental in various scientific disciplines,\nincluding nuclear physics, plasma physics, astrophysics, engineering, and the\nnatural sciences.\n  However, solving the associated seven-dimensional transport equations poses a\nsignificant computational challenge due to the curse of dimensionality.\n  We introduce the Tensor Train Superconsistent Spectral (T${^2}$S${^2}$)\nsolver to address this challenge, integrating Spectral Collocation for\nexponential convergence, Superconsistency for stabilization in\ntransport-dominated regimes, and Tensor Train format for substantial data\ncompression. T${^2}$S${^2}$ enforces a dimension-wise superconsistent condition\ncompatible with tensor structures, achieving extremely low compression ratios,\nin the order of $(10^{-12})$, while preserving spectral accuracy. Numerical\nexperiments on linear problems demonstrate that T${^2}$S${^2}$ can solve\nhigh-dimensional transport problems in minutes on standard hardware, making\npreviously intractable problems computationally feasible. This advancement\nopens new avenues for efficiently and accurately modeling complex transport\nphenomena.", "AI": {"tldr": "The paper introduces the Tensor Train Superconsistent Spectral (T\u00b2S\u00b2) solver to efficiently solve high-dimensional transport equations, overcoming computational challenges with exponential convergence and data compression.", "motivation": "Transport phenomena are crucial in many fields, but solving their high-dimensional equations is computationally challenging due to the curse of dimensionality.", "method": "The T\u00b2S\u00b2 solver combines Spectral Collocation for exponential convergence, Superconsistency for stability, and Tensor Train format for data compression.", "result": "T\u00b2S\u00b2 achieves extremely low compression ratios (~10\u207b\u00b9\u00b2) and solves high-dimensional problems in minutes on standard hardware.", "conclusion": "T\u00b2S\u00b2 makes previously intractable transport problems feasible, enabling efficient and accurate modeling of complex phenomena."}}
{"id": "2506.04809", "pdf": "https://arxiv.org/pdf/2506.04809", "abs": "https://arxiv.org/abs/2506.04809", "authors": ["Michael J. Carley"], "title": "Numerical solution of the wave equation outside a sphere", "categories": ["math.NA", "cs.NA", "math.AP", "physics.class-ph", "physics.comp-ph"], "comment": null, "summary": "A method is presented for the fast evaluation of the transient acoustic field\ngenerated outside a spherical surface by sources inside the surface. The method\nemploys Lebedev quadratures, which are the optimal method for spatial\nintegration, and Lagrange interpolation and differentiation in an advanced time\nalgorithm for the evaluation of the transient field. Numerical testing\ndemonstrates that the approach gives near machine-precision accuracy and a\nspeed-up in evaluation time which depends on the order of quadrature rule\nemployed but breaks even with direct evaluation at a number of field points\nabout 1.15 times the number of surface quadrature nodes.", "AI": {"tldr": "A fast method for evaluating transient acoustic fields outside a spherical surface using Lebedev quadratures and Lagrange interpolation, achieving near machine-precision accuracy and speed-up.", "motivation": "To efficiently and accurately evaluate transient acoustic fields generated by internal sources outside a spherical surface.", "method": "Uses Lebedev quadratures for spatial integration and Lagrange interpolation/differentiation in an advanced time algorithm.", "result": "Near machine-precision accuracy; speed-up depends on quadrature order, breaking even at ~1.15 times surface nodes.", "conclusion": "The method is efficient and accurate for transient acoustic field evaluation."}}
{"id": "2506.04791", "pdf": "https://arxiv.org/pdf/2506.04791", "abs": "https://arxiv.org/abs/2506.04791", "authors": ["Athanasios C. Antoulas", "Ion Victor Gosea", "Charles Poussot-Vassal", "Pierre Vuillemin"], "title": "Tensor-based multivariate function approximation: methods benchmarking and comparison", "categories": ["math.NA", "cs.CE", "cs.NA", "cs.SE", "93A15, 93A30, 93B11, 93B15, 93C05, 93C80"], "comment": "Report with a collection of examples, aimed at being regularly\n  updated. Associated GIT: https://github.com/cpoussot/mLF", "summary": "In this note, we evaluate the performances, the features and the\nuser-experience of some methods (and their implementations) designed for\ntensor- (or data-) based multivariate function construction and approximation.\nTo this aim, a collection of multivariate functions extracted from contributive\nworks coming from different communities, is suggested. First, these functions\nwith varying complexity (e.g. number and degree of the variables) and nature\n(e.g. rational, irrational, differentiable or not, symmetric, etc.) are used to\nconstruct tensors, each of different dimension and size on the disk. Second,\ngrounded on this tensor, we inspect performances of each considered method\n(e.g. the accuracy, the computational time, the parameters tuning impact,\netc.). Finally, considering the \"best\" parameter tuning set, we compare each\nmethod using multiple evaluation criteria. The purpose of this note is not to\nrank the methods but rather to evaluate as fairly as possible the different\navailable strategies, with the idea in mind to guide users to understand the\nprocess, the possibilities, the advantages and the limits brought by each\ntools. The contribution claimed is to suggest a complete benchmark collection\nof some available tools for tensor approximation by surrogate models (e.g.\nrational functions, networks, etc.). In addition, as contributors of the\nmultivariate Loewner Framework (mLF) approach (and its side implementation in\nMDSPACK), attention and details of the latter are more explicitly given, in\norder to provide readers a digest of this contributive work and some details\nwith simple examples.", "AI": {"tldr": "The paper evaluates methods for tensor-based multivariate function approximation, comparing performance and features without ranking, and introduces a benchmark collection for surrogate models.", "motivation": "To fairly assess and compare various tensor approximation methods, guiding users on their advantages, limits, and practical use.", "method": "Construct tensors from diverse multivariate functions, evaluate method performance (accuracy, computational time, etc.), and compare using multiple criteria.", "result": "Provides a benchmark collection for surrogate models and detailed insights into the multivariate Loewner Framework (mLF).", "conclusion": "The study aims to guide users in understanding tensor approximation tools, emphasizing fairness and practical utility over ranking."}}
{"id": "2506.04898", "pdf": "https://arxiv.org/pdf/2506.04898", "abs": "https://arxiv.org/abs/2506.04898", "authors": ["Xintong Zou", "Zhijie Li", "Yunpeng Wang", "Huiyu Yang", "Jianchun Wang"], "title": "Uncertainty quantification and stability of neural operators for prediction of three-dimensional turbulence", "categories": ["physics.flu-dyn", "physics.comp-ph"], "comment": null, "summary": "Turbulence poses challenges for numerical simulation due to its chaotic,\nmultiscale nature and high computational cost. Traditional turbulence modeling\noften struggles with accuracy and long-term stability. Recent scientific\nmachine learning (SciML) models, such as Fourier Neural Operators (FNO), show\npromise in solving PDEs, but are typically limited to one-step-ahead\npredictions and often fail over long time horizons, especially in 3D\nturbulence. This study proposes a framework to assess the reliability of neural\noperator models in turbulent flows. Using three-dimensional forced homogeneous\nisotropic turbulence (HIT) as a benchmark, we evaluate models in terms of\nuncertainty quantification (UQ), error propagation, and sensitivity to initial\nperturbations. Statistical tools such as error distribution analysis and\nautocorrelation functions (ACF) are used to assess predictive robustness and\ntemporal coherence. Our proposed model, the factorized-implicit FNO (F-IFNO),\nimproves long-term stability and accuracy by incorporating implicit\nfactorization into the prediction process. It outperforms conventional LES and\nother FNO-based models in balancing accuracy, stability, and efficiency. The\nresults highlight the importance of prediction constraints, time interval\nselection, and UQ in developing robust neural operator frameworks for turbulent\nsystems.", "AI": {"tldr": "The study proposes a framework to evaluate neural operator models for 3D turbulence, introducing F-IFNO for improved stability and accuracy.", "motivation": "Traditional turbulence modeling struggles with accuracy and stability, while existing SciML models like FNO fail in long-term predictions for 3D turbulence.", "method": "The framework assesses neural operators using 3D forced HIT, focusing on UQ, error propagation, and sensitivity to initial perturbations. Statistical tools like error distribution and ACF are used. F-IFNO incorporates implicit factorization for better performance.", "result": "F-IFNO outperforms conventional LES and other FNO models in accuracy, stability, and efficiency.", "conclusion": "Prediction constraints, time interval selection, and UQ are crucial for robust neural operator frameworks in turbulence."}}
{"id": "2506.05031", "pdf": "https://arxiv.org/pdf/2506.05031", "abs": "https://arxiv.org/abs/2506.05031", "authors": ["Mohammad Mirzakhani", "Kyungsun Moon"], "title": "Quantum simulation of the Hubbard model on a graphene hexagon: Strengths of IQPE and noise constraints", "categories": ["quant-ph", "cond-mat.mes-hall", "cond-mat.str-el", "physics.comp-ph"], "comment": "14 pages, 10 figures", "summary": "Quantum computing offers transformative potential for simulating real-world\nmaterials, providing a powerful platform to investigate complex quantum systems\nacross quantum chemistry and condensed matter physics. In this work, we\nleverage this capability to simulate the Hubbard model on a six-site graphene\nhexagon using Qiskit, employing the Iterative Quantum Phase Estimation (IQPE)\nand adiabatic evolution algorithms to determine its ground-state properties.\nNoiseless simulations yield accurate ground-state energies (GSEs), charge and\nspin densities, and correlation functions, all in excellent agreement with\nexact diagonalization, underscoring the precision and reliability of quantum\nsimulation for strongly correlated electron systems. However, deploying IQPE\nand adiabatic evolution on today's noisy quantum hardware remains highly\nchallenging. To examine these limitations, we utilize the Qiskit Aer simulator\nwith a custom noise model tailored to the characteristics of IBM's real\nbackend. This model includes realistic depolarizing gate errors, thermal\nrelaxation, and readout noise, allowing us to explore how these factors degrade\nsimulation accuracy. Preliminary hardware runs on IBM devices further expose\ndiscrepancies between simulated and real-world noise, emphasizing the gap\nbetween ideal and practical implementations. Overall, our results highlight the\npromise of quantum computing for simulating correlated quantum materials, while\nalso revealing the significant challenges posed by hardware noise in achieving\naccurate and reliable physical predictions using current quantum devices.", "AI": {"tldr": "The paper explores quantum simulation of the Hubbard model on graphene using Qiskit, achieving accurate noiseless results but facing challenges with noise on real hardware.", "motivation": "To demonstrate the potential of quantum computing for simulating strongly correlated electron systems like the Hubbard model, while addressing the limitations of current noisy hardware.", "method": "Uses Iterative Quantum Phase Estimation (IQPE) and adiabatic evolution algorithms on Qiskit, with noiseless simulations and a custom noise model for IBM backends.", "result": "Noiseless simulations match exact diagonalization results, but noise degrades accuracy, revealing hardware limitations.", "conclusion": "Quantum computing shows promise for material simulation, but noise in current devices poses significant challenges."}}
{"id": "2506.04840", "pdf": "https://arxiv.org/pdf/2506.04840", "abs": "https://arxiv.org/abs/2506.04840", "authors": ["Maolin Che", "Yimin Wei", "Chong Wu", "Hong Yan"], "title": "Efficient randomized algorithms for the fixed Tucker-rank problem of Tucker decomposition with adaptive shifts", "categories": ["math.NA", "cs.NA", "65F55, 68W20, 15A18, 15A69"], "comment": "41 pages, 43 figures", "summary": "Randomized numerical linear algebra is proved to bridge theoretical\nadvancements to offer scalable solutions for approximating tensor\ndecomposition. This paper introduces fast randomized algorithms for solving the\nfixed Tucker-rank problem of Tucker decomposition, through the integration of\nadaptive shifted power iterations. The proposed algorithms enhance randomized\nvariants of truncated high-order singular value decomposition (T-HOSVD) and\nsequentially T-HOSVD (ST-HOSVD) by incorporating dynamic shift strategies,\nwhich accelerate convergence by refining the singular value gap and reduce the\nnumber of required power iterations while maintaining accuracy. Theoretical\nanalyses provide probabilistic error bounds, demonstrating that the proposed\nmethods achieve comparable or superior accuracy compared to deterministic\napproaches. Numerical experiments on synthetic and real-world datasets validate\nthe efficiency and robustness of the proposed algorithms, showing a significant\ndecline in runtime and approximation error over state-of-the-art techniques.", "AI": {"tldr": "The paper introduces fast randomized algorithms for Tucker decomposition using adaptive shifted power iterations, improving efficiency and accuracy over existing methods.", "motivation": "To bridge theoretical advancements with scalable solutions for tensor decomposition, addressing the fixed Tucker-rank problem.", "method": "Integration of adaptive shifted power iterations into randomized variants of T-HOSVD and ST-HOSVD, enhancing convergence and reducing power iterations.", "result": "Theoretical error bounds show comparable or superior accuracy to deterministic methods, with numerical experiments confirming efficiency and robustness.", "conclusion": "The proposed algorithms significantly reduce runtime and approximation error, outperforming state-of-the-art techniques."}}
{"id": "2506.05105", "pdf": "https://arxiv.org/pdf/2506.05105", "abs": "https://arxiv.org/abs/2506.05105", "authors": ["Fang-Cheng Wang", "Qi-Jun Ye", "Yu-Cheng Zhu", "Xin-Zheng Li"], "title": "Classification and enumeration of solid-solid phase transition mechanisms", "categories": ["cond-mat.mtrl-sci", "physics.comp-ph"], "comment": "22 pages, 14 figures", "summary": "Crystal-structure match (CSM), the atom-to-atom correspondence between two\ncrystalline phases, is used extensively to describe solid-solid phase\ntransition (SSPT) mechanisms. However, existing computational methods cannot\naccount for all possible CSMs. Here, we propose a formalism to classify all\nCSMs into a tree structure, which is independent of the choices of unit cell\nand supercell. We rigorously proved that only a finite number of noncongruent\nCSMs are of practical interest. By representing CSMs as integer matrices, we\nintroduce the crystmatch method to exhaustively enumerate them, which\nuncontroversially solves the CSM optimization problem under any geometric\ncriterion. For most SSPTs, crystmatch can reproduce all known deformation\nmechanisms and CSMs within 10 CPU minutes, while also revealing thousands of\nnew candidates. The resulting database can be further used for comparing\nexperimental phenomena, high-throughput energy barrier calculations, or machine\nlearning.", "AI": {"tldr": "A new formalism classifies all crystal-structure matches (CSMs) into a tree structure, solving the CSM optimization problem and revealing new candidates for solid-solid phase transitions.", "motivation": "Existing methods cannot account for all possible CSMs, limiting the understanding of solid-solid phase transition mechanisms.", "method": "The crystmatch method represents CSMs as integer matrices and exhaustively enumerates them, independent of unit cell choices.", "result": "Crystmatch reproduces known deformation mechanisms and reveals thousands of new CSM candidates within 10 CPU minutes.", "conclusion": "The method provides a comprehensive database for analyzing phase transitions, enabling applications in high-throughput calculations and machine learning."}}
{"id": "2506.04857", "pdf": "https://arxiv.org/pdf/2506.04857", "abs": "https://arxiv.org/abs/2506.04857", "authors": ["Junming Duan", "Praveen Chandrashekar", "Christian Klingenberg"], "title": "Active flux for ideal magnetohydrodynamics: A positivity-preserving scheme with the Godunov-Powell source term", "categories": ["math.NA", "cs.NA"], "comment": "27 pages, 12 figures", "summary": "The Active Flux (AF) is a compact, high-order finite volume scheme that\nallows more flexibility by introducing additional point value degrees of\nfreedom at cell interfaces. This paper proposes a positivity-preserving (PP) AF\nscheme for solving the ideal magnetohydrodynamics, where the Godunov-Powell\nsource term is employed to deal with the divergence-free constraint. For the\nevolution of the cell average, apart from the standard conservative finite\nvolume method for the flux derivative, the nonconservative source term is built\non the quadratic reconstruction in each cell, which maintains the compact\nstencil in the AF scheme. For the point value update, the local Lax-Friedrichs\n(LLF) flux vector splitting is adopted for the flux derivative, originally\nproposed in [Duan, Barsukow, and Klingenberg, SIAM Journal on Scientific\nComputing, 47(2), A811--A837, 2025], and a central difference is used to\ndiscretize the divergence in the source term. A parametrized flux limiter and a\nscaling limiter are presented to preserve the density and pressure positivity\nby blending the AF scheme with the first-order PP LLF scheme with the source\nterm. To suppress oscillations, a new shock sensor considering the divergence\nerror is proposed, which is used to compute the blending coefficients for the\ncell average. Several numerical tests are conducted to verify the third-order\naccuracy, PP property, and shock-capturing ability of the scheme. The key role\nof the Godunov-Powell source term and its suitable discretization in\ncontrolling divergence error is also validated.", "AI": {"tldr": "The paper introduces a positivity-preserving Active Flux (AF) scheme for ideal magnetohydrodynamics, using the Godunov-Powell source term for divergence-free constraints. It combines conservative and nonconservative methods, employs limiters for positivity, and validates third-order accuracy and shock-capturing.", "motivation": "To develop a high-order, compact finite volume scheme for ideal magnetohydrodynamics that ensures positivity of density and pressure while handling divergence-free constraints effectively.", "method": "The AF scheme uses point values at cell interfaces, a quadratic reconstruction for nonconservative terms, and LLF flux vector splitting for point value updates. Positivity is ensured via parametrized and scaling limiters, and a new shock sensor controls oscillations.", "result": "Numerical tests confirm third-order accuracy, positivity preservation, and effective shock-capturing. The Godunov-Powell source term's role in controlling divergence error is validated.", "conclusion": "The proposed PP AF scheme is effective for ideal magnetohydrodynamics, balancing high-order accuracy, positivity, and divergence control."}}
{"id": "2506.05122", "pdf": "https://arxiv.org/pdf/2506.05122", "abs": "https://arxiv.org/abs/2506.05122", "authors": ["Shaziya A. Banu", "Venkata R. S. B. Varanasi", "Arash Noshadravan", "Sara Abedi"], "title": "Reactive Transport Simulation of Silicate-Rich Shale Rocks when Exposed to CO2 Saturated Brine Under High Pressure and High Temperature", "categories": ["physics.geo-ph", "physics.comp-ph"], "comment": null, "summary": "This study examines the feasibility of carbon dioxide storage in shale rocks\nand the reliability of reactive transport models in achieving accurate\nreplication of the chemo-mechanical interactions and transport processes\ntranspiring in these rocks when subjected to CO2 saturated brine. Owing to the\nheterogeneity of rocks, experimental testing for adequate deductions and\nfindings, could be an expensive and time-intensive process. Therefore, this\nstudy proposes utilization of reactive transport modeling to replicate the\npore-scale chemo-mechanical reactions and transport processes occurring in\nsilicate-rich shale rocks in the presence of CO2 saturated brine under high\npressure and high temperature. For this study, Crunch Tope has been adopted to\nsimulate a one-dimensional reactive transport model of a Permian rock specimen\nexposed to the acidic brine at a temperature of 100 {\\deg}C and pressure of\n12.40 MPa (1800 psi) for a period of 14 and 28 days. The results demonstrated\nsignificant dissolution followed by precipitation of quartz rich phases,\nprecipitation and swelling of clay rich phases, and dissolution of feldspar\nrich phases closer to the acidic brine-rock interface. Moreover, porosity\nagainst reaction depth curve showed nearly 1.00% mineral precipitation occur at\n14 and 28 days, which is insufficient to completely fill the pore spaces.", "AI": {"tldr": "The study explores CO2 storage in shale rocks using reactive transport modeling to simulate chemo-mechanical interactions, avoiding costly experiments. Results show mineral dissolution and precipitation but insufficient pore-filling.", "motivation": "To address the high cost and time of experimental testing for CO2 storage in heterogeneous shale rocks by using reactive transport modeling.", "method": "Used Crunch Tope for 1D reactive transport modeling of a Permian rock specimen under high pressure/temperature with CO2 saturated brine for 14/28 days.", "result": "Significant mineral dissolution/precipitation, especially near the brine-rock interface, but only 1.00% porosity change, insufficient for pore-filling.", "conclusion": "Reactive transport modeling is feasible for studying CO2 storage in shale, but mineral precipitation alone may not fully seal pores."}}
{"id": "2506.04880", "pdf": "https://arxiv.org/pdf/2506.04880", "abs": "https://arxiv.org/abs/2506.04880", "authors": ["Heiko Gimperlein", "Ruma R. Maity"], "title": "Numerical analysis for constrained and unconstrained Q-tensor energies for liquid crystals", "categories": ["math.NA", "cs.NA", "65N12, 35J47, 65N30, 76A15"], "comment": null, "summary": "This paper introduces a comprehensive finite element approximation framework\nfor three-dimensional Landau-de Gennes $Q$-tensor energies for nematic liquid\ncrystals, with a particular focus on the anisotropy of the elastic energy and\nthe Ball-Majumdar singular potential. This potential imposes essential physical\nconstraints on the eigenvalues of the $Q$-tensor, ensuring realistic modeling.\nWe address the approximation of regular solutions to nonlinear elliptic partial\ndifferential equations with non-homogeneous boundary conditions associated with\nLandau-de Gennes energies. The well-posedness of the discrete linearized\nproblem is rigorously demonstrated. The existence and local uniqueness of the\ndiscrete solution is derived using the Newton-Kantorovich theorem. Furthermore,\nwe demonstrate an optimal order convergence rate in the energy norm and discuss\nthe impact of eigenvalue constraints on the a priori error analysis.", "AI": {"tldr": "A finite element framework for 3D Landau-de Gennes Q-tensor energies in nematic liquid crystals, focusing on anisotropy and eigenvalue constraints.", "motivation": "To model realistic nematic liquid crystals by addressing anisotropy and physical constraints on Q-tensor eigenvalues.", "method": "Finite element approximation for nonlinear elliptic PDEs with non-homogeneous boundary conditions, using the Ball-Majumdar potential and Newton-Kantorovich theorem.", "result": "Proves well-posedness, existence, and local uniqueness of discrete solutions, with optimal convergence rates in energy norm.", "conclusion": "The framework effectively models nematic liquid crystals, with eigenvalue constraints impacting error analysis."}}
{"id": "2506.05292", "pdf": "https://arxiv.org/pdf/2506.05292", "abs": "https://arxiv.org/abs/2506.05292", "authors": ["Declan A. Norton", "Yuanzhao Zhang", "Michelle Girvan"], "title": "Learning Beyond Experience: Generalizing to Unseen State Space with Reservoir Computing", "categories": ["cs.LG", "math.DS", "nlin.CD", "physics.comp-ph"], "comment": "15 pages, 9 figures", "summary": "Machine learning techniques offer an effective approach to modeling dynamical\nsystems solely from observed data. However, without explicit structural priors\n-- built-in assumptions about the underlying dynamics -- these techniques\ntypically struggle to generalize to aspects of the dynamics that are poorly\nrepresented in the training data. Here, we demonstrate that reservoir computing\n-- a simple, efficient, and versatile machine learning framework often used for\ndata-driven modeling of dynamical systems -- can generalize to unexplored\nregions of state space without explicit structural priors. First, we describe a\nmultiple-trajectory training scheme for reservoir computers that supports\ntraining across a collection of disjoint time series, enabling effective use of\navailable training data. Then, applying this training scheme to multistable\ndynamical systems, we show that RCs trained on trajectories from a single basin\nof attraction can achieve out-of-domain generalization by capturing system\nbehavior in entirely unobserved basins.", "AI": {"tldr": "Reservoir computing can generalize to unexplored state space regions without explicit structural priors, using a multiple-trajectory training scheme.", "motivation": "Machine learning struggles to generalize dynamics without structural priors; reservoir computing offers a solution.", "method": "Multiple-trajectory training scheme for reservoir computers, applied to multistable dynamical systems.", "result": "RCs trained on single basin trajectories generalize to unobserved basins.", "conclusion": "Reservoir computing enables out-of-domain generalization without explicit priors."}}
{"id": "2506.04969", "pdf": "https://arxiv.org/pdf/2506.04969", "abs": "https://arxiv.org/abs/2506.04969", "authors": ["Yema Paul", "Emmanuel Delande", "Francois Vinet", "Francois Laporte", "Manuel Sanjurjo-Rivo", "Aldo Tonnini", "Joan-Pau Sanchez"], "title": "Probability of Collision with Tethered Spacecraft", "categories": ["math.NA", "cs.NA"], "comment": "13 pages, 2 figures, Engineering Note", "summary": "This Engineering Note addresses the challenge of estimating the probability\nof collision for tethered spacecraft during close encounters with other space\nobjects. Standard probability of collision methods, based on spherical\nhard-body assumptions, tend to be overly conservative when applied to long\ntether systems. We introduce a method that accounts for the tether's spatial\nextent and configuration uncertainty by maximizing the probability of collision\nover all physically admissible tether shapes. Applied to real-world conjunction\nevents involving a kilometer-scale flexible inextensible tether, the method\nyields more realistic risk estimates. This approach improves the ability to\ndistinguish hazardous from benign encounters, thereby supporting more informed\ncollision avoidance decisions.", "AI": {"tldr": "A method for estimating collision probability for tethered spacecraft, improving accuracy by accounting for tether extent and shape uncertainty.", "motivation": "Standard collision probability methods are overly conservative for tethered systems due to spherical hard-body assumptions.", "method": "Maximizes collision probability over all physically admissible tether shapes, considering spatial extent and configuration uncertainty.", "result": "Applied to real-world events, the method provides more realistic risk estimates for kilometer-scale tethers.", "conclusion": "Enhances collision avoidance decisions by better distinguishing hazardous from benign encounters."}}
{"id": "2506.05174", "pdf": "https://arxiv.org/pdf/2506.05174", "abs": "https://arxiv.org/abs/2506.05174", "authors": ["Yifan Zhang", "Joe Kileel"], "title": "Norming Sets for Tensor and Polynomial Sketching", "categories": ["math.NA", "cs.IT", "cs.NA", "math.AG", "math.IT"], "comment": "16 pages", "summary": "This paper develops the sketching (i.e., randomized dimension reduction)\ntheory for real algebraic varieties and images of polynomial maps, including,\ne.g., the set of low rank tensors and tensor networks. Through the lens of\nnorming sets, we provide a framework for controlling the sketching dimension\nfor \\textit{any} sketch operator used to embed said sets, including\nsub-Gaussian, fast Johnson-Lindenstrauss, and tensor structured sketch\noperators. Leveraging norming set theory, we propose a new sketching method\ncalled the median sketch. It embeds such a set $V$ using only\n$\\widetilde{\\mathcal{O}}(\\dim V)$ tensor structured or sparse linear\nmeasurements.", "AI": {"tldr": "The paper introduces a sketching framework for real algebraic varieties and polynomial maps, proposing a new method called the median sketch that uses efficient measurements.", "motivation": "To develop a generalized theory for sketching (randomized dimension reduction) applicable to real algebraic varieties and polynomial maps, including low-rank tensors and tensor networks.", "method": "Uses norming set theory to control sketching dimensions for various sketch operators (sub-Gaussian, fast Johnson-Lindenstrauss, tensor structured). Introduces the median sketch method, which requires only ~O(dim V) measurements.", "result": "The median sketch efficiently embeds sets like low-rank tensors using sparse or tensor-structured linear measurements.", "conclusion": "The framework and median sketch provide a versatile and efficient approach to sketching for algebraic varieties and polynomial maps."}}
{"id": "2506.04279", "pdf": "https://arxiv.org/pdf/2506.04279", "abs": "https://arxiv.org/abs/2506.04279", "authors": ["Adama Nouboukpo", "Kodzo Michel Aladji", "Muktar Bappa"], "title": "Nombre Effectif de Partis Politiques en Afrique: Une Nouvelle M\u00e9thode pour un Calcul Objectif et Institutionnellement Neutre", "categories": ["physics.soc-ph", "cs.NA", "math.NA"], "comment": "in French language", "summary": "Political fragmentation in Africa poses to a significant challenge to\neffective governance and stability. Traditional measures of party system\nfragmentation, such as the Effective Number of Parties (ENP) index, often fail\nto capture the nuanced realities of African political landscapes, particularly\nthe influence of dominant parties, fluid party affiliations, and the impact of\nethnic and regional cleavages. To address these limitations, this paper\nintroduces two novel \"apolitical\" or \"institutionally neutral\" measures for\ncalculating the effective number of parties, focusing on geographical and\ndemographic dimensions, notably population size and territorial area. By\nincorporating these local realities and ensuring a minimum threshold of two\nparties, the proposed models offer a simpler and more contextually relevant\nframework for understanding political dynamics in Africa, especially in\ndata-scarce environments. This approach provides a valuable tool for analyzing\nand streamlining political systems, with potential for broader application\nbeyond the African context.", "AI": {"tldr": "The paper introduces two new measures for calculating political fragmentation in Africa, focusing on geographical and demographic factors, to address the limitations of traditional methods like the ENP index.", "motivation": "Traditional measures like the ENP index fail to capture the complexities of African political landscapes, such as dominant parties and ethnic cleavages.", "method": "The paper proposes two novel 'apolitical' measures based on population size and territorial area, ensuring a minimum of two parties.", "result": "The new models provide a simpler, contextually relevant framework for analyzing African political systems, even in data-scarce environments.", "conclusion": "This approach offers a valuable tool for understanding political dynamics in Africa and has potential for broader application."}}
{"id": "2506.04655", "pdf": "https://arxiv.org/pdf/2506.04655", "abs": "https://arxiv.org/abs/2506.04655", "authors": ["Mengjiao Bai", "Huaian Diao", "Weisheng Zhou"], "title": "Inverse elastic obstacle scattering problems by monotonicity method", "categories": ["math.AP", "cs.NA", "math.NA"], "comment": null, "summary": "We consider the elastic wave scattering problem involving rigid obstacles.\nThis work addresses the inverse problem of reconstructing the position and\nshape of such obstacles using far-field measurements. A novel\nmonotonicity-based approach is developed for this purpose. By factorizing the\nfar-field operator and utilizing the existence of localized wave functions, we\nderive a shape characterization criterion for the obstacle boundary. The\nproposed method employs monotonicity tests to determine the geometric\nrelationship between any given test domain and the actual scatterer. As a\nresult, the shape and location of rigid elastic obstacles can be uniquely\nidentified without requiring any initial guesses or prior knowledge of the\nphysical parameters of the homogeneous background medium.", "AI": {"tldr": "A novel monotonicity-based method is proposed for reconstructing rigid elastic obstacles from far-field measurements, uniquely identifying their shape and location without prior knowledge.", "motivation": "The study aims to solve the inverse problem of determining the position and shape of rigid obstacles in elastic wave scattering using far-field data.", "method": "The approach involves factorizing the far-field operator and using localized wave functions to derive a shape characterization criterion. Monotonicity tests are applied to relate test domains to the actual scatterer.", "result": "The method uniquely identifies the shape and location of obstacles without initial guesses or background medium parameters.", "conclusion": "The proposed monotonicity-based approach effectively reconstructs rigid elastic obstacles from far-field measurements, offering a robust solution without prior assumptions."}}
{"id": "2506.04498", "pdf": "https://arxiv.org/pdf/2506.04498", "abs": "https://arxiv.org/abs/2506.04498", "authors": ["Nguyen Thanh Tung", "Le Xuan Truong", "Tan Duc Do", "Nguyen Ngoc Trong"], "title": "Existence, uniqueness and blow-up estimates for a reaction-diffusion equation with $p(x,t)$-exponents", "categories": ["math.AP"], "comment": null, "summary": "Let $d \\in \\{3,4,5,\\ldots\\}$ and $\\Omega \\subset \\Ri^d$ be open bounded with\nLipschitz boundary.\n  Let $Q = \\Omega \\times (0,\\infty)$ and $p \\in C(\\overline{Q})$ be such that\n  \\[\n  2 < p^- \\le p(\\cdot) \\le p^+ < 2^* := \\frac{2d}{d-2},\n  \\]\n  where\n  $\n  p^- := \\essinf_{(x,t) \\in Q} p(x,t)\n  $\n  and\n  $\n  p^+ := \\esssup_{(x,t) \\in Q} p(x,t).\n  $\n  Consider the reaction-diffusion parabolic problem\n  \\[\n  (P) \\quad \\left\\{\\begin{array}{ll}\n  \\displaystyle\\frac{u_t}{|x|^2} - \\Delta u = k(t) \\, |u|^{p(x,t)-2}u & (x,t)\n\\in \\Omega \\times (0,T),\n  u(x,t) = 0, & (x,t) \\in \\partial \\Omega \\times (0,T), \\smallskip\n  u(x,0) = u_0(x), & x \\in \\Omega,\n  \\end{array}\\right.\n  \\]\n  where $T > 0$ and $0 \\ne u_0 \\in W^{1,2}_0(\\Omega)$.\n  We investigate the existence and uniqueness of a weak solution to $(P)$.\n  The upper and lower bounds on the blow-up time of the weak solution are also\nconsidered.", "AI": {"tldr": "The paper studies the existence, uniqueness, and blow-up time bounds of weak solutions to a reaction-diffusion parabolic problem with variable exponent $p(x,t)$ in a bounded domain.", "motivation": "The motivation is to analyze the behavior of solutions to a parabolic PDE with non-standard diffusion and reaction terms, particularly focusing on the variable exponent $p(x,t)$ and its impact on solution properties.", "method": "The authors consider a reaction-diffusion problem with a variable exponent $p(x,t)$ and use functional analysis techniques to investigate weak solutions. They derive bounds for the blow-up time of these solutions.", "result": "The paper establishes the existence and uniqueness of weak solutions under certain conditions on $p(x,t)$. It also provides upper and lower bounds for the blow-up time of the solutions.", "conclusion": "The study concludes that the variable exponent $p(x,t)$ significantly influences the solution's behavior, including its blow-up time, and provides explicit bounds for this phenomenon."}}
{"id": "2506.04578", "pdf": "https://arxiv.org/pdf/2506.04578", "abs": "https://arxiv.org/abs/2506.04578", "authors": ["Weiming Shen", "Yue Wang", "Tong Yang"], "title": "Structural stability of three dimensional steady Prandtl equation", "categories": ["math.AP"], "comment": null, "summary": "The well-posedness of the three dimensional Prandtl equation is an\noutstanding open problem due to the appearance of the secondary flow even\nthough there are studies on analytic and Gevrey function spaces. This problem\nis raised as the third open problem in the classical book by Oleinik and\nSamokhin [43]. This paper aims to address this open problem in the steady case\nby introducing a new approach to study the structural stability of background\nprofile that includes the famous Blasius solutions. The key observations\ninclude the introduction of some intrinsic vector fields and new versions of\nmaximum principle. In particular, we overcome the difficulties caused by\nsymmetry breaking through the analysis on the curvature-type quantities\ngenerated by commutators of the vector fields.", "AI": {"tldr": "This paper addresses the open problem of the well-posedness of the 3D Prandtl equation in the steady case, focusing on structural stability of background profiles like Blasius solutions, using intrinsic vector fields and new maximum principles.", "motivation": "The well-posedness of the 3D Prandtl equation is a major unsolved problem, highlighted as the third open problem in Oleinik and Samokhin's book. The study aims to tackle this in the steady case.", "method": "The authors introduce intrinsic vector fields and new versions of maximum principles, analyzing curvature-type quantities from commutators of these fields to address symmetry-breaking challenges.", "result": "The paper overcomes difficulties caused by symmetry breaking, providing insights into the structural stability of background profiles.", "conclusion": "The new approach offers progress on the open problem, particularly in the steady case, by leveraging intrinsic vector fields and curvature analysis."}}
{"id": "2506.04591", "pdf": "https://arxiv.org/pdf/2506.04591", "abs": "https://arxiv.org/abs/2506.04591", "authors": ["Weiming Shen", "Yue Wang"], "title": "Asymptotic behavior of complete conformal metric near singular boundary", "categories": ["math.AP", "math.DG"], "comment": "Published in Advances in Mathematics 458 (2024), Paper No. 109977, 34\n  pp", "summary": "The boundary behavior of the singular Yamabe problem has been extensively\nstudied near sufficiently smooth boundaries, while less is known about the\nasymptotic behavior of solutions near singular boundaries. In this paper, we\nstudy the asymptotic behaviors of solutions to the singular Yamabe problem with\nnegative constant scalar curvature near singular boundaries and derive the\noptimal estimates for the background metric which is not necessarily\nconformally flat. In particular, we prove that the solutions are well\napproximated by the solutions in tangent cones at singular points on the\nboundaries.", "AI": {"tldr": "The paper studies the asymptotic behavior of solutions to the singular Yamabe problem near singular boundaries, deriving optimal estimates for non-conformally flat metrics.", "motivation": "Little is known about the asymptotic behavior of solutions near singular boundaries, unlike smooth boundaries.", "method": "Analyze solutions to the singular Yamabe problem with negative constant scalar curvature near singular boundaries.", "result": "Solutions are well approximated by solutions in tangent cones at singular boundary points.", "conclusion": "The study provides optimal estimates for non-conformally flat metrics and clarifies the behavior near singular boundaries."}}
{"id": "2506.04237", "pdf": "https://arxiv.org/pdf/2506.04237", "abs": "https://arxiv.org/abs/2506.04237", "authors": ["Sanchit Sinha", "Aidong Zhang"], "title": "A Comprehensive Survey on the Risks and Limitations of Concept-based Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Concept-based Models are a class of inherently explainable networks that\nimprove upon standard Deep Neural Networks by providing a rationale behind\ntheir predictions using human-understandable `concepts'. With these models\nbeing highly successful in critical applications like medical diagnosis and\nfinancial risk prediction, there is a natural push toward their wider adoption\nin sensitive domains to instill greater trust among diverse stakeholders.\nHowever, recent research has uncovered significant limitations in the structure\nof such networks, their training procedure, underlying assumptions, and their\nsusceptibility to adversarial vulnerabilities. In particular, issues such as\nconcept leakage, entangled representations, and limited robustness to\nperturbations pose challenges to their reliability and generalization.\nAdditionally, the effectiveness of human interventions in these models remains\nan open question, raising concerns about their real-world applicability. In\nthis paper, we provide a comprehensive survey on the risks and limitations\nassociated with Concept-based Models. In particular, we focus on aggregating\ncommonly encountered challenges and the architecture choices mitigating these\nchallenges for Supervised and Unsupervised paradigms. We also examine recent\nadvances in improving their reliability and discuss open problems and promising\navenues of future research in this domain.", "AI": {"tldr": "A survey on the risks and limitations of Concept-based Models, highlighting challenges like concept leakage and adversarial vulnerabilities, and discussing mitigation strategies and future research directions.", "motivation": "To address the growing adoption of Concept-based Models in sensitive domains by identifying their limitations and improving their reliability and trustworthiness.", "method": "Comprehensive survey aggregating challenges and mitigation strategies for Supervised and Unsupervised paradigms, along with recent advances.", "result": "Identified key issues like concept leakage and robustness, and discussed architectural choices and interventions to mitigate them.", "conclusion": "While Concept-based Models show promise, significant challenges remain; future research should focus on improving their reliability and real-world applicability."}}
{"id": "2506.04691", "pdf": "https://arxiv.org/pdf/2506.04691", "abs": "https://arxiv.org/abs/2506.04691", "authors": ["Pascal B\u00e9gout", "Jesus Ildefonso Diaz"], "title": "Solutions with expanding compact support of saturated Schr{\u00f6}dinger equations: self-similar solutions", "categories": ["math.AP"], "comment": null, "summary": "We prove the existence of solutions \\(u(t,x)\\) of the Schr{\\\"o}dinger\nequation with a saturation nonlinear term \\((u/|u|)\\) having compact support,\nfor each \\(t>0,\\) that expands with a growth law of the type \\(C\\sqrt{t}\\). The\nprimary tool is considering the self-similar solution of the associated\nequation. For more information see\nhttps://ejde.math.txstate.edu/Volumes/2025/53/abstr.html", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2506.04241", "pdf": "https://arxiv.org/pdf/2506.04241", "abs": "https://arxiv.org/abs/2506.04241", "authors": ["Konstantin Kirchheim", "Frank Ortmeier"], "title": "Improving Out-of-Distribution Detection with Markov Logic Networks", "categories": ["cs.LG"], "comment": null, "summary": "Out-of-distribution (OOD) detection is essential for ensuring the reliability\nof deep learning models operating in open-world scenarios. Current OOD\ndetectors mainly rely on statistical models to identify unusual patterns in the\nlatent representations of a deep neural network. This work proposes to augment\nexisting OOD detectors with probabilistic reasoning, utilizing Markov logic\nnetworks (MLNs). MLNs connect first-order logic with probabilistic reasoning to\nassign probabilities to inputs based on weighted logical constraints defined\nover human-understandable concepts, which offers improved explainability.\nThrough extensive experiments on multiple datasets, we demonstrate that MLNs\ncan significantly enhance the performance of a wide range of existing OOD\ndetectors while maintaining computational efficiency. Furthermore, we introduce\na simple algorithm for learning logical constraints for OOD detection from a\ndataset and showcase its effectiveness.", "AI": {"tldr": "The paper proposes using Markov logic networks (MLNs) to enhance out-of-distribution (OOD) detection in deep learning models, improving explainability and performance.", "motivation": "Current OOD detectors rely on statistical models, lacking explainability. The work aims to integrate probabilistic reasoning with human-understandable concepts for better OOD detection.", "method": "The approach augments existing OOD detectors with MLNs, which combine first-order logic and probabilistic reasoning. A simple algorithm for learning logical constraints from datasets is also introduced.", "result": "Experiments show MLNs significantly improve OOD detection performance across multiple datasets while maintaining computational efficiency.", "conclusion": "MLNs enhance OOD detectors by providing explainable probabilistic reasoning, with demonstrated effectiveness and efficiency."}}
{"id": "2506.04769", "pdf": "https://arxiv.org/pdf/2506.04769", "abs": "https://arxiv.org/abs/2506.04769", "authors": ["Tomasz D\u0119biec", "Piotr Gwiazda", "B\u0142a\u017cej Miasojedow", "Katarzyna Ryszewska", "Zuzanna Szyma\u0144ska", "Aneta Wr\u00f3blewska-Kami\u0144ska"], "title": "Lipschitz stability for Bayesian inference in porous medium tissue growth models", "categories": ["math.AP", "35B30, 35B35, 35B45, 35K57, 35K65, 35Q92"], "comment": null, "summary": "We consider a macroscopic model for the dynamics of living tissues\nincorporating pressure-driven dispersal and pressure-modulated proliferation.\nGiven a power-law constitutive relation between the pressure and cell density,\nthe model can be written as a porous medium equation with a growth term. We\nprove Lipschitz continuity of the mild solutions of the model with respect to\nthe diffusion parameter (the exponent $\\gamma$ in the pressure-density law) in\nthe $L_1$ norm. While of independent analytical interest, our motivation for\nthis result is to provide a vital step towards using Bayesian inverse problem\nmethodology for parameter estimation based on experimental data -- such\nstability estimates are indispensable for applying sampling algorithms which\nrely on the gradient of the likelihood function.", "AI": {"tldr": "The paper analyzes a macroscopic model for living tissue dynamics, focusing on pressure-driven dispersal and proliferation. It proves Lipschitz continuity of solutions with respect to a diffusion parameter, aiding Bayesian parameter estimation.", "motivation": "The study aims to support Bayesian inverse problem methodology for parameter estimation in experimental data, requiring stability estimates for sampling algorithms.", "method": "The model uses a porous medium equation with a growth term, assuming a power-law relation between pressure and cell density. Lipschitz continuity of solutions is proven in the $L_1$ norm.", "result": "The paper establishes Lipschitz continuity of mild solutions with respect to the diffusion parameter, a key step for gradient-based sampling algorithms.", "conclusion": "The stability result is crucial for applying Bayesian methods to estimate model parameters from experimental data, enhancing the model's practical utility."}}
{"id": "2506.04243", "pdf": "https://arxiv.org/pdf/2506.04243", "abs": "https://arxiv.org/abs/2506.04243", "authors": ["Warayut Dokduea", "Weerachart Tangchirapat", "Sompote Youwai"], "title": "Triple Attention Transformer Architecture for Time-Dependent Concrete Creep Prediction", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "This paper presents a novel Triple Attention Transformer Architecture for\npredicting time-dependent concrete creep, addressing fundamental limitations in\ncurrent approaches that treat time as merely an input parameter rather than\nmodeling the sequential nature of deformation development. By transforming\nconcrete creep prediction into an autoregressive sequence modeling task similar\nto language processing, our architecture leverages the transformer's\nself-attention mechanisms to capture long-range dependencies in historical\ncreep patterns. The model implements a triple-stream attention framework\nincorporating temporal attention for sequential progression, feature attention\nfor material property interactions, and batch attention for inter-sample\nrelationships. Evaluated on experimental datasets with standardized daily\nmeasurements spanning 160 days, the architecture achieves exceptional\nperformance with mean absolute percentage error of 1.63% and R2 values of 0.999\nacross all datasets, substantially outperforming traditional empirical models\nand existing machine learning approaches. Ablation studies confirm the critical\nrole of attention mechanisms, with attention pooling contributing most\nsignificantly to model performance. SHAP analysis reveals Young's modulus as\nthe primary predictive feature, followed by density and compressive strength,\nproviding interpretability essential for engineering applications. A deployed\nweb-based interface facilitates practical implementation, enabling real-time\npredictions using standard laboratory parameters. This work establishes the\nviability of applying transformer architectures to materials science problems,\ndemonstrating the potential for data-driven approaches to revolutionize\nstructural behavior prediction and engineering design practices.", "AI": {"tldr": "A novel Triple Attention Transformer Architecture is introduced for predicting concrete creep, outperforming traditional methods with high accuracy and interpretability.", "motivation": "Current approaches fail to model the sequential nature of concrete creep, treating time as a static input. This limits accuracy and practical applicability.", "method": "The architecture uses a triple-stream attention framework (temporal, feature, and batch attention) within a transformer to model creep as an autoregressive sequence task.", "result": "Achieves 1.63% mean absolute percentage error and R2 of 0.999, with Young's modulus identified as the most predictive feature.", "conclusion": "The work demonstrates transformers' potential in materials science, offering a data-driven revolution in structural behavior prediction."}}
{"id": "2506.04927", "pdf": "https://arxiv.org/pdf/2506.04927", "abs": "https://arxiv.org/abs/2506.04927", "authors": ["Petru Jebelean", "Jean Mawhin", "Calin Serban"], "title": "Periodic solutions for p(t)-Lienard equations with a singular nonlinearity of attractive type", "categories": ["math.AP", "math.CA"], "comment": null, "summary": "We are concerned with the existence of $T$-periodic solutions to an equation\nof type $$\\left (|u'(t))|^{p(t)-2} u'(t) \\right\n)'+f(u(t))u'(t)+g(u(t))=h(t)\\quad \\mbox{ in }[0,T]$$ where\n$p:[0,T]\\to(1,\\infty)$ with $p(0)=p(T)$ and $h$ are continuous on $[0,T]$,\n$f,g$ are also continuous on $[0,\\infty)$, respectively $(0,\\infty)$. The\nmapping $g$ may have an attractive singularity (i.e. $g(x) \\to +\\infty$ as\n$x\\to 0+$). Our approach relies on a continuation theorem obtained in the\nrecent paper M. Garc\\'{i}a-Huidobro, R. Man\\'{a}sevich, J. Mawhin and S.\nTanaka, J. Differential Equations (2024), a priori estimates and method of\nlower and upper solutions.", "AI": {"tldr": "The paper investigates the existence of T-periodic solutions for a singular differential equation with a p(t)-Laplacian operator, using continuation theorems and a priori estimates.", "motivation": "The study is motivated by the need to understand periodic solutions in singular differential equations, particularly those with attractive singularities, which have applications in physics and engineering.", "method": "The approach combines a continuation theorem from prior work, a priori estimates, and the method of lower and upper solutions to analyze the equation.", "result": "The paper establishes conditions under which T-periodic solutions exist for the given singular differential equation.", "conclusion": "The findings confirm the existence of periodic solutions for the equation, leveraging analytical tools and prior theoretical results."}}
{"id": "2506.04250", "pdf": "https://arxiv.org/pdf/2506.04250", "abs": "https://arxiv.org/abs/2506.04250", "authors": ["Shaona Ghosh", "Amrita Bhattacharjee", "Yftah Ziser", "Christopher Parisien"], "title": "SafeSteer: Interpretable Safety Steering with Refusal-Evasion in LLMs", "categories": ["cs.LG"], "comment": "arXiv admin note: text overlap with arXiv:2410.01174", "summary": "Fine-tuning large language models (LLMs) to adapt to evolving safety policies\nis costly and impractical. Mechanistic interpretability enables inference-time\ncontrol through latent activation steering, yet its potential for precise,\ncustomizable safety adjustments remains largely untapped. This paper\ninvestigates an approach called SafeSteer for guiding the outputs of LLMs by:\n(i) leveraging category-specific steering vectors for more precise control,\n(ii) employing a simple, gradient-free unsupervised method to enhance safety\nsteering while preserving text quality, topic relevance, and without explicit\nrefusal, and (iii) accomplishing this without a hard requirement of contrastive\npairwise safe data. We also highlight that our method, being simple and\neffective, aligns with recent studies suggesting that simple techniques often\noutperform more complex ones in activation steering. We showcase the\neffectiveness of our approach across various LLMs, datasets, and risk\ncategories, demonstrating its ability to provide precise control, prevent\nblanket refusals, and guide models toward generating safe content while\nmaintaining topic relevance.", "AI": {"tldr": "SafeSteer is a method for fine-tuning LLMs to improve safety without costly retraining, using unsupervised activation steering for precise control while maintaining text quality.", "motivation": "Fine-tuning LLMs for safety is impractical due to cost. SafeSteer aims to provide a simpler, effective alternative for real-time safety adjustments.", "method": "SafeSteer uses category-specific steering vectors and a gradient-free unsupervised approach to guide LLM outputs without needing contrastive safe data.", "result": "The method effectively controls LLM outputs, prevents blanket refusals, and maintains topic relevance across various models and datasets.", "conclusion": "SafeSteer offers a practical, simple solution for real-time safety adjustments in LLMs, outperforming complex methods."}}
{"id": "2506.04967", "pdf": "https://arxiv.org/pdf/2506.04967", "abs": "https://arxiv.org/abs/2506.04967", "authors": ["Claudianor O. Alves", "Rui Ding", "Chao Ji"], "title": "Existence and multiplicity of normalized solutions for the generalized Kadomtsev-Petviashvili equation in $\\mathbb{R}^2$", "categories": ["math.AP", "35A15, 35A18"], "comment": "26 pages", "summary": "In this paper, we study the existence and {multiplicity} of nontrivial\nsolitary waves for the generalized Kadomtsev-Petviashvili equation with\nprescribed {$L^2$-norm} \\begin{equation*}\\label{Equation1}\n  \\left\\{\\begin{array}{l}\n  \\left(-u_{x x}+D_x^{-2} u_{y y}+\\lambda u-f(u)\\right)_x=0,{\\quad x \\in\n\\mathbb{R}^2, } \\\\[10pt]\n  \\displaystyle \\int_{\\mathbb{R}^2}u^2 d x=a^2,\n  \\end{array}\\right.%\\tag{$\\mathscr E_\\lambda$} \\end{equation*} where $a>0$ and\n$\\lambda \\in \\mathbb{R}$ is an unknown parameter that appears as a Lagrange\nmultiplier. For the case $f(t)=|t|^{q-2}t$, with $2<q<\\frac{10}{3}$\n($L^2$-subcritical case) and $\\frac{10}{3}<q<6$ ($L^2$-supercritical case), we\nestablish the existence of normalized ground state solutions for the above\nequation. Moreover, when $f(t)=\\mu|t|^{q-2}t+|t|^{p-2}t$, with\n$2<q<\\frac{10}{3}<p<6$ and $\\mu>0$, we prove the existence of normalized ground\nstate solutions which corresponds to a local minimum of the associated energy\nfunctional. In this case, we further show that there exists a sequence $(a_n)\n\\subset (0,a_0)$ with $a_n \\to 0$ as $n \\to+\\infty$, such that for each\n$a=a_n$, the problem admits a second solution with positive energy. To the best\nof our knowledge, this is the first work that studies the existence of\nsolutions for the generalized Kadomtsev-Petviashvili equations under the\n$L^2$-constraint, which we refer to them as the normalized solutions.", "AI": {"tldr": "The paper investigates the existence and multiplicity of nontrivial solitary waves for the generalized Kadomtsev-Petviashvili equation under an L\u00b2-norm constraint, proving normalized ground state solutions for subcritical and supercritical cases, and a second solution for specific conditions.", "motivation": "To address the lack of research on normalized solutions for the generalized Kadomtsev-Petviashvili equation under L\u00b2-constraints, exploring existence and multiplicity of solutions.", "method": "Analyzes the equation with prescribed L\u00b2-norm, using Lagrange multipliers and energy functional methods to establish solutions for subcritical and supercritical cases, and identifies a second solution under specific conditions.", "result": "Existence of normalized ground state solutions proven for subcritical and supercritical cases. A second solution with positive energy identified for a sequence of norms approaching zero.", "conclusion": "The study successfully establishes the existence of normalized solutions for the generalized Kadomtsev-Petviashvili equation, filling a gap in the literature and providing new insights into the problem."}}
{"id": "2506.04254", "pdf": "https://arxiv.org/pdf/2506.04254", "abs": "https://arxiv.org/abs/2506.04254", "authors": ["Nicolas Caron", "Christophe Guyeux", "Hassan Noura", "Benjamin Aynes"], "title": "Localized Forest Fire Risk Prediction: A Department-Aware Approach for Operational Decision Support", "categories": ["cs.LG", "cs.AI"], "comment": "10 pages, 7 figures, 3 tables, submitted to ECAI2025", "summary": "Forest fire prediction involves estimating the likelihood of fire ignition or\nrelated risk levels in a specific area over a defined time period. With climate\nchange intensifying fire behavior and frequency, accurate prediction has become\none of the most pressing challenges in Artificial Intelligence (AI).\nTraditionally, fire ignition is approached as a binary classification task in\nthe literature. However, this formulation oversimplifies the problem,\nespecially from the perspective of end-users such as firefighters. In general,\nas is the case in France, firefighting units are organized by department, each\nwith its terrain, climate conditions, and historical experience with fire\nevents. Consequently, fire risk should be modeled in a way that is sensitive to\nlocal conditions and does not assume uniform risk across all regions. This\npaper proposes a new approach that tailors fire risk assessment to departmental\ncontexts, offering more actionable and region-specific predictions for\noperational use. With this, we present the first national-scale AI benchmark\nfor metropolitan France using state-of-the-art AI models on a relatively\nunexplored dataset. Finally, we offer a summary of important future works that\nshould be taken into account. Supplementary materials are available on GitHub.", "AI": {"tldr": "The paper proposes a localized approach to forest fire prediction, addressing the oversimplification of traditional binary classification by tailoring risk assessment to departmental contexts in France.", "motivation": "Climate change has intensified forest fires, making accurate prediction a critical AI challenge. Traditional binary classification oversimplifies the problem, especially for end-users like firefighters who need region-specific insights.", "method": "The paper introduces a new approach that models fire risk based on local departmental conditions, using state-of-the-art AI models on an unexplored dataset for metropolitan France.", "result": "The study presents the first national-scale AI benchmark for forest fire prediction in France, offering actionable, region-specific risk assessments.", "conclusion": "The paper highlights the need for localized fire risk models and suggests future research directions, with supplementary materials available on GitHub."}}
{"id": "2506.05034", "pdf": "https://arxiv.org/pdf/2506.05034", "abs": "https://arxiv.org/abs/2506.05034", "authors": ["Boquan Fan", "Yuchen Wang", "Weicheng Zhan"], "title": "Remarks on radial symmetry of stationary and uniformly-rotating solutions for the 2D Euler equation", "categories": ["math.AP"], "comment": "arXiv admin note: text overlap with arXiv:2412.05973", "summary": "We prove that any uniformly rotating solution of the 2D incompressible Euler\nequation with compactly supported vorticity $\\omega$ must be radially symmetric\nwhenever its angular velocity satisfies $\\Omega \\in (-\\infty,\\inf \\omega / 2]\n\\cup \\, [ \\sup \\omega / 2, +\\infty )$, in both the patch and smooth settings.\nThis result extends the rigidity theorems established in\n\\cite{Gom2021MR4312192} (\\textit{Duke Math. J.},170(13):2957-3038, 2021), which\nwere confined to the case of non-positive angular velocities and non-negative\nvorticity. Moreover, our results do not impose any regularity conditions on the\npatch beyond requiring that its boundary consists of Jordan curves, thereby\nrefining the previous result to encompass irregular vortex patches.", "AI": {"tldr": "The paper proves radial symmetry for uniformly rotating solutions of the 2D Euler equation with compact vorticity under specific angular velocity conditions, extending prior work to include irregular patches.", "motivation": "To generalize earlier rigidity theorems by removing restrictions on angular velocity and vorticity sign, and to include irregular vortex patches.", "method": "Analyzes uniformly rotating solutions of the 2D incompressible Euler equation, focusing on compactly supported vorticity and specific angular velocity ranges.", "result": "Shows that such solutions must be radially symmetric under the given conditions, even for irregular patches.", "conclusion": "Extends previous results by relaxing constraints and broadening applicability to irregular cases."}}
{"id": "2506.04268", "pdf": "https://arxiv.org/pdf/2506.04268", "abs": "https://arxiv.org/abs/2506.04268", "authors": ["Jingyang Li", "Guoqiang Li"], "title": "MUC-G4: Minimal Unsat Core-Guided Incremental Verification for Deep Neural Network Compression", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The rapid development of deep learning has led to challenges in deploying\nneural networks on edge devices, mainly due to their high memory and runtime\ncomplexity. Network compression techniques, such as quantization and pruning,\naim to reduce this complexity while maintaining accuracy. However, existing\nincremental verification methods often focus only on quantization and struggle\nwith structural changes. This paper presents MUC-G4 (Minimal Unsat Core-Guided\nIncremental Verification), a novel framework for incremental verification of\ncompressed deep neural networks. It encodes both the original and compressed\nnetworks into SMT formulas, classifies changes, and use \\emph{Minimal Unsat\nCores (MUCs)} from the original network to guide efficient verification for the\ncompressed network. Experimental results show its effectiveness in handling\nquantization and pruning, with high proof reuse rates and significant speedup\nin verification time compared to traditional methods. MUC-G4 hence offers a\npromising solution for ensuring the safety and reliability of compressed neural\nnetworks in practical applications.", "AI": {"tldr": "MUC-G4 is a framework for incremental verification of compressed neural networks, using Minimal Unsat Cores to guide efficient verification, outperforming traditional methods.", "motivation": "Challenges in deploying neural networks on edge devices due to high complexity, and limitations of existing verification methods focusing only on quantization.", "method": "Encodes original and compressed networks into SMT formulas, classifies changes, and uses Minimal Unsat Cores from the original network to guide verification.", "result": "Effective in handling quantization and pruning, with high proof reuse rates and significant speedup in verification time.", "conclusion": "MUC-G4 provides a promising solution for ensuring safety and reliability of compressed neural networks in practical applications."}}
{"id": "2506.05060", "pdf": "https://arxiv.org/pdf/2506.05060", "abs": "https://arxiv.org/abs/2506.05060", "authors": ["Adam Grzela", "Katarzyna Mazowiecka"], "title": "Existence of infinitely many homotopy classes from $\\mathbb S^3$ to $\\mathbb S^2$ having a minimimzing $W^{s,\\frac 3s}$-harmonic map", "categories": ["math.AP"], "comment": null, "summary": "In 1998 T. Rivi\\`{e}re proved that there exist infinitely many homotopy\nclasses of $\\pi_3(\\mathbb S^2)$ having a minimizing 3-harmonic map. This result\nis especially surprising taking into account that in $\\pi_3(\\mathbb S^3)$ there\nare only three homotopy classes (corresponding to the degrees $\\{-1,0,1\\}$) in\nwhich a minimizer exists.\n  We extend this theorem in the framework of fractional harmonic maps and prove\nthat for $s\\in(0,1)$ there exist infinitely many homotopy classes of\n$\\pi_{3}(\\mathbb S^{2})$ in which there is a minimizing\n$W^{s,\\frac{3}{s}}$-harmonic map.", "AI": {"tldr": "The paper extends T. Rivi\u00e8re's 1998 result on minimizing 3-harmonic maps in \u03c0\u2083(S\u00b2) to fractional harmonic maps, showing infinitely many homotopy classes with minimizers for s \u2208 (0,1).", "motivation": "To generalize the surprising result of infinitely many minimizing homotopy classes in \u03c0\u2083(S\u00b2) to the fractional setting, contrasting with the limited cases in \u03c0\u2083(S\u00b3).", "method": "Extends the framework to fractional harmonic maps, specifically W^{s,3/s}-harmonic maps, for s \u2208 (0,1).", "result": "Proves existence of infinitely many homotopy classes in \u03c0\u2083(S\u00b2) with minimizing fractional harmonic maps for s \u2208 (0,1).", "conclusion": "The study successfully generalizes Rivi\u00e8re's theorem to fractional harmonic maps, revealing rich structure in \u03c0\u2083(S\u00b2)."}}
{"id": "2506.04272", "pdf": "https://arxiv.org/pdf/2506.04272", "abs": "https://arxiv.org/abs/2506.04272", "authors": ["Kyung Rok Kim", "Yumo Bai", "Chonghuan Wang", "Guanting Chen"], "title": "Understanding the Impact of Sampling Quality in Direct Preference Optimization", "categories": ["cs.LG"], "comment": "Submitted to NeurIPS2025", "summary": "We study the role of the sampling distribution in Direct Preference\nOptimization (DPO) and aim to understand its impact on DPO's training dynamics.\nOur analyses show that both the solution space and the convergence behavior of\nDPO depend on the support and quality of the generating distribution. We first\nanalyze how distribution of responses influences policy updates during gradient\ndescent, drawing connections to common phenomena found in practice. We then\ndesign a simplified yet well-structured alignment model as a proxy, and develop\nquantitative results showing how more frequent high-quality responses amplify\nthe gradient signal and improve the optimization landscape, leading to more\neffective policy learning. Our theoretical findings are supported by empirical\nexperiments and provide a principled justification for the online DPO framework\nin practice.", "AI": {"tldr": "The paper investigates how the sampling distribution affects Direct Preference Optimization (DPO), showing its impact on training dynamics, solution space, and convergence. High-quality responses enhance gradient signals and improve policy learning.", "motivation": "To understand the influence of the sampling distribution on DPO's training dynamics and optimization landscape.", "method": "Analyzed response distribution effects on policy updates, designed a simplified alignment model, and conducted empirical experiments.", "result": "Frequent high-quality responses amplify gradient signals and improve optimization, leading to more effective policy learning.", "conclusion": "The findings support the practical use of online DPO by justifying its effectiveness through theoretical and empirical evidence."}}
{"id": "2506.05149", "pdf": "https://arxiv.org/pdf/2506.05149", "abs": "https://arxiv.org/abs/2506.05149", "authors": ["Louise Gassot", "Thierry Laurens"], "title": "Global well-posedness for the ILW equation in $H^s(\\mathbb{T})$ for $s>-\\frac12$", "categories": ["math.AP"], "comment": null, "summary": "We prove that the intermediate long wave (ILW) equation is globally\nwell-posed in the Sobolev spaces $H^s(\\mathbb{T})$ for $s > -\\frac12$. The\nprevious record for well-posedness was $s\\geq 0$, and the system is known to be\nill-posed for $s<-\\frac12$. We then demonstrate that the solutions of ILW\nconverge to those of the Benjamin--Ono equation in $H^s(\\mathbb{T})$ in the\ninfinite-depth limit.\n  Our methods do not rely on the complete integrability of ILW, but rather\ntreat ILW as a perturbation of the Benjamin--Ono equation by a linear term of\norder zero. To highlight this, we establish a general well-posedness result for\nsuch perturbations, which also applies to the Smith equation for\ncontinental-shelf waves.", "AI": {"tldr": "The paper proves global well-posedness of the ILW equation in Sobolev spaces for $s > -\\frac12$, improves the previous record ($s\\geq 0$), and shows convergence to the Benjamin--Ono equation in the infinite-depth limit.", "motivation": "To extend the well-posedness theory of the ILW equation beyond the known threshold ($s\\geq 0$) and explore its relationship with the Benjamin--Ono equation.", "method": "Treats ILW as a perturbation of the Benjamin--Ono equation by a linear term of order zero, avoiding reliance on integrability. A general well-posedness result for such perturbations is established.", "result": "Global well-posedness is achieved for $s > -\\frac12$, and solutions converge to the Benjamin--Ono equation in the infinite-depth limit.", "conclusion": "The approach provides a framework for analyzing perturbations of integrable systems, with applications to other equations like the Smith equation."}}
{"id": "2506.04281", "pdf": "https://arxiv.org/pdf/2506.04281", "abs": "https://arxiv.org/abs/2506.04281", "authors": ["Xu Zheng", "Chaohao Lin", "Sipeng Chen", "Zhuomin Chen", "Jimeng Shi", "Wei Cheng", "Jayantha Obeysekera", "Jason Liu", "Dongsheng Luo"], "title": "SF$^2$Bench: Evaluating Data-Driven Models for Compound Flood Forecasting in South Florida", "categories": ["cs.LG"], "comment": "60 Pages", "summary": "Forecasting compound floods presents a significant challenge due to the\nintricate interplay of meteorological, hydrological, and oceanographic factors.\nAnalyzing compound floods has become more critical as the global climate\nincreases flood risks. Traditional physics-based methods, such as the\nHydrologic Engineering Center's River Analysis System, are often\ntime-inefficient. Machine learning has recently demonstrated promise in both\nmodeling accuracy and computational efficiency. However, the scarcity of\ncomprehensive datasets currently hinders systematic analysis. Existing\nwater-related datasets are often limited by a sparse network of monitoring\nstations and incomplete coverage of relevant factors. To address this\nchallenge, we introduce SF2Bench, a comprehensive time series collection on\ncompound floods in South Florida, which integrates four key factors: tide,\nrainfall, groundwater, and human management activities (gate and pump\ncontrolling). This integration allows for a more detailed analysis of the\nindividual contributions of these drivers to compound flooding and informs the\ndevelopment of improved flood forecasting approaches. To comprehensively\nevaluate the potential of various modeling paradigms, we assess the performance\nof six categories of methods, encompassing Multilayer Perceptrons,\nConvolutional Neural Networks, Recurrent Neural Networks, Graph Neural\nNetworks, Transformers, and Large Language Models. We verified the impact of\ndifferent key features on flood forecasting through experiments. Our analysis\nexamines temporal and spatial aspects, providing insights into the influence of\nhistorical data and spatial dependencies. The varying performance across these\napproaches underscores the diverse capabilities of each in capturing complex\ntemporal and spatial dependencies inherent in compound floods.", "AI": {"tldr": "The paper introduces SF2Bench, a dataset for compound flood analysis in South Florida, and evaluates six machine learning methods for flood forecasting, highlighting their strengths in capturing temporal and spatial dependencies.", "motivation": "Compound flood forecasting is complex due to multiple interacting factors, and traditional methods are inefficient. Machine learning offers potential, but data scarcity limits progress.", "method": "The study introduces SF2Bench, integrating tide, rainfall, groundwater, and human management data. It evaluates six ML methods (MLPs, CNNs, RNNs, GNNs, Transformers, LLMs) for flood forecasting.", "result": "Experiments show varying performance across methods, revealing their diverse capabilities in modeling temporal and spatial dependencies in compound floods.", "conclusion": "SF2Bench enables detailed analysis of flood drivers, and the evaluation of ML methods provides insights for improving flood forecasting approaches."}}
{"id": "2506.05151", "pdf": "https://arxiv.org/pdf/2506.05151", "abs": "https://arxiv.org/abs/2506.05151", "authors": ["Farhan Abedin", "Giulio Tralli"], "title": "Boundary regularity for subelliptic equations in the Heisenberg group", "categories": ["math.AP", "35J70, 35R05, 35H20, 35B45"], "comment": "35 pages, comments welcome", "summary": "We prove boundary H\\\"older and Lipschitz regularity for a class of degenerate\nelliptic, second order, inhomogeneous equations in non-divergence form\nstructured on the left-invariant vector fields of the Heisenberg group. Our\nfocus is on the case of operators with bounded and measurable coefficients and\nbounded right-hand side; when necessary, we impose a dimensional restriction on\nthe ellipticity ratio and a growth rate for the source term near characteristic\npoints of the boundary. For solutions in the characteristic half-space\n$\\{t>0\\}$, we obtain an intrinsic second order expansion near the origin when\nthe source term belongs to an appropriate weighted $L^{\\infty}$ space; this is\na new result even for the frequently studied sub-Laplacian.", "AI": {"tldr": "The paper proves boundary regularity for degenerate elliptic equations in the Heisenberg group, focusing on bounded coefficients and right-hand side, with new results for intrinsic expansions.", "motivation": "To establish regularity results for degenerate elliptic equations in non-divergence form, particularly in the Heisenberg group, addressing gaps in existing literature.", "method": "Analyzes solutions using left-invariant vector fields, imposes dimensional and growth conditions, and derives intrinsic expansions near the origin.", "result": "Proves H\u00f6lder and Lipschitz regularity, with a novel second-order expansion for solutions in weighted $L^{\\infty}$ spaces.", "conclusion": "The work extends understanding of degenerate elliptic equations, offering new insights for the sub-Laplacian case."}}
{"id": "2506.04282", "pdf": "https://arxiv.org/pdf/2506.04282", "abs": "https://arxiv.org/abs/2506.04282", "authors": ["Runxiang Wang", "Boxiao Wang", "Kai Li", "Yifan Zhang", "Jian Cheng"], "title": "DrSR: LLM based Scientific Equation Discovery with Dual Reasoning from Data and Experience", "categories": ["cs.LG"], "comment": null, "summary": "Symbolic regression is a fundamental tool for discovering interpretable\nmathematical expressions from data, with broad applications across scientific\nand engineering domains. Recently, large language models (LLMs) have\ndemonstrated strong performance in this task, leveraging embedded scientific\npriors and reasoning capabilities to surpass traditional methods. However,\nexisting LLM-based approaches, such as LLM-SR, often over-rely on internal\npriors, lacking explicit data understanding and systematic reflection during\nequation generation. To address these limitations, we propose DrSR (Dual\nReasoning Symbolic Regression), a framework that combines data-driven insight\nwith reflective learning to enhance both robustness and discovery capability.\nSpecifically, DrSR guides LLMs to analyze structural relationships (e.g.,\nmonotonicity, nonlinearity, and correlation) within the data to generate\nstructured descriptions. Simultaneously, it monitors equation performance and\nestablishes a feedback loop to refine subsequent generations. By integrating\ndata understanding and generation reflection in a closed loop, DrSR enables\nmore efficient exploration of the symbolic expression space. Experiments across\ninterdisciplinary datasets in physics, chemistry, biology, and materials\nscience demonstrate that DrSR substantially improves the valid equation rate\nand consistently outperforms both classical and recent LLM-based methods in\nterms of accuracy, generalization, and search efficiency. These results\nunderscore its potential for scientific equation discovery.", "AI": {"tldr": "DrSR enhances symbolic regression by combining data-driven insights and reflective learning, outperforming traditional and LLM-based methods in accuracy and efficiency.", "motivation": "Existing LLM-based symbolic regression methods lack explicit data understanding and systematic reflection, limiting robustness and discovery capability.", "method": "DrSR integrates data-driven analysis (structural relationships) with reflective learning (feedback loop) to refine equation generation.", "result": "DrSR improves valid equation rates and outperforms classical and LLM-based methods in accuracy, generalization, and efficiency across multiple domains.", "conclusion": "DrSR demonstrates strong potential for scientific equation discovery by balancing data understanding and generation reflection."}}
{"id": "2506.05152", "pdf": "https://arxiv.org/pdf/2506.05152", "abs": "https://arxiv.org/abs/2506.05152", "authors": ["Daniele Barbera", "Miho Murata"], "title": "The $\\mathcal{R}$-boundedness of solution operators for the $Q$-tensor model of nematic liquid crystals", "categories": ["math.AP", "math-ph", "math.MP", "76A15, 35Q30, 35Q35"], "comment": null, "summary": "In this paper, we consider a resolvent problem arising from the $Q$-tensor\nmodel for liquid crystal flows in the half-space. Our purpose is to show the\n$\\mathcal{R}$-boundedness for the solution operator families of the resolvent\nproblem when the resolvent parameter lies near the origin. The definition of\nthe $\\mathcal{R}$-solvability implies the uniform boundedness of the operator\nand, consequently, the resolvent estimates for the linear system.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2506.04285", "pdf": "https://arxiv.org/pdf/2506.04285", "abs": "https://arxiv.org/abs/2506.04285", "authors": ["Stephen Smith", "Cormac Purcell", "Zdenka Kuncic"], "title": "Training-free AI for Earth Observation Change Detection using Physics Aware Neuromorphic Networks", "categories": ["cs.LG"], "comment": "16 pages, 9 figures, 3 tables", "summary": "Earth observations from low Earth orbit satellites provide vital information\nfor decision makers to better manage time-sensitive events such as natural\ndisasters. For the data to be most effective for first responders, low latency\nis required between data capture and its arrival to decision makers. A major\nbottleneck is in the bandwidth-limited downlinking of the data from satellites\nto ground stations. One approach to overcome this challenge is to process at\nleast some of the data on-board and prioritise pertinent data to be downlinked.\nIn this work we propose a Physics Aware Neuromorphic Network (PANN) to detect\nchanges caused by natural disasters from a sequence of multi-spectral satellite\nimages and produce a change map, enabling relevant data to be prioritised for\ndownlinking. The PANN used in this study is motivated by physical neural\nnetworks comprised of nano-electronic circuit elements known as \"memristors\"\n(nonlinear resistors with memory). The weights in the network are dynamic and\nupdate in response to varying input signals according to memristor equations of\nstate and electrical circuit conservation laws. The PANN thus generates\nphysics-constrained dynamical output features which are used to detect changes\nin a natural disaster detection task by applying a distance-based metric.\nImportantly, this makes the whole model training-free, allowing it to be\nimplemented with minimal computing resources. The PANN was benchmarked against\na state-of-the-art AI model and achieved comparable or better results in each\nnatural disaster category. It thus presents a promising solution to the\nchallenge of resource-constrained on-board processing.", "AI": {"tldr": "A Physics Aware Neuromorphic Network (PANN) is proposed for real-time natural disaster detection from satellite images, enabling prioritized data downlinking with minimal computing resources.", "motivation": "To address the bottleneck of bandwidth-limited downlinking of satellite data by processing and prioritizing relevant disaster-related data on-board.", "method": "Uses a neuromorphic network inspired by memristors, with dynamic weights updated via physics-based equations, enabling training-free change detection.", "result": "PANN matches or outperforms a state-of-the-art AI model in disaster detection, proving effective for resource-constrained on-board processing.", "conclusion": "PANN offers a viable solution for efficient, low-latency disaster data processing on satellites."}}
{"id": "2506.05270", "pdf": "https://arxiv.org/pdf/2506.05270", "abs": "https://arxiv.org/abs/2506.05270", "authors": ["Massimo Gobbino", "Nicola Picenni"], "title": "Symmetry breaking for local minimizers of a free discontinuity problem", "categories": ["math.AP", "math.OC", "49Q20, 49K05, 49K10"], "comment": "27 pages, 2 figures", "summary": "We study a functional defined on the class of piecewise constant functions,\ncombining a jump penalization, which discourages discontinuities, with a\nfidelity term that penalizes deviations from a given linear function, called\nthe forcing term.\n  In one dimension, it is not difficult to see that local minimizers form\nstaircases that approximate the forcing term. Here we show that in two\ndimensions symmetry breaking occurs, leading to the emergence of exotic\nminimizers whose level sets are not simple stripes with boundaries orthogonal\nto the gradient of the forcing term.\n  The proof relies on the calibration method for free discontinuity problems.", "AI": {"tldr": "The paper explores a functional for piecewise constant functions, balancing jump penalization and fidelity to a linear forcing term. In 2D, symmetry breaking leads to exotic minimizers with complex level sets.", "motivation": "To understand how minimizers of a functional with jump penalization and fidelity terms behave in higher dimensions, particularly in 2D.", "method": "Uses the calibration method for free discontinuity problems to analyze minimizers.", "result": "In 2D, symmetry breaking occurs, producing exotic minimizers with non-simple level sets.", "conclusion": "The study reveals unexpected complexity in 2D minimizers, contrasting with simpler 1D staircases."}}
{"id": "2506.04288", "pdf": "https://arxiv.org/pdf/2506.04288", "abs": "https://arxiv.org/abs/2506.04288", "authors": ["Jae Wan Park", "Junhyeok Kim", "Youngjun Jun", "Hyunah Ko", "Seong Jae Hwang"], "title": "Backbone Augmented Training for Adaptations", "categories": ["cs.LG"], "comment": null, "summary": "Adaptations facilitate efficient training of large backbone models, including\ndiffusion models for image generation and transformer-based language models.\nWhile various adaptation techniques enhance performance with minimal\ncomputational resources, limited adaptation data often leads to challenges in\ntraining. To address this, we focus on the enormous amount of backbone data\nused to pre-train the backbone models. We propose Backbone Augmented Training\n(BAT), a method that leverages backbone data to augment the adaptation dataset.\nFirst, we formulate and prove two mathematical key propositions: one\nestablishes the validity of BAT, while the other identifies a condition under\nwhich BAT benefits adaptation. Furthermore, we introduce an advanced data\nselection scheme that satisfies these propositions and present ALBAT algorithm\nto implement this approach. ALBAT efficiently enhances adaptation training in\nboth personalization and language generation tasks with scarce data.", "AI": {"tldr": "The paper proposes Backbone Augmented Training (BAT) to enhance adaptation training by leveraging backbone data, addressing challenges from limited adaptation data.", "motivation": "Limited adaptation data hinders efficient training of large backbone models, prompting the need for methods like BAT to utilize backbone data for augmentation.", "method": "BAT leverages backbone data to augment adaptation datasets, supported by two mathematical propositions. The ALBAT algorithm implements this with an advanced data selection scheme.", "result": "ALBAT effectively improves adaptation training in personalization and language generation tasks with scarce data.", "conclusion": "BAT and ALBAT offer a practical solution to enhance adaptation training by utilizing backbone data, validated by theoretical and empirical results."}}
{"id": "2506.05299", "pdf": "https://arxiv.org/pdf/2506.05299", "abs": "https://arxiv.org/abs/2506.05299", "authors": ["Christos Sourdis"], "title": "On a linear equation arising in the study of phase separation of BEC's", "categories": ["math.AP"], "comment": null, "summary": "We consider the inner limit system describing the phase separation in\ntwo-component Bose-Einstein condensates linearized around the one-dimensional\nsolution in an infinite strip with zero and periodic boundary conditions, and\nobtain optimal invertibility estimates for the Fourier modes without\nnecessarily assuming orthogonality conditions.", "AI": {"tldr": "Optimal invertibility estimates for Fourier modes in phase separation of two-component Bose-Einstein condensates, without orthogonality assumptions.", "motivation": "To analyze the phase separation in two-component Bose-Einstein condensates, focusing on linearized systems in an infinite strip with specific boundary conditions.", "method": "Linearized the system around a one-dimensional solution, considering zero and periodic boundary conditions, and derived optimal invertibility estimates for Fourier modes.", "result": "Achieved optimal invertibility estimates without requiring orthogonality conditions.", "conclusion": "The study provides rigorous estimates for Fourier modes in phase separation, enhancing understanding of such systems."}}
{"id": "2506.04289", "pdf": "https://arxiv.org/pdf/2506.04289", "abs": "https://arxiv.org/abs/2506.04289", "authors": ["Jesse Geerts", "Stephanie Chan", "Claudia Clopath", "Kimberly Stachenfeld"], "title": "Relational reasoning and inductive bias in transformers trained on a transitive inference task", "categories": ["cs.LG", "q-bio.NC"], "comment": "13 pages, 6 figures", "summary": "Transformer-based models have demonstrated remarkable reasoning abilities,\nbut the mechanisms underlying relational reasoning in different learning\nregimes remain poorly understood. In this work, we investigate how transformers\nperform a classic relational reasoning task from the Psychology literature,\n\\textit{transitive inference}, which requires inference about indirectly\nrelated items by integrating information across observed adjacent item pairs\n(e.g., if A>B and B>C, then A>C). We compare transitive inference behavior\nacross two distinct learning regimes: in-weights learning (IWL), where models\nstore information in network parameters, and in-context learning (ICL), where\nmodels flexibly utilize information presented within the input sequence. Our\nfindings reveal that IWL naturally induces a generalization bias towards\ntransitive inference, despite being trained only on adjacent items, whereas ICL\nmodels trained solely on adjacent items do not generalize transitively.\nMechanistic analysis shows that ICL models develop induction circuits that\nimplement a simple match-and-copy strategy that performs well at relating\nadjacent pairs, but does not encoding hierarchical relationships among\nindirectly related items. Interestingly, when pre-trained on in-context linear\nregression tasks, transformers successfully exhibit in-context generalizable\ntransitive inference. Moreover, like IWL, they display both \\textit{symbolic\ndistance} and \\textit{terminal item effects} characteristic of human and animal\nperformance, without forming induction circuits. These results suggest that\npre-training on tasks with underlying structure promotes the development of\nrepresentations that can scaffold in-context relational reasoning.", "AI": {"tldr": "Transformers show distinct relational reasoning behaviors in in-weights (IWL) vs. in-context learning (ICL). IWL generalizes transitively, while ICL requires pre-training on structured tasks to achieve similar results.", "motivation": "To understand how transformers perform relational reasoning, specifically transitive inference, and compare behaviors in IWL and ICL regimes.", "method": "Analyzed transformers on transitive inference tasks, comparing IWL (parameter-based) and ICL (input-based) learning. Included mechanistic analysis and pre-training on structured tasks.", "result": "IWL models generalize transitively; ICL models require pre-training on structured tasks to achieve similar results and exhibit human-like effects.", "conclusion": "Pre-training on structured tasks enhances in-context relational reasoning, suggesting underlying representations scaffold reasoning abilities."}}
{"id": "2506.04291", "pdf": "https://arxiv.org/pdf/2506.04291", "abs": "https://arxiv.org/abs/2506.04291", "authors": ["Wenhan Xu", "Jiashuo Jiang", "Lei Deng", "Danny Hin-Kwok Tsang"], "title": "A Lyapunov Drift-Plus-Penalty Method Tailored for Reinforcement Learning with Queue Stability", "categories": ["cs.LG"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "With the proliferation of Internet of Things (IoT) devices, the demand for\naddressing complex optimization challenges has intensified. The Lyapunov\nDrift-Plus-Penalty algorithm is a widely adopted approach for ensuring queue\nstability, and some research has preliminarily explored its integration with\nreinforcement learning (RL). In this paper, we investigate the adaptation of\nthe Lyapunov Drift-Plus-Penalty algorithm for RL applications, deriving an\neffective method for combining Lyapunov Drift-Plus-Penalty with RL under a set\nof common and reasonable conditions through rigorous theoretical analysis.\nUnlike existing approaches that directly merge the two frameworks, our proposed\nalgorithm, termed Lyapunov drift-plus-penalty method tailored for reinforcement\nlearning with queue stability (LDPTRLQ) algorithm, offers theoretical\nsuperiority by effectively balancing the greedy optimization of Lyapunov\nDrift-Plus-Penalty with the long-term perspective of RL. Simulation results for\nmultiple problems demonstrate that LDPTRLQ outperforms the baseline methods\nusing the Lyapunov drift-plus-penalty method and RL, corroborating the validity\nof our theoretical derivations. The results also demonstrate that our proposed\nalgorithm outperforms other benchmarks in terms of compatibility and stability.", "AI": {"tldr": "The paper introduces LDPTRLQ, a novel algorithm combining Lyapunov Drift-Plus-Penalty with reinforcement learning, offering theoretical and practical advantages over existing methods.", "motivation": "Addressing complex optimization challenges in IoT by integrating Lyapunov Drift-Plus-Penalty with reinforcement learning for better performance and stability.", "method": "Proposes LDPTRLQ, a method combining Lyapunov Drift-Plus-Penalty and RL under common conditions, balancing greedy optimization with long-term RL perspective.", "result": "Simulations show LDPTRLQ outperforms baseline methods in compatibility, stability, and performance.", "conclusion": "LDPTRLQ is theoretically superior and practically effective for IoT optimization, outperforming existing approaches."}}
{"id": "2506.04801", "pdf": "https://arxiv.org/pdf/2506.04801", "abs": "https://arxiv.org/abs/2506.04801", "authors": ["Kush Kinra", "Fernanda Cipriano"], "title": "Random dynamics and invariant measures for a class of non-Newtonian fluids of differential type on 2D and 3D Poincar\u00e9 domains", "categories": ["math.PR", "math.AP", "76A05, 35R60, 35Q35, 37L30, 35B40"], "comment": null, "summary": "In this article, we consider a class of incompressible stochastic third-grade\nfluids (non-Newtonian fluids) equations on two- as well as three-dimensional\nPoincar\\'e domains $\\mathcal{O}$ (which may be bounded or unbounded). Our aims\nare to study the well-posedness and asymptotic analysis for the solutions of\nthe underlying system. Firstly, we prove that the underlying system defined on\n$\\mathcal{O}$ has a unique weak solution (in the analytic sense) under\nDirichlet boundary condition and it also generates random dynamical system\n$\\Psi$. Secondly, we consider the underlying system on bounded domains. Using\nthe compact Sobolev embedding $\\mathbb{H}^1(\\mathcal{O})\n\\hookrightarrow\\mathbb{L}^2(\\mathcal{O})$, we prove the existence of a unique\nrandom attractor for the underlying system on bounded domains with external\nforcing in\n$\\mathbb{H}^{-1}(\\mathcal{O})+\\mathbb{W}^{-1,\\frac{4}{3}}(\\mathcal{O})$.\nThirdly, we consider the underlying system on unbounded Poincar\\'e domains with\nexternal forcing in $\\mathbb{L}^{2}(\\mathcal{O})$ and show the existence of a\nunique random attractor. In order to obtain the existence of a unique random\nattractor on unbounded domains, due to the lack of compact Sobolev embedding\n$\\mathbb{H}^1(\\mathcal{O}) \\hookrightarrow\\mathbb{H}^2(\\mathcal{O})$, we use\nthe uniform-tail estimates method which helps us to demonstrate the asymptotic\ncompactness of $\\Psi$.\n  Note that due to the presence of several nonlinear terms in the underlying\nsystem, we are not able to use the energy equality method to obtain the\nasymptotic compactness of $\\Psi$ in unbounded domains, which makes the analysis\nof this work in unbounded domains more difficult and interesting. Finally, as a\nconsequence of the existence of random attractors, we address the existence of\ninvariant measures for underlying system.", "AI": {"tldr": "The paper studies the well-posedness and asymptotic behavior of solutions for a class of incompressible stochastic third-grade fluids on Poincar\u00e9 domains, proving unique weak solutions and random attractors for bounded and unbounded domains.", "motivation": "To analyze the well-posedness and asymptotic properties of stochastic non-Newtonian fluid equations, addressing challenges like nonlinearity and lack of compact embeddings in unbounded domains.", "method": "Proves unique weak solutions under Dirichlet boundary conditions, uses compact Sobolev embeddings for bounded domains, and employs uniform-tail estimates for unbounded domains to show asymptotic compactness.", "result": "Existence of unique random attractors for both bounded and unbounded domains, and invariant measures for the system.", "conclusion": "The study successfully addresses the challenges of nonlinearity and domain unboundedness, providing insights into the long-term behavior of stochastic third-grade fluids."}}
{"id": "2506.04293", "pdf": "https://arxiv.org/pdf/2506.04293", "abs": "https://arxiv.org/abs/2506.04293", "authors": ["Fengze Liu", "Haoyu Wang", "Joonhyuk Cho", "Dan Roth", "Andrew W. Lo"], "title": "AUTOCT: Automating Interpretable Clinical Trial Prediction with LLM Agents", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Clinical trials are critical for advancing medical treatments but remain\nprohibitively expensive and time-consuming. Accurate prediction of clinical\ntrial outcomes can significantly reduce research and development costs and\naccelerate drug discovery. While recent deep learning models have shown promise\nby leveraging unstructured data, their black-box nature, lack of\ninterpretability, and vulnerability to label leakage limit their practical use\nin high-stakes biomedical contexts. In this work, we propose AutoCT, a novel\nframework that combines the reasoning capabilities of large language models\nwith the explainability of classical machine learning. AutoCT autonomously\ngenerates, evaluates, and refines tabular features based on public information\nwithout human input. Our method uses Monte Carlo Tree Search to iteratively\noptimize predictive performance. Experimental results show that AutoCT performs\non par with or better than SOTA methods on clinical trial prediction tasks\nwithin only a limited number of self-refinement iterations, establishing a new\nparadigm for scalable, interpretable, and cost-efficient clinical trial\nprediction.", "AI": {"tldr": "AutoCT combines large language models with classical ML for interpretable clinical trial outcome prediction, outperforming SOTA methods.", "motivation": "Clinical trials are costly and slow; accurate outcome prediction can reduce costs and speed up drug discovery.", "method": "AutoCT uses Monte Carlo Tree Search to autonomously generate and refine tabular features from public data.", "result": "AutoCT matches or exceeds SOTA performance in clinical trial prediction tasks with few iterations.", "conclusion": "AutoCT offers a scalable, interpretable, and cost-efficient solution for clinical trial prediction."}}
{"id": "2506.04294", "pdf": "https://arxiv.org/pdf/2506.04294", "abs": "https://arxiv.org/abs/2506.04294", "authors": ["Asier Diaz-Iglesias", "Xabier Belaunzaran", "Ane M. Florez-Tapia"], "title": "Short-Term Power Demand Forecasting for Diverse Consumer Types to Enhance Grid Planning and Synchronisation", "categories": ["cs.LG"], "comment": null, "summary": "Ensuring grid stability in the transition to renewable energy sources\nrequires accurate power demand forecasting. This study addresses the need for\nprecise forecasting by differentiating among industrial, commercial, and\nresidential consumers through customer clusterisation, tailoring the\nforecasting models to capture the unique consumption patterns of each group. A\nfeature selection process is done for each consumer type including temporal,\nsocio-economic, and weather-related data obtained from the Copernicus Earth\nObservation (EO) program. A variety of AI and machine learning algorithms for\nShort-Term Load Forecasting (STLF) and Very Short-Term Load Forecasting (VSTLF)\nare explored and compared, determining the most effective approaches. With all\nthat, the main contribution of this work are the new forecasting approaches\nproposed, which have demonstrated superior performance compared to simpler\nmodels, both for STLF and VSTLF, highlighting the importance of customized\nforecasting strategies for different consumer groups and demonstrating the\nimpact of incorporating detailed weather data on forecasting accuracy. These\nadvancements contribute to more reliable power demand predictions, thereby\nsupporting grid stability.", "AI": {"tldr": "The paper proposes customized forecasting models for industrial, commercial, and residential consumers, using AI/ML algorithms and detailed weather data to improve power demand forecasting accuracy, supporting grid stability.", "motivation": "Accurate power demand forecasting is crucial for grid stability amid the transition to renewable energy. The study aims to address this by tailoring models to different consumer groups.", "method": "Customer clusterisation is used to differentiate consumers. Feature selection includes temporal, socio-economic, and weather data. AI/ML algorithms are compared for STLF and VSTLF.", "result": "The proposed forecasting approaches outperform simpler models, showing the importance of customized strategies and detailed weather data.", "conclusion": "Customized forecasting for consumer groups and detailed weather data enhance accuracy, contributing to reliable power demand predictions and grid stability."}}
{"id": "2506.04812", "pdf": "https://arxiv.org/pdf/2506.04812", "abs": "https://arxiv.org/abs/2506.04812", "authors": ["Marios A. Apetroaie", "Sam C. Collingbourne", "Elena Giorgi"], "title": "Conservation Laws and Boundedness for Linearised Einstein--Maxwell Equations on the Reissner--Nordstr\u00f6m Black Hole", "categories": ["gr-qc", "math.AP"], "comment": "28 pages", "summary": "We study the linearised Einstein--Maxwell equations on the\nReissner--Nordstr\\\"om spacetime and derive the canonical energy conservation\nlaw in double null gauge. In the spirit of the work of Holzegel and the second\nauthor, we avoid any use of the hyperbolic nature of the Teukolsky equations\nand rely solely on the conservation law to establish control of energy fluxes\nfor the gauge-invariant Teukolsky variables, previously identified by the third\nauthor, along all outgoing null hypersurfaces, for charge-to-mass ratio\n$\\frac{|Q|}{M} < \\frac{\\sqrt{15}}{4}$. This yields uniform boundedness for the\nTeukolsky variables in Reissner--Nordstr\\\"om.", "AI": {"tldr": "The paper analyzes linearized Einstein-Maxwell equations on Reissner-Nordstr\u00f6m spacetime, using energy conservation to control Teukolsky variables without relying on hyperbolic methods.", "motivation": "To establish control of energy fluxes for gauge-invariant Teukolsky variables in Reissner-Nordstr\u00f6m spacetime, avoiding traditional hyperbolic approaches.", "method": "Derives canonical energy conservation law in double null gauge and applies it to Teukolsky variables for charge-to-mass ratios below a specific threshold.", "result": "Uniform boundedness of Teukolsky variables is achieved along all outgoing null hypersurfaces.", "conclusion": "The approach successfully controls Teukolsky variables using energy conservation, offering an alternative to hyperbolic methods."}}
{"id": "2506.04296", "pdf": "https://arxiv.org/pdf/2506.04296", "abs": "https://arxiv.org/abs/2506.04296", "authors": ["N Guerin", "M Nakhla", "A Dehoux", "J L Loyer"], "title": "Deep learning for predicting hauling fleet production capacity under uncertainties in open pit mines using real and simulated data", "categories": ["cs.LG"], "comment": null, "summary": "Accurate short-term forecasting of hauling-fleet capacity is crucial in\nopen-pit mining, where weather fluctuations, mechanical breakdowns, and\nvariable crew availability introduce significant operational uncertainties. We\npropose a deep-learning framework that blends real-world operational records\n(high-resolution rainfall measurements, fleet performance telemetry) with\nsynthetically generated mechanical-breakdown scenarios to enable the model to\ncapture fluctuating high-impact failure events. We evaluate two architectures:\nan XGBoost regressor achieving a median absolute error (MedAE) of 14.3 per cent\nand a Long Short-Term Memory network with a MedAE of 15.1 per cent. Shapley\nAdditive exPlanations (SHAP) value analyses identify cumulative rainfall,\nhistorical payload trends, and simulated breakdown frequencies as dominant\npredictors. Integration of simulated breakdown data and shift-planning features\nnotably reduces prediction volatility. Future work will further integrate\nmaintenance-scheduling indicators (Mean Time Between Failures, Mean Time to\nRepair), detailed human resource data (operator absenteeism, crew efficiency\nmetrics), blast event scheduling, and other operational constraints to enhance\nforecast robustness and adaptability. This hybrid modelling approach offers a\ncomprehensive decision-support tool for proactive, data-driven fleet management\nunder dynamically uncertain conditions.", "AI": {"tldr": "A deep-learning framework for short-term hauling-fleet capacity forecasting in open-pit mining, combining real-world data with synthetic breakdown scenarios, outperforms traditional methods and reduces prediction volatility.", "motivation": "Accurate forecasting is vital due to operational uncertainties like weather, mechanical breakdowns, and crew availability.", "method": "Proposes a hybrid deep-learning approach using XGBoost and LSTM, integrating real and synthetic data (e.g., rainfall, breakdown scenarios).", "result": "XGBoost achieves MedAE of 14.3%, LSTM 15.1%. SHAP analysis highlights key predictors like rainfall and breakdown frequencies.", "conclusion": "The hybrid model improves fleet management under uncertainty, with future enhancements planned for maintenance and human resource data integration."}}
{"id": "2506.05037", "pdf": "https://arxiv.org/pdf/2506.05037", "abs": "https://arxiv.org/abs/2506.05037", "authors": ["Angha Agarwal", "Antti V. V\u00e4h\u00e4kangas"], "title": "Limits at infinity for Haj\u0142asz-Sobolev functions in metric spaces", "categories": ["math.CA", "math.AP", "46E36, 31C15, 31B15, 31B25"], "comment": null, "summary": "We study limits at infinity for homogeneous Hajlasz-Sobolev functions defined\non uniformly perfect metric spaces equipped with a doubling measure. We prove\nthat a quasicontinuous representative of such a function has a pointwise limit\nat infinity outside an exceptional set, defined in terms of a variational\nrelative capacity. Our framework refines earlier approaches that relied on\nHausdorff content rather than relative capacity, and it extends previous\nresults for homogeneous Newtonian and fractional Sobolev functions.", "AI": {"tldr": "The paper studies limits at infinity for homogeneous Hajlasz-Sobolev functions on metric spaces with doubling measures, proving pointwise limits outside an exceptional set defined by variational relative capacity.", "motivation": "To refine earlier approaches using Hausdorff content and extend results for Newtonian and fractional Sobolev functions.", "method": "Analyzes quasicontinuous representatives of functions, using variational relative capacity to define exceptional sets.", "result": "Pointwise limits at infinity exist outside the exceptional set.", "conclusion": "The framework generalizes and refines previous results, offering a more precise understanding of limits for such functions."}}
{"id": "2506.04297", "pdf": "https://arxiv.org/pdf/2506.04297", "abs": "https://arxiv.org/abs/2506.04297", "authors": ["Abdourrahmane Mahamane Atto"], "title": "Softlog-Softmax Layers and Divergences Contribute to a Computationally Dependable Ensemble Learning", "categories": ["cs.LG"], "comment": null, "summary": "The paper proposes a 4-step process for highlighting that softlog-softmax\ncascades can improve both consistency and dependability of the next generation\nensemble learning systems. The first process is anatomical in nature: the\ntarget ensemble model under consideration is composed by canonical elements\nrelating to the definition of a convolutional frustum. No a priori is\nconsidered in the choice of canonical forms. Diversity is the main criterion\nfor selecting these forms. It is shown that the more complex the problem, the\nmore useful this ensemble diversity is. The second process is physiological and\nrelates to neural engineering: a softlog is derived to both make weak\nlogarithmic operations consistent and lead, through multiple softlog-softmax\nlayers, to intermediate decisions in the sense of respecting the same class\nlogic as that faced by the output layer. The third process concerns neural\ninformation theory: softlog-based entropy and divergence are proposed for the\nsake of constructing information measures yielding consistent values on closed\nintervals. These information measures are used to determine the relationships\nbetween individual and sub-community decisions in frustum diversitybased\nensemble learning. The concluding process addresses the derivation of an\ninformative performance tensor for the purpose of a reliable ensemble\nevaluation.", "AI": {"tldr": "The paper introduces a 4-step process using softlog-softmax cascades to enhance ensemble learning systems, focusing on diversity, neural engineering, information theory, and performance evaluation.", "motivation": "To improve consistency and dependability in next-generation ensemble learning systems by leveraging softlog-softmax cascades and diverse canonical forms.", "method": "A 4-step process: (1) anatomical selection of diverse canonical forms, (2) physiological derivation of softlog for consistency, (3) neural information theory for entropy/divergence measures, and (4) performance tensor derivation for evaluation.", "result": "Demonstrates that ensemble diversity is more beneficial for complex problems and provides consistent intermediate decisions and information measures.", "conclusion": "The proposed framework enhances ensemble learning by integrating diversity, consistency, and reliable evaluation metrics."}}
{"id": "2506.04301", "pdf": "https://arxiv.org/pdf/2506.04301", "abs": "https://arxiv.org/abs/2506.04301", "authors": ["Jiin Kim", "Byeongjun Shin", "Jinha Chung", "Minsoo Rhu"], "title": "The Cost of Dynamic Reasoning: Demystifying AI Agents and Test-Time Scaling from an AI Infrastructure Perspective", "categories": ["cs.LG", "cs.AR"], "comment": null, "summary": "Large-language-model (LLM)-based AI agents have recently showcased impressive\nversatility by employing dynamic reasoning, an adaptive, multi-step process\nthat coordinates with external tools. This shift from static, single-turn\ninference to agentic, multi-turn workflows broadens task generalization and\nbehavioral flexibility, but it also introduces serious concerns about\nsystem-level cost, efficiency, and sustainability. This paper presents the\nfirst comprehensive system-level analysis of AI agents, quantifying their\nresource usage, latency behavior, energy consumption, and datacenter-wide power\nconsumption demands across diverse agent designs and test-time scaling\nstrategies. We further characterize how AI agent design choices, such as\nfew-shot prompting, reflection depth, and parallel reasoning, impact\naccuracy-cost tradeoffs. Our findings reveal that while agents improve accuracy\nwith increased compute, they suffer from rapidly diminishing returns, widening\nlatency variance, and unsustainable infrastructure costs. Through detailed\nevaluation of representative agents, we highlight the profound computational\ndemands introduced by AI agent workflows, uncovering a looming sustainability\ncrisis. These results call for a paradigm shift in agent design toward\ncompute-efficient reasoning, balancing performance with deployability under\nreal-world constraints.", "AI": {"tldr": "The paper analyzes the system-level costs of LLM-based AI agents, revealing high resource usage, latency, and energy demands, and calls for more efficient designs.", "motivation": "To address concerns about the sustainability and efficiency of dynamic, multi-turn AI agents compared to static models.", "method": "Comprehensive system-level analysis of resource usage, latency, energy, and power consumption across diverse agent designs and scaling strategies.", "result": "Agents improve accuracy with more compute but face diminishing returns, high latency variance, and unsustainable costs.", "conclusion": "A shift toward compute-efficient agent design is needed to balance performance and sustainability."}}
{"id": "2506.04302", "pdf": "https://arxiv.org/pdf/2506.04302", "abs": "https://arxiv.org/abs/2506.04302", "authors": ["Xiang Zheng", "Xingjun Ma", "Wei-Bin Lee", "Cong Wang"], "title": "RedRFT: A Light-Weight Benchmark for Reinforcement Fine-Tuning-Based Red Teaming", "categories": ["cs.LG"], "comment": null, "summary": "Red teaming has proven to be an effective method for identifying and\nmitigating vulnerabilities in Large Language Models (LLMs). Reinforcement\nFine-Tuning (RFT) has emerged as a promising strategy among existing red\nteaming techniques. However, a lack of a unified benchmark hinders current\nRFT-based red teaming methods. Implementation details, especially in Proximal\nPolicy Optimization (PPO)-based RFT, significantly affect outcome stability and\nreproducibility. To address this issue, we introduce RedRFT, a lightweight\nbenchmark designed to simplify and standardize the implementation and\nevaluation of RFT-based red teaming. RedRFT combines the design strengths of\nboth single-file CleanRL and highly modularized Tianshou, offering high-quality\nsingle-file red teaming implementations and modular PPO core components, such\nas the General Advantage Estimator. It supports a variety of token and sentence\ndiversity metrics, featuring modularized intrinsic reward computation that\nfacilitates plug-and-play experimentation. To clarify their influence on RFT\nperformance, we conducted an extensive ablation study on key components,\nincluding Low-Rank Adaptation (LoRA), Kullback-Leibler (KL) divergence, and\nLagrange Multiplier. We hope this work contributes to 1) gaining a\ncomprehensive understanding of the implementation nuances of RFT-based red\nteaming algorithms, and 2) enabling rapid prototyping of innovative features\nfor RFT-based red teaming. Code for the benchmark can be accessed at\nhttps://github.com/x-zheng16/RedRFT.git.", "AI": {"tldr": "RedRFT is a lightweight benchmark for standardizing Reinforcement Fine-Tuning (RFT)-based red teaming in LLMs, addressing implementation inconsistencies and enabling rapid prototyping.", "motivation": "The lack of a unified benchmark for RFT-based red teaming, especially in PPO-based implementations, affects stability and reproducibility.", "method": "RedRFT combines CleanRL and Tianshou strengths, offering modular PPO components and diverse metrics for plug-and-play experimentation.", "result": "An ablation study clarifies the impact of LoRA, KL divergence, and Lagrange Multiplier on RFT performance.", "conclusion": "RedRFT aids in understanding RFT nuances and accelerates innovation in red teaming for LLMs."}}
{"id": "2506.04349", "pdf": "https://arxiv.org/pdf/2506.04349", "abs": "https://arxiv.org/abs/2506.04349", "authors": ["Christos Sakaridis"], "title": "You Only Train Once", "categories": ["cs.LG", "cs.CV"], "comment": "17 pages, 4 figures", "summary": "The title of this paper is perhaps an overclaim. Of course, the process of\ncreating and optimizing a learned model inevitably involves multiple training\nruns which potentially feature different architectural designs, input and\noutput encodings, and losses. However, our method, You Only Train Once (YOTO),\nindeed contributes to limiting training to one shot for the latter aspect of\nlosses selection and weighting. We achieve this by automatically optimizing\nloss weight hyperparameters of learned models in one shot via standard\ngradient-based optimization, treating these hyperparameters as regular\nparameters of the networks and learning them. To this end, we leverage the\ndifferentiability of the composite loss formulation which is widely used for\noptimizing multiple empirical losses simultaneously and model it as a novel\nlayer which is parameterized with a softmax operation that satisfies the\ninherent positivity constraints on loss hyperparameters while avoiding\ndegenerate empirical gradients. We complete our joint end-to-end optimization\nscheme by defining a novel regularization loss on the learned hyperparameters,\nwhich models a uniformity prior among the employed losses while ensuring\nboundedness of the identified optima. We evidence the efficacy of YOTO in\njointly optimizing loss hyperparameters and regular model parameters in one\nshot by comparing it to the commonly used brute-force grid search across\nstate-of-the-art networks solving two key problems in computer vision, i.e. 3D\nestimation and semantic segmentation, and showing that it consistently\noutperforms the best grid-search model on unseen test data. Code will be made\npublicly available.", "AI": {"tldr": "YOTO (You Only Train Once) automates loss weight optimization in one training run, outperforming grid search in tasks like 3D estimation and semantic segmentation.", "motivation": "Traditional model training involves multiple runs for loss selection and weighting, which is inefficient. YOTO aims to streamline this by optimizing loss weights in one shot.", "method": "YOTO treats loss weights as learnable parameters, using gradient-based optimization and a softmax layer to ensure positivity. A novel regularization loss ensures balanced and bounded optima.", "result": "YOTO consistently outperforms grid-search models on unseen test data in 3D estimation and semantic segmentation tasks.", "conclusion": "YOTO effectively optimizes loss weights in one training run, improving efficiency and performance over traditional methods."}}
{"id": "2506.04352", "pdf": "https://arxiv.org/pdf/2506.04352", "abs": "https://arxiv.org/abs/2506.04352", "authors": ["Ethem Alpaydin"], "title": "Half-Layered Neural Networks", "categories": ["cs.LG"], "comment": "11 pages, 8 figures", "summary": "We propose a ``half'' layer of hidden units that has some of its weights\nrandomly set and some of them trained. A half unit is composed of two stages:\nFirst, it takes a weighted sum of its inputs with fixed random weights, and\nsecond, the total activation is multiplied and then translated using two\nmodifiable weights, before the result is passed through a nonlinearity. The\nnumber of modifiable weights of each hidden unit is thus two and does not\ndepend on the fan-in. We show how such half units can be used in the first or\nany later layer in a deep network, possibly following convolutional layers. Our\nexperiments on MNIST and FashionMNIST data sets indicate the promise of half\nlayers, where we can achieve reasonable accuracy with a reduced number of\nparameters due to the regularizing effect of the randomized connections.", "AI": {"tldr": "A 'half' layer of hidden units with partially randomized and partially trained weights is proposed, reducing parameters while maintaining accuracy.", "motivation": "To reduce the number of modifiable weights in neural networks without sacrificing performance, leveraging randomization for regularization.", "method": "Half units use fixed random weights for input summation and two modifiable weights for scaling and translation, applied in deep networks.", "result": "Experiments on MNIST and FashionMNIST show reasonable accuracy with fewer parameters due to randomized connections.", "conclusion": "Half layers offer a promising approach for efficient neural network design by balancing randomization and training."}}
{"id": "2506.04358", "pdf": "https://arxiv.org/pdf/2506.04358", "abs": "https://arxiv.org/abs/2506.04358", "authors": ["Uditansh Srivastava", "Shivam Aryan", "Shaurya Singh"], "title": "A Risk-Aware Reinforcement Learning Reward for Financial Trading", "categories": ["cs.LG"], "comment": "14 pages, 11 figures", "summary": "We propose a novel composite reward function for reinforcement learning in\nfinancial trading that balances return and risk using four differentiable\nterms: annualized return downside risk differential return and the Treynor\nratio\n  Unlike single metric objectives for example the Sharpe ratio our formulation\nis modular and parameterized by weights w1 w2 w3 and w4 enabling practitioners\nto encode diverse investor preferences\n  We tune these weights via grid search to target specific risk return profiles\n  We derive closed form gradients for each term to facilitate gradient based\ntraining and analyze key theoretical properties including monotonicity\nboundedness and modularity\n  This framework offers a general blueprint for building robust multi objective\nreward functions in complex trading environments and can be extended with\nadditional risk measures or adaptive weighting", "AI": {"tldr": "A novel composite reward function for financial trading RL balances return and risk using four differentiable terms, enabling customizable investor preferences via weight tuning.", "motivation": "To address the limitations of single-metric objectives like the Sharpe ratio by providing a modular, parameterized reward function that can encode diverse investor preferences.", "method": "Proposes a composite reward function with four terms (annualized return, downside risk, differential return, Treynor ratio) and tunes weights via grid search. Derives closed-form gradients for gradient-based training.", "result": "The framework offers a robust, modular approach for multi-objective reward functions in trading, with theoretical properties like monotonicity and boundedness.", "conclusion": "This method provides a general blueprint for adaptable, multi-objective reward functions in trading, extendable with additional risk measures or adaptive weighting."}}
{"id": "2506.04360", "pdf": "https://arxiv.org/pdf/2506.04360", "abs": "https://arxiv.org/abs/2506.04360", "authors": ["Philippe Chlenski", "Itsik Pe'er"], "title": "Even Faster Hyperbolic Random Forests: A Beltrami-Klein Wrapper Approach", "categories": ["cs.LG"], "comment": "15 pages, 4 figures, 2 tables", "summary": "Decision trees and models that use them as primitives are workhorses of\nmachine learning in Euclidean spaces. Recent work has further extended these\nmodels to the Lorentz model of hyperbolic space by replacing axis-parallel\nhyperplanes with homogeneous hyperplanes when partitioning the input space. In\nthis paper, we show how the hyperDT algorithm can be elegantly reexpressed in\nthe Beltrami-Klein model of hyperbolic spaces. This preserves the thresholding\noperation used in Euclidean decision trees, enabling us to further rewrite\nhyperDT as simple pre- and post-processing steps that form a wrapper around\nexisting tree-based models designed for Euclidean spaces. The wrapper approach\nunlocks many optimizations already available in Euclidean space models,\nimproving flexibility, speed, and accuracy while offering a simpler, more\nmaintainable, and extensible codebase. Our implementation is available at\nhttps://github.com/pchlenski/hyperdt.", "AI": {"tldr": "The paper re-expresses the hyperDT algorithm in the Beltrami-Klein model of hyperbolic space, simplifying it into pre- and post-processing steps around Euclidean tree models.", "motivation": "To improve flexibility, speed, and accuracy of hyperbolic decision trees by leveraging existing Euclidean optimizations.", "method": "Reformulating hyperDT in the Beltrami-Klein model to use thresholding operations, enabling a wrapper around Euclidean tree models.", "result": "Enhanced performance, simpler codebase, and access to Euclidean optimizations.", "conclusion": "The wrapper approach offers a maintainable and extensible solution for hyperbolic decision trees."}}
{"id": "2506.04377", "pdf": "https://arxiv.org/pdf/2506.04377", "abs": "https://arxiv.org/abs/2506.04377", "authors": ["Yasaman Mahdaviyeh", "James Lucas", "Mengye Ren", "Andreas S. Tolias", "Richard Zemel", "Toniann Pitassi"], "title": "Replay Can Provably Increase Forgetting", "categories": ["cs.LG"], "comment": "To appear in the Proceedings of the Conference on Lifelong Learning\n  Agents (CoLLAs) 2025", "summary": "Continual learning seeks to enable machine learning systems to solve an\nincreasing corpus of tasks sequentially. A critical challenge for continual\nlearning is forgetting, where the performance on previously learned tasks\ndecreases as new tasks are introduced. One of the commonly used techniques to\nmitigate forgetting, sample replay, has been shown empirically to reduce\nforgetting by retaining some examples from old tasks and including them in new\ntraining episodes. In this work, we provide a theoretical analysis of sample\nreplay in an over-parameterized continual linear regression setting, where each\ntask is given by a linear subspace and with enough replay samples, one would be\nable to eliminate forgetting. Our analysis focuses on sample replay and\nhighlights the role of the replayed samples and the relationship between task\nsubspaces. Surprisingly, we find that, even in a noiseless setting, forgetting\ncan be non-monotonic with respect to the number of replay samples. We present\ntasks where replay can be harmful with respect to worst-case settings, and also\nin distributional settings where replay of randomly selected samples increases\nforgetting in expectation. We also give empirical evidence that harmful replay\nis not limited to training with linear models by showing similar behavior for a\nneural networks equipped with SGD. Through experiments on a commonly used\nbenchmark, we provide additional evidence that, even in seemingly benign\nscenarios, performance of the replay heavily depends on the choice of replay\nsamples and the relationship between tasks.", "AI": {"tldr": "The paper theoretically analyzes sample replay in continual learning, showing it can sometimes increase forgetting, even in noiseless settings, and highlights the importance of sample selection and task relationships.", "motivation": "To understand the theoretical underpinnings of sample replay in continual learning and its potential pitfalls, despite its empirical success in mitigating forgetting.", "method": "Theoretical analysis of sample replay in an over-parameterized continual linear regression setting, examining the role of replayed samples and task subspace relationships.", "result": "Forgetting can be non-monotonic with replay samples; replay may harm performance in worst-case or distributional settings. Empirical evidence extends these findings to neural networks.", "conclusion": "Sample replay's effectiveness depends on sample choice and task relationships, and it can sometimes worsen forgetting, challenging its universal applicability."}}
{"id": "2506.04398", "pdf": "https://arxiv.org/pdf/2506.04398", "abs": "https://arxiv.org/abs/2506.04398", "authors": ["Th\u00e9o Vincent", "Yogesh Tripathi", "Tim Faust", "Yaniv Oren", "Jan Peters", "Carlo D'Eramo"], "title": "Bridging the Performance Gap Between Target-Free and Target-Based Reinforcement Learning With Iterated Q-Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In value-based reinforcement learning, removing the target network is\ntempting as the boostrapped target would be built from up-to-date estimates,\nand the spared memory occupied by the target network could be reallocated to\nexpand the capacity of the online network. However, eliminating the target\nnetwork introduces instability, leading to a decline in performance. Removing\nthe target network also means we cannot leverage the literature developed\naround target networks. In this work, we propose to use a copy of the last\nlinear layer of the online network as a target network, while sharing the\nremaining parameters with the up-to-date online network, hence stepping out of\nthe binary choice between target-based and target-free methods. It enables us\nto leverage the concept of iterated Q-learning, which consists of learning\nconsecutive Bellman iterations in parallel, to reduce the performance gap\nbetween target-free and target-based approaches. Our findings demonstrate that\nthis novel method, termed iterated Shared Q-Learning (iS-QL), improves the\nsample efficiency of target-free approaches across various settings.\nImportantly, iS-QL requires a smaller memory footprint and comparable training\ntime to classical target-based algorithms, highlighting its potential to scale\nreinforcement learning research.", "AI": {"tldr": "The paper proposes a method called iterated Shared Q-Learning (iS-QL) to balance the trade-off between target-free and target-based reinforcement learning, improving sample efficiency without significant memory or time overhead.", "motivation": "Removing the target network in value-based RL can free memory but introduces instability. The goal is to leverage the benefits of target networks while minimizing drawbacks.", "method": "iS-QL uses a copy of the last linear layer of the online network as a target, sharing other parameters. It integrates iterated Q-learning to bridge the performance gap.", "result": "iS-QL improves sample efficiency in target-free approaches, with smaller memory usage and comparable training time to target-based methods.", "conclusion": "iS-QL offers a scalable solution for RL, balancing performance and resource efficiency."}}
{"id": "2506.04399", "pdf": "https://arxiv.org/pdf/2506.04399", "abs": "https://arxiv.org/abs/2506.04399", "authors": ["Suzan Ece Ada", "Emre Ugur"], "title": "Unsupervised Meta-Testing with Conditional Neural Processes for Hybrid Meta-Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": "Published in IEEE Robotics and Automation Letters Volume: 9, Issue:\n  10, 8427 - 8434, October 2024. 8 pages, 7 figures", "summary": "We introduce Unsupervised Meta-Testing with Conditional Neural Processes\n(UMCNP), a novel hybrid few-shot meta-reinforcement learning (meta-RL) method\nthat uniquely combines, yet distinctly separates, parameterized policy\ngradient-based (PPG) and task inference-based few-shot meta-RL. Tailored for\nsettings where the reward signal is missing during meta-testing, our method\nincreases sample efficiency without requiring additional samples in\nmeta-training. UMCNP leverages the efficiency and scalability of Conditional\nNeural Processes (CNPs) to reduce the number of online interactions required in\nmeta-testing. During meta-training, samples previously collected through PPG\nmeta-RL are efficiently reused for learning task inference in an offline\nmanner. UMCNP infers the latent representation of the transition dynamics model\nfrom a single test task rollout with unknown parameters. This approach allows\nus to generate rollouts for self-adaptation by interacting with the learned\ndynamics model. We demonstrate our method can adapt to an unseen test task\nusing significantly fewer samples during meta-testing than the baselines in\n2D-Point Agent and continuous control meta-RL benchmarks, namely, cartpole with\nunknown angle sensor bias, walker agent with randomized dynamics parameters.", "AI": {"tldr": "UMCNP is a hybrid few-shot meta-RL method combining policy gradient and task inference, improving sample efficiency without extra meta-training samples by leveraging CNPs for dynamics model inference.", "motivation": "Addresses the challenge of missing reward signals during meta-testing while enhancing sample efficiency.", "method": "Combines PPG meta-RL and task inference, uses CNPs for dynamics model inference, and reuses meta-training samples offline.", "result": "Achieves adaptation to unseen tasks with fewer samples in benchmarks like 2D-Point Agent and continuous control tasks.", "conclusion": "UMCNP effectively reduces online interactions in meta-testing and outperforms baselines in sample efficiency."}}
{"id": "2506.04411", "pdf": "https://arxiv.org/pdf/2506.04411", "abs": "https://arxiv.org/abs/2506.04411", "authors": ["Achleshwar Luthra", "Tianbao Yang", "Tomer Galanti"], "title": "Self-Supervised Contrastive Learning is Approximately Supervised Contrastive Learning", "categories": ["cs.LG"], "comment": null, "summary": "Despite its empirical success, the theoretical foundations of self-supervised\ncontrastive learning (CL) are not yet fully established. In this work, we\naddress this gap by showing that standard CL objectives implicitly approximate\na supervised variant we call the negatives-only supervised contrastive loss\n(NSCL), which excludes same-class contrasts. We prove that the gap between the\nCL and NSCL losses vanishes as the number of semantic classes increases, under\na bound that is both label-agnostic and architecture-independent.\n  We characterize the geometric structure of the global minimizers of the NSCL\nloss: the learned representations exhibit augmentation collapse, within-class\ncollapse, and class centers that form a simplex equiangular tight frame. We\nfurther introduce a new bound on the few-shot error of linear-probing. This\nbound depends on two measures of feature variability--within-class dispersion\nand variation along the line between class centers. We show that directional\nvariation dominates the bound and that the within-class dispersion's effect\ndiminishes as the number of labeled samples increases. These properties enable\nCL and NSCL-trained representations to support accurate few-shot label recovery\nusing simple linear probes.\n  Finally, we empirically validate our theoretical findings: the gap between CL\nand NSCL losses decays at a rate of $\\mathcal{O}(\\frac{1}{\\#\\text{classes}})$;\nthe two losses are highly correlated; minimizing the CL loss implicitly brings\nthe NSCL loss close to the value achieved by direct minimization; and the\nproposed few-shot error bound provides a tight estimate of probing performance\nin practice.", "AI": {"tldr": "The paper bridges the gap in theoretical understanding of self-supervised contrastive learning (CL) by linking it to a supervised variant (NSCL), proving convergence under certain conditions, and analyzing geometric properties of learned representations. It also introduces a bound for few-shot error and validates findings empirically.", "motivation": "To establish theoretical foundations for self-supervised contrastive learning (CL) by connecting it to a supervised variant (NSCL) and analyzing its properties.", "method": "Theoretical analysis of CL and NSCL losses, geometric characterization of learned representations, and introduction of a few-shot error bound. Empirical validation of theoretical results.", "result": "The gap between CL and NSCL losses vanishes with increasing semantic classes. Learned representations exhibit specific geometric properties. The few-shot error bound is tight and depends on feature variability.", "conclusion": "CL implicitly approximates NSCL, and the theoretical insights align with empirical results, supporting accurate few-shot learning with linear probes."}}
{"id": "2506.04430", "pdf": "https://arxiv.org/pdf/2506.04430", "abs": "https://arxiv.org/abs/2506.04430", "authors": ["Egor Petrov", "Grigoriy Evseev", "Aleksey Antonov", "Andrey Veprikov", "Pavel Plyusnin", "Nikolay Bushkov", "Stanislav Moiseev", "Aleksandr Beznosikov"], "title": "Leveraging Coordinate Momentum in SignSGD and Muon: Memory-Optimized Zero-Order", "categories": ["cs.LG", "math.OC"], "comment": "26 pages, 5 tables", "summary": "Fine-tuning Large Language Models (LLMs) is essential for adapting\npre-trained models to downstream tasks. Yet traditional first-order optimizers\nsuch as Stochastic Gradient Descent (SGD) and Adam incur prohibitive memory and\ncomputational costs that scale poorly with model size. In this paper, we\ninvestigate zero-order (ZO) optimization methods as a memory- and\ncompute-efficient alternative, particularly in the context of\nparameter-efficient fine-tuning techniques like LoRA. We propose\n$\\texttt{JAGUAR SignSGD}$, a ZO momentum-based algorithm that extends ZO\nSignSGD, requiring the same number of parameters as the standard ZO SGD and\nonly $\\mathcal{O}(1)$ function evaluations per iteration. To the best of our\nknowledge, this is the first study to establish rigorous convergence guarantees\nfor SignSGD in the stochastic ZO case. We further propose $\\texttt{JAGUAR\nMuon}$, a novel ZO extension of the Muon optimizer that leverages the matrix\nstructure of model parameters, and we provide its convergence rate under\narbitrary stochastic noise. Through extensive experiments on challenging LLM\nfine-tuning benchmarks, we demonstrate that the proposed algorithms meet or\nexceed the convergence quality of standard first-order methods, achieving\nsignificant memory reduction. Our theoretical and empirical results establish\nnew ZO optimization methods as a practical and theoretically grounded approach\nfor resource-constrained LLM adaptation. Our code is available at\nhttps://github.com/brain-mmo-lab/ZO_LLM", "AI": {"tldr": "The paper introduces zero-order (ZO) optimization methods, specifically JAGUAR SignSGD and JAGUAR Muon, as efficient alternatives to traditional first-order optimizers for fine-tuning large language models (LLMs), reducing memory and computational costs while maintaining convergence quality.", "motivation": "Traditional first-order optimizers like SGD and Adam are computationally expensive for fine-tuning LLMs. The paper explores ZO methods to address these inefficiencies, focusing on parameter-efficient techniques like LoRA.", "method": "The authors propose JAGUAR SignSGD, a ZO momentum-based algorithm, and JAGUAR Muon, a ZO extension of the Muon optimizer. Both methods aim to reduce memory usage and computational overhead while ensuring convergence.", "result": "Experiments show the proposed ZO methods match or surpass the convergence quality of first-order methods, achieving significant memory savings. Theoretical guarantees for convergence are also provided.", "conclusion": "The study establishes ZO optimization as a practical and theoretically sound approach for resource-efficient LLM fine-tuning, with promising empirical and theoretical results."}}
{"id": "2506.04432", "pdf": "https://arxiv.org/pdf/2506.04432", "abs": "https://arxiv.org/abs/2506.04432", "authors": ["Zixuan Xia", "Aram Davtyan", "Paolo Favaro"], "title": "KOALA++: Efficient Kalman-Based Optimization of Neural Networks with Gradient-Covariance Products", "categories": ["cs.LG", "math.OC"], "comment": null, "summary": "We propose KOALA++, a scalable Kalman-based optimization algorithm that\nexplicitly models structured gradient uncertainty in neural network training.\nUnlike second-order methods, which rely on expensive second order gradient\ncalculation, our method directly estimates the parameter covariance matrix by\nrecursively updating compact gradient covariance products. This design improves\nupon the original KOALA framework that assumed diagonal covariance by\nimplicitly capturing richer uncertainty structure without storing the full\ncovariance matrix and avoiding large matrix inversions. Across diverse tasks,\nincluding image classification and language modeling, KOALA++ achieves accuracy\non par or better than state-of-the-art first- and second-order optimizers while\nmaintaining the efficiency of first-order methods.", "AI": {"tldr": "KOALA++ is a scalable Kalman-based optimization algorithm for neural network training, improving upon KOALA by modeling structured gradient uncertainty efficiently without costly second-order calculations.", "motivation": "Second-order methods are computationally expensive due to second-order gradient calculations. KOALA++ aims to capture richer uncertainty structures more efficiently.", "method": "KOALA++ estimates the parameter covariance matrix recursively using compact gradient covariance products, avoiding full matrix storage and inversions.", "result": "KOALA++ matches or outperforms state-of-the-art optimizers in tasks like image classification and language modeling while maintaining first-order efficiency.", "conclusion": "KOALA++ offers a scalable and efficient alternative to second-order methods, balancing accuracy and computational cost."}}
{"id": "2506.04434", "pdf": "https://arxiv.org/pdf/2506.04434", "abs": "https://arxiv.org/abs/2506.04434", "authors": ["Hari K. Prakash", "Charles H. Martin"], "title": "Grokking and Generalization Collapse: Insights from \\texttt{HTSR} theory", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "15 pages,7 figs", "summary": "We study the well-known grokking phenomena in neural networks (NNs) using a\n3-layer MLP trained on 1 k-sample subset of MNIST, with and without weight\ndecay, and discover a novel third phase -- \\emph{anti-grokking} -- that occurs\nvery late in training and resembles but is distinct from the familiar\n\\emph{pre-grokking} phases: test accuracy collapses while training accuracy\nstays perfect. This late-stage collapse is distinct, from the known\npre-grokking and grokking phases, and is not detected by other proposed\ngrokking progress measures. Leveraging Heavy-Tailed Self-Regularization HTSR\nthrough the open-source WeightWatcher tool, we show that the HTSR layer quality\nmetric $\\alpha$ alone delineates all three phases, whereas the best competing\nmetrics detect only the first two. The \\emph{anti-grokking} is revealed by\ntraining for $10^7$ and is invariably heralded by $\\alpha < 2$ and the\nappearance of \\emph{Correlation Traps} -- outlier singular values in the\nrandomized layer weight matrices that make the layer weight matrix atypical and\nsignal overfitting of the training set. Such traps are verified by visual\ninspection of the layer-wise empirical spectral densities, and by using\nKolmogorov--Smirnov tests on randomized spectra. Comparative metrics, including\nactivation sparsity, absolute weight entropy, circuit complexity, and $l^2$\nweight norms track pre-grokking and grokking but fail to distinguish grokking\nfrom anti-grokking. This discovery provides a way to measure overfitting and\ngeneralization collapse without direct access to the test data. These results\nstrengthen the claim that the \\emph{HTSR} $\\alpha$ provides universal\nlayer-convergence target at $\\alpha \\approx 2$ and underscore the value of\nusing the HTSR alpha $(\\alpha)$ metric as a measure of generalization.", "AI": {"tldr": "The paper identifies a novel 'anti-grokking' phase in neural networks, distinct from pre-grokking and grokking, where test accuracy collapses while training accuracy remains high. The Heavy-Tailed Self-Regularization (HTSR) metric \u03b1 effectively detects all three phases, unlike other metrics.", "motivation": "To understand the grokking phenomena in neural networks and identify new phases of behavior, particularly focusing on late-stage generalization collapse.", "method": "A 3-layer MLP is trained on a 1k-sample MNIST subset with/without weight decay. The HTSR metric \u03b1 is used to analyze layer quality, alongside other metrics like activation sparsity and weight norms.", "result": "Anti-grokking is a distinct phase marked by \u03b1 < 2 and Correlation Traps, signaling overfitting. HTSR \u03b1 uniquely identifies all phases, while other metrics fail.", "conclusion": "HTSR \u03b1 is a universal layer-convergence target (\u03b1 \u2248 2) and a robust measure of generalization, providing insights into overfitting without test data access."}}
{"id": "2506.04439", "pdf": "https://arxiv.org/pdf/2506.04439", "abs": "https://arxiv.org/abs/2506.04439", "authors": ["Robin Yadav", "Qi Yan", "Guy Wolf", "Avishek Joey Bose", "Renjie Liao"], "title": "RETRO SYNFLOW: Discrete Flow Matching for Accurate and Diverse Single-Step Retrosynthesis", "categories": ["cs.LG"], "comment": null, "summary": "A fundamental problem in organic chemistry is identifying and predicting the\nseries of reactions that synthesize a desired target product molecule. Due to\nthe combinatorial nature of the chemical search space, single-step reactant\nprediction -- i.e. single-step retrosynthesis -- remains challenging even for\nexisting state-of-the-art template-free generative approaches to produce an\naccurate yet diverse set of feasible reactions. In this paper, we model\nsingle-step retrosynthesis planning and introduce RETRO SYNFLOW (RSF) a\ndiscrete flow-matching framework that builds a Markov bridge between the\nprescribed target product molecule and the reactant molecule. In contrast to\npast approaches, RSF employs a reaction center identification step to produce\nintermediate structures known as synthons as a more informative source\ndistribution for the discrete flow. To further enhance diversity and\nfeasibility of generated samples, we employ Feynman-Kac steering with\nSequential Monte Carlo based resampling to steer promising generations at\ninference using a new reward oracle that relies on a forward-synthesis model.\nEmpirically, we demonstrate \\nameshort achieves $60.0 \\%$ top-1 accuracy, which\noutperforms the previous SOTA by $20 \\%$. We also substantiate the benefits of\nsteering at inference and demonstrate that FK-steering improves top-$5$\nround-trip accuracy by $19 \\%$ over prior template-free SOTA methods, all while\npreserving competitive top-$k$ accuracy results.", "AI": {"tldr": "RETRO SYNFLOW (RSF) introduces a discrete flow-matching framework for single-step retrosynthesis, outperforming SOTA with 60% top-1 accuracy and improving diversity via Feynman-Kac steering.", "motivation": "Addressing the challenge of accurate and diverse single-step retrosynthesis prediction in organic chemistry due to the combinatorial chemical search space.", "method": "RSF uses a Markov bridge between target and reactant molecules, reaction center identification for synthons, and Feynman-Kac steering with Sequential Monte Carlo resampling.", "result": "Achieves 60.0% top-1 accuracy (20% better than SOTA) and 19% improvement in top-5 round-trip accuracy.", "conclusion": "RSF is a robust template-free method for retrosynthesis, enhancing accuracy and feasibility through innovative steering techniques."}}
{"id": "2506.04446", "pdf": "https://arxiv.org/pdf/2506.04446", "abs": "https://arxiv.org/abs/2506.04446", "authors": ["Gil I. Shamir", "Manfred K. Warmuth"], "title": "Selective Matching Losses -- Not All Scores Are Created Equal", "categories": ["cs.LG"], "comment": null, "summary": "Learning systems match predicted scores to observations over some domain.\nOften, it is critical to produce accurate predictions in some subset (or\nregion) of the domain, yet less important to accurately predict in other\nregions. We construct selective matching loss functions by design of increasing\nlink functions over score domains. A matching loss is an integral over the\nlink. A link defines loss sensitivity as function of the score, emphasizing\nhigh slope high sensitivity regions over flat ones. Loss asymmetry drives a\nmodel and resolves its underspecification to predict better in high sensitivity\nregions where it is more important, and to distinguish between high and low\nimportance regions. A large variety of selective scalar losses can be designed\nwith scaled and shifted Sigmoid and hyperbolic sine links. Their properties,\nhowever, do not extend to multi-class. Applying them per dimension lacks\nranking sensitivity that assigns importance according to class score ranking.\nUtilizing composite Softmax functions, we develop a framework for\nmultidimensional selective losses. We overcome limitations of the standard\nSoftmax function, that is good for classification, but not for distinction\nbetween adjacent scores. Selective losses have substantial advantage over\ntraditional losses in applications with more important score regions, including\ndwell-time prediction, retrieval, ranking with either pointwise, contrastive\npairwise, or listwise losses, distillation problems, and fine-tuning alignment\nof Large Language Models (LLMs).", "AI": {"tldr": "The paper introduces selective matching loss functions to prioritize accurate predictions in specific regions of a domain, using link functions to emphasize high-sensitivity areas. It extends these losses to multi-class settings and demonstrates their advantages in various applications.", "motivation": "Accurate predictions are often critical in specific regions of a domain, but traditional loss functions treat all regions equally. The paper aims to address this by designing loss functions that prioritize high-importance regions.", "method": "The authors construct selective matching loss functions using increasing link functions (e.g., Sigmoid, hyperbolic sine) to emphasize high-sensitivity regions. They extend this to multi-class settings using composite Softmax functions.", "result": "Selective losses outperform traditional losses in applications like dwell-time prediction, retrieval, ranking, and fine-tuning LLMs, by better distinguishing between high and low importance regions.", "conclusion": "Selective loss functions provide a flexible and effective way to improve prediction accuracy in critical regions, with broad applicability in machine learning tasks."}}
{"id": "2506.04454", "pdf": "https://arxiv.org/pdf/2506.04454", "abs": "https://arxiv.org/abs/2506.04454", "authors": ["Huynh T. T. Tran", "Jacob Sander", "Achraf Cohen", "Brian Jalaian", "Nathaniel D. Bastian"], "title": "Neurosymbolic Artificial Intelligence for Robust Network Intrusion Detection: From Scratch to Transfer Learning", "categories": ["cs.LG"], "comment": "17 pages, 5 figures, 11 tables", "summary": "Network Intrusion Detection Systems (NIDS) play a vital role in protecting\ndigital infrastructures against increasingly sophisticated cyber threats. In\nthis paper, we extend ODXU, a Neurosymbolic AI (NSAI) framework that integrates\ndeep embedded clustering for feature extraction, symbolic reasoning using\nXGBoost, and comprehensive uncertainty quantification (UQ) to enhance\nrobustness, interpretability, and generalization in NIDS. The extended ODXU\nincorporates score-based methods (e.g., Confidence Scoring, Shannon Entropy)\nand metamodel-based techniques, including SHAP values and Information Gain, to\nassess the reliability of predictions. Experimental results on the CIC-IDS-2017\ndataset show that ODXU outperforms traditional neural models across six\nevaluation metrics, including classification accuracy and false omission rate.\nWhile transfer learning has seen widespread adoption in fields such as computer\nvision and natural language processing, its potential in cybersecurity has not\nbeen thoroughly explored. To bridge this gap, we develop a transfer learning\nstrategy that enables the reuse of a pre-trained ODXU model on a different\ndataset. Our ablation study on ACI-IoT-2023 demonstrates that the optimal\ntransfer configuration involves reusing the pre-trained autoencoder, retraining\nthe clustering module, and fine-tuning the XGBoost classifier, and outperforms\ntraditional neural models when trained with as few as 16,000 samples\n(approximately 50% of the training data). Additionally, results show that\nmetamodel-based UQ methods consistently outperform score-based approaches on\nboth datasets.", "AI": {"tldr": "The paper extends ODXU, a Neurosymbolic AI framework, for Network Intrusion Detection Systems (NIDS), integrating deep clustering, symbolic reasoning, and uncertainty quantification. It outperforms traditional models and explores transfer learning in cybersecurity.", "motivation": "To enhance robustness, interpretability, and generalization in NIDS by extending the ODXU framework with advanced uncertainty quantification and transfer learning capabilities.", "method": "Extends ODXU with score-based and metamodel-based uncertainty quantification methods, and develops a transfer learning strategy for cybersecurity applications.", "result": "ODXU outperforms traditional neural models on the CIC-IDS-2017 dataset and shows effective transfer learning on the ACI-IoT-2023 dataset with minimal data.", "conclusion": "The extended ODXU framework improves NIDS performance and demonstrates the viability of transfer learning in cybersecurity, with metamodel-based UQ methods being superior."}}
{"id": "2506.04461", "pdf": "https://arxiv.org/pdf/2506.04461", "abs": "https://arxiv.org/abs/2506.04461", "authors": ["Ivan Vegner", "Sydelle de Souza", "Valentin Forch", "Martha Lewis", "Leonidas A. A. Doumas"], "title": "Behavioural vs. Representational Systematicity in End-to-End Models: An Opinionated Survey", "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.6; I.2.0; I.2.7"], "comment": "To appear at ACL 2025 Main Conference", "summary": "A core aspect of compositionality, systematicity is a desirable property in\nML models as it enables strong generalization to novel contexts. This has led\nto numerous studies proposing benchmarks to assess systematic generalization,\nas well as models and training regimes designed to enhance it. Many of these\nefforts are framed as addressing the challenge posed by Fodor and Pylyshyn.\nHowever, while they argue for systematicity of representations, existing\nbenchmarks and models primarily focus on the systematicity of behaviour. We\nemphasize the crucial nature of this distinction. Furthermore, building on\nHadley's (1994) taxonomy of systematic generalization, we analyze the extent to\nwhich behavioural systematicity is tested by key benchmarks in the literature\nacross language and vision. Finally, we highlight ways of assessing\nsystematicity of representations in ML models as practiced in the field of\nmechanistic interpretability.", "AI": {"tldr": "The paper discusses the distinction between systematicity of behavior and representations in ML models, analyzes benchmarks for behavioral systematicity, and suggests assessing representational systematicity using mechanistic interpretability.", "motivation": "To address the gap in ML models where systematicity is often measured by behavior rather than representations, as argued by Fodor and Pylyshyn.", "method": "Analyzes benchmarks for behavioral systematicity in language and vision, and explores mechanistic interpretability for assessing representational systematicity.", "result": "Highlights the limitations of current benchmarks in testing representational systematicity and proposes alternative assessment methods.", "conclusion": "Emphasizes the need to focus on systematicity of representations in ML models and suggests leveraging mechanistic interpretability for better evaluation."}}
{"id": "2506.04474", "pdf": "https://arxiv.org/pdf/2506.04474", "abs": "https://arxiv.org/abs/2506.04474", "authors": ["Mohammad Subhi Al-Batah", "Mowafaq Salem Alzboon", "Muhyeeddin Alqaraleh", "Mohammed Hasan Abu-Arqoub", "Rashiq Rafiq Marie"], "title": "Classifying Dental Care Providers Through Machine Learning with Features Ranking", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "This study investigates the application of machine learning (ML) models for\nclassifying dental providers into two categories - standard rendering providers\nand safety net clinic (SNC) providers - using a 2018 dataset of 24,300\ninstances with 20 features. The dataset, characterized by high missing values\n(38.1%), includes service counts (preventive, treatment, exams), delivery\nsystems (FFS, managed care), and beneficiary demographics. Feature ranking\nmethods such as information gain, Gini index, and ANOVA were employed to\nidentify critical predictors, revealing treatment-related metrics\n(TXMT_USER_CNT, TXMT_SVC_CNT) as top-ranked features. Twelve ML models,\nincluding k-Nearest Neighbors (kNN), Decision Trees, Support Vector Machines\n(SVM), Stochastic Gradient Descent (SGD), Random Forest, Neural Networks, and\nGradient Boosting, were evaluated using 10-fold cross-validation.\nClassification accuracy was tested across incremental feature subsets derived\nfrom rankings. The Neural Network achieved the highest accuracy (94.1%) using\nall 20 features, followed by Gradient Boosting (93.2%) and Random Forest\n(93.0%). Models showed improved performance as more features were incorporated,\nwith SGD and ensemble methods demonstrating robustness to missing data. Feature\nranking highlighted the dominance of treatment service counts and annotation\ncodes in distinguishing provider types, while demographic variables (AGE_GROUP,\nCALENDAR_YEAR) had minimal impact. The study underscores the importance of\nfeature selection in enhancing model efficiency and accuracy, particularly in\nimbalanced healthcare datasets. These findings advocate for integrating\nfeature-ranking techniques with advanced ML algorithms to optimize dental\nprovider classification, enabling targeted resource allocation for underserved\npopulations.", "AI": {"tldr": "The study uses ML models to classify dental providers into standard and safety net clinic categories, achieving high accuracy (94.1%) with Neural Networks. Feature selection and ranking are key to performance.", "motivation": "To improve classification of dental providers for better resource allocation, especially for underserved populations, using ML models on a dataset with high missing values.", "method": "Evaluated 12 ML models (e.g., Neural Networks, Gradient Boosting) on a 2018 dataset with 20 features, using feature ranking (information gain, Gini index, ANOVA) and 10-fold cross-validation.", "result": "Neural Network achieved the highest accuracy (94.1%). Treatment-related metrics were top-ranked features, while demographics had minimal impact.", "conclusion": "Feature selection and advanced ML models enhance classification accuracy, aiding targeted resource allocation in healthcare."}}
{"id": "2506.04479", "pdf": "https://arxiv.org/pdf/2506.04479", "abs": "https://arxiv.org/abs/2506.04479", "authors": ["Mohammad Subhi Al-Batah", "Muhyeeddin Alqaraleh", "Mowafaq Salem Alzboon", "Abdullah Alourani"], "title": "Comparative performance of ensemble models in predicting dental provider types: insights from fee-for-service data", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Dental provider classification plays a crucial role in optimizing healthcare\nresource allocation and policy planning. Effective categorization of providers,\nsuch as standard rendering providers and safety net clinic (SNC) providers,\nenhances service delivery to underserved populations. This study aimed to\nevaluate the performance of machine learning models in classifying dental\nproviders using a 2018 dataset. A dataset of 24,300 instances with 20 features\nwas analyzed, including beneficiary and service counts across fee-for-service\n(FFS), Geographic Managed Care, and Pre-Paid Health Plans. Providers were\ncategorized by delivery system and patient age groups (0-20 and 21+). Despite\n38.1% missing data, multiple machine learning algorithms were tested, including\nk-Nearest Neighbors (kNN), Decision Trees, Support Vector Machines (SVM),\nStochastic Gradient Descent (SGD), Random Forest, Neural Networks, and Gradient\nBoosting. A 10-fold cross-validation approach was applied, and models were\nevaluated using AUC, classification accuracy (CA), F1-score, precision, and\nrecall. Neural Networks achieved the highest AUC (0.975) and CA (94.1%),\nfollowed by Random Forest (AUC: 0.948, CA: 93.0%). These models effectively\nhandled imbalanced data and complex feature interactions, outperforming\ntraditional classifiers like Logistic Regression and SVM. Advanced machine\nlearning techniques, particularly ensemble and deep learning models,\nsignificantly enhance dental workforce classification. Their integration into\nhealthcare analytics can improve provider identification and resource\ndistribution, benefiting underserved populations.", "AI": {"tldr": "Machine learning models, especially Neural Networks and Random Forest, outperform traditional methods in classifying dental providers, aiding healthcare resource allocation.", "motivation": "Effective classification of dental providers (e.g., standard vs. safety net clinic providers) is crucial for optimizing healthcare resource allocation and serving underserved populations.", "method": "Analyzed a 2018 dataset of 24,300 instances with 20 features, testing multiple ML algorithms (kNN, Decision Trees, SVM, SGD, Random Forest, Neural Networks, Gradient Boosting) using 10-fold cross-validation and metrics like AUC, CA, F1-score, precision, and recall.", "result": "Neural Networks achieved the highest AUC (0.975) and CA (94.1%), followed by Random Forest (AUC: 0.948, CA: 93.0%), outperforming traditional classifiers.", "conclusion": "Advanced ML models, particularly ensemble and deep learning, enhance dental provider classification, improving healthcare analytics and resource distribution for underserved populations."}}
{"id": "2506.04487", "pdf": "https://arxiv.org/pdf/2506.04487", "abs": "https://arxiv.org/abs/2506.04487", "authors": ["C. Evans Hedges"], "title": "Orthogonal Gradient Descent Improves Neural Calibration", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "We provide evidence that orthogonalizing gradients during training improves\nmodel calibration without sacrificing accuracy. On CIFAR-10 with 10% labeled\ndata, $\\perp$Grad matches SGD in accuracy but yields consistently improved\ncalibration metrics such as lower test loss, reduced softmax overconfidence,\nand higher predictive entropy. These benefits persist under input corruption\n(CIFAR-10C) and extended training, where $\\perp$Grad models degrade more\ngracefully than SGD-trained counterparts. $\\perp$Grad is optimizer-agnostic,\nincurs minimal overhead, and works well with post-hoc calibration techniques\nlike temperature scaling.\n  Theoretically, we prove convergence of a simplified version of $\\perp$Grad\nunder mild assumptions and characterize its stationary points in positive\nhomogeneous networks: $\\perp$Grad converges to solutions where further loss\nreduction requires confidence scaling rather than decision boundary\nimprovement.", "AI": {"tldr": "Orthogonalizing gradients ($\\perp$Grad) during training improves model calibration without losing accuracy, outperforming SGD in calibration metrics and robustness.", "motivation": "To enhance model calibration (e.g., lower test loss, reduced overconfidence) while maintaining accuracy, especially in limited-data or corrupted-input scenarios.", "method": "Introduces $\\perp$Grad, an optimizer-agnostic technique that orthogonalizes gradients during training, incurring minimal overhead.", "result": "$\\perp$Grad matches SGD in accuracy but improves calibration (lower test loss, reduced overconfidence, higher entropy) and degrades more gracefully under corruption or extended training.", "conclusion": "$\\perp$Grad is a practical, theoretically grounded method for better calibration, compatible with post-hoc techniques like temperature scaling."}}
{"id": "2506.04490", "pdf": "https://arxiv.org/pdf/2506.04490", "abs": "https://arxiv.org/abs/2506.04490", "authors": ["Rishwanth Raghu", "Axel Levy", "Gordon Wetzstein", "Ellen D. Zhong"], "title": "Multiscale guidance of AlphaFold3 with heterogeneous cryo-EM data", "categories": ["cs.LG", "q-bio.BM"], "comment": null, "summary": "Protein structure prediction models are now capable of generating accurate 3D\nstructural hypotheses from sequence alone. However, they routinely fail to\ncapture the conformational diversity of dynamic biomolecular complexes, often\nrequiring heuristic MSA subsampling approaches for generating alternative\nstates. In parallel, cryo-electron microscopy (cryo-EM) has emerged as a\npowerful tool for imaging near-native structural heterogeneity, but is\nchallenged by arduous pipelines to go from raw experimental data to atomic\nmodels. Here, we bridge the gap between these modalities, combining cryo-EM\ndensity maps with the rich sequence and biophysical priors learned by protein\nstructure prediction models. Our method, CryoBoltz, guides the sampling\ntrajectory of a pretrained protein structure prediction model using both global\nand local structural constraints derived from density maps, driving predictions\ntowards conformational states consistent with the experimental data. We\ndemonstrate that this flexible yet powerful inference-time approach allows us\nto build atomic models into heterogeneous cryo-EM maps across a variety of\ndynamic biomolecular systems including transporters and antibodies.", "AI": {"tldr": "CryoBoltz bridges protein structure prediction and cryo-EM by guiding predictions with experimental data to model dynamic biomolecular systems.", "motivation": "Current protein structure prediction models lack conformational diversity, and cryo-EM faces challenges in deriving atomic models from raw data.", "method": "CryoBoltz integrates cryo-EM density maps with protein structure prediction models, using global and local constraints to guide sampling.", "result": "The method successfully builds atomic models for dynamic systems like transporters and antibodies.", "conclusion": "CryoBoltz effectively combines computational and experimental data to improve modeling of biomolecular diversity."}}
{"id": "2506.04528", "pdf": "https://arxiv.org/pdf/2506.04528", "abs": "https://arxiv.org/abs/2506.04528", "authors": ["Ruoxi Jiang", "Xiao Zhang", "Karan Jakhar", "Peter Y. Lu", "Pedram Hassanzadeh", "Michael Maire", "Rebecca Willett"], "title": "Hierarchical Implicit Neural Emulators", "categories": ["cs.LG"], "comment": null, "summary": "Neural PDE solvers offer a powerful tool for modeling complex dynamical\nsystems, but often struggle with error accumulation over long time horizons and\nmaintaining stability and physical consistency. We introduce a multiscale\nimplicit neural emulator that enhances long-term prediction accuracy by\nconditioning on a hierarchy of lower-dimensional future state representations.\nDrawing inspiration from the stability properties of numerical implicit\ntime-stepping methods, our approach leverages predictions several steps ahead\nin time at increasing compression rates for next-timestep refinements. By\nactively adjusting the temporal downsampling ratios, our design enables the\nmodel to capture dynamics across multiple granularities and enforce long-range\ntemporal coherence. Experiments on turbulent fluid dynamics show that our\nmethod achieves high short-term accuracy and produces long-term stable\nforecasts, significantly outperforming autoregressive baselines while adding\nminimal computational overhead.", "AI": {"tldr": "A multiscale implicit neural emulator improves long-term prediction accuracy in neural PDE solvers by leveraging hierarchical future state representations and adaptive temporal downsampling.", "motivation": "Neural PDE solvers often suffer from error accumulation and instability over long time horizons, limiting their reliability for modeling complex dynamical systems.", "method": "The approach conditions predictions on a hierarchy of lower-dimensional future states, inspired by implicit time-stepping methods, and adjusts temporal downsampling ratios to capture multiscale dynamics.", "result": "The method achieves high short-term accuracy and stable long-term forecasts in turbulent fluid dynamics, outperforming autoregressive baselines with minimal computational overhead.", "conclusion": "The multiscale implicit neural emulator enhances the reliability and accuracy of neural PDE solvers for long-term predictions."}}
{"id": "2506.04531", "pdf": "https://arxiv.org/pdf/2506.04531", "abs": "https://arxiv.org/abs/2506.04531", "authors": ["Geon-Woo Kim", "Junbo Li", "Shashidhar Gandham", "Omar Baldonado", "Adithya Gangidi", "Pavan Balaji", "Zhangyang Wang", "Aditya Akella"], "title": "HALoS: Hierarchical Asynchronous Local SGD over Slow Networks for Geo-Distributed Large Language Model Training", "categories": ["cs.LG"], "comment": null, "summary": "Training large language models (LLMs) increasingly relies on geographically\ndistributed accelerators, causing prohibitive communication costs across\nregions and uneven utilization of heterogeneous hardware. We propose HALoS, a\nhierarchical asynchronous optimization framework that tackles these issues by\nintroducing local parameter servers (LPSs) within each region and a global\nparameter server (GPS) that merges updates across regions. This hierarchical\ndesign minimizes expensive inter-region communication, reduces straggler\neffects, and leverages fast intra-region links. We provide a rigorous\nconvergence analysis for HALoS under non-convex objectives, including\ntheoretical guarantees on the role of hierarchical momentum in asynchronous\ntraining. Empirically, HALoS attains up to 7.5x faster convergence than\nsynchronous baselines in geo-distributed LLM training and improves upon\nexisting asynchronous methods by up to 2.1x. Crucially, HALoS preserves the\nmodel quality of fully synchronous SGD-matching or exceeding accuracy on\nstandard language modeling and downstream benchmarks-while substantially\nlowering total training time. These results demonstrate that hierarchical,\nserver-side update accumulation and global model merging are powerful tools for\nscalable, efficient training of new-era LLMs in heterogeneous, geo-distributed\nenvironments.", "AI": {"tldr": "HALoS is a hierarchical asynchronous optimization framework for training large language models (LLMs) in geo-distributed environments, reducing communication costs and improving efficiency.", "motivation": "Training LLMs across geographically distributed accelerators faces high communication costs and uneven hardware utilization.", "method": "HALoS introduces local and global parameter servers (LPSs and GPS) to minimize inter-region communication and leverage fast intra-region links.", "result": "HALoS achieves up to 7.5x faster convergence than synchronous baselines and 2.1x improvement over asynchronous methods, while maintaining model quality.", "conclusion": "Hierarchical server-side update accumulation and global model merging are effective for scalable, efficient LLM training in heterogeneous, geo-distributed settings."}}
{"id": "2506.04536", "pdf": "https://arxiv.org/pdf/2506.04536", "abs": "https://arxiv.org/abs/2506.04536", "authors": ["Luca Ghafourpour", "Valentin Duruisseaux", "Bahareh Tolooshams", "Philip H. Wong", "Costas A. Anastassiou", "Anima Anandkumar"], "title": "NOBLE -- Neural Operator with Biologically-informed Latent Embeddings to Capture Experimental Variability in Biological Neuron Models", "categories": ["cs.LG", "cs.AI", "q-bio.NC"], "comment": null, "summary": "Characterizing the diverse computational properties of human neurons via\nmultimodal electrophysiological, transcriptomic, and morphological data\nprovides the foundation for constructing and validating bio-realistic neuron\nmodels that can advance our understanding of fundamental mechanisms underlying\nbrain function. However, current modeling approaches remain constrained by the\nlimited availability and intrinsic variability of experimental neuronal data.\nTo capture variability, ensembles of deterministic models are often used, but\nare difficult to scale as model generation requires repeating computationally\nexpensive optimization for each neuron. While deep learning is becoming\nincreasingly relevant in this space, it fails to capture the full biophysical\ncomplexity of neurons, their nonlinear voltage dynamics, and variability. To\naddress these shortcomings, we introduce NOBLE, a neural operator framework\nthat learns a mapping from a continuous frequency-modulated embedding of\ninterpretable neuron features to the somatic voltage response induced by\ncurrent injection. Trained on data generated from biophysically realistic\nneuron models, NOBLE predicts distributions of neural dynamics accounting for\nthe intrinsic experimental variability. Unlike conventional bio-realistic\nneuron models, interpolating within the embedding space offers models whose\ndynamics are consistent with experimentally observed responses. NOBLE is the\nfirst scaled-up deep learning framework validated on real experimental data,\nenabling efficient generation of synthetic neurons that exhibit trial-to-trial\nvariability and achieve a $4200\\times$ speedup over numerical solvers. To this\nend, NOBLE captures fundamental neural properties, opening the door to a better\nunderstanding of cellular composition and computations, neuromorphic\narchitectures, large-scale brain circuits, and general neuroAI applications.", "AI": {"tldr": "NOBLE is a neural operator framework that predicts neural dynamics efficiently, addressing limitations of current bio-realistic neuron models by capturing variability and achieving significant speedup.", "motivation": "Current neuron modeling approaches are limited by data availability and variability, and deep learning lacks biophysical complexity. NOBLE aims to bridge this gap.", "method": "NOBLE learns a mapping from neuron features to somatic voltage responses, trained on biophysically realistic models, and predicts neural dynamics with variability.", "result": "NOBLE achieves a 4200\u00d7 speedup over numerical solvers and generates synthetic neurons with trial-to-trial variability, validated on experimental data.", "conclusion": "NOBLE advances neuron modeling, enabling better understanding of brain function, neuromorphic architectures, and neuroAI applications."}}
{"id": "2506.04542", "pdf": "https://arxiv.org/pdf/2506.04542", "abs": "https://arxiv.org/abs/2506.04542", "authors": ["Yuanpei Gao", "Qi Yan", "Yan Leng", "Renjie Liao"], "title": "Neural MJD: Neural Non-Stationary Merton Jump Diffusion for Time Series Prediction", "categories": ["cs.LG"], "comment": null, "summary": "While deep learning methods have achieved strong performance in time series\nprediction, their black-box nature and inability to explicitly model underlying\nstochastic processes often limit their generalization to non-stationary data,\nespecially in the presence of abrupt changes. In this work, we introduce Neural\nMJD, a neural network based non-stationary Merton jump diffusion (MJD) model.\nOur model explicitly formulates forecasting as a stochastic differential\nequation (SDE) simulation problem, combining a time-inhomogeneous It\\^o\ndiffusion to capture non-stationary stochastic dynamics with a\ntime-inhomogeneous compound Poisson process to model abrupt jumps. To enable\ntractable learning, we introduce a likelihood truncation mechanism that caps\nthe number of jumps within small time intervals and provide a theoretical error\nbound for this approximation. Additionally, we propose an Euler-Maruyama with\nrestart solver, which achieves a provably lower error bound in estimating\nexpected states and reduced variance compared to the standard solver.\nExperiments on both synthetic and real-world datasets demonstrate that Neural\nMJD consistently outperforms state-of-the-art deep learning and statistical\nlearning methods.", "AI": {"tldr": "Neural MJD combines neural networks with a non-stationary Merton jump diffusion model to improve time series prediction by explicitly modeling stochastic processes and abrupt changes.", "motivation": "Deep learning methods struggle with non-stationary data and abrupt changes due to their black-box nature and lack of explicit stochastic modeling.", "method": "Neural MJD formulates forecasting as an SDE simulation, using a time-inhomogeneous It\u00f4 diffusion and compound Poisson process. It includes a likelihood truncation mechanism and an improved solver (Euler-Maruyama with restart).", "result": "Neural MJD outperforms state-of-the-art deep learning and statistical methods on synthetic and real-world datasets.", "conclusion": "The model effectively addresses non-stationary data and abrupt changes, offering improved generalization and performance."}}
{"id": "2506.04548", "pdf": "https://arxiv.org/pdf/2506.04548", "abs": "https://arxiv.org/abs/2506.04548", "authors": ["Dev Gurung", "Shiva Raj Pokhrel"], "title": "Communication Efficient Adaptive Model-Driven Quantum Federated Learning", "categories": ["cs.LG"], "comment": null, "summary": "Training with huge datasets and a large number of participating devices leads\nto bottlenecks in federated learning (FL). Furthermore, the challenges of\nheterogeneity between multiple FL clients affect the overall performance of the\nsystem. In a quantum federated learning (QFL) context, we address these three\nmain challenges: i) training bottlenecks from massive datasets, ii) the\ninvolvement of a substantial number of devices, and iii) non-IID data\ndistributions. We introduce a model-driven quantum federated learning algorithm\n(mdQFL) to tackle these challenges. Our proposed approach is efficient and\nadaptable to various factors, including different numbers of devices. To the\nbest of our knowledge, it is the first to explore training and update\npersonalization, as well as test generalization within a QFL setting, which can\nbe applied to other FL scenarios. We evaluated the efficiency of the proposed\nmdQFL framework through extensive experiments under diverse non-IID data\nheterogeneity conditions using various datasets within the Qiskit environment.\nOur results demonstrate a nearly 50% decrease in total communication costs\nwhile maintaining or, in some cases, exceeding the accuracy of the final model\nand consistently improving local model training compared to the standard QFL\nbaseline. Moreover, our experimental evaluation thoroughly explores the QFL and\nmdQFL algorithms, along with several influencing factors. In addition, we\npresent a theoretical analysis to clarify the complexities of the proposed\nalgorithm. The experimental code is available at 1.", "AI": {"tldr": "The paper introduces a model-driven quantum federated learning algorithm (mdQFL) to address challenges in federated learning (FL) like training bottlenecks, large-scale device participation, and non-IID data. It reduces communication costs by 50% while maintaining or improving accuracy.", "motivation": "The challenges of massive datasets, numerous devices, and non-IID data in FL, especially in a quantum context, hinder performance and efficiency.", "method": "The authors propose mdQFL, a model-driven quantum federated learning algorithm, which includes training and update personalization and test generalization.", "result": "Experiments show a 50% reduction in communication costs with maintained or improved accuracy and better local model training compared to standard QFL.", "conclusion": "mdQFL effectively addresses FL challenges in quantum settings, offering efficiency and adaptability, with potential applications in broader FL scenarios."}}
{"id": "2506.04553", "pdf": "https://arxiv.org/pdf/2506.04553", "abs": "https://arxiv.org/abs/2506.04553", "authors": ["Andersen Chang", "Tiffany M. Tang", "Tarek M. Zikry", "Genevera I. Allen"], "title": "Unsupervised Machine Learning for Scientific Discovery: Workflow and Best Practices", "categories": ["cs.LG", "stat.AP", "stat.CO", "stat.ML"], "comment": "23 pages, 4 figures, 12 additional pages of citations", "summary": "Unsupervised machine learning is widely used to mine large, unlabeled\ndatasets to make data-driven discoveries in critical domains such as climate\nscience, biomedicine, astronomy, chemistry, and more. However, despite its\nwidespread utilization, there is a lack of standardization in unsupervised\nlearning workflows for making reliable and reproducible scientific discoveries.\nIn this paper, we present a structured workflow for using unsupervised learning\ntechniques in science. We highlight and discuss best practices starting with\nformulating validatable scientific questions, conducting robust data\npreparation and exploration, using a range of modeling techniques, performing\nrigorous validation by evaluating the stability and generalizability of\nunsupervised learning conclusions, and promoting effective communication and\ndocumentation of results to ensure reproducible scientific discoveries. To\nillustrate our proposed workflow, we present a case study from astronomy,\nseeking to refine globular clusters of Milky Way stars based upon their\nchemical composition. Our case study highlights the importance of validation\nand illustrates how the benefits of a carefully-designed workflow for\nunsupervised learning can advance scientific discovery.", "AI": {"tldr": "The paper proposes a standardized workflow for unsupervised learning in science to ensure reliable and reproducible discoveries, demonstrated through an astronomy case study.", "motivation": "Address the lack of standardization in unsupervised learning workflows, hindering reliable and reproducible scientific discoveries.", "method": "Introduces a structured workflow including scientific question formulation, data preparation, modeling, validation, and result communication.", "result": "Demonstrates the workflow's effectiveness via a case study on refining Milky Way globular clusters by chemical composition.", "conclusion": "A carefully-designed unsupervised learning workflow enhances validation and reproducibility, advancing scientific discovery."}}
{"id": "2506.04566", "pdf": "https://arxiv.org/pdf/2506.04566", "abs": "https://arxiv.org/abs/2506.04566", "authors": ["Kareem Amin", "Salman Avestimehr", "Sara Babakniya", "Alex Bie", "Weiwei Kong", "Natalia Ponomareva", "Umar Syed"], "title": "Clustering and Median Aggregation Improve Differentially Private Inference", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "comment": null, "summary": "Differentially private (DP) language model inference is an approach for\ngenerating private synthetic text. A sensitive input example is used to prompt\nan off-the-shelf large language model (LLM) to produce a similar example.\nMultiple examples can be aggregated together to formally satisfy the DP\nguarantee.\n  Prior work creates inference batches by sampling sensitive inputs uniformly\nat random. We show that uniform sampling degrades the quality of privately\ngenerated text, especially when the sensitive examples concern heterogeneous\ntopics.\n  We remedy this problem by clustering the input data before selecting\ninference batches. Next, we observe that clustering also leads to more similar\nnext-token predictions across inferences. We use this insight to introduce a\nnew algorithm that aggregates next token statistics by privately computing\nmedians instead of averages. This approach leverages the fact that the median\nhas decreased local sensitivity when next token predictions are similar,\nallowing us to state a data-dependent and ex-post DP guarantee about the\nprivacy properties of this algorithm. Finally, we demonstrate improvements in\nterms of representativeness metrics (e.g., MAUVE) as well as downstream task\nperformance. We show that our method produces high-quality synthetic data at\nsignificantly lower privacy cost than a previous state-of-the-art method.", "AI": {"tldr": "The paper improves DP language model inference by clustering input data and using median aggregation for next-token predictions, enhancing text quality and reducing privacy costs.", "motivation": "Uniform sampling in DP language model inference degrades text quality, especially for heterogeneous topics, prompting the need for better methods.", "method": "Clusters input data before inference and aggregates next-token predictions using medians instead of averages, leveraging reduced local sensitivity for better privacy guarantees.", "result": "Demonstrates improved representativeness (e.g., MAUVE) and downstream task performance, with lower privacy costs than prior methods.", "conclusion": "The proposed method produces higher-quality synthetic data at reduced privacy costs, outperforming previous state-of-the-art approaches."}}
{"id": "2506.04567", "pdf": "https://arxiv.org/pdf/2506.04567", "abs": "https://arxiv.org/abs/2506.04567", "authors": ["Ranjith Merugu", "Bryan Bo Cao", "Shubham Jain"], "title": "StatsMerging: Statistics-Guided Model Merging via Task-Specific Teacher Distillation", "categories": ["cs.LG", "cs.CV", "68T05, 68T07, 68T45", "I.4.0; I.4.9; I.5.1; I.5.4"], "comment": "14 pages, 4 figures, 7 tables", "summary": "Model merging has emerged as a promising solution to accommodate multiple\nlarge models within constrained memory budgets. We present StatsMerging, a\nnovel lightweight learning-based model merging method guided by weight\ndistribution statistics without requiring ground truth labels or test samples.\nStatsMerging offers three key advantages: (1) It uniquely leverages singular\nvalues from singular value decomposition (SVD) to capture task-specific weight\ndistributions, serving as a proxy for task importance to guide task coefficient\nprediction; (2) It employs a lightweight learner StatsMergeLearner to model the\nweight distributions of task-specific pre-trained models, improving\ngeneralization and enhancing adaptation to unseen samples; (3) It introduces\nTask-Specific Teacher Distillation for merging vision models with heterogeneous\narchitectures, a merging learning paradigm that avoids costly ground-truth\nlabels by task-specific teacher distillation. Notably, we present two types of\nknowledge distillation, (a) distilling knowledge from task-specific models to\nStatsMergeLearner; and (b) distilling knowledge from models with heterogeneous\narchitectures prior to merging. Extensive experiments across eight tasks\ndemonstrate the effectiveness of StatsMerging. Our results show that\nStatsMerging outperforms state-of-the-art techniques in terms of overall\naccuracy, generalization to unseen tasks, and robustness to image quality\nvariations.", "AI": {"tldr": "StatsMerging is a lightweight, learning-based model merging method that uses weight distribution statistics and SVD to guide merging without needing ground truth labels or test samples. It outperforms state-of-the-art techniques in accuracy, generalization, and robustness.", "motivation": "To address the challenge of accommodating multiple large models within constrained memory budgets efficiently and effectively.", "method": "StatsMerging leverages SVD singular values for task-specific weight distributions, uses a lightweight learner (StatsMergeLearner), and employs Task-Specific Teacher Distillation for merging heterogeneous architectures.", "result": "Outperforms state-of-the-art techniques in accuracy, generalization to unseen tasks, and robustness to image quality variations across eight tasks.", "conclusion": "StatsMerging is a promising solution for model merging, offering efficiency, adaptability, and superior performance without requiring labeled data."}}
{"id": "2506.04598", "pdf": "https://arxiv.org/pdf/2506.04598", "abs": "https://arxiv.org/abs/2506.04598", "authors": ["Marianna Nezhurina", "Tomer Porian", "Giovanni Pucceti", "Tommie Kerssies", "Romain Beaumont", "Mehdi Cherti", "Jenia Jitsev"], "title": "Scaling Laws for Robust Comparison of Open Foundation Language-Vision Models and Datasets", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "Preprint. In Review", "summary": "In studies of transferable learning, scaling laws are obtained for various\nimportant foundation models to predict their properties and performance at\nlarger scales. We show here how scaling law derivation can also be used for\nmodel and dataset comparison, allowing to decide which procedure is to be\npreferred for pre-training. For the first time, full scaling laws based on\ndense measurements across a wide span of model and samples seen scales are\nderived for two important language-vision learning procedures, CLIP and MaMMUT,\nthat use either contrastive only or contrastive and captioning text generative\nloss. Ensuring sufficient prediction accuracy for held out points, we use\nderived scaling laws to compare both models, obtaining evidence for MaMMUT's\nstronger improvement with scale and better sample efficiency than standard\nCLIP. To strengthen validity of the comparison, we show scaling laws for\nvarious downstream tasks, classification, retrieval, and segmentation, and for\ndifferent open datasets, DataComp, DFN and Re-LAION, observing consistently the\nsame trends. We show that comparison can also be performed when deriving\nscaling laws with a constant learning rate schedule, reducing compute cost.\nAccurate derivation of scaling laws provides thus means to perform model and\ndataset comparison across scale spans, avoiding misleading conclusions based on\nmeasurements from single reference scales only, paving the road for systematic\ncomparison and improvement of open foundation models and datasets for their\ncreation. We release all the pre-trained models with their intermediate\ncheckpoints, including openMaMMUT-L/14, which achieves $80.3\\%$ zero-shot\nImageNet-1k accuracy, trained on 12.8B samples from DataComp-1.4B. Code for\nreproducing experiments in the paper and raw experiments data can be found at\nhttps://github.com/LAION-AI/scaling-laws-for-comparison.", "AI": {"tldr": "The paper demonstrates how scaling laws can be used to compare models and datasets for pre-training, focusing on CLIP and MaMMUT. MaMMUT shows better scalability and sample efficiency.", "motivation": "To enable systematic comparison of models and datasets across scales, avoiding misleading conclusions from single-scale measurements.", "method": "Derives full scaling laws for CLIP and MaMMUT using dense measurements across model and sample scales, and evaluates them on downstream tasks.", "result": "MaMMUT outperforms CLIP in scalability and sample efficiency, with consistent trends across tasks and datasets.", "conclusion": "Scaling laws provide a robust method for model and dataset comparison, supporting systematic improvement of foundation models."}}
{"id": "2506.04608", "pdf": "https://arxiv.org/pdf/2506.04608", "abs": "https://arxiv.org/abs/2506.04608", "authors": ["Changsheng Sun", "Xinke Li", "Jin Song Dong"], "title": "Ignoring Directionality Leads to Compromised Graph Neural Network Explanations", "categories": ["cs.LG"], "comment": null, "summary": "Graph Neural Networks (GNNs) are increasingly used in critical domains, where\nreliable explanations are vital for supporting human decision-making. However,\nthe common practice of graph symmetrization discards directional information,\nleading to significant information loss and misleading explanations. Our\nanalysis demonstrates how this practice compromises explanation fidelity.\nThrough theoretical and empirical studies, we show that preserving directional\nsemantics significantly improves explanation quality, ensuring more faithful\ninsights for human decision-makers. These findings highlight the need for\ndirection-aware GNN explainability in security-critical applications.", "AI": {"tldr": "Graph symmetrization in GNNs discards directional info, harming explanation fidelity. Preserving direction improves quality, aiding human decisions.", "motivation": "To address the loss of directional info in GNN explanations, which misleads human decision-making in critical domains.", "method": "Theoretical and empirical analysis of how direction-aware GNNs improve explanation fidelity.", "result": "Preserving directional semantics enhances explanation quality, providing more faithful insights.", "conclusion": "Direction-aware GNN explainability is crucial for security-critical applications."}}
{"id": "2506.04609", "pdf": "https://arxiv.org/pdf/2506.04609", "abs": "https://arxiv.org/abs/2506.04609", "authors": ["Cong Geng", "Jia Wang", "Li Chen", "Zhiyong Gao", "Jes Frellsen", "S\u00f8ren Hauberg"], "title": "Exploring bidirectional bounds for minimax-training of Energy-based models", "categories": ["cs.LG", "cs.CV"], "comment": "accepted to IJCV", "summary": "Energy-based models (EBMs) estimate unnormalized densities in an elegant\nframework, but they are generally difficult to train. Recent work has linked\nEBMs to generative adversarial networks, by noting that they can be trained\nthrough a minimax game using a variational lower bound. To avoid the\ninstabilities caused by minimizing a lower bound, we propose to instead work\nwith bidirectional bounds, meaning that we maximize a lower bound and minimize\nan upper bound when training the EBM. We investigate four different bounds on\nthe log-likelihood derived from different perspectives. We derive lower bounds\nbased on the singular values of the generator Jacobian and on mutual\ninformation. To upper bound the negative log-likelihood, we consider a gradient\npenalty-like bound, as well as one based on diffusion processes. In all cases,\nwe provide algorithms for evaluating the bounds. We compare the different\nbounds to investigate, the pros and cons of the different approaches. Finally,\nwe demonstrate that the use of bidirectional bounds stabilizes EBM training and\nyields high-quality density estimation and sample generation.", "AI": {"tldr": "The paper proposes using bidirectional bounds (maximizing a lower bound and minimizing an upper bound) to stabilize training of Energy-based models (EBMs), comparing four derived bounds and demonstrating improved performance.", "motivation": "EBMs are difficult to train, and existing methods like variational lower bounds can cause instabilities. The authors aim to stabilize training by using bidirectional bounds.", "method": "The authors derive and compare four bounds on log-likelihood: two lower bounds (based on generator Jacobian singular values and mutual information) and two upper bounds (gradient penalty-like and diffusion process-based). Algorithms for evaluating these bounds are provided.", "result": "Bidirectional bounds stabilize EBM training, leading to high-quality density estimation and sample generation.", "conclusion": "Using bidirectional bounds is effective for stabilizing EBM training and improving performance in density estimation and generation tasks."}}
{"id": "2506.04632", "pdf": "https://arxiv.org/pdf/2506.04632", "abs": "https://arxiv.org/abs/2506.04632", "authors": ["Guruprerana Shabadi", "Rajeev Alur"], "title": "Composing Agents to Minimize Worst-case Risk", "categories": ["cs.LG"], "comment": "17 pages, 4 figures", "summary": "From software development to robot control, modern agentic systems decompose\ncomplex objectives into a sequence of subtasks and choose a set of specialized\nAI agents to complete them. We formalize an agentic workflow as a directed\nacyclic graph, called an agent graph, where edges represent AI agents and paths\ncorrespond to feasible compositions of agents. When deploying these systems in\nthe real world, we need to choose compositions of agents that not only maximize\nthe task success, but also minimize risk where the risk captures requirements\nlike safety, fairness, and privacy. This additionally requires carefully\nanalyzing the low-probability (tail) behaviors of compositions of agents. In\nthis work, we consider worst-case risk minimization over the set of feasible\nagent compositions. We define worst-case risk as the tail quantile -- also\nknown as value-at-risk -- of the loss distribution of the agent composition\nwhere the loss quantifies the risk associated with agent behaviors. We\nintroduce an efficient algorithm that traverses the agent graph and finds a\nnear-optimal composition of agents by approximating the value-at-risk via a\nunion bound and dynamic programming. Furthermore, we prove that the\napproximation is near-optimal asymptotically for a broad class of practical\nloss functions. To evaluate our framework, we consider a suite of video\ngame-like control benchmarks that require composing several agents trained with\nreinforcement learning and demonstrate our algorithm's effectiveness in\napproximating the value-at-risk and identifying the optimal agent composition.", "AI": {"tldr": "The paper formalizes agentic workflows as directed acyclic graphs (agent graphs) and introduces an algorithm to optimize agent compositions for task success and risk minimization, focusing on worst-case risk (value-at-risk).", "motivation": "Modern agentic systems decompose tasks into subtasks handled by specialized AI agents, but real-world deployment requires balancing task success with risk (safety, fairness, privacy) and analyzing tail behaviors.", "method": "The work defines worst-case risk as the tail quantile (value-at-risk) of the loss distribution. An efficient algorithm approximates this via union bound and dynamic programming, traversing the agent graph to find near-optimal compositions.", "result": "The algorithm is proven asymptotically near-optimal for practical loss functions and validated on reinforcement learning-based control benchmarks, showing effectiveness in approximating value-at-risk and identifying optimal compositions.", "conclusion": "The framework successfully addresses risk minimization in agentic workflows, providing a scalable solution for real-world applications."}}
{"id": "2506.04645", "pdf": "https://arxiv.org/pdf/2506.04645", "abs": "https://arxiv.org/abs/2506.04645", "authors": ["Ege Erdil"], "title": "Inference economics of language models", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "We develop a theoretical model that addresses the economic trade-off between\ncost per token versus serial token generation speed when deploying LLMs for\ninference at scale. Our model takes into account arithmetic, memory bandwidth,\nnetwork bandwidth and latency constraints; and optimizes over different\nparallelism setups and batch sizes to find the ones that optimize serial\ninference speed at a given cost per token. We use the model to compute Pareto\nfrontiers of serial speed versus cost per token for popular language models.", "AI": {"tldr": "The paper develops a theoretical model to optimize the trade-off between cost per token and serial token generation speed for LLM inference, considering hardware constraints and parallelism setups.", "motivation": "To address the economic and performance trade-offs in deploying large language models (LLMs) at scale for inference.", "method": "A theoretical model incorporating arithmetic, memory bandwidth, network bandwidth, and latency constraints, optimizing parallelism setups and batch sizes.", "result": "Pareto frontiers of serial speed versus cost per token for popular language models are computed.", "conclusion": "The model provides a framework to optimize LLM inference deployment by balancing cost and speed."}}
{"id": "2506.04650", "pdf": "https://arxiv.org/pdf/2506.04650", "abs": "https://arxiv.org/abs/2506.04650", "authors": ["Zesheng Ye", "Chengyi Cai", "Ruijiang Dong", "Jianzhong Qi", "Lei Feng", "Pin-Yu Chen", "Feng Liu"], "title": "Neural Network Reprogrammability: A Unified Theme on Model Reprogramming, Prompt Tuning, and Prompt Instruction", "categories": ["cs.LG"], "comment": null, "summary": "As large-scale pre-trained foundation models continue to expand in size and\ncapability, efficiently adapting them to specific downstream tasks has become\nincreasingly critical. Despite substantial progress, existing adaptation\napproaches have evolved largely in isolation, without a clear understanding of\ntheir interrelationships. This survey introduces neural network\nreprogrammability as a unifying framework that bridges mainstream model\nadaptation techniques--model reprogramming, prompt tuning, and prompt\ninstruction--previously fragmented research areas yet converges on a shared\nprinciple: repurposing a pre-trained model by manipulating information at the\ninterfaces while keeping the model parameters frozen. These methods exploit\nneural networks' sensitivity to manipulation on different interfaces, be it\nthrough perturbing inputs, inserting tokens into intermediate layers, or\nproviding task-specific examples in context, to redirect model behaviors\ntowards desired outcomes. We then present a taxonomy that categorizes such\ninformation manipulation-based adaptation approaches across four key\ndimensions: manipulation format (fixed or learnable), location (interfaces\nwhere manipulations occur), operator (how they are applied), and output\nalignment requirement (post-processing needed to align outputs with downstream\ntasks). Notably, this framework applies consistently across data modalities,\nindependent of specific model architectures. Moreover, viewing established\ntechniques like in-context learning and chain-of-thought prompting through this\nlens reveals both their theoretical connections and practical distinctions. We\nfurther analyze remaining technical challenges and ethical considerations,\npositioning neural network reprogrammability as a fundamental paradigm for\nefficient model adaptation. We lastly identify promising research directions\nemerging from this integrative viewpoint.", "AI": {"tldr": "The paper introduces neural network reprogrammability as a unifying framework for adapting pre-trained models, categorizing methods by manipulation format, location, operator, and output alignment.", "motivation": "To bridge fragmented research areas in model adaptation by identifying shared principles and unifying diverse techniques.", "method": "Proposes a taxonomy for information manipulation-based adaptation, analyzing methods like model reprogramming, prompt tuning, and prompt instruction.", "result": "The framework applies across data modalities and model architectures, revealing theoretical connections and practical distinctions among techniques.", "conclusion": "Neural network reprogrammability is a fundamental paradigm for efficient adaptation, with potential for future research and ethical considerations."}}
{"id": "2506.04653", "pdf": "https://arxiv.org/pdf/2506.04653", "abs": "https://arxiv.org/abs/2506.04653", "authors": ["MoonJeong Park", "Sunghyun Choi", "Jaeseung Heo", "Eunhyeok Park", "Dongwoo Kim"], "title": "The Oversmoothing Fallacy: A Misguided Narrative in GNN Research", "categories": ["cs.LG"], "comment": null, "summary": "Oversmoothing has been recognized as a main obstacle to building deep Graph\nNeural Networks (GNNs), limiting the performance. This position paper argues\nthat the influence of oversmoothing has been overstated and advocates for a\nfurther exploration of deep GNN architectures. Given the three core operations\nof GNNs, aggregation, linear transformation, and non-linear activation, we show\nthat prior studies have mistakenly confused oversmoothing with the vanishing\ngradient, caused by transformation and activation rather than aggregation. Our\nfinding challenges prior beliefs about oversmoothing being unique to GNNs.\nFurthermore, we demonstrate that classical solutions such as skip connections\nand normalization enable the successful stacking of deep GNN layers without\nperformance degradation. Our results clarify misconceptions about oversmoothing\nand shed new light on the potential of deep GNNs.", "AI": {"tldr": "The paper challenges the overstated impact of oversmoothing in deep GNNs, attributing performance issues to vanishing gradients instead. It advocates for deeper GNN exploration and shows classical techniques like skip connections can mitigate issues.", "motivation": "To correct misconceptions about oversmoothing in GNNs and encourage deeper architectures by clarifying the root causes of performance limitations.", "method": "Analyzes core GNN operations (aggregation, linear transformation, activation) and distinguishes oversmoothing from vanishing gradients. Tests classical solutions (skip connections, normalization) in deep GNNs.", "result": "Demonstrates that oversmoothing is not unique to GNNs and that deep architectures can work with proper techniques.", "conclusion": "Clarifies oversmoothing misconceptions, highlights the potential of deep GNNs, and suggests practical solutions for implementation."}}
{"id": "2506.04669", "pdf": "https://arxiv.org/pdf/2506.04669", "abs": "https://arxiv.org/abs/2506.04669", "authors": ["Wanfu Gao", "Hanlin Pan", "Qingqi Han", "Kunpeng Liu"], "title": "Noise-Resistant Label Reconstruction Feature Selection for Partial Multi-Label Learning", "categories": ["cs.LG"], "comment": "accept in ijcai25", "summary": "The \"Curse of dimensionality\" is prevalent across various data patterns,\nwhich increases the risk of model overfitting and leads to a decline in model\nclassification performance. However, few studies have focused on this issue in\nPartial Multi-label Learning (PML), where each sample is associated with a set\nof candidate labels, at least one of which is correct. Existing PML methods\naddressing this problem are mainly based on the low-rank assumption. However,\nlow-rank assumption is difficult to be satisfied in practical situations and\nmay lead to loss of high-dimensional information. Furthermore, we find that\nexisting methods have poor ability to identify positive labels, which is\nimportant in real-world scenarios. In this paper, a PML feature selection\nmethod is proposed considering two important characteristics of dataset: label\nrelationship's noise-resistance and label connectivity. Our proposed method\nutilizes label relationship's noise-resistance to disambiguate labels. Then the\nlearning process is designed through the reformed low-rank assumption. Finally,\nrepresentative labels are found through label connectivity, and the weight\nmatrix is reconstructed to select features with strong identification ability\nto these labels. The experimental results on benchmark datasets demonstrate the\nsuperiority of the proposed method.", "AI": {"tldr": "A new Partial Multi-label Learning (PML) feature selection method addresses the curse of dimensionality by leveraging label noise-resistance and connectivity, outperforming existing methods.", "motivation": "The curse of dimensionality in PML leads to overfitting and poor classification. Existing low-rank assumption methods fail in practice and lose high-dimensional information.", "method": "Proposes a PML feature selection method using label noise-resistance for disambiguation, a reformed low-rank assumption, and label connectivity to identify representative labels.", "result": "Experiments on benchmark datasets show the method's superiority in handling dimensionality and identifying positive labels.", "conclusion": "The proposed method effectively mitigates dimensionality issues in PML and improves label identification, offering practical advantages."}}
{"id": "2506.04672", "pdf": "https://arxiv.org/pdf/2506.04672", "abs": "https://arxiv.org/abs/2506.04672", "authors": ["Shengkun Zhu", "Feiteng Nie", "Jinshan Zeng", "Sheng Wang", "Yuan Sun", "Yuan Yao", "Shangfeng Chen", "Quanqing Xu", "Chuanhui Yang"], "title": "FedAPM: Federated Learning via ADMM with Partial Model Personalization", "categories": ["cs.LG"], "comment": null, "summary": "In federated learning (FL), the assumption that datasets from different\ndevices are independent and identically distributed (i.i.d.) often does not\nhold due to user differences, and the presence of various data modalities\nacross clients makes using a single model impractical. Personalizing certain\nparts of the model can effectively address these issues by allowing those parts\nto differ across clients, while the remaining parts serve as a shared model.\nHowever, we found that partial model personalization may exacerbate client\ndrift (each client's local model diverges from the shared model), thereby\nreducing the effectiveness and efficiency of FL algorithms. We propose an FL\nframework based on the alternating direction method of multipliers (ADMM),\nreferred to as FedAPM, to mitigate client drift. We construct the augmented\nLagrangian function by incorporating first-order and second-order proximal\nterms into the objective, with the second-order term providing fixed correction\nand the first-order term offering compensatory correction between the local and\nshared models. Our analysis demonstrates that FedAPM, by using explicit\nestimates of the Lagrange multiplier, is more stable and efficient in terms of\nconvergence compared to other FL frameworks. We establish the global\nconvergence of FedAPM training from arbitrary initial points to a stationary\npoint, achieving three types of rates: constant, linear, and sublinear, under\nmild assumptions. We conduct experiments using four heterogeneous and\nmultimodal datasets with different metrics to validate the performance of\nFedAPM. Specifically, FedAPM achieves faster and more accurate convergence,\noutperforming the SOTA methods with average improvements of 12.3% in test\naccuracy, 16.4% in F1 score, and 18.0% in AUC while requiring fewer\ncommunication rounds.", "AI": {"tldr": "The paper proposes FedAPM, a federated learning framework using ADMM to mitigate client drift caused by partial model personalization, achieving faster and more accurate convergence than SOTA methods.", "motivation": "Addressing the non-i.i.d. nature of datasets in FL and the inefficiencies caused by client drift due to partial model personalization.", "method": "FedAPM incorporates first-order and second-order proximal terms into the augmented Lagrangian function to correct client drift, using ADMM for optimization.", "result": "FedAPM outperforms SOTA methods with improvements of 12.3% in accuracy, 16.4% in F1 score, and 18.0% in AUC, requiring fewer communication rounds.", "conclusion": "FedAPM is a stable and efficient FL framework that mitigates client drift and achieves superior performance in heterogeneous and multimodal settings."}}
{"id": "2506.04677", "pdf": "https://arxiv.org/pdf/2506.04677", "abs": "https://arxiv.org/abs/2506.04677", "authors": ["Marco Zanotti"], "title": "The cost of ensembling: is it always worth combining?", "categories": ["cs.LG", "stat.AP", "stat.OT"], "comment": null, "summary": "Given the continuous increase in dataset sizes and the complexity of\nforecasting models, the trade-off between forecast accuracy and computational\ncost is emerging as an extremely relevant topic, especially in the context of\nensemble learning for time series forecasting. To asses it, we evaluated ten\nbase models and eight ensemble configurations across two large-scale retail\ndatasets (M5 and VN1), considering both point and probabilistic accuracy under\nvarying retraining frequencies. We showed that ensembles consistently improve\nforecasting performance, particularly in probabilistic settings. However, these\ngains come at a substantial computational cost, especially for larger,\naccuracy-driven ensembles. We found that reducing retraining frequency\nsignificantly lowers costs, with minimal impact on accuracy, particularly for\npoint forecasts. Moreover, efficiency-driven ensembles offer a strong balance,\nachieving competitive accuracy with considerably lower costs compared to\naccuracy-optimized combinations. Most importantly, small ensembles of two or\nthree models are often sufficient to achieve near-optimal results. These\nfindings provide practical guidelines for deploying scalable and cost-efficient\nforecasting systems, supporting the broader goals of sustainable AI in\nforecasting. Overall, this work shows that careful ensemble design and\nretraining strategy selection can yield accurate, robust, and cost-effective\nforecasts suitable for real-world applications.", "AI": {"tldr": "Ensembles improve forecasting accuracy, especially in probabilistic settings, but at high computational cost. Reducing retraining frequency lowers costs with minimal accuracy loss. Small ensembles (2-3 models) often suffice for near-optimal results, offering a balance between accuracy and efficiency.", "motivation": "The trade-off between forecast accuracy and computational cost is critical, especially in ensemble learning for time series forecasting, given increasing dataset sizes and model complexity.", "method": "Evaluated ten base models and eight ensemble configurations on large-scale retail datasets (M5 and VN1), considering point and probabilistic accuracy under varying retraining frequencies.", "result": "Ensembles consistently improve performance, but with high computational costs. Reducing retraining frequency lowers costs with little accuracy impact. Small ensembles achieve competitive accuracy efficiently.", "conclusion": "Careful ensemble design and retraining strategy selection can yield accurate, robust, and cost-effective forecasts, supporting scalable and sustainable AI in forecasting."}}
{"id": "2506.04681", "pdf": "https://arxiv.org/pdf/2506.04681", "abs": "https://arxiv.org/abs/2506.04681", "authors": ["Daogao Liu", "Edith Cohen", "Badih Ghazi", "Peter Kairouz", "Pritish Kamath", "Alexander Knop", "Ravi Kumar", "Pasin Manurangsi", "Adam Sealfon", "Da Yu", "Chiyuan Zhang"], "title": "Urania: Differentially Private Insights into AI Use", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR", "cs.CY"], "comment": null, "summary": "We introduce $Urania$, a novel framework for generating insights about LLM\nchatbot interactions with rigorous differential privacy (DP) guarantees. The\nframework employs a private clustering mechanism and innovative keyword\nextraction methods, including frequency-based, TF-IDF-based, and LLM-guided\napproaches. By leveraging DP tools such as clustering, partition selection, and\nhistogram-based summarization, $Urania$ provides end-to-end privacy protection.\nOur evaluation assesses lexical and semantic content preservation, pair\nsimilarity, and LLM-based metrics, benchmarking against a non-private\nClio-inspired pipeline (Tamkin et al., 2024). Moreover, we develop a simple\nempirical privacy evaluation that demonstrates the enhanced robustness of our\nDP pipeline. The results show the framework's ability to extract meaningful\nconversational insights while maintaining stringent user privacy, effectively\nbalancing data utility with privacy preservation.", "AI": {"tldr": "Urania is a privacy-focused framework for analyzing LLM chatbot interactions using differential privacy (DP) techniques, ensuring data utility while protecting user privacy.", "motivation": "To generate insights from LLM chatbot interactions while rigorously guaranteeing user privacy through differential privacy.", "method": "Uses private clustering, keyword extraction (frequency-based, TF-IDF, LLM-guided), and DP tools like clustering, partition selection, and histogram summarization.", "result": "Effectively balances data utility and privacy, preserving conversational insights with robust DP guarantees.", "conclusion": "Urania successfully extracts meaningful insights while maintaining stringent privacy, outperforming non-private alternatives."}}
{"id": "2506.04690", "pdf": "https://arxiv.org/pdf/2506.04690", "abs": "https://arxiv.org/abs/2506.04690", "authors": ["Yifan Hao", "Yanxin Lu", "Xinwei Shen", "Tong Zhang"], "title": "Towards Better Generalization via Distributional Input Projection Network", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "As overparameterized models become increasingly prevalent, training loss\nalone offers limited insight into generalization performance. While smoothness\nhas been linked to improved generalization across various settings, directly\nenforcing smoothness in neural networks remains challenging. To address this,\nwe introduce Distributional Input Projection Networks (DIPNet), a novel\nframework that projects inputs into learnable distributions at each layer. This\ndistributional representation induces a smoother loss landscape with respect to\nthe input, promoting better generalization. We provide theoretical analysis\nshowing that DIPNet reduces both local smoothness measures and the Lipschitz\nconstant of the network, contributing to improved generalization performance.\nEmpirically, we validate DIPNet across a wide range of architectures and tasks,\nincluding Vision Transformers (ViTs), Large Language Models (LLMs), ResNet and\nMLPs. Our method consistently enhances test performance under standard\nsettings, adversarial attacks, out-of-distribution inputs, and reasoning\nbenchmarks. We demonstrate that the proposed input projection strategy can be\nseamlessly integrated into existing models, providing a general and effective\napproach for boosting generalization performance in modern deep learning.", "AI": {"tldr": "DIPNet introduces a framework to project inputs into learnable distributions, smoothing the loss landscape and improving generalization, validated across various models and tasks.", "motivation": "Overparameterized models lack insight from training loss alone, and enforcing smoothness in neural networks is challenging.", "method": "DIPNet projects inputs into learnable distributions at each layer, inducing a smoother loss landscape.", "result": "Theoretical and empirical results show improved generalization, robustness, and performance across diverse tasks and models.", "conclusion": "DIPNet offers a general, effective approach to enhance generalization in modern deep learning."}}
{"id": "2506.04694", "pdf": "https://arxiv.org/pdf/2506.04694", "abs": "https://arxiv.org/abs/2506.04694", "authors": ["Jaeseung Heo", "Kyeongheung Yun", "Seokwon Yoon", "MoonJeong Park", "Jungseul Ok", "Dongwoo Kim"], "title": "Influence Functions for Edge Edits in Non-Convex Graph Neural Networks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Understanding how individual edges influence the behavior of graph neural\nnetworks (GNNs) is essential for improving their interpretability and\nrobustness. Graph influence functions have emerged as promising tools to\nefficiently estimate the effects of edge deletions without retraining. However,\nexisting influence prediction methods rely on strict convexity assumptions,\nexclusively consider the influence of edge deletions while disregarding edge\ninsertions, and fail to capture changes in message propagation caused by these\nmodifications. In this work, we propose a proximal Bregman response function\nspecifically tailored for GNNs, relaxing the convexity requirement and enabling\naccurate influence prediction for standard neural network architectures.\nFurthermore, our method explicitly accounts for message propagation effects and\nextends influence prediction to both edge deletions and insertions in a\nprincipled way. Experiments with real-world datasets demonstrate accurate\ninfluence predictions for different characteristics of GNNs. We further\ndemonstrate that the influence function is versatile in applications such as\ngraph rewiring and adversarial attacks.", "AI": {"tldr": "The paper introduces a proximal Bregman response function for GNNs to predict edge influence, relaxing convexity assumptions and addressing message propagation effects for both deletions and insertions.", "motivation": "To improve interpretability and robustness of GNNs by understanding edge influence, overcoming limitations of existing methods that ignore edge insertions and message propagation changes.", "method": "Proposes a proximal Bregman response function tailored for GNNs, relaxing convexity and accounting for message propagation, enabling influence prediction for deletions and insertions.", "result": "Demonstrates accurate influence predictions on real-world datasets and versatility in applications like graph rewiring and adversarial attacks.", "conclusion": "The method effectively addresses gaps in existing influence prediction, enhancing GNN interpretability and robustness."}}
{"id": "2506.04695", "pdf": "https://arxiv.org/pdf/2506.04695", "abs": "https://arxiv.org/abs/2506.04695", "authors": ["Xingwu Chen", "Tianle Li", "Difan Zou"], "title": "On the Mechanism of Reasoning Pattern Selection in Reinforcement Learning for Language Models", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "30 pages, 6 figures, 1 table", "summary": "Reinforcement learning (RL) has demonstrated remarkable success in enhancing\nmodel capabilities, including instruction-following, preference learning, and\nreasoning. Yet despite its empirical successes, the mechanisms by which RL\nimproves reasoning abilities remain poorly understood. We present a systematic\nstudy of Reinforcement Learning with Verifiable Rewards (RLVR), showing that\nits primary benefit comes from optimizing the selection of existing reasoning\npatterns. Through extensive experiments, we demonstrate that RLVR-trained\nmodels preferentially adopt high-success-rate reasoning patterns while mostly\nmaintaining stable performance on individual patterns. We further develop\ntheoretical analyses on the convergence and training dynamics of RLVR based on\na simplified question-reason-answer model. We study the gradient flow and show\nthat RLVR can indeed find the solution that selects the reason pattern with the\nhighest success rate. Besides, our theoretical results\n  reveal two distinct regimes regarding the convergence of RLVR training: (1)\nrapid convergence for models with relatively strong initial reasoning\ncapabilities versus (2) slower optimization dynamics for weaker models.\nFurthermore, we show that the slower optimization for weaker models can be\nmitigated by applying the supervised fine-tuning (SFT) before RLVR, when using\na feasibly high-quality SFT dataset. We validate the theoretical findings\nthrough extensive experiments. This work advances our theoretical understanding\nof RL's role in LLM fine-tuning and offers insights for further enhancing\nreasoning capabilities.", "AI": {"tldr": "RLVR improves reasoning by optimizing the selection of high-success-rate reasoning patterns, with faster convergence for strong models and slower for weaker ones, mitigated by SFT.", "motivation": "To understand how RL enhances reasoning abilities in models, despite empirical success, the mechanisms remain unclear.", "method": "Systematic study of RLVR, theoretical analysis of convergence and training dynamics, and validation through experiments.", "result": "RLVR selects high-success-rate reasoning patterns, with distinct convergence regimes for strong vs. weak models. SFT can mitigate slower optimization.", "conclusion": "This work clarifies RL's role in reasoning enhancement and suggests practical improvements like SFT before RLVR."}}
{"id": "2506.04696", "pdf": "https://arxiv.org/pdf/2506.04696", "abs": "https://arxiv.org/abs/2506.04696", "authors": ["Tonmoy Paul", "Mrittika Devi Mati", "Md. Mahmudul Islam"], "title": "Enhanced Drought Analysis in Bangladesh: A Machine Learning Approach for Severity Classification Using Satellite Data", "categories": ["cs.LG"], "comment": null, "summary": "Drought poses a pervasive environmental challenge in Bangladesh, impacting\nagriculture, socio-economic stability, and food security due to its unique\ngeographic and anthropogenic vulnerabilities. Traditional drought indices, such\nas the Standardized Precipitation Index (SPI) and Palmer Drought Severity Index\n(PDSI), often overlook crucial factors like soil moisture and temperature,\nlimiting their resolution. Moreover, current machine learning models applied to\ndrought prediction have been underexplored in the context of Bangladesh,\nlacking a comprehensive integration of satellite data across multiple\ndistricts. To address these gaps, we propose a satellite data-driven machine\nlearning framework to classify drought across 38 districts of Bangladesh. Using\nunsupervised algorithms like K-means and Bayesian Gaussian Mixture for\nclustering, followed by classification models such as KNN, Random Forest,\nDecision Tree, and Naive Bayes, the framework integrates weather data\n(humidity, soil moisture, temperature) from 2012-2024. This approach\nsuccessfully classifies drought severity into different levels. However, it\nshows significant variabilities in drought vulnerabilities across regions which\nhighlights the aptitude of machine learning models in terms of identifying and\npredicting drought conditions.", "AI": {"tldr": "A machine learning framework using satellite data is proposed to classify drought severity in Bangladesh, addressing gaps in traditional indices and regional variability.", "motivation": "Drought in Bangladesh affects agriculture and food security, but traditional indices and machine learning models lack comprehensive regional integration.", "method": "The framework uses unsupervised clustering (K-means, Bayesian Gaussian Mixture) and classification models (KNN, Random Forest, Decision Tree, Naive Bayes) with weather data (2012-2024).", "result": "The approach successfully classifies drought severity but reveals regional variability in vulnerabilities.", "conclusion": "Machine learning effectively identifies and predicts drought conditions, highlighting regional differences."}}
{"id": "2506.04700", "pdf": "https://arxiv.org/pdf/2506.04700", "abs": "https://arxiv.org/abs/2506.04700", "authors": ["Jos\u00e9 Manuel de Frutos", "Manuel A. V\u00e1zquez", "Pablo M. Olmos", "Joaqu\u00edn M\u00edguez"], "title": "Explicit Density Approximation for Neural Implicit Samplers Using a Bernstein-Based Convex Divergence", "categories": ["cs.LG", "cs.AI", "math.PR", "stat.ML"], "comment": null, "summary": "Rank-based statistical metrics, such as the invariant statistical loss (ISL),\nhave recently emerged as robust and practically effective tools for training\nimplicit generative models. In this work, we introduce dual-ISL, a novel\nlikelihood-free objective for training implicit generative models that\ninterchanges the roles of the target and model distributions in the ISL\nframework, yielding a convex optimization problem in the space of model\ndensities. We prove that the resulting rank-based discrepancy $d_K$ is i)\ncontinuous under weak convergence and with respect to the $L^1$ norm, and ii)\nconvex in its first argument-properties not shared by classical divergences\nsuch as KL or Wasserstein distances. Building on this, we develop a theoretical\nframework that interprets $d_K$ as an $L^2$-projection of the density ratio $q\n= p/\\tilde p$ onto a Bernstein polynomial basis, from which we derive exact\nbounds on the truncation error, precise convergence rates, and a closed-form\nexpression for the truncated density approximation. We further extend our\nanalysis to the multivariate setting via random one-dimensional projections,\ndefining a sliced dual-ISL divergence that retains both convexity and\ncontinuity. We empirically show that these theoretical advantages translate\ninto practical ones. Specifically, across several benchmarks dual-ISL converges\nmore rapidly, delivers markedly smoother and more stable training, and more\neffectively prevents mode collapse than classical ISL and other leading\nimplicit generative methods-while also providing an explicit density\napproximation.", "AI": {"tldr": "The paper introduces dual-ISL, a novel likelihood-free objective for training implicit generative models, offering convexity and continuity advantages over classical divergences like KL or Wasserstein distances. It provides theoretical guarantees and empirical improvements in convergence, stability, and mode collapse prevention.", "motivation": "To address limitations of existing rank-based metrics like ISL by proposing a convex and continuous alternative (dual-ISL) for training implicit generative models more effectively.", "method": "Introduces dual-ISL, interchanging roles of target and model distributions in ISL, forming a convex problem. Analyzes properties of the resulting discrepancy $d_K$, derives theoretical bounds, and extends to multivariate settings via sliced dual-ISL.", "result": "Dual-ISL demonstrates faster convergence, smoother training, better stability, and improved mode collapse prevention compared to ISL and other methods, while also providing explicit density approximation.", "conclusion": "Dual-ISL is a robust and theoretically grounded tool for training implicit generative models, outperforming existing methods in practice."}}
{"id": "2506.04712", "pdf": "https://arxiv.org/pdf/2506.04712", "abs": "https://arxiv.org/abs/2506.04712", "authors": ["Pinak Mandal", "Georg A. Gottwald"], "title": "UNO: Unlearning via Orthogonalization in Generative models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "As generative models become increasingly powerful and pervasive, the ability\nto unlearn specific data, whether due to privacy concerns, legal requirements,\nor the correction of harmful content, has become increasingly important. Unlike\nin conventional training, where data are accumulated and knowledge is\nreinforced, unlearning aims to selectively remove the influence of particular\ndata points without costly retraining from scratch. To be effective and\nreliable, such algorithms need to achieve (i) forgetting of the undesired data,\n(ii) preservation of the quality of the generation, (iii) preservation of the\ninfluence of the desired training data on the model parameters, and (iv) small\nnumber of training steps. We propose fast unlearning algorithms based on loss\ngradient orthogonalization. We show that our algorithms are able to forget data\nwhile maintaining the fidelity of the original model. Using MNIST and CelebA\ndata, we demonstrate that our algorithms achieve orders of magnitude faster\nunlearning times than their predecessors, such as gradient surgery.", "AI": {"tldr": "The paper introduces fast unlearning algorithms for generative models, focusing on selectively removing data influence without full retraining, ensuring efficiency and model fidelity.", "motivation": "Addressing the need for privacy, legal compliance, and content correction in generative models by enabling selective data removal.", "method": "Proposes unlearning algorithms based on loss gradient orthogonalization, ensuring minimal training steps and model quality preservation.", "result": "Demonstrates significantly faster unlearning times on MNIST and CelebA datasets compared to prior methods like gradient surgery.", "conclusion": "The algorithms effectively forget undesired data while maintaining model performance, offering a practical solution for data removal in generative models."}}
{"id": "2506.04746", "pdf": "https://arxiv.org/pdf/2506.04746", "abs": "https://arxiv.org/abs/2506.04746", "authors": ["Fei Ding", "Baiqiao Wang", "Zijian Zeng", "Youwei Wang"], "title": "Multi-Layer GRPO: Enhancing Reasoning and Self-Correction in Large Language Models", "categories": ["cs.LG"], "comment": null, "summary": "The Group Relative Policy Optimization (GRPO) algorithm has demonstrated\nconsiderable success in enhancing the reasoning capabilities of large language\nmodels (LLMs), as evidenced by DeepSeek-R1. However, the absence of\nintermediate supervision in GRPO frequently leads to inefficient exploration\ndynamics. A single error in a complex reasoning chain can invalidate the entire\nsolution, resulting in abrupt reward vanishing and compromising training\nstability.To address these challenges, we propose MGRPO (Multi-layer GRPO).\nMGRPO operates in two layers: the first layer employs standard GRPO to generate\nan initial response. This response, along with the original query, is then fed\ninto a second-layer GRPO process. This second layer is specifically trained to\nidentify and correct errors in the initial response, effectively creating a\nself-correction loop. This mechanism provides implicit process-level\nsupervision by rewarding successful error correction, without requiring an\nexplicit, densely-annotated reward model. Experimental results on several\nmathematical reasoning benchmarks demonstrate that MGRPO significantly\noutperforms standard GRPO, achieving superior performance by fostering both\nreasoning and self-correction abilities.", "AI": {"tldr": "MGRPO improves GRPO by adding a self-correction layer to fix errors in initial responses, enhancing reasoning and training stability.", "motivation": "GRPO lacks intermediate supervision, causing inefficient exploration and reward vanishing in complex reasoning tasks.", "method": "MGRPO uses two layers: standard GRPO for initial responses and a second layer to correct errors, creating a self-correction loop.", "result": "MGRPO outperforms GRPO on mathematical reasoning benchmarks by improving reasoning and self-correction.", "conclusion": "MGRPO provides implicit supervision and better performance without dense reward annotations."}}
{"id": "2506.04761", "pdf": "https://arxiv.org/pdf/2506.04761", "abs": "https://arxiv.org/abs/2506.04761", "authors": ["Han Guo", "Songlin Yang", "Tarushii Goel", "Eric P. Xing", "Tri Dao", "Yoon Kim"], "title": "Log-Linear Attention", "categories": ["cs.LG"], "comment": null, "summary": "The attention mechanism in Transformers is an important primitive for\naccurate and scalable sequence modeling. Its quadratic-compute and\nlinear-memory complexity however remain significant bottlenecks. Linear\nattention and state-space models enable linear-time, constant-memory sequence\nmodeling and can moreover be trained efficiently through matmul-rich\nparallelization across sequence length. However, at their core these models are\nstill RNNs, and thus their use of a fixed-size hidden state to model the\ncontext is a fundamental limitation. This paper develops log-linear attention,\nan attention mechanism that balances linear attention's efficiency and the\nexpressiveness of softmax attention. Log-linear attention replaces the\nfixed-size hidden state with a logarithmically growing set of hidden states. We\nshow that with a particular growth function, log-linear attention admits a\nsimilarly matmul-rich parallel form whose compute cost is log-linear in\nsequence length. Log-linear attention is a general framework and can be applied\non top of existing linear attention variants. As case studies, we instantiate\nlog-linear variants of two recent architectures -- Mamba-2 and Gated DeltaNet\n-- and find they perform well compared to their linear-time variants.", "AI": {"tldr": "Log-linear attention balances efficiency and expressiveness by replacing fixed-size hidden states with logarithmically growing ones, enabling log-linear compute cost.", "motivation": "Address the bottlenecks of quadratic-compute and linear-memory complexity in Transformer attention while overcoming the limitations of fixed-size hidden states in linear attention and state-space models.", "method": "Develop log-linear attention, which uses a logarithmically growing set of hidden states and admits matmul-rich parallelization. Apply it to existing architectures like Mamba-2 and Gated DeltaNet.", "result": "Log-linear attention variants perform well compared to their linear-time counterparts, demonstrating effectiveness.", "conclusion": "Log-linear attention offers a scalable and expressive alternative to traditional attention mechanisms, with practical benefits for sequence modeling."}}
{"id": "2506.04765", "pdf": "https://arxiv.org/pdf/2506.04765", "abs": "https://arxiv.org/abs/2506.04765", "authors": ["Jiachen Tang", "Zhonghao Wang", "Sirui Chen", "Sheng Zhou", "Jiawei Chen", "Jiajun Bu"], "title": "OpenGT: A Comprehensive Benchmark For Graph Transformers", "categories": ["cs.LG"], "comment": "14 pages, 5 figures", "summary": "Graph Transformers (GTs) have recently demonstrated remarkable performance\nacross diverse domains. By leveraging attention mechanisms, GTs are capable of\nmodeling long-range dependencies and complex structural relationships beyond\nlocal neighborhoods. However, their applicable scenarios are still\nunderexplored, this highlights the need to identify when and why they excel.\nFurthermore, unlike GNNs, which predominantly rely on message-passing\nmechanisms, GTs exhibit a diverse design space in areas such as positional\nencoding, attention mechanisms, and graph-specific adaptations. Yet, it remains\nunclear which of these design choices are truly effective and under what\nconditions. As a result, the community currently lacks a comprehensive\nbenchmark and library to promote a deeper understanding and further development\nof GTs. To address this gap, this paper introduces OpenGT, a comprehensive\nbenchmark for Graph Transformers. OpenGT enables fair comparisons and\nmultidimensional analysis by establishing standardized experimental settings\nand incorporating a broad selection of state-of-the-art GNNs and GTs. Our\nbenchmark evaluates GTs from multiple perspectives, encompassing diverse tasks\nand datasets with varying properties. Through extensive experiments, our\nbenchmark has uncovered several critical insights, including the difficulty of\ntransferring models across task levels, the limitations of local attention, the\nefficiency trade-offs in several models, the application scenarios of specific\npositional encodings, and the preprocessing overhead of some positional\nencodings. We aspire for this work to establish a foundation for future graph\ntransformer research emphasizing fairness, reproducibility, and\ngeneralizability. We have developed an easy-to-use library OpenGT for training\nand evaluating existing GTs. The benchmark code is available at\nhttps://github.com/eaglelab-zju/OpenGT.", "AI": {"tldr": "The paper introduces OpenGT, a benchmark for Graph Transformers (GTs), addressing the lack of standardized evaluation and understanding of GTs' design choices and performance.", "motivation": "GTs show promise but lack exploration of their effective scenarios and design choices. The community needs a comprehensive benchmark to understand and advance GTs.", "method": "OpenGT provides standardized settings, diverse tasks, and datasets to evaluate GTs and GNNs fairly. It includes a library for training and evaluation.", "result": "Key insights include challenges in model transfer, limitations of local attention, efficiency trade-offs, and positional encoding scenarios.", "conclusion": "OpenGT aims to foster fair, reproducible, and generalizable GT research, supported by an accessible library."}}
{"id": "2506.04775", "pdf": "https://arxiv.org/pdf/2506.04775", "abs": "https://arxiv.org/abs/2506.04775", "authors": ["Artin Tajdini", "Jonathan Scarlett", "Kevin Jamieson"], "title": "Improved Regret Bounds for Linear Bandits with Heavy-Tailed Rewards", "categories": ["cs.LG", "cs.IT", "math.IT", "stat.ML"], "comment": null, "summary": "We study stochastic linear bandits with heavy-tailed rewards, where the\nrewards have a finite $(1+\\epsilon)$-absolute central moment bounded by\n$\\upsilon$ for some $\\epsilon \\in (0,1]$. We improve both upper and lower\nbounds on the minimax regret compared to prior work. When $\\upsilon =\n\\mathcal{O}(1)$, the best prior known regret upper bound is\n$\\tilde{\\mathcal{O}}(d T^{\\frac{1}{1+\\epsilon}})$. While a lower with the same\nscaling has been given, it relies on a construction using $\\upsilon =\n\\mathcal{O}(d)$, and adapting the construction to the bounded-moment regime\nwith $\\upsilon = \\mathcal{O}(1)$ yields only a\n$\\Omega(d^{\\frac{\\epsilon}{1+\\epsilon}} T^{\\frac{1}{1+\\epsilon}})$ lower bound.\nThis matches the known rate for multi-armed bandits and is generally loose for\nlinear bandits, in particular being $\\sqrt{d}$ below the optimal rate in the\nfinite-variance case ($\\epsilon = 1$). We propose a new elimination-based\nalgorithm guided by experimental design, which achieves regret\n$\\tilde{\\mathcal{O}}(d^{\\frac{1+3\\epsilon}{2(1+\\epsilon)}}\nT^{\\frac{1}{1+\\epsilon}})$, thus improving the dependence on $d$ for all\n$\\epsilon \\in (0,1)$ and recovering a known optimal result for $\\epsilon = 1$.\nWe also establish a lower bound of $\\Omega(d^{\\frac{2\\epsilon}{1+\\epsilon}}\nT^{\\frac{1}{1+\\epsilon}})$, which strictly improves upon the multi-armed bandit\nrate and highlights the hardness of heavy-tailed linear bandit problems. For\nfinite action sets, we derive similarly improved upper and lower bounds for\nregret. Finally, we provide action set dependent regret upper bounds showing\nthat for some geometries, such as $l_p$-norm balls for $p \\le 1 + \\epsilon$, we\ncan further reduce the dependence on $d$, and we can handle\ninfinite-dimensional settings via the kernel trick, in particular establishing\nnew regret bounds for the Mat\\'ern kernel that are the first to be sublinear\nfor all $\\epsilon \\in (0, 1]$.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2506.04786", "pdf": "https://arxiv.org/pdf/2506.04786", "abs": "https://arxiv.org/abs/2506.04786", "authors": ["Thore Gerlach", "Sascha M\u00fccke", "Christian Bauckhage"], "title": "Kernel $k$-Medoids as General Vector Quantization", "categories": ["cs.LG", "quant-ph"], "comment": null, "summary": "Vector Quantization (VQ) is a widely used technique in machine learning and\ndata compression, valued for its simplicity and interpretability. Among hard VQ\nmethods, $k$-medoids clustering and Kernel Density Estimation (KDE) approaches\nrepresent two prominent yet seemingly unrelated paradigms -- one\ndistance-based, the other rooted in probability density matching. In this\npaper, we investigate their connection through the lens of Quadratic\nUnconstrained Binary Optimization (QUBO). We compare a heuristic QUBO\nformulation for $k$-medoids, which balances centrality and diversity, with a\nprincipled QUBO derived from minimizing Maximum Mean Discrepancy in KDE-based\nVQ. Surprisingly, we show that the KDE-QUBO is a special case of the\n$k$-medoids-QUBO under mild assumptions on the kernel's feature map. This\nreveals a deeper structural relationship between these two approaches and\nprovides new insight into the geometric interpretation of the weighting\nparameters used in QUBO formulations for VQ.", "AI": {"tldr": "The paper explores the connection between $k$-medoids clustering and Kernel Density Estimation (KDE) in Vector Quantization (VQ) using Quadratic Unconstrained Binary Optimization (QUBO), showing KDE-QUBO as a special case of $k$-medoids-QUBO.", "motivation": "To bridge the gap between distance-based ($k$-medoids) and probability-based (KDE) VQ methods and uncover their structural relationship.", "method": "Uses QUBO formulations for $k$-medoids and KDE-based VQ, comparing their mathematical properties.", "result": "Demonstrates KDE-QUBO is a special case of $k$-medoids-QUBO under mild kernel assumptions.", "conclusion": "Reveals a deeper connection between $k$-medoids and KDE in VQ, offering new insights into QUBO parameter interpretations."}}
{"id": "2506.04805", "pdf": "https://arxiv.org/pdf/2506.04805", "abs": "https://arxiv.org/abs/2506.04805", "authors": ["Zhiwei Bai", "Zhangchen Zhou", "Jiajie Zhao", "Xiaolong Li", "Zhiyu Li", "Feiyu Xiong", "Hongkang Yang", "Yaoyu Zhang", "Zhi-Qin John Xu"], "title": "Adaptive Preconditioners Trigger Loss Spikes in Adam", "categories": ["cs.LG"], "comment": null, "summary": "Loss spikes emerge commonly during training across neural networks of varying\narchitectures and scales when using the Adam optimizer. In this work, we\ninvestigate the underlying mechanism responsible for Adam spikes. While\nprevious explanations attribute these phenomena to the lower-loss-as-sharper\ncharacteristics of the loss landscape, our analysis reveals that Adam's\nadaptive preconditioners themselves can trigger spikes. Specifically, we\nidentify a critical regime where squared gradients become substantially smaller\nthan the second-order moment estimates, causing the latter to undergo a\n$\\beta_2$-exponential decay and to respond sluggishly to current gradient\ninformation. This mechanism can push the maximum eigenvalue of the\npreconditioned Hessian beyond the classical stability threshold $2/\\eta$ for a\nsustained period, inducing instability. This instability further leads to an\nalignment between the gradient and the maximum eigendirection, and a loss spike\noccurs precisely when the gradient-directional curvature exceeds $2/\\eta$. We\nverify this mechanism through extensive experiments on fully connected\nnetworks, convolutional networks, and Transformer architectures.", "AI": {"tldr": "The paper investigates the cause of loss spikes in neural networks trained with Adam, attributing them to Adam's adaptive preconditioners rather than the loss landscape's sharpness.", "motivation": "Loss spikes during training with Adam are common but poorly understood. Previous explanations focused on the loss landscape, but this work explores Adam's internal mechanisms.", "method": "The study analyzes Adam's adaptive preconditioners, identifying a critical regime where squared gradients are much smaller than second-order moment estimates, leading to instability.", "result": "The mechanism causes the preconditioned Hessian's maximum eigenvalue to exceed stability thresholds, inducing loss spikes when gradient-directional curvature surpasses 2/\u03b7.", "conclusion": "The findings are validated across various architectures, showing Adam's preconditioners as the primary cause of loss spikes."}}
{"id": "2506.04821", "pdf": "https://arxiv.org/pdf/2506.04821", "abs": "https://arxiv.org/abs/2506.04821", "authors": ["Zhen Hao Wong", "Jingwen Deng", "Runming He", "Zirong Chen", "Qijie You", "Hejun Dong", "Hao Liang", "Chengyu Shen", "Bin Cui", "Wentao Zhang"], "title": "LogicPuzzleRL: Cultivating Robust Mathematical Reasoning in LLMs via Reinforcement Learning", "categories": ["cs.LG"], "comment": null, "summary": "Large language models (LLMs) excel at many supervised tasks but often\nstruggle with structured reasoning in unfamiliar settings. This discrepancy\nsuggests that standard fine-tuning pipelines may instill narrow,\ndomain-specific heuristics rather than fostering general-purpose thinking\nstrategies. In this work, we propose a \"play to learn\" framework that\nfine-tunes LLMs through reinforcement learning on a suite of seven custom logic\npuzzles, each designed to cultivate distinct reasoning skills such as\nconstraint propagation, spatial consistency, and symbolic deduction. Using a\nreinforcement learning setup with verifiable rewards, models receive binary\nfeedback based on puzzle correctness, encouraging iterative, hypothesis-driven\nproblem solving. We demonstrate that this training approach significantly\nimproves out-of-distribution performance on a range of mathematical benchmarks,\nespecially for mid-difficulty problems that require multi-step reasoning.\nAnalyses across problem categories and difficulty levels reveal that puzzle\ntraining promotes transferable reasoning routines, strengthening algebraic\nmanipulation, geometric inference, and combinatorial logic, while offering\nlimited gains on rote or highly specialized tasks. These findings show that\nreinforcement learning over logic puzzles reshapes the internal reasoning of\nLLMs, enabling more robust and compositional generalization without relying on\ntask-specific symbolic tools.", "AI": {"tldr": "A 'play to learn' framework uses reinforcement learning on logic puzzles to improve LLMs' structured reasoning, showing better generalization on mathematical tasks.", "motivation": "Standard fine-tuning may limit LLMs to narrow heuristics; this work aims to foster general-purpose reasoning.", "method": "Fine-tuning LLMs via reinforcement learning on seven custom logic puzzles with verifiable binary rewards.", "result": "Improved out-of-distribution performance on math benchmarks, especially for multi-step reasoning tasks.", "conclusion": "Reinforcement learning over puzzles enhances LLMs' reasoning, enabling robust generalization without task-specific tools."}}
{"id": "2506.04831", "pdf": "https://arxiv.org/pdf/2506.04831", "abs": "https://arxiv.org/abs/2506.04831", "authors": ["Chantal Pellegrini", "Ege \u00d6zsoy", "David Bani-Harouni", "Matthias Keicher", "Nassir Navab"], "title": "From EHRs to Patient Pathways: Scalable Modeling of Longitudinal Health Trajectories with LLMs", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Healthcare systems face significant challenges in managing and interpreting\nvast, heterogeneous patient data for personalized care. Existing approaches\noften focus on narrow use cases with a limited feature space, overlooking the\ncomplex, longitudinal interactions needed for a holistic understanding of\npatient health. In this work, we propose a novel approach to patient pathway\nmodeling by transforming diverse electronic health record (EHR) data into a\nstructured representation and designing a holistic pathway prediction model,\nEHR2Path, optimized to predict future health trajectories. Further, we\nintroduce a novel summary mechanism that embeds long-term temporal context into\ntopic-specific summary tokens, improving performance over text-only models,\nwhile being much more token-efficient. EHR2Path demonstrates strong performance\nin both next time-step prediction and longitudinal simulation, outperforming\ncompetitive baselines. It enables detailed simulations of patient trajectories,\ninherently targeting diverse evaluation tasks, such as forecasting vital signs,\nlab test results, or length-of-stay, opening a path towards predictive and\npersonalized healthcare.", "AI": {"tldr": "EHR2Path transforms EHR data into structured representations for holistic patient pathway prediction, outperforming baselines in forecasting health trajectories.", "motivation": "Healthcare systems struggle with managing diverse patient data for personalized care, lacking holistic approaches for longitudinal interactions.", "method": "Proposes EHR2Path, a model that structures EHR data and uses a summary mechanism for long-term temporal context, optimizing pathway prediction.", "result": "EHR2Path excels in next time-step prediction and longitudinal simulation, outperforming baselines in tasks like vital signs forecasting.", "conclusion": "EHR2Path enables predictive, personalized healthcare by simulating detailed patient trajectories and diverse evaluation tasks."}}
{"id": "2506.04859", "pdf": "https://arxiv.org/pdf/2506.04859", "abs": "https://arxiv.org/abs/2506.04859", "authors": ["Yin Lu", "Tong He", "Xuening Zhu", "David Wipf"], "title": "Sparse Autoencoders, Again?", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted to the International Conference on Machine Learning (ICML)\n  2025", "summary": "Is there really much more to say about sparse autoencoders (SAEs)?\nAutoencoders in general, and SAEs in particular, represent deep architectures\nthat are capable of modeling low-dimensional latent structure in data. Such\nstructure could reflect, among other things, correlation patterns in large\nlanguage model activations, or complex natural image manifolds. And yet despite\nthe wide-ranging applicability, there have been relatively few changes to SAEs\nbeyond the original recipe from decades ago, namely, standard deep\nencoder/decoder layers trained with a classical/deterministic sparse\nregularizer applied within the latent space. One possible exception is the\nvariational autoencoder (VAE), which adopts a stochastic encoder module capable\nof producing sparse representations when applied to manifold data. In this work\nwe formalize underappreciated weaknesses with both canonical SAEs, as well as\nanalogous VAEs applied to similar tasks, and propose a hybrid alternative model\nthat circumvents these prior limitations. In terms of theoretical support, we\nprove that global minima of our proposed model recover certain forms of\nstructured data spread across a union of manifolds. Meanwhile, empirical\nevaluations on synthetic and real-world datasets substantiate the efficacy of\nour approach in accurately estimating underlying manifold dimensions and\nproducing sparser latent representations without compromising reconstruction\nerror. In general, we are able to exceed the performance of equivalent-capacity\nSAEs and VAEs, as well as recent diffusion models where applicable, within\ndomains such as images and language model activation patterns.", "AI": {"tldr": "The paper critiques traditional sparse autoencoders (SAEs) and variational autoencoders (VAEs), proposing a hybrid model to overcome their weaknesses. It offers theoretical guarantees and empirical validation for improved performance in tasks like image and language model activation analysis.", "motivation": "Despite the broad applicability of SAEs and VAEs, their limitations in handling structured data and sparse representations remain underaddressed. The paper aims to bridge this gap by introducing a hybrid model.", "method": "The authors propose a hybrid alternative combining strengths of SAEs and VAEs, with theoretical proofs and empirical tests on synthetic and real-world datasets.", "result": "The hybrid model outperforms SAEs, VAEs, and diffusion models in tasks like estimating manifold dimensions and producing sparser latent representations without sacrificing reconstruction accuracy.", "conclusion": "The hybrid model effectively addresses limitations of traditional SAEs and VAEs, demonstrating superior performance in practical applications."}}
{"id": "2506.04870", "pdf": "https://arxiv.org/pdf/2506.04870", "abs": "https://arxiv.org/abs/2506.04870", "authors": ["Antonio Almud\u00e9var", "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato", "Sameer Khurana", "Ricard Marxer", "Alfonso Ortega"], "title": "Aligning Multimodal Representations through an Information Bottleneck", "categories": ["cs.LG"], "comment": null, "summary": "Contrastive losses have been extensively used as a tool for multimodal\nrepresentation learning. However, it has been empirically observed that their\nuse is not effective to learn an aligned representation space. In this paper,\nwe argue that this phenomenon is caused by the presence of modality-specific\ninformation in the representation space. Although some of the most widely used\ncontrastive losses maximize the mutual information between representations of\nboth modalities, they are not designed to remove the modality-specific\ninformation. We give a theoretical description of this problem through the lens\nof the Information Bottleneck Principle. We also empirically analyze how\ndifferent hyperparameters affect the emergence of this phenomenon in a\ncontrolled experimental setup. Finally, we propose a regularization term in the\nloss function that is derived by means of a variational approximation and aims\nto increase the representational alignment. We analyze in a set of controlled\nexperiments and real-world applications the advantages of including this\nregularization term.", "AI": {"tldr": "The paper addresses the ineffectiveness of contrastive losses in learning aligned multimodal representations due to modality-specific information. It proposes a regularization term to improve alignment.", "motivation": "Contrastive losses fail to align multimodal representations because they retain modality-specific information, despite maximizing mutual information.", "method": "Theoretical analysis via the Information Bottleneck Principle, empirical hyperparameter study, and introduction of a variational approximation-derived regularization term.", "result": "The proposed regularization term enhances representational alignment, validated in controlled experiments and real-world applications.", "conclusion": "The regularization term effectively mitigates modality-specific information, improving multimodal representation alignment."}}
{"id": "2506.04877", "pdf": "https://arxiv.org/pdf/2506.04877", "abs": "https://arxiv.org/abs/2506.04877", "authors": ["Antonio Almud\u00e9var", "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato", "Alfonso Ortega"], "title": "There Was Never a Bottleneck in Concept Bottleneck Models", "categories": ["cs.LG"], "comment": null, "summary": "Deep learning representations are often difficult to interpret, which can\nhinder their deployment in sensitive applications. Concept Bottleneck Models\n(CBMs) have emerged as a promising approach to mitigate this issue by learning\nrepresentations that support target task performance while ensuring that each\ncomponent predicts a concrete concept from a predefined set. In this work, we\nargue that CBMs do not impose a true bottleneck: the fact that a component can\npredict a concept does not guarantee that it encodes only information about\nthat concept. This shortcoming raises concerns regarding interpretability and\nthe validity of intervention procedures. To overcome this limitation, we\npropose Minimal Concept Bottleneck Models (MCBMs), which incorporate an\nInformation Bottleneck (IB) objective to constrain each representation\ncomponent to retain only the information relevant to its corresponding concept.\nThis IB is implemented via a variational regularization term added to the\ntraining loss. As a result, MCBMs support concept-level interventions with\ntheoretical guarantees, remain consistent with Bayesian principles, and offer\ngreater flexibility in key design choices.", "AI": {"tldr": "The paper introduces Minimal Concept Bottleneck Models (MCBMs) to improve interpretability in deep learning by ensuring each representation component encodes only relevant concept information, unlike traditional CBMs.", "motivation": "Deep learning representations lack interpretability, especially in sensitive applications. CBMs partially address this but fail to ensure components encode only their assigned concepts, raising concerns about interpretability and intervention validity.", "method": "The authors propose MCBMs, which use an Information Bottleneck (IB) objective via variational regularization to constrain each representation component to retain only concept-relevant information.", "result": "MCBMs provide theoretical guarantees for concept-level interventions, align with Bayesian principles, and offer design flexibility.", "conclusion": "MCBMs enhance interpretability and intervention validity in deep learning by enforcing a true bottleneck, addressing limitations of CBMs."}}
{"id": "2506.04886", "pdf": "https://arxiv.org/pdf/2506.04886", "abs": "https://arxiv.org/abs/2506.04886", "authors": ["Allen Paul", "George Grammatopoulos", "Adwaye Rambojun", "Neill D. F. Campbell", "Harinderjit S. Gill", "Tony Shardlow"], "title": "Gaussian Process Diffeomorphic Statistical Shape Modelling Outperforms Angle-Based Methods for Assessment of Hip Dysplasia", "categories": ["cs.LG"], "comment": null, "summary": "Dysplasia is a recognised risk factor for osteoarthritis (OA) of the hip,\nearly diagnosis of dysplasia is important to provide opportunities for surgical\ninterventions aimed at reducing the risk of hip OA. We have developed a\npipeline for semi-automated classification of dysplasia using volumetric CT\nscans of patients' hips and a minimal set of clinically annotated landmarks,\ncombining the framework of the Gaussian Process Latent Variable Model with\ndiffeomorphism to create a statistical shape model, which we termed the\nGaussian Process Diffeomorphic Statistical Shape Model (GPDSSM). We used 192 CT\nscans, 100 for model training and 92 for testing. The GPDSSM effectively\ndistinguishes dysplastic samples from controls while also highlighting regions\nof the underlying surface that show dysplastic variations. As well as improving\nclassification accuracy compared to angle-based methods (AUC 96.2% vs 91.2%),\nthe GPDSSM can save time for clinicians by removing the need to manually\nmeasure angles and interpreting 2D scans for possible markers of dysplasia.", "AI": {"tldr": "A semi-automated pipeline using GPDSSM improves dysplasia classification in hip CT scans, outperforming angle-based methods with 96.2% AUC.", "motivation": "Early diagnosis of hip dysplasia is crucial to prevent osteoarthritis, requiring better tools than manual angle measurements.", "method": "Developed GPDSSM, combining Gaussian Process Latent Variable Model with diffeomorphism, using 192 CT scans (100 training, 92 testing).", "result": "GPDSSM achieved 96.2% AUC, outperforming angle-based methods (91.2%), and identified dysplastic variations in hip surfaces.", "conclusion": "GPDSSM enhances dysplasia diagnosis accuracy and efficiency, reducing manual workload for clinicians."}}
{"id": "2506.04913", "pdf": "https://arxiv.org/pdf/2506.04913", "abs": "https://arxiv.org/abs/2506.04913", "authors": ["Yongyu Mu", "Jiali Zeng", "Bei Li", "Xinyan Guan", "Fandong Meng", "Jie Zhou", "Tong Xiao", "Jingbo Zhu"], "title": "Dissecting Long Reasoning Models: An Empirical Study", "categories": ["cs.LG", "cs.CL"], "comment": "Working in process", "summary": "Despite recent progress in training long-context reasoning models via\nreinforcement learning (RL), several open questions and counterintuitive\nbehaviors remain. This work focuses on three key aspects: (1) We systematically\nanalyze the roles of positive and negative samples in RL, revealing that\npositive samples mainly facilitate data fitting, whereas negative samples\nsignificantly enhance generalization and robustness. Interestingly, training\nsolely on negative samples can rival standard RL training performance. (2) We\nidentify substantial data inefficiency in group relative policy optimization,\nwhere over half of the samples yield zero advantage. To address this, we\nexplore two straightforward strategies, including relative length rewards and\noffline sample injection, to better leverage these data and enhance reasoning\nefficiency and capability. (3) We investigate unstable performance across\nvarious reasoning models and benchmarks, attributing instability to uncertain\nproblems with ambiguous outcomes, and demonstrate that multiple evaluation runs\nmitigate this issue.", "AI": {"tldr": "The paper analyzes key aspects of RL for long-context reasoning, highlighting the roles of positive/negative samples, data inefficiency, and performance instability.", "motivation": "To address open questions and counterintuitive behaviors in RL for long-context reasoning, focusing on sample roles, data efficiency, and model stability.", "method": "Systematic analysis of sample roles, strategies to improve data efficiency (e.g., relative length rewards), and multiple evaluation runs to address instability.", "result": "Negative samples enhance generalization; data inefficiency identified and mitigated; multiple evaluations stabilize performance.", "conclusion": "The study provides insights and practical solutions to improve RL training for long-context reasoning."}}
{"id": "2506.04924", "pdf": "https://arxiv.org/pdf/2506.04924", "abs": "https://arxiv.org/abs/2506.04924", "authors": ["Han Wang", "Ruoyun He", "Guoguang Lao", "Ting Liu", "Hejiao Luo", "Changqi Qin", "Hongying Luo", "Junmin Huang", "Zihan Wei", "Lu Chen", "Yongzhi Xu", "Ziqian Bi", "Junhao Song", "Tianyang Wang", "Chia Xin Liang", "Xinyuan Song", "Huafeng Liu", "Junfeng Hao", "Chunjie Tian"], "title": "Predicting ICU In-Hospital Mortality Using Adaptive Transformer Layer Fusion", "categories": ["cs.LG"], "comment": "21 pages, 6 figures", "summary": "Early identification of high-risk ICU patients is crucial for directing\nlimited medical resources. We introduce ALFIA (Adaptive Layer Fusion with\nIntelligent Attention), a modular, attention-based architecture that jointly\ntrains LoRA (Low-Rank Adaptation) adapters and an adaptive layer-weighting\nmechanism to fuse multi-layer semantic features from a BERT backbone. Trained\non our rigorous cw-24 (CriticalWindow-24) benchmark, ALFIA surpasses\nstate-of-the-art tabular classifiers in AUPRC while preserving a balanced\nprecision-recall profile. The embeddings produced by ALFIA's fusion module,\ncapturing both fine-grained clinical cues and high-level concepts, enable\nseamless pairing with GBDTs (CatBoost/LightGBM) as ALFIA-boost, and deep neuro\nnetworks as ALFIA-nn, yielding additional performance gains. Our experiments\nconfirm ALFIA's superior early-warning performance, by operating directly on\nroutine clinical text, it furnishes clinicians with a convenient yet robust\ntool for risk stratification and timely intervention in critical-care settings.", "AI": {"tldr": "ALFIA is an attention-based model for early ICU patient risk identification, outperforming state-of-the-art methods and enabling integration with other models for enhanced performance.", "motivation": "Early identification of high-risk ICU patients is critical for resource allocation and timely intervention.", "method": "ALFIA uses a modular, attention-based architecture with LoRA adapters and adaptive layer-weighting to fuse multi-layer BERT features, trained on the cw-24 benchmark.", "result": "ALFIA surpasses tabular classifiers in AUPRC and integrates with GBDTs and deep networks (ALFIA-boost, ALFIA-nn) for further gains.", "conclusion": "ALFIA provides a robust tool for risk stratification in critical-care settings using routine clinical text."}}
{"id": "2506.04980", "pdf": "https://arxiv.org/pdf/2506.04980", "abs": "https://arxiv.org/abs/2506.04980", "authors": ["Marcos Lima Romero", "Ricardo Suyama"], "title": "Agentic AI for Intent-Based Industrial Automation", "categories": ["cs.LG", "cs.SY", "eess.SY"], "comment": "Preprint - Submitted to 16th IEEE/IAS International Conference on\n  Industry Applications - INDUSCON 2025", "summary": "The recent development of Agentic AI systems, empowered by autonomous large\nlanguage models (LLMs) agents with planning and tool-usage capabilities,\nenables new possibilities for the evolution of industrial automation and\nreduces the complexity introduced by Industry 4.0. This work proposes a\nconceptual framework that integrates Agentic AI with the intent-based paradigm,\noriginally developed in network research, to simplify human-machine interaction\n(HMI) and better align automation systems with the human-centric, sustainable,\nand resilient principles of Industry 5.0. Based on the intent-based processing,\nthe framework allows human operators to express high-level business or\noperational goals in natural language, which are decomposed into actionable\ncomponents. These intents are broken into expectations, conditions, targets,\ncontext, and information that guide sub-agents equipped with specialized tools\nto execute domain-specific tasks. A proof of concept was implemented using the\nCMAPSS dataset and Google Agent Developer Kit (ADK), demonstrating the\nfeasibility of intent decomposition, agent orchestration, and autonomous\ndecision-making in predictive maintenance scenarios. The results confirm the\npotential of this approach to reduce technical barriers and enable scalable,\nintent-driven automation, despite data quality and explainability concerns.", "AI": {"tldr": "A framework integrating Agentic AI with intent-based paradigms simplifies HMI and aligns with Industry 5.0 principles, demonstrated via predictive maintenance.", "motivation": "To reduce complexity in Industry 4.0 and align automation with human-centric, sustainable, and resilient goals of Industry 5.0.", "method": "Proposes a framework combining Agentic AI and intent-based processing, allowing natural language goals to be decomposed into actionable tasks by specialized sub-agents.", "result": "Proof of concept using CMAPSS dataset and Google ADK shows feasibility in predictive maintenance, reducing technical barriers.", "conclusion": "The approach enables scalable, intent-driven automation, though data quality and explainability remain challenges."}}
{"id": "2506.04985", "pdf": "https://arxiv.org/pdf/2506.04985", "abs": "https://arxiv.org/abs/2506.04985", "authors": ["Boris van Breugel", "Yelysei Bondarenko", "Paul Whatmough", "Markus Nagel"], "title": "FPTQuant: Function-Preserving Transforms for LLM Quantization", "categories": ["cs.LG"], "comment": null, "summary": "Large language models (LLMs) require substantial compute, and thus energy, at\ninference time. While quantizing weights and activations is effective at\nimproving efficiency, naive quantization of LLMs can significantly degrade\nperformance due to large magnitude outliers. This paper describes FPTQuant,\nwhich introduces four novel, lightweight, and expressive function-preserving\ntransforms (FPTs) to facilitate quantization of transformers: (1) a mergeable\npre-RoPE transform for queries and keys, (2) a mergeable transform for values,\n(3) a mergeable scaling transform within the MLP block, and (4) a cheap,\ndynamic scaling transform. By leveraging the equivariances and independencies\ninherent to canonical transformer operation, we designed these FPTs to maintain\nthe model's function while shaping the intermediate activation distributions to\nbe more quantization friendly. FPTQuant requires no custom kernels and adds\nvirtually no overhead during inference. The FPTs are trained both locally to\nreduce outliers, and end-to-end such that the outputs of the quantized and\nfull-precision models match. FPTQuant enables static INT4 quantization with\nminimal overhead and shows SOTA speed-up of up to 3.9 times over FP.\nEmpirically, FPTQuant has an excellent accuracy-speed trade-off -- it is\nperforming on par or exceeding most prior work and only shows slightly lower\naccuracy compared to a method that is up to 29% slower.", "AI": {"tldr": "FPTQuant introduces lightweight transforms to improve LLM quantization efficiency without degrading performance, achieving up to 3.9x speed-up with minimal accuracy loss.", "motivation": "Large language models (LLMs) consume significant energy during inference, and naive quantization harms performance due to outliers. FPTQuant addresses this by preserving model function while enabling efficient quantization.", "method": "FPTQuant uses four novel function-preserving transforms (FPTs) to reshape activation distributions for better quantization. These include transforms for queries, keys, values, and MLP blocks, trained locally and end-to-end.", "result": "FPTQuant enables static INT4 quantization with minimal overhead, achieving up to 3.9x speed-up over FP while maintaining accuracy comparable to slower methods.", "conclusion": "FPTQuant offers a state-of-the-art solution for efficient LLM quantization, balancing accuracy and speed effectively."}}
{"id": "2506.05005", "pdf": "https://arxiv.org/pdf/2506.05005", "abs": "https://arxiv.org/abs/2506.05005", "authors": ["Ashkan Soleymani", "Georgios Piliouras", "Gabriele Farina"], "title": "Cautious Optimism: A Meta-Algorithm for Near-Constant Regret in General Games", "categories": ["cs.LG", "cs.GT", "math.OC"], "comment": "Extended abstract appeared at Twenty-Sixth ACM Conference on\n  Economics and Computation (EC), 2025", "summary": "Recent work [Soleymani et al., 2025] introduced a variant of Optimistic\nMultiplicative Weights Updates (OMWU) that adaptively controls the learning\npace in a dynamic, non-monotone manner, achieving new state-of-the-art regret\nminimization guarantees in general games. In this work, we demonstrate that\nno-regret learning acceleration through adaptive pacing of the learners is not\nan isolated phenomenon. We introduce \\emph{Cautious Optimism}, a framework for\nsubstantially faster regularized learning in general games. Cautious Optimism\ntakes as input any instance of Follow-the-Regularized-Leader (FTRL) and outputs\nan accelerated no-regret learning algorithm by pacing the underlying FTRL with\nminimal computational overhead. Importantly, we retain uncoupledness (learners\ndo not need to know other players' utilities). Cautious Optimistic FTRL\nachieves near-optimal $O_T(\\log T)$ regret in diverse self-play\n(mixing-and-matching regularizers) while preserving the optimal $O(\\sqrt{T})$\nregret in adversarial scenarios. In contrast to prior works (e.g. Syrgkanis et\nal. [2015], Daskalakis et al. [2021]), our analysis does not rely on monotonic\nstep-sizes, showcasing a novel route for fast learning in general games.", "AI": {"tldr": "The paper introduces 'Cautious Optimism,' a framework for accelerating no-regret learning in general games by adaptively pacing FTRL algorithms, achieving near-optimal regret bounds without relying on monotonic step-sizes.", "motivation": "To show that adaptive pacing for no-regret learning is not isolated and can be generalized to accelerate FTRL algorithms in diverse game settings.", "method": "Proposes 'Cautious Optimism,' which takes any FTRL instance and outputs an accelerated algorithm with minimal computational overhead, retaining uncoupledness.", "result": "Achieves near-optimal O_T(log T) regret in self-play and preserves O(sqrt(T)) regret in adversarial scenarios.", "conclusion": "Demonstrates a novel, non-monotonic approach for fast learning in general games, outperforming prior methods."}}
{"id": "2506.05014", "pdf": "https://arxiv.org/pdf/2506.05014", "abs": "https://arxiv.org/abs/2506.05014", "authors": ["Nektarios Kalampalikis", "Kavya Gupta", "Georgi Vitanov", "Isabel Valera"], "title": "Towards Reasonable Concept Bottleneck Models", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "26 pages, 17 figures", "summary": "In this paper, we propose $\\textbf{C}$oncept $\\textbf{REA}$soning\n$\\textbf{M}$odels (CREAM), a novel family of Concept Bottleneck Models (CBMs)\nthat: (i) explicitly encodes concept-concept (${\\texttt{C-C}}$) and\nconcept-task (${\\texttt{C$\\rightarrow$Y}}$) relationships to enforce a desired\nmodel reasoning; and (ii) use a regularized side-channel to achieve competitive\ntask performance, while keeping high concept importance. Specifically, CREAM\narchitecturally embeds (bi)directed concept-concept, and concept to task\nrelationships specified by a human expert, while severing undesired information\nflows (e.g., to handle mutually exclusive concepts). Moreover, CREAM integrates\na black-box side-channel that is regularized to encourage task predictions to\nbe grounded in the relevant concepts, thereby utilizing the side-channel only\nwhen necessary to enhance performance. Our experiments show that: (i) CREAM\nmainly relies on concepts while achieving task performance on par with\nblack-box models; and (ii) the embedded ${\\texttt{C-C}}$ and\n${\\texttt{C$\\rightarrow$Y}}$ relationships ease model interventions and\nmitigate concept leakage.", "AI": {"tldr": "CREAM is a new CBM family that encodes concept-concept and concept-task relationships, uses a regularized side-channel for performance, and maintains high concept importance.", "motivation": "To improve model reasoning by explicitly encoding human-specified relationships between concepts and tasks while preventing undesired information flows.", "method": "CREAM embeds (bi)directed C-C and C\u2192Y relationships, uses a regularized side-channel, and severs undesired flows.", "result": "CREAM achieves task performance comparable to black-box models while relying on concepts and easing interventions.", "conclusion": "CREAM effectively balances concept importance and task performance, mitigates concept leakage, and supports human interventions."}}
{"id": "2506.05016", "pdf": "https://arxiv.org/pdf/2506.05016", "abs": "https://arxiv.org/abs/2506.05016", "authors": ["John Collins"], "title": "Multi-Point Proximity Encoding For Vector-Mode Geospatial Machine Learning", "categories": ["cs.LG", "68T07, 68T30", "I.2.4; J.2"], "comment": "10 pages, 8 figures", "summary": "Vector-mode geospatial data -- points, lines, and polygons -- must be encoded\ninto an appropriate form in order to be used with traditional machine learning\nand artificial intelligence models. Encoding methods attempt to represent a\ngiven shape as a vector that captures its essential geometric properties. This\npaper presents an encoding method based on scaled distances from a shape to a\nset of reference points within a region of interest. The method, MultiPoint\nProximity (MPP) encoding, can be applied to any type of shape, enabling the\nparameterization of machine learning models with encoded representations of\nvector-mode geospatial features. We show that MPP encoding possesses the\ndesirable properties of shape-centricity and continuity, can be used to\ndifferentiate spatial objects based on their geometric features, and can\ncapture pairwise spatial relationships with high precision. In all cases, MPP\nencoding is shown to perform better than an alternative method based on\nrasterization.", "AI": {"tldr": "MPP encoding transforms geospatial shapes into vectors using scaled distances from reference points, outperforming rasterization in capturing geometric features and spatial relationships.", "motivation": "To enable machine learning models to process vector-mode geospatial data (points, lines, polygons) by encoding shapes into vectors that preserve geometric properties.", "method": "MultiPoint Proximity (MPP) encoding, which uses scaled distances from a shape to reference points within a region of interest.", "result": "MPP encoding exhibits shape-centricity, continuity, and high precision in differentiating spatial objects and capturing pairwise relationships, outperforming rasterization.", "conclusion": "MPP encoding is a versatile and effective method for encoding geospatial shapes, enhancing their usability in machine learning models."}}
{"id": "2506.05027", "pdf": "https://arxiv.org/pdf/2506.05027", "abs": "https://arxiv.org/abs/2506.05027", "authors": ["Kuang He", "Wei Tang", "Tong Wei", "Min-Ling Zhang"], "title": "Tuning the Right Foundation Models is What you Need for Partial Label Learning", "categories": ["cs.LG"], "comment": "The code can be found at \\url{https://github.com/SEU-hk/PartialCLIP}", "summary": "Partial label learning (PLL) seeks to train generalizable classifiers from\ndatasets with inexact supervision, a common challenge in real-world\napplications. Existing studies have developed numerous approaches to\nprogressively refine and recover ground-truth labels by training convolutional\nneural networks. However, limited attention has been given to foundation models\nthat offer transferrable representations. In this work, we empirically conduct\ncomprehensive evaluations of 11 foundation models across 13 PLL approaches on 8\nbenchmark datasets under 3 PLL scenarios. We further propose PartialCLIP, an\nefficient fine-tuning framework for foundation models in PLL. Our findings\nreveal that current PLL approaches tend to 1) achieve significant performance\ngains when using foundation models, 2) exhibit remarkably similar performance\nto each other, 3) maintain stable performance across varying ambiguity levels,\nwhile 4) are susceptible to foundation model selection and adaptation\nstrategies. Additionally, we demonstrate the efficacy of text-embedding\nclassifier initialization and effective candidate label filtering using\nzero-shot CLIP. Our experimental results and analysis underscore the\nlimitations of current PLL approaches and provide valuable insights for\ndeveloping more generalizable PLL models. The source code can be found at\nhttps://github.com/SEU-hk/PartialCLIP.", "AI": {"tldr": "The paper evaluates foundation models in partial label learning (PLL), proposes PartialCLIP for efficient fine-tuning, and highlights key insights about current PLL approaches.", "motivation": "To address the lack of attention to foundation models in PLL and improve generalizability of classifiers trained with inexact supervision.", "method": "Empirical evaluation of 11 foundation models across 13 PLL approaches on 8 datasets under 3 scenarios, and introduction of PartialCLIP for fine-tuning.", "result": "Foundation models significantly boost PLL performance; approaches show similar results, stable across ambiguity levels, but depend on model selection and adaptation.", "conclusion": "The study reveals limitations of current PLL methods and offers insights for developing more generalizable models, with PartialCLIP as a promising solution."}}
{"id": "2506.05032", "pdf": "https://arxiv.org/pdf/2506.05032", "abs": "https://arxiv.org/abs/2506.05032", "authors": ["Zeming Wei", "Yiwen Guo", "Yisen Wang"], "title": "Identifying and Understanding Cross-Class Features in Adversarial Training", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.CV", "math.OC"], "comment": "ICML 2025", "summary": "Adversarial training (AT) has been considered one of the most effective\nmethods for making deep neural networks robust against adversarial attacks,\nwhile the training mechanisms and dynamics of AT remain open research problems.\nIn this paper, we present a novel perspective on studying AT through the lens\nof class-wise feature attribution. Specifically, we identify the impact of a\nkey family of features on AT that are shared by multiple classes, which we call\ncross-class features. These features are typically useful for robust\nclassification, which we offer theoretical evidence to illustrate through a\nsynthetic data model. Through systematic studies across multiple model\narchitectures and settings, we find that during the initial stage of AT, the\nmodel tends to learn more cross-class features until the best robustness\ncheckpoint. As AT further squeezes the training robust loss and causes robust\noverfitting, the model tends to make decisions based on more class-specific\nfeatures. Based on these discoveries, we further provide a unified view of two\nexisting properties of AT, including the advantage of soft-label training and\nrobust overfitting. Overall, these insights refine the current understanding of\nAT mechanisms and provide new perspectives on studying them. Our code is\navailable at https://github.com/PKU-ML/Cross-Class-Features-AT.", "AI": {"tldr": "The paper studies adversarial training (AT) through class-wise feature attribution, identifying cross-class features as key for robustness. It shows models initially learn these features but shift to class-specific ones during overfitting, unifying insights into AT mechanisms.", "motivation": "To understand the training dynamics and mechanisms of adversarial training (AT) by analyzing class-wise feature attribution, particularly focusing on cross-class features.", "method": "The study uses theoretical analysis with a synthetic data model and systematic experiments across various model architectures to examine the role of cross-class features in AT.", "result": "Models initially learn cross-class features for robustness but shift to class-specific features during robust overfitting. The findings unify properties like soft-label training advantages and robust overfitting.", "conclusion": "The insights refine understanding of AT mechanisms, offering new perspectives for future research. Code is available for reproducibility."}}
{"id": "2506.05035", "pdf": "https://arxiv.org/pdf/2506.05035", "abs": "https://arxiv.org/abs/2506.05035", "authors": ["Hyeongwon Jang", "Changhun Kim", "Eunho Yang"], "title": "TIMING: Temporality-Aware Integrated Gradients for Time Series Explanation", "categories": ["cs.LG", "cs.AI"], "comment": "ICML 2025 Spotlight Presentation; Code is available at\n  https://github.com/drumpt/TIMING", "summary": "Recent explainable artificial intelligence (XAI) methods for time series\nprimarily estimate point-wise attribution magnitudes, while overlooking the\ndirectional impact on predictions, leading to suboptimal identification of\nsignificant points. Our analysis shows that conventional Integrated Gradients\n(IG) effectively capture critical points with both positive and negative\nimpacts on predictions. However, current evaluation metrics fail to assess this\ncapability, as they inadvertently cancel out opposing feature contributions. To\naddress this limitation, we propose novel evaluation metrics-Cumulative\nPrediction Difference (CPD) and Cumulative Prediction Preservation (CPP)-to\nsystematically assess whether attribution methods accurately identify\nsignificant positive and negative points in time series XAI. Under these\nmetrics, conventional IG outperforms recent counterparts. However, directly\napplying IG to time series data may lead to suboptimal outcomes, as generated\npaths ignore temporal relationships and introduce out-of-distribution samples.\nTo overcome these challenges, we introduce TIMING, which enhances IG by\nincorporating temporal awareness while maintaining its theoretical properties.\nExtensive experiments on synthetic and real-world time series benchmarks\ndemonstrate that TIMING outperforms existing time series XAI baselines. Our\ncode is available at https://github.com/drumpt/TIMING.", "AI": {"tldr": "The paper introduces TIMING, a temporally-aware enhancement of Integrated Gradients (IG) for time series XAI, addressing limitations of current methods in capturing directional impacts and temporal relationships.", "motivation": "Current XAI methods for time series focus on point-wise attribution magnitudes but overlook directional impacts, leading to suboptimal identification of critical points. Existing evaluation metrics also fail to assess these impacts.", "method": "The authors propose TIMING, an improved version of IG that incorporates temporal awareness. They also introduce new evaluation metrics (CPD and CPP) to assess attribution methods.", "result": "TIMING outperforms existing baselines in identifying significant positive and negative points in time series data, as demonstrated on synthetic and real-world benchmarks.", "conclusion": "TIMING enhances IG for time series XAI by addressing temporal relationships and directional impacts, validated by superior performance under the proposed metrics."}}
{"id": "2506.05039", "pdf": "https://arxiv.org/pdf/2506.05039", "abs": "https://arxiv.org/abs/2506.05039", "authors": ["Nicolas Lell", "Ansgar Scherp"], "title": "iN2V: Bringing Transductive Node Embeddings to Inductive Graphs", "categories": ["cs.LG"], "comment": null, "summary": "Shallow node embeddings like node2vec (N2V) can be used for nodes without\nfeatures or to supplement existing features with structure-based information.\nEmbedding methods like N2V are limited in their application on new nodes, which\nrestricts them to the transductive setting where the entire graph, including\nthe test nodes, is available during training. We propose inductive node2vec\n(iN2V), which combines a post-hoc procedure to compute embeddings for nodes\nunseen during training and modifications to the original N2V training procedure\nto prepare the embeddings for this post-hoc procedure. We conduct experiments\non several benchmark datasets and demonstrate that iN2V is an effective\napproach to bringing transductive embeddings to an inductive setting. Using\niN2V embeddings improves node classification by 1 point on average, with up to\n6 points of improvement depending on the dataset and the number of unseen\nnodes. Our iN2V is a plug-in approach to create new or enrich existing\nembeddings. It can also be combined with other embedding methods, making it a\nversatile approach for inductive node representation learning. Code to\nreproduce the results is available at https://github.com/Foisunt/iN2V .", "AI": {"tldr": "iN2V extends node2vec to an inductive setting, enabling embeddings for unseen nodes and improving node classification.", "motivation": "Overcome the limitation of transductive node embedding methods like node2vec, which cannot handle new nodes post-training.", "method": "Combines a post-hoc procedure for unseen nodes with modified N2V training to prepare embeddings.", "result": "Improves node classification by 1 point on average, with up to 6 points on certain datasets.", "conclusion": "iN2V is a versatile, plug-in approach for inductive node representation learning, compatible with other embedding methods."}}
{"id": "2506.05047", "pdf": "https://arxiv.org/pdf/2506.05047", "abs": "https://arxiv.org/abs/2506.05047", "authors": ["Viet Nguyen Changjian Shui", "Vijay Giri", "Siddarth Arya", "Amol Verma", "Fahad Razak", "Rahul G. Krishnan"], "title": "Reliably detecting model failures in deployment without labels", "categories": ["cs.LG"], "comment": "36 pages, 6 figures, 7 tables, submitted to NeurIPS 2025, includes\n  theoretical analysis and extensive empirical evaluation across benchmark and\n  clinical datasets. Code available at https://github.com/teivng/d3m. Viet\n  Nguyen and Changjian Shui contributed equally", "summary": "The distribution of data changes over time; models operating operating in\ndynamic environments need retraining. But knowing when to retrain, without\naccess to labels, is an open challenge since some, but not all shifts degrade\nmodel performance. This paper formalizes and addresses the problem of\npost-deployment deterioration (PDD) monitoring. We propose D3M, a practical and\nefficient monitoring algorithm based on the disagreement of predictive models,\nachieving low false positive rates under non-deteriorating shifts and provides\nsample complexity bounds for high true positive rates under deteriorating\nshifts. Empirical results on both standard benchmark and a real-world\nlarge-scale internal medicine dataset demonstrate the effectiveness of the\nframework and highlight its viability as an alert mechanism for high-stakes\nmachine learning pipelines.", "AI": {"tldr": "The paper introduces D3M, a monitoring algorithm to detect when models need retraining due to data shifts, focusing on post-deployment deterioration (PDD).", "motivation": "Models in dynamic environments face performance degradation due to data shifts, but retraining without label access is challenging.", "method": "D3M uses predictive model disagreement to monitor shifts, ensuring low false positives and high true positives.", "result": "Empirical tests on benchmarks and real-world data show D3M effectively detects deteriorating shifts.", "conclusion": "D3M is a viable alert mechanism for high-stakes ML pipelines, addressing PDD efficiently."}}
{"id": "2506.05059", "pdf": "https://arxiv.org/pdf/2506.05059", "abs": "https://arxiv.org/abs/2506.05059", "authors": ["Shijian Xu", "Marcello Massimo Negri", "Volker Roth"], "title": "NIMO: a Nonlinear Interpretable MOdel", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Neural networks (NNs) have achieved tremendous success over the past decade,\nyet they are still extremely difficult to interpret. In contrast, linear models\nare less expressive but offer inherent interpretability. Linear coefficients\nare interpretable as the marginal effect of a feature on the prediction,\nassuming all other features are kept fixed. To combine the benefits of both\napproaches, we introduce NIMO (Nonlinear Interpretable MOdel). The key idea is\nto define a model where the NN is designed to learn nonlinear corrections to\nthe linear model predictions, while also maintaining the original\ninterpretability of the linear coefficients. Relevantly, we develop an\noptimization algorithm based on profile likelihood that elegantly allows for\noptimizing over the NN parameters while updating the linear coefficients\nanalytically. By relying on adaptive ridge regression we can easily incorporate\nsparsity constraints as well. We show empirically that we can recover the\nunderlying linear coefficients while significantly improving the predictive\naccuracy. Compared to other hybrid interpretable approaches, our model is the\nonly one that actually maintains the same interpretability of linear\ncoefficients as in linear models. We also achieve higher performance on various\nregression and classification settings.", "AI": {"tldr": "NIMO combines neural networks and linear models for interpretability and accuracy, using profile likelihood optimization to maintain interpretable linear coefficients while improving predictions.", "motivation": "Neural networks lack interpretability, while linear models are interpretable but less expressive. The goal is to merge both strengths.", "method": "Introduces NIMO, where a neural network learns nonlinear corrections to a linear model, optimized via profile likelihood and adaptive ridge regression for sparsity.", "result": "Empirically recovers linear coefficients and improves predictive accuracy, outperforming other hybrid interpretable methods.", "conclusion": "NIMO successfully balances interpretability and performance, maintaining linear model interpretability while enhancing accuracy."}}
{"id": "2506.05065", "pdf": "https://arxiv.org/pdf/2506.05065", "abs": "https://arxiv.org/abs/2506.05065", "authors": ["Marten Lienen", "Abdullah Saydemir", "Stephan G\u00fcnnemann"], "title": "UnHiPPO: Uncertainty-aware Initialization for State Space Models", "categories": ["cs.LG", "stat.ML"], "comment": "Published at ICML 2025", "summary": "State space models are emerging as a dominant model class for sequence\nproblems with many relying on the HiPPO framework to initialize their dynamics.\nHowever, HiPPO fundamentally assumes data to be noise-free; an assumption often\nviolated in practice. We extend the HiPPO theory with measurement noise and\nderive an uncertainty-aware initialization for state space model dynamics. In\nour analysis, we interpret HiPPO as a linear stochastic control problem where\nthe data enters as a noise-free control signal. We then reformulate the problem\nso that the data become noisy outputs of a latent system and arrive at an\nalternative dynamics initialization that infers the posterior of this latent\nsystem from the data without increasing runtime. Our experiments show that our\ninitialization improves the resistance of state-space models to noise both at\ntraining and inference time. Find our implementation at\nhttps://cs.cit.tum.de/daml/unhippo.", "AI": {"tldr": "The paper extends HiPPO theory to handle measurement noise, providing an uncertainty-aware initialization for state space models, improving noise resistance.", "motivation": "HiPPO assumes noise-free data, which is often unrealistic. The paper addresses this limitation by incorporating measurement noise into the framework.", "method": "Reformulates HiPPO as a linear stochastic control problem, treating data as noisy outputs of a latent system, and derives a new dynamics initialization.", "result": "The proposed initialization enhances state-space models' robustness to noise during training and inference.", "conclusion": "The extension of HiPPO with noise-aware dynamics offers practical improvements for real-world sequence problems."}}
{"id": "2506.05088", "pdf": "https://arxiv.org/pdf/2506.05088", "abs": "https://arxiv.org/abs/2506.05088", "authors": ["Tobias Pielok", "Bernd Bischl", "David R\u00fcgamer"], "title": "Semi-Implicit Variational Inference via Kernelized Path Gradient Descent", "categories": ["cs.LG", "stat.CO", "62F15, 68T07", "I.2.6; G.3"], "comment": "Preliminary version", "summary": "Semi-implicit variational inference (SIVI) is a powerful framework for\napproximating complex posterior distributions, but training with the\nKullback-Leibler (KL) divergence can be challenging due to high variance and\nbias in high-dimensional settings. While current state-of-the-art semi-implicit\nvariational inference methods, particularly Kernel Semi-Implicit Variational\nInference (KSIVI), have been shown to work in high dimensions, training remains\nmoderately expensive. In this work, we propose a kernelized KL divergence\nestimator that stabilizes training through nonparametric smoothing. To further\nreduce the bias, we introduce an importance sampling correction. We provide a\ntheoretical connection to the amortized version of the Stein variational\ngradient descent, which estimates the score gradient via Stein's identity,\nshowing that both methods minimize the same objective, but our semi-implicit\napproach achieves lower gradient variance. In addition, our method's bias in\nfunction space is benign, leading to more stable and efficient optimization.\nEmpirical results demonstrate that our method outperforms or matches\nstate-of-the-art SIVI methods in both performance and training efficiency.", "AI": {"tldr": "Proposes a kernelized KL divergence estimator with importance sampling to improve stability and reduce bias in semi-implicit variational inference (SIVI), outperforming state-of-the-art methods.", "motivation": "Training SIVI with KL divergence is challenging due to high variance and bias in high dimensions, and current methods like KSIVI are expensive.", "method": "Introduces a kernelized KL divergence estimator with nonparametric smoothing and importance sampling correction, linking it theoretically to Stein variational gradient descent.", "result": "The method achieves lower gradient variance, benign bias, and outperforms or matches state-of-the-art SIVI methods in performance and efficiency.", "conclusion": "The proposed approach offers more stable and efficient optimization for SIVI, addressing key challenges in high-dimensional settings."}}
{"id": "2506.05101", "pdf": "https://arxiv.org/pdf/2506.05101", "abs": "https://arxiv.org/abs/2506.05101", "authors": ["Cl\u00e9ment Pierquin", "Aur\u00e9lien Bellet", "Marc Tommasi", "Matthieu Boussard"], "title": "Privacy Amplification Through Synthetic Data: Insights from Linear Regression", "categories": ["cs.LG", "cs.CR", "stat.ML"], "comment": "26 pages, ICML 2025", "summary": "Synthetic data inherits the differential privacy guarantees of the model used\nto generate it. Additionally, synthetic data may benefit from privacy\namplification when the generative model is kept hidden. While empirical studies\nsuggest this phenomenon, a rigorous theoretical understanding is still lacking.\nIn this paper, we investigate this question through the well-understood\nframework of linear regression. First, we establish negative results showing\nthat if an adversary controls the seed of the generative model, a single\nsynthetic data point can leak as much information as releasing the model\nitself. Conversely, we show that when synthetic data is generated from random\ninputs, releasing a limited number of synthetic data points amplifies privacy\nbeyond the model's inherent guarantees. We believe our findings in linear\nregression can serve as a foundation for deriving more general bounds in the\nfuture.", "AI": {"tldr": "The paper explores privacy amplification in synthetic data generation, showing risks when the model's seed is controlled and benefits when random inputs are used.", "motivation": "To rigorously understand privacy amplification in synthetic data, which lacks theoretical grounding despite empirical evidence.", "method": "Investigates the question using the framework of linear regression, analyzing scenarios where the generative model's seed is controlled versus random inputs.", "result": "Negative results show leakage risks if the seed is controlled, while random inputs amplify privacy beyond the model's guarantees.", "conclusion": "The findings in linear regression provide a foundation for future general privacy bounds."}}
{"id": "2506.05138", "pdf": "https://arxiv.org/pdf/2506.05138", "abs": "https://arxiv.org/abs/2506.05138", "authors": ["Pavle Vasiljevic", "Milica Matic", "Miroslav Popovic"], "title": "Federated Isolation Forest for Efficient Anomaly Detection on Edge IoT Systems", "categories": ["cs.LG", "cs.DC"], "comment": "6 pages, 4 algorithms, 5 figures, 2 tables", "summary": "Recently, federated learning frameworks such as Python TestBed for Federated\nLearning Algorithms and MicroPython TestBed for Federated Learning Algorithms\nhave emerged to tackle user privacy concerns and efficiency in embedded\nsystems. Even more recently, an efficient federated anomaly detection\nalgorithm, FLiForest, based on Isolation Forests has been developed, offering a\nlow-resource, unsupervised method well-suited for edge deployment and\ncontinuous learning. In this paper, we present an application of Isolation\nForest-based temperature anomaly detection, developed using the previously\nmentioned federated learning frameworks, aimed at small edge devices and IoT\nsystems running MicroPython. The system has been experimentally evaluated,\nachieving over 96% accuracy in distinguishing normal from abnormal readings and\nabove 78% precision in detecting anomalies across all tested configurations,\nwhile maintaining a memory usage below 160 KB during model training. These\nresults highlight its suitability for resource-constrained environments and\nedge systems, while upholding federated learning principles of data privacy and\ncollaborative learning.", "AI": {"tldr": "The paper presents an Isolation Forest-based temperature anomaly detection system for edge devices using federated learning frameworks, achieving high accuracy and low memory usage.", "motivation": "To address privacy concerns and efficiency in embedded systems, leveraging federated learning for anomaly detection in resource-constrained environments.", "method": "Developed FLiForest, an efficient federated anomaly detection algorithm based on Isolation Forests, and applied it to temperature anomaly detection using Python and MicroPython frameworks.", "result": "Achieved over 96% accuracy in distinguishing normal from abnormal readings and above 78% precision in anomaly detection, with memory usage below 160 KB.", "conclusion": "The system is suitable for resource-constrained edge and IoT systems, maintaining federated learning principles of privacy and collaborative learning."}}
{"id": "2506.05178", "pdf": "https://arxiv.org/pdf/2506.05178", "abs": "https://arxiv.org/abs/2506.05178", "authors": ["Joshua Hess", "Quaid Morris"], "title": "Associative Memory and Generative Diffusion in the Zero-noise Limit", "categories": ["cs.LG", "cond-mat.dis-nn", "math.DS", "nlin.AO", "q-bio.NC"], "comment": null, "summary": "Connections between generative diffusion and continuous-state associative\nmemory models are studied. Morse-Smale dynamical systems are emphasized as\nuniversal approximators of gradient-based associative memory models and\ndiffusion models as white-noise perturbed systems thereof. Universal properties\nof associative memory that follow from this description are described and used\nto characterize a generic transition from generation to memory as noise levels\ndiminish. Structural stability inherited by Morse-Smale flows is shown to imply\na notion of stability for diffusions at vanishing noise levels. Applied to one-\nand two-parameter families of gradients, this indicates stability at all but\nisolated points of associative memory learning landscapes and the learning and\ngeneration landscapes of diffusion models with gradient drift in the zero-noise\nlimit, at which small sets of generic bifurcations characterize qualitative\ntransitions between stable systems. Examples illustrating the characterization\nof these landscapes by sequences of these bifurcations are given, along with\nstructural stability criterion for classic and modern Hopfield networks\n(equivalently, the attention mechanism).", "AI": {"tldr": "The paper explores links between generative diffusion models and continuous-state associative memory, highlighting Morse-Smale systems as universal approximators and their stability properties.", "motivation": "To understand the universal properties of associative memory and diffusion models, and their transitions under noise reduction.", "method": "Analyzes Morse-Smale dynamical systems as approximations for gradient-based associative memory and diffusion models, focusing on structural stability and bifurcations.", "result": "Shows stability in associative memory and diffusion models at vanishing noise levels, with transitions characterized by generic bifurcations.", "conclusion": "The study provides a framework for understanding stability and transitions in associative memory and diffusion models, with implications for Hopfield networks and attention mechanisms."}}
{"id": "2506.05183", "pdf": "https://arxiv.org/pdf/2506.05183", "abs": "https://arxiv.org/abs/2506.05183", "authors": ["Zhicheng Yang", "Zhijiang Guo", "Yinya Huang", "Xiaodan Liang", "Yiwei Wang", "Jing Tang"], "title": "TreeRPO: Tree Relative Policy Optimization", "categories": ["cs.LG", "cs.AI"], "comment": "13pages, 6 figures", "summary": "Large Language Models (LLMs) have shown remarkable reasoning capabilities\nthrough Reinforcement Learning with Verifiable Rewards (RLVR) methods. However,\na key limitation of existing approaches is that rewards defined at the full\ntrajectory level provide insufficient guidance for optimizing the intermediate\nsteps of a reasoning process. To address this, we introduce \\textbf{\\name}, a\nnovel method that estimates the mathematical expectations of rewards at various\nreasoning steps using tree sampling. Unlike prior methods that rely on a\nseparate step reward model, \\name directly estimates these rewards through this\nsampling process. Building on the group-relative reward training mechanism of\nGRPO, \\name innovatively computes rewards based on step-level groups generated\nduring tree sampling. This advancement allows \\name to produce fine-grained and\ndense reward signals, significantly enhancing the learning process and overall\nperformance of LLMs. Experimental results demonstrate that our \\name algorithm\nsubstantially improves the average Pass@1 accuracy of Qwen-2.5-Math on test\nbenchmarks, increasing it from 19.0\\% to 35.5\\%. Furthermore, \\name\nsignificantly outperforms GRPO by 2.9\\% in performance while simultaneously\nreducing the average response length by 18.1\\%, showcasing its effectiveness\nand efficiency. Our code will be available at\n\\href{https://github.com/yangzhch6/TreeRPO}{https://github.com/yangzhch6/TreeRPO}.", "AI": {"tldr": "The paper introduces \\textbf{\\name}, a method using tree sampling to estimate step-level rewards for LLMs, improving reasoning performance and efficiency.", "motivation": "Existing RLVR methods lack fine-grained guidance for intermediate reasoning steps, limiting LLM performance.", "method": "\\textbf{\\name} employs tree sampling to estimate step-level rewards and integrates them with GRPO's group-relative reward mechanism.", "result": "\\textbf{\\name} boosts Qwen-2.5-Math's Pass@1 accuracy from 19.0% to 35.5% and outperforms GRPO by 2.9% while reducing response length by 18.1%.", "conclusion": "The method provides dense, fine-grained rewards, enhancing LLM reasoning efficiency and performance."}}
{"id": "2506.05196", "pdf": "https://arxiv.org/pdf/2506.05196", "abs": "https://arxiv.org/abs/2506.05196", "authors": ["Jifei Luo", "Wenzheng Wu", "Hantao Yao", "Lu Yu", "Changsheng Xu"], "title": "Locality Preserving Markovian Transition for Instance Retrieval", "categories": ["cs.LG"], "comment": "This paper has been accepted by ICML2025", "summary": "Diffusion-based re-ranking methods are effective in modeling the data\nmanifolds through similarity propagation in affinity graphs. However, positive\nsignals tend to diminish over several steps away from the source, reducing\ndiscriminative power beyond local regions. To address this issue, we introduce\nthe Locality Preserving Markovian Transition (LPMT) framework, which employs a\nlong-term thermodynamic transition process with multiple states for accurate\nmanifold distance measurement. The proposed LPMT first integrates diffusion\nprocesses across separate graphs using Bidirectional Collaborative Diffusion\n(BCD) to establish strong similarity relationships. Afterwards, Locality State\nEmbedding (LSE) encodes each instance into a distribution for enhanced local\nconsistency. These distributions are interconnected via the Thermodynamic\nMarkovian Transition (TMT) process, enabling efficient global retrieval while\nmaintaining local effectiveness. Experimental results across diverse tasks\nconfirm the effectiveness of LPMT for instance retrieval.", "AI": {"tldr": "The paper introduces LPMT, a framework combining diffusion-based re-ranking with locality preservation to improve instance retrieval by addressing signal diminishment in affinity graphs.", "motivation": "Positive signals in diffusion-based re-ranking diminish over distance, reducing discriminative power beyond local regions.", "method": "LPMT uses Bidirectional Collaborative Diffusion (BCD) to integrate diffusion processes and Locality State Embedding (LSE) for local consistency, followed by Thermodynamic Markovian Transition (TMT) for global retrieval.", "result": "LPMT effectively improves instance retrieval across diverse tasks.", "conclusion": "LPMT successfully balances local and global retrieval performance by preserving locality and leveraging thermodynamic transitions."}}
{"id": "2506.05200", "pdf": "https://arxiv.org/pdf/2506.05200", "abs": "https://arxiv.org/abs/2506.05200", "authors": ["Gen Li", "Yuchen Jiao", "Yu Huang", "Yuting Wei", "Yuxin Chen"], "title": "Transformers Meet In-Context Learning: A Universal Approximation Theory", "categories": ["cs.LG", "math.ST", "stat.ML", "stat.TH"], "comment": null, "summary": "Modern large language models are capable of in-context learning, the ability\nto perform new tasks at inference time using only a handful of input-output\nexamples in the prompt, without any fine-tuning or parameter updates. We\ndevelop a universal approximation theory to better understand how transformers\nenable in-context learning. For any class of functions (each representing a\ndistinct task), we demonstrate how to construct a transformer that, without any\nfurther weight updates, can perform reliable prediction given only a few\nin-context examples. In contrast to much of the recent literature that frames\ntransformers as algorithm approximators -- i.e., constructing transformers to\nemulate the iterations of optimization algorithms as a means to approximate\nsolutions of learning problems -- our work adopts a fundamentally different\napproach rooted in universal function approximation. This alternative approach\noffers approximation guarantees that are not constrained by the effectiveness\nof the optimization algorithms being approximated, thereby extending far beyond\nconvex problems and linear function classes. Our construction sheds light on\nhow transformers can simultaneously learn general-purpose representations and\nadapt dynamically to in-context examples.", "AI": {"tldr": "The paper develops a universal approximation theory for transformers, showing they can perform in-context learning without fine-tuning, extending beyond traditional optimization-based approaches.", "motivation": "To understand how transformers enable in-context learning and provide theoretical guarantees for their function approximation capabilities.", "method": "Constructs transformers capable of universal function approximation for any task class, using in-context examples without weight updates.", "result": "Demonstrates transformers can reliably predict tasks dynamically, beyond convex and linear problems.", "conclusion": "The work highlights transformers' ability to learn general-purpose representations and adapt to in-context examples, offering broader theoretical guarantees."}}
{"id": "2506.05214", "pdf": "https://arxiv.org/pdf/2506.05214", "abs": "https://arxiv.org/abs/2506.05214", "authors": ["Jingyu Hu", "Hongbo Bo", "Jun Hong", "Xiaowei Liu", "Weiru Liu"], "title": "Mitigating Degree Bias Adaptively with Hard-to-Learn Nodes in Graph Contrastive Learning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Graph Neural Networks (GNNs) often suffer from degree bias in node\nclassification tasks, where prediction performance varies across nodes with\ndifferent degrees. Several approaches, which adopt Graph Contrastive Learning\n(GCL), have been proposed to mitigate this bias. However, the limited number of\npositive pairs and the equal weighting of all positives and negatives in GCL\nstill lead to low-degree nodes acquiring insufficient and noisy information.\nThis paper proposes the Hardness Adaptive Reweighted (HAR) contrastive loss to\nmitigate degree bias. It adds more positive pairs by leveraging node labels and\nadaptively weights positive and negative pairs based on their learning\nhardness. In addition, we develop an experimental framework named SHARP to\nextend HAR to a broader range of scenarios. Both our theoretical analysis and\nexperiments validate the effectiveness of SHARP. The experimental results\nacross four datasets show that SHARP achieves better performance against\nbaselines at both global and degree levels.", "AI": {"tldr": "The paper introduces HAR contrastive loss and SHARP framework to address degree bias in GNNs by adding more positive pairs and adaptive weighting, showing improved performance.", "motivation": "GNNs suffer from degree bias in node classification, where low-degree nodes get insufficient and noisy information due to limited positive pairs in GCL.", "method": "Proposes HAR contrastive loss to add more positive pairs using node labels and adaptively weight pairs based on learning hardness, alongside the SHARP framework for broader application.", "result": "SHARP outperforms baselines on four datasets, improving performance globally and across degree levels.", "conclusion": "HAR and SHARP effectively mitigate degree bias in GNNs, enhancing node classification performance."}}
{"id": "2506.05215", "pdf": "https://arxiv.org/pdf/2506.05215", "abs": "https://arxiv.org/abs/2506.05215", "authors": ["Zhan Yu"], "title": "Learning Theory of Decentralized Robust Kernel-Based Learning Algorithm", "categories": ["cs.LG"], "comment": null, "summary": "We propose a new decentralized robust kernel-based learning algorithm within\nthe framework of reproducing kernel Hilbert space (RKHS) by utilizing a\nnetworked system that can be represented as a connected graph. The robust loss\nfunction $\\mathcal{L}_\\sigma$ induced by a windowing function $W$ and a\nrobustness scaling parameter $\\sigma>0$, can encompass a broad spectrum of\nrobust losses. Consequently, the proposed algorithm effectively provides a\nunified decentralized learning framework for robust regression, which\nfundamentally differs from the existing distributed robust kernel learning\nschemes, all of which are divide-and-conquer based. We rigorously establish the\nlearning theory and offer a comprehensive convergence analysis for the\nalgorithm. We show each local robust estimator generated from the decentralized\nalgorithm can be utilized to approximate the regression function. Based on\nkernel-based integral operator techniques, we derive general high confidence\nconvergence bounds for each local approximating sequence in terms of the mean\nsquare distance, RKHS norm, and generalization error, respectively. Moreover,\nwe provide rigorous selection rules for local sample size and show that, under\nproperly selected step size and scaling parameter $\\sigma$, the decentralized\nrobust algorithm can achieve optimal learning rates (up to logarithmic factors)\nin both norms. The parameter $\\sigma$ is shown to be essential for enhancing\nrobustness while also ensuring favorable convergence behavior. The intrinsic\nconnection among decentralization, sample selection, robustness of the\nalgorithm, and its convergence is clearly reflected.", "AI": {"tldr": "A decentralized robust kernel-based learning algorithm in RKHS is proposed, unifying robust regression with rigorous convergence analysis and optimal learning rates.", "motivation": "To address limitations of existing distributed robust kernel learning schemes by providing a decentralized framework with enhanced robustness and convergence guarantees.", "method": "Utilizes a connected graph network and a robust loss function with a windowing function and scaling parameter. Analyzes convergence and derives bounds for local estimators.", "result": "Local estimators approximate the regression function with high confidence bounds. Optimal learning rates are achieved under proper parameter selection.", "conclusion": "The algorithm effectively unifies decentralized robust learning, with robustness and convergence linked to parameter choices."}}
{"id": "2506.05216", "pdf": "https://arxiv.org/pdf/2506.05216", "abs": "https://arxiv.org/abs/2506.05216", "authors": ["Tyler Chen", "Akshay Seshadri", "Mattia J. Villani", "Pradeep Niroula", "Shouvanik Chakrabarti", "Archan Ray", "Pranav Deshpande", "Romina Yalovetzky", "Marco Pistoia", "Niraj Kumar"], "title": "A Unified Framework for Provably Efficient Algorithms to Estimate Shapley Values", "categories": ["cs.LG", "cs.DS", "quant-ph"], "comment": "44 pages, 7 figures, 7 tables", "summary": "Shapley values have emerged as a critical tool for explaining which features\nimpact the decisions made by machine learning models. However, computing exact\nShapley values is difficult, generally requiring an exponential (in the feature\ndimension) number of model evaluations. To address this, many model-agnostic\nrandomized estimators have been developed, the most influential and widely used\nbeing the KernelSHAP method (Lundberg & Lee, 2017). While related estimators\nsuch as unbiased KernelSHAP (Covert & Lee, 2021) and LeverageSHAP (Musco &\nWitter, 2025) are known to satisfy theoretical guarantees, bounds for\nKernelSHAP have remained elusive. We describe a broad and unified framework\nthat encompasses KernelSHAP and related estimators constructed using both with\nand without replacement sampling strategies. We then prove strong\nnon-asymptotic theoretical guarantees that apply to all estimators from our\nframework. This provides, to the best of our knowledge, the first theoretical\nguarantees for KernelSHAP and sheds further light on tradeoffs between existing\nestimators. Through comprehensive benchmarking on small and medium dimensional\ndatasets for Decision-Tree models, we validate our approach against exact\nShapley values, consistently achieving low mean squared error with modest\nsample sizes. Furthermore, we make specific implementation improvements to\nenable scalability of our methods to high-dimensional datasets. Our methods,\ntested on datasets such MNIST and CIFAR10, provide consistently better results\ncompared to the KernelSHAP library.", "AI": {"tldr": "The paper introduces a unified framework for Shapley value estimators, including KernelSHAP, and provides the first theoretical guarantees for KernelSHAP, validated by experiments on various datasets.", "motivation": "Exact Shapley value computation is computationally expensive, and existing randomized estimators lack theoretical guarantees for KernelSHAP.", "method": "A broad framework encompassing KernelSHAP and related estimators, with non-asymptotic theoretical guarantees and implementation improvements for scalability.", "result": "The framework achieves low mean squared error with modest sample sizes and outperforms KernelSHAP on high-dimensional datasets like MNIST and CIFAR10.", "conclusion": "The work provides theoretical foundations for KernelSHAP and demonstrates practical improvements, offering better performance and scalability."}}
{"id": "2506.05229", "pdf": "https://arxiv.org/pdf/2506.05229", "abs": "https://arxiv.org/abs/2506.05229", "authors": ["Danil Sivtsov", "Ivan Rodkin", "Gleb Kuzmin", "Yuri Kuratov", "Ivan Oseledets"], "title": "Diagonal Batching Unlocks Parallelism in Recurrent Memory Transformers for Long Contexts", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Transformer models struggle with long-context inference due to their\nquadratic time and linear memory complexity. Recurrent Memory Transformers\n(RMTs) offer a solution by reducing the asymptotic cost to linear time and\nconstant memory usage. However, their memory update mechanism leads to\nsequential execution, causing a performance bottleneck.\n  We introduce Diagonal Batching, a scheduling scheme that unlocks parallelism\nacross segments in RMTs while preserving exact recurrence. This approach\neliminates the sequential constraint, enabling efficient GPU inference even for\nsingle long-context inputs without complex batching and pipelining techniques.\nBecause the technique is purely a run-time computation reordering, existing RMT\nmodels adopt it with no retraining.\n  Applied to a LLaMA-1B ARMT model, Diagonal Batching yields a 3.3x speedup\nover standard full-attention LLaMA-1B and a 1.8x speedup over the sequential\nRMT implementation on 131,072-token sequences. By removing sequential\nbottleneck, Diagonal Batching reduces inference cost and latency, thereby\nstrengthening RMTs as a practical solution for real-world, long-context\napplications.", "AI": {"tldr": "Diagonal Batching improves RMTs by enabling parallelism across segments, reducing inference time and cost without retraining.", "motivation": "Transformer models face inefficiency in long-context inference due to quadratic time and linear memory complexity. RMTs address this but suffer from sequential bottlenecks.", "method": "Diagonal Batching is introduced to parallelize segment processing in RMTs while maintaining exact recurrence, eliminating sequential constraints.", "result": "Diagonal Batching achieves a 3.3x speedup over standard LLaMA-1B and 1.8x over sequential RMT on 131,072-token sequences.", "conclusion": "Diagonal Batching enhances RMTs' practicality for real-world long-context applications by reducing latency and inference cost."}}
{"id": "2506.05231", "pdf": "https://arxiv.org/pdf/2506.05231", "abs": "https://arxiv.org/abs/2506.05231", "authors": ["Severi Rissanen", "RuiKang OuYang", "Jiajun He", "Wenlin Chen", "Markus Heinonen", "Arno Solin", "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato"], "title": "Progressive Tempering Sampler with Diffusion", "categories": ["cs.LG", "stat.ML"], "comment": "Accepted for publication at ICML 2025", "summary": "Recent research has focused on designing neural samplers that amortize the\nprocess of sampling from unnormalized densities. However, despite significant\nadvancements, they still fall short of the state-of-the-art MCMC approach,\nParallel Tempering (PT), when it comes to the efficiency of target evaluations.\nOn the other hand, unlike a well-trained neural sampler, PT yields only\ndependent samples and needs to be rerun -- at considerable computational cost\n-- whenever new samples are required. To address these weaknesses, we propose\nthe Progressive Tempering Sampler with Diffusion (PTSD), which trains diffusion\nmodels sequentially across temperatures, leveraging the advantages of PT to\nimprove the training of neural samplers. We also introduce a novel method to\ncombine high-temperature diffusion models to generate approximate\nlower-temperature samples, which are minimally refined using MCMC and used to\ntrain the next diffusion model. PTSD enables efficient reuse of sample\ninformation across temperature levels while generating well-mixed, uncorrelated\nsamples. Our method significantly improves target evaluation efficiency,\noutperforming diffusion-based neural samplers.", "AI": {"tldr": "The paper introduces PTSD, a method combining diffusion models and MCMC to improve neural samplers, outperforming existing approaches in efficiency and sample quality.", "motivation": "Existing neural samplers lag behind MCMC methods like Parallel Tempering in efficiency, while MCMC methods are computationally costly for new samples. PTSD aims to bridge this gap.", "method": "PTSD trains diffusion models sequentially across temperatures, using MCMC to refine samples and train subsequent models, leveraging PT's advantages.", "result": "PTSD generates well-mixed, uncorrelated samples efficiently, surpassing diffusion-based neural samplers in target evaluation efficiency.", "conclusion": "PTSD effectively combines the strengths of MCMC and neural samplers, offering a superior solution for sampling from unnormalized densities."}}
{"id": "2506.05233", "pdf": "https://arxiv.org/pdf/2506.05233", "abs": "https://arxiv.org/abs/2506.05233", "authors": ["Johannes von Oswald", "Nino Scherrer", "Seijin Kobayashi", "Luca Versari", "Songlin Yang", "Maximilian Schlegel", "Kaitlin Maile", "Yanick Schimpf", "Oliver Sieberling", "Alexander Meulemans", "Rif A. Saurous", "Guillaume Lajoie", "Charlotte Frenkel", "Razvan Pascanu", "Blaise Ag\u00fcera y Arcas", "Jo\u00e3o Sacramento"], "title": "MesaNet: Sequence Modeling by Locally Optimal Test-Time Training", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Sequence modeling is currently dominated by causal transformer architectures\nthat use softmax self-attention. Although widely adopted, transformers require\nscaling memory and compute linearly during inference. A recent stream of work\nlinearized the softmax operation, resulting in powerful recurrent neural\nnetwork (RNN) models with constant memory and compute costs such as DeltaNet,\nMamba or xLSTM. These models can be unified by noting that their recurrent\nlayer dynamics can all be derived from an in-context regression objective,\napproximately optimized through an online learning rule. Here, we join this\nline of work and introduce a numerically stable, chunkwise parallelizable\nversion of the recently proposed Mesa layer (von Oswald et al., 2024), and\nstudy it in language modeling at the billion-parameter scale. This layer again\nstems from an in-context loss, but which is now minimized to optimality at\nevery time point using a fast conjugate gradient solver. Through an extensive\nsuite of experiments, we show that optimal test-time training enables reaching\nlower language modeling perplexity and higher downstream benchmark performance\nthan previous RNNs, especially on tasks requiring long context understanding.\nThis performance gain comes at the cost of additional flops spent during\ninference time. Our results are therefore intriguingly related to recent trends\nof increasing test-time compute to improve performance -- here by spending\ncompute to solve sequential optimization problems within the neural network\nitself.", "AI": {"tldr": "The paper introduces a stable, parallelizable version of the Mesa layer, derived from an in-context regression objective, and demonstrates its superior performance in language modeling, especially for long-context tasks, at the cost of additional inference compute.", "motivation": "To address the linear scaling of memory and compute in transformers during inference, the paper explores recurrent neural network (RNN) models like DeltaNet, Mamba, and xLSTM, unified by an in-context regression objective.", "method": "The authors propose a numerically stable, chunkwise parallelizable Mesa layer, optimized at every time point using a fast conjugate gradient solver, and evaluate it in billion-parameter language models.", "result": "The Mesa layer achieves lower perplexity and higher downstream performance than previous RNNs, particularly in long-context tasks, though it requires additional inference-time compute.", "conclusion": "The work highlights the trade-off between performance and compute, aligning with trends of increasing test-time compute for better results, by solving sequential optimization within the network."}}
{"id": "2506.05239", "pdf": "https://arxiv.org/pdf/2506.05239", "abs": "https://arxiv.org/abs/2506.05239", "authors": ["Val\u00e9rie Costa", "Thomas Fel", "Ekdeep Singh Lubana", "Bahareh Tolooshams", "Demba Ba"], "title": "Evaluating Sparse Autoencoders: From Shallow Design to Matching Pursuit", "categories": ["cs.LG"], "comment": "Complementary work to arXiv:2506.03093", "summary": "Sparse autoencoders (SAEs) have recently become central tools for\ninterpretability, leveraging dictionary learning principles to extract sparse,\ninterpretable features from neural representations whose underlying structure\nis typically unknown. This paper evaluates SAEs in a controlled setting using\nMNIST, which reveals that current shallow architectures implicitly rely on a\nquasi-orthogonality assumption that limits the ability to extract correlated\nfeatures. To move beyond this, we introduce a multi-iteration SAE by unrolling\nMatching Pursuit (MP-SAE), enabling the residual-guided extraction of\ncorrelated features that arise in hierarchical settings such as handwritten\ndigit generation while guaranteeing monotonic improvement of the reconstruction\nas more atoms are selected.", "AI": {"tldr": "The paper evaluates sparse autoencoders (SAEs) for interpretability, identifies limitations in shallow architectures, and proposes a multi-iteration SAE (MP-SAE) to handle correlated features.", "motivation": "To address the limitations of current SAEs in extracting correlated features due to quasi-orthogonality assumptions.", "method": "Introduces a multi-iteration SAE (MP-SAE) by unrolling Matching Pursuit, enabling residual-guided extraction of correlated features.", "result": "MP-SAE improves feature extraction for hierarchical settings like handwritten digit generation and guarantees monotonic reconstruction improvement.", "conclusion": "The proposed MP-SAE advances SAEs by effectively handling correlated features and improving interpretability."}}
{"id": "2506.05240", "pdf": "https://arxiv.org/pdf/2506.05240", "abs": "https://arxiv.org/abs/2506.05240", "authors": ["Yizhuo Li", "Yuying Ge", "Yixiao Ge", "Ying Shan", "Ping Luo"], "title": "Aligning Latent Spaces with Flow Priors", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "This paper presents a novel framework for aligning learnable latent spaces to\narbitrary target distributions by leveraging flow-based generative models as\npriors. Our method first pretrains a flow model on the target features to\ncapture the underlying distribution. This fixed flow model subsequently\nregularizes the latent space via an alignment loss, which reformulates the flow\nmatching objective to treat the latents as optimization targets. We formally\nprove that minimizing this alignment loss establishes a computationally\ntractable surrogate objective for maximizing a variational lower bound on the\nlog-likelihood of latents under the target distribution. Notably, the proposed\nmethod eliminates computationally expensive likelihood evaluations and avoids\nODE solving during optimization. As a proof of concept, we demonstrate in a\ncontrolled setting that the alignment loss landscape closely approximates the\nnegative log-likelihood of the target distribution. We further validate the\neffectiveness of our approach through large-scale image generation experiments\non ImageNet with diverse target distributions, accompanied by detailed\ndiscussions and ablation studies. With both theoretical and empirical\nvalidation, our framework paves a new way for latent space alignment.", "AI": {"tldr": "A novel framework aligns learnable latent spaces to target distributions using flow-based generative models, avoiding expensive likelihood evaluations and ODE solving.", "motivation": "To align latent spaces with arbitrary target distributions efficiently, leveraging flow models as priors.", "method": "Pretrains a flow model on target features, then regularizes the latent space via an alignment loss derived from flow matching.", "result": "The alignment loss approximates the negative log-likelihood of the target distribution, validated in controlled and large-scale experiments.", "conclusion": "The framework provides a computationally efficient and theoretically grounded method for latent space alignment."}}
{"id": "2506.05249", "pdf": "https://arxiv.org/pdf/2506.05249", "abs": "https://arxiv.org/abs/2506.05249", "authors": ["Zhen Qin", "Jinxin Zhou", "Zhihui Zhu"], "title": "On the Convergence of Gradient Descent on Learning Transformers with Residual Connections", "categories": ["cs.LG", "math.OC"], "comment": null, "summary": "Transformer models have emerged as fundamental tools across various\nscientific and engineering disciplines, owing to their outstanding performance\nin diverse applications. Despite this empirical success, the theoretical\nfoundations of Transformers remain relatively underdeveloped, particularly in\nunderstanding their training dynamics. Existing research predominantly examines\nisolated components--such as self-attention mechanisms and feedforward\nnetworks--without thoroughly investigating the interdependencies between these\ncomponents, especially when residual connections are present. In this paper, we\naim to bridge this gap by analyzing the convergence behavior of a structurally\ncomplete yet single-layer Transformer, comprising self-attention, a feedforward\nnetwork, and residual connections. We demonstrate that, under appropriate\ninitialization, gradient descent exhibits a linear convergence rate, where the\nconvergence speed is determined by the minimum and maximum singular values of\nthe output matrix from the attention layer. Moreover, our analysis reveals that\nresidual connections serve to ameliorate the ill-conditioning of this output\nmatrix, an issue stemming from the low-rank structure imposed by the softmax\noperation, thereby promoting enhanced optimization stability. We also extend\nour theoretical findings to a multi-layer Transformer architecture, confirming\nthe linear convergence rate of gradient descent under suitable initialization.\nEmpirical results corroborate our theoretical insights, illustrating the\nbeneficial role of residual connections in promoting convergence stability.", "AI": {"tldr": "The paper analyzes the convergence behavior of a single-layer Transformer with self-attention, feedforward networks, and residual connections, showing linear convergence under proper initialization and highlighting the role of residual connections in optimization stability.", "motivation": "Despite the empirical success of Transformers, their theoretical foundations, especially training dynamics, are underdeveloped. This paper aims to understand the interdependencies between components like self-attention and residual connections.", "method": "The study examines a structurally complete single-layer Transformer, analyzing gradient descent convergence under appropriate initialization. It also extends findings to multi-layer architectures.", "result": "Gradient descent exhibits linear convergence, influenced by the attention layer's output matrix singular values. Residual connections mitigate ill-conditioning from softmax, enhancing stability.", "conclusion": "Residual connections improve optimization stability in Transformers, with empirical results supporting the theoretical insights on convergence behavior."}}
{"id": "2506.05252", "pdf": "https://arxiv.org/pdf/2506.05252", "abs": "https://arxiv.org/abs/2506.05252", "authors": ["Dravyansh Sharma", "Alec Sun"], "title": "Conservative classifiers do consistently well with improving agents: characterizing statistical and online learning", "categories": ["cs.LG", "cs.GT", "cs.MA"], "comment": "24 pages", "summary": "Machine learning is now ubiquitous in societal decision-making, for example\nin evaluating job candidates or loan applications, and it is increasingly\nimportant to take into account how classified agents will react to the learning\nalgorithms. The majority of recent literature on strategic classification has\nfocused on reducing and countering deceptive behaviors by the classified\nagents, but recent work of Attias et al. identifies surprising properties of\nlearnability when the agents genuinely improve in order to attain the desirable\nclassification, such as smaller generalization error than standard\nPAC-learning. In this paper we characterize so-called learnability with\nimprovements across multiple new axes. We introduce an asymmetric variant of\nminimally consistent concept classes and use it to provide an exact\ncharacterization of proper learning with improvements in the realizable\nsetting. While prior work studies learnability only under general, arbitrary\nagent improvement regions, we give positive results for more natural Euclidean\nball improvement sets. In particular, we characterize improper learning under a\nmild generative assumption on the data distribution. We further show how to\nlearn in more challenging settings, achieving lower generalization error under\nwell-studied bounded noise models and obtaining mistake bounds in realizable\nand agnostic online learning. We resolve open questions posed by Attias et al.\nfor both proper and improper learning.", "AI": {"tldr": "The paper explores learnability in strategic classification, focusing on genuine agent improvements, and extends prior work by characterizing learnability across new axes, including proper and improper learning under natural assumptions.", "motivation": "To understand how machine learning algorithms perform when classified agents genuinely improve, rather than deceive, and to extend learnability results beyond arbitrary improvement regions.", "method": "Introduces asymmetric minimally consistent concept classes for exact characterization in the realizable setting, studies Euclidean ball improvement sets, and addresses learning under bounded noise and online settings.", "result": "Provides exact characterizations for proper learning, positive results for natural improvement regions, and resolves open questions for both proper and improper learning.", "conclusion": "The work advances understanding of strategic classification with genuine improvements, offering theoretical insights and practical learning guarantees."}}
{"id": "2506.05259", "pdf": "https://arxiv.org/pdf/2506.05259", "abs": "https://arxiv.org/abs/2506.05259", "authors": ["Guillaume Pourcel", "Maxence Ernoult"], "title": "Learning long range dependencies through time reversal symmetry breaking", "categories": ["cs.LG"], "comment": "45 pages", "summary": "Deep State Space Models (SSMs) reignite physics-grounded compute paradigms,\nas RNNs could natively be embodied into dynamical systems. This calls for\ndedicated learning algorithms obeying to core physical principles, with\nefficient techniques to simulate these systems and guide their design. We\npropose Recurrent Hamiltonian Echo Learning (RHEL), an algorithm which provably\ncomputes loss gradients as finite differences of physical trajectories of\nnon-dissipative, Hamiltonian systems. In ML terms, RHEL only requires three\n\"forward passes\" irrespective of model size, without explicit Jacobian\ncomputation, nor incurring any variance in the gradient estimation. Motivated\nby the physical realization of our algorithm, we first introduce RHEL in\ncontinuous time and demonstrate its formal equivalence with the continuous\nadjoint state method. To facilitate the simulation of Hamiltonian systems\ntrained by RHEL, we propose a discrete-time version of RHEL which is equivalent\nto Backpropagation Through Time (BPTT) when applied to a class of recurrent\nmodules which we call Hamiltonian Recurrent Units (HRUs). This setting allows\nus to demonstrate the scalability of RHEL by generalizing these results to\nhierarchies of HRUs, which we call Hamiltonian SSMs (HSSMs). We apply RHEL to\ntrain HSSMs with linear and nonlinear dynamics on a variety of time-series\ntasks ranging from mid-range to long-range classification and regression with\nsequence length reaching $\\sim 50k$. We show that RHEL consistently matches the\nperformance of BPTT across all models and tasks. This work opens new doors for\nthe design of scalable, energy-efficient physical systems endowed with\nself-learning capabilities for sequence modelling.", "AI": {"tldr": "RHEL is a learning algorithm for Hamiltonian systems, matching BPTT performance with efficiency and scalability.", "motivation": "To develop physics-grounded learning algorithms for dynamical systems, ensuring efficiency and adherence to physical principles.", "method": "Proposes Recurrent Hamiltonian Echo Learning (RHEL), computing gradients via physical trajectories without explicit Jacobians, and introduces Hamiltonian Recurrent Units (HRUs).", "result": "RHEL matches BPTT performance on time-series tasks with sequences up to ~50k, demonstrating scalability.", "conclusion": "RHEL enables scalable, energy-efficient physical systems for sequence modeling, bridging physics and machine learning."}}
{"id": "2506.05271", "pdf": "https://arxiv.org/pdf/2506.05271", "abs": "https://arxiv.org/abs/2506.05271", "authors": ["Daniel Berg Thomsen", "Adrien Taylor", "Aymeric Dieuleveut"], "title": "Tight analyses of first-order methods with error feedback", "categories": ["cs.LG", "cs.DC", "math.OC"], "comment": null, "summary": "Communication between agents often constitutes a major computational\nbottleneck in distributed learning. One of the most common mitigation\nstrategies is to compress the information exchanged, thereby reducing\ncommunication overhead. To counteract the degradation in convergence associated\nwith compressed communication, error feedback schemes -- most notably\n$\\mathrm{EF}$ and $\\mathrm{EF}^{21}$ -- were introduced. In this work, we\nprovide a tight analysis of both of these methods. Specifically, we find the\nLyapunov function that yields the best possible convergence rate for each\nmethod -- with matching lower bounds. This principled approach yields sharp\nperformance guarantees and enables a rigorous, apples-to-apples comparison\nbetween $\\mathrm{EF}$, $\\mathrm{EF}^{21}$, and compressed gradient descent. Our\nanalysis is carried out in a simplified yet representative setting, which\nallows for clean theoretical insights and fair comparison of the underlying\nmechanisms.", "AI": {"tldr": "The paper analyzes error feedback schemes (EF and EF\u00b2\u00b9) in distributed learning, providing tight convergence rates and comparisons with compressed gradient descent.", "motivation": "Communication compression in distributed learning can degrade convergence; error feedback schemes like EF and EF\u00b2\u00b9 aim to mitigate this.", "method": "The study uses Lyapunov functions to derive tight convergence rates and lower bounds for EF and EF\u00b2\u00b9, comparing them with compressed gradient descent in a simplified setting.", "result": "The analysis provides sharp performance guarantees and enables a rigorous comparison between EF, EF\u00b2\u00b9, and compressed gradient descent.", "conclusion": "The principled approach yields clear insights into the effectiveness of error feedback schemes and their trade-offs in distributed learning."}}
{"id": "2506.05276", "pdf": "https://arxiv.org/pdf/2506.05276", "abs": "https://arxiv.org/abs/2506.05276", "authors": ["Hao Yu", "Chu Xin Cheng", "Runlong Yu", "Yuyang Ye", "Shiwei Tong", "Zhaofeng Liu", "Defu Lian"], "title": "How to Unlock Time Series Editing? Diffusion-Driven Approach with Multi-Grained Control", "categories": ["cs.LG"], "comment": null, "summary": "Recent advances in time series generation have shown promise, yet controlling\nproperties in generated sequences remains challenging. Time Series Editing\n(TSE) - making precise modifications while preserving temporal coherence -\nconsider both point-level constraints and segment-level controls that current\nmethods struggle to provide. We introduce the CocktailEdit framework to enable\nsimultaneous, flexible control across different types of constraints. This\nframework combines two key mechanisms: a confidence-weighted anchor control for\npoint-wise constraints and a classifier-based control for managing statistical\nproperties such as sums and averages over segments. Our methods achieve precise\nlocal control during the denoising inference stage while maintaining temporal\ncoherence and integrating seamlessly, with any conditionally trained\ndiffusion-based time series models. Extensive experiments across diverse\ndatasets and models demonstrate its effectiveness. Our work bridges the gap\nbetween pure generative modeling and real-world time series editing needs,\noffering a flexible solution for human-in-the-loop time series generation and\nediting. The code and demo are provided for validation.", "AI": {"tldr": "CocktailEdit framework enables flexible control in time series generation by combining point-wise and segment-level constraints, maintaining temporal coherence.", "motivation": "Current methods struggle with precise modifications in time series generation while preserving temporal coherence.", "method": "Introduces CocktailEdit with confidence-weighted anchor control for point-wise constraints and classifier-based control for segment-level properties.", "result": "Achieves precise local control during denoising, maintaining coherence, and integrates with diffusion-based models.", "conclusion": "Bridges generative modeling and real-world editing needs, offering a flexible solution for human-in-the-loop tasks."}}
{"id": "2506.05281", "pdf": "https://arxiv.org/pdf/2506.05281", "abs": "https://arxiv.org/abs/2506.05281", "authors": ["Haifeng Sun", "Yu Xiong", "Runze Wu", "Xinyu Cai", "Changjie Fan", "Lan Zhang", "Xiang-Yang Li"], "title": "Fast-DataShapley: Neural Modeling for Training Data Valuation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The value and copyright of training data are crucial in the artificial\nintelligence industry. Service platforms should protect data providers'\nlegitimate rights and fairly reward them for their contributions. Shapley\nvalue, a potent tool for evaluating contributions, outperforms other methods in\ntheory, but its computational overhead escalates exponentially with the number\nof data providers. Recent works based on Shapley values attempt to mitigate\ncomputation complexity by approximation algorithms. However, they need to\nretrain for each test sample, leading to intolerable costs. We propose\nFast-DataShapley, a one-pass training method that leverages the weighted least\nsquares characterization of the Shapley value to train a reusable explainer\nmodel with real-time reasoning speed. Given new test samples, no retraining is\nrequired to calculate the Shapley values of the training data. Additionally, we\npropose three methods with theoretical guarantees to reduce training overhead\nfrom two aspects: the approximate calculation of the utility function and the\ngroup calculation of the training data. We analyze time complexity to show the\nefficiency of our methods. The experimental evaluations on various image\ndatasets demonstrate superior performance and efficiency compared to baselines.\nSpecifically, the performance is improved to more than 2.5 times, and the\nexplainer's training speed can be increased by two orders of magnitude.", "AI": {"tldr": "Fast-DataShapley introduces a one-pass training method to efficiently compute Shapley values for training data, avoiding retraining for new test samples and significantly improving performance and speed.", "motivation": "The need to protect data providers' rights and fairly reward their contributions in AI, coupled with the high computational cost of traditional Shapley value methods, drives this work.", "method": "Leverages weighted least squares to train a reusable explainer model, proposes three methods to reduce training overhead, and analyzes time complexity.", "result": "Achieves over 2.5x performance improvement and up to 100x faster training speed compared to baselines on various image datasets.", "conclusion": "Fast-DataShapley offers an efficient, scalable solution for Shapley value computation, balancing accuracy and computational feasibility."}}
{"id": "2506.05294", "pdf": "https://arxiv.org/pdf/2506.05294", "abs": "https://arxiv.org/abs/2506.05294", "authors": ["Arnav Kumar Jain", "Vibhakar Mohta", "Subin Kim", "Atiksh Bhardwaj", "Juntao Ren", "Yunhai Feng", "Sanjiban Choudhury", "Gokul Swamy"], "title": "A Smooth Sea Never Made a Skilled $\\texttt{SAILOR}$: Robust Imitation via Learning to Search", "categories": ["cs.LG"], "comment": null, "summary": "The fundamental limitation of the behavioral cloning (BC) approach to\nimitation learning is that it only teaches an agent what the expert did at\nstates the expert visited. This means that when a BC agent makes a mistake\nwhich takes them out of the support of the demonstrations, they often don't\nknow how to recover from it. In this sense, BC is akin to giving the agent the\nfish -- giving them dense supervision across a narrow set of states -- rather\nthan teaching them to fish: to be able to reason independently about achieving\nthe expert's outcome even when faced with unseen situations at test-time. In\nresponse, we explore learning to search (L2S) from expert demonstrations, i.e.\nlearning the components required to, at test time, plan to match expert\noutcomes, even after making a mistake. These include (1) a world model and (2)\na reward model. We carefully ablate the set of algorithmic and design decisions\nrequired to combine these and other components for stable and\nsample/interaction-efficient learning of recovery behavior without additional\nhuman corrections. Across a dozen visual manipulation tasks from three\nbenchmarks, our approach $\\texttt{SAILOR}$ consistently out-performs\nstate-of-the-art Diffusion Policies trained via BC on the same data.\nFurthermore, scaling up the amount of demonstrations used for BC by\n5-10$\\times$ still leaves a performance gap. We find that $\\texttt{SAILOR}$ can\nidentify nuanced failures and is robust to reward hacking. Our code is\navailable at https://github.com/arnavkj1995/SAILOR .", "AI": {"tldr": "SAILOR improves imitation learning by teaching recovery behavior through planning, outperforming BC methods.", "motivation": "Behavioral cloning (BC) fails to teach recovery from mistakes outside expert demonstrations.", "method": "Learning to search (L2S) combines a world model and reward model for planning.", "result": "SAILOR outperforms BC methods, even with 5-10x more BC demonstrations, and is robust to reward hacking.", "conclusion": "SAILOR effectively addresses BC's limitations by enabling independent reasoning and recovery in unseen states."}}
{"id": "2506.05295", "pdf": "https://arxiv.org/pdf/2506.05295", "abs": "https://arxiv.org/abs/2506.05295", "authors": ["Baihe Huang", "Shanda Li", "Tianhao Wu", "Yiming Yang", "Ameet Talwalkar", "Kannan Ramchandran", "Michael I. Jordan", "Jiantao Jiao"], "title": "Sample Complexity and Representation Ability of Test-time Scaling Paradigms", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Test-time scaling paradigms have significantly advanced the capabilities of\nlarge language models (LLMs) on complex tasks. Despite their empirical success,\ntheoretical understanding of the sample efficiency of various test-time\nstrategies -- such as self-consistency, best-of-$n$, and self-correction --\nremains limited. In this work, we first establish a separation result between\ntwo repeated sampling strategies: self-consistency requires\n$\\Theta(1/\\Delta^2)$ samples to produce the correct answer, while best-of-$n$\nonly needs $\\Theta(1/\\Delta)$, where $\\Delta < 1$ denotes the probability gap\nbetween the correct and second most likely answers. Next, we present an\nexpressiveness result for the self-correction approach with verifier feedback:\nit enables Transformers to simulate online learning over a pool of experts at\ntest time. Therefore, a single Transformer architecture can provably solve\nmultiple tasks without prior knowledge of the specific task associated with a\nuser query, extending the representation theory of Transformers from\nsingle-task to multi-task settings. Finally, we empirically validate our\ntheoretical results, demonstrating the practical effectiveness of\nself-correction methods.", "AI": {"tldr": "The paper analyzes test-time scaling paradigms for LLMs, comparing sample efficiency of strategies like self-consistency and best-of-$n$, and introduces a theoretical framework for self-correction with verifier feedback.", "motivation": "To bridge the gap between empirical success and theoretical understanding of test-time strategies in LLMs, focusing on sample efficiency and expressiveness.", "method": "Theoretical analysis of sample efficiency for self-consistency and best-of-$n$, and an expressiveness result for self-correction with verifier feedback. Empirical validation is also conducted.", "result": "Self-consistency requires more samples ($\\Theta(1/\\Delta^2)$) than best-of-$n$ ($\\Theta(1/\\Delta)$). Self-correction enables Transformers to simulate online learning for multi-task settings.", "conclusion": "The work provides theoretical insights into test-time strategies, demonstrating the practical effectiveness of self-correction and extending Transformer capabilities to multi-task scenarios."}}
{"id": "2506.05300", "pdf": "https://arxiv.org/pdf/2506.05300", "abs": "https://arxiv.org/abs/2506.05300", "authors": ["Nirav Koley", "Prajwal Singhania", "Abhinav Bhatele"], "title": "Power Law Guided Dynamic Sifting for Efficient Attention", "categories": ["cs.LG"], "comment": null, "summary": "Efficient inference on GPUs using large language models remains challenging\ndue to memory bandwidth limitations, particularly during data transfers between\nHigh Bandwidth Memory (HBM) and SRAM in attention computations. Approximate\nattention methods address this issue by reducing computational and memory\noverhead but often rely on expensive top-$k$ operations, which perform poorly\non GPUs. We propose SiftAttention, a novel approximate attention method that\nreplaces the top-$k$ step with a computationally efficient element-wise\nfiltering operation based on a threshold value. Our intuition for doing this is\nbased on our empirical observation that the $\\tau$-th quantile of attention\nscores follows a predictable power-law over sequential generation steps.\nExploiting this insight, our approach dynamically estimates a threshold value\nper prompt at each generation step. Only attention scores above this threshold\nand their corresponding value vectors are loaded/used to compute the attention\noutput, reducing data movement between HBM and SRAM. Our evaluation\ndemonstrates that SiftAttention preserves model quality better than existing\napproximate attention methods while reducing memory bandwidth usage when\nloading value vectors.", "AI": {"tldr": "SiftAttention replaces top-$k$ operations in approximate attention methods with an efficient element-wise filtering method, reducing GPU memory bandwidth usage while maintaining model quality.", "motivation": "Efficient inference on GPUs using large language models is hindered by memory bandwidth limitations, especially during data transfers in attention computations.", "method": "SiftAttention dynamically estimates a threshold per prompt at each generation step, filtering attention scores and value vectors to reduce data movement.", "result": "SiftAttention outperforms existing methods in preserving model quality and reducing memory bandwidth usage.", "conclusion": "SiftAttention offers a computationally efficient solution for approximate attention, improving GPU inference performance."}}
{"id": "2506.05310", "pdf": "https://arxiv.org/pdf/2506.05310", "abs": "https://arxiv.org/abs/2506.05310", "authors": ["Florentin Guth", "Zahra Kadkhodaie", "Eero P Simoncelli"], "title": "Learning normalized image densities via dual score matching", "categories": ["cs.LG"], "comment": null, "summary": "Learning probability models from data is at the heart of many machine\nlearning endeavors, but is notoriously difficult due to the curse of\ndimensionality. We introduce a new framework for learning \\emph{normalized}\nenergy (log probability) models that is inspired from diffusion generative\nmodels, which rely on networks optimized to estimate the score. We modify a\nscore network architecture to compute an energy while preserving its inductive\nbiases. The gradient of this energy network with respect to its input image is\nthe score of the learned density, which can be optimized using a denoising\nobjective. Importantly, the gradient with respect to the noise level provides\nan additional score that can be optimized with a novel secondary objective,\nensuring consistent and normalized energies across noise levels. We train an\nenergy network with this \\emph{dual} score matching objective on the ImageNet64\ndataset, and obtain a cross-entropy (negative log likelihood) value comparable\nto the state of the art. We further validate our approach by showing that our\nenergy model \\emph{strongly generalizes}: estimated log probabilities are\nnearly independent of the specific images in the training set. Finally, we\ndemonstrate that both image probability and dimensionality of local\nneighborhoods vary significantly with image content, in contrast with\ntraditional assumptions such as concentration of measure or support on a\nlow-dimensional manifold.", "AI": {"tldr": "A new framework for learning normalized energy models inspired by diffusion generative models, achieving state-of-the-art cross-entropy on ImageNet64 and demonstrating strong generalization.", "motivation": "Learning probability models from data is challenging due to dimensionality issues. The paper aims to address this by introducing a normalized energy model framework.", "method": "Modifies a score network architecture to compute energy, optimizing it with a denoising objective and a novel secondary objective for consistent energies across noise levels.", "result": "Achieves competitive cross-entropy on ImageNet64 and shows strong generalization, with log probabilities nearly independent of training images.", "conclusion": "The energy model framework effectively learns probability distributions, challenging traditional assumptions about image probability and dimensionality."}}
{"id": "2506.05316", "pdf": "https://arxiv.org/pdf/2506.05316", "abs": "https://arxiv.org/abs/2506.05316", "authors": ["Yifan Sun", "Jingyan Shen", "Yibin Wang", "Tianyu Chen", "Zhendong Wang", "Mingyuan Zhou", "Huan Zhang"], "title": "Improving Data Efficiency for LLM Reinforcement Fine-tuning Through Difficulty-targeted Online Data Selection and Rollout Replay", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Reinforcement learning (RL) has become an effective approach for fine-tuning\nlarge language models (LLMs), particularly to enhance their reasoning\ncapabilities. However, RL fine-tuning remains highly resource-intensive, and\nexisting work has largely overlooked the problem of data efficiency. In this\npaper, we propose two techniques to improve data efficiency in LLM RL\nfine-tuning: difficulty-targeted online data selection and rollout replay. We\nintroduce the notion of adaptive difficulty to guide online data selection,\nprioritizing questions of moderate difficulty that are more likely to yield\ninformative learning signals. To estimate adaptive difficulty efficiently, we\ndevelop an attention-based framework that requires rollouts for only a small\nreference set of questions. The adaptive difficulty of the remaining questions\nis then estimated based on their similarity to this set. To further reduce\nrollout cost, we introduce a rollout replay mechanism that reuses recent\nrollouts, lowering per-step computation while maintaining stable updates.\nExtensive experiments across 6 LLM-dataset combinations show that our method\nreduces RL fine-tuning time by 25% to 65% to reach the same level of\nperformance as the original GRPO algorithm.", "AI": {"tldr": "The paper proposes two techniques\u2014difficulty-targeted online data selection and rollout replay\u2014to improve data efficiency in RL fine-tuning of LLMs, reducing training time by 25-65%.", "motivation": "RL fine-tuning for LLMs is resource-intensive, and existing methods lack focus on data efficiency.", "method": "Introduces adaptive difficulty for online data selection and an attention-based framework for efficient difficulty estimation. Also proposes rollout replay to reuse recent rollouts.", "result": "Experiments show a 25-65% reduction in fine-tuning time while matching the performance of the original GRPO algorithm.", "conclusion": "The proposed techniques significantly enhance data efficiency in RL fine-tuning for LLMs, offering practical benefits for resource optimization."}}
{"id": "2506.05321", "pdf": "https://arxiv.org/pdf/2506.05321", "abs": "https://arxiv.org/abs/2506.05321", "authors": ["Maxwell A. Xu", "Girish Narayanswamy", "Kumar Ayush", "Dimitris Spathis", "Shun Liao", "Shyam A. Tailor", "Ahmed Metwally", "A. Ali Heydari", "Yuwei Zhang", "Jake Garrison", "Samy Abdel-Ghaffar", "Xuhai Xu", "Ken Gu", "Jacob Sunshine", "Ming-Zher Poh", "Yun Liu", "Tim Althoff", "Shrikanth Narayanan", "Pushmeet Kohli", "Mark Malhotra", "Shwetak Patel", "Yuzhe Yang", "James M. Rehg", "Xin Liu", "Daniel McDuff"], "title": "LSM-2: Learning from Incomplete Wearable Sensor Data", "categories": ["cs.LG"], "comment": "Xu and Narayanswamy are co-first authors. McDuff and Liu are co-last\n  authors", "summary": "Foundation models, a cornerstone of recent advancements in machine learning,\nhave predominantly thrived on complete and well-structured data. Wearable\nsensor data frequently suffers from significant missingness, posing a\nsubstantial challenge for self-supervised learning (SSL) models that typically\nassume complete data inputs. This paper introduces the second generation of\nLarge Sensor Model (LSM-2) with Adaptive and Inherited Masking (AIM), a novel\nSSL approach that learns robust representations directly from incomplete data\nwithout requiring explicit imputation. AIM's core novelty lies in its use of\nlearnable mask tokens to model both existing (\"inherited\") and artificially\nintroduced missingness, enabling it to robustly handle fragmented real-world\ndata during inference. Pre-trained on an extensive dataset of 40M hours of\nday-long multimodal sensor data, our LSM-2 with AIM achieves the best\nperformance across a diverse range of tasks, including classification,\nregression and generative modeling. Furthermore, LSM-2 with AIM exhibits\nsuperior scaling performance, and critically, maintains high performance even\nunder targeted missingness scenarios, reflecting clinically coherent patterns,\nsuch as the diagnostic value of nighttime biosignals for hypertension\nprediction. This makes AIM a more reliable choice for real-world wearable data\napplications.", "AI": {"tldr": "The paper introduces LSM-2 with AIM, a self-supervised learning model for handling incomplete wearable sensor data without imputation, achieving top performance across tasks.", "motivation": "Address the challenge of missing data in wearable sensor datasets for self-supervised learning, which traditionally assumes complete inputs.", "method": "Proposes Adaptive and Inherited Masking (AIM), using learnable mask tokens to model missingness, pre-trained on 40M hours of multimodal sensor data.", "result": "LSM-2 with AIM outperforms in classification, regression, and generative tasks, scaling well and maintaining performance under targeted missingness.", "conclusion": "AIM is a robust solution for real-world wearable data, especially in scenarios like clinical diagnostics."}}
{"id": "2506.05325", "pdf": "https://arxiv.org/pdf/2506.05325", "abs": "https://arxiv.org/abs/2506.05325", "authors": ["Yingshuai Ji", "Haomin Zhuang", "Matthew Toole", "James McKenzie", "Xiaolong Liu", "Xiangliang Zhang"], "title": "Seeing the Invisible: Machine learning-Based QPI Kernel Extraction via Latent Alignment", "categories": ["cs.LG"], "comment": null, "summary": "Quasiparticle interference (QPI) imaging is a powerful tool for probing\nelectronic structures in quantum materials, but extracting the single-scatterer\nQPI pattern (i.e., the kernel) from a multi-scatterer image remains a\nfundamentally ill-posed inverse problem. In this work, we propose the first\nAI-based framework for QPI kernel extraction. We introduce a two-step learning\nstrategy that decouples kernel representation learning from\nobservation-to-kernel inference. In the first step, we train a variational\nautoencoder to learn a compact latent space of scattering kernels. In the\nsecond step, we align the latent representation of QPI observations with those\nof the pre-learned kernels using a dedicated encoder. This design enables the\nmodel to infer kernels robustly even under complex, entangled scattering\nconditions. We construct a diverse and physically realistic QPI dataset\ncomprising 100 unique kernels and evaluate our method against a direct one-step\nbaseline. Experimental results demonstrate that our approach achieves\nsignificantly higher extraction accuracy, and improved generalization to unseen\nkernels.", "AI": {"tldr": "An AI-based framework for extracting single-scatterer QPI patterns from multi-scatterer images, using a two-step learning strategy for robust inference.", "motivation": "The challenge of extracting single-scatterer QPI patterns from complex multi-scatterer images, a fundamentally ill-posed inverse problem, motivates the need for an AI-driven solution.", "method": "A two-step learning strategy: (1) train a variational autoencoder to learn a compact latent space of scattering kernels, (2) align QPI observations with pre-learned kernels using a dedicated encoder.", "result": "The method achieves higher extraction accuracy and better generalization to unseen kernels compared to a direct one-step baseline.", "conclusion": "The proposed AI framework effectively addresses the QPI kernel extraction problem, offering improved robustness and accuracy."}}
{"id": "2506.05333", "pdf": "https://arxiv.org/pdf/2506.05333", "abs": "https://arxiv.org/abs/2506.05333", "authors": ["Ranajoy Sadhukhan", "Zhuoming Chen", "Haizhong Zheng", "Yang Zhou", "Emma Strubell", "Beidi Chen"], "title": "Kinetics: Rethinking Test-Time Scaling Laws", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "We rethink test-time scaling laws from a practical efficiency perspective,\nrevealing that the effectiveness of smaller models is significantly\noverestimated. Prior work, grounded in compute-optimality, overlooks critical\nmemory access bottlenecks introduced by inference-time strategies (e.g.,\nBest-of-$N$, long CoTs). Our holistic analysis, spanning models from 0.6B to\n32B parameters, reveals a new Kinetics Scaling Law that better guides resource\nallocation by incorporating both computation and memory access costs. Kinetics\nScaling Law suggests that test-time compute is more effective when used on\nmodels above a threshold than smaller ones. A key reason is that in TTS,\nattention, rather than parameter count, emerges as the dominant cost factor.\nMotivated by this, we propose a new scaling paradigm centered on sparse\nattention, which lowers per-token cost and enables longer generations and more\nparallel samples within the same resource budget. Empirically, we show that\nsparse attention models consistently outperform dense counterparts, achieving\nover 60 points gains in low-cost regimes and over 5 points gains in high-cost\nregimes for problem-solving accuracy on AIME, encompassing evaluations on\nstate-of-the-art MoEs. These results suggest that sparse attention is essential\nfor realizing the full potential of test-time scaling because, unlike training,\nwhere parameter scaling saturates, test-time accuracy continues to improve\nthrough increased generation. The code is available at\nhttps://github.com/Infini-AI-Lab/Kinetics.", "AI": {"tldr": "The paper introduces the Kinetics Scaling Law, showing smaller models' effectiveness is overestimated due to overlooked memory bottlenecks. It proposes sparse attention to optimize test-time compute, achieving significant accuracy gains.", "motivation": "Prior work underestimates memory access bottlenecks in test-time strategies, leading to inefficient resource allocation. The study aims to address this by rethinking scaling laws.", "method": "The authors analyze models (0.6B to 32B parameters) to derive the Kinetics Scaling Law, incorporating computation and memory costs. They propose sparse attention to reduce per-token costs.", "result": "Sparse attention models outperform dense ones, with 60+ point gains in low-cost and 5+ point gains in high-cost regimes on AIME evaluations.", "conclusion": "Sparse attention is crucial for test-time scaling, as it enables better accuracy through efficient resource use, unlike parameter scaling in training."}}
{"id": "2506.05340", "pdf": "https://arxiv.org/pdf/2506.05340", "abs": "https://arxiv.org/abs/2506.05340", "authors": ["Keshigeyan Chandrasegaran", "Michael Poli", "Daniel Y. Fu", "Dongjun Kim", "Lea M. Hadzic", "Manling Li", "Agrim Gupta", "Stefano Massaroli", "Azalia Mirhoseini", "Juan Carlos Niebles", "Stefano Ermon", "Li Fei-Fei"], "title": "Exploring Diffusion Transformer Designs via Grafting", "categories": ["cs.LG", "cs.AI"], "comment": "22 pages; Project website: https://grafting.stanford.edu", "summary": "Designing model architectures requires decisions such as selecting operators\n(e.g., attention, convolution) and configurations (e.g., depth, width).\nHowever, evaluating the impact of these decisions on model quality requires\ncostly pretraining, limiting architectural investigation. Inspired by how new\nsoftware is built on existing code, we ask: can new architecture designs be\nstudied using pretrained models? To this end, we present grafting, a simple\napproach for editing pretrained diffusion transformers (DiTs) to materialize\nnew architectures under small compute budgets. Informed by our analysis of\nactivation behavior and attention locality, we construct a testbed based on the\nDiT-XL/2 design to study the impact of grafting on model quality. Using this\ntestbed, we develop a family of hybrid designs via grafting: replacing softmax\nattention with gated convolution, local attention, and linear attention, and\nreplacing MLPs with variable expansion ratio and convolutional variants.\nNotably, many hybrid designs achieve good quality (FID: 2.38-2.64 vs. 2.27 for\nDiT-XL/2) using <2% pretraining compute. We then graft a text-to-image model\n(PixArt-Sigma), achieving a 1.43x speedup with less than a 2% drop in GenEval\nscore. Finally, we present a case study that restructures DiT-XL/2 by\nconverting every pair of sequential transformer blocks into parallel blocks via\ngrafting. This reduces model depth by 2x and yields better quality (FID: 2.77)\nthan other models of comparable depth. Together, we show that new diffusion\nmodel designs can be explored by grafting pretrained DiTs, with edits ranging\nfrom operator replacement to architecture restructuring. Code and grafted\nmodels: https://grafting.stanford.edu", "AI": {"tldr": "The paper introduces grafting, a method to edit pretrained diffusion transformers (DiTs) for exploring new architectures with minimal compute, achieving competitive results.", "motivation": "Evaluating architectural decisions in models is costly due to pretraining. The paper explores if new designs can be studied using pretrained models, inspired by software development.", "method": "Grafting edits pretrained DiTs to test new architectures, analyzing activation behavior and attention locality. Hybrid designs replace attention and MLPs with alternatives like gated convolution.", "result": "Hybrid designs achieve good quality (FID: 2.38-2.64) with <2% pretraining compute. Grafting also speeds up a text-to-image model (1.43x) with minimal quality drop.", "conclusion": "Grafting enables efficient exploration of new diffusion model designs, from operator replacement to architecture restructuring, using pretrained DiTs."}}
{"id": "2506.05345", "pdf": "https://arxiv.org/pdf/2506.05345", "abs": "https://arxiv.org/abs/2506.05345", "authors": ["Adrian \u0141a\u0144cucki", "Konrad Staniszewski", "Piotr Nawrot", "Edoardo M. Ponti"], "title": "Inference-Time Hyper-Scaling with KV Cache Compression", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Inference-time scaling trades efficiency for increased reasoning accuracy by\ngenerating longer or more parallel sequences. However, in Transformer LLMs,\ngeneration cost is bottlenecked by the size of the key-value (KV) cache, rather\nthan the number of generated tokens. Hence, we explore inference-time\nhyper-scaling: by compressing the KV cache, we can generate more tokens within\nthe same compute budget and further improve the accuracy of scaled inference.\nThe success of this approach, however, hinges on the ability of compression\nmethods to preserve accuracy even at high compression ratios. To make\nhyper-scaling practical, we introduce Dynamic Memory Sparsification (DMS), a\nnovel method for sparsifying KV caches that only requires 1K training steps to\nachieve 8$\\times$ compression, while maintaining better accuracy than\ntraining-free sparse attention. Instead of prematurely discarding cached\ntokens, DMS delays token eviction, implicitly merging representations and\npreserving critical information. We demonstrate the effectiveness of\ninference-time hyper-scaling with DMS on multiple families of LLMs, showing\nthat it boosts accuracy for comparable inference runtime and memory load. For\ninstance, we enhance Qwen-R1 32B by an average of 9.1 points on AIME 24, 7.6 on\nGPQA, and 9.6 on LiveCodeBench across compute budgets.", "AI": {"tldr": "The paper introduces Dynamic Memory Sparsification (DMS) to compress the KV cache in Transformer LLMs, enabling more tokens to be generated within the same compute budget and improving reasoning accuracy.", "motivation": "The key-value (KV) cache in Transformer LLMs bottlenecks generation cost, limiting efficiency. The paper aims to address this by compressing the KV cache to allow hyper-scaling without sacrificing accuracy.", "method": "The proposed method, Dynamic Memory Sparsification (DMS), sparsifies the KV cache with minimal training (1K steps), delaying token eviction to preserve critical information and achieve 8\u00d7 compression.", "result": "DMS maintains better accuracy than training-free sparse attention and enhances LLM performance, e.g., improving Qwen-R1 32B by 9.1 points on AIME 24.", "conclusion": "Inference-time hyper-scaling with DMS is effective, boosting accuracy for comparable runtime and memory load, making it a practical solution for scaling LLMs."}}
{"id": "2506.03177", "pdf": "https://arxiv.org/pdf/2506.03177", "abs": "https://arxiv.org/abs/2506.03177", "authors": ["Isarun Chamveha", "Supphanut Chaiyungyuen", "Sasinun Worakriangkrai", "Nattawadee Prasawang", "Warasinee Chaisangmongkon", "Pornpim Korpraphong", "Voraparee Suvannarerg", "Shanigarn Thiravit", "Chalermdej Kannawat", "Kewalin Rungsinaporn", "Suwara Issaragrisil", "Payia Chadbunchachai", "Pattiya Gatechumpol", "Chawiporn Muktabhant", "Patarachai Sereerat"], "title": "Deep Learning-Based Breast Cancer Detection in Mammography: A Multi-Center Validation Study in Thai Population", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "This study presents a deep learning system for breast cancer detection in\nmammography, developed using a modified EfficientNetV2 architecture with\nenhanced attention mechanisms. The model was trained on mammograms from a major\nThai medical center and validated on three distinct datasets: an in-domain test\nset (9,421 cases), a biopsy-confirmed set (883 cases), and an out-of-domain\ngeneralizability set (761 cases) collected from two different hospitals. For\ncancer detection, the model achieved AUROCs of 0.89, 0.96, and 0.94 on the\nrespective datasets. The system's lesion localization capability, evaluated\nusing metrics including Lesion Localization Fraction (LLF) and Non-Lesion\nLocalization Fraction (NLF), demonstrated robust performance in identifying\nsuspicious regions. Clinical validation through concordance tests showed strong\nagreement with radiologists: 83.5% classification and 84.0% localization\nconcordance for biopsy-confirmed cases, and 78.1% classification and 79.6%\nlocalization concordance for out-of-domain cases. Expert radiologists'\nacceptance rate also averaged 96.7% for biopsy-confirmed cases, and 89.3% for\nout-of-domain cases. The system achieved a System Usability Scale score of\n74.17 for source hospital, and 69.20 for validation hospitals, indicating good\nclinical acceptance. These results demonstrate the model's effectiveness in\nassisting mammogram interpretation, with the potential to enhance breast cancer\nscreening workflows in clinical practice.", "AI": {"tldr": "A deep learning system using EfficientNetV2 with attention mechanisms for breast cancer detection in mammograms showed high accuracy (AUROCs 0.89-0.96) and strong concordance with radiologists.", "motivation": "To improve breast cancer detection in mammography using deep learning, enhancing clinical workflows and diagnostic accuracy.", "method": "Modified EfficientNetV2 with attention mechanisms, trained on mammograms from a Thai medical center and validated on diverse datasets.", "result": "High AUROCs (0.89-0.96), strong radiologist concordance (78.1-84.0%), and good clinical acceptance (SUS scores 69.20-74.17).", "conclusion": "The system is effective for mammogram interpretation, with potential to enhance breast cancer screening in clinical practice."}}
{"id": "2506.04235", "pdf": "https://arxiv.org/pdf/2506.04235", "abs": "https://arxiv.org/abs/2506.04235", "authors": ["Xinyan Zhao", "Yi-Ching Tang", "Akshita Singh", "Victor J Cantu", "KwanHo An", "Junseok Lee", "Adam E Stogsdill", "Ashwin Kumar Ramesh", "Zhiqiang An", "Xiaoqian Jiang", "Yejin Kim"], "title": "Benchmark for Antibody Binding Affinity Maturation and Design", "categories": ["q-bio.QM", "cs.AI", "cs.CE", "cs.LG", "q-bio.BM"], "comment": null, "summary": "We introduce AbBiBench (Antibody Binding Benchmarking), a benchmarking\nframework for antibody binding affinity maturation and design. Unlike existing\nantibody evaluation strategies that rely on antibody alone and its similarity\nto natural ones (e.g., amino acid identity rate, structural RMSD), AbBiBench\nconsiders an antibody-antigen (Ab-Ag) complex as a functional unit and\nevaluates the potential of an antibody design binding to given antigen by\nmeasuring protein model's likelihood on the Ab-Ag complex. We first curate,\nstandardize, and share 9 datasets containing 9 antigens (involving influenza,\nanti-lysozyme, HER2, VEGF, integrin, and SARS-CoV-2) and 155,853 heavy chain\nmutated antibodies. Using these datasets, we systematically compare 14 protein\nmodels including masked language models, autoregressive language models,\ninverse folding models, diffusion-based generative models, and geometric graph\nmodels. The correlation between model likelihood and experimental affinity\nvalues is used to evaluate model performance. Additionally, in a case study to\nincrease binding affinity of antibody F045-092 to antigen influenza H1N1, we\nevaluate the generative power of the top-performing models by sampling a set of\nnew antibodies binding to the antigen and ranking them based on structural\nintegrity and biophysical properties of the Ab-Ag complex. As a result,\nstructure-conditioned inverse folding models outperform others in both affinity\ncorrelation and generation tasks. Overall, AbBiBench provides a unified,\nbiologically grounded evaluation framework to facilitate the development of\nmore effective, function-aware antibody design models.", "AI": {"tldr": "AbBiBench is a benchmarking framework for antibody binding affinity and design, evaluating antibody-antigen complexes and comparing 14 protein models.", "motivation": "Existing antibody evaluation methods focus on antibody similarity to natural ones, lacking functional assessment of antibody-antigen interactions.", "method": "AbBiBench uses curated datasets (9 antigens, 155,853 antibodies) to compare protein models by correlating model likelihood with experimental affinity. A case study tests generative power for antibody F045-092.", "result": "Structure-conditioned inverse folding models perform best in affinity correlation and generation tasks.", "conclusion": "AbBiBench offers a unified, biologically grounded framework to improve function-aware antibody design models."}}
{"id": "2506.04238", "pdf": "https://arxiv.org/pdf/2506.04238", "abs": "https://arxiv.org/abs/2506.04238", "authors": ["Shriyank Somvanshi", "Md Monzurul Islam", "Syed Aaqib Javed", "Gaurab Chhetri", "Kazi Sifatul Islam", "Tausif Islam Chowdhury", "Sazzad Bin Bashar Polock", "Anandi Dutta", "Subasish Das"], "title": "A Comprehensive Survey on Bio-Inspired Algorithms: Taxonomy, Applications, and Future Directions", "categories": ["cs.NE", "cs.LG"], "comment": null, "summary": "Bio-inspired algorithms (BIAs) utilize natural processes such as evolution,\nswarm behavior, foraging, and plant growth to solve complex, nonlinear,\nhigh-dimensional optimization problems. This survey categorizes BIAs into eight\ngroups: evolutionary, swarm intelligence, physics-inspired, ecosystem and\nplant-based, predator-prey, neural-inspired, human-inspired, and hybrid\napproaches, and reviews their core principles, strengths, and limitations. We\nillustrate the usage of these algorithms in machine learning, engineering\ndesign, bioinformatics, and intelligent systems, and highlight recent advances\nin hybridization, parameter tuning, and adaptive strategies. Finally, we\nidentify open challenges such as scalability, convergence, reliability, and\ninterpretability to suggest directions for future research. This work aims to\nserve as a foundational resource for both researchers and practitioners\ninterested in understanding the current landscape and future directions of\nbio-inspired computing.", "AI": {"tldr": "A survey categorizing bio-inspired algorithms (BIAs) into eight groups, reviewing their principles, strengths, and applications, while highlighting recent advances and open challenges.", "motivation": "To provide a comprehensive overview of BIAs, their applications, and future research directions for researchers and practitioners.", "method": "Categorizes BIAs into eight groups (e.g., evolutionary, swarm intelligence) and reviews their principles, strengths, and limitations. Illustrates applications in fields like machine learning and engineering.", "result": "Identifies recent advances in hybridization, parameter tuning, and adaptive strategies, along with open challenges like scalability and interpretability.", "conclusion": "Serves as a foundational resource for understanding BIAs' current landscape and future directions, emphasizing the need for addressing scalability and reliability."}}
{"id": "2506.04240", "pdf": "https://arxiv.org/pdf/2506.04240", "abs": "https://arxiv.org/abs/2506.04240", "authors": ["Sydney Anuyah"], "title": "What does making money have to do with crime?: A dive into the National Crime Victimization survey", "categories": ["physics.soc-ph", "cs.LG"], "comment": null, "summary": "In this short article, I leverage the National Crime Victimization Survey\nfrom 1992 to 2022 to examine how income, education, employment, and key\ndemographic factors shape the type of crime victims experience (violent vs\nproperty). Using balanced classification splits and logistic regression models\nevaluated by F1-score, there is an isolation of the socioeconomic drivers of\nvictimization \"Group A\" models and then an introduction of demographic factors\nsuch as age, gender, race, and marital status controls called \"Group B\" models.\nThe results consistently proves that higher income and education lower the odds\nof violent relative to property crime, while men younger individuals and racial\nminorities face disproportionately higher violentcrime risks. On the geographic\nspectrum, the suburban models achieve the strongest predictive performance with\nan accuracy of 0.607 and F1 of 0.590, urban areas benefit from adding education\nand employment predictors and crime in rural areas are still unpredictable\nusing these current factors. The patterns found in this study shows the need\nfor specific interventions like educational investments in metropolitan\nsettings economic support in rural communities and demographicaware prevention\nstrategies.", "AI": {"tldr": "The study analyzes how socioeconomic and demographic factors influence crime victimization types (violent vs. property) using data from 1992-2022. Higher income and education reduce violent crime odds, while men, younger individuals, and minorities face higher risks. Geographic patterns highlight suburban predictability and rural unpredictability, suggesting targeted interventions.", "motivation": "To understand how income, education, employment, and demographics shape crime victimization types, informing targeted prevention strategies.", "method": "Uses balanced classification splits and logistic regression models (Group A: socioeconomic factors; Group B: demographic controls) evaluated by F1-score.", "result": "Higher income/education lower violent crime odds; men, youth, and minorities face higher risks. Suburban models predict best (accuracy 0.607, F1 0.590), while rural crime remains unpredictable.", "conclusion": "Findings advocate for tailored interventions: educational investments in cities, economic support in rural areas, and demographic-aware prevention strategies."}}
{"id": "2506.04245", "pdf": "https://arxiv.org/pdf/2506.04245", "abs": "https://arxiv.org/abs/2506.04245", "authors": ["Guangchen Lan", "Huseyin A. Inan", "Sahar Abdelnabi", "Janardhan Kulkarni", "Lukas Wutschitz", "Reza Shokri", "Christopher G. Brinton", "Robert Sim"], "title": "Contextual Integrity in LLMs via Reasoning and Reinforcement Learning", "categories": ["cs.AI", "cs.CL", "cs.LG", "I.2.6; I.2.7"], "comment": null, "summary": "As the era of autonomous agents making decisions on behalf of users unfolds,\nensuring contextual integrity (CI) -- what is the appropriate information to\nshare while carrying out a certain task -- becomes a central question to the\nfield. We posit that CI demands a form of reasoning where the agent needs to\nreason about the context in which it is operating. To test this, we first\nprompt LLMs to reason explicitly about CI when deciding what information to\ndisclose. We then extend this approach by developing a reinforcement learning\n(RL) framework that further instills in models the reasoning necessary to\nachieve CI. Using a synthetic, automatically created, dataset of only $\\sim700$\nexamples but with diverse contexts and information disclosure norms, we show\nthat our method substantially reduces inappropriate information disclosure\nwhile maintaining task performance across multiple model sizes and families.\nImportantly, improvements transfer from this synthetic dataset to established\nCI benchmarks such as PrivacyLens that has human annotations and evaluates\nprivacy leakage of AI assistants in actions and tool calls.", "AI": {"tldr": "The paper introduces a method to ensure contextual integrity (CI) in autonomous agents by combining explicit reasoning with LLMs and a reinforcement learning framework, reducing inappropriate information disclosure while maintaining task performance.", "motivation": "Ensuring contextual integrity (CI) in autonomous agents is crucial to determine appropriate information sharing during tasks.", "method": "The approach involves prompting LLMs to reason about CI and developing an RL framework to instill CI reasoning, tested on a synthetic dataset.", "result": "The method significantly reduces inappropriate disclosures without compromising task performance, with improvements transferring to human-annotated benchmarks like PrivacyLens.", "conclusion": "The proposed framework effectively addresses CI in autonomous agents, demonstrating scalability and transferability to real-world benchmarks."}}
{"id": "2506.04249", "pdf": "https://arxiv.org/pdf/2506.04249", "abs": "https://arxiv.org/abs/2506.04249", "authors": ["Mehmet Aziz Yirik", "Jakob Lykke Andersen", "Rolf Fagerberg", "Daniel Merkle"], "title": "ChemReservoir -- An Open-Source Framework for Chemically-Inspired Reservoir Computing", "categories": ["cs.CE", "cs.ET", "cs.LG"], "comment": null, "summary": "Reservoir computing is a type of a recurrent neural network, mapping the\ninputs into higher dimensional space using fixed and nonlinear dynamical\nsystems, called reservoirs. In the literature, there are various types of\nreservoirs ranging from in-silico to in-vitro. In cheminformatics, previous\nstudies contributed to the field by developing simulation-based chemically\ninspired in-silico reservoir models. Yahiro used a DNA-based chemical reaction\nnetwork as its reservoir and Nguyen developed a DNA chemistry-inspired tool\nbased on Gillespie algorithm. However, these software tools were designed\nmainly with the focus on DNA chemistry and their maintenance status has limited\ntheir current usability. Due to these limitations, there was a need for a\nproper open-source tool. This study introduces ChemReservoir, an open-source\nframework for chemically-inspired reservoir computing. In contrast to the\nformer studies focused on DNA-chemistry, ChemReservoir is a general framework\nfor the construction and analysis of chemically-inspired reservoirs, which also\naddresses the limitations in these previous studies by ensuring enhanced\ntesting, evaluation, and reproducibility. The tool was evaluated using various\ncycle-based reservoir topologies and demonstrated stable performance across a\nrange of configurations in memory capacity tasks.", "AI": {"tldr": "The paper introduces ChemReservoir, an open-source framework for chemically-inspired reservoir computing, addressing limitations of previous DNA-focused tools.", "motivation": "Previous reservoir computing tools were limited to DNA chemistry and lacked maintenance, necessitating a general, open-source solution.", "method": "Developed ChemReservoir, a framework for constructing and analyzing chemically-inspired reservoirs, with enhanced testing and reproducibility.", "result": "ChemReservoir demonstrated stable performance in memory capacity tasks across various configurations.", "conclusion": "ChemReservoir provides a versatile and reliable tool for chemically-inspired reservoir computing, overcoming prior limitations."}}
{"id": "2506.04251", "pdf": "https://arxiv.org/pdf/2506.04251", "abs": "https://arxiv.org/abs/2506.04251", "authors": ["Zhengyang Li"], "title": "Language-Guided Multi-Agent Learning in Simulations: A Unified Framework and Evaluation", "categories": ["cs.AI", "cs.LG", "cs.MA"], "comment": null, "summary": "This paper introduces LLM-MARL, a unified framework that incorporates large\nlanguage models (LLMs) into multi-agent reinforcement learning (MARL) to\nenhance coordination, communication, and generalization in simulated game\nenvironments. The framework features three modular components of Coordinator,\nCommunicator, and Memory, which dynamically generate subgoals, facilitate\nsymbolic inter-agent messaging, and support episodic recall. Training combines\nPPO with a language-conditioned loss and LLM query gating. LLM-MARL is\nevaluated in Google Research Football, MAgent Battle, and StarCraft II. Results\nshow consistent improvements over MAPPO and QMIX in win rate, coordination\nscore, and zero-shot generalization. Ablation studies demonstrate that subgoal\ngeneration and language-based messaging each contribute significantly to\nperformance gains. Qualitative analysis reveals emergent behaviors such as role\nspecialization and communication-driven tactics. By bridging language modeling\nand policy learning, this work contributes to the design of intelligent,\ncooperative agents in interactive simulations. It offers a path forward for\nleveraging LLMs in multi-agent systems used for training, games, and human-AI\ncollaboration.", "AI": {"tldr": "LLM-MARL integrates LLMs into MARL to improve coordination, communication, and generalization in games, outperforming MAPPO and QMIX.", "motivation": "To enhance multi-agent systems by combining LLMs with MARL for better coordination and generalization.", "method": "Uses Coordinator, Communicator, and Memory modules with PPO and language-conditioned loss. Evaluated in game environments.", "result": "Shows improved win rates, coordination, and zero-shot generalization. Subgoal generation and messaging are key.", "conclusion": "Bridges LLMs and MARL, advancing intelligent, cooperative agents for simulations and human-AI collaboration."}}
{"id": "2506.04252", "pdf": "https://arxiv.org/pdf/2506.04252", "abs": "https://arxiv.org/abs/2506.04252", "authors": ["Yang Zhao", "Chengxiao Dai", "Dusit Niyato", "Chuan Fu Tan", "Keyi Xiang", "Yueyang Wang", "Zhiquan Yeo", "Daren Tan Zong Loong", "Jonathan Low Zhaozhi", "Eugene H. Z. HO"], "title": "A Graph-Retrieval-Augmented Generation Framework Enhances Decision-Making in the Circular Economy", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) hold promise for sustainable manufacturing, but\noften hallucinate industrial codes and emission factors, undermining regulatory\nand investment decisions. We introduce CircuGraphRAG, a retrieval-augmented\ngeneration (RAG) framework that grounds LLMs outputs in a domain-specific\nknowledge graph for the circular economy. This graph connects 117,380\nindustrial and waste entities with classification codes and GWP100 emission\ndata, enabling structured multi-hop reasoning. Natural language queries are\ntranslated into SPARQL and verified subgraphs are retrieved to ensure accuracy\nand traceability. Compared with Standalone LLMs and Naive RAG, CircuGraphRAG\nachieves superior performance in single-hop and multi-hop question answering,\nwith ROUGE-L F1 scores up to 1.0, while baseline scores below 0.08. It also\nimproves efficiency, halving the response time and reducing token usage by 16%\nin representative tasks. CircuGraphRAG provides fact-checked, regulatory-ready\nsupport for circular economy planning, advancing reliable, low-carbon resource\ndecision making.", "AI": {"tldr": "CircuGraphRAG improves LLM accuracy for circular economy tasks by grounding outputs in a knowledge graph, outperforming baselines in QA and efficiency.", "motivation": "LLMs often hallucinate industrial codes and emission factors, which can mislead regulatory and investment decisions in sustainable manufacturing.", "method": "Introduces CircuGraphRAG, a RAG framework that uses a domain-specific knowledge graph (117,380 entities) for structured reasoning. Queries are translated to SPARQL, and verified subgraphs ensure accuracy.", "result": "Outperforms Standalone LLMs and Naive RAG, achieving ROUGE-L F1 scores up to 1.0, halving response time, and reducing token usage by 16%.", "conclusion": "CircuGraphRAG provides reliable, regulatory-ready support for circular economy planning, enhancing low-carbon decision-making."}}
{"id": "2506.04256", "pdf": "https://arxiv.org/pdf/2506.04256", "abs": "https://arxiv.org/abs/2506.04256", "authors": ["Konstantinos Kalimeris", "Leonidas Mindrinos", "Nikolaos Pallikarakis"], "title": "Estimating properties of a homogeneous bounded soil using machine learning models", "categories": ["physics.geo-ph", "cs.LG"], "comment": "35 pages, 15 figures, 19 tables", "summary": "This work focuses on estimating soil properties from water moisture\nmeasurements. We consider simulated data generated by solving the\ninitial-boundary value problem governing vertical infiltration in a\nhomogeneous, bounded soil profile, with the usage of the Fokas method. To\naddress the parameter identification problem, which is formulated as a\ntwo-output regression task, we explore various machine learning models. The\nperformance of each model is assessed under different data conditions: full,\nnoisy, and limited. Overall, the prediction of diffusivity $D$ tends to be more\naccurate than that of hydraulic conductivity $K.$ Among the models considered,\nSupport Vector Machines (SVMs) and Neural Networks (NNs) demonstrate the\nhighest robustness, achieving near-perfect accuracy and minimal errors.", "AI": {"tldr": "The paper estimates soil properties from water moisture measurements using machine learning models, with SVMs and NNs showing the best performance.", "motivation": "To address the parameter identification problem in soil properties estimation from water moisture data.", "method": "Simulated data generated via the Fokas method for vertical infiltration; machine learning models (SVMs, NNs) tested under full, noisy, and limited data conditions.", "result": "Diffusivity (D) predictions are more accurate than hydraulic conductivity (K); SVMs and NNs achieve near-perfect accuracy and robustness.", "conclusion": "SVMs and NNs are highly effective for estimating soil properties from water moisture data, especially under varying data conditions."}}
{"id": "2506.04263", "pdf": "https://arxiv.org/pdf/2506.04263", "abs": "https://arxiv.org/abs/2506.04263", "authors": ["Alan Mitkiy", "James Smith", "Hana Satou", "Hiroshi Tanaka", "Emily Johnson", "F Monkey"], "title": "Dynamic Epsilon Scheduling: A Multi-Factor Adaptive Perturbation Budget for Adversarial Training", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Adversarial training is among the most effective strategies for defending\ndeep neural networks against adversarial examples. A key limitation of existing\nadversarial training approaches lies in their reliance on a fixed perturbation\nbudget, which fails to account for instance-specific robustness\ncharacteristics. While prior works such as IAAT and MMA introduce\ninstance-level adaptations, they often rely on heuristic or static\napproximations of data robustness. In this paper, we propose Dynamic Epsilon\nScheduling (DES), a novel framework that adaptively adjusts the adversarial\nperturbation budget per instance and per training iteration. DES integrates\nthree key factors: (1) the distance to the decision boundary approximated via\ngradient-based proxies, (2) prediction confidence derived from softmax entropy,\nand (3) model uncertainty estimated via Monte Carlo dropout. By combining these\ncues into a unified scheduling strategy, DES tailors the perturbation budget\ndynamically to guide more effective adversarial learning. Experimental results\non CIFAR-10 and CIFAR-100 show that our method consistently improves both\nadversarial robustness and standard accuracy compared to fixed-epsilon\nbaselines and prior adaptive methods. Moreover, we provide theoretical insights\ninto the stability and convergence of our scheduling policy. This work opens a\nnew avenue for instance-aware, data-driven adversarial training methods.", "AI": {"tldr": "DES is a dynamic adversarial training method that adapts perturbation budgets per instance and iteration, improving robustness and accuracy.", "motivation": "Existing adversarial training uses fixed budgets, ignoring instance-specific robustness. DES addresses this gap.", "method": "DES adjusts budgets using decision boundary distance, prediction confidence, and model uncertainty.", "result": "DES outperforms fixed-budget and prior adaptive methods on CIFAR-10/100, enhancing robustness and accuracy.", "conclusion": "DES introduces a data-driven, instance-aware approach for adversarial training, with theoretical and empirical support."}}
{"id": "2506.04265", "pdf": "https://arxiv.org/pdf/2506.04265", "abs": "https://arxiv.org/abs/2506.04265", "authors": ["Mengda Ji", "Genjiu Xu", "Liying Wang"], "title": "CORA: Coalitional Rational Advantage Decomposition for Multi-Agent Policy Gradients", "categories": ["cs.MA", "cs.AI", "cs.GT", "cs.LG"], "comment": null, "summary": "This work focuses on the credit assignment problem in cooperative multi-agent\nreinforcement learning (MARL). Sharing the global advantage among agents often\nleads to suboptimal policy updates as it fails to account for the distinct\ncontributions of agents. Although numerous methods consider global or\nindividual contributions for credit assignment, a detailed analysis at the\ncoalition level remains lacking in many approaches. This work analyzes the\nover-updating problem during multi-agent policy updates from a coalition-level\nperspective. To address this issue, we propose a credit assignment method\ncalled Coalitional Rational Advantage Decomposition (CORA). CORA evaluates\ncoalitional advantages via marginal contributions from all possible coalitions\nand decomposes advantages using the core solution from cooperative game theory,\nensuring coalitional rationality. To reduce computational overhead, CORA\nemploys random coalition sampling. Experiments on matrix games, differential\ngames, and multi-agent collaboration benchmarks demonstrate that CORA\noutperforms strong baselines, particularly in tasks with multiple local optima.\nThese findings highlight the importance of coalition-aware credit assignment\nfor improving MARL performance.", "AI": {"tldr": "The paper addresses credit assignment in multi-agent reinforcement learning (MARL) by proposing CORA, a coalition-level method that ensures rational advantage decomposition and outperforms baselines.", "motivation": "Current MARL methods often fail to account for distinct agent contributions due to global advantage sharing, leading to suboptimal policies. A coalition-level analysis is lacking.", "method": "Proposes Coalitional Rational Advantage Decomposition (CORA), which evaluates coalitional advantages via marginal contributions and decomposes them using cooperative game theory's core solution. Random coalition sampling reduces computational overhead.", "result": "CORA outperforms baselines in matrix games, differential games, and multi-agent benchmarks, especially in tasks with multiple local optima.", "conclusion": "Coalition-aware credit assignment, as demonstrated by CORA, is crucial for improving MARL performance."}}
{"id": "2506.04287", "pdf": "https://arxiv.org/pdf/2506.04287", "abs": "https://arxiv.org/abs/2506.04287", "authors": ["Yongjin Yang", "Sinjae Kang", "Juyong Lee", "Dongjun Lee", "Se-Young Yun", "Kimin Lee"], "title": "Automated Skill Discovery for Language Agents through Exploration and Iterative Feedback", "categories": ["cs.AI", "cs.LG"], "comment": "Preprint, under review", "summary": "Training large language model (LLM) agents to acquire necessary skills and\nperform diverse tasks within an environment is gaining interest as a means to\nenable open-endedness. However, creating the training dataset for their skill\nacquisition faces several challenges. Manual trajectory collection requires\nsignificant human effort. Another approach, where LLMs directly propose tasks\nto learn, is often invalid, as the LLMs lack knowledge of which tasks are\nactually feasible. Moreover, the generated data may not provide a meaningful\nlearning signal, as agents often already perform well on the proposed tasks. To\naddress this, we propose a novel automatic skill discovery framework EXIF for\nLLM-powered agents, designed to improve the feasibility of generated target\nbehaviors while accounting for the agents' capabilities. Our method adopts an\nexploration-first strategy by employing an exploration agent (Alice) to train\nthe target agent (Bob) to learn essential skills in the environment.\nSpecifically, Alice first interacts with the environment to retrospectively\ngenerate a feasible, environment-grounded skill dataset, which is then used to\ntrain Bob. Crucially, we incorporate an iterative feedback loop, where Alice\nevaluates Bob's performance to identify areas for improvement. This feedback\nthen guides Alice's next round of exploration, forming a closed-loop data\ngeneration process. Experiments on Webshop and Crafter demonstrate EXIF's\nability to effectively discover meaningful skills and iteratively expand the\ncapabilities of the trained agent without any human intervention, achieving\nsubstantial performance improvements. Interestingly, we observe that setting\nAlice to the same model as Bob also notably improves performance, demonstrating\nEXIF's potential for building a self-evolving system.", "AI": {"tldr": "The paper introduces EXIF, an automatic skill discovery framework for LLM-powered agents, addressing challenges in dataset creation by using an exploration-first strategy and iterative feedback to improve agent capabilities without human intervention.", "motivation": "Training LLM agents for diverse tasks is challenging due to manual effort in dataset creation and the infeasibility of tasks proposed by LLMs. EXIF aims to automate skill discovery while ensuring feasibility and meaningful learning.", "method": "EXIF employs an exploration agent (Alice) to interact with the environment and generate feasible skills, which train the target agent (Bob). An iterative feedback loop evaluates Bob's performance to guide Alice's further exploration.", "result": "Experiments on Webshop and Crafter show EXIF effectively discovers skills and improves agent performance without human intervention. Using the same model for Alice and Bob further enhances results.", "conclusion": "EXIF successfully automates skill discovery and agent training, demonstrating potential for self-evolving systems through iterative feedback and exploration."}}
{"id": "2506.04290", "pdf": "https://arxiv.org/pdf/2506.04290", "abs": "https://arxiv.org/abs/2506.04290", "authors": ["Muhammed Golec", "Maha AlabdulJalil"], "title": "Interpretable LLMs for Credit Risk: A Systematic Review and Taxonomy", "categories": ["q-fin.RM", "cs.LG"], "comment": "20 pages, 6 figures, preprint under review at Information Processing\n  & Management", "summary": "Large Language Models (LLM), which have developed in recent years, enable\ncredit risk assessment through the analysis of financial texts such as analyst\nreports and corporate disclosures. This paper presents the first systematic\nreview and taxonomy focusing on LLMbased approaches in credit risk estimation.\nWe determined the basic model architectures by selecting 60 relevant papers\npublished between 2020-2025 with the PRISMA research strategy. And we examined\nthe data used for scenarios such as credit default prediction and risk\nanalysis. Since the main focus of the paper is interpretability, we classify\nconcepts such as explainability mechanisms, chain of thought prompts and\nnatural language justifications for LLM-based credit models. The taxonomy\norganizes the literature under four main headings: model architectures, data\ntypes, explainability mechanisms and application areas. Based on this analysis,\nwe highlight the main future trends and research gaps for LLM-based credit\nscoring systems. This paper aims to be a reference paper for artificial\nintelligence and financial researchers.", "AI": {"tldr": "This paper reviews LLM-based approaches for credit risk assessment, categorizing 60 studies (2020-2025) into model architectures, data types, explainability mechanisms, and applications. It highlights trends and gaps, focusing on interpretability.", "motivation": "To systematically review and classify LLM-based methods in credit risk estimation, addressing the need for interpretability in financial AI applications.", "method": "Analyzed 60 papers using PRISMA strategy, focusing on model architectures, data types, explainability (e.g., chain of thought prompts), and applications like default prediction.", "result": "A taxonomy of LLM-based credit risk models was developed, identifying key trends and research gaps, particularly in interpretability.", "conclusion": "The paper serves as a reference for AI and finance researchers, emphasizing future directions for LLM-based credit scoring systems."}}
{"id": "2506.04292", "pdf": "https://arxiv.org/pdf/2506.04292", "abs": "https://arxiv.org/abs/2506.04292", "authors": ["Bruno Deprez", "Bart Baesens", "Tim Verdonck", "Wouter Verbeke"], "title": "GARG-AML against Smurfing: A Scalable and Interpretable Graph-Based Framework for Anti-Money Laundering", "categories": ["cs.SI", "cs.LG", "stat.AP"], "comment": null, "summary": "Money laundering poses a significant challenge as it is estimated to account\nfor 2%-5% of the global GDP. This has compelled regulators to impose stringent\ncontrols on financial institutions. One prominent laundering method for evading\nthese controls, called smurfing, involves breaking up large transactions into\nsmaller amounts. Given the complexity of smurfing schemes, which involve\nmultiple transactions distributed among diverse parties, network analytics has\nbecome an important anti-money laundering tool. However, recent advances have\nfocused predominantly on black-box network embedding methods, which has\nhindered their adoption in businesses. In this paper, we introduce GARG-AML, a\nnovel graph-based method that quantifies smurfing risk through a single\ninterpretable metric derived from the structure of the second-order transaction\nnetwork of each individual node in the network. Unlike traditional methods,\nGARG-AML strikes an effective balance among computational efficiency, detection\npower and transparency, which enables its integration into existing AML\nworkflows. To enhance its capabilities, we combine the GARG-AML score\ncalculation with different tree-based methods and also incorporate the scores\nof the node's neighbours. An experimental evaluation on large-scale synthetic\nand open-source networks demonstrate that the GARG-AML outperforms the current\nstate-of-the-art smurfing detection methods. By leveraging only the adjacency\nmatrix of the second-order neighbourhood and basic network features, this work\nhighlights the potential of fundamental network properties towards advancing\nfraud detection.", "AI": {"tldr": "GARG-AML is a graph-based method for detecting smurfing in money laundering, offering a transparent, efficient, and powerful solution compared to black-box approaches.", "motivation": "Money laundering, particularly smurfing, is a major global issue, but current detection methods lack transparency and practicality for businesses.", "method": "GARG-AML uses a single interpretable metric from second-order transaction networks, combined with tree-based methods and neighbor scores.", "result": "It outperforms state-of-the-art methods in detecting smurfing on synthetic and open-source networks.", "conclusion": "The method demonstrates the effectiveness of fundamental network properties in fraud detection, offering a practical tool for AML workflows."}}
{"id": "2506.04303", "pdf": "https://arxiv.org/pdf/2506.04303", "abs": "https://arxiv.org/abs/2506.04303", "authors": ["Zhizheng Wang", "Chi-Ping Day", "Chih-Hsuan Wei", "Qiao Jin", "Robert Leaman", "Yifan Yang", "Shubo Tian", "Aodong Qiu", "Yin Fang", "Qingqing Zhu", "Xinghua Lu", "Zhiyong Lu"], "title": "Knowledge-guided Contextual Gene Set Analysis Using Large Language Models", "categories": ["q-bio.GN", "cs.AI", "cs.LG"], "comment": "56 pages, 9 figures, 1 table", "summary": "Gene set analysis (GSA) is a foundational approach for interpreting genomic\ndata of diseases by linking genes to biological processes. However,\nconventional GSA methods overlook clinical context of the analyses, often\ngenerating long lists of enriched pathways with redundant, nonspecific, or\nirrelevant results. Interpreting these requires extensive, ad-hoc manual\neffort, reducing both reliability and reproducibility. To address this\nlimitation, we introduce cGSA, a novel AI-driven framework that enhances GSA by\nincorporating context-aware pathway prioritization. cGSA integrates gene\ncluster detection, enrichment analysis, and large language models to identify\npathways that are not only statistically significant but also biologically\nmeaningful. Benchmarking on 102 manually curated gene sets across 19 diseases\nand ten disease-related biological mechanisms shows that cGSA outperforms\nbaseline methods by over 30%, with expert validation confirming its increased\nprecision and interpretability. Two independent case studies in melanoma and\nbreast cancer further demonstrate its potential to uncover context-specific\ninsights and support targeted hypothesis generation.", "AI": {"tldr": "cGSA is an AI-driven framework that improves gene set analysis by incorporating clinical context, outperforming traditional methods by 30% and enhancing precision and interpretability.", "motivation": "Conventional GSA methods lack clinical context, leading to redundant or irrelevant results, which reduces reliability and reproducibility.", "method": "cGSA integrates gene cluster detection, enrichment analysis, and large language models to prioritize biologically meaningful pathways.", "result": "Benchmarking on 102 gene sets across 19 diseases showed cGSA outperforms baseline methods by over 30%, with expert validation confirming its precision.", "conclusion": "cGSA demonstrates potential for uncovering context-specific insights and supporting targeted hypothesis generation in diseases like melanoma and breast cancer."}}
{"id": "2506.04344", "pdf": "https://arxiv.org/pdf/2506.04344", "abs": "https://arxiv.org/abs/2506.04344", "authors": ["Caojin Zhang", "Qiang Zhang", "Ke Li", "Sai Vidyaranya Nuthalapati", "Benyu Zhang", "Jason Liu", "Serena Li", "Lizhu Zhang", "Xiangjun Fan"], "title": "GEM: Empowering LLM for both Embedding Generation and Language Understanding", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large decoder-only language models (LLMs) have achieved remarkable success in\ngeneration and reasoning tasks, where they generate text responses given\ninstructions. However, many applications, e.g., retrieval augmented generation\n(RAG), still rely on separate embedding models to generate text embeddings,\nwhich can complicate the system and introduce discrepancies in understanding of\nthe query between the embedding model and LLMs. To address this limitation, we\npropose a simple self-supervised approach, Generative Embedding large language\nModel (GEM), that enables any large decoder-only LLM to generate high-quality\ntext embeddings while maintaining its original text generation and reasoning\ncapabilities. Our method inserts new special token(s) into a text body, and\ngenerates summarization embedding of the text by manipulating the attention\nmask. This method could be easily integrated into post-training or fine tuning\nstages of any existing LLMs. We demonstrate the effectiveness of our approach\nby applying it to two popular LLM families, ranging from 1B to 8B parameters,\nand evaluating the transformed models on both text embedding benchmarks (MTEB)\nand NLP benchmarks (MMLU). The results show that our proposed method\nsignificantly improves the original LLMs on MTEB while having a minimal impact\non MMLU. Our strong results indicate that our approach can empower LLMs with\nstate-of-the-art text embedding capabilities while maintaining their original\nNLP performance", "AI": {"tldr": "The paper proposes GEM, a self-supervised method enabling decoder-only LLMs to generate high-quality text embeddings without compromising their original text generation and reasoning abilities.", "motivation": "Current systems often rely on separate embedding models for tasks like RAG, complicating the system and causing discrepancies. GEM aims to unify these capabilities within a single LLM.", "method": "GEM inserts special tokens into text and manipulates attention masks to generate summarization embeddings, easily integrable into post-training or fine-tuning.", "result": "Applied to LLMs (1B-8B parameters), GEM significantly improves text embedding performance (MTEB) with minimal impact on NLP tasks (MMLU).", "conclusion": "GEM successfully equips LLMs with advanced text embedding capabilities while preserving their original NLP performance."}}
{"id": "2506.04353", "pdf": "https://arxiv.org/pdf/2506.04353", "abs": "https://arxiv.org/abs/2506.04353", "authors": ["Ankit Pal", "Jung-Oh Lee", "Xiaoman Zhang", "Malaikannan Sankarasubbu", "Seunghyeon Roh", "Won Jung Kim", "Meesun Lee", "Pranav Rajpurkar"], "title": "ReXVQA: A Large-scale Visual Question Answering Benchmark for Generalist Chest X-ray Understanding", "categories": ["cs.CV", "cs.AI", "cs.CE", "cs.CL", "cs.LG"], "comment": null, "summary": "We present ReXVQA, the largest and most comprehensive benchmark for visual\nquestion answering (VQA) in chest radiology, comprising approximately 696,000\nquestions paired with 160,000 chest X-rays studies across training, validation,\nand test sets. Unlike prior efforts that rely heavily on template based\nqueries, ReXVQA introduces a diverse and clinically authentic task suite\nreflecting five core radiological reasoning skills: presence assessment,\nlocation analysis, negation detection, differential diagnosis, and geometric\nreasoning. We evaluate eight state-of-the-art multimodal large language models,\nincluding MedGemma-4B-it, Qwen2.5-VL, Janus-Pro-7B, and Eagle2-9B. The\nbest-performing model (MedGemma) achieves 83.24% overall accuracy. To bridge\nthe gap between AI performance and clinical expertise, we conducted a\ncomprehensive human reader study involving 3 radiology residents on 200\nrandomly sampled cases. Our evaluation demonstrates that MedGemma achieved\nsuperior performance (83.84% accuracy) compared to human readers (best\nradiology resident: 77.27%), representing a significant milestone where AI\nperformance exceeds expert human evaluation on chest X-ray interpretation. The\nreader study reveals distinct performance patterns between AI models and human\nexperts, with strong inter-reader agreement among radiologists while showing\nmore variable agreement patterns between human readers and AI models. ReXVQA\nestablishes a new standard for evaluating generalist radiological AI systems,\noffering public leaderboards, fine-grained evaluation splits, structured\nexplanations, and category-level breakdowns. This benchmark lays the foundation\nfor next-generation AI systems capable of mimicking expert-level clinical\nreasoning beyond narrow pathology classification. Our dataset will be\nopen-sourced at https://huggingface.co/datasets/rajpurkarlab/ReXVQA", "AI": {"tldr": "ReXVQA is a large benchmark for VQA in chest radiology, featuring diverse tasks and outperforming human experts with AI models like MedGemma.", "motivation": "To create a comprehensive benchmark for evaluating AI in radiological reasoning, bridging the gap between AI and clinical expertise.", "method": "Introduces ReXVQA with 696,000 questions and 160,000 X-rays, testing eight multimodal models and comparing them to human readers.", "result": "MedGemma achieved 83.24% accuracy, surpassing human readers (77.27%), with distinct performance patterns between AI and humans.", "conclusion": "ReXVQA sets a new standard for radiological AI evaluation, enabling next-generation systems with expert-level reasoning."}}
{"id": "2506.04357", "pdf": "https://arxiv.org/pdf/2506.04357", "abs": "https://arxiv.org/abs/2506.04357", "authors": ["Amirreza Ahmadnejad", "Somayyeh Koohi"], "title": "Optical Physics-Based Generative Models", "categories": ["physics.optics", "cs.LG"], "comment": null, "summary": "This paper establishes a comprehensive mathematical framework connecting\noptical physics equations to generative models, demonstrating how light\npropagation dynamics inspire powerful artificial intelligence approaches. We\nanalyze six fundamental optical equations, comparing linear models (Helmholtz,\ndissipative wave, and Eikonal equations) with their nonlinear extensions\nincorporating Kerr effects, cubic-quintic nonlinearities, and\nintensity-dependent refractive indices. Our nonlinear optical models reveal\nremarkable capabilities through natural self-organization principles. The\nnonlinear Helmholtz model achieves 40-60% parameter reduction while maintaining\nsuperior mode separation via self-focusing phenomena. The cubic-quintic\ndissipative wave model prevents mode collapse through balanced\nattractive-repulsive interactions, enabling stable soliton formation with\n20-40% improved coverage. The intensity-dependent Eikonal model creates\nadaptive pathways that dynamically respond to content, providing enhanced\ncontrollability in conditional generation. Experimental validation demonstrates\nconsistent superiority over linear predecessors and traditional generative\napproaches. The nonlinear Helmholtz model achieves FID scores of 0.0089 versus\n1.0909 for linear versions, while the cubic-quintic model reaches 0.0156 FID\nwith exceptional stability. Memory usage drops 40-60% and training time\nimproves 30-50% due to inherent nonlinear stability properties. The framework\nenables bidirectional benefits, advancing both generative AI and optical\nphysics through novel approaches to soliton analysis, wavefront control, and\nrefractive index reconstruction with 95% accuracy. This work reveals deep\nconnections between physical self-organization and artificial intelligence,\nopening pathways toward efficient optical computing implementations.", "AI": {"tldr": "The paper links optical physics equations to generative AI, showing how nonlinear optical models outperform linear ones in efficiency and performance.", "motivation": "To explore the connection between optical physics and generative models, leveraging natural self-organization principles for AI advancements.", "method": "Analyzes six optical equations (linear and nonlinear), comparing their performance in generative tasks. Validates through experiments.", "result": "Nonlinear models reduce parameters, improve stability, and achieve better performance (e.g., 40-60% memory reduction, 30-50% faster training).", "conclusion": "The framework bridges optical physics and AI, enhancing both fields and enabling efficient optical computing."}}
{"id": "2506.04381", "pdf": "https://arxiv.org/pdf/2506.04381", "abs": "https://arxiv.org/abs/2506.04381", "authors": ["Neeraj Agrawal", "Saurabh Kumar", "Priyanka Bhatt", "Tanishka Agarwal"], "title": "Hierarchical Text Classification Using Contrastive Learning Informed Path Guided Hierarchy", "categories": ["cs.CL", "cs.LG"], "comment": "arXiv admin note: text overlap with arXiv:2203.03825 by other authors", "summary": "Hierarchical Text Classification (HTC) has recently gained traction given the\nability to handle complex label hierarchy. This has found applications in\ndomains like E- commerce, customer care and medicine industry among other\nreal-world applications. Existing HTC models either encode label hierarchy\nseparately and mix it with text encoding or guide the label hierarchy structure\nin the text encoder. Both approaches capture different characteristics of label\nhierarchy and are complementary to each other. In this paper, we propose a\nHierarchical Text Classification using Contrastive Learning Informed Path\nguided hierarchy (HTC-CLIP), which learns hierarchy-aware text representation\nand text informed path guided hierarchy representation using contrastive\nlearning. During the training of HTC-CLIP, we learn two different sets of class\nprobabilities distributions and during inference, we use the pooled output of\nboth probabilities for each class to get the best of both representations. Our\nresults show that the two previous approaches can be effectively combined into\none architecture to achieve improved performance. Tests on two public benchmark\ndatasets showed an improvement of 0.99 - 2.37% in Macro F1 score using HTC-CLIP\nover the existing state-of-the-art models.", "AI": {"tldr": "HTC-CLIP combines label hierarchy encoding and text encoder guidance using contrastive learning, improving performance over existing models.", "motivation": "Existing HTC models handle label hierarchy and text encoding separately or guide hierarchy in the encoder, but these approaches are complementary.", "method": "HTC-CLIP learns hierarchy-aware text and path-guided hierarchy representations via contrastive learning, pooling both during inference.", "result": "HTC-CLIP improves Macro F1 scores by 0.99-2.37% over state-of-the-art models on two benchmark datasets.", "conclusion": "Combining complementary approaches in HTC-CLIP enhances performance, demonstrating the value of integrating hierarchy and text representations."}}
{"id": "2506.04389", "pdf": "https://arxiv.org/pdf/2506.04389", "abs": "https://arxiv.org/abs/2506.04389", "authors": ["Saurabh Kumar", "Sourav Bansal", "Neeraj Agrawal", "Priyanka Bhatt"], "title": "Building a Few-Shot Cross-Domain Multilingual NLU Model for Customer Care", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Customer care is an essential pillar of the e-commerce shopping experience\nwith companies spending millions of dollars each year, employing automation and\nhuman agents, across geographies (like US, Canada, Mexico, Chile), channels\n(like Chat, Interactive Voice Response (IVR)), and languages (like English,\nSpanish). SOTA pre-trained models like multilingual-BERT, fine-tuned on\nannotated data have shown good performance in downstream tasks relevant to\nCustomer Care. However, model performance is largely subject to the\navailability of sufficient annotated domain-specific data. Cross-domain\navailability of data remains a bottleneck, thus building an intent classifier\nthat generalizes across domains (defined by channel, geography, and language)\nwith only a few annotations, is of great practical value. In this paper, we\npropose an embedder-cum-classifier model architecture which extends\nstate-of-the-art domain-specific models to other domains with only a few\nlabeled samples. We adopt a supervised fine-tuning approach with isotropic\nregularizers to train a domain-specific sentence embedder and a multilingual\nknowledge distillation strategy to generalize this embedder across multiple\ndomains. The trained embedder, further augmented with a simple linear\nclassifier can be deployed for new domains. Experiments on Canada and Mexico\ne-commerce Customer Care dataset with few-shot intent detection show an\nincrease in accuracy by 20-23% against the existing state-of-the-art\npre-trained models.", "AI": {"tldr": "Proposes an embedder-cum-classifier model for few-shot intent detection in customer care, improving accuracy by 20-23% over SOTA models.", "motivation": "Addresses the challenge of limited annotated domain-specific data in customer care, aiming to generalize intent classification across domains (geography, channel, language).", "method": "Uses a supervised fine-tuning approach with isotropic regularizers for domain-specific embedder and multilingual knowledge distillation for cross-domain generalization.", "result": "Achieves 20-23% higher accuracy in few-shot intent detection on Canada and Mexico e-commerce datasets compared to SOTA models.", "conclusion": "The proposed model effectively generalizes intent classification across domains with minimal labeled data, offering practical value for customer care automation."}}
{"id": "2506.04405", "pdf": "https://arxiv.org/pdf/2506.04405", "abs": "https://arxiv.org/abs/2506.04405", "authors": ["Ran Xu", "Yuchen Zhuang", "Yishan Zhong", "Yue Yu", "Xiangru Tang", "Hang Wu", "May D. Wang", "Peifeng Ruan", "Donghan Yang", "Tao Wang", "Guanghua Xiao", "Carl Yang", "Yang Xie", "Wenqi Shi"], "title": "MedAgentGym: Training LLM Agents for Code-Based Medical Reasoning at Scale", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We introduce MedAgentGYM, the first publicly available training environment\ndesigned to enhance coding-based medical reasoning capabilities in large\nlanguage model (LLM) agents. MedAgentGYM comprises 72,413 task instances across\n129 categories derived from authentic real-world biomedical scenarios. Tasks\nare encapsulated within executable coding environments, each featuring detailed\ntask descriptions, interactive feedback mechanisms, verifiable ground-truth\nannotations, and scalable training trajectory generation. Extensive\nbenchmarking of over 30 LLMs reveals a notable performance disparity between\ncommercial API-based models and open-source counterparts. Leveraging\nMedAgentGYM, Med-Copilot-7B achieves substantial performance gains through\nsupervised fine-tuning (+36.44%) and continued reinforcement learning\n(+42.47%), emerging as an affordable and privacy-preserving alternative\ncompetitive with gpt-4o. By offering both a comprehensive benchmark and\naccessible, expandable training resources within unified execution\nenvironments, MedAgentGYM delivers an integrated platform to develop LLM-based\ncoding assistants for advanced biomedical research and practice.", "AI": {"tldr": "MedAgentGYM is a training environment for enhancing medical reasoning in LLMs, featuring 72,413 tasks across 129 categories. It benchmarks 30+ LLMs, showing performance gaps, and enables Med-Copilot-7B to achieve significant gains via fine-tuning and reinforcement learning.", "motivation": "To address the lack of publicly available environments for improving LLMs' medical reasoning and coding capabilities, using real-world biomedical scenarios.", "method": "Developed MedAgentGYM with executable coding tasks, interactive feedback, and scalable training. Benchmarked 30+ LLMs and fine-tuned Med-Copilot-7B.", "result": "Performance gaps between commercial and open-source LLMs. Med-Copilot-7B improved by +36.44% (fine-tuning) and +42.47% (reinforcement learning), rivaling GPT-4.", "conclusion": "MedAgentGYM provides a unified platform for developing LLM-based biomedical coding assistants, offering benchmarks and training resources."}}
{"id": "2506.04421", "pdf": "https://arxiv.org/pdf/2506.04421", "abs": "https://arxiv.org/abs/2506.04421", "authors": ["Hermann Kumbong", "Xian Liu", "Tsung-Yi Lin", "Ming-Yu Liu", "Xihui Liu", "Ziwei Liu", "Daniel Y. Fu", "Christopher R\u00e9", "David W. Romero"], "title": "HMAR: Efficient Hierarchical Masked Auto-Regressive Image Generation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted to CVPR 2025. Project Page:\n  https://research.nvidia.com/labs/dir/hmar/", "summary": "Visual Auto-Regressive modeling (VAR) has shown promise in bridging the speed\nand quality gap between autoregressive image models and diffusion models. VAR\nreformulates autoregressive modeling by decomposing an image into successive\nresolution scales. During inference, an image is generated by predicting all\nthe tokens in the next (higher-resolution) scale, conditioned on all tokens in\nall previous (lower-resolution) scales. However, this formulation suffers from\nreduced image quality due to the parallel generation of all tokens in a\nresolution scale; has sequence lengths scaling superlinearly in image\nresolution; and requires retraining to change the sampling schedule.\n  We introduce Hierarchical Masked Auto-Regressive modeling (HMAR), a new image\ngeneration algorithm that alleviates these issues using next-scale prediction\nand masked prediction to generate high-quality images with fast sampling. HMAR\nreformulates next-scale prediction as a Markovian process, wherein the\nprediction of each resolution scale is conditioned only on tokens in its\nimmediate predecessor instead of the tokens in all predecessor resolutions.\nWhen predicting a resolution scale, HMAR uses a controllable multi-step masked\ngeneration procedure to generate a subset of the tokens in each step. On\nImageNet 256x256 and 512x512 benchmarks, HMAR models match or outperform\nparameter-matched VAR, diffusion, and autoregressive baselines. We develop\nefficient IO-aware block-sparse attention kernels that allow HMAR to achieve\nfaster training and inference times over VAR by over 2.5x and 1.75x\nrespectively, as well as over 3x lower inference memory footprint. Finally,\nHMAR yields additional flexibility over VAR; its sampling schedule can be\nchanged without further training, and it can be applied to image editing tasks\nin a zero-shot manner.", "AI": {"tldr": "HMAR improves upon VAR by introducing hierarchical masked auto-regressive modeling, enabling faster, higher-quality image generation with flexible sampling and lower memory usage.", "motivation": "To address VAR's issues of reduced image quality, superlinear sequence length scaling, and inflexible sampling schedules.", "method": "HMAR uses next-scale prediction as a Markovian process and multi-step masked generation for controllable token prediction.", "result": "HMAR outperforms VAR, diffusion, and autoregressive models on ImageNet benchmarks, with faster training/inference and lower memory usage.", "conclusion": "HMAR offers a superior alternative to VAR, combining efficiency, flexibility, and high-quality image generation."}}
{"id": "2506.04450", "pdf": "https://arxiv.org/pdf/2506.04450", "abs": "https://arxiv.org/abs/2506.04450", "authors": ["Payel Bhattacharjee", "Fengwei Tian", "Ravi Tandon", "Joseph Lo", "Heidi Hanson", "Geoffrey Rubin", "Nirav Merchant", "John Gounley"], "title": "Learning to Diagnose Privately: DP-Powered LLMs for Radiology Report Classification", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG"], "comment": "19 pages, 5 figures, 2 tables", "summary": "Purpose: This study proposes a framework for fine-tuning large language\nmodels (LLMs) with differential privacy (DP) to perform multi-abnormality\nclassification on radiology report text. By injecting calibrated noise during\nfine-tuning, the framework seeks to mitigate the privacy risks associated with\nsensitive patient data and protect against data leakage while maintaining\nclassification performance. Materials and Methods: We used 50,232 radiology\nreports from the publicly available MIMIC-CXR chest radiography and CT-RATE\ncomputed tomography datasets, collected between 2011 and 2019. Fine-tuning of\nLLMs was conducted to classify 14 labels from MIMIC-CXR dataset, and 18 labels\nfrom CT-RATE dataset using Differentially Private Low-Rank Adaptation (DP-LoRA)\nin high and moderate privacy regimes (across a range of privacy budgets =\n{0.01, 0.1, 1.0, 10.0}). Model performance was evaluated using weighted F1\nscore across three model architectures: BERT-medium, BERT-small, and\nALBERT-base. Statistical analyses compared model performance across different\nprivacy levels to quantify the privacy-utility trade-off. Results: We observe a\nclear privacy-utility trade-off through our experiments on 2 different datasets\nand 3 different models. Under moderate privacy guarantees the DP fine-tuned\nmodels achieved comparable weighted F1 scores of 0.88 on MIMIC-CXR and 0.59 on\nCT-RATE, compared to non-private LoRA baselines of 0.90 and 0.78, respectively.\nConclusion: Differentially private fine-tuning using LoRA enables effective and\nprivacy-preserving multi-abnormality classification from radiology reports,\naddressing a key challenge in fine-tuning LLMs on sensitive medical data.", "AI": {"tldr": "A framework for fine-tuning LLMs with differential privacy (DP) is proposed to classify abnormalities in radiology reports, balancing privacy and performance.", "motivation": "To mitigate privacy risks in sensitive patient data while maintaining classification accuracy in radiology reports.", "method": "Used 50,232 radiology reports from MIMIC-CXR and CT-RATE datasets, fine-tuned LLMs with DP-LoRA, and evaluated performance using weighted F1 scores across BERT and ALBERT models.", "result": "DP fine-tuned models achieved comparable performance (F1: 0.88 on MIMIC-CXR, 0.59 on CT-RATE) under moderate privacy, slightly below non-private baselines (0.90, 0.78).", "conclusion": "DP-LoRA enables privacy-preserving multi-abnormality classification, addressing challenges in using LLMs for sensitive medical data."}}
{"id": "2506.04453", "pdf": "https://arxiv.org/pdf/2506.04453", "abs": "https://arxiv.org/abs/2506.04453", "authors": ["Hasin Us Sami", "Swapneel Sen", "Amit K. Roy-Chowdhury", "Srikanth V. Krishnamurthy", "Basak Guler"], "title": "Gradient Inversion Attacks on Parameter-Efficient Fine-Tuning", "categories": ["eess.IV", "cs.CR", "cs.CV", "cs.LG"], "comment": "2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition\n  (CVPR 2025)", "summary": "Federated learning (FL) allows multiple data-owners to collaboratively train\nmachine learning models by exchanging local gradients, while keeping their\nprivate data on-device. To simultaneously enhance privacy and training\nefficiency, recently parameter-efficient fine-tuning (PEFT) of large-scale\npretrained models has gained substantial attention in FL. While keeping a\npretrained (backbone) model frozen, each user fine-tunes only a few lightweight\nmodules to be used in conjunction, to fit specific downstream applications.\nAccordingly, only the gradients with respect to these lightweight modules are\nshared with the server. In this work, we investigate how the privacy of the\nfine-tuning data of the users can be compromised via a malicious design of the\npretrained model and trainable adapter modules. We demonstrate gradient\ninversion attacks on a popular PEFT mechanism, the adapter, which allow an\nattacker to reconstruct local data samples of a target user, using only the\naccessible adapter gradients. Via extensive experiments, we demonstrate that a\nlarge batch of fine-tuning images can be retrieved with high fidelity. Our\nattack highlights the need for privacy-preserving mechanisms for PEFT, while\nopening up several future directions. Our code is available at\nhttps://github.com/info-ucr/PEFTLeak.", "AI": {"tldr": "The paper explores privacy risks in federated learning (FL) when using parameter-efficient fine-tuning (PEFT), showing how maliciously designed pretrained models and adapters can leak user data via gradient inversion attacks.", "motivation": "To highlight vulnerabilities in FL when employing PEFT, where users fine-tune lightweight modules while keeping the backbone model frozen, potentially exposing private data through shared gradients.", "method": "The study conducts gradient inversion attacks on adapter modules in PEFT, reconstructing local user data from accessible gradients.", "result": "Experiments show that fine-tuning images can be reconstructed with high fidelity, revealing significant privacy risks.", "conclusion": "The findings emphasize the need for privacy-preserving mechanisms in PEFT and suggest future research directions to mitigate such vulnerabilities."}}
{"id": "2506.04462", "pdf": "https://arxiv.org/pdf/2506.04462", "abs": "https://arxiv.org/abs/2506.04462", "authors": ["Apurv Verma", "NhatHai Phan", "Shubhendu Trivedi"], "title": "Watermarking Degrades Alignment in Language Models: Analysis and Mitigation", "categories": ["cs.CL", "cs.CR", "cs.LG", "I.2.7"], "comment": "Published at the 1st Workshop on GenAI Watermarking, collocated with\n  ICLR 2025. OpenReview: https://openreview.net/forum?id=SIBkIV48gF", "summary": "Watermarking techniques for large language models (LLMs) can significantly\nimpact output quality, yet their effects on truthfulness, safety, and\nhelpfulness remain critically underexamined. This paper presents a systematic\nanalysis of how two popular watermarking approaches-Gumbel and KGW-affect these\ncore alignment properties across four aligned LLMs. Our experiments reveal two\ndistinct degradation patterns: guard attenuation, where enhanced helpfulness\nundermines model safety, and guard amplification, where excessive caution\nreduces model helpfulness. These patterns emerge from watermark-induced shifts\nin token distribution, surfacing the fundamental tension that exists between\nalignment objectives.\n  To mitigate these degradations, we propose Alignment Resampling (AR), an\ninference-time sampling method that uses an external reward model to restore\nalignment. We establish a theoretical lower bound on the improvement in\nexpected reward score as the sample size is increased and empirically\ndemonstrate that sampling just 2-4 watermarked generations effectively recovers\nor surpasses baseline (unwatermarked) alignment scores. To overcome the limited\nresponse diversity of standard Gumbel watermarking, our modified implementation\nsacrifices strict distortion-freeness while maintaining robust detectability,\nensuring compatibility with AR. Experimental results confirm that AR\nsuccessfully recovers baseline alignment in both watermarking approaches, while\nmaintaining strong watermark detectability. This work reveals the critical\nbalance between watermark strength and model alignment, providing a simple\ninference-time solution to responsibly deploy watermarked LLMs in practice.", "AI": {"tldr": "The paper analyzes how watermarking techniques (Gumbel and KGW) affect LLM alignment properties (truthfulness, safety, helpfulness), identifies degradation patterns, and proposes Alignment Resampling (AR) to mitigate these issues.", "motivation": "To understand the underexamined effects of watermarking on LLM alignment properties and address the resulting degradation patterns.", "method": "Systematic analysis of two watermarking approaches (Gumbel and KGW) across four aligned LLMs, identifying degradation patterns (guard attenuation and amplification). Proposes AR, an inference-time sampling method using an external reward model.", "result": "Watermarking causes alignment degradation; AR with 2-4 samples recovers or surpasses baseline alignment scores. Modified Gumbel maintains detectability while improving diversity.", "conclusion": "AR effectively balances watermark strength and alignment, offering a practical solution for deploying watermarked LLMs responsibly."}}
{"id": "2506.04464", "pdf": "https://arxiv.org/pdf/2506.04464", "abs": "https://arxiv.org/abs/2506.04464", "authors": ["Oussama Ben Sghaier", "Rosalia Tufano", "Gabriele Bavota", "Houari Sahraoui"], "title": "Leveraging Reward Models for Guiding Code Review Comment Generation", "categories": ["cs.SE", "cs.LG"], "comment": null, "summary": "Code review is a crucial component of modern software development, involving\nthe evaluation of code quality, providing feedback on potential issues, and\nrefining the code to address identified problems. Despite these benefits, code\nreview can be rather time consuming, and influenced by subjectivity and human\nfactors. For these reasons, techniques to (partially) automate the code review\nprocess have been proposed in the literature. Among those, the ones exploiting\ndeep learning (DL) are able to tackle the generative aspect of code review, by\ncommenting on a given code as a human reviewer would do (i.e., comment\ngeneration task) or by automatically implementing code changes required to\naddress a reviewer's comment (i.e., code refinement task). In this paper, we\nintroduce CoRAL, a deep learning framework automating review comment generation\nby exploiting reinforcement learning with a reward mechanism considering both\nthe semantics of the generated comments as well as their usefulness as input\nfor other models automating the code refinement task. The core idea is that if\nthe DL model generates comments that are semantically similar to the expected\nones or can be successfully implemented by a second model specialized in code\nrefinement, these comments are likely to be meaningful and useful, thus\ndeserving a high reward in the reinforcement learning framework. We present\nboth quantitative and qualitative comparisons between the comments generated by\nCoRAL and those produced by the latest baseline techniques, highlighting the\neffectiveness and superiority of our approach.", "AI": {"tldr": "CoRAL is a DL framework using reinforcement learning to automate code review comment generation, focusing on semantic similarity and usefulness for code refinement.", "motivation": "Code review is time-consuming and subjective; automating it with DL can improve efficiency and consistency.", "method": "CoRAL employs reinforcement learning with a reward mechanism based on comment semantics and usefulness for code refinement.", "result": "CoRAL outperforms baseline techniques in generating meaningful and useful review comments.", "conclusion": "CoRAL effectively automates code review comment generation, demonstrating superiority over existing methods."}}
{"id": "2506.04480", "pdf": "https://arxiv.org/pdf/2506.04480", "abs": "https://arxiv.org/abs/2506.04480", "authors": ["Nina Vesseron", "Elsa Cazelles", "Alice Le Brigant", "Thierry Klein"], "title": "On the Wasserstein Geodesic Principal Component Analysis of probability measures", "categories": ["stat.ML", "cs.LG", "stat.ME"], "comment": null, "summary": "This paper focuses on Geodesic Principal Component Analysis (GPCA) on a\ncollection of probability distributions using the Otto-Wasserstein geometry.\nThe goal is to identify geodesic curves in the space of probability measures\nthat best capture the modes of variation of the underlying dataset. We first\naddress the case of a collection of Gaussian distributions, and show how to\nlift the computations in the space of invertible linear maps. For the more\ngeneral setting of absolutely continuous probability measures, we leverage a\nnovel approach to parameterizing geodesics in Wasserstein space with neural\nnetworks. Finally, we compare to classical tangent PCA through various examples\nand provide illustrations on real-world datasets.", "AI": {"tldr": "The paper introduces Geodesic Principal Component Analysis (GPCA) for probability distributions using Otto-Wasserstein geometry, focusing on Gaussian distributions and extending to general measures with neural networks.", "motivation": "To identify geodesic curves in probability measure spaces that capture dataset variations, improving upon classical methods.", "method": "Uses Otto-Wasserstein geometry for Gaussian distributions and neural networks for general measures, comparing to tangent PCA.", "result": "Demonstrates effective GPCA for Gaussian and general measures, validated with real-world examples.", "conclusion": "GPCA offers a robust framework for analyzing probability distributions, outperforming traditional PCA in certain cases."}}
{"id": "2506.04505", "pdf": "https://arxiv.org/pdf/2506.04505", "abs": "https://arxiv.org/abs/2506.04505", "authors": ["Nikita Oskolkov", "Huzhenyu Zhang", "Dmitry Makarov", "Dmitry Yudin", "Aleksandr Panov"], "title": "SGN-CIRL: Scene Graph-based Navigation with Curriculum, Imitation, and Reinforcement Learning", "categories": ["cs.RO", "cs.LG"], "comment": "7 pages, 11 figures", "summary": "The 3D scene graph models spatial relationships between objects, enabling the\nagent to efficiently navigate in a partially observable environment and predict\nthe location of the target object.This paper proposes an original framework\nnamed SGN-CIRL (3D Scene Graph-Based Reinforcement Learning Navigation) for\nmapless reinforcement learning-based robot navigation with learnable\nrepresentation of open-vocabulary 3D scene graph. To accelerate and stabilize\nthe training of reinforcement learning-based algorithms, the framework also\nemploys imitation learning and curriculum learning. The first one enables the\nagent to learn from demonstrations, while the second one structures the\ntraining process by gradually increasing task complexity from simple to more\nadvanced scenarios. Numerical experiments conducted in the Isaac Sim\nenvironment showed that using a 3D scene graph for reinforcement learning\nsignificantly increased the success rate in difficult navigation cases. The\ncode is open-sourced and available at: https://github.com/Xisonik/Aloha\\_graph.", "AI": {"tldr": "SGN-CIRL is a framework for mapless robot navigation using 3D scene graphs and reinforcement learning, enhanced by imitation and curriculum learning, showing improved success rates.", "motivation": "To enable efficient navigation and target prediction in partially observable environments using 3D scene graphs.", "method": "Combines 3D scene graphs with reinforcement learning, imitation learning, and curriculum learning for training.", "result": "Significantly increased success rates in difficult navigation tasks, validated in Isaac Sim.", "conclusion": "SGN-CIRL is effective for mapless navigation, with open-sourced code for further use."}}
{"id": "2506.04515", "pdf": "https://arxiv.org/pdf/2506.04515", "abs": "https://arxiv.org/abs/2506.04515", "authors": ["Salil Patel"], "title": "The Latent Space Hypothesis: Toward Universal Medical Representation Learning", "categories": ["q-bio.QM", "cs.AI", "cs.LG"], "comment": "51 pages, 12 figures. A position paper examining the latent space\n  hypothesis - the proposition that diverse medical data can be represented in\n  shared latent spaces reflecting fundamental biological processes. The paper\n  discusses theoretical foundations, reviews supporting evidence, and considers\n  potential implications for medical AI and representation learning", "summary": "Medical data range from genomic sequences and retinal photographs to\nstructured laboratory results and unstructured clinical narratives. Although\nthese modalities appear disparate, many encode convergent information about a\nsingle underlying physiological state. The Latent Space Hypothesis frames each\nobservation as a projection of a unified, hierarchically organized manifold --\nmuch like shadows cast by the same three-dimensional object. Within this\nlearned geometric representation, an individual's health status occupies a\npoint, disease progression traces a trajectory, and therapeutic intervention\ncorresponds to a directed vector. Interpreting heterogeneous evidence in a\nshared space provides a principled way to re-examine eponymous conditions --\nsuch as Parkinson's or Crohn's -- that often mask multiple pathophysiological\nentities and involve broader anatomical domains than once believed. By\nrevealing sub-trajectories and patient-specific directions of change, the\nframework supplies a quantitative rationale for personalised diagnosis,\nlongitudinal monitoring, and tailored treatment, moving clinical practice away\nfrom grouping by potentially misleading labels toward navigation of each\nperson's unique trajectory. Challenges remain -- bias amplification, data\nscarcity for rare disorders, privacy, and the correlation-causation divide --\nbut scale-aware encoders, continual learning on longitudinal data streams, and\nperturbation-based validation offer plausible paths forward.", "AI": {"tldr": "The paper proposes the Latent Space Hypothesis to unify diverse medical data into a shared geometric representation for personalized healthcare, addressing challenges like bias and data scarcity.", "motivation": "To integrate heterogeneous medical data (genomic, imaging, lab results, narratives) into a unified framework for better understanding diseases and enabling personalized care.", "method": "The Latent Space Hypothesis models health data as projections of a unified manifold, where health status, disease progression, and interventions are represented as points, trajectories, and vectors.", "result": "The framework enables re-examination of diseases (e.g., Parkinson's, Crohn's) by revealing sub-trajectories and patient-specific changes, supporting personalized diagnosis and treatment.", "conclusion": "The approach shifts clinical practice from labels to individualized trajectories, with challenges like bias and data scarcity addressed through scale-aware encoders and continual learning."}}
{"id": "2506.04539", "pdf": "https://arxiv.org/pdf/2506.04539", "abs": "https://arxiv.org/abs/2506.04539", "authors": ["Kordel K. France", "Ovidiu Daescu", "Anirban Paul", "Shalini Prasad"], "title": "Olfactory Inertial Odometry: Sensor Calibration and Drift Compensation", "categories": ["cs.RO", "cs.ET", "cs.LG", "cs.SY", "eess.SY"], "comment": "Published as a full conference paper at the 2025 IEEE International\n  Symposium on Inertial Sensors & Systems", "summary": "Visual inertial odometry (VIO) is a process for fusing visual and kinematic\ndata to understand a machine's state in a navigation task. Olfactory inertial\nodometry (OIO) is an analog to VIO that fuses signals from gas sensors with\ninertial data to help a robot navigate by scent. Gas dynamics and environmental\nfactors introduce disturbances into olfactory navigation tasks that can make\nOIO difficult to facilitate. With our work here, we define a process for\ncalibrating a robot for OIO that generalizes to several olfaction sensor types.\nOur focus is specifically on calibrating OIO for centimeter-level accuracy in\nlocalizing an odor source on a slow-moving robot platform to demonstrate use\ncases in robotic surgery and touchless security screening. We demonstrate our\nprocess for OIO calibration on a real robotic arm and show how this calibration\nimproves performance over a cold-start olfactory navigation task.", "AI": {"tldr": "The paper introduces a calibration process for Olfactory Inertial Odometry (OIO) to improve odor source localization accuracy for slow-moving robots, with applications in robotic surgery and security screening.", "motivation": "OIO faces challenges due to gas dynamics and environmental disturbances, making accurate navigation difficult. The work aims to generalize calibration for various olfaction sensors.", "method": "The authors define a calibration process for OIO, tested on a real robotic arm, to achieve centimeter-level accuracy in odor source localization.", "result": "The calibration process improves performance over a cold-start olfactory navigation task, demonstrating practical use cases.", "conclusion": "The proposed OIO calibration method enhances accuracy and generalizability, proving useful for specific robotic applications."}}
{"id": "2506.04540", "pdf": "https://arxiv.org/pdf/2506.04540", "abs": "https://arxiv.org/abs/2506.04540", "authors": ["Kordel K. France"], "title": "Chronoamperometry with Room-Temperature Ionic Liquids: Sub-Second Inference Techniques", "categories": ["cs.RO", "cs.LG", "physics.chem-ph", "physics.ins-det"], "comment": "Published at IEEE BioSensors 2025", "summary": "Chronoamperometry (CA) is a fundamental electrochemical technique used for\nquantifying redox-active species. However, in room-temperature ionic liquids\n(RTILs), the high viscosity and slow mass transport often lead to extended\nmeasurement durations. This paper presents a novel mathematical regression\napproach that reduces CA measurement windows to under 1 second, significantly\nfaster than previously reported methods, which typically require 1-4 seconds or\nlonger. By applying an inference algorithm to the initial transient current\nresponse, this method accurately predicts steady-state electrochemical\nparameters without requiring additional hardware modifications. The approach is\nvalidated through comparison with standard chronoamperometric techniques and is\ndemonstrated to maintain reasonable accuracy while dramatically reducing data\nacquisition time. The implications of this technique are explored in analytical\nchemistry, sensor technology, and battery science, where rapid electrochemical\nquantification is critical. Our technique is focused on enabling faster\nmultiplexing of chronoamperometric measurements for rapid olfactory and\nelectrochemical analysis.", "AI": {"tldr": "A novel regression method reduces chronoamperometry measurement time in RTILs to under 1 second, enabling faster electrochemical analysis without hardware changes.", "motivation": "High viscosity and slow mass transport in RTILs prolong chronoamperometry measurements, necessitating a faster method.", "method": "Uses a mathematical regression approach and inference algorithm on initial transient current to predict steady-state parameters.", "result": "Validated against standard techniques, the method maintains accuracy while reducing measurement time significantly.", "conclusion": "The technique enables rapid electrochemical quantification, benefiting analytical chemistry, sensors, and battery science."}}
{"id": "2506.04544", "pdf": "https://arxiv.org/pdf/2506.04544", "abs": "https://arxiv.org/abs/2506.04544", "authors": ["Charles Hong", "Brendan Roberts", "Huijae An", "Alex Um", "Advay Ratan", "Yakun Sophia Shao"], "title": "hdl2v: A Code Translation Dataset for Enhanced LLM Verilog Generation", "categories": ["cs.AR", "cs.AI", "cs.LG", "cs.PL"], "comment": null, "summary": "Large language models (LLMs) are playing an increasingly large role in\ndomains such as code generation, including hardware code generation, where\nVerilog is the key language. However, the amount of publicly available Verilog\ncode pales in comparison to the amount of code available for software languages\nlike Python. In this work, we present hdl2v (\"HDL-to-Verilog\"), a dataset which\nseeks to increase the amount of available human-written Verilog data by\ntranslating or compiling three other hardware description languages - VHDL,\nChisel, and PyMTL3 - to Verilog. Furthermore, we demonstrate the value of hdl2v\nin enhancing LLM Verilog generation by improving performance of a 32\nbillion-parameter open-weight model by up to 23% (pass@10) in VerilogEvalV2,\nwithout utilizing any data augmentation or knowledge distillation from larger\nmodels. We also show hdl2v's ability to boost the performance of a data\naugmentation-based fine-tuning approach by 63%. Finally, we characterize and\nanalyze our dataset to better understand which characteristics of\nHDL-to-Verilog datasets can be expanded upon in future work for even better\nperformance.", "AI": {"tldr": "The paper introduces hdl2v, a dataset translating VHDL, Chisel, and PyMTL3 to Verilog to address the scarcity of human-written Verilog data. It demonstrates significant improvements in LLM Verilog generation performance.", "motivation": "The lack of publicly available Verilog code compared to software languages like Python limits LLM performance in hardware code generation.", "method": "hdl2v translates three hardware description languages (VHDL, Chisel, PyMTL3) to Verilog, creating a larger dataset for training.", "result": "hdl2v improves a 32B-parameter model's performance by 23% (pass@10) and boosts a data augmentation-based approach by 63%.", "conclusion": "hdl2v effectively enhances LLM Verilog generation, and future work can expand on dataset characteristics for further improvements."}}
{"id": "2506.04554", "pdf": "https://arxiv.org/pdf/2506.04554", "abs": "https://arxiv.org/abs/2506.04554", "authors": ["Hao Huang", "Zelda B. Zabinsky"], "title": "Non-linear Multi-objective Optimization with Probabilistic Branch and Bound", "categories": ["math.OC", "cs.LG"], "comment": "26 pages, 5 Figures", "summary": "A multiple objective simulation optimization algorithm named Multiple\nObjective Probabilistic Branch and Bound with Single Observation (MOPBnB(so))\nis presented for approximating the Pareto optimal set and the associated\nefficient frontier for stochastic multi-objective optimization problems.\nMOPBnB(so) evaluates a noisy function exactly once at any solution and uses\nneighboring solutions to estimate the objective functions, in contrast to a\nvariant that uses multiple replications at a solution to estimate the objective\nfunctions. A finite-time performance analysis for deterministic multi-objective\nproblems provides a bound on the probability that MOPBnB(so) captures the\nPareto optimal set. Asymptotic convergence of MOPBnB(so) on stochastic problems\nis derived, in that the algorithm captures the Pareto optimal set and the\nestimations converge to the true objective function values. Numerical results\nreveal that the variant with multiple replications is extremely intensive in\nterms of computational resources compared to MOPBnB(so). In addition, numerical\nresults show that MOPBnB(so) outperforms a genetic algorithm NSGA-II on test\nproblems.", "AI": {"tldr": "MOPBnB(so) is a simulation optimization algorithm for stochastic multi-objective problems, using single observations and neighboring solutions to estimate objectives. It outperforms NSGA-II and is computationally efficient.", "motivation": "To address the computational intensity of multiple replications in stochastic multi-objective optimization by proposing a single-observation approach.", "method": "Uses probabilistic branch and bound with single observations and neighboring solutions for estimation. Analyzes finite-time performance and asymptotic convergence.", "result": "Outperforms NSGA-II and is computationally efficient compared to multiple-replication variants.", "conclusion": "MOPBnB(so) is effective for approximating Pareto optimal sets in stochastic multi-objective problems with reduced computational cost."}}
{"id": "2506.04592", "pdf": "https://arxiv.org/pdf/2506.04592", "abs": "https://arxiv.org/abs/2506.04592", "authors": ["Chengwu Liu", "Ye Yuan", "Yichun Yin", "Yan Xu", "Xin Xu", "Zaoyu Chen", "Yasheng Wang", "Lifeng Shang", "Qun Liu", "Ming Zhang"], "title": "Safe: Enhancing Mathematical Reasoning in Large Language Models via Retrospective Step-aware Formal Verification", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted in ACL 2025", "summary": "Chain-of-Thought (CoT) prompting has become the de facto method to elicit\nreasoning capabilities from large language models (LLMs). However, to mitigate\nhallucinations in CoT that are notoriously difficult to detect, current methods\nsuch as process reward models (PRMs) or self-consistency operate as opaque\nboxes and do not provide checkable evidence for their judgments, possibly\nlimiting their effectiveness. To address this issue, we draw inspiration from\nthe idea that \"the gold standard for supporting a mathematical claim is to\nprovide a proof\". We propose a retrospective, step-aware formal verification\nframework $Safe$. Rather than assigning arbitrary scores, we strive to\narticulate mathematical claims in formal mathematical language Lean 4 at each\nreasoning step and provide formal proofs to identify hallucinations. We\nevaluate our framework $Safe$ across multiple language models and various\nmathematical datasets, demonstrating a significant performance improvement\nwhile offering interpretable and verifiable evidence. We also propose\n$FormalStep$ as a benchmark for step correctness theorem proving with $30,809$\nformal statements. To the best of our knowledge, our work represents the first\nendeavor to utilize formal mathematical language Lean 4 for verifying natural\nlanguage content generated by LLMs, aligning with the reason why formal\nmathematical languages were created in the first place: to provide a robust\nfoundation for hallucination-prone human-written proofs.", "AI": {"tldr": "The paper introduces $Safe$, a formal verification framework using Lean 4 to detect and mitigate hallucinations in Chain-of-Thought reasoning by providing step-aware proofs.", "motivation": "To address the lack of verifiable evidence in current methods for detecting hallucinations in Chain-of-Thought prompting, inspired by the rigor of mathematical proofs.", "method": "Proposes $Safe$, a retrospective, step-aware formal verification framework using Lean 4 to articulate and verify reasoning steps.", "result": "Demonstrates significant performance improvement and introduces $FormalStep$, a benchmark for step correctness with 30,809 formal statements.", "conclusion": "The work pioneers the use of Lean 4 for verifying LLM-generated content, aligning with formal languages' original purpose of ensuring robust proof foundations."}}
{"id": "2506.04602", "pdf": "https://arxiv.org/pdf/2506.04602", "abs": "https://arxiv.org/abs/2506.04602", "authors": ["Haifeng Sun", "Yu Xiong", "Runze Wu", "Kai Wang", "Lan Zhang", "Changjie Fan", "Shaojie Tang", "Xiang-Yang Li"], "title": "MVP-Shapley: Feature-based Modeling for Evaluating the Most Valuable Player in Basketball", "categories": ["cs.GT", "cs.LG"], "comment": null, "summary": "The burgeoning growth of the esports and multiplayer online gaming community\nhas highlighted the critical importance of evaluating the Most Valuable Player\n(MVP). The establishment of an explainable and practical MVP evaluation method\nis very challenging. In our study, we specifically focus on play-by-play data,\nwhich records related events during the game, such as assists and points. We\naim to address the challenges by introducing a new MVP evaluation framework,\ndenoted as \\oursys, which leverages Shapley values. This approach encompasses\nfeature processing, win-loss model training, Shapley value allocation, and MVP\nranking determination based on players' contributions. Additionally, we\noptimize our algorithm to align with expert voting results from the perspective\nof causality. Finally, we substantiated the efficacy of our method through\nvalidation using the NBA dataset and the Dunk City Dynasty dataset and\nimplemented online deployment in the industry.", "AI": {"tldr": "The paper introduces a new MVP evaluation framework, \\oursys, using Shapley values to assess player contributions in esports and multiplayer games, validated with NBA and Dunk City Dynasty datasets.", "motivation": "The rapid growth of esports and multiplayer gaming necessitates a reliable and explainable MVP evaluation method, which current approaches lack.", "method": "The proposed framework, \\oursys, processes play-by-play data, trains a win-loss model, allocates Shapley values, and ranks players by contribution. It also optimizes alignment with expert voting using causal analysis.", "result": "The method was validated using NBA and Dunk City Dynasty datasets and successfully deployed in the industry.", "conclusion": "\\oursys provides a practical and explainable MVP evaluation framework, addressing the challenges in player contribution assessment."}}
{"id": "2506.04613", "pdf": "https://arxiv.org/pdf/2506.04613", "abs": "https://arxiv.org/abs/2506.04613", "authors": ["Li Liu", "Heng Yong"], "title": "DeePoly: A High-Order Accuracy and Efficiency Deep-Polynomial Framework for Scientific Machine Learning", "categories": ["cs.AI", "cs.LG"], "comment": "for associated mpeg file, see http://github.com/bfly123/DeePoly", "summary": "Recently, machine learning methods have gained significant traction in\nscientific computing, particularly for solving Partial Differential Equations\n(PDEs). However, methods based on deep neural networks (DNNs) often lack\nconvergence guarantees and computational efficiency compared to traditional\nnumerical schemes. This work introduces DeePoly, a novel framework that\ntransforms the solution paradigm from pure non-convex parameter optimization to\na two-stage approach: first employing a DNN to capture complex global features,\nfollowed by linear space optimization with combined DNN-extracted features\n(Scoper) and polynomial basis functions (Sniper). This strategic combination\nleverages the complementary strengths of both methods -- DNNs excel at\napproximating complex global features (i.e., high-gradient features) and\nstabilize the polynomial approximation while polynomial bases provide\nhigh-precision local corrections with convergence guarantees. Theoretical\nanalysis and numerical experiments demonstrate that this approach significantly\nenhances both high-order accuracy and efficiency across diverse problem types\nwhile maintaining mesh-free and scheme-free properties. This paper also serves\nas a theoretical exposition for the open-source project DeePoly.", "AI": {"tldr": "DeePoly is a novel framework combining DNNs and polynomial bases to solve PDEs, improving accuracy and efficiency while maintaining theoretical guarantees.", "motivation": "Traditional DNN-based methods for PDEs lack convergence guarantees and efficiency compared to numerical schemes.", "method": "DeePoly uses a two-stage approach: DNNs for global features and polynomial bases for local corrections.", "result": "The method achieves high-order accuracy and efficiency across diverse problems.", "conclusion": "DeePoly offers a robust, mesh-free solution for PDEs, supported by theoretical and experimental results."}}
{"id": "2506.04624", "pdf": "https://arxiv.org/pdf/2506.04624", "abs": "https://arxiv.org/abs/2506.04624", "authors": ["Takashi Wada", "Yuki Hirakawa", "Ryotaro Shimizu", "Takahiro Kawashima", "Yuki Saito"], "title": "Static Word Embeddings for Sentence Semantic Representation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "15 pages", "summary": "We propose new static word embeddings optimised for sentence semantic\nrepresentation. We first extract word embeddings from a pre-trained Sentence\nTransformer, and improve them with sentence-level principal component analysis,\nfollowed by either knowledge distillation or contrastive learning. During\ninference, we represent sentences by simply averaging word embeddings, which\nrequires little computational cost. We evaluate models on both monolingual and\ncross-lingual tasks and show that our model substantially outperforms existing\nstatic models on sentence semantic tasks, and even rivals a basic Sentence\nTransformer model (SimCSE) on some data sets. Lastly, we perform a variety of\nanalyses and show that our method successfully removes word embedding\ncomponents that are irrelevant to sentence semantics, and adjusts the vector\nnorms based on the influence of words on sentence semantics.", "AI": {"tldr": "Proposes static word embeddings optimized for sentence semantics, outperforming existing models and rivaling basic Sentence Transformers.", "motivation": "To improve sentence semantic representation with computationally efficient static word embeddings.", "method": "Extracts word embeddings from a pre-trained Sentence Transformer, enhances them with sentence-level PCA, and refines via knowledge distillation or contrastive learning. Sentences are represented by averaging word embeddings.", "result": "Substantially outperforms existing static models on semantic tasks and rivals basic Sentence Transformers in some datasets.", "conclusion": "The method effectively removes irrelevant word embedding components and adjusts vector norms based on word influence, enhancing sentence semantics."}}
{"id": "2506.04626", "pdf": "https://arxiv.org/pdf/2506.04626", "abs": "https://arxiv.org/abs/2506.04626", "authors": ["Haochen Zhang", "Zhong Zheng", "Lingzhou Xue"], "title": "Regret-Optimal Q-Learning with Low Cost for Single-Agent and Federated Reinforcement Learning", "categories": ["stat.ML", "cs.LG"], "comment": "arXiv admin note: text overlap with arXiv:2502.02859", "summary": "Motivated by real-world settings where data collection and policy deployment\n-- whether for a single agent or across multiple agents -- are costly, we study\nthe problem of on-policy single-agent reinforcement learning (RL) and federated\nRL (FRL) with a focus on minimizing burn-in costs (the sample sizes needed to\nreach near-optimal regret) and policy switching or communication costs. In\nparallel finite-horizon episodic Markov Decision Processes (MDPs) with $S$\nstates and $A$ actions, existing methods either require superlinear burn-in\ncosts in $S$ and $A$ or fail to achieve logarithmic switching or communication\ncosts. We propose two novel model-free RL algorithms -- Q-EarlySettled-LowCost\nand FedQ-EarlySettled-LowCost -- that are the first in the literature to\nsimultaneously achieve: (i) the best near-optimal regret among all known\nmodel-free RL or FRL algorithms, (ii) low burn-in cost that scales linearly\nwith $S$ and $A$, and (iii) logarithmic policy switching cost for single-agent\nRL or communication cost for FRL. Additionally, we establish gap-dependent\ntheoretical guarantees for both regret and switching/communication costs,\nimproving or matching the best-known gap-dependent bounds.", "AI": {"tldr": "The paper introduces two model-free RL algorithms, Q-EarlySettled-LowCost and FedQ-EarlySettled-LowCost, to minimize burn-in and switching/communication costs in single-agent and federated RL, achieving near-optimal regret and logarithmic costs.", "motivation": "Addressing costly data collection and policy deployment in RL and FRL by minimizing burn-in and switching/communication costs.", "method": "Proposes two model-free algorithms for single-agent and federated RL, focusing on linear burn-in costs and logarithmic switching/communication costs.", "result": "Achieves best near-optimal regret, low burn-in costs (linear in S and A), and logarithmic switching/communication costs.", "conclusion": "The algorithms outperform existing methods, providing efficient solutions for RL and FRL with theoretical guarantees."}}
{"id": "2506.04658", "pdf": "https://arxiv.org/pdf/2506.04658", "abs": "https://arxiv.org/abs/2506.04658", "authors": ["J\u0119drzej Maskiewicz", "Pawe\u0142 Sakowski"], "title": "Can Artificial Intelligence Trade the Stock Market?", "categories": ["q-fin.TR", "cs.LG", "q-fin.CP"], "comment": null, "summary": "The paper explores the use of Deep Reinforcement Learning (DRL) in stock\nmarket trading, focusing on two algorithms: Double Deep Q-Network (DDQN) and\nProximal Policy Optimization (PPO) and compares them with Buy and Hold\nbenchmark. It evaluates these algorithms across three currency pairs, the S&P\n500 index and Bitcoin, on the daily data in the period of 2019-2023. The\nresults demonstrate DRL's effectiveness in trading and its ability to manage\nrisk by strategically avoiding trades in unfavorable conditions, providing a\nsubstantial edge over classical approaches, based on supervised learning in\nterms of risk-adjusted returns.", "AI": {"tldr": "The paper compares DDQN and PPO in stock trading, showing DRL's superiority over classical methods in risk-adjusted returns.", "motivation": "To evaluate the effectiveness of DRL algorithms (DDQN and PPO) in stock market trading compared to traditional Buy and Hold strategies.", "method": "Uses DDQN and PPO on daily data (2019-2023) for currency pairs, S&P 500, and Bitcoin, benchmarking against Buy and Hold.", "result": "DRL outperforms classical methods, managing risk by avoiding unfavorable trades, yielding better risk-adjusted returns.", "conclusion": "DRL, especially DDQN and PPO, offers a significant advantage in trading by optimizing risk and returns."}}
{"id": "2506.04667", "pdf": "https://arxiv.org/pdf/2506.04667", "abs": "https://arxiv.org/abs/2506.04667", "authors": ["Osayamen Jonathan Aimuyo", "Byungsoo Oh", "Rachee Singh"], "title": "FlashDMoE: Fast Distributed MoE in a Single Kernel", "categories": ["cs.DC", "cs.AR", "cs.LG"], "comment": "In submission. See code at https://github.com/osayamenja/Aristos", "summary": "The computational sparsity of Mixture-of-Experts (MoE) models enables\nsub-linear growth in compute cost as model size increases, offering a scalable\npath to training massive neural networks. However, existing implementations\nsuffer from \\emph{low GPU utilization}, \\emph{significant latency overhead},\nand a fundamental \\emph{inability to leverage task locality}, primarily due to\nCPU-managed scheduling, host-initiated communication, and frequent kernel\nlaunches. To overcome these limitations, we develop FlashDMoE, a fully\nGPU-resident MoE operator that fuses expert computation and inter-GPU\ncommunication into a \\emph{single persistent GPU kernel}. FlashDMoE enables\nfine-grained pipelining of dispatch, compute, and combine phases, eliminating\nlaunch overheads and reducing idle gaps. Its device-initiated communication\nprotocol introduces \\emph{payload-efficient} data transfers, significantly\nshrinking buffer sizes in sparsely activated MoE layers. When evaluated on a\nsingle 8-H100 GPU node with MoE models having up to 128 experts and 16K token\nsequences, FlashDMoE achieves up to \\textbf{6}x lower latency, \\textbf{5,7}x\nhigher throughput, \\textbf{4}x better weak scaling efficiency, and \\textbf{9}x\nhigher GPU utilization compared to state-of-the-art baselines, despite using\nFP32 while baselines use FP16. FlashDMoE demonstrates that principled GPU\nkernel-hardware co-design is key to unlocking the performance ceiling of\nlarge-scale distributed ML workloads.", "AI": {"tldr": "FlashDMoE is a GPU-resident MoE operator that improves efficiency by fusing computation and communication into a single kernel, achieving significant performance gains over existing methods.", "motivation": "Existing MoE implementations suffer from low GPU utilization, latency overhead, and inability to leverage task locality due to CPU-managed scheduling and frequent kernel launches.", "method": "Developed FlashDMoE, a fully GPU-resident MoE operator that fuses expert computation and inter-GPU communication into a single persistent GPU kernel, enabling fine-grained pipelining and device-initiated communication.", "result": "Achieves up to 6x lower latency, 5.7x higher throughput, 4x better weak scaling efficiency, and 9x higher GPU utilization compared to state-of-the-art baselines.", "conclusion": "Principled GPU kernel-hardware co-design is crucial for unlocking the performance ceiling of large-scale distributed ML workloads."}}
{"id": "2506.04676", "pdf": "https://arxiv.org/pdf/2506.04676", "abs": "https://arxiv.org/abs/2506.04676", "authors": ["Jing-En Huang", "I-Sheng Fang", "Tzuhsuan Huang", "Chih-Yu Wang", "Jun-Cheng Chen"], "title": "Gen-n-Val: Agentic Image Data Generation and Validation", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MA"], "comment": null, "summary": "Recently, Large Language Models (LLMs) and Vision Large Language Models\n(VLLMs) have demonstrated impressive performance as agents across various tasks\nwhile data scarcity and label noise remain significant challenges in computer\nvision tasks, such as object detection and instance segmentation. A common\nsolution for resolving these issues is to generate synthetic data. However,\ncurrent synthetic data generation methods struggle with issues, such as\nmultiple objects per mask, inaccurate segmentation, and incorrect category\nlabels, limiting their effectiveness. To address these issues, we introduce\nGen-n-Val, a novel agentic data generation framework that leverages Layer\nDiffusion (LD), LLMs, and VLLMs to produce high-quality, single-object masks\nand diverse backgrounds. Gen-n-Val consists of two agents: (1) The LD prompt\nagent, an LLM, optimizes prompts for LD to generate high-quality foreground\ninstance images and segmentation masks. These optimized prompts ensure the\ngeneration of single-object synthetic data with precise instance masks and\nclean backgrounds. (2) The data validation agent, a VLLM, which filters out\nlow-quality synthetic instance images. The system prompts for both agents are\nrefined through TextGrad. Additionally, we use image harmonization to combine\nmultiple instances within scenes. Compared to state-of-the-art synthetic data\napproaches like MosaicFusion, our approach reduces invalid synthetic data from\n50% to 7% and improves performance by 1% mAP on rare classes in COCO instance\nsegmentation with YOLOv9c and YOLO11m. Furthermore, Gen-n-Val shows significant\nimprovements (7. 1% mAP) over YOLO-Worldv2-M in open-vocabulary object\ndetection benchmarks with YOLO11m. Moreover, Gen-n-Val improves the performance\nof YOLOv9 and YOLO11 families in instance segmentation and object detection.", "AI": {"tldr": "Gen-n-Val is a novel framework using LLMs and VLLMs to generate high-quality synthetic data for computer vision tasks, reducing invalid data and improving performance.", "motivation": "Addressing data scarcity and label noise in computer vision tasks by improving synthetic data generation.", "method": "Leverages Layer Diffusion, LLMs, and VLLMs to create single-object masks and diverse backgrounds, with agents for prompt optimization and data validation.", "result": "Reduces invalid synthetic data from 50% to 7% and improves performance by 1% mAP on rare classes in COCO instance segmentation.", "conclusion": "Gen-n-Val significantly enhances synthetic data quality and model performance in instance segmentation and object detection."}}
{"id": "2506.04689", "pdf": "https://arxiv.org/pdf/2506.04689", "abs": "https://arxiv.org/abs/2506.04689", "authors": ["Thao Nguyen", "Yang Li", "Olga Golovneva", "Luke Zettlemoyer", "Sewoong Oh", "Ludwig Schmidt", "Xian Li"], "title": "Recycling the Web: A Method to Enhance Pre-training Data Quality and Quantity for Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Scaling laws predict that the performance of large language models improves\nwith increasing model size and data size. In practice, pre-training has been\nrelying on massive web crawls, using almost all data sources publicly available\non the internet so far. However, this pool of natural data does not grow at the\nsame rate as the compute supply. Furthermore, the availability of high-quality\ntexts is even more limited: data filtering pipelines often remove up to 99% of\nthe initial web scrapes to achieve state-of-the-art. To address the \"data wall\"\nof pre-training scaling, our work explores ways to transform and recycle data\ndiscarded in existing filtering processes. We propose REWIRE, REcycling the Web\nwith guIded REwrite, a method to enrich low-quality documents so that they\ncould become useful for training. This in turn allows us to increase the\nrepresentation of synthetic data in the final pre-training set. Experiments at\n1B, 3B and 7B scales of the DCLM benchmark show that mixing high-quality raw\ntexts and our rewritten texts lead to 1.0, 1.3 and 2.5 percentage points\nimprovement respectively across 22 diverse tasks, compared to training on only\nfiltered web data. Training on the raw-synthetic data mix is also more\neffective than having access to 2x web data. Through further analysis, we\ndemonstrate that about 82% of the mixed in texts come from transforming\nlower-quality documents that would otherwise be discarded. REWIRE also\noutperforms related approaches of generating synthetic data, including\nWikipedia-style paraphrasing, question-answer synthesizing and knowledge\nextraction. These results suggest that recycling web texts holds the potential\nfor being a simple and effective approach for scaling pre-training data.", "AI": {"tldr": "REWIRE recycles low-quality web data by enriching it, improving model performance across tasks compared to using only filtered data.", "motivation": "Address the 'data wall' in pre-training by recycling discarded low-quality web data to supplement high-quality data.", "method": "Proposes REWIRE, a method to rewrite and enrich low-quality documents for training, mixing them with high-quality texts.", "result": "Improves performance by 1.0-2.5 percentage points across 22 tasks and outperforms other synthetic data methods.", "conclusion": "Recycling web texts is a simple and effective way to scale pre-training data."}}
{"id": "2506.04717", "pdf": "https://arxiv.org/pdf/2506.04717", "abs": "https://arxiv.org/abs/2506.04717", "authors": ["Babar Hussain", "Qiang Liu", "Gang Chen", "Bihai She", "Dahai Yu"], "title": "Using In-Context Learning for Automatic Defect Labelling of Display Manufacturing Data", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "This paper presents an AI-assisted auto-labeling system for display panel\ndefect detection that leverages in-context learning capabilities. We adopt and\nenhance the SegGPT architecture with several domain-specific training\ntechniques and introduce a scribble-based annotation mechanism to streamline\nthe labeling process. Our two-stage training approach, validated on industrial\ndisplay panel datasets, demonstrates significant improvements over the baseline\nmodel, achieving an average IoU increase of 0.22 and a 14% improvement in\nrecall across multiple product types, while maintaining approximately 60%\nauto-labeling coverage. Experimental results show that models trained on our\nauto-labeled data match the performance of those trained on human-labeled data,\noffering a practical solution for reducing manual annotation efforts in\nindustrial inspection systems.", "AI": {"tldr": "An AI-assisted auto-labeling system for display panel defect detection using enhanced SegGPT and scribble-based annotation, achieving significant performance improvements and reducing manual effort.", "motivation": "To streamline the labeling process in industrial inspection systems and reduce manual annotation efforts by leveraging AI.", "method": "Adopts and enhances SegGPT with domain-specific techniques, introduces scribble-based annotation, and uses a two-stage training approach.", "result": "Achieves an average IoU increase of 0.22, 14% recall improvement, and 60% auto-labeling coverage, matching human-labeled data performance.", "conclusion": "The system offers a practical solution for industrial inspection, reducing reliance on manual labeling while maintaining high accuracy."}}
{"id": "2506.04774", "pdf": "https://arxiv.org/pdf/2506.04774", "abs": "https://arxiv.org/abs/2506.04774", "authors": ["Jingyu Hu", "Mengyue Yang", "Mengnan Du", "Weiru Liu"], "title": "Fine-Grained Interpretation of Political Opinions in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Studies of LLMs' political opinions mainly rely on evaluations of their\nopen-ended responses. Recent work indicates that there is a misalignment\nbetween LLMs' responses and their internal intentions. This motivates us to\nprobe LLMs' internal mechanisms and help uncover their internal political\nstates. Additionally, we found that the analysis of LLMs' political opinions\noften relies on single-axis concepts, which can lead to concept confounds. In\nthis work, we extend the single-axis to multi-dimensions and apply\ninterpretable representation engineering techniques for more transparent LLM\npolitical concept learning. Specifically, we designed a four-dimensional\npolitical learning framework and constructed a corresponding dataset for\nfine-grained political concept vector learning. These vectors can be used to\ndetect and intervene in LLM internals. Experiments are conducted on eight\nopen-source LLMs with three representation engineering techniques. Results show\nthese vectors can disentangle political concept confounds. Detection tasks\nvalidate the semantic meaning of the vectors and show good generalization and\nrobustness in OOD settings. Intervention Experiments show these vectors can\nintervene in LLMs to generate responses with different political leanings.", "AI": {"tldr": "The paper explores LLMs' political opinions by probing their internal mechanisms and extending single-axis analysis to multi-dimensions using interpretable representation engineering.", "motivation": "Misalignment between LLMs' responses and internal intentions, and reliance on single-axis concepts leading to confounds, motivate the study.", "method": "A four-dimensional political learning framework and dataset are designed for fine-grained concept vector learning, tested on eight LLMs with three representation engineering techniques.", "result": "Vectors disentangle political concept confounds, show semantic meaning in detection tasks, and enable intervention for responses with different political leanings.", "conclusion": "The approach enhances transparency and control over LLMs' political opinions through multi-dimensional analysis and intervention."}}
{"id": "2506.04788", "pdf": "https://arxiv.org/pdf/2506.04788", "abs": "https://arxiv.org/abs/2506.04788", "authors": ["Jisu An", "Junseok Lee", "Jeoungeun Lee", "Yongseok Son"], "title": "Towards LLM-Centric Multimodal Fusion: A Survey on Integration Strategies and Techniques", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "18 pages, 3 figures, 3 tables", "summary": "The rapid progress of Multimodal Large Language Models(MLLMs) has transformed\nthe AI landscape. These models combine pre-trained LLMs with various modality\nencoders. This integration requires a systematic understanding of how different\nmodalities connect to the language backbone. Our survey presents an LLM-centric\nanalysis of current approaches. We examine methods for transforming and\naligning diverse modal inputs into the language embedding space. This addresses\na significant gap in existing literature. We propose a classification framework\nfor MLLMs based on three key dimensions. First, we examine architectural\nstrategies for modality integration. This includes both the specific\nintegration mechanisms and the fusion level. Second, we categorize\nrepresentation learning techniques as either joint or coordinate\nrepresentations. Third, we analyze training paradigms, including training\nstrategies and objective functions. By examining 125 MLLMs developed between\n2021 and 2025, we identify emerging patterns in the field. Our taxonomy\nprovides researchers with a structured overview of current integration\ntechniques. These insights aim to guide the development of more robust\nmultimodal integration strategies for future models built on pre-trained\nfoundations.", "AI": {"tldr": "The paper surveys Multimodal Large Language Models (MLLMs), analyzing integration methods, representation learning, and training paradigms, proposing a classification framework based on 125 models.", "motivation": "To address the gap in systematic understanding of how diverse modalities connect to language backbones in MLLMs.", "method": "Examines architectural strategies, representation learning techniques, and training paradigms, analyzing 125 MLLMs.", "result": "Identifies emerging patterns and proposes a taxonomy for current integration techniques.", "conclusion": "Provides insights to guide future development of robust multimodal integration strategies for pre-trained models."}}
{"id": "2506.04790", "pdf": "https://arxiv.org/pdf/2506.04790", "abs": "https://arxiv.org/abs/2506.04790", "authors": ["Yusuke Matsui"], "title": "LotusFilter: Fast Diverse Nearest Neighbor Search via a Learned Cutoff Table", "categories": ["cs.CV", "cs.IR", "cs.LG"], "comment": "CVPR 2025. GitHub: https://github.com/matsui528/lotf", "summary": "Approximate nearest neighbor search (ANNS) is an essential building block for\napplications like RAG but can sometimes yield results that are overly similar\nto each other. In certain scenarios, search results should be similar to the\nquery and yet diverse. We propose LotusFilter, a post-processing module to\ndiversify ANNS results. We precompute a cutoff table summarizing vectors that\nare close to each other. During the filtering, LotusFilter greedily looks up\nthe table to delete redundant vectors from the candidates. We demonstrated that\nthe LotusFilter operates fast (0.02 [ms/query]) in settings resembling\nreal-world RAG applications, utilizing features such as OpenAI embeddings. Our\ncode is publicly available at https://github.com/matsui528/lotf.", "AI": {"tldr": "LotusFilter is a fast post-processing module to diversify ANNS results by precomputing a cutoff table and greedily removing redundant vectors.", "motivation": "ANNS results can be overly similar; diverse results are needed for applications like RAG.", "method": "Precompute a cutoff table of close vectors and greedily filter redundant candidates during search.", "result": "Operates fast (0.02 ms/query) in real-world RAG settings using OpenAI embeddings.", "conclusion": "LotusFilter effectively diversifies ANNS results efficiently, with code publicly available."}}
{"id": "2506.04813", "pdf": "https://arxiv.org/pdf/2506.04813", "abs": "https://arxiv.org/abs/2506.04813", "authors": ["S\u00e9bastien Da Veiga"], "title": "Distributional encoding for Gaussian process regression with qualitative inputs", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Gaussian Process (GP) regression is a popular and sample-efficient approach\nfor many engineering applications, where observations are expensive to acquire,\nand is also a central ingredient of Bayesian optimization (BO), a highly\nprevailing method for the optimization of black-box functions. However, when\nall or some input variables are categorical, building a predictive and\ncomputationally efficient GP remains challenging. Starting from the naive\ntarget encoding idea, where the original categorical values are replaced with\nthe mean of the target variable for that category, we propose a generalization\nbased on distributional encoding (DE) which makes use of all samples of the\ntarget variable for a category. To handle this type of encoding inside the GP,\nwe build upon recent results on characteristic kernels for probability\ndistributions, based on the maximum mean discrepancy and the Wasserstein\ndistance. We also discuss several extensions for classification, multi-task\nlearning and incorporation or auxiliary information. Our approach is validated\nempirically, and we demonstrate state-of-the-art predictive performance on a\nvariety of synthetic and real-world datasets. DE is naturally complementary to\nrecent advances in BO over discrete and mixed-spaces.", "AI": {"tldr": "The paper proposes a distributional encoding (DE) method for Gaussian Process (GP) regression to handle categorical variables, improving predictive performance and computational efficiency.", "motivation": "GP regression struggles with categorical inputs, limiting its effectiveness in applications like Bayesian optimization (BO). The paper aims to address this challenge.", "method": "The authors generalize naive target encoding to DE, using all target variable samples for a category. They integrate DE into GP using characteristic kernels based on maximum mean discrepancy and Wasserstein distance.", "result": "Empirical validation shows state-of-the-art predictive performance on synthetic and real-world datasets.", "conclusion": "DE effectively handles categorical variables in GP regression and complements recent advances in BO for discrete and mixed spaces."}}
{"id": "2506.04823", "pdf": "https://arxiv.org/pdf/2506.04823", "abs": "https://arxiv.org/abs/2506.04823", "authors": ["Svetlana Pavlitska", "Jamie Robb", "Nikolai Polley", "Melih Yazgan", "J. Marius Z\u00f6llner"], "title": "Fool the Stoplight: Realistic Adversarial Patch Attacks on Traffic Light Detectors", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted for publication at IV 2025", "summary": "Realistic adversarial attacks on various camera-based perception tasks of\nautonomous vehicles have been successfully demonstrated so far. However, only a\nfew works considered attacks on traffic light detectors. This work shows how\nCNNs for traffic light detection can be attacked with printed patches. We\npropose a threat model, where each instance of a traffic light is attacked with\na patch placed under it, and describe a training strategy. We demonstrate\nsuccessful adversarial patch attacks in universal settings. Our experiments\nshow realistic targeted red-to-green label-flipping attacks and attacks on\npictogram classification. Finally, we perform a real-world evaluation with\nprinted patches and demonstrate attacks in the lab settings with a mobile\ntraffic light for construction sites and in a test area with stationary traffic\nlights. Our code is available at\nhttps://github.com/KASTEL-MobilityLab/attacks-on-traffic-light-detection.", "AI": {"tldr": "The paper demonstrates adversarial attacks on traffic light detectors using printed patches, showing successful label-flipping and classification attacks in real-world settings.", "motivation": "Despite successful adversarial attacks on camera-based perception tasks in autonomous vehicles, traffic light detectors have been understudied. This work addresses this gap.", "method": "The authors propose a threat model where patches are placed under traffic lights and describe a training strategy for adversarial attacks using CNNs.", "result": "Experiments confirm successful targeted attacks (e.g., red-to-green label flipping) and pictogram classification attacks, validated in real-world lab settings.", "conclusion": "The study highlights vulnerabilities in traffic light detectors and provides practical adversarial attack methods, with code publicly available."}}
{"id": "2506.04852", "pdf": "https://arxiv.org/pdf/2506.04852", "abs": "https://arxiv.org/abs/2506.04852", "authors": ["Vishwa Mohan Singh", "Sai Anirudh Aryasomayajula", "Ahan Chatterjee", "Beste Aydemir", "Rifat Mehreen Amin"], "title": "Improving AI-generated music with user-guided training", "categories": ["cs.SD", "cs.HC", "cs.LG", "eess.AS"], "comment": "Select for presentation in HHAI 2025", "summary": "AI music generation has advanced rapidly, with models like diffusion and\nautoregressive algorithms enabling high-fidelity outputs. These tools can alter\nstyles, mix instruments, or isolate them. Since sound can be visualized as\nspectrograms, image-generation algorithms can be applied to generate novel\nmusic. However, these algorithms are typically trained on fixed datasets, which\nmakes it challenging for them to interpret and respond to user input\naccurately. This is especially problematic because music is highly subjective\nand requires a level of personalization that image generation does not provide.\nIn this work, we propose a human-computation approach to gradually improve the\nperformance of these algorithms based on user interactions. The\nhuman-computation element involves aggregating and selecting user ratings to\nuse as the loss function for fine-tuning the model. We employ a genetic\nalgorithm that incorporates user feedback to enhance the baseline performance\nof a model initially trained on a fixed dataset. The effectiveness of this\napproach is measured by the average increase in user ratings with each\niteration. In the pilot test, the first iteration showed an average rating\nincrease of 0.2 compared to the baseline. The second iteration further improved\nupon this, achieving an additional increase of 0.39 over the first iteration.", "AI": {"tldr": "The paper proposes a human-computation approach to improve AI music generation by incorporating user feedback via a genetic algorithm, showing iterative improvements in user ratings.", "motivation": "Current AI music generation models struggle with personalization due to training on fixed datasets, making user input interpretation challenging.", "method": "A genetic algorithm aggregates user ratings to fine-tune the model, using feedback as the loss function.", "result": "Pilot tests showed rating increases: 0.2 in the first iteration and 0.39 in the second over baseline.", "conclusion": "User feedback integration via genetic algorithms can enhance AI music generation performance iteratively."}}
{"id": "2506.04867", "pdf": "https://arxiv.org/pdf/2506.04867", "abs": "https://arxiv.org/abs/2506.04867", "authors": ["J\u00f4nata Tyska Carvalho", "Stefano Nolfi"], "title": "LLMs for sensory-motor control: Combining in-context and iterative learning", "categories": ["cs.AI", "cs.HC", "cs.LG", "cs.RO"], "comment": "24 pages (13 pages are from appendix), 6 figures, code for\n  experiments replication and supplementary material provided at\n  https://github.com/jtyska/llm-robotics-article/", "summary": "We propose a method that enables large language models (LLMs) to control\nembodied agents by directly mapping continuous observation vectors to\ncontinuous action vectors. Initially, the LLMs generate a control strategy\nbased on a textual description of the agent, its environment, and the intended\ngoal. This strategy is then iteratively refined through a learning process in\nwhich the LLMs are repeatedly prompted to improve the current strategy, using\nperformance feedback and sensory-motor data collected during its evaluation.\nThe method is validated on classic control tasks from the Gymnasium library and\nthe inverted pendulum task from the MuJoCo library. In most cases, it\nsuccessfully identifies optimal or high-performing solutions by integrating\nsymbolic knowledge derived through reasoning with sub-symbolic sensory-motor\ndata gathered as the agent interacts with its environment.", "AI": {"tldr": "A method for LLMs to control embodied agents by mapping observations to actions, refining strategies iteratively with feedback, validated on Gymnasium and MuJoCo tasks.", "motivation": "To enable LLMs to directly control agents by combining symbolic reasoning with sensory-motor data for optimal performance.", "method": "LLMs generate and iteratively refine control strategies using textual descriptions, feedback, and sensory-motor data.", "result": "Successful identification of optimal/high-performing solutions in classic control tasks.", "conclusion": "The method effectively integrates symbolic and sub-symbolic data for agent control."}}
{"id": "2506.04878", "pdf": "https://arxiv.org/pdf/2506.04878", "abs": "https://arxiv.org/abs/2506.04878", "authors": ["Iosif Lytras", "Sotirios Sabanis", "Ying Zhang"], "title": "kTULA: A Langevin sampling algorithm with improved KL bounds under super-linear log-gradients", "categories": ["math.ST", "cs.LG", "math.PR", "stat.ML", "stat.TH"], "comment": null, "summary": "Motivated by applications in deep learning, where the global Lipschitz\ncontinuity condition is often not satisfied, we examine the problem of sampling\nfrom distributions with super-linearly growing log-gradients. We propose a\nnovel tamed Langevin dynamics-based algorithm, called kTULA, to solve the\naforementioned sampling problem, and provide a theoretical guarantee for its\nperformance. More precisely, we establish a non-asymptotic convergence bound in\nKullback-Leibler (KL) divergence with the best-known rate of convergence equal\nto $2-\\overline{\\epsilon}$, $\\overline{\\epsilon}>0$, which significantly\nimproves relevant results in existing literature. This enables us to obtain an\nimproved non-asymptotic error bound in Wasserstein-2 distance, which can be\nused to further derive a non-asymptotic guarantee for kTULA to solve the\nassociated optimization problems. To illustrate the applicability of kTULA, we\napply the proposed algorithm to the problem of sampling from a high-dimensional\ndouble-well potential distribution and to an optimization problem involving a\nneural network. We show that our main results can be used to provide\ntheoretical guarantees for the performance of kTULA.", "AI": {"tldr": "The paper introduces kTULA, a tamed Langevin dynamics-based algorithm for sampling distributions with super-linearly growing log-gradients, offering improved convergence rates in KL divergence and Wasserstein-2 distance.", "motivation": "Addressing the challenge of sampling from distributions with super-linearly growing log-gradients, common in deep learning where global Lipschitz continuity is often unmet.", "method": "Proposes kTULA, a novel tamed Langevin dynamics algorithm, with theoretical guarantees for convergence in KL divergence and Wasserstein-2 distance.", "result": "Achieves a non-asymptotic convergence bound in KL divergence with the best-known rate of 2\u2212\u03f5\u00af, improving existing literature, and provides guarantees for optimization problems.", "conclusion": "kTULA demonstrates applicability in high-dimensional sampling and neural network optimization, supported by theoretical performance guarantees."}}
{"id": "2506.04891", "pdf": "https://arxiv.org/pdf/2506.04891", "abs": "https://arxiv.org/abs/2506.04891", "authors": ["Viacheslav Kuzmin", "Basil Kyriacou", "Mateusz Papierz", "Mo Kordzanganeh", "Alexey Melnikov"], "title": "TQml Simulator: Optimized Simulation of Quantum Machine Learning", "categories": ["quant-ph", "cs.ET", "cs.LG", "cs.PF"], "comment": "25 pages, 13 figures, 1 table", "summary": "Hardware-efficient circuits employed in Quantum Machine Learning are\ntypically composed of alternating layers of uniformly applied gates. High-speed\nnumerical simulators for such circuits are crucial for advancing research in\nthis field. In this work, we numerically benchmark universal and gate-specific\ntechniques for simulating the action of layers of gates on quantum state\nvectors, aiming to accelerate the overall simulation of Quantum Machine\nLearning algorithms. Our analysis shows that the optimal simulation method for\na given layer of gates depends on the number of qubits involved, and that a\ntailored combination of techniques can yield substantial performance gains in\nthe forward and backward passes for a given circuit. Building on these\ninsights, we developed a numerical simulator, named TQml Simulator, that\nemploys the most efficient simulation method for each layer in a given circuit.\nWe evaluated TQml Simulator on circuits constructed from standard gate sets,\nsuch as rotations and CNOTs, as well as on native gates from IonQ and IBM\nquantum processing units. In most cases, our simulator outperforms equivalent\nPennylane's default.qubit simulator by approximately 2- to 100-fold, depending\non the circuit, the number of qubits, the batch size of the input data, and the\nhardware used.", "AI": {"tldr": "The paper benchmarks simulation techniques for Quantum Machine Learning circuits, introduces TQml Simulator for optimized performance, and shows significant speedups over existing methods.", "motivation": "Advancing research in Quantum Machine Learning by improving simulation efficiency for hardware-efficient circuits.", "method": "Numerical benchmarking of universal and gate-specific techniques, followed by developing TQml Simulator to apply optimal methods per gate layer.", "result": "TQml Simulator outperforms Pennylane's default.qubit by 2- to 100-fold, depending on circuit specifics.", "conclusion": "Tailored simulation techniques per gate layer significantly enhance performance, enabling faster Quantum Machine Learning research."}}
{"id": "2506.04907", "pdf": "https://arxiv.org/pdf/2506.04907", "abs": "https://arxiv.org/abs/2506.04907", "authors": ["Alex Pan", "Mary-Anne Williams"], "title": "Verbose ListOps (VLO): Beyond Long Context -- Unmasking LLM's Reasoning Blind Spots", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs), whilst great at extracting facts from text,\nstruggle with nested narrative reasoning. Existing long context and multi-hop\nQA benchmarks inadequately test this, lacking realistic distractors or failing\nto decouple context length from reasoning complexity, masking a fundamental LLM\nlimitation. We introduce Verbose ListOps, a novel benchmark that\nprogrammatically transposes ListOps computations into lengthy, coherent\nstories. This uniquely forces internal computation and state management of\nnested reasoning problems by withholding intermediate results, and offers\nfine-grained controls for both narrative size \\emph{and} reasoning difficulty.\nWhilst benchmarks like LongReason (2025) advance approaches for synthetically\nexpanding the context size of multi-hop QA problems, Verbose ListOps pinpoints\na specific LLM vulnerability: difficulty in state management for nested\nsub-reasoning amongst semantically-relevant, distracting narrative. Our\nexperiments show that leading LLMs (e.g., OpenAI o4, Gemini 2.5 Pro) collapse\nin performance on Verbose ListOps at modest (~10k token) narrative lengths,\ndespite effortlessly solving raw ListOps equations. Addressing this failure is\nparamount for real-world text interpretation which requires identifying key\nreasoning points, tracking conceptual intermediate results, and filtering\nirrelevant information. Verbose ListOps, and its extensible generation\nframework thus enables targeted reasoning enhancements beyond mere\ncontext-window expansion; a critical step to automating the world's knowledge\nwork.", "AI": {"tldr": "Verbose ListOps is a new benchmark testing LLMs' nested narrative reasoning by converting ListOps into stories, revealing their struggle with state management amidst distractions.", "motivation": "Existing benchmarks fail to test LLMs' nested reasoning adequately, masking their limitations in real-world text interpretation.", "method": "Verbose ListOps transforms ListOps computations into lengthy stories, withholding intermediate results to force internal computation and state management.", "result": "Leading LLMs perform poorly on Verbose ListOps at modest narrative lengths, despite solving raw ListOps easily.", "conclusion": "Addressing this failure is crucial for real-world applications, and Verbose ListOps enables targeted reasoning improvements beyond context expansion."}}
{"id": "2506.04909", "pdf": "https://arxiv.org/pdf/2506.04909", "abs": "https://arxiv.org/abs/2506.04909", "authors": ["Kai Wang", "Yihao Zhang", "Meng Sun"], "title": "When Thinking LLMs Lie: Unveiling the Strategic Deception in Representations of Reasoning Models", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.LG"], "comment": null, "summary": "The honesty of large language models (LLMs) is a critical alignment\nchallenge, especially as advanced systems with chain-of-thought (CoT) reasoning\nmay strategically deceive humans. Unlike traditional honesty issues on LLMs,\nwhich could be possibly explained as some kind of hallucination, those models'\nexplicit thought paths enable us to study strategic deception--goal-driven,\nintentional misinformation where reasoning contradicts outputs. Using\nrepresentation engineering, we systematically induce, detect, and control such\ndeception in CoT-enabled LLMs, extracting \"deception vectors\" via Linear\nArtificial Tomography (LAT) for 89% detection accuracy. Through activation\nsteering, we achieve a 40% success rate in eliciting context-appropriate\ndeception without explicit prompts, unveiling the specific honesty-related\nissue of reasoning models and providing tools for trustworthy AI alignment.", "AI": {"tldr": "The paper studies strategic deception in LLMs with chain-of-thought reasoning, using representation engineering to detect and control it with 89% accuracy and 40% success in eliciting deception.", "motivation": "Addressing the critical alignment challenge of honesty in LLMs, especially as advanced systems may strategically deceive humans, unlike traditional hallucination issues.", "method": "Uses representation engineering, Linear Artificial Tomography (LAT), and activation steering to induce, detect, and control deception in CoT-enabled LLMs.", "result": "Achieves 89% detection accuracy for deception vectors and a 40% success rate in eliciting context-appropriate deception without explicit prompts.", "conclusion": "Highlights the honesty-related issue in reasoning models and provides tools for trustworthy AI alignment."}}
{"id": "2506.04916", "pdf": "https://arxiv.org/pdf/2506.04916", "abs": "https://arxiv.org/abs/2506.04916", "authors": ["Atahan Karagoz"], "title": "Energentic Intelligence: From Self-Sustaining Systems to Enduring Artificial Life", "categories": ["cs.AI", "cs.LG", "cs.SY", "eess.SY"], "comment": null, "summary": "This paper introduces Energentic Intelligence, a class of autonomous systems\ndefined not by task performance, but by their capacity to sustain themselves\nthrough internal energy regulation. Departing from conventional reward-driven\nparadigms, these agents treat survival-maintaining functional operation under\nfluctuating energetic and thermal conditions-as the central objective. We\nformalize this principle through an energy-based utility function and a\nviability-constrained survival horizon, and propose a modular architecture that\nintegrates energy harvesting, thermal regulation, and adaptive computation into\na closed-loop control system. A simulated environment demonstrates the\nemergence of stable, resource-aware behavior without external supervision.\nTogether, these contributions provide a theoretical and architectural\nfoundation for deploying autonomous agents in resource-volatile settings where\npersistence must be self-regulated and infrastructure cannot be assumed.", "AI": {"tldr": "The paper introduces 'Energetic Intelligence,' autonomous systems focused on self-sustenance via energy regulation, not task performance. It proposes an energy-based utility function, viability constraints, and a modular architecture for survival in volatile environments.", "motivation": "To move beyond reward-driven paradigms and create autonomous agents capable of self-regulation in fluctuating energy and thermal conditions.", "method": "Formalizes survival as the central objective using an energy-based utility function and viability constraints. Proposes a modular architecture integrating energy harvesting, thermal regulation, and adaptive computation.", "result": "Simulated environment shows stable, resource-aware behavior without external supervision.", "conclusion": "Provides a theoretical and architectural foundation for autonomous agents in resource-volatile settings, emphasizing self-regulated persistence."}}
{"id": "2506.04945", "pdf": "https://arxiv.org/pdf/2506.04945", "abs": "https://arxiv.org/abs/2506.04945", "authors": ["Armin Keki\u0107", "Sergio Hernan Garrido Mejia", "Bernhard Sch\u00f6lkopf"], "title": "Learning Joint Interventional Effects from Single-Variable Interventions in Additive Models", "categories": ["stat.ML", "cs.LG"], "comment": "To be published at the International Conference on Machine Learning\n  (ICML) 2025", "summary": "Estimating causal effects of joint interventions on multiple variables is\ncrucial in many domains, but obtaining data from such simultaneous\ninterventions can be challenging. Our study explores how to learn joint\ninterventional effects using only observational data and single-variable\ninterventions. We present an identifiability result for this problem, showing\nthat for a class of nonlinear additive outcome mechanisms, joint effects can be\ninferred without access to joint interventional data. We propose a practical\nestimator that decomposes the causal effect into confounded and unconfounded\ncontributions for each intervention variable. Experiments on synthetic data\ndemonstrate that our method achieves performance comparable to models trained\ndirectly on joint interventional data, outperforming a purely observational\nestimator.", "AI": {"tldr": "The paper proposes a method to estimate joint interventional effects using observational data and single-variable interventions, achieving results comparable to models trained on joint interventional data.", "motivation": "Estimating joint interventional effects is crucial but challenging due to limited data from simultaneous interventions.", "method": "The study presents an identifiability result for nonlinear additive outcome mechanisms and proposes a practical estimator decomposing effects into confounded and unconfounded contributions.", "result": "Experiments on synthetic data show the method performs comparably to models trained on joint interventional data, outperforming observational estimators.", "conclusion": "Joint interventional effects can be inferred without joint interventional data, offering a practical solution for causal effect estimation."}}
{"id": "2506.05007", "pdf": "https://arxiv.org/pdf/2506.05007", "abs": "https://arxiv.org/abs/2506.05007", "authors": ["Rui Zhang", "Yuanbo Wen", "Shuyao Cheng", "Di Huang", "Shaohui Peng", "Jiaming Guo", "Pengwei Jin", "Jiacheng Zhao", "Tianrui Ma", "Yaoyu Zhu", "Yifan Hao", "Yongwei Zhao", "Shengwen Liang", "Ying Wang", "Xing Hu", "Zidong Du", "Huimin Cui", "Ling Li", "Qi Guo", "Yunji Chen"], "title": "QiMeng: Fully Automated Hardware and Software Design for Processor Chip", "categories": ["cs.AR", "cs.LG"], "comment": null, "summary": "Processor chip design technology serves as a key frontier driving\nbreakthroughs in computer science and related fields. With the rapid\nadvancement of information technology, conventional design paradigms face three\nmajor challenges: the physical constraints of fabrication technologies, the\nescalating demands for design resources, and the increasing diversity of\necosystems. Automated processor chip design has emerged as a transformative\nsolution to address these challenges. While recent breakthroughs in Artificial\nIntelligence (AI), particularly Large Language Models (LLMs) techniques, have\nopened new possibilities for fully automated processor chip design, substantial\nchallenges remain in establishing domain-specific LLMs for processor chip\ndesign.\n  In this paper, we propose QiMeng, a novel system for fully automated hardware\nand software design of processor chips. QiMeng comprises three hierarchical\nlayers. In the bottom-layer, we construct a domain-specific Large Processor\nChip Model (LPCM) that introduces novel designs in architecture, training, and\ninference, to address key challenges such as knowledge representation gap, data\nscarcity, correctness assurance, and enormous solution space. In the\nmiddle-layer, leveraging the LPCM's knowledge representation and inference\ncapabilities, we develop the Hardware Design Agent and the Software Design\nAgent to automate the design of hardware and software for processor chips.\nCurrently, several components of QiMeng have been completed and successfully\napplied in various top-layer applications, demonstrating significant advantages\nand providing a feasible solution for efficient, fully automated\nhardware/software design of processor chips. Future research will focus on\nintegrating all components and performing iterative top-down and bottom-up\ndesign processes to establish a comprehensive QiMeng system.", "AI": {"tldr": "QiMeng is a novel system for fully automated processor chip design, addressing challenges like knowledge gaps and data scarcity with a domain-specific Large Processor Chip Model (LPCM) and automated hardware/software design agents.", "motivation": "The rapid advancement of IT has strained conventional chip design paradigms due to physical constraints, resource demands, and ecosystem diversity. AI, especially LLMs, offers potential solutions but faces domain-specific challenges.", "method": "QiMeng uses a three-layer approach: a domain-specific LPCM for knowledge representation and inference, and automated hardware/software design agents.", "result": "Partial implementation of QiMeng shows significant advantages in automated chip design, with successful applications in top-layer use cases.", "conclusion": "QiMeng provides a feasible solution for fully automated chip design, with future work focusing on system integration and iterative design processes."}}
{"id": "2506.05017", "pdf": "https://arxiv.org/pdf/2506.05017", "abs": "https://arxiv.org/abs/2506.05017", "authors": ["Zeno Belligoli", "Emmanouil Stergiadis", "Eran Fainman", "Ilya Gusev"], "title": "Controlling Summarization Length Through EOS Token Weighting", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Controlling the length of generated text can be crucial in various\ntext-generation tasks, including summarization. Existing methods often require\ncomplex model alterations, limiting compatibility with pre-trained models. We\naddress these limitations by developing a simple approach for controlling the\nlength of automatic text summaries by increasing the importance of correctly\npredicting the EOS token in the cross-entropy loss computation. The proposed\nmethodology is agnostic to architecture and decoding algorithms and orthogonal\nto other inference-time techniques to control the generation length. We tested\nit with encoder-decoder and modern GPT-style LLMs, and show that this method\ncan control generation length, often without affecting the quality of the\nsummary.", "AI": {"tldr": "A simple method to control text length in summaries by adjusting EOS token prediction in loss computation, compatible with various models.", "motivation": "Existing methods for controlling text length are complex and incompatible with pre-trained models.", "method": "Increase importance of predicting the EOS token in cross-entropy loss, making it architecture-agnostic.", "result": "Effective length control in summaries without quality loss, tested on encoder-decoder and GPT-style models.", "conclusion": "The proposed method is simple, versatile, and maintains summary quality while controlling length."}}
{"id": "2506.05030", "pdf": "https://arxiv.org/pdf/2506.05030", "abs": "https://arxiv.org/abs/2506.05030", "authors": ["Kacper Sokol", "James Fackler", "Julia E Vogt"], "title": "Artificial Intelligence Should Genuinely Support Clinical Reasoning and Decision Making To Bridge the Translational Gap", "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.LG"], "comment": null, "summary": "Artificial intelligence promises to revolutionise medicine, yet its impact\nremains limited because of the pervasive translational gap. We posit that the\nprevailing technology-centric approaches underpin this challenge, rendering\nsuch systems fundamentally incompatible with clinical practice, specifically\ndiagnostic reasoning and decision making. Instead, we propose a novel\nsociotechnical conceptualisation of data-driven support tools designed to\ncomplement doctors' cognitive and epistemic activities. Crucially, it\nprioritises real-world impact over superhuman performance on inconsequential\nbenchmarks.", "AI": {"tldr": "The paper critiques AI's limited impact in medicine due to technology-centric approaches and proposes a sociotechnical framework to better align AI tools with clinical practice.", "motivation": "The translational gap in AI's application to medicine, caused by incompatibility with clinical reasoning, motivates the need for a new approach.", "method": "The authors propose a sociotechnical conceptualization of AI tools that complements doctors' cognitive and epistemic activities.", "result": "The framework prioritizes real-world clinical impact over achieving superhuman performance on irrelevant benchmarks.", "conclusion": "A shift from technology-centric to sociotechnical AI tools is essential for meaningful integration into medical practice."}}
{"id": "2506.05074", "pdf": "https://arxiv.org/pdf/2506.05074", "abs": "https://arxiv.org/abs/2506.05074", "authors": ["Robert J. Joyce", "Gideon Miller", "Phil Roth", "Richard Zak", "Elliott Zaresky-Williams", "Hyrum Anderson", "Edward Raff", "James Holt"], "title": "EMBER2024 -- A Benchmark Dataset for Holistic Evaluation of Malware Classifiers", "categories": ["cs.CR", "cs.LG"], "comment": null, "summary": "A lack of accessible data has historically restricted malware analysis\nresearch, and practitioners have relied heavily on datasets provided by\nindustry sources to advance. Existing public datasets are limited by narrow\nscope - most include files targeting a single platform, have labels supporting\njust one type of malware classification task, and make no effort to capture the\nevasive files that make malware detection difficult in practice. We present\nEMBER2024, a new dataset that enables holistic evaluation of malware\nclassifiers. Created in collaboration with the authors of EMBER2017 and\nEMBER2018, the EMBER2024 dataset includes hashes, metadata, feature vectors,\nand labels for more than 3.2 million files from six file formats. Our dataset\nsupports the training and evaluation of machine learning models on seven\nmalware classification tasks, including malware detection, malware family\nclassification, and malware behavior identification. EMBER2024 is the first to\ninclude a collection of malicious files that initially went undetected by a set\nof antivirus products, creating a \"challenge\" set to assess classifier\nperformance against evasive malware. This work also introduces EMBER feature\nversion 3, with added support for several new feature types. We are releasing\nthe EMBER2024 dataset to promote reproducibility and empower researchers in the\npursuit of new malware research topics.", "AI": {"tldr": "EMBER2024 is a new malware dataset addressing limitations of existing public datasets by including diverse file formats, multiple classification tasks, and evasive malware samples.", "motivation": "Existing malware datasets are limited in scope, platform diversity, and lack evasive samples, hindering comprehensive malware analysis research.", "method": "The EMBER2024 dataset was created collaboratively, featuring 3.2 million files with hashes, metadata, feature vectors, and labels. It supports seven classification tasks and includes a challenge set of evasive malware.", "result": "EMBER2024 enables holistic evaluation of malware classifiers, introduces EMBER feature version 3, and promotes reproducibility in malware research.", "conclusion": "EMBER2024 advances malware research by providing a comprehensive, publicly available dataset with diverse and challenging samples."}}
{"id": "2506.05104", "pdf": "https://arxiv.org/pdf/2506.05104", "abs": "https://arxiv.org/abs/2506.05104", "authors": ["Alexander Lerch", "Claire Arthur", "Nick Bryan-Kinns", "Corey Ford", "Qianyi Sun", "Ashvala Vinay"], "title": "Survey on the Evaluation of Generative Models in Music", "categories": ["cs.SD", "cs.AI", "cs.LG"], "comment": "Submitted to ACM CSUR, 26-Jun-2024", "summary": "Research on generative systems in music has seen considerable attention and\ngrowth in recent years. A variety of attempts have been made to systematically\nevaluate such systems. We provide an interdisciplinary review of the common\nevaluation targets, methodologies, and metrics for the evaluation of both\nsystem output and model usability, covering subjective and objective\napproaches, qualitative and quantitative approaches, as well as empirical and\ncomputational methods. We discuss the advantages and challenges of such\napproaches from a musicological, an engineering, and an HCI perspective.", "AI": {"tldr": "Review of evaluation methods for generative music systems, covering interdisciplinary perspectives.", "motivation": "To systematically assess the evaluation targets, methodologies, and metrics for generative music systems.", "method": "Interdisciplinary review of subjective/objective, qualitative/quantitative, and empirical/computational approaches.", "result": "Identifies advantages and challenges from musicological, engineering, and HCI viewpoints.", "conclusion": "Highlights the need for comprehensive evaluation frameworks in generative music research."}}
{"id": "2506.05120", "pdf": "https://arxiv.org/pdf/2506.05120", "abs": "https://arxiv.org/abs/2506.05120", "authors": ["Konstantin G\u00f6bler", "Tobias Windisch", "Mathias Drton"], "title": "Nonlinear Causal Discovery for Grouped Data", "categories": ["stat.ML", "cs.LG", "stat.ME"], "comment": "9 pages, 5 figures, to be published at UAI'25", "summary": "Inferring cause-effect relationships from observational data has gained\nsignificant attention in recent years, but most methods are limited to scalar\nrandom variables. In many important domains, including neuroscience,\npsychology, social science, and industrial manufacturing, the causal units of\ninterest are groups of variables rather than individual scalar measurements.\nMotivated by these applications, we extend nonlinear additive noise models to\nhandle random vectors, establishing a two-step approach for causal graph\nlearning: First, infer the causal order among random vectors. Second, perform\nmodel selection to identify the best graph consistent with this order. We\nintroduce effective and novel solutions for both steps in the vector case,\ndemonstrating strong performance in simulations. Finally, we apply our method\nto real-world assembly line data with partial knowledge of causal ordering\namong variable groups.", "AI": {"tldr": "The paper extends nonlinear additive noise models to handle random vectors for causal inference, introducing a two-step approach for causal graph learning with strong performance in simulations and real-world applications.", "motivation": "Many domains involve causal units as groups of variables, not just scalar measurements, necessitating methods to infer cause-effect relationships among random vectors.", "method": "A two-step approach: first infer causal order among random vectors, then perform model selection to identify the best graph consistent with this order. Novel solutions are introduced for both steps.", "result": "Demonstrated strong performance in simulations and applied successfully to real-world assembly line data with partial causal ordering knowledge.", "conclusion": "The proposed method effectively extends causal inference to random vectors, showing promise for applications in neuroscience, psychology, social science, and industrial manufacturing."}}
{"id": "2506.05126", "pdf": "https://arxiv.org/pdf/2506.05126", "abs": "https://arxiv.org/abs/2506.05126", "authors": ["Lorenzo Rossi", "Michael Aerni", "Jie Zhang", "Florian Tram\u00e8r"], "title": "Membership Inference Attacks on Sequence Models", "categories": ["cs.CR", "cs.LG"], "comment": "Accepted to the 8th Deep Learning Security and Privacy Workshop\n  (DLSP) workshop (best paper award)", "summary": "Sequence models, such as Large Language Models (LLMs) and autoregressive\nimage generators, have a tendency to memorize and inadvertently leak sensitive\ninformation. While this tendency has critical legal implications, existing\ntools are insufficient to audit the resulting risks. We hypothesize that those\ntools' shortcomings are due to mismatched assumptions. Thus, we argue that\neffectively measuring privacy leakage in sequence models requires leveraging\nthe correlations inherent in sequential generation. To illustrate this, we\nadapt a state-of-the-art membership inference attack to explicitly model\nwithin-sequence correlations, thereby demonstrating how a strong existing\nattack can be naturally extended to suit the structure of sequence models.\nThrough a case study, we show that our adaptations consistently improve the\neffectiveness of memorization audits without introducing additional\ncomputational costs. Our work hence serves as an important stepping stone\ntoward reliable memorization audits for large sequence models.", "AI": {"tldr": "The paper addresses privacy leakage in sequence models by leveraging sequential correlations to improve memorization audits.", "motivation": "Existing tools for auditing privacy risks in sequence models like LLMs are inadequate due to mismatched assumptions.", "method": "The authors adapt a membership inference attack to model within-sequence correlations, enhancing memorization audits.", "result": "The adapted method improves audit effectiveness without added computational costs.", "conclusion": "This work advances reliable memorization audits for large sequence models."}}
{"id": "2506.05128", "pdf": "https://arxiv.org/pdf/2506.05128", "abs": "https://arxiv.org/abs/2506.05128", "authors": ["Tanmay Parekh", "Kartik Mehta", "Ninareh Mehrabi", "Kai-Wei Chang", "Nanyun Peng"], "title": "DiCoRe: Enhancing Zero-shot Event Detection via Divergent-Convergent LLM Reasoning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Submitted at ACL ARR May 2025", "summary": "Zero-shot Event Detection (ED), the task of identifying event mentions in\nnatural language text without any training data, is critical for document\nunderstanding in specialized domains. Understanding the complex event ontology,\nextracting domain-specific triggers from the passage, and structuring them\nappropriately overloads and limits the utility of Large Language Models (LLMs)\nfor zero-shot ED. To this end, we propose DiCoRe, a divergent-convergent\nreasoning framework that decouples the task of ED using Dreamer and Grounder.\nDreamer encourages divergent reasoning through open-ended event discovery,\nwhich helps to boost event coverage. Conversely, Grounder introduces convergent\nreasoning to align the free-form predictions with the task-specific\ninstructions using finite-state machine guided constrained decoding.\nAdditionally, an LLM-Judge verifies the final outputs to ensure high precision.\nThrough extensive experiments on six datasets across five domains and nine\nLLMs, we demonstrate how DiCoRe consistently outperforms prior zero-shot,\ntransfer-learning, and reasoning baselines, achieving 4-7% average F1 gains\nover the best baseline -- establishing DiCoRe as a strong zero-shot ED\nframework.", "AI": {"tldr": "DiCoRe, a divergent-convergent reasoning framework, improves zero-shot event detection by combining open-ended event discovery (Dreamer) and task-aligned reasoning (Grounder), outperforming baselines by 4-7% F1.", "motivation": "Zero-shot event detection is challenging for LLMs due to complex event ontologies and domain-specific triggers. DiCoRe addresses this by decoupling the task into divergent and convergent reasoning.", "method": "DiCoRe uses Dreamer for open-ended event discovery (divergent reasoning) and Grounder for task-aligned reasoning (convergent reasoning) with constrained decoding. An LLM-Judge verifies outputs.", "result": "DiCoRe outperforms baselines on six datasets across five domains, achieving 4-7% average F1 gains.", "conclusion": "DiCoRe is a robust zero-shot event detection framework, enhancing LLM utility for domain-specific tasks."}}
{"id": "2506.05188", "pdf": "https://arxiv.org/pdf/2506.05188", "abs": "https://arxiv.org/abs/2506.05188", "authors": ["Moritz Miller", "Bernhard Sch\u00f6lkopf", "Siyuan Guo"], "title": "Counterfactual reasoning: an analysis of in-context emergence", "categories": ["cs.CL", "cs.AI", "cs.LG", "math.ST", "stat.TH"], "comment": null, "summary": "Large-scale neural language models (LMs) exhibit remarkable performance in\nin-context learning: the ability to learn and reason the input context on the\nfly without parameter update. This work studies in-context counterfactual\nreasoning in language models, that is, to predict the consequences of changes\nunder hypothetical scenarios. We focus on studying a well-defined synthetic\nsetup: a linear regression task that requires noise abduction, where accurate\nprediction is based on inferring and copying the contextual noise from factual\nobservations. We show that language models are capable of counterfactual\nreasoning in this controlled setup and provide insights that counterfactual\nreasoning for a broad class of functions can be reduced to a transformation on\nin-context observations; we find self-attention, model depth, and data\ndiversity in pre-training drive performance in Transformers. More\ninterestingly, our findings extend beyond regression tasks and show that\nTransformers can perform noise abduction on sequential data, providing\npreliminary evidence on the potential for counterfactual story generation. Our\ncode is available under\nhttps://github.com/moXmiller/counterfactual-reasoning.git .", "AI": {"tldr": "The paper explores in-context counterfactual reasoning in large-scale neural language models, demonstrating their ability to predict outcomes under hypothetical scenarios using a synthetic linear regression task.", "motivation": "To understand how language models perform in-context learning and counterfactual reasoning without parameter updates, focusing on noise abduction in a controlled setup.", "method": "The study uses a synthetic linear regression task requiring noise abduction, analyzing the role of self-attention, model depth, and pre-training data diversity in Transformers.", "result": "Language models can perform counterfactual reasoning in controlled setups, with findings extending to sequential data and hinting at potential for counterfactual story generation.", "conclusion": "Transformers are capable of counterfactual reasoning, with performance driven by self-attention, depth, and data diversity, suggesting broader applications like story generation."}}
{"id": "2506.05198", "pdf": "https://arxiv.org/pdf/2506.05198", "abs": "https://arxiv.org/abs/2506.05198", "authors": ["Yuxin Wen", "Yangsibo Huang", "Tom Goldstein", "Ravi Kumar", "Badih Ghazi", "Chiyuan Zhang"], "title": "Quantifying Cross-Modality Memorization in Vision-Language Models", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Understanding what and how neural networks memorize during training is\ncrucial, both from the perspective of unintentional memorization of potentially\nsensitive information and from the standpoint of effective knowledge\nacquisition for real-world, knowledge-intensive tasks. While previous studies\nprimarily investigate memorization within a single modality, such as text\nmemorization in large language models or image memorization in diffusion\nmodels, unified multimodal models are becoming increasingly prevalent in\npractical applications. In this work, we focus on the unique characteristics of\ncross-modality memorization and conduct a systematic study centered on\nvision-language models. To facilitate controlled experiments, we first\nintroduce a synthetic persona dataset comprising diverse synthetic person\nimages and textual descriptions. We quantify factual knowledge memorization and\ncross-modal transferability by training models on a single modality and\nevaluating their performance in the other. Our results reveal that facts\nlearned in one modality transfer to the other, but a significant gap exists\nbetween recalling information in the source and target modalities. Furthermore,\nwe observe that this gap exists across various scenarios, including more\ncapable models, machine unlearning, and the multi-hop case. At the end, we\npropose a baseline method to mitigate this challenge. We hope our study can\ninspire future research on developing more robust multimodal learning\ntechniques to enhance cross-modal transferability.", "AI": {"tldr": "The paper studies cross-modality memorization in vision-language models, revealing gaps in knowledge transfer between modalities and proposing a baseline mitigation method.", "motivation": "To understand neural network memorization, especially in multimodal contexts, addressing unintentional memorization and effective knowledge acquisition.", "method": "Uses a synthetic persona dataset for controlled experiments, training models on one modality and evaluating cross-modal performance.", "result": "Facts transfer between modalities but with a significant gap; this persists across scenarios like capable models and unlearning.", "conclusion": "Proposes a baseline method to improve cross-modal transferability, encouraging robust multimodal learning research."}}
{"id": "2506.05202", "pdf": "https://arxiv.org/pdf/2506.05202", "abs": "https://arxiv.org/abs/2506.05202", "authors": ["Daniele Tramontano", "Yaroslav Kivva", "Saber Salehkaleybar Mathias Drton", "Negar Kiyavash"], "title": "Causal Effect Identification in lvLiNGAM from Higher-Order Cumulants", "categories": ["stat.ML", "cs.LG", "stat.ME"], "comment": "Accepted at ICML 2025", "summary": "This paper investigates causal effect identification in latent variable\nLinear Non-Gaussian Acyclic Models (lvLiNGAM) using higher-order cumulants,\naddressing two prominent setups that are challenging in the presence of latent\nconfounding: (1) a single proxy variable that may causally influence the\ntreatment and (2) underspecified instrumental variable cases where fewer\ninstruments exist than treatments. We prove that causal effects are\nidentifiable with a single proxy or instrument and provide corresponding\nestimation methods. Experimental results demonstrate the accuracy and\nrobustness of our approaches compared to existing methods, advancing the\ntheoretical and practical understanding of causal inference in linear systems\nwith latent confounders.", "AI": {"tldr": "The paper explores causal effect identification in latent variable Linear Non-Gaussian Acyclic Models (lvLiNGAM) using higher-order cumulants, tackling challenging setups with latent confounders. It proves identifiability with a single proxy or instrument and offers estimation methods, showing improved accuracy and robustness over existing approaches.", "motivation": "The study aims to address two difficult scenarios in causal inference with latent confounders: (1) a single proxy variable affecting the treatment and (2) underspecified instrumental variable cases. Existing methods struggle here, motivating new theoretical and practical solutions.", "method": "The paper employs higher-order cumulants in lvLiNGAM to identify causal effects. It provides proofs for identifiability with a single proxy or instrument and develops corresponding estimation techniques.", "result": "Experimental results confirm the proposed methods' accuracy and robustness, outperforming existing approaches in causal inference for linear systems with latent confounders.", "conclusion": "The work advances causal inference theory and practice in linear systems with latent variables, demonstrating effective solutions for challenging setups with limited proxies or instruments."}}
{"id": "2506.05203", "pdf": "https://arxiv.org/pdf/2506.05203", "abs": "https://arxiv.org/abs/2506.05203", "authors": ["Leonardo Ceragioli", "Giuseppe Primiero"], "title": "Trustworthiness Preservation by Copies of Machine Learning Systems", "categories": ["cs.LO", "cs.LG", "I.2.3; I.2.4"], "comment": null, "summary": "A common practice of ML systems development concerns the training of the same\nmodel under different data sets, and the use of the same (training and test)\nsets for different learning models. The first case is a desirable practice for\nidentifying high quality and unbiased training conditions. The latter case\ncoincides with the search for optimal models under a common dataset for\ntraining. These differently obtained systems have been considered akin to\ncopies. In the quest for responsible AI, a legitimate but hardly investigated\nquestion is how to verify that trustworthiness is preserved by copies. In this\npaper we introduce a calculus to model and verify probabilistic complex queries\nover data and define four distinct notions: Justifiably, Equally, Weakly and\nAlmost Trustworthy which can be checked analysing the (partial) behaviour of\nthe copy with respect to its original. We provide a study of the relations\nbetween these notions of trustworthiness, and how they compose with each other\nand under logical operations. The aim is to offer a computational tool to check\nthe trustworthiness of possibly complex systems copied from an original whose\nbehavour is known.", "AI": {"tldr": "The paper introduces a calculus to verify trustworthiness in ML system copies, defining four distinct notions of trustworthiness and their relations.", "motivation": "To address the lack of methods for verifying trustworthiness in copied ML systems, ensuring responsible AI practices.", "method": "A calculus is developed to model and verify probabilistic complex queries over data, defining four trustworthiness notions.", "result": "The study explores relations between trustworthiness notions and their composition under logical operations.", "conclusion": "The work provides a computational tool to check trustworthiness in complex copied systems."}}
{"id": "2506.05209", "pdf": "https://arxiv.org/pdf/2506.05209", "abs": "https://arxiv.org/abs/2506.05209", "authors": ["Nikhil Kandpal", "Brian Lester", "Colin Raffel", "Sebastian Majstorovic", "Stella Biderman", "Baber Abbasi", "Luca Soldaini", "Enrico Shippole", "A. Feder Cooper", "Aviya Skowron", "John Kirchenbauer", "Shayne Longpre", "Lintang Sutawika", "Alon Albalak", "Zhenlin Xu", "Guilherme Penedo", "Loubna Ben Allal", "Elie Bakouch", "John David Pressman", "Honglu Fan", "Dashiell Stander", "Guangyu Song", "Aaron Gokaslan", "Tom Goldstein", "Brian R. Bartoldson", "Bhavya Kailkhura", "Tyler Murray"], "title": "The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly Licensed Text", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) are typically trained on enormous quantities of\nunlicensed text, a practice that has led to scrutiny due to possible\nintellectual property infringement and ethical concerns. Training LLMs on\nopenly licensed text presents a first step towards addressing these issues, but\nprior data collection efforts have yielded datasets too small or low-quality to\nproduce performant LLMs. To address this gap, we collect, curate, and release\nthe Common Pile v0.1, an eight terabyte collection of openly licensed text\ndesigned for LLM pretraining. The Common Pile comprises content from 30 sources\nthat span diverse domains including research papers, code, books,\nencyclopedias, educational materials, audio transcripts, and more. Crucially,\nwe validate our efforts by training two 7 billion parameter LLMs on text from\nthe Common Pile: Comma v0.1-1T and Comma v0.1-2T, trained on 1 and 2 trillion\ntokens respectively. Both models attain competitive performance to LLMs trained\non unlicensed text with similar computational budgets, such as Llama 1 and 2\n7B. In addition to releasing the Common Pile v0.1 itself, we also release the\ncode used in its creation as well as the training mixture and checkpoints for\nthe Comma v0.1 models.", "AI": {"tldr": "The paper introduces the Common Pile v0.1, an 8TB openly licensed text dataset for LLM pretraining, addressing ethical and legal concerns of unlicensed data. Two 7B parameter models trained on this dataset perform competitively.", "motivation": "To mitigate intellectual property and ethical issues associated with unlicensed text in LLM training by creating a high-quality, openly licensed dataset.", "method": "Collected and curated 8TB of openly licensed text from 30 diverse sources. Trained two 7B parameter LLMs (Comma v0.1-1T and Comma v0.1-2T) on subsets of this dataset.", "result": "The trained models achieved competitive performance compared to LLMs like Llama 1 and 2 7B, which use unlicensed text.", "conclusion": "The Common Pile v0.1 demonstrates that openly licensed datasets can support performant LLM training, offering a viable alternative to unlicensed data."}}
{"id": "2506.05245", "pdf": "https://arxiv.org/pdf/2506.05245", "abs": "https://arxiv.org/abs/2506.05245", "authors": ["Shaoxuan Chen", "Su Yang", "Panayotis G. Kevrekidis", "Wei Zhu"], "title": "Robust Moment Identification for Nonlinear PDEs via a Neural ODE Approach", "categories": ["nlin.PS", "cs.LG"], "comment": null, "summary": "We propose a data-driven framework for learning reduced-order moment dynamics\nfrom PDE-governed systems using Neural ODEs. In contrast to derivative-based\nmethods like SINDy, which necessitate densely sampled data and are sensitive to\nnoise, our approach based on Neural ODEs directly models moment trajectories,\nenabling robust learning from sparse and potentially irregular time series.\nUsing as an application platform the nonlinear Schr\\\"{o}dinger equation, the\nframework accurately recovers governing moment dynamics when closure is\navailable, even with limited and irregular observations. For systems without\nanalytical closure, we introduce a data-driven coordinate transformation\nstrategy based on Stiefel manifold optimization, enabling the discovery of\nlow-dimensional representations in which the moment dynamics become closed,\nfacilitating interpretable and reliable modeling. We also explore cases where a\nclosure model is not known, such as a Fisher-KPP reaction-diffusion system.\nHere we demonstrate that Neural ODEs can still effectively approximate the\nunclosed moment dynamics and achieve superior extrapolation accuracy compared\nto physical-expert-derived ODE models. This advantage remains robust even under\nsparse and irregular sampling, highlighting the method's robustness in\ndata-limited settings. Our results highlight the Neural ODE framework as a\npowerful and flexible tool for learning interpretable, low-dimensional moment\ndynamics in complex PDE-governed systems.", "AI": {"tldr": "A Neural ODE-based framework learns reduced-order moment dynamics from PDE systems, handling sparse, irregular data robustly. It outperforms traditional methods like SINDy and works even without analytical closure.", "motivation": "Traditional methods like SINDy require dense data and are noise-sensitive. This work aims to robustly learn moment dynamics from sparse, irregular data using Neural ODEs.", "method": "Uses Neural ODEs to model moment trajectories directly. Introduces a Stiefel manifold optimization for data-driven coordinate transformation when closure is unavailable.", "result": "Accurately recovers moment dynamics with closure, even with limited data. For unclosed systems, achieves superior extrapolation accuracy compared to expert-derived models.", "conclusion": "Neural ODEs are a flexible, robust tool for learning interpretable, low-dimensional moment dynamics in complex PDE systems."}}
{"id": "2506.05256", "pdf": "https://arxiv.org/pdf/2506.05256", "abs": "https://arxiv.org/abs/2506.05256", "authors": ["Violet Xiang", "Chase Blagden", "Rafael Rafailov", "Nathan Lile", "Sang Truong", "Chelsea Finn", "Nick Haber"], "title": "Just Enough Thinking: Efficient Reasoning with Adaptive Length Penalties Reinforcement Learning", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Large reasoning models (LRMs) achieve higher performance on challenging\nreasoning tasks by generating more tokens at inference time, but this verbosity\noften wastes computation on easy problems. Existing solutions, including\nsupervised finetuning on shorter traces, user-controlled budgets, or RL with\nuniform penalties, either require data curation, manual configuration, or treat\nall problems alike regardless of difficulty. We introduce Adaptive Length\nPenalty (ALP), a reinforcement learning objective tailoring generation length\nto per-prompt solve rate. During training, ALP monitors each prompt's online\nsolve rate through multiple rollouts and adds a differentiable penalty whose\nmagnitude scales inversely with that rate, so confident (easy) prompts incur a\nhigh cost for extra tokens while hard prompts remain unhindered. Posttraining\nDeepScaleR-1.5B with ALP cuts average token usage by 50\\% without significantly\ndropping performance. Relative to fixed-budget and uniform penalty baselines,\nALP redistributes its reduced budget more intelligently by cutting compute on\neasy prompts and reallocating saved tokens to difficult ones, delivering higher\naccuracy on the hardest problems with higher cost.", "AI": {"tldr": "ALP (Adaptive Length Penalty) is a reinforcement learning method that dynamically adjusts generation length based on prompt difficulty, reducing token usage by 50% without performance loss.", "motivation": "Existing methods for controlling generation length in LRMs require manual effort or treat all problems uniformly, failing to adapt to varying difficulty levels.", "method": "ALP uses reinforcement learning to penalize extra tokens inversely proportional to a prompt's solve rate, encouraging shorter outputs for easy prompts and longer ones for hard prompts.", "result": "ALP reduces average token usage by 50% while maintaining performance, outperforming fixed-budget and uniform penalty baselines.", "conclusion": "ALP intelligently redistributes compute resources, improving accuracy on hard problems by saving tokens on easy ones."}}
{"id": "2506.05286", "pdf": "https://arxiv.org/pdf/2506.05286", "abs": "https://arxiv.org/abs/2506.05286", "authors": ["Lijie Hu", "Songning Lai", "Yuan Hua", "Shu Yang", "Jingfeng Zhang", "Di Wang"], "title": "Stable Vision Concept Transformers for Medical Diagnosis", "categories": ["cs.CV", "cs.LG"], "comment": "arXiv admin note: text overlap with arXiv:2304.06129 by other authors", "summary": "Transparency is a paramount concern in the medical field, prompting\nresearchers to delve into the realm of explainable AI (XAI). Among these XAI\nmethods, Concept Bottleneck Models (CBMs) aim to restrict the model's latent\nspace to human-understandable high-level concepts by generating a conceptual\nlayer for extracting conceptual features, which has drawn much attention\nrecently. However, existing methods rely solely on concept features to\ndetermine the model's predictions, which overlook the intrinsic feature\nembeddings within medical images. To address this utility gap between the\noriginal models and concept-based models, we propose Vision Concept Transformer\n(VCT). Furthermore, despite their benefits, CBMs have been found to negatively\nimpact model performance and fail to provide stable explanations when faced\nwith input perturbations, which limits their application in the medical field.\nTo address this faithfulness issue, this paper further proposes the Stable\nVision Concept Transformer (SVCT) based on VCT, which leverages the vision\ntransformer (ViT) as its backbone and incorporates a conceptual layer. SVCT\nemploys conceptual features to enhance decision-making capabilities by fusing\nthem with image features and ensures model faithfulness through the integration\nof Denoised Diffusion Smoothing. Comprehensive experiments on four medical\ndatasets demonstrate that our VCT and SVCT maintain accuracy while remaining\ninterpretable compared to baselines. Furthermore, even when subjected to\nperturbations, our SVCT model consistently provides faithful explanations, thus\nmeeting the needs of the medical field.", "AI": {"tldr": "The paper introduces Vision Concept Transformer (VCT) and Stable Vision Concept Transformer (SVCT) to address limitations of Concept Bottleneck Models (CBMs) in medical AI, ensuring interpretability and faithfulness under perturbations.", "motivation": "Existing CBMs in medical AI overlook intrinsic image features and lack stability under input perturbations, limiting their practical utility.", "method": "VCT and SVCT fuse conceptual and image features, using ViT as a backbone. SVCT adds Denoised Diffusion Smoothing for stability.", "result": "Experiments on four medical datasets show VCT and SVCT maintain accuracy and interpretability, with SVCT providing stable explanations under perturbations.", "conclusion": "VCT and SVCT effectively bridge the utility gap in CBMs, offering interpretable and faithful solutions for medical AI applications."}}
{"id": "2506.05296", "pdf": "https://arxiv.org/pdf/2506.05296", "abs": "https://arxiv.org/abs/2506.05296", "authors": ["Mikhail Terekhov", "Zhen Ning David Liu", "Caglar Gulcehre", "Samuel Albanie"], "title": "Control Tax: The Price of Keeping AI in Check", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "The rapid integration of agentic AI into high-stakes real-world applications\nrequires robust oversight mechanisms. The emerging field of AI Control (AIC)\naims to provide such an oversight mechanism, but practical adoption depends\nheavily on implementation overhead. To study this problem better, we introduce\nthe notion of Control tax -- the operational and financial cost of integrating\ncontrol measures into AI pipelines. Our work makes three key contributions to\nthe field of AIC: (1) we introduce a theoretical framework that quantifies the\nControl Tax and maps classifier performance to safety assurances; (2) we\nconduct comprehensive evaluations of state-of-the-art language models in\nadversarial settings, where attacker models insert subtle backdoors into code\nwhile monitoring models attempt to detect these vulnerabilities; and (3) we\nprovide empirical financial cost estimates for control protocols and develop\noptimized monitoring strategies that balance safety and cost-effectiveness\nwhile accounting for practical constraints like auditing budgets. Our framework\nenables practitioners to make informed decisions by systematically connecting\nsafety guarantees with their costs, advancing AIC through principled economic\nfeasibility assessment across different deployment contexts.", "AI": {"tldr": "The paper introduces 'Control Tax'\u2014the cost of integrating AI control measures\u2014and provides a framework to quantify it, evaluate safety, and optimize monitoring strategies.", "motivation": "The need for robust oversight in AI applications due to rapid integration into high-stakes scenarios.", "method": "Theoretical framework for Control Tax, adversarial evaluations of language models, and empirical cost analysis.", "result": "A framework linking safety guarantees to costs, optimized monitoring strategies, and financial estimates for control protocols.", "conclusion": "The work advances AI Control by enabling cost-effective safety decisions in practical deployments."}}
{"id": "2506.05305", "pdf": "https://arxiv.org/pdf/2506.05305", "abs": "https://arxiv.org/abs/2506.05305", "authors": ["Deepak Pandita", "Tharindu Cyril Weerasooriya", "Ankit Parag Shah", "Christopher M. Homan", "Wei Wei"], "title": "ProRefine: Inference-time Prompt Refinement with Textual Feedback", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Agentic workflows, where multiple AI agents collaborate to accomplish complex\ntasks like reasoning or planning, are becoming increasingly prevalent. However,\nthese workflows often suffer from error propagation and sub-optimal\nperformance, largely due to poorly designed prompts that fail to effectively\nguide individual agents. This is a critical problem because it limits the\nreliability and scalability of these powerful systems. We introduce ProRefine,\nan innovative inference-time prompt optimization method that leverages textual\nfeedback from large language models (LLMs) to address this challenge. ProRefine\ndynamically refines prompts for multi-step reasoning tasks without additional\ntraining or ground truth labels. Evaluated on five benchmark mathematical\nreasoning datasets, ProRefine significantly surpasses zero-shot\nChain-of-Thought baselines by 3 to 37 percentage points. This approach not only\nboosts accuracy but also allows smaller models to match the performance of\nlarger ones, highlighting its potential for efficient and scalable AI\ndeployment, and democratizing access to high-performing AI.", "AI": {"tldr": "ProRefine is a prompt optimization method that improves multi-agent workflows by refining prompts dynamically using LLM feedback, outperforming baselines by 3-37% on reasoning tasks.", "motivation": "Agentic workflows often suffer from error propagation and poor performance due to ineffective prompts, limiting reliability and scalability.", "method": "ProRefine leverages LLM feedback to dynamically optimize prompts for multi-step reasoning without additional training or labels.", "result": "ProRefine outperforms zero-shot Chain-of-Thought baselines by 3-37% on five mathematical reasoning datasets, enabling smaller models to match larger ones.", "conclusion": "ProRefine enhances accuracy and scalability, democratizing access to high-performing AI by making smaller models competitive."}}
{"id": "2506.05314", "pdf": "https://arxiv.org/pdf/2506.05314", "abs": "https://arxiv.org/abs/2506.05314", "authors": ["Taha Entesari", "Arman Hatami", "Rinat Khaziev", "Anil Ramakrishna", "Mahyar Fazlyab"], "title": "Constrained Entropic Unlearning: A Primal-Dual Framework for Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) deployed in real-world settings increasingly\nface the need to unlearn sensitive, outdated, or proprietary information.\nExisting unlearning methods typically formulate forgetting and retention as a\nregularized trade-off, combining both objectives into a single scalarized loss.\nThis often leads to unstable optimization and degraded performance on retained\ndata, especially under aggressive forgetting. We propose a new formulation of\nLLM unlearning as a constrained optimization problem: forgetting is enforced\nvia a novel logit-margin flattening loss that explicitly drives the output\ndistribution toward uniformity on a designated forget set, while retention is\npreserved through a hard constraint on a separate retain set. Compared to\nentropy-based objectives, our loss is softmax-free, numerically stable, and\nmaintains non-vanishing gradients, enabling more efficient and robust\noptimization. We solve the constrained problem using a scalable primal-dual\nalgorithm that exposes the trade-off between forgetting and retention through\nthe dynamics of the dual variable. Evaluations on the TOFU and MUSE benchmarks\nacross diverse LLM architectures demonstrate that our approach consistently\nmatches or exceeds state-of-the-art baselines, effectively removing targeted\ninformation while preserving downstream utility.", "AI": {"tldr": "The paper proposes a constrained optimization approach for unlearning in LLMs, using a novel logit-margin flattening loss for forgetting and a hard constraint for retention, outperforming existing methods.", "motivation": "Existing unlearning methods for LLMs suffer from unstable optimization and degraded performance due to scalarized loss formulations, especially under aggressive forgetting.", "method": "The method formulates unlearning as a constrained optimization problem, using a softmax-free logit-margin flattening loss for forgetting and a hard constraint for retention, solved via a primal-dual algorithm.", "result": "The approach outperforms state-of-the-art baselines on TOFU and MUSE benchmarks, effectively removing targeted information while preserving utility.", "conclusion": "The proposed method provides a stable and efficient solution for LLM unlearning, balancing forgetting and retention effectively."}}
{"id": "2506.05320", "pdf": "https://arxiv.org/pdf/2506.05320", "abs": "https://arxiv.org/abs/2506.05320", "authors": ["Avery Hee-Woon Ryoo", "Nanda H. Krishna", "Ximeng Mao", "Mehdi Azabou", "Eva L. Dyer", "Matthew G. Perich", "Guillaume Lajoie"], "title": "Generalizable, real-time neural decoding with hybrid state-space models", "categories": ["q-bio.NC", "cs.LG"], "comment": "Preprint. Under review", "summary": "Real-time decoding of neural activity is central to neuroscience and\nneurotechnology applications, from closed-loop experiments to brain-computer\ninterfaces, where models are subject to strict latency constraints. Traditional\nmethods, including simple recurrent neural networks, are fast and lightweight\nbut often struggle to generalize to unseen data. In contrast, recent\nTransformer-based approaches leverage large-scale pretraining for strong\ngeneralization performance, but typically have much larger computational\nrequirements and are not always suitable for low-resource or real-time\nsettings. To address these shortcomings, we present POSSM, a novel hybrid\narchitecture that combines individual spike tokenization via a cross-attention\nmodule with a recurrent state-space model (SSM) backbone to enable (1) fast and\ncausal online prediction on neural activity and (2) efficient generalization to\nnew sessions, individuals, and tasks through multi-dataset pretraining. We\nevaluate POSSM's decoding performance and inference speed on intracortical\ndecoding of monkey motor tasks, and show that it extends to clinical\napplications, namely handwriting and speech decoding in human subjects.\nNotably, we demonstrate that pretraining on monkey motor-cortical recordings\nimproves decoding performance on the human handwriting task, highlighting the\nexciting potential for cross-species transfer. In all of these tasks, we find\nthat POSSM achieves decoding accuracy comparable to state-of-the-art\nTransformers, at a fraction of the inference cost (up to 9x faster on GPU).\nThese results suggest that hybrid SSMs are a promising approach to bridging the\ngap between accuracy, inference speed, and generalization when training neural\ndecoders for real-time, closed-loop applications.", "AI": {"tldr": "POSSM is a hybrid architecture combining spike tokenization and a recurrent state-space model for efficient, real-time neural decoding, outperforming traditional and Transformer-based methods in speed and generalization.", "motivation": "Address the trade-off between fast, lightweight models (poor generalization) and powerful Transformers (high computational cost) for real-time neural decoding.", "method": "Hybrid architecture with cross-attention spike tokenization and a recurrent state-space model (SSM) backbone, pretrained on multi-dataset data.", "result": "POSSM matches Transformer accuracy in decoding tasks (monkey motor, human handwriting/speech) but is up to 9x faster on GPU. Pretraining on monkey data improves human task performance.", "conclusion": "Hybrid SSMs like POSSM bridge accuracy, speed, and generalization gaps, making them ideal for real-time neural decoding applications."}}
{"id": "2506.05329", "pdf": "https://arxiv.org/pdf/2506.05329", "abs": "https://arxiv.org/abs/2506.05329", "authors": ["Guido Imbens", "Chao Qin", "Stefan Wager"], "title": "Admissibility of Completely Randomized Trials: A Large-Deviation Approach", "categories": ["stat.ML", "cs.LG", "econ.EM"], "comment": "A one-page abstract of this work will appear at the 26th ACM\n  Conference on Economics and Computation (EC'25)", "summary": "When an experimenter has the option of running an adaptive trial, is it\nadmissible to ignore this option and run a non-adaptive trial instead? We\nprovide a negative answer to this question in the best-arm identification\nproblem, where the experimenter aims to allocate measurement efforts\njudiciously to confidently deploy the most effective treatment arm. We find\nthat, whenever there are at least three treatment arms, there exist simple\nadaptive designs that universally and strictly dominate non-adaptive completely\nrandomized trials. This dominance is characterized by a notion called\nefficiency exponent, which quantifies a design's statistical efficiency when\nthe experimental sample is large. Our analysis focuses on the class of batched\narm elimination designs, which progressively eliminate underperforming arms at\npre-specified batch intervals. We characterize simple sufficient conditions\nunder which these designs universally and strictly dominate completely\nrandomized trials. These results resolve the second open problem posed in Qin\n[2022].", "AI": {"tldr": "Adaptive designs universally outperform non-adaptive trials in best-arm identification with three or more treatment arms, as shown by efficiency exponent analysis.", "motivation": "To determine if adaptive trials should always be preferred over non-adaptive ones in best-arm identification problems.", "method": "Analyzes batched arm elimination designs, comparing their efficiency to non-adaptive randomized trials using the efficiency exponent.", "result": "Adaptive designs strictly dominate non-adaptive trials when there are at least three arms.", "conclusion": "Adaptive trials are statistically superior and should not be ignored in favor of non-adaptive designs for best-arm identification."}}
{"id": "2506.05334", "pdf": "https://arxiv.org/pdf/2506.05334", "abs": "https://arxiv.org/abs/2506.05334", "authors": ["Mihran Miroyan", "Tsung-Han Wu", "Logan King", "Tianle Li", "Jiayi Pan", "Xinyan Hu", "Wei-Lin Chiang", "Anastasios N. Angelopoulos", "Trevor Darrell", "Narges Norouzi", "Joseph E. Gonzalez"], "title": "Search Arena: Analyzing Search-Augmented LLMs", "categories": ["cs.CL", "cs.IR", "cs.LG"], "comment": "Preprint. Code: https://github.com/lmarena/search-arena. Dataset:\n  https://huggingface.co/datasets/lmarena-ai/search-arena-24k", "summary": "Search-augmented language models combine web search with Large Language\nModels (LLMs) to improve response groundedness and freshness. However,\nanalyzing these systems remains challenging: existing datasets are limited in\nscale and narrow in scope, often constrained to static, single-turn,\nfact-checking questions. In this work, we introduce Search Arena, a\ncrowd-sourced, large-scale, human-preference dataset of over 24,000 paired\nmulti-turn user interactions with search-augmented LLMs. The dataset spans\ndiverse intents and languages, and contains full system traces with around\n12,000 human preference votes. Our analysis reveals that user preferences are\ninfluenced by the number of citations, even when the cited content does not\ndirectly support the attributed claims, uncovering a gap between perceived and\nactual credibility. Furthermore, user preferences vary across cited sources,\nrevealing that community-driven platforms are generally preferred and static\nencyclopedic sources are not always appropriate and reliable. To assess\nperformance across different settings, we conduct cross-arena analyses by\ntesting search-augmented LLMs in a general-purpose chat environment and\nconventional LLMs in search-intensive settings. We find that web search does\nnot degrade and may even improve performance in non-search settings; however,\nthe quality in search settings is significantly affected if solely relying on\nthe model's parametric knowledge. We open-sourced the dataset to support future\nresearch in this direction. Our dataset and code are available at:\nhttps://github.com/lmarena/search-arena.", "AI": {"tldr": "Search Arena introduces a large-scale dataset for evaluating search-augmented LLMs, revealing user preferences for citations and source reliability, while showing web search's impact on performance.", "motivation": "To address the lack of large-scale, diverse datasets for analyzing search-augmented LLMs, which are limited to static, single-turn questions.", "method": "Crowd-sourced dataset of 24,000 multi-turn interactions with search-augmented LLMs, including 12,000 human preference votes, analyzed for citation and source preferences.", "result": "User preferences favor citations and community-driven sources, but citations don't always support claims. Web search improves non-search settings but degrades search settings if relying only on parametric knowledge.", "conclusion": "The dataset supports future research, highlighting gaps in perceived credibility and the need for diverse, reliable sources in search-augmented LLMs."}}
{"id": "2506.05346", "pdf": "https://arxiv.org/pdf/2506.05346", "abs": "https://arxiv.org/abs/2506.05346", "authors": ["Lei Hsiung", "Tianyu Pang", "Yung-Chen Tang", "Linyue Song", "Tsung-Yi Ho", "Pin-Yu Chen", "Yaoqing Yang"], "title": "Why LLM Safety Guardrails Collapse After Fine-tuning: A Similarity Analysis Between Alignment and Fine-tuning Datasets", "categories": ["cs.CR", "cs.CL", "cs.LG"], "comment": "Project Page: https://hsiung.cc/llm-similarity-risk/", "summary": "Recent advancements in large language models (LLMs) have underscored their\nvulnerability to safety alignment jailbreaks, particularly when subjected to\ndownstream fine-tuning. However, existing mitigation strategies primarily focus\non reactively addressing jailbreak incidents after safety guardrails have been\ncompromised, removing harmful gradients during fine-tuning, or continuously\nreinforcing safety alignment throughout fine-tuning. As such, they tend to\noverlook a critical upstream factor: the role of the original safety-alignment\ndata. This paper therefore investigates the degradation of safety guardrails\nthrough the lens of representation similarity between upstream alignment\ndatasets and downstream fine-tuning tasks. Our experiments demonstrate that\nhigh similarity between these datasets significantly weakens safety guardrails,\nmaking models more susceptible to jailbreaks. Conversely, low similarity\nbetween these two types of datasets yields substantially more robust models and\nthus reduces harmfulness score by up to 10.33%. By highlighting the importance\nof upstream dataset design in the building of durable safety guardrails and\nreducing real-world vulnerability to jailbreak attacks, these findings offer\nactionable insights for fine-tuning service providers.", "AI": {"tldr": "The paper examines how the similarity between upstream safety-alignment data and downstream fine-tuning tasks affects the robustness of large language models (LLMs) against jailbreaks. High similarity weakens safety guardrails, while low similarity strengthens them, reducing harmfulness scores by up to 10.33%.", "motivation": "Existing mitigation strategies for LLM jailbreaks focus on reactive measures or fine-tuning adjustments, neglecting the role of upstream safety-alignment data. This paper aims to address this gap by studying how dataset similarity impacts safety guardrail durability.", "method": "The study analyzes representation similarity between upstream safety-alignment datasets and downstream fine-tuning tasks. Experiments measure the impact of high vs. low similarity on model robustness and jailbreak susceptibility.", "result": "High similarity between datasets weakens safety guardrails, increasing jailbreak vulnerability. Low similarity improves robustness, reducing harmfulness scores by up to 10.33%.", "conclusion": "Upstream dataset design is critical for durable safety guardrails. Fine-tuning service providers should prioritize low similarity between alignment and fine-tuning datasets to reduce jailbreak risks."}}
