<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 24]
- [math.AP](#math.AP) [Total: 31]
- [physics.comp-ph](#physics.comp-ph) [Total: 5]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 10]
- [nlin.CD](#nlin.CD) [Total: 1]
- [math.PR](#math.PR) [Total: 4]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [math.FA](#math.FA) [Total: 2]
- [physics.optics](#physics.optics) [Total: 2]
- [math.DG](#math.DG) [Total: 1]
- [cs.CV](#cs.CV) [Total: 1]
- [cs.LG](#cs.LG) [Total: 5]
- [cs.SE](#cs.SE) [Total: 1]
- [math.SP](#math.SP) [Total: 1]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 2]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 2]
- [math-ph](#math-ph) [Total: 1]
- [nlin.SI](#nlin.SI) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [cond-mat.mes-hall](#cond-mat.mes-hall) [Total: 2]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Energy-stable Port-Hamiltonian Systems](https://arxiv.org/abs/2506.06471)
*Patrick Buchfink,Silke Glas,Hans Zwart*

Main category: math.NA

TL;DR: The paper introduces energy-stable port-Hamiltonian (espH) systems by combining energy-stable and port-Hamiltonian systems, preserving structure through discretization and reduction.


<details>
  <summary>Details</summary>
Motivation: To extend energy-stable systems with input-output ports for a pH formulation, enhancing structural preservation in discretization and reduction.

Method: Combine energy-stable and port-Hamiltonian systems to create espH systems, ensuring natural preservation of structure.

Result: The espH formulation maintains its structure during discretization (space/time) and model reduction.

Conclusion: The espH framework successfully integrates energy stability and port-Hamiltonian properties, offering robust structural preservation.

Abstract: We combine energy-stable and port-Hamiltonian (pH) systems to obtain
energy-stable port-Hamiltonian (espH) systems. The idea is to extend the known
energy-stable systems with an input-output port, which results in a pH
formulation. One advantage of the new espH formulation is that it naturally
preserves its espH structure throughout discretization (in space and time) and
model reduction.

</details>


### [2] [Efficient implementation of high-order isospectral symplectic Runge-Kutta schemes](https://arxiv.org/abs/2506.06533)
*Clauson Carvalho da Silva,Christian Lessig,Carlos Tomei*

Main category: math.NA

TL;DR: A novel algorithm simplifies higher-order Isospectral Runge-Kutta methods by reducing implicit equations to one per step, improving efficiency and conservation properties, especially for low-dimensional systems.


<details>
  <summary>Details</summary>
Motivation: Higher-order Isospectral Runge-Kutta methods are computationally intensive due to solving many implicit equations, limiting their practicality despite their benefits.

Method: The proposed algorithm uses block matrix structures to reduce implicit equations to one per time step, solvable via fixed-point iteration.

Result: For low-dimensional systems, higher-order integrators show better conservation with similar cost; high-dimensional systems benefit less but can improve with parallelization.

Conclusion: The new algorithm makes higher-order isospectral methods more practical, especially for low-dimensional systems, while offering potential for high-dimensional cases with optimization.

Abstract: Isospectral Runge-Kutta methods are well-suited for the numerical solution of
isospectral systems such as the rigid body and the Toda lattice. More recently,
these integrators have been applied to geophysical fluid models, where their
isospectral property has provided insights into the long-time behavior of such
systems. However, higher-order Isospectral Runge-Kutta methods require solving
a large number of implicit equations. This makes the implicit midpoint rule the
most commonly used due to its relative simplicity and computational efficiency.
In this work, we introduce a novel algorithm that simplifies the implementation
of general isospectral Runge-Kutta integrators. Our approach leverages block
matrix structures to reduce the number of implicit equations per time step to a
single one. This equation can be solved efficiently using fixed-point
iteration. We present numerical experiments comparing performance and accuracy
of higher-order integrators implemented with our algorithm against the implicit
midpoint rule. Results show that, for low-dimensional systems, the higher-order
integrators yield improved conservation properties with comparable
computational cost. For high-dimensional systems, while our algorithm continues
to show better conservation properties, its performance is less competitive,
though it can be improved through parallelization.

</details>


### [3] [A robust finite element method for linearized magnetohydrodynamics on general domains](https://arxiv.org/abs/2506.06685)
*L. Beirao da Veiga,C. Lovadina,M. Trezzi*

Main category: math.NA

TL;DR: A new finite element method for linearized Magnetohydrodynamics is introduced, handling non-convex domains and less regular solutions, with proven pressure and quasi-robustness.


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing methods for linearized Magnetohydrodynamics, particularly in non-convex domains and with irregular solutions.

Method: Proposes a novel finite element scheme designed for non-convex domains and less regular solutions, ensuring pressure robustness and quasi-robustness against fluid and magnetic Reynolds numbers.

Result: The method successfully handles non-convex domains and less regular solutions while maintaining robustness properties.

Conclusion: The proposed scheme offers a robust and versatile solution for linearized Magnetohydrodynamics, expanding applicability to more complex scenarios.

Abstract: We propose a new finite element method for linearized Magnetohydrodynamics.
The main novelty is that the proposed scheme is able to handle also non-convex
domains and less regular solutions. The method is proved to be pressure robust
and quasi-robust with respect to both fluid and magnetic Reynolds numbers.

</details>


### [4] [Fully discrete finite element approximation for the projection method to solve the Chemotaxis-Fluid System](https://arxiv.org/abs/2506.06792)
*Chenyang Li*

Main category: math.NA

TL;DR: A numerical method for solving a chemotaxis-fluid interaction model using a pressure-correction projection finite element method is proposed and validated.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of numerically solving the coupled incompressible Navier-Stokes and Keller-Segel chemotaxis system.

Method: A pressure-correction projection finite element method with backward Euler temporal discretization and mixed finite element spatial discretization. Nonlinear terms are treated semi-implicitly.

Result: Rigorous error estimates and numerical experiments confirm the method's stability, accuracy, and effectiveness in capturing system dynamics.

Conclusion: The proposed scheme successfully models the chemotaxis-fluid system, demonstrating convergence and practical applicability.

Abstract: In this paper, we investigate a chemotaxis-fluid interaction model governed
by the incompressible Navier-Stokes equations coupled with the classical
Keller-Segel chemotaxis system. To numerically solve this coupled system, we
develop a pressure-correction projection finite element method based on a
projection framework. The proposed scheme employs a backward Euler method for
temporal discretization and a mixed finite element method for spatial
discretization. Nonlinear terms are treated semi-implicitly to enhance
computational stability and efficiency. We further establish rigorous error
estimates for the fully discrete scheme, demonstrating the convergence of the
numerical method. A series of numerical experiments are conducted to validate
the stability, accuracy, and effectiveness of the proposed method. The results
confirm the scheme's capability to capture the essential dynamical behaviors
and characteristic features of the chemotaxis-fluid system.

</details>


### [5] [Fourth- and higher-order finite element methods for the incompressible Navier-Stokes equations with Dirichlet boundary conditions](https://arxiv.org/abs/2506.06863)
*Yang Li,Heyu Wang,Qinghai Zhang*

Main category: math.NA

TL;DR: The paper presents a high-order finite-element solver for incompressible Navier-Stokes equations using the GePUP formulation, achieving efficiency and accuracy with adaptive mesh refinement and implicit-explicit time integration.


<details>
  <summary>Details</summary>
Motivation: To improve numerical solutions for incompressible Navier-Stokes equations by leveraging the GePUP formulation, which avoids solenoidal constraints and enhances flexibility in finite element spaces.

Method: High-order finite-element discretization with equal-order Lagrange elements, combined with implicit-explicit Runge-Kutta methods for time integration and adaptive mesh refinement.

Result: Demonstrates high-order accuracy in time and space, efficient resolution of flow physics, and flexibility in finite element choices without inf-sup conditions.

Conclusion: The GePUP-FEM solver is effective for high Reynolds number flows, offering accuracy, efficiency, and adaptability.

Abstract: Inspired by the unconstrained pressure Poisson equation (PPE) formulation
[Liu, Liu, \& Pego, Comm. Pure Appl. Math. 60 (2007): 1443-1487], we previously
proposed the generic projection and unconstrained PPE (GePUP) formulation
[Zhang, J. Sci. Comput. 67 (2016): 1134-1180] for numerically solving the
incompressible Navier-Stokes equations (INSE) with no-slip boundary conditions.
In GePUP, the main evolutionary variable does not have to be solenoidal with
its divergence controlled by a heat equation. This work presents high-order
finite-element solvers for the INSE under the framework of method-of-lines.
Continuous Lagrange finite elements of equal order are utilized for the
velocity and pressure finite element spaces to discretize the weak form of
GePUP in space, while high-order implicit-explicit Runge-Kutta methods are then
employed to treat the stiff diffusion term implicitly and the other terms
explicitly. Due to the implicit treatment of the diffusion term, the time step
size is only restricted by convection. The solver is efficient in that
advancing the solution at each time step only involves solving a sequence of
linear systems either on the velocity or on the pressure with geometric
multigrid methods. Furthermore, the solver is enhanced with adaptive mesh
refinement so that the multiple length scales and time scales in flows at
moderate or high Reynolds numbers can be efficiently resolved. Numerical tests
with various Reynolds numbers are performed for the single-vortex test, the
lid-driven cavity, and the flow past a cylinder/sphere, demonstrating the
high-order accuracy of GePUP-FEM both in time and in space and its capability
of accurately and efficiently capturing the right physics. Moreover, our solver
offers the flexibility in choosing velocity and pressure finite element spaces
and is free of the standard inf-sup condition.

</details>


### [6] [A structure-preserving, second-order-in-time scheme for the von Neumann equation with power nonlinearity](https://arxiv.org/abs/2506.06879)
*Agissilaos Athanassoulis,Fotini Karakatsani,Irene Kyza*

Main category: math.NA

TL;DR: A second-order-in-time, structure-preserving scheme for solving the von Neumann equation with power nonlinearity, using fourth-order finite differences for spatial discretization. Correct initialization is key for achieving expected convergence.


<details>
  <summary>Details</summary>
Motivation: To develop a numerical method for the von Neumann equation with power nonlinearity, ensuring structure preservation and high-order accuracy in space and time.

Method: A linearly implicit, second-order-in-time scheme with fourth-order finite differences for spatial discretization, emphasizing correct initialization.

Result: Achieved expected convergence order; computed amplification factors in modulation instability for the first time.

Conclusion: The proposed method is effective for solving the von Neumann equation, with correct initialization being crucial for accuracy.

Abstract: In this paper we propose a structure-preserving, linearly implicit,
second-order-in-time scheme for the numerical solution of the von Neumann
equation with power nonlinearity (also known as the Alber equation). Fourth
order finite differences are used for the spatial discretization. We highlight
the importance of the correct initialization of the method in achieving the
expected order of convergence in space and time. As illustrative examples, we
investigate the bifurcation from Landau damping to modulation instability. In
that context, amplification factors in the fully developed modulation
instability for this nonlinear equation are computed for the first time.

</details>


### [7] [Estimation of sparse polynomial approximation error to continuous function](https://arxiv.org/abs/2506.06880)
*Renzhong Feng,Bowen Zhang*

Main category: math.NA

TL;DR: The paper addresses sparse polynomial approximation of continuous functions, focusing on error estimation and reconstruction using l1-minimization methods.


<details>
  <summary>Details</summary>
Motivation: Accurate estimation of approximation errors in sparse polynomial approximation is a key challenge in function approximation theory.

Method: Uses uniformly bounded orthonormal polynomial systems to formulate noise constraints and l1-minimization problems, analyzing approximation errors.

Result: Exact reconstruction is possible for sparse polynomials; smoother functions require fewer polynomial degrees for given accuracy.

Conclusion: The method provides error bounds and extends to L2-norm approximation, relaxing the need for uniformly bounded orthonormal systems.

Abstract: The sparse polynomial approximation of continuous functions has emerged as a
prominent area of interest in function approximation theory in recent years. A
key challenge within this domain is the accurate estimation of approximation
errors. This paper focuses on continuous functions, characterizing their
sampled values as a combination of the values of their best approximation
polynomials within a finite-dimensional polynomial space and the associated
remainder terms. Consequently, the sampled values of a function can be
interpreted as noisy samples of the values of its best approximation
polynomial, with the noise equivalent to the remainder term's values at those
points. By selecting a uniformly bounded orthonormal polynomial system as the
basis for this finite-dimensional space, it becomes feasible to formulate noise
constraint inequalities and l1-minimization problems or their weighted
l1-minimization variants. This paper provides estimations for the approximation
error of the sparse polynomial derived from the l1-minimization method,
characterizing the error in terms of the quasi-norm of the sampled function or
its best uniform approximation polynomial, the sparsity, and the best
approximation error. The analysis reveals that if the sampled function is a
sparse polynomial from a finite-dimensional space, it can be reconstructed
exactly. Moreover, it is observed that the smoother the sampled function, the
fewer degrees of the sparse polynomial are required to attain a given
approximation accuracy. The paper also extends this analysis to estimate the
L2-norm approximation error for the sparse polynomial obtained via the weighted
l1-minimization method, noting that in this context, the orthonormal polynomial
system does not need to be uniformly bounded for the conclusions to hold.

</details>


### [8] [On the randomized SVD in infinite dimensions](https://arxiv.org/abs/2506.06882)
*Daniel Kressner,David Persson,André Uschmajew*

Main category: math.NA

TL;DR: The paper introduces an infinite-dimensional randomized SVD method without requiring a covariance operator, matching finite-dimensional error bounds and linking discretized operator approximations. It also extends the Nyström approximation for self-adjoint operators.


<details>
  <summary>Details</summary>
Motivation: Existing randomized SVD methods for infinite-dimensional operators rely on a covariance operator, which can lead to poor performance if chosen poorly. The goal is to avoid this dependency while maintaining strong error bounds.

Method: The authors propose a new infinite-dimensional randomized SVD that eliminates the need for a covariance operator. They also extend the Nyström approximation for self-adjoint positive semi-definite trace class operators.

Result: The new method matches finite-dimensional error bounds and shows how discretized operator approximations converge to the infinite-dimensional case. Theoretical results validate the approach.

Conclusion: The proposed method avoids the pitfalls of covariance operator choices, aligns with practical isotropic randomness, and provides rigorous theoretical guarantees for both randomized SVD and Nyström extensions.

Abstract: Randomized methods, such as the randomized SVD (singular value decomposition)
and Nystr\"om approximation, are an effective way to compute low-rank
approximations of large matrices. Motivated by applications to operator
learning, Boull\'e and Townsend (FoCM, 2023) recently proposed an
infinite-dimensional extension of the randomized SVD for a Hilbert--Schmidt
operator $A$ that invokes randomness through a Gaussian process with a
covariance operator $K$. While the non-isotropy introduced by $K$ allows one to
incorporate prior information on $A$, an unfortunate choice may lead to
unfavorable performance and large constants in the error bounds. In this work,
we introduce a novel infinite-dimensional extension of the randomized SVD that
does not require such a choice and enjoys error bounds that match those for the
finite-dimensional case. Moreover, it reflects the common practice of using the
randomized SVD with isotropic random vectors, also when approximating
discretized operators. In fact, the theoretical results of this work show how
the usual randomized SVD applied to a discretization of $A$ approaches our
infinite-dimensional extension as the discretization gets refined, both in
terms of error bounds and the Wasserstein distance. We also present and analyze
a novel extension of the Nystr\"om approximation for self-adjoint positive
semi-definite trace class operators.

</details>


### [9] [Deep regularization networks for inverse problems with noisy operators](https://arxiv.org/abs/2506.07008)
*Fatemeh Pourahmadian,Yang Xu*

Main category: math.NA

TL;DR: A supervised learning approach accelerates spatiotemporal regularization for inverse problems, enabling real-time imaging by training a neural operator to map patterns to regularization parameters.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of real-time imaging in large inverse problems with noisy data, particularly in superresolution imaging via inverse scattering theory.

Method: A neural operator is trained in two steps: first on low-resolution maps using the Morozov discrepancy principle, then optimized via Tikhonov loss minimization. The approach avoids needing a-priori knowledge of optimal maps.

Result: The method generates high-quality, high-resolution images quickly, with networks informed by the discrepancy principle yielding higher contrast images.

Conclusion: The proposed approach significantly speeds up imaging and improves image quality in complex environments, validated through synthetic tests on damage evolution in an elastic plate.

Abstract: A supervised learning approach is proposed for regularization of large
inverse problems where the main operator is built from noisy data. This is
germane to superresolution imaging via the sampling indicators of the inverse
scattering theory. We aim to accelerate the spatiotemporal regularization
process for this class of inverse problems to enable real-time imaging. In this
approach, a neural operator maps each pattern on the right-hand side of the
scattering equation to its affiliated regularization parameter. The network is
trained in two steps which entails: (1) training on low-resolution
regularization maps furnished by the Morozov discrepancy principle with
nonoptimal thresholds, and (2) optimizing network predictions through
minimization of the Tikhonov loss function regulated by the validation loss.
Step 2 allows for tailoring of the approximate maps of Step 1 toward
construction of higher quality images. This approach enables direct learning
from test data and dispenses with the need for a-priori knowledge of the
optimal regularization maps. The network, trained on low-resolution data,
quickly generates dense regularization maps for high-resolution imaging. We
highlight the importance of the training loss function on the network's
generalizability. In particular, we demonstrate that networks informed by the
logic of discrepancy principle lead to images of higher contrast. In this case,
the training process involves many-objective optimization. We propose a new
method to adaptively select the appropriate loss weights during training
without requiring an additional optimization process. The proposed approach is
synthetically examined for imaging damage evolution in an elastic plate. The
results indicate that the discrepancy-informed regularization networks not only
accelerate the imaging process, but also remarkably enhance the image quality
in complex environments.

</details>


### [10] [A novel efficient structure-preserving exponential integrator for Hamiltonian systems](https://arxiv.org/abs/2506.07072)
*Pan Zhang,Fengyang Xiao,Lu Li*

Main category: math.NA

TL;DR: A linearly implicit, structure-preserving method for semilinear Hamiltonian systems with polynomial nonlinearities, combining Kahan's method and exponential integrators, balancing cost, accuracy, and geometric properties.


<details>
  <summary>Details</summary>
Motivation: To develop a computationally efficient method that preserves geometric properties like symmetry and energy in semilinear Hamiltonian systems.

Method: Combines Kahan's method and exponential integrators, solving a single linear system per time step.

Result: Demonstrates stability, efficiency, and long-term accuracy in numerical experiments on systems like Henon-Heiles and Fermi-Pasta-Ulam.

Conclusion: The method offers computational advantages over existing symmetric energy-preserving exponential integrators while maintaining accuracy and structure preservation.

Abstract: We propose a linearly implicit structure-preserving numerical method for
semilinear Hamiltonian systems with polynomial nonlinearities, combining
Kahan's method and exponential integrator. This approach efficiently balances
computational cost, accuracy and the preservation of key geometric properties,
including symmetry and near-preservation of energy. By requiring only the
solution of a single linear system per time step, the proposed method offers
significant computational advantages while comparing with the state-of-the-art
symmetric energy-preserving exponential integrators. The stability, efficiency
and long-term accuracy of the method are demonstrated through numerical
experiments on systems such as the Henon-Heiles system, the Fermi-Pasta-Ulam
system and the two-dimensional Zakharov-Kuznestov equation.

</details>


### [11] [The PML method for calculating the propagating modes of electromagnetic wave in periodic structures](https://arxiv.org/abs/2506.07084)
*Lide Cai,Junqing Chen,Yanpeng Gao*

Main category: math.NA

TL;DR: The paper studies propagating modes in periodic structures under electromagnetic waves, formulating it as a nonlinear eigenvalue problem, truncating the domain with perfectly matched layers, and solving it numerically.


<details>
  <summary>Details</summary>
Motivation: To accurately calculate propagating modes in periodic structures when electromagnetic waves are incident, addressing the challenges of unbounded domains.

Method: Formulate the problem as a nonlinear eigenvalue problem, truncate the domain using perfectly matched layers, recast it as a quadratic eigenvalue problem, and solve numerically with the finite element method.

Result: Theoretical approximation properties of truncation are proven, and numerical examples validate the method.

Conclusion: The proposed method effectively calculates propagating modes in periodic structures, verified by theoretical and numerical results.

Abstract: When the electromagnetic wave is incident on the periodic structures, in
addition to the scattering field, some propagating modes that are traveling in
the periodic medium could be generated. In the present paper, we study the
calculation of propagating modes. We formulate the problem as a nonlinear
eigenvalue problem in an unbounded periodic domain. Then we use perfectly
matched layers to truncate the unbounded domain, recast the problem to a
quadratic eigenvalue problem, and prove the approximation property of the
truncation. Finally, we formulate the quadratic eigenvalue problem to a general
eigenvalue problem, use the finite element method to discrete the truncation
problem, and show numerical examples to verify theoretical results.

</details>


### [12] [Computational homogenization of parabolic equations with memory effects for a periodic heterogeneous medium](https://arxiv.org/abs/2506.07111)
*P. N. Vabishchevich*

Main category: math.NA

TL;DR: The paper presents a computational framework for homogenizing nonstationary processes with memory effects in diffusion equations with contrasting coefficients, using finite element discretization and spectral methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of modeling macro-level processes in diffusion equations with weakly conducting inclusions, where nonlocal effects and memory are significant.

Method: The method involves solving nonstationary cell problems, approximating the memory kernel with exponentials, and transforming the nonlocal problem into a local one using finite element discretization and spectral techniques.

Result: The framework successfully computes the effective diffusion tensor and incorporates memory effects, with proven unconditional stability of the discrete solutions.

Conclusion: The proposed technique is validated through a two-dimensional model problem, demonstrating its effectiveness for homogenizing nonstationary processes.

Abstract: In homogenization theory, mathematical models at the macro level are
constructed based on the solution of auxiliary cell problems at the micro level
within a single periodicity cell. These problems are formulated using
asymptotic expansions of the solution with respect to a small parameter, which
represents the characteristic size of spatial heterogeneity. When studying
diffusion equations with contrasting coefficients, special attention is given
to nonlocal models with weakly conducting inclusions. In this case, macro-level
processes are described by integro-differential equations, where the difference
kernel is determined by the solution of a nonstationary cell problem. The main
contribution of this work is the development of a computational framework for
the homogenization of nonstationary processes, accounting for memory effects.
The effective diffusion tensor is computed using a standard numerical procedure
based on finite element discretization in space. The memory kernel is
approximated by a sum of exponentials obtained from solving a partial spectral
problem on the periodicity cell. The nonlocal macro-level problem is
transformed into a local one, where memory effects are incorporated through the
solution of auxiliary nonstationary problems. Standard two-level time
discretization schemes are employed, and unconditional stability of the
discrete solutions is proved in appropriate norms. Key aspects of the proposed
computational homogenization technique are illustrated by solving a
two-dimensional model problem.

</details>


### [13] [New highly efficient and accurate numerical scheme for the Cahn-Hilliard-Brinkman system](https://arxiv.org/abs/2506.07128)
*Dawei Chen,Qinzhen Ren,Minghui Li*

Main category: math.NA

TL;DR: The paper introduces high-order BDF schemes with variable time steps for the CHB system using R-GSAV, proving unconditional energy stability and proposing adaptive algorithms, including a novel hybrid-order method.


<details>
  <summary>Details</summary>
Motivation: To enhance robustness and accuracy in solving the Cahn-Hilliard-Brinkman system by developing adaptive time-step algorithms with high-order BDF schemes.

Method: A generalized scalar auxiliary variable approach with relaxation (R-GSAV) is used to construct high-order BDF schemes with variable time steps, including a hybrid-order adaptive algorithm.

Result: The schemes are unconditionally energy-stable, and the hybrid-order adaptive algorithm outperforms traditional methods in robustness and accuracy.

Conclusion: Numerical experiments validate the effectiveness and accuracy of the new adaptive algorithms, highlighting their advantages over existing methods.

Abstract: In this paper, based on a generalized scalar auxiliary variable approach with
relaxation (R-GSAV), we construct a class of high-order backward
differentiation formula (BDF) schemes with variable time steps for the
Cahn-Hilliard-Brinkman(CHB) system. In theory, it is strictly proved that the
designed schemes are unconditionally energy-stable. With the delicate treatment
of adaptive strategies, we propose several adaptive time-step algorithms to
enhance the robustness of the schemes. More importantly, a novel hybrid-order
adaptive time steps algorithm performs outstanding for the coupled system. The
hybrid-order algorithm inherits the advantages of some traditional high-order
BDF adaptive strategies. A comprehensive comparison with some adaptive
time-step algorithms is given, and the advantages of the new adaptive time-step
algorithms are emphasized. Finally, the effectiveness and accuracy of the new
methods are validated through a series of numerical experiments.

</details>


### [14] [Highly efficient linear energy stable methods for preserving the original energy dissipation law of the incompressible Navier-Stokes equation](https://arxiv.org/abs/2506.07141)
*Zihan Weng,Qi Hong,Yuezheng Gong*

Main category: math.NA

TL;DR: A computational framework for energy-stable methods for the incompressible Navier-Stokes equation is introduced, preserving energy dissipation and requiring efficient linear solves.


<details>
  <summary>Details</summary>
Motivation: To develop efficient and stable numerical methods for the incompressible Navier-Stokes equation that retain the original energy dissipation law.

Method: The framework involves recasting the model with a modified convection term and stabilization, discretizing using Crank-Nicolson and backward differentiation, and employing finite difference approximation on a staggered grid.

Result: The proposed schemes preserve energy dissipation, require solving only three linear Stokes systems and a small linear system per time step, and are proven uniquely solvable.

Conclusion: Numerical experiments confirm the accuracy, efficacy, and performance advantages of the new methods.

Abstract: In this paper, we introduce a comprehensive computational framework to
construct highly efficient linear energy stable methods for the incompressible
Navier-Stokes equation, which preserve the original energy dissipation law. By
multiplying the convection term by an identity-one term and incorporating a
zero stabilization term, we recast the original model as a strongly equivalent
system, while ensuring the retention of the original energy dissipation law.
Such nonlinear system is then discretized in time based on the Crank-Nicolson
schemes and the backward differentiation formulas, resulting in highly
efficient time-discrete schemes. The proposed schemes are designed to preserve
the original energy dissipation law while requiring only the solutions of three
linear Stokes systems and a $2\times 2$ linear system at each time step. The
finite difference approximation on a staggered grid is employed for the
time-discrete systems to derive fully discrete energy stable schemes, which are
proven to preserve the original energy dissipation law and be uniquely
solvable. We present the efficient implementation of these methods. Various
numerical experiments are carried out to verify the accuracy, efficacy, and
advantageous performance of our newly developed methods.

</details>


### [15] [2N-storage Runge-Kutta methods: Order conditions, general properties and some analytic solutions](https://arxiv.org/abs/2506.07359)
*Alexei Bazavov*

Main category: math.NA

TL;DR: The paper examines low-storage Runge-Kutta schemes (2N-storage) of Williamson's type, derives explicit constraints, corrects an error in Williamson's formula, and presents new four- and five-stage schemes with third-order accuracy.


<details>
  <summary>Details</summary>
Motivation: To improve understanding and accuracy of low-storage Runge-Kutta schemes by deriving explicit constraints and correcting errors in existing formulations.

Method: Derives explicit 2N-storage constraints, establishes new relations in the Butcher tableau, corrects Williamson's formula, and develops closed-form solutions for four- and five-stage schemes.

Result: New four- and five-stage 2N-storage schemes with third-order accuracy are derived and numerically examined.

Conclusion: The study advances the theory of low-storage Runge-Kutta schemes by correcting errors and providing new, efficient methods with rational coefficients.

Abstract: Low-storage Runge-Kutta schemes of Williamson's type, so-called 2N-storage
schemes, are examined. Explicit 2N-storage constraints are derived for the
first time and used to establish new relations between the entries of the
Butcher tableau. An error in the Williamson's formula for converting
coefficients between the standard and 2N-storage formats in the special case is
pointed out and corrected. The new relations are used to derive a closed-form
solution for four- and five-stage 2N-storage methods with the third order of
global accuracy. Several new four- and five-stage schemes with rational
coefficients are presented and numerically examined for illustration.

</details>


### [16] [Numerical Approximation and Analysis of the Inverse Robin Problem Using the Kohn-Vogelius Method](https://arxiv.org/abs/2506.07370)
*Erik Burman,Siyu Cen,Bangti Jin,Zhi Zhou*

Main category: math.NA

TL;DR: The paper numerically investigates recovering a piecewise constant Robin coefficient in elliptic/parabolic problems using a Kohn-Vogelius functional and finite element method, providing error estimates and numerical validation.


<details>
  <summary>Details</summary>
Motivation: The problem arises in applications like non-destructive corrosion detection, requiring accurate recovery of Robin coefficients from boundary data.

Method: A Kohn-Vogelius variational functional is used for regularized reconstruction, discretized via Galerkin finite element method on a graded mesh.

Result: Rigorous error estimates for the recovered Robin coefficient are derived, considering mesh size, step size, and noise. Numerical experiments validate the approach.

Conclusion: The method effectively recovers Robin coefficients, with theoretical and numerical results supporting its accuracy and robustness.

Abstract: In this work, we numerically investigate the inverse Robin problem of
recovering a piecewise constant Robin coefficient in an elliptic or parabolic
problem from the Cauchy data on a part of the boundary, a problem that commonly
arises in applications such as non-destructive corrosion detection. We employ a
Kohn-Vogelius type variational functional for the regularized reconstruction,
and discretize the resulting optimization problem using the Galerkin finite
element method on a graded mesh. We establish rigorous error estimates on the
recovered Robin coefficient in terms of the mesh size, temporal step size and
noise level. This is achieved by combining the approximation error of the
direct problem, a priori estimates on the functional, and suitable conditional
stability estimates of the continuous inverse problem. We present several
numerical experiments to illustrate the approach and to complement the
theoretical findings.

</details>


### [17] [Multiscale model reduction and two-level Schwarz preconditioner for H(curl) elliptic problems](https://arxiv.org/abs/2506.07381)
*Chupeng Ma,Yongwei Zhang*

Main category: math.NA

TL;DR: The paper presents an efficient method for solving linear systems from curl-conforming finite element discretizations of $H(\mathrm{curl})$ elliptic problems with heterogeneous coefficients, using a multiscale spectral generalized finite element method (MS-GFEM) for model reduction and proving exponential convergence.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of solving linear systems from $H(\mathrm{curl})$ elliptic problems with heterogeneous coefficients efficiently, leveraging model reduction and iterative solvers.

Method: Uses MS-GFEM for model reduction, proves exponential convergence, and formulates an iterative solver with a two-level restricted additive Schwarz preconditioner. GMRES is applied to the preconditioned system.

Result: The method achieves exponential convergence and superior dimensionality reduction, validated by numerical experiments in 2D and 3D.

Conclusion: The proposed MS-GFEM-based approach is effective for solving complex $H(\mathrm{curl})$ problems, offering high performance and broad applicability.

Abstract: This paper addresses the efficient solution of linear systems arising from
curl-conforming finite element discretizations of $H(\mathrm{curl})$ elliptic
problems with heterogeneous coefficients. We first employ the discrete form of
a multiscale spectral generalized finite element method (MS-GFEM) for model
reduction and prove that the method exhibits exponential convergence with
respect to the number of local degrees of freedom. The proposed method and its
convergence analysis are applicable in broad settings, including general
heterogeneous ($L^{\infty}$) coefficients, domains and subdomains with
nontrivial topology, irregular subdomain geometries, and high-order finite
element discretizations. Furthermore, we formulate the method as an iterative
solver, yielding a two-level restricted additive Schwarz type preconditioner
based on the MS-GFEM coarse space. The GMRES algorithm, applied to the
preconditioned system, is shown to converge at a rate of at least $\Lambda$,
where $\Lambda$ denotes the error bound of the discrete MS-GFEM approximation.
Numerical experiments in both two and three dimensions demonstrate the superior
performance of the proposed methods in terms of dimensionality reduction.

</details>


### [18] [The pollution effect for the Ginzburg-Landau equation](https://arxiv.org/abs/2506.07433)
*Théophile Chaumont-Frelet,Patrick Henning*

Main category: math.NA

TL;DR: The paper analyzes error properties of Ginzburg-Landau equation solutions in finite element spaces, focusing on the interplay between mesh size, polynomial degree, and material parameter. It introduces a new error analysis to quantify pre-asymptotic pollution effects and shows higher polynomial degrees mitigate these effects.


<details>
  <summary>Details</summary>
Motivation: To understand and quantify the numerical pollution effects in finite element approximations of the Ginzburg-Landau equation, particularly how mesh size and polynomial degree interact with the material parameter.

Method: A new error analysis is developed to explicitly quantify the pre-asymptotic regime and pollution effects, with focus on resolution conditions. Theoretical error estimates in $H^1$- and $L^2$-norms are provided and validated with numerical examples.

Result: Higher polynomial degrees reduce pollution effects, enabling better accuracy under relaxed mesh size conditions. The analysis provides explicit resolution conditions for achieving best-approximation error.

Conclusion: The study successfully quantifies the pre-asymptotic pollution effects and demonstrates the benefits of higher polynomial degrees in finite element approximations of the Ginzburg-Landau equation.

Abstract: In this paper, we investigate the approximation properties of solutions to
the Ginzburg-Landau equation (GLE) in finite element spaces. Special attention
is given to how the errors are influenced by coupling the mesh size $h$ and the
polynomial degree $p$ of the finite element space to the size of the so-called
Ginzburg-Landau material parameter $\kappa$. As observed in previous works, the
finite element approximations to the GLE are suffering from a numerical
pollution effect, that is, the best-approximation error in the finite element
space converges under mild coupling conditions between $h$ and $\kappa$,
whereas the actual finite element solutions possess poor accuracy in a large
pre-asymptotic regime which depends on $\kappa$. In this paper, we provide a
new error analysis that allows us to quantify the pre-asymptotic regime and the
corresponding pollution effect in terms of explicit resolution conditions. In
particular, we are able to prove that higher polynomial degrees reduce the
pollution effect, i.e., the accuracy of the best-approximation is achieved
under relaxed conditions for the mesh size. We provide both error estimates in
the $H^1$- and the $L^2$-norm and we illustrate our findings with numerical
examples.

</details>


### [19] [IDENT Review: Recent Advances in Identification of Differential Equations from Noisy Data](https://arxiv.org/abs/2506.07604)
*Roy Y. He,Hao Liu,Wenjing Liao,Sung Ha Kang*

Main category: math.NA

TL;DR: The paper reviews recent methods for identifying differential equations from time-dependent data, emphasizing the importance of denoising, sparsity, and coefficient recovery for accuracy.


<details>
  <summary>Details</summary>
Motivation: To model real-world phenomena using differential equations by identifying them from observed data, leveraging modern computational techniques.

Method: Formulate the problem as solving a linear system to find the optimal coefficient vector representing the data's time derivative, incorporating denoising, sparsity, and model selection.

Result: Improved accuracy in identifying differential equations by addressing denoising, sparsity, and coefficient recovery.

Conclusion: Recent advancements highlight the significance of proper system formulation, sparsity utilization, and coefficient recovery for accurate differential equation identification.

Abstract: Differential equations and numerical methods are extensively used to model
various real-world phenomena in science and engineering. With modern
developments, we aim to find the underlying differential equation from a single
observation of time-dependent data. If we assume that the differential equation
is a linear combination of various linear and nonlinear differential terms,
then the identification problem can be formulated as solving a linear system.
The goal then reduces to finding the optimal coefficient vector that best
represents the time derivative of the given data. We review some recent works
on the identification of differential equations. We find some common themes for
the improved accuracy: (i) The formulation of linear system with proper
denoising is important, (ii) how to utilize sparsity and model selection to
find the correct coefficient support needs careful attention, and (iii) there
are ways to improve the coefficient recovery. We present an overview and
analysis of these approaches about some recent developments on the topic.

</details>


### [20] [Data-Informed Mathematical Characterization of Absorption Properties in Artificial and Natural Porous Materials](https://arxiv.org/abs/2506.07656)
*Elishan C. Braun,Gabriella Bretti,Melania Di Fazio,Laura Medeghini,Mario Pezzella*

Main category: math.NA

TL;DR: The paper studies water absorption in porous materials using lab experiments and modeling, focusing on cultural heritage preservation.


<details>
  <summary>Details</summary>
Motivation: To understand and preserve porous materials in cultural heritage by characterizing their water absorption properties.

Method: Combines lab experiments (imbibition tests on materials like marble) with mathematical modeling (PDE-based simulations) and data preprocessing to reduce noise.

Result: The method effectively characterizes absorption properties and serves as a reliable tool for cultural heritage studies.

Conclusion: The combined approach is effective for analyzing and preserving porous materials in cultural heritage.

Abstract: In this work, we characterize the water absorption properties of selected
porous materials through a combined approach that integrates laboratory
experiments and mathematical modeling. Specifically, experimental data from
imbibition tests on marble, travertine, wackestone and mortar mock-ups are used
to inform and validate the mathematical and simulation frameworks. First, a
monotonicity-preserving fitting procedure is developed to preprocess the
measurements, aiming to reduce noise and mitigate instrumental errors. The
imbibition process is then simulated through a partial differential equation
model, with parameters calibrated against rough and smoothed data. The proposed
procedure appears particularly effective to characterize absorption properties
of different materials and it represents a reliable tool for the study and
preservation of cultural heritage.

</details>


### [21] [Minimal Subsampled Rank-1 Lattices for Multivariate Approximation with Optimal Convergence Rate](https://arxiv.org/abs/2506.07729)
*Felix Bartel,Alexander D. Gilbert,Frances Y. Kuo,Ian H. Sloan*

Main category: math.NA

TL;DR: Error bounds for randomly subsampled rank-1 lattices are analyzed, focusing on subset-to-lattice size ratio. Optimal polynomial sampling complexity is achieved in Korobov spaces, and frequency index sets are characterized using reciprocal worst-case error. Implementation details and numerical experiments are provided.


<details>
  <summary>Details</summary>
Motivation: To understand and improve error bounds for subsampled rank-1 lattices, particularly focusing on computational efficiency and optimal sampling complexity.

Method: Analyze subset-to-lattice size ratio, characterize frequency index sets using reciprocal worst-case error, and conduct numerical experiments with different algorithms.

Result: Optimal polynomial sampling complexity achieved in Korobov spaces; frequency index sets characterized for reconstructing lattices.

Conclusion: The study connects existing approaches for error bounds, provides implementation insights, and validates results through numerical experiments.

Abstract: In this paper we show error bounds for randomly subsampled rank-1 lattices.
We pay particular attention to the ratio of the size of the subset to the size
of the initial lattice, which is decisive for the computational complexity. In
the special case of Korobov spaces, we achieve the optimal polynomial sampling
complexity whilst having the smallest initial lattice possible. We further
characterize the frequency index set for which a given lattice is
reconstructing by using the reciprocal of the worst-case error achieved using
the lattice in question. This connects existing approaches used in proving
error bounds for lattices. We make detailed comments on the implementation and
test different algorithms using the subsampled lattice in numerical
experiments.

</details>


### [22] [Quantum-Enhanced Spectral Solution of the Poisson Equation](https://arxiv.org/abs/2506.07743)
*G. Intoccia,U. Chirico,G. Pepe,S. Cuomo*

Main category: math.NA

TL;DR: A hybrid numerical-quantum method using the Quantum Fourier Transform (QFT) solves the Poisson equation efficiently, reducing time and space complexity compared to classical methods.


<details>
  <summary>Details</summary>
Motivation: Classical methods for solving the Poisson equation are computationally expensive for large datasets, prompting the need for more efficient approaches.

Method: The method leverages QFT to estimate solution coefficients directly in the quantum framework, avoiding integration-heavy classical calculations.

Result: Numerical experiments show improved time and space complexity, along with higher solution accuracy.

Conclusion: This work demonstrates the potential of quantum-assisted techniques for PDEs and sets a foundation for future research in quantum numerical methods.

Abstract: We present a hybrid numerical-quantum method for solving the Poisson equation
under homogeneous Dirichlet boundary conditions, leveraging the Quantum Fourier
Transform (QFT) to enhance computational efficiency and reduce time and space
complexity. This approach bypasses the integration-heavy calculations of
classical methods, which have to deal with high computational costs for large
number of points. The proposed method estimates the coefficients of the series
expansion of the solution directly within the quantum framework. Numerical
experiments validate its effectiveness and reveal significant improvements in
terms of time and space complexity and solution accuracy, demonstrating the
capability of quantum-assisted techniques to contribute in solving partial
differential equations (PDEs). Despite the inherent challenges of quantum
implementation, the present work serves as a starting point for future
researches aimed at refining and expanding quantum numerical methods.

</details>


### [23] [Lengthscale-informed sparse grids for kernel methods in high dimensions](https://arxiv.org/abs/2506.07797)
*Elliot J. Addy,Jonas Latz,Aretha L. Teckentrup*

Main category: math.NA

TL;DR: The paper proposes a generalized sparse grid method incorporating anisotropy via Matérn kernels to address the curse of dimensionality in high-dimensional kernel interpolation.


<details>
  <summary>Details</summary>
Motivation: High-dimensional kernel interpolation suffers from exponential growth in required evaluations. Functional anisotropy can mitigate this by prioritizing sensitive dimensions.

Method: Generalize sparse grid methods using Matérn kernel lengthscales to encode anisotropy. Derive error bounds and validate with numerical experiments.

Result: The approach enables effective emulation in arbitrarily high dimensions for sufficiently anisotropic functions.

Conclusion: The method successfully reduces dimension dependence in error, making high-dimensional emulation feasible.

Abstract: Kernel interpolation, especially in the context of Gaussian process
emulation, is a widely used technique in surrogate modelling, where the goal is
to cheaply approximate an input-output map using a limited number of function
evaluations. However, in high-dimensional settings, such methods typically
suffer from the curse of dimensionality; the number of required evaluations to
achieve a fixed approximation error grows exponentially with the input
dimension. To overcome this, a common technique used in high-dimensional
approximation methods, such as quasi-Monte Carlo and sparse grids, is to
exploit functional anisotropy: the idea that some input dimensions are more
'sensitive' than others. In doing so, such methods can significantly reduce the
dimension dependence in the error. In this work, we propose a generalisation of
sparse grid methods that incorporates a form of anisotropy encoded by the
lengthscale parameter in Mat\'ern kernels. We derive error bounds and perform
numerical experiments that show that our approach enables effective emulation
over arbitrarily high dimensions for functions exhibiting sufficient
anisotropy.

</details>


### [24] [FractionalDiffEq.jl: High Performance Fractional Differential Equation Solver in Julia](https://arxiv.org/abs/2506.07926)
*Qingyu Qu,Wei Ruan*

Main category: math.NA

TL;DR: FractionalDiffEq.jl is a high-performance Julia-based solver suite for fractional differential equations, offering user-friendly, scalable solutions with diverse numerical methods and real-world applications.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive, efficient, and accessible tool for solving fractional differential equations in Julia, addressing the need for high-performance numerical solvers.

Method: Utilizes predictor-corrector, product-integral, and linear multistep methods, with a unifying API for diverse solver features. Benchmarked against other implementations.

Result: Proven superior performance on stiff and non-stiff problems, validated through extensive benchmarks and real-life applications like tequila fermentation and harmonic oscillators.

Conclusion: FractionalDiffEq.jl is robust, flexible, and efficient, making it a valuable tool for modeling scientific problems involving fractional differential equations.

Abstract: We present FractionalDiffEq.jl, a comprehensive solver suite for solving
fractional differential equations, featuring high-performance numerical
algorithms in the Julia programming language. FractionalDiffEq.jl is designed
to be user-friendly and scalable, tackling different types of fractional
differential equations, encompassing powerful numerical algorithms including
predictor-corrector methods, product-integral methods, and linear multistep
methods, etc, and providing a unifying API to accommodate diverse solver
features. This paper illustrates the convenient usage of FractionalDiffEq.jl in
modeling various scientific problems, accompanied by detailed examples and
applications. FractionalDiffEq.jl leverages best practices in Julia to ensure
the high performance of numerical solvers. To validate the efficiency of
FractionalDiffEq.jl , we conducted extensive benchmarks that prove the
superiority of FractionalDiffEq.jl against other implementations on both stiff
and non-stiff problems. We further demonstrate its capability on several
challenging real-life scenarios including parameter estimation in
fractional-order tequila fermentation processes, and harmonic oscillator
problems, etc, emphasizing the robustness and flexibility of
FractionalDiffEq.jl.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [25] [A Directional-ODE Framework for Discretization of Advection-Diffusion Equations](https://arxiv.org/abs/2506.06543)
*Amin Jafarimoghaddam,Manuel Soler,Irene Ortiz*

Main category: math.AP

TL;DR: A novel approach reformulates advection-diffusion equations (ADEs) as directional ODEs, yielding analytical solutions for improved stability, efficiency, and resolution.


<details>
  <summary>Details</summary>
Motivation: To enhance the traditional interpretation of explicit and implicit discretization methods for solving ADEs with nonlinear advection, diffusion, and source terms.

Method: Reformulate discrete ADEs as directional ODEs (representative ODEs) to derive analytical update formulas.

Result: Improved stability, computational efficiency, and spatiotemporal resolution; extended to systems with uncertain parameters.

Conclusion: The framework offers a versatile solution for complex ADEs in diverse scientific and engineering applications.

Abstract: We present a novel approach that redefines the traditional interpretation of
explicit and implicit discretization methods for solving a general class of
advection-diffusion equations (ADEs) featuring nonlinear advection, diffusion
operators, and potential source terms. By reformulating the discrete ADEs as
directional ordinary differential equations (ODEs) along temporal or spatial
dimensions, we derive analytical solutions that lead to novel update formulas.
In essence, the information of discrete ADEs is compressed into these
directional ODEs, which we refer to as representative ODEs. The analytical
update formulas derived from the representative ODEs significantly enhance
stability, computational efficiency, and spatiotemporal resolution.
Furthermore, we extend the framework to systems with uncertain parameters and
coefficients, showcasing its versatility in addressing complex ADEs encountered
in modeling and simulation across diverse scientific and engineering
disciplines.

</details>


### [26] [Wellposedness of inviscid SQG in the half-plane](https://arxiv.org/abs/2506.06601)
*In-Jee Jeong,Junha Kim,Hideyuki Miura*

Main category: math.AP

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We consider the SQG equation without dissipation on the half-plane with
Dirichlet boundary condition, and prove local wellposedness in the spaces
$W^{3,p}$ and $C^{2,\beta}$ for any $1<p<\infty$ and $0<\beta<1$. We complement
this wellposedness by showing that for generic $C^{\infty}_{0}$ initial data,
the unique corresponding solution does not belong to $W^{3,\infty}$.

</details>


### [27] [Weak and mild solutions to the MHD equations and the viscoelastic Navier-Stokes equations with damping in Wiener amalgam spaces](https://arxiv.org/abs/2506.06621)
*Chen-Chih Lai*

Main category: math.AP

TL;DR: The paper studies 3D incompressible MHD and viscoelastic Navier-Stokes equations with damping, proving mild solutions in Wiener amalgam spaces and constructing global-in-time local energy weak solutions.


<details>
  <summary>Details</summary>
Motivation: To extend existing techniques for analyzing fluid dynamics equations to more complex systems like MHD and viscoelastic flows, ensuring solutions meet specific spacetime bounds.

Method: Uses techniques from Bradshaw, Lai, and Tsai (2024) for mild solutions and Bradshaw and Tsai (2021) for weak solutions, with additional focus on properties like regularity and uniqueness.

Result: Existence of mild and weak solutions in Wiener amalgam spaces, with proven properties like initial/eventual regularity and small-large uniqueness.

Conclusion: The work extends prior results for Navier-Stokes to MHD and viscoelastic systems, providing a framework for analyzing such equations in amalgam spaces.

Abstract: We study the three-dimensional incompressible magnetohydrodynamic (MHD)
equations and the incompressible viscoelastic Navier-Stokes equations with
damping. Building on techniques developed by Bradshaw, Lai, and Tsai (Math.
Ann. 2024), we prove the existence of mild solutions in Wiener amalgam spaces
that satisfy the corresponding spacetime integral bounds. In addition, we
construct global-in-time local energy weak solutions in these amalgam spaces
using the framework introduced by Bradshaw and Tsai (SIAM J. Math. Anal. 2021).
As part of this construction, we also establish several properties of local
energy solutions with $L^2_{\rm uloc}$ initial data, including initial and
eventual regularity as well as small-large uniqueness, extending analogous
results obtained for the Navier-Stokes equations by Bradshaw and Tsai (Comm.
Partial Differential Equations 2020).

</details>


### [28] [Existence of traveling waves for vector valued gradient flows](https://arxiv.org/abs/2506.06647)
*Xinfu Chen,Zhilei Liang*

Main category: math.AP

TL;DR: Existence of traveling waves for vector-valued Allen-Cahn equations is proven, with bounds on the largest wave speed.


<details>
  <summary>Details</summary>
Motivation: To understand phase transitions in multi-component mixtures using the Allen-Cahn equation.

Method: Variation technique applied to systems with gradient flow structure.

Result: Existence of traveling waves and bounds on the largest wave speed.

Conclusion: The method is effective for analyzing such systems and provides quantitative results.

Abstract: Allen-Cahn equation is a fundamental continuum model that describes phase
transitions in multi-component mixtures. We prove the existence of traveling
waves for vector valued Allen-Cahn equations in the context of Ginzburg-Landau
theories; in addition, we find the largest wave speed and provide its bounds
from upper and below. Our method is based on a variation technique and can be
applied to system of equations with a gradient flow structure.

</details>


### [29] [Mixing for generic passive scalars by incompressible flows](https://arxiv.org/abs/2506.06706)
*Zeyu Jin,Ruo Li*

Main category: math.AP

TL;DR: The paper addresses the genericity of mixing in incompressible flows, showing classical criteria fail under small perturbations and developing a Young-measure theory to characterize mixing for passive scalars.


<details>
  <summary>Details</summary>
Motivation: To understand whether mixing occurs for typical incompressible flows and initial data, a question left mathematically unclear despite prior focus on optimal mixing rates.

Method: Develops a Young-measure theory for $L^\infty$ data to characterize mixing and examines the persistence of classical mixing criteria under perturbations.

Result: Classical mixing criteria fail under small perturbations; a single mixed density implies mixing for generic bounded data, linked to non-precompactness of flow maps in $L^p$.

Conclusion: The findings lay groundwork for a general theory of generic mixing in non-autonomous incompressible flows.

Abstract: Mixing by incompressible flows is a ubiquitous yet incompletely understood
phenomenon in fluid dynamics. While previous studies have focused on optimal
mixing rates, the question of its genericity, i.e., whether mixing occurs for
typical incompressible flows and typical initial data, remains mathematically
unclear. In this paper, it is shown that classical mixing criteria, e.g.
topological mixing or non-precompactness in $L^2$ for all nontrivial densities,
fail to persist under arbitrarily small perturbations of velocity fields. A
Young-measure theory adapted to $L^\infty$ data is then developed to
characterize exactly which passive scalars mix. As a consequence, the existence
of a single mixed density is equivalent to mixing for generic bounded data, and
this equivalence is further tied to the non-precompactness of the associated
measure-preserving flow maps in $L^p$. These results provide a foundation for a
general theory of generic mixing in non-autonomous incompressible flows.

</details>


### [30] [Global regularity results for the fractional heat equation and application to a class of non-linear KPZ problems](https://arxiv.org/abs/2506.06875)
*Boumediene Abdellaoui,Somia Atmani,Kheireddine Biroud,El-Haj Laamri*

Main category: math.AP

TL;DR: The paper proves global regularity for a fractional heat equation and applies it to solve a class of Kardar-Parisi-Zhang equations with fractional diffusion.


<details>
  <summary>Details</summary>
Motivation: To establish regularity and existence results for fractional heat equations and extend these findings to nonlocal gradient problems in KPZ equations.

Method: Uses a new pointwise estimate on the fractional gradient of the kernel and proves compactness of the solution mapping.

Result: Global regularity in parabolic Bessel-Potential and fractional Sobolev spaces, plus existence and regularity for KPZ equations with fractional diffusion.

Conclusion: The results provide foundational insights into fractional diffusion problems and extend applicability to nonlocal gradient terms in KPZ equations.

Abstract: In the first part of this paper, we prove the global regularity, in an
adequate parabolic Bessel-Potential space and then in the corresponding
parabolic fractional Sobolev space, of the unique solution to following
fractional heat equation $ w_t+(-\Delta)^sw= h\;;\; w(x,t)=0 \text{ in } \;
(\mathbb{R}^N\setminus\Omega)\times(0,T)\;;\; w(x,0)=w_0(x) \; \text{in}\;
\Omega$, where $\Omega$ is an open bounded subset of $\mathbb{R}^N$. The proof
is based on a new pointwise estimate on the fractional gradient of the
corresponding kernel. Moreover, we establish the compactness of $(w_0,h)\mapsto
w$. As a majeur application, in the second part , we establish existence and
regularity of solutions to a class of Kardar--Parisi--Zhang equations with
fractional diffusion and a nonlocal gradient term. Additionally, several
auxiliary results of independent interest are obtained.

</details>


### [31] [Asymptotics of nonlinear Robin energies](https://arxiv.org/abs/2506.06914)
*Giuseppe Buttazzo,Roberto Ognibene*

Main category: math.AP

TL;DR: The paper analyzes the asymptotic behavior of nonlinear variational problems with Robin-type boundary conditions, focusing on the Neumann and Dirichlet limits as the parameter α approaches 0 and infinity, respectively.


<details>
  <summary>Details</summary>
Motivation: To understand how the energy functional behaves under extreme scaling (α→0 and α→∞) and how boundary conditions transition between Neumann and Dirichlet limits.

Method: Variational methods are used to derive first-order expansions of the minimum energy for the two limits.

Result: In the Dirichlet limit, the energy converges with a power-type rate. In the Neumann limit, the behavior depends on a compatibility condition: linear convergence if met, power divergence otherwise.

Conclusion: The study provides quantified rates for energy convergence/divergence in extreme scaling regimes, revealing a dichotomy in the Neumann limit.

Abstract: This paper investigates the asymptotic behavior of a class of nonlinear
variational problems with Robin-type boundary conditions on a bounded Lipschitz
domain. The energy functional contains a bulk term (the $p$-norm of the
gradient), a boundary term (the $q$-norm of the trace) scaled by a parameter
$\alpha>0$, and a linear source term. By variational methods, we derive
first-order expansions of the minimum as $\alpha\to 0^+$ (Neumann limit) and as
$\alpha\to+\infty$ (Dirichlet limit). In the Dirichlet limit, the energy
converges to the one of Dirichlet problem with a power-type quantified rate
(depending only on $q$), while the Neumann limit exhibits a dichotomy: under a
compatibility condition, the energy linearly approaches the one of Neumann
problem, otherwise, it diverges as a power of $\alpha$ depending only on $q$.

</details>


### [32] [CDF-Generated Damage Laws: Admissibility, Gamma-Convergence to Griffith Fracture, and Well-Posedness](https://arxiv.org/abs/2506.06949)
*Huilong Ren*

Main category: math.AP

TL;DR: The paper introduces a family of scalar softening laws using CDFs, proving their thermodynamic admissibility and linking them to Griffith fracture mechanics. It also demonstrates compactness, Γ-convergence, and existence of quasi-static evolutions.


<details>
  <summary>Details</summary>
Motivation: To bridge probabilistic damage formulations with Griffith-type fracture mechanics by formulating thermodynamically admissible softening laws.

Method: Formulate softening laws using various CDFs, prove their properties (monotonicity, boundedness, dissipativity), and analyze compactness and Γ-convergence in spatial dimensions d=2,3.

Result: Established thermodynamic admissibility, compactness, Γ-convergence, and existence of quasi-static evolutions. Provided a rigorous link between probabilistic damage and fracture mechanics.

Conclusion: The work successfully connects probabilistic damage models to Griffith fracture mechanics, supported by analytical proofs and an illustrative example.

Abstract: We formulate a family of scalar softening laws by setting the stored-energy
density $\psi(\eta)=\int_{0}^{\eta}[1-F(s)]d s$, where $F$ ranges over
exponential, Cauchy, logistic, half-normal, Gudermannian, hypergeometric,
radical, rational, piece-wise, and rapid-decay cumulative-distribution
functions (CDFs). We prove that every such law yields a degradation map that is
monotone, bounded, and dissipative, rendering the associated hyperelastic
material thermodynamically admissible. Working directly in spatial dimensions
$d=2,3$, we establish compactness and $\Gamma$-convergence of the CDF-based
energies to a sharp-interface Griffith functional. We further show the
existence of rate-independent quasi-static evolutions by constructing global
energetic solutions that satisfy both stability and energy balance. These
analytical results provide a rigorous bridge between the probabilistic damage
formulation and Griffith-type fracture mechanics. One illustrative example is
presented to show the effectiveness of the current damage laws.

</details>


### [33] [Global well-posedness and orbital stability of solitary waves for Zakharov-Ito equation](https://arxiv.org/abs/2506.07053)
*Fan Wu,Yaqing Liu,Feng Shao,Boling Guo*

Main category: math.AP

TL;DR: The paper analyzes the Zakharov-Ito equation, proving local and global well-posedness in certain Sobolev spaces and orbital stability of solitary waves.


<details>
  <summary>Details</summary>
Motivation: To extend understanding of the Zakharov-Ito equation, particularly its well-posedness and stability properties, building on known results for the KdV equation.

Method: Mathematical analysis involving Sobolev spaces, variational methods, and the framework of Grillakis, Shatah, and Strauss for stability.

Result: Local well-posedness for s>3/2 and global well-posedness for s≥2 in H^s×H^s; orbital stability of solitary waves in H^1×L^2.

Conclusion: The Zakharov-Ito equation shares properties with the KdV equation, and its solitary waves are orbitally stable under certain conditions.

Abstract: In this paper, we consider the Zakharov-Ito equation \begin{equation*}
  \begin{cases}
  u_t+u_{xxx}+3uu_x+\rho\rho_x=0,
  \rho_t+{(u\rho)}_x=0.
  \end{cases}
  \end{equation*}
  We prove the local well-posedness in $H^s\times H^s$ for $s>3/2$ and global
well-posedness in $H^s\times H^s$ for $s\geq2$. When $\rho=0$, the Zakharov-Ito
equation reduces to the KdV equation, hence has solitary waves with speeds
$c\in(0,+\infty)$. We prove the orbital stability of these solitary waves in
$H^1\times L^2$ by combining a variational approach and the framework of
Grillakis, Shatah and Strauss \cite{GSS1987}.

</details>


### [34] [Exponential local energy decay of solutions to the wave equation with $L^\infty$ electric and magnetic potentials](https://arxiv.org/abs/2506.07058)
*Andrés Larraín-Hubach,Jacob Shapiro,Georgi Vodev*

Main category: math.AP

TL;DR: Sharp resolvent estimates for the magnetic Schrödinger operator in ℝᵈ (d≥3) with L∞ short-range potentials, extended to Dirichlet self-adjoint realizations in non-trapping obstacles (d≥2) for zero magnetic potential. Applications include exponential local energy decay for wave equations with L∞ potentials, with low-frequency cutoff in all dimensions, and no cutoff needed in odd dimensions under certain conditions.


<details>
  <summary>Details</summary>
Motivation: To establish resolvent estimates for the magnetic Schrödinger operator and extend them to non-trapping obstacles, enabling applications in wave equation energy decay.

Method: Proving sharp resolvent estimates for the operator with L∞ potentials, analyzing Dirichlet realizations, and applying these to wave equations with exponential decay conditions.

Result: Resolvent estimates hold for the operator and Dirichlet realizations, leading to exponential local energy decay in wave equations, with varying conditions on low frequencies.

Conclusion: The results provide robust resolvent estimates and energy decay properties, with dimensionality and potential conditions influencing the need for low-frequency cutoff.

Abstract: In this paper we prove sharp resolvent estimates for the magnetic
Schr\"odinger operator in $\mathbb{R}^d$, $d\ge 3$, with $L^\infty$ short-range
electric and magnetic potentials. We also show that these resolvent estimates
still hold for the Dirichlet self-adjoint realization of the Schr\"odinger
operator in the exterior of a non-trapping obstacle in $\mathbb{R}^d$, $d\ge
2$, provided the magnetic potential is supposed identically zero. As an
application of the resolvent estimates, we obtain an exponential decay of the
local energy of solutions to the wave equation with $L^\infty$ electric and
magnetic potentials which decay exponentially at infinity, in all odd and even
dimensions, provided the low frequencies are cut off in a suitable way. We also
show that in odd dimensions there is no need to cut off the low frequencies in
order to get an exponential local energy decay, provided we assume that zero is
neither an eigenvalue nor a resonance.

</details>


### [35] [Almost rigidity of the Talenti-type comparison theorem on $\mathrm{RCD}(0,N)$ space](https://arxiv.org/abs/2506.07100)
*Wenjing Wu*

Main category: math.AP

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this paper, we prove a Talenti-type comparison theorem for the
$p$-Laplacian with Dirichlet boundary conditions on open subsets of a
$\mathrm{RCD}(0,N)$ space with $N\in (1,\infty)$. We also obtain an almost
rigidity result of the Talenti-type comparison theorem, whose proof relies on a
compactness on varying spaces.

</details>


### [36] [A PDE-Based Image Restoration Method: Mathematical Analysis and Implementation](https://arxiv.org/abs/2506.07132)
*Dragos-Patru Covei*

Main category: math.AP

TL;DR: A method for image restoration using an elliptic PDE, with theoretical analysis and practical implementation in Python.


<details>
  <summary>Details</summary>
Motivation: To address image degradation by formulating and solving a PDE-based model for restoration.

Method: Derives a PDE model, proves solution existence/uniqueness via Lax-Milgram, and discretizes using finite differences and fixed-point iteration.

Result: Theoretical validation of the model and a functional Python implementation are provided.

Conclusion: The proposed PDE-based method is effective for image restoration, with both theoretical and practical support.

Abstract: In this paper we present an image restoration method based on solving an
elliptic partial differential equation (PDE) of the form \begin{equation*}
-\Delta u+\lambda \,G(u)=f, \end{equation*} where $\Delta $ denotes the
Laplacian, $G(u)$ is a nonlocal operator (with the Gaussian convolution being a
particular example), and $f$ is the degraded image. We detail the derivation of
the model, prove the existence and uniqueness of its weak solution via the
Lax--Milgram theorem (assuming $% f\in L^{2}(\Omega )$), and describe the
discretization scheme using finite differences and a fixed-point iterative
method. The complete Python implementation is integrated into the article.

</details>


### [37] [Decay estimates for the compressible viscoelastic equations in an exterior domain](https://arxiv.org/abs/2506.07215)
*Jieling Deng,Yong Wang,Jianquan Yang*

Main category: math.AP

TL;DR: The paper analyzes compressible viscoelastic equations in an exterior domain, proving $L_2$ estimates for linearized solutions and decay estimates for nonlinear solutions, including optimal decay rates.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of compressible viscoelastic equations in exterior domains, focusing on solution estimates and decay properties.

Method: Proving $L_2$ estimates for linearized problems and deriving decay estimates for nonlinear solutions, emphasizing optimal decay rates.

Result: Optimal decay rates for the solution and its spatial-time derivatives in the $L_2$-norm are established.

Conclusion: The study successfully provides decay estimates and $L_2$ bounds for compressible viscoelastic equations in exterior domains.

Abstract: In this paper, we study the compressible viscoelastic equations in an
exterior domain. We prove the $L_2$ estimates for the solution to the
linearized problem and show the decay estimates for the solution to the
nonlinear problem. In particular, we obtain the optimal decay rates of the
solution itself and its spatial-time derivatives in the $L_2$-norm.

</details>


### [38] [Gradient estimates for Leibenson's equation on Riemannian manifolds](https://arxiv.org/abs/2506.07221)
*Philipp Sürig*

Main category: math.AP

TL;DR: Gradient estimates for positive solutions of the doubly nonlinear evolution equation on Riemannian manifolds with Ricci curvature bounded below.


<details>
  <summary>Details</summary>
Motivation: To analyze solutions of the Leibenson equation (doubly nonlinear evolution equation) on Riemannian manifolds, particularly under curvature constraints.

Method: Proving gradient estimates for positive solutions, distinguishing between slow diffusion (q(p-1)>1) and fast diffusion (q(p-1)<1) cases.

Result: Gradient estimates are derived under the condition that the Ricci curvature is bounded below by a non-positive constant.

Conclusion: The study provides insights into the behavior of solutions under different diffusion regimes and curvature conditions.

Abstract: We consider on Riemannian manifolds solutions of the Leibenson equation
\begin{equation*} \partial _{t}u=\Delta _{p}u^{q}. \end{equation*} This
equation is also known as doubly nonlinear evolution equation. We prove
gradient estimates for positive solutions $u$ under the condition that the
Ricci curvature on $M$ is bounded from below by a non-positive constant. We
distinguish between the case $q(p-1)>1$ (slow diffusion case) and the case
$q(p-1)<1$ (fast diffusion case).

</details>


### [39] [Solvability of the Zakharov-Shabat systems with meromorphic potentials by quadrature](https://arxiv.org/abs/2506.07246)
*Kazuyuki Yagasaki*

Main category: math.AP

TL;DR: The paper proves that two-dimensional Zakharov-Shabat systems with meromorphic potentials are solvable by quadrature if and only if the potentials are reflectionless, under certain integrability conditions.


<details>
  <summary>Details</summary>
Motivation: Understanding solvability of ZS systems is crucial for analytical solutions to integrable PDEs via the inverse scattering transform.

Method: The study uses differential Galois theory to analyze solvability by quadrature for ZS systems with meromorphic potentials.

Result: ZS systems are integrable (solvable by quadrature) if and only if potentials are reflectionless, given specific integrability conditions.

Conclusion: This extends prior results, confirming solvability conditions for a broader class of potentials in integrable systems.

Abstract: We study the solvability of the general two-dimensional Zakharov-Shabat (ZS)
systems with meromorphic potentials by quadrature. These systems appear in
application of the inverse scattering transform (IST) to an important class of
nonlinear partial differential equations (PDEs) called integrable systems.
Their solvability by quadrature is a key to obtain analytical expressions for
solutions to the initial value problems of the integrable PDEs by using the
IST. We prove that the ZS systems are always integrable in the sense of
differential Galois theory, i.e., solvable by quadrature, if and only if the
meromporphic potentials are reflectionless, under the condition that the
potentials are absolutely integrable on $\mathbb{R}\setminus(-R_0,R_0)$ for
some $R_0>0$. Similar statements were previously proved to be true by the
author for a limited class of potentials and the linear Schr\"odinger
equations.

</details>


### [40] [Uniform a priori bounds for Slightly Subcritical Elliptic Problems](https://arxiv.org/abs/2506.07269)
*Mabel Cuesta,Rosa Pardo*

Main category: math.AP

TL;DR: The paper derives a uniform $L^{\infty}(\Omega)$ bound for positive weak solutions to a subcritical, superlinear elliptic problem using weighted norm estimates, moving planes method, and Pohozaev's identity.


<details>
  <summary>Details</summary>
Motivation: To establish uniform bounds for solutions of elliptic problems with nonlinearities that are slightly subcritical and superlinear, which are common in PDE analysis.

Method: Combines weighted $L^1(\Omega)$ norm estimates, moving planes method, elliptic regularity theory, Pohozaev's identity, and Morrey's Theorem to derive bounds.

Result: A uniform $L^{\infty}(\Omega)$ bound for solutions, valid near the boundary and in the interior of the domain.

Conclusion: The approach successfully provides uniform bounds for solutions of such elliptic problems, leveraging a mix of analytical techniques.

Abstract: We obtain a uniform $L^{\infty}(\Omega)$ a priori bound, for any positive
weak solutions to elliptic problem with a nonlinearity $f$ slightly
subcritical, slightly superlinear, and regularly varying.
  To achieve our result, we first obtain a uniform estimate of an specific
$L^1(\Omega)$ weighted norm. This, combined with moving planes method and
elliptic regularity theory, provides a uniform $L^\infty$ bound in a
neighborhood of the boundary of $\Omega$. Next, by using Pohozaev's identity,
we obtain a uniform estimate of one weighted norm of the solutions. Joining now
elliptic regularity theory, and Morrey's Theorem, we estimate from below the
radius of a ball where a solution exceeds the half of its
$L^\infty(\Omega)$-norm. Finally, going back to the previous uniform weighted
norm estimate, we conclude our result.

</details>


### [41] [A functional inequality between Hessians in spaces with non-zero curvature](https://arxiv.org/abs/2506.07292)
*Tomasz Cieślak,Michał Gaczkowski,Wojciech Kryński*

Main category: math.AP

TL;DR: A functional inequality between Hessians of square root and logarithm of positive functions is extended to spaces with non-zero curvature.


<details>
  <summary>Details</summary>
Motivation: To generalize a known functional inequality to more complex geometric settings, specifically spaces with non-zero curvature.

Method: Proving the inequality by adapting techniques to account for curvature in the space.

Result: The inequality holds in spaces with non-zero curvature, extending its applicability.

Conclusion: The work broadens the scope of the original inequality, demonstrating its validity in curved spaces.

Abstract: A version of the recent functional inequality between the Hessians of the
square root and the logarithm of positive functions is proven in spaces with
non-zero curvature.

</details>


### [42] [On the stationary Navier-Stokes equations in distorted pipes under energy-stable outflow boundary conditions](https://arxiv.org/abs/2506.07331)
*Alessio Falocchi,Ana Leonor Silvestre,Gianmarco Sperone*

Main category: math.AP

TL;DR: The paper models fluid motion in distorted pipes using Navier-Stokes equations with mixed boundary conditions, proving existence of weak solutions without data restrictions and unique solvability under small data assumptions.


<details>
  <summary>Details</summary>
Motivation: To analyze viscous incompressible fluid flow in finite-length distorted pipes under mixed boundary conditions, addressing challenges in modeling and solution existence.

Method: Uses Navier-Stokes equations with mixed boundary conditions (inlet: Lions-Magenes class; outlet: directional do-nothing; walls: no-slip). Existence proved via Leray-Schauder Principle and contradiction argument using Bernoulli's law and harmonic divergence-free vector fields.

Result: Existence of weak solutions is proven without data restrictions. Unique solvability is shown under small data assumptions.

Conclusion: The study successfully models and analyzes fluid flow in distorted pipes, providing theoretical guarantees for solution existence and uniqueness under specific conditions.

Abstract: The steady motion of a viscous incompressible fluid in distorted pipes, of
finite length, is modeled through the Navier-Stokes equations with mixed
boundary conditions: the inflow is given by an arbitrary member of the
Lions-Magenes class with positive influx, and the fluid motion is subject to a
directional do-nothing boundary condition on the outlet, together with the
standard no-slip assumption on the remaining walls of the domain. Existence of
a weak solution to such Navier-Stokes system is proved without any restriction
on the data (that is, inlet velocity and external force) by means of the
Leray-Schauder Principle, in which the required a priori estimate is obtained
by a contradiction argument that employs Bernoulli's law for solutions of the
stationary Euler equations, as well as some properties of harmonic
divergence-free vector fields. Under a suitable smallness assumption on the
data, we also prove the unique solvability of the boundary-value problem.

</details>


### [43] [Another look at quasilinear Schrödinger equations with prescribed mass via dual method](https://arxiv.org/abs/2506.07346)
*Jianhua Chen,Vicentiu D. Radulescu,Jijiang Sun,Jian Zhang*

Main category: math.AP

TL;DR: The paper studies ground state normalized solutions for a quasilinear Schrödinger equation using a transformation and dual methods, proving existence and analyzing asymptotic behavior.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of finding normalized solutions under mass constraints and general growth conditions for quasilinear Schrödinger equations.

Method: Employs a transformation $u=f(v)$, a novel stretching mapping, and dual methods to reformulate and solve the problem.

Result: Proves existence of solutions via constrained minimization, ground state solutions under supercritical growth, and solutions under critical growth. Also analyzes asymptotic behavior.

Conclusion: The methodology extends prior work and can be adapted for related problems in quasilinear Schrödinger equations.

Abstract: In this paper, we aim to study the existence of ground state normalized
solutions for the following quasilinear Schr\"{o}dinger equation $-\Delta
u-\Delta(u^2)u=h(u)+\lambda u,\,\, x\in\R^N$, under the mass constraint
$\int_{\R^N}|u|^2\text{d}x=a,$ where $N\geq2$, $a>0$ is a given mass, $\lambda$
is a Lagrange multiplier and $h$ is a nonlinear reaction term with some
suitable conditions. By employing a suitable transformation $u=f(v)$, we
reformulate the original problem into the equivalent form $-\Delta v
=h(f(v))f'(v)+\lambda f(v)f'(v),\,\, x\in\R^N,$ with prescribed mass $
\int_{\R^N}|f(v)|^2\text{d}x=a. $ To address the challenge posed by the
$L^2$-norm $\|f(v)\|^2_2$ not necessarily equaling
  $a$, we introduce a novel stretching mapping: $
v_t(x):=f^{-1}(t^{N/2}f(v(tx))). $ This construction, combined with a dual
method and detailed analytical techniques, enables us to establish the
following existence results:
  (1)Existence of solutions via constrained minimization using dual methods;
  (2) Existence of ground state normalized solutions under general
  $L^2$-supercritical growth conditions, along with nonexistence results,
analyzed via dual methods;
  (3)Existence of normalized solutions under critical growth conditions,
treated via dual methods.
  Additionally, we analyze the asymptotic behavior of the ground state energy
obtained in {\bf(P2)}. Our results extend and refine those of
Colin-Jeanjean-Squassina [Nonlinearity 20: 1353-1385, 2010], of
Jeanjean-Luo-Wang [J. Differ. Equ. 259: 3894-3928, 2015], of Li-Zou [Pacific J.
Math. 322: 99-138, 2023], of Zhang-Li-Wang [Topol. Math. Nonl. Anal. 61:
465-489, 2023] and so on. We believe that the methodology developed here can be
adapted to study related problems concerning the existence of normalized
solutions for quasilinear Schr\"{o}dinger equations via the dual method.

</details>


### [44] [Singular Dirichlet boundary problems for a class of fully nonlinear parabolic equations in one dimension](https://arxiv.org/abs/2506.07415)
*Takashi Kagaya*

Main category: math.AP

TL;DR: The paper examines the initial value problem for fully nonlinear parabolic equations with singular Dirichlet boundary conditions, focusing on solution existence and non-existence based on the equation and initial function.


<details>
  <summary>Details</summary>
Motivation: To understand how the interior equation and initial function boundedness affect solution existence for singular Dirichlet boundary problems in nonlinear parabolic equations.

Method: Analyzes the problem for fully nonlinear parabolic equations, including examples like $p$-Laplace heat equation and $eta$-power curvature flow, with singular Dirichlet conditions.

Result: Identifies conditions under which solutions exist or do not exist, depending on the interior equation and initial function properties.

Conclusion: The study provides insights into the solvability of such problems, linking it to the equation's structure and initial data.

Abstract: In this paper, we deal with the initial value problem for a class of fully
nonlinear parabolic equations with a singular Dirichlet boundary condition in
one space dimension. The interior equation includes, for example, a fully
nonlinear $p$-Laplace type heat equation and a $\beta$-power type curvature
flow. The singular Dirichlet boundary condition depicts, for example, the
asymptoticness of the ends of complete curve to parallel two lines in geometric
flow of graphs. We study the dependence of the existence and non-existence of
solution to the problem on the interior equation and the boundedness of the
initial function.

</details>


### [45] [Critical singular problems in Carnot groups](https://arxiv.org/abs/2506.07521)
*Stefano Biagi,Mattia Galeotti,Eugenio Vecchi*

Main category: math.AP

TL;DR: Existence of two positive weak solutions for a power-type mild singular perturbation of a Dirichlet semilinear critical problem in a Carnot group.


<details>
  <summary>Details</summary>
Motivation: To address the lack of results on singular perturbations of power-type in the context of semilinear PDEs in Carnot groups.

Method: First solution via variational Perron's method; second solution using estimates of a family of functions analogous to Aubin-Talenti functions.

Result: Two positive weak solutions are proven to exist.

Conclusion: The study extends semilinear PDE results in Carnot groups to include power-type singular perturbations.

Abstract: We consider a power-type mild singular perturbation of a Dirichlet semilinear
critical problem settled in an open and bounded set in a Carnot group. Here,
the term critical has to be understood in the sense of the Sobolev embedding.
We aim to prove the existence of two positive weak solutions: the first one is
obtained by means of the variational Perron's method, while for the second one
we adapt a classical argument relying on proper estimates of a family of
functions which mimic the role of the classical Aubin-Talenti functions in the
Euclidean setting. Our results fall in the framework of semilinear PDEs in
Carnot group but, as far as we know, are the first ones dealing with singular
perturbations of power-type.

</details>


### [46] [Tug-of-war games related to $p$-Laplace type equations with lower order terms](https://arxiv.org/abs/2506.07537)
*Jeongmin Han*

Main category: math.AP

TL;DR: The paper studies a tug-of-war game with a constant payoff discount rate, modeling it with $p$-Laplace type PDEs, and analyzes existence, uniqueness, and regularity of solutions.


<details>
  <summary>Details</summary>
Motivation: To understand the mathematical properties of tug-of-war games with discounted payoffs and their corresponding PDE models.

Method: Uses $p$-Laplace type equations with measurable ingredients to model the game and analyzes existence, uniqueness, and regularity of solutions.

Result: Provides existence, uniqueness, and regularity results for the game value functions and discusses properties of solutions.

Conclusion: The study offers insights into the mathematical behavior of tug-of-war games and their PDE models, with implications for game theory and analysis.

Abstract: In this paper, we investigate a certain type of tug-of-war game, including a
constant payoff discount rate at every turn. The model PDEs of these games are
$p$-Laplace type equations with measurable ingredients. We provide existence,
uniqueness and regularity results for game value functions. Furthermore, we
consider relevant properties of solutions to the model problems based on the
discussion about the games.

</details>


### [47] [Mission $p<n-1$: Possible -- Nonlinear Elasticity Beyond Conventional Limits](https://arxiv.org/abs/2506.07543)
*Daniel Campbell,Anna Doležalová,Stanislav Hencl*

Main category: math.AP

TL;DR: The paper proves lower semicontinuity for a Neohookean-type energy in Nonlinear Elasticity, extending to cases where $p<n-1$, and introduces a model for cavitations with surface measure added to the energy.


<details>
  <summary>Details</summary>
Motivation: To address gaps in Nonlinear Elasticity by allowing for $p<n-1$ and modeling cavitations, which were previously unexplored.

Method: Uses weak limits of Sobolev $W^{1,p}$ homeomorphisms and introduces a cavitation model with surface measure in the energy functional.

Result: Lower semicontinuity is proven for both the Neohookean-type energy and the new cavitation model.

Conclusion: The work extends theoretical understanding of Nonlinear Elasticity and provides a foundation for modeling cavitations.

Abstract: In this paper we prove the lower semicontinuity of a Neohookean-type energy
for a model of Nonlinear Elasticity that allows, for the first time, for
$p<n-1$. Our class of admissible deformations consists of weak limits of
Sobolev $W^{1,p}$ homeomorphisms. We also introduce a model that allows for
cavitations by studying weak limits of homeomorphisms that can open cavities at
some points. In this model we add the measure of the created surface to the
energy functional and for this functional we again prove lower semicontinuity.

</details>


### [48] [Existence and Uniqueness for the Fractional Gelfand Equation in $\mathbb{R}$](https://arxiv.org/abs/2506.07577)
*Florian P. Lanz,Enno Lenzmann*

Main category: math.AP

TL;DR: The paper proves existence, symmetry, and uniqueness of solutions to the fractional Gelfand equation in ℝ, along with properties like finite Morse index and nondegenerate linearized operator. It extends results to a generalized equation with a class of functions K.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the fractional Gelfand equation, a nonlinear problem, and generalize it to include a broader class of functions K, contributing to understanding fractional calculus and nonlinear PDEs.

Method: The authors use a fixed point scheme involving the function v=√(e^u) and a nonlocal shooting method with compact nonlinear maps.

Result: Existence, symmetry, and uniqueness of solutions are proven for s ∈ (1/2,1), with additional properties like finite Morse index and nondegenerate linearized operator. The results extend to a generalized equation with K.

Conclusion: The paper successfully solves the fractional Gelfand equation and its generalization, providing insights into fractional nonlinear problems and their solutions.

Abstract: We prove existence, symmetry and uniqueness of solutions to the fractional
Gelfand equation
  $$
  (-\Delta)^s u = e^u \quad \mbox{in $\mathbb{R}$} \quad \mbox{with} \quad
\int_{\mathbb{R}} e^u dx < +\infty
  $$ for all exponents $s \in (\frac{1}{2},1)$. Furthermore, we show $u$ has
finite Morse index and that its linearized operator is nondegenerate. Our
arguments are based on a fixed point scheme in terms of the function $v=
\sqrt{e^u}$ and we devise a nonlocal shooting method involving (locally)
compact nonlinear maps. We also study existence, symmetry and uniqueness of
solutions to $(-\Delta)^s u = K e^u$ in $\mathbb{R}$ with $K e^u \in
L^1(\mathbb{R})$ for a general class of positive, even and monotone-decreasing
functions $K > 0$.

</details>


### [49] [On the stability of the annulus for the torsion of multiply connected domains](https://arxiv.org/abs/2506.07592)
*Vincenzo Amato,Luca Barbato*

Main category: math.AP

TL;DR: A quantitative isoperimetric inequality for torsion in multiply connected domains shows optimal shapes are annuli, and domains with near-optimal torsion are close to annuli.


<details>
  <summary>Details</summary>
Motivation: To understand how domains with given area and hole areas behave under torsional rigidity constraints, particularly how they approximate annular shapes when torsion is near optimal.

Method: Establish a quantitative version of the isoperimetric inequality for torsion in multiply connected domains, analyzing domains with given area and hole areas.

Result: Domains with nearly optimal torsional rigidity must be close to an annulus.

Conclusion: The study confirms the annulus as the optimal shape for torsion in such domains and quantifies proximity to annularity for near-optimal cases.

Abstract: We establish a quantitative version of the isoperimetric inequality for the
torsion of multiply connected domains, among sets with given area and with
given joint area of the holes.
  Since the optimal shape is the annulus, we investigate how a given domain
approaches an annular configuration when its torsion is close to the optimal
value. Our result shows that when the torsional rigidity is nearly optimal, the
domain $\Omega$ must be close to an annulus.

</details>


### [50] [Sharp quantitative stability estimates for the Brezis-Nirenberg problem](https://arxiv.org/abs/2506.07602)
*Haixia Chen,Seunghyeok Kim,Juncheng Wei*

Main category: math.AP

TL;DR: First quantitative stability result for Sobolev inequality on bounded domains, revealing unexpected stability exponents due to interactions between solution, linear term, bubble formation, and boundary effects.


<details>
  <summary>Details</summary>
Motivation: To address the lack of quantitative stability results for the Sobolev inequality on bounded domains, focusing on the Brezis-Nirenberg problem.

Method: Refines and streamlines existing arguments while tackling new analytical challenges, particularly capturing boundary effects quantitatively.

Result: Discovery of unexpected stability exponents and successful quantification of boundary effects, distinguishing this work from prior Euclidean and manifold cases.

Conclusion: Provides a foundational quantitative stability result for the Sobolev inequality on bounded domains, advancing understanding of the Brezis-Nirenberg problem.

Abstract: We study the quantitative stability for the classical Brezis-Nirenberg
problem associated with the critical Sobolev embedding $H^1_0(\Omega)
\hookrightarrow L^{\frac{2n}{n-2}}(\Omega)$ in a smooth bounded domain $\Omega
\subset \mathbb{R}^n$ ($n \geq 3$). To the best of our knowledge, this work
presents the first quantitative stability result for the Sobolev inequality on
bounded domains. A key discovery is the emergence of unexpected stability
exponents in our estimates, which arise from the intricate interaction among
the nonnegative solution $u_0$ and the linear term $\lambda u$ of the
Brezis--Nirenberg equation, bubble formation, and the boundary effect of the
domain $\Omega$. One of the main challenges is to capture the boundary effect
quantitatively, a feature that fundamentally distinguishes our setting from the
Euclidean case treated in \cite{CFM, FG, DSW} and the smooth closed manifold
case studied in \cite{CK}. In addressing a variety of difficulties, our proof
refines and streamlines several arguments from the existing literature while
also resolving new analytical challenges specific to our setting.

</details>


### [51] [Fractional Sobolev spaces and fractional $p$-Laplace equations on locally finite graphs](https://arxiv.org/abs/2506.07694)
*Mengjie Zhang,Yong Lin,Yunyan Yang*

Main category: math.AP

TL;DR: The paper constructs fractional Sobolev spaces on general graphs and introduces a fractional $p$-Laplace operator to study nonlinear Schrödinger equations, providing analytical tools for fractional problems on graphs.


<details>
  <summary>Details</summary>
Motivation: While graph-based analysis is well-studied, fractional problems on graphs remain underexplored, especially beyond lattice graphs. This paper aims to fill this gap.

Method: Fractional Sobolev spaces are constructed on connected, locally finite, and stochastically complete graphs. A fractional $p$-Laplace operator is proposed to analyze nonlinear Schrödinger equations.

Result: The constructed spaces exhibit completeness and reflexivity. Solutions to nonlinear Schrödinger equations involving the fractional $p$-Laplace operator are studied.

Conclusion: The paper establishes a comprehensive framework for analyzing fractional problems on general graphs, advancing the field beyond lattice graphs.

Abstract: Graph-based analysis holds both theoretical and applied significance,
attracting considerable attention from researchers and yielding abundant
results in recent years. However, research on fractional problems remains
limited, with most of established results restricted to lattice graphs. In this
paper, fractional Sobolev spaces are constructed on general graphs that are
connected, locally finite and stochastically complete. Under certain
assumptions, these spaces exhibit completeness, reflexivity, and other
properties. Moreover, we propose a fractional $p$-Laplace operator, and study
the existence of solutions to some nonlinear Schr\"odinger type equations
involving this nonlocal operator. The main contribution of this paper is to
establish a relatively comprehensive set of analytical tools for studying
fractional problems on graphs.

</details>


### [52] [Stability of 2-soliton solutions for the modified Camassa-Holm equation with cubic nonlinearity](https://arxiv.org/abs/2506.07791)
*Xijun Deng,Stéphane Lafortune,Zhisu Liu*

Main category: math.AP

TL;DR: The paper proves the nonlinear stability of 2-soliton solutions for the modified Camassa-Holm equation with cubic nonlinearity under perturbations in the Sobolev space $H^2$.


<details>
  <summary>Details</summary>
Motivation: To analyze the stability of 2-soliton solutions in the modified Camassa-Holm equation, focusing on perturbations in the momentum variable.

Method: Uses conserved quantities in terms of the momentum variable $m$ to study stability.

Result: The 2-soliton solution is nonlinearly stable to perturbations in $H^2$.

Conclusion: The findings confirm the robustness of 2-soliton solutions under perturbations in the specified space.

Abstract: In this paper, we are concerned with the stability of 2-soliton solutions for
the modified Camassa-Holm equation with cubic nonlinearity. By employing
conserved quantities in terms of the momentum variable $m$, we show that the
2-soliton, when regarded as a solution to the initial-value problem for the
modified Camassa-Holm equation, is nonlinearly stable to perturbations with
respect to the momentum variable in the Sobolev space $H^2$.

</details>


### [53] [Large-time behavior of pressureless Euler--Poisson equations with background states](https://arxiv.org/abs/2506.07812)
*Young-Pil Choi,Dong-ha Kim,Dowan Koo,Eitan Tadmor*

Main category: math.AP

TL;DR: The paper analyzes the long-term behavior of solutions to a damped pressureless Euler-Poisson system with variable background states, showing convergence to equilibrium under specific conditions.


<details>
  <summary>Details</summary>
Motivation: To understand the asymptotic stability of solutions in damped Euler-Poisson systems, particularly with nontrivial background structures, which is relevant in cold plasma ion dynamics.

Method: Combines phase plane analysis and hypocoercivity-type estimates to prove convergence of global classical solutions to equilibrium.

Result: Solutions converge to equilibrium when the background density approaches a positive constant, with exponential convergence demonstrated in the cold plasma application.

Conclusion: The study rigorously characterizes asymptotic stability for damped Euler-Poisson systems with nontrivial backgrounds, providing insights for applications like plasma dynamics.

Abstract: We study the large-time asymptotic behavior of solutions to the
one-dimensional damped pressureless Euler-Poisson system with variable
background states, subject to a neutrality condition. In the case where the
background density converges asymptotically to a positive constant, we
establish the convergence of global classical solutions toward the
corresponding equilibrium state. The proof combines phase plane analysis with
hypocoercivity-type estimates. As an application, we analyze the damped
pressureless Euler--Poisson system arising in cold plasma ion dynamics, where
the electron density is modeled by a Maxwell-Boltzmann relation. We show that
solutions converge exponentially to the steady state under suitable a priori
bounds on the density and velocity fields. Our results provide a rigorous
characterization of asymptotic stability for damped Euler-Poisson systems with
nontrivial background structures.

</details>


### [54] [Global weak solutions to a compressible Navier--Stokes/Cahn--Hilliard system with singular entropy of mixing](https://arxiv.org/abs/2506.07835)
*Danica Basarić,Andrea Giorgini*

Main category: math.AP

TL;DR: The paper establishes global-in-time weak solutions for a Navier-Stokes/Cahn-Hilliard system with Flory-Huggins entropy, valid for large initial data in 3D bounded domains.


<details>
  <summary>Details</summary>
Motivation: To address the lack of studies incorporating the physically relevant Flory-Huggins entropy in compressible binary mixtures, unlike previous works using regular potentials.

Method: Derives new estimates for the chemical potential and Flory-Huggins entropy in a density-dependent Cahn-Hilliard equation, assuming non-negative γ-integrable density (γ>3/2).

Result: Existence of global weak solutions is proven, with the phase variable confined to the physical interval [-1,1] where density is positive.

Conclusion: The work extends the understanding of phase separation in compressible mixtures by incorporating realistic entropy and minimal assumptions.

Abstract: We study a Navier-Stokes/Cahn-Hilliard system modeling the evolution of a
compressible binary mixture of viscous fluids undergoing phase separation. The
novelty of this work is a free energy potential including the physically
relevant Flory-Huggins (logarithmic) entropy, as opposed to previous studies in
the literature, which only consider regular potentials with polynomial growth.
Our main result establishes the existence of global-in-time weak solutions in
three-dimensional bounded domains for arbitrarily large initial data. The core
contribution is the derivation of new estimates for the chemical potential and
the Flory-Huggins entropy arising from a density-dependent Cahn-Hilliard
equation under minimal assumptions: non-negative $\gamma$-integrable density
with $\gamma>\frac32$. In addition, we prove that the phase variable, which
represents the difference of the mass concentrations, takes value within the
physical interval $[-1,1]$ almost everywhere on the set where the density is
positive.

</details>


### [55] [Control strategies and trends to equilibrium for kinetic models of opinion dynamics driven by social activity](https://arxiv.org/abs/2506.07840)
*Andrea Bondesan,Jacopo Borsotti*

Main category: math.AP

TL;DR: The paper introduces kinetic equations for opinion dynamics, showing polarization in low-activity agents and consensus in active ones. It proposes a control strategy to boost social activity and analyzes convergence to equilibrium using entropy methods.


<details>
  <summary>Details</summary>
Motivation: To model how social activity levels influence opinion dynamics and prevent extreme polarization by promoting active interactions.

Method: Uses kinetic equations and a control strategy to adjust agent activity. Analyzes convergence via entropy methods and Fokker-Planck equations.

Result: Polarization occurs in low-activity agents, while active ones reach consensus. Control strategies effectively shift the population toward higher activity.

Conclusion: Social activity is key to preventing extreme opinions. The proposed control strategy and entropy-based analysis provide tools for managing opinion dynamics.

Abstract: We introduce new kinetic equations modeling opinion dynamics inside a
population of individuals, whose propensity to interact with each other is
described by their level of social activity. We show that opinion polarization
can arise among agents with a low activity level, while active ones develop a
consensus, highlighting the importance of social interactions to prevent the
formation of extreme opinions. Moreover, we present a realistic control
strategy aimed at reducing the number of inactive agents and increasing the
number of socially active ones. At last, we prove several (weak and strong)
convergence to equilibrium results for such controlled model. In particular, by
considering additional interactions between individuals and opinion leaders
capable of steering the average opinion of the population, we use entropy
method-like techniques to estimate the relaxation toward equilibrium of
solutions to a Fokker-Planck equation with adjoint given by a
Wright-Fisher-type model with time-dependent coefficients.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [56] [Scientific machine learning in Hydrology: a unified perspective](https://arxiv.org/abs/2506.06308)
*Adoubi Vincent De Paul Adombi*

Main category: physics.comp-ph

TL;DR: The paper reviews SciML in hydrology, proposing a unified framework for its diverse methods to address fragmentation and foster progress.


<details>
  <summary>Details</summary>
Motivation: The fragmentation of SciML methods in hydrology complicates assessing novelty and identifying advances, necessitating a unified framework.

Method: The review categorizes SciML into families (e.g., physics-informed ML) and proposes a coherent structure for each.

Result: A unified framework is introduced to clarify concepts and support cumulative progress in hydrological modeling.

Conclusion: The paper highlights limitations and future opportunities of SciML in hydrology, guiding systematic research.

Abstract: Scientific machine learning (SciML) provides a structured approach to
integrating physical knowledge into data-driven modeling, offering significant
potential for advancing hydrological research. In recent years, multiple
methodological families have emerged, including physics-informed machine
learning, physics-guided machine learning, hybrid physics-machine learning, and
data-driven physics discovery. Within each of these families, a proliferation
of heterogeneous approaches has developed independently, often without
conceptual coordination. This fragmentation complicates the assessment of
methodological novelty and makes it difficult to identify where meaningful
advances can still be made in the absence of a unified conceptual framework.
This review, the first focused overview of SciML in hydrology, addresses these
limitations by proposing a unified methodological framework for each SciML
family, bringing together representative contributions into a coherent
structure that fosters conceptual clarity and supports cumulative progress in
hydrological modeling. Finally, we highlight the limitations and future
opportunities of each unified family to guide systematic research in hydrology,
where these methods remain underutilized.

</details>


### [57] [Nature of Hydrated Electron in Varied Solvation Environments](https://arxiv.org/abs/2506.07157)
*Ritama Kar,Nisanth N. Nair*

Main category: physics.comp-ph

TL;DR: The study investigates solvated electrons in various aqueous environments using MD simulations with hybrid density functionals, revealing factors like dangling OH groups and water molecule arrangement that influence electron localization and stability.


<details>
  <summary>Details</summary>
Motivation: Understanding solvated electrons is crucial for chemical and biological phenomena, prompting this detailed examination of their behavior in different aqueous environments.

Method: Molecular dynamics (MD) simulations with hybrid density functionals, using a computationally efficient resonance-free multiple time-stepping method.

Result: The study provides insights into how excess electrons are solvated, identifying factors like dangling OH groups and water molecule spatial arrangement as key determinants of electron localization and stability.

Conclusion: The findings offer a detailed understanding of solvated electrons in diverse aqueous environments, highlighting structural and dynamical influences on their behavior.

Abstract: Understanding the nature of solvated electrons is important in studying a
range of chemical and biological phenomena. This study investigates the
structural and dynamical behavior of an excess electron in water, examining
different solvation environments, including liquid water, ice, monolayer, and
chain. To accurately model these systems, we carry out molecular dynamics (MD)
simulations using hybrid density functionals, employing the computationally
efficient resonance-free multiple time-stepping based adaptively compressed
exchange operator method. Through these simulations, we create a comprehensive
and detailed picture of how excess electrons are solvated across different
aqueous environments. We report the factors influence the localization and
dynamic stability of the hydrated electron. The determinants include the
presence and reorganization flexibility of the dangling OH groups and the
spatial arrangement of the surrounding water molecules.

</details>


### [58] [A Study on the Fine-Tuning Performance of Universal Machine-Learned Interatomic Potentials (U-MLIPs)](https://arxiv.org/abs/2506.07401)
*Xiaoqing Liu,Kehan Zeng,Yangshuai Wang,Teng Zhao*

Main category: physics.comp-ph

TL;DR: Fine-tuning MACE-based foundation models improves accuracy and convergence in atomistic simulations, with dataset selection playing a key role.


<details>
  <summary>Details</summary>
Motivation: To enhance the task-specific accuracy of universal machine-learned interatomic potentials (U-MLIPs) through fine-tuning.

Method: Investigate fine-tuning of MACE-MP-0 and MACE-MP-0b models on task-specific datasets, using filtering or active learning for dataset optimization.

Result: Fine-tuned models achieve higher accuracy and faster convergence compared to models trained from scratch.

Conclusion: Careful dataset selection and fine-tuning strategies are crucial for optimizing foundation models in atomistic simulations, with potential for future development.

Abstract: Universal machine-learned interatomic potentials (U-MLIPs) have demonstrated
effectiveness across diverse atomistic systems but often require fine-tuning
for task-specific accuracy. We investigate the fine-tuning of two MACE-based
foundation models, MACE-MP-0 and its variant MACE-MP-0b, and identify key
insights. Fine-tuning on task-specific datasets enhances accuracy and, in some
cases, outperforms models trained from scratch. Additionally, fine-tuned models
benefit from faster convergence due to the strong initial predictions provided
by the foundation model. The success of fine-tuning also depends on careful
dataset selection, which can be optimized through filtering or active learning.
We further discuss practical strategies for achieving better fine-tuning
foundation models in atomistic simulations and explore future directions for
their development and applications.

</details>


### [59] [First-principles Quantum Insights into Bandgap Engineering, Valley Quantum Hall Effect, and Nonlinear Optical Response of Ge-Doped Graphene for Potential Optoelectronic Applications](https://arxiv.org/abs/2506.07745)
*Abdul Sattar,Sana Maroof,Azmat Iqbal Bashir,Muhammad Irfan,Hamid Latif,Hina Mustafa,Ahmad Saeed,Raja Junaid Amjad,Farah Alvi*

Main category: physics.comp-ph

TL;DR: The study explores Ge-doped graphene for valleytronics, showing tunable bandgap and valley properties via doping, with potential for nonlinear optics and quantum applications.


<details>
  <summary>Details</summary>
Motivation: To enhance valley selectivity and bandgap in graphene for next-gen valleytronic devices, quantum computing, and optoelectronics.

Method: First-principles density functional theory to study Ge doping effects (2%-12.5%) on bandgap, valley Hall effect, transport, and optical properties.

Result: Ge doping tunes bandgap, valley polarization, and second harmonic generation; Berry curvature asymmetry enables valley-dependent transport and Hall effect.

Conclusion: Ge-doped graphene shows promise for valleytronics and nonlinear optics, offering insights into topological and transport properties.

Abstract: The valley in the band structure of materials has gained a lot of attention
recently. The promising applications of the valley degree of freedom include
the next-generation valleytronic devices, quantum information processing,
quantum computing, and optoelectronic devices. Graphene is an ideal quantum
material for high-speed valleytronic applications because of its high carrier
mobility and convenience of bandgap engineering. Employing first-principles
density functional theoretical approach, this study opted bandgap engineering
strategy via Germanium doping to open bandgap and enhance valley selectivity in
graphene monolayers. The impact of Ge dopant concentration of 2%, 3.125%, 5.5%,
and 12.5% is explored on the valleytronic; valley Hall effect, valley
transport, and optical properties. The reported results demonstrate that
bandgap, valley polarization, and second harmonic generation can be tuned
effectively by varying doping concentration of Germanium. The Berry curvature
profile is antisymmetric for corresponding K and K' valleys, thus leading to
valley-dependent transport properties and a potential valley Hall effect.
Finally, the second-order susceptibilities exhibit corresponding optical
absorption peaks, indicating efficient second-harmonic generation due to the
broken inversion symmetry. These findings highlight the potential of Ge-doped
graphene for nonlinear optics and valleytronics applications, while providing
novel insights into its topological phase and transport properties.

</details>


### [60] [A unified fluid model for nonthermal plasmas and reacting flows](https://arxiv.org/abs/2506.07792)
*Xiao Shao,Deanna A. Lacoste,Hong G. Im*

Main category: physics.comp-ph

TL;DR: A unified fluid modeling framework for reacting flows coupled with nonthermal plasmas (NTPs) is presented, integrating ChemPlasKin and OpenFOAM into reactPlasFOAM. The solver dynamically switches between modes (streamer, spark, reacting flow, ionic wind) and solves the electron Boltzmann equation for high-fidelity plasma chemistry. Adaptive mesh refinement and parallelization enhance efficiency. Validated against benchmarks, it predicts NTP properties and plasma-combustion coupling.


<details>
  <summary>Details</summary>
Motivation: To develop a versatile, high-performance solver for simulating coupled plasma-combustion systems, addressing limitations of conventional methods that rely on pre-tabulated data.

Method: The solver integrates ChemPlasKin and OpenFOAM, dynamically switches modes, solves the electron Boltzmann equation, and employs adaptive mesh refinement and parallelization.

Result: Validated against benchmarks, the solver accurately predicts NTP properties like fast heating and radical production, demonstrating plasma-combustion coupling.

Conclusion: The framework successfully simulates coupled plasma-combustion systems with high fidelity and efficiency, revealing insights into their interactions.

Abstract: This work presents a unified fluid modeling framework for reacting flows
coupled with nonthermal plasmas (NTPs). Building upon the gas-plasma kinetics
solver, ChemPlasKin, and the CFD library, OpenFOAM, the integrated solver,
reactPlasFOAM, allows simulation of fully coupled plasma-combustion systems
with versatility and high performance. By simplifying the governing equations
according to the dominant physical phenomena at each stage, the solver
seamlessly switches between four operating modes: streamer, spark, reacting
flow, and ionic wind, using coherent data structures. Unlike conventional
streamer solvers that rely on pre-tabulated or fitted electron transport
properties and reaction rates as functions of the reduced electric field or
electron temperature, our approach solves the electron Boltzmann equation (EBE)
on the fly to update the electron energy distribution function (EEDF) at the
cell level. This enables a high-fidelity representation of evolving plasma
chemistry and dynamics by capturing temporal and spatial variations in mixture
composition and temperature. To improve computational efficiency for this
multiscale, multiphysics system, we employ adaptive mesh refinement (AMR) in
the plasma channel, dynamic load balancing for parallelization, and time-step
subcycling for fast and slow transport processes. The solver is first verified
against six established plasma codes for positive-streamer simulations and
benchmarked against Cantera for a freely propagating hydrogen flame, then
applied to three cases: (1) spark discharge in airflow; (2) streamer
propagation in a premixed flame; and (3) flame dynamics under non-breakdown
electric fields. These applications validate the model's ability to predict NTP
properties such as fast heating and radical production and demonstrate its
potential to reveal two-way coupling between plasma and combustion.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [61] [Self-seeded photon acceleration by electron beam-driven transition radiation](https://arxiv.org/abs/2506.06531)
*Chaolu Ding,Xuesong Geng,Liangliang Ji*

Main category: physics.plasm-ph

TL;DR: Proposes using transition radiation (TR) from electron beams for photon acceleration, avoiding synchronization issues of laser-based methods, achieving a 20-fold frequency boost.


<details>
  <summary>Details</summary>
Motivation: Current methods rely on external lasers, facing synchronization and alignment challenges. TR offers a simpler, more practical alternative.

Method: Utilizes TR generated by electron beams at a vacuum-gas interface, accelerating photons in a two-stage plasma setup.

Result: Demonstrated acceleration of TR from 4.4 µm to 184 nm, achieving a 20-fold frequency boost with a 1 GeV electron bunch.

Conclusion: This scheme resolves synchronization issues and provides a viable path for beam-driven photon acceleration.

Abstract: Photon acceleration (PA) driven by ultra-relativistic electron beams offers a
promising approach to generating high-power, high-frequency coherent radiation
sources. While current methods typically rely on external optical laser pulses
injected into beam-driven plasma wakefields, they face significant challenges
in synchronization and alignment between electron accelerators and laser
systems. We propose utilizing transition radiation (TR) generated by the drive
electron bunch transversing the vacuum-gas interface as the seed photons of PA.
Using a 1 GeV electron bunch, we demonstrate acceleration of TR from 4.4 {\mu}m
to 184 nm in 1.6 mm of two-stage uniform plasma, achieving more than a 20-fold
frequency boost. Further frequency increases can be achieved with optimized
setups. This scheme addresses the synchronization and alignment issues present
in previous approaches, providing a practical path toward beam-driven photon
acceleration.

</details>


### [62] [An Extended-MHD Model for Handling Low-density Plasmas with Tabular Material Models](https://arxiv.org/abs/2506.06625)
*Nathaniel D. Hamlin,Matthew R. Martin,Jeffrey M. Woolstrum*

Main category: physics.plasm-ph

TL;DR: An extended-MHD model in PERSEUS simulates plasma-vacuum interfaces for pulsed-power systems, showing convergence with low density floors and minimal sensitivity to vacuum parameters.


<details>
  <summary>Details</summary>
Motivation: To predictively model energy and current coupling onto targets in pulsed-power systems under experimentally-relevant conditions.

Method: Developed an extended-MHD model interfaced with tabular equation-of-state and conductivity models in PERSEUS, tested for convergence with low density floors and vacuum parameters.

Result: Demonstrated convergence for test problems with sufficiently low density floors and minimal sensitivity to vacuum parameters.

Conclusion: The model is crucial for predictive modeling of pulsed-power systems, ensuring accurate energy and current coupling onto targets.

Abstract: An extended-MHD model, interfaced with tabular equation-of-state and
conductivity models, has been developed in PERSEUS (Physics as an Extended-MHD
Relaxation System with an Efficient Upwind Scheme) for simulating a
plasma-vacuum interface under experimentally-relevant conditions for a
pulsed-power system, and with minimal sensitivity to parameters characterizing
the numerical vacuum. For several test problems, we demonstrate convergence of
this model for sufficiently low density floors and with respect to certain
vacuum parameters. This capability is crucial for predictively modeling the
coupling of energy and current onto a target in a pulsed-power system.

</details>


### [63] [A Multiscale Eulerian Vlasov-Rosenbluth-Fokker-Planck Algorithm for Thermonuclear Burning Plasmas](https://arxiv.org/abs/2506.06672)
*Benjamin L. Reichelt,William T. Taitano,Brett D. Keenan,Luis Chacon,Andrei N. Simakov,Steven E. Anderson,Hans R. Hammer*

Main category: physics.plasm-ph

TL;DR: A two-grid method for simulating alpha particles in plasmas splits the distribution into energetic and thermal components, addressing numerical challenges in capturing their behavior.


<details>
  <summary>Details</summary>
Motivation: Accurate kinetic description of fusion byproducts like alpha particles is needed due to their high energy and long mean-free-paths, but traditional methods struggle with resolving sharp velocity-space features.

Method: A two-grid approach divides alpha particles into suprathermal and thermal components, using a Gaussian sink term for energy transfer and a conservative projection scheme to preserve mass, momentum, and energy.

Result: The method is robust and handles multiscale problems effectively, including scenarios like inertial confinement fusion ignition.

Conclusion: The proposed approach overcomes limitations of strict velocity-scale separation and provides a flexible, accurate solution for simulating alpha particles in plasmas.

Abstract: Accurate treatment of energetic fusion byproducts in laboratory plasmas often
requires a kinetic description, owing to their large birth kinetic energy and
long mean-free-paths compared with the characteristic system scale lengths. For
example, alpha particles produced by deuterium--tritium fusion reactions are
born at high energies (\SI{3.5}{MeV}) and predominantly slow down through
interactions with electrons traveling at comparable speeds. As an alpha
particle slows, its distribution collapses near the background ion-thermal
speed, forming a sharp structure in velocity space. Such sharp features pose
numerical challenges in grid-based Eulerian methods: capturing the full
alpha-particle energies demands a large velocity domain, while resolving the
near-thermal region requires a sufficiently fine mesh. Inspired by the work of
Peigney et al.[J. Comput. Phys. 278 (2014)], we present a two-grid approach
that splits the alpha-particle distribution into energetic (suprathermal) and
ash (thermal) components. A Gaussian-based sink term transfers particles from
the energetic population to the ash population as they slow to the thermal
regime, and a conservative projection scheme ensures that mass, momentum, and
energy of the alpha and ash interactions are preserved. Unlike the formulation
of Peigney, our method does not require a strict asymptotic separation of
velocity scales, which can, in principle, be arbitrary. We demonstrate the
robustness of this approach on challenging multiscale problems, including a
surrogate for an igniting inertial confinement fusion capsule.

</details>


### [64] [Numerical investigation of stability of low-current needle-to-plane negative corona discharges in air](https://arxiv.org/abs/2506.06744)
*N. G. C. Ferreira,P. G. C. Almeida,A. Eivazpour Taher,G. V. Naidis,M. S. Benilov*

Main category: physics.plasm-ph

TL;DR: The paper investigates conditions for a pulseless negative corona discharge in a needle-to-plane geometry, aiming to facilitate its experimental observation. It explores stability, transitions to pulsed regimes, and compares modeling with experimental data.


<details>
  <summary>Details</summary>
Motivation: To understand why negative corona discharges sometimes start in a steady-state (pulseless) mode and transition to pulsed regimes (Trichel pulses) at higher voltages, and to identify conditions for a stable pulseless mode over a wide voltage range.

Method: The study uses modeling of a needle-to-plane geometry to analyze the stability and behavior of negative corona discharges, comparing results with experimental data.

Result: The modeling confirms a stable pulseless mode in a narrow voltage range, transitions to pulsed regimes (small or regular Trichel pulses), and aligns with experimental observations. Stochastic Trichel pulses are also explained.

Conclusion: The work provides insights into the stability and transitions of negative corona discharges, aiding experimental observation and understanding of Trichel pulse mechanisms.

Abstract: Negative DC corona discharges are known for their self-pulsing regime: the
Trichel pulses. In some works, pulsed regimes, stochastic or periodic, have
been observed immediately upon the inception of the discharge, while in other
works the discharge was found to be ignited in a stedy-state (pulseless) mode,
with the Trichel pulses developing at higher voltages. Recent theoretical and
modelling work showed that the stationary negative corona between concentric
cylinders in atmospheric-pressure air is stable immediately after the ignition.
The pulseless mode was found also in the modelling of the needle-to-plane
geometry, however in a quite narrow voltage range. This work studies conditions
for a pulseless negative corona discharge in a needle-to-plane geometry to
occur over a wide range of voltages, which will facilitate its unambiguous
observation in the experiment. After the negative corona loses stability, the
current evolution shows, after a small region of quasi-harmonic oscillations,
pulses. These can be of small amplitude or regular Trichel pulses, which
develop via standing-wave or ionization-wave mechanisms. Modelling results
agree with available experimental data, both for the current-voltage
characteristics and the stability limit of the pulseless negative corona
discharge. An insight is given into stochastic Trichel pulses, which have been
observed in experiments under certain conditions.

</details>


### [65] [An analytical optimization of plasma density profiles for downramp injection in laser wake-field acceleration](https://arxiv.org/abs/2506.06814)
*Gaetano Fiore,Paolo Tomassini*

Main category: physics.plasm-ph

TL;DR: A multi-step analytical procedure optimizes plasma density for laser wake field acceleration, enhancing electron bunch acceleration with strong agreement to simulations.


<details>
  <summary>Details</summary>
Motivation: To control and optimize plasma wave-breaking for maximizing electron acceleration in laser wake field acceleration.

Method: Improved fully relativistic plane model for tailoring plasma density to laser pulse profile, focusing on wave-breaking control.

Result: Excellent agreement with Particle In Cell simulations, validating the method's effectiveness.

Conclusion: The proposed procedure effectively optimizes electron acceleration in laser wake field scenarios.

Abstract: We propose and detail a multi-step analytical procedure, based on an improved
fully relativistic plane model for Laser Wake Field Acceleration, to tailor the
initial density of a cold diluted plasma to the laser pulse profile, so as to
control and optimize the partial wave-breaking of the plasma wave and maximize
the acceleration of small bunches of electrons self-injected by the first
wave-breaking at the density down-ramp, at least in the first stages of their
acceleration phase. We find an excellent agreement with the results of Particle
In Cell simulations obtained with the same input data.

</details>


### [66] [Collimated Hard X-Rays from Hybrid Laser and Plasma Wakefield Accelerators](https://arxiv.org/abs/2506.06833)
*Hong Zhang,Jianmeng Wei,Mengyuan Chu,Jiale Zheng,Zhiheng Lou,Ruoxuan Ma,Xizhuan Chen,Hao Wang,Jiacheng Zhu,Zongxin Zhang,Yi Xu,Yuxin Leng,Song Li,Ke Feng,Wentao Wang,Ruxin Li*

Main category: physics.plasm-ph

TL;DR: A hybrid laser and plasma wakefield acceleration scheme enhances betatron radiation, achieving GeV-energy electron beams and high photon flux.


<details>
  <summary>Details</summary>
Motivation: To improve the photon energy and flux of betatron radiation by combining laser and plasma wakefield acceleration.

Method: Quasi-phase-stable acceleration in an up-ramp plasma density generates GeV-energy electron beams, which drive PWFA to further accelerate a witness beam. Quasi-3D particle-in-cell simulations model the process.

Result: Experimental results show a divergence of $(6.1 \pm 1.9)\times(5.8\pm 1.6)$ mrad$^2$, critical energy of $71 \pm 8$ keV, and flux exceeding $10^{14}$ photons per steradian above 5 keV, an order of magnitude improvement.

Conclusion: The scheme establishes a new paradigm for compact, collimated hard X-ray sources.

Abstract: We report a synergistic enhancement of betatron radiation based on the hybrid
laser and plasma wakefield acceleration scheme. Quasi-phase-stable acceleration
in an up-ramp plasma density first generates GeV-energy electron beams that act
as a drive beam for PWFA, which then further accelerates the witness beam to
GeV energies, enhancing both photon energy and flux. A full width at half
maximum divergence $(6.1 \pm 1.9)\times(5.8\pm 1.6) $ mrad$^2$ of betatron
radiation, a critical energy of $71 \pm 8$ keV, and an average flux of more
than $10^{14}$ photons per steradian above 5 keV were all experimentally
obtained thanks to this scheme, which was an order of magnitude higher than the
previous reports. Quasi-three-dimensional particle-in-cell simulations were
used to model the acceleration and radiation of the electrons in our
experimental conditions, establishing a new paradigm for compact collimated
hard X-ray sources.

</details>


### [67] [Enhanced plasma heating via interaction with high-contrast laser and cone-shaped target](https://arxiv.org/abs/2506.06963)
*Yuga Karaki,Yoshitaka Mori,Eigo Ebisawa,Yuichi Inubushi,Sadaoki Kojima,Kohei Yamanoi,Yuki Abe,Takumi Tsuido,Hiroki Matsubara,Rinya Akematsu,Ryo Omura,Ryunosuke Takizawa,King Fai Farley Law,Eisuke Miura,Yasunobu Arikawa,Keisuke Shigemori,Akifumi Iwamoto,Katsuhiro Ishii,Ryohei Hanayama,Yoneyoshi Kitagawa,Hiroshi Sawada,Takayoshi Sano,Natsumi Iwata,Yasuhiko Sentoku,Atsushi Sunahara,Tomoyuki Johzaki,Kenichi Nagaoka,Shinsuke Fujioka*

Main category: physics.plasm-ph

TL;DR: High-contrast laser and cone-attached target enhance plasma heating, increasing electron temperature and laser energy coupling.


<details>
  <summary>Details</summary>
Motivation: To improve plasma heating efficiency by combining high-contrast laser irradiation with cone-target geometry.

Method: Used high-intensity, high-contrast laser and cone-attached target; measured fast electron spectra and X-ray emission with spectrometers; analyzed results with simulations and scaling laws.

Result: Cone increased fast electron slope temperature threefold; X-ray spectral analysis showed ~9 keV electron temperature, 17.5x higher than low-contrast laser.

Conclusion: High-contrast laser with cone-target geometry significantly enhances laser energy coupling and plasma heating efficiency.

Abstract: We investigated plasma heating enhancement using a high-intensity,
high-contrast laser and a cone-attached target. Fast electron spectra and X-ray
emission were measured with an electron spectrometer and a Bragg crystal
spectrometer. The results were analyzed using PrismSPECT simulations with a
two-component electron distribution model and empirical scaling laws. X-ray
pinhole images showed that the cone effectively focused multi-spot laser light
near its tip, enhancing local emission. While high-contrast laser irradiation
reduced the fast electron slope temperature for flat targets, the use of a cone
increased it by over threefold, corresponding to a fourfold rise in laser
intensity. X-ray spectral analysis indicated an electron temperature of ~9~keV
for the cone case, 17.5 times higher than that with a low-contrast laser. These
findings demonstrate that combining high-contrast laser irradiation with
cone-target geometry significantly improves laser energy coupling and plasma
heating efficiency.

</details>


### [68] [Some features in 4-level generation in LIPLs](https://arxiv.org/abs/2506.07220)
*Lev Nagli,Kirill Kulikov,Dima Cheskis*

Main category: physics.plasm-ph

TL;DR: The paper explores forbidden transitions in LIPL, showing spin-orbit coupling can enhance oscillator strength and electron-atom collisions can create population inversion.


<details>
  <summary>Details</summary>
Motivation: To understand and improve inversion population mechanisms in Laser-Induced Plasma Lasers by analyzing forbidden transitions and collision effects.

Method: Uses linear-response TDDFT in the Casida formalism to estimate oscillator strengths and collisional transition rates.

Result: Spin-orbit coupling increases oscillator strength; electron-atom collisions boost energy, creating population inversion at E_up.

Conclusion: Forbidden transitions in LIPL can be manipulated via spin-orbit coupling and collisions, enhancing laser performance.

Abstract: This paper shows that in Laser-Induced Plasma Lasers (LIPL), the
collisionally assisted transitions that lead to the inversion population on an
upper-generation level E<sub>up</sub> may be partly forbidden. The spin-orbit
coupling may increase the oscillator strength of such transitions. It also
demonstrates that collisions between electrons and excited atoms can strongly
increase the atoms' energy, creating a population inversion at the
E<sub>up</sub> level, which may lie about 1 eV above the pumped level
E<sub>pump</sub>. Examples of oscillator strengths and collisional transition
rate estimates are provided using linear-response time-dependent density
functional theory (TDDFT) in the Casida formalism.

</details>


### [69] [Thermal Radiation Exchange between Nanoparticles Heated by Arc Discharge](https://arxiv.org/abs/2506.07808)
*A. Povitsky,M. N. Shneider*

Main category: physics.plasm-ph

TL;DR: The paper highlights the importance of radiation heat transfer in plasma-nanoparticle interactions, emphasizing the neglected role of scattered thermal radiation in prior studies. It reveals that nanoparticles heated by radiation achieve higher temperatures than the surrounding gas, with larger particles heating more than smaller ones. The study also explores the impact of gas pressure reduction and noble gases on convection heat flux and radiation enhancement.


<details>
  <summary>Details</summary>
Motivation: The study aims to address gaps in prior research by incorporating the effect of scattered thermal radiation between nanoparticles, which was previously ignored. This is crucial for accurate temperature estimation in applications like space science and industrial processes involving plasma.

Method: The research employs numerical modeling of thermal radiation scattered by nanoparticles in the Rayleigh regime, focusing on scenarios where particle radii are much smaller than the radiation wavelength and inter-particle distances exceed the dominant wavelength. It also examines the effects of reduced gas pressure and alternating noble gases on convection heat flux.

Result: Computational results demonstrate that re-radiation by larger nanoparticles significantly influences particle temperatures. Smaller particles exhibit higher temperatures than predicted by models assuming thermal isolation, due to inter-particle thermal interactions.

Conclusion: The study underscores the necessity of accounting for inter-particle thermal radiation in numerical models to accurately predict nanoparticle temperatures in plasma environments, with implications for both theoretical and applied research.

Abstract: The heating of particles by plasma radiation plays a critical role in space
science involving dusty plasma as well as in industrial processes such as
plasma vapor deposition, microchip production, etching and plasma fusion.
Numerical modeling of radiation heat transfer from plasma to nano-scale
particles includes exchange of scattered thermal radiation between particles-an
effect that was neglected in prior studies in which temperature of particles
was estimated. Thermal modeling of gas loaded with nanoparticles differs from a
typical multiphase flow, where particles are assumed to be in thermal
equilibrium with the surrounding gas. In contrast, the temperature of
nanoparticles heated by radiation is significantly higher than the local gas
temperature. The nanoparticles volume heating by radiation is markedly
different from conventional surface heating experience by macroscale particles.
The larger particles are heated to higher temperatures than smaller ones. The
study includes numerical modeling of thermal radiation scattered by particles
in the Rayleigh regime in where particles radii are much smaller compared to
the radiation wavelength and the distance between particles is larger than the
dominant radiation wavelength. The study investigates the effects of reduction
in convection heat flux by reducing the gas pressure and using alternating
noble gases. Additionally, it investigates the role of enhancement of radiation
heat flux from the arc. The computational results show that the re-radiation by
larger, heated nanoparticles is important to obtain the accurate temperature of
particles. This inter-particle thermal interaction leads to higher temperatures
in smaller particles than models assuming thermally isolated particles would
predict.

</details>


### [70] [Density jump as a function of the field for parallel relativistic collisionless shocks](https://arxiv.org/abs/2506.07973)
*Antoine Bret,Ramesh Narayan*

Main category: physics.plasm-ph

TL;DR: The paper extends a non-relativistic model of collisionless shocks to the relativistic regime, showing significant deviations from MHD predictions, supported by Particle-in-Cell simulations.


<details>
  <summary>Details</summary>
Motivation: To address the inadequacy of MHD formalism for collisionless shocks and explore relativistic effects, particularly with strong flow-aligned magnetic fields.

Method: Extension of a prior non-relativistic model to relativistic conditions (zero upstream pressure, high Lorentz factor), validated against Particle-in-Cell simulations.

Result: The density jump in the downstream frame deviates from MHD predictions, behaving as r ∼ 2 + 1/γ_up, compared to MHD's r = 4.

Conclusion: The relativistic model confirms significant departures from MHD behavior in collisionless shocks, particularly for pair plasmas.

Abstract: Collisionless shocks are frequently analyzed using the magnetohydrodynamic
formalism (MHD), even though the required collisionality hypothesis is not
fulfilled. In a previous work \citep{BretJPP2018}, we presented a model of
collisionless shock displaying an important departure from the expected MHD
behavior, in the case of a strong flow aligned magnetic field. This model was
non-relativistic. Here, it is extended to the relativistic regime, considering
zero upstream pressure and upstream Lorentz factor $\gg 1$. The result agrees
satisfactorily with Particle-in-Cell simulations and shows a similar, and
important, departure from the MHD prediction. In the strong field regime, the
density jump $r$, seen in the downstream frame, behaves like $r \sim 2 +
1/\gamma_{\mathrm{up}}$ while MHD predicts 4 ($\gamma_{\mathrm{up}}$ is the
Lorentz factor of the upstream measured in the downstream frame). Only pair
plasmas are considered.

</details>


<div id='nlin.CD'></div>

# nlin.CD [[Back]](#toc)

### [71] [Stochastic Gradient-Descent Calibration of Pyragas Delayed-Feedback Control for Chaos Suppression in the Sprott Circuit](https://arxiv.org/abs/2506.06639)
*Adib Kabir,Onil Morshed,Oishi Kabir*

Main category: nlin.CD

TL;DR: The paper explores chaos control in the Sprott circuit using delayed feedback and compares calibration techniques (SSE and SGD) for model alignment with experimental data. SGD shows promise but faces scalability challenges.


<details>
  <summary>Details</summary>
Motivation: To suppress chaos in the Sprott circuit and improve calibration techniques for chaotic nonlinear systems.

Method: Modeled the circuit using a third-order nonlinear differential equation, implemented delayed feedback control, and tested SSE and SGD calibration methods.

Result: SGD outperformed grid search in phase synchronization, but amplitude discrepancies remained. Joint optimization of control parameters and resistor alignment improved accuracy.

Conclusion: SGD-based calibration holds potential for advancing modeling in chaotic systems, despite computational challenges.

Abstract: This paper investigates chaos control in the Sprott circuit, a minimal
electronic system exhibiting complex nonlinear dynamics. Using the third-order
nonlinear differential equation from Kaveh Merat paper, we model the circuit
and implement delayed feedback control to suppress chaos. Experimental voltage
data were extracted from published figures via WebPlotDigitizer. Then we
explore two calibration techniques: Minimizing sum of squared errors (SSE), and
stochastic gradient descent (SGD) with finite differences. Joint optimization
of control parameters and the variable resistor achieves the best alignment
with experimental data, accurately capturing phase and amplitude. SGD
outperforms grid search in phase synchronization, though amplitude
discrepancies persist due to model simplifications. The trade-off between
accuracy and computational cost is analyzed, revealing scalability challenges
in chaotic system calibration. Phase space analysis validates the model ability
to replicate the chaotic attractor geometry, despite minor deviations. Overall,
Stochastic Gradient Descent based calibration of chaotic nonlinear systems
shows significant potential for advancing mathematical modeling and electrical
engineering.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [72] [CIR bridge for modeling of fish migration on sub-hourly scale](https://arxiv.org/abs/2506.07094)
*Hidekazu Yoshioka*

Main category: math.PR

TL;DR: The paper introduces a CIR bridge, a stochastic process, to model intraday fish migration data, demonstrating its theoretical novelty and practical application.


<details>
  <summary>Details</summary>
Motivation: To address the need for a stochastic model that accurately represents the intermittent nature of fish migration data, particularly for diadromous fish in rivers.

Method: The CIR bridge is derived as a solution to a stochastic differential equation with unbounded coefficients, featuring closed-form averages and variances for efficient parameter identification.

Result: The CIR bridge effectively models the intraday migration of Plecoglossus altivelis altivelis in the Nagara River, validated by sub-hourly data from February to June.

Conclusion: The CIR bridge is a theoretically sound and computationally feasible model for fish migration, offering insights into on-off intermittency and practical applications in ecological studies.

Abstract: Bridges, which are stochastic processes with pinned initial and terminal
conditions, have recently been applied to solve various problems. We show that
a bridge based on the Cox-Ingersoll-Ross process, called a CIR bridge in this
paper, reasonably models the intraday number of migrating fish at an
observation point in a river. The studied fish migrates between sunrise and
sunset each day, which are considered the initial and terminal times,
respectively. The CIR bridge is well-defined as a unique pathwise continuous
solution to a stochastic differential equation with unbounded drift and
diffusion coefficients and potentially represents the on-off intermittency of
the fish count data. Our bridge is theoretically novel in that it admits
closed-form time-dependent averages and variances, with which the model
parameters can be identified efficiently, and is computable by a
recently-developed one-step numerical method. The CIR bridge is applied to the
sub-hourly migration data of the diadromous fish Plecoglossus altivelis
altivelis in the Nagara River, Japan, from February to June.

</details>


### [73] [Well-posedness of Fractional Stochastic p-Laplace Equations Driven by Superlinear Transport Noise](https://arxiv.org/abs/2506.06766)
*Bixiang Wang*

Main category: math.PR

TL;DR: The paper proves the existence and uniqueness of solutions for a fractional p-Laplace equation with polynomial drift and superlinear transport noise using monotone arguments and Galerkin approximations.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of solving stochastic fractional p-Laplace equations with complex drift and noise terms, ensuring rigorous mathematical foundations.

Method: Uses monotone arguments for abstract stochastic differential equations, Galerkin approximations, and the Skorokhod-Jakubowski representation theorem for tightness and uniform estimates.

Result: Existence and uniqueness of solutions are established, overcoming difficulties in tightness and integrability of approximate solutions.

Conclusion: The approach successfully solves the fractional stochastic p-Laplace equation, leveraging weak convergence and representation theorems.

Abstract: In this paper, we prove the existence and uniqueness of solutions of the
fractional p-Laplace equation with a polynomial drift of arbitrary order driven
by superlinear transport noise. By the monotone argument, we first prove the
existence and uniqueness of solutions of an abstract stochastic differential
equation satisfying a fully local monotonicity condition. We then apply the
abstract result to the fractional stochastic p-Laplace equation defined in a
bounded domain. The main difficulty is to establish the tightness as well as
the uniform integrability of a sequence of approximate solutions defined by the
Galerkin method. To obtain the necessary uniform estimates, we employ the
Skorokhod-Jakubowski representation theorem on a topological space instead of a
metric space. Since the strong Skorokhod representation theorem is incorrect
even in a complete separable metric space, we pass to the limit of stochastic
integrals with respect to a sequence of Wiener processes by a weak convergence
argument.

</details>


### [74] [Zero-noise selection and Large Deviations in $L^\infty_t L^p_x$ for the stochastic transport equation beyond DiPerna-Lions](https://arxiv.org/abs/2506.06947)
*Gianluca Crippa,Eliseo Luongo,Umberto Pappalettera*

Main category: math.PR

TL;DR: The paper studies $L^\infty_t L^p_x$ solutions of the stochastic transport equation, proving strong existence and pathwise uniqueness in a regime where deterministic weak solutions are non-unique. It also shows convergence to renormalized solutions as noise vanishes, governed by a Large Deviations Principle.


<details>
  <summary>Details</summary>
Motivation: To address the lack of uniqueness in deterministic transport equations by introducing stochasticity and analyzing convergence and large deviations.

Method: Analyzes $L^\infty_t L^p_x$ solutions with drift in $L^\infty_t W^{1,q}_x$, proving existence, uniqueness, and convergence to renormalized solutions. Uses Large Deviations Principle in a non-separable space.

Result: Strong existence and pathwise uniqueness in a specific parameter regime. Convergence to renormalized solutions as noise vanishes, with Large Deviations Principle governing the convergence.

Conclusion: Stochasticity resolves non-uniqueness in deterministic transport equations, and the Large Deviations Principle provides a framework for convergence in non-separable spaces.

Abstract: We consider $L^\infty_t L^p_x$ solutions of the stochastic transport equation
with drift in $L^\infty_t W^{1,q}_x$. We show strong existence and pathwise
uniqueness of solutions in a regime of parameters $p,q$ for which non-unique
weak solutions of the deterministic transport equation exist. When the
intensity of the noise goes to zero, we prove that the solutions of the
stochastic transport equation converge to the unique renormalized solution of
the transport equation in the sense of DiPerna-Lions. Furthermore, we show that
the convergence is governed by a Large Deviations Principle in the space
$L^\infty_t L^p_x$. Since the space $L^\infty_t L^p_x$ is not separable, and
solutions of the stochastic transport equation lack compactness therein, the
weak convergence approach to Large Deviations by Budhiraja, Dupuis, and
Maroulas is not directly applicable.

</details>


### [75] [The diffusivity of supercritical Bernoulli percolation is infinitely differentiable](https://arxiv.org/abs/2506.07158)
*Chenlin Gu,Wenhao Zhao*

Main category: math.PR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We prove that, the diffusivity and conductivity on $\mathbb{Z}^d$-Bernoulli
percolation ($d \geq 2$) are infinitely differentiable in supercritical regime.
This extends a result by Kozlov [Uspekhi Mat. Nauk 44 (1989), no. 2(266), pp 79
- 120]. The key to the proof is a uniform estimate for the finite-volume
approximation of derivatives, which relies on the perturbed corrector equations
in homogenization theory. The renormalization of geometry is then implemented
in a sequence of scales to gain sufficient degrees of regularity. To handle the
higher-order perturbation on percolation, new techniques, including
cluster-growth decomposition and hole separation, are developed.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [76] [Predicting aqueous and electrochemical stability of 2D materials from extended Pourbaix analyses](https://arxiv.org/abs/2506.07839)
*Stefano Americo,Ivano E. Castelli,Kristian S. Thygesen*

Main category: cond-mat.mtrl-sci

TL;DR: The paper introduces a Surface Pourbaix Diagram (SPD) method to improve the prediction of electrochemical stability for 2D materials, addressing limitations of conventional Pourbaix diagrams (CPDs).


<details>
  <summary>Details</summary>
Motivation: Current methods like CPDs often mispredict material stability due to ignoring thermodynamic barriers, leading to unreliable results despite practical stability.

Method: The SPD framework incorporates 'early intermediate states' to model initial steps of surface reactions, applied to MoS2, phosphorene, and Ti2C.

Result: SPD accurately predicts stability windows and degradation mechanisms, matching experimental data for MoS2, phosphorene, and Ti2C.

Conclusion: SPD is a robust tool for predicting electrochemical stability and analyzing surface structures, outperforming CPDs.

Abstract: A key challenge for computational discovery of electrocatalytic materials is
the reliable prediction of thermodynamic stability in aqueous environment and
under different electrochemical conditions. In this work, we first evaluate the
electrochemical stability of more than 3000 two-dimensional (2D) materials
using conventional Pourbaix diagrams (CPDs). Due to the complete neglect of
thermodynamic barriers along the (often complex) reaction pathways, the vast
majority of the materials are predicted to be unstable even though some are
known to be stable in practice. We then introduce an analysis based on the
surface Pourbaix diagram (SPD) including 'early intermediate states' that
represent the first steps of the key surface passivation and dissolution
reactions. The SPD framework is applied to the 2D materials MoS$_2$,
phosphorene, and the MXene Ti$_2$C, all of which are predicted to be unstable
by the CPD. For MoS$_2$, our approach reproduces the experimental pH-U
stability window as well as the experimental desulphurization potential. For
phosphorene and Ti2$_C$, the SPD approach is used to investigate the
spontaneous degradation mechanism and the potential-dependent surface
termination, respectively, again yielding good agreement with experiments. The
SPD-based stability analysis emerges as a versatile and quantitative method for
prediction of stability and investigation of surface structures in
electrochemical environments.

</details>


<div id='math.FA'></div>

# math.FA [[Back]](#toc)

### [77] [Homogenization of non-symmetric convolution type operators](https://arxiv.org/abs/2506.07176)
*Andrey Piatnitski,Vladimir Sloushch,Tatiana Suslina,Elena Zhizhina*

Main category: math.FA

TL;DR: The paper analyzes the homogenization of a non-self-adjoint convolution operator in $L_2(\mathbb R^d)$, providing a sharp resolvent approximation with $O(\eps)$ error.


<details>
  <summary>Details</summary>
Motivation: To study the behavior of a bounded convolution operator under homogenization, especially without symmetry assumptions, and derive accurate approximations for small $\eps$.

Method: The operator ${\mathbb A}_\eps$ is analyzed using assumptions on the moments of $a(\x)$ and periodicity of $\mu(\x,\y)$. A resolvent approximation is derived involving an effective operator and a periodic function.

Result: A sharp approximation of the resolvent $({\mathbb A}_\eps + I)^{-1}$ is obtained, with an error of order $O(\eps)$, using an effective operator and a periodic correction.

Conclusion: The study successfully approximates the resolvent for small $\eps$, demonstrating the effectiveness of the method despite the lack of symmetry assumptions.

Abstract: The paper studies homogenization problem for a bounded in $L_2(\mathbb R^d)$
convolution type operator ${\mathbb A}_\eps$, $\eps >0$, of the form $$
({\mathbb A}_\eps u) (\x) = \eps^{-d-2} \int_{\R^d} a((\x-\y)/\eps)
\mu(\x/\eps, \y/\eps) \left( u(\x) - u(\y) \right)\,d\y. $$ It is assumed that
$a(\x)$ is a non-negative function from $L_1(\R^d)$, and $\mu(\x,\y)$ is a
periodic in $\x$ and $\y$ function such that $0< \mu_- \leqslant \mu(\x,\y)
\leqslant \mu_+< \infty$. No symmetry assumption on $a(\cdot)$ and $\mu(\cdot)$
is imposed, so the operator ${\mathbb A}_\eps$ need not be self-adjoint. Under
the assumption that the moments $M_k = \int_{\R^d} |\x|^k a(\x)\,d\x$,
$k=1,2,3$, are finite we obtain, for small $\eps>0$, sharp in order
approximation of the resolvent $({\mathbb A}_\eps + I)^{-1}$ in the operator
norm in $L_2(\mathbb R^d)$, the discrepancy being of order $O(\eps)$. The
approximation is given by an operator of the form $({\mathbb A}^0 + \eps^{-1}
\langle \boldsymbol{\alpha},\nabla \rangle + I)^{-1}$ multiplied on the right
by a periodic function $q_0(\x/\eps)$; here ${\mathbb A}^0 = -
\operatorname{div}g^0 \nabla$ is the effective operator, and
$\boldsymbol{\alpha}$ is a constant vector.

</details>


### [78] [On Herz-Bochkarev limiting problem](https://arxiv.org/abs/2506.07478)
*Erlan Nursultanov,Arash Ghorbanalizadeh,Durvudkhan Suragan*

Main category: math.FA

TL;DR: The paper derives optimal bounds for Hausdorff-Young-type inequalities in Lorentz spaces, refining prior estimates and using grand Lorentz space techniques.


<details>
  <summary>Details</summary>
Motivation: To address the Herz-Bochkarev problem and improve upon existing bounds in the limiting case where the integrability parameter approaches 2.

Method: Utilizes new grand Lorentz space techniques to analyze inequalities in Lorentz spaces.

Result: Optimal bounds are obtained, refining earlier estimates and aligning with recent advances.

Conclusion: The study advances understanding of Hausdorff-Young inequalities in Lorentz spaces, offering refined results and methodological innovations.

Abstract: This paper studies Hausdorff-Young-type inequalities within the framework of
Lorentz spaces $L_{p,q}$. Focusing on the dependence of the associated
constants on the integrability parameter $p$, we derive optimal bounds in the
limiting case $p\rightarrow 2$, addressing the Herz-Bochkarev problem. The
results obtained refine the pioneering estimates in [3] and are comparable to
recent advances in [16]. The main ingredients of our approach are new grand
Lorentz space techniques.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [79] [Tunable Coloration in Core-Shell Plasmonic Nanopixels Based on Organic Conductive Polymers: A First-Principles and FDTD Study](https://arxiv.org/abs/2506.07544)
*Md. Shariful Islam,Ahmed Zubair*

Main category: physics.optics

TL;DR: The paper presents a low-power nanoscale pixel design using electrochromic nanoparticles on a mirror (eNPoM) with tunable optical properties for display applications.


<details>
  <summary>Details</summary>
Motivation: To develop efficient, tunable nanoscale pixels for display devices by leveraging electromagnetic field scattering and electrochromic materials.

Method: Combined first-principles DFT calculations and FDTD simulations to design and analyze core-shell eNPoM structures with electrochromic polymers (PANI, PEDOT, PPy, PTh) on Au nanoparticles.

Result: Achieved significant wavelength tunability (100nm, 40nm, 70nm, 40nm) with different polymers and explored RGB color production using TiN. Demonstrated color change capabilities and absorption peak shifts in complex structures.

Conclusion: The study highlights the potential of organic electrochromic materials for plasmonic nanopixels, offering tunable and efficient display solutions.

Abstract: From raindrops to planets, the scattering of electromagnetic fields
introduces exciting phenomena that can be utilized for display devices. Here,
we designed an electrochromic nanoparticle on mirror (eNPoM) structure with
core-shell geometries for low-power nanoscale pixels with rapid coloration
abilities based on four electrochromic organic conducting polymers utilizing
the first-principles calculations based on density functional theory (DFT) and
the finite-difference time-domain (FDTD) simulations. Au nanoparticles are
coated with electrochromic conductive polymers (such as PANI, PEDOT, PPy, and
PTh) and positioned on the metal mirror. The electric field enhancement and the
impact of shell thickness are analyzed. Dielectric properties of all polymers
resulting from atomistic calculation were utilized for FDTD simulation, which
helps to correlate the direct relationship between polymer structure and
optical properties. Notably, the study reveals significant wavelength
tunability of 100nm, 40nm, 70nm, and over 40nm using PANI, PEDOT, PPy, and PTh
shells, respectively. Additionally, the potential for RGB color production
using a TiN layer on the mirror is explored. For the first time, complex
structures such as bow tie and gear were utilized to model the nanopixels
studied and a significant absorption peak shift was observed. Chromaticity
coordinates in the CIE 1931 color space and CIELAB2000 color difference
quantify color change capabilities during the redox cycle, and a comparative
analysis of organic and inorganic materials highlights the prospects of the
proposed plasmonic nanopixels.

</details>


### [80] [Dense Associative Memory in a Nonlinear Optical Hopfield Neural Network](https://arxiv.org/abs/2506.07849)
*Khalid Musa,Santosh Kumar,Michael Katidis,Yu-Ping Huang*

Main category: physics.optics

TL;DR: The paper introduces a nonlinear optical Hopfield neural network (NOHNN) for Dense Associative Memories (DAMs), achieving significant improvements in pattern storage and retrieval compared to traditional HNNs.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between theoretical research on DAMs and practical implementations by proposing an optical system for correlated pattern recognition and storage.

Method: The NOHNN incorporates 2-body and 4-body interactions in its energy function, tested on MNIST handwritten digit patterns.

Result: The system shows a 10-fold improvement for uncorrelated patterns and up to 50 times more storage for correlated patterns, with cleaner retrieval.

Conclusion: NOHNN demonstrates potential for big-data optimization, computer vision, and graph network tasks.

Abstract: Modern Hopfield Neural Networks (HNNs), also known as Dense Associative
Memories (DAMs), enhance the performance of simple recurrent neural networks by
leveraging the nonlinearities in their energy functions. They have broad
applications in combinatorial optimization, high-capacity memory storage, deep
learning transformers, and correlated pattern recognition. Thus far, research
on DAMs has been primarily theoretical, with implementations limited to CPUs
and GPUs. In this work, for the first time to our knowledge, we propose and
experimentally demonstrate a nonlinear optical Hopfield neural network (NOHNN)
system for realizing DAMs using correlated patterns. Our NOHNN incorporates
effective 2-body and 4-body interactions in its energy function. The inclusion
of 4-body interaction scores a minimum ten-fold improvement in the number of
uncorrelated patterns that can be stored and retrieved, significantly
surpassing the traditional capacity limit for traditional HNNs. For correlated
patterns, depending on their average correlation, up to 50 times more patterns
can be stored compared to traditional HNNs. To test the system's robustness,
the benchmark testing is performed on MNIST handwritten digit patterns. The
results show a 5.5 times improvement in the pattern storage along with the
retrieval of cleaner and less noisy patterns. These results highlight the
potential of nonlinear optical DAMs for practical applications in challenging
big-data optimization, computer vision and graph network tasks.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [81] [A note on isothermic coordinate system for spacelike surfaces with constant mean curvature in Lorentz-Minkowski space](https://arxiv.org/abs/2506.07516)
*Yu Kawakami,Kaito Satake*

Main category: math.DG

TL;DR: Elementary proof of isothermic coordinate system existence and its application to space-like surfaces with constant mean curvature in Lorentz-Minkowski space.


<details>
  <summary>Details</summary>
Motivation: To provide a simpler proof for the existence of isothermic coordinates and study their use in analyzing space-like surfaces with constant mean curvature.

Method: Elementary proof techniques are employed to demonstrate the existence of isothermic coordinates. The application involves studying global properties of surfaces in Lorentz-Minkowski space.

Result: The proof confirms the existence of isothermic coordinates, and their application reveals insights into the global behavior of the specified surfaces.

Conclusion: The work simplifies the understanding of isothermic coordinates and extends their utility to surfaces in Lorentz-Minkowski space.

Abstract: In this note, we present an elementary proof of the existence of isothermic
coordinate system and explore its application to global property for space-like
surfaces with constant mean curvature in the Lorentz-Minkowski three-space.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [82] [Active Contour Models Driven by Hyperbolic Mean Curvature Flow for Image Segmentation](https://arxiv.org/abs/2506.06712)
*Saiyu Hu,Chunlei He,Jianfeng Zhang,Dexing Kong,Shoujun Huang*

Main category: cs.CV

TL;DR: The paper introduces hyperbolic mean curvature flow-driven active contour models (HMCF-ACMs) and hyperbolic dual-mode regularized flow-driven ACMs (HDRF-ACMs) for adaptive image segmentation, improving precision and noise resistance.


<details>
  <summary>Details</summary>
Motivation: Existing parabolic mean curvature flow-driven ACMs (PMCF-ACMs) heavily depend on initial curve configurations, limiting adaptability. The paper aims to overcome this by introducing tunable initial velocity fields.

Method: Proposes HMCF-ACMs and HDRF-ACMs, leveraging hyperbolic flows and edge-aware force modulation. Uses level set method and a weighted fourth-order Runge-Kutta algorithm for numerical solutions.

Result: HMCF-ACMs and HDRF-ACMs achieve more precise segmentations with better noise resistance and numerical stability due to adaptive initial configurations.

Conclusion: The proposed hyperbolic flow-driven ACMs offer improved adaptability and performance in image segmentation, particularly in handling weak boundaries and noise.

Abstract: Parabolic mean curvature flow-driven active contour models (PMCF-ACMs) are
widely used in image segmentation, which however depend heavily on the
selection of initial curve configurations. In this paper, we firstly propose
several hyperbolic mean curvature flow-driven ACMs (HMCF-ACMs), which introduce
tunable initial velocity fields, enabling adaptive optimization for diverse
segmentation scenarios. We shall prove that HMCF-ACMs are indeed normal flows
and establish the numerical equivalence between dissipative HMCF formulations
and certain wave equations using the level set method with signed distance
function. Building on this framework, we furthermore develop hyperbolic
dual-mode regularized flow-driven ACMs (HDRF-ACMs), which utilize smooth
Heaviside functions for edge-aware force modulation to suppress over-diffusion
near weak boundaries. Then, we optimize a weighted fourth-order Runge-Kutta
algorithm with nine-point stencil spatial discretization when solving the
above-mentioned wave equations. Experiments show that both HMCF-ACMs and
HDRF-ACMs could achieve more precise segmentations with superior noise
resistance and numerical stability due to task-adaptive configurations of
initial velocities and initial contours.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [83] [LT-PINN: Lagrangian Topology-conscious Physics-informed Neural Network for Boundary-focused Engineering Optimization](https://arxiv.org/abs/2506.06300)
*Yuanye Zhou,Zhaokun Wang,Kai Zhou,Hui Tang,Xiaofan Li*

Main category: cs.LG

TL;DR: LT-PINNs improve topology optimization by eliminating manual interpolation and enabling precise boundary determination, outperforming conventional PINNs in accuracy and versatility.


<details>
  <summary>Details</summary>
Motivation: Conventional PINNs rely on density-based topology descriptions, which require manual interpolation and limit applicability to complex geometries.

Method: Proposes LT-PINNs, parameterizing topology boundary curves as learnable parameters and introducing specialized boundary and topology loss functions.

Result: LT-PINNs achieve lower relative L2 errors, handle arbitrary boundary conditions, and infer clear topology boundaries without manual interpolation.

Conclusion: LT-PINNs offer a robust and versatile framework for boundary-focused engineering optimization, applicable to a wide range of PDEs.

Abstract: Physics-informed neural networks (PINNs) have emerged as a powerful meshless
tool for topology optimization, capable of simultaneously determining optimal
topologies and physical solutions. However, conventional PINNs rely on
density-based topology descriptions, which necessitate manual interpolation and
limit their applicability to complex geometries. To address this, we propose
Lagrangian topology-conscious PINNs (LT-PINNs), a novel framework for
boundary-focused engineering optimization. By parameterizing the control
variables of topology boundary curves as learnable parameters, LT-PINNs
eliminate the need for manual interpolation and enable precise boundary
determination. We further introduce specialized boundary condition loss
function and topology loss function to ensure sharp and accurate boundary
representations, even for intricate topologies. The accuracy and robustness of
LT-PINNs are validated via two types of partial differential equations (PDEs),
including elastic equation with Dirichlet boundary conditions and Laplace's
equation with Neumann boundary conditions. Furthermore, we demonstrate
effectiveness of LT-PINNs on more complex time-dependent and time-independent
flow problems without relying on measurement data, and showcase their
engineering application potential in flow velocity rearrangement, transforming
a uniform upstream velocity into a sine-shaped downstream profile. The results
demonstrate (1) LT-PINNs achieve substantial reductions in relative L2 errors
compared with the state-of-art density topology-oriented PINNs (DT-PINNs), (2)
LT-PINNs can handle arbitrary boundary conditions, making them suitable for a
wide range of PDEs, and (3) LT-PINNs can infer clear topology boundaries
without manual interpolation, especially for complex topologies.

</details>


### [84] [Jarzynski Reweighting and Sampling Dynamics for Training Energy-Based Models: Theoretical Analysis of Different Transition Kernels](https://arxiv.org/abs/2506.07843)
*Davide Carbone*

Main category: cs.LG

TL;DR: The paper explores Jarzynski reweighting for training Energy-Based Models (EBMs), addressing biases in traditional methods like contrastive divergence and score matching. It analyzes kernel choice and applies the technique to flow-based diffusion models and Restricted Boltzmann Machines, improving sample quality and correcting biases.


<details>
  <summary>Details</summary>
Motivation: Traditional EBM training methods introduce biases and struggle with normalization constants and sampling. The paper aims to leverage Jarzynski reweighting for more accurate and efficient training.

Method: Theoretical analysis of Jarzynski reweighting, focusing on kernel choice. Applied to flow-based diffusion models (mitigating discretization errors) and Restricted Boltzmann Machines (correcting biases).

Result: Improved sample quality in diffusion models and bias correction in RBMs. Insights into kernel choice's impact on performance.

Conclusion: Jarzynski reweighting is a principled tool for generative learning, enhancing EBM training by addressing biases and improving efficiency.

Abstract: Energy-Based Models (EBMs) provide a flexible framework for generative
modeling, but their training remains theoretically challenging due to the need
to approximate normalization constants and efficiently sample from complex,
multi-modal distributions. Traditional methods, such as contrastive divergence
and score matching, introduce biases that can hinder accurate learning. In this
work, we present a theoretical analysis of Jarzynski reweighting, a technique
from non-equilibrium statistical mechanics, and its implications for training
EBMs. We focus on the role of the choice of the kernel and we illustrate these
theoretical considerations in two key generative frameworks: (i) flow-based
diffusion models, where we reinterpret Jarzynski reweighting in the context of
stochastic interpolants to mitigate discretization errors and improve sample
quality, and (ii) Restricted Boltzmann Machines, where we analyze its role in
correcting the biases of contrastive divergence. Our results provide insights
into the interplay between kernel choice and model performance, highlighting
the potential of Jarzynski reweighting as a principled tool for generative
learning.

</details>


### [85] [FunDiff: Diffusion Models over Function Spaces for Physics-Informed Generative Modeling](https://arxiv.org/abs/2506.07902)
*Sifan Wang,Zehao Dou,Tong-Rui Liu,Lu Lu*

Main category: cs.LG

TL;DR: FunDiff is a novel framework for generative modeling in function spaces, combining latent diffusion with a function autoencoder to handle continuous functions and incorporate physical priors, achieving high fidelity and robustness.


<details>
  <summary>Details</summary>
Motivation: Adapting generative models to physical applications is challenging due to the continuous nature of governed functions and complex physical laws. FunDiff addresses this gap.

Method: FunDiff uses a latent diffusion process and function autoencoder to manage varying discretizations, generate continuous functions, and enforce physical priors via architectural constraints or physics-informed loss functions.

Result: The framework achieves minimax optimality for density estimation in function spaces and demonstrates high fidelity and robustness in fluid dynamics and solid mechanics applications.

Conclusion: FunDiff effectively generates physically consistent samples, handles noisy/low-resolution data, and provides theoretical guarantees, making it a powerful tool for physical applications.

Abstract: Recent advances in generative modeling -- particularly diffusion models and
flow matching -- have achieved remarkable success in synthesizing discrete data
such as images and videos. However, adapting these models to physical
applications remains challenging, as the quantities of interest are continuous
functions governed by complex physical laws. Here, we introduce
$\textbf{FunDiff}$, a novel framework for generative modeling in function
spaces. FunDiff combines a latent diffusion process with a function autoencoder
architecture to handle input functions with varying discretizations, generate
continuous functions evaluable at arbitrary locations, and seamlessly
incorporate physical priors. These priors are enforced through architectural
constraints or physics-informed loss functions, ensuring that generated samples
satisfy fundamental physical laws. We theoretically establish minimax
optimality guarantees for density estimation in function spaces, showing that
diffusion-based estimators achieve optimal convergence rates under suitable
regularity conditions. We demonstrate the practical effectiveness of FunDiff
across diverse applications in fluid dynamics and solid mechanics. Empirical
results show that our method generates physically consistent samples with high
fidelity to the target distribution and exhibits robustness to noisy and
low-resolution data. Code and datasets are publicly available at
https://github.com/sifanexisted/fundiff.

</details>


### [86] [Uncovering the Functional Roles of Nonlinearity in Memory](https://arxiv.org/abs/2506.07919)
*Manuel Brenner,Georgia Koppe*

Main category: cs.LG

TL;DR: The study explores the role of nonlinearity in recurrent networks, finding minimal nonlinearity often suffices and optimizes performance, robustness, and interpretability.


<details>
  <summary>Details</summary>
Motivation: To systematically dissect the functional role of nonlinearity in recurrent networks, identifying when it is necessary and what mechanisms it enables.

Method: Uses Almost Linear Recurrent Neural Networks (AL-RNNs) to control nonlinearity and analyze memory mechanisms across sequence modeling tasks.

Result: Minimal nonlinearity is sufficient and often optimal, yielding simpler, more robust, and interpretable models.

Conclusion: Provides a framework for selectively introducing nonlinearity, bridging dynamical systems theory with functional demands in recurrent networks, impacting both artificial and biological systems.

Abstract: Memory and long-range temporal processing are core requirements for sequence
modeling tasks across natural language processing, time-series forecasting,
speech recognition, and control. While nonlinear recurrence has long been
viewed as essential for enabling such mechanisms, recent work suggests that
linear dynamics may often suffice. In this study, we go beyond performance
comparisons to systematically dissect the functional role of nonlinearity in
recurrent networks--identifying both when it is computationally necessary, and
what mechanisms it enables. We use Almost Linear Recurrent Neural Networks
(AL-RNNs), which allow fine-grained control over nonlinearity, as both a
flexible modeling tool and a probe into the internal mechanisms of memory.
Across a range of classic sequence modeling tasks and a real-world stimulus
selection task, we find that minimal nonlinearity is not only sufficient but
often optimal, yielding models that are simpler, more robust, and more
interpretable than their fully nonlinear or linear counterparts. Our results
provide a principled framework for selectively introducing nonlinearity,
bridging dynamical systems theory with the functional demands of long-range
memory and structured computation in recurrent neural networks, with
implications for both artificial and biological neural systems.

</details>


### [87] [Neural Tangent Kernel Analysis to Probe Convergence in Physics-informed Neural Solvers: PIKANs vs. PINNs](https://arxiv.org/abs/2506.07958)
*Salah A. Faroughi,Farinaz Mostajeran*

Main category: cs.LG

TL;DR: The paper analyzes Chebyshev-based Physics-informed Kolmogorov-Arnold Networks (cPIKANs) using Neural Tangent Kernel (NTK) theory to understand their training dynamics and convergence behavior for solving PDEs.


<details>
  <summary>Details</summary>
Motivation: To advance the theoretical understanding of cPIKANs, which are promising for solving PDEs but lack explored training dynamics and convergence behavior.

Method: Derives the NTK for cKANs in supervised settings, extends it to physics-informed contexts, and analyzes spectral properties of NTK matrices for four PDEs. Investigates optimization strategies' impact on NTK evolution.

Result: NTK behavior in cPIKANs is tractable, revealing learning dynamics beyond standard PINNs. Spectral trends link kernel behavior to convergence rates and domain decomposition benefits.

Conclusion: This first systematic NTK study of cPIKANs provides theoretical insights that clarify and predict their empirical performance.

Abstract: Physics-informed Kolmogorov-Arnold Networks (PIKANs), and in particular their
Chebyshev-based variants (cPIKANs), have recently emerged as promising models
for solving partial differential equations (PDEs). However, their training
dynamics and convergence behavior remain largely unexplored both theoretically
and numerically. In this work, we aim to advance the theoretical understanding
of cPIKANs by analyzing them using Neural Tangent Kernel (NTK) theory. Our
objective is to discern the evolution of kernel structure throughout
gradient-based training and its subsequent impact on learning efficiency. We
first derive the NTK of standard cKANs in a supervised setting, and then extend
the analysis to the physics-informed context. We analyze the spectral
properties of NTK matrices, specifically their eigenvalue distributions and
spectral bias, for four representative PDEs: the steady-state Helmholtz
equation, transient diffusion and Allen-Cahn equations, and forced vibrations
governed by the Euler-Bernoulli beam equation. We also conduct an investigation
into the impact of various optimization strategies, e.g., first-order,
second-order, and hybrid approaches, on the evolution of the NTK and the
resulting learning dynamics. Results indicate a tractable behavior for NTK in
the context of cPIKANs, which exposes learning dynamics that standard
physics-informed neural networks (PINNs) cannot capture. Spectral trends also
reveal when domain decomposition improves training, directly linking kernel
behavior to convergence rates under different setups. To the best of our
knowledge, this is the first systematic NTK study of cPIKANs, providing
theoretical insight that clarifies and predicts their empirical performance.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [88] [Centrality Change Proneness: an Early Indicator of Microservice Architectural Degradation](https://arxiv.org/abs/2506.07690)
*Alexander Bakhtin,Matteo Esposito,Valentina Lenarduzzi,Davide Taibi*

Main category: cs.SE

TL;DR: The study explores temporal centrality metrics for early detection of microservice architectural degradation, analyzing correlations with software metrics in an OSS project.


<details>
  <summary>Details</summary>
Motivation: To prevent Microservice Architectural Degradation by identifying early indicators through temporal centrality metrics.

Method: Reconstructed architecture of 7 releases of an OSS microservice project, computed software and centrality metrics, and explored correlations.

Result: Identified 7 size and 5 complexity metrics correlated with centrality; Centrality Change Proneness did not affect software metrics but serves as an early indicator.

Conclusion: Temporal centrality metrics, particularly Centrality Change Proneness, offer a new perspective for early detection of architectural degradation.

Abstract: Over the past decade, the wide adoption of Microservice Architecture has
required the identification of various patterns and anti-patterns to prevent
Microservice Architectural Degradation. Frequently, the systems are modelled as
a network of connected services. Recently, the study of temporal networks has
emerged as a way to describe and analyze evolving networks. Previous research
has explored how software metrics such as size, complexity, and quality are
related to microservice centrality in the architectural network. This study
investigates whether temporal centrality metrics can provide insight into the
early detection of architectural degradation by correlating or affecting
software metrics. We reconstructed the architecture of 7 releases of an OSS
microservice project with 42 services. For every service in every release, we
computed the software and centrality metrics. From one of the latter, we
derived a new metric, Centrality Change Proneness. We then explored the
correlation between the metrics. We identified 7 size and 5 complexity metrics
that have a consistent correlation with centrality, while Centrality Change
Proneness did not affect the software metrics, thus providing yet another
perspective and an early indicator of microservice architectural degradation.

</details>


<div id='math.SP'></div>

# math.SP [[Back]](#toc)

### [89] [Stable Computation of Laplacian Eigenfunctions Corresponding to Clustered Eigenvalues](https://arxiv.org/abs/2506.07340)
*Ryoki Endo,Xuefeng Liu*

Main category: math.SP

TL;DR: A stable method for computing eigenfunctions of tightly clustered Laplacian eigenvalues using shape difference quotient.


<details>
  <summary>Details</summary>
Motivation: Accurate computation of eigenfunctions for clustered eigenvalues is challenging, especially under domain perturbation.

Method: Proposes using the shape difference quotient of eigenvalues for stable computation.

Result: Provides a stable approach to compute eigenfunctions for clustered eigenvalues.

Conclusion: The method addresses the difficulty in computing eigenfunctions for tightly clustered eigenvalues under domain perturbation.

Abstract: The accurate computation of eigenfunctions corresponding to tightly clustered
Laplacian eigenvalues remains an extremely difficult problem. In this paper,
using the shape difference quotient of eigenvalues, we propose a stable
computation method for the eigenfunctions of clustered eigenvalues caused by
domain perturbation.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [90] [An analysis of capital market through the lens of integral transforms: exploring efficient markets and information asymmetry](https://arxiv.org/abs/2506.06350)
*Kiran Sharma,Abhijit Dutta,Rupak Mukherjee*

Main category: q-fin.ST

TL;DR: The paper explores arbitrage's role in financial markets, linking it to information dissemination and price discovery. It critiques traditional analysis methods and proposes spectrum analysis to decompose price cycles for better understanding.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of conventional techniques in interpreting price cycles and understand the impact of information on price formation in financial markets.

Method: Uses spectrum analysis to decompose and analyze price cycles in the National Stock Exchange of India (NSE).

Result: The approach allows for a deeper interpretation of price behavior by breaking down cyclical movements.

Conclusion: Spectrum analysis provides a more coherent method to study price cycles and their relation to information dissemination in financial markets.

Abstract: Post Modigliani and Miller (1958), the concept of usage of arbitrage created
a permanent mark on the discourses of financial framework. The arbitrage
process is largely based on information dissemination amongst the stakeholders
operating in the financial market. The advent of the efficient market
Hypothesis draws close to the M&M hypothesis. Giving importance to the
arbitrage process, which effects the price discovery in the stock market. This
divided the market as random and efficient cohort system. The focus was on
which information forms a key factor in deciding the price formation in the
market. However, the conventional techniques of analysis do not permit the
price cycles to be interpreted beyond its singular wave-like cyclical movement.
The apparent cyclic measurement is not coherent as the technical analysis does
not give sustained result. Hence adaption of theories and computation from
mathematical methods of physics ensures that these cycles are decomposed and
the effect of the broken-down cycles is interpreted to understand the overall
effect of information on price formation and discovery. In order to break the
cycle this paper uses spectrum analysis to decompose and understand the
above-said phenomenon in determining the price behavior in National Stock
Exchange of India (NSE).

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [91] [Goal-based portfolio selection with mental accounting](https://arxiv.org/abs/2506.06654)
*Erhan Bayraktar,Bingyan Han*

Main category: q-fin.PM

TL;DR: A continuous-time portfolio selection framework incorporates goal-based investing and mental accounting, showing unique solutions and complex optimal strategies.


<details>
  <summary>Details</summary>
Motivation: To model investment behavior where investors manage multiple goals separately with penalties for fund transfers, reflecting real-world mental accounting.

Method: Uses stochastic Perron's method to derive the value function as a constrained viscosity solution of a Hamilton-Jacobi-Bellman equation system.

Result: Numerical analysis reveals complex free boundaries, goal-dependent strategies, cross-portfolio diversification, and delayed reallocation behavior.

Conclusion: The framework effectively captures goal-based investing dynamics, offering insights into optimal strategies and investor behavior.

Abstract: We present a continuous-time portfolio selection framework that reflects
goal-based investment principles and mental accounting behavior. In this
framework, an investor with multiple investment goals constructs separate
portfolios, each corresponding to a specific goal, with penalties imposed on
fund transfers between these goals, referred to as mental costs. By applying
the stochastic Perron's method, we demonstrate that the value function is the
unique constrained viscosity solution of a Hamilton-Jacobi-Bellman equation
system. Numerical analysis reveals several key features: the free boundaries
exhibit complex shapes with bulges and notches; the optimal strategy for one
portfolio depends on the wealth level of another; investors must diversify both
among stocks and across portfolios; and they may postpone reallocating surplus
from an important goal to a less important one until the former's deadline
approaches.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [92] [ChemGraph: An Agentic Framework for Computational Chemistry Workflows](https://arxiv.org/abs/2506.06363)
*Thang D. Pham,Aditya Tanikanti,Murat Keçeli*

Main category: physics.chem-ph

TL;DR: ChemGraph is an AI-powered framework automating computational chemistry workflows, combining graph neural networks and LLMs for efficient and intuitive task execution.


<details>
  <summary>Details</summary>
Motivation: Atomistic simulations are complex and require expert knowledge; ChemGraph aims to simplify and automate these workflows.

Method: Uses graph neural networks for calculations and LLMs for task planning, offering methods from tight-binding to DFT.

Result: Smaller LLMs handle simple tasks well, while complex tasks benefit from larger models; task decomposition improves performance.

Conclusion: ChemGraph effectively automates workflows, with smaller LLMs performing comparably to larger ones when tasks are decomposed.

Abstract: Atomistic simulations are essential tools in chemistry and materials science,
accelerating the discovery of novel catalysts, energy storage materials, and
pharmaceuticals. However, running these simulations remains challenging due to
the wide range of computational methods, diverse software ecosystems, and the
need for expert knowledge and manual effort for the setup, execution, and
validation stages. In this work, we present ChemGraph, an agentic framework
powered by artificial intelligence and state-of-the-art simulation tools to
streamline and automate computational chemistry and materials science
workflows. ChemGraph leverages graph neural network-based foundation models for
accurate yet computationally efficient calculations and large language models
(LLMs) for natural language understanding, task planning, and scientific
reasoning to provide an intuitive and interactive interface. Users can perform
tasks such as molecular structure generation, single-point energy, geometry
optimization, vibrational analysis, and thermochemistry calculations with
methods ranging from tight-binding and machine learning interatomic potentials
to density functional theory or wave function theory-based methods. We evaluate
ChemGraph across 13 benchmark tasks and demonstrate that smaller LLMs
(GPT-4o-mini, Claude-3.5-haiku, Qwen2.5-14B) perform well on simple workflows,
while more complex tasks benefit from using larger models like GPT-4o.
Importantly, we show that decomposing complex tasks into smaller subtasks
through a multi-agent framework enables smaller LLM models to match or exceed
GPT-4o's performance in specific scenarios.

</details>


### [93] [Neural Operators for Forward and Inverse Potential-Density Mappings in Classical Density Functional Theory](https://arxiv.org/abs/2506.06623)
*Runtong Pan,Xinyi Fang,Kamyar Azizzadenesheli,Miguel Liu-Schiaffini,Mengyang Gu,Jianzhong Wu*

Main category: physics.chem-ph

TL;DR: Neural operators are evaluated for learning functional relationships in cDFT, with FNO and GK-RMSCNN-DeepONet showing top performance in predicting excess free energy and density profiles.


<details>
  <summary>Details</summary>
Motivation: To assess neural operators' ability to model complex functional relationships in classical density functional theory (cDFT) for inhomogeneous 1D hard-rod fluids.

Method: Several neural operator architectures are tested using training data from analytical solutions, evaluated via MSE loss and free energy prediction on interpolation and extrapolation test sets.

Result: FNO excels in predicting excess free energy, while GK-RMSCNN-DeepONet performs best among DeepONet variants. Models are also compared with GPR and ALEC.

Conclusion: Neural operators, particularly FNO and GK-RMSCNN-DeepONet, are effective for functional mapping in cDFT, outperforming traditional methods like GPR with ALEC.

Abstract: Neural operators are capable of capturing nonlinear mappings between
infinite-dimensional functional spaces, offering a data-driven approach to
modeling complex functional relationships in classical density functional
theory (cDFT). In this work, we evaluate the performance of several neural
operator architectures in learning the functional relationships between the
one-body density profile $\rho(x)$, the one-body direct correlation function
$c_1(x)$, and the external potential $V_{ext}(x)$ of inhomogeneous
one-dimensional (1D) hard-rod fluids, using training data generated from
analytical solutions of the underlying statistical-mechanical model. We
compared their performance in terms of the Mean Squared Error (MSE) loss in
establishing the functional relationships as well as in predicting the excess
free energy across two test sets: (1) a group test set generated via random
cross-validation (CV) to assess interpolation capability, and (2) a newly
constructed dataset for leave-one-group CV to evaluate extrapolation
performance. Our results show that FNO achieves the most accurate predictions
of the excess free energy, with the squared ReLU activation function
outperforming other activation choices. Among the DeepONet variants, the
Residual Multiscale Convolutional Neural Network (RMSCNN) combined with a
trainable Gaussian derivative kernel (GK-RMSCNN-DeepONet) demonstrates the best
performance. Additionally, we applied the trained models to solve for the
density profiles at various external potentials and compared the results with
those obtained from the direct mapping $V_{ext} \mapsto \rho$ with neural
operators, as well as with Gaussian Process Regression (GPR) combined with
Active Learning by Error Control (ALEC), which has shown strong performance in
previous studies.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [94] [Energy partition in magnetohydrodynamic turbulence](https://arxiv.org/abs/2506.06611)
*Xing Wei*

Main category: physics.flu-dyn

TL;DR: The paper derives energy partition in MHD turbulence, showing turbulent viscous and ohmic dissipations are comparable. Energy ratios depend on lengthscale ratios, leading to equipartition under certain conditions.


<details>
  <summary>Details</summary>
Motivation: To understand energy distribution in MHD turbulence, building on prior work by Lee and Chandrasekhar.

Method: Analyzes the energy equation to compare turbulent viscous and ohmic dissipations, and relates energy ratios to lengthscale ratios.

Result: Turbulent viscous and ohmic dissipations are comparable. Energy equipartition occurs when largest lengthscales are similar.

Conclusion: Energy partition in MHD turbulence depends on lengthscale ratios, with equipartition under comparable scales.

Abstract: We use a simple and straightforward method to derive the energy partition in
magnetohydrodynamics (MHD) turbulence that was first studied by Lee and then
more rigorously by Chandrasekhar. By investigating the energy equation we find
that the turbulent viscous and ohmic dissipations are comparable to each other.
Under the condition that turbulent viscosity and turbulent magnetic diffusivity
are comparable, we deduce that the ratio of kinetic to magnetic energies
depends on the ratio of the turbulent magnetic lengthscale to turbulent
velocity lengthscale of the largest eddies. When the two largest lengthscales
are comparable, the two energies are in equipartition.

</details>


### [95] [Direct numerical simulation of complete transition to turbulence with a fluid at supercritical pressure](https://arxiv.org/abs/2506.06703)
*Pietro Carlo Boldini,Benjamin Bugeat,Jurriaan W. R. Peeters,Markus Kloker,Rene Pecnik*

Main category: physics.flu-dyn

TL;DR: The study explores the laminar-to-turbulent transition in a heated flat-plate boundary layer with supercritical fluid, comparing subcritical and transcritical regimes. It reveals unique instability mechanisms and breakdown behaviors, with implications for turbulence modeling.


<details>
  <summary>Details</summary>
Motivation: To understand the transition dynamics in supercritical-pressure boundary layers, which are less explored compared to ideal-gas or subcritical cases, and to identify unique instability mechanisms.

Method: Fully compressible direct numerical simulations are used to analyze linear/nonlinear instabilities, breakdown to turbulence, and fully developed turbulent boundary layers under subcritical and transcritical conditions.

Result: In the transcritical regime, 2D forcing creates billow-like structures and flow reversals, while 3D forcing leads to abrupt vortex formation. Transition can be triggered early with a single 2D wave, and variable-property scaling predicts turbulent skin-friction and heat-transfer accurately.

Conclusion: The transcritical regime exhibits distinct transition behaviors, enabling early turbulence onset with minimal forcing. Variable-property scaling is effective for turbulent predictions, offering insights for supercritical fluid applications.

Abstract: The objective of this work is to investigate the unexplored
laminar-to-turbulent transition of a heated flat-plate boundary layer with a
fluid at supercritical pressure. Two temperature ranges are considered: a
subcritical case, where the fluid remains entirely in the liquid-like regime,
and a transcritical case, where the pseudo-critical (Widom) line is crossed and
pseudo-boiling occurs. Fully compressible direct numerical simulations are used
to study (i) the linear and nonlinear instabilities, (ii) the breakdown to
turbulence, and (iii) the fully developed turbulent boundary layer. In the
transcritical regime, two-dimensional forcing generates not only a train of
billow-like structures around the Widom line, resembling Kelvin-Helmholtz
instability, but also near-wall travelling regions of flow reversal. These
spanwise-oriented billows dominate the early nonlinear stage. When high
subharmonic three-dimensional forcing is applied, staggered $\Lambda$-vortices
emerge more abruptly than in the subcritical case. However, unlike the classic
H-type breakdown under zero pressure gradient observed in ideal-gas and
subcritical regimes, the H-type breakdown is triggered by strong shear layers
caused by flow reversals -- similar to that observed in
adverse-pressure-gradient boundary layers. Without oblique wave forcing,
transition is only slightly delayed and follows a naturally selected
fundamental breakdown (K-type) scenario. Hence, in the transcritical regime, it
is possible to trigger nonlinearities and achieve transition to turbulence
relatively early using only a single two-dimensional wave that strongly
amplifies background noise. In the fully turbulent region, we demonstrate that
variable-property scaling accurately predicts turbulent skin-friction and
heat-transfer coefficients.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [96] [Self-consistent equations and quantum diffusion for the Anderson model](https://arxiv.org/abs/2506.06468)
*Adam Black,Reuben Drogin,Felipe Hernández*

Main category: math-ph

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We consider the Anderson tight-binding model on $\mathbb{Z}^d$, $d\geq 2$,
with Gaussian noise and at low disorder $\lambda>0$. We derive a diffusive
scaling limit for the entries of the resolvent $R(z)$ at imaginary part
$\operatorname*{Im} z\sim\lambda^{2+\kappa_d}$, $\kappa_d>0$, with high
probability. As consequences, we establish quantum diffusion (in a
time-averaged sense) for the Schr\"{o}dinger propagator at the longest
timescale known to date and improve the best available lower bounds on the
localization length of eigenfunctions. Our results for $d=2$ are the first
quantum diffusion results for the Anderson model on $\mathbb{Z}^2$. The proof
avoids the use of diagrammatic expansions and instead proceeds by analyzing
certain self-consistent equations for $R(z)$. This is facilitated by new
estimates for $\|R(z)\|_{\ell^p\rightarrow \ell^q}$ that control the
recollisions.

</details>


<div id='nlin.SI'></div>

# nlin.SI [[Back]](#toc)

### [97] [GBDT with nontrivial seeds: explicit solutions of the focusing NLS equations and the corresponding Weyl functions](https://arxiv.org/abs/2506.07702)
*Alexander Sakhnovich*

Main category: nlin.SI

TL;DR: The paper extends the GBDT method to construct explicit solutions for the focusing NLS equation using an exponential seed, covering rogue waves, step-like solutions, and N-modulation solutions.


<details>
  <summary>Details</summary>
Motivation: To advance previous work by generalizing the GBDT approach for non-trivial exponential seeds in the NLS equation.

Method: Uses GBDT to derive explicit solutions, Baker-Akhiezer functions, and Weyl function evolutions for the NLS equation.

Result: Obtained solutions include rogue waves, step-like solutions, and N-modulation solutions, expanding on prior work with trivial seeds.

Conclusion: The study significantly extends previous results, providing new insights into NLS equation solutions.

Abstract: Our GBDT (generalised B\"acklund-Darboux transformation) approach is used to
construct explicit solutions of the focusing nonlinear Schr\"odinger (NLS)
equation in the case of the exponential seed $a \exp\{2 i (cx +dt)\}$. The
corresponding Baker-Akhiezer functions and evolution of the Weyl functions are
obtained as well. In particular, the solutions, which appear in the study of
rogue waves, step-like solutions and $N$-modulation solutions of the NLS
equation are considered. This work is an essential development of our joint
work with Rien Kaashoek and Israel Gohberg, where the seed was trivial, as well
as several other of our previous works.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [98] [Analyticity of Exponential Dirichlet Series and Applications to the Approximate Controllability of Parabolic Equations](https://arxiv.org/abs/2506.07892)
*Mohamed Ouzahra*

Main category: math.OC

TL;DR: The paper studies the analyticity of exponential Dirichlet series, derives their power series coefficients, and estimates the remainder. It applies these results to analyze the approximate controllability of linear parabolic equations using the moment method.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand the analytic properties of exponential Dirichlet series and apply these insights to control theory, specifically for linear parabolic equations.

Method: The method involves analyzing the power series decomposition of exponential Dirichlet series and using the moment method to study controllability in linear parabolic equations.

Result: The paper explicitly determines the coefficients of the power series decomposition and provides remainder estimates. It also demonstrates the application to approximate controllability.

Conclusion: The study successfully links the analytic properties of exponential Dirichlet series to control theory, offering practical insights for controllability in linear parabolic systems.

Abstract: In this paper, we investigate the analyticity of a class of exponential
Dirichlet series. We then explicitly determine the coefficients of their power
series decomposition and provide an estimate for the remainder. As an
application, we study the approximate controllability property of linear
parabolic equations with locally distributed or lumped controls by employing
the moment method, which relies on the exponential Dirichlet series associated
with the spectrum of the system's operator.

</details>


<div id='cond-mat.mes-hall'></div>

# cond-mat.mes-hall [[Back]](#toc)

### [99] [Electronic structure and transport in materials with flat bands: 2D materials and quasicrystals](https://arxiv.org/abs/2506.06721)
*Guy Trambly de Laissardière,Somepalli Venkateswarlu,Ahmed Misssaoui,Ghassen Jemaï,Khouloud Chika,Javad Vahedi,Omid Faizy Namarvar,Jean-Pierre Julien,Andreas Honecker,Laurence Magaud,Jouda Jemaa Khabthani,Didier Mayou*

Main category: cond-mat.mes-hall

TL;DR: The paper reviews materials with flat bands, highlighting electronic confinement due to geometric structures or defects, and their impact on transport and correlation effects.


<details>
  <summary>Details</summary>
Motivation: To explore the role of flat bands in electronic confinement and their influence on material properties, especially in graphene and TMDs.

Method: Studied periodic approximants of quasiperiodic tilings, twisted bilayers at magic angles, and 2D nanomaterials with defects like vacancies or adsorbates.

Result: Flat bands cause unique transport properties, magnetic states, and correlation effects. Functionalization in bilayer graphene alters electronic behavior.

Conclusion: Flat bands and defects significantly influence electronic properties, with functionalization emerging as a key control parameter in 2D materials.

Abstract: In this review, we present our recent works on materials whose common point
is the presence of electronic bands of very low dispersion, called "flat
bands", which are always the signature of an electronic confinement. A first
part is devoted to the cases where this confinement is due to the long-range
geometry of the defect-free structure. We have thus studied periodic
approximant structures of quasiperiodic Penrose and octagonal tilings, and
twisted bilayers of graphene or transition metal dichalcogenides (TMDs) whose
rotation angle between the two layers assumes a special value, called "magic
angle". In these materials, the flat bands correspond to electronic states
distributed over a very large number of atoms (several hundreds or even
thousands of atoms) and are very sensitive to small structural distortions such
as "heterostrain". Their electronic transport properties cannot be described by
usual Bloch-Boltzmann theories, because the interband terms of the velocity
operator dominate the intraband terms as far as quantum diffusion is concerned.
In twisted bilayer graphene, flat bands can induce a magnetic state and other
electron-electron correlation effects. The second part focuses on 2D
nanomaterials in the presence of local point defects that cause resonant
electronic states (vacancies, adsorbed atoms or molecules). We present studies
on monolayer graphene, twisted or Bernal bilayer graphene, carbon nanotubes,
monolayer and multilayer black phosphorene, and monolayer TMDs. A recent result
is the discovery that the selective functionalization of a Bernal bilayer
graphene sublattice leads to a metallic or insulating behavior depending on the
functionalized sublattice type. This result, which seems to be confirmed by
very recent experimental measurements, suggests that functionalization can be a
key parameter to control the electronic properties of two-dimensional
materials.

</details>


### [100] [Scalable Machine Learning Models for Predicting Quantum Transport in Disordered 2D Hexagonal Materials](https://arxiv.org/abs/2506.07983)
*Seyed Mahdi Mastoor,Amirhossein Ahmadkhan Kordbacheh*

Main category: cond-mat.mes-hall

TL;DR: Scalable ML models predict quantum transport properties in 2D hexagonal materials with magnetic disorder, using a tight-binding Hamiltonian and NEGF. Random Forest models show accuracy but struggle with extrapolation.


<details>
  <summary>Details</summary>
Motivation: To accurately predict quantum transport properties (T(E) and LDOS) in 2D materials for spintronic and nanoelectronic applications.

Method: Uses a tight-binding Hamiltonian and NEGF to generate a dataset of 400,000 configurations. Develops a geometry-driven feature space and evaluates Random Forest models.

Result: Regression outperforms classification for in-domain data, but extrapolation performance is poor.

Conclusion: Highlights ML potential for transport prediction but notes limitations, suggesting physics-informed or graph-based models for better generalization.

Abstract: We introduce scalable machine learning models to accurately predict two key
quantum transport properties, the transmission coefficient T(E) and the local
density of states (LDOS) in two-dimensional (2D) hexagonal materials with
magnetic disorder. Using a tight binding Hamiltonian combined with the
Non-Equilibrium Green's Function (NEGF) formalism, we generate a large dataset
of over 400,000 unique configurations across graphene, germanene, silicene, and
stanene nanoribbons with varying geometries, impurity concentrations, and
energy levels. A central contribution of this work is the development of a
geometrydriven, physically interpretable feature space that enables the models
to generalize across material types and device sizes. Random Forest regression
and classification models are evaluated in terms of accuracy, stability, and
extrapolation ability. Regression consistently outperforms classification in
capturing continuous transport behavior on in-domain data. However,
extrapolation performance degrades significantly, revealing the limitations of
tree-based models in unseen regimes. This study highlights both the potential
and constraints of scalable ML models for quantum transport prediction and
motivates future research into physics-informed or graph-based learning
architectures for improved generalization in spintronic and nanoelectronic
device design.

</details>
