<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 12]
- [math.AP](#math.AP) [Total: 23]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 3]
- [physics.space-ph](#physics.space-ph) [Total: 2]
- [math.DG](#math.DG) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [physics.atom-ph](#physics.atom-ph) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [math.SP](#math.SP) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [math.FA](#math.FA) [Total: 1]
- [hep-lat](#hep-lat) [Total: 1]
- [hep-ph](#hep-ph) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 5]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [astro-ph.SR](#astro-ph.SR) [Total: 1]
- [gr-qc](#gr-qc) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [On Finite Element Methods for Heterogeneous Elliptic Problems](https://arxiv.org/abs/2506.08251)
*Abimael F. D. Loula,Maicon R. Correa,João N. C. Guerreiro,Elson M. Toledo*

Main category: math.NA

TL;DR: The paper revisits variational formulations for second-order elliptic problems with discontinuous coefficients, extending stabilized mixed methods for Darcy flow to heterogeneous media with discontinuous interfaces.


<details>
  <summary>Details</summary>
Motivation: To address challenges in modeling Darcy flow in heterogeneous porous media with discontinuous interfaces, ensuring flux continuity and exact constraint imposition.

Method: Extends residual-based stabilized mixed methods from homogeneous to heterogeneous media, focusing on smooth interfaces and preserving flux continuity.

Result: Convergence studies show the same rates for heterogeneous/anisotropic media as for homogeneous cases with smooth solutions.

Conclusion: The proposed formulations effectively handle discontinuous interfaces while maintaining convergence rates comparable to homogeneous problems.

Abstract: Dealing with variational formulations of second order elliptic problems with
discontinuous coefficients, we recall a single field minimization problem of an
extended functional presented by Bevilacqua et al (1974), which we associate
with the basic idea supporting discontinuous Galerkin finite element methods.
We review residual based stabilized mixed methods applied to Darcy flow in
homogeneous porous media and extend them to heterogeneous media with an
interface of discontinuity. For smooth interfaces, the proposed formulations
preserve the continuity of the flux and exactly imposes the constraint between
the tangent components of Darcy velocity on the interface. Convergence studies
for a heterogeneous and anisotropic porous medium confirm the same rates of
convergence predicted for homogeneous problem with smooth solutions.

</details>


### [2] [A structure preserving H-curl algebraic multigrid method for the eddy current equations](https://arxiv.org/abs/2506.08284)
*Raymond Tuminaro,Christian Glusa*

Main category: math.NA

TL;DR: A new AMG method (SpHcurlAMG) improves upon RSAMG by enforcing a commuting relationship without relying on piece-wise constant interpolation, achieving mesh-independent convergence.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of RSAMG, which relies on sub-optimal piece-wise constant interpolation and lacks mesh-independent convergence.

Method: The new SpHcurlAMG uses energy minimization AMG (EAMG) to construct edge interpolation grid transfers, embedding the commuting relationship as constraints.

Result: Demonstrates mesh-independent convergence across various test problems.

Conclusion: SpHcurlAMG successfully overcomes RSAMG's limitations, offering a more robust and general solution for the eddy current approximation.

Abstract: A new algebraic multigrid method (AMG) is presented for solving the linear
systems associated with the eddy current approximation to the Maxwell
equations. This AMG method extends an idea proposed by Reitzinger and Schoberl.
The main feature of the Reitzinger and Schoberl algorithm (RSAMG) is that it
maintains null-space properties of the Curl-Curl operator throughout all levels
of the AMG hierarchy. It does this by enforcing a commuting relationship
involving grid transfers and the discrete gradient operator. This null-space
preservation property is critical to the algorithm's success, however enforcing
this commuting relationship is non-trivial except in the special case where one
leverages a piece-wise constant nodal interpolation operator. For this reason,
mesh independent convergence rates are generally not observed for RSAMG due to
its reliance on sub-optimal piece-wise constant interpolation. We present a new
AMG algorithm that enforces the same commuting relationship. The main advance
is that the new structure preserving H-curl algorithm (SpHcurlAMG) does not
rely on piece-wise constant interpolation and can leverage fairly general and
more sophisticated nodal interpolation operators. The key idea is to employ
energy minimization AMG (EAMG) to construct edge interpolation grid transfers
and to enforce the commuting relationship by embedding it as constraints within
an EAMG procedure. While it might appear that solving such a constrained energy
minimization is costly, we illustrate how this is not the case in our context.
Numerical results are then given demonstrating mesh independent convergence
over a range of test problems.

</details>


### [3] [An efficient Fourier spectral algorithm for the Bogoliubov-de Gennes excitation eigenvalue problem](https://arxiv.org/abs/2506.08308)
*Yu Li,Zhixuan Li,Manting Xie,Yong Zhang*

Main category: math.NA

TL;DR: The paper proposes a Fourier spectral algorithm for solving the BdG equation in spin-1 BEC, focusing on analytical properties, numerical stability, and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the eigenvalue problem in BdG equations for spin-1 BEC, requiring efficient and stable numerical methods for large-scale systems.

Method: Combines Fourier spectral spatial discretization with a Gram-Schmidt bi-orthogonal algorithm for subspace iteration, leveraging FFT for efficiency.

Result: The solver is matrix-free, memory-friendly, and achieves spectral accuracy, validated by numerical experiments in 1-3 dimensions.

Conclusion: The proposed method is stable, efficient, and accurate for large-scale BdG eigenvalue problems, with rigorous numerical analysis and practical validation.

Abstract: In this paper, we propose an efficient Fourier spectral algorithm for an
eigenvalue problem, that is, the Bogoliubov-de Gennes (BdG) equation arsing
from spin-1 Bose-Einstein condensates (BEC) to describe the
elementary/collective excitations around the mean-field ground state. The BdG
equation is essentially a constrained eigenvalue/eigenfunction system. Firstly,
we investigate its analytical properties, including exact eigenpairs,
generalized nullspace, and bi-orthogonality of eigenspaces. Secondly, by
combining the standard Fourier spectral method for spatial discretization and a
stable Gram-Schmidt bi-orthogonal algorithm, we develop a subspace iterative
solver for such a large-scale dense eigenvalue problem, and it proves to be
numerically stable, efficient, and accurate. Our solver is matrix-free and the
operator-function evaluation is accelerated by discrete Fast Fourier Transform
(FFT) with almost optimal efficiency. Therefore, it is memory-friendly and
efficient for large-scale problems. Furthermore, we give a rigorous and
detailed numerical analysis on the stability and spectral convergence. Finally,
we present extensive numerical results to illustrate the spectral accuracy and
efficiency, and investigate the excitation spectrum and Bogoliubov amplitudes
around the ground state in 1-3 spatial dimensions.

</details>


### [4] [A Bi-Orthogonal Structure-Preserving eigensolver for large-scale linear response eigenvalue problem](https://arxiv.org/abs/2506.08355)
*Yu Li,Zijing Wang,Yong Zhang*

Main category: math.NA

TL;DR: A Bi-Orthogonal Structure-Preserving subspace iterative solver is proposed for large-scale linear response eigenvalue problems, ensuring stability, efficiency, and parallel scalability.


<details>
  <summary>Details</summary>
Motivation: The challenge of solving large-scale sparse/dense linear response eigenvalue problems, especially with zero eigenvalues, motivates the development of a stable and efficient solver.

Method: The method involves a direct sum decomposition of biorthogonal invariant subspaces, minimization principles, and a modified Gram-Schmidt biorthogonalization (MGS-Biorth) algorithm. It deflates converged eigenvectors naturally and processes eigenpairs in batches.

Result: The solver is stable, efficient, and scalable, with improved performance in parallel computing environments. Numerical examples validate its effectiveness.

Conclusion: The proposed solver addresses the challenges of large-scale eigenvalue problems effectively, offering a practical solution with demonstrated stability and scalability.

Abstract: The linear response eigenvalue problem, which arises from many scientific and
engineering fields, is quite challenging numerically for large-scale
sparse/dense system, especially when it has zero eigenvalues. Based on a direct
sum decomposition of biorthogonal invariant subspaces and the minimization
principles in the biorthogonal complement, using the structure of generalized
nullspace, we propose a Bi-Orthogonal Structure-Preserving subspace iterative
solver, which is stable, efficient, and of excellent parallel scalability. The
biorthogonality is of essential importance and created by a modified
Gram-Schmidt biorthogonalization (MGS-Biorth) algorithm. We naturally deflate
out converged eigenvectors by computing the rest eigenpairs in the biorthogonal
complementary subspace without introducing any artificial parameters. When the
number of requested eigenpairs is large, we propose a moving mechanism to
compute them batch by batch such that the projection matrix size is small and
independent of the requested eigenpair number. For large-scale problems, one
only needs to provide the matrix-vector product, thus waiving explicit matrix
storage. The numerical performance is further improved when the matrix-vector
product is implemented using parallel computing. Ample numerical examples are
provided to demonstrate the stability, efficiency, and parallel scalability.

</details>


### [5] [Stochastic gradient descent based variational inference for infinite-dimensional inverse problems](https://arxiv.org/abs/2506.08380)
*Jiaming Sui,Junxiong Jia,Jinglai Li*

Main category: math.NA

TL;DR: The paper introduces two variational inference methods for infinite-dimensional inverse problems using constant-rate stochastic gradient descent (cSGD), with a randomization strategy to link approximate and true posterior distributions. It also explores regularization and develops a preconditioned cSGD for efficiency, validated through numerical experiments.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of efficient approximate sampling from target posterior distributions in infinite-dimensional inverse problems.

Method: Proposes two variational inference approaches using cSGD with a constant learning rate, incorporating stochastic gradient noise and a preconditioned version for improved efficiency.

Result: Theoretical validation of cSGD as a variational inference method, with numerical results confirming its effectiveness for linear and non-linear inverse problems.

Conclusion: The methods provide efficient sampling and theoretical guarantees, with practical applications demonstrated in solving inverse problems.

Abstract: This paper introduces two variational inference approaches for
infinite-dimensional inverse problems, developed through gradient descent with
a constant learning rate. The proposed methods enable efficient approximate
sampling from the target posterior distribution using a constant-rate
stochastic gradient descent (cSGD) iteration. Specifically, we introduce a
randomization strategy that incorporates stochastic gradient noise, allowing
the cSGD iteration to be viewed as a discrete-time process. This transformation
establishes key relationships between the covariance operators of the
approximate and true posterior distributions, thereby validating cSGD as a
variational inference method. We also investigate the regularization properties
of the cSGD iteration and provide a theoretical analysis of the discretization
error between the approximated posterior mean and the true background function.
Building on this framework, we develop a preconditioned version of cSGD to
further improve sampling efficiency. Finally, we apply the proposed methods to
two practical inverse problems: one governed by a simple smooth equation and
the other by the steady-state Darcy flow equation. Numerical results confirm
our theoretical findings and compare the sampling performance of the two
approaches for solving linear and non-linear inverse problems.

</details>


### [6] [2N-storage Runge-Kutta methods: c-reflection symmetry and factorization of the Butcher tableau](https://arxiv.org/abs/2506.08444)
*Alexei Bazavov*

Main category: math.NA

TL;DR: The paper examines 2N-storage Runge-Kutta schemes, revealing a c-reflection symmetry in methods of order less than five and providing new schemes for higher orders.


<details>
  <summary>Details</summary>
Motivation: To explore the properties and symmetries of low-storage Runge-Kutta schemes, building on prior work to enhance understanding and optimization.

Method: Analyzes the factorization of Butcher tableaux, identifies c-reflection symmetry, and constructs new schemes numerically.

Result: Discovered symmetry in methods, derived transformation for pairs, and presented new (5,4), (6,4), and (8,4) schemes.

Conclusion: The findings aid in developing and optimizing 2N-storage schemes, especially where closed-form solutions are unavailable.

Abstract: Low-storage Runge-Kutta schemes of Williamson's type, so-called 2N-storage
schemes, are further examined as a follow-up to the recent work. It is found
that the augmented Butcher tableau factorizes into a product of matrices with
special properties. Those properties reveal that the 2N-storage methods of the
order of global accuracy less than five possess a symmetry, called c-reflection
symmetry, i.e. most methods exist in pairs. A transformation that relates the
Butcher tableaux of the pairs is found and the fact that the c-reflected method
satisfies the same order conditions as the original one is proven. Numerical
evidence that validates the analytic results is presented. Branches of
solutions for (5,4) methods, first explored by Carpenter and Kennedy, are
constructed numerically. Four new (5,4) schemes with coefficients expressed in
radicals and one with rational coefficients are examined for illustration.
Eight new (6,4) schemes, some of which can be expressed in rationals or
radicals, and one (8,4) scheme, are studied to understand the practical
implications of the c-reflection symmetry for methods with higher number of
stages. In the absence of closed-form analytic solutions for 2N-storage
Runge-Kutta methods of order four and above, the general symmetry properties,
as well as some specific analytic solutions presented here, may help in
development and optimization of 2N-storage schemes.

</details>


### [7] [Forecasting Public Sentiments via Mean Field Games](https://arxiv.org/abs/2506.08465)
*Michael V. Klibanov,Kevin McGoff,Trung Truong*

Main category: math.NA

TL;DR: A model using Mean Field Games theory forecasts public sentiments, with a convexification method showing global convergence and promising results.


<details>
  <summary>Details</summary>
Motivation: To develop a reliable mathematical model for predicting public sentiments using Mean Field Games theory.

Method: Proposes a convexification-based numerical method for the model, analyzing its convergence and rate.

Result: The method demonstrates global convergence and accurate performance in numerical experiments.

Conclusion: The approach is effective and shows promise for forecasting public sentiments.

Abstract: A mathematical model for forecasting of public sentiments via the Mean Field
Games theory is proposed. A numerical method is developed. This is a version of
the so-called convexification method. Convergence analysis demonstrates the
global convergence of this method. Convergence rate is established. Numerical
experiments demonstrate both an accurate performance of the convexification
technique and some promising features of this approach.

</details>


### [8] [Structured Variational $D$-Decomposition for Accurate and Stable Low-Rank Approximation](https://arxiv.org/abs/2506.08535)
*Ronald Katende*

Main category: math.NA

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We introduce the $D$-decomposition, a non-orthogonal matrix factorization of
the form $A \approx P D Q$, where $P \in \mathbb{R}^{n \times k}$, $D \in
\mathbb{R}^{k \times k}$, and $Q \in \mathbb{R}^{k \times n}$. The
decomposition is defined variationally by minimizing a regularized Frobenius
loss, allowing control over rank, sparsity, and conditioning. Unlike algebraic
factorizations such as LU or SVD, it is computed by alternating minimization.
We establish existence and perturbation stability of the solution and show that
each update has complexity $\mathcal{O}(n^2k)$. Benchmarks against truncated
SVD, CUR, and nonnegative matrix factorization show improved reconstruction
accuracy on MovieLens, MNIST, Olivetti Faces, and gene expression matrices,
particularly under sparsity and noise.

</details>


### [9] [sparseGeoHOPCA: A Geometric Solution to Sparse Higher-Order PCA Without Covariance Estimation](https://arxiv.org/abs/2506.08670)
*Renjie Xu,Chong Wu,Maolin Che,Zhuoheng Ran,Yimin Wei,Hong Yan*

Main category: math.NA

TL;DR: sparseGeoHOPCA is a sparse higher-order PCA framework using geometric optimization for efficient tensor decomposition, avoiding covariance estimation and iterative deflation.


<details>
  <summary>Details</summary>
Motivation: Addressing computational inefficiency and interpretability challenges in high-dimensional tensor decomposition.

Method: Unfolds tensors into structured binary linear optimization problems, transforming nonconvex objectives into tractable geometric forms.

Result: Achieves linear computational complexity, accurate sparse support recovery, and high-quality image reconstruction.

Conclusion: sparseGeoHOPCA is robust, versatile, and efficient for high-dimensional data.

Abstract: We propose sparseGeoHOPCA, a novel framework for sparse higher-order
principal component analysis (SHOPCA) that introduces a geometric perspective
to high-dimensional tensor decomposition. By unfolding the input tensor along
each mode and reformulating the resulting subproblems as structured binary
linear optimization problems, our method transforms the original nonconvex
sparse objective into a tractable geometric form. This eliminates the need for
explicit covariance estimation and iterative deflation, enabling significant
gains in both computational efficiency and interpretability, particularly in
high-dimensional and unbalanced data scenarios. We theoretically establish the
equivalence between the geometric subproblems and the original SHOPCA
formulation, and derive worst-case approximation error bounds based on
classical PCA residuals, providing data-dependent performance guarantees. The
proposed algorithm achieves a total computational complexity of
$O\left(\sum_{n=1}^{N} (k_n^3 + J_n k_n^2)\right)$, which scales linearly with
tensor size. Extensive experiments demonstrate that sparseGeoHOPCA accurately
recovers sparse supports in synthetic settings, preserves classification
performance under 10$\times$ compression, and achieves high-quality image
reconstruction on ImageNet, highlighting its robustness and versatility.

</details>


### [10] [Normalized Radon Cumulative Distribution Transforms for Invariance and Robustness in Optimal Transport Based Image Classification](https://arxiv.org/abs/2506.08761)
*Matthias Beckmann,Robert Beinert,Jonas Bresch*

Main category: math.NA

TL;DR: The paper introduces max-normalized and mean-normalized versions of the Radon cumulative distribution transform (R-CDT) to improve robustness against non-affine image deformations and noise, ensuring linear separability in classification tasks.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of the R-CDT in handling general affine transformations and non-affine deformations in real-world applications like watermark recognition.

Method: Proposes max-normalized and mean-normalized R-CDT variants, analyzing their separability and robustness using Wasserstein distances.

Result: Shows that the proposed variants maintain separability under controlled deformations and noise, supported by numerical experiments.

Conclusion: The new R-CDT variants are effective and robust for image classification, especially in challenging scenarios with non-affine deformations and noise.

Abstract: The Radon cumulative distribution transform (R-CDT), is an easy-to-compute
feature extractor that facilitates image classification tasks especially in the
small data regime. It is closely related to the sliced Wasserstein distance and
provably guaranties the linear separability of image classes that emerge from
translations or scalings. In many real-world applications, like the recognition
of watermarks in filigranology, however, the data is subject to general affine
transformations originating from the measurement process. To overcome this
issue, we recently introduced the so-called max-normalized R-CDT that only
requires elementary operations and guaranties the separability under arbitrary
affine transformations. The aim of this paper is to continue our study of the
max-normalized R-CDT especially with respect to its robustness against
non-affine image deformations. Our sensitivity analysis shows that its
separability properties are stable provided the Wasserstein-infinity distance
between the samples can be controlled. Since the Wasserstein-infinity distance
only allows small local image deformations, we moreover introduce a
mean-normalized version of the R-CDT. In this case, robustness relates to the
Wasserstein-2 distance and also covers image deformations caused by impulsive
noise for instance. Our theoretical results are supported by numerical
experiments showing the effectiveness of our novel feature extractors as well
as their robustness against local non-affine deformations and impulsive noise.

</details>


### [11] [Enabling stratified sampling in high dimensions via nonlinear dimensionality reduction](https://arxiv.org/abs/2506.08921)
*Gianluca Geraci,Daniele E. Schiavazzi,Andrea Zanoni*

Main category: math.NA

TL;DR: The paper proposes a method to efficiently propagate uncertainty from many random inputs through expensive models using stratified sampling and nonlinear dimensionality reduction.


<details>
  <summary>Details</summary>
Motivation: The challenge lies in applying stratified sampling to high-dimensional models due to difficulties in uniform partitioning.

Method: Stratification is performed on the unit interval, and original space strata are derived using nonlinear dimensionality reduction.

Result: The approach is effective in high dimensions and reduces variance in multifidelity Monte Carlo estimators.

Conclusion: The method successfully addresses high-dimensional uncertainty propagation with improved efficiency.

Abstract: We consider the problem of propagating the uncertainty from a possibly large
number of random inputs through a computationally expensive model. Stratified
sampling is a well-known variance reduction strategy, but its application, thus
far, has focused on models with a limited number of inputs due to the
challenges of creating uniform partitions in high dimensions. To overcome these
challenges, we perform stratification with respect to the uniform distribution
defined over the unit interval, and then derive the corresponding strata in the
original space using nonlinear dimensionality reduction. We show that our
approach is effective in high dimensions and can be used to further reduce the
variance of multifidelity Monte Carlo estimators.

</details>


### [12] [Asymptotic error distribution for stochastic Runge--Kutta methods of strong order one](https://arxiv.org/abs/2506.08937)
*Diancong Jin*

Main category: math.NA

TL;DR: The paper derives the asymptotic error distribution of stochastic Runge-Kutta (SRK) methods for Stratonovich SDEs, introduces a framework for implicit methods, and identifies key parameters affecting error growth.


<details>
  <summary>Details</summary>
Motivation: To analyze and improve the accuracy of SRK methods for SDEs by understanding their asymptotic error behavior.

Method: Uses a framework to derive asymptotic error distributions for implicit methods, enabling explicit methods with similar properties. Analyzes multiplicative and additive noise cases.

Result: Shows the limit distribution's mean-square error growth depends on a parameter η₁. Weak order 2 methods (η₁=0) have minimal errors.

Conclusion: First to provide asymptotic error distributions for fully implicit SDE methods, highlighting the importance of η₁ in error control.

Abstract: This work gives the asymptotic error distribution of the stochastic
Runge--Kutta (SRK) method of strong order $1$ applied to Stratonovich-type
stochastic differential equations. For dealing with the implicitness introduced
in the diffusion term, we provide a framework to derive the asymptotic error
distribution of diffusion-implicit or fully implicit numerical methods, which
enables us to construct a fully explicit numerical method sharing the same
asymptotic error distribution as the SRK method. Further, we show for the
multiplicative noise that the limit distribution $U(T)$ satisfies $\mathbf
E|U(T)|^2\le e^{L_1T}(1+\eta_1)T^3$ for some $\eta_1$ only depending on the
coefficients of the SRK method. Thus, we infer that $\eta_1$ is the key
parameter reflecting the growth rate of the mean-square error of the SRK
method. Especially, among the SRK methods of strong order $1$, those of weak
order $2$ ($\eta_1=0$) share the unified asymptotic error distribution and have
the smallest mean-square errors after a long time. This property is also found
for the case of additive noise. It seems that we are the first to give the
asymptotic error distribution of fully implicit numerical methods for SDEs.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [13] [Stratified equatorial flows in cylindrical coordinates with surface tension](https://arxiv.org/abs/2506.08016)
*Cristina Gheorghe,Andrei Stan*

Main category: math.AP

TL;DR: The paper models steady flows of an inviscid, incompressible fluid with depth-varying density, focusing on large-scale equatorial dynamics under gravity and surface tension.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of fluid flows in equatorial regions, considering gravity and surface tension effects.

Method: Mathematical modeling of steady flows in cylindrical coordinates, incorporating free surface dynamics and surface tension.

Result: A solution for large-scale equatorial fluid dynamics and a regularity proof for the free surface.

Conclusion: The study provides insights into equatorial fluid behavior under specific conditions, with proven regularity for the free surface.

Abstract: This paper considers a mathematical model of steady flows of an inviscid and
incompressible fluid moving in the azimuthal direction. The water density
varies with depth and the waves are propagating under the force of gravity,
over a flat bed and with a free surface, on which acts a force of surface
tension. Our solution pertains to large scale equatorial dynamics of a fluid
with free surface expressed in cylindrical coordinates. We also prove a
regularity result for the free surface.

</details>


### [14] [Mass conservation and gelation for the Smoluchowski coagulation equation: a generalized moment approach](https://arxiv.org/abs/2506.08017)
*Masato Kimura,Hisanori Miyata*

Main category: math.AP

TL;DR: The paper analyzes mass conservation and gelation in the Smoluchowski coagulation equation (SCE) for weak solutions with inhomogeneous kernels, providing sharp conditions for these phenomena.


<details>
  <summary>Details</summary>
Motivation: To rigorously study mass conservation and gelation in the SCE, especially for inhomogeneous coagulation kernels, which exhibit complex behavior like sudden mass loss.

Method: Introduces a generalized moment framework to derive conditions for mass conservation and gelation, based on initial data and kernel properties.

Result: Provides sharp sufficient conditions for mass conservation and gelation in the SCE with inhomogeneous kernels.

Conclusion: The framework and conditions offer a rigorous understanding of mass dynamics in the SCE, addressing gelation and conservation.

Abstract: The Smoluchowski coagulation equation (SCE) is a population balance model
that describes the time evolution of cluster size distributions resulting from
particle aggregation. Although it is formally a mass-conserving system,
solutions may exhibit a gelation phenomenon-a sudden loss of mass-when the
coagulation kernel grows superlinearly. In this paper, we rigorously analyze
mass conservation and gelation for weak solutions to the SCE with inhomogeneous
coagulation kernels. By introducing a generalized moment framework, we derive
sharp sufficient conditions for both mass conservation and gelation, expressed
in terms of the initial data and the properties of the coagulation kernel.

</details>


### [15] [Solvability of the Korteweg-de Vries equation under meromorphic initial conditions by quadrature](https://arxiv.org/abs/2506.08046)
*Kazuyuki Yagasaki*

Main category: math.AP

TL;DR: The paper analyzes the solvability of the Korteweg-de Vries equation under meromorphic initial conditions using the inverse scattering transform (IST), focusing on the integrability of the Schrödinger equation in the Lax pair.


<details>
  <summary>Details</summary>
Motivation: To understand when the Schrödinger equation in the IST framework is solvable by quadrature, particularly under meromorphic initial conditions.

Method: Applies differential Galois theory to the Schrödinger equation, examining integrability conditions for reflectionless potentials and rational potentials.

Result: The Schrödinger equation is integrable (solvable by quadrature) if and only if the potential is reflectionless and satisfies an integrability condition. Rational potentials failing this condition are not integrable.

Conclusion: The study extends previous results by proving integrability for a broader class of potentials and highlights limitations for rational potentials.

Abstract: We study the solvability of the Korteweg-de Vries equation under meromorphic
initial conditions by quadrature when the inverse scattering transform (IST) is
applied. It is a key to solve the Schr\"odinger equation appearing in the Lax
pair in application of the IST. We show that the Schr\"odinger equation is
always integrable in the sense of differential Galois theory, i.e., solvable by
quadrature, if and only if the meromporphic potential is reflectionless, under
the condition that the potential is absolutely integrable on
$\mathbb{R}\setminus(-R_0,R_0)$ for some $R_0>0$.This statement was previously
proved to be true by the author for a limited class of potentials. We also show
that the Schr\"odinger equation is not integrable in this sense for rational
potentials that decay at infinity but do not satisfy the weak condition.

</details>


### [16] [Heat kernel bounds for the fractional Laplacian with Hardy potential in angular momentum channels](https://arxiv.org/abs/2506.08115)
*Krzysztof Bogdan,Konstantin Merz*

Main category: math.AP

TL;DR: Sharp heat kernel bounds for the Hardy operator in relativistic atoms, focusing on specific function forms in $L^2(\R^d)$.


<details>
  <summary>Details</summary>
Motivation: Study of relativistic atoms.

Method: Proving sharp heat kernel bounds for the Hardy operator acting on functions of a specific form in $L^2(\R^d)$.

Result: Sharp bounds established for the given operator and function class.

Conclusion: Provides rigorous mathematical support for understanding relativistic atomic systems.

Abstract: Motivated by the study of relativistic atoms, we prove sharp heat kernel
bounds for the Hardy operator $(-\Delta)^{\alpha/2}-\kappa|x|^{-\alpha}$ acting
on functions of the form $u(|x|) |x|^{\ell} Y_{\ell,m}(x/|x|)$ in $L^2(\R^d)$,
when $\alpha\in(0,2]\cap(0,d+2\ell)$.

</details>


### [17] [The Riemann problem for three-phase foam flow in porous media](https://arxiv.org/abs/2506.08152)
*Luis Fernando Lozano,Grigori Chapiro,Dan Marchesin*

Main category: math.AP

TL;DR: The paper studies three-phase foam flow in porous media, addressing challenges like excessive gas mobility and umbilic points. It develops a solution for the Riemann problem under specific conditions and validates results numerically.


<details>
  <summary>Details</summary>
Motivation: Gas injection in porous media (e.g., for oil recovery or CCUS) faces issues like high gas mobility. Foam can mitigate this, but the underlying flow dynamics are complex due to non-linear equations and umbilic points.

Method: The study uses Corey relative permeability functions and assumes local foam equilibrium (constant MRF) to solve the Riemann problem for three-phase foam flow, focusing on cases where gas viscosity exceeds oil and water.

Result: The methodology classifies solutions for foamed gas-water injection under various initial conditions, aiding in understanding oil bank formation and simulator calibration.

Conclusion: The analytical approach, validated numerically, enhances understanding of foam flow and supports practical applications like numerical simulator calibration and uncertainty quantification.

Abstract: Gas injection in the context of the three-phase flow in porous media appears
in applications such as Enhanced Oil Recovery, aquifer remediation, and carbon
capture, utilization, and storage (CCUS). In general, this technique suffers
from a difficulty related to excessive gas mobility, which can be circumvented
by using foam. This study addresses the non-linear system of differential
equations describing the three-phase foam flow based on Corey relative
permeability functions. A major obstacle is an umbilic point, where the
characteristic wave velocities for different families coincide, complicating
the identification of stable wave structures.
  We developed a methodology to solve the Riemann problem describing the
three-phase foam displacement in the case when the gas viscosity exceeds that
of oil and water. To allow the analysis, we assume foam in local equilibrium
(or maximum foam texture), resulting in a constant mobility reduction factor
(MRF). These simplifications allowed the classification of possible solutions
for the injection of foamed gas and water mixtures under a wide range of
initial conditions within the framework of non-classical Conservation Law
Theory. As a relevant industrial application of the proposed solution, we
investigate the conditions resulting in oil bank formation. Besides improving
the general physical understanding of foam flow in a porous medium, this
analysis can be applied to calibrate numerical simulators and perform
uncertainty quantification. Our analytical estimates were validated through
numerical simulations.

</details>


### [18] [A porous medium equation with rough weights: sharp Widder theory](https://arxiv.org/abs/2506.08159)
*Gabriele Grillo,Matteo Muratori,Troy Petitt,Nikita Simonov*

Main category: math.AP

TL;DR: The paper establishes a Widder theory for a weighted porous medium equation with rough, inhomogeneous density, identifying a class of initial data for very weak solutions, proving uniqueness, and deriving a smoothing estimate.


<details>
  <summary>Details</summary>
Motivation: To extend the classical Widder theory to a weighted porous medium equation lacking properties like continuity and scale invariance, requiring new methods.

Method: Develops new proofs for initial trace identification, uniqueness, and smoothing estimates without relying on classical tools like scale invariance or reflection principles.

Result: Shows non-negative very weak solutions admit initial traces, are unique, and locally bounded with finite energy. Also provides a new a priori smoothing estimate.

Conclusion: The work completes the Widder theory for the weighted equation and introduces novel methods applicable even to the classical case.

Abstract: We establish an optimal \emph{Widder theory} for a weighted porous medium
equation with rough and inhomogeneous density that may be singular at a point
and tends to zero at spatial infinity. Specifically, for this equation, we
identify a class $X$ of initial measure data that give rise to very weak
solutions, we show that non-negative very weak solutions necessarily admit an
initial trace in $X$ at time $t=0$, and we prove that any two non-negative
solutions having the same initial trace are equal. The corresponding theory for
the classical (unweighted) equation was established by exploiting various
properties that are not available in our weighted setting, such as the
continuity of solutions, the explicit scale invariance of the equation,
Aleksandrov's reflection principle, and the Aronson--B\'enilan inequality.
Therefore, to complete the Widder theory, we must devise several proofs by
means of entirely new methods. We also establish an optimal quantitative
\emph{a priori} smoothing estimate for unsigned local solutions without
resorting to scale invariance, which seems to be new in this form even for the
classical porous medium equation. Finally, we show that non-negative very weak
solutions are always locally bounded, and in particular that they have locally
finite energy.

</details>


### [19] [Geometric aspects of the Harnack Inequality for a nonlocal heat equation](https://arxiv.org/abs/2506.08187)
*Mateusz Dembny,Mikołaj Sierżęga*

Main category: math.AP

TL;DR: The paper links a sharp Harnack bound for solutions of a fractional heat equation to circular geometry in higher dimensions, generalizing prior results.


<details>
  <summary>Details</summary>
Motivation: To extend and generalize previous findings on the relationship between Harnack bounds and geometric properties.

Method: Analyzes the double-sided Harnack bound for positive solutions of a fractional heat equation and connects it to circular geometry in higher dimensions.

Result: Establishes a connection between the Harnack bound and circular geometry, extending prior work.

Conclusion: The study successfully generalizes earlier results, highlighting the geometric implications of Harnack bounds in higher dimensions.

Abstract: We establish a connection between a sharp double-sided Harnack bound for
positive solutions of a fractional heat equation and the circular geometry in
higher dimensions. The present work extends and generalizes the results
obtained in the preceding paper.

</details>


### [20] [Homogenization of elasto-plastic plate equations with vanishing hardening](https://arxiv.org/abs/2506.08215)
*Marin Bužančić,Igor Velčić,Josip Žubrinić*

Main category: math.AP

TL;DR: The paper analyzes the interaction between homogenization and dimension reduction in perfectly plastic materials, focusing on a regime where plate thickness vanishes faster than material oscillation period. It derives a quasistatic evolution for a multiphase elasto-plastic composite without yield surface constraints.


<details>
  <summary>Details</summary>
Motivation: To understand the extreme regime of homogenization and dimension reduction in perfectly plastic materials and derive a quasistatic evolution for multiphase composites without yield surface constraints.

Method: Uses evolutionary Γ-convergence for dimension reduction to derive a heterogeneous plate model with hardening, then performs two-scale homogenization while letting hardening parameters tend to zero via a stress-strain approach.

Result: Reconciles Kirchhoff-Love structure of limiting displacements with the requirement that the effective dissipation potential at phase interfaces is the inf-convolution of potentials in either phase.

Conclusion: The approach successfully bridges the gap between dimension reduction and homogenization in the extreme regime, providing insights into quasistatic evolution for multiphase composites.

Abstract: Building on recent work analyzing the interaction between homogenization and
dimension reduction in perfectly plastic materials, we focus on the extreme
regime where the plate thickness vanishes much faster than the period of
oscillation of the heterogeneous material. Our investigation is devoted to the
derivation of the existence of a quasistatic evolution for a multiphase
elasto-plastic composite without imposing any kind of ordering constraints on
admissible yield surfaces of the various phases. The analysis is carried out in
two steps: we first derive a heterogeneous plate model with hardening via
dimension reduction using evolutionary $\Gamma$-convergence techniques, and
then perform two-scale homogenization while simultaneously letting the
hardening parameters tend to zero using a stress-strain approach. This approach
allows us to reconcile the Kirchhoff-Love structure of limiting displacements
with the expected requirement that the effective dissipation potential at each
point of the interface between two phases needs to be the pointwise-in-space
inf-convolution of that in either phase.

</details>


### [21] [Refined regularity at critical points for linear elliptic equations](https://arxiv.org/abs/2506.08281)
*Jongkeun Choi,Hongjie Dong,Seick Kim*

Main category: math.AP

TL;DR: The paper refines regularity results for solutions to linear elliptic equations with Dini mean oscillation coefficients, showing existence and continuity of second derivatives at critical points.


<details>
  <summary>Details</summary>
Motivation: To extend and refine existing regularity theorems for solutions to linear elliptic equations, particularly at critical points, by leveraging Dini mean oscillation conditions on coefficients.

Method: Analyzes solutions to divergence and non-divergence form equations, proving the existence and sharp continuity of second derivatives at points where the first derivative vanishes.

Result: Demonstrates that if $Du(x^o)=0$, then $D^2u(x^o)$ exists and satisfies continuity estimates, leading to $C^{2,\alpha}$ regularity at critical points for $C^\alpha$ coefficients.

Conclusion: The results refine Teixeira's theorem in the linear setting and extend analogous findings to non-divergence form equations.

Abstract: We investigate the regularity of solutions to linear elliptic equations in
both divergence and non-divergence forms, particularly when the principal
coefficients have Dini mean oscillation. We show that if a solution $u$ to a
divergence-form equation satisfies $Du(x^o)=0$ at a point, then the second
derivative $D^2u(x^o)$ exists and satisfies sharp continuity estimates. As a
consequence, we obtain ``$C^{2,\alpha}$ regularity'' at critical points when
the coefficients of $L$ are $C^\alpha$. This result refines a theorem of
Teixeira (Math. Ann. 358 (2014), no. 1--2, 241--256) in the linear setting,
where both linear and nonlinear equations were considered. We also establish an
analogous result for equations in non-divergence form.

</details>


### [22] [Symmetry and symmetry breaking in interpolation inequalities for two-dimensional spinors -- Preliminary results](https://arxiv.org/abs/2506.08318)
*Jean Dolbeault,Rupert L. Frank,Jonte Weixler*

Main category: math.AP

TL;DR: The paper studies a spinorial version of the Caffarelli-Kohn-Nirenberg inequality (SCKN) in 2D Euclidean space, exploring symmetry properties of optimal functions and stability via linearization. It connects to magnetic field inequalities and provides numerical evidence for extended symmetry-breaking regions.


<details>
  <summary>Details</summary>
Motivation: To understand the symmetry properties and stability of solutions to the spinorial Caffarelli-Kohn-Nirenberg inequality (SCKN) in 2D, and compare it with the 3D case.

Method: Linearize the functional around radial minimizers, analyze stability via a matrix-valued differential operator, and combine analytical and numerical approaches.

Result: Numerical evidence shows extended symmetry-breaking regions, while known symmetry thresholds remain stable. Refined phase transition estimates are provided.

Conclusion: The study reveals differences between 2D and 3D SCKN inequalities and advances understanding of symmetry-breaking behavior.

Abstract: On the two-dimensional Euclidean space, we study a spinorial analogue of the
Caffarelli-Kohn-Nirenberg inequality involving weighted gradient norms. This
(SCKN) inequality is equivalent to a spinorial Gagliardo-Nirenberg type
interpolation inequality on a cylinder as well as to an interpolation
inequality involving Aharonov-Bohm magnetic fields, which was analyzed in a
paper of 2020. We examine the symmetry properties of optimal functions by
linearizing the associated functional around radial minimizers. We prove that
the stability of the linearized problem is equivalent to the positivity of a
$2\times2$ matrix-valued differential operator. We study the positivity issue
via a combination of analytical arguments and numerical computations. In
particular, our results provide numerical evidence that the region of symmetry
breaking extends beyond what was previously known, while the threshold of the
known symmetry region is linearly stable. Altogether, we obtain refined
estimates of the phase transition between symmetry and symmetry breaking. Our
results also put in evidence striking differences with the three-dimensional
(SCKN) inequality that was recently investigated.

</details>


### [23] [On the singular set of the free boundary for a Monge-Ampère obstacle problem](https://arxiv.org/abs/2506.08387)
*Tianling Jin,Xushan Tu,Jingang Xiong*

Main category: math.AP

TL;DR: The paper extends prior work on the Monge-Ampère obstacle problem, focusing on the non-strictly convex part of the free boundary, deriving optimal dimension bounds for its flat portion, and exploring the strong maximum principle and stability.


<details>
  <summary>Details</summary>
Motivation: To further understand the Monge-Ampère obstacle problem by examining the non-strictly convex part of the free boundary, which was not covered in previous work.

Method: Analyzes the non-strictly convex part of the free boundary, establishes dimension bounds for its flat portion, and investigates the strong maximum principle and stability properties.

Result: Optimal dimension bounds for the flat portion of the non-strictly convex free boundary are derived, and insights into the strong maximum principle and stability are provided.

Conclusion: The study advances the understanding of the Monge-Ampère obstacle problem by addressing the non-strictly convex part of the free boundary and its properties.

Abstract: This is a continuation of our earlier work [14] on the Monge-Amp\`ere
obstacle problem \[ \det D^2 v = v^q \chi_{\{v>0\}}, \quad v \geq 0 \text{
convex} \] with $q \in [0,n)$, where we studied the regularity of the strictly
convex part of the free boundary.
  In this work, we examine the non-strictly convex part of the free boundary
and establish optimal dimension bounds for its flat portion. Additionally, we
investigate the strong maximum principle and a stability property for this
Monge-Amp\`ere obstacle problem.

</details>


### [24] [MHS equilibria in the non-resistive limit to the randomly forced resistive magnetic relaxation equations](https://arxiv.org/abs/2506.08394)
*Ken Abe,In-Jee Jeong,Federico Pasqualotto,Naoki Sato*

Main category: math.AP

TL;DR: The paper studies resistive magnetic relaxation equations (MRE) with random forcing, proving global well-posedness, invariant measures, and constructing a random magnetohydrostatic equilibrium as a non-resistive limit. For 2D, the equilibrium measure avoids compact sets in H1, and solutions are not finite Fourier modes.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of resistive MRE under random forcing and explore the existence and properties of invariant measures and equilibria in the non-resistive limit.

Method: Analyzes the system on a flat d-torus, proving global well-posedness and constructing invariant measures. Examines the limit as resistivity approaches zero to derive a random MHS equilibrium.

Result: Path-wise global well-posedness is shown, and a random MHS equilibrium is constructed. For d=2, the measure avoids compact sets in H1, and solutions are not finite Fourier modes.

Conclusion: The study provides insights into the statistical behavior of MRE under random forcing, highlighting the non-trivial structure of equilibria in the non-resistive limit, especially in 2D.

Abstract: We consider randomly forced resistive magnetic relaxation equations (MRE)
with resistivity $\kappa>0$ and a force proportional to $\sqrt{\kappa}\ $ on
the flat $d$-torus $\mathbb{T}^{d}$ for $d\geq 2$. We show the path-wise global
well-posedness of the system and the existence of the invariant measures, and
construct a random magnetohydrostatic (MHS) equilibrium $B(x)$ in
$H^{1}(\mathbb{T}^{d})$ with law $D(B)=\mu$ as a non-resistive limit $\kappa\to
0$ of statistically stationary solutions $B_{\kappa}(x,t)$. For $d=2$, the
measure $\mu$ does not concentrate on any compact sets in
$H^{1}(\mathbb{T}^{2})$ with finite Hausdorff dimension. In particular, all
realizations of the random MHS equilibrium $B(x)$ are almost surely not finite
Fourier mode solutions.

</details>


### [25] [Notes on tangent bicharacteristics and ill-posedness of the Cauchy problem](https://arxiv.org/abs/2506.08481)
*Enrico Bernardi,Tatsuo Nishitani*

Main category: math.AP

TL;DR: A family of second-order hyperbolic differential operators shows spectral transition in the Hamilton map, leading to non-local solvability in Gevrey classes beyond a certain order.


<details>
  <summary>Details</summary>
Motivation: To explore spectral transitions in hyperbolic operators and their implications for solvability in Gevrey classes.

Method: Analyzing a family of second-order hyperbolic differential operators with bicharacteristics tangent to the double manifold.

Result: Proved non-local solvability in Gevrey classes of order above a fixed value due to spectral transition.

Conclusion: The study highlights the impact of spectral transitions on solvability, particularly in Gevrey classes.

Abstract: We exhibit a family of second-order hyperbolic differential operators
presenting spectral transition of the Hamilton map. As a consequence we prove
that the Cauchy problem is not locally solvable at the origin in Gevrey classes
of order greater than some fixed value. The main feature of these operators is
that they may all have bicharacteristics tangent to the double manifold.

</details>


### [26] [Resonant frequencies distribution for multiple closely spaced subwavelength resonators](https://arxiv.org/abs/2506.08522)
*Haigang Li,Junhua Zhang*

Main category: math.AP

TL;DR: The paper studies how spatial arrangements of $N$ spherical resonators affect resonant frequencies and modes, showing distinct trends for single-row, multi-row, and ring configurations.


<details>
  <summary>Details</summary>
Motivation: To understand how the spatial arrangement of resonators influences resonant frequency distribution and mode behavior.

Method: Analysis of leading-order terms and derivation of explicit analytical expressions for frequency distributions.

Result: Single-row alignment shows $N$ distinct frequencies; multi-row broadens the range but may reduce frequencies; ring configurations resemble chains but with fewer frequencies. Resonant modes exhibit varying asymptotic behavior.

Conclusion: Spatial arrangement significantly impacts resonant frequency distribution and mode behavior, with analytical expressions provided for quantification.

Abstract: In this paper, we investigate a resonant system comprising $N$ closely packed
spherical resonators ($N>2$). We analyze how the spatial arrangement of these
resonators influences the distribution of resonant frequencies, focusing on
leading-order terms. Furthermore, we characterize the asymptotic behavior of
resonant modes linked to their respective frequencies. Our results demonstrate
distinct trends across configurations: For single-row alignment, the system
exhibits $N$ clearly separated resonant frequencies; For multi-row
arrangements, the resonant frequency range broadens, though the total number of
frequencies may diminish; while for ring configurations, comparable frequency
ranges to chain arrangements emerge, but with fewer resonant frequencies. We
derive explicit analytical expressions to quantify these frequency
distributions. Regarding resonant modes, we identify that at specific
frequencies, the gradient of these modes may exhibit different asymptotic
behavior between different resonators.

</details>


### [27] [Averaged models for compressible two-phase stratified flows on thin domains](https://arxiv.org/abs/2506.08542)
*Nicolas Seguin,Khaled Saleh,Pierrick Le Vourc'H*

Main category: math.AP

TL;DR: Derivation of compressible two-phase flow models using thin domain approximation and asymptotic analysis to recover source terms and two-velocity models.


<details>
  <summary>Details</summary>
Motivation: To develop accurate models for compressible two-phase flows by addressing the discontinuity of tangential velocity at the interface and scaling friction/viscosity coefficients.

Method: Thin domain approximation of a two-layer Navier-Stokes configuration, removing tangential velocity continuity, and scaling coefficients to derive two-velocity one-pressure models.

Result: Successful derivation of two-velocity one-pressure models, first for the barotropic case and then for the full Navier-Stokes-Fourier case.

Conclusion: The approach effectively derives compressible two-phase flow models, extending applicability to more complex scenarios.

Abstract: This paper deals with the derivation of compressible two-phase flow models.
We use a thin domain approximation of a two-layer configuration governed by the
Navier-Stokes equations, following the works [H. B. Stewart and B. Wendroff, J.
Comp. Phys., 56 (1984)] and [V. H. Ransom and D. L. Hicks, J. Comput. Phys., 75
(1988)]. In order to recover source terms and two-velocity models directly from
this asymptotic analysis, we remove the continuity of the tangential component
of the velocity at the interface, and properly scale friction and viscosity
coefficients. We are then able to derive two-velocity one-pressure models,
first in the barotropic case and then in the full Navier-Stokes-Fourier case.

</details>


### [28] [Global well-posedness for the 1D cubic nonlinear Shcrödinger equation in $L^p,\,p>2$](https://arxiv.org/abs/2506.08554)
*Ryosuke Hyakuna*

Main category: math.AP

TL;DR: The paper extends global well-posedness of the 1D cubic nonlinear Schrödinger equation from L² to Lᵖ (2 ≤ p < 13/6), proving persistence for a twisted variable.


<details>
  <summary>Details</summary>
Motivation: To generalize classical results of global well-posedness in L² to Lᵖ spaces, addressing gaps in the theory.

Method: Uses a data-decomposition argument by Vargas-Vega within Zhou's functional framework.

Result: Global solution in Lᵖ (2 ≤ p < 13/6) with persistence property for a twisted variable.

Conclusion: The work naturally extends classical L² results to broader Lᵖ spaces, leveraging innovative decomposition techniques.

Abstract: In this paper, we show that the one dimensional cubic nonlinear Schr\"odinger
equation is globally well posed in $L^p$ for $2\le p <13/6$. In particular, we
prove that the global solution enjoys the persistence property for a twisted
variable at any time, which implies the result is a natural exetension of the
classical global well-posedness in $L^2$ to $L^p$. The proof exploits the
data-decomposition argument originally developed by Vargas-Vega in the
functional framework introduced by Zhou.

</details>


### [29] [Boundary Control for Wildfire Mitigation](https://arxiv.org/abs/2506.08631)
*Mohamed Camil Belhadjoudja,M Maghenem,E Witrant,Didier Georges*

Main category: math.AP

TL;DR: A feedback control strategy for wildfire protection using PDEs, with controllers for known and unknown wind velocities, ensuring temperature convergence to ambient levels.


<details>
  <summary>Details</summary>
Motivation: To protect vulnerable areas from wildfires by controlling temperature in protected regions using boundary feedback.

Method: Coupled PDEs model heat and fuel dynamics. Neumann-type boundary controllers are designed for known and unknown wind velocities, using temperature measurements.

Result: Controllers ensure exponential (known wind) or asymptotic (unknown wind) temperature convergence to ambient levels, validated by simulations.

Conclusion: The proposed boundary feedback strategy effectively controls wildfire temperature in protected regions under varying wind conditions.

Abstract: In this paper, we propose a feedback control strategy to protect vulnerable
areas from wildfires. We consider a system of coupled partial differential
equations (PDEs) that models heat propagation and fuel depletion in wildfires
and study two cases. First, when the wind velocity is known, we design a
Neumann-type boundary controller guaranteeing that the temperature of some
protected region converges exponentially, in the $L^2$ norm, to the ambient
temperature. Second, when the wind velocity is unknown, we design an adaptive
Neumann-type boundary controller guaranteeing the asymptotic convergence, in
the $L^2$ norm, of the temperature of the protected region to the ambient
temperature. In both cases, the controller acts along the boundary of the
protected region and relies solely on temperature measurements along that
boundary. Our results are supported by numerical simulations.

</details>


### [30] [Pohozaev-type identities for classes of quasilinear elliptic local and nonlocal equations and systems, with applications](https://arxiv.org/abs/2506.08667)
*Gurdev Chand Anthal,Prashanta Garain*

Main category: math.AP

TL;DR: The paper derives Pohozaev-type identities for quasilinear elliptic equations and systems with local and nonlocal $p$-Laplace operators, including anisotropic and fractional cases, and extends them to systems.


<details>
  <summary>Details</summary>
Motivation: To establish new identities for mixed anisotropic and fractional $p$-Laplace equations, filling a gap in existing literature.

Method: Derives Pohozaev-type identities in $\mathbb{R}^n$ for various $p$-Laplace equations and systems, including mixed cases.

Result: New identities are obtained, especially for the mixed anisotropic and fractional $p$-Laplace case, even for $p=2$.

Conclusion: The results provide foundational identities for further analysis and applications in the study of quasilinear elliptic equations and systems.

Abstract: In this article, we establish Pohozaev-type identities for a class of
quasilinear elliptic equations and systems involving both local and nonlocal
$p$-Laplace operators. Specifically, we obtain these identities in
$\mathbb{R}^n$ for the purely anisotropic $p$-Laplace equations, the purely
fractional $p$-Laplace equations, as well as for equations that incorporate
both anisotropic and fractional $p$-Laplace features. We also extend these
results to the corresponding systems. To the best of our knowledge, the
identities we derive in the mixed case are new even when $p=2$. Finally, we
illustrate some of the applications of our main results.

</details>


### [31] [Nonexistence results for the semilinear wave equation on graphs](https://arxiv.org/abs/2506.08697)
*Dario Daniele Monticelli,Fabio Punzo,Jacopo Somaglia*

Main category: math.AP

TL;DR: Study of semilinear wave equations on weighted graphs, focusing on conditions for nonexistence of global solutions, including sign-changing cases.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of semilinear wave equations on weighted graphs, especially conditions preventing global solutions.

Method: Analysis of nonnegative and sign-changing solutions, using a novel technique for the latter.

Result: Established sufficient conditions for the nonexistence of global-in-time solutions.

Conclusion: The findings extend understanding of wave equations on graphs, with a new method for sign-changing solutions.

Abstract: We investigate the semilinear wave equation with potential on weighted
graphs. We establish sufficient conditions for the nonexistence of
global-in-time solutions. Both nonnegative and sign-changing solutions are
considered. In particular, the proof for sign-changing solutions relies on a
novel technique for this type of result.

</details>


### [32] [Unbounded sets of solutions of non-cooperative elliptic systems on symmetric spaces](https://arxiv.org/abs/2506.08705)
*Piotr Stefaniak*

Main category: math.AP

TL;DR: The paper proves that for a class of non-cooperative elliptic systems on compact symmetric spaces, any continuum of nontrivial solutions bifurcating from trivial solutions is unbounded.


<details>
  <summary>Details</summary>
Motivation: To analyze the behavior of solutions for non-cooperative elliptic systems on compact symmetric spaces.

Method: Uses the degree for invariant strongly indefinite functionals and leverages the torus-equivariant structure of Laplace--Beltrami eigenspaces.

Result: Any continuum of nontrivial solutions bifurcating from trivial solutions is unbounded.

Conclusion: The result is derived by excluding return to the trivial branch in an equivariant Rabinowitz alternative.

Abstract: The aim of this paper is to show that, for a class of non-cooperative
elliptic systems on compact symmetric spaces, any continuum of nontrivial
solutions bifurcating from the set of trivial solutions is unbounded. The main
tool is the degree for invariant strongly indefinite functionals. The analysis
relies on the torus-equivariant structure of the Laplace--Beltrami eigenspaces.
The result is obtained by ruling out return to the trivial branch in an
equivariant version of the Rabinowitz alternative.

</details>


### [33] [New Trends in Kinetic Theory Towards the Complexity of Living Systems](https://arxiv.org/abs/2506.08752)
*Nicola Bellomo,Diletta Burini,Jie Liao*

Main category: math.AP

TL;DR: A critical analysis of post-Prigogine research on mathematical theories for living systems, aiming to unify diverse approaches and explore interactions with AI studies.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of developing a unified mathematical theory for living systems, inspired by Prigogine's work and subsequent developments.

Method: Review and critical analysis of existing mathematical methods, focusing on kinetic theory and active particles, while considering interactions with AI.

Result: Highlights the diversity of approaches and the need for unification in the mathematical modeling of living systems.

Conclusion: Proposes the development of a unified theory, integrating insights from various methods and exploring synergies with AI research.

Abstract: The development of a mathematics for living systems is one of the most
challenging prospects of this century. The search began with the pioneering
contribution of Ilia Prigogine, who developed methods from statistical physics
to describe the dynamics of vehicular traffic. This visionary seminal research
contribution has given rise to a great deal of research activity, which began
at the end of the last century and has been further developed in this century
by several authors who have developed mathematical methods, generally focused
on applications. These methods are somewhat inspired by the classical kinetic
theory, but significant differences have led to the concept of active particles
and to a kinetic theory that is ultimately very different from the classical
theory. Different approaches have been developed, each of which is in some way
an alternative to the others. This paper develops a critical analysis of the
scientific activity after Prigogine with the aim of developing a unified
mathematical theory, taking into account the conceivable interactions that a
mathematical theory of living systems can have with studies of artificial
intelligence.

</details>


### [34] [Stability of Stochastically Driven Couette Flow in 2D with Navier Boundary Conditions at high Reynolds number via Averaging Principle](https://arxiv.org/abs/2506.08769)
*Ryan Arbon,Jacob Bedrossian*

Main category: math.AP

TL;DR: The paper analyzes stochastic Navier-Stokes equations near Couette flow under small stochastic forcing, revealing an averaging principle separating slow and fast modes.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of stochastic Navier-Stokes at high Reynolds numbers and its stability threshold for shear flows.

Method: Splits the perturbation into zero and non-zero x-modes, deriving a closed nonlinear equation for slow modes and a pseudo-linearized equation for fast modes.

Result: An averaging principle emerges, with slow modes governed by a nonlinear equation and fast modes dominated by inviscid damping and enhanced dissipation.

Conclusion: The work provides insights into the stochastic stability threshold for shear flows, extending deterministic results to stochastic settings.

Abstract: We characterize the behavior of stochastic Navier-Stokes on $\mathbb{T}
\times [-1,1]$ with Navier boundary conditions at high Reynolds number when
initialized near Couette flow subject to small additive stochastic forcing. We
take additive noise of strength $\nu^{1/2+} \Phi dV_t + \nu^{2/3+\alpha} \Psi
dW_t$, where $\Phi dV_t$ has spatial correlation in $H_0^3$ and acts only on
$x$-independent modes of the vorticity, while $\Psi dW_t$ has spatial
correlation in a lower order, anisotropic, Sobolev space $\mathcal{H}$ and acts
on $x$-dependent-modes. We take the initial $x$-independent modes in the
perturbation to be small in $H_0^3$ in a $\nu$-independent sense, while the
non-zero $x$-modes are taken to be $O(\nu^{1/2 + \alpha})$ in $\mathcal{H}$.
Letting $\omega$ solve the resulting perturbation equation, we split $\omega$
into the zero $x$-modes $\omega_0$ and the non-zero $x$-modes $\omega_{\neq}$.
We demonstrate an averaging principle holds wherein $\omega_{\neq}$ is the fast
variable and $\omega_0$ is the slow variable, deriving a closed nonlinear
evolution equation on $\omega_0$ that holds over long time-scales (while the
fast $\omega_{\neq}$ modes solve a `pseudo-linearized' equation to leading
order with dynamics dominated by inviscid damping and enhanced dissipation).
This work can also be considered the stochastic analogue of the stability
threshold problem for shear flows.

</details>


### [35] [Dispersive estimates for Dirac Operators in dimension four with obstructions at threshold energies](https://arxiv.org/abs/2506.08831)
*William R. Green,Connor Lane,Benjamin Lyons*

Main category: math.AP

TL;DR: The paper studies $L^1\to L^\infty$ dispersive estimates for the Dirac equation in four dimensions, classifying threshold obstructions and proving decay rates.


<details>
  <summary>Details</summary>
Motivation: To understand the dispersive behavior of the Dirac equation with a potential, particularly near thresholds, and classify resonances and eigenfunctions.

Method: Analyzes the structure of threshold obstructions, proving decay rates and introducing a time-dependent operator for resonant or eigenvalue cases.

Result: Shows $t^{-2}$ decay for regular thresholds and introduces a finite-rank operator with logarithmic decay for resonant or eigenvalue cases.

Conclusion: Provides a complete description of dispersive bounds, including high energy behavior, and clarifies the role of threshold resonances and eigenvalues.

Abstract: We investigate $L^1\to L^\infty$ dispersive estimates for the Dirac equation
with a potential in four spatial dimensions. We classify the structure of the
obstructions at the thresholds as being composed of an at most two dimensional
space of resonances per threshold, and finitely many eigenfunctions. Similar to
the Schr\"odinger evolution, we prove the natural $t^{-2}$ decay rate when the
thresholds are regular. When there is a threshold resonance or eigenvalue, we
show that there is a time dependent, finite rank operator satisfying
$\|F_t\|_{L^1\to L^\infty}\lesssim (\log t)^{-1}$ for $t>2$ such that $$
  \|e^{it\mathcal H}P(\mathcal H)-F_t\|_{L^1\to L^\infty}\lesssim t^{-1} \quad
\text{for } t>2, $$ with $P$ a projection onto a subspace of the absolutely
continuous spectrum in a small neighborhood of the thresholds. We further show
that the operator $F_t=0$ if there is a threshold eigenvalue but no threshold
resonance. We pair this with high energy bounds for the evolution and provide a
complete description of the dispersive bounds.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [36] [Physics-Informed Neural Networks for Irregular Domain Mapping and Partial Differential Equations solving](https://arxiv.org/abs/2506.08622)
*Cuizhi Zhou,Kaien Zhu*

Main category: physics.comp-ph

TL;DR: The paper presents a method using physics-informed neural networks (PINNs) to map irregular domains to regular ones, simplifying PDE solving on rectangular grids.


<details>
  <summary>Details</summary>
Motivation: Solving PDEs on irregular domains is challenging; this work aims to simplify the process by leveraging PINNs for domain mapping.

Method: The approach uses PINNs for irregular-to-regular domain mapping, enabling PDE solving on rectangular grids with finite difference methods and physics-informed convolutional neural networks.

Result: PINNs successfully generate structured grids on irregular domains, tailored to computational needs, easing PDE solving.

Conclusion: PINNs offer a versatile and effective solution for transforming irregular domains into structured grids, enhancing PDE solving efficiency.

Abstract: The solution of partial differential equations (PDES) on irregular domains
has long been a subject of significant research interest. In this work, we
present an approach utilizing physics-informed neural networks (PINNs) to
achieve irregular-to-regular domain mapping. Thus we can use finite difference
method and physics-informed convolutional neural networks to solve PDEs on
rectangular grids instead of the original irregular boundary.
  Structured grids on irregular domains are obtained by inverse mapping. We
demonstrate PINN's versatile capability to produce customized structured grids
tailored to diverse computational requirements, thereby significantly
facilitating PDEs solving.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [37] [The effect of plasma expansion on the dispersion properties of MHD waves](https://arxiv.org/abs/2506.08248)
*Sebastián Saldivia,Felipe A. Asenjo,Pablo S. Moya*

Main category: physics.plasm-ph

TL;DR: The paper uses the Expanding Box Model (EBM) to study how radial solar wind expansion affects MHD waves. It finds wave frequencies decrease with distance from the Sun, influenced by the polytropic index, with notable effects on the fast magnetosonic mode.


<details>
  <summary>Details</summary>
Motivation: To understand how solar wind expansion impacts MHD wave propagation and bridge theory with observations.

Method: Analytical derivation of dispersion relations from MHD-EBM equations, considering three polytropic index models.

Result: Wave frequencies decrease with distance; fast magnetosonic mode accelerates in the distant heliosphere for certain polytropic index profiles.

Conclusion: Solar wind expansion significantly influences MHD wave dynamics, with the polytropic index playing a key role in wave behavior.

Abstract: In this work, we employ the set of ideal expanding magnetohydrodynamic (MHD)
equations within the Expanding Box Model (EBM) framework to theoretically
characterize the effects of radial solar wind expansion on its characteristic
linear MHD waves. Through the analytical derivation of dispersion relations by
a first-order expansion of the MHD-EBM equations, we explore the changes in
wave propagation across a range of heliocentric distances on the linear
magnetohydrodynamic modes: the Alfv\'en mode and the fast and slow magnetosonic
modes, as obtained from the ideal MHD-EBM equations. Our findings reveal a
spatial dependence in the derived dispersion relations that aligns with both
the literature and the traditional ideal MHD case in the non-expanding limit,
thereby helping to bridge the gap between theory and observation in solar wind
dynamics. We observe a general decrease in wave frequencies as the plasma
expands farther from the Sun. This decrease is reflected in the dispersion
relations through the radial decrease of both the Alfv\'en and sound speeds,
which decrease proportionally to $1/R$ and $1/R^{\gamma - 1}$, respectively,
where $\gamma$ is the plasma polytropic index. The fast magnetosonic mode
frequency and phase speed are significantly affected by the polytropic index
value. We consider three models for the polytropic index evolution in the
expanding solar wind: a constant (quasi-adiabatic) case, a radially decreasing
profile in the outer heliosphere, and a model incorporating thermodynamic
heating effects. Notably, we find that in the case of a decreasing polytropic
index, the fast magnetosonic mode experiences an acceleration in the distant
heliosphere, highlighting the significant influence of expansion on solar wind
dynamics.

</details>


### [38] [Particle collisionality in scaled kinetic plasma simulations](https://arxiv.org/abs/2506.08495)
*S. R. Totorica,K. V. Lezhnin,W. Fox*

Main category: physics.plasm-ph

TL;DR: A method to scale particle collisionality in kinetic plasma simulations using species-dependent scaling factors, preserving transport properties and relaxation rates.


<details>
  <summary>Details</summary>
Motivation: Understanding the impact of artificial parameters (e.g., reduced speed of light, ion-to-electron mass ratio) on particle collisionality in kinetic plasma simulations.

Method: Introduces species-dependent scaling factors to adjust inter- and intra-species collision rates, maintaining fidelity of transport properties and relaxation rates.

Result: Benchmarking tests confirm the method's accuracy in replicating theoretical relaxation rates and fluid theory, retaining key transport properties.

Conclusion: The scaling method can be easily integrated into existing collisional PIC implementations, aiding the study of marginally collisional plasmas.

Abstract: Kinetic plasma processes, such as magnetic reconnection, collisionless
shocks, and turbulence, are fundamental to the dynamics of astrophysical and
laboratory plasmas. Simulating these processes often requires particle-in-cell
(PIC) methods, but the computational cost of fully kinetic simulations can
necessitate the use of artificial parameters, such as a reduced speed of light
and ion-to-electron mass ratio, to decrease expense. While these approximations
can preserve overall dynamics under specific conditions, they introduce
nontrivial impacts on particle collisionality that are not yet well understood.
In this work, we develop a method to scale particle collisionality in
simulations employing such approximations. By introducing species-dependent
scaling factors, we independently adjust inter- and intra-species collision
rates to better replicate the collisional properties of the physical system.
Our approach maintains the fidelity of electron and ion transport properties
while preserving critical relaxation rates, such as energy exchange timescales,
within the limits of weakly collisional plasma theory. We demonstrate the
accuracy of this scaling method through benchmarking tests against theoretical
relaxation rates and connecting to fluid theory, highlighting its ability to
retain key transport properties. Existing collisional PIC implementations can
be easily modified to include this scaling, which will enable deeper insights
into the behavior of marginally collisional plasmas across various contexts.

</details>


### [39] [Numerical modeling of isochoric heating experiments using the TROLL code in the warm dense matter regime](https://arxiv.org/abs/2506.08912)
*Sébastien Rassou,Marie Bonneau,Christophe Rousseaux,Xavier Vaisseau,Witold Cayzac,Adrien Denoeud,Frédéric Perez,Tom Beaumont,Morris Demoulins,Jean-Christophe Pain*

Main category: physics.plasm-ph

TL;DR: Proton heating experiments at LULI laser facilities achieved warm dense matter conditions, with simulations matching experimental results using SESAME EOS and SRIM stopping power.


<details>
  <summary>Details</summary>
Motivation: To study isochoric heating of solid materials by protons and validate simulation models against experimental data.

Method: Protons from TNSA heated aluminum/copper foils; temperature measured via streaked optical pyrometry. Simulations used TROLL code with Monte-Carlo transport and SRIM stopping power.

Result: Material reached 1-5 eV temperatures; simulations agreed with experiments using SESAME EOS and SRIM.

Conclusion: The study successfully validated the simulation approach for proton heating in warm dense matter.

Abstract: Experiments of isochoric heating by protons of solid material were recently
performed at LULI laser facilities. In these experiments, protons, produced
from target normal sheath acceleration (TNSA) of Au foil with the PICO2000
laser, deposit their energy into an aluminum or copper foil initially at room
temperature and solid density. The heated material reaches the warm dense
matter regime with temperature in the rear face of the material between 1 and 5
eV. The temperature is inferred by streaked optical pyrometry and the proton
beam is characterized by Thomson parabola. The high-energy protons produced by
TNSA are modeled to deduce the initial proton distribution before the slowing
down in the target. Hydrodynamic radiative simulations were next performed
using the TROLL code in multidimensional geometry. In the TROLL code, the
heating of protons is modeled with a Monte-Carlo transport module of charged
particle and the calculation of the energy deposited by the protons in the
matter is performed using stopping power formulas like SRIM functions. The
results of simulations with the TROLL code are compared with the experimental
results. An acceptable agreement between experiment and simulation is found for
the temperature at the rear of the material using SESAME equation of state and
SRIM stopping power for protons in aluminum.

</details>


<div id='physics.space-ph'></div>

# physics.space-ph [[Back]](#toc)

### [40] [Planar Collisionless Shock Simulations with Semi-Implicit Particle-in-Cell Model FLEKS](https://arxiv.org/abs/2506.08384)
*Hongyang Zhou,Yuxi Chen,Chuanfei Dong,Liang Wang,Ying Zou,Brian Walsh,Gábor Tóth*

Main category: physics.space-ph

TL;DR: The study evaluates FLEKS, a semi-implicit particle-in-cell code, for simulating collisionless shocks in magnetosphere modeling. It successfully captures shock features and wave dynamics, emphasizing the need for 2D simulations for accuracy.


<details>
  <summary>Details</summary>
Motivation: To assess FLEKS's capability in simulating collisionless shocks relevant to global magnetosphere modeling, focusing on solar wind conditions at 1 au.

Method: One- and two-dimensional local planar shock simulations using MHD states, examining quasi-perpendicular and quasi-parallel configurations with refined algorithms.

Result: FLEKS accurately simulates shock structures, waves, and non-Maxwellian distributions, highlighting the necessity of 2D simulations for downstream wave physics and complex dynamics.

Conclusion: The study offers guidance for parameter selection in semi-implicit PIC codes, facilitating kinetic shock integration into large-scale space plasma simulations.

Abstract: This study investigates the applicability of the semi-implicit
particle-in-cell code FLEKS to collisionless shock simulations, with a focus on
the parameter regime relevant to global magnetosphere modeling. We examine one-
and two-dimensional local planar shock simulations, initialized using MHD
states with upstream conditions representative of the solar wind at 1 au, for
both quasi-perpendicular and quasi-parallel configurations. The refined
algorithm in FLEKS proves robust, enabling accurate shock simulations with a
grid resolution on the order of the electron inertial length $d_e$. Our
simulations successfully capture key shock features, including shock structures
(foot, ramp, overshoot, and undershoot), upstream and downstream waves (fast
magnetosonic, whistler, Alfv\'en ion-cyclotron, and mirror modes), and
non-Maxwellian particle distributions. Crucially, we find that at least two
spatial dimensions are critical for accurately reproducing downstream wave
physics in quasi-perpendicular shocks and capturing the complex dynamics of
quasi-parallel shocks, including surface rippling, shocklets, SLAMS, and jets.
Furthermore, our parameter studies demonstrate the impact of mass ratio and
grid resolution on shock physics. This work provides valuable guidance for
selecting appropriate physical and numerical parameters for a semi-implicit PIC
code, paving the way for incorporating kinetic shock processes into large-scale
space plasma simulations with the MHD-AEPIC model.

</details>


### [41] [Meter-scale Observations of Equatorial Plasma Turbulence](https://arxiv.org/abs/2506.08665)
*Magnus F Ivarsen,Lasse B N Clausen,Yaqi Jin,Jaeheung Park*

Main category: physics.space-ph

TL;DR: A multi-Needle Langmuir Probe on the ISS measures electron currents to infer plasma properties, revealing detailed turbulent structures in equatorial plasma bubbles.


<details>
  <summary>Details</summary>
Motivation: To study turbulent structuring in the F-region plasma using high-frequency sampling from the ISS.

Method: Uses a multi-needle probe with fixed-bias cylindrical copper needles and polynomial fitting in the current-voltage plane.

Result: Observes detailed density timeseries and power spectral density estimates, identifying break-points at scale-sizes between 1 m and 300 m.

Conclusion: The findings suggest turbulent dissipation of kilometer-scale swirls in the plasma, linked to the gradient-drift instability.

Abstract: The multi-Needle Langmuir Probe collects an electron current through four
fixed-bias cylindrical copper needles. This allows for an extremely high
sampling frequency, with plasma properties being inferred through polynomial
fitting in the current-voltage plane. We present initial results from such a
multi-needle probe mounted on the International Space Station, orbiting Earth
at an altitude of around 400 km. That altitude, and its orbital inclination
(~50 degrees), place the ISS as a suitable platform for observing equatorial
plasma bubbles. In case studies of such turbulent structuring of the F-region
plasma, we observe density timeseries that conserve considerable detail at
virtually every level of magnification down to its Nyquist scale of 2-5 meters.
We present power spectral density estimates of the turbulent structuring found
inside equatorial plasma bubbles, and we discuss apparent break-points at
scale-sizes between 1 m and 300 m, which we interpret in the light of turbulent
dissipation as kilometer-scale swirls produced by the gradient-drift
instability dissipate in the plasma.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [42] [Morse Index Stability of Branched Willmore Immersions](https://arxiv.org/abs/2506.09005)
*Alexis Michelat*

Main category: math.DG

TL;DR: Morse index stability for sequences of Willmore immersions with bounded energy, even with branch points in the limit or bubbles.


<details>
  <summary>Details</summary>
Motivation: To extend the understanding of Morse index stability in Willmore immersions by relaxing the assumption of branch point absence in the limit or bubbles.

Method: Analyze sequences of Willmore immersions with bounded energy, allowing for branch points in the limiting immersion and bubbles.

Result: Demonstrates Morse index stability under these generalized conditions.

Conclusion: The findings broaden the applicability of Morse index stability results in the study of Willmore immersions.

Abstract: We show the Morse index stability for sequences of Willmore immersions of
bounded energy without assuming that the limiting immersion and the bubbles are
free of branch points.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [43] [Thermodynamically Consistent Latent Dynamics Identification for Parametric Systems](https://arxiv.org/abs/2506.08475)
*Xiaolong He,Yeonjong Shin,Anthony Gruber,Sohyeon Jung,Kookjin Lee,Youngsoo Choi*

Main category: cs.LG

TL;DR: The paper introduces tLaSDI, a thermodynamics-informed framework for reduced-order modeling of nonlinear dynamical systems, combining autoencoders with pGFINNs to preserve thermodynamic principles and achieve high efficiency.


<details>
  <summary>Details</summary>
Motivation: To efficiently model parametric nonlinear dynamical systems while preserving thermodynamic principles like free energy conservation and entropy generation.

Method: Uses autoencoders for dimensionality reduction and pGFINNs for learning parametric latent dynamics, enhanced by a physics-informed active learning strategy.

Result: Achieves up to 3,528x speed-up with 1-3% relative errors, reduces training (50-90%) and inference (57-61%) costs, and reveals thermodynamic behavior in latent space.

Conclusion: tLaSDI is an efficient and accurate framework for reduced-order modeling, offering insights into system thermodynamics and significant computational savings.

Abstract: We propose an efficient thermodynamics-informed latent space dynamics
identification (tLaSDI) framework for the reduced-order modeling of parametric
nonlinear dynamical systems. This framework integrates autoencoders for
dimensionality reduction with newly developed parametric GENERIC
formalism-informed neural networks (pGFINNs), which enable efficient learning
of parametric latent dynamics while preserving key thermodynamic principles
such as free energy conservation and entropy generation across the parameter
space. To further enhance model performance, a physics-informed active learning
strategy is incorporated, leveraging a greedy, residual-based error indicator
to adaptively sample informative training data, outperforming uniform sampling
at equivalent computational cost. Numerical experiments on the Burgers'
equation and the 1D/1V Vlasov-Poisson equation demonstrate that the proposed
method achieves up to 3,528x speed-up with 1-3% relative errors, and
significant reduction in training (50-90%) and inference (57-61%) cost.
Moreover, the learned latent space dynamics reveal the underlying thermodynamic
behavior of the system, offering valuable insights into the physical-space
dynamics.

</details>


### [44] [Flow Matching Meets PDEs: A Unified Framework for Physics-Constrained Generation](https://arxiv.org/abs/2506.08604)
*Giacomo Baldan,Qiang Liu,Alberto Guardone,Nils Thuerey*

Main category: cs.LG

TL;DR: PBFM is a new generative framework embedding physical constraints into flow matching, improving accuracy and reducing residuals compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Current generative methods learn physics implicitly; PBFM explicitly integrates physical constraints for better surrogate modeling.

Method: PBFM combines flow matching loss with physics-based residual loss, uses temporal unrolling, and analyzes noise level impact.

Result: PBFM achieves up to 8× better physical residuals and outperforms existing methods in distributional accuracy.

Conclusion: PBFM offers an efficient framework for surrogate modeling and simulation in physics and engineering.

Abstract: Generative machine learning methods, such as diffusion models and flow
matching, have shown great potential in modeling complex system behaviors and
building efficient surrogate models. However, these methods typically learn the
underlying physics implicitly from data. We propose Physics-Based Flow Matching
(PBFM), a novel generative framework that explicitly embeds physical
constraints, both PDE residuals and algebraic relations, into the flow matching
objective. We also introduce temporal unrolling at training time that improves
the accuracy of the final, noise-free sample prediction. Our method jointly
minimizes the flow matching loss and the physics-based residual loss without
requiring hyperparameter tuning of their relative weights. Additionally, we
analyze the role of the minimum noise level, $\sigma_{\min}$, in the context of
physical constraints and evaluate a stochastic sampling strategy that helps to
reduce physical residuals. Through extensive benchmarks on three representative
PDE problems, we show that our approach yields up to an $8\times$ more accurate
physical residuals compared to FM, while clearly outperforming existing
algorithms in terms of distributional accuracy. PBFM thus provides a principled
and efficient framework for surrogate modeling, uncertainty quantification, and
accelerated simulation in physics and engineering applications.

</details>


<div id='physics.atom-ph'></div>

# physics.atom-ph [[Back]](#toc)

### [45] [Portal for High-Precision Atomic Data and Computation](https://arxiv.org/abs/2506.08170)
*Amani Kiruga,Charles Cheung,Dmytro Filin,Parinaz Barakhshan,Akshay Bhosale,Vipul Badhan,Bindiya Arora,Rudolf Eigenmann,Marianna S. Safronova*

Main category: physics.atom-ph

TL;DR: A scalable online atomic data portal with automated updates provides comprehensive atomic data for 28 atoms/ions, supported by open-access software and accuracy assessments.


<details>
  <summary>Details</summary>
Motivation: To create a sustainable and user-friendly platform for atomic data, leveraging automation and open-access tools for accuracy and scalability.

Method: Developed an automated interface for data updates, supported by open-access atomic software and workflow algorithms for accuracy assessment. Includes comparison with NIST database.

Result: Portal offers energies, transition rates, polarizabilities, and more for 28 systems, with uncertainties and experimental references.

Conclusion: The portal is a scalable, accurate resource for atomic data, with plans to expand to more systems.

Abstract: We've developed a scalable and sustainable online atomic data portal with an
automated interface for easy update and addition of new data. The current
portal provides energies, transition matrix elements, transition rates,
radiative lifetimes, branching ratios, polarizabilities, hyperfine constants,
and other data, for 28 atoms and ions. It also features an interactive
polarizability plotting interface for neutral atoms and singly-charged ions.
The data production is supported by recent developments of open-access atomic
software based on our research codes, including new workflow algorithms, which
allow large volumes of such data to be generated with automated accuracy
assessments. This entails a new method of comparing our calculated values with
data from the NIST Atomic Spectra Database. All calculated values include
estimated uncertainties. Data for more systems will be added in the future.
Experimental values are included with references, where high-precision data are
available.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [46] [Adaptive quantum dynamics with the time-dependent variational Monte Carlo method](https://arxiv.org/abs/2506.08575)
*Raffaele Salioni,Rocco Martinazzo,Davide Emilio Galli,Christian Apostoli*

Main category: quant-ph

TL;DR: An adaptive tVMC method is introduced to control the expressivity of variational quantum states, improving numerical stability by selectively evolving significant parameters based on their contribution to reducing local-in-time error.


<details>
  <summary>Details</summary>
Motivation: Address numerical instabilities in tVMC simulations caused by overparameterized or redundant variational ansätze.

Method: Uses local-in-time error (LITE) to quantify parameter relevance, selectively evolving significant parameters while maintaining accuracy.

Result: Benchmarked on quantum quenches in the 1D transverse-field Ising model, the method improves stability and reduces the need for strong regularization.

Conclusion: The adaptive tVMC scheme enables reliable simulations with highly expressive variational ansätze by focusing on relevant parameters.

Abstract: We introduce an extension of the time-dependent variational Monte Carlo
(tVMC) method that adaptively controls the expressivity of the variational
quantum state during the simulation. This adaptive tVMC approach addresses
numerical instabilities that arise when the variational ansatz is
overparameterized or contains redundant degrees of freedom. Building on the
concept of the local-in-time error (LITE), a measure of the deviation between
variational and exact evolution, we introduce a procedure to quantify each
parameter's contribution to reducing the LITE, using only quantities already
computed in standard tVMC simulations. These relevance estimates guide the
selective evolution of only the most significant parameters at each time step,
while maintaining a prescribed level of accuracy. We benchmark the algorithm on
quantum quenches in the one-dimensional transverse-field Ising model using both
spin-Jastrow and restricted Boltzmann machine wave functions, with an emphasis
on overparameterized regimes. The adaptive scheme significantly improves
numerical stability and reduces the need for strong regularization, enabling
reliable simulations with highly expressive variational ans\"atze.

</details>


<div id='math.SP'></div>

# math.SP [[Back]](#toc)

### [47] [Introduction to Nonlinear Spectral Analysis](https://arxiv.org/abs/2506.08754)
*Leon Bungert,Yury Korolev*

Main category: math.SP

TL;DR: Introduction to nonlinear spectral theory, covering variational forms of nonlinear eigenvalue problems, gradient flows, and applications in PDEs and $L^\infty$ variational problems, with numerical methods discussed.


<details>
  <summary>Details</summary>
Motivation: To provide a foundational understanding of nonlinear spectral theory, its variational forms, and connections with gradient flows, along with practical applications and numerical solutions.

Method: Uses convex analysis and calculus of variations to study nonlinear eigenvalue problems, gradient flows, and their asymptotic behaviors. Includes numerical methods like the nonlinear power method.

Result: Presents conditions for finite time extinction, convergence rates, and applications in nonlinear PDEs and $L^\infty$ problems.

Conclusion: The notes offer a comprehensive introduction to nonlinear spectral theory, bridging theory and applications, with numerical methods for practical solutions.

Abstract: These notes are meant as an introduction to the theory of nonlinear spectral
theory. We will discuss the variational form of nonlninear eigenvalue problems
and the corresponding non-linear Euler--Lagrange equations, as well as
connections with gradient flows. For the latter ones, we will give precise
conditions for finite time extinction and discuss convergence rates. We will
use this theory to study asymptotic behaviour of nonlinear PDEs and present
applications in $L^\infty$ variational problems. Finally we will discuss
numerical methods for solving gradient flows and computing nonlinear
eigenfunctions based on a nonlinear power method. Our main tools are convex
analysis and calculus of variations, necessary background on which will be
provided. It is expected that the reader is familiar with Hilbert spaces;
familiarity with Banach spaces is beneficial but not strictly necessary. The
notes are based on the lectures taught by the authors at the universities of
Bonn and Cambridge in 2022.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [48] [Probing Strong-Field QED via Angle-Discriminated Emissions from Electrons Traversing Colliding Laser Pulses](https://arxiv.org/abs/2506.08741)
*C. Olofsson,A. Gonoskov*

Main category: physics.optics

TL;DR: A proposed collider geometry using multiple laser pulses suppresses radiative losses and enhances signal discrimination for high-χ QED events in strong-field experiments.


<details>
  <summary>Details</summary>
Motivation: To enable studies of strong-field QED in extreme regimes by mitigating premature radiative losses and improving signal discrimination.

Method: Electrons traverse the waist of two or four perpendicularly propagating, tightly focused laser pulses, leveraging short interaction lengths and helical motion for signal discrimination.

Result: Four circularly polarized pulses achieve near-perfect high-χ angle-discrimination, ensuring high signal-to-noise ratio.

Conclusion: The proposed collider geometry is a viable layout for future high-χ QED experiments at PW laser facilities.

Abstract: Future laser-electron colliders will reach quantum parameters $\chi$ well in
excess of unity, enabling studies of strong-field QED in extreme regimes.
However, statistical inference in such experiments requires mitigating
premature radiative losses of electrons to enable high-$\chi$ QED events, as
well as separating the detectable signal of these events from that of
lower-$\chi$ particles and photons produced by QED cascades. We propose a
collider geometry in which electrons traverse the waist of two or four
perpendicularly propagating, tightly focused laser pulses. This configuration
suppresses both outlined difficulties by leveraging the short interaction
length of the waist, rather than relying on the more technically demanding
reduction of pulse duration. Moreover, altering the phase and polarization of
each pulse causes the electrons to undergo helical motion where the deflection
angle is correlated with the field strength, permitting an angle-based
discrimination of the signal from high-$\chi$ events. Analysis and simulations
show that the case of four circularly polarized pulses uniquely permits
achieving helical motion throughout the entire focal region, leading to
near-perfect high-$\chi$ angle-discrimination and thereby high signal-to-noise
ratio. These findings support the consideration of the proposed concept as a
viable layout for future experiments at PW laser facilities.

</details>


<div id='math.FA'></div>

# math.FA [[Back]](#toc)

### [49] [On the domains of first order differential operators on the Sierpiński gasket](https://arxiv.org/abs/2506.08679)
*Waldemar Schefer*

Main category: math.FA

TL;DR: The paper analyzes $L^2$-differential one-forms on the Sierpiński gasket, focusing on their first-order structure, self-similarity, and introduces first-order differential operators. It provides a pointwise representation result and examples with discontinuities.


<details>
  <summary>Details</summary>
Motivation: To understand the first-order structure and self-similarity properties of $L^2$-differential one-forms on the Sierpiński gasket, extending insights from energy finite functions.

Method: Study piecewise energy finite functions, introduce first-order differential operators, and analyze their domains. Prove a pointwise representation result for certain elements.

Result: A pointwise representation result for elements in the domain of the first-order differential operator, demonstrated through examples with discontinuities.

Conclusion: The work extends understanding of differential forms on fractals, providing tools for further analysis and applications in similar settings.

Abstract: We study the first order structure of $L^2$-differential one-forms on the
Sierpi\'{n}ski gasket. We consider piecewise energy finite functions related to
one-forms, normal parts, and show self-similarity properties of one-forms as in
the case of energy finite functions on the Sierpi\'{n}ski gasket. We introduce
first order differential operators, taking functions into functions, that can
be understood as total derivatives with respect to some reference function or
form. The main result is a pointwise representation result for certain elements
in the domain of said first order differential operator by a ratio of normal
derivatives. At last we provide three classes of examples in the domain of said
first order differential operator with different types of discontinuities.

</details>


<div id='hep-lat'></div>

# hep-lat [[Back]](#toc)

### [50] [Numerical stability of force-gradient integrators and their Hessian-free variants in lattice QCD simulations](https://arxiv.org/abs/2506.08813)
*Kevin Schäfers,Jacob Finkenrath,Michael Günther,Francesco Knechtli*

Main category: hep-lat

TL;DR: The paper analyzes the numerical stability of force-gradient integrators and their Hessian-free variants in Hamiltonian Monte Carlo for lattice QCD, identifying stable and accurate integrator variants.


<details>
  <summary>Details</summary>
Motivation: To evaluate the stability and efficiency of force-gradient integrators in lattice QCD simulations, ensuring computational accuracy.

Method: Linear stability analysis using the harmonic oscillator test equation, with simulations in the two-dimensional Schwinger model and lattice QCD.

Result: Stable integrator variants were identified, with no significant stability differences between force-gradient and Hessian-free integrators.

Conclusion: Numerical stability is crucial for evaluating integrator efficiency in lattice QCD, with promising variants offering a balance of accuracy and stability.

Abstract: We investigate the numerical stability of force-gradient integrators and
their Hessian-free variants within the molecular dynamics step of the
Hamiltonian Monte Carlo algorithm in lattice QCD simulations. A linear
stability analysis of (Hessian-free) force-gradient integrators is conducted by
investigating the harmonic oscillator as a test equation. By performing
detailed stability investigations for the entire family of self-adjoint
integrators with up to eleven exponentials per time step, we detect promising
integrator variants that are providing a good trade-off between accuracy and
numerical stability. Simulations for the two-dimensional Schwinger model
demonstrate that there are no significant differences in the stability domain
of a force-gradient integrator and its Hessian-free counterpart. Furthermore,
lattice QCD simulations are conducted to emphasize the significance of
numerical stability as a metric for evaluating the computational efficiency of
integrators when applied to lattice QCD simulations.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [51] [Towards AI-assisted Neutrino Flavor Theory Design](https://arxiv.org/abs/2506.08080)
*Jason Benjamin Baretz,Max Fieg,Vijay Ganesh,Aishik Ghosh,V. Knapp-Perez,Jake Rudolph,Daniel Whiteson*

Main category: hep-ph

TL;DR: AMBer is a reinforcement learning framework for efficiently constructing particle physics models by selecting symmetry groups and particle content, validated in known and novel theory spaces.


<details>
  <summary>Details</summary>
Motivation: The vast landscape of particle physics models requires intuitive and labor-intensive construction. AMBer automates this process to streamline theory-building.

Method: AMBer uses reinforcement learning to interact with a physics software pipeline, selecting symmetry groups, particle content, and representations while minimizing free parameters.

Result: The framework was validated in known theory spaces and applied to a novel symmetry group, demonstrating its effectiveness.

Conclusion: AMBer's approach can be extended beyond neutrino flavor theories to other model-building problems in physics.

Abstract: Particle physics theories, such as those which explain neutrino flavor
mixing, arise from a vast landscape of model-building possibilities. A model's
construction typically relies on the intuition of theorists. It also requires
considerable effort to identify appropriate symmetry groups, assign field
representations, and extract predictions for comparison with experimental data.
We develop an Autonomous Model Builder (AMBer), a framework in which a
reinforcement learning agent interacts with a streamlined physics software
pipeline to search these spaces efficiently. AMBer selects symmetry groups,
particle content, and group representation assignments to construct viable
models while minimizing the number of free parameters introduced. We validate
our approach in well-studied regions of theory space and extend the exploration
to a novel, previously unexamined symmetry group. While demonstrated in the
context of neutrino flavor theories, this approach of reinforcement learning
with physics software feedback may be extended to other theoretical
model-building problems in the future.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [52] [Superlinear Drift in Consensus-Based Optimization with Condensation Phenomena](https://arxiv.org/abs/2506.09001)
*Jonathan Franceschi,Lorenzo Pareschi,Mattia Zanella*

Main category: math.OC

TL;DR: A novel Consensus-Based Optimization (CBO) method inspired by the Kaniadakis-Quarati model is introduced, featuring superlinear drift and nonconstant diffusion, with mean-field analysis showing condensation-like phenomena. A marginal-based formulation avoids dimensionality issues, and numerical experiments demonstrate improved performance over classical CBO.


<details>
  <summary>Details</summary>
Motivation: To address limitations of classical CBO by developing a more robust method inspired by physics, ensuring global optimization performance and handling high-dimensional problems effectively.

Method: Develops a CBO method using SDEs with superlinear drift and nonconstant diffusion, analyzes mean-field behavior, and proposes a marginal-based formulation for scalability.

Result: The method exhibits condensation-like phenomena (e.g., finite-time blow-up) in 1D and shows improved performance in numerical experiments.

Conclusion: The novel CBO method offers theoretical and practical advantages over classical approaches, particularly in handling high-dimensional problems.

Abstract: Consensus-based optimization (CBO) is a class of metaheuristic algorithms
designed for global optimization problems. In the many-particle limit,
classical CBO dynamics can be rigorously connected to mean-field equations that
ensure convergence toward global minimizers under suitable conditions. In this
work, we draw inspiration from recent extensions of the Kaniadakis--Quarati
model for indistinguishable bosons to develop a novel CBO method governed by a
system of SDEs with superlinear drift and nonconstant diffusion. The resulting
mean-field formulation in one dimension exhibits condensation-like phenomena,
including finite-time blow-up and loss of $L^2$-regularity. To avoid the curse
of dimensionality a marginal based formulation which permits to leverage the
one-dimensional results to multiple dimensions is proposed. We support our
approach with numerical experiments that highlight both its consistency and
potential performance improvements compared to classical CBO methods.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [53] [Physics-Informed Neural Operators for Generalizable and Label-Free Inference of Temperature-Dependent Thermoelectric Properties](https://arxiv.org/abs/2506.08057)
*Hyeonbin Moon,Songho Lee,Wabi Demeke,Byungki Ryu,Seunghwa Ryu*

Main category: cond-mat.mtrl-sci

TL;DR: A physics-informed machine learning approach using PINN and PINO is developed for accurate and scalable thermoelectric property identification, enabling generalization across materials without retraining.


<details>
  <summary>Details</summary>
Motivation: Accurate characterization of thermoelectric properties (TEPs) is crucial for device modeling and design, but nonlinear temperature dependence and sparse measurements complicate forward simulation and inverse identification.

Method: The study employs physics-informed neural networks (PINN) for solving forward and inverse problems and neural operators (PINO) for generalization across materials, embedding transport equations into the loss function.

Result: The PINO model accurately infers TEPs from sparse field data without retraining, validated on 60 unseen materials after training on 20 p-type materials.

Conclusion: The framework provides a scalable, generalizable, and data-efficient solution for thermoelectric property identification, supporting high-throughput screening and inverse design.

Abstract: Accurate characterization of temperature-dependent thermoelectric properties
(TEPs), such as thermal conductivity and the Seebeck coefficient, is essential
for reliable modeling and efficient design of thermoelectric devices. However,
their nonlinear temperature dependence and coupled transport behavior make both
forward simulation and inverse identification difficult, particularly under
sparse measurement conditions. In this study, we develop a physics-informed
machine learning approach that employs physics-informed neural networks (PINN)
for solving forward and inverse problems in thermoelectric systems, and neural
operators (PINO) to enable generalization across diverse material systems. The
PINN enables field reconstruction and material property inference by embedding
governing transport equations into the loss function, while the PINO
generalizes this inference capability across diverse materials without
retraining. Trained on simulated data for 20 p-type materials and evaluated on
60 unseen materials, the PINO model demonstrates accurate and label-free
inference of TEPs using only sparse field data. The proposed framework offers a
scalable, generalizable, and data-efficient approach for thermoelectric
property identification, paving the way for high-throughput screening and
inverse design of advanced thermoelectric materials.

</details>


### [54] [AI-Assisted Rapid Crystal Structure Generation Towards a Target Local Environment](https://arxiv.org/abs/2506.08224)
*Osman Goni Ridwan,Sylvain Pitié,Monish Soundar Raj,Dong Dai,Gilles Frapper,Hongfei Xue,Qiang Zhu*

Main category: cond-mat.mtrl-sci

TL;DR: LEGO-xtal is a symmetry-informed AI generative method for crystal structure prediction, overcoming limitations of traditional and existing AI approaches by leveraging machine learning descriptors and generating diverse, low-energy structures efficiently.


<details>
  <summary>Details</summary>
Motivation: Traditional crystal structure prediction is computationally expensive, and existing AI models lack symmetry awareness and scalability. LEGO-xtal aims to address these gaps.

Method: Combines AI generative models trained on small datasets with machine learning structure descriptors for optimization, avoiding traditional energy-based methods.

Result: Generated over 1,700 low-energy sp2 carbon allotropes from 25 known ones, all within 0.5 eV/atom of graphite's ground-state energy.

Conclusion: LEGO-xtal provides a scalable, efficient framework for material design, applicable to modular materials like metal-organic frameworks and battery materials.

Abstract: In the field of material design, traditional crystal structure prediction
approaches require extensive structural sampling through computationally
expensive energy minimization methods using either force fields or quantum
mechanical simulations. While emerging artificial intelligence (AI) generative
models have shown great promise in generating realistic crystal structures more
rapidly, most existing models fail to account for the unique symmetries and
periodicity of crystalline materials, and they are limited to handling
structures with only a few tens of atoms per unit cell. Here, we present a
symmetry-informed AI generative approach called Local Environment
Geometry-Oriented Crystal Generator (LEGO-xtal) that overcomes these
limitations. Our method generates initial structures using AI models trained on
an augmented small dataset, and then optimizes them using machine learning
structure descriptors rather than traditional energy-based optimization. We
demonstrate the effectiveness of LEGO-xtal by expanding from 25 known
low-energy sp2 carbon allotropes to over 1,700, all within 0.5 eV/atom of the
ground-state energy of graphite. This framework offers a generalizable strategy
for the targeted design of materials with modular building blocks, such as
metal-organic frameworks and next-generation battery materials.

</details>


### [55] [Theory of Semi-Deterministic Quantum Dot Placement in Heteroepitaxy](https://arxiv.org/abs/2506.08432)
*Zihang Wang,Dirk Bouwmeester*

Main category: cond-mat.mtrl-sci

TL;DR: The paper demonstrates how engineering boundary geometry can enable deterministic placement of quantum dots (QDs) during epitaxial growth, overcoming stochastic nucleation challenges.


<details>
  <summary>Details</summary>
Motivation: Deterministic QD placement is crucial for high-quality single-photon sources and cQED systems but is hindered by stochastic nucleation processes.

Method: Theoretical demonstration of boundary geometry engineering to guide QD nucleation, leveraging adatom dynamics and many-body interactions.

Result: Primary QDs form along boundaries, reshaping the chemical potential landscape to enable secondary QDs in pristine regions.

Conclusion: This approach provides a theoretical basis for reliable QD patterning, promising advancements in quantum photonic devices.

Abstract: Achieving deterministic placement of self-assembled quantum dots (QDs) during
epitaxial growth is essential for the reliable and efficient fabrication of
high-quality single-photon sources and solid-state cavity quantum
electrodynamics (cQED) systems, yet it remains a significant challenge due to
the inherent stochasticity of QD nucleation processes. In this work, we
theoretically demonstrate that deterministic QD nucleation within a pristine
growth region can be achieved by engineering the boundary geometry of that
region. During epitaxial growth, adatoms initially move toward the boundary and
promote the formation of primary QDs along the boundary, driven by curvature
and diffusion anisotropy. The resulting primary QDs distribution will generate
many-body interactions that dynamically reshape the chemical potential
landscape for subsequently deposited adatoms, enabling the formation of
secondary QDs within the pristine growth region. These findings provide a
theoretical foundation for reliable patterning of high optical-quality QDs,
with potential applications in next-generation quantum photonic devices.

</details>


### [56] [Full ab initio atomistic approach for morphology prediction of hetero-integrated crystals: A confrontation with experiments](https://arxiv.org/abs/2506.08766)
*Sreejith Pallikkara Chandrasekharan,Sofia Apergi,Chen Wei,Federico Panciera,Laurent Travers,Gilles Patriarche,Jean-Christophe Harmand,Laurent Pedesseau,Charles Cornet*

Main category: cond-mat.mtrl-sci

TL;DR: A first-principle atomistic method predicts crystal shapes and wetting properties for heterogeneous integration, validated by GaP on Si experiments.


<details>
  <summary>Details</summary>
Motivation: To predict equilibrium shapes and wetting properties of crystals on dissimilar materials for optimizing hetero-structured devices.

Method: Uses density functional theory to calculate surface and interface energies, predicting morphology and wetting across chemical potentials.

Result: Predicted GaP crystal shapes on Si match experimental TEM observations.

Conclusion: The method aids in optimizing hetero-structured and smart materials.

Abstract: Here, we propose a comprehensive first-principle atomistic approach to
predict the Wulff-Kaischew equilibrium shape of crystals heterogeneously
integrated on a dissimilar material. This method uses both reconstructed
surface and interface absolute energies, as determined by density functional
theory, to infer the morphology and wetting properties of Volmer-Weber islands
over the whole range of accessible chemical potentials. The predicted
equilibrium shapes of GaP crystals heterogeneously grown on Si, are found to be
in good agreements with experimental observations performed by Transmission
Electron Microscopy. Such method provides a tool for optimization of
hetero-structured, multifunctional and smart materials and devices.

</details>


### [57] [A multi-physics model for dislocation driven spontaneous grain nucleation and microstructure evolution in polycrystals](https://arxiv.org/abs/2506.08843)
*Izzet Tarik Tandogan,Michael Budnitzki,Stefan Sandfeld*

Main category: cond-mat.mtrl-sci

TL;DR: A unified field theory integrates Cosserat crystal plasticity and orientation phase field to model spontaneous nucleation and grain boundary migration in metals during thermomechanical processing.


<details>
  <summary>Details</summary>
Motivation: Traditional models use staggered frameworks for deformation and microstructural kinetics, requiring ad hoc nucleation. This work aims for a thermodynamically consistent, unified approach.

Method: Combines Cosserat crystal plasticity with the Henry-Mellenthin-Plapp orientation phase field to simulate defects and grain boundary migration. Nucleation is driven by stored dislocations.

Result: Demonstrated in bicrystal and polycrystal simulations, capturing mechanisms like strain-induced boundary migration and subgrain growth.

Conclusion: The model provides a novel, unified framework for recrystallization simulation, enabling spontaneous nucleation and seamless grain boundary identification.

Abstract: The granular microstructure of metals evolves significantly during
thermomechanical processing through viscoplastic deformation and
recrystallization. Microstructural features such as grain boundaries (GBs),
subgrains, localized deformation bands, and non-uniform dislocation
distributions critically influence grain nucleation and growth during
recrystallization. Traditionally, modeling this coupled evolution involves
separate, specialized frameworks for mechanical deformation and microstructural
kinetics, typically used in a staggered manner. Nucleation is often introduced
ad hoc, with nuclei seeded at predefined sites based on criteria like critical
dislocation density, stress or strain. This is a consequence of the inherent
limitations of the staggered approach, where newly formed GBs or grains have to
be incorporated with additional processing. In this work, we propose a unified,
thermodynamically consistent field theory that enables spontaneous nucleation
driven by stored dislocations at GBs. The model integrates Cosserat crystal
plasticity with the Henry-Mellenthin-Plapp orientation phase field approach,
allowing the simulation of key microstructural defects, as well as curvature-
and stored energy-driven grain boundary migration. The unified approach enables
seamless identification of GBs that emerge from deformation and nucleation.
Nucleation is activated through a coupling function that links
dislocation-related free energy contributions to the phase field. Dislocation
recovery occurs both at newly formed nuclei and behind migrating GBs. The
model's capabilities are demonstrated using periodic bicrystal and polycrystal
simulations, where mechanisms such as strain-induced boundary migration,
subgrain growth, and coalescence are captured. The proposed spontaneous
nucleation mechanism offers a novel addition to the capabilities of phase field
models for recrystallization simulation.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [58] [Exploring the energy landscape of the Thomson problem: local minima and stationary states](https://arxiv.org/abs/2506.08398)
*Paolo Amore,Victor Figueroa,Enrique Diaz,Jorge A. López,Trevor Vincent*

Main category: cond-mat.soft

TL;DR: The paper investigates the Thomson problem's energy landscape for systems up to N=150, revealing exponential growth in distinct configurations and decaying energy gaps. A novel method reformulates the problem, enabling detailed exploration for N≤24.


<details>
  <summary>Details</summary>
Motivation: To understand the complexity and behavior of the Thomson problem's energy landscape, especially for larger systems.

Method: A numerical investigation and a novel approach reformulating the search for stationary points as a minimization problem using a designed potential.

Result: Exponential growth in distinct configurations and exponentially decaying energy gaps with N. Detailed exploration for N≤24 shows exponential growth in stationary states.

Conclusion: The Thomson problem's complexity increases dramatically with system size, and the novel method provides insights for similar systems.

Abstract: We conducted a comprehensive numerical investigation of the energy landscape
of the Thomson problem for systems up to $N=150$. Our results show the number
of distinct configurations grows exponentially with $N$, but significantly
faster than previously reported. Furthermore, we find that the average energy
gap between independent configurations at a given $N$ decays exponentially with
$N$, dramatically increasing the computational complexity for larger systems.
Finally, we developed a novel approach that reformulates the search for
stationary points in the Thomson problem (or similar systems) as an equivalent
minimization problem using a specifically designed potential. Leveraging this
method, we performed a detailed exploration of the solution landscape for
$N\leq24$ and estimated the growth of the number of stationary states to be
exponential in $N$.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [59] [KP-PINNs: Kernel Packet Accelerated Physics Informed Neural Networks](https://arxiv.org/abs/2506.08563)
*Siyuan Yang,Cheng Song,Zhilu Lai,Wenjia Wang*

Main category: cs.CE

TL;DR: The paper introduces KP-PINNs, a new framework for solving differential equations using neural networks, improving stability and accuracy over traditional PINNs.


<details>
  <summary>Details</summary>
Motivation: Traditional PINNs using L2 loss can be unstable for complex equations, prompting the need for a more robust solution.

Method: Proposes KP-PINNs, which uses RKHS norm for the loss function and Kernel Packet method for faster computation.

Result: KP-PINNs show stability across various equations and solve them effectively and efficiently.

Conclusion: KP-PINNs offer a promising improvement for PINNs-based solvers in scientific computing.

Abstract: Differential equations are involved in modeling many engineering problems.
Many efforts have been devoted to solving differential equations. Due to the
flexibility of neural networks, Physics Informed Neural Networks (PINNs) have
recently been proposed to solve complex differential equations and have
demonstrated superior performance in many applications. While the L2 loss
function is usually a default choice in PINNs, it has been shown that the
corresponding numerical solution is incorrect and unstable for some complex
equations. In this work, we propose a new PINNs framework named Kernel Packet
accelerated PINNs (KP-PINNs), which gives a new expression of the loss function
using the reproducing kernel Hilbert space (RKHS) norm and uses the Kernel
Packet (KP) method to accelerate the computation. Theoretical results show that
KP-PINNs can be stable across various differential equations. Numerical
experiments illustrate that KP-PINNs can solve differential equations
effectively and efficiently. This framework provides a promising direction for
improving the stability and accuracy of PINNs-based solvers in scientific
computing.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [60] [Driven phase-mixed Alfvén waves in a partially ionized solar plasma](https://arxiv.org/abs/2506.08732)
*Max McMurdo,Istvan Ballai,Gary Verth,Viktor Fedun*

Main category: astro-ph.SR

TL;DR: The paper explores phase mixing as a mechanism for Alfvén wave dissipation and heating in the solar atmosphere, focusing on the effects of plasma ionization and Alfvén speed gradients.


<details>
  <summary>Details</summary>
Motivation: To understand how phase mixing enhances Alfvén wave dissipation and heating in the solar atmosphere, particularly under varying plasma conditions and wave drivers.

Method: A single-fluid model of a partially ionized chromospheric plasma is used, with a pulse wave driver to study shear Alfvén wave damping via phase mixing.

Result: Dissipation length depends strongly on plasma ionization and Alfvén speed gradient. Pulse-driven waves show similar initial heating rates to continuous waves but decay slower due to algebraic damping.

Conclusion: The study highlights the interplay between wave drivers, damping mechanisms, and propagation dynamics in atmospheric heating, emphasizing the need for detailed modeling.

Abstract: Phase mixing has long been understood to be a viable mechanism for expediting
the dissipation of Alfv\'en wave energy resulting in the subsequent heating of
the solar atmosphere. To fulfil the conditions necessary for phase mixing to
occur, we consider the cross-field gradient in the Alfv\'en speed as a free
parameter in our model. Using a single-fluid description of a partially ionized
chromospheric plasma, we explore the efficiency of damping of shear Alfv\'en
waves subject to phase mixing when a pulse wave driver is employed. Our results
demonstrate a strong dependence of the dissipation length of shear Alfv\'en
waves on both the ionization degree of the plasma and the gradient of the
Alfv\'en speed. When assessing the efficiency of phase mixing across various
inhomogeneities, our findings indicate that waves originating from a pulse
driver exhibit initially identical heating rates as those generated by a
continuous wave driver. One key difference observed was that Alfv\'en pulses
possess a lower overall decay rate due to a change in damping profile from
exponential to algebraic. This discrepancy arises from the absence of a
consistent injection of energy into the base of the domain, that preserves
longitudinal gradients of the magnetic field perturbations more effectively.
These findings demonstrate the importance of understanding the relations
between the wave driver, damping mechanisms, and propagation dynamics in
resolving the atmospheric heating problem.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [61] [Mass inflation from rough initial data for the spherically symmetric Einstein-Maxwell-scalar field system with $Λ$](https://arxiv.org/abs/2506.08075)
*Flavio Rossetti*

Main category: gr-qc

TL;DR: The paper explores how rough initial data can trigger instability at the Cauchy horizon in black holes, countering smooth data results that suggest violations of the strong cosmic censorship conjecture.


<details>
  <summary>Details</summary>
Motivation: To investigate whether non-smooth initial data can suppress violations of the strong cosmic censorship conjecture, contrasting with smooth data results.

Method: Analyzes a characteristic initial value problem for the spherically symmetric Einstein-Maxwell-real scalar field system, focusing on rough initial data.

Result: The Hawking mass diverges at the Cauchy horizon for rough initial data, indicating instability and suggesting smooth data violations are non-generic.

Conclusion: Rough initial data can destabilize the Cauchy horizon, making smooth data results non-generic and supporting the strong cosmic censorship conjecture in certain cases.

Abstract: Recent rigorous results on black hole interiors clearly suggest that the
strong cosmic censorship conjecture fails in its most fundamental, i.e. weak,
formulation: violations are expected for a class of spherically symmetric
charged black holes in the presence of a positive cosmological constant near
extremality. These results require sufficiently regular solutions. Conversely,
when non-smooth, finite-energy initial data are prescribed for linear waves
propagating on a fixed black hole background belonging to the aforementioned
family, it was shown that the local energy of these linear waves blows up at
the Cauchy horizon, hence hinting that non-smooth initial data may suppress the
possible violations of the $H^1$ formulation of strong cosmic censorship. In
line with this intuition, we prove that rough initial data can also trigger an
instability at the Cauchy horizon in the non-linear setting, via mass
inflation. In particular, we analyse a characteristic initial value problem for
the spherically symmetric Einstein-Maxwell-real scalar field system describing
the interior of a black hole. Our results show that, when prescribing 1)
initial data asymptotically approaching those of a sub-extremal
Reissner-Nordstr\"om-de Sitter solution, and 2) initial data belonging to
$W^{1, 2}\setminus W^{1, q}$, for every $q > 2$, along the initial ingoing
compact segment; then the Hawking mass diverges at the Cauchy horizon of the
black hole solution we construct, for every parameter choice of the reference
black hole. In this larger family of configurations, we prove that the smooth
data suggesting violations of strong cosmic censorship are non-generic in a
``positive co-dimension'' sense, conditionally to the validity of the expected
Price law bounds. Moreover, we illustrate the transition between smooth and
rough initial data.

</details>
