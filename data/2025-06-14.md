<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 16]
- [math.AP](#math.AP) [Total: 13]
- [physics.comp-ph](#physics.comp-ph) [Total: 3]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 5]
- [math.DG](#math.DG) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [cs.CE](#cs.CE) [Total: 3]
- [astro-ph.EP](#astro-ph.EP) [Total: 1]
- [math.PR](#math.PR) [Total: 1]
- [cs.RO](#cs.RO) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [math.NT](#math.NT) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 3]
- [physics.atm-clus](#physics.atm-clus) [Total: 1]
- [math.OC](#math.OC) [Total: 3]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Data-driven balanced truncation for second-order systems with generalized proportional damping](https://arxiv.org/abs/2506.10118)
*Sean Reiter,Steffen W. R. Werner*

Main category: math.NA

TL;DR: A data-driven method for structured reduced-order modeling of second-order linear dynamical systems, generalizing balanced truncation techniques.


<details>
  <summary>Details</summary>
Motivation: To create efficient, low-dimensional models with meaningful structures for control system design, leveraging data-driven approaches.

Method: Reformulates position-velocity balanced truncation for second-order systems, generalizing quadrature-based balanced truncation. Infers damping coefficients via least-squares minimization.

Result: Effective surrogate models with generalized proportional damping structure, validated through numerical examples.

Conclusion: The method successfully extends balanced truncation to second-order systems, offering a practical data-driven solution for structured modeling.

Abstract: Structured reduced-order modeling is a central component in the
computer-aided design of control systems in which cheap-to-evaluate
low-dimensional models with physically meaningful internal structures are
computed. In this work, we develop a new approach for the structured
data-driven surrogate modeling of linear dynamical systems described by
second-order time derivatives via balanced truncation model-order reduction.
The proposed method is a data-driven reformulation of position-velocity
balanced truncation for second-order systems and generalizes the
quadrature-based balanced truncation for unstructured first-order systems to
the second-order case. The computed surrogates encode a generalized
proportional damping structure, and the damping coefficients are inferred
solely from data by minimizing a least-squares error over the coefficients.
Several numerical examples demonstrate the effectiveness of the proposed
method.

</details>


### [2] [R-PINN: Recovery-type a-posteriori estimator enhanced adaptive PINN](https://arxiv.org/abs/2506.10243)
*Rongxin Lu,Jiwei Jia,Young Ju Lee,Zheng Lu,Chensong Zhang*

Main category: math.NA

TL;DR: The paper proposes R-PINN, an adaptive PINN algorithm, to improve accuracy for PDEs with large local gradients by adjusting collocation points based on error estimates, outperforming FI-PINN.


<details>
  <summary>Details</summary>
Motivation: Standard PINNs perform poorly for PDEs with large local gradients, leading to localized errors.

Method: Introduces R-PINN, combining recovery-type a-posteriori error estimators from adaptive FEM with PINNs to adaptively adjust collocation points.

Result: R-PINN converges faster with fewer points and outperforms FI-PINN in cases with multiple error regions.

Conclusion: R-PINN is a hybrid approach integrating adaptive FEM with PINNs, offering superior performance for challenging PDEs.

Abstract: In recent years, with the advancements in machine learning and neural
networks, algorithms using physics-informed neural networks (PINNs) to solve
PDEs have gained widespread applications. While these algorithms are
well-suited for a wide range of equations, they often exhibit suboptimal
performance when applied to equations with large local gradients, resulting in
substantial localized errors. To address this issue, this paper proposes an
adaptive PINN algorithm designed to improve accuracy in such cases. The core
idea of the algorithm is to adaptively adjust the distribution of collocation
points based on the recovery-type a-posterior error of the current numerical
solution, enabling a better approximation of the true solution. This approach
is inspired by the adaptive finite element method. By combining the
recovery-type a-posteriori estimator, a gradient-recovery estimator commonly
used in the adaptive finite element method (FEM) with PINNs, we introduce the
Recovery-type a-posteriori estimator enhanced adaptive PINN (R-PINN) and
compare its performance with a typical adaptive PINN algorithm, FI-PINN. Our
results demonstrate that R-PINN achieves faster convergence with fewer adaptive
points and significantly outperforms in the cases with multiple regions of
large errors than FI-PINN. Notably, our method is a hybrid numerical approach
for solving partial differential equations, integrating adaptive FEM with
PINNs.

</details>


### [3] [Enhanced randomized Douglas-Rachford method: Improved probabilities and adaptive momentum](https://arxiv.org/abs/2506.10261)
*Liqi Guo,Ruike Xiang,Deren Han,Jiaxin Xie*

Main category: math.NA

TL;DR: The paper enhances the randomized Douglas-Rachford (RDR) method with improved sampling strategies and adaptive momentum, achieving stronger convergence guarantees and practical performance gains.


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency and convergence of randomized iterative methods for solving large-scale linear systems, particularly the RDR method.

Method: Incorporates without-replacement and volume sampling into RDR, and introduces an adaptive heavy-ball momentum scheme with dynamic parameter adjustment.

Result: The enhanced RDR method achieves linear convergence in expectation with improved bounds and outperforms the original in numerical experiments.

Conclusion: The proposed enhancements to RDR offer significant theoretical and practical improvements for solving linear systems.

Abstract: Randomized iterative methods have gained recent interest in machine learning
and signal processing for solving large-scale linear systems. One such example
is the randomized Douglas-Rachford (RDR) method, which updates the iterate by
reflecting it through two randomly selected hyperplanes and taking a convex
combination with the current point. In this work, we enhance RDR by introducing
improved sampling strategies and an adaptive heavy-ball momentum scheme.
Specifically, we incorporate without-replacement and volume sampling into RDR,
and establish stronger convergence guarantees compared to conventional i.i.d.
sampling. Furthermore, we develop an adaptive momentum mechanism that
dynamically adjusts step sizes and momentum parameters based on previous
iterates, and prove that the resulting method achieves linear convergence in
expectation with improved convergence bounds. Numerical experiments demonstrate
that the enhanced RDR method consistently outperforms the original version,
providing substantial practical benefits across a range of problem settings.

</details>


### [4] [Complex scaling for open waveguides](https://arxiv.org/abs/2506.10263)
*Charles L. Epstein,Tristan Goodwill,Jeremy Hoskins,Solomon Quinn,Manas Rachh*

Main category: math.NA

TL;DR: The paper analyzes the complex scaling method for time-harmonic scalar wave propagation in leaky dielectric waveguide junctions, showing kernels decay rapidly and solutions admit analytic continuation, enabling efficient numerical discretization.


<details>
  <summary>Details</summary>
Motivation: To address the problem of wave propagation in open dielectric waveguides by reducing it to a system of Fredholm integral equations and analyzing their properties for practical numerical solutions.

Method: The study involves analytic continuation of kernels and solutions, deformation of integral equations to suitable contours, and numerical discretization with exponential error decay.

Result: Kernels and solutions exhibit rapid decay and analytic continuation, allowing efficient numerical treatment with exponentially decaying truncation error.

Conclusion: The method provides a robust framework for solving wave propagation problems in leaky waveguides, validated by numerical examples.

Abstract: In this work we analyze the complex scaling method applied to the problem of
time-harmonic scalar wave propagation in junctions between `leaky,' or open
dielectric waveguides. In [arXiv:2302.04353, arXiv:2310.05816,
arXiv:2401.04674, arXiv:2411.11204], it was shown that under suitable
assumptions the problem can be reduced to a system of Fredholm second-kind
integral equations on an infinite interface, transverse to the waveguides.
Here, we show that the kernels appearing in the integral equation admit a
rapidly decaying analytic continuation on certain natural totally real
submanifolds of $\mathbb{C}^2.$ We then show that for suitable,
physically-meaningful, boundary data the resulting solutions to the integral
equations themselves admit analytic continuation and satisfy related asymptotic
estimates. By deforming the integral equation to a suitable contour, the decay
in the kernels, density, and data enable straightforward discretization and
truncation, with an error that decays exponentially in the truncation length.
We illustrate our results with several representative numerical examples.

</details>


### [5] [Penalty-Based Feedback Control and Finite Element Analysis for the Stabilization of Nonlinear Reaction-Diffusion Equations](https://arxiv.org/abs/2506.10428)
*Sudeep Kundu,Shishu pal Singh*

Main category: math.NA

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this work, first we employ the penalization technique to analyze the
Dirichlet boundary feedback control problem pertaining to reaction-diffusion
equation. We establish the stabilization result of the equivalent Robin problem
in the \(H^{2}\)-norm with respect to the penalty parameter. Furthermore, we
prove that the solution of the penalized control problem converges to the
corresponding solution of the Dirichlet boundary feedback control problem as
the penalty parameter \(\epsilon\) approaches zero. A \(C^{0}\)-conforming
finite element method is applied to this problem for the spatial variable while
keeping the time variable continuous. We discuss the stabilization of the
semi-discrete scheme for the penalized control problem and present an error
analysis of its solution. Finally, we validate our theoretical findings through
numerical experiments.

</details>


### [6] [Stability analysis of the free-surface Stokes problem and an unconditionally stable explicit scheme](https://arxiv.org/abs/2506.10447)
*Igor Tominec,Lukas Lundgren,André Löfgren,Josefin Ahlkrona*

Main category: math.NA

TL;DR: The paper analyzes stability and conservation properties of the Stokes/free-surface system for viscous flows, proposing a stabilization term for explicit Euler discretization to ensure stability and volume conservation.


<details>
  <summary>Details</summary>
Motivation: To address the need for accurate simulations of highly viscous free-surface flows like ice sheet dynamics and mantle convection by analyzing the stability and conservation properties of the coupled Stokes/free-surface system.

Method: Theoretical analysis of the weak form of the system for Newtonian and non-Newtonian fluids at continuous and discrete levels, with fully discrete stability analysis for finite element methods and explicit/implicit Euler time-stepping. A stabilization term for explicit Euler is proposed.

Result: The proposed stabilization term ensures unconditional time stability and domain volume conservation, validated by numerical experiments.

Conclusion: The study provides theoretical and numerical validation for the stabilization term, enhancing the stability and conservation properties of simulations for highly viscous free-surface flows.

Abstract: Accurate simulations of ice sheet dynamics, mantle convection, lava flow, and
other highly viscous free-surface flows involve solving the coupled
Stokes/free-surface equations. In this paper, we theoretically analyze the
stability and conservation properties of the weak form of this system for
Newtonian fluids and non-Newtonian fluids, at both the continuous and discrete
levels. We perform the fully discrete stability analysis for finite element
methods used in space with explicit and implicit Euler time-stepping methods
used in time. Motivated by the theory, we propose a stabilization term designed
for the explicit Euler discretization, which ensures unconditional time
stability and permits conservation of the domain volume. Numerical experiments
validate and support our theoretical findings.

</details>


### [7] [Convergence of adaptive boundary element methods driven by functional a posteriori error estimates](https://arxiv.org/abs/2506.10499)
*Alexander Freiszlinger,Dirk Pauly,Dirk Praetorius*

Main category: math.NA

TL;DR: Functional a posteriori error estimates for BEMs are proposed, covering Galerkin and collocation methods, focusing on potential approximation errors in the domain rather than boundary density errors. Adaptive mesh-refinement on auxiliary strip domains ensures convergence of potential error to zero for Galerkin BEM.


<details>
  <summary>Details</summary>
Motivation: To address the practical relevance of potential approximation errors in the domain, rather than boundary density errors, and to provide a unified approach for Galerkin and collocation BEM.

Method: Functional error estimates are derived by solving auxiliary problems on adaptive strip domains along the boundary. Adaptive mesh-refinement is applied to these strips.

Result: The proposed adaptive mesh-refinement algorithm ensures convergence of the potential error to zero for Galerkin BEM.

Conclusion: The functional error estimators and adaptive strategy offer a practical and unified approach for BEM error control, with proven convergence for Galerkin methods.

Abstract: The recent work [Kurz et al., Numer. Math., 147 (2021)] proposed functional a
posteriori error estimates for boundary element methods (BEMs) together with a
related adaptive mesh-refinement strategy. Unlike most a posteriori BEM error
estimators, the proposed functional error estimators cover Galerkin as well as
collocation BEM and, more importantly, do not control the error in the integral
density on the boundary, but the error of the potential approximation in the
domain, which is of greater relevance in practice. The estimates rely on the
numerical solution of auxiliary problems on auxiliary strip domains along the
boundary, where the strips are affected by the adaptive mesh-refinement and
hence vary. For Galerkin BEM, we prove that the proposed adaptive
mesh-refinement algorithm yields convergence of the potential error to zero.
Due to the structural difference to residual-based estimators, the proof
requires new ideas.

</details>


### [8] [A semi-Lagrangian scheme for First-Order Mean Field Games based on monotone operators](https://arxiv.org/abs/2506.10509)
*Elisabetta Carlini,Valentina Coscetti*

Main category: math.NA

TL;DR: A semi-Lagrangian scheme for non-local Mean Field Games is developed, with convergence proven and a Learning Value Algorithm implemented. An acceleration strategy improves performance.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of solving first-order, time-dependent, non-local Mean Field Games efficiently.

Method: A semi-Lagrangian scheme is constructed, and a Learning Value Algorithm is implemented with a Policy iteration-based acceleration strategy.

Result: The scheme converges to a weak solution, and the accelerated version significantly enhances performance.

Conclusion: The proposed schemes are effective, with the accelerated version offering notable performance improvements.

Abstract: We construct a semi-Lagrangian scheme for first-order, time-dependent, and
non-local Mean Field Games. The convergence of the scheme to a weak solution of
the system is analyzed by exploiting a key monotonicity property. To solve the
resulting discrete problem, we implement a Learning Value Algorithm, prove its
convergence, and propose an acceleration strategy based on a Policy iteration
method. Finally, we present numerical experiments that validate the
effectiveness of the proposed schemes and show that the accelerated version
significantly improves performance.

</details>


### [9] [Non-augmented velocity-vorticity-pressure formulation for the Navier--Stokes--Brinkman--Forchheimer problem](https://arxiv.org/abs/2506.10533)
*Santiago Badia,Carsten Carstensen,Alberto F. Martin,Ricardo Ruiz-Baier,Segundo Villa-Fuentes*

Main category: math.NA

TL;DR: The paper addresses the flow of incompressible fluid in porous media, solving a double saddle-point problem using Crouzeix--Raviart finite elements. It provides existence proofs, error estimates, and adaptive algorithms for efficient simulations.


<details>
  <summary>Details</summary>
Motivation: To tackle the complex Navier--Stokes--Brinkman--Forchheimer equations in porous media, ensuring accurate and efficient solutions for small sources.

Method: Uses lowest-order piecewise divergence-free Crouzeix--Raviart finite elements, penalisation terms for velocity jumps, and Raviart--Thomas interpolants for pressure-robust error estimates.

Result: Establishes existence of solutions, provides a priori and a posteriori error estimates, and demonstrates improved convergence with adaptive mesh-refining.

Conclusion: The approach is robust, efficient, and scalable, validated by numerical simulations.

Abstract: The flow of incompressible fluid in highly permeable porous media in
vorticity - velocity - Bernoulli pressure form leads to a double saddle-point
problem in the Navier--Stokes--Brinkman--Forchheimer equations. The paper
establishes, for small sources, the existence of solutions on the continuous
and discrete level of lowest-order piecewise divergence-free Crouzeix--Raviart
finite elements. The vorticity employs a vector version of the pressure space
with normal and tangential velocity jump penalisation terms. A simple
Raviart--Thomas interpolant leads to pressure-robust a priori error estimates.
An explicit residual-based a posteriori error estimate allows for efficient and
reliable a posteriori error control. The efficiency for the Forchheimer
nonlinearity requires a novel discrete inequality of independent interest. The
implementation is based upon a light-weight forest-of-trees data structure
handled by a highly parallel set of adaptive {mesh refining} algorithms.
Numerical simulations reveal robustness of the a posteriori error estimates and
improved convergence rates by adaptive mesh-refining.

</details>


### [10] [Structure and asymptotic preserving deep neural surrogates for uncertainty quantification in multiscale kinetic equations](https://arxiv.org/abs/2506.10636)
*Wei Chen,Giacomo Dimarco,Lorenzo Pareschi*

Main category: math.NA

TL;DR: The paper introduces a multiscale control variates strategy and surrogate models (SAPNNs) to improve Monte Carlo sampling efficiency for high-dimensional kinetic equations with stochastic parameters, achieving better accuracy and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional Monte Carlo methods for uncertainty quantification in kinetic equations suffer from slow convergence and high variance, especially in high-dimensional parameter spaces.

Method: The approach combines multiscale control variates with low-fidelity solutions and introduces SAPNNs, designed to preserve physical properties, trained on low-fidelity models and enriched with high-fidelity samples.

Result: The method significantly reduces variance while maintaining physical consistency and asymptotic accuracy, outperforming standard Monte Carlo techniques.

Conclusion: The proposed methodology enables efficient large-scale prediction in kinetic uncertainty quantification, validated across various regimes.

Abstract: The high dimensionality of kinetic equations with stochastic parameters poses
major computational challenges for uncertainty quantification (UQ). Traditional
Monte Carlo (MC) sampling methods, while widely used, suffer from slow
convergence and high variance, which become increasingly severe as the
dimensionality of the parameter space grows. To accelerate MC sampling, we
adopt a multiscale control variates strategy that leverages low-fidelity
solutions from simplified kinetic models to reduce variance. To further improve
sampling efficiency and preserve the underlying physics, we introduce surrogate
models based on structure and asymptotic preserving neural networks (SAPNNs).
These deep neural networks are specifically designed to satisfy key physical
properties, including positivity, conservation laws, entropy dissipation, and
asymptotic limits. By training the SAPNNs on low-fidelity models and enriching
them with selected high-fidelity samples from the full Boltzmann equation, our
method achieves significant variance reduction while maintaining physical
consistency and asymptotic accuracy. The proposed methodology enables efficient
large-scale prediction in kinetic UQ and is validated across both homogeneous
and nonhomogeneous multiscale regimes. Numerical results demonstrate improved
accuracy and computational efficiency compared to standard MC techniques.

</details>


### [11] [Alternating steepest descent methods for tensor completion with applications to spectromicroscopy](https://arxiv.org/abs/2506.10661)
*Oliver Townsend,Sergey Dolgov,Silvia Gazzola,Misha Kilmer*

Main category: math.NA

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this paper we develop two new Tensor Alternating Steepest Descent
algorithms for tensor completion in the low-rank $\star_{M}$-product format,
whereby we aim to reconstruct an entire low-rank tensor from a small number of
measurements thereof. Both algorithms are rooted in the Alternating Steepest
Descent (ASD) method for matrix completion, first proposed in [J. Tanner and K.
Wei, Appl. Comput. Harmon. Anal., 40 (2016), pp. 417-429]. In deriving the new
methods we target the X-ray spectromicroscopy undersampling problem, whereby
data are collected by scanning a specimen on a rectangular viewpoint with X-ray
beams of different energies. The recorded absorptions coefficients of the mixed
specimen materials are naturally stored in a third-order tensor, with spatial
horizontal and vertical axes, and an energy axis. To speed the X-ray
spectromicroscopy measurement process up, only a fraction of tubes from (a
reshaped version of) this tensor are fully scanned, leading to a tensor
completion problem. In this framework we can apply any transform (such as the
Fourier transform) to the tensor tube by tube, providing a natural way to work
with the $\star_{M}$-tensor algebra, and propose: (1) a tensor completion
algorithm that is essentially ASD reformulated in the $\star_{M}$-induced
metric space and (2) a tensor completion algorithm that solves a set of
(readily parallelizable) independent matrix completion problems for the frontal
slices of the transformed tensor. The two new methods are tested on real X-ray
spectromicroscopy data, demonstrating that they achieve the same reconstruction
error with fewer samples from the tensor compared to the matrix completion
algorithms applied to a flattened tensor.

</details>


### [12] [Semi-discrete moduli of smoothness and their applications in one- and two- sided error estimates](https://arxiv.org/abs/2506.10723)
*Danilo Costarelli,Donato Lavella*

Main category: math.NA

TL;DR: The paper introduces a new semi-discrete modulus of smoothness, generalizing prior work, and provides broad error estimates for non-trigonometric operators, including sharper results than classical methods.


<details>
  <summary>Details</summary>
Motivation: To extend and generalize the definition of semi-discrete moduli of smoothness beyond trigonometric operators, enabling broader applications and sharper estimates.

Method: Uses regularization and approximation properties of Steklov integrals, avoiding reliance on trigonometric best approximation results. Introduces a new K-functional and proves its equivalence to the semi-discrete modulus.

Result: Establishes one- and two-sided error estimates for various operators (e.g., Bernstein polynomials, Shannon sampling series) and introduces a Rathore-type theorem.

Conclusion: The new modulus of smoothness and K-functional offer versatile tools for approximation theory, with potential for further research in algebraic Lagrange approximation.

Abstract: In this paper, we introduce a new semi-discrete modulus of smoothness, which
generalizes the definition given by Kolomoitsev and Lomako (KL) in 2023 (in the
paper published in the J. Approx. Theory), and we establish very general one-
and two- sided error estimates under non-restrictive assumptions. The proposed
results have been proved exploiting the regularization and approximation
properties of certain Steklov integrals introduced by Sendov and Popov in 1983,
and differ from the ones given by Kolomoitsev and Lomako. In addition, the
proof of the original KL approximation theorems were strictly related to the
application of certain classical results of the trigonometric best
approximation, and thus, they are applicable only for operators of the
trigonometric type. By the definition of semi-discrete moduli of smoothness
here proposed, we are able to deduce applications also for operators that are
not necessarily of the trigonometric type, and can also be used to derive
sharper estimates than those that can be achieved by the classical averaged
moduli of smoothness ($\tau$-moduli). Furthermore, a Rathore-type theorem is
established, and a new notion of K-functional is also introduced showing its
equivalence with the semi-discrete modulus of smoothness and its realization.
One-sided estimates of approximation can be established for classical operators
on bounded domains, such as the Bernstein polynomials. In the case of
approximation operators on the whole real line, one-sided estimates can be
achieved, e.g., for the Shannon sampling (cardinal) series, as well as for the
so-called generalized sampling operators. At the end of the paper, the case of
algebraic Lagrange approximation has been considered, showing the main open
problems in order to derive two-sided error estimates in this noteworthy case.

</details>


### [13] [Reduced-Order Time Splitting for Navier-Stokes with Open Boundaries](https://arxiv.org/abs/2506.10763)
*Mejdi Azaïez,Tomás Chacón Rebollo,Carlos Núñez Fernández,Samuele Rubino*

Main category: math.NA

TL;DR: The paper proposes a POD-ROM for solving Navier-Stokes equations with open boundary conditions, combining time-splitting, domain reduction, and reduced order modeling. It compares intrusive and hybrid ROMs using numerical tests.


<details>
  <summary>Details</summary>
Motivation: To reduce computational time for solving Navier-Stokes equations by integrating time-splitting, domain reduction, and reduced order modeling.

Method: Combines time-splitting, non-standard boundary conditions, and POD-ROM with Galerkin projection. Compares intrusive and hybrid (intrusive + data-driven) ROMs.

Result: Numerical tests (bifurcated tube flow and cylinder flow) demonstrate efficiency and accuracy of the proposed ROMs.

Conclusion: The hybrid POD-ROM shows promise in balancing accuracy and computational efficiency for Navier-Stokes simulations.

Abstract: In this work, we propose a Proper Orthogonal Decomposition-Reduced Order
Model (POD-ROM) applied to time-splitting schemes for solving the Navier-Stokes
equations with open boundary conditions. In this method, we combine three
strategies to reduce the computing time to solve NSE: time splitting, reduction
of the computational domain through non-standard treatment of open boundary
conditions and reduced order modelling. To make the work self-contained, we
first present the formulation of the time-splitting scheme applied to the
Navier-Stokes equations with open boundary conditions, employing a first-order
Euler time discretization and deriving the non-standard boundary condition for
pressure. Then, we construct a Galerkin projection-based ROM using POD with two
different treatments of the pressure boundary condition on the outlet. We
propose a comparative performance analysis between the standard
projection-based POD-ROM (fully intrusive) and a hybrid POD-ROM that combines a
projection-based approach (intrusive) with a data-driven technique
(non-intrusive) using Radial Basis Functions (RBF). We illustrate this
comparison through two different numerical tests: the flow in a bifurcated tube
and the benchmark numerical test of the flow past cylinder, numerically
investigating the efficiency and accuracy of both ROMs.

</details>


### [14] [A Combined Parallel-in-time Direct Inverse (ParaDIn)-Parareal Method for Nonlinear Differential Equations](https://arxiv.org/abs/2506.10820)
*Subhash Paudel,Nail K. Yamaleev*

Main category: math.NA

TL;DR: The paper combines the ParaDIn method with the Parareal algorithm to parallelize time derivatives in nonlinear PDEs, achieving a speedup of 124 on 480 cores.


<details>
  <summary>Details</summary>
Motivation: The ParaDIn method has constraints on parallel time levels, limiting speedup. Combining it with Parareal aims to overcome this.

Method: Uses a block-Jacobi preconditioner with ParaDIn for each block, accelerated by Parareal (a two-level multigrid in time). Both coarse- and fine-grid propagators are parallelized.

Result: Achieves a speedup of 124 on 480 cores compared to sequential BDF1 for 2-D nonlinear heat and Burgers equations.

Conclusion: The combined ParaDIn-Parareal method significantly improves parallel performance for solving nonlinear PDEs.

Abstract: As has been shown in our previous work, the parallel-in-time direct inverse
(ParaDIn) method introduced by Yamaleev and Paudel in (arXiv: 2406.00878v1,
2024) imposes some constraint on the maximum number of time levels, $N_t$, that
can be integrated in parallel. To circumvent this problem and further increase
the speedup, we combine the ParaDIn method with the Parareal algorithm to
efficiently parallelize the first-order time derivative term in nonlinear
partial differential equations discretized by the method of lines. The main
idea of the proposed approach is to use a block-Jacobi preconditioner, so that
each block is solved by using the ParaDIn method. To accelerate the convergence
of Jacobi iterations, we use the Parareal method which can be interpreted as a
two-level multigrid method in time. In contrast to the conventional Parareal
algorithm whose coarse grid correction step is performed sequentially, both the
coarse- and fine-grid propagators in the proposed approach are implemented in
parallel by using the ParaDIn method, thus significantly increasing the
parallel performance of the combined algorithm. Numerical results show that the
new combined ParaDIn-Parareal method provides the speedup of up to 124 on 480
computing cores as compared with the sequential first-order implicit backward
difference (BDF1) scheme for the 2-D nonlinear heat and Burgers equations with
both smooth and discontinuous solutions.

</details>


### [15] [Numerical approximation of a PDE-constrained Optimization problem that appears in Data-Driven Computational Mechanics](https://arxiv.org/abs/2506.10894)
*Pedro B. Bazon,Cristian G. Gebhardt,Gustavo C. Buscaglia,Roberto F. Ausas*

Main category: math.NA

TL;DR: The paper addresses an optimization problem in Data-Driven Computational Mechanics, focusing on continuous primal fields for diffusion-reaction problems, ensuring physical and geometrical constraints. It proves well-posedness, proposes stable finite element methods, and validates with numerical examples.


<details>
  <summary>Details</summary>
Motivation: To solve optimization problems in Data-Driven Computational Mechanics by finding continuous primal fields close to predefined discrete data, while adhering to physical and geometrical constraints.

Method: Establishes well-posedness in continuous setting, proposes stable finite element discretizations preserving saddle-point structure, and uses equal-order interpolation for all fields.

Result: Demonstrates effectiveness through numerical examples, showing the proposed methods work well in practice.

Conclusion: The study successfully addresses the optimization problem, providing a robust framework for Data-Driven Computational Mechanics with validated numerical results.

Abstract: We investigate an optimization problem that arises when working within the
paradigm of Data-Driven Computational Mechanics. In the context of the
diffusion-reaction problem, such an optimization problem seeks for the
continuous primal fields (gradient and flux) that are closest to some
predefined discrete fields taken from a material data set. The optimization is
performed over primal fields that satisfy the physical conservation law and the
geometrical compatibility. We consider a reaction term in the conservation law,
which has the effect of coupling all the optimality conditions. We first
establish the well-posedness in the continuous setting. Then, we propose stable
finite element discretizations that consistently approximate the continuous
formulation, preserving its saddle-point structure and allowing for equal-order
interpolation of all fields. Finally, we demonstrate the effectiveness of the
proposed methods through a set of numerical examples.

</details>


### [16] [Accelerating Newton-Schulz Iteration for Orthogonalization via Chebyshev-type Polynomials](https://arxiv.org/abs/2506.10935)
*Ekaterina Grishina,Matvey Smirnov,Maxim Rakhuba*

Main category: math.NA

TL;DR: The paper introduces Chebyshev-optimized Newton-Schulz (CANS), a method to improve orthogonal matrix approximation by optimizing coefficients using Chebyshev's theorem and Remez algorithm.


<details>
  <summary>Details</summary>
Motivation: Existing Newton-Schulz iteration is efficient but uses fixed coefficients, limiting its adaptability. The paper aims to optimize these coefficients for better performance.

Method: The authors derive optimal coefficients for the 3rd-order Newton-Schulz iteration using Chebyshev's alternance theorem and extend it to higher-degree polynomials via the Remez algorithm.

Result: CANS improves orthogonalization in applications like the Muon optimizer and Riemannian optimization on the Stiefel manifold.

Conclusion: The proposed CANS method offers a more efficient and adaptable solution for orthogonal matrix approximation, with practical benefits in machine learning.

Abstract: The problem of computing optimal orthogonal approximation to a given matrix
has attracted growing interest in machine learning. Notable applications
include the recent Muon optimizer or Riemannian optimization on the Stiefel
manifold. Among existing approaches, the Newton-Schulz iteration has emerged as
a particularly effective solution, as it relies solely on matrix
multiplications and thus achieves high computational efficiency on GPU
hardware. Despite its efficiency, the method has inherent limitations - its
coefficients are fixed and thus not optimized for a given matrix. In this paper
we address this issue by proposing a Chebyshev-optimized version of
Newton-Schulz (CANS). Based on the Chebyshev's alternance theorem, we
theoretically derive optimal coefficients for the 3-rd order Newton-Schulz
iteration and apply a Remez algorithm to compute optimal higher-degree
polynomials. We leverage these polynomials to construct controlled approximate
orthogonalization schemes, which is of interest in deep learning applications.
Practically, we demonstrate the method's effectiveness in two key applications:
orthogonalization in the Muon optimizer, and providing an efficient retraction
alternative for Riemannian optimization on the Stiefel manifold.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [17] [Wave-front tracking for a quasi-linear scalar conservation law with hysteresis II: the case of Preisach](https://arxiv.org/abs/2506.10087)
*Fabio Bagagiolo,Stefan Moreti*

Main category: math.AP

TL;DR: The paper extends the analysis of a quasi-linear scalar conservation law with hysteresis from the Play operator to the more versatile Preisach operator, focusing on the Cauchy problem and employing wave-front tracking for solutions.


<details>
  <summary>Details</summary>
Motivation: The study aims to generalize previous work on hysteresis in conservation laws by using the Preisach operator, which better models real-world applications due to its internal variables.

Method: The authors analyze the Riemann problem and apply the wave-front tracking method to solve the Cauchy problem for bounded variation initial data, supplemented by an entropy-like condition for uniqueness.

Result: The paper provides a framework for solving the Cauchy problem with the Preisach hysteresis operator, ensuring uniqueness through an entropy condition.

Conclusion: The extension to the Preisach operator enhances the applicability of hysteresis models in conservation laws, with rigorous analysis and solution methods.

Abstract: We consider the Cauchy problem for the quasi-linear scalar conservation law
\[u_t+\mathcal{F}(u)_t+u_x=0,\] where $\mathcal{F}$ is a specific hysteresis
operator. Hysteresis models a rate-independent memory relationship between the
input $u$ and its output, giving a non-local feature to the equation. In a
previous work the authors studied the case when $\mathcal{F}$ is the Play
operator. In the present article, we extend the analysis to the case of
Preisach operator, which is probably the most versatile mathematical model to
the describe hysteresis in the applications, especially for the presence of
some kind of internal variables. This fact has required a new analysis of the
equation. Starting from the Riemann problem, we address the so-called
wave-front tracking method for a solution to the Cauchy problem with bounded
variation initial data. An entropy-like condition is also exploited for
uniqueness.

</details>


### [18] [Well--posedness for the biharmonic scattering problem for a penetrable obstacle](https://arxiv.org/abs/2506.10176)
*Rafael Ceja Ayala,Isaac Harris,Tonatiuh Sánchez-Vizuet*

Main category: math.AP

TL;DR: The paper analyzes the scattering problem for a penetrable obstacle in a 2D elastic plate using the biharmonic wave equation, coupling Helmholtz and modified Helmholtz equations, and validates results numerically.


<details>
  <summary>Details</summary>
Motivation: To understand wave propagation in thin elastic plates and solve the scattering problem for penetrable obstacles.

Method: Uses the biharmonic wave equation in the frequency domain, coupled with Helmholtz and modified Helmholtz equations, and employs operator factorization for analysis.

Result: Establishes well-posedness and reciprocity relations, supported by numerical examples.

Conclusion: The theoretical framework is validated, providing insights into wave scattering in elastic plates.

Abstract: We address the direct scattering problem for a penetrable obstacle in an
infinite elastic two--dimensional Kirchhoff--Love plate. Under the assumption
that the plate's thickness is small relative to the wavelength of the incident
wave, the propagation of perturbations on the plate is governed by the
two-dimensional biharmonic wave equation, which we study in the frequency
domain. With the help of an operator factorization, the scattering problem is
analyzed from the perspective of a coupled boundary value problem involving the
Helmholtz and modified Helmholtz equations. Well--posedness and reciprocity
relations for the problem are established. Numerical examples for some special
cases are provided to validate the theoretical findings.

</details>


### [19] [Optimal decay of global strong solutions to nematic liquid crystal flows in the half-space](https://arxiv.org/abs/2506.10255)
*Haokun Chen,Yong Wang*

Main category: math.AP

TL;DR: The paper analyzes the decay rates of higher-order spatial and first-order time derivatives for nematic liquid crystal flows in a half-space, showing faster decay than the heat kernel under certain initial conditions.


<details>
  <summary>Details</summary>
Motivation: To understand the asymptotic behavior of derivatives in nematic liquid crystal flows and establish decay rates under weighted Sobolev space conditions.

Method: Uses $L^p-L^q$ estimates of the Stokes semigroup, a priori estimates of the steady Stokes system, and the Leray projection operator's representation formula.

Result: Demonstrates faster decay rates than the heat kernel for the derivatives when initial data lie in a weighted Sobolev space.

Conclusion: The study provides insights into the decay behavior of derivatives in nematic liquid crystal flows, leveraging advanced analytical tools.

Abstract: We study asymptotic behaviors of the higher-order spatial derivatives and the
first-order time derivatives for the strong solution to nematic liquid crystal
flows in the half-space $\mathbb{R}_+^3$. Furthermore, when the initial data
lie in an appropriately weighted Sobolev space, we obtain the decay rates that
are faster than the heat kernel. The main tools employed in this paper are the
$L^p-L^q$ estimates of the Stokes semigroup, the a priori estimates of the
steady Stokes system in $\mathbb{R}_+^3$, and the representation formula of the
Leray projection operator.

</details>


### [20] [Generalized Poisson kernel and solution of the Dirichlet problem for the radial Schrödinger equation](https://arxiv.org/abs/2506.10273)
*Víctor A Vicente-Benítez*

Main category: math.AP

TL;DR: The paper provides an explicit solution to the Dirichlet boundary value problem for the radial Schrödinger equation in a unit ball with a complex-valued potential, using orthogonal solutions and formal spherical polynomials. It also introduces a generalized Poisson kernel and explores solvability and uniqueness conditions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the Dirichlet problem for the radial Schrödinger equation with a complex potential, extending classical results to more general boundary conditions and potentials.

Method: The method involves constructing an explicit orthogonal set of solutions for the radial equation and expressing the solution as a series expansion using formal spherical polynomials. A generalized Poisson kernel is introduced and analyzed.

Result: The paper establishes conditions for solvability and uniqueness of the Dirichlet problem and demonstrates how the solution can be represented distributionally for boundary conditions given by complex Radon measures.

Conclusion: The work generalizes classical results by providing explicit solutions and tools (like the generalized Poisson kernel) for the Dirichlet problem under broader conditions, including complex potentials and distributional boundary values.

Abstract: We present an explicit construction of the solution to the Dirichlet boundary
value problem for the radial Schr\"odinger equation in the unit ball, with a
complex-valued potential $V$ satisfying the condition
$\int_0^1r|V(r)|dr<\infty$. The solution is based on the construction of an
explicit orthogonal set of solutions for the radial equation. In the case of a
Dirichlet problem with boundary data in $W^{\frac{1}{2},2}(\mathbb{S}^{d-1})$,
the solution is expressed as a series expansion in terms of the so-called
formal spherical polynomials. We establish conditions for the solvability and
uniqueness of the Dirichlet problem. Based on this series representation, we
introduce the concept of generalized Poisson kernel, develop its main
properties, and investigate the conditions under which the Dirichlet problem,
with a boundary condition being a complex Radon measure on $\mathbb{S}^{d-1}$,
admits a solution in the sense of a distributional boundary values.

</details>


### [21] [Mixtures of nonhomogeneous viscoelastic incompressible fluids governed by the Kelvin-Voigt equations](https://arxiv.org/abs/2506.10278)
*S. N. Antontsev,H. B. de Oliveira,I. V. Kuznetsov,D. A. Prokudin,Kh. Khompysh*

Main category: math.AP

TL;DR: The paper studies a Kelvin-Voigt system for a mixture of incompressible, viscoelastic fluids with non-constant density, proving global weak solutions and uniqueness under extra regularity.


<details>
  <summary>Details</summary>
Motivation: To address the mathematical modeling of mixtures of incompressible and viscoelastic fluids with variable density, focusing on solution existence and uniqueness.

Method: Investigates an initial-and boundary-value problem for the Kelvin-Voigt system, analyzing weak solutions for velocity, density, and pressure.

Result: Establishes global-in-time weak solutions and proves uniqueness under additional regularity conditions.

Conclusion: The work successfully demonstrates the existence and uniqueness of solutions for the described fluid mixture system.

Abstract: An initial-and boundary-value problem for the Kelvin-Voigt system, modeling a
mixture of n incompressible and viscoelastic fluids, with non-constant density,
is investigated in this work. The existence of global-in-time weak solutions is
established: velocity, density and pressure. Under additional regularity
assumptions, we also prove the uniqueness of the solution.

</details>


### [22] [Exponential mixing for the randomly forced NLS equation](https://arxiv.org/abs/2506.10318)
*Yuxuan Chen,Shengquan Xiang,Zhifei Zhang,Jia-Cheng Zhao*

Main category: math.AP

TL;DR: The paper explores exponential mixing in the invariant measure for a randomly forced nonlinear Schrödinger equation with damping and localized noise, highlighting the importance of exponential asymptotic compactness and control properties.


<details>
  <summary>Details</summary>
Motivation: To understand the ergodic properties of random dynamical systems, extending prior work on the statistical behavior of randomly forced dispersive equations.

Method: Focuses on exponential asymptotic compactness and control properties to analyze the system.

Result: Demonstrates exponential mixing of the invariant measure for the studied equation.

Conclusion: The findings extend knowledge on the statistical behavior of randomly forced dispersive equations, emphasizing key properties for ergodicity.

Abstract: This paper investigates exponential mixing of the invariant measure for
randomly forced nonlinear Schr\"{o}dinger equation, with damping and random
noise localized in space. Our study emphasizes the crucial role of exponential
asymptotic compactness and control properties in establishing the ergodic
properties of random dynamical systems. This work extends the series [15, 45]
on the statistical behavior of randomly forced dispersive equations.

</details>


### [23] [On the sharp critical mass threshold for the 3D Patlak-Keller-Segel-Navier-Stokes system via Couette flow](https://arxiv.org/abs/2506.10578)
*Shikun Cui,Lili Wang,Wendong Wang,Juncheng Wei*

Main category: math.AP

TL;DR: The paper investigates the suppression of blow-up in the 3D Patlak-Keller-Segel-Navier-Stokes system using Couette flow, proving global solutions exist for initial cell mass below 16π² if the flow is strong enough.


<details>
  <summary>Details</summary>
Motivation: To understand how Couette flow can prevent blow-up in the 3D Patlak-Keller-Segel-Navier-Stokes system and identify the critical mass threshold.

Method: Combines quasi-linear methods with zero-mode density estimates using logarithmic Hardy-Littlewood-Sobolev inequality, leveraging dissipative decay properties.

Result: Solutions are global for initial mass < 16π² under strong Couette flow, aligning with the 2D critical mass of 8π.

Conclusion: The study provides a sharp threshold for global existence, linking 3D behavior to 2D dynamics and highlighting the role of Couette flow in stabilization.

Abstract: As is well-known, the solution of the Patlak-Keller-Segel system in 3D may
blow up in finite time regardless of any initial cell mass. In this paper, we
are interested in the suppression of blow-up and the critical mass threshold
for the 3D Patlak-Keller-Segel-Navier-Stokes system via the Couette flow $(Ay,
0, 0)$. It is proved that if the Couette flow is sufficiently strong ($A$ is
large enough), then the solutions for the system are global in time in the
periodic domain $(x,y,z)\in\mathbb{T}^{3}$ as long as the initial cell mass is
less than $16\pi^{2}$. This result seems to be sharp, since the zero-mode
function (the mean value in $x-$direction) of the three dimensional density is
a complication of the two-dimensional Keller-Segel equations, whose critical
mass in 2D is $8\pi$. One new observation is the dissipative decay of
$(\widetilde{u}_{2,0},\widetilde{u}_{3,0})$ (see Lemma 4.3 for more details),
then we combine the quasi-linear method proposed by Wei-Zhang (Comm. Pure Appl.
Math., 2021) with the zero-mode estimate of the density by the logarithmic
Hardy-Littlewood-Sobolev inequality as Bedrossian-He (SIAM J. Math. Anal.,
2017) or He (Nonlinearity, 2025) to obtain the bounded-ness of the density and
the velocity.

</details>


### [24] [On local well-posedness for the nonlinear Schrödinger equation with general power nonlinearity](https://arxiv.org/abs/2506.10595)
*Lucia Arens,Marius Gritl*

Main category: math.AP

TL;DR: The paper compares semigroup theory and Strichartz estimates for local well-posedness of nonlinear Schrödinger equations, extending results beyond the case p=2.


<details>
  <summary>Details</summary>
Motivation: To explore and compare two mathematical approaches for solving nonlinear Schrödinger equations, which are key in quantum mechanics and Bose-Einstein condensation.

Method: Uses semigroup theory (functional analysis) and Strichartz estimates (dispersive equations) to analyze local well-posedness.

Result: Demonstrates applicability of both methods for nonlinearities beyond p=2.

Conclusion: Both approaches are effective, with semigroup theory offering generality and Strichartz estimates providing refined estimates.

Abstract: The nonlinear Schr\"odinger equation plays a fundamental role in mathematical
physics, particularly in the study of quantum mechanics and Bose-Einstein
condensation. This paper explores two distinct approaches to establishing the
local well-posedness of solutions: the semigroup theory ansatz and the
Strichartz estimates ansatz. Semigroup theory provides a general and elegant
framework rooted in functional analysis, allowing for the interpretation of the
time evolution of solutions as operator semigroups. Strichartz estimates,
developed specifically for dispersive equations, offer an alternative technique
based on refined space-time estimates and fixed-point arguments. We
systematically analyze and compare both approaches and apply them to nonlinear
Schr\"odinger equations where the nonlinearity is given by $F(u)=\lambda|u|^p
u$ for some $\lambda \in \mathbb{R}$. So our results extend beyond the
physically relevant case $p=2$.

</details>


### [25] [Harnack inequality for degenerate fully nonlinear parabolic equations](https://arxiv.org/abs/2506.10608)
*Vedansh Arya,Vesa Julin*

Main category: math.AP

TL;DR: The paper proves intrinsic and weak Harnack inequalities for nonnegative solutions and supersolutions of degenerate fully nonlinear parabolic equations, extending results from divergence form to nondivergence form.


<details>
  <summary>Details</summary>
Motivation: To generalize the p-parabolic equation (p>2) to nondivergence form operators and establish Harnack inequalities, bridging a gap in existing literature.

Method: The authors analyze degenerate fully nonlinear parabolic equations, adapting techniques from divergence form to nondivergence form.

Result: Intrinsic Harnack inequality for nonnegative solutions and weak Harnack inequality for nonnegative supersolutions are proven.

Conclusion: The results extend prior work on divergence form equations to nondivergence form, providing new tools for analyzing such equations.

Abstract: We consider degenerate fully nonlinear parabolic equations, which generalize
the p-parabolic equation with $p>2$ to nondivergence form operators. We prove
an intrinsic Harnack inequality for nonnegative solutions and a weak Harnack
inequality for nonnegative supersolutions. These results can be seen as the
nondivergence form counterparts of the results by DiBenedetto, Gianazza and
Vespri (Acta Math. 2008) and Kuusi (Ann. Sc. Norm. Super. Pisa 2008).

</details>


### [26] [Cazenave-Dickstein-Weissler-type extension of Fujita's problem on Heisenberg groups](https://arxiv.org/abs/2506.10611)
*Mokhtar Kirane,Ahmad Z. Fino,Berikbol T. Torebek,Zineb Sabbagh*

Main category: math.AP

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper examines the critical exponents for the existence of global
solutions to the equation \begin{equation*} \begin{array}{ll} \displaystyle
u_t-\Delta_{\mathbb{H}}u=\int_0^t(t-s)^{-\gamma}|u(s)|^{p-1}u(s)\,ds,&\qquad
0\leq\gamma<1,\,\,\, {\eta\in \mathbb{H}^n,\,\,\,t>0,}
\end{array}\end{equation*} on the Heisenberg groups $\mathbb{H}^n.$ There
exists a critical exponent $$p_c=
\max\Big\{\frac{1}{\gamma},p_\gamma\Big\}\in(0,+\infty],\quad\hbox{with}\quad
p_\gamma=1+\frac{2(2-\gamma)}{Q-2+2\gamma},\,\,Q=2n+2$$ such that for all
$1<p\leq p_c,$ no global solution exists regardless of the non-negative initial
data, while for $p>p_c$, a global positive solution exists if the initial data
is sufficiently small. The results obtained are a natural extension of the
results of Cazenave et al. [Nonlinear Analysis 68 (2008), 862-874], where
similar studies were carried out in $\mathbb{R}^n$. Also given are several
theorems concerning the lifespan estimates of local solutions for different
cases of initial data. The proofs of the main results are based on test
function methods and Banach fixed point principle.

</details>


### [27] [Stability of the Morse Index for the $p$-harmonic Approximation of Harmonic Maps into Homogeneous Spaces](https://arxiv.org/abs/2506.10761)
*Dominik Schlagenhauf*

Main category: math.AP

TL;DR: The paper extends previous results on Morse index stability for Sacks-Uhlenbeck sequences into spheres to homogeneous spaces, using methods from Bayer and Roberts, and shows upper semicontinuity of Morse index plus nullity and improved gradient estimates in neck regions.


<details>
  <summary>Details</summary>
Motivation: To generalize Morse index stability results from spheres to homogeneous spaces and improve understanding of gradient behavior near blow-up points.

Method: Extends prior work by incorporating Bayer and Roberts' strategy for homogeneous spaces, focusing on Sacks-Uhlenbeck sequences and analyzing Morse index and nullity.

Result: Upper semicontinuity of Morse index plus nullity is proven, along with improved gradient estimates in neck regions around blow-up points.

Conclusion: The study successfully generalizes Morse index stability to homogeneous spaces and provides refined gradient estimates, advancing the understanding of critical points in this context.

Abstract: In the joint work of the author with Da Lio and Rivi\`ere (Morse Index
Stability for Sequences of Sacks-Uhlenbeck Maps into a Sphere) we studied the
stability of the Morse index for Sacks-Uhlenbeck sequences into spheres as
$p\searrow2$. These are critical points of the energy $E_p(u) := \int_\Sigma
\left( 1+|\nabla u|^2\right)^{p/2} \ dvol_\Sigma,$ where $u:\Sigma \rightarrow
S^n$ is a map from a closed Riemannian surface $\Sigma$ into a sphere $ S^n$.
In this paper we extend the results found in our previous work to the case of
Sacks-Uhlenbeck sequences into homogeneous spaces, by incorporating the
strategy introduced by Bayer and Roberts (Energy identity and no neck property
for $\epsilon$-harmonic and $\alpha$-harmonic maps into homogeneous target
manifolds). In the spirit of the work of Da Lio, Gianocca and Rivi\`ere (Morse
Index Stability for Critical Points to Conformally invariant Lagrangians), we
show in this setting the upper semicontinuity of the Morse index plus nullity
and an improved pointwise estimate of the gradient in the neck regions around
blow up points.

</details>


### [28] [New class of time-periodic solutions to the 1D cubic wave equation](https://arxiv.org/abs/2506.10839)
*Filip Ficek,Maciej Maliborski*

Main category: math.AP

TL;DR: The paper confirms the existence of a new class of time-periodic solutions for the defocusing cubic wave equation using rigorous construction and rational arithmetic computations.


<details>
  <summary>Details</summary>
Motivation: To validate earlier findings suggesting new time-periodic solutions for the defocusing cubic wave equation under Dirichlet boundary conditions.

Method: Rigorous construction of solutions using rational arithmetic computations to verify operator bounds.

Result: Successful confirmation and construction of the proposed time-periodic solutions.

Conclusion: The study rigorously establishes the existence of the new class of solutions, supporting prior theoretical suggestions.

Abstract: In recent papers (arXiv:2407.16507, arXiv:2408.05158) we presented results
suggesting the existence of a new class of time-periodic solutions to the
defocusing cubic wave equation on a one-dimensional interval with Dirichlet
boundary conditions. Here we confirm these findings by rigorously constructing
solutions from this class. The proof uses rational arithmetic computations to
verify essential operator bounds.

</details>


### [29] [Normalized solutions for a Sobolev critical quasilinear Schrödinger equation](https://arxiv.org/abs/2506.10870)
*Yuxin Li,Meijie Yang,Xiaojun Chang*

Main category: math.AP

TL;DR: The paper studies normalized solutions for a quasilinear Schrödinger equation with critical exponent under a mass constraint, providing existence results for various parameter ranges and deriving non-existence conditions.


<details>
  <summary>Details</summary>
Motivation: To address the existence of normalized solutions for quasilinear Schrödinger equations with critical exponents, a problem with significant implications in mathematical physics.

Method: The authors use precise energy level estimates, new convergence theorems, and the perturbation method to analyze the equation under different parameter conditions.

Result: Existence of solutions is proven for specific ranges of parameters, including local minimizers and mountain pass solutions, with non-existence results for certain cases.

Conclusion: The study offers a comprehensive analysis of normalized solutions across a broad parameter range, with methods adaptable to other nonlinearities.

Abstract: In this paper, we study the existence of normalized solutions for the
following quasilinear Schr\"odinger equation with critical exponent:
  \begin{equation*}
  -\Delta u-u\Delta (u^2)+\lambda
u=\tau|u|^{q-2}u+|u|^{2\cdot2^*-2}u,~~~~x\in\R^N,
  \end{equation*}
  under the mass constraint $\int_{\R^N}|u|^2dx=c$ for some prescribed $c>0$.
Here $\tau\in \mathbb{R}$ is a parameter, $\lambda\in\R$ appears as a Lagrange
multiplier, $N\ge3$, $2^*:=\frac{2N}{N-2}$ and $2<q<2\cdot2^*$. By deriving
precise energy level estimates and establishing new convergence theorems, we
apply the perturbation method to establish several existence results for
$\tau>0$ in the Sobolev critical regime:
  [label=(\alph*)]
  \item For the case of $2<q<2+\frac{4}{N}$, we obtain the existence of two
solutions, one of which is a local minimizer, and the other is a mountain pass
type solution, under explicit conditions on $c>0$;
  \item For the case of $2+\frac{4}{N}\leq q<4+\frac{4}{N}$, we obtain the
existence of normalized solutions of mountain pass type under different
conditions on $c>0$;
  \item For the case of $4+\frac{4}{N}\leq q<2\cdot2^*$, we obtain the
existence of a ground state normalized solution under different conditions on
$c>0$. Moreover, when $\tau\le 0$, we derive the non-existence result for
$2<q<2\cdot2^*$ and all $c>0$. Our research provides a comprehensive analysis
across the entire range $q\in(2, 2 \cdot 2^*)$ and for all $N\ge3$. The methods
we have developed are flexible and can be extended to a broader class of
nonlinearities.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [30] [Phase-Space Engineering and Dynamical Long-Range Order in Memcomputing](https://arxiv.org/abs/2506.10149)
*Chesson Sipling,Yuan-Hang Zhang,Massimiliano Di Ventra*

Main category: physics.comp-ph

TL;DR: The paper analyzes how hyper-parameters in Digital Memcomputing Machines (DMMs) affect phase-space exploration and solution efficiency, highlighting the role of memory and dynamical long-range order.


<details>
  <summary>Details</summary>
Motivation: To understand how hyper-parameters in DMMs influence their computational efficiency and phase-space dynamics for solving combinatorial optimization problems.

Method: Numerical simulations on a prototypical DMM to study the impact of hyper-parameters on phase-space geometry and solution search efficiency.

Result: DMMs explore phase space efficiently with well-chosen hyper-parameters, aided by dynamical long-range correlations. Poor hyper-parameters reduce efficiency, but dynamical long-range order often persists.

Conclusion: Memory and hyper-parameters are crucial for optimizing DMMs' computational efficiency, with dynamical long-range order playing a key role.

Abstract: Digital Memcomputing machines (DMMs) are dynamical systems with memory (time
non-locality) that have been designed to solve combinatorial optimization
problems. Their corresponding ordinary differential equations depend on a few
hyper-parameters that define both the system's relevant time scales and its
phase-space geometry. Using numerical simulations on a prototypical DMM, we
analyze the role of these physical parameters in engineering the phase space to
either help or hinder the solution search by DMMs. We find that the DMM
explores its phase space efficiently for a wide range of parameters, aided by
the long-range correlations in their fast degrees of freedom that emerge
dynamically due to coupling with the (slow) memory degrees of freedom. In this
regime, the time it takes for the system to find a solution scales well as the
number of variables increases. When these hyper-parameters are chosen poorly,
the system navigates its phase space far less efficiently. However, we find
that, in many cases, dynamical long-range order (DLRO) persists even when the
phase-space exploration process is inefficient. DLRO only disappears if the
memories are made to evolve as quickly as the fast degrees of freedom. This
study points to the important role of memory and hyper-parameters in
engineering the DMMs' phase space for optimal computational efficiency.

</details>


### [31] [Unravelling the mystery of enhanced open-circuit voltages in nanotextured perovskite solar cells](https://arxiv.org/abs/2506.10691)
*Dilara Abdel,Jacob Relle,Thomas Kirchartz,Patrick Jaap,Jürgen Fuhrmann,Sven Burger,Christiane Becker,Klaus Jäger,Patricio Farrell*

Main category: physics.comp-ph

TL;DR: Nanotextures in perovskite solar cells improve efficiency by reducing recombination losses, not just optical effects, with an optimal texture height for real-world applications.


<details>
  <summary>Details</summary>
Motivation: To understand why nanotextures enhance open-circuit voltage (V_OC) in perovskite solar cells, a phenomenon not fully explained before.

Method: Combined multi-dimensional optical and charge-transport simulations for a single-junction perovskite solar cell.

Result: Texturing reduces Shockley-Read-Hall recombination, increasing V_OC, with an optimal texture height for efficiency.

Conclusion: Texturing offers opto-electronic benefits, guiding design for future perovskite-based devices.

Abstract: Perovskite solar cells have reached power conversion efficiencies that rival
those of established silicon photovoltaic technologies. Nanotextures in
perovskite solar cells optimise light trapping and scattering, thereby
improving optical absorption. In addition, nanotextures have been
experimentally shown to enhance electronic performance, in particular, by
increasing the open-circuit voltage $V_{\text{OC}}$ -- a phenomenon that, until
now, has remained not fully understood. This study investigates the underlying
reasons by combining multi-dimensional optical and charge-transport simulations
for a single-junction perovskite solar cell. Our results reveal that the
increased open-circuit voltage is not driven by optical effects but by the
textured geometry itself. For voltages near $V_{\text{OC}}$, texturing one of
the absorber/transport layer interfaces increases the imbalance between
electron and hole densities in the absorber, thereby reducing
Shockley-Read-Hall (SRH) recombination, which is the dominant loss mechanism in
this study. While idealised solar cells benefit unconditionally from increasing
texture height, in realistic cells there is an optimal texture height which
maximizes the power conversion efficiency. These findings provide new insights
into the opto-electronic advantages of texturing and offer guidance for the
design of next-generation textured perovskite-based solar cells, light emitting
diodes, and photodetectors.

</details>


### [32] [Distillation of atomistic foundation models across architectures and chemical domains](https://arxiv.org/abs/2506.10956)
*John L. A. Gardner,Daniel F. Thomas du Toit,Chiheb Ben Mahmoud,Zoé Faure Beaulieu,Veronika Juraskova,Laura-Bianca Paşca,Louise A. M. Rosset,Fernanda Duarte,Fausto Martelli,Chris J. Pickard,Volker L. Deringer*

Main category: physics.comp-ph

TL;DR: The paper demonstrates how distillation via synthetic data can efficiently transfer knowledge from large atomistic foundation models to smaller, faster potentials, achieving significant speed-ups and broad applicability.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency and resource-intensive nature of current atomistic foundation models, enabling their practical use in scientific research.

Method: Distillation via synthetic data to transfer knowledge from large foundation models to smaller architectures, tested across various chemical and materials domains.

Result: Achieved speed-ups of >10× and >100× for different architectures, with successful applications in diverse domains like liquid water, hydrogen under extreme conditions, and organic reactions.

Conclusion: Distillation enables computationally efficient use of atomistic foundation models, making them viable for real-world scientific applications.

Abstract: Machine-learned interatomic potentials have transformed computational
research in the physical sciences. Recent atomistic `foundation' models have
changed the field yet again: trained on many different chemical elements and
domains, these potentials are widely applicable, but comparably slow and
resource-intensive to run. Here we show how distillation via synthetic data can
be used to cheaply transfer knowledge from atomistic foundation models to a
range of different architectures, unlocking much smaller, more efficient
potentials. We demonstrate speed-ups of $> 10\times$ by distilling from one
graph-network architecture into another, and $> 100\times$ by leveraging the
atomic cluster expansion framework. We showcase applicability across chemical
and materials domains: from liquid water to hydrogen under extreme conditions;
from porous silica and a hybrid halide perovskite solar-cell material to
modelling organic reactions. Our work shows how distillation can support the
routine and computationally efficient use of current and future atomistic
foundation models in real-world scientific research.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [33] [Synergistic control of radical generation in a radio frequency atmospheric pressure plasma jet via voltage waveform tailoring and structured electrodes](https://arxiv.org/abs/2506.10099)
*Mate Vass,Xiaokun Wang,Ihor Korolov,Julian Schulze,Thomas Mussenbrock*

Main category: physics.plasm-ph

TL;DR: The study explores how combining tailored voltage waveforms with structured electrodes in an RF microplasma jet enhances localized electron power absorption and radical generation.


<details>
  <summary>Details</summary>
Motivation: To investigate the synergy between voltage waveform tailoring and electrode structuring for precise control of plasma properties.

Method: Uses 2D plasma fluid simulations and optical diagnostics (PROES, TDLAS) with asymmetric waveforms and structured electrodes.

Result: Asymmetric waveforms and electrode structuring localize electron power absorption and radical generation, unlike symmetric excitation.

Conclusion: The synergy enables selective, reversible enhancement of electron power absorption at desired locations.

Abstract: The synergy between voltage waveform tailoring and structured electrodes is
investigated in a radio-frequency (RF) atmospheric-pressure microplasma jet
operated in helium with a 0.1% oxygen admixture. The device incorporates
rectangular trenches in both electrodes and is driven by "Peaks" and "Valleys"
waveforms synthesized from four harmonics (base frequency $f_{\rm b} =
13.56$~MHz, $V_{\rm pp} = 500$~V, $P=$1.2~W). Two-dimensional plasma fluid
simulations, together with spatially and temporally resolved optical
diagnostics (Phase-Resolved Optical Emission Spectroscopy and Tunable Diode
Laser Absorption Spectroscopy), are used to demonstrate that the combination of
asymmetric voltage waveforms with electrode structuring leads to strong spatial
localization of electron power absorption and radical generation. This synergy
results in a single pronounced maximum inside a trench at either the powered or
grounded electrode, depending on the applied waveform, unlike a symmetric
excitation, which produces a spatially symmetric enhancement at both
electrodes. The effect is attributed to the interplay between waveform-induced
sheath dynamics and geometric focusing provided by the trenches, enabling
electrically reversible and selective enhancement of electron power absorption
at a chosen location.

</details>


### [34] [Exact series expansion for even frequency moments of the dynamic structure factor](https://arxiv.org/abs/2506.10410)
*Panagiotis Tolias,Jan Vorberger,Tobias Dornheim*

Main category: physics.plasm-ph

TL;DR: The paper derives an exact series for even frequency moments of the dynamic structure factor, proposes truncations for unknown moments, and validates their range using non-interacting limits and Monte Carlo simulations.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of evaluating unknown frequency moments (second, fourth, fifth) for the finite-temperature uniform electron gas.

Method: Derives an exact series representation and proposes truncations; validates using non-interacting limits and path integral Monte Carlo simulations.

Result: Identifies the applicability range of the proposed truncations in terms of degeneracy parameter and wavenumber.

Conclusion: The derived truncations are validated and provide a practical approach for evaluating unknown frequency moments.

Abstract: An exact series representation of the even frequency moments of the dynamic
structure factor is derived. Truncations are proposed that allow to evaluate
the explicitly unknown second, fourth and fifth frequency moments for the
finite temperature uniform electron gas. Their applicability range in terms of
degeneracy parameter and wavenumber is determined by exploiting the
non-interacting limit and by comparing with the quasi-exact results of path
integral Monte Carlo simulations.

</details>


### [35] [Runaway electron-induced plasma facing component damage in tokamaks](https://arxiv.org/abs/2506.10411)
*S. Ratynskaia,M. Hoelzl,E. Nardon,P. Aleynikov,F. J. Artola,V. Bandaru,M. Beidler,B. Breizman,D. del-Castillo-Negrete,M. De Angeli,V. Dimitriou,R. Ding,J. Eriksson,O. Ficker,R. S. Granetz,E. Hollmann,M. Hoppe,M. Houry,I. Jepu,H. R. Koslowski,C. Liu,J. R. Martin-Solis,G. Pautasso,Y. Peneliau,R. A. Pitts,G. I. Pokol,C. Reux,U. Sheikh,S. A. Silburn,T. Tang,R. A. Tinguely,P. Tolias,E. Tomesova,R. Villari*

Main category: physics.plasm-ph

TL;DR: The paper addresses the challenge of plasma-facing component (PFC) damage from runaway electrons (REs) in tokamaks, proposing a coordinated, interdisciplinary approach to bridge gaps in understanding and mitigation.


<details>
  <summary>Details</summary>
Motivation: The threat of RE-induced PFC damage to future fusion reactors like ITER and DEMO necessitates a holistic understanding and mitigation strategy.

Method: The approach involves reviewing experimental evidence, advancing diagnostics, and improving multi-scale, multi-fidelity modelling.

Result: Key findings include insights into RE beam formation, PFC damage mechanisms, and observations from major facilities.

Conclusion: The paper aims to guide future mitigation strategies and resilient PFC designs for safe, sustainable fusion power plants.

Abstract: This Roadmap article addresses the critical and multifaceted challenge of
plasma-facing component (PFC) damage caused by runaway electrons (REs) in
tokamaks, a phenomenon that poses a significant threat to the viability and
longevity of future fusion reactors such as ITER and DEMO. The dramatically
increased RE production expected in future high-current tokamaks makes it
difficult to avoid or mitigate REs when a plasma discharge terminates
abnormally. Preventing damage from the intense localised heat loads REs can
cause requires a holistic approach that considers plasma, REs and PFC damage.
Despite decades of progress in understanding the physics of REs and the
thermomechanical response of PFCs, their complex interplay remains poorly
understood. This document aims to initiate a coordinated, interdisciplinary
approach to bridge this gap by reviewing experimental evidence, advancing
diagnostic capabilities, and improving modelling tools across different scales,
dimensionalities and fidelities. Key topics include RE beam formation and
transport, damage mechanisms in brittle and metallic PFCs, and observations in
major facilities such as JET, DIII-D, WEST and EAST. The Roadmap emphasises the
urgency of predictive, high-fidelity modelling validated against well-diagnosed
controlled experiments, particularly in the light of recent changes in ITER's
wall material strategy and the growing importance of private sector
initiatives. Each section of the article is written to provide a concise
overview of one area of this multidisciplinary subject, with an assessment of
the status, a look at current and future challenges, and a brief summary. The
ultimate goal of this initiative is to guide future mitigation strategies and
design resilient components that can withstand the loads imposed by REs, thus
ensuring the safe and sustainable operation of the next generation of fusion
power plants.

</details>


### [36] [Spatial and temporal evolutions of blue-core helicon discharge driven by planar antenna with concentric rings](https://arxiv.org/abs/2506.10493)
*Chao Wang,Lei Chang,Ling-Feng Lu,Shunjiro Shinohara,Zhi-De Zeng,Ilya Zadiriev,Elena Kralkina,Zhi Li,Shi-Jie Zhang,Zi-Chen Kan,Ye Tao,Ding-Zhou Li*

Main category: physics.plasm-ph

TL;DR: The paper explores blue-core helicon discharge evolution driven by a planar antenna, revealing unique hollow plasma column behavior, rotation direction changes, and centrifugal instability dynamics.


<details>
  <summary>Details</summary>
Motivation: To characterize the spatial and temporal evolution of blue-core helicon discharge using a novel planar antenna, focusing on mode transitions, rotation, and instability.

Method: Experiments conducted on the LEAD device with a four-ring planar antenna, analyzing discharge modes, rotation, and instability using a two-fluid plasma model.

Result: Observed hollow blue-core plasma, rotation direction reversal, and centrifugal instability peaking off-axis. Instability weakens with high axial wave numbers and exceeds thresholds for magnetic field/flow rate.

Conclusion: First detailed study of blue-core helicon plasma with a planar antenna, highlighting unique rotation and instability behaviors, advancing understanding of helicon discharge dynamics.

Abstract: The spatial and temporal evolutions of blue-core helicon discharge driven by
a planar antenna with four concentric rings are explored on the Linear
Experimental Advanced Device (LEAD). The discharge experiences distinct density
jumps from E mode to H mode, W mode, and blue-core mode, when RF input power
increases. This is similar to previous observations using other typical helicon
antennas; however, this special antenna could drive modes of even higher levels
for which the blue-core plasma column is actually hollow in radius, i.e.
peaking off-axis, which was not presented before. The column shows
counterclockwise rotation for blue-core mode and clockwise rotation for
non-blue-core mode. The reason could be attributed to the radial electric field
differenceses for both modes which reverses the rotation direction via ExB
drive. Moreover, the centrifugal instability of blue-core helicon plasma is
computed using a two-fluid flowing plasma model. It shows that the instability
is strong for small axial wave number but becomes weak for large axial wave
number. Perturbed density peaks at radius of 0.045 m, while the equilibrium
density gradient peaks at radius of 0.055 m. The coincidence of their radial
locations suggests that it is a resistive drift mode driven by density
gradient. The blue-core mode weakens once the magnetic field or flow rate
exceeds the threshold value. Increasing power further leads to a smoother
plasma density gradient. The electron temperature profiles decrease with
increased power, and the radial gradient of the electron temperature inside the
core is smaller as the magnetic field changes. To our best knowledge, it is the
first detailed characterization of blue-core helicon plasma driven by planar
antenna, especially in terms of azimuthal rotation and centrifugal instability.

</details>


### [37] [Analytic model for neutral penetration and plasma fueling](https://arxiv.org/abs/2506.10906)
*George J. Wilkie*

Main category: physics.plasm-ph

TL;DR: The paper develops analytic models for neutral density near plasma walls, validated with simulations, and simplifies charge exchange effects.


<details>
  <summary>Details</summary>
Motivation: To understand and model how recycled neutral atoms from wall interactions refuel confined plasma, especially near walls or X-points.

Method: Progression of analytic models for neutral density near planar/linear sources, validated with DEGAS2 simulations, and generalized for plasma gradients or charge exchange.

Result: Charge exchange can be simplified as a loss with reasonable accuracy, and the model isolates and solves neutral fueling from recycling.

Conclusion: The model provides a closed-form solution for neutral fueling, adaptable to various neutral behaviors in divertors and separatrix crossings.

Abstract: Neutral atoms recycled from wall interaction interact with confined plasma,
thereby refueling it, most strongly in the region closest to the wall. This
occurs near the X-point in diverted configurations, or else near the wall
itself in limited configurations. A progression of analytic models are
developed for neutral density in the vicinity of a planar or linear source in
an ionizing domain. First-principles neutral transport simulations with DEGAS2
are used throughout to test the validity and limits of the model when using
equivalent sources. The model is further generalized for strong plasma
gradients or the inclusion of charge exchange. An important part of the problem
of neutral fueling from recycling is thereby isolated and solved with a
closed-form analytic model. A key finding is that charge exchange with the
confined plasma can be significantly simplified with a reasonable sacrifice of
accuracy by treating it as a loss. The several assumptions inherent to the
model (and the simulations to which it is compared) can be adapted according to
the particular behavior of neutrals in the divertor and the manner in which
they cross the separatrix.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [38] [On the stability of the generalized equator map](https://arxiv.org/abs/2506.10652)
*Volker Branding,Anna Siffert*

Main category: math.DG

TL;DR: The paper studies the generalized equator map as a critical point of energy, $p$-energy, and extrinsic $k$-energy, focusing on its stability analysis.


<details>
  <summary>Details</summary>
Motivation: To extend classical stability results for the equator map by analyzing a generalized version under multiple energy functionals.

Method: The authors first prove the generalized equator map is a critical point of extrinsic $k$-energy, then conduct a detailed stability analysis for both extrinsic $k$-energy and $p$-energy.

Result: The study generalizes classical (in)stability results, providing new insights into the stability of the generalized equator map.

Conclusion: The work extends prior findings and offers a comprehensive stability analysis of the generalized equator map under various energy functionals.

Abstract: The energy, the $p$-energy ($p\in\mathbb{R}$ with $p\geq 2$) and the
extrinsic $k$-energy ($k\in\mathbb{N}$) for maps between Riemannian manifolds
are central objects in the geometric calculus of variations. The equator map
from the unit ball to the Euclidean sphere provides an explicit critical point
of all aforementioned energy functionals. During the last four decades many
researchers studied the stability of this particular map when considered as a
critical point of one of these energy functionals, see e.g. \cite{MR4436204},
\cite{MR705882}.
  Recently, Nakauchi \cite{MR4593065} introduced a generalized radial
projection map and proved that this map is both a critical point of the energy
and a critical point of the $p$-energy. This generalized radial projection map
gives rise to a generalized equator map which is also both a critical point of
the energy and a critical point of the $p$-energy.
  In this manuscript we first of all show that the generalized equator map is
also a critical point of the extrinsic $k$-energy. Then, the main focus is a
detailed stability analysis of this map, considered as a critical point of both
the extrinsic $k$-energy and the $p$-energy. We thus establish a number of
interesting generalizations of the classical (in)stability results of J\"ager
and Kaul \cite{MR705882}.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [39] [Principled Approaches for Extending Neural Architectures to Function Spaces for Operator Learning](https://arxiv.org/abs/2506.10973)
*Julius Berner,Miguel Liu-Schiaffini,Jean Kossaifi,Valentin Duruisseaux,Boris Bonev,Kamyar Azizzadenesheli,Anima Anandkumar*

Main category: cs.LG

TL;DR: The paper introduces neural operators as a way to generalize neural networks for mappings between infinite-dimensional function spaces, enabling deep learning's success in scientific problems like PDEs. It provides a practical guide for converting existing neural architectures into neural operators.


<details>
  <summary>Details</summary>
Motivation: The disparity between finite-dimensional deep learning applications and infinite-dimensional scientific problems limits neural networks' success in fields like PDEs. Neural operators bridge this gap.

Method: The paper distills key principles for constructing mappings between function spaces and proposes a recipe to convert popular neural architectures into neural operators with minimal changes.

Result: The approach enables neural operators to learn solution operators for PDEs, adapting to varying conditions like boundary constraints and geometries.

Conclusion: The paper provides a practical framework for implementing neural operators, extending deep learning's impact to scientific applications.

Abstract: A wide range of scientific problems, such as those described by
continuous-time dynamical systems and partial differential equations (PDEs),
are naturally formulated on function spaces. While function spaces are
typically infinite-dimensional, deep learning has predominantly advanced
through applications in computer vision and natural language processing that
focus on mappings between finite-dimensional spaces. Such fundamental
disparities in the nature of the data have limited neural networks from
achieving a comparable level of success in scientific applications as seen in
other fields. Neural operators are a principled way to generalize neural
networks to mappings between function spaces, offering a pathway to replicate
deep learning's transformative impact on scientific problems. For instance,
neural operators can learn solution operators for entire classes of PDEs, e.g.,
physical systems with different boundary conditions, coefficient functions, and
geometries. A key factor in deep learning's success has been the careful
engineering of neural architectures through extensive empirical testing.
Translating these neural architectures into neural operators allows operator
learning to enjoy these same empirical optimizations. However, prior neural
operator architectures have often been introduced as standalone models, not
directly derived as extensions of existing neural network architectures. In
this paper, we identify and distill the key principles for constructing
practical implementations of mappings between infinite-dimensional function
spaces. Using these principles, we propose a recipe for converting several
popular neural architectures into neural operators with minimal modifications.
This paper aims to guide practitioners through this process and details the
steps to make neural operators work in practice. Our code can be found at
https://github.com/neuraloperator/NNs-to-NOs

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [40] [Coupled Lindblad pseudomode theory for simulating open quantum systems](https://arxiv.org/abs/2506.10308)
*Zhen Huang,Gunhee Park,Garnet Kin-Lic Chan,Lin Lin*

Main category: quant-ph

TL;DR: Coupled Lindblad pseudomode theory efficiently simulates non-Markovian quantum dynamics with polylog scaling in time and precision, avoiding non-convex optimization.


<details>
  <summary>Details</summary>
Motivation: To improve the simulation of non-Markovian quantum dynamics for classical and quantum platforms by simplifying the scaling and optimization challenges.

Method: Develops a robust numerical algorithm for constructing coupled pseudomodes, avoiding non-convex optimization, and tests it on the spin-boson model.

Result: Demonstrates effective computation of population dynamics and absorption spectra, showing polylog scaling in simulation time and precision.

Conclusion: The work enhances the coupled Lindblad framework, benefiting applications from quantum impurity problems to near-term quantum simulations.

Abstract: Coupled Lindblad pseudomode theory is a promising approach for simulating
non-Markovian quantum dynamics on both classical and quantum platforms, with
dynamics that can be realized as a quantum channel. We provide theoretical
evidence that the number of coupled pseudomodes only needs to scale as
$\mathrm{polylog}(T/\varepsilon)$ in the simulation time $T$ and precision
$\varepsilon$. Inspired by the realization problem in control theory, we also
develop a robust numerical algorithm for constructing the coupled modes that
avoids the non-convex optimization required by existing approaches. We
demonstrate the effectiveness of our method by computing population dynamics
and absorption spectra for the spin-boson model. This work provides a
significant theoretical and computational improvement to the coupled Lindblad
framework, which impacts a broad range of applications from classical
simulations of quantum impurity problems to quantum simulations on near-term
quantum platforms.

</details>


### [41] [Hamiltonian Learning via Inverse Physics-Informed Neural Networks](https://arxiv.org/abs/2506.10379)
*Jie Liu,Xin Wang*

Main category: quant-ph

TL;DR: iPINN-HL integrates physics into ML for Hamiltonian learning, outperforming DNN-HL in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Conventional HL methods struggle with noise and resource efficiency under limited measurements.

Method: iPINN-HL embeds the Schrödinger equation into ML, combining data and physics for parameter inference.

Result: iPINN-HL approaches the Heisenberg limit, shows noise robustness, and beats DNN-HL in accuracy and efficiency.

Conclusion: iPINN-HL is a robust, flexible framework for quantum system characterization.

Abstract: Hamiltonian learning (HL), enabling precise estimation of system parameters
and underlying dynamics, plays a critical role in characterizing quantum
systems. However, conventional HL methods face challenges in noise robustness
and resource efficiency, especially under limited measurements. In this work,
we present \textit{Inverse Physics-Informed Neural Networks for Hamiltonian
Learning (iPINN-HL)}, an approach that embeds the Schr\"{o}dinger equation
directly into the machine learning procedure. This formulation allows the model
to integrate both observational data and known physical laws to infer
Hamiltonian parameters with greater accuracy and resource efficiency. We
benchmark iPINN-HL against a deep-neural-network-based quantum state tomography
method (denoted as DNN-HL) and demonstrate its effectiveness across several
different scenarios, including one-dimensional spin chains, cross-resonance
gate calibration, crosstalk identification, and real-time compensation to
parameter drift. Our results show that iPINN-HL can approach the Heisenberg
limit in certain settings and exhibits robustness to noises, while
outperforming DNN-HL in accuracy and resource efficiency. Therefore, iPINN-HL
is a powerful and flexible framework for quantum system characterization for
practical tasks.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [42] [Vortex-magnetic competition and regime transitions in antiparallel flux tubes](https://arxiv.org/abs/2506.10648)
*Weiyu Shen,Rodolfo Ostilla-Mónico,Xiaojue Zhu*

Main category: physics.flu-dyn

TL;DR: The paper explores how vortex-magnetic interactions in MHD turbulence affect energy transfer across different regimes, revealing three distinct behaviors based on the Lorentz-to-inertial force balance.


<details>
  <summary>Details</summary>
Motivation: Understanding the interplay between vortex dynamics and magnetic fields in MHD turbulence is crucial for astrophysical, geophysical, and industrial systems, particularly for energy transfer processes.

Method: The study revisits the problem of two antiparallel flux tubes, varying the magnetic flux to analyze the interaction parameter (Ni), which measures Lorentz-to-inertial force balance.

Result: Three regimes emerge: vortex-dominated reconnection (low Ni), instability-triggered cascade (moderate Ni), and Lorentz-induced vortex disruption (high Ni). Each regime exhibits unique energy transfer mechanisms.

Conclusion: The inertial-Lorentz balance dictates energy transfer and structure formation in MHD turbulence, providing insights into vortex-magnetic coevolution in astrophysical plasmas.

Abstract: Vortex-magnetic interactions shape magnetohydrodynamic (MHD) turbulence,
influencing energy transfer in astrophysical, geophysical, and industrial
systems. On the Sun, granular-scale vortex flows couple strongly with magnetic
fields, channelling energy into the corona. At high Reynolds numbers, vorticity
and magnetic fields are nearly frozen into the charged fluid, and MHD flows
emerge from the interplay between vortex dynamics and Lorentz forces. To probe
this competition in a controlled setting, we revisit the canonical problem of
two antiparallel flux tubes. By varying the magnetic flux threading each
tube--and thus sweeping the interaction parameter $N_i$, which gauges
Lorentz-to-inertial force balance--we uncover three distinct regimes:
vortex-dominated joint reconnection, instability-triggered cascade, and
Lorentz-induced vortex disruption. At low $N_i$, classical vortex dynamics
dominate, driving joint vortex-magnetic reconnection and amplifying magnetic
energy via a dynamo effect. At moderate $N_i$, the system oscillates between
vorticity-driven attraction and magnetic damping, triggering instabilities and
nonlinear interactions that spawn secondary filaments and drive an energy
cascade. At high $N_i$, Lorentz forces suppress vortex interactions, aligning
the tubes axially while disrupting vortex cores and rapidly converting magnetic
to kinetic energy. These findings reveal how the inertial-Lorentz balance
governs energy transfer and coherent structure formation in MHD turbulence,
offering insight into vortex-magnetic coevolution in astrophysical plasmas.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [43] [Efficient nanophotonic devices optimization using deep neural network trained with physics-based transfer learning (PBTL) methodology](https://arxiv.org/abs/2506.10418)
*Gibaek Kim,Jungho Kim*

Main category: physics.optics

TL;DR: A NN-based surrogate modeling framework is proposed for photonic device optimization, combining PBTL-enhanced surrogate modeling and multi-objective GAs, achieving significant speed-up and improved accuracy with limited data.


<details>
  <summary>Details</summary>
Motivation: To address challenges in photonic device optimization, such as imbalanced feature importance and high data generation costs, by providing a generalizable solution with minimal data resources.

Method: The framework integrates physics-based transfer learning (PBTL) for enhanced surrogate modeling and scalarized multi-objective genetic algorithms (GAs) for optimization. A DNN-TP surrogate model replaces expensive simulations, and PBTL transfers knowledge from a DNN-CP trained on active-region structures.

Result: Achieves an 80,000x speed-up in optimization, a 0.69% increase in prediction accuracy, and a 60% improvement in evaluation metrics for feasible device structures.

Conclusion: The proposed framework efficiently optimizes photonic devices with limited data, demonstrating significant improvements in speed, accuracy, and feasibility.

Abstract: We propose a neural network(NN)-based surrogate modeling framework for
photonic device optimization, especially in domains with imbalanced feature
importance and high data generation costs. Our framework, which comprises
physics-based transfer learning (PBTL)-enhanced surrogate modeling and
scalarized multi-objective genetic algorithms (GAs), offers a generalizable
solution for photonic design automation with minimal data resources.To validate
the framework, we optimize mid-infrared quantum cascade laser (QCL) structures
consisting of two regions, active and injection, which have different levels of
feature importance. The optimization targets include five key QCL performance
metrics such as modal gain, emission wavelength, linewidth, and effective
injection, extraction energies. To address the challenge of multiple local
optima in the output latent space, we integrate a deep neural network total
predictor (DNN-TP) with a GA, enabling scalable and nature-inspired
optimization. By replacing computationally expensive numerical simulations with
the DNN-TP surrogate model, the optimization achieves a speed-up of over 80,000
times, allowing large-scale exploration of the QCL design space.To improve
model generalization with limited data, we introduce PBTL, which transfers
knowledge from a DNN core predictor (DNN-CP) trained on active-region
structures. This approach yields a 0.69 percentage increase in prediction
accuracy, equivalent to a 50 percentage reduction in training data
requirements, and leads to generate more feasible device structure with 60
percentage improvement in evaluation metric during optimization.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [44] [Interpretable and flexible non-intrusive reduced-order models using reproducing kernel Hilbert spaces](https://arxiv.org/abs/2506.10224)
*Alejandro N Diaz,Shane A McQuarrie,John T Tencer,Patrick J Blonigan*

Main category: cs.CE

TL;DR: The paper introduces a non-intrusive reduced-order modeling technique using regularized kernel interpolation, offering interpretability and flexibility by combining feature maps and kernel terms. It includes error bounds and demonstrates effectiveness in numerical experiments.


<details>
  <summary>Details</summary>
Motivation: Existing non-intrusive methods lack interpretability and flexibility. The goal is to develop a technique that mirrors full-order model structure while allowing user-defined feature maps and kernel terms.

Method: The approach uses regularized kernel interpolation to approximate ROM dynamics, embedding feature maps for interpretability and combining them with nonlinear kernel terms. It also derives a posteriori error bounds.

Result: The method produces interpretable ROMs that reflect full-order model structure and performs well in numerical experiments, including comparisons with other techniques.

Conclusion: The kernel-based approach is effective, flexible, and interpretable, offering a promising alternative to existing non-intrusive ROM methods.

Abstract: This paper develops an interpretable, non-intrusive reduced-order modeling
technique using regularized kernel interpolation. Existing non-intrusive
approaches approximate the dynamics of a reduced-order model (ROM) by solving a
data-driven least-squares regression problem for low-dimensional matrix
operators. Our approach instead leverages regularized kernel interpolation,
which yields an optimal approximation of the ROM dynamics from a user-defined
reproducing kernel Hilbert space. We show that our kernel-based approach can
produce interpretable ROMs whose structure mirrors full-order model structure
by embedding judiciously chosen feature maps into the kernel. The approach is
flexible and allows a combination of informed structure through feature maps
and closure terms via more general nonlinear terms in the kernel. We also
derive a computable a posteriori error bound that combines standard error
estimates for intrusive projection-based ROMs and kernel interpolants. The
approach is demonstrated in several numerical experiments that include
comparisons to operator inference using both proper orthogonal decomposition
and quadratic manifold dimension reduction.

</details>


### [45] [PDESpectralRefiner: Achieving More Accurate Long Rollouts with Spectral Adjustment](https://arxiv.org/abs/2506.10711)
*Li Luo,Shangsong Liang*

Main category: cs.CE

TL;DR: PDERefiner uses diffusion models to refine PDE solutions, improving high-frequency accuracy. For complex PDEs like Navier-Stokes, spectral adjustments enhance performance.


<details>
  <summary>Details</summary>
Motivation: High-frequency accuracy in PDE solutions is challenging. Diffusion models help refine outputs but may over-degrade frequencies in complex cases.

Method: Enhanced refiner model with spectral adjustments (Blurring diffusion models) and v-prediction technique for better accuracy.

Result: Improved accuracy for one-step MSE loss and rollout loss across model backbones like U-Net and neural operators.

Conclusion: Spectral adjustments in refiner models address over-degradation, enhancing accuracy for complex PDEs.

Abstract: Generating accurate and stable long rollouts is a notorious challenge for
time-dependent PDEs (Partial Differential Equations). Recently, motivated by
the importance of high-frequency accuracy, a refiner model called PDERefiner
utilizes diffusion models to refine outputs for every time step, since the
denoising process could increase the correctness of modeling high frequency
part. For 1-D Kuramoto-Sivashinsky equation, refiner models can degrade the
amplitude of high frequency part better than not doing refinement process.
However, for some other cases, the spectrum might be more complicated. For
example, for a harder PDE like Navior-Stokes equation, diffusion models could
over-degrade the higher frequency part. This motivates us to release the
constraint that each frequency weighs the same. We enhance our refiner model
with doing adjustments on spectral space, which recovers Blurring diffusion
models. We developed a new v-prediction technique for Blurring diffusion
models, recovering the MSE training objective on the first refinement step. We
show that in this case, for different model backbones, such as U-Net and neural
operators, the outputs of PDE-SpectralRefiner are more accurate for both
one-step MSE loss and rollout loss.

</details>


### [46] [Spectral Analysis of Discretized Boundary Integral Operators in 3D: a High-Frequency Perspective](https://arxiv.org/abs/2506.10880)
*V. Giunzioni,A. Merlini,F. P. Andriulli*

Main category: cs.CE

TL;DR: The paper challenges the common practice of using mesh sizes of λ/10 for boundary element method discretization, showing that accuracy degrades with increasing frequency.


<details>
  <summary>Details</summary>
Motivation: To investigate the accuracy of the widely used discretization approach (mesh size ≈ λ/10) in boundary element methods for wave propagation and scattering.

Method: Analyzed the spectra of operator matrices to compare discretized and continuous operators.

Result: Found a growing discrepancy with increasing frequency, indicating that the λ/10 rule may not maintain accuracy at higher frequencies.

Conclusion: The common discretization approach may not suffice for high-frequency simulations, suggesting a need for reevaluation.

Abstract: When modeling propagation and scattering phenomena using integral equations
discretized by the boundary element method, it is common practice to
approximate the boundary of the scatterer with a mesh comprising elements of
size approximately equal to a fraction of the wavelength $\lambda$ of the
incident wave, e.g., $\lambda/10$. In this work, by analyzing the spectra of
the operator matrices, we show a discrepancy with respect to the continuous
operators which grows with the simulation frequency, challenging the common
belief that the aforementioned widely used discretization approach is
sufficient to maintain the accuracy of the solution constant when increasing
the frequency.

</details>


<div id='astro-ph.EP'></div>

# astro-ph.EP [[Back]](#toc)

### [47] [Self-gravity in thin protoplanetary discs: 1. The smoothing-length approximation versus the exact self-gravity kernel](https://arxiv.org/abs/2506.10812)
*S. Rendon Restrepo,T. Rometsch,U. Ziegler,O. Gressel*

Main category: astro-ph.EP

TL;DR: The paper introduces an exact self-gravity kernel for thin discs, addressing flaws in traditional softening prescriptions and enabling more accurate 2D simulations of planet-forming discs.


<details>
  <summary>Details</summary>
Motivation: Traditional 2D models use ad hoc softening for self-gravity, which neglects vertical structure, distorts Newtonian gravity, and violates Newton's third law. This work aims to provide a precise alternative.

Method: Developed an analytical self-gravity kernel for thin, hydrostatically supported discs, validated through benchmarks and 2D/3D tests. The kernel uses modified Bessel functions to maintain Newtonian features.

Result: The new kernel preserves Newtonian gravitation properties, identifies a new gravitational runaway source, and remains computationally efficient with FFT-based methods.

Conclusion: The exact kernel overcomes smoothing-length limitations and enables consistent self-gravity treatment in thin discs, useful for planet formation studies. Future work will explore its impact on gravitational instability.

Abstract: Planet-forming discs often contain structures like spiral arms, typically
linked to the disc's gravitational forces. In 2D models, an ad hoc softening
prescription is commonly used for self-gravity, but this overlooks the vertical
structure's impact, suppresses the Newtonian nature of gravity at short
distances and doesn't respect Newton's third law.
  To address these issues, associated with a Plummer potential approximation,
we developed an exact self-gravity kernel for thin, hydrostatically supported
discs, including a dust fluid component. Our analytical framework provides a
precise 2D self-gravity prescription validated by benchmarks and 2D/3D
numerical tests.
  The derived kernel, based on modified Bessel functions, maintains Newtonian
gravitation features, such as point-wise symmetry, a smooth transition from
light to massive discs and a singularity at zero distance, among others. In
contrast to other prescriptions found in the literature, it proves capable of
leading to an additional, and previously unnoticed, source of gravitational
runaway discernible only at infinitesimal distances.
  We finally note that our new prescription remains compatible with methods
based on the fast Fourier transform, affording superior computational
efficiency. Our exact kernel formulation overcomes substantial limitations
inherent in the smoothing-length approach. It permits a novel, fully consistent
treatment of self-gravity in Gaussian-stratified thin discs. The approach, that
makes the usage of the Plummer potential obsolete, will prove useful to
studying all common planet formation scenarios, which are often backed by
2D-flat numerical simulations. Accordingly, in an accompanying paper, we will
investigate how the occurence of the gravitational instability is affected.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [48] [Polynomial slowdown in space-inhomogeneous branching Brownian motion](https://arxiv.org/abs/2506.10623)
*Julien Berestycki,David Geldbach,Michel Pain*

Main category: math.PR

TL;DR: The paper studies a 2D branching Brownian motion with angle-dependent branching rates, showing tightness of the maximum distance from the origin after subtracting a time-dependent function.


<details>
  <summary>Details</summary>
Motivation: To understand how inhomogeneous branching rates, particularly favoring a specific direction, affect the spatial spread of particles in branching Brownian motion.

Method: Analyzes a branching Brownian motion in ℝ² with branching rate b(θ) depending on angle θ, focusing on the maximum distance M_t from the origin and deriving a tightness result for M_t - m(t).

Result: Proves tightness of (M_t - m(t))_{t≥1}, where m(t) is a deterministic function involving time t and parameters α, β, and an eigenvalue ϑ₁.

Conclusion: The study provides insights into the spatial distribution of particles in branching Brownian motion with directional bias in branching rates.

Abstract: We consider a branching Brownian motion in $\mathbb{R}^2$ in which particles
independently diffuse as standard Brownian motions and branch at an
inhomogeneous rate $b(\theta)$ which depends only on the angle $\theta$ of the
particle. We assume that $b$ is maximal when $\theta=0$, which is the preferred
direction for breeding. Furthermore we assume that $b(\theta ) = 1 - \beta
\abs{\theta }^\alpha + O(\theta ^2)$, as $\theta \to 0$, for $\alpha \in
(2/3,2)$ and $\beta>0.$ We show that if $M_t$ is the maximum distance to the
origin at time $t$, then $(M_t-m(t))_{t\ge 1}$ is tight where $$m(t) = \sqrt{2}
t - \frac{\vartheta_1}{\sqrt{2}} t^{(2-\alpha)/(2+\alpha)} -
\left(\frac{3}{2\sqrt{2}} - \frac{\alpha}{2\sqrt{2}(2+\alpha)}\right) \log t.
$$ and $\vartheta_1$ is explicit in terms of the first eigenvalue of a certain
operator.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [49] [Data-Driven Prediction of Dynamic Interactions Between Robot Appendage and Granular Material](https://arxiv.org/abs/2506.10875)
*Guanjin Wang,Xiangxue Zhao,Shapour Azarm,Balakumar Balachandran*

Main category: cs.RO

TL;DR: A data-driven approach combining dimension reduction, surrogate modeling, and data assimilation significantly reduces computational time while maintaining accuracy in predicting robot-granular terrain interactions.


<details>
  <summary>Details</summary>
Motivation: To improve robot navigation in complex terrains by developing a computationally efficient and accurate modeling approach.

Method: Integration of dimension reduction (ST-HOSVD), surrogate modeling (Gaussian Process), and data assimilation (Reduced Order Particle Filter) using offline simulation and sparse experimental data.

Result: Orders of magnitude reduction in computational time; comparable accuracy to simulations; potential outperformance in long-horizon predictions with experimental data.

Conclusion: The approach enhances robot navigation in unknown terrains, offering efficiency and accuracy beyond traditional simulations.

Abstract: An alternative data-driven modeling approach has been proposed and employed
to gain fundamental insights into robot motion interaction with granular
terrain at certain length scales. The approach is based on an integration of
dimension reduction (Sequentially Truncated Higher-Order Singular Value
Decomposition), surrogate modeling (Gaussian Process), and data assimilation
techniques (Reduced Order Particle Filter). This approach can be used online
and is based on offline data, obtained from the offline collection of
high-fidelity simulation data and a set of sparse experimental data. The
results have shown that orders of magnitude reduction in computational time can
be obtained from the proposed data-driven modeling approach compared with
physics-based high-fidelity simulations. With only simulation data as input,
the data-driven prediction technique can generate predictions that have
comparable accuracy as simulations. With both simulation data and sparse
physical experimental measurement as input, the data-driven approach with its
embedded data assimilation techniques has the potential in outperforming only
high-fidelity simulations for the long-horizon predictions. In addition, it is
demonstrated that the data-driven modeling approach can also reproduce the
scaling relationship recovered by physics-based simulations for maximum
resistive forces, which may indicate its general predictability beyond a
case-by-case basis. The results are expected to help robot navigation and
exploration in unknown and complex terrains during both online and offline
phases.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [50] [Time-dependent Gaussian basis sets for many-body systems using Rothe's method: A mean-field study](https://arxiv.org/abs/2506.10701)
*Simon Elias Schrader,Håkon Emil Kristiansen,Thomas Bondo Pedersen,Simen Kvaal*

Main category: physics.chem-ph

TL;DR: The paper proposes using Rothe's method with Gaussian basis sets for efficient modeling of time-dependent strong-field processes in many-body systems, eliminating the need for grids.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of representing the electronic continuum in time-dependent strong-field processes like high-harmonic generation for many-body systems.

Method: Applies Rothe's method to TDHF and TDDFT equations, reformulating them as an optimization problem, and uses thawed, complex-valued Gaussian basis sets for orbital propagation.

Result: Demonstrates that few flexible Gaussians yield qualitatively correct results, and grid calculations can be quantitatively reproduced with 30–100 Gaussians for intensities up to 4×10¹⁴ W/cm² in 1D systems.

Conclusion: The method offers an efficient, grid-free approach for modeling strong-field processes in many-body systems.

Abstract: A challenge in modeling time-dependent strong-field processes such as
high-harmonic generation for many-body systems, is how to effectively represent
the electronic continuum. We apply Rothe's method to the time-dependent
Hartree-Fock (TDHF) and density functional theory (TDDFT) equations of motion
for the orbitals, which reformulates them as an optimization problem. We show
that thawed, complex-valued Gaussian basis sets can be propagated efficiently
for these orbital-based approaches, removing the need for grids. In particular,
we illustrate that qualitatively correct results can often be obtained by using
just a few fully flexible Gaussians that describe the unbound dynamics for both
TDHF and TDDFT. Grid calculations can be reproduced quantitatively using
$30$--$100$ Gaussians for intensities up to $4\times10^{14}$ W/cm$^2$ for the
one-dimensional molecular systems considered in this work.

</details>


<div id='math.NT'></div>

# math.NT [[Back]](#toc)

### [51] [Fast Ramanujan--type Series for Logarithms. Part II](https://arxiv.org/abs/2506.10321)
*Jorge Zuniga*

Main category: math.NT

TL;DR: The paper extends prior work on Ramanujan-type series for logarithms, introducing new formulas for arctangents and fast multiseries evaluations. It develops methods for computing multiple logarithms simultaneously via integer programming, achieving highly efficient results and setting a record for log(10) precision.


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency and precision of computing logarithms, especially for multiple values simultaneously, by leveraging hypergeometric identities and integer programming.

Method: Derives formulas by solving an integer programming problem within a finite lattice, optimizing variable values to create linear combinations of series for fast evaluations.

Result: Achieves highly efficient formulas for single logarithms and the fastest known hypergeometric formulas for multivalued logarithms, extending log(10) precision to 2.0·10¹² digits.

Conclusion: The work significantly advances the computational efficiency and precision of logarithm evaluations, with practical applications in high-precision calculations.

Abstract: This work extends the results of the preprint Ramanujan type Series for
Logarithms, Part I, arXiv:2506.08245, which introduced single hypergeometric
type identities for the efficient computing of $\log(p)$, where
$p\in\mathbb{Z}_{>1}$. We present novel formulas for arctangents and methods
for a very fast multiseries evaluation of logarithms. Building upon a
$\mathcal{O}((p-1)^{6})$ Ramanujan type series asymptotic approximation for
$\log(p)$ as $p\rightarrow1$, formulas for computing $n$ simultaneous
logarithms are developed. These formulas are derived by solving an integer
programming problem to identify optimal variable values within a finite lattice
$\mathbb{Z}^{n}$. This approach yields linear combinations of series that
provide: (i) highly efficient formulas for single logarithms of natural numbers
and (ii) the fastest known hypergeometric formulas for multivalued logarithms
of $n$ selected integers in $\mathbb{Z}_{>1}$. An application of these results
was to extend the number of decimal places known for log(10) up to
2.0$\cdot$10$^{12}$ digits (June 06 2025).

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [52] [Going beyond density functional theory accuracy: Leveraging experimental data to refine pre-trained machine learning interatomic potentials](https://arxiv.org/abs/2506.10211)
*Shriya Gumber,Lorena Alzate-Vargas,Benjamin T. Nebgen,Arjen van Veelen,Smit Kadvani,Tammie Gibson,Richard Messerly*

Main category: cond-mat.mtrl-sci

TL;DR: The paper introduces a trajectory re-weighting technique to refine DFT-trained MLIPs using experimental EXAFS spectra, improving accuracy for nuclear fuels like UO2 and UN.


<details>
  <summary>Details</summary>
Motivation: MLIPs inherit errors from DFT approximations, leading to discrepancies with experimental data. Refining MLIPs to match EXAFS spectra can enhance their predictive accuracy for structural properties.

Method: A trajectory re-weighting technique combined with transfer learning and minimal training epochs refines DFT-trained MLIPs to align with experimental EXAFS spectra.

Result: The refined MLIPs show significant improvement in predicting various properties (lattice parameters, bulk modulus, etc.) for UO2 and UN compared to unrefined versions.

Conclusion: The approach enhances MLIP accuracy for nuclear fuels, reducing reliance on costly and hazardous experimental tests.

Abstract: Machine learning interatomic potentials (MLIPs) are inherently limited by the
accuracy of the training data, usually consisting of energies and forces
obtained from quantum mechanical calculations, such as density functional
theory (DFT). Since DFT itself is based on several approximations, MLIPs may
inherit systematic errors that lead to discrepancies with experimental data. In
this paper, we use a trajectory re-weighting technique to refine DFT
pre-trained MLIPs to match the target experimental Extended X-ray Absorption
Fine Structure (EXAFS) spectra. EXAFS spectra are sensitive to the local
structural environment around an absorbing atom. Thus, refining an MLIP to
improve agreement with experimental EXAFS spectra also improves the MLIP
prediction of other structural properties that are not directly involved in the
refinement process. We combine this re-weighting technique with transfer
learning and a minimal number of training epochs to avoid overfitting to the
limited experimental data. The refinement approach demonstrates significant
improvement for two MLIPs reported in previous work, one for an established
nuclear fuel: uranium dioxide (UO$_2$) and second one for a nuclear fuel
candidate: uranium mononitride (UN). We validate the effectiveness of our
approach by comparing the results obtained from the original (unrefined)
DFT-based MLIP and the EXAFS-refined MLIP across various properties, such as
lattice parameters, bulk modulus, heat capacity, point defect energies, elastic
constants, phonon dispersion spectra, and diffusion coefficients. An accurate
MLIP for nuclear fuels is extremely beneficial as it enables reliable atomistic
simulation, which greatly reduces the need for large number of expensive and
inherently dangerous experimental nuclear integral tests, traditionally
required for the qualification of efficient and resilient fuel candidates.

</details>


### [53] [GEARS H: Accurate machine-learned Hamiltonians for next-generation device-scale modeling](https://arxiv.org/abs/2506.10298)
*Anubhab Haldar,Ali K. Hamze,Nikhil Sivadas,Yongwoo Shin*

Main category: cond-mat.mtrl-sci

TL;DR: GEARS H is a machine-learning Hamiltonian framework for large-scale electronic structure simulations, outperforming existing methods with high accuracy.


<details>
  <summary>Details</summary>
Motivation: To enable DFT-accuracy simulations at device-scale, which conventional methods cannot handle due to computational limits.

Method: Uses GEARS H for statistical analysis of hole concentration in defective WSe2 interfaced with Ni-doped HfO2, and trains models for diverse systems.

Result: Achieves mean absolute error below 2.4 meV for Hamiltonian matrix elements, surpassing other frameworks.

Conclusion: GEARS H is production-ready for DFT-accuracy device-scale simulations, marking a milestone for machine-learning Hamiltonian methods.

Abstract: We introduce GEARS H, a state-of-the-art machine-learning Hamiltonian
framework for large-scale electronic structure simulations. Using GEARS H, we
present a statistical analysis of the hole concentration induced in defective
$\mathrm{WSe}_2$ interfaced with Ni-doped amorphous $\mathrm{HfO}_2$ as a
function of the Ni doping rate, system density, and Se vacancy rate in 72
systems ranging from 3326 to 4160 atoms-a quantity and scale of interface
electronic structure calculation beyond the reach of conventional density
functional theory codes and other machine-learning-based methods. We further
demonstrate the versatility of our architecture by training models for a
molecular system, 2D materials with and without defects, solid solution
crystals, and bulk amorphous systems with covalent and ionic bonds. The mean
absolute error of the inferred Hamiltonian matrix elements from the validation
set is below 2.4 meV for all of these models. GEARS H outperforms other
proposed machine-learning Hamiltonian frameworks, and our results indicate that
machine-learning Hamiltonian methods, starting with GEARS H, are now
production-ready techniques for DFT-accuracy device-scale simulation.

</details>


### [54] [Coupled reaction and diffusion governing interface evolution in solid-state batteries](https://arxiv.org/abs/2506.10944)
*Jingxuan Ding,Laura Zichi,Matteo Carli,Menghang Wang,Albert Musaelian,Yu Xie,Boris Kozinsky*

Main category: cond-mat.mtrl-sci

TL;DR: The paper uses large-scale reactive simulations with quantum accuracy to study SEI formation in solid-state batteries, revealing a new crystalline disordered phase and Li creep mechanisms.


<details>
  <summary>Details</summary>
Motivation: Understanding atomistic-level reactions in SEI formation is critical for next-generation solid-state batteries, but experimental and simulation challenges persist.

Method: Large-scale explicit reactive simulations with quantum accuracy, enabled by active learning and deep equivariant neural network interatomic potentials, plus unsupervised classification techniques.

Result: Discovery of a new crystalline disordered phase (Li$_2$S$_{0.72}$P$_{0.14}$Cl$_{0.14}$) in SEI and insights into Li creep mechanisms.

Conclusion: The approach provides a parameter-free digital twin for studying complex heterogeneous processes in solid-state synthesis and electrochemistry.

Abstract: Understanding and controlling the atomistic-level reactions governing the
formation of the solid-electrolyte interphase (SEI) is crucial for the
viability of next-generation solid state batteries. However, challenges persist
due to difficulties in experimentally characterizing buried interfaces and
limits in simulation speed and accuracy. We conduct large-scale explicit
reactive simulations with quantum accuracy for a symmetric battery cell,
{\symcell}, enabled by active learning and deep equivariant neural network
interatomic potentials. To automatically characterize the coupled reactions and
interdiffusion at the interface, we formulate and use unsupervised
classification techniques based on clustering in the space of local atomic
environments. Our analysis reveals the formation of a previously unreported
crystalline disordered phase, Li$_2$S$_{0.72}$P$_{0.14}$Cl$_{0.14}$, in the
SEI, that evaded previous predictions based purely on thermodynamics,
underscoring the importance of explicit modeling of full reaction and transport
kinetics. Our simulations agree with and explain experimental observations of
the SEI formations and elucidate the Li creep mechanisms, critical to dendrite
initiation, characterized by significant Li motion along the interface. Our
approach is to crease a digital twin from first principles, without adjustable
parameters fitted to experiment. As such, it offers capabilities to gain
insights into atomistic dynamics governing complex heterogeneous processes in
solid-state synthesis and electrochemistry.

</details>


<div id='physics.atm-clus'></div>

# physics.atm-clus [[Back]](#toc)

### [55] [Dipole-quadrupole coupling in triplet exciton-polaron quenching in a phosphorescent OLED emission layer](https://arxiv.org/abs/2506.10794)
*Clint van Hoesel,Reinder Coehoorn,Peter A. Bobbert*

Main category: physics.atm-clus

TL;DR: The paper investigates the triplet exciton-polaron quenching (TPQ) mechanism in phosphorescent OLEDs, revealing that dipole-quadrupole coupling dominates over Förster approximation, providing a new metric for TPQ rate.


<details>
  <summary>Details</summary>
Motivation: Understanding TPQ is crucial for improving OLED efficiency and stability, but current lack of knowledge hinders progress.

Method: Quantum-chemical calculations were used to study TPQ for triplet excitons on a phosphorescent guest interacting with hole polarons on a host.

Result: The study found that dipole-quadrupole coupling dominates TPQ at relevant distances, resolving discrepancies in TPQ rate estimates.

Conclusion: The findings offer a new approach to quantify TPQ and can guide the development of more efficient and stable phosphorescent OLEDs.

Abstract: Improving the efficiency and stability of organic light-emitting diodes
(OLEDs) will further expand their present success in display applications.
Triplet exciton-polaron quenching (TPQ) is an important cause of limited
efficiency and stability in modern phosphorescent OLEDs, where triplet excitons
are the emitting species. Lack of understanding of the TPQ mechanism in these
OLEDs impedes the development of more efficient and stable OLEDs. We
investigate the TPQ mechanism for triplet excitons on a phosphorescent guest
interacting with hole polarons on a host. Our quantum-chemical calculations
show that at distances relevant for TPQ the F\"orster approximation for the TPQ
rate fails and that dipole-quadrupole coupling is dominant. This resolves a
discrepancy between estimates of the TPQ rate obtained from an OLED device
study and from the overlap between the emission spectrum of the emitter and
absorption spectrum of the charged host. Equivalently to the F\"orster radius
for dipole-dipole TPQ, the dipole-quadrupole TPQ rate can be quantified by a
dipole-quadrupole radius obtained from the overlap between the emission
spectrum of the emitter and the quadrupolar absorption spectrum of the charged
host. The findings of this work are expected to have a broad relevance and to
be useful in developing phosphorescent emitter-host combinations with reduced
TPQ.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [56] [Approximate Controllability Problems for the Heat Equation in a Half-Plane Controlled by the Dirichlet Boundary Condition with a Bounded Control](https://arxiv.org/abs/2506.10466)
*Larissa Fardigola,Kateryna Khalina*

Main category: math.OC

TL;DR: The paper studies approximate controllability for a heat equation system, proving initial states can be controlled to arbitrary end states using specific controls, with a numerical algorithm provided.


<details>
  <summary>Details</summary>
Motivation: To address the controllability of a heat equation system with boundary controls, ensuring practical applicability through theoretical proofs and numerical methods.

Method: Theoretical analysis of the control system and development of a numerical algorithm for solving the approximate controllability problem.

Result: Initial states in $L^2$ can be approximately controlled to arbitrary end states in $L^2$ using the specified controls.

Conclusion: The study successfully demonstrates controllability and provides a practical numerical solution, validated by an example.

Abstract: In the paper, the problems of approximate controllability are studied for the
control system $w_t=\Delta w$, $w(0,x_2,t)=u(x_2,t)$, $x_1\in\mathbb
R_+=(0,+\infty)$, $x_2\in\mathbb R$, $t\in(0,T)$, where $u$ is a control
belonging to a special subset of $L^\infty(\mathbb R\times (0,T))\cap
L^2(\mathbb R\times (0,T))$. It is proved that each initial state belonging to
$L^2(\mathbb R_+\times\mathbb R)$ is approximately controllable to an arbitrary
end state belonging to $L^2(\mathbb R_+\times\mathbb R)$ by applying these
controls. A numerical algorithm of solving the approximate controllability
problem for this system is given. The results are illustrated by an example.

</details>


### [57] [On a mean-field Pontryagin minimum principle for stochastic optimal control](https://arxiv.org/abs/2506.10506)
*Manfred Opper,Sebastian Reich*

Main category: math.OC

TL;DR: A deterministic, mean-field extension of the Pontryagin minimum principle for stochastic optimal control, simplifying solution via gauge variables and tested on inverted pendulum and Lorenz-63 systems.


<details>
  <summary>Details</summary>
Motivation: To address stochastic optimal control problems with a deterministic, mean-field approach, avoiding the complexity of forward-backward stochastic differential equations.

Method: Introduces a gauge variable to achieve Hamiltonian structure, decoupling forward and reverse time equations, and applies mean-field ordinary differential equations for infinite horizon problems.

Result: Simplifies the boundary value problem and converts optimal control law computation into solving forward mean-field ODEs, validated numerically.

Conclusion: The proposed mean-field Pontryagin principle offers a practical and efficient alternative for stochastic optimal control, demonstrated through numerical examples.

Abstract: This papers outlines a novel extension of the classical Pontryagin minimum
(maximum) principle to stochastic optimal control problems. Contrary to the
well-known stochastic Pontryagin minimum principle involving forward-backward
stochastic differential equations, the proposed formulation is deterministic
and of mean-field type. The Hamiltonian structure of the proposed Pontryagin
minimum principle is achieved via the introduction of an appropriate gauge
variable. The gauge freedom can be used to decouple the forward and reverse
time equations; hence simplifying the solution of the underlying boundary value
problem. We also consider infinite horizon discounted cost optimal control
problems. In this case, the mean-field formulation allows converting the
computation of the desired optimal control law into solving a pair of forward
mean-field ordinary differential equations. The proposed mean-field formulation
of the Pontryagin minimum principle is tested numerically for a controlled
inverted pendulum and a controlled Lorenz-63 system.

</details>


### [58] [A space-time interface-fitted method for moving-subdomain distributed control problems with energy regularization](https://arxiv.org/abs/2506.10924)
*Quang Huy Nguyen,Phuong Cuc Hoang,Van Chien Le,Thi Thanh Mai Ta*

Main category: math.OC

TL;DR: The paper proposes a space-time interface-fitted method for solving a moving-interface optimal control problem with energy regularization, proving its equivalence to the original problem and providing optimal error estimates.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of moving-interface optimal control problems by developing a robust numerical approximation method.

Method: A Petrov-Galerkin approximation on fully unstructured, space-time interface-fitted meshes, reformulating optimality conditions into a variational problem.

Result: Optimal error estimates are derived under specific regularity assumptions, supported by numerical experiments.

Conclusion: The proposed method effectively solves the problem, with theoretical and numerical validation.

Abstract: This paper investigates a space-time interface-fitted approximation of a
moving-interface optimal control problem with energy regularization. We
reformulate the optimality conditions into a variational problem involving both
the state and adjoint. This problem is shown to be equivalent to our optimal
control problem. Based on fully unstructured, space-time interface-fitted
meshes, we propose and analyze a Petrov-Galerkin approximation of the problem.
An optimal error estimate with respect to a discrete norm is established under
a specific regularity assumption on the state and adjoint. Several numerical
results are presented to corroborate our theoretical results.

</details>
