<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 16]
- [math.AP](#math.AP) [Total: 13]
- [physics.comp-ph](#physics.comp-ph) [Total: 3]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 5]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 3]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [physics.atm-clus](#physics.atm-clus) [Total: 1]
- [astro-ph.EP](#astro-ph.EP) [Total: 1]
- [cs.CE](#cs.CE) [Total: 3]
- [math.DG](#math.DG) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]
- [math.NT](#math.NT) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [math.OC](#math.OC) [Total: 3]
- [math.PR](#math.PR) [Total: 1]
- [cs.RO](#cs.RO) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Data-driven balanced truncation for second-order systems with generalized proportional damping](https://arxiv.org/abs/2506.10118)
*Sean Reiter,Steffen W. R. Werner*

Main category: math.NA

TL;DR: A new data-driven method for structured reduced-order modeling of second-order linear dynamical systems is proposed, generalizing balanced truncation techniques.


<details>
  <summary>Details</summary>
Motivation: To enable efficient, low-dimensional models with meaningful structures for control system design.

Method: Data-driven reformulation of position-velocity balanced truncation for second-order systems, inferring damping coefficients via least-squares minimization.

Result: Surrogates with generalized proportional damping structure are computed, validated by numerical examples.

Conclusion: The method effectively extends balanced truncation to second-order systems, offering practical utility in control system design.

Abstract: Structured reduced-order modeling is a central component in the
computer-aided design of control systems in which cheap-to-evaluate
low-dimensional models with physically meaningful internal structures are
computed. In this work, we develop a new approach for the structured
data-driven surrogate modeling of linear dynamical systems described by
second-order time derivatives via balanced truncation model-order reduction.
The proposed method is a data-driven reformulation of position-velocity
balanced truncation for second-order systems and generalizes the
quadrature-based balanced truncation for unstructured first-order systems to
the second-order case. The computed surrogates encode a generalized
proportional damping structure, and the damping coefficients are inferred
solely from data by minimizing a least-squares error over the coefficients.
Several numerical examples demonstrate the effectiveness of the proposed
method.

</details>


### [2] [R-PINN: Recovery-type a-posteriori estimator enhanced adaptive PINN](https://arxiv.org/abs/2506.10243)
*Rongxin Lu,Jiwei Jia,Young Ju Lee,Zheng Lu,Chensong Zhang*

Main category: math.NA

TL;DR: The paper introduces R-PINN, an adaptive PINN algorithm, to improve accuracy for PDEs with large local gradients by adjusting collocation points based on error estimates.


<details>
  <summary>Details</summary>
Motivation: Existing PINNs perform poorly for PDEs with large local gradients, leading to localized errors.

Method: Proposes R-PINN, combining a recovery-type a-posteriori estimator (from adaptive FEM) with PINNs to adaptively adjust collocation points.

Result: R-PINN converges faster with fewer points and outperforms FI-PINN in cases with multiple error regions.

Conclusion: R-PINN is a hybrid method integrating adaptive FEM and PINNs, offering superior performance for challenging PDEs.

Abstract: In recent years, with the advancements in machine learning and neural
networks, algorithms using physics-informed neural networks (PINNs) to solve
PDEs have gained widespread applications. While these algorithms are
well-suited for a wide range of equations, they often exhibit suboptimal
performance when applied to equations with large local gradients, resulting in
substantial localized errors. To address this issue, this paper proposes an
adaptive PINN algorithm designed to improve accuracy in such cases. The core
idea of the algorithm is to adaptively adjust the distribution of collocation
points based on the recovery-type a-posterior error of the current numerical
solution, enabling a better approximation of the true solution. This approach
is inspired by the adaptive finite element method. By combining the
recovery-type a-posteriori estimator, a gradient-recovery estimator commonly
used in the adaptive finite element method (FEM) with PINNs, we introduce the
Recovery-type a-posteriori estimator enhanced adaptive PINN (R-PINN) and
compare its performance with a typical adaptive PINN algorithm, FI-PINN. Our
results demonstrate that R-PINN achieves faster convergence with fewer adaptive
points and significantly outperforms in the cases with multiple regions of
large errors than FI-PINN. Notably, our method is a hybrid numerical approach
for solving partial differential equations, integrating adaptive FEM with
PINNs.

</details>


### [3] [Enhanced randomized Douglas-Rachford method: Improved probabilities and adaptive momentum](https://arxiv.org/abs/2506.10261)
*Liqi Guo,Ruike Xiang,Deren Han,Jiaxin Xie*

Main category: math.NA

TL;DR: The paper enhances the randomized Douglas-Rachford (RDR) method with improved sampling strategies and adaptive momentum, achieving better convergence and practical performance.


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency and convergence of randomized iterative methods for solving large-scale linear systems.

Method: Incorporates without-replacement and volume sampling into RDR, and introduces an adaptive heavy-ball momentum scheme.

Result: Stronger convergence guarantees and linear convergence in expectation with improved bounds. Numerical experiments show superior performance over the original RDR.

Conclusion: The enhanced RDR method offers significant practical and theoretical improvements over the original, making it more effective for large-scale problems.

Abstract: Randomized iterative methods have gained recent interest in machine learning
and signal processing for solving large-scale linear systems. One such example
is the randomized Douglas-Rachford (RDR) method, which updates the iterate by
reflecting it through two randomly selected hyperplanes and taking a convex
combination with the current point. In this work, we enhance RDR by introducing
improved sampling strategies and an adaptive heavy-ball momentum scheme.
Specifically, we incorporate without-replacement and volume sampling into RDR,
and establish stronger convergence guarantees compared to conventional i.i.d.
sampling. Furthermore, we develop an adaptive momentum mechanism that
dynamically adjusts step sizes and momentum parameters based on previous
iterates, and prove that the resulting method achieves linear convergence in
expectation with improved convergence bounds. Numerical experiments demonstrate
that the enhanced RDR method consistently outperforms the original version,
providing substantial practical benefits across a range of problem settings.

</details>


### [4] [Complex scaling for open waveguides](https://arxiv.org/abs/2506.10263)
*Charles L. Epstein,Tristan Goodwill,Jeremy Hoskins,Solomon Quinn,Manas Rachh*

Main category: math.NA

TL;DR: The paper analyzes the complex scaling method for time-harmonic scalar wave propagation in leaky dielectric waveguide junctions, showing rapid kernel decay and enabling efficient numerical solutions.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of solving wave propagation in open dielectric waveguides by leveraging analytic continuations and integral equations.

Method: Reduces the problem to Fredholm second-kind integral equations, analyzes kernel decay, and deforms the integral equation for efficient discretization.

Result: Demonstrates exponential error decay in truncation and provides numerical examples.

Conclusion: The method offers a practical and accurate approach for solving wave propagation in such junctions.

Abstract: In this work we analyze the complex scaling method applied to the problem of
time-harmonic scalar wave propagation in junctions between `leaky,' or open
dielectric waveguides. In [arXiv:2302.04353, arXiv:2310.05816,
arXiv:2401.04674, arXiv:2411.11204], it was shown that under suitable
assumptions the problem can be reduced to a system of Fredholm second-kind
integral equations on an infinite interface, transverse to the waveguides.
Here, we show that the kernels appearing in the integral equation admit a
rapidly decaying analytic continuation on certain natural totally real
submanifolds of $\mathbb{C}^2.$ We then show that for suitable,
physically-meaningful, boundary data the resulting solutions to the integral
equations themselves admit analytic continuation and satisfy related asymptotic
estimates. By deforming the integral equation to a suitable contour, the decay
in the kernels, density, and data enable straightforward discretization and
truncation, with an error that decays exponentially in the truncation length.
We illustrate our results with several representative numerical examples.

</details>


### [5] [Penalty-Based Feedback Control and Finite Element Analysis for the Stabilization of Nonlinear Reaction-Diffusion Equations](https://arxiv.org/abs/2506.10428)
*Sudeep Kundu,Shishu pal Singh*

Main category: math.NA

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this work, first we employ the penalization technique to analyze the
Dirichlet boundary feedback control problem pertaining to reaction-diffusion
equation. We establish the stabilization result of the equivalent Robin problem
in the \(H^{2}\)-norm with respect to the penalty parameter. Furthermore, we
prove that the solution of the penalized control problem converges to the
corresponding solution of the Dirichlet boundary feedback control problem as
the penalty parameter \(\epsilon\) approaches zero. A \(C^{0}\)-conforming
finite element method is applied to this problem for the spatial variable while
keeping the time variable continuous. We discuss the stabilization of the
semi-discrete scheme for the penalized control problem and present an error
analysis of its solution. Finally, we validate our theoretical findings through
numerical experiments.

</details>


### [6] [Stability analysis of the free-surface Stokes problem and an unconditionally stable explicit scheme](https://arxiv.org/abs/2506.10447)
*Igor Tominec,Lukas Lundgren,André Löfgren,Josefin Ahlkrona*

Main category: math.NA

TL;DR: The paper analyzes stability and conservation properties of the Stokes/free-surface system for viscous flows, proposing a stabilization term for explicit Euler discretization to ensure stability and volume conservation.


<details>
  <summary>Details</summary>
Motivation: To address the need for accurate simulations of highly viscous free-surface flows like ice sheets and lava flows by analyzing and improving the stability and conservation properties of the coupled Stokes/free-surface system.

Method: Theoretical analysis of the weak form of the system for Newtonian and non-Newtonian fluids, including fully discrete stability analysis for finite element methods with explicit/implicit Euler time-stepping. A stabilization term for explicit Euler is proposed.

Result: The proposed stabilization term ensures unconditional time stability and domain volume conservation, validated by numerical experiments.

Conclusion: The study provides theoretical and numerical validation of the stabilization term, enhancing the reliability of simulations for viscous free-surface flows.

Abstract: Accurate simulations of ice sheet dynamics, mantle convection, lava flow, and
other highly viscous free-surface flows involve solving the coupled
Stokes/free-surface equations. In this paper, we theoretically analyze the
stability and conservation properties of the weak form of this system for
Newtonian fluids and non-Newtonian fluids, at both the continuous and discrete
levels. We perform the fully discrete stability analysis for finite element
methods used in space with explicit and implicit Euler time-stepping methods
used in time. Motivated by the theory, we propose a stabilization term designed
for the explicit Euler discretization, which ensures unconditional time
stability and permits conservation of the domain volume. Numerical experiments
validate and support our theoretical findings.

</details>


### [7] [Convergence of adaptive boundary element methods driven by functional a posteriori error estimates](https://arxiv.org/abs/2506.10499)
*Alexander Freiszlinger,Dirk Pauly,Dirk Praetorius*

Main category: math.NA

TL;DR: Functional a posteriori error estimates for BEMs, covering Galerkin and collocation methods, focus on potential approximation errors in the domain. Adaptive mesh-refinement on auxiliary strips ensures convergence.


<details>
  <summary>Details</summary>
Motivation: Existing BEM error estimators often focus on boundary integral density errors, which are less relevant than domain potential errors in practice.

Method: Proposes functional error estimates by solving auxiliary problems on adaptive strip domains along the boundary.

Result: Proves convergence of potential error to zero for Galerkin BEM using adaptive mesh-refinement.

Conclusion: The approach provides a practical and theoretically sound method for error control in BEM, with broader applicability than traditional estimators.

Abstract: The recent work [Kurz et al., Numer. Math., 147 (2021)] proposed functional a
posteriori error estimates for boundary element methods (BEMs) together with a
related adaptive mesh-refinement strategy. Unlike most a posteriori BEM error
estimators, the proposed functional error estimators cover Galerkin as well as
collocation BEM and, more importantly, do not control the error in the integral
density on the boundary, but the error of the potential approximation in the
domain, which is of greater relevance in practice. The estimates rely on the
numerical solution of auxiliary problems on auxiliary strip domains along the
boundary, where the strips are affected by the adaptive mesh-refinement and
hence vary. For Galerkin BEM, we prove that the proposed adaptive
mesh-refinement algorithm yields convergence of the potential error to zero.
Due to the structural difference to residual-based estimators, the proof
requires new ideas.

</details>


### [8] [A semi-Lagrangian scheme for First-Order Mean Field Games based on monotone operators](https://arxiv.org/abs/2506.10509)
*Elisabetta Carlini,Valentina Coscetti*

Main category: math.NA

TL;DR: A semi-Lagrangian scheme for non-local Mean Field Games is proposed, with convergence analysis, a Learning Value Algorithm, and an acceleration strategy. Numerical experiments validate the scheme's effectiveness.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of solving first-order, time-dependent, non-local Mean Field Games by developing efficient numerical methods.

Method: Construct a semi-Lagrangian scheme, analyze its convergence, implement a Learning Value Algorithm, and propose an acceleration strategy using Policy iteration.

Result: The scheme converges to a weak solution, and the accelerated version significantly improves performance.

Conclusion: The proposed methods are effective for solving non-local Mean Field Games, with the accelerated version offering notable performance gains.

Abstract: We construct a semi-Lagrangian scheme for first-order, time-dependent, and
non-local Mean Field Games. The convergence of the scheme to a weak solution of
the system is analyzed by exploiting a key monotonicity property. To solve the
resulting discrete problem, we implement a Learning Value Algorithm, prove its
convergence, and propose an acceleration strategy based on a Policy iteration
method. Finally, we present numerical experiments that validate the
effectiveness of the proposed schemes and show that the accelerated version
significantly improves performance.

</details>


### [9] [Non-augmented velocity-vorticity-pressure formulation for the Navier--Stokes--Brinkman--Forchheimer problem](https://arxiv.org/abs/2506.10533)
*Santiago Badia,Carsten Carstensen,Alberto F. Martin,Ricardo Ruiz-Baier,Segundo Villa-Fuentes*

Main category: math.NA

TL;DR: The paper addresses the flow of incompressible fluid in porous media, solving a double saddle-point problem using Crouzeix-Raviart finite elements. It provides existence proofs, error estimates, and adaptive mesh refinement for efficient simulations.


<details>
  <summary>Details</summary>
Motivation: To solve the Navier-Stokes-Brinkman-Forchheimer equations for incompressible fluid flow in highly permeable porous media, focusing on small sources and efficient numerical methods.

Method: Uses lowest-order piecewise divergence-free Crouzeix-Raviart finite elements, penalisation terms for velocity jumps, and Raviart-Thomas interpolant for pressure-robust error estimates. Includes adaptive mesh refinement.

Result: Establishes existence of solutions, provides a priori and a posteriori error estimates, and demonstrates improved convergence with adaptive mesh refinement.

Conclusion: The method is robust and efficient, with adaptive mesh refinement enhancing convergence rates for simulations.

Abstract: The flow of incompressible fluid in highly permeable porous media in
vorticity - velocity - Bernoulli pressure form leads to a double saddle-point
problem in the Navier--Stokes--Brinkman--Forchheimer equations. The paper
establishes, for small sources, the existence of solutions on the continuous
and discrete level of lowest-order piecewise divergence-free Crouzeix--Raviart
finite elements. The vorticity employs a vector version of the pressure space
with normal and tangential velocity jump penalisation terms. A simple
Raviart--Thomas interpolant leads to pressure-robust a priori error estimates.
An explicit residual-based a posteriori error estimate allows for efficient and
reliable a posteriori error control. The efficiency for the Forchheimer
nonlinearity requires a novel discrete inequality of independent interest. The
implementation is based upon a light-weight forest-of-trees data structure
handled by a highly parallel set of adaptive {mesh refining} algorithms.
Numerical simulations reveal robustness of the a posteriori error estimates and
improved convergence rates by adaptive mesh-refining.

</details>


### [10] [Structure and asymptotic preserving deep neural surrogates for uncertainty quantification in multiscale kinetic equations](https://arxiv.org/abs/2506.10636)
*Wei Chen,Giacomo Dimarco,Lorenzo Pareschi*

Main category: math.NA

TL;DR: The paper proposes a multiscale control variates strategy and surrogate models (SAPNNs) to improve Monte Carlo sampling efficiency for high-dimensional kinetic equations with stochastic parameters, achieving better accuracy and computational performance.


<details>
  <summary>Details</summary>
Motivation: High dimensionality in kinetic equations with stochastic parameters makes uncertainty quantification computationally expensive, and traditional Monte Carlo methods suffer from slow convergence and high variance.

Method: A multiscale control variates strategy uses low-fidelity solutions to reduce variance. Surrogate models (SAPNNs) are introduced, designed to preserve physical properties and trained on low-fidelity models enriched with high-fidelity samples.

Result: The method achieves significant variance reduction while maintaining physical consistency and asymptotic accuracy, outperforming standard Monte Carlo techniques in accuracy and efficiency.

Conclusion: The proposed approach enables efficient large-scale prediction in kinetic uncertainty quantification, validated across homogeneous and nonhomogeneous regimes.

Abstract: The high dimensionality of kinetic equations with stochastic parameters poses
major computational challenges for uncertainty quantification (UQ). Traditional
Monte Carlo (MC) sampling methods, while widely used, suffer from slow
convergence and high variance, which become increasingly severe as the
dimensionality of the parameter space grows. To accelerate MC sampling, we
adopt a multiscale control variates strategy that leverages low-fidelity
solutions from simplified kinetic models to reduce variance. To further improve
sampling efficiency and preserve the underlying physics, we introduce surrogate
models based on structure and asymptotic preserving neural networks (SAPNNs).
These deep neural networks are specifically designed to satisfy key physical
properties, including positivity, conservation laws, entropy dissipation, and
asymptotic limits. By training the SAPNNs on low-fidelity models and enriching
them with selected high-fidelity samples from the full Boltzmann equation, our
method achieves significant variance reduction while maintaining physical
consistency and asymptotic accuracy. The proposed methodology enables efficient
large-scale prediction in kinetic UQ and is validated across both homogeneous
and nonhomogeneous multiscale regimes. Numerical results demonstrate improved
accuracy and computational efficiency compared to standard MC techniques.

</details>


### [11] [Alternating steepest descent methods for tensor completion with applications to spectromicroscopy](https://arxiv.org/abs/2506.10661)
*Oliver Townsend,Sergey Dolgov,Silvia Gazzola,Misha Kilmer*

Main category: math.NA

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this paper we develop two new Tensor Alternating Steepest Descent
algorithms for tensor completion in the low-rank $\star_{M}$-product format,
whereby we aim to reconstruct an entire low-rank tensor from a small number of
measurements thereof. Both algorithms are rooted in the Alternating Steepest
Descent (ASD) method for matrix completion, first proposed in [J. Tanner and K.
Wei, Appl. Comput. Harmon. Anal., 40 (2016), pp. 417-429]. In deriving the new
methods we target the X-ray spectromicroscopy undersampling problem, whereby
data are collected by scanning a specimen on a rectangular viewpoint with X-ray
beams of different energies. The recorded absorptions coefficients of the mixed
specimen materials are naturally stored in a third-order tensor, with spatial
horizontal and vertical axes, and an energy axis. To speed the X-ray
spectromicroscopy measurement process up, only a fraction of tubes from (a
reshaped version of) this tensor are fully scanned, leading to a tensor
completion problem. In this framework we can apply any transform (such as the
Fourier transform) to the tensor tube by tube, providing a natural way to work
with the $\star_{M}$-tensor algebra, and propose: (1) a tensor completion
algorithm that is essentially ASD reformulated in the $\star_{M}$-induced
metric space and (2) a tensor completion algorithm that solves a set of
(readily parallelizable) independent matrix completion problems for the frontal
slices of the transformed tensor. The two new methods are tested on real X-ray
spectromicroscopy data, demonstrating that they achieve the same reconstruction
error with fewer samples from the tensor compared to the matrix completion
algorithms applied to a flattened tensor.

</details>


### [12] [Semi-discrete moduli of smoothness and their applications in one- and two- sided error estimates](https://arxiv.org/abs/2506.10723)
*Danilo Costarelli,Donato Lavella*

Main category: math.NA

TL;DR: The paper introduces a new semi-discrete modulus of smoothness, generalizing prior work, and provides general error estimates using Steklov integrals. It extends applicability beyond trigonometric operators and offers sharper estimates than classical methods.


<details>
  <summary>Details</summary>
Motivation: To generalize and improve upon the semi-discrete modulus of smoothness introduced by Kolomoitsev and Lomako, enabling broader applications and sharper error estimates.

Method: Uses regularization and approximation properties of Steklov integrals, introduces a new K-functional, and establishes equivalence with the semi-discrete modulus.

Result: Achieves one- and two-sided error estimates for non-trigonometric operators, including Bernstein polynomials and Shannon sampling series, and introduces a Rathore-type theorem.

Conclusion: The new modulus and K-functional provide versatile tools for approximation theory, though open problems remain for algebraic Lagrange approximation.

Abstract: In this paper, we introduce a new semi-discrete modulus of smoothness, which
generalizes the definition given by Kolomoitsev and Lomako (KL) in 2023 (in the
paper published in the J. Approx. Theory), and we establish very general one-
and two- sided error estimates under non-restrictive assumptions. The proposed
results have been proved exploiting the regularization and approximation
properties of certain Steklov integrals introduced by Sendov and Popov in 1983,
and differ from the ones given by Kolomoitsev and Lomako. In addition, the
proof of the original KL approximation theorems were strictly related to the
application of certain classical results of the trigonometric best
approximation, and thus, they are applicable only for operators of the
trigonometric type. By the definition of semi-discrete moduli of smoothness
here proposed, we are able to deduce applications also for operators that are
not necessarily of the trigonometric type, and can also be used to derive
sharper estimates than those that can be achieved by the classical averaged
moduli of smoothness ($\tau$-moduli). Furthermore, a Rathore-type theorem is
established, and a new notion of K-functional is also introduced showing its
equivalence with the semi-discrete modulus of smoothness and its realization.
One-sided estimates of approximation can be established for classical operators
on bounded domains, such as the Bernstein polynomials. In the case of
approximation operators on the whole real line, one-sided estimates can be
achieved, e.g., for the Shannon sampling (cardinal) series, as well as for the
so-called generalized sampling operators. At the end of the paper, the case of
algebraic Lagrange approximation has been considered, showing the main open
problems in order to derive two-sided error estimates in this noteworthy case.

</details>


### [13] [Reduced-Order Time Splitting for Navier-Stokes with Open Boundaries](https://arxiv.org/abs/2506.10763)
*Mejdi Azaïez,Tomás Chacón Rebollo,Carlos Núñez Fernández,Samuele Rubino*

Main category: math.NA

TL;DR: The paper proposes a POD-ROM method for solving Navier-Stokes equations with open boundary conditions, combining time-splitting, domain reduction, and reduced order modeling. It compares intrusive and hybrid ROM approaches.


<details>
  <summary>Details</summary>
Motivation: To reduce computational time for solving Navier-Stokes equations by integrating time-splitting, domain reduction, and reduced order modeling.

Method: Combines time-splitting, non-standard boundary treatment, and POD-ROM with intrusive and hybrid (intrusive + data-driven) approaches.

Result: Comparative analysis shows efficiency and accuracy of the proposed ROMs in bifurcated tube flow and cylinder flow benchmarks.

Conclusion: The hybrid POD-ROM offers a balanced approach, combining accuracy and computational efficiency for solving Navier-Stokes equations.

Abstract: In this work, we propose a Proper Orthogonal Decomposition-Reduced Order
Model (POD-ROM) applied to time-splitting schemes for solving the Navier-Stokes
equations with open boundary conditions. In this method, we combine three
strategies to reduce the computing time to solve NSE: time splitting, reduction
of the computational domain through non-standard treatment of open boundary
conditions and reduced order modelling. To make the work self-contained, we
first present the formulation of the time-splitting scheme applied to the
Navier-Stokes equations with open boundary conditions, employing a first-order
Euler time discretization and deriving the non-standard boundary condition for
pressure. Then, we construct a Galerkin projection-based ROM using POD with two
different treatments of the pressure boundary condition on the outlet. We
propose a comparative performance analysis between the standard
projection-based POD-ROM (fully intrusive) and a hybrid POD-ROM that combines a
projection-based approach (intrusive) with a data-driven technique
(non-intrusive) using Radial Basis Functions (RBF). We illustrate this
comparison through two different numerical tests: the flow in a bifurcated tube
and the benchmark numerical test of the flow past cylinder, numerically
investigating the efficiency and accuracy of both ROMs.

</details>


### [14] [A Combined Parallel-in-time Direct Inverse (ParaDIn)-Parareal Method for Nonlinear Differential Equations](https://arxiv.org/abs/2506.10820)
*Subhash Paudel,Nail K. Yamaleev*

Main category: math.NA

TL;DR: The paper combines ParaDIn and Parareal methods to parallelize time integration in nonlinear PDEs, achieving significant speedup.


<details>
  <summary>Details</summary>
Motivation: To overcome constraints on parallel time levels in ParaDIn and enhance speedup by integrating it with the Parareal algorithm.

Method: Uses a block-Jacobi preconditioner with ParaDIn for each block, accelerated by Parareal as a two-level multigrid method in time. Both coarse- and fine-grid propagators are parallelized.

Result: Achieves up to 124x speedup on 480 cores compared to sequential BDF1 for 2-D nonlinear heat and Burgers equations.

Conclusion: The combined ParaDIn-Parareal method significantly improves parallel performance for time integration in nonlinear PDEs.

Abstract: As has been shown in our previous work, the parallel-in-time direct inverse
(ParaDIn) method introduced by Yamaleev and Paudel in (arXiv: 2406.00878v1,
2024) imposes some constraint on the maximum number of time levels, $N_t$, that
can be integrated in parallel. To circumvent this problem and further increase
the speedup, we combine the ParaDIn method with the Parareal algorithm to
efficiently parallelize the first-order time derivative term in nonlinear
partial differential equations discretized by the method of lines. The main
idea of the proposed approach is to use a block-Jacobi preconditioner, so that
each block is solved by using the ParaDIn method. To accelerate the convergence
of Jacobi iterations, we use the Parareal method which can be interpreted as a
two-level multigrid method in time. In contrast to the conventional Parareal
algorithm whose coarse grid correction step is performed sequentially, both the
coarse- and fine-grid propagators in the proposed approach are implemented in
parallel by using the ParaDIn method, thus significantly increasing the
parallel performance of the combined algorithm. Numerical results show that the
new combined ParaDIn-Parareal method provides the speedup of up to 124 on 480
computing cores as compared with the sequential first-order implicit backward
difference (BDF1) scheme for the 2-D nonlinear heat and Burgers equations with
both smooth and discontinuous solutions.

</details>


### [15] [Numerical approximation of a PDE-constrained Optimization problem that appears in Data-Driven Computational Mechanics](https://arxiv.org/abs/2506.10894)
*Pedro B. Bazon,Cristian G. Gebhardt,Gustavo C. Buscaglia,Roberto F. Ausas*

Main category: math.NA

TL;DR: The paper addresses an optimization problem in Data-Driven Computational Mechanics for diffusion-reaction, ensuring physical and geometrical constraints, and validates the approach numerically.


<details>
  <summary>Details</summary>
Motivation: To find continuous primal fields closest to predefined discrete material data while satisfying conservation laws and compatibility.

Method: Proposes stable finite element discretizations preserving the saddle-point structure and allowing equal-order interpolation.

Result: Demonstrates well-posedness in continuous setting and effectiveness through numerical examples.

Conclusion: The method is effective for solving the optimization problem in diffusion-reaction contexts.

Abstract: We investigate an optimization problem that arises when working within the
paradigm of Data-Driven Computational Mechanics. In the context of the
diffusion-reaction problem, such an optimization problem seeks for the
continuous primal fields (gradient and flux) that are closest to some
predefined discrete fields taken from a material data set. The optimization is
performed over primal fields that satisfy the physical conservation law and the
geometrical compatibility. We consider a reaction term in the conservation law,
which has the effect of coupling all the optimality conditions. We first
establish the well-posedness in the continuous setting. Then, we propose stable
finite element discretizations that consistently approximate the continuous
formulation, preserving its saddle-point structure and allowing for equal-order
interpolation of all fields. Finally, we demonstrate the effectiveness of the
proposed methods through a set of numerical examples.

</details>


### [16] [Accelerating Newton-Schulz Iteration for Orthogonalization via Chebyshev-type Polynomials](https://arxiv.org/abs/2506.10935)
*Ekaterina Grishina,Matvey Smirnov,Maxim Rakhuba*

Main category: math.NA

TL;DR: The paper introduces Chebyshev-optimized Newton-Schulz (CANS) to improve orthogonal matrix approximation by deriving optimal coefficients and using higher-degree polynomials for better performance in deep learning applications.


<details>
  <summary>Details</summary>
Motivation: Existing Newton-Schulz iteration for orthogonal approximation has fixed coefficients, limiting its optimization for specific matrices. The paper aims to address this by optimizing the coefficients.

Method: The authors propose CANS, using Chebyshev's alternance theorem to derive optimal coefficients for the 3rd-order Newton-Schulz iteration and apply the Remez algorithm for higher-degree polynomials.

Result: The method is shown effective in orthogonalization for the Muon optimizer and as a retraction alternative in Riemannian optimization on the Stiefel manifold.

Conclusion: CANS provides a more optimized and efficient solution for orthogonal matrix approximation, enhancing performance in key applications.

Abstract: The problem of computing optimal orthogonal approximation to a given matrix
has attracted growing interest in machine learning. Notable applications
include the recent Muon optimizer or Riemannian optimization on the Stiefel
manifold. Among existing approaches, the Newton-Schulz iteration has emerged as
a particularly effective solution, as it relies solely on matrix
multiplications and thus achieves high computational efficiency on GPU
hardware. Despite its efficiency, the method has inherent limitations - its
coefficients are fixed and thus not optimized for a given matrix. In this paper
we address this issue by proposing a Chebyshev-optimized version of
Newton-Schulz (CANS). Based on the Chebyshev's alternance theorem, we
theoretically derive optimal coefficients for the 3-rd order Newton-Schulz
iteration and apply a Remez algorithm to compute optimal higher-degree
polynomials. We leverage these polynomials to construct controlled approximate
orthogonalization schemes, which is of interest in deep learning applications.
Practically, we demonstrate the method's effectiveness in two key applications:
orthogonalization in the Muon optimizer, and providing an efficient retraction
alternative for Riemannian optimization on the Stiefel manifold.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [17] [Wave-front tracking for a quasi-linear scalar conservation law with hysteresis II: the case of Preisach](https://arxiv.org/abs/2506.10087)
*Fabio Bagagiolo,Stefan Moreti*

Main category: math.AP

TL;DR: The paper extends the analysis of a quasi-linear scalar conservation law with hysteresis from the Play operator to the more versatile Preisach operator, addressing the Cauchy problem using wave-front tracking and an entropy condition for uniqueness.


<details>
  <summary>Details</summary>
Motivation: The study aims to generalize previous work on hysteresis in conservation laws by incorporating the Preisach operator, which better models real-world hysteresis due to internal variables.

Method: The authors use the wave-front tracking method to solve the Cauchy problem for bounded variation initial data and apply an entropy-like condition to ensure solution uniqueness.

Result: The analysis successfully extends the framework to the Preisach operator, providing a method to handle its non-local features and ensuring unique solutions under the entropy condition.

Conclusion: The work advances the understanding of hysteresis in conservation laws by adapting techniques for the Preisach operator, highlighting its applicability in practical scenarios.

Abstract: We consider the Cauchy problem for the quasi-linear scalar conservation law
\[u_t+\mathcal{F}(u)_t+u_x=0,\] where $\mathcal{F}$ is a specific hysteresis
operator. Hysteresis models a rate-independent memory relationship between the
input $u$ and its output, giving a non-local feature to the equation. In a
previous work the authors studied the case when $\mathcal{F}$ is the Play
operator. In the present article, we extend the analysis to the case of
Preisach operator, which is probably the most versatile mathematical model to
the describe hysteresis in the applications, especially for the presence of
some kind of internal variables. This fact has required a new analysis of the
equation. Starting from the Riemann problem, we address the so-called
wave-front tracking method for a solution to the Cauchy problem with bounded
variation initial data. An entropy-like condition is also exploited for
uniqueness.

</details>


### [18] [Well--posedness for the biharmonic scattering problem for a penetrable obstacle](https://arxiv.org/abs/2506.10176)
*Rafael Ceja Ayala,Isaac Harris,Tonatiuh Sánchez-Vizuet*

Main category: math.AP

TL;DR: The paper analyzes the scattering problem for a penetrable obstacle in a 2D elastic plate, using the biharmonic wave equation and coupled Helmholtz equations, proving well-posedness and reciprocity.


<details>
  <summary>Details</summary>
Motivation: To understand wave scattering in thin elastic plates, which is relevant for structural and acoustic engineering applications.

Method: Uses the biharmonic wave equation in the frequency domain, coupled with Helmholtz and modified Helmholtz equations, and employs operator factorization.

Result: Establishes well-posedness and reciprocity relations, supported by numerical validation.

Conclusion: The theoretical framework is validated, providing insights into wave scattering in elastic plates.

Abstract: We address the direct scattering problem for a penetrable obstacle in an
infinite elastic two--dimensional Kirchhoff--Love plate. Under the assumption
that the plate's thickness is small relative to the wavelength of the incident
wave, the propagation of perturbations on the plate is governed by the
two-dimensional biharmonic wave equation, which we study in the frequency
domain. With the help of an operator factorization, the scattering problem is
analyzed from the perspective of a coupled boundary value problem involving the
Helmholtz and modified Helmholtz equations. Well--posedness and reciprocity
relations for the problem are established. Numerical examples for some special
cases are provided to validate the theoretical findings.

</details>


### [19] [Optimal decay of global strong solutions to nematic liquid crystal flows in the half-space](https://arxiv.org/abs/2506.10255)
*Haokun Chen,Yong Wang*

Main category: math.AP

TL;DR: The paper analyzes the decay rates of higher-order spatial and first-order time derivatives for nematic liquid crystal flows in a half-space, showing faster decay than the heat kernel under specific initial conditions.


<details>
  <summary>Details</summary>
Motivation: To understand the asymptotic behavior of derivatives in nematic liquid crystal flows and improve decay rate estimates.

Method: Uses $L^p-L^q$ estimates of the Stokes semigroup, a priori estimates of the steady Stokes system, and the Leray projection operator's representation.

Result: Faster decay rates than the heat kernel are achieved for initial data in a weighted Sobolev space.

Conclusion: The study provides improved decay estimates for nematic liquid crystal flows in a half-space, leveraging advanced analytical tools.

Abstract: We study asymptotic behaviors of the higher-order spatial derivatives and the
first-order time derivatives for the strong solution to nematic liquid crystal
flows in the half-space $\mathbb{R}_+^3$. Furthermore, when the initial data
lie in an appropriately weighted Sobolev space, we obtain the decay rates that
are faster than the heat kernel. The main tools employed in this paper are the
$L^p-L^q$ estimates of the Stokes semigroup, the a priori estimates of the
steady Stokes system in $\mathbb{R}_+^3$, and the representation formula of the
Leray projection operator.

</details>


### [20] [Generalized Poisson kernel and solution of the Dirichlet problem for the radial Schrödinger equation](https://arxiv.org/abs/2506.10273)
*Víctor A Vicente-Benítez*

Main category: math.AP

TL;DR: The paper presents an explicit solution to the Dirichlet boundary value problem for the radial Schrödinger equation in a unit ball with a complex potential, using orthogonal solutions and formal spherical polynomials. It also introduces a generalized Poisson kernel and analyzes solvability and uniqueness conditions.


<details>
  <summary>Details</summary>
Motivation: The study aims to solve the Dirichlet problem for the radial Schrödinger equation with a complex potential, addressing challenges in constructing explicit solutions and ensuring solvability and uniqueness.

Method: The solution is constructed using an explicit orthogonal set of solutions for the radial equation and expressed as a series expansion in formal spherical polynomials. The generalized Poisson kernel is introduced and analyzed.

Result: Conditions for solvability and uniqueness of the Dirichlet problem are established. The solution is extended to cases where the boundary condition is a complex Radon measure.

Conclusion: The paper provides a rigorous framework for solving the Dirichlet problem for the radial Schrödinger equation, including the introduction of the generalized Poisson kernel and conditions for distributional boundary values.

Abstract: We present an explicit construction of the solution to the Dirichlet boundary
value problem for the radial Schr\"odinger equation in the unit ball, with a
complex-valued potential $V$ satisfying the condition
$\int_0^1r|V(r)|dr<\infty$. The solution is based on the construction of an
explicit orthogonal set of solutions for the radial equation. In the case of a
Dirichlet problem with boundary data in $W^{\frac{1}{2},2}(\mathbb{S}^{d-1})$,
the solution is expressed as a series expansion in terms of the so-called
formal spherical polynomials. We establish conditions for the solvability and
uniqueness of the Dirichlet problem. Based on this series representation, we
introduce the concept of generalized Poisson kernel, develop its main
properties, and investigate the conditions under which the Dirichlet problem,
with a boundary condition being a complex Radon measure on $\mathbb{S}^{d-1}$,
admits a solution in the sense of a distributional boundary values.

</details>


### [21] [Mixtures of nonhomogeneous viscoelastic incompressible fluids governed by the Kelvin-Voigt equations](https://arxiv.org/abs/2506.10278)
*S. N. Antontsev,H. B. de Oliveira,I. V. Kuznetsov,D. A. Prokudin,Kh. Khompysh*

Main category: math.AP

TL;DR: The paper studies a Kelvin-Voigt system for a mixture of n incompressible, viscoelastic fluids with non-constant density, proving global weak solutions and uniqueness under certain conditions.


<details>
  <summary>Details</summary>
Motivation: To address the mathematical challenges of modeling mixtures of viscoelastic fluids with variable density and establish rigorous solutions.

Method: Investigates the initial-and boundary-value problem for the Kelvin-Voigt system, focusing on weak solutions for velocity, density, and pressure.

Result: Existence of global-in-time weak solutions is proven; uniqueness is demonstrated under additional regularity assumptions.

Conclusion: The work provides a foundation for understanding complex fluid mixtures with viscoelastic properties and variable density.

Abstract: An initial-and boundary-value problem for the Kelvin-Voigt system, modeling a
mixture of n incompressible and viscoelastic fluids, with non-constant density,
is investigated in this work. The existence of global-in-time weak solutions is
established: velocity, density and pressure. Under additional regularity
assumptions, we also prove the uniqueness of the solution.

</details>


### [22] [Exponential mixing for the randomly forced NLS equation](https://arxiv.org/abs/2506.10318)
*Yuxuan Chen,Shengquan Xiang,Zhifei Zhang,Jia-Cheng Zhao*

Main category: math.AP

TL;DR: The paper explores exponential mixing in the invariant measure for a randomly forced nonlinear Schrödinger equation, highlighting the importance of exponential asymptotic compactness and control properties.


<details>
  <summary>Details</summary>
Motivation: To understand the ergodic properties of random dynamical systems, particularly for dispersive equations with damping and localized noise.

Method: Focuses on exponential asymptotic compactness and control properties to analyze the system.

Result: Extends prior work on the statistical behavior of randomly forced dispersive equations.

Conclusion: The study advances understanding of ergodic properties in such systems, building on previous research.

Abstract: This paper investigates exponential mixing of the invariant measure for
randomly forced nonlinear Schr\"{o}dinger equation, with damping and random
noise localized in space. Our study emphasizes the crucial role of exponential
asymptotic compactness and control properties in establishing the ergodic
properties of random dynamical systems. This work extends the series [15, 45]
on the statistical behavior of randomly forced dispersive equations.

</details>


### [23] [On the sharp critical mass threshold for the 3D Patlak-Keller-Segel-Navier-Stokes system via Couette flow](https://arxiv.org/abs/2506.10578)
*Shikun Cui,Lili Wang,Wendong Wang,Juncheng Wei*

Main category: math.AP

TL;DR: The paper investigates the suppression of blow-up in the 3D Patlak-Keller-Segel-Navier-Stokes system using Couette flow, proving global solutions for initial cell mass below 16π² when the flow is strong enough.


<details>
  <summary>Details</summary>
Motivation: To understand how Couette flow can prevent blow-up in the 3D Patlak-Keller-Segel-Navier-Stokes system and identify the critical mass threshold.

Method: Combines quasi-linear methods with zero-mode estimates using logarithmic Hardy-Littlewood-Sobolev inequality to analyze density and velocity.

Result: Solutions are global if the Couette flow is strong (large A) and initial cell mass is below 16π².

Conclusion: The critical mass threshold is sharp, and the method provides insights into blow-up suppression in similar systems.

Abstract: As is well-known, the solution of the Patlak-Keller-Segel system in 3D may
blow up in finite time regardless of any initial cell mass. In this paper, we
are interested in the suppression of blow-up and the critical mass threshold
for the 3D Patlak-Keller-Segel-Navier-Stokes system via the Couette flow $(Ay,
0, 0)$. It is proved that if the Couette flow is sufficiently strong ($A$ is
large enough), then the solutions for the system are global in time in the
periodic domain $(x,y,z)\in\mathbb{T}^{3}$ as long as the initial cell mass is
less than $16\pi^{2}$. This result seems to be sharp, since the zero-mode
function (the mean value in $x-$direction) of the three dimensional density is
a complication of the two-dimensional Keller-Segel equations, whose critical
mass in 2D is $8\pi$. One new observation is the dissipative decay of
$(\widetilde{u}_{2,0},\widetilde{u}_{3,0})$ (see Lemma 4.3 for more details),
then we combine the quasi-linear method proposed by Wei-Zhang (Comm. Pure Appl.
Math., 2021) with the zero-mode estimate of the density by the logarithmic
Hardy-Littlewood-Sobolev inequality as Bedrossian-He (SIAM J. Math. Anal.,
2017) or He (Nonlinearity, 2025) to obtain the bounded-ness of the density and
the velocity.

</details>


### [24] [On local well-posedness for the nonlinear Schrödinger equation with general power nonlinearity](https://arxiv.org/abs/2506.10595)
*Lucia Arens,Marius Gritl*

Main category: math.AP

TL;DR: The paper compares semigroup theory and Strichartz estimates for local well-posedness of nonlinear Schrödinger equations, extending results beyond the case p=2.


<details>
  <summary>Details</summary>
Motivation: To explore and compare two mathematical approaches (semigroup theory and Strichartz estimates) for solving nonlinear Schrödinger equations, which are key in quantum mechanics and Bose-Einstein condensation.

Method: Uses semigroup theory for a functional analysis framework and Strichartz estimates for dispersive equations, applying both to nonlinearities of the form F(u)=λ|u|^p u.

Result: Demonstrates the applicability and comparison of both methods, extending results to cases where p≠2.

Conclusion: Both approaches are effective, with semigroup theory offering elegance and generality, while Strichartz estimates provide refined space-time analysis.

Abstract: The nonlinear Schr\"odinger equation plays a fundamental role in mathematical
physics, particularly in the study of quantum mechanics and Bose-Einstein
condensation. This paper explores two distinct approaches to establishing the
local well-posedness of solutions: the semigroup theory ansatz and the
Strichartz estimates ansatz. Semigroup theory provides a general and elegant
framework rooted in functional analysis, allowing for the interpretation of the
time evolution of solutions as operator semigroups. Strichartz estimates,
developed specifically for dispersive equations, offer an alternative technique
based on refined space-time estimates and fixed-point arguments. We
systematically analyze and compare both approaches and apply them to nonlinear
Schr\"odinger equations where the nonlinearity is given by $F(u)=\lambda|u|^p
u$ for some $\lambda \in \mathbb{R}$. So our results extend beyond the
physically relevant case $p=2$.

</details>


### [25] [Harnack inequality for degenerate fully nonlinear parabolic equations](https://arxiv.org/abs/2506.10608)
*Vedansh Arya,Vesa Julin*

Main category: math.AP

TL;DR: The paper proves intrinsic and weak Harnack inequalities for nonnegative solutions and supersolutions of degenerate fully nonlinear parabolic equations, extending results from divergence form to nondivergence form operators.


<details>
  <summary>Details</summary>
Motivation: To generalize the p-parabolic equation (p>2) to nondivergence form operators and establish Harnack inequalities, bridging a gap in existing literature.

Method: The authors analyze degenerate fully nonlinear parabolic equations, focusing on nonnegative solutions and supersolutions, and derive intrinsic and weak Harnack inequalities.

Result: Proven intrinsic Harnack inequality for nonnegative solutions and weak Harnack inequality for nonnegative supersolutions.

Conclusion: The results extend prior work on divergence form operators to nondivergence form, providing new tools for analyzing such equations.

Abstract: We consider degenerate fully nonlinear parabolic equations, which generalize
the p-parabolic equation with $p>2$ to nondivergence form operators. We prove
an intrinsic Harnack inequality for nonnegative solutions and a weak Harnack
inequality for nonnegative supersolutions. These results can be seen as the
nondivergence form counterparts of the results by DiBenedetto, Gianazza and
Vespri (Acta Math. 2008) and Kuusi (Ann. Sc. Norm. Super. Pisa 2008).

</details>


### [26] [Cazenave-Dickstein-Weissler-type extension of Fujita's problem on Heisenberg groups](https://arxiv.org/abs/2506.10611)
*Mokhtar Kirane,Ahmad Z. Fino,Berikbol T. Torebek,Zineb Sabbagh*

Main category: math.AP

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper examines the critical exponents for the existence of global
solutions to the equation \begin{equation*} \begin{array}{ll} \displaystyle
u_t-\Delta_{\mathbb{H}}u=\int_0^t(t-s)^{-\gamma}|u(s)|^{p-1}u(s)\,ds,&\qquad
0\leq\gamma<1,\,\,\, {\eta\in \mathbb{H}^n,\,\,\,t>0,}
\end{array}\end{equation*} on the Heisenberg groups $\mathbb{H}^n.$ There
exists a critical exponent $$p_c=
\max\Big\{\frac{1}{\gamma},p_\gamma\Big\}\in(0,+\infty],\quad\hbox{with}\quad
p_\gamma=1+\frac{2(2-\gamma)}{Q-2+2\gamma},\,\,Q=2n+2$$ such that for all
$1<p\leq p_c,$ no global solution exists regardless of the non-negative initial
data, while for $p>p_c$, a global positive solution exists if the initial data
is sufficiently small. The results obtained are a natural extension of the
results of Cazenave et al. [Nonlinear Analysis 68 (2008), 862-874], where
similar studies were carried out in $\mathbb{R}^n$. Also given are several
theorems concerning the lifespan estimates of local solutions for different
cases of initial data. The proofs of the main results are based on test
function methods and Banach fixed point principle.

</details>


### [27] [Stability of the Morse Index for the $p$-harmonic Approximation of Harmonic Maps into Homogeneous Spaces](https://arxiv.org/abs/2506.10761)
*Dominik Schlagenhauf*

Main category: math.AP

TL;DR: The paper extends previous results on Morse index stability for Sacks-Uhlenbeck sequences into spheres to homogeneous spaces, incorporating new strategies and proving upper semicontinuity of the Morse index plus nullity.


<details>
  <summary>Details</summary>
Motivation: To generalize the stability of the Morse index for Sacks-Uhlenbeck sequences from spheres to homogeneous spaces, leveraging prior work and new methodologies.

Method: Extends previous techniques by incorporating strategies from Bayer and Roberts, focusing on energy identity and no neck property for harmonic maps into homogeneous manifolds.

Result: Upper semicontinuity of the Morse index plus nullity is proven, along with improved gradient estimates in neck regions around blow-up points.

Conclusion: The study successfully generalizes Morse index stability to homogeneous spaces, providing new insights into the behavior of critical points in these settings.

Abstract: In the joint work of the author with Da Lio and Rivi\`ere (Morse Index
Stability for Sequences of Sacks-Uhlenbeck Maps into a Sphere) we studied the
stability of the Morse index for Sacks-Uhlenbeck sequences into spheres as
$p\searrow2$. These are critical points of the energy $E_p(u) := \int_\Sigma
\left( 1+|\nabla u|^2\right)^{p/2} \ dvol_\Sigma,$ where $u:\Sigma \rightarrow
S^n$ is a map from a closed Riemannian surface $\Sigma$ into a sphere $ S^n$.
In this paper we extend the results found in our previous work to the case of
Sacks-Uhlenbeck sequences into homogeneous spaces, by incorporating the
strategy introduced by Bayer and Roberts (Energy identity and no neck property
for $\epsilon$-harmonic and $\alpha$-harmonic maps into homogeneous target
manifolds). In the spirit of the work of Da Lio, Gianocca and Rivi\`ere (Morse
Index Stability for Critical Points to Conformally invariant Lagrangians), we
show in this setting the upper semicontinuity of the Morse index plus nullity
and an improved pointwise estimate of the gradient in the neck regions around
blow up points.

</details>


### [28] [New class of time-periodic solutions to the 1D cubic wave equation](https://arxiv.org/abs/2506.10839)
*Filip Ficek,Maciej Maliborski*

Main category: math.AP

TL;DR: The paper confirms the existence of time-periodic solutions for the defocusing cubic wave equation on a 1D interval with Dirichlet boundary conditions, using rigorous construction and rational arithmetic computations.


<details>
  <summary>Details</summary>
Motivation: To validate earlier findings suggesting a new class of time-periodic solutions for the defocusing cubic wave equation.

Method: Rigorous construction of solutions using rational arithmetic computations to verify operator bounds.

Result: Existence of the proposed time-periodic solutions is confirmed.

Conclusion: The study successfully constructs and verifies the existence of the new class of solutions, supporting prior claims.

Abstract: In recent papers (arXiv:2407.16507, arXiv:2408.05158) we presented results
suggesting the existence of a new class of time-periodic solutions to the
defocusing cubic wave equation on a one-dimensional interval with Dirichlet
boundary conditions. Here we confirm these findings by rigorously constructing
solutions from this class. The proof uses rational arithmetic computations to
verify essential operator bounds.

</details>


### [29] [Normalized solutions for a Sobolev critical quasilinear Schrödinger equation](https://arxiv.org/abs/2506.10870)
*Yuxin Li,Meijie Yang,Xiaojun Chang*

Main category: math.AP

TL;DR: The paper studies normalized solutions for a quasilinear Schrödinger equation with critical exponent, providing existence and non-existence results across different parameter ranges.


<details>
  <summary>Details</summary>
Motivation: To address the existence of normalized solutions under mass constraints for a critical quasilinear Schrödinger equation, filling gaps in the literature.

Method: Derives energy level estimates, establishes convergence theorems, and applies perturbation methods to analyze solutions.

Result: Existence of solutions (local minimizer, mountain pass type, ground state) for specific parameter ranges, and non-existence for others.

Conclusion: Comprehensive analysis for the entire parameter range, with flexible methods applicable to broader nonlinearities.

Abstract: In this paper, we study the existence of normalized solutions for the
following quasilinear Schr\"odinger equation with critical exponent:
  \begin{equation*}
  -\Delta u-u\Delta (u^2)+\lambda
u=\tau|u|^{q-2}u+|u|^{2\cdot2^*-2}u,~~~~x\in\R^N,
  \end{equation*}
  under the mass constraint $\int_{\R^N}|u|^2dx=c$ for some prescribed $c>0$.
Here $\tau\in \mathbb{R}$ is a parameter, $\lambda\in\R$ appears as a Lagrange
multiplier, $N\ge3$, $2^*:=\frac{2N}{N-2}$ and $2<q<2\cdot2^*$. By deriving
precise energy level estimates and establishing new convergence theorems, we
apply the perturbation method to establish several existence results for
$\tau>0$ in the Sobolev critical regime:
  [label=(\alph*)]
  \item For the case of $2<q<2+\frac{4}{N}$, we obtain the existence of two
solutions, one of which is a local minimizer, and the other is a mountain pass
type solution, under explicit conditions on $c>0$;
  \item For the case of $2+\frac{4}{N}\leq q<4+\frac{4}{N}$, we obtain the
existence of normalized solutions of mountain pass type under different
conditions on $c>0$;
  \item For the case of $4+\frac{4}{N}\leq q<2\cdot2^*$, we obtain the
existence of a ground state normalized solution under different conditions on
$c>0$. Moreover, when $\tau\le 0$, we derive the non-existence result for
$2<q<2\cdot2^*$ and all $c>0$. Our research provides a comprehensive analysis
across the entire range $q\in(2, 2 \cdot 2^*)$ and for all $N\ge3$. The methods
we have developed are flexible and can be extended to a broader class of
nonlinearities.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [30] [Phase-Space Engineering and Dynamical Long-Range Order in Memcomputing](https://arxiv.org/abs/2506.10149)
*Chesson Sipling,Yuan-Hang Zhang,Massimiliano Di Ventra*

Main category: physics.comp-ph

TL;DR: The paper explores how hyper-parameters in Digital Memcomputing Machines (DMMs) affect phase-space efficiency for solving combinatorial optimization problems. Efficient phase-space exploration is linked to long-range correlations, while poor parameter choices hinder performance.


<details>
  <summary>Details</summary>
Motivation: To understand how hyper-parameters and memory dynamics in DMMs influence their computational efficiency in solving combinatorial optimization problems.

Method: Numerical simulations on a prototypical DMM to analyze the impact of hyper-parameters on phase-space geometry and solution search efficiency.

Result: DMMs explore phase space efficiently with well-chosen parameters, aided by long-range correlations. Poor parameters reduce efficiency, but dynamical long-range order often persists.

Conclusion: Memory and hyper-parameters are crucial for optimizing DMMs' computational efficiency, with long-range correlations playing a key role.

Abstract: Digital Memcomputing machines (DMMs) are dynamical systems with memory (time
non-locality) that have been designed to solve combinatorial optimization
problems. Their corresponding ordinary differential equations depend on a few
hyper-parameters that define both the system's relevant time scales and its
phase-space geometry. Using numerical simulations on a prototypical DMM, we
analyze the role of these physical parameters in engineering the phase space to
either help or hinder the solution search by DMMs. We find that the DMM
explores its phase space efficiently for a wide range of parameters, aided by
the long-range correlations in their fast degrees of freedom that emerge
dynamically due to coupling with the (slow) memory degrees of freedom. In this
regime, the time it takes for the system to find a solution scales well as the
number of variables increases. When these hyper-parameters are chosen poorly,
the system navigates its phase space far less efficiently. However, we find
that, in many cases, dynamical long-range order (DLRO) persists even when the
phase-space exploration process is inefficient. DLRO only disappears if the
memories are made to evolve as quickly as the fast degrees of freedom. This
study points to the important role of memory and hyper-parameters in
engineering the DMMs' phase space for optimal computational efficiency.

</details>


### [31] [Unravelling the mystery of enhanced open-circuit voltages in nanotextured perovskite solar cells](https://arxiv.org/abs/2506.10691)
*Dilara Abdel,Jacob Relle,Thomas Kirchartz,Patrick Jaap,Jürgen Fuhrmann,Sven Burger,Christiane Becker,Klaus Jäger,Patricio Farrell*

Main category: physics.comp-ph

TL;DR: Nanotextures in perovskite solar cells enhance open-circuit voltage by reducing SRH recombination, not optical effects, with an optimal texture height for efficiency.


<details>
  <summary>Details</summary>
Motivation: To understand why nanotextures improve open-circuit voltage in perovskite solar cells, a phenomenon not fully explained.

Method: Combined multi-dimensional optical and charge-transport simulations for a single-junction perovskite solar cell.

Result: Texturing reduces SRH recombination by increasing charge imbalance, with an optimal texture height for realistic cells.

Conclusion: Texturing offers opto-electronic benefits, guiding design for next-gen perovskite devices like solar cells and LEDs.

Abstract: Perovskite solar cells have reached power conversion efficiencies that rival
those of established silicon photovoltaic technologies. Nanotextures in
perovskite solar cells optimise light trapping and scattering, thereby
improving optical absorption. In addition, nanotextures have been
experimentally shown to enhance electronic performance, in particular, by
increasing the open-circuit voltage $V_{\text{OC}}$ -- a phenomenon that, until
now, has remained not fully understood. This study investigates the underlying
reasons by combining multi-dimensional optical and charge-transport simulations
for a single-junction perovskite solar cell. Our results reveal that the
increased open-circuit voltage is not driven by optical effects but by the
textured geometry itself. For voltages near $V_{\text{OC}}$, texturing one of
the absorber/transport layer interfaces increases the imbalance between
electron and hole densities in the absorber, thereby reducing
Shockley-Read-Hall (SRH) recombination, which is the dominant loss mechanism in
this study. While idealised solar cells benefit unconditionally from increasing
texture height, in realistic cells there is an optimal texture height which
maximizes the power conversion efficiency. These findings provide new insights
into the opto-electronic advantages of texturing and offer guidance for the
design of next-generation textured perovskite-based solar cells, light emitting
diodes, and photodetectors.

</details>


### [32] [Distillation of atomistic foundation models across architectures and chemical domains](https://arxiv.org/abs/2506.10956)
*John L. A. Gardner,Daniel F. Thomas du Toit,Chiheb Ben Mahmoud,Zoé Faure Beaulieu,Veronika Juraskova,Laura-Bianca Paşca,Louise A. M. Rosset,Fernanda Duarte,Fausto Martelli,Chris J. Pickard,Volker L. Deringer*

Main category: physics.comp-ph

TL;DR: The paper demonstrates how distillation via synthetic data can efficiently transfer knowledge from large atomistic foundation models to smaller, faster potentials, achieving significant speed-ups and broad applicability.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency and resource-intensive nature of current atomistic foundation models, enabling their practical use in scientific research.

Method: Distillation via synthetic data is used to transfer knowledge from foundation models to smaller architectures, including graph-network and atomic cluster expansion frameworks.

Result: Achieved speed-ups of >10× and >100×, with applicability across diverse chemical and materials domains.

Conclusion: Distillation enables efficient use of atomistic foundation models in real-world research, making them more accessible and practical.

Abstract: Machine-learned interatomic potentials have transformed computational
research in the physical sciences. Recent atomistic `foundation' models have
changed the field yet again: trained on many different chemical elements and
domains, these potentials are widely applicable, but comparably slow and
resource-intensive to run. Here we show how distillation via synthetic data can
be used to cheaply transfer knowledge from atomistic foundation models to a
range of different architectures, unlocking much smaller, more efficient
potentials. We demonstrate speed-ups of $> 10\times$ by distilling from one
graph-network architecture into another, and $> 100\times$ by leveraging the
atomic cluster expansion framework. We showcase applicability across chemical
and materials domains: from liquid water to hydrogen under extreme conditions;
from porous silica and a hybrid halide perovskite solar-cell material to
modelling organic reactions. Our work shows how distillation can support the
routine and computationally efficient use of current and future atomistic
foundation models in real-world scientific research.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [33] [Synergistic control of radical generation in a radio frequency atmospheric pressure plasma jet via voltage waveform tailoring and structured electrodes](https://arxiv.org/abs/2506.10099)
*Mate Vass,Xiaokun Wang,Ihor Korolov,Julian Schulze,Thomas Mussenbrock*

Main category: physics.plasm-ph

TL;DR: The study explores how combining tailored voltage waveforms with structured electrodes in an RF microplasma jet enhances localized electron power absorption and radical generation, depending on waveform asymmetry.


<details>
  <summary>Details</summary>
Motivation: To investigate the synergy between voltage waveform tailoring and electrode structuring for spatial control of plasma properties.

Method: Uses 2D plasma fluid simulations and optical diagnostics (PROES, TDLAS) with asymmetric waveforms and rectangular trenches in electrodes.

Result: Asymmetric waveforms with structured electrodes create strong, localized electron power absorption and radical generation at one electrode, unlike symmetric excitation.

Conclusion: The interplay between waveform-induced sheath dynamics and geometric focusing enables selective, reversible enhancement of electron power absorption at a chosen location.

Abstract: The synergy between voltage waveform tailoring and structured electrodes is
investigated in a radio-frequency (RF) atmospheric-pressure microplasma jet
operated in helium with a 0.1% oxygen admixture. The device incorporates
rectangular trenches in both electrodes and is driven by "Peaks" and "Valleys"
waveforms synthesized from four harmonics (base frequency $f_{\rm b} =
13.56$~MHz, $V_{\rm pp} = 500$~V, $P=$1.2~W). Two-dimensional plasma fluid
simulations, together with spatially and temporally resolved optical
diagnostics (Phase-Resolved Optical Emission Spectroscopy and Tunable Diode
Laser Absorption Spectroscopy), are used to demonstrate that the combination of
asymmetric voltage waveforms with electrode structuring leads to strong spatial
localization of electron power absorption and radical generation. This synergy
results in a single pronounced maximum inside a trench at either the powered or
grounded electrode, depending on the applied waveform, unlike a symmetric
excitation, which produces a spatially symmetric enhancement at both
electrodes. The effect is attributed to the interplay between waveform-induced
sheath dynamics and geometric focusing provided by the trenches, enabling
electrically reversible and selective enhancement of electron power absorption
at a chosen location.

</details>


### [34] [Exact series expansion for even frequency moments of the dynamic structure factor](https://arxiv.org/abs/2506.10410)
*Panagiotis Tolias,Jan Vorberger,Tobias Dornheim*

Main category: physics.plasm-ph

TL;DR: Derived an exact series for even frequency moments of the dynamic structure factor, with truncations for evaluating unknown moments in the finite-temperature electron gas, validated via simulations.


<details>
  <summary>Details</summary>
Motivation: To address the lack of explicit knowledge of higher frequency moments (second, fourth, fifth) in the finite-temperature uniform electron gas.

Method: Proposed truncations of the exact series, validated using the non-interacting limit and path integral Monte Carlo simulations.

Result: Determined the applicability range of the truncations in terms of degeneracy parameter and wavenumber.

Conclusion: The derived truncations provide a practical way to evaluate unknown frequency moments, validated by comparison with quasi-exact simulations.

Abstract: An exact series representation of the even frequency moments of the dynamic
structure factor is derived. Truncations are proposed that allow to evaluate
the explicitly unknown second, fourth and fifth frequency moments for the
finite temperature uniform electron gas. Their applicability range in terms of
degeneracy parameter and wavenumber is determined by exploiting the
non-interacting limit and by comparing with the quasi-exact results of path
integral Monte Carlo simulations.

</details>


### [35] [Runaway electron-induced plasma facing component damage in tokamaks](https://arxiv.org/abs/2506.10411)
*S. Ratynskaia,M. Hoelzl,E. Nardon,P. Aleynikov,F. J. Artola,V. Bandaru,M. Beidler,B. Breizman,D. del-Castillo-Negrete,M. De Angeli,V. Dimitriou,R. Ding,J. Eriksson,O. Ficker,R. S. Granetz,E. Hollmann,M. Hoppe,M. Houry,I. Jepu,H. R. Koslowski,C. Liu,J. R. Martin-Solis,G. Pautasso,Y. Peneliau,R. A. Pitts,G. I. Pokol,C. Reux,U. Sheikh,S. A. Silburn,T. Tang,R. A. Tinguely,P. Tolias,E. Tomesova,R. Villari*

Main category: physics.plasm-ph

TL;DR: The paper addresses the challenge of plasma-facing component (PFC) damage from runaway electrons (REs) in tokamaks, proposing a coordinated, interdisciplinary approach to bridge gaps in understanding and mitigation.


<details>
  <summary>Details</summary>
Motivation: The threat of RE-induced PFC damage to future fusion reactors like ITER and DEMO necessitates a holistic understanding and mitigation strategy.

Method: The paper reviews experimental evidence, advances diagnostics, and improves multi-scale modeling of RE behavior and PFC damage.

Result: Key insights include RE beam formation, damage mechanisms, and observations from major facilities, emphasizing predictive modeling.

Conclusion: The goal is to guide resilient PFC designs and mitigation strategies for safe, sustainable fusion reactor operation.

Abstract: This Roadmap article addresses the critical and multifaceted challenge of
plasma-facing component (PFC) damage caused by runaway electrons (REs) in
tokamaks, a phenomenon that poses a significant threat to the viability and
longevity of future fusion reactors such as ITER and DEMO. The dramatically
increased RE production expected in future high-current tokamaks makes it
difficult to avoid or mitigate REs when a plasma discharge terminates
abnormally. Preventing damage from the intense localised heat loads REs can
cause requires a holistic approach that considers plasma, REs and PFC damage.
Despite decades of progress in understanding the physics of REs and the
thermomechanical response of PFCs, their complex interplay remains poorly
understood. This document aims to initiate a coordinated, interdisciplinary
approach to bridge this gap by reviewing experimental evidence, advancing
diagnostic capabilities, and improving modelling tools across different scales,
dimensionalities and fidelities. Key topics include RE beam formation and
transport, damage mechanisms in brittle and metallic PFCs, and observations in
major facilities such as JET, DIII-D, WEST and EAST. The Roadmap emphasises the
urgency of predictive, high-fidelity modelling validated against well-diagnosed
controlled experiments, particularly in the light of recent changes in ITER's
wall material strategy and the growing importance of private sector
initiatives. Each section of the article is written to provide a concise
overview of one area of this multidisciplinary subject, with an assessment of
the status, a look at current and future challenges, and a brief summary. The
ultimate goal of this initiative is to guide future mitigation strategies and
design resilient components that can withstand the loads imposed by REs, thus
ensuring the safe and sustainable operation of the next generation of fusion
power plants.

</details>


### [36] [Spatial and temporal evolutions of blue-core helicon discharge driven by planar antenna with concentric rings](https://arxiv.org/abs/2506.10493)
*Chao Wang,Lei Chang,Ling-Feng Lu,Shunjiro Shinohara,Zhi-De Zeng,Ilya Zadiriev,Elena Kralkina,Zhi Li,Shi-Jie Zhang,Zi-Chen Kan,Ye Tao,Ding-Zhou Li*

Main category: physics.plasm-ph

TL;DR: The paper explores blue-core helicon discharge evolution driven by a planar antenna, revealing unique hollow blue-core plasma, rotation direction changes, and centrifugal instability analysis.


<details>
  <summary>Details</summary>
Motivation: To characterize the spatial and temporal evolution of blue-core helicon discharge using a planar antenna, focusing on unique phenomena like hollow plasma and rotation direction.

Method: Experiments conducted on the LEAD device with a four-ring planar antenna, analyzing density jumps, rotation, and instability using a two-fluid model.

Result: Hollow blue-core plasma, rotation direction reversal, and centrifugal instability driven by density gradient were observed. Instability weakens with axial wave number.

Conclusion: First detailed study of planar antenna-driven blue-core helicon plasma, highlighting rotation and instability, with implications for plasma control.

Abstract: The spatial and temporal evolutions of blue-core helicon discharge driven by
a planar antenna with four concentric rings are explored on the Linear
Experimental Advanced Device (LEAD). The discharge experiences distinct density
jumps from E mode to H mode, W mode, and blue-core mode, when RF input power
increases. This is similar to previous observations using other typical helicon
antennas; however, this special antenna could drive modes of even higher levels
for which the blue-core plasma column is actually hollow in radius, i.e.
peaking off-axis, which was not presented before. The column shows
counterclockwise rotation for blue-core mode and clockwise rotation for
non-blue-core mode. The reason could be attributed to the radial electric field
differenceses for both modes which reverses the rotation direction via ExB
drive. Moreover, the centrifugal instability of blue-core helicon plasma is
computed using a two-fluid flowing plasma model. It shows that the instability
is strong for small axial wave number but becomes weak for large axial wave
number. Perturbed density peaks at radius of 0.045 m, while the equilibrium
density gradient peaks at radius of 0.055 m. The coincidence of their radial
locations suggests that it is a resistive drift mode driven by density
gradient. The blue-core mode weakens once the magnetic field or flow rate
exceeds the threshold value. Increasing power further leads to a smoother
plasma density gradient. The electron temperature profiles decrease with
increased power, and the radial gradient of the electron temperature inside the
core is smaller as the magnetic field changes. To our best knowledge, it is the
first detailed characterization of blue-core helicon plasma driven by planar
antenna, especially in terms of azimuthal rotation and centrifugal instability.

</details>


### [37] [Analytic model for neutral penetration and plasma fueling](https://arxiv.org/abs/2506.10906)
*George J. Wilkie*

Main category: physics.plasm-ph

TL;DR: The paper develops analytic models for neutral density near plasma walls, validated by simulations, and simplifies charge exchange effects.


<details>
  <summary>Details</summary>
Motivation: To understand and model neutral atom interactions with confined plasma, particularly near walls or X-points, for better plasma fueling.

Method: Progression of analytic models for neutral density near planar/linear sources, validated with DEGAS2 simulations, and generalized for plasma gradients or charge exchange.

Result: Charge exchange can be simplified as a loss with reasonable accuracy, and the model isolates key aspects of neutral fueling from recycling.

Conclusion: The analytic model effectively addresses neutral fueling from recycling, with adaptable assumptions for different scenarios.

Abstract: Neutral atoms recycled from wall interaction interact with confined plasma,
thereby refueling it, most strongly in the region closest to the wall. This
occurs near the X-point in diverted configurations, or else near the wall
itself in limited configurations. A progression of analytic models are
developed for neutral density in the vicinity of a planar or linear source in
an ionizing domain. First-principles neutral transport simulations with DEGAS2
are used throughout to test the validity and limits of the model when using
equivalent sources. The model is further generalized for strong plasma
gradients or the inclusion of charge exchange. An important part of the problem
of neutral fueling from recycling is thereby isolated and solved with a
closed-form analytic model. A key finding is that charge exchange with the
confined plasma can be significantly simplified with a reasonable sacrifice of
accuracy by treating it as a loss. The several assumptions inherent to the
model (and the simulations to which it is compared) can be adapted according to
the particular behavior of neutrals in the divertor and the manner in which
they cross the separatrix.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [38] [Going beyond density functional theory accuracy: Leveraging experimental data to refine pre-trained machine learning interatomic potentials](https://arxiv.org/abs/2506.10211)
*Shriya Gumber,Lorena Alzate-Vargas,Benjamin T. Nebgen,Arjen van Veelen,Smit Kadvani,Tammie Gibson,Richard Messerly*

Main category: cond-mat.mtrl-sci

TL;DR: The paper refines DFT-trained MLIPs using EXAFS spectra to reduce systematic errors, improving predictions for nuclear fuels like UO$_2$ and UN.


<details>
  <summary>Details</summary>
Motivation: MLIPs inherit DFT's approximations, leading to discrepancies with experimental data. Refining them with EXAFS spectra can enhance accuracy.

Method: A trajectory re-weighting technique combined with transfer learning refines MLIPs to match EXAFS spectra, avoiding overfitting.

Result: Refined MLIPs show significant improvement in predicting structural and thermodynamic properties for UO$_2$ and UN.

Conclusion: The approach reduces reliance on costly nuclear tests by providing reliable atomistic simulations for nuclear fuel candidates.

Abstract: Machine learning interatomic potentials (MLIPs) are inherently limited by the
accuracy of the training data, usually consisting of energies and forces
obtained from quantum mechanical calculations, such as density functional
theory (DFT). Since DFT itself is based on several approximations, MLIPs may
inherit systematic errors that lead to discrepancies with experimental data. In
this paper, we use a trajectory re-weighting technique to refine DFT
pre-trained MLIPs to match the target experimental Extended X-ray Absorption
Fine Structure (EXAFS) spectra. EXAFS spectra are sensitive to the local
structural environment around an absorbing atom. Thus, refining an MLIP to
improve agreement with experimental EXAFS spectra also improves the MLIP
prediction of other structural properties that are not directly involved in the
refinement process. We combine this re-weighting technique with transfer
learning and a minimal number of training epochs to avoid overfitting to the
limited experimental data. The refinement approach demonstrates significant
improvement for two MLIPs reported in previous work, one for an established
nuclear fuel: uranium dioxide (UO$_2$) and second one for a nuclear fuel
candidate: uranium mononitride (UN). We validate the effectiveness of our
approach by comparing the results obtained from the original (unrefined)
DFT-based MLIP and the EXAFS-refined MLIP across various properties, such as
lattice parameters, bulk modulus, heat capacity, point defect energies, elastic
constants, phonon dispersion spectra, and diffusion coefficients. An accurate
MLIP for nuclear fuels is extremely beneficial as it enables reliable atomistic
simulation, which greatly reduces the need for large number of expensive and
inherently dangerous experimental nuclear integral tests, traditionally
required for the qualification of efficient and resilient fuel candidates.

</details>


### [39] [GEARS H: Accurate machine-learned Hamiltonians for next-generation device-scale modeling](https://arxiv.org/abs/2506.10298)
*Anubhab Haldar,Ali K. Hamze,Nikhil Sivadas,Yongwoo Shin*

Main category: cond-mat.mtrl-sci

TL;DR: GEARS H is a machine-learning Hamiltonian framework for large-scale electronic structure simulations, outperforming conventional methods with high accuracy.


<details>
  <summary>Details</summary>
Motivation: To enable large-scale electronic structure simulations beyond the capabilities of traditional density functional theory and other machine-learning methods.

Method: Uses GEARS H to analyze hole concentration in defective WSe2 interfaced with Ni-doped amorphous HfO2, and trains models for various systems.

Result: Achieves mean absolute error below 2.4 meV for Hamiltonian matrix elements, outperforming other frameworks.

Conclusion: GEARS H is production-ready for DFT-accuracy device-scale simulations, marking a significant advancement in the field.

Abstract: We introduce GEARS H, a state-of-the-art machine-learning Hamiltonian
framework for large-scale electronic structure simulations. Using GEARS H, we
present a statistical analysis of the hole concentration induced in defective
$\mathrm{WSe}_2$ interfaced with Ni-doped amorphous $\mathrm{HfO}_2$ as a
function of the Ni doping rate, system density, and Se vacancy rate in 72
systems ranging from 3326 to 4160 atoms-a quantity and scale of interface
electronic structure calculation beyond the reach of conventional density
functional theory codes and other machine-learning-based methods. We further
demonstrate the versatility of our architecture by training models for a
molecular system, 2D materials with and without defects, solid solution
crystals, and bulk amorphous systems with covalent and ionic bonds. The mean
absolute error of the inferred Hamiltonian matrix elements from the validation
set is below 2.4 meV for all of these models. GEARS H outperforms other
proposed machine-learning Hamiltonian frameworks, and our results indicate that
machine-learning Hamiltonian methods, starting with GEARS H, are now
production-ready techniques for DFT-accuracy device-scale simulation.

</details>


### [40] [Coupled reaction and diffusion governing interface evolution in solid-state batteries](https://arxiv.org/abs/2506.10944)
*Jingxuan Ding,Laura Zichi,Matteo Carli,Menghang Wang,Albert Musaelian,Yu Xie,Boris Kozinsky*

Main category: cond-mat.mtrl-sci

TL;DR: The paper uses large-scale reactive simulations and AI-driven interatomic potentials to study SEI formation in solid-state batteries, revealing a new crystalline phase and Li creep mechanisms.


<details>
  <summary>Details</summary>
Motivation: Understanding atomistic reactions in SEI formation is critical for next-gen batteries, but experimental and simulation challenges persist.

Method: Large-scale explicit reactive simulations with quantum accuracy, active learning, deep equivariant neural network potentials, and unsupervised classification techniques.

Result: Discovery of a new crystalline disordered phase (Li$_2$S$_{0.72}$P$_{0.14}$Cl$_{0.14}$) and insights into Li creep mechanisms.

Conclusion: The approach provides a parameter-free digital twin for studying complex heterogeneous processes in solid-state synthesis and electrochemistry.

Abstract: Understanding and controlling the atomistic-level reactions governing the
formation of the solid-electrolyte interphase (SEI) is crucial for the
viability of next-generation solid state batteries. However, challenges persist
due to difficulties in experimentally characterizing buried interfaces and
limits in simulation speed and accuracy. We conduct large-scale explicit
reactive simulations with quantum accuracy for a symmetric battery cell,
{\symcell}, enabled by active learning and deep equivariant neural network
interatomic potentials. To automatically characterize the coupled reactions and
interdiffusion at the interface, we formulate and use unsupervised
classification techniques based on clustering in the space of local atomic
environments. Our analysis reveals the formation of a previously unreported
crystalline disordered phase, Li$_2$S$_{0.72}$P$_{0.14}$Cl$_{0.14}$, in the
SEI, that evaded previous predictions based purely on thermodynamics,
underscoring the importance of explicit modeling of full reaction and transport
kinetics. Our simulations agree with and explain experimental observations of
the SEI formations and elucidate the Li creep mechanisms, critical to dendrite
initiation, characterized by significant Li motion along the interface. Our
approach is to crease a digital twin from first principles, without adjustable
parameters fitted to experiment. As such, it offers capabilities to gain
insights into atomistic dynamics governing complex heterogeneous processes in
solid-state synthesis and electrochemistry.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [41] [Time-dependent Gaussian basis sets for many-body systems using Rothe's method: A mean-field study](https://arxiv.org/abs/2506.10701)
*Simon Elias Schrader,Håkon Emil Kristiansen,Thomas Bondo Pedersen,Simen Kvaal*

Main category: physics.chem-ph

TL;DR: The paper proposes using Rothe's method with Gaussian basis sets to model time-dependent strong-field processes like high-harmonic generation, eliminating the need for grids.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of representing the electronic continuum in many-body systems for time-dependent processes.

Method: Applies Rothe's method to TDHF and TDDFT equations, using thawed, complex-valued Gaussian basis sets for orbital propagation.

Result: Shows that a few flexible Gaussians can yield qualitatively correct results, and grid calculations can be matched with 30–100 Gaussians for certain intensities.

Conclusion: Demonstrates the efficiency and accuracy of Gaussian basis sets for modeling strong-field processes in one-dimensional molecular systems.

Abstract: A challenge in modeling time-dependent strong-field processes such as
high-harmonic generation for many-body systems, is how to effectively represent
the electronic continuum. We apply Rothe's method to the time-dependent
Hartree-Fock (TDHF) and density functional theory (TDDFT) equations of motion
for the orbitals, which reformulates them as an optimization problem. We show
that thawed, complex-valued Gaussian basis sets can be propagated efficiently
for these orbital-based approaches, removing the need for grids. In particular,
we illustrate that qualitatively correct results can often be obtained by using
just a few fully flexible Gaussians that describe the unbound dynamics for both
TDHF and TDDFT. Grid calculations can be reproduced quantitatively using
$30$--$100$ Gaussians for intensities up to $4\times10^{14}$ W/cm$^2$ for the
one-dimensional molecular systems considered in this work.

</details>


<div id='physics.atm-clus'></div>

# physics.atm-clus [[Back]](#toc)

### [42] [Dipole-quadrupole coupling in triplet exciton-polaron quenching in a phosphorescent OLED emission layer](https://arxiv.org/abs/2506.10794)
*Clint van Hoesel,Reinder Coehoorn,Peter A. Bobbert*

Main category: physics.atm-clus

TL;DR: The paper investigates the triplet exciton-polaron quenching (TPQ) mechanism in phosphorescent OLEDs, revealing dipole-quadrupole coupling as dominant, resolving prior discrepancies and aiding in developing more efficient OLEDs.


<details>
  <summary>Details</summary>
Motivation: Understanding TPQ is crucial for improving OLED efficiency and stability, as current knowledge gaps hinder progress.

Method: Quantum-chemical calculations were used to analyze TPQ rates, comparing F"orster approximation with dipole-quadrupole coupling.

Result: Dipole-quadrupole coupling dominates TPQ rates, resolving discrepancies between device studies and spectral overlap estimates.

Conclusion: The findings provide a framework for designing phosphorescent emitter-host combinations with reduced TPQ, enhancing OLED performance.

Abstract: Improving the efficiency and stability of organic light-emitting diodes
(OLEDs) will further expand their present success in display applications.
Triplet exciton-polaron quenching (TPQ) is an important cause of limited
efficiency and stability in modern phosphorescent OLEDs, where triplet excitons
are the emitting species. Lack of understanding of the TPQ mechanism in these
OLEDs impedes the development of more efficient and stable OLEDs. We
investigate the TPQ mechanism for triplet excitons on a phosphorescent guest
interacting with hole polarons on a host. Our quantum-chemical calculations
show that at distances relevant for TPQ the F\"orster approximation for the TPQ
rate fails and that dipole-quadrupole coupling is dominant. This resolves a
discrepancy between estimates of the TPQ rate obtained from an OLED device
study and from the overlap between the emission spectrum of the emitter and
absorption spectrum of the charged host. Equivalently to the F\"orster radius
for dipole-dipole TPQ, the dipole-quadrupole TPQ rate can be quantified by a
dipole-quadrupole radius obtained from the overlap between the emission
spectrum of the emitter and the quadrupolar absorption spectrum of the charged
host. The findings of this work are expected to have a broad relevance and to
be useful in developing phosphorescent emitter-host combinations with reduced
TPQ.

</details>


<div id='astro-ph.EP'></div>

# astro-ph.EP [[Back]](#toc)

### [43] [Self-gravity in thin protoplanetary discs: 1. The smoothing-length approximation versus the exact self-gravity kernel](https://arxiv.org/abs/2506.10812)
*S. Rendon Restrepo,T. Rometsch,U. Ziegler,O. Gressel*

Main category: astro-ph.EP

TL;DR: The paper introduces an exact self-gravity kernel for thin discs, addressing limitations of ad hoc softening prescriptions in 2D models, and validates it with benchmarks and numerical tests.


<details>
  <summary>Details</summary>
Motivation: Current 2D models use ad hoc softening for self-gravity, which neglects vertical structure, distorts Newtonian gravity, and violates Newton's third law.

Method: Developed an analytical framework for a precise 2D self-gravity kernel using modified Bessel functions, validated by benchmarks and 2D/3D tests.

Result: The kernel preserves Newtonian features, reveals a new gravitational runaway source, and ensures computational efficiency via FFT compatibility.

Conclusion: The new kernel overcomes smoothing-length limitations, enabling consistent self-gravity treatment in thin discs, and will aid planet formation studies.

Abstract: Planet-forming discs often contain structures like spiral arms, typically
linked to the disc's gravitational forces. In 2D models, an ad hoc softening
prescription is commonly used for self-gravity, but this overlooks the vertical
structure's impact, suppresses the Newtonian nature of gravity at short
distances and doesn't respect Newton's third law.
  To address these issues, associated with a Plummer potential approximation,
we developed an exact self-gravity kernel for thin, hydrostatically supported
discs, including a dust fluid component. Our analytical framework provides a
precise 2D self-gravity prescription validated by benchmarks and 2D/3D
numerical tests.
  The derived kernel, based on modified Bessel functions, maintains Newtonian
gravitation features, such as point-wise symmetry, a smooth transition from
light to massive discs and a singularity at zero distance, among others. In
contrast to other prescriptions found in the literature, it proves capable of
leading to an additional, and previously unnoticed, source of gravitational
runaway discernible only at infinitesimal distances.
  We finally note that our new prescription remains compatible with methods
based on the fast Fourier transform, affording superior computational
efficiency. Our exact kernel formulation overcomes substantial limitations
inherent in the smoothing-length approach. It permits a novel, fully consistent
treatment of self-gravity in Gaussian-stratified thin discs. The approach, that
makes the usage of the Plummer potential obsolete, will prove useful to
studying all common planet formation scenarios, which are often backed by
2D-flat numerical simulations. Accordingly, in an accompanying paper, we will
investigate how the occurence of the gravitational instability is affected.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [44] [Interpretable and flexible non-intrusive reduced-order models using reproducing kernel Hilbert spaces](https://arxiv.org/abs/2506.10224)
*Alejandro N Diaz,Shane A McQuarrie,John T Tencer,Patrick J Blonigan*

Main category: cs.CE

TL;DR: The paper introduces a non-intrusive reduced-order modeling technique using regularized kernel interpolation, offering interpretability and flexibility by combining feature maps and kernel terms.


<details>
  <summary>Details</summary>
Motivation: Existing non-intrusive methods rely on least-squares regression, lacking interpretability and structure mirroring full-order models. This work aims to address these limitations.

Method: The approach uses regularized kernel interpolation to approximate ROM dynamics, embedding feature maps for interpretability and combining them with nonlinear kernel terms.

Result: The method produces interpretable ROMs with structure mirroring full-order models and provides a computable error bound. Numerical experiments validate its effectiveness.

Conclusion: The kernel-based approach is flexible, interpretable, and outperforms traditional methods like operator inference, as demonstrated in experiments.

Abstract: This paper develops an interpretable, non-intrusive reduced-order modeling
technique using regularized kernel interpolation. Existing non-intrusive
approaches approximate the dynamics of a reduced-order model (ROM) by solving a
data-driven least-squares regression problem for low-dimensional matrix
operators. Our approach instead leverages regularized kernel interpolation,
which yields an optimal approximation of the ROM dynamics from a user-defined
reproducing kernel Hilbert space. We show that our kernel-based approach can
produce interpretable ROMs whose structure mirrors full-order model structure
by embedding judiciously chosen feature maps into the kernel. The approach is
flexible and allows a combination of informed structure through feature maps
and closure terms via more general nonlinear terms in the kernel. We also
derive a computable a posteriori error bound that combines standard error
estimates for intrusive projection-based ROMs and kernel interpolants. The
approach is demonstrated in several numerical experiments that include
comparisons to operator inference using both proper orthogonal decomposition
and quadratic manifold dimension reduction.

</details>


### [45] [PDESpectralRefiner: Achieving More Accurate Long Rollouts with Spectral Adjustment](https://arxiv.org/abs/2506.10711)
*Li Luo,Shangsong Liang*

Main category: cs.CE

TL;DR: PDERefiner uses diffusion models to refine PDE solutions, improving high-frequency accuracy. For complex PDEs like Navier-Stokes, spectral adjustments enhance performance, yielding more accurate results.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of accurate long rollouts in time-dependent PDEs, especially for high-frequency accuracy.

Method: Enhanced refiner model with spectral adjustments (Blurring diffusion models) and a new v-prediction technique.

Result: Improved accuracy for one-step MSE loss and rollout loss across different model backbones (e.g., U-Net, neural operators).

Conclusion: Spectral adjustments in refiner models significantly enhance accuracy for complex PDEs, outperforming traditional methods.

Abstract: Generating accurate and stable long rollouts is a notorious challenge for
time-dependent PDEs (Partial Differential Equations). Recently, motivated by
the importance of high-frequency accuracy, a refiner model called PDERefiner
utilizes diffusion models to refine outputs for every time step, since the
denoising process could increase the correctness of modeling high frequency
part. For 1-D Kuramoto-Sivashinsky equation, refiner models can degrade the
amplitude of high frequency part better than not doing refinement process.
However, for some other cases, the spectrum might be more complicated. For
example, for a harder PDE like Navior-Stokes equation, diffusion models could
over-degrade the higher frequency part. This motivates us to release the
constraint that each frequency weighs the same. We enhance our refiner model
with doing adjustments on spectral space, which recovers Blurring diffusion
models. We developed a new v-prediction technique for Blurring diffusion
models, recovering the MSE training objective on the first refinement step. We
show that in this case, for different model backbones, such as U-Net and neural
operators, the outputs of PDE-SpectralRefiner are more accurate for both
one-step MSE loss and rollout loss.

</details>


### [46] [Spectral Analysis of Discretized Boundary Integral Operators in 3D: a High-Frequency Perspective](https://arxiv.org/abs/2506.10880)
*V. Giunzioni,A. Merlini,F. P. Andriulli*

Main category: cs.CE

TL;DR: The paper challenges the common practice of using a fixed mesh size (e.g., λ/10) for boundary element method discretization, showing that accuracy degrades with increasing frequency.


<details>
  <summary>Details</summary>
Motivation: To investigate whether the standard discretization approach (mesh size ~λ/10) maintains accuracy at higher frequencies.

Method: Analyzes spectra of operator matrices to compare discretized and continuous operators.

Result: Discrepancy grows with frequency, indicating the standard approach fails to maintain accuracy.

Conclusion: The widely used discretization method is insufficient for high-frequency accuracy, challenging common assumptions.

Abstract: When modeling propagation and scattering phenomena using integral equations
discretized by the boundary element method, it is common practice to
approximate the boundary of the scatterer with a mesh comprising elements of
size approximately equal to a fraction of the wavelength $\lambda$ of the
incident wave, e.g., $\lambda/10$. In this work, by analyzing the spectra of
the operator matrices, we show a discrepancy with respect to the continuous
operators which grows with the simulation frequency, challenging the common
belief that the aforementioned widely used discretization approach is
sufficient to maintain the accuracy of the solution constant when increasing
the frequency.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [47] [On the stability of the generalized equator map](https://arxiv.org/abs/2506.10652)
*Volker Branding,Anna Siffert*

Main category: math.DG

TL;DR: The paper studies the stability of the generalized equator map as a critical point of the extrinsic $k$-energy and $p$-energy, generalizing classical results.


<details>
  <summary>Details</summary>
Motivation: To extend the understanding of stability for critical points of energy functionals in geometric calculus of variations, building on prior work about the equator map and generalized radial projection.

Method: Analyzes the generalized equator map as a critical point of extrinsic $k$-energy and $p$-energy, conducting a detailed stability analysis.

Result: The generalized equator map is confirmed as a critical point of extrinsic $k$-energy, and stability results are generalized from classical findings.

Conclusion: The study advances stability analysis for critical points of energy functionals, providing new insights and generalizations of classical theorems.

Abstract: The energy, the $p$-energy ($p\in\mathbb{R}$ with $p\geq 2$) and the
extrinsic $k$-energy ($k\in\mathbb{N}$) for maps between Riemannian manifolds
are central objects in the geometric calculus of variations. The equator map
from the unit ball to the Euclidean sphere provides an explicit critical point
of all aforementioned energy functionals. During the last four decades many
researchers studied the stability of this particular map when considered as a
critical point of one of these energy functionals, see e.g. \cite{MR4436204},
\cite{MR705882}.
  Recently, Nakauchi \cite{MR4593065} introduced a generalized radial
projection map and proved that this map is both a critical point of the energy
and a critical point of the $p$-energy. This generalized radial projection map
gives rise to a generalized equator map which is also both a critical point of
the energy and a critical point of the $p$-energy.
  In this manuscript we first of all show that the generalized equator map is
also a critical point of the extrinsic $k$-energy. Then, the main focus is a
detailed stability analysis of this map, considered as a critical point of both
the extrinsic $k$-energy and the $p$-energy. We thus establish a number of
interesting generalizations of the classical (in)stability results of J\"ager
and Kaul \cite{MR705882}.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [48] [Principled Approaches for Extending Neural Architectures to Function Spaces for Operator Learning](https://arxiv.org/abs/2506.10973)
*Julius Berner,Miguel Liu-Schiaffini,Jean Kossaifi,Valentin Duruisseaux,Boris Bonev,Kamyar Azizzadenesheli,Anima Anandkumar*

Main category: cs.LG

TL;DR: The paper introduces neural operators as a way to extend neural networks to function spaces, enabling their application to scientific problems like PDEs. It provides a practical guide for converting existing neural architectures into neural operators.


<details>
  <summary>Details</summary>
Motivation: Deep learning has excelled in finite-dimensional spaces (e.g., computer vision) but struggles with infinite-dimensional function spaces common in scientific problems. Neural operators bridge this gap.

Method: The paper identifies key principles for mapping function spaces and proposes a recipe to convert popular neural architectures into neural operators with minimal changes.

Result: Neural operators can learn solution operators for PDEs across varying conditions, leveraging deep learning's empirical optimizations.

Conclusion: The work provides a practical framework for implementing neural operators, aiming to replicate deep learning's success in scientific domains.

Abstract: A wide range of scientific problems, such as those described by
continuous-time dynamical systems and partial differential equations (PDEs),
are naturally formulated on function spaces. While function spaces are
typically infinite-dimensional, deep learning has predominantly advanced
through applications in computer vision and natural language processing that
focus on mappings between finite-dimensional spaces. Such fundamental
disparities in the nature of the data have limited neural networks from
achieving a comparable level of success in scientific applications as seen in
other fields. Neural operators are a principled way to generalize neural
networks to mappings between function spaces, offering a pathway to replicate
deep learning's transformative impact on scientific problems. For instance,
neural operators can learn solution operators for entire classes of PDEs, e.g.,
physical systems with different boundary conditions, coefficient functions, and
geometries. A key factor in deep learning's success has been the careful
engineering of neural architectures through extensive empirical testing.
Translating these neural architectures into neural operators allows operator
learning to enjoy these same empirical optimizations. However, prior neural
operator architectures have often been introduced as standalone models, not
directly derived as extensions of existing neural network architectures. In
this paper, we identify and distill the key principles for constructing
practical implementations of mappings between infinite-dimensional function
spaces. Using these principles, we propose a recipe for converting several
popular neural architectures into neural operators with minimal modifications.
This paper aims to guide practitioners through this process and details the
steps to make neural operators work in practice. Our code can be found at
https://github.com/neuraloperator/NNs-to-NOs

</details>


<div id='math.NT'></div>

# math.NT [[Back]](#toc)

### [49] [Fast Ramanujan--type Series for Logarithms. Part II](https://arxiv.org/abs/2506.10321)
*Jorge Zuniga*

Main category: math.NT

TL;DR: The paper extends previous work on hypergeometric identities for computing logarithms, introducing new formulas for arctangents and fast multiseries evaluation. It develops methods for computing multiple logarithms simultaneously using integer programming, achieving highly efficient and record-breaking results.


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency and accuracy of computing logarithms, especially for multiple values simultaneously, by leveraging hypergeometric identities and integer programming.

Method: The approach involves solving an integer programming problem to optimize variable values in a finite lattice, deriving linear combinations of series for efficient single and multivalued logarithm computations.

Result: The method produces highly efficient formulas for single logarithms and the fastest known hypergeometric formulas for multivalued logarithms, demonstrated by extending the known digits of log(10) to 2.0·10¹².

Conclusion: The work significantly advances the computational efficiency and precision of logarithm calculations, particularly for multivalued cases, with practical applications in high-precision computations.

Abstract: This work extends the results of the preprint Ramanujan type Series for
Logarithms, Part I, arXiv:2506.08245, which introduced single hypergeometric
type identities for the efficient computing of $\log(p)$, where
$p\in\mathbb{Z}_{>1}$. We present novel formulas for arctangents and methods
for a very fast multiseries evaluation of logarithms. Building upon a
$\mathcal{O}((p-1)^{6})$ Ramanujan type series asymptotic approximation for
$\log(p)$ as $p\rightarrow1$, formulas for computing $n$ simultaneous
logarithms are developed. These formulas are derived by solving an integer
programming problem to identify optimal variable values within a finite lattice
$\mathbb{Z}^{n}$. This approach yields linear combinations of series that
provide: (i) highly efficient formulas for single logarithms of natural numbers
and (ii) the fastest known hypergeometric formulas for multivalued logarithms
of $n$ selected integers in $\mathbb{Z}_{>1}$. An application of these results
was to extend the number of decimal places known for log(10) up to
2.0$\cdot$10$^{12}$ digits (June 06 2025).

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [50] [Efficient nanophotonic devices optimization using deep neural network trained with physics-based transfer learning (PBTL) methodology](https://arxiv.org/abs/2506.10418)
*Gibaek Kim,Jungho Kim*

Main category: physics.optics

TL;DR: A neural network-based surrogate modeling framework for photonic device optimization, combining physics-based transfer learning and genetic algorithms, achieves significant speed-up and accuracy improvements.


<details>
  <summary>Details</summary>
Motivation: To address challenges in photonic device optimization, such as imbalanced feature importance and high data generation costs, by providing a generalizable solution with minimal data resources.

Method: The framework uses physics-based transfer learning (PBTL)-enhanced surrogate modeling and scalarized multi-objective genetic algorithms (GAs), integrating a deep neural network total predictor (DNN-TP) for optimization.

Result: Achieves an 80,000x speed-up in optimization, a 0.69% increase in prediction accuracy, and a 60% improvement in feasible device structures.

Conclusion: The proposed framework is effective for photonic design automation, offering scalable and efficient optimization with limited data.

Abstract: We propose a neural network(NN)-based surrogate modeling framework for
photonic device optimization, especially in domains with imbalanced feature
importance and high data generation costs. Our framework, which comprises
physics-based transfer learning (PBTL)-enhanced surrogate modeling and
scalarized multi-objective genetic algorithms (GAs), offers a generalizable
solution for photonic design automation with minimal data resources.To validate
the framework, we optimize mid-infrared quantum cascade laser (QCL) structures
consisting of two regions, active and injection, which have different levels of
feature importance. The optimization targets include five key QCL performance
metrics such as modal gain, emission wavelength, linewidth, and effective
injection, extraction energies. To address the challenge of multiple local
optima in the output latent space, we integrate a deep neural network total
predictor (DNN-TP) with a GA, enabling scalable and nature-inspired
optimization. By replacing computationally expensive numerical simulations with
the DNN-TP surrogate model, the optimization achieves a speed-up of over 80,000
times, allowing large-scale exploration of the QCL design space.To improve
model generalization with limited data, we introduce PBTL, which transfers
knowledge from a DNN core predictor (DNN-CP) trained on active-region
structures. This approach yields a 0.69 percentage increase in prediction
accuracy, equivalent to a 50 percentage reduction in training data
requirements, and leads to generate more feasible device structure with 60
percentage improvement in evaluation metric during optimization.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [51] [Approximate Controllability Problems for the Heat Equation in a Half-Plane Controlled by the Dirichlet Boundary Condition with a Bounded Control](https://arxiv.org/abs/2506.10466)
*Larissa Fardigola,Kateryna Khalina*

Main category: math.OC

TL;DR: The paper studies approximate controllability for a heat equation system, proving initial states in $L^2$ can be controlled to arbitrary end states in $L^2$ using specific controls. A numerical algorithm is provided.


<details>
  <summary>Details</summary>
Motivation: To address the controllability of a heat equation system with boundary controls, ensuring practical applicability.

Method: Analyzes the system $w_t=\Delta w$ with boundary control $u$, using $L^\infty$ and $L^2$ controls. Proves controllability and provides a numerical algorithm.

Result: Initial states in $L^2$ are approximately controllable to arbitrary end states in $L^2$.

Conclusion: The system is approximately controllable, with a numerical method demonstrated for practical use.

Abstract: In the paper, the problems of approximate controllability are studied for the
control system $w_t=\Delta w$, $w(0,x_2,t)=u(x_2,t)$, $x_1\in\mathbb
R_+=(0,+\infty)$, $x_2\in\mathbb R$, $t\in(0,T)$, where $u$ is a control
belonging to a special subset of $L^\infty(\mathbb R\times (0,T))\cap
L^2(\mathbb R\times (0,T))$. It is proved that each initial state belonging to
$L^2(\mathbb R_+\times\mathbb R)$ is approximately controllable to an arbitrary
end state belonging to $L^2(\mathbb R_+\times\mathbb R)$ by applying these
controls. A numerical algorithm of solving the approximate controllability
problem for this system is given. The results are illustrated by an example.

</details>


### [52] [On a mean-field Pontryagin minimum principle for stochastic optimal control](https://arxiv.org/abs/2506.10506)
*Manfred Opper,Sebastian Reich*

Main category: math.OC

TL;DR: A deterministic, mean-field extension of the Pontryagin minimum principle for stochastic optimal control, simplifying boundary value problems via gauge variables and tested on inverted pendulum and Lorenz-63 systems.


<details>
  <summary>Details</summary>
Motivation: To address stochastic optimal control problems with a deterministic, mean-field approach, avoiding the complexity of forward-backward stochastic differential equations.

Method: Introduces a gauge variable to achieve Hamiltonian structure, decoupling forward and reverse time equations, and applies mean-field ordinary differential equations for infinite horizon problems.

Result: Simplifies solution of boundary value problems and converts optimal control law computation into solving forward mean-field ODEs, validated numerically.

Conclusion: The mean-field Pontryagin minimum principle offers a practical, deterministic alternative for stochastic control problems, demonstrated through numerical examples.

Abstract: This papers outlines a novel extension of the classical Pontryagin minimum
(maximum) principle to stochastic optimal control problems. Contrary to the
well-known stochastic Pontryagin minimum principle involving forward-backward
stochastic differential equations, the proposed formulation is deterministic
and of mean-field type. The Hamiltonian structure of the proposed Pontryagin
minimum principle is achieved via the introduction of an appropriate gauge
variable. The gauge freedom can be used to decouple the forward and reverse
time equations; hence simplifying the solution of the underlying boundary value
problem. We also consider infinite horizon discounted cost optimal control
problems. In this case, the mean-field formulation allows converting the
computation of the desired optimal control law into solving a pair of forward
mean-field ordinary differential equations. The proposed mean-field formulation
of the Pontryagin minimum principle is tested numerically for a controlled
inverted pendulum and a controlled Lorenz-63 system.

</details>


### [53] [A space-time interface-fitted method for moving-subdomain distributed control problems with energy regularization](https://arxiv.org/abs/2506.10924)
*Quang Huy Nguyen,Phuong Cuc Hoang,Van Chien Le,Thi Thanh Mai Ta*

Main category: math.OC

TL;DR: The paper presents a space-time interface-fitted approximation for a moving-interface optimal control problem with energy regularization, using a Petrov-Galerkin method and validating results numerically.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of moving-interface optimal control problems by reformulating optimality conditions into a variational problem involving state and adjoint variables.

Method: Proposes a Petrov-Galerkin approximation on fully unstructured, space-time interface-fitted meshes, with theoretical analysis and numerical validation.

Result: Establishes an optimal error estimate under specific regularity assumptions and provides numerical results supporting the theory.

Conclusion: The method effectively approximates the problem, with theoretical and numerical validation confirming its accuracy.

Abstract: This paper investigates a space-time interface-fitted approximation of a
moving-interface optimal control problem with energy regularization. We
reformulate the optimality conditions into a variational problem involving both
the state and adjoint. This problem is shown to be equivalent to our optimal
control problem. Based on fully unstructured, space-time interface-fitted
meshes, we propose and analyze a Petrov-Galerkin approximation of the problem.
An optimal error estimate with respect to a discrete norm is established under
a specific regularity assumption on the state and adjoint. Several numerical
results are presented to corroborate our theoretical results.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [54] [Polynomial slowdown in space-inhomogeneous branching Brownian motion](https://arxiv.org/abs/2506.10623)
*Julien Berestycki,David Geldbach,Michel Pain*

Main category: math.PR

TL;DR: The paper analyzes a branching Brownian motion in 2D with angle-dependent branching rates, showing tightness of the maximum distance from the origin.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of particles in a branching Brownian motion where branching rates vary with direction, focusing on the preferred breeding direction.

Method: Study particles diffusing as standard Brownian motions with inhomogeneous branching rates, analyzing the maximum distance over time.

Result: The maximum distance $M_t$ is tight around a deterministic function $m(t)$, with explicit corrections based on branching parameters.

Conclusion: The study provides precise asymptotic behavior for the maximum distance in such systems, linking it to branching rate parameters and eigenvalues.

Abstract: We consider a branching Brownian motion in $\mathbb{R}^2$ in which particles
independently diffuse as standard Brownian motions and branch at an
inhomogeneous rate $b(\theta)$ which depends only on the angle $\theta$ of the
particle. We assume that $b$ is maximal when $\theta=0$, which is the preferred
direction for breeding. Furthermore we assume that $b(\theta ) = 1 - \beta
\abs{\theta }^\alpha + O(\theta ^2)$, as $\theta \to 0$, for $\alpha \in
(2/3,2)$ and $\beta>0.$ We show that if $M_t$ is the maximum distance to the
origin at time $t$, then $(M_t-m(t))_{t\ge 1}$ is tight where $$m(t) = \sqrt{2}
t - \frac{\vartheta_1}{\sqrt{2}} t^{(2-\alpha)/(2+\alpha)} -
\left(\frac{3}{2\sqrt{2}} - \frac{\alpha}{2\sqrt{2}(2+\alpha)}\right) \log t.
$$ and $\vartheta_1$ is explicit in terms of the first eigenvalue of a certain
operator.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [55] [Data-Driven Prediction of Dynamic Interactions Between Robot Appendage and Granular Material](https://arxiv.org/abs/2506.10875)
*Guanjin Wang,Xiangxue Zhao,Shapour Azarm,Balakumar Balachandran*

Main category: cs.RO

TL;DR: A data-driven approach combining dimension reduction, surrogate modeling, and data assimilation reduces computational time significantly while maintaining accuracy in robot-granular terrain interaction predictions.


<details>
  <summary>Details</summary>
Motivation: To improve robot navigation in complex terrains by reducing computational costs of high-fidelity simulations while maintaining accuracy.

Method: Integrates Sequentially Truncated Higher-Order Singular Value Decomposition, Gaussian Process surrogate modeling, and Reduced Order Particle Filter for data assimilation.

Result: Achieves orders of magnitude faster computation than physics-based simulations, with comparable or better accuracy when combining simulation and experimental data.

Conclusion: The approach enhances robot navigation in unknown terrains, offering scalable and efficient predictions beyond case-by-case analysis.

Abstract: An alternative data-driven modeling approach has been proposed and employed
to gain fundamental insights into robot motion interaction with granular
terrain at certain length scales. The approach is based on an integration of
dimension reduction (Sequentially Truncated Higher-Order Singular Value
Decomposition), surrogate modeling (Gaussian Process), and data assimilation
techniques (Reduced Order Particle Filter). This approach can be used online
and is based on offline data, obtained from the offline collection of
high-fidelity simulation data and a set of sparse experimental data. The
results have shown that orders of magnitude reduction in computational time can
be obtained from the proposed data-driven modeling approach compared with
physics-based high-fidelity simulations. With only simulation data as input,
the data-driven prediction technique can generate predictions that have
comparable accuracy as simulations. With both simulation data and sparse
physical experimental measurement as input, the data-driven approach with its
embedded data assimilation techniques has the potential in outperforming only
high-fidelity simulations for the long-horizon predictions. In addition, it is
demonstrated that the data-driven modeling approach can also reproduce the
scaling relationship recovered by physics-based simulations for maximum
resistive forces, which may indicate its general predictability beyond a
case-by-case basis. The results are expected to help robot navigation and
exploration in unknown and complex terrains during both online and offline
phases.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [56] [Vortex-magnetic competition and regime transitions in antiparallel flux tubes](https://arxiv.org/abs/2506.10648)
*Weiyu Shen,Rodolfo Ostilla-Mónico,Xiaojue Zhu*

Main category: physics.flu-dyn

TL;DR: The paper explores how vortex-magnetic interactions in MHD turbulence influence energy transfer across three regimes, revealing insights into astrophysical plasmas.


<details>
  <summary>Details</summary>
Motivation: Understanding the interplay between vortex dynamics and magnetic fields in MHD turbulence, particularly in systems like the Sun, to explain energy transfer mechanisms.

Method: Investigates two antiparallel flux tubes by varying the magnetic flux (interaction parameter $N_i$) to study Lorentz-to-inertial force balance.

Result: Identifies three regimes: vortex-dominated reconnection, instability-triggered cascade, and Lorentz-induced vortex disruption, each affecting energy transfer differently.

Conclusion: The inertial-Lorentz balance dictates energy transfer and structure formation in MHD turbulence, providing insights into astrophysical plasma behavior.

Abstract: Vortex-magnetic interactions shape magnetohydrodynamic (MHD) turbulence,
influencing energy transfer in astrophysical, geophysical, and industrial
systems. On the Sun, granular-scale vortex flows couple strongly with magnetic
fields, channelling energy into the corona. At high Reynolds numbers, vorticity
and magnetic fields are nearly frozen into the charged fluid, and MHD flows
emerge from the interplay between vortex dynamics and Lorentz forces. To probe
this competition in a controlled setting, we revisit the canonical problem of
two antiparallel flux tubes. By varying the magnetic flux threading each
tube--and thus sweeping the interaction parameter $N_i$, which gauges
Lorentz-to-inertial force balance--we uncover three distinct regimes:
vortex-dominated joint reconnection, instability-triggered cascade, and
Lorentz-induced vortex disruption. At low $N_i$, classical vortex dynamics
dominate, driving joint vortex-magnetic reconnection and amplifying magnetic
energy via a dynamo effect. At moderate $N_i$, the system oscillates between
vorticity-driven attraction and magnetic damping, triggering instabilities and
nonlinear interactions that spawn secondary filaments and drive an energy
cascade. At high $N_i$, Lorentz forces suppress vortex interactions, aligning
the tubes axially while disrupting vortex cores and rapidly converting magnetic
to kinetic energy. These findings reveal how the inertial-Lorentz balance
governs energy transfer and coherent structure formation in MHD turbulence,
offering insight into vortex-magnetic coevolution in astrophysical plasmas.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [57] [Coupled Lindblad pseudomode theory for simulating open quantum systems](https://arxiv.org/abs/2506.10308)
*Zhen Huang,Gunhee Park,Garnet Kin-Lic Chan,Lin Lin*

Main category: quant-ph

TL;DR: The paper introduces a coupled Lindblad pseudomode theory for simulating non-Markovian quantum dynamics efficiently, requiring only polylogarithmic scaling in simulation time and precision. It also presents a robust numerical algorithm for constructing coupled modes, avoiding non-convex optimization.


<details>
  <summary>Details</summary>
Motivation: To improve the simulation of non-Markovian quantum dynamics on classical and quantum platforms by addressing scalability and computational challenges.

Method: Develops a coupled Lindblad pseudomode theory with polylogarithmic scaling and introduces a robust numerical algorithm inspired by control theory to construct coupled modes.

Result: Demonstrates effectiveness by computing population dynamics and absorption spectra for the spin-boson model.

Conclusion: The work significantly advances the coupled Lindblad framework, benefiting applications in quantum simulations and impurity problems.

Abstract: Coupled Lindblad pseudomode theory is a promising approach for simulating
non-Markovian quantum dynamics on both classical and quantum platforms, with
dynamics that can be realized as a quantum channel. We provide theoretical
evidence that the number of coupled pseudomodes only needs to scale as
$\mathrm{polylog}(T/\varepsilon)$ in the simulation time $T$ and precision
$\varepsilon$. Inspired by the realization problem in control theory, we also
develop a robust numerical algorithm for constructing the coupled modes that
avoids the non-convex optimization required by existing approaches. We
demonstrate the effectiveness of our method by computing population dynamics
and absorption spectra for the spin-boson model. This work provides a
significant theoretical and computational improvement to the coupled Lindblad
framework, which impacts a broad range of applications from classical
simulations of quantum impurity problems to quantum simulations on near-term
quantum platforms.

</details>


### [58] [Hamiltonian Learning via Inverse Physics-Informed Neural Networks](https://arxiv.org/abs/2506.10379)
*Jie Liu,Xin Wang*

Main category: quant-ph

TL;DR: iPINN-HL integrates physics into ML for robust and efficient Hamiltonian learning, outperforming DNN-HL in accuracy and resource use.


<details>
  <summary>Details</summary>
Motivation: Conventional Hamiltonian learning methods struggle with noise and limited measurements, necessitating a more robust and efficient approach.

Method: iPINN-HL embeds the Schrödinger equation into ML, combining observational data and physical laws for parameter inference.

Result: iPINN-HL approaches the Heisenberg limit, shows noise robustness, and outperforms DNN-HL in accuracy and efficiency.

Conclusion: iPINN-HL is a powerful, flexible framework for practical quantum system characterization.

Abstract: Hamiltonian learning (HL), enabling precise estimation of system parameters
and underlying dynamics, plays a critical role in characterizing quantum
systems. However, conventional HL methods face challenges in noise robustness
and resource efficiency, especially under limited measurements. In this work,
we present \textit{Inverse Physics-Informed Neural Networks for Hamiltonian
Learning (iPINN-HL)}, an approach that embeds the Schr\"{o}dinger equation
directly into the machine learning procedure. This formulation allows the model
to integrate both observational data and known physical laws to infer
Hamiltonian parameters with greater accuracy and resource efficiency. We
benchmark iPINN-HL against a deep-neural-network-based quantum state tomography
method (denoted as DNN-HL) and demonstrate its effectiveness across several
different scenarios, including one-dimensional spin chains, cross-resonance
gate calibration, crosstalk identification, and real-time compensation to
parameter drift. Our results show that iPINN-HL can approach the Heisenberg
limit in certain settings and exhibits robustness to noises, while
outperforming DNN-HL in accuracy and resource efficiency. Therefore, iPINN-HL
is a powerful and flexible framework for quantum system characterization for
practical tasks.

</details>
