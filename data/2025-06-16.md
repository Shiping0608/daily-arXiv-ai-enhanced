<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 18]
- [math.AP](#math.AP) [Total: 22]
- [physics.comp-ph](#physics.comp-ph) [Total: 2]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 4]
- [cs.RO](#cs.RO) [Total: 1]
- [stat.ML](#stat.ML) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 2]
- [cs.CE](#cs.CE) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [math.OC](#math.OC) [Total: 2]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [math.DG](#math.DG) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Analysis of Floating-Point Matrix Multiplication Computed via Integer Arithmetic](https://arxiv.org/abs/2506.11277)
*Ahmad Abdelfattah,Jack Dongarra,Massimiliano Fasi,Mantas Mikaitis,Françoise Tisseur*

Main category: math.NA

TL;DR: The paper proposes a method to approximate floating-point matrix multiplication using integer matrix products, balancing performance and accuracy, and provides an error analysis and experimental validation.


<details>
  <summary>Details</summary>
Motivation: To leverage mixed-precision matrix multiply-accumulate units (e.g., NVIDIA tensor cores) by recasting floating-point operations into integer products for efficiency.

Method: Split matrices into integer slices, compute exact integer products, and accumulate them in floating-point arithmetic, with tradeoffs between slices (accuracy) and multiplications (performance).

Result: The method allows performance-accuracy tradeoffs but may suffer from inaccuracy or inefficiency with badly scaled rows/columns. Numerical experiments validate the approach.

Conclusion: The technique is effective for mixed-precision hardware but requires careful scaling and slice selection to balance accuracy and performance.

Abstract: Ootomo, Ozaki, and Yokota [Int. J. High Perform. Comput. Appl., 38 (2024), p.
297-313] have proposed a strategy to recast a floating-point matrix
multiplication in terms of integer matrix products. The factors A and B are
split into integer slices, the product of these slices is computed exactly, and
AB is approximated by accumulating these integer products in floating-point
arithmetic. This technique is particularly well suited to mixed-precision
matrix multiply-accumulate units with integer support, such as the NVIDIA
tensor cores or the AMD matrix cores. The number of slices allows for
performance-accuracy tradeoffs: more slices yield better accuracy but require
more multiplications, which in turn reduce performance. We propose an
inexpensive way to estimate the minimum number of multiplications needed to
achieve a prescribed level of accuracy. Our error analysis shows that the
algorithm may become inaccurate (or inefficient) if rows of A or columns of B
are badly scaled. We perform a range of numerical experiments, both in
simulation and on the latest NVIDIA GPUs, that confirm the analysis and
illustrate strengths and weaknesses of the algorithm.

</details>


### [2] [Regularization for time-dependent inverse problems: Geometry of Lebesgue-Bochner spaces and algorithms](https://arxiv.org/abs/2506.11291)
*Gesa Sarnighausen,Thorsten Hohage,Martin Burger,Andreas Hauptmann,Anne Wald*

Main category: math.NA

TL;DR: The paper explores time-dependent inverse problems using Lebesgue-Bochner spaces, presenting two regularization methods and testing them on dynamic computerized tomography.


<details>
  <summary>Details</summary>
Motivation: To recover functions from time-dependent observations, leveraging the distinct nature of time and space through Lebesgue-Bochner spaces.

Method: 1. Classical Tikhonov regularization in Banach spaces. 2. Variational regularization penalizing the time-derivative.

Result: Implemented Tikhonov regularization with different regularities for time and space, and tested methods on dynamic computerized tomography.

Conclusion: Lebesgue-Bochner spaces effectively handle time-dependent inverse problems, with both regularization methods showing promise.

Abstract: We consider time-dependent inverse problems in a mathematical setting using
Lebesgue-Bochner spaces. Such problems arise when one aims to recover a
function from given observations where the function or the data depend on time.
Lebesgue-Bochner spaces allow to easily incorporate the different nature of
time and space.
  In this manuscript, we present two different regularization methods in
Lebesgue Bochner spaces:
  1. classical Tikhonov regularization in Banach spaces
  2. variational regularization by penalizing the time-derivative
  In the first case, we additionally investigate geometrical properties of
Lebesgue Bochner spaces. In particular, we compute the duality mapping and show
that these spaces are smooth of power type. With this we can implement
Tikhononv regularization in Lebesgue-Bochner spaces using different
regularities for time and space.
  We test both methods using the example of dynamic computerized tomography.

</details>


### [3] [On existence of a variational regularization parameter under Morozov's discrepancy principle](https://arxiv.org/abs/2506.11397)
*Liang Ding,Long Li,Weimin Han,Wei Wang*

Main category: math.NA

TL;DR: The paper proves the existence of a regularization parameter under Morozov's discrepancy principle for nonlinear inverse problems and shows convergence of regularized solutions, supported by numerical results.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of continuity in the discrepancy for nonlinear inverse problems and questioning the existence of a suitable regularization parameter.

Method: Proving the existence of α under Morozov's discrepancy principle with a condition on τ₂ and τ₁, and analyzing convergence of regularized solutions.

Result: Existence of α is proven if τ₂ ≥ (3+2γ)τ₁, and convergence results are provided. Numerical results validate the approach.

Conclusion: The study confirms the feasibility of Morozov's discrepancy principle for nonlinear problems under specific conditions, with practical efficiency demonstrated numerically.

Abstract: Morozov's discrepancy principle is commonly adopted in Tikhonov
regularization for choosing the regularization parameter. Nevertheless, for a
general non-linear inverse problem, the discrepancy
$\|F(x_{\alpha}^{\delta})-y^{\delta}\|_Y$ does not depend continuously on
$\alpha$ and it is questionable whether there exists a regularization parameter
$\alpha$ such that $\tau_1\delta\leq
\|F(x_{\alpha}^{\delta})-y^{\delta}\|_Y\leq \tau_2 \delta$ $(1\le
\tau_1<\tau_2)$. In this paper, we prove the existence of $\alpha$ under
Morozov's discrepancy principle if $\tau_2\ge (3+2\gamma)\tau_1$, where
$\gamma>0$ is a parameter in a tangential cone condition for the nonlinear
operator $F$. Furthermore, we present results on the convergence of the
regularized solutions under Morozov's discrepancy principle. Numerical results
are reported on the efficiency of the proposed approach.

</details>


### [4] [Transformed Diffusion-Wave fPINNs: Enhancing Computing Efficiency for PINNs Solving Time-Fractional Diffusion-Wave Equations](https://arxiv.org/abs/2506.11518)
*Jing Li,Zhengqi Zhang*

Main category: math.NA

TL;DR: tDWfPINNs efficiently solve time-fractional diffusion-wave equations (α∈(1,2)) by reducing computational costs of fractional derivatives while maintaining accuracy, outperforming conventional methods.


<details>
  <summary>Details</summary>
Motivation: Conventional methods for time-fractional diffusion-wave equations compromise PINNs' mesh-free advantage or are computationally expensive for fractional derivatives.

Method: Introduces an integrand transformation to avoid first-order derivative calculations, tested with Monte Carlo and Gauss-Jacobi quadrature schemes.

Result: tDWfPINNs achieve superior efficiency without accuracy loss; Gauss-Jacobi outperforms Monte Carlo but requires careful quadrature point selection.

Conclusion: tDWfPINNs provide an accurate, scalable mesh-free solution for challenging fractional models, advancing numerical methods for such equations.

Abstract: We propose transformed Diffsuion-Wave fractional Physics-Informed Neural
Networks (tDWfPINNs) for efficiently solving time-fractional diffusion-wave
equations with fractional order $\alpha\in(1,2)$. Conventional numerical
methods for these equations often compromise the mesh-free advantage of
Physics-Informed Neural Networks (PINNs) or impose high computational costs
when computing fractional derivatives. The proposed method avoids first-order
derivative calculations at quadrature points by introducing an integrand
transformation technique, significantly reducing computational costs associated
with fractional derivative evaluation while preserving accuracy. We conduct a
comprehensive comparative analysis applying this integrand transformation in
conjunction with both Monte Carlo integration and Gauss-Jacobi quadrature
schemes across various time-fractional PDEs. Our results demonstrate that
tDWfPINNs achieve superior computational efficiency without sacrificing
accuracy. Furthermore, we incorporate the proposed approach into adaptive
sampling approaches such as the residual-based adaptive distribution (RAD) for
the time-fractional Burgers equation with order $\alpha\in(1,2)$, which
exhibits complex solution dynamics. The experiments show that the Gauss-Jacobi
method typically outperforms the Monte Carlo approach; however, careful
consideration is required when selecting the number of quadrature points.
Overall, the proposed tDWfPINNs offer a significant advancement in the
numerical solution of time-fractional diffusion-wave equations, providing an
accurate and scalable mesh-free alternative for challenging fractional models.

</details>


### [5] [Error Analysis of Truncation Legendre Method for Solving Numerical Differentiation](https://arxiv.org/abs/2506.11529)
*Maksym Kyselov*

Main category: math.NA

TL;DR: The paper introduces a truncation Legendre method for numerical differentiation of functions from weighted Wiener classes, providing error estimates in integral and uniform metrics for arbitrary order derivatives.


<details>
  <summary>Details</summary>
Motivation: To address the gap in existing studies, which mainly focus on first-order derivatives and specific functional spaces, by offering a comprehensive analysis across various function regularity parameters and error metrics.

Method: A truncation Legendre method is constructed and analyzed to recover arbitrary order derivatives, with a focus on error estimates in C and L_q metrics.

Result: Precise error bounds are established, and optimal truncation parameters are determined, showing the method achieves optimal convergence rates with minimal perturbed Fourier-Legendre coefficients.

Conclusion: The truncation method is effective for derivative recovery in weighted Wiener classes, achieving optimal convergence rates and requiring an optimal number of coefficients.

Abstract: We study the problem of numerical differentiation of functions from weighted
Wiener classes. We construct and analyze a truncation Legendre method to
recover arbitrary order derivatives. The main focus is on obtaining error
estimates in integral and uniform metrics. Unlike previous studies, which
predominantly focused on first-order derivatives and specific functional
spaces, we conduct a comprehensive analysis across a wide spectrum of function
regularity parameters and various metrics for measuring errors. We establish
precise error bounds for the truncation method in the metrics of C and L_q for
2 less than or equal to q less than or equal to infinity, and determine optimal
truncation parameters as functions of the error level and smoothness
parameters. Our results demonstrate that the truncation method achieves optimal
convergence rates on weighted Wiener classes, requiring an optimal number of
perturbed Fourier-Legendre coefficients for effective derivative recovery.

</details>


### [6] [Refined stability estimates for mixed problems by exploiting semi norm arguments](https://arxiv.org/abs/2506.11566)
*Nicolas Gauger,Alexander Linke,Christian Merdon*

Main category: math.NA

TL;DR: Refined stability estimates for mixed problems, focusing on semi-norms and their connection to physical regimes, leading to sharper stability estimates.


<details>
  <summary>Details</summary>
Motivation: To improve stability estimates for mixed problems by leveraging semi-norms inspired by pressure-robust discretizations in Navier-Stokes equations.

Method: Analyzes semi-norms on data functionals, linking their kernels to physical regimes and consistency errors in classical discretizations.

Result: Sharper stability estimates for solutions near these physical regimes.

Conclusion: The approach enhances understanding and accuracy of stability estimates in mixed problems.

Abstract: Refined stability estimates are derived for classical mixed problems. The
novel emphasis is on the importance of semi norms on data functionals, inspired
by recent progress on pressure-robust discretizations for the incompressible
Navier--Stokes equations. In fact, kernels of these semi norms are shown to be
connected to physical regimes in applications and are related to some
well-known consistency errors in classical discretizations of mixed problems.
Consequently, significantly sharper stability estimates for solutions close to
these physical regimes are obtained.

</details>


### [7] [Quasi-Monte Carlo hyperinterpolation](https://arxiv.org/abs/2506.11622)
*Congpei An,Mou Cai,Takashi Goda*

Main category: math.NA

TL;DR: The paper generalizes hyperinterpolation over high-dimensional cubes, replacing exact quadrature with QMC rules, and introduces a Lasso-based method for robustness against noise.


<details>
  <summary>Details</summary>
Motivation: Traditional hyperinterpolation relies on impractical exact quadrature assumptions in high dimensions, prompting the need for more flexible and scalable methods.

Method: Proposes QMC hyperinterpolation using quasi-Monte Carlo rules and a Lasso-based approach to handle noise.

Result: QMC hyperinterpolation matches traditional accuracy while avoiding dimensionality issues, with numerical experiments confirming improvements.

Conclusion: The new method offers a practical, high-dimensional solution with enhanced robustness and accuracy.

Abstract: This paper studies a generalization of hyperinterpolation over the
high-dimensional unit cube. Hyperinterpolation of degree \( m \) serves as a
discrete approximation of the \( L_2 \)-orthogonal projection of the same
degree, using Fourier coefficients evaluated by a positive-weight quadrature
rule that exactly integrates all polynomials of degree up to \( 2m \).
Traditional hyperinterpolation methods often depend on exact quadrature
assumptions, which can be impractical in high-dimensional contexts. We address
the challenges and advancements in hyperinterpolation, bypassing the assumption
of exactness for quadrature rules by replacing it with quasi-Monte Carlo (QMC)
rules and propose a novel approximation scheme with an index set \( I \), which
is referred to as QMC hyperinterpolation of range \( I \). In particular, we
provide concrete construction algorithms for QMC hyperinterpolation with
certain lattice rules. Consequently, we show that QMC hyperinterpolation
achieves accuracy comparable to traditional hyperinterpolation while avoiding
the curse of dimensionality.
  Furthermore, we introduce a Lasso-based approach to improve the robustness of
QMC hyperinterpolation against noise from sampling processes. Numerical
experiments validate the efficacy of our proposed methods, showing significant
improvements in approximation accuracy.

</details>


### [8] [Deep Symmetric Autoencoders from the Eckart-Young-Schmidt Perspective](https://arxiv.org/abs/2506.11641)
*Simone Brivio,Nicola Rares Franco*

Main category: math.NA

TL;DR: The paper analyzes symmetric autoencoders, a class of deep learning architectures, providing theoretical insights and introducing the EYS initialization strategy based on SVD.


<details>
  <summary>Details</summary>
Motivation: Despite the empirical success of deep autoencoders, their theoretical foundation is lacking compared to classical methods. This work aims to bridge that gap.

Method: The study introduces a formal classification of symmetric autoencoders, analyzes their properties mathematically, and proposes the EYS initialization strategy.

Result: The reconstruction error of symmetric autoencoders with orthonormality constraints is explained using the EYS theorem. Numerical experiments validate the proposed initialization.

Conclusion: The work advances the theoretical understanding of symmetric autoencoders and demonstrates the effectiveness of the EYS initialization strategy.

Abstract: Deep autoencoders have become a fundamental tool in various machine learning
applications, ranging from dimensionality reduction and reduced order modeling
of partial differential equations to anomaly detection and neural machine
translation. Despite their empirical success, a solid theoretical foundation
for their expressiveness remains elusive, particularly when compared to
classical projection-based techniques. In this work, we aim to take a step
forward in this direction by presenting a comprehensive analysis of what we
refer to as symmetric autoencoders, a broad class of deep learning
architectures ubiquitous in the literature. Specifically, we introduce a formal
distinction between different classes of symmetric architectures, analyzing
their strengths and limitations from a mathematical perspective. For instance,
we show that the reconstruction error of symmetric autoencoders with
orthonormality constraints can be understood by leveraging the well-renowned
Eckart-Young-Schmidt (EYS) theorem. As a byproduct of our analysis, we end up
developing the EYS initialization strategy for symmetric autoencoders, which is
based on an iterated application of the Singular Value Decomposition (SVD). To
validate our findings, we conduct a series of numerical experiments where we
benchmark our proposal against conventional deep autoencoders, discussing the
importance of model design and initialization.

</details>


### [9] [Fast parallel transient electromagnetic modeling using a uniform-in-time approximation to the exponential](https://arxiv.org/abs/2506.11657)
*Ralph-Uwe Börner,Stefan Güttel*

Main category: math.NA

TL;DR: A parallel method for modeling transient electromagnetic fields using uniform-in-time rational approximants with high parallel efficiency.


<details>
  <summary>Details</summary>
Motivation: To improve computational efficiency in modeling transient electromagnetic (TEM) fields by leveraging parallel processing.

Method: Uses a family of uniform-in-time rational approximants to the matrix exponential, exploiting partial fraction decomposition for parallel solving.

Result: Achieves high parallel efficiency with a solver whose complexity is independent of time channels or spatial discretization.

Conclusion: The approach is scalable and can potentially enhance efficiency in solving inverse TEM problems.

Abstract: A new approach for the parallel forward modeling of transient electromagnetic
(TEM) fields is presented. It is based on a family of uniform-in-time rational
approximants to the matrix exponential that share a common denominator
independent of the evaluation time points. The partial fraction decomposition
of this family is exploited to devise a fast solver with high parallel
efficiency. The number of shifted linear systems that need to be solved in
parallel does not depend on the number of required time channels nor the
spatial discretization. We also argue that similar parallel efficiency gains
can be expected when solving the inverse TEM problem.

</details>


### [10] [Automatic differentiation for Lax-Wendroff-type discretizations](https://arxiv.org/abs/2506.11719)
*Arpit Babbar,Valentin Churavy,Michael Schlottke Lakemper,Hendrik Ranocha*

Main category: math.NA

TL;DR: The paper introduces automatic differentiation (AD) in the predictor step of Lax-Wendroff methods for solving hyperbolic conservation laws, offering a high-order, single-stage, quadrature-free approach.


<details>
  <summary>Details</summary>
Motivation: To simplify and generalize the predictor step in Lax-Wendroff methods, avoiding the need for order-specific finite difference formulas and positivity corrections.

Method: Combines Lax-Wendroff methods with discontinuous Galerkin/flux reconstruction, using AD for element-local time average flux computation. The method is Jacobian-free and problem-independent.

Result: Numerical experiments confirm the method's order and positivity preservation, with AD performance matching the approximate Lax-Wendroff method in wall-clock time.

Conclusion: The AD-based approach is efficient, versatile, and avoids the limitations of traditional Lax-Wendroff methods, making it suitable for a wide range of applications.

Abstract: Lax-Wendroff methods combined with discontinuous Galerkin/flux reconstruction
spatial discretization provide a high-order, single-stage, quadrature-free
method for solving hyperbolic conservation laws. In this work, we introduce
automatic differentiation (AD) in the element-local time average flux
computation step (the predictor step) of Lax-Wendroff methods. The application
of AD is similar for methods of any order and does not need positivity
corrections during the predictor step. This contrasts with the approximate
Lax-Wendroff procedure, which requires different finite difference formulas for
different orders of the method and positivity corrections in the predictor step
for fluxes that can only be computed on admissible states. The method is
Jacobian-free and problem-independent, allowing direct application to any
physical flux function. Numerical experiments demonstrate the order and
positivity preservation of the method. Additionally, performance comparisons
indicate that the wall-clock time of automatic differentiation is always on par
with the approximate Lax-Wendroff method.

</details>


### [11] [Data-driven approaches to inverse problems](https://arxiv.org/abs/2506.11732)
*Carola-Bibiane Schönlieb,Zakhar Shumaylov*

Main category: math.NA

TL;DR: The paper introduces data-driven approaches for solving ill-posed inverse problems, contrasting classical methods with modern deep learning techniques like adversarial regularization and plug-and-play denoisers.


<details>
  <summary>Details</summary>
Motivation: Inverse problems are crucial in fields like medical imaging and remote sensing but are often ill-posed, requiring robust solutions. Classical methods are limited by modeling constraints, prompting exploration of data-driven alternatives.

Method: The paper discusses classical inverse problem solutions and introduces modern data-driven approaches using deep neural networks, focusing on adversarial regularization and provably convergent plug-and-play denoisers.

Result: Data-driven methods offer improved accuracy and computational efficiency compared to classical approaches, as demonstrated through theoretical analysis and numerical examples.

Conclusion: The paper highlights the potential of data-driven methods for inverse problems, identifies open challenges, and outlines future research directions.

Abstract: Inverse problems are concerned with the reconstruction of unknown physical
quantities using indirect measurements and are fundamental across diverse
fields such as medical imaging, remote sensing, and material sciences. These
problems serve as critical tools for visualizing internal structures beyond
what is visible to the naked eye, enabling quantification, diagnosis,
prediction, and discovery. However, most inverse problems are ill-posed,
necessitating robust mathematical treatment to yield meaningful solutions.
While classical approaches provide mathematically rigorous and computationally
stable solutions, they are constrained by the ability to accurately model
solution properties and implement them efficiently.
  A more recent paradigm considers deriving solutions to inverse problems in a
data-driven manner. Instead of relying on classical mathematical modeling, this
approach utilizes highly over-parameterized models, typically deep neural
networks, which are adapted to specific inverse problems using carefully
selected training data. Current approaches that follow this new paradigm
distinguish themselves through solution accuracy paired with computational
efficiency that was previously inconceivable.
  These notes offer an introduction to this data-driven paradigm for inverse
problems. The first part of these notes will provide an introduction to inverse
problems, discuss classical solution strategies, and present some applications.
The second part will delve into modern data-driven approaches, with a
particular focus on adversarial regularization and provably convergent linear
plug-and-play denoisers. Throughout the presentation of these methodologies,
their theoretical properties will be discussed, and numerical examples will be
provided. The lecture series will conclude with a discussion of open problems
and future perspectives in the field.

</details>


### [12] [Learning to Integrate](https://arxiv.org/abs/2506.11801)
*Oliver G. Ernst,Hanno Gottschalk,Toni Kowalewitz,Patrick Krüger*

Main category: math.NA

TL;DR: The paper introduces a method for uncertainty quantification in simulations with complex input distributions by transforming them to multivariate normals using transport maps, enabling sparse grid integration.


<details>
  <summary>Details</summary>
Motivation: Input data in practical simulations often doesn't fit separable distributions like Gaussian, necessitating a transformation method to leverage efficient sparse grid techniques.

Method: Uses transport maps (e.g., ACF, CFM) to transform complex distributions to multivariate normals, then applies sparse grid quadrature for integration.

Result: Demonstrated effectiveness for monomials and diffusion equations, while highlighting limitations when assumptions are violated.

Conclusion: The approach is viable for certain distributions but has limitations when assumptions don't hold.

Abstract: This work deals with uncertainty quantification for a generic input
distribution to some resource-intensive simulation, e.g., requiring the
solution of a partial differential equation. While efficient numerical methods
exist to compute integrals for high-dimensional Gaussian and other separable
distributions based on sparse grids (SG), input data arising in practice often
does not fall into this class. We therefore employ transport maps to transform
complex distributions to multivatiate standard normals. In generative learning,
a number of neural network architectures have been introduced that accomplish
this task approximately. Examples are affine coupling flows (ACF) and ordinary
differential equation-based networks such as conditional flow matching (CFM).
To compute the expectation of a quantity of interest, we numerically integrate
the composition of the inverse of the learned transport map with the simulation
code output. As this map is integrated over a multivariate Gaussian
distribution, SG techniques can be applied. Viewing the images of the SG
quadrature nodes as learned quadrature nodes for a given complex distribution
motivates our title. We demonstrate our method for monomials of total degrees
for which the unmapped SG rules are exact. We also apply our approach to the
stationary diffusion equation with coefficients modeled by exponentiated L\'evy
random fields, using a Karhunen-Lo\`eve-like modal expansions with 9 and 25
modes. In a series of numerical experiments, we investigate errors due to
learning accuracy, quadrature, statistical estimation, truncation of the modal
series of the input random field, and training data size for three normalizing
flows (ACF, conditional Flow Matching and Optimal transport Flow Matching) We
discuss the mathematical assumptions on which our approach is based and
demonstrate its shortcomings when these are violated.

</details>


### [13] [Random Batch Methods for Discretized PDEs on Graphs](https://arxiv.org/abs/2506.11809)
*Martín Hernández,Enrique Zuazua*

Main category: math.NA

TL;DR: The paper proposes the Random Batch Method (RBM) for solving PDEs on graphs, focusing on the heat equation. It combines discretization with RBM, proving convergence and extending to optimal control, with numerical validation.


<details>
  <summary>Details</summary>
Motivation: Addressing the high computational demands of solving PDEs on graphs, particularly for gas transport and similar challenges.

Method: A two-step approach: discretizing the PDE into a finite-dimensional problem, then applying RBM (discretize+RBM). Analyzes convergence and extends to optimal control.

Result: Proves convergence in expectation and demonstrates computational efficiency via numerical experiments.

Conclusion: The method is versatile for linear PDEs on graphs, offering computational savings while maintaining analytical guarantees.

Abstract: Gas transport and other complex real-world challenges often require solving
and controlling partial differential equations (PDEs) defined on graph
structures, which typically demand substantial memory and computational
resources. The Random Batch Method (RBM) offers significant relief from these
demands by enabling the simulation of large-scale systems with reduced
computational cost.
  In this paper, we analyze the application of RBM for solving PDEs on
one-dimensional graphs, specifically concentrating on the heat equation. Our
approach involves a two-step process: initially discretizing the PDE to
transform it into a finite-dimensional problem, followed by the application of
the RBM. We refer to this integrated approach as discretize+RBM. We establish
the convergence of this method in expectation, under the appropriate selection
and simultaneous reduction of the switching parameter in RBM and the
discretization parameters. Moreover, we extend these findings to include the
optimal control of the heat equation on graphs, enhancing the practical utility
of our methodology. The efficacy and computational efficiency of our proposed
solution are corroborated through numerical experiments that not only
demonstrate convergence but also show significant reductions in computational
costs.
  Our algorithm can be viewed as a randomized variant of domain decomposition,
specifically adapted for PDEs defined on graph structures. It is sufficiently
versatile to be applied to a wide range of linear PDEs -- not just the heat
equation -- while maintaining comparable analytical guarantees and convergence
properties.

</details>


### [14] [Second-Order Linear Relaxation Schemes for Time-Fractional Phase-Field Models](https://arxiv.org/abs/2506.11817)
*Hui Yu,Zhaoyang Wang,Ping Lin*

Main category: math.NA

TL;DR: The paper proposes a linear relaxation method for solving time-fractional Allen-Cahn and Cahn-Hilliard equations, using the L1+-CN formula and an auxiliary variable for efficient nonlinear term approximation. The scheme is linear, second-order accurate, and unconditionally energy stable.


<details>
  <summary>Details</summary>
Motivation: To develop efficient numerical schemes for time-fractional Allen-Cahn and Cahn-Hilliard equations, addressing challenges in nonlinear term approximation and ensuring energy stability.

Method: Uses the L1+-CN formula for fractional derivative discretization and introduces an auxiliary variable to approximate the nonlinear term via an algebraic equation, avoiding differential equations as in IEQ and SAV approaches.

Result: The semi-discrete scheme is linear, second-order accurate, and maintains consistency between auxiliary and original variables. It is proven unconditionally energy stable.

Conclusion: Numerical results confirm the scheme's effectiveness, offering a stable and accurate solution for the targeted equations.

Abstract: This work uses a linear relaxation method to develop efficient numerical
schemes for the time-fractional Allen-Cahn and Cahn-Hilliard equations. The
L1+-CN formula is used to discretize the fractional derivative, and an
auxiliary variable is introduced to approximate the nonlinear term by solving
an algebraic equation rather than a differential equation as in the invariant
energy quadratization (IEQ) and scalar auxiliary variable (SAV) approaches. The
proposed semi-discrete scheme is linear, second-order accurate in time, and the
inconsistency between the auxiliary and the original variables does not
deteriorate over time. Furthermore, we prove that the scheme is unconditionally
energy stable. Numerical results demonstrate the effectiveness of the proposed
scheme.

</details>


### [15] [Fourth- and Higher-order Interface Tracking of Three or More Materials with Arbitrarily Complex Topology and Geometry](https://arxiv.org/abs/2506.11897)
*Yan Tan,Yixiao Qian,Zhiqi Li,Qinghai Zhang*

Main category: math.NA

TL;DR: A multiphase cubic MARS method is proposed for 2D interface tracking, offering high-order accuracy, efficient topology representation, and handling complex junctions.


<details>
  <summary>Details</summary>
Motivation: To address challenges in interface tracking for multiple materials with complex geometry and topology, especially where traditional methods like VOF and level-set struggle.

Method: Uses graphs, cycles, and cubic splines for interface representation, maintains (r,h)-regularity, and achieves fourth- to eighth-order accuracy in time and space.

Result: Proven fourth- and higher-order convergence rates, validated by benchmark tests showing superior accuracy and efficiency.

Conclusion: The method effectively handles complex junctions and outperforms existing techniques in accuracy and efficiency.

Abstract: For interface tracking of an arbitrary number of materials in two dimensions,
we propose a multiphase cubic MARS method that (a) accurately and efficiently
represents the topology and geometry of the interface via graphs, cycles, and
cubic splines, (b) maintains a $(r,h)$-regularity condition of the interface so
that the distance between any pair of adjacent markers is within a
user-specified range that may vary according to the local curvature, (c)
applies to multiple materials with arbitrarily complex topology and geometry,
and (d) achieves fourth-, sixth-, and eighth-order accuracy both in time and in
space. In particular, all possible types of junctions, which pose challenges to
VOF methods and level-set methods, are handled with ease. The fourth- and
higher-order convergence rates of the proposed method are proven under the MARS
framework. Results of classic benchmark tests confirm the analysis and
demonstrate the superior accuracy and efficiency of the proposed method.

</details>


### [16] [A visco-plastic constitutive model for accurate densification and shape predictions in powder metallurgy hot isostatic pressing](https://arxiv.org/abs/2506.11946)
*Subrato Sarkar,Jason R Mayeur,KPK Ajjarapu,Fred A List III,Soumya Nag,Ryan R Dehoff*

Main category: math.NA

TL;DR: The paper introduces a visco-plastic model for PM-HIP simulations, addressing limitations of the plastic model, and presents a novel calibration approach requiring less experimental data.


<details>
  <summary>Details</summary>
Motivation: The application of PM-HIP to large-scale components is limited due to unpredictable shape distortions, necessitating better computational models.

Method: A visco-plastic model with a modified calibration approach is developed, using less experimental data than existing methods.

Result: Both plastic and visco-plastic models produce similar results when calibrated with the same data, with the visco-plastic model showing good agreement with experiments.

Conclusion: The visco-plastic model improves predictions for PM-HIP, enabling better understanding and application to complex geometries.

Abstract: Powder metallurgy hot isostatic pressing (PM-HIP) is an advanced
manufacturing process that produces near net shape parts with high material
utilization and uniform microstructures. Despite being used frequently to
produce small-scale components, the application of PM-HIP to large-scale
components is limited due to inadequate understanding of its complex mechanisms
that cause unpredictable post-HIP shape distortions. A computational model can
provide necessary information about the intermediate and final stages of the
HIP process that can help understand it better and make accurate predictions.
Generally, two types of computational models are employed for PM-HIP
simulations, namely, plastic and visco-plastic models. Between these, the
plastic model is preferred due to its cheaper calibration approach requiring
less experimental data. However, the plastic model sometimes produces incorrect
predictions when slight variations of the HIP conditions are encountered in
practical situations. Therefore, this work presents a visco-plastic model that
addresses these limitations of the plastic model. A novel modified calibration
approach is employed for the visco-plastic model that utilizes less
experimental data than existing approaches. With the new approach, the data
requirement is same for both plastic and visco-plastic models. This also
enables a quantitative comparison of plastic and visco-plastic models, which
have been only qualitatively compared in the past. When calibrated with the
same experimental data, both the models are found to produce similar results.
The calibrated visco-plastic model is applied to several complex geometries,
and the predictions are found to be in good agreement with experimental
observations.

</details>


### [17] [Analysis of BDDC preconditioners for non-conforming polytopal hybrid discretisation methods](https://arxiv.org/abs/2506.11956)
*Santiago Badia,Jerome Droniou,Jordi Manyer,Jai Tushar*

Main category: math.NA

TL;DR: The paper analyzes the convergence rate of the BDDC preconditioner for non-conforming polytopal hybrid discretizations, proving polylogarithmic condition number bounds independent of mesh parameters and subdomains. Numerical experiments validate the theory.


<details>
  <summary>Details</summary>
Motivation: To extend discrete trace theory for analyzing BDDC preconditioners in non-conforming polytopal hybrid discretizations, ensuring robustness on polytopal meshes.

Method: Uses discrete trace theory and proves continuity of a face truncation operator in a fully discrete polytopal setting. Validates with numerical experiments.

Result: Polylogarithmic condition number bounds for BDDC, independent of mesh parameters and subdomains, verified numerically.

Conclusion: The BDDC preconditioner is robust for second-order elliptic problems on polytopal meshes, validated by theory and experiments.

Abstract: In this work, we build on the discrete trace theory developed in [Badia,
Droniou, Tushar, arXiv (2024)] to analyse the rate of convergence of the
Balancing Domain Decomposition by Constraints (BDDC) preconditioner generated
from non-conforming polytopal hybrid discretisation. We prove polylogarithmic
condition number bounds for the preconditioner that are independent of the mesh
parameter and number of subdomains, and hold on polytopal meshes. The analysis
hinges on the continuity of a face truncation operator, which we prove in the
fully discrete polytopal setting. To validate the theory, we present two
numerical experiments: the first verifies the truncation estimate, and the
second -- a weak scalability test -- verifies the robustness of the condition
number bounds for BDDC when applied to second-order elliptic problems
discretised using discontinuous skeletal methods, specifically Hybridizable
Discontinuous Galerkin (HDG) and Hybrid High-Order (HHO) methods.

</details>


### [18] [Learning the Analytic Geometry of Transformations to Achieve Efficient Computation](https://arxiv.org/abs/2506.11990)
*Pei-Chun Su,Ronald R. Coifman*

Main category: math.NA

TL;DR: A novel framework for fast integral operations using adaptive hierarchical partition trees and data-driven techniques to exploit hidden geometries in matrices, reducing storage complexity and enabling efficient computation.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency of classical methods that rely on prior knowledge of geometry, this work aims to uncover hidden structures in matrices for fast operations, especially for irregular or unknown distributions.

Method: The framework uses iterative construction of adaptive hierarchical partition trees to reveal multiscale organization and local low-rank structures. It employs the butterfly algorithm and adaptive best tilings in space and frequency using Haar-Walsh wavelet packets.

Result: The method reduces storage complexity from O(N²) to O(N log N) and demonstrates effectiveness on acoustic heterogeneous potential operators and orthogonal polynomials.

Conclusion: The proposed data-driven approach efficiently compresses matrices and enables scalable, fast computation without relying on prior geometric knowledge.

Abstract: We propose a novel framework for fast integral operations by uncovering
hidden geometries in the row and column structures of the underlying operators.
This is accomplished through an iterative procedure that constructs adaptive
hierarchical partition trees, revealing latent multiscale organization and
exposing local low-rank structures within the data. Guided by this geometry, we
employ two complementary techniques: (1) the \emph{butterfly algorithm}, which
exploits the learned hierarchical low-rank structure; and (2) \emph{adaptive
best tilings} in both space and frequency using all levels of the generalized
Haar--Walsh wavelet packet tree. These techniques enable efficient matrix
factorization and multiplication. Unlike classical approaches that rely on
prior knowledge of the underlying geometry, our method is fully data-driven and
applicable to matrices arising from irregular or unknown distributions. We
demonstrate the effectiveness of our approach on matrices associated with
acoustic heterogeneous potential operators and families of orthogonal
polynomials. The resulting compressed representations reduce storage complexity
from $\mathcal{O}(N^2)$ to $\mathcal{O}(N \log N)$, enabling fast computation
and scalable implementation.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [19] [Poisson-type problems with transmission conditions at boundaries of infinite metric trees](https://arxiv.org/abs/2506.11218)
*Maryna Kachanovska,Kiyan Naderi,Konstantin Pankrashkin*

Main category: math.AP

TL;DR: The paper studies a Poisson-type problem on a mixed-dimensional structure (Euclidean domain + infinite metric tree) with gluing conditions, using embedded trace maps and Green-type formulas. It proves existence, uniqueness, and efficient approximation via finite tree sections.


<details>
  <summary>Details</summary>
Motivation: To rigorously define and solve Poisson-type problems on mixed-dimensional structures, combining Euclidean domains and lower-dimensional self-similar components (infinite metric trees), with focus on gluing conditions.

Method: Uses embedded trace maps and abstract Green-type formulas to define gluing conditions. Reduces the problem to analyzing Fredholm properties of Dirichlet-to-Neumann maps for the tree and Euclidean domain.

Result: Existence and uniqueness of solutions are proven. Finite sections of the tree provide efficient approximations.

Conclusion: The framework successfully addresses Poisson-type problems on mixed-dimensional structures, with practical approximation methods.

Abstract: The paper introduces a Poisson-type problem on a mixed-dimensional structure
combining a Euclidean domain and a lower-dimensional self-similar component
touching a compact surface (interface). The lower-dimensional piece is a
so-called infinite metric tree (one-dimensional branching structure), and the
key ingredient of the study is a rigorous definition of the gluing conditions
between the two components. These constructions are based on the recent concept
of embedded trace maps and some abstract machineries derived from a suitable
Green-type formula. The problem is then reduced to the study of Fredholm
properties of a linear combination of Dirichlet-to-Neumann maps for the tree
and the Euclidean domain, which yields desired existence and uniqueness
results. One also shows that finite sections of tree can be used for an
efficient approximation of the solutions.

</details>


### [20] [A detailed and comprehensive account of fractional Physics-Informed Neural Networks: From implementation to efficiency](https://arxiv.org/abs/2506.11241)
*Donya Dabiri,Joshua DaRosa,Milad Saadat,Deepak Mangal,Safa Jamali*

Main category: math.AP

TL;DR: The paper explores fractional physics-informed neural networks (fPINNs) for solving fractional differential equations (FDEs), addressing challenges like computational cost and accuracy.


<details>
  <summary>Details</summary>
Motivation: Fractional differential equations are complex to solve numerically, and robust data-driven methods like fPINNs offer promising solutions.

Method: The study evaluates fPINNs using two numerical methods based on the Caputo formalism to solve fractional ordinary and partial differential equations in 2D and 3D.

Result: fPINNs accurately solve FDEs with minor initial discrepancies, though Caputo formalism's computational burden is noted.

Conclusion: Strategies to enhance fPINN accuracy without excessive computational costs are discussed, highlighting their potential for solving FDEs.

Abstract: Fractional differential equations are powerful mathematical descriptors for
intricate physical phenomena in a compact form. However, compared to integer
ordinary or partial differential equations, solving fractional differential
equations can be challenging considering the intricate details involved in
their numerical solutions. Robust data-driven solutions hence can be of great
interest for solving fractional differential equations. In the recent years,
fractional physics-informed neural network has appeared as a platform for
solving fractional differential equations and till now, efforts have been made
to improve its performance. In this work, we present a fully detailed
interrogation of fractional physics-informed neural networks with different
foundations to solve different categories of fractional differential equations:
fractional ordinary differntial equation, as well as two and three dimensional
fractional partial differential equations. These equations are solved employing
two numerical methods based on the Caputo formalism. We show that these
platforms are generally able to accurately solve the equations with minor
discrepancies at initial times. Nonetheless, since in Caputo formalism, the
value of a fractional derivative at each point requires the function's value in
all of its previous history, it is computationally burdensome. Here, we discuss
strategies to improve accuracy of fractional physics-informed neural networks
solutions without imposing heavy computational costs.

</details>


### [21] [Asymptotic behavior of the fundamental solution of space-time fractional equations with a reaction term](https://arxiv.org/abs/2506.11296)
*Luciano Abadías,Claudio Carrasco,Juan C. Pozo*

Main category: math.AP

TL;DR: The paper extends results on the invasion speed of solutions to space-time fractional PDEs with reactive terms, using subordination to generalize findings across dimensions and parameters.


<details>
  <summary>Details</summary>
Motivation: To broaden understanding of invasion speeds in fractional PDEs beyond one-dimensional cases and specific fractional parameters.

Method: Uses subordination to analyze the fundamental solution's invasion speed for any spatial dimension, fractional parameters, and polynomial invasion speeds.

Result: Demonstrates the applicability of subordination in deriving invasion speeds for a wide range of cases.

Conclusion: Subordination is a powerful tool for generalizing results on invasion speeds in fractional PDEs.

Abstract: In this paper, we consider a space-time fractional partial differential
equation with a reactive term. We describe the speed of invasion of its
fundamental solution, extending recent results in this topic, which had been
proved for the one dimensional spatial setting and some fractional parameters
involved in the equation. The key tool to achieve these results in a wide range
of cases (any spatial dimension, any time and spatial fractional differential
parameters, and any polynomial speed of invasion) is the subordination.

</details>


### [22] [Asymptotics and Scattering for Critically Weakly Hyperbolic and Singular Systems](https://arxiv.org/abs/2506.11348)
*Bolys Sabitbek,Arick Shao*

Main category: math.AP

TL;DR: The paper analyzes first-order linear hyperbolic systems with weakly hyperbolic and singular coefficients, improving existing results and extending analysis to critically singular cases. It quantifies solution asymptotics, scattering problems, and regularity loss, with applications in relativity and cosmology.


<details>
  <summary>Details</summary>
Motivation: To address the challenges posed by weakly hyperbolic systems and singular coefficients, particularly at critical times like $t = 0$, and to extend existing results to more complex scenarios.

Method: The study involves analyzing the asymptotics of solutions, solving scattering problems with asymptotic data, and quantifying regularity loss due to degeneracies.

Result: Precise quantifications for solution asymptotics, scattering problems, and regularity loss are provided, with applications in weakly hyperbolic and singular wave equations, higher-order equations, and cosmology.

Conclusion: The results advance understanding of degenerate systems and singular coefficients, offering tools for applications in physics and cosmology, such as big bang singularities.

Abstract: We study a very general class of first-order linear hyperbolic systems that
both become weakly hyperbolic and contain lower-order coefficients that blow up
at a single time $t = 0$. In "critical" weakly hyperbolic settings, it is
well-known that solutions lose a finite amount of regularity at the degenerate
time $t = 0$. In this paper, we both improve upon the results in the weakly
hyperbolic setting, and we extend this analysis to systems containing
critically singular coefficients, which may also exhibit significantly modified
asymptotics at $t = 0$.
  In particular, we give precise quantifications for (1) the asymptotics of
solutions as $t$ approaches $0$; (2) the scattering problem of solving the
system with asymptotic data at $t = 0$; and (3) the loss of regularity due to
the degeneracies at $t = 0$. Finally, we discuss a variety of applications for
these results, including to weakly hyperbolic and singular wave equations,
equations of higher order, and equations arising from relativity and cosmology,
e.g. at big bang singularities.

</details>


### [23] [Analysis and Simulation of Plasmons in Graphene with Time- and Space-dependent Drude Weight](https://arxiv.org/abs/2506.11390)
*Fadil Santosa,Tong Shi*

Main category: math.AP

TL;DR: The paper studies plasmon propagation on graphene using a 2D TM electromagnetic field model, reducing it to a 1D problem via an integro-differential equation solved with finite differences.


<details>
  <summary>Details</summary>
Motivation: To understand plasmon behavior on graphene and simplify computational modeling.

Method: Model graphene as a conductive sheet with Maxwell's equations coupled to current density (Drude's law), then derive a time-dependent integro-differential equation for current density and solve it numerically.

Result: Numerical examples reveal previously unreported system behavior.

Conclusion: The proposed method simplifies plasmon propagation analysis on graphene and uncovers new insights.

Abstract: We study the propagation of plasmons on graphene. The problem is considered
in two dimensions with a transverse magnetic (TM) electromagnetic field. The
graphene material is assumed to be flat and is modeled as a conductive sheet.
This leads to a jump condition for the magnetic field on the sheet where it is
related to the current density on the sheet. The current density itself
satisfies Drude's law. The model then consists of Maxwell's equations coupled
to current density on the sheet. To make the problem more computationally
approachable, we develop a time-dependent integro-differential equation for the
current density. This effectively reduces the problem to one space dimension. A
finite difference method is proposed to solve the resulting equation. Numerical
examples are given to illustrate previously unreported behavior of the system.

</details>


### [24] [Sharp Stability of Global Compactness on the Heisenberg Group](https://arxiv.org/abs/2506.11404)
*Hua Chen,Yun-lu Fan,Xin Liao*

Main category: math.AP

TL;DR: A sharp stability inequality for functions near the sum of weakly interacting Jerison-Lee bubbles on the Heisenberg group is established, leading to a sharp quantitative stability of global compactness for non-negative functions.


<details>
  <summary>Details</summary>
Motivation: To analyze and quantify the stability of functions close to weakly interacting Jerison-Lee bubbles on the Heisenberg group, extending understanding of global compactness.

Method: Established a sharp stability inequality for such functions, leveraging properties of the Heisenberg group and Jerison-Lee bubbles.

Result: Achieved a sharp quantitative stability result for global compactness of non-negative functions on the Heisenberg group.

Conclusion: The work provides a rigorous stability framework for functions near weakly interacting bubbles, enhancing the understanding of global compactness in this context.

Abstract: In this paper, we establish a sharp stability inequality on the Heisenberg
group for functions that are close to the sum of m weakly interacting
Jerison-Lee bubbles. As a consequence, we obtain a sharp quantitative stability
of global compactness for non-negative functions on Heisenberg group.

</details>


### [25] [Exponential Vorticity Hessian Growth in Capillary Liquid Drop in Two Dimensions](https://arxiv.org/abs/2506.11414)
*Zhongtian Hu*

Main category: math.AP

TL;DR: The paper studies the evolution of a 2D ideal fluid droplet with bulk vorticity and surface tension, showing exponential growth of vorticity Hessian over time.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of a 2D ideal fluid droplet with nontrivial vorticity under surface tension effects.

Method: Constructs initial data with a disk-shaped domain and arbitrarily small velocity to analyze vorticity Hessian growth.

Result: The vorticity Hessian grows exponentially infinitely in time.

Conclusion: The study reveals unbounded exponential growth of vorticity Hessian in such fluid systems.

Abstract: In this work, we concern ourselves with the evolution of a droplet of an
ideal fluid in two dimensions, which has nontrivial bulk vorticity and is only
subject to the effects of surface tension. We construct initial data with
initial domain being a disk and initial velocity being arbitrarily small, such
that the vorticity Hessian grows exponentially infinitely in time.

</details>


### [26] [Doubly nonlinear parabolic equation with perturbation](https://arxiv.org/abs/2506.11503)
*Shun Uchida*

Main category: math.AP

TL;DR: The paper addresses a doubly nonlinear parabolic equation with nonlinear perturbation, aiming to relax growth conditions and exponent largeness for broader applicability.


<details>
  <summary>Details</summary>
Motivation: To extend results to singular and degenerate parabolic equations by reducing growth conditions and exponent requirements.

Method: Uses $L^{\infty}$-estimate and $L^{\infty}$-energy method on a time-discrete problem, leveraging prior work.

Result: Demonstrates applicability to singular and degenerate cases and discusses solution uniqueness.

Conclusion: The approach successfully broadens the scope of applicability for such equations while ensuring uniqueness.

Abstract: In this paper, we consider the initial boundary value problem of a doubly
nonlinear parabolic equation with nonlinear perturbation. We impose the
homogeneous Dirichlet condition on this problem. We aim to reduce the growth
condition of the nonlinear term and the largeness of exponent as possible so
that we can apply our result to both singular and degenerate type parabolic
equations. We use in our proof the $L ^{\infty }$-estimate of the time-discrete
equation derived in the previous work and apply the so-called $L ^{\infty
}$-energy method to the time-discrete problem. We also discuss the uniqueness
of solution by using the previous result.

</details>


### [27] [Source Identification Problem for a Nonlinear Subdiffusion Equation](https://arxiv.org/abs/2506.11519)
*R. R. Ashurov,O. T. Mukhiddinova*

Main category: math.AP

TL;DR: The paper studies the inverse problem of determining the right-hand side of a nonlinear subdiffusion equation with a Caputo derivative, focusing on reconstructing a coefficient dependent on both time and space.


<details>
  <summary>Details</summary>
Motivation: To address the gap in existing research, which only covers cases where the right-hand side depends on either time or space, but not both.

Method: The Galerkin method is used to seek a weak solution, supported by a priori estimates.

Result: The existence and uniqueness of the solution to the inverse problem are established.

Conclusion: The findings are novel and applicable to diffusion equations, expanding the understanding of such inverse problems.

Abstract: The work is devoted to the study of the inverse problem of determining the
right-hand side of a nonlinear subdiffusion equation with a Caputo derivative
with respect to time. Nonlinearity of the equation means that the right-hand
side of the equation depends nonlinearly on the solution of the equation. The
inverse problem consists of reconstructing the coefficient of the right-hand
side, which depends on both time and spatial variables, under a measurement in
an integral form. Similar inverse problems were previously studied in the case
when the right-hand side depends only on time or on a spatial variable. A weak
solution is sought by the Galerkin method. A priori estimates are proved, and
with their help, the existence and uniqueness of a solution to the inverse
problem under consideration are established. It is noteworthy that the results
obtained are new for diffusion equations as well.

</details>


### [28] [On scattering for NLS: rigidity properties and numerical simulations via the lens transform](https://arxiv.org/abs/2506.11560)
*Rémi Carles,Georg Maierhofer*

Main category: math.AP

TL;DR: The paper introduces a novel numerical method using the lens transform to efficiently compute the scattering operator for the defocusing nonlinear Schrödinger equation, supported by theoretical proofs and numerical experiments.


<details>
  <summary>Details</summary>
Motivation: The scattering operator's asymptotic nature makes its computation challenging, requiring a new approach to overcome these difficulties.

Method: The lens transform is employed for space-time compactification, enabling efficient numerical simulation of the scattering operator. New theoretical identities and properties are introduced and proven.

Result: The method is validated through numerical experiments, confirming known analytical properties and exploring new regimes, leading to new conjectures.

Conclusion: The approach is highly effective, opening avenues for further exploration of the scattering operator in both defocusing and focusing cases.

Abstract: We analyse the scattering operator associated with the defocusing nonlinear
Schr{\"o}dinger equation which captures the evolution of solutions over an
infinite time-interval under the nonlinear flow of this equation. The
asymptotic nature of the scattering operator (involving unbounded time) makes
its computation particularly challenging. We overcome this by exploiting the
space-time compactification provided by the lens transform, marking the first
use of this technique in numerical simulations. This results in a highly
efficient and reliable methodology for computing the scattering operator in
various regimes. In developing this approach we introduce and prove several new
identities and theoretical properties of the scattering operator. We support
our construction with several numerical experiments which we show to agree with
known analytical properties of the scattering operator, and also address the
case of long-range scattering for the one-dimensional cubic Schr{\"o}dinger
equation. Our simulations permit us to further explore regimes beyond current
analytical understanding, and lead us to formulate new conjectures concerning
fixed and rotating points of the operator, as well as its existence in the
long-range setting for both defocusing and focusing cases.

</details>


### [29] [A note on instantaneous gelation for coagulation kernels vanishing on the diagonal](https://arxiv.org/abs/2506.11573)
*Iulia Cristian,Barbara Niethammer,Juan J. L. Velázquez*

Main category: math.AP

TL;DR: Instantaneous gelation occurs in coagulation equations with sum-type kernels of homogeneity >1, vanishing on the diagonal, for solutions as Radon measures, excluding single Dirac delta initial data.


<details>
  <summary>Details</summary>
Motivation: To understand conditions under which instantaneous gelation (mass loss) occurs in coagulation equations, particularly for sum-type kernels with specific properties.

Method: Analyzing coagulation equations with sum-type kernels of homogeneity >1, vanishing on the diagonal, and considering solutions as Radon measures.

Result: Instantaneous gelation is proven for these kernels, except when initial data is a single Dirac delta.

Conclusion: The study confirms the occurrence of instantaneous gelation under the specified conditions, providing insights into the behavior of coagulation equations.

Abstract: We prove that instantaneous gelation (i.e., instantaneous loss of mass)
occurs for coagulation equations with sum-type kernels of homogeneity greater
than one which vanish on the diagonal. Our proof includes solutions that are
Radon measures if we exclude the case of initial data that are a single Dirac
delta.

</details>


### [30] [Regularizing effects of absorption terms in local-nonlocal mild singular problems](https://arxiv.org/abs/2506.11656)
*Stefano Biagi,Enzo Maria Merlino,Eugenio Vecchi*

Main category: math.AP

TL;DR: Existence, uniqueness, and summability results for energy solutions of singular problems with absorption driven by local-nonlocal operators.


<details>
  <summary>Details</summary>
Motivation: To address singular problems with absorption involving local-nonlocal operators, ensuring robust mathematical foundations.

Method: Proving existence and uniqueness of energy solutions, and establishing a Talenti-style comparison principle.

Result: Existence and uniqueness of solutions, along with improved summability properties.

Conclusion: The study provides rigorous solutions and summability gains for singular problems with absorption in local-nonlocal settings.

Abstract: In this paper we prove existence and uniqueness of energy solutionns for
singular problems with absorption driven by local-nonlocal operators. Moreover,
we establish a comparison principle \`a la Talenti, leading to a gain of
summability result for the solutions of these problems.

</details>


### [31] [Asymptotic large time behavior of singular solutions of the fast diffusion equation](https://arxiv.org/abs/2506.11692)
*Kin Ming Hui,Jongmyeong Kim*

Main category: math.AP

TL;DR: The paper proves the existence of singular radially symmetric forward self-similar solutions for the fast diffusion equation using a fixed point method and analyzes their asymptotic behavior.


<details>
  <summary>Details</summary>
Motivation: To address the existence and properties of singular solutions for the fast diffusion equation, which is a key problem in mathematical analysis and PDE theory.

Method: A fixed point method is employed to construct singular radially symmetric solutions of the form $V(x,t)=t^{-\alpha} f(t^{-\beta}x)$. The asymptotic behavior of these solutions near the origin and at large times is also studied.

Result: Existence of singular solutions is proven, and their asymptotic expansions near the origin and large time behavior are derived.

Conclusion: The results provide a deeper understanding of singular solutions in the context of the fast diffusion equation, with implications for PDE theory and applications.

Abstract: Let $n\ge 3$, $0<m<\frac{n-2}{n}$, $\alpha=\frac{2\beta-1}{1-m}$ and
$\frac{2}{1-m}<\frac{\alpha}{\beta}<\frac{n-2}{m}$. We give a new direct proof
using fixed point method on the existence of singular radially symmetric
forward self-similar solution of the form $V(x,t)=t^{-\alpha} f(t^{-\beta}x)$
$\forall x\in\mathbb{R}^n\setminus\{0\}$, $t>0$, for the fast diffusion
equation $u_t=\Delta (u^m/m)$ in $(\mathbb{R}^n\setminus\{0\})\times
(0,\infty)$, where $f$ satisfies \begin{equation*} \Delta (f^m/m) + \alpha f +
\beta x \cdot \nabla f =0 \quad \text{in} \; \mathbb{R}^n\setminus\{0\}
\end{equation*} with $\lim_{|x| \to 0} |x|^{ \frac{\alpha}{\beta}}f(x)=A$ and
$\lim_{|x| \to \infty}f(x) = D_A$ for some constants $A>0$, $D_A > 0$. We also
obtain an asymptotic expansion of such singular radially symmetric solution $f$
near the origin. We will also prove the asymptotic large time behaviour of the
singular solutions of the fast diffusion equation $u_t= \Delta (u^m/m)$ in
$(\mathbb{R}^n\setminus\{0\})\times (0,\infty)$, $u(x,0)=u_0(x)$ in
$\mathbb{R}^n\setminus\{0\}$, satisfying the condition $A_1|x|^{-\gamma}\leq
u_0(x)\leq A_2|x|^{-\gamma}$ in $\mathbb{R}^n\setminus\{0\}$, for some
constants $A_2>A_1>0$ and $n\le\gamma<\frac{n-2}{m}$.

</details>


### [32] [Gradient regularity for widely degenerate elliptic partial differential equations](https://arxiv.org/abs/2506.11708)
*Michael Strunk*

Main category: math.AP

TL;DR: The paper investigates the regularity of weak solutions to degenerate elliptic equations, proving continuity of certain transformations of the gradient.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of weak solutions to elliptic equations where ellipticity degenerates in a specific set.

Method: Analyzes solutions to the equation div ∇F(x,Du) = f, where F vanishes in a convex set E and is regular outside it.

Result: Shows that for any continuous function K vanishing on E, K(Du) is continuous in the domain.

Conclusion: The study confirms the continuity of transformed gradients for solutions to such degenerate elliptic equations.

Abstract: In this paper, we investigate the regularity of weak solutions
$u\colon\Omega\to\mathbb{R}$ to elliptic equations of the type
\begin{equation*}
  \mathrm{div}\, \nabla \mathcal{F}(x,Du) = f\qquad\text{in $\Omega$},
\end{equation*} whose ellipticity degenerates in a fixed bounded and convex set
$E\subset\mathbb{R}^n$ with $0\in \mathrm{Int}\, E$. Here,
$\Omega\subset\mathbb{R}^n$ denotes a bounded domain, and $\mathcal{F} \colon
\Omega\times\mathbb{R}^n \to\mathbb{R}_{\geq 0}$ is a function with the
properties: for any $x\in\Omega$, the mapping $\xi\mapsto \mathcal{F}(x,\xi)$
is regular outside $E$ and vanishes entirely within this set. Additionally, we
assume $f\in L^{n+\sigma}(\Omega)$ for some $\sigma > 0$, representing an
arbitrary datum. Our main result establishes the regularity
  \begin{equation*}
  \mathcal{K}(Du)\in C^0(\Omega)
  \end{equation*} for any continuous function $\mathcal{K}\in
C^0(\mathbb{R}^n)$ vanishing on $E$.

</details>


### [33] [Hele-Shaw limit of chemotaxis-Navier-Stokes flows](https://arxiv.org/abs/2506.11757)
*Qingyou He,Ling-Yun Shou,Leyun Wu*

Main category: math.AP

TL;DR: The paper explores the link between a chemotaxis-Navier-Stokes system with porous medium diffusion and the Hele-Shaw problem, proving global weak solutions and justifying the Hele-Shaw limit.


<details>
  <summary>Details</summary>
Motivation: To understand the connection between the chemotaxis-Navier-Stokes system and the Hele-Shaw problem, and to rigorously analyze their behavior under large diffusion limits.

Method: Proves global-in-time existence of weak solutions for the Cauchy problem and justifies the Hele-Shaw limit as diffusion range approaches infinity.

Result: Establishes convergence to a Hele-Shaw type free boundary problem and verifies a complementarity relation for the limiting bacterium pressure.

Conclusion: The study successfully bridges the two systems and provides rigorous analysis of their limiting behavior.

Abstract: This paper investigates the connection between the chemotaxis--Navier--Stokes
system with porous medium type nonlinear diffusion and the Hele--Shaw problem
in $\mathbb{R}^d$ ($d\geq2$). First, we prove the global-in-time existence of
weak solutions for the Cauchy problem of the chemotaxis-Navier-Stokes system
with the general initial data, uniformly in the diffusion range $m\in
[3,\infty)$. Then, we rigorously justify the Hele--Shaw limit for this system
as $m\rightarrow\infty$, showing the convergence to a free boundary problem of
Hele--Shaw type, where the bacterium (cell) diffusion is governed by the stiff
pressure law. Moreover, the complementarity relation characterizing the
limiting bacterium (cell) pressure via a degenerate elliptic equation is
verified by a novel application of the Hele--Shaw framework.

</details>


### [34] [Hessian matrix estimates of heat-type equations via Bismut-Stroock Hessian formula](https://arxiv.org/abs/2506.11805)
*Li-Juan Cheng,Rui-Yu Yang*

Main category: math.AP

TL;DR: New global Hessian matrix estimate for heat-type equations on Riemannian manifolds using a Bismut-type formula, with explicit coefficients and applications like a backward weak Harnack inequality and pointwise Hessian estimates for eigenfunctions.


<details>
  <summary>Details</summary>
Motivation: To derive explicit and practical estimates for Hessian matrices in heat-type equations on Riemannian manifolds, addressing gaps in existing methods.

Method: Utilizes a Bismut-type Hessian formula to establish global estimates with explicit coefficients and delay/growth rate functions.

Result: Achieves a novel backward weak Harnack inequality and precise pointwise Hessian estimates for eigenfunctions.

Conclusion: The method provides robust and explicit Hessian estimates with significant applications in analysis and geometry.

Abstract: In this paper, we establish a new global Hessian matrix estimate for
heat-type equations on Riemannian manifolds using a Bismut-type Hessian
formula. Our results feature fully explicit coefficients as well as delay /
growth rate functions. These estimates yield two key applications: a novel
backward weak Harnack inequality and a precise pointwise Hessian estimate for
eigenfunctions.

</details>


### [35] [Direct and inverse scattering for an isotropic medium with a second-order boundary condition](https://arxiv.org/abs/2506.11818)
*Govanni Granados,Isaac Harris,Andreas Kleefeld*

Main category: math.AP

TL;DR: The paper addresses the direct and inverse scattering problem for a penetrable, isotropic obstacle with a second-order Robin boundary condition, proposing a direct sampling method for shape recovery and studying the transmission eigenvalue problem.


<details>
  <summary>Details</summary>
Motivation: The study aims to model delamination of the scatterer's boundary and solve the inverse shape problem, extending methods to penetrable, isotropic scatterers with second-order Robin conditions.

Method: A direct sampling method is developed to recover the scatterer's shape using measured Cauchy data on a boundary enclosing it. The transmission eigenvalue problem is also analyzed.

Result: The transmission eigenvalues are proven to be at most a discrete set. Numerical examples validate the method for circular and non-circular scatterers in 2D.

Conclusion: The proposed method effectively solves the inverse shape problem and transmission eigenvalue problem, with numerical results supporting its applicability.

Abstract: We consider the direct and inverse scattering problem for a penetrable,
isotropic obstacle with a second-order Robin boundary condition which
asymptotically models the delamination of boundary of the scatterer. We develop
a direct sampling method to solve the inverse shape problem by numerically
recovering the scatterer. Here we assume that the corresponding Cauchy data is
measured on the boundary of a region which fully contains the scatterer.
Similar methods have been applied to other inverse shape problems, but they
have not been studied for a penetrable, isotropic scatterer with a second-order
Robin boundary condition. We also initiate the study of the corresponding
transmission eigenvalue problem, which is derived from assuming zero Cauchy
data is measured on the boundary of the region which fully contains the
scatterer. We prove that the transmission eigenvalues for this problem are at
most a discrete set. Numerical examples will be presented for the inverse shape
problem and transmission eigenvalue problem in two dimensions for circular and
non-circular scatterers.

</details>


### [36] [Infinitely many solutions for nonlinear superposition operators of mixed fractional order involving critical exponent](https://arxiv.org/abs/2506.11832)
*Souvik Bhowmick,Sekhar Ghosh,Vishvesh Kumar*

Main category: math.AP

TL;DR: The paper proves the existence of infinitely many nontrivial weak solutions for elliptic problems with nonlinear fractional operators and critical Sobolev exponents, using a variational method.


<details>
  <summary>Details</summary>
Motivation: To address challenges posed by critical exponents and lack of compactness in sublinear regimes, extending prior work and complementing recent studies.

Method: A variational framework combining truncation arguments and the notion of genus, verifying the Palais-Smale condition.

Result: Existence of infinitely many nontrivial weak solutions, even in classical cases, demonstrating broader applicability.

Conclusion: The approach generalizes and extends earlier results, providing new insights even for classical scenarios.

Abstract: This paper addresses elliptic problems involving the superposition of
nonlinear fractional operators with the critical Sobolev exponent in sublinear
regimes. We establish the existence of infinitely many nontrivial weak
solutions using a variational framework that combines a truncation argument
with the notion of genus. A central part of our analysis is the verification of
the Palais--Smale $(\mathrm{PS})_c$ condition for every $q \in (1,
p_{s_\sharp}^*)$, despite the challenges posed by the lack of compactness due
to the critical exponent.
  On the one hand, our approach extends and generalizes earlier results by
Garc{\'i}a Azorero and Peral Alonso \emph{[Trans. Amer. Math. Soc., 1991]} and
by Da Silva, Fiscella, and Viloria \emph{[J. Differential Equations, 2024]}; on
the other hand, the results obtained here complement the study of the
Brezis--Nirenberg-type problem by Dipierro, Perera, Sportelli, and Valdinoci
\emph{[Commun. Contemp. Math., 2024]} for $q = p$ and by Aikyn, Ghosh, Kumar,
and Ruzhansky \emph{[arXiv:2504.05105, 2025]} for $p < q < p_{s_\sharp}^*$.
Notably, our results are new even in the classical case $p = 2$, highlighting
the broader applicability of the methods developed here.

</details>


### [37] [Mean Field Games without Rational Expectations](https://arxiv.org/abs/2506.11838)
*Benjamin Moll,Lenya Ryzhik*

Main category: math.AP

TL;DR: The paper proposes a Mean Field Game (MFG) framework with non-rational expectations, avoiding the complex Master equation by focusing on low-dimensional couplings, simplifying the problem to finite-dimensional HJB equations.


<details>
  <summary>Details</summary>
Motivation: The unrealistic assumption of rational expectations in MFGs, especially with common noise, leads to impractical Master equations. The paper aims to address this by introducing non-rational expectations.

Method: The authors reformulate MFGs with non-rational expectations, particularly in low-dimensional coupling scenarios, and introduce adaptive learning as an example.

Result: By departing from rational expectations, the Master equation is avoided, and simpler finite-dimensional HJB equations can be solved.

Conclusion: Non-rational expectations in MFGs, especially with low-dimensional couplings, offer a more practical and computationally feasible alternative to traditional approaches.

Abstract: Mean Field Game (MFG) models implicitly assume "rational expectations",
meaning that the heterogeneous agents being modeled correctly know all relevant
transition probabilities for the complex system they inhabit. When there is
common noise, this assumption results in the "Master equation" (a.k.a. "Monster
equation"), a Hamilton-Jacobi-Bellman equation in which the
infinite-dimensional density of agents is a state variable. The rational
expectations assumption and the implication that agents solve Master equations
is unrealistic in many applications. We show how to instead formulate MFGs with
non-rational expectations. Departing from rational expectations is particularly
relevant in "MFGs with a low-dimensional coupling", i.e. MFGs in which agents'
running reward function depends on the density only through low-dimensional
functionals of this density. This happens, for example, in most macroeconomics
MFGs in which these low-dimensional functionals have the interpretation of
"equilibrium prices." In MFGs with a low-dimensional coupling, departing from
rational expectations allows for completely sidestepping the Master equation
and for instead solving much simpler finite-dimensional HJB equations. We
introduce an adaptive learning model as a particular example of non-rational
expectations and discuss its properties.

</details>


### [38] [A "trembling hand perfect" equilibrium for a certain class of mean field games](https://arxiv.org/abs/2506.11868)
*P. Jameson Graber*

Main category: math.AP

TL;DR: The paper explores mean field games with non-unique Nash equilibria, connects them to a scalar transport equation, and proposes entropy solutions for equilibrium selection with explicit error estimates.


<details>
  <summary>Details</summary>
Motivation: To understand non-uniqueness in Nash equilibria for a class of mean field games and develop criteria for selecting equilibria.

Method: Connects solutions to a scalar transport equation, constructs explicit examples of non-uniqueness, and uses entropy solutions for selection.

Result: Explicit error estimates are obtained for the vanishing noise limit, measured in a specific norm.

Conclusion: Entropy solutions provide a rational criterion for equilibrium selection with quantifiable error bounds.

Abstract: We study a particular class of mean field games whose solutions can be
formally connected to a scalar transport equation on the Wasserstein space of
measures. For this class, we construct some interesting explicit examples of
non-uniqueness of Nash equilibria. We then address the selection problem of
finding rational criteria by which to choose one equilibrium over others. We
show that when the theory of entropy solutions is used, we can obtain explicit
error estimates for the ``vanishing noise limit,'' where the error is measured
in a certain norm that measures the distance between two functions over the set
of empirical measures.

</details>


### [39] [Optimal trace norms for Helmholtz problems](https://arxiv.org/abs/2506.11944)
*Benedikt Gräßle*

Main category: math.AP

TL;DR: The paper analyzes weighted trace norms for Helmholtz problems, showing their dependence on geometry and weight, and provides improved continuity estimates for boundary operators.


<details>
  <summary>Details</summary>
Motivation: To rigorously analyze and characterize weighted trace norms for Helmholtz problems, focusing on their dependence on geometry and weight, and to improve continuity estimates for boundary operators.

Method: Uses weighted Sobolev-Slobodeckij norms and scaling estimates to analyze trace norms, with explicit conditions for intrinsic norms and trace inequalities.

Result: Identifies conditions for intrinsic trace norms and provides σ-explicit estimates, showing improved continuity for boundary operators as σ→0.

Conclusion: The analysis offers a rigorous framework for weighted trace norms in Helmholtz problems, with practical implications for boundary operator estimates.

Abstract: The natural $H^1(\Omega)$ energy norm for Helmholtz problems is weighted with
the wavenumber modulus $\sigma$ and induces natural weighted norms on the trace
spaces $H^{\pm1/2}(\Gamma)$ by minimial extension to $\Omega\subset\mathbb
R^n$. This paper presents a rigorous analysis for these trace norms with an
explicit characterisation by weighted Sobolev-Slobodeckij norms and scaling
estimates, highlighting their dependence on the geometry of the extension set
$\Omega\subset\mathbb R^n$ and the weight $\sigma$. The analysis identifies
conditions under which these trace norms are intrinsic to the isolated boundary
component $\Gamma\subset\partial\Omega$ and provides $\sigma$-explicit
estimates for trace inequalities in weighted spaces. In these natural
wavenumber-weighted norms, the boundary integral operators allow improved
continuity estimates that do \emph{not} deterioriate as $\sigma\to 0$.

</details>


### [40] [The conformal limit for bimerons in easy-plane chiral magnets](https://arxiv.org/abs/2506.11955)
*Bin Deng,Radu Ignat,Xavier Lamy*

Main category: math.AP

TL;DR: The paper studies minimizers of an energy functional in thin ferromagnetic films with Dzyaloshinskii-Moriya interaction and easy-plane anisotropy, proving their existence and describing them as perturbations of Möbius maps.


<details>
  <summary>Details</summary>
Motivation: The research is motivated by understanding bimeron configurations in thin ferromagnetic films, which are influenced by Dzyaloshinskii-Moriya interaction and easy-plane anisotropy.

Method: The authors use a strategy inspired by prior work for a similar model with easy-axis anisotropy, adapting it to address the less coercive easy-plane anisotropy and different symmetry properties.

Result: They prove the existence of minimizers and characterize them as localized perturbations of Möbius maps at a scale of order 1/|ln(σ²)|.

Conclusion: The study successfully extends prior methods to a new context, providing precise descriptions of bimeron configurations in easy-plane anisotropic systems.

Abstract: We study minimizers $\boldsymbol{m}\colon \mathbb R^2\to\mathbb S^2$ of the
energy functional \begin{align*} E_\sigma(\boldsymbol{m}) = \int_{\mathbb R^2}
\bigg(\frac 12 |\nabla\boldsymbol{m}|^2 +\sigma^2 \boldsymbol{ m} \cdot \nabla
\times\boldsymbol{m} +\sigma^2 m_3^2
  \bigg)\, dx\,, \end{align*} for $0<\sigma\ll 1$, with prescribed topological
degree \begin{align*} Q(\boldsymbol{m})=\frac{1}{4\pi} \int_{\mathbb
R^2}\boldsymbol{m} \cdot \partial_1
\boldsymbol{m}\times\partial_2\boldsymbol{m}\, dx =\pm 1\,. \end{align*} This
model arises in thin ferromagnetic films with Dzyaloshinskii-Moriya interaction
and easy-plane anisotropy, where these minimizers represent bimeron
configurations. We prove their existence, and describe them precisely as
perturbations of specific M\"obius maps: we establish in particular that they
are localized at scale of order $1/|\ln(\sigma^2)|$. The proof follows a
strategy introduced by Bernand-Mantel, Muratov and Simon (Arch. Ration. Mech.
Anal., 2021) for a similar model with easy-axis anisotropy, but requires
several adaptations to deal with the less coercive easy-plane anisotropy and
different symmetry properties.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [41] [Suppressing spurious oscillations and particle noise in particle-in-cell simulations](https://arxiv.org/abs/2506.11320)
*Yuxi Chen,Hongyang Zhou,Gabor Toth*

Main category: physics.comp-ph

TL;DR: The paper introduces numerical techniques in the FLEKS PIC code to reduce noise and spurious oscillations in kinetic plasma simulations, using a Lax-Friedrichs-type diffusion term and a novel current density calculation method.


<details>
  <summary>Details</summary>
Motivation: PIC simulations often suffer from statistical noise and spurious oscillations, especially in plasmas with fast flows, leading to unphysical solutions.

Method: The authors enhance the FLEKS PIC code by adding a Lax-Friedrichs-type diffusion term with a flux limiter to the Maxwell solver and propose a new current density calculation method in the comoving frame.

Result: Numerical tests show the methods effectively reduce oscillations and noise in shock and magnetic reconnection simulations.

Conclusion: The proposed techniques improve the accuracy and reliability of PIC simulations for plasmas with fast flows.

Abstract: Particle-in-cell (PIC) simulations are essential for studying kinetic plasma
processes, but they often suffer from statistical noise, especially in plasmas
with fast flows. We have also found that the typical central difference scheme
used in PIC codes to solve Maxwell's equations produces spurious oscillations
near discontinuities, which can lead to unphysical solutions. In this work, we
present numerical techniques to address these challenges within the
semi-implicit PIC code FLEKS, which is based on the Gauss's Law-satisfying
Energy-Conserving Semi-Implicit Particle-in-Cell method (GL-ECSIM). First, we
introduce a Lax-Friedrichs-type diffusion term with a flux limiter into the
Maxwell solver to suppress unphysical oscillations near discontinuities.
Second, we propose a novel approach for calculating the current density in the
comoving frame, which significantly reduces particle noise in simulations with
fast plasma flows. Numerical tests are presented to demonstrate the
effectiveness of these methods in mitigating spurious oscillations and noise in
shock and magnetic reconnection simulations.

</details>


### [42] [Accurate Reduced Floating-Point Precision Implicit Monte Carlo](https://arxiv.org/abs/2506.11962)
*Simon Butson,Mathew Cleveland,Alex Long,Todd Palmer*

Main category: physics.comp-ph

TL;DR: The paper presents methods for implementing Implicit Monte Carlo (IMC) in reduced-precision arithmetic, using scaling and floating-point manipulations to maintain accuracy.


<details>
  <summary>Details</summary>
Motivation: To address challenges in reduced-precision implementations of IMC, which involve nonlinear dependencies, wide-ranging quantities, and sensitive coupling between radiation and material energy.

Method: Scaling approaches (re-scaling values) and floating-point arithmetic manipulations (changing operation order, alternative summation algorithms) to minimize errors.

Result: Demonstrated that reduced-precision IMC with these methods achieves accuracy comparable to double-precision implementations.

Conclusion: Scaling and floating-point manipulations enable accurate reduced-precision IMC, making it viable for practical use.

Abstract: This work describes methodologies to successfully implement the Implicit
Monte Carlo (IMC) scheme for thermal radiative transfer in reduced-precision
floating-point arithmetic. The methods used can be broadly categorized into
scaling approaches and floating-point arithmetic manipulations. Scaling
approaches entail re-scaling values to ensure computations stay within a
representable range. Floating-point arithmetic manipulations involve changes to
order of operations and alternative summation algorithms to minimize errors in
calculations. The Implicit Monte Carlo method has nonlinear dependencies,
quantities spanning many orders of magnitude, and a sensitive coupling between
radiation and material energy that provide significant difficulties to accurate
reduced-precision implementations. Results from reduced and higher-precision
implementations of IMC solving the Su & Olson volume source benchmark problem
are compared to demonstrate the accuracy of a correctly implemented
reduced-precision IMC code. We show that the scaling approaches and
floating-point manipulations used in this work can produce solutions with
similar accuracy using half-precision data types as compared to a standard
double-precision implementation.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [43] [Scattering Theory in Noncanonical Phase Space: A Drift-Kinetic Collision Operator for Weakly Collisional Plasmas](https://arxiv.org/abs/2506.11215)
*Naoki Sato,Philip J. Morrison*

Main category: physics.plasm-ph

TL;DR: A collision operator for guiding center dynamics in 5D phase space is introduced, conserving key invariants and satisfying an H-theorem, with potential applications in plasma turbulence and transport.


<details>
  <summary>Details</summary>
Motivation: To address plasma regimes with long wavelengths, low frequencies, and weak collisionality by leveraging noncanonical Hamiltonian structures.

Method: Developed a scattering theory for grazing collisions in noncanonical phase spaces, leading to a guiding center collision operator.

Result: The operator conserves particle number, momentum, energy, and Casimir invariants, and satisfies an H-theorem.

Conclusion: The operator and its structure could advance understanding of turbulence, transport, and self-organization in plasmas.

Abstract: After developing a scattering theory for grazing collisions in general
noncanonical phase spaces, we introduce a guiding center collision operator in
five-dimensional phase space designed for plasma regimes characterized by long
wavelengths (relative to the Larmor radius), low frequencies (relative to the
cyclotron frequency), and weak collisionality (where repeated Coulomb
collisions induce cumulatively small changes in particle magnetic moment). The
collision operator is fully determined by the noncanonical Hamiltonian
structure of guiding center dynamics and exhibits a metriplectic structure,
ensuring the conservation of particle number, momentum, energy, and interior
Casimir invariants. It also satisfies an H-theorem, allowing for deviations
from Maxwell-Boltzmann statistics due to the nontrivial kernel of the
noncanonical guiding center Poisson tensor, spanned by the magnetic moment. We
propose that this collision operator and its underlying mathematical structure
may offer valuable insights into the study of turbulence, transport, and
self-organizing phenomena in both laboratory and astrophysical plasmas.

</details>


### [44] [In situ studies of a molten metal anode ablation in a nearly atmospheric pressure DC arc](https://arxiv.org/abs/2506.11308)
*Stanislav Musikhin,Valerian Nemchinsky,Hengfei Gu,Bruce E. Koel,Yevgeny Raitses*

Main category: physics.plasm-ph

TL;DR: High-speed pyrometry measures molten anode temperature in DC arc for SWCNT production, revealing ablation dynamics and CH4 effects.


<details>
  <summary>Details</summary>
Motivation: Monitoring anode ablation is critical for SWCNT growth, but traditional weighing fails due to carburization.

Method: Implemented 2D, 2-color pyrometry to measure molten anode temperature and calculate ablation rates.

Result: Resolved arc dynamics and reflections; CH4 addition significantly alters ablation rates.

Conclusion: Pyrometry enables better control for scaling SWCNT production.

Abstract: A DC arc with a meltable metal anode in a near-atmospheric pressure
hydrocarbon gas is an emerging method for producing single-walled carbon
nanotubes (SWCNTs). In these systems, evaporation of the molten metal anode
determines the formation of catalyst seed particles needed for SWCNT growth,
and therefore, should be monitored, controlled, and optimized. Evaluating the
anode ablation rate by weighing the anode before and after a synthesis run is
unfeasible due to anode carburization in the hydrocarbon atmosphere. To
overcome this, we implemented a high-speed, 2D, 2-color pyrometry for reliable
temperature measurements of the molten anode in a DC arc. The obtained
temperature fields were used to calculate the anode ablation rates. Results
showed the importance of resolving the arc and molten pool dynamics, as well as
addressing the issue of reflections. Furthermore, significant changes in
ablation rates were revealed upon addition of CH4, which must be considered
when scaling up the production of SWCNTs.

</details>


### [45] [Electron Heating in Hypersonic Flows: A New Thermodynamically Consistent Model](https://arxiv.org/abs/2506.11457)
*Felipe Martin Rodriguez Fuentes,Bernard Parent*

Main category: physics.plasm-ph

TL;DR: A new thermodynamically consistent electron heating model for hypersonic flows ensures accurate electron temperature prediction by correcting flaws in prior models.


<details>
  <summary>Details</summary>
Motivation: Prior models for electron heating in hypersonic flows were inadequate, relying on flawed assumptions and failing to ensure thermal equilibrium between electron and vibrational temperatures.

Method: The paper introduces a model derived from detailed balance, assuming a Boltzmann vibrational distribution and using an effective activation energy to guarantee convergence of temperatures at equilibrium.

Result: The model predicts electron temperatures up to six times lower than previous methods, aligning better with flight-test data.

Conclusion: This improved model enhances plasma modeling accuracy for hypersonic technologies, aiding applications like RF blackout mitigation and electromagnetic shielding.

Abstract: Accurate prediction of electron temperature ($T_{\rm e}$) in hypersonic
non-equilibrium flows is critical, yet hampered by inadequate models for
electron heating from vibrationally excited nitrogen. Prior models often relied
on ad-hoc scaling or flawed applications of detailed balance that failed to
ensure the convergence of electron and vibrational temperatures ($T_{\rm v}$)
at thermal equilibrium. This paper introduces a novel, thermodynamically
consistent electron heating model derived rigorously from the principle of
detailed balance. By assuming a Boltzmann vibrational distribution and
employing an effective activation energy, our approach yields a simple
heating-to-cooling ratio of $\exp(\theta_{\rm v}/T_{\rm e}-\theta_{\rm
v}/T_{\rm v})$, where $\theta_{\rm v}$ is the characteristic vibrational
temperature of nitrogen. This formulation guarantees that $T_{\rm e}$ correctly
converges to $T_{\rm v}$ at equilibrium. A key advantage is that our model can
utilize total cooling rates determined from swarm experiments, leading to
higher accuracy at the low electron temperatures typical of hypersonic flight.
This results in simulations that better agree with flight-test data, predicting
an electron temperature up to six times lower than previous models. These more
reliable predictions can significantly enhance the fidelity of plasma modeling
for hypersonic technologies, including radio-frequency blackout mitigation, MHD
aerocapture, electron transpiration cooling, and electromagnetic shielding.

</details>


### [46] [Surfaces with Klein bottle topology occur in fusion reactor fields](https://arxiv.org/abs/2506.11883)
*Christopher Berg Smiet*

Main category: physics.plasm-ph

TL;DR: The paper demonstrates that magnetic surfaces in fusion reactors can have the topology of an immersed Klein bottle, not just toroidal surfaces.


<details>
  <summary>Details</summary>
Motivation: To challenge the implicit assumption that magnetic surfaces in fusion devices are always toroidal, showing that other genus-1 surfaces like the Klein bottle can exist.

Method: Analyzes magnetic field configurations in fusion reactors, particularly around reflection-hyperbolic fixed points of the Poincaré map, using examples from the QUASR stellarator database.

Result: Identifies that Klein bottle topology can occur in fusion reactor fields, especially around specific fixed points, as seen in QUASR and abnormal sawtooth crashes.

Conclusion: Expands the understanding of possible magnetic surface topologies in fusion devices beyond toroidal forms, with implications for reactor design and stability.

Abstract: Magnetic confinement fusion devices, such as tokamaks and stellarators, are
designed such that magnetic field lines lie in magnetic surfaces that form a
foliation of nested genus-1 tori. The Poincar\'e-Hopf index theorem implies
that only surfaces with genus 1 allow for vector fields that lie tangent to the
surface and are are smooth and nowhere-vanishing. It is often implicitly
assumed that magnetic surfaces are always toroidal. This paper we show that
surfaces with the topology of an immersed Klein bottle (the only other genus-1
compact surface) can also occur in fusion reactor fields.
  These surfaces occur around fixed points of the Poincar\'e map that are
reflection-hyperbolic, and are spanned by the field lines that asymptotically
approach and depart from this closed line. Configurations in which this occurs
appear in the stellarator database QUASR, and have been described in literature
in the context of abnormal satwooth crashes.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [47] [Sensor Model Identification via Simultaneous Model Selection and State Variable Determination](https://arxiv.org/abs/2506.11263)
*Christian Brommer,Alessandro Fornasier,Jan Steinbrener,Stephan Weiss*

Main category: cs.RO

TL;DR: A method for identifying sensor models from unknown measurement data, including calibration states and reference frames, with a health metric for reliability.


<details>
  <summary>Details</summary>
Motivation: Simplify sensor model identification for inexperienced users and improve integration in modular robotic systems.

Method: Gray-box identification of sensor models, health metric for verification, and calibration state initialization.

Result: Accurate sensor model identification and parameterization for state estimation applications.

Conclusion: Enhances sensor integration and avoids common pitfalls in localization approaches.

Abstract: We present a method for the unattended gray-box identification of sensor
models commonly used by localization algorithms in the field of robotics. The
objective is to determine the most likely sensor model for a time series of
unknown measurement data, given an extendable catalog of predefined sensor
models. Sensor model definitions may require states for rigid-body calibrations
and dedicated reference frames to replicate a measurement based on the robot's
localization state. A health metric is introduced, which verifies the outcome
of the selection process in order to detect false positives and facilitate
reliable decision-making. In a second stage, an initial guess for identified
calibration states is generated, and the necessity of sensor world reference
frames is evaluated. The identified sensor model with its parameter information
is then used to parameterize and initialize a state estimation application,
thus ensuring a more accurate and robust integration of new sensor elements.
This method is helpful for inexperienced users who want to identify the source
and type of a measurement, sensor calibrations, or sensor reference frames. It
will also be important in the field of modular multi-agent scenarios and
modularized robotic platforms that are augmented by sensor modalities during
runtime. Overall, this work aims to provide a simplified integration of sensor
modalities to downstream applications and circumvent common pitfalls in the
usage and development of localization approaches.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [48] [Spectral Estimation with Free Decompression](https://arxiv.org/abs/2506.11994)
*Siavash Ameli,Chris van der Heide,Liam Hodgkinson,Michael W. Mahoney*

Main category: stat.ML

TL;DR: A novel method called 'free decompression' is introduced to estimate the spectrum of extremely large matrices using principles from free probability theory, even when direct access to the matrix is impractical.


<details>
  <summary>Details</summary>
Motivation: The need to compute eigenvalues of very large matrices in machine learning tasks is hindered by impractical matrix sizes and restricted access to data, especially in distributed learning or indirect matrix definitions.

Method: The method leverages free probability theory to extrapolate the eigenspectrum from small submatrices, enabling estimation without full matrix access or matrix-vector products.

Result: The approach effectively estimates spectra, validated against synthetic examples and real-world datasets, matching known distributions and empirical spectra.

Conclusion: Free decompression provides a viable solution for spectral estimation of impalpable matrices, addressing limitations of traditional methods.

Abstract: Computing eigenvalues of very large matrices is a critical task in many
machine learning applications, including the evaluation of log-determinants,
the trace of matrix functions, and other important metrics. As datasets
continue to grow in scale, the corresponding covariance and kernel matrices
become increasingly large, often reaching magnitudes that make their direct
formation impractical or impossible. Existing techniques typically rely on
matrix-vector products, which can provide efficient approximations, if the
matrix spectrum behaves well. However, in settings like distributed learning,
or when the matrix is defined only indirectly, access to the full data set can
be restricted to only very small sub-matrices of the original matrix. In these
cases, the matrix of nominal interest is not even available as an implicit
operator, meaning that even matrix-vector products may not be available. In
such settings, the matrix is "impalpable," in the sense that we have access to
only masked snapshots of it. We draw on principles from free probability theory
to introduce a novel method of "free decompression" to estimate the spectrum of
such matrices. Our method can be used to extrapolate from the empirical
spectral densities of small submatrices to infer the eigenspectrum of extremely
large (impalpable) matrices (that we cannot form or even evaluate with full
matrix-vector products). We demonstrate the effectiveness of this approach
through a series of examples, comparing its performance against known limiting
distributions from random matrix theory in synthetic settings, as well as
applying it to submatrices of real-world datasets, matching them with their
full empirical eigenspectra.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [49] [Physical Constraint Preserving Higher Order Finite Volume Schemes for Divergence-Free Astrophysical MHD and RMHD](https://arxiv.org/abs/2506.11181)
*Dinshaw S. Balsara,Deepak Bhoriya,Chetan Singh,Harish Kumar,Roger Käppeli,Federico Gatti*

Main category: astro-ph.IM

TL;DR: Higher-order PCP methods for MHD/RMHD improve stability in extreme astrophysical conditions, featuring a novel 2D Riemann solver and divergence-free magnetic field evolution.


<details>
  <summary>Details</summary>
Motivation: Address brittleness of higher-order codes in extreme conditions (high Mach numbers, Lorentz factors, strong magnetic fields).

Method: Develops PCP methods, including a 2D Riemann solver, for divergence-free magnetic field evolution, integrated with face-centered fields.

Result: Methods achieve design accuracy and handle extreme problems unsuitable for typical higher-order Godunov methods.

Conclusion: PCP methods enhance robustness and accuracy for stringent astrophysical MHD/RMHD simulations without significant computational overhead.

Abstract: Higher order finite volume schemes for magnetohydrodynamics (MHD) and
relativistic magnetohydrodynamics (RMHD) are very valuable because they allow
us to carry out astrophysical simulations with very high accuracy. However,
astrophysical problems sometimes have unusually large Mach numbers,
exceptionally high Lorentz factors and very strong magnetic fields. All these
effects cause higher order codes to become brittle and prone to code crashes.
In this paper we document physical constraint preserving (PCP) methods for
treating numerical MHD and RMHD. While unnecessary for standard problems, for
stringent astrophysical problems these methods show their value. We describe
higher order methods that allow divergence-free evolution of the magnetic
field. We present a novel two-dimensional Riemann solver. This two-dimensional
Riemann solver plays a key role in the design of PCP schemes for MHD and RMHD.
We present a very simple PCP formulation and show how it is amalgamated with
the evolution of face-centered magnetic fields. The methods presented here are
time-explicit and do not add much to the computational cost. We show that the
methods meet their design accuracies and work well on problems that would
otherwise be considered too extreme for typical higher order Godunov methods of
the type used in computational astrophysics.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [50] [Mechanism and Stability of Li-Dynamics in Amorphous Li-Ti-P-S Based Mixed Ionic-Electronic Conductor](https://arxiv.org/abs/2506.11199)
*Selva Chandrasekaran Selvaraj,Daiwei Wang,Donghai Wang,Anh T. Ngo*

Main category: cond-mat.mtrl-sci

TL;DR: The paper investigates Li-ion transport in Ti-doped LPS using MLFF-based MD simulations, revealing optimal doping levels (10-20% Ti) for stable transport channels.


<details>
  <summary>Details</summary>
Motivation: To understand the mechanism and stability of transport channels in Ti-doped LPS, a promising MIEC material for batteries.

Method: Molecular dynamics simulations with a 99% accurate machine-learning force field (MLFF) trained on ab-initio MD data, tested at three Ti concentrations and six temperatures.

Result: Ionic conductivities and activation energies match experiments; Li-ion transport occurs via free-volume diffusion. Optimal stability is observed at 10-20% Ti doping.

Conclusion: MLFF-based MD simulations efficiently explain Li-ion transport mechanisms in Ti-doped LPS, highlighting the importance of doping levels for stability.

Abstract: Mixed ionic-electronic conductor (MIEC) exhibit both high ionic and
electronic conductivity to improve the battery performance. In this work, we
investigate the mechanism and stability of transport channels in recently
developed our MIEC material, amorphous Ti-doped lithium phosphorus sulfide
(LPS), using molecular dynamics (MD) simulations with a 99\% accurate
machine-learning force field (MLFF) trained on \textit{ab-initio} MD data. The
achieved MLFF helps efficient large-scale MD simulations on LPS with three Ti
concentrations (10\%, 20\%, and 30\%) and six temperatures (25$^\mathrm{o}$C to
225$^\mathrm{o}$C) to calculate ionic conductivity, activation energy, Li-ion
transport mechanism, and configurational entropy. Results show that ionic
conductivities and activation energies are consistent with our recent
experimental values. Moreover, Li-ion transport occurs via free-volume
diffusion facilitated by the formation of disordered Li-S polyhedra. Enhanced
stability of transport channels at 10\% and 20\% Ti doping, compared to 0\% and
30\%, is observed through the analysis of the vibrational and configurational
entropy of these disordered Li-S polyhedra. Overall, this study highlights the
utility of MLFF-based large-scale MD simulations in explaining the transport
mechanism and its stability of Li-ion in Ti doped LPS electrolyte with
significant computational efficiency.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [51] [SIMSHIFT: A Benchmark for Adapting Neural Surrogates to Distribution Shifts](https://arxiv.org/abs/2506.12007)
*Paul Setinek,Gianluca Galletti,Thomas Gross,Dominik Schnürer,Johannes Brandstetter,Werner Zellinger*

Main category: cs.LG

TL;DR: The paper introduces SIMSHIFT, a benchmark for evaluating neural surrogates in PDEs under unseen configurations, and applies domain adaptation techniques to improve generalization.


<details>
  <summary>Details</summary>
Motivation: Neural surrogates for PDEs perform poorly on unseen configurations, while domain adaptation (DA) is underutilized in this context.

Method: Introduces SIMSHIFT dataset and extends DA methods to neural surrogates, using parametric descriptions and simulations from source configurations to predict target simulations.

Result: Experiments show challenges in out-of-distribution modeling but demonstrate DA's potential for improving neural surrogates in industrial simulations.

Conclusion: Highlights the need for robust neural surrogates under distribution shifts and identifies open problems in industrial applications.

Abstract: Neural surrogates for Partial Differential Equations (PDEs) often suffer
significant performance degradation when evaluated on unseen problem
configurations, such as novel material types or structural dimensions.
Meanwhile, Domain Adaptation (DA) techniques have been widely used in vision
and language processing to generalize from limited information about unseen
configurations. In this work, we address this gap through two focused
contributions. First, we introduce SIMSHIFT, a novel benchmark dataset and
evaluation suite composed of four industrial simulation tasks: hot rolling,
sheet metal forming, electric motor design and heatsink design. Second, we
extend established domain adaptation methods to state of the art neural
surrogates and systematically evaluate them. These approaches use parametric
descriptions and ground truth simulations from multiple source configurations,
together with only parametric descriptions from target configurations. The goal
is to accurately predict target simulations without access to ground truth
simulation data. Extensive experiments on SIMSHIFT highlight the challenges of
out of distribution neural surrogate modeling, demonstrate the potential of DA
in simulation, and reveal open problems in achieving robust neural surrogates
under distribution shifts in industrially relevant scenarios. Our codebase is
available at https://github.com/psetinek/simshift

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [52] [A fast mesh-free boundary integral method for two-phase flow with soluble surfactant](https://arxiv.org/abs/2506.11282)
*Samantha G. Evans,Michael Siegel,Johannes Tausch,Michael R. Booty*

Main category: physics.flu-dyn

TL;DR: An efficient boundary integral method for simulating drop/bubble deformation in Stokes flow with soluble surfactant, using a novel Fast Multipole Method to handle time convolution.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of simulating surfactant-coupled fluid dynamics, especially the nonlinear advection-diffusion equation in the high Péclet number regime.

Method: A boundary integral method with a Green's function formulation for surfactant concentration, enhanced by a causal Fast Multipole Method to reduce computational cost.

Result: Achieves accurate simulation of moving interfaces with soluble surfactant, reducing computational cost from O(P²) to O(P log₂² P).

Conclusion: The method is effective for high Péclet number problems and can be extended to similar advection-diffusion challenges.

Abstract: We present an accurate and efficient boundary integral (BI) method for
simulating the deformation of drops and bubbles in Stokes flow with soluble
surfactant. Soluble surfactant advects and diffuses in bulk fluids while
adsorbing and desorbing at interfaces. Since the fluid velocity is coupled to
the surfactant concentration, the advection-diffusion equation governing the
bulk surfactant concentration $C$ is nonlinear, precluding the Green's function
formulation necessary for a BI method. However, in the physically
representative large P\'eclet number limit, an analytical reduction of the
surfactant dynamics permits a Green's function formulation for $C$ as an
Abel-type time-convolution integral at each Lagrangian interface point. A
challenge in developing a practical numerical method based on this formulation
is the fast evaluation of the time convolution, since the kernel depends on the
time history of quantities at the interface, which is only found during the
time-stepping process. To address this, we develop a novel, causal version of
the Fast Multipole Method that reduces the computational cost from $O(P^2)$ for
direct evaluation of the time convolution to $O(P \log_2^2 P)$ per surface grid
point, where $P$ is the number of time steps. In the bulk phase, the resulting
method is mesh-free and provides an accurate solution to the fully coupled
moving interface problem with soluble surfactant. The approach extends
naturally to a broader class of advection-diffusion problems in the high
P\'eclet number regime.

</details>


### [53] [Phase-Field Modeling and Energy-Stable Schemes for Osmotic Flow through Semi-Permeable](https://arxiv.org/abs/2506.11374)
*Ruihan Guo,Jie Shen,Shixin Xu,Xianmin Xu*

Main category: physics.flu-dyn

TL;DR: A thermodynamically consistent phase-field model for fluid transport across semi-permeable membranes, focusing on osmotic pressure, is developed. High-order, energy-stable numerical schemes are proposed for efficient simulation.


<details>
  <summary>Details</summary>
Motivation: To model fluid transport across semi-permeable membranes, particularly addressing osmotic pressure effects, extending the classical NSCH system.

Method: Extends NSCH with an Allen-Cahn-type transmembrane flux. Uses LDG for spatial discretization and SDC for temporal integration.

Result: Numerical experiments validate the scheme, showing the impact of osmotic pressure and membrane permeability on droplet equilibrium.

Conclusion: The model is a robust tool for studying transmembrane fluid transport in biological and industrial contexts.

Abstract: We present a thermodynamically consistent phase-field model for simulating
fluid transport across semi-permeable membranes, with a particular focus on
osmotic pressure effects. The model extends the classical
Navier-Stokes-Cahn-Hilliard (NSCH) system by introducing an Allen-Cahn-type
transmembrane flux governed by chemical potential imbalances, resulting in a
strongly coupled system involving fluid motion, solute transport, and interface
dynamics. To solve this system efficiently and accurately, we develop
high-order, energy-stable numerical schemes. The local discontinuous Galerkin
(LDG) method is employed for spatial discretization, offering high-order
accuracy and geometric flexibility. For temporal integration, we first
construct a first-order decoupled scheme with rigorous energy stability, and
then improve temporal accuracy via a semi-implicit spectral deferred correction
(SDC) method. Numerical experiments confirm the theoretical properties of the
proposed scheme and demonstrate the influence of osmotic pressure and membrane
permeability on droplet morphology at equilibrium. The framework offers a
robust and versatile tool for modeling transmembrane fluid transport in both
biological and industrial applications.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [54] [Convergence of physics-informed neural networks modeling time-harmonic wave fields](https://arxiv.org/abs/2506.11395)
*Stefan Schoder,Aneta Furmanová,Viktor Hruška*

Main category: cs.CE

TL;DR: PINNs for Helmholtz equation in 3D room acoustics show accurate results with at least six training points per wavelength.


<details>
  <summary>Details</summary>
Motivation: To extend PINNs from 2D to 3D for modeling low-frequency room acoustics, addressing realistic sources, boundary conditions, and absorption.

Method: Study 3D cases with varied source definitions, boundary conditions, and complex speed of sound. Assess convergence via loss landscape and L² error against finite element simulations.

Result: At least six training points per wavelength are needed for accurate PINN predictions.

Conclusion: PINNs are viable for low-frequency room acoustics, with further potential for modeling absorbers.

Abstract: Studying physics-informed neural networks (PINNs) for modeling partial
differential equations to solve the acoustic wave field has produced promising
results for simple geometries in two-dimensional domains. One option is to
compute the time-harmonic wave field using the Helmholtz equation. Compared to
existing numerical models, the physics-informed neural networks forward problem
has to overcome several topics related to the convergence of the optimization
toward the "true" solution. The topics reach from considering the physical
dimensionality (from 2D to 3D), the modeling of realistic sources (from a
self-similar source to a realistic confined point source), the modeling of
sound-hard (Neumann) boundary conditions, and the modeling of the full wave
field by considering the complex solution quantities. Within this contribution,
we study 3D room acoustic cases at low frequency, varying the source definition
and the number of boundary condition sets and using a complex speed of sound
model to account for some degree of absorption. We assess the convergence
behavior by looking at the loss landscape of the PINN architecture, the $L^2$
error compared to a finite element reference simulation for each network
architecture and configuration. The convergence studies showed that at least
six training points per wavelength are necessary for accurate training and
subsequent predictions of the PINN. The developments are part of an initiative
aiming to model the low-frequency behavior of room acoustics, including
absorbers.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [55] [The QUEST Database of Highly-Accurate Excitation Energies](https://arxiv.org/abs/2506.11590)
*Pierre-François Loos,Martial Boggio-Pasqua,Aymeric Blondel,Filippo Lipparini,Denis Jacquemin*

Main category: physics.chem-ph

TL;DR: The paper introduces the QUEST database, providing theoretical best estimates of vertical transition energies (VTEs) for a wide range of excited states and molecules, aiming to benchmark excited-state methodologies.


<details>
  <summary>Details</summary>
Motivation: To create a comprehensive and accurate database (QUEST) of VTEs for benchmarking the performance of various excited-state computational methods.

Method: The database includes 1489 VTEs for valence and Rydberg transitions, covering singlets, doublets, triplets, and quartets, with a focus on challenging double-excitation states. Accuracy is ensured by comparing to FCI/aug-cc-pVTZ estimates.

Result: Most VTEs in the database are chemically accurate (±0.05 eV). Benchmarks for single- and multi-reference wavefunction approaches are provided.

Conclusion: The QUEST database serves as a valuable resource for assessing and improving excited-state methodologies, with all data and tools available on GitHub.

Abstract: We report theoretical best estimates of vertical transition energies (VTEs)
for a large number of excited states and molecules: the \textsc{quest}
database. This database includes 1489 \emph{aug}-cc-pVTZ VTEs (731 singlets,
233 doublets, 461 triplets, and 64 quartets) for both valence and Rydberg
transitions occurring in molecules containing from 1 to 16 non-hydrogen atoms.
\textsc{Quest} also includes a significant list of VTEs for states
characterized by a partial or genuine double-excitation character, known to be
particularly challenging for many computational methods. The vast majority of
the reported values are deemed chemically-accurate, that is, are within
$\pm0.05$ eV of the FCI/\emph{aug}-cc-pVTZ estimate. This allows for a balanced
assessment of the performance of popular excited-state methodologies. We report
the results of such benchmarks for various single- and multi-reference
wavefunction approaches, and provide extensive supporting information allowing
testing of other models. All corresponding data associated with the
\textsc{quest} database, along with analysis tools, can be found in the
associated \textsc{GitHub} repository at the following URL:
https://github.com/pfloos/QUESTDB.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [56] [A stochastic Galerkin method for optimal Dirichlet boundary control problems with uncertain data](https://arxiv.org/abs/2506.11479)
*Max Winkler,Hamdullah Yücel*

Main category: math.OC

TL;DR: The paper presents a stochastic Galerkin method for elliptic Dirichlet boundary control problems with random data, deriving error estimates and proposing preconditioners for efficient solving.


<details>
  <summary>Details</summary>
Motivation: To address elliptic Dirichlet boundary control problems with random input data, minimizing tracking cost and ensuring computational efficiency.

Method: Stochastic Galerkin approximation, error estimation for control and state variables, and preconditioning for large linear systems.

Result: Error estimates for control and state variables, with efficient preconditioners validated by numerical experiments.

Conclusion: The proposed methods are valid and efficient for solving the described problems, supported by numerical results.

Abstract: The paper deals with a stochastic Galerkin approximation of elliptic
Dirichlet boundary control problems with random input data. The expectation of
a tracking cost functional with the deterministic constrained control is
minimized. Error estimates are derived for the control variable in
$L^2(\partial \mathcal D)$-norm and state variable in $L^2(\Omega\times\mathcal
D)$-norm. To solve large linear systems, appropriate preconditioners are
proposed for both unconstrained and constrained scenarios. To illustrate the
validity and efficiency of the proposed approaches, some numerical experiments
are performed.

</details>


### [57] [A DC-Reformulation for Gradient-$L^0$-Constrained Problems in Function Spaces](https://arxiv.org/abs/2506.11917)
*Bastian Dittrich,Evelyn Herberg,Roland Herzog,Georg Müller*

Main category: math.OC

TL;DR: The paper extends a DC-reformulation approach to handle $L^0$-type cardinality constraints on gradient sparsity, enabling piecewise constant solutions.


<details>
  <summary>Details</summary>
Motivation: Cardinality constraints in optimization often require sparsely supported optimizers, and existing methods for convex objectives need extension to gradient sparsity.

Method: Reformulates $L^0$-constraints using $L^1$- and largest-$K$-norms, solving penalized DC subproblems.

Result: The approach successfully handles gradient sparsity, leading to piecewise constant optimizers.

Conclusion: The DC-reformulation method is effective for problems with $L^0$-type constraints on gradient support.

Abstract: Cardinality constraints in optimization are commonly of $L^0$-type, and they
lead to sparsely supported optimizers. An efficient way of dealing with these
constraints algorithmically, when the objective functional is convex, is
reformulating the constraint using the difference of suitable $L^1$- and
largest-$K$-norms and subsequently solving a sequence of penalized subproblems
in the difference-of-convex (DC) class. We extend this DC-reformulation
approach to problems with $L^0$-type cardinality constraints on the support of
the gradients, \ie, problems where sparsity of the gradient and thus piecewise
constant functions are the target.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [58] [The Integral Decimation Method for Quantum Dynamics and Statistical Mechanics](https://arxiv.org/abs/2506.11341)
*Ryan T. Grimm,Alexander J. Staat,Joel D. Eaves*

Main category: cond-mat.stat-mech

TL;DR: Integral Decimation (ID) is a method for numerically evaluating functional integrals using spectral tensor trains (STTs), enabling high-dimensional integration and analytical differentiation.


<details>
  <summary>Details</summary>
Motivation: The method addresses memory bottlenecks in high-dimensional integration and improves upon earlier STT-based approaches by circumventing the barren plateau problem.

Method: ID constructs a separable decomposition of the integrand as an STT, mapping the integrand to an auxiliary many-body wavefunction evolved via quantum gates. Decimation during gate sequences compresses the integral.

Result: ID successfully calculates partition functions (e.g., classical XY model) and solves non-Markovian quantum relaxation problems, achieving high-accuracy simulations in systems as large as 40-site chains.

Conclusion: ID is a versatile and efficient method for functional integration, overcoming limitations of previous approaches and enabling accurate simulations in complex systems.

Abstract: We present a method to numerically evaluate functional integrals called
integral decimation (ID). It constructs a separable decomposition of the
integrand as a spectral tensor train (STT), a continuous generalization of the
matrix product state. ID builds the STT by mapping the integrand to an
auxiliary many-body wavefunction that evolves in time from an initially
unentangled state. Each body-ordered term of the action corresponds to a
quantum gate, applied to the state during its evolution. The gates generate
entanglement, and decimation during the gate sequence compresses the integral,
alleviating memory bottlenecks in high dimensional integration. In the
application of ID to moment-generating and partition functions, the continuous
nature of the STTs allows for analytical differentiation of the result. To
demonstrate its versatility, we employ ID to calculate the partition function
of a classical XY model and to solve a non-Markovian quantum relaxation
problem. By circumventing the barren plateau problem that limited our earlier
STT-based approaches to quantum relaxation [J. Chem. Phys. 161, 234111 (2024)],
ID enables high-accuracy simulations of quantum dynamics in systems as large as
a 40-site chain.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [59] [(CMC) 1-immersions of surfaces into hyperbolic 3-manifolds](https://arxiv.org/abs/2506.11894)
*Gabriella Tarantello,Stefano Trapani*

Main category: math.DG

TL;DR: The paper explores CMC 1-immersions of surfaces into hyperbolic 3-manifolds, highlighting their critical nature, singularities, and connections to minimal immersions and group representations. It addresses existence, uniqueness, and blow-up phenomena across all genera.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by Bryant's surprising links between CMC 1-immersions and minimal immersions, and Uhlenbeck's interest in irreducible representations of fundamental groups. The critical value of mean curvature (1) and singularities add complexity.

Method: The analysis involves parametrizing CMC c-immersions for |c| < 1 via the Teichmueller space's tangent bundle. For c=1, the study focuses on limit behavior, blow-up phenomena, and introduces an orthogonality condition to handle these.

Result: Existence and uniqueness of CMC 1-immersions are established under a generic condition, overcoming blow-up issues for surfaces of any genus.

Conclusion: The paper resolves critical challenges in CMC 1-immersions, providing a framework for their study and applications in hyperbolic geometry and group theory.

Abstract: Constant Mean Curvature (CMC) 1-immersions of surfaces into hyperbolic
3-manifolds are natural and yet rather curious objects in hyperbolic geometry
with interesting applications.
  Firstly, Bryant revealed surprising relations between (CMC) $1$-immersions of
surfaces into $\mathbb H^3$ (Bryant surfaces) and (cousins) minimal immersions
into $\mathbb E^3.$ In addition, the interest to (CMC) immersions of a surface
$S$ (closed, orientable, with genus $\mathfrak{g} \geq2$) into hyperbolic
3-manifolds was motivated by Uhlenbeck in connection to irreducible
representations of the fundamental group $\pi_{1}(S)$ into $PSL(2,\mathbb{C}).$
However a (CMC) 1-immersed compact surface is likely to develop singularities
(punctures at finitely many points), and indeed in our analysis the prescribed
value 1 of the mean curvature enters as a "critical" parameter.
  In fact, Huang-Lucia-Tarantello showed that (CMC) $c$-immersions of $S$ into
hyperbolic 3-manifolds exist for $|c | <1$ and are parametrized by elements of
the tangent bundle of the Teichmueller space of $S.$ More importantly, (CMC)
$1$-immersions are attained only as "limits" for $|c| \to 1^-$ . In general the
passage to the limit can be prevented by possible blow-up phenomena captured in
terms of the Kodaira map and its suitable extension respectively for genus
$\mathfrak{g}=2$ and $\mathfrak{g}=3.$ Here we handle the case of surfaces of
any genus. In Theorem , we are able to encompass the blow up situation in terms
of an appropriate "orthogonality" condition. Subsequently, we can provide the
existence and uniqueness of (CMC) 1-immersions under an appropriate "generic"
condition, see Theorem 2.

</details>
