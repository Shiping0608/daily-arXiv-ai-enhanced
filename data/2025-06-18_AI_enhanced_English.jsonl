{"id": "2506.13880", "pdf": "https://arxiv.org/pdf/2506.13880", "abs": "https://arxiv.org/abs/2506.13880", "authors": ["Lea Happel", "Hanne Hardering", "Simon Praetorius", "Axel Voigt"], "title": "Surface Minkowski tensors to characterize shapes on curved surfaces", "categories": ["math.NA"], "comment": "33 pages, 30 figures", "summary": "We introduce surface Minkowski tensors to characterize rotational symmetries of shapes embedded in curved surfaces. The definition is based on a modified vector transport of the shapes boundary co-normal into a reference point which accounts for the angular defect that a classical parallel transport would introduce. This modified transport can be easily implemented for general surfaces and differently defined embedded shapes, and the associated irreducible surface Minkowski tensors give rise to the classification of shapes by their normalized eigenvalues, which are introduced as shape measures following the flat-space analog. We analyze different approximations of the embedded shapes, their influence on the surface Minkowski tensors, and the stability to perturbations of the shape and the surface. The work concludes with a series of numerical experiments showing the applicability of the approach on various surfaces and shape representations and an application in biology in which the characterization of cells in a curved monolayer of cells is considered.", "AI": {"tldr": "Surface Minkowski tensors are introduced to analyze rotational symmetries of shapes on curved surfaces, using a modified vector transport to account for angular defects. The method is versatile and applied to various surfaces and shapes, including biological cell characterization.", "motivation": "To characterize rotational symmetries of shapes embedded in curved surfaces, addressing the limitations of classical parallel transport by accounting for angular defects.", "method": "Uses a modified vector transport of boundary co-normals into a reference point, creating irreducible surface Minkowski tensors for shape classification via normalized eigenvalues.", "result": "Demonstrates applicability across surfaces and shape representations, with stability analysis against perturbations. Numerical experiments validate the method, including biological applications.", "conclusion": "The approach effectively classifies shapes on curved surfaces, with practical applications in fields like biology, showcasing robustness and versatility."}}
{"id": "2506.13950", "pdf": "https://arxiv.org/pdf/2506.13950", "abs": "https://arxiv.org/abs/2506.13950", "authors": ["Dimitrios G. Patsatzis", "Nikolaos Kazantzis", "Ioannis G. Kevrekidis", "Constantinos Siettos"], "title": "A Hybrid Neural Network -- Polynomial Series Scheme for Learning Invariant Manifolds of Discrete Dynamical Systems", "categories": ["math.NA", "cs.LG", "math.DS"], "comment": "36 pages (31 pages of main text and Appendix, 5 of Supplement), 8 Figures (6 in the main text and Appendix and 2 in the Supplement)", "summary": "We propose a hybrid machine learning scheme to learn -- in physics-informed and numerical analysis-informed fashion -- invariant manifolds (IM) of discrete maps for constructing reduced-order models (ROMs) for dynamical systems. The proposed scheme combines polynomial series with shallow neural networks, exploiting the complementary strengths of both approaches. Polynomials enable an efficient and accurate modeling of ROMs with guaranteed local exponential convergence rate around the fixed point, where, under certain assumptions, the IM is demonstrated to be analytic. Neural networks provide approximations to more complex structures beyond the reach of the polynomials' convergence. We evaluate the efficiency of the proposed scheme using three benchmark examples, examining convergence behavior, numerical approximation accuracy, and computational training cost. Additionally, we compare the IM approximations obtained solely with neural networks and with polynomial expansions. We demonstrate that the proposed hybrid scheme outperforms both pure polynomial approximations (power series, Legendre and Chebyshev polynomials) and standalone shallow neural network approximations in terms of numerical approximation accuracy.", "AI": {"tldr": "A hybrid machine learning scheme combining polynomials and shallow neural networks is proposed for learning invariant manifolds in reduced-order models, outperforming pure polynomial or neural network methods in accuracy.", "motivation": "To improve the accuracy and efficiency of reduced-order models (ROMs) for dynamical systems by leveraging the strengths of both polynomial series and neural networks.", "method": "The hybrid scheme integrates polynomial series (for local exponential convergence) and shallow neural networks (for complex structures beyond polynomials), evaluated on three benchmark examples.", "result": "The hybrid scheme achieves higher numerical approximation accuracy than pure polynomial or neural network methods, with better convergence and computational efficiency.", "conclusion": "The proposed hybrid approach is effective for constructing ROMs, combining the advantages of polynomials and neural networks for superior performance."}}
{"id": "2506.14073", "pdf": "https://arxiv.org/pdf/2506.14073", "abs": "https://arxiv.org/abs/2506.14073", "authors": ["Timo Sprekeler", "Han Wu", "Zhiwen Zhang"], "title": "Numerical approximation of effective diffusivities in homogenization of nondivergence-form equations with large drift by a Lagrangian method", "categories": ["math.NA"], "comment": null, "summary": "In this paper, we study numerical methods for the homogenization of linear second-order elliptic equations in nondivergence-form with periodic diffusion coefficients and large drift terms. Upon noting that the effective diffusion matrix can be characterized through the long-time variance of an associated diffusion process, we construct a Lagrangian numerical scheme based on a direct simulation of the underlying stochastic differential equation and utilizing the framework of modified equations, thereby avoiding the need to solve the Fokker--Planck--Kolmogorov equation. Through modified equation analysis, we derive higher-order weak convergence rates for our method. Finally, we conduct numerical experiments to demonstrate the accuracy of the proposed method. The results show that the method efficiently computes effective diffusivities, even in high dimensions.", "AI": {"tldr": "A Lagrangian numerical scheme is proposed for homogenizing linear second-order elliptic equations with periodic diffusion and large drift, avoiding Fokker--Planck--Kolmogorov equations. Higher-order weak convergence rates are derived, and numerical experiments confirm accuracy, even in high dimensions.", "motivation": "To address the challenge of homogenizing linear second-order elliptic equations with periodic diffusion and large drift terms efficiently, especially in high dimensions.", "method": "A Lagrangian numerical scheme based on simulating the underlying stochastic differential equation, using modified equations to avoid solving the Fokker--Planck--Kolmogorov equation.", "result": "The method efficiently computes effective diffusivities, with numerical experiments validating its accuracy, even in high dimensions.", "conclusion": "The proposed Lagrangian scheme is effective for homogenization problems, offering higher-order convergence and scalability to high dimensions."}}
{"id": "2506.14455", "pdf": "https://arxiv.org/pdf/2506.14455", "abs": "https://arxiv.org/abs/2506.14455", "authors": ["Neela Nataraj", "Ricardo Ruiz-Baier", "Aamir Yousuf"], "title": "Unified numerical analysis for thermoelastic diffusion and thermo-poroelasticity of thin plates", "categories": ["math.NA"], "comment": null, "summary": "We investigate a coupled hyperbolic-parabolic system modeling thermoelastic diffusion (resp. thermo-poroelasticity) in plates, consisting of a fourth-order hyperbolic partial differential equation for plate deflection and two second-order parabolic partial differential equations for the first moments of temperature and chemical potential (resp. pore pressure). The unique solvability of the system is established via Galerkin approach, and the additional regularity of the solution is obtained under appropriately strengthened data. For numerical approximation, we employ the Newmark method for time discretization of the hyperbolic term and a continuous interior penalty scheme for the spatial discretization of displacement. For the parabolic equations that represent the first moments of temperature and chemical potential (resp. pore pressure), we use the Crank--Nicolson method for time discretization and conforming finite elements for spatial discretization. The convergence of the fully discrete scheme with quasi-optimal rates in space and time is established. The numerical experiments demonstrate the effectiveness of the 2D Kirchhoff--Love plate model in capturing thermoelastic diffusion and thermo-poroelastic behavior in specific materials. We illustrate that as plate thickness decreases, the two-dimensional simulations closely approximate the results of three-dimensional problem. Finally, the numerical experiments also validate the theoretical rates of convergence.", "AI": {"tldr": "The paper studies a coupled hyperbolic-parabolic system for thermoelastic diffusion/thermo-poroelasticity in plates, proving unique solvability and regularity. It proposes a numerical scheme with convergence proofs and validates results via experiments.", "motivation": "To model and analyze thermoelastic diffusion and thermo-poroelasticity in plates, addressing the interplay of hyperbolic and parabolic PDEs and their numerical approximation.", "method": "Galerkin approach for solvability, Newmark and Crank-Nicolson for time discretization, and finite elements for spatial discretization.", "result": "Unique solvability, regularity, and quasi-optimal convergence rates are proven. Numerical experiments validate the 2D model's accuracy and convergence.", "conclusion": "The 2D Kirchhoff-Love plate model effectively captures the behavior of the system, with numerical results aligning with theoretical predictions."}}
{"id": "2506.13789", "pdf": "https://arxiv.org/pdf/2506.13789", "abs": "https://arxiv.org/abs/2506.13789", "authors": ["Subin Choi", "Chanmi Jung", "Dae Hoon Lee", "Jeongan Choi", "Jaekwang Kim"], "title": "Numerical Modeling of n-Hexane Pyrolysis with an Optimized Kinetic Mechanism in a Hydrogen Plasma Reactor", "categories": ["physics.plasm-ph", "physics.flu-dyn"], "comment": "31 pages, 13 figures", "summary": "The physicochemical mechanisms underlying the pyrolysis of n-hexane in a high temperature Ar-H2 environment were investigated for plasma pyrolysis process. An optimal chemical kinetics model was developed using the Reaction Mechanism Generator (RMG), an automated tool for constructing reaction mechanisms. This model was validated through 0-D analyses, where simulation result were compared with existing kinetic models (LLNL,JetSurf) and experimental data from conventional n-hexane pyrolysis. Subsequently, 1-D analysis were conducted to identify the optimal operational flow rate in plasma pyrolysis reactor, the results of which informed detailed three-dimensional (2-D) computational fluid dynamics (CFD) modeling of the plasma reactor. The CFD simulations reveal that fluid mixing dynamics play a dominant role in determining the extent of conversion and product selectivity, highlighting the limitations of lower-dimensional models in capturing essential transport phenomena. Notably, the simulations indicate a higher C2 monomer selectivity of approximately 50 % under plasma-based n-hexane pyrolysis, in contrast to the roughly 30 % selectivity achieved via conventional fossil-fuel-based methods. These findings underscore the potential advantages of plasma-driven pyrolysis and represent a critical step toward a comprehensive understanding of the complex thermochemical behavior governing plasma-assisted processes.", "AI": {"tldr": "The paper investigates n-hexane pyrolysis in an Ar-H2 plasma environment, develops a kinetic model using RMG, validates it, and uses CFD to show higher C2 monomer selectivity (50%) compared to conventional methods (30%).", "motivation": "To understand the physicochemical mechanisms of n-hexane pyrolysis in plasma environments and improve process efficiency.", "method": "Developed a kinetic model with RMG, validated via 0-D analyses, conducted 1-D analysis for flow rate optimization, and performed 2-D CFD modeling.", "result": "CFD simulations showed fluid mixing dynamics are crucial, with plasma pyrolysis achieving ~50% C2 monomer selectivity vs. ~30% conventionally.", "conclusion": "Plasma-driven pyrolysis offers advantages in selectivity, highlighting the need for higher-dimensional models to capture transport phenomena."}}
{"id": "2506.13873", "pdf": "https://arxiv.org/pdf/2506.13873", "abs": "https://arxiv.org/abs/2506.13873", "authors": ["M. Janssen", "C. -k. Chan", "J. Davelaar", "I. Natarajan", "H. Olivares", "B. Ripperda", "J. R\u00f6der", "M. Rynge", "M. Wielgus"], "title": "Deep learning inference with the Event Horizon Telescope I. Calibration improvements and a comprehensive synthetic data library", "categories": ["astro-ph.IM", "astro-ph.HE", "physics.comp-ph"], "comment": "15 pages, 8 figures, A&A submitted 16/01/2025, accepted 08/04/2025, published 07/06/2025", "summary": "(abridged) In a series of publications, we describe a comprehensive comparison of Event Horizon Telescope (EHT) data with theoretical models of Sgr A* and M87*. Here, we report on improvements made to our observational data reduction pipeline and present the generation of observables derived from the EHT models. We make use of ray-traced GRMHD simulations that are based on different black hole spacetime metrics and accretion physics parameters. These broad classes of models provide a good representation of the primary targets observed by the EHT. To generate realistic synthetic data from our models, we took the signal path as well as the calibration process, and thereby the aforementioned improvements, into account. We could thus produce synthetic visibilities akin to calibrated EHT data and identify salient features for the discrimination of model parameters. We have produced a library consisting of an unparalleled 962,000 synthetic Sgr A* and M87* datasets. In terms of baseline coverage and noise properties, the library encompasses 2017 EHT measurements as well as future observations with an extended telescope array. We differentiate between robust visibility data products related to model features and data products that are strongly affected by data corruption effects. Parameter inference is mostly limited by intrinsic model variability, which highlights the importance of long-term monitoring observations with the EHT. In later papers in this series, we will show how a Bayesian neural network trained on our synthetic data is capable of dealing with the model variability and extracting physical parameters from EHT observations. With our calibration improvements, our newly reduced EHT datasets have a considerably better quality compared to previously analyzed data.", "AI": {"tldr": "The paper discusses improvements to the EHT data reduction pipeline, generation of synthetic observables from GRMHD simulations, and a large library of synthetic datasets for Sgr A* and M87*. It highlights challenges in parameter inference and future applications using Bayesian neural networks.", "motivation": "To enhance the comparison of EHT data with theoretical models of black holes (Sgr A* and M87*) by improving data reduction and generating realistic synthetic datasets.", "method": "Uses ray-traced GRMHD simulations based on various black hole metrics and accretion physics, incorporating signal path and calibration improvements to produce synthetic visibilities.", "result": "Created a library of 962,000 synthetic datasets, identified robust visibility data products, and noted limitations due to intrinsic model variability.", "conclusion": "Improved calibration enhances data quality, and future work will use Bayesian neural networks to handle model variability and extract physical parameters."}}
{"id": "2506.13891", "pdf": "https://arxiv.org/pdf/2506.13891", "abs": "https://arxiv.org/abs/2506.13891", "authors": ["Bernd Rummler", "Michael Ruzicka", "Gudrun Th\u00e4ter"], "title": "Exact Poincar\u00e9 Constants in three-dimensional Annuli", "categories": ["math.AP"], "comment": null, "summary": "We study 3d-annuli. In our non-dimensional setting each annulus $\u03a9_{\\cal A}$ is defined via two concentrical balls with radii ${\\cal A}/2$ and ${\\cal A}/2 +1$. For these geometries we provide the exact value for the Poincar\u00e9 constants for scalar functions and calculate precise Poincar\u00e9 constants for solenoidal vector fields (in both cases with vanishing Dirichlet traces on the boundary). For this we use the first eigenvalues of the scalar Laplacian and the Stokes operator, respectively. Additionally, corresponding problems in domains $\u03a9_\u03c3^{*}$, the 3d-annuli are investigated - for comparison but also to provide limits for ${\\cal\n  A}\\,\\to\\,0$. In particular, the Green's function of the Laplacian on $\u03a9_\u03c3^{*}$ with vanishing Dirichlet traces on $\\partial \u03a9_\u03c3^{*}$ is used to show that for $\u03c3\\,\\to\\,0$ the first eigenvalue here tends to the first eigenvalue of the corresponding problem on the open unit ball. On the other hand, we take advantage of the so-called small-gap limit for ${\\cal A}\\to\\infty$.", "AI": {"tldr": "The paper studies 3D-annuli, providing exact Poincar\u00e9 constants for scalar functions and solenoidal vector fields, using eigenvalues of the Laplacian and Stokes operator. It also compares results with domains \u03a9_\u03c3\u2217 and analyzes limits as A\u21920 and A\u2192\u221e.", "motivation": "To understand and quantify Poincar\u00e9 constants in 3D-annuli and related domains, providing insights into their behavior under varying geometric parameters.", "method": "Utilizes eigenvalues of the scalar Laplacian and Stokes operator to derive Poincar\u00e9 constants. Compares results with domains \u03a9_\u03c3\u2217 and analyzes limits for A\u21920 and A\u2192\u221e.", "result": "Exact Poincar\u00e9 constants for scalar functions and solenoidal vector fields in 3D-annuli are derived. The first eigenvalue for \u03a9_\u03c3\u2217 tends to that of the unit ball as \u03c3\u21920, and the small-gap limit is explored for A\u2192\u221e.", "conclusion": "The study provides precise Poincar\u00e9 constants for 3D-annuli and related domains, with insights into their limiting behaviors, contributing to the understanding of these geometric configurations."}}
{"id": "2506.14543", "pdf": "https://arxiv.org/pdf/2506.14543", "abs": "https://arxiv.org/abs/2506.14543", "authors": ["Bosco Garc\u00eda-Archilla", "Alicia Garc\u00eda-Mascaraque", "Julia Novo"], "title": "Using BDF schemes in the temporal integration of POD-ROM methods", "categories": ["math.NA"], "comment": null, "summary": "In this paper we consider the numerical approximation of a semilinear reaction-diffusion model problem (PDEs) by means of reduced order methods (ROMs) based on proper orthogonal decomposition (POD). We focus on the time integration of the fully discrete reduced order model. Most of the analysis in the literature has been carried out for the implicit Euler method as time integrator. We integrate in time the reduced order model with the BDF-q time stepping ($1\\le q\\le 5$) and prove optimal rate of convergence of order $q$ in time. Our set of snapshots is obtained from finite element approximations to the original model problem computed at different times. These finite element approximations can be obtained with any time integrator. The POD method is based on first order difference quotients of the snapshots. The reason for doing this is twofold. On the one hand, the use of difference quotients allow us to provide pointwise-in-time error bounds. On the other, the use of difference quotients is essential to get the expected rate $q$ in time since we apply that the BDF-q time stepping, $1\\le q\\le 5$, can be written as a linear combination of first order difference quotients.", "AI": {"tldr": "The paper analyzes the numerical approximation of a semilinear reaction-diffusion PDE using reduced order methods (ROMs) with POD, focusing on BDF-q time integration for optimal convergence rates.", "motivation": "To improve the time integration of reduced order models (ROMs) beyond the implicit Euler method, ensuring optimal convergence rates with BDF-q schemes.", "method": "Uses POD-based ROMs with BDF-q time stepping (1 \u2264 q \u2264 5) and snapshots from finite element approximations, leveraging first-order difference quotients for analysis.", "result": "Proves optimal convergence rate of order q in time for BDF-q schemes, enabled by the use of difference quotients in POD.", "conclusion": "The approach successfully achieves optimal time convergence rates for ROMs, generalizing beyond the implicit Euler method."}}
{"id": "2506.14048", "pdf": "https://arxiv.org/pdf/2506.14048", "abs": "https://arxiv.org/abs/2506.14048", "authors": ["Madox C. McGrae-Menge", "Jacob R. Pierce", "Frederico Fiuza", "E. Paulo Alves"], "title": "Embedding physical symmetries into machine-learned reduced plasma physics models via data augmentation", "categories": ["physics.plasm-ph"], "comment": null, "summary": "Machine learning is offering powerful new tools for the development and discovery of reduced models of nonlinear, multiscale plasma dynamics from the data of first-principles kinetic simulations. However, ensuring the physical consistency of such models requires embedding fundamental symmetries of plasma dynamics. In this work, we explore a symmetry-embedding strategy based on data augmentation, where symmetry-preserving transformations (e.g., Lorentz and Galilean boosts) are applied to simulation data. Using both sparse regression and neural networks, we show that models trained on symmetry-augmented data more accurately infer the plasma fluid equations and pressure tensor closures from fully kinetic particle-in-cell simulations of magnetic reconnection. We show that this approach suppresses spurious inertial-frame-dependent correlations between dynamical variables, improves data efficiency, and significantly outperforms models trained without symmetry-augmented data, as well as commonly used theoretical pressure closure models. Our results establish symmetry-based data augmentation as a broadly applicable method for incorporating physical structure into machine-learned reduced plasma models.", "AI": {"tldr": "The paper proposes using symmetry-based data augmentation to improve machine-learned reduced plasma models, ensuring physical consistency and outperforming traditional methods.", "motivation": "To develop physically consistent reduced models of plasma dynamics by embedding fundamental symmetries into machine learning frameworks.", "method": "Uses data augmentation with symmetry-preserving transformations (e.g., Lorentz and Galilean boosts) on simulation data, employing sparse regression and neural networks.", "result": "Models trained on symmetry-augmented data infer plasma fluid equations and pressure tensor closures more accurately, suppress spurious correlations, and improve data efficiency.", "conclusion": "Symmetry-based data augmentation is a broadly applicable method for embedding physical structure into machine-learned plasma models."}}
{"id": "2506.13875", "pdf": "https://arxiv.org/pdf/2506.13875", "abs": "https://arxiv.org/abs/2506.13875", "authors": ["M. Janssen", "C. -k. Chan", "J. Davelaar", "M. Wielgus"], "title": "Deep learning inference with the Event Horizon Telescope II. The Zingularity framework for Bayesian artificial neural networks", "categories": ["astro-ph.IM", "astro-ph.HE", "physics.comp-ph"], "comment": "19 pages, 6 figures, A&A submitted 16/01/2025, accepted 30/03/2025, published 07/06/2025", "summary": "(abridged) In this second paper in our publication series, we present the open-source Zingularity framework for parameter inference with deep Bayesian artificial neural networks. We carried out out supervised learning with synthetic millimeter very long baseline interferometry observations of the EHT. Our ground-truth models are based on GRMHD simulations of Sgr A* and M87* on horizon scales. We investigated how well Zingularity neural networks are able to infer key model parameters from EHT observations, such as the black hole spin and the magnetic state of the accretion disk, when uncertainties in the data are accurately taken into account. Zingularity makes use of the TensorFlow Probability library and is able to handle large amounts of data with a combination of the efficient TFRecord data format plus the Horovod framework. Our approach is the first analysis of EHT data with Bayesian neural networks, where an unprecedented training data size, under consideration of a closely modeled EHT signal path, and the full information content of the observational data are used. Zingularity infers parameters based on salient features in the data and is containerized. Through parameter surveys and dedicated validation tests, we identified neural network architectures, that are robust against internal stochastic processes and unaffected by noise in the observational and model data. We give examples of how different data properties affect the network training. We show how the Bayesian nature of our networks gives trustworthy uncertainties and uncovers failure modes for uncharacterizable data. It is easy to achieve low validation errors during training on synthetic data with neural networks, particularly when the forward modeling is too simplified. Through careful studies, we demonstrate that our trained networks can generalize well so that reliable results can be obtained from observational data.", "AI": {"tldr": "The paper introduces the Zingularity framework for deep Bayesian neural network-based parameter inference, tested on EHT data, demonstrating robust performance and trustworthy uncertainties.", "motivation": "To address the challenge of inferring key parameters (e.g., black hole spin, accretion disk state) from EHT observations with accurate uncertainty handling.", "method": "Uses TensorFlow Probability and Horovod for scalable Bayesian neural networks, trained on synthetic GRMHD-based EHT data with realistic noise modeling.", "result": "Zingularity successfully infers parameters, handles large data, and provides reliable uncertainties, even with noisy or uncharacterizable data.", "conclusion": "The framework generalizes well to observational data, offering a robust tool for EHT parameter inference with trustworthy uncertainty estimates."}}
{"id": "2506.14119", "pdf": "https://arxiv.org/pdf/2506.14119", "abs": "https://arxiv.org/abs/2506.14119", "authors": ["Meng Zhao"], "title": "Level-3 large deviations for the white-forced 2D Navier-Stokes system in a bounded domain", "categories": ["math.AP", "math.PR"], "comment": null, "summary": "We study the large deviations principle (LDP) of Donsker-Varadhan type for the white-forced Navier-Stokes system in a bounded domain. Under the assumption that the noise is non-degenerate, we establish level-2 and level-3 LDPs with rate functions given by the Donsker-Varadhan formulas. The proof relies on an improved version of Kifer's criterion, a lift argument inspired from [DV83], an improved abstract result on the large-time asymptotics of generalized Markov semigroups, and a delicate approximation scheme utilizing the resolvent operators of the Markov semigroup.", "AI": {"tldr": "The paper establishes level-2 and level-3 large deviations principles (LDPs) for the white-forced Navier-Stokes system in a bounded domain, using Donsker-Varadhan formulas under non-degenerate noise.", "motivation": "To understand the large deviations behavior of the Navier-Stokes system under stochastic forcing, which is crucial for rare event analysis in fluid dynamics.", "method": "The proof combines Kifer's criterion, a lift argument, an abstract result on Markov semigroups, and a resolvent-based approximation scheme.", "result": "Level-2 and level-3 LDPs are proven with explicit Donsker-Varadhan rate functions.", "conclusion": "The work provides rigorous LDP results for the Navier-Stokes system, advancing stochastic fluid dynamics."}}
{"id": "2506.14558", "pdf": "https://arxiv.org/pdf/2506.14558", "abs": "https://arxiv.org/abs/2506.14558", "authors": ["Tim Jahn", "Mikhail Kirilin"], "title": "Convergence of generalized cross-validation with applications to ill-posed integral equations", "categories": ["math.NA"], "comment": null, "summary": "In this article, we rigorously establish the consistency of generalized cross-validation as a parameter-choice rule for solving inverse problems. We prove that the index chosen by leave-one-out GCV achieves a non-asymptotic, order-optimal error bound with high probability for polynomially ill-posed compact operators. Hereby it is remarkable that the unknown true solution need not satisfy a self-similarity condition, which is generally needed for other heuristic parameter choice rules. We quantify the rate and demonstrate convergence numerically on integral equation test cases, including image deblurring and CT reconstruction.", "AI": {"tldr": "The paper proves that generalized cross-validation (GCV) is a consistent parameter-choice rule for inverse problems, achieving optimal error bounds without requiring self-similarity of the true solution.", "motivation": "To rigorously validate GCV as a reliable parameter-choice rule for inverse problems, especially for polynomially ill-posed operators, without needing restrictive assumptions like self-similarity.", "method": "The authors use leave-one-out GCV to select parameters and prove non-asymptotic, order-optimal error bounds with high probability. Numerical tests on integral equations, image deblurring, and CT reconstruction validate the theory.", "result": "GCV achieves optimal error bounds without requiring self-similarity, and numerical experiments confirm its effectiveness.", "conclusion": "GCV is a robust and theoretically sound parameter-choice rule for inverse problems, even without restrictive assumptions on the true solution."}}
{"id": "2506.14343", "pdf": "https://arxiv.org/pdf/2506.14343", "abs": "https://arxiv.org/abs/2506.14343", "authors": ["A. V. Titova", "S. D. Korolkov", "V. V. Izmodenov"], "title": "Mathematical Modeling of Kelvin-Helmholtz Instability at Tangential Discontinuities in Partially Ionized Plasma: Application to Heliopause Dynamics", "categories": ["astro-ph.SR", "physics.plasm-ph"], "comment": "Accepted by Lobachavskii Journal of Mathematics", "summary": "The stability of the heliopause, the tangential discontinuity separating the solar wind from the interstellar medium, is influenced by various processes, including the Kelvin-Helmholtz instability. This study investigates the role of charge exchange collisions between protons and hydrogen (H) atoms in reduction of the Kelvin-Helmholtz growth rate at the heliopause. Using a two-dimensional gasdynamic model with the inclusion of H atoms, we perform numerical simulations of the plasma flow near the heliopause flanks. We conduct a parametric study by varying the Knudsen number. Our results indicate that charge exchange collisions play a crucial role in suppressing the Kelvin-Helmholtz instability. As the Knudsen number decreases, the flow transitions from an unstable regime to a smoother state.", "AI": {"tldr": "Charge exchange collisions between protons and hydrogen atoms reduce the Kelvin-Helmholtz instability at the heliopause, stabilizing the plasma flow as the Knudsen number decreases.", "motivation": "To understand how charge exchange collisions influence the stability of the heliopause by suppressing the Kelvin-Helmholtz instability.", "method": "A two-dimensional gasdynamic model incorporating hydrogen atoms is used for numerical simulations of plasma flow near the heliopause flanks, with a parametric study varying the Knudsen number.", "result": "Charge exchange collisions significantly suppress the Kelvin-Helmholtz instability, leading to smoother plasma flow as the Knudsen number decreases.", "conclusion": "Charge exchange processes are critical for stabilizing the heliopause by reducing the growth rate of the Kelvin-Helmholtz instability."}}
{"id": "2506.14044", "pdf": "https://arxiv.org/pdf/2506.14044", "abs": "https://arxiv.org/abs/2506.14044", "authors": ["Matheus Rolim Sales", "Leonardo Costa de Souza", "Daniel Borin", "Michele Mugnaine", "Jos\u00e9 Danilo Szezech", "Ricardo Luiz Viana", "Iber\u00ea Luiz Caldas", "Edson Denis Leonel", "Chris G. Antonopoulos"], "title": "pynamicalsys: A Python toolkit for the analysis of dynamical systems", "categories": ["nlin.CD", "physics.comp-ph"], "comment": null, "summary": "Since Lorenz's seminal work on a simplified weather model, the numerical analysis of nonlinear dynamical systems has become one of the main subjects of research in physics. Despite of that, there remains a need for accessible, efficient, and easy-to-use computational tools to study such systems. In this paper, we introduce pynamicalsys, a simple yet powerful open-source Python module for the analysis of nonlinear dynamical systems. In particular, pynamicalsys implements tools for trajectory simulation, bifurcation diagrams, Lyapunov exponents and several others chaotic indicators, period orbit detection and their manifolds, as well as escape and basins analysis. It also includes many built-in models and the use of custom models is straighforward. We demonstrate the capabilities of pynamicalsys through a series of examples that reproduces well-known results in the literature while developing the mathematical analysis at the same time. We also provide the Jupyter notebook containing all the code used in this paper, including performance benchmarks. pynamicalsys is freely available via the Python Package Index (PyPI) and is indented to support both research and teaching in nonlinear dynamics.", "AI": {"tldr": "The paper introduces pynamicalsys, a Python module for analyzing nonlinear dynamical systems, offering tools like trajectory simulation and Lyapunov exponents, with built-in and custom models.", "motivation": "There's a lack of accessible, efficient tools for studying nonlinear dynamical systems, despite their importance in physics.", "method": "The paper presents pynamicalsys, an open-source Python module with various analysis tools and built-in models, demonstrated through examples.", "result": "pynamicalsys successfully reproduces known results and supports research and teaching in nonlinear dynamics.", "conclusion": "pynamicalsys is a powerful, user-friendly tool for nonlinear dynamics analysis, freely available for research and education."}}
{"id": "2506.14258", "pdf": "https://arxiv.org/pdf/2506.14258", "abs": "https://arxiv.org/abs/2506.14258", "authors": ["Simone Ciani", "Eurica Henriques", "Mariia O. Savchenko", "Igor I. Skrypnik"], "title": "Parabolic De Giorgi classes with doubly nonlinear, nonstandard growth: local boundedness under exact integrability assumptions", "categories": ["math.AP"], "comment": null, "summary": "We define a suitable class $\\mathcal{PDG}$ of functions bearing unbalanced energy estimates, that are embodied by local weak subsolutions to doubly nonlinear, double-phase, Orlicz-type and fully anisotropic operators. Yet we prove that members of $\\mathcal{PDG}$ are locally bounded, under critical, sub-critical and limit growth conditions typical of singular parabolic operators, with quantitative a priori estimates that follow the lines of the pioneering work of Ladyzhenskaya, Solonnikov and Uraltseva \\cite{LadSolUra}. These local bounds are new in the sub-critical cases, even for the classic $p$-Laplacean equations, since no extra-integrability condition is needed.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2506.14685", "pdf": "https://arxiv.org/pdf/2506.14685", "abs": "https://arxiv.org/abs/2506.14685", "authors": ["Erik Burman", "Mingfei Lu"], "title": "Posterior contraction rates of computational methods for Bayesian data assimilation", "categories": ["math.NA", "math.PR", "math.ST"], "comment": null, "summary": "In this paper, we analyze posterior consistency of a Bayesian data assimilation problem under discretization. We prove convergence rates for the discrete posterior to ground truth solution under both conforming discretization and finite element discretization (usually non-conforming). The analysis is based on the coupling of asymptotics between the number of samples and the dimension of discrete spaces. In the finite element discretization, tailor-made discrete priors, instead of the discretization of continuous priors, are used to generate an optimal convergence rate.", "AI": {"tldr": "Analysis of Bayesian data assimilation's posterior consistency under discretization, proving convergence rates for discrete posteriors to ground truth under conforming and finite element discretization.", "motivation": "To understand and quantify the convergence behavior of Bayesian data assimilation under discretization, ensuring reliable results in practical applications.", "method": "Coupling asymptotics between sample numbers and discrete space dimensions; using tailor-made discrete priors for finite element discretization.", "result": "Proven convergence rates for discrete posteriors to ground truth, with optimal rates achieved using tailored priors in finite element cases.", "conclusion": "The study provides theoretical guarantees for Bayesian data assimilation under discretization, highlighting the importance of tailored priors for optimal performance."}}
{"id": "2506.14668", "pdf": "https://arxiv.org/pdf/2506.14668", "abs": "https://arxiv.org/abs/2506.14668", "authors": ["Chengcai Shen", "Xiaocan Li", "Yuan-Kuen Ko", "John C. Raymond", "Fan Guo", "Vanessa Polito", "Viviane Pierrard"], "title": "Modeling of Ionization and Recombination Processes in Plasma with Arbitrary Non-Maxwellian Electron Distributions", "categories": ["astro-ph.SR", "physics.plasm-ph"], "comment": null, "summary": "In astronomical environments, the high-temperature emission of plasma mainly depends on ion charge states, which requires accurate analysis of the ionization and recombination processes. For various phenomena involving energetic particles, the non-Maxwellian distributions of electrons exhibiting high-energy tails can significantly enhance the ionization process. Therefore, accurately computing ionization and recombination rates with non-Maxwellian electron distributions is essential for emission diagnostic analysis. In this work, we report two methods for fitting various non-Maxwellian distributions by using the Maxwellian decomposition strategy. For standard \\{kappa} distributions, the calculated ionization and recombination rate coefficients show comparable accuracy to other public packages. We apply the above methods to two specific non-Maxwellian distribution scenarios: (I) accelerated electron distributions due to magnetic reconnection revealed in a combined MHD-particle simulation; (II) the high-energy truncated \\{kappa} distribution predicted by the exospheric model of the solar wind. During the electron acceleration process, ionization rates of high-temperature iron ions increase significantly compared to their initial Maxwellian distribution, while the recombination rates may decrease due to the electron distribution changes in low-energy ranges. This can potentially lead to an overestimation of the plasma temperature when analyzing the Fe emission lines under the Maxwellian distribution assumption. For the truncated \\{kappa} distribution in the solar wind, the ionization rates are lower than those for the standard \\{kappa} distribution, while the recombination rates remain similar. This leads to an overestimation of plasma temperature when assuming a \\{kappa} distribution.", "AI": {"tldr": "The paper discusses the impact of non-Maxwellian electron distributions on ionization and recombination rates in astronomical plasmas, proposing methods to fit such distributions and highlighting temperature estimation errors.", "motivation": "Accurate analysis of ionization and recombination processes is crucial for high-temperature plasma emission diagnostics, especially with non-Maxwellian electron distributions.", "method": "Two methods using Maxwellian decomposition to fit non-Maxwellian distributions, applied to scenarios like magnetic reconnection and solar wind.", "result": "Non-Maxwellian distributions significantly alter ionization and recombination rates, leading to potential temperature overestimation in emission line analysis.", "conclusion": "The study underscores the importance of considering non-Maxwellian distributions for accurate plasma diagnostics, revealing biases in temperature estimates under Maxwellian assumptions."}}
{"id": "2506.14097", "pdf": "https://arxiv.org/pdf/2506.14097", "abs": "https://arxiv.org/abs/2506.14097", "authors": ["Bryce Palmer", "Hasan Metin Aktulga", "Tong Gao"], "title": "ReLCP: Scalable Complementarity-Based Collision Resolution for Smooth Rigid Bodies", "categories": ["cs.RO", "cond-mat.soft", "physics.comp-ph"], "comment": null, "summary": "We present a complementarity-based collision resolution algorithm for smooth, non-spherical, rigid bodies. Unlike discrete surface representation approaches, which approximate surfaces using discrete elements (e.g., tessellations or sub-spheres) with constraints between nearby faces, edges, nodes, or sub-objects, our algorithm solves a recursively generated linear complementarity problem (ReLCP) to adaptively identify potential collision locations during the collision resolution procedure. Despite adaptively and in contrast to Newton-esque schemes, we prove conditions under which the resulting solution exists and the center of mass translational and rotational dynamics are unique. Our ReLCP also converges to classical LCP-based collision resolution for sufficiently small timesteps. Because increasing the surface resolution in discrete representation methods necessitates subdividing geometry into finer elements -- leading to a super-linear increase in the number of collision constraints -- these approaches scale poorly with increased surface resolution. In contrast, our adaptive ReLCP framework begins with a single constraint per pair of nearby bodies and introduces new constraints only when unconstrained motion would lead to overlap, circumventing the oversampling required by discrete methods. By requiring one to two orders of magnitude fewer collision constraints to achieve the same surface resolution, we observe 10-100x speedup in densely packed applications. We validate our ReLCP method against multisphere and single-constraint methods, comparing convergence in a two-ellipsoid collision test, scalability and performance in a compacting ellipsoid suspension and growing bacterial colony, and stability in a taut chainmail network, highlighting our ability to achieve high-fidelity surface representations without suffering from poor scalability or artificial surface roughness.", "AI": {"tldr": "A complementarity-based collision resolution algorithm (ReLCP) for smooth, non-spherical rigid bodies is introduced, offering adaptive collision detection and resolution with fewer constraints and better scalability than discrete methods.", "motivation": "Discrete surface representation methods for collision resolution scale poorly with increased surface resolution due to super-linear increases in constraints. The paper aims to address this inefficiency.", "method": "The algorithm solves a recursively generated linear complementarity problem (ReLCP) to adaptively identify collision locations, starting with minimal constraints and adding more only when necessary.", "result": "The ReLCP method requires 10-100x fewer constraints than discrete methods, achieving significant speedups (10-100x) in densely packed scenarios while maintaining high-fidelity surface representations.", "conclusion": "The ReLCP framework outperforms traditional discrete methods in scalability, performance, and stability, enabling high-resolution collision resolution without artificial roughness or poor scaling."}}
{"id": "2506.14309", "pdf": "https://arxiv.org/pdf/2506.14309", "abs": "https://arxiv.org/abs/2506.14309", "authors": ["Xuanrui Feng", "Zhenfu Wang"], "title": "Kac's Program for the Landau Equation", "categories": ["math.AP"], "comment": "53 pages", "summary": "We study the derivation of the spatially homogeneous Landau equation from the mean-field limit of some conservative $N$-particle system, which is formulated by passing to the grazing limit on Kac's walk in his program for the Boltzmann equation. It is the first time that propagation of chaos is resolved for some many-particle system approximating the Landau equation with Coulomb interactions, and that Kac's program is extended to the Landau equation in the regime of soft potentials. The convergence result is established in the weak sense, in the Wasserstein sense, and in the entropic sense, along with strong convergence in $L^1$.\n  Motivated by the recent work of Bresch, Duerinckx and Jabin \\cite{bresch2024duality}, we develop the duality approach to treat the singular Landau collision operator and adapt to Kac's program in kinetic theory. To overcome the difficulty generated by the Coulomb singularity, we prove some key functional estimates concerning the weighted Fisher information and the second-order Fisher information.", "AI": {"tldr": "Derivation of the Landau equation from a conservative N-particle system, extending Kac's program to soft potentials and proving propagation of chaos for Coulomb interactions.", "motivation": "Extend Kac's program to the Landau equation and resolve propagation of chaos for Coulomb interactions, inspired by recent work on duality approaches.", "method": "Duality approach to handle the singular Landau collision operator, with key functional estimates for weighted and second-order Fisher information.", "result": "Convergence established in weak, Wasserstein, entropic, and strong L1 senses.", "conclusion": "First resolution of propagation of chaos for Landau equation with Coulomb interactions, extending Kac's program to soft potentials."}}
{"id": "2506.14383", "pdf": "https://arxiv.org/pdf/2506.14383", "abs": "https://arxiv.org/abs/2506.14383", "authors": ["Sabri Koraltan", "Fabrizio Porrati", "Robert Kraft", "Sven Barth", "Markus Weigand", "Claas Abert", "Dieter Suess", "Michael Huth", "Sebastian Wintz"], "title": "Reconfigurable three dimensional magnetic nanoarchitectures", "categories": ["cond-mat.mes-hall", "cond-mat.mtrl-sci", "physics.comp-ph"], "comment": "10 pages, 6 figures", "summary": "Three-dimensional (3D) nanomagnetism is a rapidly developing field within magnetic materials research, where exploiting the third dimension unlocks opportunities for innovative applications in areas such as sensing, data storage, and neuromorphic computing. Among various fabrication techniques, focused electron beam-induced deposition (FEBID) offers high flexibility in creating complex 3D nanostructures with sub-100 nm resolution. A key challenge in the development of 3D nanomagnets is the ability to locally control the magnetic configuration, which is essential to achieve desired functionalities. In this work, the magnetization reversal mechanism of a three-dimensional nanoarchitecture fabricated using focused electron beam-induced deposition is investigated by combining direct observation via scanning transmission X-ray microscopy with finite element micromagnetic simulations. In particular, our investigation shows that the magnetization of the components of a three-dimensional Co3 Fe tetrapod can be reversed individually and sequentially. Finally, it is demonstrated that complete control and reconfigurability of the system can be achieved by tuning the direction of the applied magnetic field.", "AI": {"tldr": "The paper investigates magnetization reversal in 3D nanomagnets made via FEBID, combining X-ray microscopy and simulations to achieve local control.", "motivation": "To unlock innovative applications in sensing, data storage, and neuromorphic computing by controlling magnetic configurations in 3D nanostructures.", "method": "Combines scanning transmission X-ray microscopy and finite element micromagnetic simulations to study Co3 Fe tetrapods made via FEBID.", "result": "Individual and sequential magnetization reversal of tetrapod components is achieved, with full control by tuning the applied magnetic field direction.", "conclusion": "Demonstrates reconfigurable control of 3D nanomagnets, paving the way for advanced applications."}}
{"id": "2506.14423", "pdf": "https://arxiv.org/pdf/2506.14423", "abs": "https://arxiv.org/abs/2506.14423", "authors": ["Andrea Kubin"], "title": "On variational scheme modeling the anisotropic surface diffusion with elasticity in the plane", "categories": ["math.AP"], "comment": null, "summary": "In this paper, we prove the existence of classical solutions for the anisotropic surface diffusion with elasticity in the plane using a minimizing movements scheme, provided that the initial set is sufficiently regular. This scheme is inspired by the one introduced by Cahn-Taylor [15] to modeling the surface diffusion. Moreover, we prove that this scheme converges to the global solution of the equation.", "AI": {"tldr": "Existence of classical solutions for anisotropic surface diffusion with elasticity in the plane is proven using a minimizing movements scheme for sufficiently regular initial sets. The scheme converges to the global solution.", "motivation": "To address the problem of anisotropic surface diffusion with elasticity, extending the work of Cahn-Taylor [15] by proving existence and convergence of solutions.", "method": "A minimizing movements scheme inspired by Cahn-Taylor [15] is employed to model surface diffusion.", "result": "Existence of classical solutions is proven for sufficiently regular initial sets, and the scheme converges to the global solution.", "conclusion": "The minimizing movements scheme effectively solves the anisotropic surface diffusion problem with elasticity, confirming convergence to global solutions."}}
{"id": "2506.14424", "pdf": "https://arxiv.org/pdf/2506.14424", "abs": "https://arxiv.org/abs/2506.14424", "authors": ["Richard Schussnig", "Niklas Fehn", "Douglas Ramalho Queiroz Pacheco", "Martin Kronbichler"], "title": "Higher-Oder Splitting Schemes for Fluids with Variable Viscosity", "categories": ["cs.CE", "math-ph", "physics.comp-ph", "physics.flu-dyn"], "comment": null, "summary": "This article investigates matrix-free higher-order discontinuous Galerkin (DG) discretizations of the Navier-Stokes equations for incompressible flows with variable viscosity. The viscosity field may be prescribed analytically or governed by a rheological law, as often found in biomedical or industrial applications. We compare several linearized variants of saddle point block systems and projection-based splitting time integration schemes in terms of their computational performance. Compared to the velocity-pressure block-system for the former, the splitting scheme allows solving a sequence of simple problems such as mass, convection-diffusion and Poisson equations. We investigate under which conditions the improved temporal stability of fully implicit schemes and resulting expensive nonlinear solves outperform the splitting schemes and linearized variants that are stable under hyperbolic time step restrictions.\n  The key aspects of this work are i) the extension of the dual splitting method originally proposed by G.E. Karniadakis et al. (J. Comput. Phys. 97, 414-443, 1991) towards non-constant viscosity, ii) a higher-order DG method for incompressible flows with variable viscosity, iii) accelerated nonlinear solver variants and suitable linearizations adopting a matrix-free $hp$-multigrid solver, and iv) a detailed comparison of the monolithic and projection-based solvers in terms of their (non-)linear solver performance.\n  The presented schemes are evaluated in a series of numerical examples verifying their spatial and temporal accuracy, and the preconditioner performance under increasing viscosity contrasts, while their efficiency is showcased in the backward-facing step benchmark.", "AI": {"tldr": "The paper explores matrix-free higher-order DG methods for incompressible Navier-Stokes with variable viscosity, comparing monolithic and splitting schemes for computational efficiency.", "motivation": "To address challenges in simulating incompressible flows with variable viscosity, common in biomedical/industrial applications, by improving solver performance and stability.", "method": "Extends the dual splitting method to non-constant viscosity, uses higher-order DG, and compares monolithic vs. projection-based solvers with matrix-free hp-multigrid.", "result": "Numerical tests confirm accuracy and efficiency, with splitting schemes often outperforming monolithic ones under certain conditions.", "conclusion": "Splitting schemes and linearized variants can be more efficient than fully implicit methods, especially for high viscosity contrasts."}}
{"id": "2506.14462", "pdf": "https://arxiv.org/pdf/2506.14462", "abs": "https://arxiv.org/abs/2506.14462", "authors": ["Riccardo Cristoferi", "Luca Pignatelli"], "title": "Phase separation in multiply periodic materials with fine microstructures", "categories": ["math.AP"], "comment": null, "summary": "We study a Cahn-Hilliard model for phase separation in composite materials with multiple periodic microstructures. These are modeled by considering a highly oscillating potential. The focus of this paper is in the case where the scales of the microstructures are smaller than that of phase separation. We provide a compactness result and prove that the \u0393-limit of the energy is a multiple of the perimeter. In particular, using the recently introduced unfolding operator for multiple scales, we show that the taking the limit of all of the scales together is equivalent to taking one limit at the time, starting from the smaller scale and keeping the larger fixed.", "AI": {"tldr": "The paper analyzes a Cahn-Hilliard model for phase separation in composite materials with multiple periodic microstructures, focusing on cases where microstructure scales are smaller than phase separation scales. It provides a compactness result and proves the \u0393-limit of the energy is a multiple of the perimeter.", "motivation": "To understand phase separation in composite materials with highly oscillating potentials and multiple periodic microstructures, especially when microstructure scales are smaller than phase separation scales.", "method": "Uses the unfolding operator for multiple scales to analyze the \u0393-limit of the energy, showing equivalence between taking all limits together and sequentially.", "result": "The \u0393-limit of the energy is a multiple of the perimeter, and the limit process is equivalent whether taken simultaneously or sequentially.", "conclusion": "The study provides insights into phase separation in composite materials with multiple microstructures, demonstrating the equivalence of limit processes and the form of the \u0393-limit."}}
{"id": "2506.14500", "pdf": "https://arxiv.org/pdf/2506.14500", "abs": "https://arxiv.org/abs/2506.14500", "authors": ["Zi-Yang Zhan", "Zhen Chen"], "title": "Versatile SPH Open Boundary Conditions for Multiphase Flows in Extreme Condition", "categories": ["physics.flu-dyn", "physics.comp-ph"], "comment": "42 pages, 30 figures", "summary": "Although the Smoothed Particle Hydrodynamics (SPH) method has been demonstrated as a promising numerical solver for multiphase flow problems due to its Lagrangian nature, its application to complex channel flow may encounter additional issues such as the open boundary condition and numerical instability in the extreme flow state. The present work aims to establish a general SPH algorithm for accurate and stable simulation of complex multiphase flows in configurations with open boundaries. The general scheme of weakly compressible SPH is adopted with special treatments implemented to ease the numerical oscillation in the density discontinuity scenario, and the turbulent model is implemented for interpretations of extreme flow conditions in high Reynolds numbers. Then, the conventional open boundary condition is fine-tuned by two new algorithms to guarantee the numerical stability in the inflow and the outflow regions. Firstly, a density relaxation is proposed to alleviate the pressure instability in the inflow region, which improves the smoothness of the particle pre-processing procedure. Secondly, the particle shifting technique with adaptive damper is implemented to adjust the magnitude of correction in the outflow region, which helps to suppress the velocity oscillation near the outlet. Validations of the proposed algorithms are carried out through four classic numerical examples, presenting appealing agreements with the analytical solutions and thus demonstrating the versatility in various flow conditions. Then, the robustness of the method is established through the turbulent multiphase channel flow cases and the horizontal slug channel flow with large density ratios. These results shed light on the value of the proposed algorithm as a general solver for complex multiphase flow problems with open boundaries.", "AI": {"tldr": "The paper proposes a refined SPH method for stable and accurate simulation of complex multiphase flows with open boundaries, addressing issues like numerical instability and boundary conditions.", "motivation": "The SPH method, while promising for multiphase flows, faces challenges like open boundary conditions and instability in extreme flows, motivating the development of a more robust algorithm.", "method": "The study adopts a weakly compressible SPH scheme with density relaxation for inflow stability and an adaptive particle shifting technique for outflow stability, validated through numerical examples.", "result": "The proposed algorithms show good agreement with analytical solutions and demonstrate robustness in turbulent and high-density-ratio flows.", "conclusion": "The refined SPH method proves versatile and reliable for complex multiphase flows with open boundaries."}}
{"id": "2506.14463", "pdf": "https://arxiv.org/pdf/2506.14463", "abs": "https://arxiv.org/abs/2506.14463", "authors": ["Mourad Choulli"], "title": "Upper bound on the multiplicity of eigenvalues of the Sch\u00f6dinger-Dirichlet operator in dimension two", "categories": ["math.AP"], "comment": null, "summary": "We establish an upper bound on the multiplicity of eigenvalues of the Sch\u00f6dinger-Dirichlet operator in dimension two. We give a proof based on a generalized Morse Lemma due to Cheng \\cite{Ch}.", "AI": {"tldr": "The paper establishes an upper bound for eigenvalue multiplicity of the Sch\u00f6dinger-Dirichlet operator in 2D, using a generalized Morse Lemma.", "motivation": "To address the problem of eigenvalue multiplicity in the Sch\u00f6dinger-Dirichlet operator, particularly in two dimensions.", "method": "The proof relies on a generalized Morse Lemma by Cheng.", "result": "An upper bound on the multiplicity of eigenvalues is derived.", "conclusion": "The generalized Morse Lemma provides a viable approach to bounding eigenvalue multiplicity in this context."}}
{"id": "2506.14658", "pdf": "https://arxiv.org/pdf/2506.14658", "abs": "https://arxiv.org/abs/2506.14658", "authors": ["Tianyu Yuan", "Ivan Surovtsev", "Megan C. King", "Simon G. J. Mochrie"], "title": "First-passage time to capture for diffusion in a 3D harmonic potential", "categories": ["math-ph", "physics.chem-ph", "physics.comp-ph"], "comment": "7 pages, 4 figures", "summary": "We determine the survival probability and first-passage time (FPT) to capture for a harmonically trapped particle, diffusing outside an absorbing spherical boundary by directly solving the differential equation for the survival probability. This solution, obtained as an infinite sum over the relevant eigenfunctions, corrects previously published results [D. S. Grebenkov, J. Phys. A 48, 013001 (2014)]. To verify our calculations, we perform simulations of the survival probability, that accurately reproduce the analytic solutions for a range of parameter values. We then obtain the corresponding FPT distribution as the negative time derivative of the survival probability. Finally, we derive an expression for mean first-passage time (MFPT), also as a sum over eigenfunctions. Numerical evaluation of the first twenty-five terms in this sum closely matches the MFPT obtained by a different method in D. S. Grebenkov, J. Phys. A 48, 013001 (2014). We also find that, in the limit of vanishing trap stiffness, the amplitude of the first term in our infinite-sum solution for the survival probability matches the theoretical escape probability for the potential-free diffusion-to-capture process.", "AI": {"tldr": "The paper corrects previous results on the survival probability and first-passage time (FPT) of a trapped particle diffusing outside an absorbing boundary, providing analytic solutions and simulations for validation.", "motivation": "To address inaccuracies in prior work (D. S. Grebenkov, 2014) and provide correct solutions for the survival probability and FPT of a harmonically trapped particle.", "method": "Directly solving the differential equation for survival probability, expressing it as an infinite sum of eigenfunctions, and validating with simulations. The FPT distribution is derived from the survival probability.", "result": "Analytic solutions match simulations, and the mean first-passage time (MFPT) aligns with Grebenkov's results. The first term in the survival probability sum matches the escape probability for potential-free diffusion.", "conclusion": "The corrected solutions and derived expressions for survival probability and FPT are validated, bridging gaps in prior work and confirming theoretical limits."}}
{"id": "2506.14520", "pdf": "https://arxiv.org/pdf/2506.14520", "abs": "https://arxiv.org/abs/2506.14520", "authors": ["Matthias R\u00f6ger", "Fabian Rupp"], "title": "Elastic bilayer membranes under confinement -- Existence, regularity, and rigidity", "categories": ["math.AP", "math.DG"], "comment": "35 pages, comments welcome!", "summary": "Motivated by applications to cell biology, we study the constrained minimization of the Helfrich energy among closed surfaces confined to a container. We show existence of minimizers in the class of bubble trees of spherical weak branched immersions and derive the Euler--Lagrange equations which involve a measure-valued Lagrange multiplier that is concentrated on the coincidence set with the container boundary. We provide a careful analysis of this elliptic system and prove optimal regularity for solutions throughout the branch points. For surfaces confined in the unit ball we show that the minimization problem behaves rather rigid and identify a parameter range for which minimizers are always round spheres.", "AI": {"tldr": "Existence of minimizers for Helfrich energy among closed surfaces in containers, with analysis of Euler-Lagrange equations and optimal regularity. Rigidity shown for surfaces in unit balls.", "motivation": "Applications to cell biology drive the study of constrained minimization of Helfrich energy among closed surfaces in containers.", "method": "Study existence of minimizers in the class of bubble trees of spherical weak branched immersions, derive Euler-Lagrange equations, and analyze the system.", "result": "Existence of minimizers proven; optimal regularity for solutions, including branch points. For unit ball confinement, minimizers are round spheres in certain parameter ranges.", "conclusion": "The constrained minimization problem exhibits rigidity, with round spheres as minimizers in specific cases."}}
{"id": "2506.14665", "pdf": "https://arxiv.org/pdf/2506.14665", "abs": "https://arxiv.org/abs/2506.14665", "authors": ["Giulia Luise", "Chin-Wei Huang", "Thijs Vogels", "Derk P. Kooi", "Sebastian Ehlert", "Stephanie Lanius", "Klaas J. H. Giesbertz", "Amir Karton", "Deniz Gunceler", "Megan Stanley", "Wessel P. Bruinsma", "Lin Huang", "Xinran Wei", "Jos\u00e9 Garrido Torres", "Abylay Katbashev", "B\u00e1lint M\u00e1t\u00e9", "S\u00e9kou-Oumar Kaba", "Roberto Sordillo", "Yingrong Chen", "David B. Williams-Young", "Christopher M. Bishop", "Jan Hermann", "Rianne van den Berg", "Paola Gori-Giorgi"], "title": "Accurate and scalable exchange-correlation with deep learning", "categories": ["physics.chem-ph", "cs.AI", "cs.CE", "cs.LG", "physics.comp-ph"], "comment": "Main: 13 pages plus references, 11 figures and tables. Supplementary information: 19 pages, 12 figures and tables", "summary": "Density Functional Theory (DFT) is the most widely used electronic structure method for predicting the properties of molecules and materials. Although DFT is, in principle, an exact reformulation of the Schr\u00f6dinger equation, practical applications rely on approximations to the unknown exchange-correlation (XC) functional. Most existing XC functionals are constructed using a limited set of increasingly complex, hand-crafted features that improve accuracy at the expense of computational efficiency. Yet, no current approximation achieves the accuracy and generality for predictive modeling of laboratory experiments at chemical accuracy -- typically defined as errors below 1 kcal/mol. In this work, we present Skala, a modern deep learning-based XC functional that bypasses expensive hand-designed features by learning representations directly from data. Skala achieves chemical accuracy for atomization energies of small molecules while retaining the computational efficiency typical of semi-local DFT. This performance is enabled by training on an unprecedented volume of high-accuracy reference data generated using computationally intensive wavefunction-based methods. Notably, Skala systematically improves with additional training data covering diverse chemistry. By incorporating a modest amount of additional high-accuracy data tailored to chemistry beyond atomization energies, Skala achieves accuracy competitive with the best-performing hybrid functionals across general main group chemistry, at the cost of semi-local DFT. As the training dataset continues to expand, Skala is poised to further enhance the predictive power of first-principles simulations.", "AI": {"tldr": "Skala is a deep learning-based XC functional that achieves chemical accuracy for small molecules while maintaining computational efficiency, outperforming traditional hand-crafted functionals.", "motivation": "Existing XC functionals in DFT lack accuracy and generality for predictive modeling at chemical accuracy (errors below 1 kcal/mol).", "method": "Skala uses deep learning to learn representations directly from data, trained on extensive high-accuracy reference data from wavefunction-based methods.", "result": "Skala achieves chemical accuracy for atomization energies and matches hybrid functional performance for main group chemistry, with scalability as more data is added.", "conclusion": "Skala demonstrates the potential of deep learning to revolutionize DFT by improving accuracy without sacrificing efficiency, with further gains expected as training data expands."}}
{"id": "2506.14533", "pdf": "https://arxiv.org/pdf/2506.14533", "abs": "https://arxiv.org/abs/2506.14533", "authors": ["Matei P. Coiculescu", "Jincheng Yang"], "title": "Conditional Liouville theorems for the Navier-Stokes equations", "categories": ["math.AP"], "comment": "24 pages, 1 figure", "summary": "We present a novel approach to the Liouville problem for the stationary Navier-Stokes equations. As an application of our method, we prove conditional Liouville theorems with assumptions on the antiderivative of the velocity that represent substantial improvements on what was heretofore known.", "AI": {"tldr": "A novel method for the Liouville problem in stationary Navier-Stokes equations, improving conditional Liouville theorems with assumptions on velocity antiderivatives.", "motivation": "To address the Liouville problem in stationary Navier-Stokes equations and enhance existing conditional Liouville theorems.", "method": "Introduces a new approach focusing on the antiderivative of the velocity.", "result": "Substantial improvements in conditional Liouville theorems under specific assumptions.", "conclusion": "The method advances understanding of the Liouville problem and offers better results for conditional theorems."}}
{"id": "2506.14722", "pdf": "https://arxiv.org/pdf/2506.14722", "abs": "https://arxiv.org/abs/2506.14722", "authors": ["Ankitha E Bangera"], "title": "A stochastic noise model based excess noise factor expressions for staircase avalanche photodiodes", "categories": ["eess.SP", "cs.IT", "math.PR", "physics.app-ph", "physics.comp-ph"], "comment": "This article derives the excess noise factor expressions for staircase APDs using a stochastic noise model (18 pages, 4 figures, preprint under submission) and can be used as a replacement to Capasso's expressions. Capasso's formula and its equivalents are incorrect, detailed in our article titled \"Mathematical proof of errors in Capasso's excess noise factor formula... .\"", "summary": "Multistep staircase avalanche photodiodes (APDs) are the solid-state analogue of photomultiplier tubes, owing to their deterministic amplification with twofold stepwise gain via impact ionization. Yet, the stepwise impact ionization irregularities worsen with increasing step counts, which are a major source of internal noise in these APDs. Some noise models for staircase APDs have been previously reported, where the excess noise factor expressions are based on Friis' noise factor formula for cascade networks, erroneously considering the power gains as the gains. Excess noise factor being a key component in staircase APDs' noise models, we formulate generalized excess noise factor expressions for multilayer graded-bandgap APDs in terms of their layer-wise ionization probabilities, applicable for all operating biases, which include the sub-threshold, staircase, and tunnelling breakdown regimes. We further derive simplified expressions for staircase APDs and prove that these expressions match Bangera's corrections to Friis' noise factor formulas for cascade networks.", "AI": {"tldr": "The paper addresses noise issues in multistep staircase avalanche photodiodes (APDs) by correcting excess noise factor expressions, providing generalized formulas for multilayer graded-bandgap APDs.", "motivation": "Existing noise models for staircase APDs inaccurately use Friis' noise factor formula, leading to errors in understanding stepwise impact ionization irregularities.", "method": "The authors formulate generalized excess noise factor expressions based on layer-wise ionization probabilities, applicable across all operating biases (sub-threshold, staircase, and tunnelling breakdown regimes). Simplified expressions for staircase APDs are also derived.", "result": "The derived expressions correct Bangera's adjustments to Friis' noise factor formulas, proving their accuracy for cascade networks.", "conclusion": "The paper provides accurate noise models for multilayer graded-bandgap APDs, improving understanding of their internal noise sources."}}
{"id": "2506.14579", "pdf": "https://arxiv.org/pdf/2506.14579", "abs": "https://arxiv.org/abs/2506.14579", "authors": ["Yuanyuan Lian", "Jing Wu"], "title": "Modica type estimates and curvature results for overdetermined $p$-Laplace problems", "categories": ["math.AP"], "comment": null, "summary": "In this paper we prove Modica type estimates for the following overdetermined $p$-Laplace problem \\begin{equation*}\n  \\begin{cases}\n  \\mathrm{div} \\left(|\\nabla u|^{p-2}\\nabla u\\right)+f(u) =0& \\mbox{in $\u03a9$, }\n  u>0 &\\mbox{in $\u03a9$, }\n  u=0 &\\mbox{on $\\partial\u03a9$, }\n  \\partial_\u03bd u=-\u03ba&\\mbox{on $\\partial\u03a9$, }\n  \\end{cases} \\end{equation*} where $1<p<+\\infty$, $f\\in C^1(\\mathbb{R})$, $\u03a9\\subset \\mathbb{R}^n$ ($n\\geq 2$) is a $C^1$ domain (bounded or unbounded), $\u03bd$ is the exterior unit normal of $\\partial \u03a9$ and $\u03ba\\geq 0$ is a constant. Based on Modica type estimates, we obtain rigidity results for bounded solutions. In particular, we prove that if there exists a nonpositive primitive $F$ of $f$ satisfying $F(0)\\geq -(p-1)\u03ba^p / p$ (for $p>2$ we also assume that if $F(u_0)=0$, $F(u)=O(|u-u_0|^p)$ as $u\\rightarrow u_0$), then either the mean curvature of $\\partial \u03a9$ is strictly negative or $\u03a9$ is a half-space.", "AI": {"tldr": "The paper proves Modica type estimates for an overdetermined p-Laplace problem and derives rigidity results for bounded solutions, showing that under certain conditions, the domain must either have strictly negative mean curvature or be a half-space.", "motivation": "The study aims to extend Modica type estimates to overdetermined p-Laplace problems, providing insights into the geometric properties of solutions and domains.", "method": "The authors analyze the overdetermined p-Laplace problem using Modica type estimates and derive conditions under which rigidity results hold.", "result": "Under specific conditions on the primitive of f, the domain must either have strictly negative mean curvature or be a half-space.", "conclusion": "The results generalize Modica type estimates to p-Laplace problems and highlight the geometric constraints on solutions and domains."}}
{"id": "2506.14618", "pdf": "https://arxiv.org/pdf/2506.14618", "abs": "https://arxiv.org/abs/2506.14618", "authors": ["Gabriele Cora", "Roberta Musina", "Alexander I. Nazarov"], "title": "Hardy-Sobolev inequalities involving mixed radially and cylindrically symmetric weights", "categories": ["math.AP"], "comment": "33 pages", "summary": "We deal with weighted Hardy-Sobolev type inequalities for functions on $\\mathbb{R}^d$, $d\\geq 2$. The weights involved are anisotropic, given by products of powers of the distance to the origin and to a nontrivial subspace. We establish necessary and sufficient conditions for validity of these inequalities, and investigate the existence/nonexistence of extremal functions.", "AI": {"tldr": "Analysis of weighted Hardy-Sobolev inequalities on \u211d\u1d48 with anisotropic weights, focusing on validity conditions and extremal functions.", "motivation": "To understand the validity and extremal behavior of anisotropic weighted Hardy-Sobolev inequalities in higher dimensions.", "method": "Investigates necessary and sufficient conditions for the inequalities using anisotropic weights based on distance to the origin and subspaces.", "result": "Establishes validity conditions and explores existence/nonexistence of extremal functions.", "conclusion": "Provides insights into the behavior of weighted Hardy-Sobolev inequalities with anisotropic weights in \u211d\u1d48."}}
{"id": "2506.14631", "pdf": "https://arxiv.org/pdf/2506.14631", "abs": "https://arxiv.org/abs/2506.14631", "authors": ["Jorge J. Betancor", "Estefan\u00eda Dalmasso", "Pablo Quijano"], "title": "Variational inequalities associated with the semigroups generated by fractional Kolmogorov operators", "categories": ["math.AP"], "comment": null, "summary": "In this paper we consider fractional Kolmogorov operators defined, in $\\mathbb{R}^d$, by\n  \\[\u039b_\u03ba=(-\u0394)^{\u03b1/2}+\\frac\u03ba{|x|^\u03b1} x\\cdot \\nabla,\\]\n  with $\u03b1\\in (1,2)$, $\u03b1<(d+2)/2$ and $\u03ba\\in \\mathbb{R}$. The operator $\u039b_\u03b1$ generates a holomorphic semigroup $\\{T_t^\u03b1\\}_{t>0}$ in $L^2(\\mathbb{R}^d)$ provided that $\u03ba<\u03ba_c$ where $\u03ba_c$ is a critical coupling constant. We establish $L^p$-boundedness properties for the variation operators $V_\u03c1\\left(\\{t^\\ell\\partial_t^\\ell T_t^\u03b1\\}_{t>0}\\right)$ with $\u03c1> 2$, $\\ell\\in \\mathbb{N}$ and $1\\vee \\frac{d}\u03b2<p<\\infty$, where $\u03b2$ depends on $\u03ba$. We also study the behavior of these variation operators in the endpoint $L^{1\\vee \\frac{d}\u03b2}(\\mathbb{R}^d)$ and we prove that $V_2(\\{T_t^\u03b1\\}_{t>0})$ is not bounded from $L^p(\\mathbb{R}^d)$ to $L^{p,\\infty}(\\mathbb{R}^d)$ for any $1< p<\\infty$.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2506.14639", "pdf": "https://arxiv.org/pdf/2506.14639", "abs": "https://arxiv.org/abs/2506.14639", "authors": ["Ariel Barton", "Svitlana Mayboroda", "Alberto Pacati"], "title": "The Poisson-Dirichlet problem in domains with Ahlfors regular boundary", "categories": ["math.AP"], "comment": "25 pages, 4 figures", "summary": "We present an announcement of some recent results concerning well-posedness of the Poisson-Dirichlet problem with boundary data in Besov spaces with fractional smoothness. This is a far-reaching generalization as previously known theorems concerning well-posedness of the Poisson problem in such intermediate smoothness classes were mostly restricted to the context of Lipschitz domains and coefficients satisfying strong regularity assumptions.", "AI": {"tldr": "Recent results generalize well-posedness of the Poisson-Dirichlet problem to Besov spaces with fractional smoothness, extending beyond Lipschitz domains and strict regularity conditions.", "motivation": "To address limitations in existing theorems, which were confined to Lipschitz domains and highly regular coefficients, by exploring well-posedness in broader settings.", "method": "The study focuses on the Poisson-Dirichlet problem, analyzing its well-posedness in Besov spaces with fractional smoothness, without relying on Lipschitz domains or stringent coefficient regularity.", "result": "The findings demonstrate well-posedness in more general contexts, significantly broadening the applicability of the Poisson problem.", "conclusion": "This work provides a substantial generalization of previous results, enabling the study of the Poisson-Dirichlet problem in less restrictive settings."}}
{"id": "2506.14735", "pdf": "https://arxiv.org/pdf/2506.14735", "abs": "https://arxiv.org/abs/2506.14735", "authors": ["Xiao Li", "Nguyen Nguyen", "Deping Ye"], "title": "A Minkowski problem for $\u03b1$-concave functions via optimal transport", "categories": ["math.FA", "math.AP", "math.MG"], "comment": null, "summary": "The notions of the Euclidean surface area measure and the spherical surface area measure of $\u03b1$-concave functions in $\\mathbb{R}^n$, with $-\\frac{1}{n}<\u03b1<0$, are introduced via a first variation of the total mass functional with respect to the $\u03b1$-sum operation. Subsequently, these notions are extended to those for $\u03b1$-concave measures. We then study the Minkowski problem associated with the Euclidean surface area measures of $\u03b1$-concave measures via optimal transport.", "AI": {"tldr": "The paper introduces Euclidean and spherical surface area measures for \u03b1-concave functions and measures, extending them via \u03b1-sum operations, and solves the Minkowski problem for these measures using optimal transport.", "motivation": "To generalize surface area measures for \u03b1-concave functions and measures, and address the Minkowski problem in this context.", "method": "Introduces measures via first variation of total mass functional with \u03b1-sum operations, then uses optimal transport to solve the Minkowski problem.", "result": "Extension of surface area measures to \u03b1-concave measures and solution of the associated Minkowski problem.", "conclusion": "The work successfully generalizes surface area measures and solves the Minkowski problem for \u03b1-concave measures using optimal transport."}}
