<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 22]
- [math.AP](#math.AP) [Total: 15]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 7]
- [math.DG](#math.DG) [Total: 2]
- [math.ST](#math.ST) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 3]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [astro-ph.SR](#astro-ph.SR) [Total: 1]
- [stat.CO](#stat.CO) [Total: 1]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [cs.LG](#cs.LG) [Total: 2]
- [cs.CE](#cs.CE) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Faster Computation of Entropic Optimal Transport via Stable Low Frequency Modes](https://arxiv.org/abs/2506.14780)
*Reda Chhaibi,Serge Gratton,Samuel Vaiter*

Main category: math.NA

TL;DR: An accelerated Sinkhorn algorithm is proposed for Entropic Optimal Transport, addressing slow convergence with a spectral warm-start strategy.


<details>
  <summary>Details</summary>
Motivation: The Sinkhorn algorithm's slow convergence as regularization weakens (ε → 0) is a major drawback.

Method: A spectral warm-start strategy is introduced, leveraging insights into the Hessian's behavior.

Result: The proposed method achieves faster convergence than the reference Sinkhorn algorithm.

Conclusion: The spectral warm-start strategy effectively mitigates slow convergence, improving the Sinkhorn algorithm's performance.

Abstract: In this paper, we propose an accelerated version for the Sinkhorn algorithm,
which is the reference method for computing the solution to Entropic Optimal
Transport.
  Its main draw-back is the exponential slow-down of convergence as the
regularization weakens $\varepsilon \rightarrow 0$.
  Thanks to spectral insights on the behavior of the Hessian, we propose to
mitigate the problem via an original spectral warm-start strategy. This leads
to faster convergence compared to the reference method, as also demonstrated in
our numerical experiments.

</details>


### [2] [Moment-enhanced shallow water equations for non-slip boundary conditions](https://arxiv.org/abs/2506.14785)
*Shiping Zhou,Juntao Huang,Andrew J. Christlieb*

Main category: math.NA

TL;DR: Modified shallow water equations and moment-enhanced models are proposed to better capture vertical velocity profiles under non-slip and slip boundary conditions, outperforming standard models.


<details>
  <summary>Details</summary>
Motivation: Standard shallow water equations and moment-enhanced models fail to accurately represent vertical velocity profiles under non-slip conditions due to stiff source terms.

Method: Proposes modified shallow water equations and moment-enhanced models with revised source term treatment, ensuring compatibility with hyperbolicity analysis.

Result: Modified models perform well under both non-slip and slip boundary conditions, validated via numerical comparison with incompressible Navier-Stokes equations.

Conclusion: The modified models offer improved accuracy and generalization for vertical velocity profiles, addressing limitations of existing approaches.

Abstract: The shallow water equations often assume a constant velocity profile along
the vertical axis. However, this assumption does not hold in many practical
applications. To better approximate the vertical velocity distribution, models
such as the shallow water moment expansion models have been proposed.
Nevertheless, under non-slip bottom boundary conditions, both the standard
shallow water equation and its moment-enhanced models struggle to accurately
capture the vertical velocity profile due to the stiff source terms. In this
work, we propose modified shallow water equations and corresponding
moment-enhanced models that perform well under both non-slip and slip boundary
conditions. The primary difference between the modified and original models
lies in the treatment of the source term, which allows our modified moment
expansion models to be readily generalized, while maintaining compatibility
with our previous analysis on the hyperbolicity of the model. To assess the
performance of both the standard and modified moment expansion models, we
conduct a comprehensive numerical comparison with the incompressible
Navier--Stokes equations -- a comparison that is absent from existing
literature.

</details>


### [3] [Energy-consistent dynamic fracture phase field models: unilateral constraints and finite element simulations](https://arxiv.org/abs/2506.14788)
*Md Mamun Miah,Ryuhei Wakida,Masato Kimura*

Main category: math.NA

TL;DR: A dynamic fracture phase field model (DF-PFM) is proposed, incorporating elastodynamics and unilateral contact for accurate fault rupture simulation under high pressure.


<details>
  <summary>Details</summary>
Motivation: Phase field models are effective for simulating complex fracture mechanics, but a dynamic framework with contact conditions is needed for realistic fault rupture scenarios.

Method: The study develops DF-PFM based on the elastodynamic wave equation, extends it with a unilateral contact condition, and validates it using numerical experiments.

Result: The unilateral contact condition is crucial for accurate shear-dominated crack propagation and preventing non-physical interpenetration under high compression.

Conclusion: The proposed DF-PFM with unilateral contact is validated as a robust tool for simulating fault rupture, especially under high-pressure conditions.

Abstract: Phase field models have emerged as a powerful and flexible framework for
simulating complex interface-driven phenomena across a wide range of scientific
and engineering applications. In fracture mechanics, the phase field
approach--formulated as a gradient flow of the Griffith fracture energy with
Ambrosio-Tortorelli regularization--has gained significant attention for its
ability to capture complex crack topologies. In this study, we propose a
dynamic fracture phase field model (DF-PFM) based on the elastodynamic wave
equation. We further extend this framework by incorporating a unilateral
contact condition, yielding a refined model suitable for simulating fault
rupture under high pressure. For both models, we rigorously derive energy
dissipation identities under mixed boundary conditions, ensuring energy
consistency of the formulations. To validate the proposed approach, we conduct
numerical experiments using linear implicit time discretization and finite
element methods. Our simulations demonstrate that the unilateral contact
condition is essential for accurately capturing shear-dominated crack
propagation and preventing non-physical interpenetration, especially under
high-compression loading scenarios relevant to seismic faulting.

</details>


### [4] [Fast automated adjoints for spectral PDE solvers](https://arxiv.org/abs/2506.14792)
*Calum S. Skene,Keaton J. Burns*

Main category: math.NA

TL;DR: Automated adjoint solver for PDEs using reverse-mode automatic differentiation in Dedalus, enabling gradient-based optimization without extra code.


<details>
  <summary>Details</summary>
Motivation: To simplify gradient computation and optimization for PDEs in spectral methods, eliminating the need for manual adjoint solver development.

Method: Reverse-mode automatic differentiation applied to symbolic PDE graphs, retaining efficiency of sparse spectral methods.

Result: Efficient gradient computation for time-dependent and nonlinear systems, demonstrated on canonical problems with strong performance.

Conclusion: The approach enables easy and efficient gradient-based optimization and sensitivity analyses in spectral simulations.

Abstract: We present a general and automated approach for computing model gradients for
PDE solvers built on sparse spectral methods, and implement this capability in
the widely used open-source Dedalus framework. We apply reverse-mode automatic
differentiation to symbolic graph representations of PDEs, efficiently
constructing adjoint solvers that retain the speed and memory efficiency of
this important class of modern numerical methods. This approach enables users
to compute gradients and perform optimization for a wide range of
time-dependent and nonlinear systems without writing additional code. The
framework supports a broad class of equations, geometries, and boundary
conditions, and runs efficiently in parallel using MPI. We demonstrate the
flexibility and capabilities of this system using canonical problems from the
literature, showing both strong performance and practical utility for a wide
variety of inverse problems. By integrating automatic adjoints into a flexible
high-level solver, our approach enables researchers to perform gradient-based
optimization and sensitivity analyses in spectral simulations with ease and
efficiency.

</details>


### [5] [A micromorphic-based artificial diffusion method for stabilized finite element approximation of convection-diffusion problems](https://arxiv.org/abs/2506.14800)
*Soheil Firooz,B. Daya Reddy,Paul Steinmann*

Main category: math.NA

TL;DR: A novel artificial diffusion method is introduced to stabilize finite element approximations of convection-diffusion equations, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To address instabilities in standard finite element approximations of convection-diffusion equations.

Method: Introduces an auxiliary variable linked to the gradient of the field, creating a coupled problem, with well-posedness conditions established.

Result: The method outperforms established approaches in accurately approximating solutions in 1D and 2D settings.

Conclusion: The proposed artificial diffusion method is effective for solving challenging convection-diffusion problems.

Abstract: We present a novel artificial diffusion method to circumvent the
instabilities associated with the standard finite element approximation of
convection-diffusion equations. Motivated by the micromorphic approach, we
introduce an auxiliary variable, which is related to the gradient of the field
of interest, and which leads to a coupled problem. Conditions for
well-posedness of the resulting formulation are established. We carry out a
comprehensive numerical study to compare the proposed methodology against some
well-established approaches in one- and two-dimensional settings. The proposed
method outperforms established approaches in general in approximating
accurately the solutions to pertinent and challenging problems.

</details>


### [6] [An explicit computational approach for a three-dimensional system of nonlinear elastodynamic sine-Gordon problem](https://arxiv.org/abs/2506.14807)
*Eric Ngondiep*

Main category: math.NA

TL;DR: A high-order explicit computational method for solving 3D nonlinear elastodynamic sine-Gordon equations, combining interpolation for time derivatives and finite elements for space derivatives, with proven stability and convergence.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of solving complex 3D nonlinear elastodynamic sine-Gordon equations efficiently and accurately under given initial and boundary conditions.

Method: Uses interpolation for time derivatives and finite elements for space derivatives, ensuring high-order accuracy and stability with a suitable time step restriction.

Result: The method is temporally second-order and spatially third-order accurate, validated by numerical examples.

Conclusion: The proposed technique is efficient, stable, and practically applicable for solving the target equations.

Abstract: This paper proposes an explicit computational method for solving a
three-dimensional system of nonlinear elastodynamic sine-Gordon equations
subject to appropriate initial and boundary conditions. The time derivative is
approximated by interpolation technique whereas the finite element approach is
used to approximate the space derivatives. The developed numerical scheme is
so-called, high-order explicit computational technique. The new algorithm
efficiently treats the time derivative term and provides a suitable time step
restriction for stability and convergence. Under this time step limitation,
both stability and error estimates of the proposed approach are deeply analyzed
using a constructed strong norm. The theoretical studies indicate that the
developed approach is temporal second-order convergent and spatially
third-order accurate. Some numerical examples are carried out to confirm the
theory, to validate the computational efficiency and to demonstrate the
practical applicability of the new computational technique.

</details>


### [7] [Weak TransNet: A Petrov-Galerkin based neural network method for solving elliptic PDEs](https://arxiv.org/abs/2506.14812)
*Zhihang Xu,Min Wang,Zhu Wang*

Main category: math.NA

TL;DR: The paper introduces Weak TransNet (WTN), a method for solving elliptic PDEs with low regularity or singularities, using a Petrov-Galerkin approach and neural networks.


<details>
  <summary>Details</summary>
Motivation: Deep learning struggles with PDEs involving low regularity or singularities, prompting the need for a robust method like WTN.

Method: WTN uses a neural feature space (TransNet) as the trial space and radial basis functions as the test space, minimizing weak PDE residuals via least squares.

Result: WTN effectively addresses non-convexity and ill-conditioning in neural network training and handles multiscale or sharp-varying solutions.

Conclusion: Numerical experiments confirm WTN's robustness and efficiency for challenging PDE problems.

Abstract: While deep learning has achieved remarkable success in solving partial
differential equations (PDEs), it still faces significant challenges,
particularly when the PDE solutions have low regularity or singularities. To
address these issues, we propose the Weak TransNet (WTN) method, based on a
Petrov-Galerkin formulation, for solving elliptic PDEs in this work, though its
framework may extend to other classes of equations. Specifically, the neural
feature space defined by TransNet (Zhang et al., 2023) is used as the trial
space, while the test space is composed of radial basis functions. Since the
solution is expressed as a linear combination of trial functions, the
coefficients can be determined by minimizing the weak PDE residual via least
squares. Thus, this approach could help mitigate the challenges of
non-convexity and ill-conditioning that often arise in neural network training.
Furthermore, the WTN method is extended to handle problems whose solutions
exhibit multiscale features or possess sharp variations. Several numerical
experiments are presented to demonstrate the robustness and efficiency of the
proposed methods.

</details>


### [8] [Optimal alignment of Lorentz orientation and generalization to matrix Lie groups](https://arxiv.org/abs/2506.14994)
*Congzhou M Sha*

Main category: math.NA

TL;DR: The paper addresses the challenge of aligning point clouds in Minkowski space, proposing two solutions for optimal Lorentz transformation between reference frames.


<details>
  <summary>Details</summary>
Motivation: Existing methods for point cloud alignment in Euclidean space don't extend to the indefinite Minkowski metric, necessitating new approaches.

Method: The paper outlines a conceptually simple method to find the optimal Lorentz transformation between reference frames using 4-vector measurements.

Result: The proposed method effectively solves the alignment problem in Minkowski space and can be generalized to other matrix Lie groups.

Conclusion: The paper provides a practical solution for aligning point clouds in Minkowski space, with potential applications in broader contexts.

Abstract: There exist elegant methods of aligning point clouds in $\mathbb R^3$.
Unfortunately, these methods rely on the positive definite property of the
Euclidean metric, and do not easily extend to the indefinite Minkowski metric.
In this paper, we propose two solutions to the following problem: given
inertial reference frames $A$ and $B$, and given (possibly noisy) measurements
of a set of 4-vectors $\{v_i\}$ made in those reference frames with components
$\{v_{A,i}\}$ and $\{v_{B,i}\}$, find the optimal Lorentz transformation
$\Lambda$ such that $\Lambda v_{A,i}=v_{B,i}$. The method we outline is
conceptually simple and easily extends to alignment problems in other matrix
Lie groups.

</details>


### [9] [Semi-orthogonal Tribonacci Wavelets and Numerical Solutions of Nonlinear Singular BVPs Arising in a Chemical Reaction](https://arxiv.org/abs/2506.14814)
*Ankita Yadav,Amit K. Verma*

Main category: math.NA

TL;DR: Introduction of a semi-orthogonal tribonacci wavelet collocation method for solving non-linear singular BVPs.


<details>
  <summary>Details</summary>
Motivation: To provide an effective numerical method for addressing non-linear singular boundary value problems (BVPs).

Method: Development of a semi-orthogonal tribonacci wavelet collocation method.

Result: The method offers a solution for a class of non-linear singular BVPs.

Conclusion: The proposed method is effective for solving the targeted problems.

Abstract: In this article, we introduce a semi-orthogonal tribonacci wavelet and
develop a semi-orthogonal tribonacci wavelet collocation method, offering an
effective numerical method for solving a class of non-linear singular BVPs.

</details>


### [10] [Interpolation-based reproducing kernel particle method](https://arxiv.org/abs/2506.14916)
*Jennifer E. Fromm,John A. Evans,J. S. Chen*

Main category: math.NA

TL;DR: Interpolation-based RKPM enables easy implementation in FEM software, matching classic RKPM accuracy while reducing computational cost.


<details>
  <summary>Details</summary>
Motivation: RKPM's integration challenges in FEM software due to Gauss-quadrature limitations motivated the development of interpolation-based methods.

Method: Interpolation-based RKPM uses Lagrange polynomial foreground basis functions on a boundary-conforming mesh, applied to PDEs and multi-material problems with Heaviside enrichment.

Result: The method achieves error convergence rates equivalent to classic RKPM, solves higher-order PDEs, and reduces computational costs.

Conclusion: Interpolation-based RKPM is efficient, accurate, and easily integrable into existing FEM software, offering significant advantages over traditional methods.

Abstract: Meshfree methods, including the reproducing kernel particle method (RKPM),
have been widely used within the computational mechanics community to model
physical phenomena in materials undergoing large deformations or extreme
topology changes. RKPM shape functions and their derivatives cannot be
accurately integrated with the Gauss-quadrature methods widely employed for the
finite element method (FEM) and typically require sophisticated nodal
integration techniques, preventing them from easily being implemented in
existing FEM software. Interpolation-based methods have been developed to
address similar problems with isogeometric and immersed boundary methods,
allowing these techniques to be implemented within open-source finite element
software. With interpolation-based methods, background basis functions are
represented as linear combinations of Lagrange polynomial foreground basis
functions defined upon a boundary-conforming foreground mesh. This work extends
the applications of interpolation-based methods to implement RKPM within
open-source finite element software. Interpolation-based RKPM is applied to
several PDEs, and error convergence rates are equivalent to classic RKPM
integrated using high-order Gauss-quadrature schemes. The interpolation-based
method is able to exploit the continuity of the RKPM basis to solve
higher-order PDEs, demonstrated through the biharmonic problem. The method is
extended to multi-material problems through Heaviside enrichment schemes, using
local foreground refinement to reduce geometric integration error and achieve
high-order accuracy. The computational cost of interpolation-based RKPM is
similar to the smoothed gradient nodal integration schemes, offering
significant savings over Gauss-quadrature-based meshfree methods while enabling
easy implementation within existing finite element software.

</details>


### [11] [A Nonconforming Finite Element Method for Elliptic Interface Problems on Locally Anisotropic Meshes](https://arxiv.org/abs/2506.15077)
*Hua Wang,Qichen Zhang*

Main category: math.NA

TL;DR: A new nonconforming P1 finite element method for elliptic interface problems is proposed, using anisotropic mixed meshes and removing quasi-regularity assumptions.


<details>
  <summary>Details</summary>
Motivation: To address elliptic interface problems more effectively by leveraging anisotropic mixed meshes and simplifying assumptions.

Method: Constructed on locally anisotropic mixed meshes, using interpolation error estimates and a novel consistency error analysis.

Result: Numerical results confirm theoretical convergence rates, showing robustness and accuracy.

Conclusion: The method is effective, robust, and accurate for elliptic interface problems.

Abstract: We propose a new nonconforming \(P_1\) finite element method for elliptic
interface problems. The method is constructed on a locally anisotropic mixed
mesh, which is generated by fitting the interface through a simple connection
of intersection points on an interface-unfitted background mesh, as introduced
in \cite{Hu2021optimal}. We first establish interpolation error estimates on
quadrilateral elements satisfying the regular decomposition property (RDP).
Building on this, the main contribution of this work is a novel consistency
error analysis for nonconforming elements, which removes the quasi-regularity
assumption commonly required in existing approaches. Numerical results confirm
the theoretical convergence rates and demonstrate the robustness and accuracy
of the proposed method.

</details>


### [12] [Fourth- and Higher-Order Semi-Lagrangian Finite Volume Methods for the Two-dimensional Advection Equation on Arbitrarily Complex Domains](https://arxiv.org/abs/2506.15142)
*Yunxia Sun,Kaiyi Liang,Yuke Zhu,Zhi Lin,Qinghai Zhang*

Main category: math.NA

TL;DR: A family of high-order semi-Lagrangian finite volume (SLFV) methods is proposed for solving the 2D advection equation, offering high convergence rates, flexibility for complex domains, and ease of handling source terms.


<details>
  <summary>Details</summary>
Motivation: To address the need for accurate and flexible numerical methods for solving the 2D advection equation, especially in complex domains with varying conditions.

Method: Proposes fourth-, sixth-, and eighth-order SLFV methods applicable to regular/irregular domains, handling zero/nonzero source terms, and accommodating periodic/incoming penetration conditions.

Result: Test results confirm high accuracy, flexibility, robustness, and excellent conditioning of the SLFV method.

Conclusion: The proposed SLFV methods are effective for solving the 2D advection equation, offering high-order accuracy and adaptability to diverse conditions.

Abstract: To numerically solve the two-dimensional advection equation, we propose a
family of fourth- and higher-order semi-Lagrangian finite volume (SLFV) methods
that feature (1) fourth-, sixth-, and eighth-order convergence rates, (2)
applicability to both regular and irregular domains with arbitrarily complex
topology and geometry, (3) ease of handling both zero and nonzero source terms,
and (4) the same algorithmic steps for both periodic and incoming penetration
conditions. Test results confirm the analysis and demonstrate the accuracy,
flexibility, robustness, and excellent conditioning of the proposed SLFV
method.

</details>


### [13] [A time-frequency method for acoustic scattering with trapping](https://arxiv.org/abs/2506.15165)
*Heather Wilber,Wietse Vaes,Abinand Gopal,Gunnar Martinsson*

Main category: math.NA

TL;DR: A Fourier transform method is introduced for hybrid time-frequency schemes to solve acoustic scattering problems with oscillatory behavior and slow decay, extending applicability to trapping regions.


<details>
  <summary>Details</summary>
Motivation: To address challenges in acoustic scattering problems where solutions exhibit both highly oscillatory behavior and slow decay in time, particularly in domains with trapping regions.

Method: Combines a fast sinc transform technique for handling oscillations and long time horizons with a contour integration scheme to improve integrand smoothness.

Result: Extends the applicability of hybrid time-frequency schemes to more complex domains, improving efficiency and accuracy.

Conclusion: The proposed method effectively addresses oscillatory and decay challenges in acoustic scattering, enhancing hybrid time-frequency schemes.

Abstract: A Fourier transform method is introduced for a class of hybrid time-frequency
methods that solve the acoustic scattering problem in regimes where the
solution exhibits both highly oscillatory behavior and slow decay in time. This
extends the applicability of hybrid time-frequency schemes to domains with
trapping regions. A fast sinc transform technique for managing highly
oscillatory behavior and long time horizons is combined with a contour
integration scheme that improves smoothness properties in the integrand.

</details>


### [14] [Heterogeneous and anisotropic elastic parameter estimation using a novel semi-analytical forward solver](https://arxiv.org/abs/2506.15185)
*Xiaopeng Zhu,Zhongyi Huang*

Main category: math.NA

TL;DR: A novel semi-analytical method for identifying heterogeneous and anisotropic elastic parameters using one full-field measurement, solved via Adam algorithm with TV regularization.


<details>
  <summary>Details</summary>
Motivation: To efficiently identify elastic parameters from limited data while addressing irregularities, anisotropy, and heterogeneity.

Method: Formulates an inverse problem as energy functional minimization with TV regularization, solved by Adam algorithm. Uses a semi-analytical forward solver (direct method of lines) for irregular regions and singularities.

Result: Numerical experiments confirm reliable performance in forward modeling and reconstructing six elastic parameters.

Conclusion: The proposed method is effective for elastic parameter identification with minimal data and computational cost.

Abstract: An efficient procedure using a novel semi-analytical forward solver for
identifying heterogeneous and anisotropic elastic parameters from only one
full-field measurement is proposed and explored. We formulate the inverse
problem as an special energy functional minimization with total variation(TV)
regularization. The minimization problem is solved by Adam algorithm, which
only requires solving one forward problem and no adjoint problem in each
iteration. In order to deal with the irregularity of the elastic regions, the
anisotropy and heterogeneity of parameters and potential singularities in
forward-modeled issues, a novel semi-analytical forward solver named the direct
method of lines is proposed, which discretizes angular variable while
preserving analytical solutions along remaining coordinates. To validate the
efficacy of our procedure, a series of numerical experiments are implemented
subsequently, achieving reliable performance in both forward modeling and the
six elastic arguments reconstruction scenarios.

</details>


### [15] [Reduced Particle in Cell method for the Vlasov-Poisson system using auto-encoder and Hamiltonian neural](https://arxiv.org/abs/2506.15203)
*Emmanuel Franck,Laurent Navoret,Vincent Vigon,Raphaël Côte,Guillaume Steimer*

Main category: math.NA

TL;DR: A nonlinear, data-driven model order reduction method for Hamiltonian particle-based plasma simulations is introduced, combining linear and nonlinear projections with a Hamiltonian neural network for improved accuracy and stability.


<details>
  <summary>Details</summary>
Motivation: The computational intensity of Hamiltonian particle-based plasma simulations, especially in many-query contexts, necessitates reduced order models to lower costs while preserving Hamiltonian structure for stability.

Method: A two-step projection framework: linear projection via Proper Symplectic Decomposition, followed by a nonlinear projection using an autoencoder neural network. Reduced dynamics are modeled with a Hamiltonian neural network.

Result: Validated on benchmarks like Landau damping and two-stream instability, the method outperforms standard linear Hamiltonian reduction methods.

Conclusion: The proposed method effectively reduces computational costs while maintaining accuracy and stability in Hamiltonian particle-based simulations.

Abstract: Hamiltonian particle-based simulations of plasma dynamics are inherently
computationally intensive, primarily due to the large number of particles
required to obtain accurate solutions. This challenge becomes even more acute
in many-query contexts, where numerous simulations must be conducted across a
range of time and parameter values. Consequently, it is essential to construct
reduced order models from such discretizations to significantly lower
computational costs while ensuring validity across the specified time and
parameter domains. Preserving the Hamiltonian structure in these reduced models
is also crucial, as it helps maintain long-term stability. In this paper, we
introduce a nonlinear, non-intrusive, data-driven model order reduction method
for the 1D-1V Vlasov--Poisson system, discretized using a Hamiltonian
Particle-In-Cell scheme. Our approach relies on a two-step projection
framework: an initial linear projection based on the Proper Symplectic
Decomposition, followed by a nonlinear projection learned via an autoencoder
neural network. The reduced dynamics are then modeled using a Hamiltonian
neural network. The offline phase of the method is split into two stages:
first, constructing the linear projection using full-order model snapshots;
second, jointly training the autoencoder and the Hamiltonian neural network to
simultaneously learn the encoder-decoder mappings and the reduced dynamics. We
validate the proposed method on several benchmarks, including Landau damping
and two-stream instability. The results show that our method has better
reduction properties than standard linear Hamiltonian reduction methods.

</details>


### [16] [Splitting-based randomised dynamical low-rank approximations for stiff matrix differential equations](https://arxiv.org/abs/2506.15259)
*Zi Wu,Yong-Liang Zhao*

Main category: math.NA

TL;DR: A dynamic numerical integrator for low-rank approximations of large-scale stiff matrix differential equations, combining exponential integrators and dynamic low-rank methods, validated on canonical problems.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of solving large-scale semilinear stiff matrix differential equations efficiently with low-rank approximations.

Method: Decompose the equation into stiff linear and nonstiff nonlinear parts, then use exponential integrators and dynamic low-rank approaches for each subsystem, with rank-adaptation support.

Result: Achieves predicted convergence orders and demonstrates robustness and accuracy on problems like Allen-Cahn and Riccati equations.

Conclusion: The proposed method is effective for low-rank approximations in stiff matrix differential equations, validated by numerical results.

Abstract: In the fields of control theory and machine learning, the dynamic low-rank
approximation for large-scale matrices has received substantial attention.
Considering the large-scale semilinear stiff matrix differential equations, we
propose a dynamic numerical integrator for obtaining low-rank approximations of
solutions. We first decompose the differential equation into a stiff linear
component and a nonstiff nonlinear term, then employ an exponential integrator
along with a dynamic low-rank approach to resolve these subsystems,
respectively. Furthermore, the proposed framework naturally extends to
rank-adaptation scenarios. Through rigorous validation on canonical stiff
matrix differential problems, including spatially discretized Allen-Cahn
equations and differential Riccati equations, we demonstrate that the method
achieves the theoretically predicted convergence orders. Numerical evidence
confirms the robustness and accuracy of the proposed methods.

</details>


### [17] [Stochastic Diagonal Estimation Based on Matrix Quadratic Form Oracles](https://arxiv.org/abs/2506.15360)
*Haishan Ye,Xiangyu Chang*

Main category: math.NA

TL;DR: A stochastic method for estimating the diagonal of an implicitly given matrix using Gaussian-distributed queries, with proven sample complexities and validated effectiveness.


<details>
  <summary>Details</summary>
Motivation: The problem of estimating the diagonal of an implicitly given matrix, where only matrix quadratic form evaluations are available, is addressed.

Method: A stochastic diagonal estimation method using random Gaussian vectors to query the matrix quadratic form.

Result: Element-wise and norm-wise sample complexities are provided, and numerical experiments confirm the method's effectiveness and theoretical tightness.

Conclusion: The proposed method is effective for diagonal estimation in implicitly given matrices, with theoretical guarantees supported by experiments.

Abstract: We study the problem of estimating the diagonal of an implicitly given matrix
$\Ab$. For such a matrix we have access to an oracle that allows us to evaluate
the matrix quadratic form $ \ub^\top \Ab \ub$. Based on this query oracle, we
propose a stochastic diagonal estimation method with random variable $\ub$
drawn from the standard Gaussian distribution. We provide the element-wise and
norm-wise sample complexities of the proposed method. Our numerical experiments
on different types and dimensions matrices demonstrate the effectiveness of our
method and validate the tightness of theoretical results.

</details>


### [18] [A deep shotgun method for solving high-dimensional parabolic partial differential equations](https://arxiv.org/abs/2506.15481)
*Wenjun Xu,Wenzhong Zhang*

Main category: math.NA

TL;DR: A deep 'shotgun method' is proposed to solve high-dimensional parabolic PDEs more efficiently by using data distribution instead of full trajectories, outperforming existing methods in performance and accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing methods for solving high-dimensional parabolic PDEs via FBSDE formulations require simulating multiple trajectories with small time steps, limiting performance, especially over large time intervals.

Method: The proposed 'shotgun method' avoids simulating full trajectories and instead leverages the data distribution of these trajectories.

Result: Numerical results, including tests in dimensions up to 10000, show the shotgun method is competitive in both performance and accuracy.

Conclusion: The shotgun method offers a more efficient and accurate solution for high-dimensional parabolic PDEs compared to traditional trajectory-based approaches.

Abstract: Recent advances in deep learning makes solving parabolic partial differential
equations (PDEs) in high dimensional spaces possible via forward-backward
stochastic differential equation (FBSDE) formulations. The implementation of
most existing methods requires simulating multiple trajectories of stochastic
processes with a small step size of time discretization to ensure accuracy,
hence having limited performance, especially when solving on a large time
interval. To address such issue, we propose a deep "shotgun method" that does
not exploit full trajectories, but only utilizes the data distribution of them.
Numerical results including examples with dimensionality up to 10000
demonstrate the competitiveness of the proposed shotgun method in both
performance and accuracy.

</details>


### [19] [Intrinsic and Extrinsic Organized Attention: Softmax Invariance and Network Sparsity](https://arxiv.org/abs/2506.15541)
*Oluwadamilola Fasina,Ruben V. C. Pohle,Pei-Chun Su,Ronald R. Coifman*

Main category: math.NA

TL;DR: The paper analyzes the structure of self-attention in transformers, showing invariance to softmax activation and using hierarchical tensor organization for signal processing tasks.


<details>
  <summary>Details</summary>
Motivation: To understand the intrinsic and extrinsic structure of self-attention mechanisms in transformers and leverage this for interpretability and practical applications.

Method: Theoretical analysis using paradifferential calculus and hierarchical tensor organization, supported by computational examples in vision and language transformers.

Result: Demonstrates softmax invariance, hierarchical organization of attention heads, and applications like model pruning and architecture comparison.

Conclusion: The findings advance interpretability and enable practical uses like pruning, offering a theoretical and empirical framework for transformer analysis.

Abstract: We examine the intrinsic (within the attention head) and extrinsic (amongst
the attention heads) structure of the self-attention mechanism in transformers.
Theoretical evidence for invariance of the self-attention mechanism to softmax
activation is obtained by appealing to paradifferential calculus, (and is
supported by computational examples), which relies on the intrinsic
organization of the attention heads. Furthermore, we use an existing
methodology for hierarchical organization of tensors to examine network
structure by constructing hierarchal partition trees with respect to the query,
key, and head axes of network 3-tensors. Such an organization is consequential
since it allows one to profitably execute common signal processing tasks on a
geometry where the organized network 3-tensors exhibit regularity. We exemplify
this qualitatively, by visualizing the hierarchical organization of the tree
comprised of attention heads and the diffusion map embeddings, and
quantitatively by investigating network sparsity with the expansion
coefficients of individual attention heads and the entire network with respect
to the bi and tri-haar bases (respectively) on the space of queries, keys, and
heads of the network. To showcase the utility of our theoretical and
methodological findings, we provide computational examples using vision and
language transformers. The ramifications of these findings are two-fold: (1) a
subsequent step in interpretability analysis is theoretically admitted, and can
be exploited empirically for downstream interpretability tasks (2) one can use
the network 3-tensor organization for empirical network applications such as
model pruning (by virtue of network sparsity) and network architecture
comparison.

</details>


### [20] [Pathwise convergence of a novel numerical scheme based on semi-implicit method for stochastic differential-algebraic equations with non-global Lipschitz coefficients](https://arxiv.org/abs/2506.15627)
*Guy Tsafack,Antoine Tambue*

Main category: math.NA

TL;DR: The paper proposes a semi-implicit numerical scheme for non-autonomous SDAEs with nonlinear coefficients, addressing singularity challenges and proving pathwise convergence.


<details>
  <summary>Details</summary>
Motivation: The challenge lies in handling the singularity of the matrix in SDAEs, making numerical integration difficult. The goal is to develop an efficient method for high-dimensional SDAEs.

Method: A semi-implicit scheme splits the drift term into linear (implicit) and nonlinear (explicit) parts, avoiding nonlinear algebraic equations in constraints. A dual scheme is used for analysis.

Result: The scheme achieves pathwise convergence with rate $\frac{1}{2}-\epsilon$ and is validated through numerical simulations.

Conclusion: The proposed method efficiently handles high-dimensional SDAEs, with theoretical and experimental results aligning.

Abstract: This paper delves into the well-posedness and the numerical approximation of
non-autonomous stochastic differential algebraic equations (SDAEs) with
nonlinear local Lipschitz coefficients that satisfy the more general
monotonicity condition called Khasminskii condition. The key challenge is the
presence of a singular matrix which makes the numerical integration hard and
heavy. To address this challenge, we propose a novel numerical scheme based on
semi-implicit method for the drift component of the SDAEs. More precisely we
split the drift term as the sum of a linear term and a nonlinear term. The
linear part is approximated implicitly, while the nonlinear part is
approximated explicitly. The linear component's role is to handle the
singularity issues during the numerical integration without the resolution of
nonlinear algebraic equations in the constraint equations. This novel scheme is
therefore very efficient for SDAEs in high dimension that come after the
spatial discretisation of stochastic partial differential algebraic equations
(SPDAEs). To prove the pathwise convergence of our novel scheme, we first
derive a equivalent scheme called dual scheme, suitable for mathematical
analysis and linked to the inherent stochastic differential equation resulting
from the elimination of constraints in the initial SDAEs. We prove that our
novel scheme converges to the exact solution with rate $\frac{1}{2}-\epsilon$,
for arbitrary $\epsilon>0$ in the pathwise sense. Numerical simulations are
performed to demonstrate the efficiency of the scheme in high dimension and to
show that our theoretical results are in agreement with numerical experiments.

</details>


### [21] [Non-uniform finite-element meshes defined by ray dynamics for Helmholtz problems](https://arxiv.org/abs/2506.15630)
*Martin Averseng,Jeffrey Galkowski,Euan A. Spence*

Main category: math.NA

TL;DR: The paper shows that quasioptimality (QO) and bounded relative error (BRE) in the $h$-FEM for the Helmholtz equation can be achieved with non-uniform meshes, even violating classic conditions, particularly in non-trapping regions and PML.


<details>
  <summary>Details</summary>
Motivation: To explore how non-uniform meshes can achieve QO and BRE in the $h$-FEM for the Helmholtz equation, reducing computational costs without sacrificing accuracy.

Method: Uses duality arguments and analyzes the Helmholtz data-to-solution map, considering billiard trajectories and mesh structure to influence local FEM errors.

Result: Demonstrates that QO and BRE are achievable with coarser meshes in non-trapping regions and PML, where only $hk$ needs to be small, eliminating pollution in PML.

Conclusion: Non-uniform meshes can optimize computational efficiency for the Helmholtz equation while maintaining accuracy, with mesh requirements dependent on local solution behavior and billiard trajectories.

Abstract: The $h$-version of the finite-element method ($h$-FEM) applied to the
high-frequency Helmholtz equation has been a classic topic in numerical
analysis since the 1990s. It is now rigorously understood that (using piecewise
polynomials of degree $p$ on a mesh of a maximal width $h$) the conditions
"$(hk)^p \rho$ sufficiently small" and "$(hk)^{2p} \rho$ sufficiently small"
guarantee, respectively, $k$-uniform quasioptimality (QO) and bounded relative
error (BRE), where $\rho$ is the norm of the solution operator with $\rho\sim
k$ for non-trapping problems. Empirically, these conditions are observed to be
optimal in the context of $h$-FEM with a uniform mesh. This paper demonstrates
that QO and BRE can be achieved using certain non-uniform meshes that violate
the conditions above on $h$ and involve coarser meshes away from trapping and
in the perfectly matched layer (PML). The main theorem details how varying the
meshwidth in one region affects errors both in that region and elsewhere. One
notable consequence is that, for any scattering problem (trapping or
nontrapping), in the PML one only needs $hk$ to be sufficiently small; i.e.
there is no pollution in the PML.
  The motivating idea for the analysis is that the Helmholtz data-to-solution
map behaves differently depending on the locations of both the measurement and
data, in particular, on the properties of billiards trajectories (i.e. rays)
through these sets. Because of this, it is natural that the approximation
requirements for finite-element spaces in a subset should depend on the
properties of billiard rays through that set. Inserting this behaviour into the
latest duality arguments for the FEM applied to the high-frequency Helmholtz
equation allows us to retain detailed information about the influence of
$\textit{both}$ the mesh structure $\textit{and}$ the behaviour of the true
solution on local errors in FEM.

</details>


### [22] [On the Upper Bounds for the Matrix Spectral Norm](https://arxiv.org/abs/2506.15660)
*Alexey Naumov,Maxim Rakhuba,Denis Ryapolov,Sergey Samsonov*

Main category: math.NA

TL;DR: A new Counterbalance estimator for spectral norm estimation via matrix-vector products, offering tighter bounds than the power method, especially for matrices with fast-decaying spectra.


<details>
  <summary>Details</summary>
Motivation: To improve spectral norm estimation accuracy, especially for matrices with fast-decaying spectra, common in deep learning and inverse problems.

Method: Proposes a Counterbalance estimator using matrix-vector products, with probabilistic guarantees on underestimation.

Result: Produces significantly tighter upper bounds than standard methods like the power method in synthetic and real-world settings.

Conclusion: The Counterbalance estimator is effective for spectral norm estimation, particularly for matrices with fast-decaying spectra.

Abstract: We consider the problem of estimating the spectral norm of a matrix using
only matrix-vector products. We propose a new Counterbalance estimator that
provides upper bounds on the norm and derive probabilistic guarantees on its
underestimation. Compared to standard approaches such as the power method, the
proposed estimator produces significantly tighter upper bounds in both
synthetic and real-world settings. Our method is especially effective for
matrices with fast-decaying spectra, such as those arising in deep learning and
inverse problems.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [23] [On the Control of Solutions of a Viscoelastic Plate Problem with a Frictional Damping Term](https://arxiv.org/abs/2506.14908)
*Bilel Madjour,Amel Boudiaf*

Main category: math.AP

TL;DR: The paper analyzes the stability of solutions to a nonlinear viscoelastic plate problem with frictional damping and a logarithmic source, extending prior work.


<details>
  <summary>Details</summary>
Motivation: To improve understanding and results in viscoelastic plate problems by incorporating frictional damping and a logarithmic source.

Method: Study the problem in a bounded domain with a relaxation function satisfying specific decay conditions.

Result: The work extends and improves earlier results in the field.

Conclusion: The study advances the analysis of viscoelastic plate stability under new conditions.

Abstract: In this article, we study the stability of solutions to a nonlinear
viscoelastic plate problem with frictional damping of a memory on a part of the
boundary, and a logarithmic source in a bounded domain $\Omega \subset
\mathbb{R}^2.$ In this problem the relaxation function $r$ satisfies
$r^{\prime}\left( t\right)\leq -h\left( t\right) \mathcal{G}\left( r\left(
t\right) \right)$ for all $t\geq 0$, where $h$ is a nonincreasing positive
function. This work extends previous works with viscoelastic plate problems and
improves earlier results in the

</details>


### [24] [Optimal regularity results in Sobolev-Lorentz spaces for lilnear elliptic equations with $L^1$- or measure data](https://arxiv.org/abs/2506.15005)
*Hyunseok Kim,Young-Ran Lee,Jihoon Ok*

Main category: math.AP

TL;DR: The paper establishes optimal regularity properties for solutions of Poisson and more general elliptic equations in Sobolev-Lorentz and Besov spaces, extending known results for bounded $C^1$-domains and $C^{1,1}$-domains.


<details>
  <summary>Details</summary>
Motivation: To generalize and refine the regularity results for weak and very weak solutions of elliptic equations, particularly in Sobolev-Lorentz and Besov spaces, for domains with varying smoothness.

Method: The authors use Sobolev-Lorentz spaces and embedding results into Besov spaces to analyze the regularity of solutions to Poisson and more general elliptic equations with nonhomogeneous boundary data.

Result: Optimal regularity is proven for solutions: $u \in L_{\alpha+1}^{p(\alpha),\infty}(\Omega ) \cap B_{\alpha+1}^{p(\alpha),\infty}(\Omega )$ for $0 \le \alpha<1$, and similar results for very weak solutions in $C^{1,1}$-domains.

Conclusion: The paper extends and sharpens existing regularity results for elliptic equations, providing a framework for analyzing solutions in Sobolev-Lorentz and Besov spaces.

Abstract: It has been well known that if $\Omega$ is a bounded $C^1$-domain in $\R^n,\
n \ge 2$, then for every Radon measure $f$ on $\Omega$ with finite total
variation, there exists a unique weak solution $u\in W_0^{1,1}(\Omega )$ of the
Poisson equation $-\Delta u=f$ in $\Omega$ satisfying $\nabla u \in
L^{n/(n-1),\infty}(\Omega;\R^n )$. In this paper, optimal regularity properties
of the solution $u$ are established in Sobolev-Lorentz spaces
$L_{\alpha}^{p,q}(\Omega )$ of order $ \alpha$ less than but arbitrarily close
to $2$. More precisely, for any $0 \le \alpha<1$, we show that $u\in
L_{\alpha+1}^{p(\alpha),\infty}(\Omega )$, where $p(\alpha )= n/(n-1+\alpha )$.
Moreover, using an embedding result for Sobolev-Lorentz spaces
$L_{\alpha}^{p,q}(\Omega )$ into classical Besov spaces $B_\alpha^{p,q}(\Omega
)$, we deduce that $u\in B_{\alpha+1}^{p(\alpha),\infty}(\Omega )$. Indeed,
these regularity results are proved for solutions of the Dirichlet problems for
more general linear elliptic equations with nonhomogeneous boundary data.
  On the other hand, it is known that if $\Omega$ is of class $C^{1,1}$, then
for each $G\in L^1 (\Omega ;\R^n )$ there exists a unique very weak solution
$v\in L^{n/(n-1),\infty} (\Omega )$ of $-\Delta v= {\rm div}\, G$ in $\Omega$
satisfying the boundary condition $v=0$ in some sense. We prove that $v$ has
the optimal regularity property, that is, $v\in
L_{\alpha}^{p(\alpha),\infty}(\Omega )\cap B_{\alpha}^{p(\alpha),\infty}(\Omega
)$ for every $0 \le \alpha < 1$. This regularity result is also proved for more
general equations with nonhomogeneous boundary data.

</details>


### [25] [Dynamic Optimal Transport with optimal star shaped graphs](https://arxiv.org/abs/2506.15007)
*Marcello Carioni,Juliane Krautz,Jan-F. Pietschmann*

Main category: math.AP

TL;DR: Existence of solutions for optimal transport problems coupled with dynamic transport on metric graphs, with extensions to star-shaped graphs and topology preservation.


<details>
  <summary>Details</summary>
Motivation: To address optimal transport problems in a compact convex set coupled with dynamic transport on embedded metric graphs, ensuring solutions exist and graph topology is preserved.

Method: Prove existence for fixed graphs, then extend to star-shaped graphs with a penalty to prevent edge overlap, using standard variational techniques.

Result: Existence of minimizers is shown for both fixed and star-shaped graphs, with topology preserved in the latter case.

Conclusion: The approach successfully ensures solution existence and topology preservation for the studied optimal transport problem.

Abstract: We study an optimal transport problem in a compact convex set
$\Omega\subset\mathbb{R}^d$ where bulk transport is coupled to dynamic optimal
transport on a metric graph $ \mathsf{G} = (\mathsf{V},\mathsf{E})$ which is
embedded in $\Omega$. We prove existence of solutions for fixed graphs. Next,
we consider varying graphs, yet only for the case of star-shaped ones. Here,
the action functional is augmented by an additional penalty that prevents the
edges of the graph to overlap. This allows to preserve the graph topology and
thus to rely on standard techniques in Calculus of Variations in order to show
existence of minimizers.

</details>


### [26] [On the solvability of some systems of quadratic integral equations in dimensions two and three](https://arxiv.org/abs/2506.15069)
*Vitali Vougalter*

Main category: math.AP

TL;DR: Existence of solutions for a quadratic integral system in H^2(R^d,R^N) is proven using fixed point methods.


<details>
  <summary>Details</summary>
Motivation: To address the solvability of quadratic integral equations in higher-dimensional spaces.

Method: Fixed point technique applied to the system.

Result: Existence of a perturbed solution in H^2(R^d,R^N) for d=2,3.

Conclusion: The fixed point method successfully proves solution existence for the given system.

Abstract: The work deals with the existence of solutions of a certain system of
quadratic integral equations in H^2(R^d,R^N), d = 2, 3. We demonstrate the
existence of a perturbed solution by virtue of a fixed point technique.

</details>


### [27] [On a reaction-diffusion system modeling strong competition between two mosquito populations](https://arxiv.org/abs/2506.15202)
*Nicolas Vauchelet*

Main category: math.AP

TL;DR: The paper analyzes a reaction-diffusion system modeling competition between two mosquito species, Aedes aegypti and Aedes albopictus, in spatially heterogeneous environments.


<details>
  <summary>Details</summary>
Motivation: To understand the spatial segregation of mosquito species in tropical regions, where Aedes aegypti dominates urban areas and Aedes albopictus thrives in forests, due to larval competition.

Method: A mathematical model using reaction-diffusion equations for aquatic and aerial phases of both species, with strong competition in the larval stage, reducing system dimensionality.

Result: A sufficient condition prevents species invasion in homogeneous environments, and spatial segregation is demonstrated in heterogeneous settings, supported by numerical simulations.

Conclusion: The model explains observed spatial segregation between mosquito species due to competition and environmental heterogeneity.

Abstract: This paper is devoted to the analysis of a reaction-diffusion system with
strong competition and spatial heterogeneities modelling the interaction
between two species of mosquitoes. In particular, we propose a mathematical
model that accounts for the spatial segregation observed between two species of
mosquito vectors of numerous viruses. Indeed, it has been observed that, in
tropical regions, Aedes aegypti mosquitoes are well established in urban areas
whereas Aedes albopictus mosquitoes spread widely in forest regions. Moreover,
these species of mosquitoes compete with each other in the larval stage. Based
on these observations, we introduce a simple mathematical model to account for
this phenomenon. This model consists of a system of reaction-diffusion
equations describing the dynamics of the aquatic and aerial phases of each
species in a spatially heterogeneous environment. The competition takes place
at the aquatic phase and is assumed to be strong which allows us to reduce the
dimensionality of the system. We first establish a sufficient condition on the
parameters to prevent one species from invading another in a homogeneous
environment. Next, using this sufficient condition, we show that spatial
segregation may be observed in a spatially heterogeneous environment. Our
theoretical results are also illustrated by some numerical simulations.

</details>


### [28] [A toy model for frequency cascade in the nonlinear Schrodinger equation](https://arxiv.org/abs/2506.15226)
*Rémi Carles,Erwan Faou*

Main category: math.AP

TL;DR: The paper presents a simple method to observe frequency cascades in forced nonlinear Schrödinger equations, using a perturbed Gaussian well. Stability results and numerical simulations support the findings.


<details>
  <summary>Details</summary>
Motivation: To understand frequency cascades in forced nonlinear Schrödinger equations with a perturbed Gaussian well forcing term.

Method: Algebraic computations are used to derive an explicit frequency cascade by discarding time and space derivatives. Stability is analyzed when derivatives are included.

Result: The initial algebraic solution remains largely unaffected when derivatives are incorporated, supported by numerical simulations.

Conclusion: The approach provides insights into frequency cascades and stability in forced nonlinear Schrödinger equations.

Abstract: We present an elementary approach to observe frequency cascade on forced
nonlinear Schr{\"o}dinger equations. The forcing term consists of a constant
term, perturbed by a modulated Gaussian well. Algebraic computations provide an
explicit frequency cascade when time and space derivatives are discarded from
the nonlinear Schr{\"o}dinger equation. We provide stability results, showing
that when derivatives are incorporated in the model, the initial algebraic
solution may be little affected, possibly over long time intervals. Numerical
simulations are provided, which support the analysis.

</details>


### [29] [Enstrophy dynamics for flow past a solid body with no-slip boundary condition](https://arxiv.org/abs/2506.15317)
*Aleksei Gorshkov*

Main category: math.AP

TL;DR: Study of boundary vorticity's impact on enstrophy dynamics for streamlined body flows, deriving a new energy identity and proving enstrophy dissipativity for Stokes system, with a new equation for Navier-Stokes.


<details>
  <summary>Details</summary>
Motivation: To understand how boundary vorticity distribution influences enstrophy dynamics in flows around streamlined bodies.

Method: Derived a new energy identity incorporating boundary vortex function values, analyzed Stokes and Navier-Stokes systems.

Result: Proved enstrophy dissipativity for Stokes system; obtained a new enstrophy dynamics equation for Navier-Stokes.

Conclusion: Boundary vorticity significantly impacts enstrophy dynamics, with distinct behaviors in Stokes and Navier-Stokes systems.

Abstract: In the paper we study the impact of the boundary vorticity distribution on
the dynamics of enstrophy for flows around streamlined body. A new energy
identity is derived in the article, which includes the boundary values of the
vortex function. For the Stokes system the dissipativity of enstrophy is
proved. For the Navier-Stokes system a new equation of the enstrophy dynamics
is obtained.

</details>


### [30] [The superposition principle for the continuity equation with singular flux](https://arxiv.org/abs/2506.15333)
*Stefano Almi,Riccarda Rossi,Giuseppe Savaré*

Main category: math.AP

TL;DR: The paper extends representation results for absolutely continuous curves in Wasserstein spaces to the case of bounded variation curves and $p=1$, addressing the superposition principle for measure-valued solutions to the continuity equation.


<details>
  <summary>Details</summary>
Motivation: To generalize existing results for evolutionary PDEs in measure-theoretic settings to cases with non-trivial singular flux measures and bounded variation curves.

Method: The study involves analyzing curves in ${\rm BV}_{loc}([0,+\infty);\mathcal{P}_1(\mathbb{R}^d))$ and their relation to solutions of the continuity equation, focusing on minimal singular flux. An auxiliary continuity equation in an augmented phase space is introduced.

Result: A probabilistic representation of solutions $(\mu,\nu)$ is derived using Lipschitz trajectories in the augmented space. A superposition principle for BV curves is also proven, detailing behavior at jump points.

Conclusion: The paper successfully extends representation results to the $p=1$ case and bounded variation curves, providing new tools for studying measure-valued solutions with singular fluxes.

Abstract: Representation results for absolutely continuous curves $\mu:[0,T]\to
\mathcal{P}_p(\mathbb{R}^d)$, $p>1$, with values in the Wasserstein space
$(\mathcal{P}_p(\mathbb{R}^d),W_p)$ of Borel probability measures in
$\mathbb{R}^d$ with finite $p$-moment, provide a crucial tool to study
evolutionary PDEs in a measure-theoretic setting. They are strictly related to
the superposition principle for measure-valued solutions to the continuity
equation. This paper addresses the extension of these results to the case
$p=1$, and to curves $\mu:[0,+\infty)\to\mathcal{P}_1(\mathbb{R}^d)$ that are
only of bounded variation in time: in the corresponding continuity equation,
the flux measure
$\nu\in\mathcal{M}_{loc}([0,+\infty)\times\mathbb{R}^{d};\mathbb{R}^{d})$ thus
possesses a non-trivial singular part w.r.t. $\mu$ in addition to the
absolutely continuous part featuring the velocity field. Firstly, we carefully
address the relation between curves in ${\rm
BV}_{loc}([0,+\infty);\mathcal{P}_1(\mathbb{R}^d))$ and solutions to the
associated continuity equation, among which we select those with minimal
singular (contribution to the) flux $\nu$. We show that, with those
distinguished solutions it is possible to associate an `auxiliary' continuity
equation, in an augmented phase space, solely driven by its velocity field. For
that continuity equation, a standard version of the superposition principle can
be thus obtained. In this way, we derive a first probabilistic representation
of the pair $(\mu,\nu)$ solutions by projection over the time and space
marginals. This representation involves Lipschitz trajectories in the augmented
phase space, reparametrized in time and solving the characteristic system of
ODEs. Finally, for the same pair $(\mu,\nu)$ we also prove a superposition
principle in terms of BV curves on the actual time interval, providing a fine
description of their behaviour at jump points.

</details>


### [31] [Boundary behaviour of layer potentials for the multi-term time-fractional diffusion equation](https://arxiv.org/abs/2506.15356)
*Karolina Pawlak*

Main category: math.AP

TL;DR: The paper analyzes the boundary behavior of layer potentials for the multi-term time-fractional diffusion equation (MTFDE) across a moving boundary, establishing jump relations and continuity properties.


<details>
  <summary>Details</summary>
Motivation: To extend Krasnoschok's results for the time-fractional diffusion equation to the more complex multi-term case, where the fundamental solution lacks standard scaling properties.

Method: Establishes jump relations for the double-layer potential of the inhomogeneous MTFDE and proves continuity for the homogeneous case.

Result: Key results include the jump relation and continuity of the double-layer potential, crucial for boundary integral equations in time-dependent domains.

Conclusion: The findings are vital for analyzing boundary integral equations related to MTFDE in dynamic domains, addressing complexities in the multi-term case.

Abstract: This paper investigates the boundary behaviour of layer potentials for the
multi-term time-fractional diffusion equation (MTFDE) across the moving
boundary. First, we establish the jump relation for the double-layer potential
associated with the fundamental solution of the inhomogeneous MTFDE. Second, we
prove the continuity of the double-layer potential generated by the kernel
corresponding to the homogeneous MTFDE. Krasnoschok obtained similar results
for the time-fractional diffusion equation. However, in the multi-term case,
the fundamental solution has more complex structure and does not admit standard
scaling properties, which requires a different approach. Our results are
essential for the analysis of boundary integral equations related to the MTFDE
in time-dependent domains.

</details>


### [32] [Admissible solutions of the 2D Onsager's conjecture](https://arxiv.org/abs/2506.15396)
*Lili Du,Xinliang Li,Weikui Ye*

Main category: math.AP

TL;DR: The paper constructs Hölder continuous weak solutions for the 2D incompressible Euler equations that dissipate kinetic energy, improving prior work. It also shows initial data density and introduces new methods for energy modulation.


<details>
  <summary>Details</summary>
Motivation: To advance understanding of dissipative weak solutions in fluid dynamics, particularly below the Onsager critical exponent, and refine existing techniques.

Method: Introduces new traveling waves and a multiple iteration scheme combining Newton-Nash and Picard-type iterations for energy control.

Result: Existence of dissipative weak solutions in C^γ for γ < 1/3, with dense initial data in B^γ_∞,r<∞.

Conclusion: The framework extends to higher dimensions and provides a refined approach for constructing dissipative solutions.

Abstract: We show that for any $\gamma < \frac{1}{3}$ there exist H\"{o}lder continuous
weak solutions $v \in C^{\gamma}([0,T] \times \mathbb{T}^2)$ of the
two-dimensional incompressible Euler equations that strictly dissipate the
total kinetic energy, improving upon the elegant work of Giri and Radu [Invent.
Math., 238 (2), 2024]. Furthermore, we prove that the initial data of these
\textit{admissible} solutions are dense in $B^{\gamma}_{\infty,r<\infty}$.
  Our approach introduces a new class of traveling waves, refining the
traditional temporal oscillation function first proposed by Cheskidov and Luo
[Invent. Math., 229(3), 2022], to effectively modulate energy on any time
intervals. Additionally, we propose a novel ``multiple iteration scheme''
combining Newton-Nash iteration with a Picard-type iteration to generate an
energy corrector for controlling total kinetic energy during the perturbation
step. This framework enables us to construct dissipative weak solutions below
the Onsager critical exponent in any dimension $d \geq 2$.

</details>


### [33] [Existence, uniqueness, regularity and stability of solutions to linear $X$-elliptic equations with measurable coefficients](https://arxiv.org/abs/2506.15409)
*Marco Picerni*

Main category: math.AP

TL;DR: Existence and uniqueness of solutions for linear X-elliptic equations with L1 data and zero Dirichlet conditions, with continuous dependence on data and summability improvements.


<details>
  <summary>Details</summary>
Motivation: To extend results for uniformly elliptic equations to X-elliptic equations, ensuring robust solutions under weaker data conditions.

Method: Analyze linear X-elliptic equations with L1 data and zero Dirichlet boundary conditions, proving solution properties.

Result: Solutions exist, are unique, depend continuously on data, and improve in summability with better data.

Conclusion: The findings generalize uniform ellipticity results to X-elliptic cases, maintaining key properties under weaker assumptions.

Abstract: We prove an existence and uniqueness result for solutions to linear
$X$-elliptic equations with $L^1$ data and zero Dirichlet boundary conditions.
Such solutions depend continuously on the datum. Moreover, we show that an
improvement in the summability of the data yields a corresponding improvement
in the summability of the solutions, in a manner analogous to the one that
occurs in the case of uniformly elliptic equations.

</details>


### [34] [Existence and uniqueness of global large-data solutions for the Chemotaxis-Navier-Stokes system in $\mathbb{R}^2$](https://arxiv.org/abs/2506.15434)
*Fan Xu,Feng Dai,Bin Liu*

Main category: math.AP

TL;DR: Global existence and uniqueness of smooth solutions for the Chemotaxis-Navier-Stokes system in 2D under large initial data, using entropy-energy estimates and bootstrap arguments.


<details>
  <summary>Details</summary>
Motivation: Addressing the unresolved issue of global existence and uniqueness of smooth solutions for the CNS system in 2D under large initial data.

Method: Derive entropy-energy estimates for low regularity data, then use bootstrap arguments for higher-order energy estimates.

Result: Established global existence and uniqueness of strong, classical, and arbitrarily smooth solutions.

Conclusion: The intrinsic entropy structure and parabolic nature of the CNS system enable rigorous control of regularity, solving the problem.

Abstract: This work investigates the Cauchy problem for the classical
Chemotaxis-Navier-Stokes (CNS) system in $\mathbb{R}^2$. We establish the
global existence and uniqueness of strong, classical, and arbitrarily smooth
solutions under large initial data, which has not been addressed in the
existing literature. The key idea is to first derive an entropy-energy estimate
for initial data with low regularity, by leveraging the intrinsic entropy
structure of the system. Building on this foundation, we then obtain
higher-order energy estimates for smoother initial data via a bootstrap
argument, in which the parabolic nature of the CNS system plays a crucial role
in the iterative control of regularity.

</details>


### [35] [The Rayleigh-Boltzmann equation with shear deformations in the hyperbolic-dominated regime](https://arxiv.org/abs/2506.15449)
*Nicola Miele,Alessia Nota,Juan J. L. Velázquez*

Main category: math.AP

TL;DR: The paper analyzes homoenergetic solutions of the Rayleigh-Boltzmann equation, focusing on the hyperbolic-dominated regime for collision kernels with γ ∈ (-1,0). It provides formal long-time asymptotics for velocity distribution and a probabilistic interpretation of the process.


<details>
  <summary>Details</summary>
Motivation: To understand the long-term behavior of solutions in the hyperbolic-dominated regime, where deformation dominates collisions, and to describe the asymptotic profile of velocity distributions.

Method: Formal analysis of long-time asymptotics for velocity distribution, explicit derivation of the asymptotic profile, and probabilistic interpretation of the process as a combination of shear flows and collisions.

Result: Explicit form of the asymptotic profile for velocity distribution and identification of different asymptotic behaviors for γ < -1. The process is modeled as a Markov process combining shear flows and random jumps.

Conclusion: The study provides insights into the long-term dynamics of homoenergetic solutions in the hyperbolic-dominated regime, offering a formal description of asymptotic behavior and a probabilistic framework for the process.

Abstract: In this paper we consider a particular class of solutions of the
Rayleigh-Boltzmann equation, known in the nonlinear setting as homoenergetic
solutions, which have the form $g\left( x,v,t \right) =f\left( v-L\left(
t\right)x,t\right)$ where the matrix $L(t)$ describes a shear flow deformation.
We began this analysis in [22] where we rigorously proved the existence of a
stationary non-equilibrium solution and established the different behaviour of
the solutions for small and large values of the shear parameter, for cut-off
collision kernels with homogeneity parameter $0\leq \gamma <1$, including
Maxwell molecules and hard potentials. In this paper, we concentrate in the
case where the deformation term dominates the collision term for large times
(hyperbolic-dominated regime). This occurs for collision kernels with $\gamma <
0$ and in particular we focus on $\gamma \in (-1,0)$. In such a
hyperbolic-dominated regime, it appears challenging to provide a clear
description of the long-term asymptotics of the solutions. Here we present a
formal analysis of the long-time asymptotics for the distribution of velocities
and provide the explicit form for the asymptotic profile. Additionally, we
discuss the different asymptotic behaviour expected in the case of homogeneity
$\gamma < -1$. Furthermore, we provide a probabilistic interpretation
describing a stochastic process consisting in a combination of collisions and
shear flows. The tagged particle velocity $\{v(t)\}_{t\geq 0}$ is a Markov
process that arises from the combination of free flights in a shear flow along
with random jumps caused by collisions.

</details>


### [36] [Strichartz estimates for the generalized Zakharov-Kuznetsov equation on $\mathbb{R} \times \mathbb{T}$ and applications](https://arxiv.org/abs/2506.15517)
*Jakob Nowicki-Koth*

Main category: math.AP

TL;DR: The paper improves the well-posedness threshold for the $k$-generalized Zakharov-Kuznetsov equation using linear and bilinear estimates, achieving local well-posedness for $s > \frac{3}{8}$.


<details>
  <summary>Details</summary>
Motivation: To address the Cauchy problem for the $k$-generalized Zakharov-Kuznetsov equation and improve its well-posedness conditions.

Method: Established an almost optimal linear $L^4$-estimate and a family of bilinear refinements.

Result: Lowered the well-posedness threshold for all $k \geq 2$, showing local well-posedness in $H^s(\mathbb{R} \times \mathbb{T})$ for $s > \frac{3}{8}$.

Conclusion: The modified Zakharov-Kuznetsov equation is locally well-posed for $s > \frac{3}{8}$, marking a significant improvement.

Abstract: In this article, we address the Cauchy problem associated with the
$k$-generalized Zakharov-Kuznetsov equation posed on $\mathbb{R} \times
\mathbb{T}$. By establishing an almost optimal linear $L^4$-estimate, along
with a family of bilinear refinements, we significantly lower the
well-posedness threshold for all $k \geq 2$. In particular, we show that the
modified Zakharov-Kuznetsov equation is locally well-posed in $H^s(\mathbb{R}
\times \mathbb{T})$ for all $s > \frac{3}{8}$.

</details>


### [37] [Rigidity of solutions to singular/degenerate semilinear critical equations](https://arxiv.org/abs/2506.15611)
*Giovanni Catino,Dario Daniele Monticelli,Alberto Roncoroni*

Main category: math.AP

TL;DR: The paper classifies positive solutions for singular/degenerate semilinear critical equations linked to Caffarelli-Kohn-Nirenberg inequalities in ℝᵈ (d≥2), focusing on rigidity results for solutions with possibly infinite energy when the intrinsic dimension n satisfies 2<n<4.


<details>
  <summary>Details</summary>
Motivation: To understand and classify solutions of critical equations derived from Caffarelli-Kohn-Nirenberg inequalities, particularly for positive solutions with infinite energy in specific intrinsic dimensions.

Method: The study involves analyzing singular/degenerate semilinear critical equations as Euler-Lagrange equations, with a focus on rigidity results.

Result: Several rigidity results for positive solutions are proven, including classification of solutions with possibly infinite energy for intrinsic dimensions 2<n<4.

Conclusion: The paper successfully classifies solutions under specified conditions, contributing to the understanding of critical equations in higher dimensions.

Abstract: This paper deals with singular/degenerate semilinear critical equations which
arise as the Euler-Lagrange equation of Caffarelli-Kohn-Nirenberg inequalities
in $\mathbb{R}^d$, with $d\geq 2$. We prove several rigidity results for
positive solutions, in particular we classify solutions with possibily infinite
energy when the intrinsic dimension $n$ satisfies $2<n<4$.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [38] [An Atomic Cluster Expansion Potential for Twisted Multilayer Graphene](https://arxiv.org/abs/2506.15061)
*Yangshuai Wang,Drake Clark,Sambit Das,Ziyan Zhu,Daniel Massatt,Vikram Gavini,Mitchell Luskin,Christoph Ortner*

Main category: physics.comp-ph

TL;DR: The paper develops a machine-learning interatomic potential (ACE) for twisted multilayer graphene, addressing computational challenges of traditional methods.


<details>
  <summary>Details</summary>
Motivation: Twisted multilayer graphene's quantum phenomena are hard to simulate accurately with first-principles or empirical potentials, motivating the use of MLIPs.

Method: An Atomic Cluster Expansion (ACE) potential is developed, with training datasets incorporating all twist angles and local stacking, refined via active learning.

Result: The model is validated for accuracy and robustness through extensive numerical tests.

Conclusion: The ACE potential offers a computationally efficient and accurate alternative for simulating twisted multilayer graphene.

Abstract: Twisted multilayer graphene, characterized by its moire patterns arising from
inter-layer rotational misalignment, serves as a rich platform for exploring
quantum phenomena. While first-principles calculations are computationally
prohibitive and empirical interatomic potentials often lack accuracy,
machine-learning interatomic potentials (MLIPs) present a promising
alternative, offering (near-)DFT accuracy at a significantly reduced
computational cost. Despite their success in two-dimensional monolayer
materials, MLIPs remain under-explored in twisted multilayer graphene systems.
In this work, we develop an Atomic Cluster Expansion (ACE) potential for
simulating twisted multilayer graphene and test it on a range of simulation
tasks. We propose an approach to construct training and test datasets that
incorporate all possible twist angles and local stacking, including
incommensurate ones. To achieve this, we generate configurations with periodic
boundary conditions suitable for DFT calculations, and then introduce an
internal twist and shift within those supercell structures. We further refine
the dataset through active learning filtering, guided by Bayesian uncertainty
quantification. Our model is validated for accuracy and robustness through a
wide range of numerical tests.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [39] [Electrically insulating materials for centrifugal mirrors](https://arxiv.org/abs/2506.14847)
*Nick R. Schwartz,Carlos A. Romero-Talamás,Marlene I. Patino,Daisuke Nishijima,Matthew J. Baldwin,Russel P. Doerner,Artur Perevalov,Nathan Eschbach,Zachary D. Short,John Cumings,Ian G. Abel,Brian L. Beaudoin,Timothy W. Koeth*

Main category: physics.plasm-ph

TL;DR: The paper explores the use of hexagonal boron nitride (hBN) in centrifugal mirror fusion devices, testing its performance under plasma conditions and comparing it to other materials like silicon carbide.


<details>
  <summary>Details</summary>
Motivation: To investigate hBN's suitability for plasma-facing components in centrifugal mirror fusion devices, given its thermal and electrical properties, and assess its performance under intense plasma conditions.

Method: Computational modeling (RustBCA and OpenMC) for ion- and neutron-material interactions, followed by experimental plasma exposure tests in PISCES-A and CMFX.

Result: hBN showed better performance than silicon carbide, with less erosion from sputtering and grain ejection.

Conclusion: hBN is a promising material for plasma-facing components in centrifugal mirrors due to its superior erosion resistance compared to alternatives.

Abstract: The centrifugal mirror confinement scheme incorporates supersonic rotation
into a magnetic mirror device, which stabilizes and heats the plasma. This
concept is under investigation in the Centrifugal Mirror Fusion Experiment
(CMFX) at the University of Maryland. Plasma rotation is driven by an axial
magnetic field and a radial electric field that lead to velocity drifts in the
azimuthal direction. An electrically insulating material is required to prevent
the applied voltage from shorting on the grounded chamber. Hexagonal boron
nitride (hBN) is a promising candidate material for plasma-facing components in
future centrifugal mirrors due to its exceptional thermal and electrical
properties. However, its performance under intense particle and heat fluxes
characteristic of the plasma edge in fusion devices remains largely unexplored.
Computational modeling for ion- and neutron-material interactions was carried
out with RustBCA and OpenMC, respectively, and predicts relatively good
performance in comparison to other insulating materials. Material coupons were
then exposed to plasma in PISCES-A at UCSD and CMFX. A load-locked sample
feedthrough was constructed and installed on CMFX to test coupons. Two erosion
mechanisms were identified -- sputtering and grain ejection -- both of which
were more apparent in silicon carbide than hBN.

</details>


### [40] [Determining the Validity of Tokamak Perturbed Equilibrium Modeling Using Nonlinear Equilibria](https://arxiv.org/abs/2506.14938)
*J. Halpern,N. C. Logan,E. Paul,C. Paz-Soldan*

Main category: physics.plasm-ph

TL;DR: The paper determines the correct reference frame for predicting perturbed equilibrium in tokamaks with small 3D fields, validating linear MHD theory for existing tolerances but noting limitations for future devices.


<details>
  <summary>Details</summary>
Motivation: To address the dependency of perturbed equilibrium predictions on the assumed reference frame in tokamaks with small 3D fields, ensuring accurate modeling for engineering applications.

Method: Uses fully 3D equilibria from VMEC to analyze the correct reference frame under n=1 coil asymmetries, examining cases for SPARC and NSTX-U, and evaluates magnetic axis shifts and field line displacements.

Result: The TF coil centroid approximates the correct frame for SPARC, while the TF coil inboard legs' radial location works for NSTX-U. Linear MHD theory is valid for current tolerances but may falter for future devices with larger tolerances.

Conclusion: The study validates perturbative models for setting assembly tolerances, providing clarity on theory application but cautioning for future devices with higher tolerances.

Abstract: The prediction of perturbed equilibrium models for tokamaks with small 3D
fields is strongly dependent on which reference frame for axisymmetry is
assumed - for example, the toroidal field (TF) coil vs the poloidal field (PF)
coil centroid. We use fully 3D equilibria generated by VMEC to determine the
correct reference frame in these models when subject to n=1 coil asymmetries.
We analyze a case for SPARC and find that the appropriate frame can be well
approximated by the centroid of the TF coil set. We also consider the case of
NSTX-U and find that the frame can be approximated by the radial location of
the TF coil inboard legs at the midplane. We determine the correct frame by
analyzing the shifted magnetic axis in the nonlinear equilibria. We also
analyze the magnetic field line displacement to identify where the linearized
MHD theory begins to break down, and compare that to typical coil tolerances in
existing tokamaks. We find that linear theory is valid for existing tolerances,
validating the use of perturbative codes to set these tolerances, but with a
sufficiently small margin to be of note for future devices with relative
tolerances larger than approximately $1\%$ of the minor radius. This study
enables engineers to confidently use 3D perturbative models for determining
assembly tolerances by providing insight into the correct applications of the
theory.

</details>


### [41] [Imaginary part of the conductivity using Kramers-Kronig relations](https://arxiv.org/abs/2506.14941)
*Jean-Christophe Pain,Mikael Tacu*

Main category: physics.plasm-ph

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In order to obtain the frequency-dependent photo-absorption in a plasma, both
the real and imaginary parts of the AC conductivity are required. The real part
can be deduced from the knowledge of the static conductivity (given by the
Ziman-Evans formula for instance) and the Drude model. The imaginary part,
required for the refraction index, can be obtained using the Kramers-Kronig
relations. Usually, it is obtained by complex integration in the complex plane
of the usual Kramers-Kronig relations, having $\omega'-\omega$ in the
denominator. However, an alternate form of the Kramers-Kronig relation is often
used in physics, especially for determining response functions. It has
$\omega'^2-\omega^2$ in the denominator. We provide two determinations of the
imaginary part of the conductivity for this latter form, one using a
decomposition into simple elements, and the other involving a complex
integration in a quarter of the complex plane.

</details>


### [42] [Fourier Transform Method Of A Detailed Configuration Accounting In Hot Plasma Bound-Bound Opacity Calculations](https://arxiv.org/abs/2506.14953)
*Evgeniya Arapova,Yulia Koryakina,Mikhail Vronskiy*

Main category: physics.plasm-ph

TL;DR: Extension of Hazak-Kurzweil method for bound-bound opacity calculations using Detailed Configuration Accounting.


<details>
  <summary>Details</summary>
Motivation: To improve the Super Transition Arrays approach by extending the Hazak-Kurzweil method for more detailed configuration accounting.

Method: Based on Fourier transform representation, linearity of transition energy, and factorization of configuration probabilities.

Result: Alternative expressions for bound-bound opacity, suitable for numerical implementation.

Conclusion: The extended method provides a convenient and effective alternative for opacity calculations.

Abstract: G.~Hazak and J.~Kurzweil discovered a method of configurational resolution of
transition arrays for the Super Transition Arrays approach to the bound-bound
opacity calculation. Their method is based on the representation of the
photoabsorption coefficient as the Fourier transform, the linearity of the
transition energy between configurations with respect to shell occupation
numbers, and factorization of the probabilities of configurations on shell
occupation numbers. We extend the Hazak -- Kurzweil method for the calculations
with Detailed Configuration Accounting. The resulting expressions for the
bound-bound opacity represent an alternative to the widely used ones and are
quite convenient for numerical implementation.

</details>


### [43] [The Gardner equation and acoustic solitary waves in plasmas](https://arxiv.org/abs/2506.15024)
*Frank Verheest,Willy A. Hereman*

Main category: physics.plasm-ph

TL;DR: The paper investigates ion-acoustic waves in a dusty plasma using Cairns-distributed ions and Boltzmann-distributed electrons, comparing results from Sagdeev pseudopotential analysis (SPA) and reductive perturbation theory (RPT).


<details>
  <summary>Details</summary>
Motivation: To understand and compare the accuracy and applicability of two theoretical methods (SPA and RPT) for analyzing soliton profiles in dusty plasmas.

Method: SPA (incorporates all nonlinearities, requires numerical integration) and RPT (yields the Gardner equation, provides analytic soliton profiles).

Result: SPA is more accurate but computationally intensive, while RPT offers analytic solutions under specific conditions (quadratic term much smaller than cubic term).

Conclusion: Both methods provide valuable insights, with SPA being more precise and RPT being more tractable for analytic solutions under constrained conditions.

Abstract: Ion-acoustic waves in a dusty plasma are investigated where it is assumed
that the ions follow a Cairns distribution and the electrons are Boltzmann
distributed. Two theoretical methods are applied: Sagdeev pseudopotential
analysis (SPA) and reductive perturbation theory (RPT). Since SPA incorporates
all nonlinearities of the model it is the most accurate but deriving soliton
profiles requires numerical integration of Poisson's equation. By contrast, RPT
is a perturbation method which at second order yields the Gardner equation
incorporating both the quadratic nonlinearity of the KdV equation with the
cubic nonlinearity of the modified KdV equation. For consistency with the
perturbation scheme the coefficient of the quadratic term needs to be at least
an order of magnitude smaller than the coefficient of the cubic term. Solving
the Gardner equation yields an analytic expression of the soliton profile.
Selecting an appropriate set of compositional parameters, the soliton solutions
obtained from SPA and RPT are analyzed and compared.

</details>


### [44] [Helical Electron Beam Micro-Bunching by High-Order Modes in a Micro-Plasma Waveguide](https://arxiv.org/abs/2506.15094)
*Xinju Guo,Longqing Yi*

Main category: physics.plasm-ph

TL;DR: High-energy, high-charge electron beams are generated using a Laguerre-Gaussian laser in a micro-plasma waveguide, with controllable helicity via laser polarization.


<details>
  <summary>Details</summary>
Motivation: To explore efficient electron acceleration and beam control using high-power lasers and waveguide modes for applications in fundamental science and technology.

Method: 3D particle-in-cell simulations to study electron acceleration by a Laguerre-Gaussian laser in a micro-plasma waveguide, focusing on longitudinal field effects.

Result: Production of ~100 MeV electron beams with high charge (~10 nC), short duration (~30 fs), and small divergence (~1 deg), with helical micro-bunching controlled by laser polarization.

Conclusion: This method enables controllable, high-quality electron beams with helicity, promising for scientific and practical advancements.

Abstract: Electron acceleration by a high-power Laguerre-Gaussian pulse in a
micro-plasma waveguide is investigated. When the incident laser travels in the
waveguide, electrons on the wall are extracted into the vacuum core and
accelerated by the longitudinal field of the waveguide mode. Using 3D
particle-in-cell simulations, we demonstrate that high energy (~100 MeV)
electron beams with extremely high charge (~10 nC), ultrashort duration (~30
fs) and small divergence (~1 deg) can be produced by a 100-TW, few-Joule class
laser system. In particular, when the drive Laguerre-Gaussian pulse is
circularly polarized, it excites high-order waveguide modes that exhibit
helical longitudinal electric fields. The 3D profile of this accelerating field
is imprinted into the high energy electron beam, leading to helical
micro-bunching. This process can be controlled by the spin and orbital angular
momentum of the drive pulse. This work paves the way to the generation of
highcharge, relativistic electron beams with controlled helicity, which holds
great potential for advances in fundamental science and a variety of
applications.

</details>


### [45] [Laser-Driven Annular Shock Waves as Laboratory Analogues of $w$CDM Cosmologies and Cosmological Gravitational Waves](https://arxiv.org/abs/2506.15493)
*Felipe A. Asenjo,Felipe Veloso,Julio C. Valenzuela*

Main category: physics.plasm-ph

TL;DR: Experimental evolution of a plasma shock wave mimics cosmological universe dynamics, enabling lab-scale study of $w$CDM cosmologies and gravitational perturbations.


<details>
  <summary>Details</summary>
Motivation: To explore cosmological models and gravitational waves through controlled laboratory experiments using plasma shock waves.

Method: Laser-driven annular plasma shock waves are evolved and analyzed for self-interaction and propagation dynamics, comparing them to cosmological scenarios.

Result: Shock wave dynamics resemble $w$CDM cosmologies, follow a Hubble-like law, and perturbations mimic cosmological gravitational effects.

Conclusion: This approach provides a novel experimental method for simulating cosmological models and gravitational phenomena in the lab.

Abstract: We demonstrate that the experimental evolution of an annular, laser-driven
plasma shock wave, expanding over time and undergoing self-interaction gives
rise to multiple shock structures that evolve analogously to a multicomponent
cosmological universe. Different propagation trajectories along the shock
surface correspond to various forms of $w$CDM cosmologies, enabling the study
of scenarios ranging from simple radiation- or matter-dominated universes to
those including dark energy. We further show that the dynamics of the Mach
stems approximately follows a Hubble-like law. Additionally, perturbations in
the shock fronts serve as experimental analogues of cosmological gravitational
perturbations in a matter-dominated universe. This work opens a new
experimental pathway for classically simulating complex cosmological models and
gravitational waves at macroscopic scales in the laboratory.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [46] [Shi-type and Hamilton-type gradient estimates for a general parabolic equation under compact Finsler $CD(-K,N)$ geometric flows](https://arxiv.org/abs/2506.14776)
*Yijie Miao,Bin Shen*

Main category: math.DG

TL;DR: The paper studies Li-Yau-type gradient estimates for parabolic equations under Finsler geometric flow, proposing Shi-type and Hamilton-type estimates to relax stricter curvature conditions.


<details>
  <summary>Details</summary>
Motivation: To address the stricter derivative bounds imposed by curvature conditions in Finsler geometry compared to Riemannian cases.

Method: Uses Shi-type and Hamilton-type gradient estimates to analyze positive solutions of the parabolic equation under compact Finsler CD(-K,N) geometric flow.

Result: Demonstrates the possibility of removing stricter curvature conditions.

Conclusion: The proposed gradient estimates offer a way to relax constraints in Finsler geometric flows.

Abstract: Recently, the Li-Yau-type gradient estimates for positive solutions to
parabolic equations
  \begin{equation}
  \partial_t u=\Delta
u+\mathcal{R}_1u+\mathcal{R}_2u^{\alpha}+\mathcal{R}_3u(\log u)^{\beta},\notag
  \end{equation}
  under the general compact Finsler $CD(-K,N)$ geometric flow are studied. Here
$\mathcal{R}_1$,$\mathcal{R}_2$,$\mathcal{R}_3$ $\in$ $ C^{1}(M,[0,T])$,
$\alpha$ and $\beta$ are both positive constants, $T$ is the maximal existence
time for the flow.
  However, compared with the Riemannian case, the curvature conditions impose
stricter derivative bounds on the development term in the geometric flow, as
well as on the derivative bounds of the distortion of the manifold. In this
manuscript, we present Shi-type and Hamilton-type gradient estimates to
demonstrate the possibility of removing such conditions.

</details>


### [47] [Dominating manifolds by radial balls](https://arxiv.org/abs/2506.15621)
*Samuel Bronstein*

Main category: math.DG

TL;DR: The paper explores functional rearrangement on noncollapsed RCD(k,n)-spaces, proving inequalities like Polya-Szego and Moser-Trudinger on certain manifolds.


<details>
  <summary>Details</summary>
Motivation: To extend functional inequalities to a broader class of manifolds with bounded Ricci curvature.

Method: Rearrangement of functions on RCD(k,n)-spaces and proving Polya-Szego type inequalities.

Result: Characterization of manifolds admitting Moser-Trudinger type inequalities among those with lower bounded Ricci curvature.

Conclusion: The work generalizes functional inequalities to noncollapsed RCD(k,n)-spaces, providing new insights into their properties.

Abstract: This paper is devoted to a kind of rearrangement of functions on noncollapsed
RCD(k,n)-spaces, which satisfy a Polya-Szego type inequality. This allows to
prove functional inequalities on a wide class of manifolds, that we describe.
As a consequence, we give a characterization, among manifolds with lower
bounded Ricci curvature, of those admitting a Moser-Trudinger type inequality

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [48] [Density estimation via periodic scaled Korobov kernel method with exponential decay condition](https://arxiv.org/abs/2506.15419)
*Ziyang Ye,Haoyuan Tan,Xiaoqun Wang,Zhijian He*

Main category: math.ST

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We propose the periodic scaled Korobov kernel (PSKK) method for nonparametric
density estimation on $\mathbb{R}^d$. By first wrapping the target density into
a periodic version through modulo operation and subsequently applying kernel
ridge regression in scaled Korobov spaces, we extend the kernel approach
proposed by Kazashi and Nobile (SIAM J. Numer. Anal., 2023) and eliminate its
requirement for inherent periodicity of the density function. This key
modification enables effective estimation of densities defined on unbounded
domains. We establish rigorous mean integrated squared error (MISE) bounds,
proving that for densities with smoothness of order $\alpha$ and exponential
decay, our method achieves the $\mathcal{O}(M^{-1/(1+1/(2\alpha)+\epsilon)})$
MISE convergence rate with an arbitrarily small $\epsilon>0$. While matching
the convergence rate of the previous kernel approach, our approach applies to a
broader class of non-periodic distributions. Numerical experiments confirm the
theoretical results and demonstrate significant improvement over traditional
kernel density estimation in large-sample regimes.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [49] [MOFClassifier: A Machine Learning Approach for Validating Computation-Ready Metal-Organic Frameworks](https://arxiv.org/abs/2506.14845)
*Guobin Zhao,Pengyu Zhao,Yongchul G. Chung*

Main category: physics.chem-ph

TL;DR: MOFClassifier, a machine learning model using PU-CGCNN, improves MOF database accuracy by predicting crystal-likeness scores, outperforming rule-based methods.


<details>
  <summary>Details</summary>
Motivation: Existing MOF databases contain errors, and rule-based methods for correction have limitations, hindering efficient computational screening.

Method: Developed MOFClassifier, a PU-CGCNN-based model, to predict CLscores and classify MOFs as computation-ready.

Result: Achieved a ROC value of 0.979, identifying errors undetectable by rule-based methods and recovering misclassified structures.

Conclusion: MOFClassifier enhances database accuracy, aiding large-scale screening and accelerating MOF discovery.

Abstract: The computational discovery and design of new crystalline materials,
particularly metal-organic frameworks (MOFs), heavily relies on high-quality,
computation-ready structural data. However, recent studies have revealed
significant error rates within existing MOF databases, posing a critical data
problem that hinders efficient high-throughput computational screening. While
rule-based algorithms like MOSAEC, MOFChecker, and the Chen and Manz method
(Chen-Manz) have been developed to address this, they often suffer from
inherent limitations and misclassification of structures. To overcome this
challenge, we introduce MOFClassifier, a novel machine learning approach built
upon a positive-unlabeled crystal graph convolutional neural network (PU-CGCNN)
model. MOFClassifier learns intricate patterns from perfect crys-tal structures
to predict a crystal-likeness score (CLscore), effectively classifying MOFs as
computation-ready. Our model achieves a ROC value of 0.979 (previous best
0.912) and, importantly, can identify subtle structural and chemical errors
that are fundamentally undetectable by current rule-based methods. By
accurately recovering previously misclassified false-negative structures,
MOFClassifier reduces the risk of overlooking promising material candidates in
large-scale computational screening efforts. This user-friendly tool is freely
available and has been integrated into the preparation workflow for the updated
CoRE MOF DB 2025 v1, contributing to accelerated computational discovery of MOF
materials.

</details>


### [50] [Understanding multi-fidelity training of machine-learned force-fields](https://arxiv.org/abs/2506.14963)
*John L. A. Gardner,Hannes Schulz,Jean Helie,Lixin Sun,Gregor N. C. Simm*

Main category: physics.chem-ph

TL;DR: The paper compares two multi-fidelity training strategies (pre-training/fine-tuning and multi-headed training) for machine-learned force fields (MLFFs), highlighting their mechanisms, trade-offs, and potential for universal MLFFs.


<details>
  <summary>Details</summary>
Motivation: To improve the applicability of MLFFs across diverse chemical systems by effectively leveraging data from multiple quantum-chemical methods.

Method: Systematic investigation of pre-training/fine-tuning and multi-headed training strategies, analyzing their mechanisms and adaptability.

Result: Pre-training requires backbone adaptation; multi-headed models learn method-agnostic representations but slightly compromise accuracy.

Conclusion: Multi-headed training offers extensibility and cost-efficiency, advancing towards universal MLFFs despite minor accuracy trade-offs.

Abstract: Effectively leveraging data from multiple quantum-chemical methods is
essential for building machine-learned force fields (MLFFs) that are applicable
to a wide range of chemical systems. This study systematically investigates two
multi-fidelity training strategies, pre-training/fine-tuning and multi-headed
training, to elucidate the mechanisms underpinning their success. We identify
key factors driving the efficacy of pre-training followed by fine-tuning, but
find that internal representations learned during pre-training are inherently
method-specific, requiring adaptation of the model backbone during fine-tuning.
Multi-headed models offer an extensible alternative, enabling simultaneous
training on multiple fidelities. We demonstrate that a multi-headed model
learns method-agnostic representations that allow for accurate predictions
across multiple label sources. While this approach introduces a slight accuracy
compromise compared to sequential fine-tuning, it unlocks new cost-efficient
data generation strategies and paves the way towards developing universal
MLFFs.

</details>


### [51] [How CO Affects the Composition of Titan's Tholins Generated with ECR Plasma](https://arxiv.org/abs/2506.14841)
*Zhengbo Yang,Yu Liu,Chao He,Pengcheng Yu,Rong Jin,Xiangqun Liu,Jinpu Zhang,Jiuhou Lei*

Main category: physics.chem-ph

TL;DR: The study investigates how CO affects Titan's haze formation by simulating its atmosphere with varying CO levels, revealing CO enhances organic compound diversity and nitrogen reactivity.


<details>
  <summary>Details</summary>
Motivation: Understanding the role of CO in Titan's atmospheric chemistry and haze formation, given its significant impact on oxygen-containing organic compounds.

Method: Experimental simulation using N2/CH4 mixtures with 0.2% to 9% CO, exposed to ECR plasma. Analyzed with optical emission spectroscopy, infrared spectroscopy, and mass spectrometry.

Result: CO enriches chemical complexity, supplies oxygen, and boosts nitrogen reactivity, increasing organic product variety and quantity.

Conclusion: CO plays a crucial role in Titan's atmospheric chemistry, enhancing organic synthesis and haze formation.

Abstract: Titan's atmosphere possesses thick haze layers, but their formation
mechanisms remain poorly understood, including the influence of
oxygen-containing gas components on organic matter synthesis. As the most
abundant oxygen-containing gas, the presence of CO has been found to exert a
significant impact on the generation of oxygen-containing organic compounds.
Therefore, investigating the influence of CO on the production and composition
of Tholins through laboratory simulations, holds profound scientific
significance in the context of Titan. The work presented here is an
experimental simulation designed to evaluate the impact of CO on the
atmospheric chemistry of Titan. To this end, CO was introduced into the
standard N2/CH4 mixture at varying mixing ratios from 0.2% to 9%, and exposed
to Electron Cyclotron Resonance (ECR) plasma to initiate photochemical
reactions. Optical emission spectroscopy was employed for gas-phase in situ
characterization, while infrared spectroscopy and high-resolution mass
spectrometry were used to analyze the resulting solid products (tholins). Our
results demonstrate that the addition of CO enriches the complexity of the
chemical system. CO not only supplies oxygen to the system, but also enhances
nitrogen's reactivity and incorporation, enhancing the number and quantity of
the organic products.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [52] [Machine learning based prediction of dynamical clustering in granular gases](https://arxiv.org/abs/2506.15657)
*Sai Preetham Sata,Ralf Stannarius,Benjamin Noack,Dmitry Puzyrev*

Main category: cond-mat.soft

TL;DR: The paper explores dynamical clustering in dense granular gases under microgravity, using DEM simulations and machine learning to predict gas-cluster transitions.


<details>
  <summary>Details</summary>
Motivation: To understand the emergence of spatial inhomogeneities (clusters) in granular gases due to energy influx and dissipation, and to find efficient methods to predict these transitions.

Method: Discrete Element Method (DEM) simulations for frictional spheres in a cuboid container, combined with Kolmogorov-Smirnov tests and machine learning models.

Result: Machine learning models effectively predict dynamical clustering, offering an alternative to complex DEM simulations.

Conclusion: Machine learning is a promising tool for studying gas-cluster transitions, reducing reliance on time-consuming simulations.

Abstract: When dense granular gases are continuously excited under microgravity
conditions, spatial inhomogeneities of the particle number density can emerge.
A significant share of particles may collect in strongly overpopulated regions,
called clusters. This dynamical clustering, or gas-cluster transition, is
caused by a complex interplay and balance between the energy influx and
dissipation in particle collisions. Particle number density, container
geometry, and excitation strength influence this transition. We perform
Discrete Element Method (DEM) simulations for ensembles of frictional spheres
in a cuboid container and apply the Kolmogorov Smirnov test and a caging
criterion to the local packing fraction profiles to detect clusters. Machine
learning can be used to study the gas-cluster transition, and can be a
promising alternative to identify the state of the system for a given set of
system parameters without time-consuming complex DEM simulations. We test
various machine learning models and identify the best models to predict
dynamical clustering of frictional spheres in a specific experimental geometry.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [53] [The Quasi-Radial Field-line Tracing (QRaFT): an Adaptive Segmentation of the Open-Flux Solar Corona](https://arxiv.org/abs/2506.14894)
*Vadim M. Uritsky,Christopher E. Rura,Cooper Downs,Shaela I. Jones,Charles Nickolos Arge,Nathalia Alzate*

Main category: astro-ph.SR

TL;DR: The paper introduces QRaFT, a new method for detecting field-aligned optical coronal features to approximate the steady-state open magnetic field, tested with synthetic and real-life images.


<details>
  <summary>Details</summary>
Motivation: Understanding the large-scale open field of the solar corona is crucial for solar wind dynamics and space weather forecasting, but current methods face challenges due to low signal-to-noise ratios.

Method: The Quasi-Radial Field-line Tracing (QRaFT) methodology is developed and tested using synthetic coronagraph images from a 3D magnetohydrodynamic model and real-life coronal images.

Result: QRaFT-detected features align within ∼4-7 degrees with the local magnetic field in simulations and perform well on real-life images.

Conclusion: QRaFT provides valuable empirical data on coronal morphology, potentially enhancing coronal, solar wind models, and space weather forecasts.

Abstract: Optical observations of solar corona provide key information on its magnetic
geometry. The large-scale open field of the corona plays an important role in
shaping the ambient solar wind and constraining the propagation dynamics of the
embedded structures, such as interplanetary coronal mass ejections. Rigorous
analysis of the open-flux coronal regions based on coronagraph images can be
quite challenging because of the depleted plasma density resulting in low
signal-to-noise ratios. In this paper, we present an in-depth description of a
new image segmentation methodology, the Quasi-Radial Field-line Tracing
(QRaFT), enabling a detection of field-aligned optical coronal features
approximating the orientation of the steady-state open magnetic field. The
methodology is tested using synthetic coronagraph images generated by a
three-dimensional magnetohydrodynamic model. The results of the numerical tests
indicate that the extracted optical features are aligned within $\sim 4-7$
degrees with the local magnetic field in the underlying numerical solution. We
also demonstrate the performance of the method on real-life coronal images
obtained from a space-borne coronagraph and a ground-based camera. We argue
that QRaFT outputs contain valuable empirical information about the global
steady-state morphology of the corona which could help improving the accuracy
of coronal and solar wind models and space weather forecasts.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [54] [Analysis and conditional optimization of projection estimates for the distribution of random variable using Legendre polynomials](https://arxiv.org/abs/2506.14822)
*Tatyana A. Averina,Konstantin A. Rybakov*

Main category: stat.CO

TL;DR: Algorithms for estimating density and distribution functions using Legendre polynomials, optimized for accuracy and computational efficiency, are tested on varying smoothness examples.


<details>
  <summary>Details</summary>
Motivation: To improve the accuracy of density and distribution function approximations while minimizing computational costs.

Method: Proposes algorithms based on Legendre polynomials and solves a conditional optimization problem.

Result: Tested on examples with different degrees of smoothness, demonstrating effectiveness.

Conclusion: The algorithms offer a balance between accuracy and computational efficiency for density and distribution function estimation.

Abstract: Algorithms for jointly obtaining projection estimates of the density and
distribution function of a random variable using the Legendre polynomials are
proposed. For these algorithms, a problem of the conditional optimization is
solved. Such an optimization allows one increasing the approximation accuracy
with a minimum computational costs. The proposed algorithms are tested on
examples with different degree of smoothness of the density.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [55] [High-Throughput Computation of Anharmonic Low-Frequency Protein Vibrations](https://arxiv.org/abs/2506.15109)
*Michael A. Sauer,Souvik Mondal,Madeline Cano,Matthias Heyden*

Main category: cond-mat.stat-mech

TL;DR: FRESEAN mode analysis isolates low-frequency vibrations from molecular dynamics simulations without harmonic approximations, aiding in studying protein conformational transitions. Coarse-graining reduces computational costs for large biomolecules.


<details>
  <summary>Details</summary>
Motivation: To study intrinsically anharmonic low-frequency vibrations in proteins, which provide insights into slow conformational transitions, but face computational challenges with large biomolecules.

Method: FRESEAN mode analysis, based on velocity time correlations, is combined with coarse-graining of all-atom simulation trajectories to reduce computational costs.

Result: Low-frequency vibrations from FRESEAN are effective collective variables for enhanced sampling, and coarse-graining enables their extraction at minimal computational cost.

Conclusion: Coarse-graining combined with FRESEAN mode analysis efficiently extracts low-frequency vibrational information, facilitating future applications for large biomolecules.

Abstract: At room temperature, low frequency vibrations at far-infrared frequencies are
thermally excited ($k_B T > h \nu$) and not restricted to harmonic fluctuations
around a single potential energy minimum. For folded proteins, these
intrinsically anharmonic vibrations can contain information on slow
conformational transitions. Recently, we have developed FREquency-SElective
ANharmonic (FRESEAN) mode analysis, a method based on time correlation
functions that isolates low-frequency vibrational motions from molecular
dynamics simulation trajectories without relying on harmonic approximations. We
recently showed that low-frequency vibrations obtained from FRESEAN mode
analysis are effective collective variables in enhanced sampling simulations of
conformational ensembles. However, FRESEAN mode analysis is based on velocity
time correlations between all degrees of freedom, which creates computational
challenges for large biomolecules. To facilitate future applications, we
demonstrate here how coarse-graining of all-atom simulation trajectories can be
combined with FRESEAN mode analysis to extract information on low-frequency
vibrations at minimal computational cost.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [56] [Learning to Maximize Quantum Neural Network Expressivity via Effective Rank](https://arxiv.org/abs/2506.15375)
*Juan Yao*

Main category: quant-ph

TL;DR: The paper introduces the effective rank (κ) as a measure of QNN expressivity, demonstrates its theoretical bounds, and uses reinforcement learning to optimize quantum circuit designs.


<details>
  <summary>Details</summary>
Motivation: Accurately characterizing QNN expressivity is challenging but crucial for optimal quantum circuit design.

Method: Proposes κ to measure expressivity, analyzes circuit architecture, data, and measurement, and uses reinforcement learning for automated circuit design.

Result: κ can reach its theoretical upper bound (4ⁿ−1) under optimal conditions, and reinforcement learning effectively designs expressive circuits.

Conclusion: κ is a robust metric for QNN expressivity, and reinforcement learning aids in designing high-performance quantum circuits, advancing quantum machine learning.

Abstract: Quantum neural networks (QNNs) are widely employed as ans\"atze for solving
variational problems, where their expressivity directly impacts performance.
Yet, accurately characterizing QNN expressivity remains an open challenge,
impeding the optimal design of quantum circuits. In this work, we introduce the
effective rank, denoted as $\kappa$, as a novel quantitative measure of
expressivity. Specifically, $\kappa$ captures the number of effectively
independent parameters among all the variational parameters in a parameterized
quantum circuit, thus reflecting the true degrees of freedom contributing to
expressivity. Through a systematic analysis considering circuit architecture,
input data distributions, and measurement protocols, we demonstrate that
$\kappa$ can saturate its theoretical upper bound, $d_n=4^n-1$, for an
$n$-qubit system when each of the three factors is optimally expressive. This
result provides a rigorous framework for assessing QNN expressivity and
quantifying their functional capacity. Building on these theoretical insights,
and motivated by the vast and highly structured nature of the circuit design
space, we employ $\kappa$ as a guiding metric for the automated design of
highly expressive quantum circuit configurations. To this end, we develop a
reinforcement learning framework featuring a self-attention transformer agent
that autonomously explores and optimizes circuit architectures. By integrating
theoretical characterization with practical optimization, our work establishes
$\kappa$ as a robust tool for quantifying QNN expressivity and demonstrates the
effectiveness of reinforcement learning in designing high-performance quantum
circuits. This study paves the way for building more expressive QNN
architectures, ultimately enhancing the capabilities of quantum machine
learning.

</details>


### [57] [Randomised composite linear-combination-of-unitaries: its role in quantum simulation and observable estimation](https://arxiv.org/abs/2506.15658)
*Jinzhao Sun,Pei Zeng*

Main category: quant-ph

TL;DR: Randomised LCU in quantum algorithms reduces gate and qubit requirements but faces challenges with unphysical state preparation. A quantum instrument is introduced to address this, enabling unbiased estimation and broader applications like shadow tomography.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of randomised LCU, particularly the inability to deterministically prepare unphysical states, and to extend its utility beyond single Pauli operator estimation.

Method: Analysis of randomised LCU, introduction of a quantum instrument for non-completely-positive maps, and construction of unbiased estimators for unphysical states and their generalisations.

Result: Demonstrated efficient realisation of states from composite LCU forms and a connection to shadow tomography, enabling simultaneous observable estimation.

Conclusion: The work expands the applicability of randomised LCU, linking it to shadow tomography and providing practical solutions for Hamiltonian simulation and eigenstate preparation.

Abstract: Randomisation is widely used in quantum algorithms to reduce the number of
quantum gates and ancillary qubits required. A range of randomised algorithms,
including eigenstate property estimation by spectral filters, Hamiltonian
simulation, and perturbative quantum simulation, though motivated and designed
for different applications, share common features in the use of unitary
decomposition and Hadamard-test-based implementation. In this work, we start by
analysing the role of randomised linear-combination-of-unitaries (LCU) in
quantum simulations, and present several quantum circuits that realise the
randomised composite LCU. A caveat of randomisation, however, is that the
resulting state cannot be deterministically prepared, which often takes an
unphysical form $U \rho V^\dagger$ with unitaries $U$ and $V$. Therefore,
randomised LCU algorithms are typically restricted to only estimating the
expectation value of a single Pauli operator. To address this, we introduce a
quantum instrument that can realise a non-completely-positive map, whose
feature of frequent measurement and reset on the ancilla makes it particularly
suitable in the fault-tolerant regime. We then show how to construct an
unbiased estimator of the effective (unphysical) state $U \rho V^\dagger$ and
its generalisation. Moreover, we demonstrate how to effectively realise the
state prepared by applying an operator that admits a composite LCU form. Our
results reveal a natural connection between randomised LCU algorithms and
shadow tomography, thereby allowing simultaneous estimation of many observables
efficiently. As a concrete example, we construct the estimators and present the
simulation complexity for three use cases of randomised LCU in Hamiltonian
simulation and eigenstate preparation tasks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [58] [Knowledge Distillation Framework for Accelerating High-Accuracy Neural Network-Based Molecular Dynamics Simulations](https://arxiv.org/abs/2506.15337)
*Naoki Matsumura,Yuta Yoshimoto,Yuto Iwasaki,Meguru Yamazaki,Yasufumi Sakai*

Main category: cs.LG

TL;DR: A novel knowledge distillation framework for neural network potentials (NNPs) improves MD simulations by using a non-fine-tuned teacher model to generate training data, reducing DFT calculations by 10x without losing accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional KD methods for NNPs increase energy barriers, limiting high-energy structure training data, which is crucial for stable MD simulations.

Method: Proposes a two-stage training: first, train a student NNP with data from an off-the-shelf teacher; then fine-tune with a small DFT dataset.

Result: Achieves comparable or better accuracy for organic and inorganic materials while reducing DFT calculations by 10x.

Conclusion: The framework efficiently generates accurate NNPs with fewer DFT calculations, enhancing MD simulation stability.

Abstract: Neural network potentials (NNPs) offer a powerful alternative to traditional
force fields for molecular dynamics (MD) simulations. Accurate and stable MD
simulations, crucial for evaluating material properties, require training data
encompassing both low-energy stable structures and high-energy structures.
Conventional knowledge distillation (KD) methods fine-tune a pre-trained NNP as
a teacher model to generate training data for a student model. However, in
material-specific models, this fine-tuning process increases energy barriers,
making it difficult to create training data containing high-energy structures.
To address this, we propose a novel KD framework that leverages a
non-fine-tuned, off-the-shelf pre-trained NNP as a teacher. Its gentler energy
landscape facilitates the exploration of a wider range of structures, including
the high-energy structures crucial for stable MD simulations. Our framework
employs a two-stage training process: first, the student NNP is trained with a
dataset generated by the off-the-shelf teacher; then, it is fine-tuned with a
smaller, high-accuracy density functional theory (DFT) dataset. We demonstrate
the effectiveness of our framework by applying it to both organic (polyethylene
glycol) and inorganic (L$_{10}$GeP$_{2}$S$_{12}$) materials, achieving
comparable or superior accuracy in reproducing physical properties compared to
existing methods. Importantly, our method reduces the number of expensive DFT
calculations by 10x compared to existing NNP generation methods, without
sacrificing accuracy.

</details>


### [59] [HiPreNets: High-Precision Neural Networks through Progressive Training](https://arxiv.org/abs/2506.15064)
*Ethan Mulle,Wei Kang,Qi Gong*

Main category: cs.LG

TL;DR: A progressive framework (HiPreNets) improves neural network accuracy by learning residuals sequentially, addressing non-convex optimization and L∞ error challenges.


<details>
  <summary>Details</summary>
Motivation: Training high-accuracy neural networks is difficult due to non-convex optimization and hyperparameter tuning, with traditional methods often neglecting L∞ error.

Method: HiPreNets refines staged training by sequentially learning residuals, guiding loss function choice, parameter count, and adaptive data sampling.

Result: Validated on benchmark problems, the framework enhances overall accuracy.

Conclusion: HiPreNets effectively addresses training challenges and improves neural network precision.

Abstract: Deep neural networks are powerful tools for solving nonlinear problems in
science and engineering, but training highly accurate models becomes
challenging as problem complexity increases. Non-convex optimization and
numerous hyperparameters to tune make performance improvement difficult, and
traditional approaches often prioritize minimizing mean squared error (MSE)
while overlooking $L^{\infty}$ error, which is the critical focus in many
applications. To address these challenges, we present a progressive framework
for training and tuning high-precision neural networks (HiPreNets). Our
approach refines a previously explored staged training technique for neural
networks that improves an existing fully connected neural network by
sequentially learning its prediction residuals using additional networks,
leading to improved overall accuracy. We discuss how to take advantage of the
structure of the residuals to guide the choice of loss function, number of
parameters to use, and ways to introduce adaptive data sampling techniques. We
validate our framework's effectiveness through several benchmark problems.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [60] [Simulation of parametrized cardiac electrophysiology in three dimensions using physics-informed neural networks](https://arxiv.org/abs/2506.15405)
*Roshan Antony Gomez,Julien Stöcker,Barış Cansız,Michael Kaliske*

Main category: cs.CE

TL;DR: PINNs are used to predict cardiac electrophysiology in 3D using the Aliev-Panfilov model, with optimized hyperparameters and loss control.


<details>
  <summary>Details</summary>
Motivation: To improve the prediction of myocardial electrophysiological activity in 3D by optimizing PINN training behavior and hyperparameters.

Method: Employed an FCNN with strong-form PDEs, consistent scaling, and FE-generated training data. Studied loss weighting effects.

Result: Trained models predicted action potential and recovery variable fields, compared with FE simulations.

Conclusion: Optimal hyperparameters and loss control methods were identified for accurate 3D electrophysiology predictions.

Abstract: Physics-informed neural networks (PINNs) are extensively used to represent
various physical systems across multiple scientific domains. The same can be
said for cardiac electrophysiology, wherein fully-connected neural networks
(FCNNs) have been employed to predict the evolution of an action potential in a
2D space following the two-parameter phenomenological Aliev-Panfilov (AP)
model. In this paper, the training behaviour of PINNs is investigated to
determine optimal hyperparameters to predict the electrophysiological activity
of the myocardium in 3D according to the AP model, with the inclusion of
boundary and material parameters. An FCNN architecture is employed with the
governing partial differential equations in their strong form, which are scaled
consistently with normalization of network inputs. The finite element (FE)
method is used to generate training data for the network. Numerical examples
with varying spatial dimensions and parameterizations are generated using the
trained models. The network predicted fields for both the action potential and
the recovery variable are compared with the respective FE simulations. Network
losses are weighed with individual scalar values. Their effect on training and
prediction is studied to arrive at a method of controlling losses during
training.

</details>
