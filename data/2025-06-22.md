<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 22]
- [math.AP](#math.AP) [Total: 15]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 7]
- [math.DG](#math.DG) [Total: 2]
- [quant-ph](#quant-ph) [Total: 2]
- [math.ST](#math.ST) [Total: 1]
- [astro-ph.SR](#astro-ph.SR) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [stat.CO](#stat.CO) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 3]
- [cs.CE](#cs.CE) [Total: 1]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Faster Computation of Entropic Optimal Transport via Stable Low Frequency Modes](https://arxiv.org/abs/2506.14780)
*Reda Chhaibi,Serge Gratton,Samuel Vaiter*

Main category: math.NA

TL;DR: An accelerated Sinkhorn algorithm is proposed to improve convergence speed in Entropic Optimal Transport by using a spectral warm-start strategy.


<details>
  <summary>Details</summary>
Motivation: The Sinkhorn algorithm's slow convergence as regularization weakens (ε → 0) is a major drawback.

Method: A spectral warm-start strategy is introduced, leveraging insights from the Hessian's behavior.

Result: The proposed method achieves faster convergence compared to the reference Sinkhorn algorithm.

Conclusion: The spectral warm-start strategy effectively mitigates slow convergence, enhancing the Sinkhorn algorithm's performance.

Abstract: In this paper, we propose an accelerated version for the Sinkhorn algorithm,
which is the reference method for computing the solution to Entropic Optimal
Transport.
  Its main draw-back is the exponential slow-down of convergence as the
regularization weakens $\varepsilon \rightarrow 0$.
  Thanks to spectral insights on the behavior of the Hessian, we propose to
mitigate the problem via an original spectral warm-start strategy. This leads
to faster convergence compared to the reference method, as also demonstrated in
our numerical experiments.

</details>


### [2] [Moment-enhanced shallow water equations for non-slip boundary conditions](https://arxiv.org/abs/2506.14785)
*Shiping Zhou,Juntao Huang,Andrew J. Christlieb*

Main category: math.NA

TL;DR: Modified shallow water equations and moment-enhanced models are proposed to better capture vertical velocity profiles under non-slip and slip boundary conditions, outperforming standard models.


<details>
  <summary>Details</summary>
Motivation: Standard shallow water equations and moment-enhanced models fail to accurately represent vertical velocity profiles under non-slip boundary conditions due to stiff source terms.

Method: Proposed modified shallow water equations and moment-enhanced models with revised source term treatment, ensuring compatibility with hyperbolicity analysis.

Result: Modified models perform well under both non-slip and slip boundary conditions, validated through numerical comparison with Navier-Stokes equations.

Conclusion: The modified models offer improved accuracy and generalization for vertical velocity profiles, addressing limitations of existing approaches.

Abstract: The shallow water equations often assume a constant velocity profile along
the vertical axis. However, this assumption does not hold in many practical
applications. To better approximate the vertical velocity distribution, models
such as the shallow water moment expansion models have been proposed.
Nevertheless, under non-slip bottom boundary conditions, both the standard
shallow water equation and its moment-enhanced models struggle to accurately
capture the vertical velocity profile due to the stiff source terms. In this
work, we propose modified shallow water equations and corresponding
moment-enhanced models that perform well under both non-slip and slip boundary
conditions. The primary difference between the modified and original models
lies in the treatment of the source term, which allows our modified moment
expansion models to be readily generalized, while maintaining compatibility
with our previous analysis on the hyperbolicity of the model. To assess the
performance of both the standard and modified moment expansion models, we
conduct a comprehensive numerical comparison with the incompressible
Navier--Stokes equations -- a comparison that is absent from existing
literature.

</details>


### [3] [Energy-consistent dynamic fracture phase field models: unilateral constraints and finite element simulations](https://arxiv.org/abs/2506.14788)
*Md Mamun Miah,Ryuhei Wakida,Masato Kimura*

Main category: math.NA

TL;DR: The paper proposes a dynamic fracture phase field model (DF-PFM) for simulating crack propagation, incorporating a unilateral contact condition for fault rupture under high pressure. It validates the model numerically, showing its accuracy in shear-dominated scenarios.


<details>
  <summary>Details</summary>
Motivation: Phase field models are effective for simulating interface-driven phenomena, but a dynamic fracture model with contact conditions is needed for accurate fault rupture simulation under high pressure.

Method: The study develops DF-PFM based on the elastodynamic wave equation, extends it with a unilateral contact condition, and validates it using linear implicit time discretization and finite element methods.

Result: Numerical experiments confirm the model's accuracy in capturing shear-dominated crack propagation and preventing non-physical interpenetration under high compression.

Conclusion: The DF-PFM with unilateral contact is essential for realistic fault rupture simulation, especially in high-pressure scenarios like seismic faulting.

Abstract: Phase field models have emerged as a powerful and flexible framework for
simulating complex interface-driven phenomena across a wide range of scientific
and engineering applications. In fracture mechanics, the phase field
approach--formulated as a gradient flow of the Griffith fracture energy with
Ambrosio-Tortorelli regularization--has gained significant attention for its
ability to capture complex crack topologies. In this study, we propose a
dynamic fracture phase field model (DF-PFM) based on the elastodynamic wave
equation. We further extend this framework by incorporating a unilateral
contact condition, yielding a refined model suitable for simulating fault
rupture under high pressure. For both models, we rigorously derive energy
dissipation identities under mixed boundary conditions, ensuring energy
consistency of the formulations. To validate the proposed approach, we conduct
numerical experiments using linear implicit time discretization and finite
element methods. Our simulations demonstrate that the unilateral contact
condition is essential for accurately capturing shear-dominated crack
propagation and preventing non-physical interpenetration, especially under
high-compression loading scenarios relevant to seismic faulting.

</details>


### [4] [Fast automated adjoints for spectral PDE solvers](https://arxiv.org/abs/2506.14792)
*Calum S. Skene,Keaton J. Burns*

Main category: math.NA

TL;DR: Automated approach for computing gradients in PDE solvers using sparse spectral methods, implemented in Dedalus, enabling efficient adjoint solvers for optimization.


<details>
  <summary>Details</summary>
Motivation: To enable gradient-based optimization and sensitivity analyses in spectral simulations without requiring additional code.

Method: Reverse-mode automatic differentiation applied to symbolic graph representations of PDEs, retaining speed and memory efficiency.

Result: Efficient adjoint solvers for a wide range of systems, demonstrated through canonical problems with strong performance.

Conclusion: Integrates automatic adjoints into high-level solvers, facilitating gradient-based optimization in spectral simulations.

Abstract: We present a general and automated approach for computing model gradients for
PDE solvers built on sparse spectral methods, and implement this capability in
the widely used open-source Dedalus framework. We apply reverse-mode automatic
differentiation to symbolic graph representations of PDEs, efficiently
constructing adjoint solvers that retain the speed and memory efficiency of
this important class of modern numerical methods. This approach enables users
to compute gradients and perform optimization for a wide range of
time-dependent and nonlinear systems without writing additional code. The
framework supports a broad class of equations, geometries, and boundary
conditions, and runs efficiently in parallel using MPI. We demonstrate the
flexibility and capabilities of this system using canonical problems from the
literature, showing both strong performance and practical utility for a wide
variety of inverse problems. By integrating automatic adjoints into a flexible
high-level solver, our approach enables researchers to perform gradient-based
optimization and sensitivity analyses in spectral simulations with ease and
efficiency.

</details>


### [5] [A micromorphic-based artificial diffusion method for stabilized finite element approximation of convection-diffusion problems](https://arxiv.org/abs/2506.14800)
*Soheil Firooz,B. Daya Reddy,Paul Steinmann*

Main category: math.NA

TL;DR: A novel artificial diffusion method is introduced to address instabilities in finite element approximations of convection-diffusion equations, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To circumvent instabilities in standard finite element approximations of convection-diffusion equations.

Method: Introduces an auxiliary variable linked to the gradient of the field, creating a coupled problem, with conditions for well-posedness established.

Result: The method outperforms established approaches in accurately approximating solutions in one- and two-dimensional settings.

Conclusion: The proposed artificial diffusion method is effective for challenging convection-diffusion problems.

Abstract: We present a novel artificial diffusion method to circumvent the
instabilities associated with the standard finite element approximation of
convection-diffusion equations. Motivated by the micromorphic approach, we
introduce an auxiliary variable, which is related to the gradient of the field
of interest, and which leads to a coupled problem. Conditions for
well-posedness of the resulting formulation are established. We carry out a
comprehensive numerical study to compare the proposed methodology against some
well-established approaches in one- and two-dimensional settings. The proposed
method outperforms established approaches in general in approximating
accurately the solutions to pertinent and challenging problems.

</details>


### [6] [An explicit computational approach for a three-dimensional system of nonlinear elastodynamic sine-Gordon problem](https://arxiv.org/abs/2506.14807)
*Eric Ngondiep*

Main category: math.NA

TL;DR: A high-order explicit computational method for solving 3D nonlinear elastodynamic sine-Gordon equations, combining interpolation for time derivatives and finite elements for space derivatives, with proven stability and convergence.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient numerical scheme for solving complex 3D nonlinear elastodynamic sine-Gordon equations with stability and accuracy guarantees.

Method: Uses interpolation for time derivatives and finite elements for space derivatives, ensuring high-order accuracy and stability under a specific time step restriction.

Result: The method is temporally second-order and spatially third-order accurate, validated by numerical examples.

Conclusion: The proposed technique is efficient, stable, and practically applicable for solving the target equations.

Abstract: This paper proposes an explicit computational method for solving a
three-dimensional system of nonlinear elastodynamic sine-Gordon equations
subject to appropriate initial and boundary conditions. The time derivative is
approximated by interpolation technique whereas the finite element approach is
used to approximate the space derivatives. The developed numerical scheme is
so-called, high-order explicit computational technique. The new algorithm
efficiently treats the time derivative term and provides a suitable time step
restriction for stability and convergence. Under this time step limitation,
both stability and error estimates of the proposed approach are deeply analyzed
using a constructed strong norm. The theoretical studies indicate that the
developed approach is temporal second-order convergent and spatially
third-order accurate. Some numerical examples are carried out to confirm the
theory, to validate the computational efficiency and to demonstrate the
practical applicability of the new computational technique.

</details>


### [7] [Weak TransNet: A Petrov-Galerkin based neural network method for solving elliptic PDEs](https://arxiv.org/abs/2506.14812)
*Zhihang Xu,Min Wang,Zhu Wang*

Main category: math.NA

TL;DR: The paper proposes Weak TransNet (WTN), a method for solving elliptic PDEs with low regularity or singularities, using a Petrov-Galerkin framework and neural networks.


<details>
  <summary>Details</summary>
Motivation: Deep learning struggles with PDEs involving low regularity or singularities, prompting the need for a robust solution.

Method: WTN combines TransNet's neural feature space (trial space) with radial basis functions (test space), solving via least squares minimization of weak PDE residuals.

Result: WTN addresses non-convexity and ill-conditioning in neural network training and handles multiscale or sharp-variation solutions effectively.

Conclusion: Numerical experiments confirm WTN's robustness and efficiency for challenging PDE problems.

Abstract: While deep learning has achieved remarkable success in solving partial
differential equations (PDEs), it still faces significant challenges,
particularly when the PDE solutions have low regularity or singularities. To
address these issues, we propose the Weak TransNet (WTN) method, based on a
Petrov-Galerkin formulation, for solving elliptic PDEs in this work, though its
framework may extend to other classes of equations. Specifically, the neural
feature space defined by TransNet (Zhang et al., 2023) is used as the trial
space, while the test space is composed of radial basis functions. Since the
solution is expressed as a linear combination of trial functions, the
coefficients can be determined by minimizing the weak PDE residual via least
squares. Thus, this approach could help mitigate the challenges of
non-convexity and ill-conditioning that often arise in neural network training.
Furthermore, the WTN method is extended to handle problems whose solutions
exhibit multiscale features or possess sharp variations. Several numerical
experiments are presented to demonstrate the robustness and efficiency of the
proposed methods.

</details>


### [8] [Optimal alignment of Lorentz orientation and generalization to matrix Lie groups](https://arxiv.org/abs/2506.14994)
*Congzhou M Sha*

Main category: math.NA

TL;DR: The paper proposes two methods for aligning 4-vectors in Minkowski space using Lorentz transformations, addressing limitations of Euclidean-based methods.


<details>
  <summary>Details</summary>
Motivation: Existing point cloud alignment methods rely on Euclidean metrics and fail for indefinite Minkowski metrics, necessitating new solutions.

Method: The paper outlines a conceptually simple method to find the optimal Lorentz transformation for aligning 4-vectors in inertial reference frames, extendable to other matrix Lie groups.

Result: The proposed solutions effectively align 4-vectors in Minkowski space, overcoming the limitations of Euclidean-based methods.

Conclusion: The method is versatile and applicable to alignment problems in various matrix Lie groups, offering a robust solution for Minkowski space.

Abstract: There exist elegant methods of aligning point clouds in $\mathbb R^3$.
Unfortunately, these methods rely on the positive definite property of the
Euclidean metric, and do not easily extend to the indefinite Minkowski metric.
In this paper, we propose two solutions to the following problem: given
inertial reference frames $A$ and $B$, and given (possibly noisy) measurements
of a set of 4-vectors $\{v_i\}$ made in those reference frames with components
$\{v_{A,i}\}$ and $\{v_{B,i}\}$, find the optimal Lorentz transformation
$\Lambda$ such that $\Lambda v_{A,i}=v_{B,i}$. The method we outline is
conceptually simple and easily extends to alignment problems in other matrix
Lie groups.

</details>


### [9] [Semi-orthogonal Tribonacci Wavelets and Numerical Solutions of Nonlinear Singular BVPs Arising in a Chemical Reaction](https://arxiv.org/abs/2506.14814)
*Ankita Yadav,Amit K. Verma*

Main category: math.NA

TL;DR: A semi-orthogonal Tribonacci wavelet collocation method is introduced for solving nonlinear singular BVPs.


<details>
  <summary>Details</summary>
Motivation: To provide an effective numerical method for solving nonlinear singular boundary value problems (BVPs).

Method: Development of a semi-orthogonal Tribonacci wavelet collocation method.

Result: The method offers a numerical solution for nonlinear singular BVPs.

Conclusion: The semi-orthogonal Tribonacci wavelet collocation method is effective for such problems.

Abstract: In this article, we introduce a semi-orthogonal tribonacci wavelet and
develop a semi-orthogonal tribonacci wavelet collocation method, offering an
effective numerical method for solving a class of non-linear singular BVPs.

</details>


### [10] [Interpolation-based reproducing kernel particle method](https://arxiv.org/abs/2506.14916)
*Jennifer E. Fromm,John A. Evans,J. S. Chen*

Main category: math.NA

TL;DR: Interpolation-based RKPM enables easy implementation in FEM software, achieving accuracy similar to classic RKPM with lower computational cost.


<details>
  <summary>Details</summary>
Motivation: RKPM's integration challenges in FEM software due to Gauss-quadrature limitations.

Method: Interpolation-based RKPM uses Lagrange polynomials on a foreground mesh for integration.

Result: Equivalent error convergence to classic RKPM, solves higher-order PDEs, and handles multi-material problems.

Conclusion: Interpolation-based RKPM is efficient, accurate, and compatible with existing FEM software.

Abstract: Meshfree methods, including the reproducing kernel particle method (RKPM),
have been widely used within the computational mechanics community to model
physical phenomena in materials undergoing large deformations or extreme
topology changes. RKPM shape functions and their derivatives cannot be
accurately integrated with the Gauss-quadrature methods widely employed for the
finite element method (FEM) and typically require sophisticated nodal
integration techniques, preventing them from easily being implemented in
existing FEM software. Interpolation-based methods have been developed to
address similar problems with isogeometric and immersed boundary methods,
allowing these techniques to be implemented within open-source finite element
software. With interpolation-based methods, background basis functions are
represented as linear combinations of Lagrange polynomial foreground basis
functions defined upon a boundary-conforming foreground mesh. This work extends
the applications of interpolation-based methods to implement RKPM within
open-source finite element software. Interpolation-based RKPM is applied to
several PDEs, and error convergence rates are equivalent to classic RKPM
integrated using high-order Gauss-quadrature schemes. The interpolation-based
method is able to exploit the continuity of the RKPM basis to solve
higher-order PDEs, demonstrated through the biharmonic problem. The method is
extended to multi-material problems through Heaviside enrichment schemes, using
local foreground refinement to reduce geometric integration error and achieve
high-order accuracy. The computational cost of interpolation-based RKPM is
similar to the smoothed gradient nodal integration schemes, offering
significant savings over Gauss-quadrature-based meshfree methods while enabling
easy implementation within existing finite element software.

</details>


### [11] [A Nonconforming Finite Element Method for Elliptic Interface Problems on Locally Anisotropic Meshes](https://arxiv.org/abs/2506.15077)
*Hua Wang,Qichen Zhang*

Main category: math.NA

TL;DR: A new nonconforming P1 finite element method for elliptic interface problems is proposed, using locally anisotropic mixed meshes and a novel consistency error analysis.


<details>
  <summary>Details</summary>
Motivation: To address elliptic interface problems with a method that avoids the quasi-regularity assumption and improves robustness and accuracy.

Method: Constructed on locally anisotropic mixed meshes, fitting the interface via intersection points on an unfitted background mesh. Includes interpolation error estimates and a novel consistency error analysis.

Result: Numerical results confirm theoretical convergence rates and demonstrate the method's robustness and accuracy.

Conclusion: The proposed method is effective for elliptic interface problems, offering improved analysis and performance.

Abstract: We propose a new nonconforming \(P_1\) finite element method for elliptic
interface problems. The method is constructed on a locally anisotropic mixed
mesh, which is generated by fitting the interface through a simple connection
of intersection points on an interface-unfitted background mesh, as introduced
in \cite{Hu2021optimal}. We first establish interpolation error estimates on
quadrilateral elements satisfying the regular decomposition property (RDP).
Building on this, the main contribution of this work is a novel consistency
error analysis for nonconforming elements, which removes the quasi-regularity
assumption commonly required in existing approaches. Numerical results confirm
the theoretical convergence rates and demonstrate the robustness and accuracy
of the proposed method.

</details>


### [12] [Fourth- and Higher-Order Semi-Lagrangian Finite Volume Methods for the Two-dimensional Advection Equation on Arbitrarily Complex Domains](https://arxiv.org/abs/2506.15142)
*Yunxia Sun,Kaiyi Liang,Yuke Zhu,Zhi Lin,Qinghai Zhang*

Main category: math.NA

TL;DR: A family of high-order semi-Lagrangian finite volume (SLFV) methods is proposed for solving the 2D advection equation, offering high convergence rates, flexibility, and robustness.


<details>
  <summary>Details</summary>
Motivation: To address the need for accurate and flexible numerical solutions for the 2D advection equation, especially in complex domains and with varying conditions.

Method: Proposes fourth-, sixth-, and eighth-order SLFV methods applicable to regular/irregular domains, handling zero/nonzero source terms and periodic/penetration conditions uniformly.

Result: Test results confirm high accuracy, flexibility, robustness, and excellent conditioning of the SLFV methods.

Conclusion: The proposed SLFV methods are effective for solving the 2D advection equation with high-order convergence and adaptability.

Abstract: To numerically solve the two-dimensional advection equation, we propose a
family of fourth- and higher-order semi-Lagrangian finite volume (SLFV) methods
that feature (1) fourth-, sixth-, and eighth-order convergence rates, (2)
applicability to both regular and irregular domains with arbitrarily complex
topology and geometry, (3) ease of handling both zero and nonzero source terms,
and (4) the same algorithmic steps for both periodic and incoming penetration
conditions. Test results confirm the analysis and demonstrate the accuracy,
flexibility, robustness, and excellent conditioning of the proposed SLFV
method.

</details>


### [13] [A time-frequency method for acoustic scattering with trapping](https://arxiv.org/abs/2506.15165)
*Heather Wilber,Wietse Vaes,Abinand Gopal,Gunnar Martinsson*

Main category: math.NA

TL;DR: A Fourier transform method enhances hybrid time-frequency schemes for acoustic scattering, handling oscillatory behavior and slow decay, especially in trapping regions.


<details>
  <summary>Details</summary>
Motivation: To extend hybrid time-frequency methods to domains with trapping regions, addressing highly oscillatory behavior and slow decay in time.

Method: Combines a fast sinc transform for oscillatory behavior and long time horizons with a contour integration scheme to improve integrand smoothness.

Result: The method successfully extends applicability to trapping regions and manages oscillatory and decay challenges.

Conclusion: The proposed Fourier transform method effectively broadens the scope of hybrid time-frequency schemes for acoustic scattering problems.

Abstract: A Fourier transform method is introduced for a class of hybrid time-frequency
methods that solve the acoustic scattering problem in regimes where the
solution exhibits both highly oscillatory behavior and slow decay in time. This
extends the applicability of hybrid time-frequency schemes to domains with
trapping regions. A fast sinc transform technique for managing highly
oscillatory behavior and long time horizons is combined with a contour
integration scheme that improves smoothness properties in the integrand.

</details>


### [14] [Heterogeneous and anisotropic elastic parameter estimation using a novel semi-analytical forward solver](https://arxiv.org/abs/2506.15185)
*Xiaopeng Zhu,Zhongyi Huang*

Main category: math.NA

TL;DR: A novel semi-analytical method for identifying heterogeneous and anisotropic elastic parameters from one full-field measurement, using TV-regularized energy minimization solved by Adam algorithm.


<details>
  <summary>Details</summary>
Motivation: To efficiently and accurately identify elastic parameters from limited data, addressing challenges like irregularity, anisotropy, and heterogeneity.

Method: Formulates the inverse problem as an energy functional minimization with TV regularization, solved by Adam algorithm. Introduces a semi-analytical forward solver (direct method of lines) for handling irregular regions and singularities.

Result: Numerical experiments confirm reliable performance in forward modeling and reconstruction of six elastic parameters.

Conclusion: The proposed method is effective for elastic parameter identification, offering computational efficiency and accuracy.

Abstract: An efficient procedure using a novel semi-analytical forward solver for
identifying heterogeneous and anisotropic elastic parameters from only one
full-field measurement is proposed and explored. We formulate the inverse
problem as an special energy functional minimization with total variation(TV)
regularization. The minimization problem is solved by Adam algorithm, which
only requires solving one forward problem and no adjoint problem in each
iteration. In order to deal with the irregularity of the elastic regions, the
anisotropy and heterogeneity of parameters and potential singularities in
forward-modeled issues, a novel semi-analytical forward solver named the direct
method of lines is proposed, which discretizes angular variable while
preserving analytical solutions along remaining coordinates. To validate the
efficacy of our procedure, a series of numerical experiments are implemented
subsequently, achieving reliable performance in both forward modeling and the
six elastic arguments reconstruction scenarios.

</details>


### [15] [Reduced Particle in Cell method for the Vlasov-Poisson system using auto-encoder and Hamiltonian neural](https://arxiv.org/abs/2506.15203)
*Emmanuel Franck,Laurent Navoret,Vincent Vigon,Raphaël Côte,Guillaume Steimer*

Main category: math.NA

TL;DR: A nonlinear, data-driven model order reduction method for Hamiltonian particle-based plasma simulations is introduced, combining Proper Symplectic Decomposition and autoencoder neural networks to improve computational efficiency and stability.


<details>
  <summary>Details</summary>
Motivation: The computational intensity of Hamiltonian particle-based plasma simulations, especially in many-query contexts, necessitates reduced order models to lower costs while preserving Hamiltonian structure for stability.

Method: A two-step projection framework: linear projection via Proper Symplectic Decomposition, followed by nonlinear projection using an autoencoder neural network. Reduced dynamics are modeled with a Hamiltonian neural network.

Result: The method outperforms standard linear Hamiltonian reduction methods in benchmarks like Landau damping and two-stream instability.

Conclusion: The proposed nonlinear, non-intrusive approach effectively reduces computational costs while maintaining accuracy and stability in Hamiltonian plasma simulations.

Abstract: Hamiltonian particle-based simulations of plasma dynamics are inherently
computationally intensive, primarily due to the large number of particles
required to obtain accurate solutions. This challenge becomes even more acute
in many-query contexts, where numerous simulations must be conducted across a
range of time and parameter values. Consequently, it is essential to construct
reduced order models from such discretizations to significantly lower
computational costs while ensuring validity across the specified time and
parameter domains. Preserving the Hamiltonian structure in these reduced models
is also crucial, as it helps maintain long-term stability. In this paper, we
introduce a nonlinear, non-intrusive, data-driven model order reduction method
for the 1D-1V Vlasov--Poisson system, discretized using a Hamiltonian
Particle-In-Cell scheme. Our approach relies on a two-step projection
framework: an initial linear projection based on the Proper Symplectic
Decomposition, followed by a nonlinear projection learned via an autoencoder
neural network. The reduced dynamics are then modeled using a Hamiltonian
neural network. The offline phase of the method is split into two stages:
first, constructing the linear projection using full-order model snapshots;
second, jointly training the autoencoder and the Hamiltonian neural network to
simultaneously learn the encoder-decoder mappings and the reduced dynamics. We
validate the proposed method on several benchmarks, including Landau damping
and two-stream instability. The results show that our method has better
reduction properties than standard linear Hamiltonian reduction methods.

</details>


### [16] [Splitting-based randomised dynamical low-rank approximations for stiff matrix differential equations](https://arxiv.org/abs/2506.15259)
*Zi Wu,Yong-Liang Zhao*

Main category: math.NA

TL;DR: A dynamic numerical integrator for low-rank approximations in large-scale semilinear stiff matrix differential equations, combining exponential integrators and dynamic low-rank methods, validated on canonical problems.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of solving large-scale semilinear stiff matrix differential equations efficiently with low-rank approximations.

Method: Decomposes the equation into stiff linear and nonstiff nonlinear parts, using exponential integrators and dynamic low-rank approaches, with rank-adaptation capability.

Result: Achieves predicted convergence orders, validated on Allen-Cahn and Riccati equations, showing robustness and accuracy.

Conclusion: The method is effective for low-rank approximations in stiff matrix differential equations, with theoretical and numerical validation.

Abstract: In the fields of control theory and machine learning, the dynamic low-rank
approximation for large-scale matrices has received substantial attention.
Considering the large-scale semilinear stiff matrix differential equations, we
propose a dynamic numerical integrator for obtaining low-rank approximations of
solutions. We first decompose the differential equation into a stiff linear
component and a nonstiff nonlinear term, then employ an exponential integrator
along with a dynamic low-rank approach to resolve these subsystems,
respectively. Furthermore, the proposed framework naturally extends to
rank-adaptation scenarios. Through rigorous validation on canonical stiff
matrix differential problems, including spatially discretized Allen-Cahn
equations and differential Riccati equations, we demonstrate that the method
achieves the theoretically predicted convergence orders. Numerical evidence
confirms the robustness and accuracy of the proposed methods.

</details>


### [17] [Stochastic Diagonal Estimation Based on Matrix Quadratic Form Oracles](https://arxiv.org/abs/2506.15360)
*Haishan Ye,Xiangyu Chang*

Main category: math.NA

TL;DR: A stochastic method for estimating the diagonal of an implicitly given matrix using Gaussian-distributed random queries, with theoretical and empirical validation.


<details>
  <summary>Details</summary>
Motivation: The problem involves estimating the diagonal of a matrix where only matrix quadratic form evaluations are available, requiring an efficient and accurate method.

Method: Proposes a stochastic diagonal estimation method using random Gaussian vectors to query the matrix quadratic form.

Result: Provides element-wise and norm-wise sample complexities; numerical experiments confirm method effectiveness and theoretical tightness.

Conclusion: The method is effective for diagonal estimation of implicitly given matrices, with validated theoretical guarantees.

Abstract: We study the problem of estimating the diagonal of an implicitly given matrix
$\Ab$. For such a matrix we have access to an oracle that allows us to evaluate
the matrix quadratic form $ \ub^\top \Ab \ub$. Based on this query oracle, we
propose a stochastic diagonal estimation method with random variable $\ub$
drawn from the standard Gaussian distribution. We provide the element-wise and
norm-wise sample complexities of the proposed method. Our numerical experiments
on different types and dimensions matrices demonstrate the effectiveness of our
method and validate the tightness of theoretical results.

</details>


### [18] [A deep shotgun method for solving high-dimensional parabolic partial differential equations](https://arxiv.org/abs/2506.15481)
*Wenjun Xu,Wenzhong Zhang*

Main category: math.NA

TL;DR: Proposes a deep 'shotgun method' for solving high-dimensional parabolic PDEs without full trajectory simulation, improving performance and accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing methods for solving parabolic PDEs via FBSDE formulations require simulating multiple trajectories with small time steps, limiting performance on large time intervals.

Method: Introduces a deep 'shotgun method' that uses data distribution of trajectories instead of full simulations.

Result: Demonstrates competitiveness in performance and accuracy, even in dimensions up to 10000.

Conclusion: The shotgun method offers a viable alternative to traditional approaches for high-dimensional PDEs.

Abstract: Recent advances in deep learning makes solving parabolic partial differential
equations (PDEs) in high dimensional spaces possible via forward-backward
stochastic differential equation (FBSDE) formulations. The implementation of
most existing methods requires simulating multiple trajectories of stochastic
processes with a small step size of time discretization to ensure accuracy,
hence having limited performance, especially when solving on a large time
interval. To address such issue, we propose a deep "shotgun method" that does
not exploit full trajectories, but only utilizes the data distribution of them.
Numerical results including examples with dimensionality up to 10000
demonstrate the competitiveness of the proposed shotgun method in both
performance and accuracy.

</details>


### [19] [Intrinsic and Extrinsic Organized Attention: Softmax Invariance and Network Sparsity](https://arxiv.org/abs/2506.15541)
*Oluwadamilola Fasina,Ruben V. C. Pohle,Pei-Chun Su,Ronald R. Coifman*

Main category: math.NA

TL;DR: The paper analyzes the intrinsic and extrinsic structure of self-attention in transformers, showing invariance to softmax activation and proposing hierarchical tensor organization for tasks like model pruning and interpretability.


<details>
  <summary>Details</summary>
Motivation: To understand and exploit the structural properties of self-attention mechanisms in transformers for interpretability and practical applications like model pruning.

Method: Uses paradifferential calculus for theoretical analysis and hierarchical tensor organization (partition trees) to examine network structure, supported by computational examples in vision and language transformers.

Result: Demonstrates invariance of self-attention to softmax activation, hierarchical organization of attention heads, and utility in tasks like sparsity analysis and model pruning.

Conclusion: The findings advance interpretability and enable practical applications like pruning and architecture comparison, leveraging the structured organization of network tensors.

Abstract: We examine the intrinsic (within the attention head) and extrinsic (amongst
the attention heads) structure of the self-attention mechanism in transformers.
Theoretical evidence for invariance of the self-attention mechanism to softmax
activation is obtained by appealing to paradifferential calculus, (and is
supported by computational examples), which relies on the intrinsic
organization of the attention heads. Furthermore, we use an existing
methodology for hierarchical organization of tensors to examine network
structure by constructing hierarchal partition trees with respect to the query,
key, and head axes of network 3-tensors. Such an organization is consequential
since it allows one to profitably execute common signal processing tasks on a
geometry where the organized network 3-tensors exhibit regularity. We exemplify
this qualitatively, by visualizing the hierarchical organization of the tree
comprised of attention heads and the diffusion map embeddings, and
quantitatively by investigating network sparsity with the expansion
coefficients of individual attention heads and the entire network with respect
to the bi and tri-haar bases (respectively) on the space of queries, keys, and
heads of the network. To showcase the utility of our theoretical and
methodological findings, we provide computational examples using vision and
language transformers. The ramifications of these findings are two-fold: (1) a
subsequent step in interpretability analysis is theoretically admitted, and can
be exploited empirically for downstream interpretability tasks (2) one can use
the network 3-tensor organization for empirical network applications such as
model pruning (by virtue of network sparsity) and network architecture
comparison.

</details>


### [20] [Pathwise convergence of a novel numerical scheme based on semi-implicit method for stochastic differential-algebraic equations with non-global Lipschitz coefficients](https://arxiv.org/abs/2506.15627)
*Guy Tsafack,Antoine Tambue*

Main category: math.NA

TL;DR: The paper proposes a semi-implicit numerical scheme for solving non-autonomous SDAEs with nonlinear local Lipschitz coefficients, addressing singularity challenges and proving pathwise convergence.


<details>
  <summary>Details</summary>
Motivation: The challenge lies in the singularity of the matrix in SDAEs, making numerical integration difficult. The goal is to develop an efficient method for high-dimensional SDAEs derived from SPDAEs.

Method: A semi-implicit scheme splits the drift term into linear (implicitly approximated) and nonlinear (explicitly approximated) parts to handle singularity without solving nonlinear algebraic equations.

Result: The scheme achieves pathwise convergence with a rate of 1/2−ϵ. Numerical simulations confirm its efficiency and alignment with theoretical results.

Conclusion: The proposed scheme is effective for high-dimensional SDAEs, balancing computational efficiency and accuracy.

Abstract: This paper delves into the well-posedness and the numerical approximation of
non-autonomous stochastic differential algebraic equations (SDAEs) with
nonlinear local Lipschitz coefficients that satisfy the more general
monotonicity condition called Khasminskii condition. The key challenge is the
presence of a singular matrix which makes the numerical integration hard and
heavy. To address this challenge, we propose a novel numerical scheme based on
semi-implicit method for the drift component of the SDAEs. More precisely we
split the drift term as the sum of a linear term and a nonlinear term. The
linear part is approximated implicitly, while the nonlinear part is
approximated explicitly. The linear component's role is to handle the
singularity issues during the numerical integration without the resolution of
nonlinear algebraic equations in the constraint equations. This novel scheme is
therefore very efficient for SDAEs in high dimension that come after the
spatial discretisation of stochastic partial differential algebraic equations
(SPDAEs). To prove the pathwise convergence of our novel scheme, we first
derive a equivalent scheme called dual scheme, suitable for mathematical
analysis and linked to the inherent stochastic differential equation resulting
from the elimination of constraints in the initial SDAEs. We prove that our
novel scheme converges to the exact solution with rate $\frac{1}{2}-\epsilon$,
for arbitrary $\epsilon>0$ in the pathwise sense. Numerical simulations are
performed to demonstrate the efficiency of the scheme in high dimension and to
show that our theoretical results are in agreement with numerical experiments.

</details>


### [21] [Non-uniform finite-element meshes defined by ray dynamics for Helmholtz problems](https://arxiv.org/abs/2506.15630)
*Martin Averseng,Jeffrey Galkowski,Euan A. Spence*

Main category: math.NA

TL;DR: The paper shows that non-uniform meshes can achieve quasioptimality (QO) and bounded relative error (BRE) for the high-frequency Helmholtz equation, even violating classic conditions, with coarser meshes away from trapping and in the PML.


<details>
  <summary>Details</summary>
Motivation: To explore how non-uniform meshes can improve FEM performance for the Helmholtz equation by leveraging the behavior of billiard trajectories and reducing mesh requirements in certain regions.

Method: Uses duality arguments and analyzes the Helmholtz data-to-solution map, focusing on how mesh structure and solution behavior influence local errors.

Result: Demonstrates that QO and BRE can be achieved with coarser meshes in non-critical regions (e.g., PML), where only $hk$ needs to be small, eliminating pollution in the PML.

Conclusion: Non-uniform meshes tailored to billiard ray properties can optimize FEM performance for the Helmholtz equation, reducing computational costs while maintaining accuracy.

Abstract: The $h$-version of the finite-element method ($h$-FEM) applied to the
high-frequency Helmholtz equation has been a classic topic in numerical
analysis since the 1990s. It is now rigorously understood that (using piecewise
polynomials of degree $p$ on a mesh of a maximal width $h$) the conditions
"$(hk)^p \rho$ sufficiently small" and "$(hk)^{2p} \rho$ sufficiently small"
guarantee, respectively, $k$-uniform quasioptimality (QO) and bounded relative
error (BRE), where $\rho$ is the norm of the solution operator with $\rho\sim
k$ for non-trapping problems. Empirically, these conditions are observed to be
optimal in the context of $h$-FEM with a uniform mesh. This paper demonstrates
that QO and BRE can be achieved using certain non-uniform meshes that violate
the conditions above on $h$ and involve coarser meshes away from trapping and
in the perfectly matched layer (PML). The main theorem details how varying the
meshwidth in one region affects errors both in that region and elsewhere. One
notable consequence is that, for any scattering problem (trapping or
nontrapping), in the PML one only needs $hk$ to be sufficiently small; i.e.
there is no pollution in the PML.
  The motivating idea for the analysis is that the Helmholtz data-to-solution
map behaves differently depending on the locations of both the measurement and
data, in particular, on the properties of billiards trajectories (i.e. rays)
through these sets. Because of this, it is natural that the approximation
requirements for finite-element spaces in a subset should depend on the
properties of billiard rays through that set. Inserting this behaviour into the
latest duality arguments for the FEM applied to the high-frequency Helmholtz
equation allows us to retain detailed information about the influence of
$\textit{both}$ the mesh structure $\textit{and}$ the behaviour of the true
solution on local errors in FEM.

</details>


### [22] [On the Upper Bounds for the Matrix Spectral Norm](https://arxiv.org/abs/2506.15660)
*Alexey Naumov,Maxim Rakhuba,Denis Ryapolov,Sergey Samsonov*

Main category: math.NA

TL;DR: A new Counterbalance estimator for spectral norm estimation via matrix-vector products, offering tighter bounds than the power method, especially for matrices with fast-decaying spectra.


<details>
  <summary>Details</summary>
Motivation: To improve spectral norm estimation accuracy, particularly for matrices with fast-decaying spectra, common in deep learning and inverse problems.

Method: Proposes the Counterbalance estimator, which uses matrix-vector products to derive upper bounds with probabilistic guarantees.

Result: Produces significantly tighter upper bounds than standard methods like the power method, validated in synthetic and real-world settings.

Conclusion: The Counterbalance estimator is effective for spectral norm estimation, especially for matrices with rapidly decaying spectra.

Abstract: We consider the problem of estimating the spectral norm of a matrix using
only matrix-vector products. We propose a new Counterbalance estimator that
provides upper bounds on the norm and derive probabilistic guarantees on its
underestimation. Compared to standard approaches such as the power method, the
proposed estimator produces significantly tighter upper bounds in both
synthetic and real-world settings. Our method is especially effective for
matrices with fast-decaying spectra, such as those arising in deep learning and
inverse problems.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [23] [On the Control of Solutions of a Viscoelastic Plate Problem with a Frictional Damping Term](https://arxiv.org/abs/2506.14908)
*Bilel Madjour,Amel Boudiaf*

Main category: math.AP

TL;DR: The paper analyzes the stability of solutions to a nonlinear viscoelastic plate problem with frictional damping and a logarithmic source, extending prior work.


<details>
  <summary>Details</summary>
Motivation: To study the stability of solutions in a viscoelastic plate problem with boundary damping and a logarithmic source, improving earlier results.

Method: The relaxation function r satisfies a specific inequality involving a nonincreasing positive function h.

Result: The work extends previous studies and enhances stability results for such problems.

Conclusion: The findings contribute to better understanding and improved stability conditions for viscoelastic plate problems.

Abstract: In this article, we study the stability of solutions to a nonlinear
viscoelastic plate problem with frictional damping of a memory on a part of the
boundary, and a logarithmic source in a bounded domain $\Omega \subset
\mathbb{R}^2.$ In this problem the relaxation function $r$ satisfies
$r^{\prime}\left( t\right)\leq -h\left( t\right) \mathcal{G}\left( r\left(
t\right) \right)$ for all $t\geq 0$, where $h$ is a nonincreasing positive
function. This work extends previous works with viscoelastic plate problems and
improves earlier results in the

</details>


### [24] [Optimal regularity results in Sobolev-Lorentz spaces for lilnear elliptic equations with $L^1$- or measure data](https://arxiv.org/abs/2506.15005)
*Hyunseok Kim,Young-Ran Lee,Jihoon Ok*

Main category: math.AP

TL;DR: The paper establishes optimal regularity properties for solutions of Poisson and more general elliptic equations in Sobolev-Lorentz and Besov spaces, extending known results to broader function spaces and boundary conditions.


<details>
  <summary>Details</summary>
Motivation: To generalize and refine existing regularity results for solutions of elliptic equations, particularly in Sobolev-Lorentz and Besov spaces, and to handle nonhomogeneous boundary data.

Method: The authors analyze weak and very weak solutions of Poisson and general elliptic equations, using Sobolev-Lorentz spaces and embedding results to derive optimal regularity properties.

Result: Solutions are shown to belong to Sobolev-Lorentz and Besov spaces with optimal regularity, even for nonhomogeneous boundary data.

Conclusion: The paper extends and sharpens known regularity results, providing a comprehensive framework for analyzing solutions of elliptic equations in various function spaces.

Abstract: It has been well known that if $\Omega$ is a bounded $C^1$-domain in $\R^n,\
n \ge 2$, then for every Radon measure $f$ on $\Omega$ with finite total
variation, there exists a unique weak solution $u\in W_0^{1,1}(\Omega )$ of the
Poisson equation $-\Delta u=f$ in $\Omega$ satisfying $\nabla u \in
L^{n/(n-1),\infty}(\Omega;\R^n )$. In this paper, optimal regularity properties
of the solution $u$ are established in Sobolev-Lorentz spaces
$L_{\alpha}^{p,q}(\Omega )$ of order $ \alpha$ less than but arbitrarily close
to $2$. More precisely, for any $0 \le \alpha<1$, we show that $u\in
L_{\alpha+1}^{p(\alpha),\infty}(\Omega )$, where $p(\alpha )= n/(n-1+\alpha )$.
Moreover, using an embedding result for Sobolev-Lorentz spaces
$L_{\alpha}^{p,q}(\Omega )$ into classical Besov spaces $B_\alpha^{p,q}(\Omega
)$, we deduce that $u\in B_{\alpha+1}^{p(\alpha),\infty}(\Omega )$. Indeed,
these regularity results are proved for solutions of the Dirichlet problems for
more general linear elliptic equations with nonhomogeneous boundary data.
  On the other hand, it is known that if $\Omega$ is of class $C^{1,1}$, then
for each $G\in L^1 (\Omega ;\R^n )$ there exists a unique very weak solution
$v\in L^{n/(n-1),\infty} (\Omega )$ of $-\Delta v= {\rm div}\, G$ in $\Omega$
satisfying the boundary condition $v=0$ in some sense. We prove that $v$ has
the optimal regularity property, that is, $v\in
L_{\alpha}^{p(\alpha),\infty}(\Omega )\cap B_{\alpha}^{p(\alpha),\infty}(\Omega
)$ for every $0 \le \alpha < 1$. This regularity result is also proved for more
general equations with nonhomogeneous boundary data.

</details>


### [25] [Dynamic Optimal Transport with optimal star shaped graphs](https://arxiv.org/abs/2506.15007)
*Marcello Carioni,Juliane Krautz,Jan-F. Pietschmann*

Main category: math.AP

TL;DR: Existence of solutions for optimal transport coupled with dynamic transport on metric graphs, with focus on star-shaped graphs and topology preservation.


<details>
  <summary>Details</summary>
Motivation: To address optimal transport problems in a compact convex set coupled with dynamic transport on embedded metric graphs, ensuring solutions exist and graph topology is preserved.

Method: Prove existence for fixed graphs; augment action functional with a penalty for varying star-shaped graphs to prevent edge overlap and preserve topology.

Result: Existence of minimizers is shown for both fixed and varying star-shaped graphs.

Conclusion: The approach successfully ensures solution existence and topology preservation in the studied optimal transport problem.

Abstract: We study an optimal transport problem in a compact convex set
$\Omega\subset\mathbb{R}^d$ where bulk transport is coupled to dynamic optimal
transport on a metric graph $ \mathsf{G} = (\mathsf{V},\mathsf{E})$ which is
embedded in $\Omega$. We prove existence of solutions for fixed graphs. Next,
we consider varying graphs, yet only for the case of star-shaped ones. Here,
the action functional is augmented by an additional penalty that prevents the
edges of the graph to overlap. This allows to preserve the graph topology and
thus to rely on standard techniques in Calculus of Variations in order to show
existence of minimizers.

</details>


### [26] [On the solvability of some systems of quadratic integral equations in dimensions two and three](https://arxiv.org/abs/2506.15069)
*Vitali Vougalter*

Main category: math.AP

TL;DR: Existence of solutions for quadratic integral equations in H^2(R^d,R^N) using fixed point techniques.


<details>
  <summary>Details</summary>
Motivation: To address the existence of solutions for a specific system of quadratic integral equations in higher-dimensional spaces (d=2,3).

Method: Fixed point technique is employed to demonstrate the existence of perturbed solutions.

Result: Existence of solutions is proven for the given system in H^2(R^d,R^N).

Conclusion: The fixed point technique successfully establishes the existence of perturbed solutions for the quadratic integral equations.

Abstract: The work deals with the existence of solutions of a certain system of
quadratic integral equations in H^2(R^d,R^N), d = 2, 3. We demonstrate the
existence of a perturbed solution by virtue of a fixed point technique.

</details>


### [27] [On a reaction-diffusion system modeling strong competition between two mosquito populations](https://arxiv.org/abs/2506.15202)
*Nicolas Vauchelet*

Main category: math.AP

TL;DR: The paper analyzes a reaction-diffusion model for spatial segregation between two mosquito species, Aedes aegypti and Aedes albopictus, due to strong competition in larval stages.


<details>
  <summary>Details</summary>
Motivation: To mathematically explain observed spatial segregation of mosquito species in tropical regions, where Aedes aegypti dominates urban areas and Aedes albopictus thrives in forests.

Method: A reaction-diffusion system models aquatic and aerial phases of both species in heterogeneous environments, with strong larval competition reducing system dimensionality.

Result: A sufficient condition prevents species invasion in homogeneous environments, and spatial segregation is demonstrated in heterogeneous settings, supported by numerical simulations.

Conclusion: The model successfully explains spatial segregation due to competition, with theoretical and numerical validation.

Abstract: This paper is devoted to the analysis of a reaction-diffusion system with
strong competition and spatial heterogeneities modelling the interaction
between two species of mosquitoes. In particular, we propose a mathematical
model that accounts for the spatial segregation observed between two species of
mosquito vectors of numerous viruses. Indeed, it has been observed that, in
tropical regions, Aedes aegypti mosquitoes are well established in urban areas
whereas Aedes albopictus mosquitoes spread widely in forest regions. Moreover,
these species of mosquitoes compete with each other in the larval stage. Based
on these observations, we introduce a simple mathematical model to account for
this phenomenon. This model consists of a system of reaction-diffusion
equations describing the dynamics of the aquatic and aerial phases of each
species in a spatially heterogeneous environment. The competition takes place
at the aquatic phase and is assumed to be strong which allows us to reduce the
dimensionality of the system. We first establish a sufficient condition on the
parameters to prevent one species from invading another in a homogeneous
environment. Next, using this sufficient condition, we show that spatial
segregation may be observed in a spatially heterogeneous environment. Our
theoretical results are also illustrated by some numerical simulations.

</details>


### [28] [A toy model for frequency cascade in the nonlinear Schrodinger equation](https://arxiv.org/abs/2506.15226)
*Rémi Carles,Erwan Faou*

Main category: math.AP

TL;DR: An elementary approach to observe frequency cascades in forced nonlinear Schrödinger equations, with stability results and numerical simulations.


<details>
  <summary>Details</summary>
Motivation: To study frequency cascades in nonlinear Schrödinger equations under specific forcing terms.

Method: Algebraic computations discard time and space derivatives to derive explicit frequency cascades; stability is analyzed when derivatives are included.

Result: Initial algebraic solutions remain stable when derivatives are incorporated, supported by numerical simulations.

Conclusion: The approach provides insights into frequency cascades and stability in forced nonlinear Schrödinger equations.

Abstract: We present an elementary approach to observe frequency cascade on forced
nonlinear Schr{\"o}dinger equations. The forcing term consists of a constant
term, perturbed by a modulated Gaussian well. Algebraic computations provide an
explicit frequency cascade when time and space derivatives are discarded from
the nonlinear Schr{\"o}dinger equation. We provide stability results, showing
that when derivatives are incorporated in the model, the initial algebraic
solution may be little affected, possibly over long time intervals. Numerical
simulations are provided, which support the analysis.

</details>


### [29] [Enstrophy dynamics for flow past a solid body with no-slip boundary condition](https://arxiv.org/abs/2506.15317)
*Aleksei Gorshkov*

Main category: math.AP

TL;DR: Study of boundary vorticity's impact on enstrophy dynamics for streamlined flows, deriving a new energy identity and proving enstrophy dissipativity for Stokes systems, while obtaining a new enstrophy dynamics equation for Navier-Stokes systems.


<details>
  <summary>Details</summary>
Motivation: To understand how boundary vorticity distribution affects enstrophy dynamics in flows around streamlined bodies.

Method: Derived a new energy identity incorporating boundary vortex function values, analyzed Stokes and Navier-Stokes systems.

Result: Proved enstrophy dissipativity for Stokes systems and derived a new enstrophy dynamics equation for Navier-Stokes systems.

Conclusion: The study provides insights into enstrophy dynamics influenced by boundary vorticity, with implications for both Stokes and Navier-Stokes systems.

Abstract: In the paper we study the impact of the boundary vorticity distribution on
the dynamics of enstrophy for flows around streamlined body. A new energy
identity is derived in the article, which includes the boundary values of the
vortex function. For the Stokes system the dissipativity of enstrophy is
proved. For the Navier-Stokes system a new equation of the enstrophy dynamics
is obtained.

</details>


### [30] [The superposition principle for the continuity equation with singular flux](https://arxiv.org/abs/2506.15333)
*Stefano Almi,Riccarda Rossi,Giuseppe Savaré*

Main category: math.AP

TL;DR: The paper extends representation results for absolutely continuous curves in Wasserstein spaces to the case of bounded variation (BV) curves and p=1, focusing on continuity equations with singular flux measures.


<details>
  <summary>Details</summary>
Motivation: To generalize existing results for evolutionary PDEs in measure-theoretic settings to include BV curves and singular flux measures, enabling broader applications.

Method: The study analyzes the relation between BV curves and solutions to continuity equations, introduces an auxiliary continuity equation in an augmented phase space, and derives probabilistic representations via superposition principles.

Result: A probabilistic representation of solutions (μ,ν) is derived, involving Lipschitz trajectories and BV curves, with a detailed description of behavior at jump points.

Conclusion: The work successfully extends superposition principles to BV curves and singular flux measures, providing new tools for studying measure-valued solutions.

Abstract: Representation results for absolutely continuous curves $\mu:[0,T]\to
\mathcal{P}_p(\mathbb{R}^d)$, $p>1$, with values in the Wasserstein space
$(\mathcal{P}_p(\mathbb{R}^d),W_p)$ of Borel probability measures in
$\mathbb{R}^d$ with finite $p$-moment, provide a crucial tool to study
evolutionary PDEs in a measure-theoretic setting. They are strictly related to
the superposition principle for measure-valued solutions to the continuity
equation. This paper addresses the extension of these results to the case
$p=1$, and to curves $\mu:[0,+\infty)\to\mathcal{P}_1(\mathbb{R}^d)$ that are
only of bounded variation in time: in the corresponding continuity equation,
the flux measure
$\nu\in\mathcal{M}_{loc}([0,+\infty)\times\mathbb{R}^{d};\mathbb{R}^{d})$ thus
possesses a non-trivial singular part w.r.t. $\mu$ in addition to the
absolutely continuous part featuring the velocity field. Firstly, we carefully
address the relation between curves in ${\rm
BV}_{loc}([0,+\infty);\mathcal{P}_1(\mathbb{R}^d))$ and solutions to the
associated continuity equation, among which we select those with minimal
singular (contribution to the) flux $\nu$. We show that, with those
distinguished solutions it is possible to associate an `auxiliary' continuity
equation, in an augmented phase space, solely driven by its velocity field. For
that continuity equation, a standard version of the superposition principle can
be thus obtained. In this way, we derive a first probabilistic representation
of the pair $(\mu,\nu)$ solutions by projection over the time and space
marginals. This representation involves Lipschitz trajectories in the augmented
phase space, reparametrized in time and solving the characteristic system of
ODEs. Finally, for the same pair $(\mu,\nu)$ we also prove a superposition
principle in terms of BV curves on the actual time interval, providing a fine
description of their behaviour at jump points.

</details>


### [31] [Boundary behaviour of layer potentials for the multi-term time-fractional diffusion equation](https://arxiv.org/abs/2506.15356)
*Karolina Pawlak*

Main category: math.AP

TL;DR: The paper studies boundary behavior of layer potentials for the multi-term time-fractional diffusion equation (MTFDE) across a moving boundary, establishing jump relations and continuity for double-layer potentials.


<details>
  <summary>Details</summary>
Motivation: To analyze boundary integral equations for MTFDE in time-dependent domains, addressing the complexity of the multi-term case compared to simpler fractional diffusion equations.

Method: Establishes jump relations for the double-layer potential of the inhomogeneous MTFDE and proves continuity for the homogeneous case.

Result: Demonstrates the jump relation and continuity, overcoming challenges posed by the multi-term structure.

Conclusion: The findings are crucial for understanding boundary integral equations in MTFDE, especially in dynamic domains.

Abstract: This paper investigates the boundary behaviour of layer potentials for the
multi-term time-fractional diffusion equation (MTFDE) across the moving
boundary. First, we establish the jump relation for the double-layer potential
associated with the fundamental solution of the inhomogeneous MTFDE. Second, we
prove the continuity of the double-layer potential generated by the kernel
corresponding to the homogeneous MTFDE. Krasnoschok obtained similar results
for the time-fractional diffusion equation. However, in the multi-term case,
the fundamental solution has more complex structure and does not admit standard
scaling properties, which requires a different approach. Our results are
essential for the analysis of boundary integral equations related to the MTFDE
in time-dependent domains.

</details>


### [32] [Admissible solutions of the 2D Onsager's conjecture](https://arxiv.org/abs/2506.15396)
*Lili Du,Xinliang Li,Weikui Ye*

Main category: math.AP

TL;DR: The paper constructs Hölder continuous weak solutions for the 2D Euler equations that dissipate kinetic energy, improving prior work. It also shows initial data density and introduces new methods for energy modulation.


<details>
  <summary>Details</summary>
Motivation: To advance understanding of energy dissipation in weak solutions of Euler equations and refine existing techniques.

Method: Introduces new traveling waves and a multiple iteration scheme combining Newton-Nash and Picard-type iterations.

Result: Existence of dissipative weak solutions below the Onsager critical exponent in any dimension ≥2.

Conclusion: The work provides new tools and insights for studying energy dissipation in fluid dynamics.

Abstract: We show that for any $\gamma < \frac{1}{3}$ there exist H\"{o}lder continuous
weak solutions $v \in C^{\gamma}([0,T] \times \mathbb{T}^2)$ of the
two-dimensional incompressible Euler equations that strictly dissipate the
total kinetic energy, improving upon the elegant work of Giri and Radu [Invent.
Math., 238 (2), 2024]. Furthermore, we prove that the initial data of these
\textit{admissible} solutions are dense in $B^{\gamma}_{\infty,r<\infty}$.
  Our approach introduces a new class of traveling waves, refining the
traditional temporal oscillation function first proposed by Cheskidov and Luo
[Invent. Math., 229(3), 2022], to effectively modulate energy on any time
intervals. Additionally, we propose a novel ``multiple iteration scheme''
combining Newton-Nash iteration with a Picard-type iteration to generate an
energy corrector for controlling total kinetic energy during the perturbation
step. This framework enables us to construct dissipative weak solutions below
the Onsager critical exponent in any dimension $d \geq 2$.

</details>


### [33] [Existence, uniqueness, regularity and stability of solutions to linear $X$-elliptic equations with measurable coefficients](https://arxiv.org/abs/2506.15409)
*Marco Picerni*

Main category: math.AP

TL;DR: Existence and uniqueness of solutions for linear $X$-elliptic equations with $L^1$ data and zero Dirichlet conditions, with continuous dependence on data and improved summability for solutions when data summability improves.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous results for solutions of $X$-elliptic equations with $L^1$ data, extending known properties from uniformly elliptic cases.

Method: Proving existence, uniqueness, and continuous dependence on data for solutions, and analyzing summability improvements in solutions when data summability improves.

Result: Solutions exist, are unique, depend continuously on data, and exhibit improved summability with better data summability.

Conclusion: The paper generalizes properties of uniformly elliptic equations to $X$-elliptic cases, providing a foundation for further analysis.

Abstract: We prove an existence and uniqueness result for solutions to linear
$X$-elliptic equations with $L^1$ data and zero Dirichlet boundary conditions.
Such solutions depend continuously on the datum. Moreover, we show that an
improvement in the summability of the data yields a corresponding improvement
in the summability of the solutions, in a manner analogous to the one that
occurs in the case of uniformly elliptic equations.

</details>


### [34] [Existence and uniqueness of global large-data solutions for the Chemotaxis-Navier-Stokes system in $\mathbb{R}^2$](https://arxiv.org/abs/2506.15434)
*Fan Xu,Feng Dai,Bin Liu*

Main category: math.AP

TL;DR: The paper proves global existence and uniqueness of smooth solutions for the Chemotaxis-Navier-Stokes system in 2D under large initial data, using entropy-energy estimates and bootstrap arguments.


<details>
  <summary>Details</summary>
Motivation: To address the unresolved problem of global existence and uniqueness of strong, classical, and smooth solutions for the CNS system under large initial data.

Method: Derives entropy-energy estimates for low regularity data, then uses bootstrap arguments to achieve higher-order energy estimates for smoother data.

Result: Establishes global existence and uniqueness of strong, classical, and arbitrarily smooth solutions.

Conclusion: The intrinsic entropy structure and parabolic nature of the CNS system are key to achieving these results.

Abstract: This work investigates the Cauchy problem for the classical
Chemotaxis-Navier-Stokes (CNS) system in $\mathbb{R}^2$. We establish the
global existence and uniqueness of strong, classical, and arbitrarily smooth
solutions under large initial data, which has not been addressed in the
existing literature. The key idea is to first derive an entropy-energy estimate
for initial data with low regularity, by leveraging the intrinsic entropy
structure of the system. Building on this foundation, we then obtain
higher-order energy estimates for smoother initial data via a bootstrap
argument, in which the parabolic nature of the CNS system plays a crucial role
in the iterative control of regularity.

</details>


### [35] [The Rayleigh-Boltzmann equation with shear deformations in the hyperbolic-dominated regime](https://arxiv.org/abs/2506.15449)
*Nicola Miele,Alessia Nota,Juan J. L. Velázquez*

Main category: math.AP

TL;DR: The paper analyzes homoenergetic solutions of the Rayleigh-Boltzmann equation in a hyperbolic-dominated regime, focusing on collision kernels with γ ∈ (-1,0). It provides formal long-time asymptotics for velocity distribution and a probabilistic interpretation of the process.


<details>
  <summary>Details</summary>
Motivation: To understand the long-term behavior of solutions in a hyperbolic-dominated regime, where deformation dominates collisions, and to describe the asymptotic profile of velocity distributions.

Method: Formal analysis of long-time asymptotics for velocity distribution, focusing on collision kernels with γ ∈ (-1,0). A probabilistic interpretation is given via a Markov process combining shear flow and collisions.

Result: Explicit form of the asymptotic profile for velocity distribution is provided. Different asymptotic behaviors are discussed for γ < -1.

Conclusion: The study offers insights into the long-term dynamics of homoenergetic solutions in hyperbolic-dominated regimes, with implications for understanding shear flow and collision interactions.

Abstract: In this paper we consider a particular class of solutions of the
Rayleigh-Boltzmann equation, known in the nonlinear setting as homoenergetic
solutions, which have the form $g\left( x,v,t \right) =f\left( v-L\left(
t\right)x,t\right)$ where the matrix $L(t)$ describes a shear flow deformation.
We began this analysis in [22] where we rigorously proved the existence of a
stationary non-equilibrium solution and established the different behaviour of
the solutions for small and large values of the shear parameter, for cut-off
collision kernels with homogeneity parameter $0\leq \gamma <1$, including
Maxwell molecules and hard potentials. In this paper, we concentrate in the
case where the deformation term dominates the collision term for large times
(hyperbolic-dominated regime). This occurs for collision kernels with $\gamma <
0$ and in particular we focus on $\gamma \in (-1,0)$. In such a
hyperbolic-dominated regime, it appears challenging to provide a clear
description of the long-term asymptotics of the solutions. Here we present a
formal analysis of the long-time asymptotics for the distribution of velocities
and provide the explicit form for the asymptotic profile. Additionally, we
discuss the different asymptotic behaviour expected in the case of homogeneity
$\gamma < -1$. Furthermore, we provide a probabilistic interpretation
describing a stochastic process consisting in a combination of collisions and
shear flows. The tagged particle velocity $\{v(t)\}_{t\geq 0}$ is a Markov
process that arises from the combination of free flights in a shear flow along
with random jumps caused by collisions.

</details>


### [36] [Strichartz estimates for the generalized Zakharov-Kuznetsov equation on $\mathbb{R} \times \mathbb{T}$ and applications](https://arxiv.org/abs/2506.15517)
*Jakob Nowicki-Koth*

Main category: math.AP

TL;DR: The paper improves the well-posedness threshold for the $k$-generalized Zakharov-Kuznetsov equation using linear and bilinear estimates, achieving local well-posedness in $H^s$ for $s > \frac{3}{8}$.


<details>
  <summary>Details</summary>
Motivation: To address the Cauchy problem for the $k$-generalized Zakharov-Kuznetsov equation and lower its well-posedness threshold.

Method: Established an almost optimal linear $L^4$-estimate and a family of bilinear refinements.

Result: Local well-posedness in $H^s(\mathbb{R} \times \mathbb{T})$ for all $s > \frac{3}{8}$.

Conclusion: The method significantly improves the well-posedness threshold for the equation.

Abstract: In this article, we address the Cauchy problem associated with the
$k$-generalized Zakharov-Kuznetsov equation posed on $\mathbb{R} \times
\mathbb{T}$. By establishing an almost optimal linear $L^4$-estimate, along
with a family of bilinear refinements, we significantly lower the
well-posedness threshold for all $k \geq 2$. In particular, we show that the
modified Zakharov-Kuznetsov equation is locally well-posed in $H^s(\mathbb{R}
\times \mathbb{T})$ for all $s > \frac{3}{8}$.

</details>


### [37] [Rigidity of solutions to singular/degenerate semilinear critical equations](https://arxiv.org/abs/2506.15611)
*Giovanni Catino,Dario Daniele Monticelli,Alberto Roncoroni*

Main category: math.AP

TL;DR: The paper analyzes singular/degenerate semilinear critical equations from Caffarelli-Kohn-Nirenberg inequalities in ℝᵈ (d≥2), proving rigidity results for positive solutions, especially classifying infinite-energy solutions for intrinsic dimensions 2<n<4.


<details>
  <summary>Details</summary>
Motivation: To understand and classify solutions of critical semilinear equations derived from Caffarelli-Kohn-Nirenberg inequalities, focusing on rigidity and infinite-energy cases.

Method: The study employs mathematical analysis and proofs to classify positive solutions, particularly those with infinite energy, for intrinsic dimensions 2<n<4.

Result: Rigidity results for positive solutions are established, including classification of infinite-energy solutions in the specified dimension range.

Conclusion: The paper successfully classifies solutions and provides rigidity results, advancing understanding of critical semilinear equations in higher dimensions.

Abstract: This paper deals with singular/degenerate semilinear critical equations which
arise as the Euler-Lagrange equation of Caffarelli-Kohn-Nirenberg inequalities
in $\mathbb{R}^d$, with $d\geq 2$. We prove several rigidity results for
positive solutions, in particular we classify solutions with possibily infinite
energy when the intrinsic dimension $n$ satisfies $2<n<4$.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [38] [An Atomic Cluster Expansion Potential for Twisted Multilayer Graphene](https://arxiv.org/abs/2506.15061)
*Yangshuai Wang,Drake Clark,Sambit Das,Ziyan Zhu,Daniel Massatt,Vikram Gavini,Mitchell Luskin,Christoph Ortner*

Main category: physics.comp-ph

TL;DR: A machine-learning interatomic potential (ACE) is developed for twisted multilayer graphene, addressing computational challenges with a novel dataset approach and active learning.


<details>
  <summary>Details</summary>
Motivation: Traditional methods like first-principles calculations are computationally expensive, and empirical potentials lack accuracy, prompting the need for MLIPs in twisted multilayer graphene.

Method: An Atomic Cluster Expansion (ACE) potential is developed, with datasets incorporating all twist angles and local stacking, refined via active learning and Bayesian uncertainty.

Result: The model achieves (near-)DFT accuracy at reduced computational cost, validated through extensive numerical tests.

Conclusion: The ACE potential is a robust and accurate tool for simulating twisted multilayer graphene, overcoming limitations of existing methods.

Abstract: Twisted multilayer graphene, characterized by its moire patterns arising from
inter-layer rotational misalignment, serves as a rich platform for exploring
quantum phenomena. While first-principles calculations are computationally
prohibitive and empirical interatomic potentials often lack accuracy,
machine-learning interatomic potentials (MLIPs) present a promising
alternative, offering (near-)DFT accuracy at a significantly reduced
computational cost. Despite their success in two-dimensional monolayer
materials, MLIPs remain under-explored in twisted multilayer graphene systems.
In this work, we develop an Atomic Cluster Expansion (ACE) potential for
simulating twisted multilayer graphene and test it on a range of simulation
tasks. We propose an approach to construct training and test datasets that
incorporate all possible twist angles and local stacking, including
incommensurate ones. To achieve this, we generate configurations with periodic
boundary conditions suitable for DFT calculations, and then introduce an
internal twist and shift within those supercell structures. We further refine
the dataset through active learning filtering, guided by Bayesian uncertainty
quantification. Our model is validated for accuracy and robustness through a
wide range of numerical tests.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [39] [Electrically insulating materials for centrifugal mirrors](https://arxiv.org/abs/2506.14847)
*Nick R. Schwartz,Carlos A. Romero-Talamás,Marlene I. Patino,Daisuke Nishijima,Matthew J. Baldwin,Russel P. Doerner,Artur Perevalov,Nathan Eschbach,Zachary D. Short,John Cumings,Ian G. Abel,Brian L. Beaudoin,Timothy W. Koeth*

Main category: physics.plasm-ph

TL;DR: The paper investigates the use of hexagonal boron nitride (hBN) in centrifugal mirror fusion devices, highlighting its potential due to thermal and electrical properties, and evaluates its performance under plasma exposure.


<details>
  <summary>Details</summary>
Motivation: To explore hBN's suitability for plasma-facing components in centrifugal mirror fusion, given its promising properties but limited testing under fusion-relevant conditions.

Method: Combined computational modeling (RustBCA, OpenMC) and experimental plasma exposure tests (PISCES-A, CMFX) to assess hBN's performance.

Result: hBN showed better resistance to erosion (sputtering and grain ejection) compared to silicon carbide under plasma exposure.

Conclusion: hBN is a viable candidate for plasma-facing components in centrifugal mirror fusion devices due to its superior performance under tested conditions.

Abstract: The centrifugal mirror confinement scheme incorporates supersonic rotation
into a magnetic mirror device, which stabilizes and heats the plasma. This
concept is under investigation in the Centrifugal Mirror Fusion Experiment
(CMFX) at the University of Maryland. Plasma rotation is driven by an axial
magnetic field and a radial electric field that lead to velocity drifts in the
azimuthal direction. An electrically insulating material is required to prevent
the applied voltage from shorting on the grounded chamber. Hexagonal boron
nitride (hBN) is a promising candidate material for plasma-facing components in
future centrifugal mirrors due to its exceptional thermal and electrical
properties. However, its performance under intense particle and heat fluxes
characteristic of the plasma edge in fusion devices remains largely unexplored.
Computational modeling for ion- and neutron-material interactions was carried
out with RustBCA and OpenMC, respectively, and predicts relatively good
performance in comparison to other insulating materials. Material coupons were
then exposed to plasma in PISCES-A at UCSD and CMFX. A load-locked sample
feedthrough was constructed and installed on CMFX to test coupons. Two erosion
mechanisms were identified -- sputtering and grain ejection -- both of which
were more apparent in silicon carbide than hBN.

</details>


### [40] [Determining the Validity of Tokamak Perturbed Equilibrium Modeling Using Nonlinear Equilibria](https://arxiv.org/abs/2506.14938)
*J. Halpern,N. C. Logan,E. Paul,C. Paz-Soldan*

Main category: physics.plasm-ph

TL;DR: The study identifies the correct reference frame for predicting perturbed equilibrium in tokamaks with small 3D fields, validating linear MHD theory for existing tolerances but noting limitations for future devices.


<details>
  <summary>Details</summary>
Motivation: To determine the correct reference frame for axisymmetry in tokamak models with small 3D fields, as predictions vary based on the assumed frame (e.g., TF coil vs. PF coil centroid).

Method: Uses fully 3D equilibria from VMEC to analyze the correct reference frame under n=1 coil asymmetries, examining cases for SPARC and NSTX-U by studying shifted magnetic axes and field line displacements.

Result: For SPARC, the TF coil centroid approximates the correct frame; for NSTX-U, the radial location of TF coil inboard legs at the midplane does. Linear MHD theory is valid for current tolerances but may break down for future devices with tolerances >1% of minor radius.

Conclusion: The findings enable confident use of 3D perturbative models for assembly tolerances by clarifying the theory's correct application, though future devices may require adjustments.

Abstract: The prediction of perturbed equilibrium models for tokamaks with small 3D
fields is strongly dependent on which reference frame for axisymmetry is
assumed - for example, the toroidal field (TF) coil vs the poloidal field (PF)
coil centroid. We use fully 3D equilibria generated by VMEC to determine the
correct reference frame in these models when subject to n=1 coil asymmetries.
We analyze a case for SPARC and find that the appropriate frame can be well
approximated by the centroid of the TF coil set. We also consider the case of
NSTX-U and find that the frame can be approximated by the radial location of
the TF coil inboard legs at the midplane. We determine the correct frame by
analyzing the shifted magnetic axis in the nonlinear equilibria. We also
analyze the magnetic field line displacement to identify where the linearized
MHD theory begins to break down, and compare that to typical coil tolerances in
existing tokamaks. We find that linear theory is valid for existing tolerances,
validating the use of perturbative codes to set these tolerances, but with a
sufficiently small margin to be of note for future devices with relative
tolerances larger than approximately $1\%$ of the minor radius. This study
enables engineers to confidently use 3D perturbative models for determining
assembly tolerances by providing insight into the correct applications of the
theory.

</details>


### [41] [Imaginary part of the conductivity using Kramers-Kronig relations](https://arxiv.org/abs/2506.14941)
*Jean-Christophe Pain,Mikael Tacu*

Main category: physics.plasm-ph

TL;DR: The paper discusses methods to determine the imaginary part of AC conductivity in plasmas using Kramers-Kronig relations, comparing traditional and alternate forms.


<details>
  <summary>Details</summary>
Motivation: To accurately model frequency-dependent photo-absorption in plasmas, both real and imaginary parts of AC conductivity are needed. The imaginary part is crucial for determining the refraction index.

Method: Two methods are proposed: decomposition into simple elements and complex integration in a quarter of the complex plane, using an alternate form of the Kramers-Kronig relations.

Result: The paper provides two distinct approaches to derive the imaginary part of conductivity, offering flexibility in calculations.

Conclusion: The alternate form of Kramers-Kronig relations, with ω'²−ω² in the denominator, is viable and provides alternative methods for determining the imaginary part of conductivity.

Abstract: In order to obtain the frequency-dependent photo-absorption in a plasma, both
the real and imaginary parts of the AC conductivity are required. The real part
can be deduced from the knowledge of the static conductivity (given by the
Ziman-Evans formula for instance) and the Drude model. The imaginary part,
required for the refraction index, can be obtained using the Kramers-Kronig
relations. Usually, it is obtained by complex integration in the complex plane
of the usual Kramers-Kronig relations, having $\omega'-\omega$ in the
denominator. However, an alternate form of the Kramers-Kronig relation is often
used in physics, especially for determining response functions. It has
$\omega'^2-\omega^2$ in the denominator. We provide two determinations of the
imaginary part of the conductivity for this latter form, one using a
decomposition into simple elements, and the other involving a complex
integration in a quarter of the complex plane.

</details>


### [42] [Fourier Transform Method Of A Detailed Configuration Accounting In Hot Plasma Bound-Bound Opacity Calculations](https://arxiv.org/abs/2506.14953)
*Evgeniya Arapova,Yulia Koryakina,Mikhail Vronskiy*

Main category: physics.plasm-ph

TL;DR: Extension of Hazak-Kurzweil method for bound-bound opacity calculations using Detailed Configuration Accounting.


<details>
  <summary>Details</summary>
Motivation: To improve the Super Transition Arrays approach by extending the Hazak-Kurzweil method for more detailed calculations.

Method: Based on Fourier transform representation, linearity of transition energy, and factorization of configuration probabilities.

Result: Alternative expressions for bound-bound opacity, suitable for numerical implementation.

Conclusion: The extended method provides a practical alternative for opacity calculations.

Abstract: G.~Hazak and J.~Kurzweil discovered a method of configurational resolution of
transition arrays for the Super Transition Arrays approach to the bound-bound
opacity calculation. Their method is based on the representation of the
photoabsorption coefficient as the Fourier transform, the linearity of the
transition energy between configurations with respect to shell occupation
numbers, and factorization of the probabilities of configurations on shell
occupation numbers. We extend the Hazak -- Kurzweil method for the calculations
with Detailed Configuration Accounting. The resulting expressions for the
bound-bound opacity represent an alternative to the widely used ones and are
quite convenient for numerical implementation.

</details>


### [43] [The Gardner equation and acoustic solitary waves in plasmas](https://arxiv.org/abs/2506.15024)
*Frank Verheest,Willy A. Hereman*

Main category: physics.plasm-ph

TL;DR: The paper investigates ion-acoustic waves in a dusty plasma using Cairns-distributed ions and Boltzmann-distributed electrons. Two methods, SPA and RPT, are compared for soliton solutions.


<details>
  <summary>Details</summary>
Motivation: To analyze and compare soliton solutions in dusty plasmas using two theoretical methods, SPA and RPT, for accuracy and practicality.

Method: Sagdeev pseudopotential analysis (SPA) for full nonlinearity and reductive perturbation theory (RPT) for perturbative solutions, yielding the Gardner equation.

Result: SPA provides accurate soliton profiles numerically, while RPT offers analytic solutions via the Gardner equation under specific conditions.

Conclusion: Both methods are effective, with SPA being more accurate but computationally intensive, and RPT providing practical analytic solutions under constraints.

Abstract: Ion-acoustic waves in a dusty plasma are investigated where it is assumed
that the ions follow a Cairns distribution and the electrons are Boltzmann
distributed. Two theoretical methods are applied: Sagdeev pseudopotential
analysis (SPA) and reductive perturbation theory (RPT). Since SPA incorporates
all nonlinearities of the model it is the most accurate but deriving soliton
profiles requires numerical integration of Poisson's equation. By contrast, RPT
is a perturbation method which at second order yields the Gardner equation
incorporating both the quadratic nonlinearity of the KdV equation with the
cubic nonlinearity of the modified KdV equation. For consistency with the
perturbation scheme the coefficient of the quadratic term needs to be at least
an order of magnitude smaller than the coefficient of the cubic term. Solving
the Gardner equation yields an analytic expression of the soliton profile.
Selecting an appropriate set of compositional parameters, the soliton solutions
obtained from SPA and RPT are analyzed and compared.

</details>


### [44] [Helical Electron Beam Micro-Bunching by High-Order Modes in a Micro-Plasma Waveguide](https://arxiv.org/abs/2506.15094)
*Xinju Guo,Longqing Yi*

Main category: physics.plasm-ph

TL;DR: High-energy electron beams (~100 MeV) with high charge (~10 nC), ultrashort duration (~30 fs), and small divergence (~1 deg) are produced using a 100-TW Laguerre-Gaussian laser in a micro-plasma waveguide. Helical micro-bunching is achieved by controlling the laser's spin and orbital angular momentum.


<details>
  <summary>Details</summary>
Motivation: To explore efficient electron acceleration and beam control using high-power Laguerre-Gaussian lasers in micro-plasma waveguides, enabling applications in fundamental science and technology.

Method: 3D particle-in-cell simulations are used to study electron acceleration by a Laguerre-Gaussian laser in a micro-plasma waveguide, focusing on the effects of circular polarization and high-order waveguide modes.

Result: The method produces high-energy, high-charge, ultrashort electron beams with helical micro-bunching, controllable via the laser's angular momentum.

Conclusion: This approach enables the generation of relativistic electron beams with controlled helicity, offering significant potential for scientific and technological advancements.

Abstract: Electron acceleration by a high-power Laguerre-Gaussian pulse in a
micro-plasma waveguide is investigated. When the incident laser travels in the
waveguide, electrons on the wall are extracted into the vacuum core and
accelerated by the longitudinal field of the waveguide mode. Using 3D
particle-in-cell simulations, we demonstrate that high energy (~100 MeV)
electron beams with extremely high charge (~10 nC), ultrashort duration (~30
fs) and small divergence (~1 deg) can be produced by a 100-TW, few-Joule class
laser system. In particular, when the drive Laguerre-Gaussian pulse is
circularly polarized, it excites high-order waveguide modes that exhibit
helical longitudinal electric fields. The 3D profile of this accelerating field
is imprinted into the high energy electron beam, leading to helical
micro-bunching. This process can be controlled by the spin and orbital angular
momentum of the drive pulse. This work paves the way to the generation of
highcharge, relativistic electron beams with controlled helicity, which holds
great potential for advances in fundamental science and a variety of
applications.

</details>


### [45] [Laser-Driven Annular Shock Waves as Laboratory Analogues of $w$CDM Cosmologies and Cosmological Gravitational Waves](https://arxiv.org/abs/2506.15493)
*Felipe A. Asenjo,Felipe Veloso,Julio C. Valenzuela*

Main category: physics.plasm-ph

TL;DR: Experimental evolution of a plasma shock wave mimics cosmological universe dynamics, enabling lab-scale simulation of complex models like $w$CDM cosmologies and gravitational waves.


<details>
  <summary>Details</summary>
Motivation: To explore cosmological phenomena like dark energy and gravitational perturbations through controlled plasma shock wave experiments.

Method: Studying the evolution of laser-driven annular plasma shock waves and their self-interaction, mapping trajectories to $w$CDM cosmologies.

Result: Shock dynamics follow a Hubble-like law, and perturbations in shock fronts resemble cosmological gravitational perturbations.

Conclusion: This approach provides a novel experimental method for simulating cosmological models and gravitational waves in the lab.

Abstract: We demonstrate that the experimental evolution of an annular, laser-driven
plasma shock wave, expanding over time and undergoing self-interaction gives
rise to multiple shock structures that evolve analogously to a multicomponent
cosmological universe. Different propagation trajectories along the shock
surface correspond to various forms of $w$CDM cosmologies, enabling the study
of scenarios ranging from simple radiation- or matter-dominated universes to
those including dark energy. We further show that the dynamics of the Mach
stems approximately follows a Hubble-like law. Additionally, perturbations in
the shock fronts serve as experimental analogues of cosmological gravitational
perturbations in a matter-dominated universe. This work opens a new
experimental pathway for classically simulating complex cosmological models and
gravitational waves at macroscopic scales in the laboratory.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [46] [Shi-type and Hamilton-type gradient estimates for a general parabolic equation under compact Finsler $CD(-K,N)$ geometric flows](https://arxiv.org/abs/2506.14776)
*Yijie Miao,Bin Shen*

Main category: math.DG

TL;DR: The paper studies Li-Yau-type gradient estimates for parabolic equations under Finsler geometric flow, proposing Shi-type and Hamilton-type estimates to relax stricter curvature conditions.


<details>
  <summary>Details</summary>
Motivation: To address the stricter derivative bounds imposed by curvature conditions in Finsler geometry compared to Riemannian cases.

Method: Uses Shi-type and Hamilton-type gradient estimates to analyze solutions to the parabolic equation under Finsler geometric flow.

Result: Demonstrates the possibility of removing strict curvature conditions.

Conclusion: The proposed gradient estimates offer a way to relax constraints in Finsler geometric flows.

Abstract: Recently, the Li-Yau-type gradient estimates for positive solutions to
parabolic equations
  \begin{equation}
  \partial_t u=\Delta
u+\mathcal{R}_1u+\mathcal{R}_2u^{\alpha}+\mathcal{R}_3u(\log u)^{\beta},\notag
  \end{equation}
  under the general compact Finsler $CD(-K,N)$ geometric flow are studied. Here
$\mathcal{R}_1$,$\mathcal{R}_2$,$\mathcal{R}_3$ $\in$ $ C^{1}(M,[0,T])$,
$\alpha$ and $\beta$ are both positive constants, $T$ is the maximal existence
time for the flow.
  However, compared with the Riemannian case, the curvature conditions impose
stricter derivative bounds on the development term in the geometric flow, as
well as on the derivative bounds of the distortion of the manifold. In this
manuscript, we present Shi-type and Hamilton-type gradient estimates to
demonstrate the possibility of removing such conditions.

</details>


### [47] [Dominating manifolds by radial balls](https://arxiv.org/abs/2506.15621)
*Samuel Bronstein*

Main category: math.DG

TL;DR: The paper explores functional rearrangement on noncollapsed RCD(k,n)-spaces, proving inequalities like Polya-Szego and Moser-Trudinger on certain manifolds.


<details>
  <summary>Details</summary>
Motivation: To extend functional inequalities to a broader class of manifolds with bounded Ricci curvature.

Method: Rearrangement of functions on noncollapsed RCD(k,n)-spaces, leveraging Polya-Szego type inequalities.

Result: Characterization of manifolds with lower bounded Ricci curvature that admit Moser-Trudinger type inequalities.

Conclusion: The work provides a framework for functional inequalities on specific manifolds, expanding their applicability.

Abstract: This paper is devoted to a kind of rearrangement of functions on noncollapsed
RCD(k,n)-spaces, which satisfy a Polya-Szego type inequality. This allows to
prove functional inequalities on a wide class of manifolds, that we describe.
As a consequence, we give a characterization, among manifolds with lower
bounded Ricci curvature, of those admitting a Moser-Trudinger type inequality

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [48] [Learning to Maximize Quantum Neural Network Expressivity via Effective Rank](https://arxiv.org/abs/2506.15375)
*Juan Yao*

Main category: quant-ph

TL;DR: The paper introduces the effective rank ($\kappa$) as a measure of quantum neural network (QNN) expressivity, demonstrates its theoretical bounds, and uses reinforcement learning to design optimal QNN circuits.


<details>
  <summary>Details</summary>
Motivation: Accurately characterizing QNN expressivity is challenging, hindering optimal quantum circuit design. The work aims to provide a rigorous framework for assessing and optimizing QNN expressivity.

Method: The authors propose the effective rank ($\kappa$) to quantify expressivity, analyze its theoretical bounds, and develop a reinforcement learning framework with a self-attention transformer agent for automated circuit design.

Result: The study shows that $\kappa$ can reach its theoretical upper bound under optimal conditions and successfully applies reinforcement learning to design highly expressive QNN circuits.

Conclusion: The work establishes $\kappa$ as a robust metric for QNN expressivity and demonstrates the potential of reinforcement learning in quantum circuit design, advancing quantum machine learning capabilities.

Abstract: Quantum neural networks (QNNs) are widely employed as ans\"atze for solving
variational problems, where their expressivity directly impacts performance.
Yet, accurately characterizing QNN expressivity remains an open challenge,
impeding the optimal design of quantum circuits. In this work, we introduce the
effective rank, denoted as $\kappa$, as a novel quantitative measure of
expressivity. Specifically, $\kappa$ captures the number of effectively
independent parameters among all the variational parameters in a parameterized
quantum circuit, thus reflecting the true degrees of freedom contributing to
expressivity. Through a systematic analysis considering circuit architecture,
input data distributions, and measurement protocols, we demonstrate that
$\kappa$ can saturate its theoretical upper bound, $d_n=4^n-1$, for an
$n$-qubit system when each of the three factors is optimally expressive. This
result provides a rigorous framework for assessing QNN expressivity and
quantifying their functional capacity. Building on these theoretical insights,
and motivated by the vast and highly structured nature of the circuit design
space, we employ $\kappa$ as a guiding metric for the automated design of
highly expressive quantum circuit configurations. To this end, we develop a
reinforcement learning framework featuring a self-attention transformer agent
that autonomously explores and optimizes circuit architectures. By integrating
theoretical characterization with practical optimization, our work establishes
$\kappa$ as a robust tool for quantifying QNN expressivity and demonstrates the
effectiveness of reinforcement learning in designing high-performance quantum
circuits. This study paves the way for building more expressive QNN
architectures, ultimately enhancing the capabilities of quantum machine
learning.

</details>


### [49] [Randomised composite linear-combination-of-unitaries: its role in quantum simulation and observable estimation](https://arxiv.org/abs/2506.15658)
*Jinzhao Sun,Pei Zeng*

Main category: quant-ph

TL;DR: The paper explores randomised linear-combination-of-unitaries (LCU) in quantum algorithms, addressing limitations like non-deterministic state preparation and introducing a quantum instrument for non-completely-positive maps. It connects randomised LCU to shadow tomography for efficient multi-observable estimation.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of randomised LCU, particularly the inability to deterministically prepare states and estimate multiple observables, and to bridge the gap between randomised LCU and shadow tomography.

Method: Analyzes randomised LCU, introduces a quantum instrument for non-completely-positive maps, constructs unbiased estimators for unphysical states, and demonstrates state preparation via composite LCU.

Result: Shows how to efficiently estimate multiple observables by connecting randomised LCU to shadow tomography, with practical applications in Hamiltonian simulation and eigenstate preparation.

Conclusion: Randomised LCU, enhanced by the proposed quantum instrument and estimators, offers a versatile tool for quantum simulations, enabling efficient multi-observable estimation and broader applications.

Abstract: Randomisation is widely used in quantum algorithms to reduce the number of
quantum gates and ancillary qubits required. A range of randomised algorithms,
including eigenstate property estimation by spectral filters, Hamiltonian
simulation, and perturbative quantum simulation, though motivated and designed
for different applications, share common features in the use of unitary
decomposition and Hadamard-test-based implementation. In this work, we start by
analysing the role of randomised linear-combination-of-unitaries (LCU) in
quantum simulations, and present several quantum circuits that realise the
randomised composite LCU. A caveat of randomisation, however, is that the
resulting state cannot be deterministically prepared, which often takes an
unphysical form $U \rho V^\dagger$ with unitaries $U$ and $V$. Therefore,
randomised LCU algorithms are typically restricted to only estimating the
expectation value of a single Pauli operator. To address this, we introduce a
quantum instrument that can realise a non-completely-positive map, whose
feature of frequent measurement and reset on the ancilla makes it particularly
suitable in the fault-tolerant regime. We then show how to construct an
unbiased estimator of the effective (unphysical) state $U \rho V^\dagger$ and
its generalisation. Moreover, we demonstrate how to effectively realise the
state prepared by applying an operator that admits a composite LCU form. Our
results reveal a natural connection between randomised LCU algorithms and
shadow tomography, thereby allowing simultaneous estimation of many observables
efficiently. As a concrete example, we construct the estimators and present the
simulation complexity for three use cases of randomised LCU in Hamiltonian
simulation and eigenstate preparation tasks.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [50] [Density estimation via periodic scaled Korobov kernel method with exponential decay condition](https://arxiv.org/abs/2506.15419)
*Ziyang Ye,Haoyuan Tan,Xiaoqun Wang,Zhijian He*

Main category: math.ST

TL;DR: The paper introduces the PSKK method for nonparametric density estimation on unbounded domains, eliminating the need for inherent periodicity and achieving competitive convergence rates.


<details>
  <summary>Details</summary>
Motivation: To extend kernel-based density estimation to non-periodic distributions on unbounded domains, overcoming limitations of prior methods.

Method: Uses a periodic wrapping technique and kernel ridge regression in scaled Korobov spaces.

Result: Achieves a convergence rate of O(M^(-1/(1+1/(2α)+ε))) for smooth densities with exponential decay.

Conclusion: PSKK outperforms traditional kernel density estimation for large samples and broader distribution classes.

Abstract: We propose the periodic scaled Korobov kernel (PSKK) method for nonparametric
density estimation on $\mathbb{R}^d$. By first wrapping the target density into
a periodic version through modulo operation and subsequently applying kernel
ridge regression in scaled Korobov spaces, we extend the kernel approach
proposed by Kazashi and Nobile (SIAM J. Numer. Anal., 2023) and eliminate its
requirement for inherent periodicity of the density function. This key
modification enables effective estimation of densities defined on unbounded
domains. We establish rigorous mean integrated squared error (MISE) bounds,
proving that for densities with smoothness of order $\alpha$ and exponential
decay, our method achieves the $\mathcal{O}(M^{-1/(1+1/(2\alpha)+\epsilon)})$
MISE convergence rate with an arbitrarily small $\epsilon>0$. While matching
the convergence rate of the previous kernel approach, our approach applies to a
broader class of non-periodic distributions. Numerical experiments confirm the
theoretical results and demonstrate significant improvement over traditional
kernel density estimation in large-sample regimes.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [51] [The Quasi-Radial Field-line Tracing (QRaFT): an Adaptive Segmentation of the Open-Flux Solar Corona](https://arxiv.org/abs/2506.14894)
*Vadim M. Uritsky,Christopher E. Rura,Cooper Downs,Shaela I. Jones,Charles Nickolos Arge,Nathalia Alzate*

Main category: astro-ph.SR

TL;DR: A new method, QRaFT, is introduced to detect field-aligned optical coronal features, improving analysis of the solar corona's open magnetic field.


<details>
  <summary>Details</summary>
Motivation: Analyzing the solar corona's open magnetic field is challenging due to low signal-to-noise ratios in coronagraph images.

Method: QRaFT (Quasi-Radial Field-line Tracing) is developed and tested using synthetic and real-life coronal images.

Result: QRaFT-detected features align within ~4-7 degrees with the local magnetic field, validated by numerical tests.

Conclusion: QRaFT provides valuable empirical data to enhance coronal and solar wind models, aiding space weather forecasts.

Abstract: Optical observations of solar corona provide key information on its magnetic
geometry. The large-scale open field of the corona plays an important role in
shaping the ambient solar wind and constraining the propagation dynamics of the
embedded structures, such as interplanetary coronal mass ejections. Rigorous
analysis of the open-flux coronal regions based on coronagraph images can be
quite challenging because of the depleted plasma density resulting in low
signal-to-noise ratios. In this paper, we present an in-depth description of a
new image segmentation methodology, the Quasi-Radial Field-line Tracing
(QRaFT), enabling a detection of field-aligned optical coronal features
approximating the orientation of the steady-state open magnetic field. The
methodology is tested using synthetic coronagraph images generated by a
three-dimensional magnetohydrodynamic model. The results of the numerical tests
indicate that the extracted optical features are aligned within $\sim 4-7$
degrees with the local magnetic field in the underlying numerical solution. We
also demonstrate the performance of the method on real-life coronal images
obtained from a space-borne coronagraph and a ground-based camera. We argue
that QRaFT outputs contain valuable empirical information about the global
steady-state morphology of the corona which could help improving the accuracy
of coronal and solar wind models and space weather forecasts.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [52] [Knowledge Distillation Framework for Accelerating High-Accuracy Neural Network-Based Molecular Dynamics Simulations](https://arxiv.org/abs/2506.15337)
*Naoki Matsumura,Yuta Yoshimoto,Yuto Iwasaki,Meguru Yamazaki,Yasufumi Sakai*

Main category: cs.LG

TL;DR: A novel knowledge distillation framework for neural network potentials (NNPs) avoids fine-tuning the teacher model, enabling better exploration of high-energy structures and reducing DFT calculations by 10x.


<details>
  <summary>Details</summary>
Motivation: Traditional KD methods for NNPs increase energy barriers, limiting training data for high-energy structures needed for stable MD simulations.

Method: A two-stage training process: first, train the student NNP with data from an off-the-shelf teacher model; then fine-tune with a small DFT dataset.

Result: Achieves comparable or superior accuracy in reproducing physical properties for organic and inorganic materials while reducing DFT calculations by 10x.

Conclusion: The proposed framework improves efficiency and accuracy in NNP training, enabling stable MD simulations with fewer costly DFT calculations.

Abstract: Neural network potentials (NNPs) offer a powerful alternative to traditional
force fields for molecular dynamics (MD) simulations. Accurate and stable MD
simulations, crucial for evaluating material properties, require training data
encompassing both low-energy stable structures and high-energy structures.
Conventional knowledge distillation (KD) methods fine-tune a pre-trained NNP as
a teacher model to generate training data for a student model. However, in
material-specific models, this fine-tuning process increases energy barriers,
making it difficult to create training data containing high-energy structures.
To address this, we propose a novel KD framework that leverages a
non-fine-tuned, off-the-shelf pre-trained NNP as a teacher. Its gentler energy
landscape facilitates the exploration of a wider range of structures, including
the high-energy structures crucial for stable MD simulations. Our framework
employs a two-stage training process: first, the student NNP is trained with a
dataset generated by the off-the-shelf teacher; then, it is fine-tuned with a
smaller, high-accuracy density functional theory (DFT) dataset. We demonstrate
the effectiveness of our framework by applying it to both organic (polyethylene
glycol) and inorganic (L$_{10}$GeP$_{2}$S$_{12}$) materials, achieving
comparable or superior accuracy in reproducing physical properties compared to
existing methods. Importantly, our method reduces the number of expensive DFT
calculations by 10x compared to existing NNP generation methods, without
sacrificing accuracy.

</details>


### [53] [HiPreNets: High-Precision Neural Networks through Progressive Training](https://arxiv.org/abs/2506.15064)
*Ethan Mulle,Wei Kang,Qi Gong*

Main category: cs.LG

TL;DR: A progressive framework (HiPreNets) is introduced to train high-precision neural networks by refining residuals, improving accuracy beyond traditional MSE-focused methods.


<details>
  <summary>Details</summary>
Motivation: Training highly accurate neural networks is challenging due to non-convex optimization and hyperparameter tuning, with traditional methods often neglecting critical $L^{\infty}$ error.

Method: The approach refines staged training by sequentially learning prediction residuals using additional networks, optimizing loss functions, parameters, and adaptive data sampling.

Result: Validated on benchmark problems, the framework demonstrates improved accuracy.

Conclusion: HiPreNets effectively addresses training challenges and enhances neural network precision.

Abstract: Deep neural networks are powerful tools for solving nonlinear problems in
science and engineering, but training highly accurate models becomes
challenging as problem complexity increases. Non-convex optimization and
numerous hyperparameters to tune make performance improvement difficult, and
traditional approaches often prioritize minimizing mean squared error (MSE)
while overlooking $L^{\infty}$ error, which is the critical focus in many
applications. To address these challenges, we present a progressive framework
for training and tuning high-precision neural networks (HiPreNets). Our
approach refines a previously explored staged training technique for neural
networks that improves an existing fully connected neural network by
sequentially learning its prediction residuals using additional networks,
leading to improved overall accuracy. We discuss how to take advantage of the
structure of the residuals to guide the choice of loss function, number of
parameters to use, and ways to introduce adaptive data sampling techniques. We
validate our framework's effectiveness through several benchmark problems.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [54] [High-Throughput Computation of Anharmonic Low-Frequency Protein Vibrations](https://arxiv.org/abs/2506.15109)
*Michael A. Sauer,Souvik Mondal,Madeline Cano,Matthias Heyden*

Main category: cond-mat.stat-mech

TL;DR: FRESEAN mode analysis isolates low-frequency vibrations from molecular dynamics simulations, aiding in studying protein conformational transitions. Coarse-graining reduces computational costs.


<details>
  <summary>Details</summary>
Motivation: To study intrinsically anharmonic vibrations in proteins and their role in slow conformational transitions, overcoming computational challenges.

Method: FRESEAN mode analysis, based on velocity time correlations, combined with coarse-graining of all-atom simulation trajectories.

Result: Low-frequency vibrations are effective collective variables for enhanced sampling, and coarse-graining reduces computational costs.

Conclusion: Coarse-graining with FRESEAN mode analysis efficiently extracts low-frequency vibrational information for large biomolecules.

Abstract: At room temperature, low frequency vibrations at far-infrared frequencies are
thermally excited ($k_B T > h \nu$) and not restricted to harmonic fluctuations
around a single potential energy minimum. For folded proteins, these
intrinsically anharmonic vibrations can contain information on slow
conformational transitions. Recently, we have developed FREquency-SElective
ANharmonic (FRESEAN) mode analysis, a method based on time correlation
functions that isolates low-frequency vibrational motions from molecular
dynamics simulation trajectories without relying on harmonic approximations. We
recently showed that low-frequency vibrations obtained from FRESEAN mode
analysis are effective collective variables in enhanced sampling simulations of
conformational ensembles. However, FRESEAN mode analysis is based on velocity
time correlations between all degrees of freedom, which creates computational
challenges for large biomolecules. To facilitate future applications, we
demonstrate here how coarse-graining of all-atom simulation trajectories can be
combined with FRESEAN mode analysis to extract information on low-frequency
vibrations at minimal computational cost.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [55] [Analysis and conditional optimization of projection estimates for the distribution of random variable using Legendre polynomials](https://arxiv.org/abs/2506.14822)
*Tatyana A. Averina,Konstantin A. Rybakov*

Main category: stat.CO

TL;DR: Algorithms for estimating density and distribution functions using Legendre polynomials, optimizing for accuracy and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: To improve approximation accuracy of density and distribution functions with minimal computational costs.

Method: Jointly solving conditional optimization problems using Legendre polynomials.

Result: Tested on examples with varying smoothness of density, demonstrating effectiveness.

Conclusion: Proposed algorithms efficiently balance accuracy and computational efficiency for density and distribution estimation.

Abstract: Algorithms for jointly obtaining projection estimates of the density and
distribution function of a random variable using the Legendre polynomials are
proposed. For these algorithms, a problem of the conditional optimization is
solved. Such an optimization allows one increasing the approximation accuracy
with a minimum computational costs. The proposed algorithms are tested on
examples with different degree of smoothness of the density.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [56] [MOFClassifier: A Machine Learning Approach for Validating Computation-Ready Metal-Organic Frameworks](https://arxiv.org/abs/2506.14845)
*Guobin Zhao,Pengyu Zhao,Yongchul G. Chung*

Main category: physics.chem-ph

TL;DR: MOFClassifier, a machine learning model using PU-CGCNN, improves accuracy in classifying computation-ready MOFs, outperforming rule-based methods with a ROC of 0.979.


<details>
  <summary>Details</summary>
Motivation: Existing MOF databases have high error rates, and rule-based methods for correcting them are limited and prone to misclassification.

Method: MOFClassifier uses a positive-unlabeled crystal graph convolutional neural network (PU-CGCNN) to predict crystal-likeness scores (CLscore) for MOFs.

Result: Achieves a ROC value of 0.979, identifies subtle errors undetectable by rule-based methods, and recovers misclassified structures.

Conclusion: MOFClassifier enhances computational screening accuracy, reduces overlooked candidates, and is integrated into CoRE MOF DB 2025 v1.

Abstract: The computational discovery and design of new crystalline materials,
particularly metal-organic frameworks (MOFs), heavily relies on high-quality,
computation-ready structural data. However, recent studies have revealed
significant error rates within existing MOF databases, posing a critical data
problem that hinders efficient high-throughput computational screening. While
rule-based algorithms like MOSAEC, MOFChecker, and the Chen and Manz method
(Chen-Manz) have been developed to address this, they often suffer from
inherent limitations and misclassification of structures. To overcome this
challenge, we introduce MOFClassifier, a novel machine learning approach built
upon a positive-unlabeled crystal graph convolutional neural network (PU-CGCNN)
model. MOFClassifier learns intricate patterns from perfect crys-tal structures
to predict a crystal-likeness score (CLscore), effectively classifying MOFs as
computation-ready. Our model achieves a ROC value of 0.979 (previous best
0.912) and, importantly, can identify subtle structural and chemical errors
that are fundamentally undetectable by current rule-based methods. By
accurately recovering previously misclassified false-negative structures,
MOFClassifier reduces the risk of overlooking promising material candidates in
large-scale computational screening efforts. This user-friendly tool is freely
available and has been integrated into the preparation workflow for the updated
CoRE MOF DB 2025 v1, contributing to accelerated computational discovery of MOF
materials.

</details>


### [57] [Understanding multi-fidelity training of machine-learned force-fields](https://arxiv.org/abs/2506.14963)
*John L. A. Gardner,Hannes Schulz,Jean Helie,Lixin Sun,Gregor N. C. Simm*

Main category: physics.chem-ph

TL;DR: The paper compares two multi-fidelity training strategies for MLFFs: pre-training/fine-tuning and multi-headed training, highlighting their strengths and trade-offs.


<details>
  <summary>Details</summary>
Motivation: To improve the applicability of MLFFs across diverse chemical systems by effectively leveraging data from multiple quantum-chemical methods.

Method: Systematically investigates pre-training/fine-tuning and multi-headed training strategies, analyzing their mechanisms and adaptability.

Result: Pre-training requires backbone adaptation, while multi-headed models learn method-agnostic representations but slightly compromise accuracy.

Conclusion: Multi-headed training offers extensibility and cost-efficiency, advancing the development of universal MLFFs.

Abstract: Effectively leveraging data from multiple quantum-chemical methods is
essential for building machine-learned force fields (MLFFs) that are applicable
to a wide range of chemical systems. This study systematically investigates two
multi-fidelity training strategies, pre-training/fine-tuning and multi-headed
training, to elucidate the mechanisms underpinning their success. We identify
key factors driving the efficacy of pre-training followed by fine-tuning, but
find that internal representations learned during pre-training are inherently
method-specific, requiring adaptation of the model backbone during fine-tuning.
Multi-headed models offer an extensible alternative, enabling simultaneous
training on multiple fidelities. We demonstrate that a multi-headed model
learns method-agnostic representations that allow for accurate predictions
across multiple label sources. While this approach introduces a slight accuracy
compromise compared to sequential fine-tuning, it unlocks new cost-efficient
data generation strategies and paves the way towards developing universal
MLFFs.

</details>


### [58] [How CO Affects the Composition of Titan's Tholins Generated with ECR Plasma](https://arxiv.org/abs/2506.14841)
*Zhengbo Yang,Yu Liu,Chao He,Pengcheng Yu,Rong Jin,Xiangqun Liu,Jinpu Zhang,Jiuhou Lei*

Main category: physics.chem-ph

TL;DR: The study investigates how CO affects Titan's haze formation, showing it enriches organic compound diversity by enhancing oxygen and nitrogen reactivity.


<details>
  <summary>Details</summary>
Motivation: Understanding the role of CO in Titan's atmospheric chemistry, particularly its impact on organic matter synthesis, is unclear and scientifically significant.

Method: CO was added to N2/CH4 mixtures (0.2%-9%) and exposed to ECR plasma. Gas-phase and solid products were analyzed using optical emission spectroscopy, IR spectroscopy, and mass spectrometry.

Result: CO increases chemical complexity, supplying oxygen and boosting nitrogen reactivity, leading to more diverse and abundant organic products.

Conclusion: CO plays a key role in enriching Titan's atmospheric chemistry, influencing haze formation and organic compound production.

Abstract: Titan's atmosphere possesses thick haze layers, but their formation
mechanisms remain poorly understood, including the influence of
oxygen-containing gas components on organic matter synthesis. As the most
abundant oxygen-containing gas, the presence of CO has been found to exert a
significant impact on the generation of oxygen-containing organic compounds.
Therefore, investigating the influence of CO on the production and composition
of Tholins through laboratory simulations, holds profound scientific
significance in the context of Titan. The work presented here is an
experimental simulation designed to evaluate the impact of CO on the
atmospheric chemistry of Titan. To this end, CO was introduced into the
standard N2/CH4 mixture at varying mixing ratios from 0.2% to 9%, and exposed
to Electron Cyclotron Resonance (ECR) plasma to initiate photochemical
reactions. Optical emission spectroscopy was employed for gas-phase in situ
characterization, while infrared spectroscopy and high-resolution mass
spectrometry were used to analyze the resulting solid products (tholins). Our
results demonstrate that the addition of CO enriches the complexity of the
chemical system. CO not only supplies oxygen to the system, but also enhances
nitrogen's reactivity and incorporation, enhancing the number and quantity of
the organic products.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [59] [Simulation of parametrized cardiac electrophysiology in three dimensions using physics-informed neural networks](https://arxiv.org/abs/2506.15405)
*Roshan Antony Gomez,Julien Stöcker,Barış Cansız,Michael Kaliske*

Main category: cs.CE

TL;DR: The paper explores optimizing hyperparameters for Physics-Informed Neural Networks (PINNs) to predict 3D cardiac electrophysiology using the Aliev-Panfilov model, comparing results with finite element simulations.


<details>
  <summary>Details</summary>
Motivation: To improve the accuracy and efficiency of PINNs in modeling cardiac electrophysiology by determining optimal hyperparameters and loss control methods.

Method: Uses a fully-connected neural network (FCNN) with strong-form PDEs, scaled inputs, and finite element-generated training data. Studies loss weighting effects.

Result: Trained models predict action potential and recovery variables, compared with FE simulations, showing the impact of loss control on training and predictions.

Conclusion: The study provides insights into optimizing PINNs for 3D cardiac modeling, emphasizing loss control and hyperparameter tuning for better performance.

Abstract: Physics-informed neural networks (PINNs) are extensively used to represent
various physical systems across multiple scientific domains. The same can be
said for cardiac electrophysiology, wherein fully-connected neural networks
(FCNNs) have been employed to predict the evolution of an action potential in a
2D space following the two-parameter phenomenological Aliev-Panfilov (AP)
model. In this paper, the training behaviour of PINNs is investigated to
determine optimal hyperparameters to predict the electrophysiological activity
of the myocardium in 3D according to the AP model, with the inclusion of
boundary and material parameters. An FCNN architecture is employed with the
governing partial differential equations in their strong form, which are scaled
consistently with normalization of network inputs. The finite element (FE)
method is used to generate training data for the network. Numerical examples
with varying spatial dimensions and parameterizations are generated using the
trained models. The network predicted fields for both the action potential and
the recovery variable are compared with the respective FE simulations. Network
losses are weighed with individual scalar values. Their effect on training and
prediction is studied to arrive at a method of controlling losses during
training.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [60] [Machine learning based prediction of dynamical clustering in granular gases](https://arxiv.org/abs/2506.15657)
*Sai Preetham Sata,Ralf Stannarius,Benjamin Noack,Dmitry Puzyrev*

Main category: cond-mat.soft

TL;DR: Dense granular gases under microgravity form clusters due to energy balance. DEM simulations and machine learning predict clustering in frictional spheres.


<details>
  <summary>Details</summary>
Motivation: Understand the gas-cluster transition in dense granular gases under microgravity, influenced by energy influx, dissipation, and system parameters.

Method: Use DEM simulations for frictional spheres in a cuboid container, apply Kolmogorov-Smirnov test and caging criterion, and employ machine learning models.

Result: Identified best machine learning models to predict dynamical clustering without complex DEM simulations.

Conclusion: Machine learning offers an efficient alternative to DEM for predicting gas-cluster transitions in specific geometries.

Abstract: When dense granular gases are continuously excited under microgravity
conditions, spatial inhomogeneities of the particle number density can emerge.
A significant share of particles may collect in strongly overpopulated regions,
called clusters. This dynamical clustering, or gas-cluster transition, is
caused by a complex interplay and balance between the energy influx and
dissipation in particle collisions. Particle number density, container
geometry, and excitation strength influence this transition. We perform
Discrete Element Method (DEM) simulations for ensembles of frictional spheres
in a cuboid container and apply the Kolmogorov Smirnov test and a caging
criterion to the local packing fraction profiles to detect clusters. Machine
learning can be used to study the gas-cluster transition, and can be a
promising alternative to identify the state of the system for a given set of
system parameters without time-consuming complex DEM simulations. We test
various machine learning models and identify the best models to predict
dynamical clustering of frictional spheres in a specific experimental geometry.

</details>
