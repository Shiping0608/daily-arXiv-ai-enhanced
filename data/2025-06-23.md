<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 20]
- [math.AP](#math.AP) [Total: 28]
- [physics.comp-ph](#physics.comp-ph) [Total: 5]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 11]
- [astro-ph.SR](#astro-ph.SR) [Total: 2]
- [math.OC](#math.OC) [Total: 2]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 2]
- [astro-ph.GA](#astro-ph.GA) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [gr-qc](#gr-qc) [Total: 2]
- [cs.LG](#cs.LG) [Total: 2]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [cond-mat.quant-gas](#cond-mat.quant-gas) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.CV](#cs.CV) [Total: 1]
- [math.AG](#math.AG) [Total: 1]
- [stat.CO](#stat.CO) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [astro-ph.HE](#astro-ph.HE) [Total: 1]
- [math.CA](#math.CA) [Total: 1]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 4]
- [physics.acc-ph](#physics.acc-ph) [Total: 1]
- [cs.MS](#cs.MS) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Convergent Methods for Koopman Operators on Reproducing Kernel Hilbert Spaces](https://arxiv.org/abs/2506.15782)
*Nicolas Boullé,Matthew J. Colbrook,Gustav Conradie*

Main category: math.NA

TL;DR: The paper introduces data-driven algorithms for spectral analysis of Koopman operators on RKHS, offering advantages like pointwise predictions, improved spectral properties, and efficiency in high dimensions. The methods are proven optimal and tested on real-world datasets.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of traditional $L^2$ methods for Koopman operator analysis, the paper aims to leverage RKHS for better computational efficiency, flexibility, and accuracy in spectral analysis.

Method: The authors develop general, provably convergent algorithms for computing spectral properties of Koopman and Perron--Frobenius operators on RKHS, exploiting kernel structure to avoid large-data limits.

Result: The algorithms efficiently compute spectra, pseudospectra, and spectral measures, demonstrating effectiveness on high-dimensional datasets like turbulent flow and molecular dynamics.

Conclusion: The proposed methods are optimal and outperform traditional approaches, with practical applications showcased in diverse real-world scenarios. The software package $	exttt{SpecRKHS}$ is made publicly available.

Abstract: Data-driven spectral analysis of Koopman operators is a powerful tool for
understanding numerous real-world dynamical systems, from neuronal activity to
variations in sea surface temperature. The Koopman operator acts on a function
space and is most commonly studied on the space of square-integrable functions.
However, defining it on a suitable reproducing kernel Hilbert space (RKHS)
offers numerous practical advantages, including pointwise predictions with
error bounds, improved spectral properties that facilitate computations, and
more efficient algorithms, particularly in high dimensions. We introduce the
first general, provably convergent, data-driven algorithms for computing
spectral properties of Koopman and Perron--Frobenius operators on RKHSs. These
methods efficiently compute spectra and pseudospectra with error control and
spectral measures while exploiting the RKHS structure to avoid the large-data
limits required in the $L^2$ settings. The function space is determined by a
user-specified kernel, eliminating the need for quadrature-based sampling as in
$L^2$ and enabling greater flexibility with finite, externally provided
datasets. Using the Solvability Complexity Index hierarchy, we construct
adversarial dynamical systems for these problems to show that no algorithm can
succeed in fewer limits, thereby proving the optimality of our algorithms.
Notably, this impossibility extends to randomized algorithms and datasets. We
demonstrate the effectiveness of our algorithms on challenging,
high-dimensional datasets arising from real-world measurements and
high-fidelity numerical simulations, including turbulent channel flow,
molecular dynamics of a binding protein, Antarctic sea ice concentration, and
Northern Hemisphere sea surface height. The algorithms are publicly available
in the software package $\texttt{SpecRKHS}$.

</details>


### [2] [Tree-based adaptive finite element methods for deformable image registration](https://arxiv.org/abs/2506.15876)
*Nicolás A. Barnafi,Alberto F. Martın,Ricardo Ruiz-Baier*

Main category: math.NA

TL;DR: An adaptive FEM for Deformable Image Registration (DIR) with a residual-based error estimator, guiding AMR. Solves nonlinear equations using pseudo time-stepping and Anderson Acceleration, implemented via forests-of-octrees.


<details>
  <summary>Details</summary>
Motivation: To improve accuracy and efficiency in DIR by combining adaptive FEM, error estimation, and advanced solvers.

Method: Proposes an adaptive FEM with residual-based error estimator for DIR, using AMR. Solves nonlinear equations with pseudo time-stepping and Anderson Acceleration, implemented via forests-of-octrees.

Result: Demonstrates performance in application-oriented DIR problems.

Conclusion: The method enhances DIR accuracy and efficiency through adaptive techniques and advanced solvers.

Abstract: In this work we propose an adaptive Finite Element Method (FEM) formulation
for the Deformable Image Registration problem (DIR) together with a
residual-based a posteriori error estimator, whose efficiency and reliability
are theoretically established. This estimator is used to guide Adaptive Mesh
Refinement and coarsening (AMR). The nonlinear Euler-Lagrange equations
associated with the minimisation of the relevant functional are solved with a
pseudo time-stepping fixed-point scheme which is further accelerated using
Anderson Acceleration (AA). The efficient implementation of these solvers
relies on an efficient adaptive mesh data structure based on forests-of-octrees
endowed with space-filling-curves. Several numerical results illustrate the
performance of the proposed methods applied to adaptive DIR in
application-oriented problems.

</details>


### [3] [Reactive Transport Modeling with Physics-Informed Machine Learning for Critical Minerals Applications](https://arxiv.org/abs/2506.15960)
*K. Adhikari,Md. Lal Mamud,M. K. Mudunuru,K. B. Nakshatrala*

Main category: math.NA

TL;DR: A physics-informed neural network (PINN) framework is introduced for modeling reactive transport in porous media, focusing on fast bimolecular reactions.


<details>
  <summary>Details</summary>
Motivation: Accurate simulation of chemical interactions and product formation in subsurface environments is crucial for geoscience applications like mineral extraction.

Method: The study employs a physics-informed neural network (PINN) framework for reactive transport modeling.

Result: Not explicitly stated in the abstract.

Conclusion: The framework aims to improve simulations for geoscience applications, particularly in mineral extraction.

Abstract: This study presents a physics-informed neural network (PINN) framework for
reactive transport modeling for simulating fast bimolecular reactions in porous
media. Accurate characterization of chemical interactions and product formation
in surface and subsurface environments is essential for advancing critical
mineral extraction and related geoscience applications.

</details>


### [4] [Preconditioning and Linearly Implicit Time Integration for the Serre-Green-Naghdi Equations](https://arxiv.org/abs/2506.16045)
*Linwan Feng,David Shirokoff,Wooyoung Choi*

Main category: math.NA

TL;DR: The paper introduces a constant coefficient preconditioner for the Serre-Green-Naghdi (SGN) equations, proving rigorous bounds on its conditioning number. It also presents two time integration strategies for solving the SGN equations, validated through computational experiments.


<details>
  <summary>Details</summary>
Motivation: The key challenge in solving the SGN equations is the treatment of the differential PDE constraint. This work aims to address this by developing an efficient preconditioner and time integration methods.

Method: A constant coefficient preconditioner is introduced for the SGN constraint operator, with rigorous conditioning bounds. Two time integration strategies are devised: one combining explicit schemes with the preconditioner, and another using a linearly implicit scheme.

Result: The preconditioner shows quasi-optimal conditioning bounds and mesh-independent performance. Computational experiments validate its robustness and effectiveness in solving the SGN equations, including complex scenarios like solitary waves.

Conclusion: The proposed preconditioner and time integration methods effectively address the challenges of the SGN equations, demonstrating robustness and efficiency in numerical solutions.

Abstract: The treatment of the differential PDE constraint poses a key challenge in
computing the numerical solution of the Serre-Green-Naghdi (SGN) equations. In
this work, we introduce a constant coefficient preconditioner for the SGN
constraint operator and prove rigorous bounds on the preconditioned
conditioning number. The conditioning bounds incorporate the effects of
bathymetry in two dimensions, are quasi-optimal within a class of constant
coefficient operators, highlight fundamental scalings for a loss of
conditioning, and ensure mesh independent performance for iterative Krylov
methods.
  Utilizing the conditioning bounds, we devise and test two time integration
strategies for solving the full SGN equations. The first class combines
classical explicit time integration schemes (4th order Runge-Kutta and 2nd--4th
order Adams-Bashforth) with the new preconditioner. The second is a linearly
implicit scheme where the differential constraint is split into a constant
coefficient implicit part and remaining (stiff) explicit part. The linearly
implicit methods require a single linear solve of a constant coefficient
operator at each time step. We provide a host of computational experiments that
validate the robustness of the preconditioners, as well as full solutions of
the SGN equations including solitary waves traveling over an underwater shelf
(in 1d) and a circular bump (in 2d).

</details>


### [5] [General-domain FC-based shock-dynamics solver I: Basic elements](https://arxiv.org/abs/2506.16076)
*Oscar P. Bruno,Daniel V. Leibovici*

Main category: math.NA

TL;DR: A general-domain FC-SDNN spectral scheme for nonlinear conservation laws is introduced, featuring multi-patch domain decomposition and shock detection via neural networks, with applications in high-speed flow simulations.


<details>
  <summary>Details</summary>
Motivation: To extend the FC-SDNN method to general domains with arbitrary boundary conditions, enabling accurate spectral solutions for nonlinear conservation laws without problem-dependent parameters.

Method: Uses Fourier Continuation for non-periodic functions, a Shock-Detecting Neural Network (SDNN) for localized artificial viscosity, and introduces a multi-patch/subpatch domain decomposition strategy for complex domains.

Result: Demonstrates effectiveness through numerical tests, including simulations of supersonic and hypersonic flows at Mach 25 speeds, without requiring problem-specific tuning.

Conclusion: The FC-SDNN algorithm is geometrically flexible, efficiently parallelized, and applicable to complex domains, with Part II extending it to non-smooth boundaries.

Abstract: This contribution, Part I in a two-part article series, presents a
general-domain version of the FC-SDNN (Fourier Continuation Shock-detecting
Neural Network) spectral scheme for the numerical solution of nonlinear
conservation laws, which is applicable under arbitrary boundary conditions and
in general domains. Like the previous simple-domain contribution (Journal of
Computational Physics X 15, (2022)), the present approach relies on the use of
the Fourier Continuation method for accurate spectral representation of
non-periodic functions in conjunction with smooth artificial viscosity
assignments localized in regions detected by means of a Shock-Detecting Neural
Network (SDNN). Relying on such techniques, the present Part I paper introduces
a novel multi-patch/subpatch artificial viscosity-capable domain decomposition
strategy for complex domains with smooth boundaries, and it illustrates the
methodology by means of a variety of computational results produced by an
associated parallel implementation of the resulting shock-capturing algorithm
in a present-day computing cluster. The subsequent Part II contribution then
extends the algorithm to enable treatment of obstacles with non-smooth
boundaries, it considers questions concerning parallelization and accuracy, and
it presents comparisons with physical theory and prior experimental and
computational results. The resulting multi-patch FC-SDNN algorithm does not
require use of problem-dependent algorithmic parameters or
positivity-preserving limiters, and, on account of its use of an
overlapping-patch discretization, it is geometrically flexible and efficiently
parallelized. A variety of numerical tests for the 2D Euler equations are
presented, including the simulation of supersonic and hypersonic flows and
shocks past physical obstacles at high speeds, such as Mach 25 re-entry flow
speeds.

</details>


### [6] [Two-dimensional greedy randomized extended Kaczmarz methods](https://arxiv.org/abs/2506.16106)
*Xin-Fang Zhang,Meng-Long Xiao,Tao Li*

Main category: math.NA

TL;DR: The paper introduces a novel two-dimensional greedy randomized extended Kaczmarz method for solving large linear least-squares problems, improving upon the traditional randomized approach by selecting rows and columns more strategically.


<details>
  <summary>Details</summary>
Motivation: The traditional randomized extended Kaczmarz method's random selection strategy is inefficient compared to greedy methods, prompting the development of a more effective approach.

Method: The proposed method selects two rows and columns per iteration based on larger residual vector entries, with further improvements via semi-randomized and modified versions for big data.

Result: Numerical results show the proposed methods outperform state-of-the-art randomized extended Kaczmarz methods, particularly in computing time.

Conclusion: The new methods offer superior performance for solving large linear least-squares problems, especially in big data contexts.

Abstract: The randomized extended Kaczmarz method, proposed by Zouzias and Freris (SIAM
J. Matrix Anal. Appl. 34: 773-793, 2013), is appealing for solving
least-squares problems. However, its randomly selecting rows and columns of A
with probability proportional to their squared norm is unattractive compared to
the greedy strategy. In this paper, we first consider a novel two-dimensional
greedy randomized extended Kaczmarz method for solving large linear
least-squares problems. The proposed method randomly selects two rows and two
columns of A by grasping two larger entries in the magnitude of the
corresponding residual vector per iteration. To improve its convergence, we
then propose a two-dimensional semi-randomized extended Kaczmarz method and its
modified version with simple random sampling, which is particularly favorable
for big data problems. The convergence analysis of which is also established.
Numerical results on some practical applications illustrate the superiority of
the proposed methods compared with state-of-the-art randomized extended
Kaczmarz methods, especially in terms of computing time.

</details>


### [7] [Monolithic and Block Overlapping Schwarz Preconditioners for the Incompressible Navier--Stokes Equations](https://arxiv.org/abs/2506.16179)
*Alexander Heinlein,Axel Klawonn,Jascha Knepper,Lea Saßmannshausen*

Main category: math.NA

TL;DR: Monolithic preconditioners outperform block preconditioners for incompressible Navier-Stokes equations, with OSM and GDSW-type coarse spaces enhancing robustness and scalability.


<details>
  <summary>Details</summary>
Motivation: To improve robustness and reduce sensitivity to parameters in solving incompressible Navier-Stokes equations by comparing monolithic and block preconditioners.

Method: Uses two-level additive overlapping Schwarz methods (OSM) with GDSW-type coarse spaces, implemented in FROSch. Introduces GDSW* and compares with PCD, SIMPLE, and LSC block preconditioners.

Result: Monolithic preconditioners show lower iterations and better robustness across Reynolds and CFL numbers.

Conclusion: Monolithic preconditioners, especially with OSM and tailored coarse spaces, are robust and scalable for fluid flow problems.

Abstract: Monolithic preconditioners applied to the linear systems arising during the
solution of the discretized incompressible Navier--Stokes equations are
typically more robust than preconditioners based on incomplete block
factorizations. Lower number of iterations and a reduced sensitivity to
parameters like velocity and viscosity can significantly outweigh the
additional cost for their setup. Different monolithic preconditioning
techniques are introduced and compared to a selection of block preconditioners.
In particular, two-level additive overlapping Schwarz methods (OSM) are used to
set up monolithic preconditioners and to approximate the inverses arising in
the block preconditioners. GDSW-type (Generalized Dryja--Smith--Widlund) coarse
spaces are used for the second level. These highly scalable, parallel
preconditioners have been implemented in the solver framework \texttt{FROSch}
(Fast and Robust Overlapping Schwarz), which is part of the software library
\texttt{Trilinos}. The new GDSW-type coarse space GDSW\expStar{} is introduced;
combining it with other techniques results in a robust algorithm. The block
preconditioners PCD (Pressure Convection--Diffusion), SIMPLE (Semi-Implicit
Method for Pressure Linked Equations), and LSC (Least-Squares Commutator) are
considered to various degrees. The OSM for the monolithic as well as the block
approach allows the optimized combination of different coarse spaces for the
velocity and pressure component, enabling the use of tailored coarse spaces.
The numerical and parallel performance of the different preconditioning methods
for finite element discretizations of stationary as well as time-dependent
incompressible fluid flow problems is investigated and compared. Their
robustness is analyzed for a range of Reynolds and Courant-Friedrichs-Lewy
(CFL) numbers with respect to a realistic problem setting.

</details>


### [8] [From eigenvector nonlinearities to eigenvalue nonlinearities](https://arxiv.org/abs/2506.16182)
*Elias Jarlebring,Vilhelm P. Lithell*

Main category: math.NA

TL;DR: The paper introduces a transformation to reframe eigenvalue problems with eigenvector nonlinearities (NEPv) into eigenvalue problems with eigenvalue nonlinearities (NEP), using polynomial systems and proposing solution methods.


<details>
  <summary>Details</summary>
Motivation: To simplify and solve eigenvalue problems with eigenvector nonlinearities by transforming them into more tractable eigenvalue nonlinearities.

Method: Constructs a transformation using polynomial systems, proposes solving methods including a multiparameter eigenvalue problem (MEP), and adapts NEP solvers like deflation and iterative methods.

Result: Demonstrates efficiency by solving a modified Gross-Pitaevskii equation (GPE) problem, with reproducible simulations.

Conclusion: The transformation and methods effectively address NEPv problems, offering practical solutions with demonstrated success.

Abstract: Over the past decades, transformations between different classes of
eigenvalue problems have played a central role in the development of numerical
methods for eigenvalue computations. One of the most well-known and successful
examples of this is the companion linearization. In this paper, we construct a
transformation that equivalently re-frames a specific type of eigenvalue
problem with eigenvector nonlinearities (NEPv) into an eigenvalue problem with
eigenvalue nonlinearities (NEP). The NEPv class considered consists of
nonlinearities expressed as sums of products of matrices and scalar functions,
where the scalar functions depend nonlinearly on the eigenvector. Our
transformation defines the scalar nonlinearities through a polynomial system,
resulting in NEP nonlinearities of algebraic type. We propose methods to solve
the polynomial system, one involving a multiparameter eigenvalue problem (MEP).
We adapt well-established NEP solvers to this setting, with the most effective
strategy being a combination of deflation and a locally quadratically
convergent iterative method. The efficiency and properties of the approach is
illustrated by solving a problem related to a modification of a
Gross-Pitaevskii equation (GPE). The simulations are reproducible and publicly
available.

</details>


### [9] [A third-order finite volume semi-implicit method for the Shallow Water-Exner model](https://arxiv.org/abs/2506.16287)
*Enrique D. Fernandez-Nieto,Jose Garres-Diaz,Emanuele Macca,Giovanni Russo*

Main category: math.NA

TL;DR: Third-order semi-implicit schemes for shallow water and Saint-Venant-Exner systems are introduced, reducing computational effort by focusing on velocity-dependent stability. Key innovations include third-order pressure gradient approximation and a novel semi-analytical solution.


<details>
  <summary>Details</summary>
Motivation: To improve computational efficiency and accuracy in subcritical flow simulations, especially for slow bedload processes.

Method: Uses third-order IMEX time discretization, CWENO reconstruction for water thickness, and fourth-degree polynomial interpolation for free surface. Introduces a semi-analytical solution for comparison.

Result: Achieves third-order accuracy in tests, maintains well-balance, and effectively simulates slow bedload processes.

Conclusion: The proposed schemes are efficient and accurate, particularly for subcritical flows and long-term simulations.

Abstract: In this work, third-order semi-implicit schemes on staggered meshes for the
shallow water and Saint-Venant-Exner systems are presented. They are based on a
third-order extension of the technique introduced in Cassulli \& Cheng [1]. The
stability conditions for these schemes depend on the velocity and not on the
celerity, allowing us to reduce computational efforts, especially in
subcritical flow simulations, which is the regime we are mainly interested in.
The main novelty consists in the third-order approximation of the pressure
gradient term in the momentum equation through appropriate polynomial
reconstructions. Concretely, CWENO conservative reconstruction is considered
for the water thickness $h$ and a centered fourth-degree polynomial is adopted
interpolating the cell averages of the free surface $\eta$. For time
discretization, a third-order IMEX scheme is applied. In addition, a novel
time-dependent semi-analytical solution for Saint-Venant-Exner system is
introduced and compared with the numerical ones. Several tests are performed,
including accuracy tests showing third-order accuracy, well-balance tests, and
simulations of slow bedload processes for large time.

</details>


### [10] [Quasiseparable LU decay bounds for inverses of banded matrices](https://arxiv.org/abs/2506.16339)
*Paola Boito,Yuli Eidelman*

Main category: math.NA

TL;DR: New exponential decay bounds for inverses of banded matrices, leveraging quasiseparable representation and diagonal dominance, without needing spectral info.


<details>
  <summary>Details</summary>
Motivation: To provide easily computable bounds for matrix inverses, especially useful for nonsymmetric or symmetric indefinite matrices.

Method: Uses quasiseparable representation of Green matrices and relies on diagonal dominance.

Result: Numerical experiments show the bounds are advantageous for nonsymmetric or symmetric indefinite matrices.

Conclusion: The new bounds are effective and practical for certain matrix types without requiring spectral details.

Abstract: We develop new, easily computable exponential decay bounds for inverses of
banded matrices, based on the quasiseparable representation of Green matrices.
The bounds rely on a diagonal dominance hypothesis and do not require explicit
spectral information. Numerical experiments and comparisons show that these new
bounds can be advantageous especially for nonsymmetric or symmetric indefinite
matrices.

</details>


### [11] [On a quantitative partial imaging problem in vector tomography](https://arxiv.org/abs/2506.16455)
*Hiroshi Fujiwara,Kamran Sadiq,Alexandru Tamasan*

Main category: math.NA

TL;DR: Reconstructing a 2D vector field from partial zeroth and first moment ray transforms on lines intersecting a given arc. The method recovers the field in the arc's convex hull, with numerical experiments showing stabilization despite ill-posedness.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of reconstructing vector fields from limited data (partial moments on specific lines), a non-local and severely ill-posed problem.

Method: Develop a reconstruction method for the convex hull of the arc, implemented via an algorithm tested in numerical experiments.

Result: The method successfully recovers the vector field in the convex hull, with discretization stabilizing the reconstruction despite inherent ill-posedness.

Conclusion: The proposed method provides a viable solution for partial data reconstruction, though the problem remains ill-posed.

Abstract: In two dimensions, we consider the problem of reconstructing a vector field
from partial knowledge of its zeroth and first moment ray transforms. Different
from existing works the data is known on a subset of lines, namely the ones
intersecting a given arc. The problem is non-local and, for partial data,
severely ill-posed. We present a reconstruction method which recovers the
vector field in the convex hull of the arc. An algorithm based on this method
is implemented on some numerical experiments. While still ill-posed the
discretization stabilizes the numerical reconstruction.

</details>


### [12] [Scientific Applications Leveraging Randomized Linear Algebra](https://arxiv.org/abs/2506.16457)
*Vivak Patel,D. Adrian Maldonado,Maksim Melnichenko,Nathaniel Pritchard,Vishwas Rao,Elizaveta Rebrova,Sriram Sankararaman*

Main category: math.NA

TL;DR: The report highlights Randomized Numerical Linear Algebra (RNLA) as a solution for large-scale linear algebra challenges in imaging, genomics, and time-varying systems, addressing bottlenecks like memory constraints and computational costs. It outlines future directions and invites domain scientists to engage with RNLA.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the computational challenges of large-scale linear algebra in scientific applications, where traditional methods face bottlenecks like memory and latency issues.

Method: The method involves high-level discussion of RNLA routines, demonstrating their ability to overcome traditional linear algebra limitations in applications like imaging and genomics.

Result: RNLA is shown to effectively tackle large-scale matrix problems, with open challenges identified in algorithm design, hardware co-design, and software infrastructure.

Conclusion: The report serves as both an invitation for domain scientists to adopt RNLA and a guide for future research, emphasizing real-world applications and unresolved challenges.

Abstract: This report showcases the role of, and future directions for, the field of
Randomized Numerical Linear Algebra (RNLA) in a selection of scientific
applications. These applications span the domains of imaging, genomics and
time-varying systems, and are thematically connected by needing to perform
linear algebra routines on large-scale matrices (with up to quantillions of
entries). At such scales, the linear algebra routines face typical bottlenecks:
memory constraints, data access latencies, and substantial floating-point
operation costs. RNLA routines are discussed at a high level to demonstrate how
RNLA is able to solve the challenges faced by traditional linear algebra
routines, and, consequently, address the computational problem posed in the
underlying application. For each application, RNLA's open challenges and
possible future directions are also presented, which broadly fall into the
categories: creating structure-aware RNLA algorithms; co-designing RNLA
algorithms with hardware and mixed-precision considerations; and advancing
modular, composable software infrastructure. Ultimately, this report serves two
purposes: it invites domain scientists to engage with RNLA; and it offers a
guide for future RNLA research grounded in real applications.

</details>


### [13] [IMEX-RB: a self-adaptive IMEX time integration scheme exploiting the RB method](https://arxiv.org/abs/2506.16470)
*Micol Bassanini,Simone Deparis,Francesco Sala,Riccardo Tenderini*

Main category: math.NA

TL;DR: IMEX-RB is a self-adaptive implicit-explicit scheme for ODEs from PDEs, using Reduced Basis for dynamic subspace projection and integration, outperforming backward Euler in accuracy and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional RB methods, which require parametrization and offline-online splitting, by dynamically building the reduced subspace using solution history.

Method: IMEX-RB projects the high-fidelity problem onto a low-dimensional subspace implicitly, then uses the result for a full-order explicit step, dynamically adapting the subspace.

Result: IMEX-RB is unconditionally stable under certain hyperparameter conditions, outperforms backward Euler in accuracy, and achieves computational gains for timesteps above the forward Euler stability threshold.

Conclusion: IMEX-RB is a promising method for efficient and accurate time integration of PDE-derived ODEs, provided hyperparameters like reduced basis size and stability tolerance are properly tuned.

Abstract: In this work, we introduce a self-adaptive implicit-explicit (IMEX) time
integration scheme, named IMEX-RB, for the numerical integration of systems of
ordinary differential equations (ODEs), arising from spatial discretizations of
partial differential equations (PDEs) by finite difference methods. Leveraging
the Reduced Basis (RB) method, at each timestep we project the high-fidelity
problem onto a suitable low-dimensional subspace and integrate its dynamics
implicitly. Following the IMEX paradigm, the resulting solution then serves as
an educated guess within a full-order explicit step. Notably, compared to the
canonical RB method, IMEX-RB neither requires a parametrization of the
underlying PDE nor features an offline-online splitting, since the reduced
subspace is built dynamically, exploiting the high-fidelity solution history.
We present the first-order formulation of IMEX-RB, demonstrating and showcasing
its convergence and stability properties. In particular, under appropriate
conditions on the method's hyperparameters, IMEX-RB is unconditionally stable.
The theoretical analysis is corroborated by numerical experiments performed on
representative model problems in two and three dimensions. The results
demonstrate that our approach can outperform conventional time integration
schemes like backward Euler. Indeed, IMEX-RB yields high-fidelity accurate
solutions, provided that its main hyperparameters - namely the reduced basis
size and the stability tolerance - are suitably tuned. Moreover, IMEX-RB
realizes computational gains over backward Euler for a range of timestep sizes
above the forward Euler stability threshold.

</details>


### [14] [Do high-order Gauss-Legendre methods admit a composition representation and a conjugate-symplectic counterpart?](https://arxiv.org/abs/2506.16809)
*Felice Iavernaro,Francesca Mazzia*

Main category: math.NA

TL;DR: The paper explores whether higher-order Gauss--Legendre methods, like the fourth-order method, share a composition structure similar to the Midpoint and Trapezoidal rules.


<details>
  <summary>Details</summary>
Motivation: The question arises from interpreting the Midpoint method and Trapezoidal rule as compositions of Implicit and Explicit Euler methods.

Method: The authors investigate and confirm this composition structure for the fourth-order Gauss--Legendre method, also deriving high-order dense output.

Result: A positive answer is provided for the fourth-order method, alongside the derivation of high-order dense output.

Conclusion: The study confirms the composition structure for higher-order methods and extends the technique to derive additional results.

Abstract: One of the most classical pairs of symplectic and conjugate-symplectic
schemes is given by the Midpoint method (the Gauss--Runge--Kutta method of
order 2) and the Trapezoidal rule. These can be interpreted as compositions of
the Implicit and Explicit Euler methods, taken in direct and reverse order,
respectively. This naturally raises the question of whether a similar
composition structure exists for higher-order Gauss--Legendre methods. In this
paper, we provide a positive answer in the case of the fourth-order method. The
technique we employ also enables the derivation of a high-order dense output.

</details>


### [15] [Comparison of substructured non-overlapping domain decomposition and overlapping additive Schwarz methods for large-scale Helmholtz problems with multiple sources](https://arxiv.org/abs/2506.16875)
*Boris Martin,Pierre Jolivet,Christophe Geuzaine*

Main category: math.NA

TL;DR: Comparison of non-overlapping substructured DDM and ORAS preconditioners for solving large-scale Helmholtz problems, showing non-overlapping methods outperform overlapping ones when tuned properly.


<details>
  <summary>Details</summary>
Motivation: Large-scale Helmholtz problems are challenging to solve, especially in 3D, due to high computational costs and convergence issues with iterative methods.

Method: Domain decomposition methods (DDM) are used, comparing non-overlapping substructured DDM and Optimized Restricted Additive Schwarz (ORAS) preconditioners.

Result: Non-overlapping methods, when tuned, reduce convergence gaps and outperform overlapping methods in a realistic geophysical test-case.

Conclusion: Non-overlapping substructured DDM is a promising approach for solving large-scale Helmholtz problems with multiple sources.

Abstract: Solving large-scale Helmholtz problems discretized with high-order finite
elements is notoriously difficult, especially in 3D where direct factorization
of the system matrix is very expensive and memory demanding, and robust
convergence of iterative methods is difficult to obtain. Domain decomposition
methods (DDM) constitute one of the most promising strategy so far, by
combining direct and iterative approaches: using direct solvers on overlapping
or non-overlapping subdomains, as a preconditioner for a Krylov subspace method
on the original Helmholtz system or as an iterative solver on a substructured
problem involving field values or Lagrange multipliers on the interfaces
between the subdomains. In this work we compare the computational performance
of non-overlapping substructured DDM and Optimized Restricted Additive Schwarz
(ORAS) preconditioners for solving large-scale Helmholtz problems with multiple
sources, as is encountered, e.g., in frequency-domain Full Waveform Inversion.
We show on a realistic geophysical test-case that, when appropriately tuned,
the non-overlapping methods can reduce the convergence gap sufficiently to
significantly outperform the overlapping methods.

</details>


### [16] [Uncertainty Quantification for Linear Inverse Problems with Besov Prior: A Randomize-Then-Optimize Method](https://arxiv.org/abs/2506.16888)
*Andreas Horst,Babak Maboudi Afkham,Yiqiu Dong,Jakob Lemvig*

Main category: math.NA

TL;DR: The paper explores Besov priors in Bayesian inverse problems, proposing a method for sampling and estimating posterior modes, validated through numerical experiments.


<details>
  <summary>Details</summary>
Motivation: To address uncertainty interpretation in Bayesian inverse problems using discretization-invariant and sparsity-promoting Besov priors.

Method: Randomize-then-optimize method for sampling from the posterior distribution with Besov priors, tested on 1D/2D problems.

Result: Effective sampling of posterior distributions with Besov priors, with detailed analysis of parameter and wavelet basis choices.

Conclusion: The proposed method is effective for posterior sampling with general Besov priors, outperforming state-of-the-art methods.

Abstract: In this work, we investigate the use of Besov priors in the context of
Bayesian inverse problems. The solution to Bayesian inverse problems is the
posterior distribution which naturally enables us to interpret the
uncertainties. Besov priors are discretization invariant and can promote
sparsity in terms of wavelet coefficients. We propose the
randomize-then-optimize method to draw samples from the posterior distribution
with Besov priors under a general parameter setting and estimate the modes of
the posterior distribution. The performance of the proposed method is studied
through numerical experiments of a 1D inpainting problem, a 1D deconvolution
problem, and a 2D computed tomography problem. Further, we discuss the
influence of the choice of the Besov parameters and the wavelet basis in
detail, and we compare the proposed method with the state-of-the-art methods.
The numerical results suggest that the proposed method is an effective tool for
sampling the posterior distribution equipped with general Besov priors.

</details>


### [17] [Magnus Methods for Stochastic Delay-Differential Equations](https://arxiv.org/abs/2506.16908)
*Mitchell T. Griggs,Kevin Burrage,Pamela M. Burrage*

Main category: math.NA

TL;DR: Magnus-based methods (MEM and MM) for SDDEs are introduced, combining stochastic Magnus integrators with Taylor methods. They show convergence and stability, outperforming traditional EM in some cases.


<details>
  <summary>Details</summary>
Motivation: To develop efficient and stable numerical methods for solving stochastic delay-differential equations (SDDEs) and stochastic partial delay-differential equations (SPDDEs).

Method: Constructed Magnus--Euler--Maruyama (MEM) and Magnus--Milstein (MM) schemes by integrating stochastic Magnus integrators with Taylor methods for SDDEs. Applied incrementally between delay times.

Result: Proved convergence orders and demonstrated rates numerically. MEM and MM schemes performed well on linear and nonlinear problems. MEM outperformed EM in stability for SPDDEs.

Conclusion: Magnus-based methods (MEM and MM) are effective and stable for SDDEs and SPDDEs, offering computational advantages over traditional methods like EM.

Abstract: This paper introduces Magnus-based methods for solving stochastic
delay-differential equations (SDDEs). We construct Magnus--Euler--Maruyama
(MEM) and Magnus--Milstein (MM) schemes by combining stochastic Magnus
integrators with Taylor methods for SDDEs. These schemes are applied
incrementally between multiples of the delay times. We present proofs of their
convergence orders and demonstrate these rates through numerical examples and
error graphs. Among the examples, we apply the MEM and MM schemes to both
linear and nonlinear problems. We also apply the MEM scheme to a stochastic
partial delay-differential equation (SPDDE), comparing its performance with the
traditional Euler--Maruyama (EM) method. Under fine spatial discretization, the
MEM scheme remains numerically stable while the EM method becomes unstable,
yielding a significant computational advantage.

</details>


### [18] [Error analysis of BDF schemes for the evolutionary incompressible Navier--Stokes equations](https://arxiv.org/abs/2506.16917)
*Bosco García-Archilla,V. John,Julia Novo*

Main category: math.NA

TL;DR: Error bounds for fully discrete schemes for the incompressible Navier-Stokes equations are derived, focusing on BDF-$q$ time integration and mixed finite elements, with and without grad-div stabilization.


<details>
  <summary>Details</summary>
Motivation: To address the lack of error bounds for BDF-$q$ methods ($q\ge3$) in the literature and to improve stability and accuracy in numerical simulations of the Navier-Stokes equations.

Method: Uses BDF-$q$ methods for time integration and inf-sup stable mixed finite elements for spatial approximation, comparing standard Galerkin and grad-div stabilized methods.

Result: Proves optimal error bounds: $O((\Delta t)^q)$ in time and $O(h^{k+1})$ or $O(h^k)$ in space for velocity, depending on stabilization.

Conclusion: Grad-div stabilization yields viscosity-independent error bounds, enhancing robustness and accuracy for high-order schemes.

Abstract: Error bounds for fully discrete schemes for the evolutionary incompressible
Navier--Stokes equations are derived in this paper. For the time integration we
apply BDF-$q$ methods, $q\le 5$, for which error bounds for $q\ge 3$ cannot be
found in the literature. Inf-sup stable mixed finite elements are used as
spatial approximation. First, we analyze the standard Galerkin method and
second a grad-div stabilized method. The grad-div stabilization allows to prove
error bounds with constants independent of inverse powers of the viscosity
coefficient. We prove optimal bounds for the velocity and pressure with order
$(\Delta t)^q$ in time for the BDF-$q$ scheme and order $h^{k+1}$ for the
$L^2(\Omega)$ error of the velocity in the first case and $h^k$ in the second
case, $k$ being the degree of the polynomials in finite element velocity space.

</details>


### [19] [Structure-preserving scheme for 1D KWC system](https://arxiv.org/abs/2506.16963)
*Makoto Okumura,Shodai Kubota,Ken Shirakawa*

Main category: math.NA

TL;DR: The paper develops a structure-preserving numerical scheme for the KWC system of parabolic PDEs, focusing on range preservation and energy dissipation, with theoretical guarantees.


<details>
  <summary>Details</summary>
Motivation: To address the need for numerical methods that preserve key structural properties (range preservation and energy dissipation) in the KWC phase-field model for grain boundary motion.

Method: Constructs a numerical scheme under suitable assumptions, verifying structural properties, convergence conditions, and providing error estimates.

Result: The scheme successfully preserves structural properties and includes theoretical analysis of convergence and error estimates.

Conclusion: The proposed numerical scheme is effective for the KWC system, maintaining structural integrity and providing reliable computational results.

Abstract: In this paper, we consider a system of one-dimensional parabolic PDEs, known
as the KWC system, as a phase-field model for grain boundary motion. A key
feature of this system is that the equation for the crystalline orientation
angle is described as a quasilinear diffusion equation with variable mobility.
The goal of this paper is to establish a structure-preserving numerical scheme
for the system, focusing on two main structural properties: $\sharp\,1)$ range
preservation; and $\sharp\,2)$ energy dissipation. Under suitable assumptions,
we construct a structure-preserving numerical scheme and address the following
in the main theorems: (O) verification of the structural properties; (I)
clarification of the convergence conditions; and (II) error estimate for the
scheme.

</details>


### [20] [Any nonincreasing convergence curves are simultaneously possible for GMRES and weighted GMRES, as well as for left and right preconditioned GMRES](https://arxiv.org/abs/2506.17193)
*Pierre Matalon,Nicole Spillane*

Main category: math.NA

TL;DR: The paper extends the idea of arbitrary convergence curves in GMRES to weighted GMRES, proving the existence of a weight matrix M for any prescribed curve and characterizing M. It also provides conditions for simultaneous prescription of two curves and applies these findings to preconditioned GMRES.


<details>
  <summary>Details</summary>
Motivation: To explore and generalize the unpredictability of GMRES convergence by extending the concept to weighted GMRES and preconditioned variants.

Method: The authors build upon prior results to derive theoretical proofs about the existence and form of weight matrices M for prescribed convergence curves in weighted GMRES. They also analyze conditions for simultaneous curves and apply these to preconditioned GMRES.

Result: For any linear system and convergence curve, a weight matrix M exists for weighted GMRES to match the curve. A condition is given for simultaneous prescription of two curves. Applications show any two curves are possible for left and right preconditioned GMRES.

Conclusion: The work generalizes the flexibility of GMRES convergence to weighted and preconditioned variants, providing theoretical insights and practical implications for solver behavior.

Abstract: The convergence of the GMRES linear solver is notoriously hard to predict. A
particularly enlightening result by [Greenbaum, Pt\'ak, Strako\v{s}, 1996] is
that, given any convergence curve, one can build a linear system for which
GMRES realizes that convergence curve. What is even more extraordinary is that
the eigenvalues of the problem matrix can be chosen arbitrarily. We build upon
this idea to derive novel results about weighted GMRES. We prove that for any
linear system and any prescribed convergence curve, there exists a weight
matrix M for which weighted GMRES (i.e., GMRES in the inner product induced by
M) realizes that convergence curve, and we characterize the form of M.
Additionally, we exhibit a necessary and sufficient condition on M for the
simultaneous prescription of two convergence curves, one realized by GMRES in
the Euclidean inner product, and the other in the inner product induced by M.
These results are then applied to infer some properties of preconditioned GMRES
when the preconditioner is applied either on the left or on the right. For
instance, we show that any two convergence curves are simultaneously possible
for left and right preconditioned GMRES.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [21] [Propagation of chaos for the Landau equation with very soft potentials](https://arxiv.org/abs/2506.15795)
*Côme Tabary*

Main category: math.AP

TL;DR: The paper proves the convergence of an empirical measure for a drift-diffusion process to the Landau equation solution in the soft potentials regime, using tightness/uniqueness methods and exploiting Fisher information dissipation.


<details>
  <summary>Details</summary>
Motivation: To extend the understanding of particle systems' behavior in the soft potentials regime, which was previously unexplored, and to establish convergence to the Landau equation.

Method: Uses a tightness/uniqueness approach, leveraging high integrability estimates derived from Fisher information dissipation at the particle system level.

Result: Demonstrates convergence of the empirical measure to the Landau equation solution, with new results on entropy production and Fisher information dissipation in infinite dimensions.

Conclusion: The study successfully extends the analysis of particle systems to soft potentials, providing new insights into entropy and Fisher information behavior in infinite dimensions.

Abstract: We consider a drift-diffusion process of $N$ stochastic particles and show
that its empirical measure converges, as $N\rightarrow\infty$, to the solution
of the Landau equation. We work in the regime of very soft potentials, which
had never been covered before, using a tightness/uniqueness method. To claim
uniqueness, we need high integrability estimates that we obtain by crucially
exploiting the dissipation of the Fisher information at the level of the
particle system. To be able to exploit these estimates as $N\rightarrow\infty$,
we prove the affinity in infinite dimension of the entropy production and
Fisher information dissipation (and general higher-order versions of the Fisher
information through an abstract theorem), results which were up to now only
known for the entropy and the Fisher information.

</details>


### [22] [Sharp Well-Posedness and Parameter Asymptotics for a Nonlocal Model of Thin Film Flows](https://arxiv.org/abs/2506.15863)
*Manuel Fernando Cortez,Oscar Jarrin,Miguel Yangari*

Main category: math.AP

TL;DR: Analysis of a 2D thin fluid film equation under gravity and electric field, proving well-posedness in Sobolev spaces for $s>-2$ and ill-posedness for $s<-2$, with asymptotic behavior study.


<details>
  <summary>Details</summary>
Motivation: To understand the dynamics of thin fluid films under gravity and electric fields, and their mathematical properties in Sobolev spaces.

Method: Mathematical analysis of the Cauchy problem, focusing on global well-posedness in $H^s(\mathbb{R}^2)$ and asymptotic behavior with respect to physical parameters.

Result: Well-posedness for $s>-2$, ill-posedness for $s<-2$, and convergence to a vertical plane model as electric field vanishes and inclination angle increases.

Conclusion: The study provides rigorous mathematical insights into the model's behavior, linking it to physical scenarios and validating its asymptotic limits.

Abstract: This work focuses on the mathematical analysis of the Cauchy problem
associated with a two-dimensional equation describing the dynamics of a thin
fluid film flowing down an inclined flat plate under the influence of gravity
and an electric field. As a first objective, we study the sharp global
well-posedness of solutions within the framework of Sobolev spaces.
Specifically, we show that the equation is well-posed in $H^s(\mathbb{R}^2)$
for $s>-2$, and ill-posed for $s<-2$. Our main contribution is to investigate
the asymptotic behavior of solutions with respect to the physical parameters of
the model. This behavior is not only of mathematical interest but also of
physical relevance. We rigorously show that, as the effects of the electric
field vanish and the inclination angle increases, the model converges to a
related one describing thin film flow down a vertical plane.

</details>


### [23] [Crystalline elastic flow of polygonal curves: long time behaviour and convergence to stationary solutions](https://arxiv.org/abs/2506.15869)
*Giovanni Bellettini,Shokhrukh Yu. Kholmatov,Matteo Novaga*

Main category: math.AP

TL;DR: The paper studies the crystalline elastic flow of planar polygonal curves under anisotropy, proving existence, uniqueness, and global evolution properties, with long-time convergence to stationary curves.


<details>
  <summary>Details</summary>
Motivation: To understand the evolution of polygonal curves under crystalline anisotropy, including unbounded cases, and analyze their long-time behavior.

Method: Analyze the flow assuming segments evolve by parallel translation, prove existence and uniqueness, and use a Lojasiewicz-Simon-type inequality for long-time behavior.

Result: Unique regular flow exists until maximal time; closed curves can restart flow finitely, preserving index. Long-time convergence to stationary curves is proven.

Conclusion: The flow converges to stationary curves, with a classification of stationary and partial classification of translating solutions for square anisotropy.

Abstract: Given a planar crystalline anisotropy, we study the crystalline elastic flow
of immersed polygonal curves, possibly also unbounded. Assuming that the
segments evolve by parallel translation (as it happens in the standard
crystalline curvature flow), we prove that a unique regular flow exists until a
maximal time when some segments having zero crystalline curvature disappear.
Furthermore, for closed polygonal curves, we analyze the behaviour at the
maximal time, and show that it is possible to restart the flow finitely many
times, yielding a globally in time evolution, that preserves the index of the
curve. Next, we investigate the long-time properties of the flow using a
Lojasiewicz-Simon-type inequality, and show that, as time tends to infinity,
the flow fully converges to a stationary curve. We also provide a complete
classification of the stationary solutions and a partial classification of the
translating solutions in the case of the square anisotropy.

</details>


### [24] [$Γ$-convergence of the non-local Massari functional and applications to inhomogeneous Allen-Cahn equations](https://arxiv.org/abs/2506.15946)
*Serena Dipierro,Enrico Valdioci,Riccardo Villa*

Main category: math.AP

TL;DR: The paper shows that the fractional Massari functional Γ-converges to the classical one, preserving minimizers, and provides insights into the inhomogeneous Allen-Cahn equation, introducing a new geometric object called 'non-local hybrid mean curvature.'


<details>
  <summary>Details</summary>
Motivation: To study the asymptotic behavior of the non-local Massari Problem and its connection to the classical Massari functional, particularly in the context of the inhomogeneous Allen-Cahn equation.

Method: Asymptotic analysis of the fractional Massari functional, demonstrating Γ-convergence to the classical Massari functional.

Result: The fractional Massari functional Γ-converges to the classical one, preserving minimizers, and reveals new geometric insights.

Conclusion: The study bridges non-local and classical Massari problems, offering new understanding of the Allen-Cahn equation and introducing a novel geometric concept.

Abstract: We present several asymptotic results concerning the non-local Massari
Problem for sets with prescribed mean curvature. In particular, we show that
the fractional Massari functional $\Gamma$-converges to the classical one, and
this convergence preserves minimizers in the $L^1_{\mbox{loc}}$-topology. This
returns useful information about the asymptotic behavior of the solutions of
the inhomogeneous Allen-Cahn equation in the forced and the mass-prescribed
settings. In this context, a new geometric object, which we refer to as
"non-local hybrid mean curvature", naturally appears.

</details>


### [25] [Average estimate for Eigenfunctions along geodesics in the quantum completely integrable case](https://arxiv.org/abs/2506.15992)
*Weiwei Wang,Xianchao Wu*

Main category: math.AP

TL;DR: The paper improves the upper bound for the integral of $L^2$-normalized joint eigenfunctions over geodesics in a 2D quantum integrable system, showing an asymptotic decay rate of $O(h^{1/2}|\ln h|^{1/2})$, better than the previous $O(1)$ bound.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of the known $O(1)$ bound for integrals of eigenfunctions over geodesics in quantum integrable systems.

Method: The study focuses on two-dimensional quantum completely integrable systems and analyzes the integral of $L^2$-normalized joint eigenfunctions over admissible geodesics.

Result: The paper establishes a new asymptotic decay rate of $O(h^{1/2}|\ln h|^{1/2})$, a polynomial improvement over the previous $O(1)$ bound.

Conclusion: The improved bound provides a better understanding of eigenfunction behavior in quantum integrable systems, with potential implications for related mathematical and physical theories.

Abstract: This paper investigates the upper bound of the integral of $L^2$-normalized
joint eigenfunctions over geodesics in a two-dimensional quantum completely
integrable system. For admissible geodesics, we rigorously establish an
asymptotic decay rate of $O(h^{1/2}|\ln h|^{1/2})$. This represents a
polynomial improvement over the previously well known $O(1)$ bound.

</details>


### [26] [Transporting a Dirac mass in a mean field planning problem](https://arxiv.org/abs/2506.16041)
*Pierre Cardaliaguet,Sebastian Munoz,Alessio Porretta*

Main category: math.AP

TL;DR: The paper analyzes a mean field planning problem with an initial Dirac mass, proving uniqueness and convergence to a self-similar profile as time approaches zero.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of solutions in mean field planning problems when the initial condition is a Dirac mass.

Method: Study a continuous rescaling of the solution and analyze its behavior near the initial time using a Lyapunov functional.

Result: Existence and uniqueness of a solution that converges to a self-similar profile as time tends to zero.

Conclusion: The solution exhibits a self-similar behavior near the initial time, characterized by the Lyapunov functional.

Abstract: We study a mean field planning problem in which the initial density is a
Dirac mass. We show that there exists a unique solution which converges to a
self-similar profile as time tends to $0$. We proceed by studying a continuous
rescaling of the solution, and characterizing its behavior near the initial
time through an appropriate Lyapunov functional.

</details>


### [27] [Local well-posedness of a perturbation problem for the Abels-Garcke-Grün model in three dimensions](https://arxiv.org/abs/2506.16105)
*Maoyin Lv*

Main category: math.AP

TL;DR: Local existence and uniqueness of a strong solution for the perturbed Abels-Garcke-Grün model in 3D, using iteration and energy methods, with implications for Rayleigh-Taylor instability studies.


<details>
  <summary>Details</summary>
Motivation: To analyze the motion of two viscous incompressible fluids with unmatched densities under gravity, focusing on perturbations around equilibrium.

Method: Employed an iteration scheme and energy method to establish local existence and uniqueness of a strong solution.

Result: Proved local existence and uniqueness of a strong solution for the perturbed system in three dimensions.

Conclusion: Provides a foundation for future research on Rayleigh-Taylor instability in nonhomogeneous two-phase flows using diffuse interface models.

Abstract: We investigate the Abels-Garcke-Gr\"un model that describes the motion of two
viscous incompressible fluids with unmatched densities in the presence of a
uniform gravitational field. For the perturbated system with respect to a given
equilibrium state in three dimensions, we establish the local existence and
uniqueness of a strong solution using a suitable iteration scheme and the
energy method. This work lays the foundation for further studies on the
Rayleigh-Taylor instability problem of nonhomogeneous two-phase flows within
the framework of diffuse interface models.

</details>


### [28] [Sobolev inequality and its extremal functions for homogeneous Hörmander vector fields](https://arxiv.org/abs/2506.16125)
*Hua Chen,Hong-Ge Chen,Jin-Ning Li*

Main category: math.AP

TL;DR: The paper connects volume growth rates of subunit balls with Sobolev exponents, derives Sobolev inequalities, and identifies conditions for optimal exponents. It also proves attainment of the optimal Sobolev constant using a refined concentration-compactness lemma.


<details>
  <summary>Details</summary>
Motivation: To explore the relationship between volume growth rates of subunit balls and Sobolev exponents in homogeneous Hörmander vector fields, aiming to derive optimal Sobolev inequalities.

Method: Analyzes properties of volume growth rates and Sobolev exponents, develops a refined concentration-compactness lemma for degenerate cases, and examines maximal level sets of the pointwise homogeneous dimension.

Result: Establishes Sobolev inequalities and conditions for optimal Sobolev exponents, proving attainment of the optimal constant under specific conditions.

Conclusion: The study provides a framework for understanding Sobolev inequalities in degenerate cases and identifies conditions for optimality, with implications for bounded and unbounded domains.

Abstract: Let $X=(X_1,X_{2},\ldots,X_m)$ be a system of homogeneous H\"{o}rmander
vector fields defined on $\mathbb{R}^n$. In this paper, we establish
connections between the volume growth rate set of subunit balls and the
admissible conjugate Sobolev exponents set on arbitrary open subsets
$\Omega\subset \mathbb{R}^n$. By analyzing further properties of these sets, we
derive Sobolev inequalities and provide explicit conditions for the optimality
of the corresponding Sobolev exponents, particularly for bounded and certain
classes of unbounded domains. Furthermore, by developing a refined
concentration-compactness lemma, specifically adapted to degenerate cases, we
prove that the optimal Sobolev constant is attained when the maximal level set
of the pointwise homogeneous dimension is spanned by the translation directions
of $X$.

</details>


### [29] [Critical curve for the weakly coupled system of damped wave equations with mixed nonlinearities](https://arxiv.org/abs/2506.16126)
*Dinh Van Duong,Tuan Anh Dao*

Main category: math.AP

TL;DR: The paper analyzes the Cauchy problem for a weakly coupled system of semi-linear damped wave equations with mixed nonlinear terms, focusing on the critical curve using Harmonic Analysis.


<details>
  <summary>Details</summary>
Motivation: To understand the impact of nonlinearities of time derivative-type on the critical curve of the system.

Method: Uses tools from Harmonic Analysis to derive the critical curve and examines global existence and blow-up of solutions.

Result: A new critical curve $pq = 1+ \frac{2}{n}$ is established, with global existence for $pq > 1 +\frac{2}{n}$ and blow-up for $pq < 1+ \frac{2}{n}$.

Conclusion: The study highlights the influence of nonlinearities on the critical curve, providing insights into solution behavior for different parameter ranges.

Abstract: In this paper, we would like to consider the Cauchy problem for a weakly
coupled system of semi-linear damped wave equations with mixed nonlinear terms.
Our main objective is to draw conclusions about the critical curve of this
problem using tools from Harmonic Analysis. Precisely, we obtain a new critical
curve $pq = 1+ \frac{2}{n}$ for $n =1,2$ by proving global (in time) existence
of small data Sobolev solutions when $pq > 1 +\frac{2}{n}$ and blow-up of weak
solutions in finite time even for small data when $pq < 1+ \frac{2}{n}$ for $n
\geq 1$. From this, we infer the impact of the nonlinearities of time
derivative-type on the critical curve associated with the system.

</details>


### [30] [Anisotropic Improved Leray-Trudinger Inequality](https://arxiv.org/abs/2506.16167)
*Giuseppina Di Blasio,Giovanni Pisante,Georgios Psaradakis*

Main category: math.AP

TL;DR: A Leray-Trudinger type inequality is generalized to anisotropic Sobolev spaces using a Finsler norm, extending classical results and achieving the optimal constant for radial functions.


<details>
  <summary>Details</summary>
Motivation: To extend classical exponential integrability inequalities for Sobolev functions to anisotropic settings, leveraging Finsler norms.

Method: The study uses anisotropic Sobolev spaces $W^{1,n}_0(\Omega)$, replacing the Euclidean norm with a Finsler norm $F$ and its polar norm $F^o$.

Result: The paper establishes the inequality in the anisotropic setting and derives the optimal constant for anisotropically radial functions, akin to Moser's sharp inequality.

Conclusion: The work successfully generalizes classical inequalities to anisotropic spaces and provides sharp results for radial functions.

Abstract: We establish a Leray- Trudinger Type inequality in the anisotropic setting
induced by a strongly convex Finsler norm F. The result generalizes classical
exponential integrability inequalities for Sobolev functions to the framework
of anisotropic Sobolev spaces $W^{1,n}_0(\Omega)$, where the standard Euclidean
norm is replaced by F and associated polar norm $F^o$. Moreover, in the class
of anisotropically radial functions, we obtain the optimal constant in the
spirit of Moser's sharp inequality.

</details>


### [31] [Global well-posedness for 2D compressible radially symmetric Navier-Stokes equations with swirl](https://arxiv.org/abs/2506.16261)
*Xiangdi Huang,Weili Meng*

Main category: math.AP

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this paper, we consider the radially symmetric compressible Navier-Stokes
equations with swirl in two-dimensional disks, where the shear viscosity
coefficient \(\mu = \text{const}> 0\), and the bulk one \(\lambda =
\rho^\beta(\beta>0)\). When \(\beta \geq 1\), we prove the global existence and
asymptotic behavior of the large strong solutions for initial values that allow
for vacuum. One of the key ingredients is to show the uniform boundedness of
the density independent of the time. When \(\beta\in(0,1)\), we prove the same
conclusion holds when the initial value satisfies \(\norm{\rho_0}_{L^\infty}
\leq a_0\), where \(a_0\) is given by \eqref{def a_0} as in Theorem \ref{Thm3}.
To the best of our knowledge, this is the first result on the global existence
of large strong solutions for 2D compressible Navier-Stokes equation with real
non-slip (non Navier-slip) boundary conditions when $\beta\ge1$ and the first
result on the global existence of strong solutions when $\beta\in(0,1)$

</details>


### [32] [Existence of weak solutions for two-phase matrix-valued harmonic map flows](https://arxiv.org/abs/2506.16278)
*Wei Wang,Wei Wang,Zhifei Zhang*

Main category: math.AP

TL;DR: Existence of weak solutions for matrix-valued two-phase harmonic map flows with optimal lifespan, using a modified minimizing movement scheme.


<details>
  <summary>Details</summary>
Motivation: The study arises from the limiting system of the matrix-valued Rubinstein-Sternberg-Keller problem, aiming to extend understanding of harmonic map flows.

Method: A modified minimizing movement scheme is used, discretizing time and interpolating solutions to the functional problem in small intervals.

Result: Weak solutions with optimal lifespan are proven to exist.

Conclusion: The approach successfully addresses the existence of weak solutions for the given harmonic map flows.

Abstract: We investigate the existence of weak solutions for matrix-valued two-phase
harmonic map flows with optimal lifespan, which arises as the limiting system
of the matrix-valued Rubinstein-Sternberg-Keller problem studied by ({\em
Invent. Math.}, 233(1):1--80, 2023). Our approach employs a modified minimizing
movement scheme, discretizing the time domain and constructing approximate
solutions by interpolating solutions to the associated functional problem
within each small time interval.

</details>


### [33] [A remark for fully non-linear elliptic equations on compact almost Hermitian manifolds](https://arxiv.org/abs/2506.16305)
*Liding Huang*

Main category: math.AP

TL;DR: The paper generalizes sub-slope to almost Hermitian manifolds, proving solutions for non-linear equations and solving specific equations in this setting.


<details>
  <summary>Details</summary>
Motivation: To extend the concept of sub-slope to almost Hermitian manifolds and address fully non-linear equations in this context.

Method: Generalizing the definition of sub-slope and proving existence of solutions for non-linear equations on compact almost Hermitian manifolds.

Result: Solutions are found for the complex Hessian quotient equation and the deformed Hermitian-Yang-Mills equation in the almost Hermitian setting.

Conclusion: The work successfully extends sub-slope and solves key equations, advancing the understanding of almost Hermitian manifolds.

Abstract: In this paper, we generalize the definition of sub-slope, introduced by
Guo-Song, to almost Hermitian manifolds and prove the existence of solutions
for a general class of fully non-linear equations on compact almost Hermitian
manifolds. As an application, we solve the complex Hessian quotient equation
and the deformed Hermitian-Yang-Mills equation in the almost Hermitian setting.

</details>


### [34] [$L^p$ boundedness of wave operators for higher order schrödinger operators with threshold eigenvalues](https://arxiv.org/abs/2506.16378)
*M. Burak Erdogan,William R. Green,Kevin LaMaster*

Main category: math.AP

TL;DR: The paper analyzes the higher-order Schrödinger operator in dimensions $n>2m$, focusing on boundedness of wave operators on $L^p$ spaces under specific conditions, including threshold eigenvalues and orthogonality constraints.


<details>
  <summary>Details</summary>
Motivation: To extend results on wave operator boundedness for higher-order Schrödinger operators to lower dimensions ($2m<n\leq4m$) and refine conditions for $L^p$ boundedness, including orthogonality constraints on eigenfunctions.

Method: Adapts recent results for $n>4m$ to $2m<n\leq4m$, leveraging threshold eigenvalues and absence of resonances. Uses orthogonality conditions of eigenfunctions to $x^\alpha V(x)$ to derive $L^p$ boundedness ranges.

Result: Wave operators are bounded on $L^p(\mathbb R^n)$ for specific ranges depending on $n$'s parity and orthogonality constraints. For $k_0<2m$, boundedness holds for $1\leq p<\frac{n}{2m-k_0}$; for $k_0\geq2m$, ranges extend to $[1,\infty)$ or $[1,\infty]$.

Conclusion: The study generalizes and refines $L^p$ boundedness results for wave operators in lower dimensions, providing new insights for $n>3$ and streamlining proofs for eigenvalue-only cases.

Abstract: We consider the higher order Schr\"odinger operator $H=(-\Delta)^m+V(x)$ in
$n$ dimensions with real-valued potential $V$ when $n>2m$, $m\in \mathbb N$
when $H$ has a threshold eigenvalue. We adapt our recent results for $m\geq 1$
when $n>4m$ to lower dimensions $2m<n\leq 4m$ to show that when $H$ has a
threshold eigenvalue and no resonances, the wave operators are bounded on
$L^p(\mathbb R^n)$ for the natural range $1\leq p<\frac{2n}{n-1}$ when $n$ is
odd and $1\leq p<\frac{2n}{n-2}$ when $n$ is even. We further show that if the
zero energy eigenfunctions are orthogonal to $x^\alpha V(x)$ for all
$|\alpha|<k_0$, then the wave operators are bounded on $1\leq
p<\frac{n}{2m-k_0}$ when $k_0<2m$ in all dimensions $n>2m$. The range is $p\in
[1,\infty)$ and $p\in[1,\infty]$ when $k_0=2m$ and $k_0>2m$ respectively. The
proofs apply in the classical $m=1$ case as well and streamlines existing
arguments in the eigenvalue only case, in particular the $L^\infty(\mathbb
R^n)$ boundedness is new when $n>3$.

</details>


### [35] [Hölder continuity of Minimizing $W^{s,p}$-Harmonic Maps](https://arxiv.org/abs/2506.16442)
*Akshara Vincent*

Main category: math.AP

TL;DR: Minimizers of a fractional energy for mappings into simple-topology manifolds are locally Hölder continuous outside a small singular set, using a blow-up argument.


<details>
  <summary>Details</summary>
Motivation: To prove regularity of energy-minimizing mappings into manifolds without relying on a monotonicity formula, especially when $p \neq 2$.

Method: A blow-up argument is employed to analyze the behavior of minimizers, avoiding the need for a monotonicity formula.

Result: Local Hölder continuity is established outside a singular set of Hausdorff dimension less than $n-sp$.

Conclusion: The blow-up method successfully replaces the monotonicity formula, providing regularity results for a broader range of $p$.

Abstract: We show that the mappings $u\in \dot{W}^{s,p}(\mathbb{R}^n,\mathcal{N})$ into
manifolds $\mathcal{N}$ of a sufficiently simple topology that minimize the
energy
$$\int_{\mathbb{R}^n}\int_{\mathbb{R}^n}\frac{|u(x)-u(y)|^p}{|x-y|^{n+sp}}
\;dx\;dy$$ are locally H\"older continuous in a bounded domain $\Omega$ outside
a singular set $\Sigma $ with Hausdorff dimension strictly smaller than $n-sp$.
We avoid the use of a monotonicity formula (which is unknown if $p \neq 2$) by
using a blow-up argument instead.

</details>


### [36] [Existence Result for Singular Second Order Dynamic Equations with Mixed Boundary Conditions](https://arxiv.org/abs/2506.16505)
*Shalmali Bandyopadhyay,Curtis J Kunkel*

Main category: math.AP

TL;DR: Existence of positive solutions for singular second-order boundary value problems on time scales using lower/upper solutions and Brouwer's fixed point theorem.


<details>
  <summary>Details</summary>
Motivation: To address singular second-order boundary value problems with mixed boundary conditions on general time scales.

Method: Lower and upper solutions method combined with Brouwer's fixed point theorem; sequence of nonsingular equations.

Result: Demonstrated existence of a positive solution.

Conclusion: Positive solutions are obtainable via limiting sequences of nonsingular equations.

Abstract: We explore singular second-order boundary value problems with mixed boundary
conditions on a general time scale. Using the lower and upper solutions method
combined with the Brouwer fixed point theorem we demonstrate the existence of a
positive solution and obtain the desired solution by using a sequence of
solutions to a sequence of nonsingular second-order equations and passing to
the limit.

</details>


### [37] [Boundedness and asymptotic stability of classical solutions to a model for tuberculosis granuloma formation](https://arxiv.org/abs/2506.16752)
*Masaaki Mizukami,Yuya Tanaka*

Main category: math.AP

TL;DR: The paper analyzes a mathematical model of tuberculosis granuloma formation, proving global existence and exponential convergence to a steady state under small initial data and specific conditions.


<details>
  <summary>Details</summary>
Motivation: To understand the dynamics of tuberculosis granuloma formation through a mathematical model, focusing on conditions for global stability.

Method: A system of partial differential equations (PDEs) under Neumann boundary conditions is analyzed, with assumptions on initial data and parameters.

Result: For small initial data and β>1 with R₀<1, the solution converges exponentially to (β,0,0,0).

Conclusion: The model provides insights into granuloma stability, showing convergence under controlled conditions.

Abstract: This paper deals with a problem which describes tuberculosis granuloma
formation \begin{align*} \begin{cases} u_t = \Delta u - \nabla \cdot (u \nabla
v) - uv - u + \beta, &x \in \Omega,\ t>0, \\ v_t = \Delta v + v -uv + \mu w, &x
\in \Omega,\ t>0, \\ w_t = \Delta w + uv - wz - w, &x \in \Omega,\ t>0, \\ z_t
= \Delta z - \nabla \cdot (z \nabla w) + f(w)z -z, &x \in \Omega,\ t>0
\end{cases} \end{align*} under homogeneous Neumann boundary conditions and
initial conditions, where $\Omega \subset \mathbb{R}^n$ ($n\ge 2$) is a smooth
bounded domain, $\beta,\mu>0$ and $f$ is some function, and shows that if
initial data are small in some sense then the solution $(u,v,w,z)$ of the
problem exists globally and convergences to $(\beta,0,0,0)$ exponentially when
$\beta>1$ and the reproduction number $R_0 := \frac{\mu \beta + 1}{\beta}$
satisfies $R_0<1$.

</details>


### [38] [Large solutions for subordinate spectral Laplacian](https://arxiv.org/abs/2506.16780)
*Ivan Biočić,Vanja Wagner*

Main category: math.AP

TL;DR: The paper studies a semilinear Dirichlet problem for a non-local operator, extending the spectral fractional Laplacian, and provides bounds for large solutions and interior regularity results.


<details>
  <summary>Details</summary>
Motivation: To address the explosion rate and regularity of solutions for a non-local operator in bounded domains, extending known results for the spectral fractional Laplacian.

Method: Analyzes the semilinear Dirichlet problem using the operator ϕ(−Δ|D), focusing on large solutions and their explosion rates, along with interior regularity.

Result: Derives an upper bound for the explosion rate of large solutions, expressed via the renewal function, boundary distance, and a Keller-Osserman-type transformation. Also proves higher interior regularity.

Conclusion: The work generalizes and extends results for the spectral fractional Laplacian, providing insights into the behavior of solutions for non-local operators in bounded domains.

Abstract: We find a large solution to a semilinear Dirichlet problem in a bounded
$C^{1,1}$ domain for a non-local operator $\phi(-\Delta\vert_{D})$, an
extension of the infinitesimal generator of a subordinate killed Brownian
motion. The setting covers and extends the case of the spectral fractional
Laplacian. The upper bound for the explosion rate of the large solution is
obtained, and is given in terms of the renewal function, distance to the
boundary, and the Keller-Osserman-type transformation of the semilinearity.
Additionally, we prove interior higher regularity results for this operator.

</details>


### [39] [Controllability of a Fluid-Structure Interaction System Governed by the Heat and Damped Beam Equations](https://arxiv.org/abs/2506.16880)
*Mehdi Badra,Jérémi Dardé,Emmanuel Zongo*

Main category: math.AP

TL;DR: The paper studies null-controllability and observability in a 2D fluid-structure system, using a global Carleman estimate and analyzing damping effects.


<details>
  <summary>Details</summary>
Motivation: To understand control and observation properties in coupled heat and damped beam systems.

Method: Proving a global Carleman estimate for the coupled system and analyzing damping parameter effects.

Result: Demonstrated inequalities for the beam equation under various damping regimes.

Conclusion: The study provides insights into control and observability for such coupled systems.

Abstract: In this article, we study the null-controllability and observability
properties of a bi-dimensio\-nal fluid-structure interaction system, governed
by the heat equation coupled with the damped beam equation. To do so, we prove
a global Carleman estimate for the coupled system, making explicit the
dependence on the damping parameter. In the course of the study, we demonstrate
several inequalities for the beam equation alone, valid for different damping
parameter regimes.

</details>


### [40] [Density estimates for Ginzburg-Landau energies with degenerate double-well potentials](https://arxiv.org/abs/2506.17000)
*Ovidiu Savin,Chilin Zhang*

Main category: math.AP

TL;DR: The paper extends density estimates for level sets of minimizers in Allen-Cahn equations with degenerate double-well potentials, generalizing prior results for specific parameter ranges.


<details>
  <summary>Details</summary>
Motivation: To generalize existing density estimates for minimizers of Ginzburg-Landau energies involving degenerate potentials, beyond previously limited parameter ranges.

Method: Analyzes a class of Allen-Cahn equations with degenerate double-well potentials, focusing on nontrivial minimizers and their level sets.

Result: Establishes density estimates for level sets of minimizers, extending prior results for a broader range of parameters.

Conclusion: The work generalizes and extends previous findings, providing density estimates for a wider class of degenerate potentials.

Abstract: We consider a class of Allen-Cahn equations associated with Ginzburg-Landau
energies involving degenerate double-well potentials that vanish of order $m$
at the minima \begin{equation}
  J(v,\Omega)=\int_{\Omega}\Big\{|\nabla v|^{p}+(1-v^{2})^{m}\Big\}dx,\quad
1<p<m, \end{equation} and establish density estimates for the level sets of
nontrivial minimizers $|v| \leq 1$. This extends a result of
Dipierro-Farina-Valdinoci where the density estimates for such degenerate
potentials were obtained for a bounded range of $m$'s. The original estimates
for the classical case $p=m=2$ were established by Caffarelli-C\'ordoba.

</details>


### [41] [Large-amplitude periodic solutions to the steady Euler equations with piecewise constant vorticity](https://arxiv.org/abs/2506.17002)
*Alex Doak,Karsten Matthies,Jonathan Sewell,Miles H. Wheeler*

Main category: math.AP

TL;DR: The paper rigorously constructs steady solutions for incompressible Euler equations in a 2D channel with two periodic layers of constant vorticity, using global bifurcation theory. Solutions terminate due to stagnation or breakdown of conformal equivalence, with numerical evidence of corner formation or thin regions.


<details>
  <summary>Details</summary>
Motivation: To analyze steady solutions in a multi-layer flow with rigid walls, addressing challenges like overhanging wave profiles and stagnation points.

Method: Uses global bifurcation theory, elliptic systems for velocity components, conformal mappings, and horizontal distortion to ensure interface agreement.

Result: Constructs solution curves terminating at stagnation or conformal breakdown, with numerical evidence of corner formation or thin regions.

Conclusion: Provides a novel local formulation for multi-layer problems, enabling analysis of complex flow behaviors like overhanging waves and stagnation.

Abstract: We consider steady solutions to the incompressible Euler equations in a
two-dimensional channel with rigid walls. The flow consists of two periodic
layers of constant vorticity separated by an unknown interface. Using global
bifurcation theory, we rigorously construct curves of solutions that terminate
either with stagnation on the interface or when the conformal equivalence
between one of the layers and a strip breaks down in a $C^1$ sense. We give
numerical evidence that, depending on parameters, these occur either as a
corner forming on the interface or as one of the layers developing regions of
arbitrarily thin width. Our proof relies on a novel formulation of the problem
as an elliptic system for the velocity components in each layer, conformal
mappings for each layer, and a horizontal distortion which makes these mappings
agree on the interface. This appears to be the first local formulation for a
multi-layer problem which allows for both overhanging wave profiles and
stagnation points.

</details>


### [42] [Optimal Sobolev inequalities of high order with $L^2$-remainder](https://arxiv.org/abs/2506.17028)
*Lorenzo Carletti,Frédéric Robert*

Main category: math.AP

TL;DR: The paper examines the validity of a higher-order Sobolev inequality on closed Riemannian manifolds, showing it fails for k>1 and providing sharp geometric conditions for its validity.


<details>
  <summary>Details</summary>
Motivation: To understand when the optimal higher-order Sobolev inequality holds on Riemannian manifolds, especially for k>1, where it generally fails.

Method: Analyzing the inequality's validity and non-validity based on the manifold's geometry, with sharp conditions for k=2 and small dimensions.

Result: The inequality does not hold for k>1 in general; validity depends on geometric conditions, with sharp results for k=2 and small dimensions.

Conclusion: Geometric properties of the manifold determine the validity of the higher-order Sobolev inequality, with sharp conditions identified for specific cases.

Abstract: We investigate the validity of the optimal higher-order Sobolev inequality
$H_k^2(M^n)\hookrightarrow L^{\frac{2n}{n-2k}}(M^n)$ on a closed Riemannian
manifold when the remainder term is the $L^2-$norm. Unlike the case $k=1$, the
optimal inequality does not hold in general for $k>1$. We prove conditions for
the validity and non-validity that depend on the geometry of the manifold. Our
conditions are sharp when $k=2$ and in small dimensions.

</details>


### [43] [Shock formation in 1D conservation laws I: Inviscid structure](https://arxiv.org/abs/2506.17148)
*John Anderson,Sanchit Chaturvedi,Cole Graham*

Main category: math.AP

TL;DR: The paper analyzes shock formation stability and structure in 1D hyperbolic conservation laws, showing stability near simple waves and characterizing the boundary of classical development. It also details nondegenerate shock formation via fractional homogeneous functions.


<details>
  <summary>Details</summary>
Motivation: To understand the stability and structure of shock formation in 1D hyperbolic conservation laws, particularly near simple waves and the first singularity.

Method: Analysis of shock formation stability, characterization of classical development boundaries, and expansion in homogeneous functions of fractional degree.

Result: Shock formation is stable near simple waves, and the boundary of classical development is characterized. Nondegenerate shock formation is described precisely.

Conclusion: The findings are applied in a companion paper to study the vanishing viscosity limit near shock formation.

Abstract: We study the stability and structure of shock formation in 1D hyperbolic
conservation laws. We show that shock formation is stable near shocking simple
waves: perturbations form a shock nearby in spacetime. We also characterize the
boundary of the classical development in a spacetime neighborhood of the first
time singularity. Finally, we describe the precise nature of nondegenerate
shock formation through an expansion in homogeneous functions of fractional
degree. We use these results in a companion paper to study the vanishing
viscosity limit near shock formation.

</details>


### [44] [Sacks-Uhlenbeck type regularity for subcritical generalized $p$-harmonic maps into Homogeneous targets](https://arxiv.org/abs/2506.17151)
*Gianmichele Di Matteo,Tobias Lamm*

Main category: math.AP

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Adapting \cite{strz3}, we define generalized $p$-harmonic maps into
Riemannian homogeneous targets, a notion of solutions not belonging to the
energy space. Restricting our attention to the subcritical range $p$ greater
than the domain dimension $n$, we show a uniform $C^{1,\alpha}$-regularity
result for a sequence of such maps in the limit $p \searrow n$, assuming a
uniform $n$-energy bound on its elements. The method of the proof follows the
exact same lines as in \cite{strz3} but we need to check uniformity of
estimates not previously considered there.

</details>


### [45] [Shock formation in 1D conservation laws II: Vanishing viscosity](https://arxiv.org/abs/2506.17156)
*John Anderson,Sanchit Chaturvedi,Cole Graham*

Main category: math.AP

TL;DR: The paper analyzes how weak viscosity affects shock formation in 1D hyperbolic conservation laws, focusing on convergence rates and universal viscous behavior near singularities.


<details>
  <summary>Details</summary>
Motivation: To understand the impact of small viscous regularization on inviscid shock formation and identify universal behaviors near singularities.

Method: Uses matched asymptotic expansion and develops an approximation scheme to decouple shocking and nonshocking characteristics.

Result: Determines sharp convergence rates in strong norms and identifies universal viscous behavior near the first singularity.

Conclusion: The method applies broadly, including to compressible Navier-Stokes equations with degenerate physical viscosity, with minimal assumptions.

Abstract: We study the effects of weak viscosity on shock formation in 1D hyperbolic
conservation laws. Given an inviscid solution that forms a nondegenerate shock,
we add a small viscous regularization and study the limit as the viscosity
vanishes. Using a matched asymptotic expansion, we determine the sharp rate of
convergence in strong norms up to the time of inviscid shock formation, and we
identify universal viscous behavior near the first singularity. To treat the
complex interactions between multiple characteristics and the viscosity, we
develop an approximation scheme that exploits a certain decoupling between
shocking and nonshocking characteristics. Our analysis makes minimal
assumptions on the equation, and in particular applies to the compressible
Navier--Stokes equations with degenerate physical viscosity.

</details>


### [46] [Higher dimensional Sacks-Uhlenbeck-type functionals and applications](https://arxiv.org/abs/2506.17166)
*Gianmichele Di Matteo,Tobias Lamm*

Main category: math.AP

TL;DR: Generalization of Sacks-Uhlenbeck's result to construct regular, non-trivial n-harmonic n-spheres, yielding infinite null-homotopic maps.


<details>
  <summary>Details</summary>
Motivation: Extend existence results for harmonic spheres to higher dimensions and more general target manifolds.

Method: Perturbative argument, refined neck-analysis, and Struwe-type entropy bounds to handle degenerate Euler-Lagrange systems.

Result: Infinite family of null-homotopic n-harmonic n-spheres; solved general min-max problems for n-energy modulo bubbling.

Conclusion: Successfully generalized and solved min-max problems, though uniform regularity and bubbling posed challenges.

Abstract: In this work, we generalize Sacks-Uhlenbeck's existence result for harmonic
spheres, constructing for $n \ge 2$, regular, non-trivial, $n$-harmonic
$n$-spheres into suitable target manifolds. We obtain an infinite family of new
null-homotopic such maps. The proof follows a similar perturbative argument,
which in high dimensions leads to a degenerate and double-phase-type
Euler-Lagrange system, making the uniform regularity needed to formalize the
bubbling harder to achieve. Then, we develop a refined neck-analysis leading to
an energy identity along the approximation, assuming a suitable Struwe-type
entropy bound along a sequence of critical points. Finally, we combine these
results to solve quite general min-max problems for the $n$-energy modulo
bubbling.

</details>


### [47] [The effect of target orientation on the mean first passage time of a Brownian particle to a small elliptical absorber](https://arxiv.org/abs/2506.17173)
*Sanchita Chakraborty,Theodore Kolokolnikov,Alan E. Lindsay*

Main category: math.AP

TL;DR: High-order asymptotic expansion for MFPT of Brownian particles captured by a small elliptical trap in 2D, revealing trap orientation's impact on capture rate.


<details>
  <summary>Details</summary>
Motivation: Extend existing results on trap position's role in capture rate by studying trap orientation's effect.

Method: Develop high-order asymptotic expansion for MFPT, validated via numerical simulations.

Result: Identified bifurcation in unit disk domain; GMFPT correction minimized by radial or angular trap orientation depending on position. General 2D geometry results linked to Neumann Green's function.

Conclusion: Theory validated on regular domains (disks, ellipses, rectangles), providing insights into trap orientation's role in capture dynamics.

Abstract: We develop a high order asymptotic expansion for the mean first passage time
(MFPT) of the capture of Brownian particles by a small elliptical trap in a
bounded two dimensional region. This new result describes the effect that trap
orientation plays on the capture rate and extends existing results that give
information only on the role of trap position on the capture rate. Our results
are validated against numerical simulations which confirm the accuracy of the
asymptotic approximation. In the case of the unit disk domain, we identify a
bifurcation such that the high order correction to the global MFPT (GMFPT) is
minimized when the trap is orientated in the radial direction for traps
centered at $0<r<r_c :=\sqrt{2-\sqrt{2}}$. When centered at position $r_c<r<1$,
the GMFPT correction is minimized by orientating the trap in the angular
direction. In the scenario of a general two-dimensional geometry, we identify
the orientation that minimizes the GMFPT in terms of the regular part of the
Neumann Green's function. This theory is demonstrated on several regular
domains such as disks, ellipses and rectangles.

</details>


### [48] [Scattering of the 2D modified Zakharov-Kuznetsov equation](https://arxiv.org/abs/2506.17179)
*Philippe Anjolras*

Main category: math.AP

TL;DR: Small, localized solutions to the modified Zakharov-Kuznetsov equation in 2D scatter over time, proven using space-time resonance methods.


<details>
  <summary>Details</summary>
Motivation: To understand the long-term behavior of solutions to the modified Zakharov-Kuznetsov equation in two dimensions.

Method: Employ the method of space-time resonances to analyze scattering behavior.

Result: Solutions for small, localized initial data scatter for large time.

Conclusion: The study confirms scattering behavior for small, localized solutions using space-time resonance techniques.

Abstract: We study the modified Zakharov-Kuznetsov equation in dimension $2$ : \[
\partial_t u + \partial_x \left( \Delta u + u^3 \right) = 0 \] where $u : (t,
(x, y)) \in \mathbb{R} \times \mathbb{R}^2 \mapsto u(t, x, y) \in \mathbb{R}$
and $\Delta = \partial_x^2 + \partial_y^2$ is the full Laplacian. We prove that
solutions for small and localized initial data scatter for large time. Our
proof relies on the method of space-time resonances.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [49] [Optimal Navigation in Microfluidics via the Optimization of a Discrete Loss](https://arxiv.org/abs/2506.15902)
*Petr Karnakov,Lucas Amoudruz,Petros Koumoutsakos*

Main category: physics.comp-ph

TL;DR: ODIL, a closed-loop control method, outperforms reinforcement learning in microdevice navigation by being faster, more robust, and effective in high-dimensional spaces.


<details>
  <summary>Details</summary>
Motivation: The need for efficient path planning and control of microscopic devices in fluid environments for applications like drug delivery and monitoring.

Method: Introduces ODIL, a method optimizing discrete loss for dynamics and path objectives.

Result: ODIL is up to three orders faster, more robust, and excels in high-dimensional spaces compared to reinforcement learning.

Conclusion: ODIL is a powerful tool for navigating complex flow environments, offering significant advantages over traditional methods.

Abstract: Optimal path planning and control of microscopic devices navigating in fluid
environments is essential for applications ranging from targeted drug delivery
to environmental monitoring. These tasks are challenging due to the complexity
of microdevice-flow interactions. We introduce a closed-loop control method
that optimizes a discrete loss (ODIL) in terms of dynamics and path objectives.
In comparison with reinforcement learning, ODIL is more robust, up to three
orders faster, and excels in high-dimensional action/state spaces, making it a
powerful tool for navigating complex flow environments.

</details>


### [50] [UGKWP and IUGKP methods for Multi-Scale Phonon Transport with Dispersion and Polarization](https://arxiv.org/abs/2506.16203)
*Hongyu Liu,Xiaojian Yang,Chuang Zhang,Xing Ji,Kun Xu*

Main category: physics.comp-ph

TL;DR: Two novel methods, UGKWP and IUGKP, are introduced for multi-scale phonon transport, combining wave-particle and particle-based approaches for efficiency across regimes.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of solving multi-scale phonon transport problems with dispersion and polarization effects efficiently.

Method: UGKWP uses wave-particle adaptation for multiscale fluxes; IUGKP focuses on steady-state problems with adaptive particle evolution. Both employ BGK equations and adaptive frequency-space sampling.

Result: Validated through numerical tests, both methods efficiently handle phonon transport across scales, with UGKWP excelling in non-equilibrium and IUGKP in steady-state scenarios.

Conclusion: The UGKWP and IUGKP methods offer accurate, efficient solutions for multi-scale phonon transport, validated by extensive testing.

Abstract: This paper presents two novel methods for solving multi-scale phonon
transport problems with dispersion and polarization effects: the unified
gas-kinetic wave-particle (UGKWP) method and the implicit unified gas-kinetic
particle (IUGKP) method. Both approaches are based on solving multiple groups
of BGK equations at discrete frequency points. The UGKWP method constructs
multiscale macroscopic fluxes at cell interfaces through the integral solution
of the unsteady BGK equation and efficiently captures non-equilibrium transport
using statistical particles. Its wave-particle adaptive framework ensures
computational efficiency across different regimes: in the diffusive limit, it
matches the cost of explicit diffusion equation solutions, while in the
ballistic limit, it performs comparably to pure particle methods. The IUGKP
method, specifically designed for steady-state problems, determines the
particle evolution scale based on the physical mean free path. This approach
enables rapid convergence at both large and small Knudsen numbers, with the
latter facilitated by a newly constructed macroscopic prediction equation. Both
methods incorporate an adaptive frequency-space sampling technique that
maintains particle counts per cell comparable to single-frequency methods,
significantly improving computational efficiency and memory usage. The accuracy
and efficiency of both methods are validated through various numerical tests,
including large-scale three-dimensional conduction heat transfer simulations.
Results demonstrate their effectiveness in handling complex phonon transport
phenomena across multiple scales.

</details>


### [51] [A Neural Operator based Hybrid Microscale Model for Multiscale Simulation of Rate-Dependent Materials](https://arxiv.org/abs/2506.16918)
*Dhananjeyan Jeyaraj,Hamidreza Eivazi,Jendrik-Alexander Tröger,Stefan Wittek,Stefan Hartmann,Andreas Rausch*

Main category: physics.comp-ph

TL;DR: The paper proposes a hybrid multiscale modeling approach combining deep learning (neural operators) with physics-based methods to efficiently predict microscale behavior in materials, achieving high accuracy and significant computational speedup.


<details>
  <summary>Details</summary>
Motivation: To address the computational intensity of traditional multiscale modeling methods (e.g., FE²) by integrating deep learning to accelerate simulations while maintaining accuracy.

Method: Uses neural operators to predict microscale physics, combining data-driven and physics-based approaches. Applied to viscoelastic materials with microscale internal variables.

Result: Achieves homogenized stress predictions with <6% error and ~100x faster computation.

Conclusion: The hybrid approach is computationally efficient and flexible, suitable for diverse materials and discretizations.

Abstract: The behavior of materials is influenced by a wide range of phenomena
occurring across various time and length scales. To better understand the
impact of microstructure on macroscopic response, multiscale modeling
strategies are essential. Numerical methods, such as the $\text{FE}^2$
approach, account for micro-macro interactions to predict the global response
in a concurrent manner. However, these methods are computationally intensive
due to the repeated evaluations of the microscale. This challenge has led to
the integration of deep learning techniques into computational homogenization
frameworks to accelerate multiscale simulations. In this work, we employ neural
operators to predict the microscale physics, resulting in a hybrid model that
combines data-driven and physics-based approaches. This allows for
physics-guided learning and provides flexibility for different materials and
spatial discretizations. We apply this method to time-dependent solid mechanics
problems involving viscoelastic material behavior, where the state is
represented by internal variables only at the microscale. The constitutive
relations of the microscale are incorporated into the model architecture and
the internal variables are computed based on established physical principles.
The results for homogenized stresses ($<6\%$ error) show that the approach is
computationally efficient ($\sim 100 \times$ faster).

</details>


### [52] [Great Restraining Wall in Multidimentional Collective Variable Space](https://arxiv.org/abs/2506.17043)
*Zhijun Pan,Maodong Li,Dechin Chen,Yi Isaac Yang*

Main category: physics.comp-ph

TL;DR: The paper introduces the Great Restraining Wall (GW) method, a robust framework for efficient free energy surface (FES) sampling in high-dimensional collective variable (CV) spaces, addressing limitations of existing techniques like metadynamics and path-CV.


<details>
  <summary>Details</summary>
Motivation: Existing enhanced sampling methods struggle with confinement stability, hyperparameter sensitivity, and geometric flexibility in high-dimensional CV spaces, necessitating a more efficient and stable approach.

Method: GW employs a novel kernel density estimation (KDE)-derived restraining potential to confine sampling to predefined CV subspaces, using asymptotically half-harmonic barriers without iterative bias deposition.

Result: GW provides a versatile, stable, and efficient framework for targeted FES sampling, particularly useful for complex biomolecular systems with intricate CV landscapes.

Conclusion: GW enhances precision in studying rare events like ligand binding and conformational transitions, with potential future extensions to adaptive regions and machine learning-guided CV discovery.

Abstract: Enhanced sampling methods are pivotal for exploring rare events in molecular
dynamics (MD), yet face challenges in high-dimensional collective variable (CV)
spaces where exhaustive sampling becomes computationally prohibitive. While
techniques like metadynamics (MetaD) and path-CV enable targeted free energy
surface (FES) reconstruction, they often struggle with confinement stability,
hyperparameter sensitivity, and geometric flexibility. This work introduces the
Great Restraining Wall (GW) method, a robust framework for efficient FES
sampling within predefined CV subspaces, addressing these limitations through a
novel kernel density estimation (KDE)-derived restraining potential. GW
operates by constructing a bias potential that confines sampling to user
defined regions ranging from multidimensional masks to 1D pathways via
asymptotically half-harmonic barriers. Unlike MetaD variants requiring
iterative bias deposition, GW potential is derived from a cumulative
distribution function, ensuring confinement without manual hyperparameter
tuning. GW provides a versatile, stable, and efficient framework for targeted
FES sampling, particularly beneficial for complex biomolecular systems with
intricate CV landscapes. Its integration with existing enhanced sampling
protocols opens avenues for studying ligand binding, conformational
transitions, and other rare events with unprecedented precision. Future work
will explore GW extension to adaptive regions and machine learning-guided CV
discovery.

</details>


### [53] [PCG-Informed Neural Solvers for High-Resolution Homogenization of Periodic Microstructures](https://arxiv.org/abs/2506.17087)
*Yu Xing,Yang Liu,Lipeng Chen,Huiping Tang,Lin Lu*

Main category: physics.comp-ph

TL;DR: CGINS, a neural network for homogenization problems, combines sparse 3D convolution, multi-level architecture, and PCG iterations for high accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: Traditional numerical solvers are computationally expensive, and existing learning-based methods lack accuracy and generalization for complex microstructures.

Method: CGINS uses sparse 3D convolution, multi-level architecture, self-supervised learning with minimum potential energy loss, and PCG iterations for fast convergence.

Result: CGINS achieves <1% relative error, outperforms baselines and solvers, and offers 2-10x speedups at resolutions up to 512^3.

Conclusion: CGINS provides a fast, accurate, and physically consistent solution for homogenization problems, advancing beyond traditional and learning-based methods.

Abstract: The mechanical properties of periodic microstructures are pivotal in various
engineering applications. Homogenization theory is a powerful tool for
predicting these properties by averaging the behavior of complex
microstructures over a representative volume element. However, traditional
numerical solvers for homogenization problems can be computationally expensive,
especially for high-resolution and complicated topology and geometry. Existing
learning-based methods, while promising, often struggle with accuracy and
generalization in such scenarios. To address these challenges, we present
CGINS, a preconditioned-conjugate-gradient-solver-informed neural network for
solving homogenization problems. CGINS leverages sparse and periodic 3D
convolution to enable high-resolution learning while ensuring structural
periodicity. It features a multi-level network architecture that facilitates
effective learning across different scales and employs minimum potential energy
as label-free loss functions for self-supervised learning. The integrated
preconditioned conjugate gradient iterations ensure that the network provides
PCG-friendly initial solutions for fast convergence and high accuracy.
Additionally, CGINS imposes a global displacement constraint to ensure physical
consistency, addressing a key limitation in prior methods that rely on
Dirichlet anchors. Evaluated on large-scale datasets with diverse topologies
and material configurations, CGINS achieves state-of-the-art accuracy (relative
error below 1%) and outperforms both learning-based baselines and
GPU-accelerated numerical solvers. Notably, it delivers 2 times to 10 times
speedups over traditional methods while maintaining physically reliable
predictions at resolutions up to $512^3$.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [54] [The Atomic Beam Probe Synthetic Diagnostic and its Application in Fusion Plasmas](https://arxiv.org/abs/2506.15812)
*Mátyás Aradi,Dániel I. Réfy,Shimpei Futatani,Ors Asztalos,Miklós Berta,Pavel Háček,Jaroslav Krbec,Sándor Zoletnik,Gergo I. Pokol*

Main category: physics.plasm-ph

TL;DR: The paper discusses the development of a GPGPU-based trajectory solver (TAIGA) for the Atomic Beam Probe on the COMPASS tokamak, leading to a 1000x performance boost, and introduces the TAIGA-SD synthetic diagnostic for improved simulations.


<details>
  <summary>Details</summary>
Motivation: The initial trajectory solver (ABPIons) was insufficient for modeling needs, requiring a more efficient and sophisticated tool to enhance diagnostics and measurement scenarios.

Method: The trajectory solver was redesigned using CUDA C for GPGPU computing, resulting in TAIGA. Further, the TAIGA-SD synthetic diagnostic was developed to incorporate all essential physical parameters.

Result: The new solver achieved a three-order-of-magnitude performance improvement. The synthetic diagnostic (TAIGA-SD) successfully supported measurements and linked synthetic data to real measurements.

Conclusion: The TAIGA-SD synthetic diagnostic is a significant advancement for the Atomic Beam Probe, enabling more accurate simulations and better integration with experimental data.

Abstract: The Atomic Beam Probe was installed on the COMPASS tokamak in Prague as an
extension of the Beam Emission Spectroscopy. The construction and installation
of the diagnostic were preceded by conceptional design and measurements with a
test detector. This development work was supported by a trajectory solver,
written as a sequential code, called ABPIons, whose shortcomings were
discovered early on. For further modeling, orders of magnitude larger numerical
performance were required. Therefore, the trajectory calculator has been
redesigned and implemented as a GPGPU (general-purpose computing on graphics
processing units) code in CUDA C language, which resulted in a performance
improvement of three orders of magnitude. The new solver, the Trajectory
simulator of ABP Ions with GPU Acceleration (TAIGA), was used to establish the
final diagnostics. To aid the improvement of the diagnostics and to design
further measurement scenarios, we improved the code and developed a complete
synthetic diagnostic. The trajectory solver is an essential tool for designing
the ABP. However, a more sophisticated tool was required to consider all
essential physical parameters in the simulation. Therefore, we developed the
TAIGA-SD synthetic diagnostic, which is the subject of the recent paper.
  In this article, we introduce the concept of the Atomic Beam Probe synthetic
diagnostics. We then show the initial input conditions, followed by the
trajectory simulator, the mathematical and physical models we used, and the
synthetic signal processing. Finally, we present how synthetic diagnostics
supported the measurements and what the relation is between the synthetic and
the measurement data.

</details>


### [55] [Magnetic stagnation of two counterstreaming plasma jets induced by intense laser](https://arxiv.org/abs/2506.16069)
*R. S. Zemskov,S. E. Perevalov,A. V. Kotov,A. A. Murzanev,A. I. Korytin,K. F. Burdonov,V. N. Ginzburg,A. A. Kochetkov,S. E. Stukachev,I. V. Yakovlev,I. A. Shaikin,A. A. Kuzmin,E. V. Derishev,A. V. Korzhimanov,A. A. Soloviev,A. A. Shaykin,A. N. Stepanov,M. V. Starodubtsev,E. A. Khazanov*

Main category: physics.plasm-ph

TL;DR: Experimental observations show stagnation and redirection of laser plasma jets due to enhanced toroidal magnetic fields, aiding astrophysical plasma studies.


<details>
  <summary>Details</summary>
Motivation: To address fundamental plasma physics questions and understand astrophysical phenomena like shock waves and solar flares.

Method: Direct experiments with high-velocity laser plasma flows and hybrid (PIC-fluid) modeling to analyze magnetic field effects.

Result: Enhanced toroidal magnetic fields cause plasma flow stagnation and jet redirection.

Conclusion: The study provides insights into plasma behavior and magnetic field dynamics relevant to astrophysics.

Abstract: Experiments with interacting high-velocity flows of laser plasma can help
answer the fundamental questions in plasma physics and improve the
understanding of the mechanisms behind astrophysical phenomena, such as
formation of collisionless shock waves, deceleration of accretion flows, and
evolution of solar (stellar) flares. This work presents the first direct
experimental observations of stagnation and redirection of counterstreaming
flows (jets) of laser plasma induced by ultra-intense laser pulses with
intensity $I \sim$ 2 $\times$ $10^{18}$ $W/cm^2$. Hybrid (PIC - fluid)
modeling, which takes into account the kinetic effects of ion motion and the
evolution of the pressure tensor for electrons, demonstrates the compression of
counterdirected toroidal self-generated magnetic fields embedded in the
counterstreaming plasma flows. The enhancement of the toroidal magnetic field
in the interaction region results in plasma flow stagnation and redirection of
the jets across the line of their initial propagation.

</details>


### [56] [Electrical charge decay on dielectric surface in nitrogen/C4F7N mixtures](https://arxiv.org/abs/2506.16142)
*D. Prokop,M. Mrkvickova,J. Tungli,Z. Bonaventura,P. Dvorak,S. Kadlec,T. Hoder*

Main category: physics.plasm-ph

TL;DR: The study investigates charge decay on dielectric surfaces in nitrogen-C4F7N mixtures using EFISH and electrical measurements, revealing slower decay with C4F7N admixtures due to modified charge traps.


<details>
  <summary>Details</summary>
Motivation: Understanding charge decay on dielectric surfaces in gas mixtures is crucial for applications like insulation systems, where surface charge affects performance.

Method: Charge is deposited via barrier discharge, measured using EFISH and electrical current, and modeled numerically. Different C4F7N admixtures (0%, 10%, 50%) are tested.

Result: C4F7N admixtures slow charge decay, creating/modifying traps with higher energies. EFISH helps estimate surface charge density and hyperpolarizability.

Conclusion: C4F7N mixtures prolong surface charge decay, offering insights for dielectric material design and applications.

Abstract: The decay of electrical charge on a dielectric surface in nitrogen-C4F7N
(Novec4710, C4) mixtures is investigated using measurement of electric field
via in-situ electric field-induced second harmonic (EFISH) technique. The
charge is deposited on the surface of the alumina by generating a barrier
discharge in the gap, and the amount of charge is determined from electrical
current measurements and numerical modeling. For different admixtures (0, 10,
and 50 percent) of C4F7N in nitrogen, the presence of surface charge is
detected even 60 hours after charge deposition. It is found that C4F7N
admixture lead to a significantly longer-lasting surface charge, indicating a
slower charge decay. Using an isothermal charge decay model, charge traps are
identified for pure nitrogen charge deposition, which are in agreement with
results found in the literature. Charge deposition in C4F7N admixtures leads to
modification or creation of new traps with higher trap energies. The EFISH
measurements are used to determine the C4F7N nonlinear hyperpolarizability
tensor component. Direct comparison of the experimental results from two
developed methods (EFISH and electrical measurements) and the numerical model
gives a closer insight into the surface charge spread over the dielectrics,
resulting in surface charge density estimation.

</details>


### [57] [Characterization of discharge capillaries via benchmarked hydrodynamic plasma simulations](https://arxiv.org/abs/2506.16192)
*S. M. Mewes,G. J. Boyle,R. D'Arcy,J. M. Garland,M. Huck,H. Jones,G. Loisch,A. R. Maier,J. Osterhoff,T. Parikh,S. Wesch,J. C. Wood,M. Thévenet*

Main category: physics.plasm-ph

TL;DR: The paper explores hydrodynamic plasma simulations to study discharge capillaries in plasma accelerators, validating the model with experimental data and discussing implications for future systems.


<details>
  <summary>Details</summary>
Motivation: Discharge capillaries are crucial for plasma accelerators but are hard to simulate and diagnose. This work aims to improve understanding through simulations.

Method: Hydrodynamic plasma simulations are used to model the discharge process in a capillary, validated with experimental measurements at specific conditions.

Result: The simulation validated at 20 kV and 8.7mbar shows 178mJ energy deposition. Challenges with Hα emission spectroscopy for density measurement are noted.

Conclusion: The model aids in studying repeatability, heat flow, and plasma profile tailoring, advancing discharge capillary applications in accelerators.

Abstract: Plasma accelerators utilize strong electric fields in plasma waves to
accelerate charged particles, making them a compact alternative to
radiofrequency technologies. Discharge capillaries are plasma sources used in
plasma accelerator research to provide acceleration targets, or as plasma
lenses to capture or focus accelerated beams. They have applications for
beam-driven and laser-driven plasma accelerators and can sustain high
repetition rates for extended periods of time. Despite these advantages,
high-fidelity simulations of discharge capillaries remain challenging due to
the range of mechanisms involved and the difficulty to diagnose them in
experiments. In this work, we utilize hydrodynamic plasma simulations to
examine the discharge process of a plasma cell and discuss implications for
future accelerator systems. The simulation model is validated with experimental
measurements in a 50-mm-long, 1-mm-wide plasma capillary operating a 12-27 kV
discharge at 2-12mbar hydrogen pressure. For 20 kV at 8.7mbar the discharge is
shown to deposit 178mJ of energy in the plasma. Potential difficulties with the
common density measurement method using H{\alpha} emission spectroscopy are
discussed. This simulation model enables investigations of repeatability, heat
flow management and fine tailoring of the plasma profile with discharges.

</details>


### [58] [Energy conversion and scaling analysis of relativistic magnetic reconnection](https://arxiv.org/abs/2506.16227)
*Harihar Pradhan,Kirit D Makwana,Bart Ripperda*

Main category: physics.plasm-ph

TL;DR: The paper studies relativistic magnetic reconnection, validating Sweet-Parker scaling and analyzing energy conversion dynamics, inflow compressibility, and guide field effects.


<details>
  <summary>Details</summary>
Motivation: To understand relativistic magnetic reconnection's role in particle acceleration and high-energy radiation production.

Method: Uses relativistic resistive magnetohydrodynamics simulations, starting with Harris sheet configuration, to study reconnection rates, energy conversion, and scaling laws.

Result: Validates Sweet-Parker scaling; finds inflow slower due to thermal energy conversion; guide field reduces reconnection rate but not energy distribution.

Conclusion: Thermal energy dominates outflow; guide field and initial conditions have minimal impact on energy composition.

Abstract: Relativistic magnetic reconnection is a key process for accelerating charged
particles and producing high-energy radiation. We study this process using
relativistic resistive magnetohydrodynamics simulations. Starting with Harris
sheet configuration, we study time evolution of reconnection rate and the
Alfven four Mach number for outflow. These measurements validate the
Sweet-Parker scaling, consistent with previous studies. To study energy
conversion processes, we calculate Ohmic dissipation, crucial for understanding
how energy is converted between plasma and electromagnetic fields. Decomposing
electric field components relative to velocity field, we find that energy
conversion is initially dominated by the resistive electric field, but
convective electric fields take over as reconnection progresses. Plasma
primarily gains energy within the current sheet and near the separatrix. We
perform a scan of magnetization for mildly relativistic plasma to examine
scaling laws previously derived for non-relativistic inflow. We find the inflow
is slower than predicted, due to conversion of magnetic energy mostly into
thermal energy, causing strong compressibility. We calculate and verify the
scaling of the compressibility factor, providing a more accurate representation
of inflow dynamics. We analyze the impact of a guide field on reconnection and
energy partition, finding that a stronger guide field reduces the reconnection
rate but has minimal effect on the relative distribution of kinetic, magnetic,
and thermal energy. Addition of rotating guide field and variations in initial
pressure and density have little effect on the energy composition of the
outflow, with thermal energy consistently dominating at nearly 90%.

</details>


### [59] [Eigenvalues and Eigenfunctions of Landau Damping Oscillations in Very Weakly Collisional Plasma](https://arxiv.org/abs/2506.16415)
*Evgeny V. Polyachenko,Ilia G. Shukhman*

Main category: physics.plasm-ph

TL;DR: The paper analyzes Landau-damped oscillations in collisionless plasmas, showing how rare collisions transform singular modes into discrete regular modes with complex eigenvalues. It provides analytical and numerical validation for these findings.


<details>
  <summary>Details</summary>
Motivation: To understand the transition from singular eigenfunctions in collisionless plasmas to discrete regular modes under rare collisions, and to derive and validate analytical expressions for eigenvalue corrections and eigenfunction shapes.

Method: Analytical derivation of eigenvalue corrections and eigenfunction shapes, followed by high-precision numerical validation using a linear matrix eigenvalue problem approach.

Result: Rare collisions replace singular modes with discrete regular modes, with derived expressions for eigenvalue corrections, resonance region properties, and oscillation periods. Numerical results confirm analytical predictions.

Conclusion: The study bridges the gap between collisionless and collisional plasmas, providing insights into the behavior of Landau-damped oscillations under rare collisions, validated by both theory and numerics.

Abstract: Landau-damped oscillations in collisionless plasmas, described by van Kampen
and Case, are quasi-modes, representing a continuous superposition of singular
eigenfunctions, not true eigenmodes. Recent work by Ng et al. shows that even
rare collisions replace these singular modes with discrete regular modes having
complex eigenvalues for the phase velocity (or frequency), approaching Landau
eigenvalues in the collisionless limit. We analytically derive approximate
expressions for the eigenvalue correction due to rare collisions and for the
shape of the eigenfunction describing DF perturbations in velocity space,
demonstrating its increasing oscillations in the resonance region as the
collision frequency tends to zero. We also obtain approximate expressions for
the resonance region's width and peak value, and the oscillation period within
it. We validate these analytical results with high-precision numerical
calculations using a standard linear matrix eigenvalue problem approach.

</details>


### [60] [The Dielectric Response of Plasmas with Arbitrary Gyrotropic Velocity Distributions](https://arxiv.org/abs/2506.16431)
*Kristopher Klein,Daniel Verscharen*

Main category: physics.plasm-ph

TL;DR: The paper introduces an updated version of the Arbitrary Linear Plasma Solver (ALPS) for analyzing plasma dielectric tensor and normal mode responses, focusing on improved accuracy for weakly and moderately damped waves.


<details>
  <summary>Details</summary>
Motivation: Hot and tenuous plasmas often deviate from local thermodynamic equilibrium, requiring advanced methods to study their dielectric properties and wave behaviors.

Method: The updated ALPS tool uses an improved analytic continuation with a polynomial basis representation to model plasma waves accurately.

Result: The study shows continuity between bi-Maxwellian and arbitrary velocity distribution functions (VDFs) and evaluates VDF structure's impact on mode polarization and wave power dynamics.

Conclusion: The improved ALPS solver provides a robust framework for studying plasma wave responses in non-equilibrium conditions.

Abstract: Hot and tenuous plasmas are frequently far from local thermodynamic
equilibrium, necessitating sophisticated methods for determining the associated
plasma dielectric tensor and normal mode response. The Arbitrary Linear Plasma
Solver (\texttt{alps}) is a numerical tool for calculating such responses of
plasmas with arbitrary gyrotropic background velocity distribution functions
(VDFs). In order to model weakly and moderately damped plasma waves accurately,
we have updated to the code to use an improved analytic continuation enabled by
a polynomial basis representation. We demonstrate the continuity of solutions
to the linear Vlasov--Maxwell dispersion relation between bi-Maxwellian and
arbitrary VDF representations and evaluate the influence of VDF structure on
mode polarization and wave power emission and absorption.

</details>


### [61] [Snowplow Model Predictions for Plasma Temperature in Z pinch Discharges](https://arxiv.org/abs/2506.16551)
*Miguel Cárdenas,Alejandro Nettle,Leandro Núñez*

Main category: physics.plasm-ph

TL;DR: The paper introduces equations for plasma temperature and energy transfers in Z pinch discharges using the snowplow model, applies them to a prototype experiment, and finds a linear relationship between plasma temperature and initial capacitor bank voltage.


<details>
  <summary>Details</summary>
Motivation: To quantify plasma temperature and energy transfers in Z pinch discharges and analyze their behavior under parameter variations.

Method: Uses the snowplow model framework to derive equations for plasma temperature and energy transfers, then applies them to a prototype experiment.

Result: Plasma temperature grows linearly with the initial capacitor bank voltage, summarized in a compact formula.

Conclusion: The study provides a practical methodology and insights into plasma behavior in Z pinch discharges, with a key linear relationship identified.

Abstract: Using the general framework of the snowplow model, we introduce the equations
for computing plasma temperature in Z pinch discharges. We also present
expressions to quantify energy transfers in Z pinch discharges. We then apply
this methodology to estimate the temperature and energy transfers of a real
prototypical experiment and analyze how the plasma temperature behaves under
various modifications of the parameters of the prototypical experiment. Among
our discoveries we highlight that the temperature of the plasma in the
discharges grows linearly with the initial voltage at the capacitor bank. We
summarize this and other findings through a single, fairly compact formula.

</details>


### [62] [Learning Heat Transport Kernels Using a Nonlocal Heat Transport Theory-Informed Neural Network](https://arxiv.org/abs/2506.16619)
*Mufei Luo,Charles Heaton,Yizhen Wang,Daniel Plummer,Mila Fitzgerald,Francesco Miniati,Sam M. Vinko,Gianluca Gregori*

Main category: physics.plasm-ph

TL;DR: A data-driven framework using neural networks to model nonlocal heat transport in plasmas, trained on kinetic simulations, outperforms classical models by capturing dynamic behaviors.


<details>
  <summary>Details</summary>
Motivation: To address limitations of classical nonlocal heat transport models, which lack time-evolving kernels and adaptability to varying plasma conditions.

Method: Uses a nonlocal theory informed neural network trained on kinetic Particle-in-Cell simulations to learn spatiotemporal heat flux kernels.

Result: The model achieves strong agreement with kinetic benchmarks, capturing dynamic transport behaviors beyond classical formulations.

Conclusion: This approach advances data-driven modeling of nonlocal heat transport and enhances understanding of plasma dynamics.

Abstract: We present a data-driven framework for the modeling of nonlocal heat
transport in plasmas using a nonlocal theory informed neural network trained on
kinetic Particle-in-Cell simulations that span both local and nonlocal regimes.
The model learns spatiotemporal heat flux kernels directly from simulation
data, capturing dynamic transport behaviors beyond the reach of classical
formulations. Unlike time-independent kernel models such as Luciani Mora
Virmont and Schurtz Nicola\"i Busquet models, our approach yields physically
grounded, time-evolving kernels that adapt to varying plasma conditions. The
resulting predictions show strong agreement with kinetic benchmarks across
regimes. This offers a promising direction for data-driven modeling of nonlocal
heat transport and contributes to a deeper understanding of plasma dynamics.

</details>


### [63] [Fast solvers for Tokamak fluid models with PETSC -- Part I](https://arxiv.org/abs/2506.16676)
*Mark F. Adams,Jin Chen,Benjamin Sturdevant*

Main category: physics.plasm-ph

TL;DR: The paper introduces multigrid solvers to Tokamak MHD models, focusing on M3D-C1, and shows a 5x speedup over existing methods.


<details>
  <summary>Details</summary>
Motivation: To improve scalability and parallelism in Tokamak MHD models by reducing reliance on direct solvers, which are inefficient for modern hardware.

Method: Uses toroidal semi-coarsening multigrid with PETSC for the velocity solve in M3D-C1, leveraging existing solver structures.

Result: Demonstrates a 5x speedup over the one-level method in an MHD disruption test case with runaway electrons.

Conclusion: Multigrid solvers are effective for Tokamak MHD models, offering significant performance improvements with minimal code changes.

Abstract: This report develops the first step in adding multigrid solvers to scientific
and engineering-relevant magnetohydrodynamics (MHD) models of Tokamaks. These
models are characterized by a distinguished direction in the toroidal
coordinate that is partially aligned with the magnetic guide field, which
dominates the plasma dynamics. All Tokamak models exploit this structure, for
example, NIMROD (https://nimrodteam.org/) uses $2D$, unstructured, high-order
finite elements in the poloidal plane with Fourier modes in the toroidal
coordinate, and the $3D$, extended MHD code M3D-C1
(https://w3.pppl.gov/~nferraro/m3dc1.html) uses $2D$, unstructured $C^1$
elements in the poloidal plane with cubic Hermite functions in the toroidal
direction. This structure suggests adding toroidal semi-coarsening multigrid to
the existing solver and thereby reducing reliance on direct solvers, which do
not scale optimally and are not well suited to modern hardware that demands
extreme levels of parallelism. This report focuses on the velocity solve in
M3D-C1, using the PETSC -- the Portable, Extensible Toolkit for Scientific
Computation -- numerical library (https://petsc.org), and shows that with
little new application code, one-dimensional multigrid is about $5x$ faster
than the existing one-level method on an MHD disruption, with runaway
electrons, test problem.

</details>


### [64] [A low-cost plasma source aimed for medical applications using Ar as the working gas](https://arxiv.org/abs/2506.17072)
*Fellype do Nascimento,Bruno Henrique da Silva Leal,Konstantin Georgiev Kostov*

Main category: physics.plasm-ph

TL;DR: A low-cost plasma source modification was tested for medical use, reducing electrical current to safe levels while maintaining efficacy.


<details>
  <summary>Details</summary>
Motivation: To address the high manufacturing and operating costs of plasma sources for medical applications, ensuring safety and clinical viability.

Method: Modified a plasma source to use argon gas and post-discharge effluent instead of direct plasma jet, reducing electrical current.

Result: The modified source kept electrical current below safety thresholds and achieved comparable water activation results.

Conclusion: The modification makes the plasma source viable for medical applications by balancing cost, safety, and effectiveness.

Abstract: Because of the advances in equipment and several studies on medical
applications of atmospheric pressure plasmas over the last few years, plasma
sources are now being produced for clinical use. It has been demonstrated that
plasma sources can be constructed in such a way that their operation and
application are safe, and that they present relevant clinical results. However,
the manufacturing and operating costs of plasma sources can be decisive for
their adoption in medical procedures. In this work, a simple modification of a
plasma source that has a low production cost was investigated in order to
evaluate the viability of its use for medical applications using argon as the
working gas. Without this modification, the plasma source produces high levels
of electrical current, which makes it unfeasible for use in medical
applications. With the proposed modification, the treatment is no longer
carried out using the plasma jet directly, but rather with the post-discharge
effluent enriched with reactive oxygen and nitrogen species. As a result, the
electrical current reaching the target remains below the safety threshold
established for plasma applications in humans. Application tests on water
activation indicate that both operating conditions can yield comparable
outcomes.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [65] [Catastrophic Formation of Macro-Scale Flow and Magnetic Fields in the Relativistic Gas of Binary Systems](https://arxiv.org/abs/2506.16761)
*E. Saralidze,N. L. Shatashvili,S. M. Mahajan,E. Dadiani*

Main category: astro-ph.SR

TL;DR: A quasi-equilibrium analysis of a multi-component plasma explains catastrophic energy transformations in binary astrophysical systems, particularly white dwarfs.


<details>
  <summary>Details</summary>
Motivation: To understand sudden energy changes in astrophysical plasmas, especially in binary systems with degenerate and hot electron components.

Method: Analytical derivation of conditions for catastrophic changes in a plasma with classical ions, degenerate electrons, and hot electrons.

Result: Identified pathways for sudden energy shifts, leading to macro-scale flow kinetic and magnetic energies, with hot electrons carrying most kinetic energy.

Conclusion: The model explains observed characteristics of magnetic and dense white dwarf systems, highlighting energy conversion mechanisms.

Abstract: It is shown that a simple quasi-equilibrium analysis of a multi-component
plasma can be harnessed to explain catastrophic energy transformations in
astrophysical objects. We limit ourselves to the particular class of binary
systems for which the typical plasma consists of one classical ion component,
and two relativistic electron components - the bulk degenerate electron gas
with a small contamination of hot electrons. We derive, analytically, the
conditions conducive to such a catastrophic change. The pathway to such sudden
changes is created by the slow changes in the initial parameters so that the
governing equilibrium state can no longer be sustained and the system must find
a new equilibrium that could have vastly different energy mix - of thermal,
flow-kinetic and magnetic energies. In one such scenario, macro-scale flow
kinetic, and magnetic energies abound in the final state. For the given
multi-component plasma, we show that the flow (strongly Super-Alfv\'enic)
kinetic energy is mostly carried by the small hot electron component. Under
specific conditions, it is possible to generate strong macro-scale magnetic
(velocity) field when all of the flow (magnetic) field energy is converted to
the magnetic (velocity) field energy at the catastrophe. The analysis is
applied to explain various observed characteristics of white dwarf (WD)
systems, in particular, of the magnetic and dense/degenerate type.

</details>


### [66] [Electromagnetic radiation by turbulent, magnetized and randomly inhomogeneous solar radio sources generated by electron beams](https://arxiv.org/abs/2506.16816)
*Catherine Krafft,Alexandr Volokitin,Francisco Javier Polanco-Rodríguez,Philippe Savoini*

Main category: astro-ph.SR

TL;DR: Only a small fraction of electromagnetic energy (≤10%) escapes Type III solar radio bursts, mainly as O-mode waves, with most energy radiated in Z-mode, observable only near sources.


<details>
  <summary>Details</summary>
Motivation: To understand the radiation properties and energy distribution of Type III solar radio bursts, which are generated by electron beams in inhomogeneous solar plasmas.

Method: Three independent and converging approaches were used to analyze the electromagnetic energy radiated at plasma frequency and its harmonics.

Result: Most energy is radiated in Z-mode (observable near sources), with ≤10% escaping as O-mode or X-mode waves.

Conclusion: Findings support interpretations of solar radio observations by Parker Solar Probe and Solar Orbiter, highlighting limited energy escape from beam-generated sources.

Abstract: During Type III solar radio bursts, electromagnetic waves are radiated at
plasma frequency $\omega_p$ and its harmonics by electrostatic wave turbulence
generated by electron beams ejected by Sun in randomly inhomogeneous solar wind
and coronal plasmas. These emissions, detected since decades by spacecraft and
radiotelescopes, are split by the plasma magnetic field into three modes
$\mathcal{X}$, $\mathcal{O}$ and $\mathcal{Z}$ of different dispersion,
polarization and radiation properties. This work demonstrates, using three
independent and converging approaches, that only a small fraction of
electromagnetic energy radiated at $\omega_p$ ($\lesssim10\%$) is escaping from
beam-generated radio sources, mainly as $\mathcal{O}$-mode waves and, depending
on plasma conditions, as $\mathcal{X}$-mode waves. Most energy is radiated in
$\mathcal{Z}$-mode and can therefore be only observed close to sources. Results
have major implications for solar radio emission and provide strong support for
interpretation of observations performed up to close distances to Sun by Parker
Solar Probe and Solar Orbiter spacecraft.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [67] [Ahlfors-regularity for minimizers of a multiphase optimal design problem](https://arxiv.org/abs/2506.15861)
*Luca Esposito,Lorenzo Lamberti,Giovanni Pisante*

Main category: math.OC

TL;DR: The paper proves Alhfors-regularity for minimizers in a multiphase optimal design problem, combining perimeter and Dirichlet energy terms.


<details>
  <summary>Details</summary>
Motivation: To extend regularity results to multiphase variational problems with discontinuous energy terms.

Method: Uses a penalization method and decay estimates of the energy to analyze the interfaces.

Result: The optimal interfaces are shown to be (n-1)-Alhfors-regular.

Conclusion: The study confirms the regularity of interfaces in such multiphase problems, contributing to broader understanding.

Abstract: We establish an Alhfors-regularity result for minimizers of a multiphase
optimal design problem. It is a variant of the classical variational problem
which involves a finite number of chambers $\mathcal{E}(i)$ of prescribed
volume that partition a given domain $\Omega\subset\mathbb{R}^n$. The cost
functional associated with a configuration
$\left(\{\mathcal{E}(i)\}_i,u\right)$ is made up of the perimeter of the
partition interfaces and a Dirichlet energy term, which is discontinuous across
the interfaces. We prove that the union of the optimal interfaces is
$(n-1)$-Alhfors-regular via a penalization method and decay estimates of the
energy.

</details>


### [68] [Output Regulation for Impedance Passive Systems with Input Saturation](https://arxiv.org/abs/2506.16365)
*Thavamani Govindaraj,Lassi Paunonen*

Main category: math.OC

TL;DR: The paper addresses output tracking and disturbance rejection in infinite-dimensional systems with input saturation, proposing a control law combining error feedback and feedforward terms.


<details>
  <summary>Details</summary>
Motivation: To solve control problems in systems with input saturation, particularly for infinite-dimensional systems like heat and wave equations.

Method: A control law with stabilizing error feedback and feedforward terms based on reference and disturbance signals is proposed.

Result: The method is applied to design control laws for a 2D heat equation and a 1D wave equation with boundary control and observation.

Conclusion: The proposed control law effectively handles output tracking and disturbance rejection in the studied systems.

Abstract: We consider output tracking and disturbance rejection for abstract
infinite-dimensional systems with input saturation. We solve the control
problem using a control law which consists of a stabilising error feedback term
and a feedforward term based on the reference and disturbance signals. We use
our results to design control laws for output tracking and disturbance
rejection for a two-dimensional heat equation and a one-dimensional wave
equation with boundary control and observation.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [69] [A many-body characterization of the fundamental gap in monolayer CrI$_3$](https://arxiv.org/abs/2506.17038)
*Daniel Staros,Abdulgani Annaberdiyev,Kevin Gasperich,Anouar Benali,Panchapakesan Ganesh,Brenda Rubenstein*

Main category: cond-mat.mtrl-sci

TL;DR: DMC methods predict CrI$_3$'s fundamental gap as 2.9(1) eV, matching experiments and $GW$ results, emphasizing correlation over spin-orbit effects.


<details>
  <summary>Details</summary>
Motivation: To accurately predict the fundamental gap of monolayer CrI$_3$ and validate DMC's reliability for 2D materials.

Method: Applied fixed-node and fixed-phase spin-orbit DMC methods, comparing neutral promotions and quasiparticle definitions.

Result: Fundamental gap of 2.9(1) eV aligns with optical spectroscopy and $GW$ results, showing correlation's critical role.

Conclusion: DMC is effective for benchmarking 2D materials, stressing the need for beyond-DFT methods.

Abstract: The many-body fixed-node and fixed-phase spin-orbit Diffusion Monte Carlo
(DMC) methods are applied to accurately predict the fundamental gap of
monolayer CrI$_3$ - the first experimentally-realized 2D material with
intrinsic magnetism. The fundamental gap obtained, 2.9(1)~eV, agrees well with
the highest peak in optical spectroscopy measurements and a previous $GW$
result. We numerically show that as expected in DMC the same value of the
fundamental gap is obtained in the thermodynamic limit using both neutral
promotions and the standard quasiparticle definition of the gap based on the
ionization potential and electron affinity. Additional analysis of the
differences between density matrices formed in different bases using
configuration interaction calculations explains why a single-reference trial
wave function can produce an accurate excitation. We find that accounting for
electron correlation is more crucial than accounting for spin-orbit effects in
determining the fundamental gap. These results highlight how DMC can be used to
benchmark 2D material physics and emphasize the importance of using beyond-DFT
methods for studying 2D materials.

</details>


### [70] [Mesoscale FEM Model of Concrete: Statistical Assessment of Inherent Stress Concentrations in Dependence on Phase Heterogeneity](https://arxiv.org/abs/2506.16242)
*Jan Mašek,Petr Miarka*

Main category: cond-mat.mtrl-sci

TL;DR: A mesoscale finite element model (MFEM) is developed to study fracture at the aggregate-cement interface in concrete, capturing detailed stress distributions and damage evolution.


<details>
  <summary>Details</summary>
Motivation: To understand the mechanical response of concrete by analyzing stress concentrations and damage at the mesoscale, particularly at the aggregate-matrix interface.

Method: The MFEM approach is used to model stress distributions and damage, leveraging high-performance computing (HPC) for detailed analysis. Various stiffness ratios between matrix and aggregates are considered.

Result: The model reveals localized stress effects and evaluates the impact of material heterogeneity, including recycled crushed bricks as aggregates, on stress distribution and failure mechanisms.

Conclusion: The MFEM provides valuable insights into concrete's mechanical behavior, highlighting the importance of mesoscale analysis for understanding damage and failure.

Abstract: Concrete heterogeneity originates from its production process, which involves
bonding aggregates with a binder matrix. This study presents a mesoscale finite
element model (MFEM) that offers detailed insights into the fracture process at
the aggregate-cement matrix interface, focusing on one of concrete's key
properties: its mechanical response. Unlike discrete models, which often
average out critical stress concentrations within the mesostructure, the MFEM
approach captures detailed stress distributions, revealing localized effects
crucial for understanding damage evolution. Although computationally more
demanding, the MFEM leverages modern high-performance computing (HPC) to
provide a detailed description of the stress field and material damage across
different phases and interfaces. Various matrix-to-aggregate stiffness ratios
are considered to evaluate the influence of material heterogeneity on the
stress field. The results are based on a statistical evaluation of stress
concentrations arising from variations in material stiffness. The model is
applied to investigate the impact of using recycled crushed bricks as
aggregates in concrete, with particular emphasis on the stiffness mismatch
between the matrix and aggregates. The study examines how this stiffness
contrast affects stress distribution and ultimately influences the composite's
failure mechanisms.

</details>


<div id='astro-ph.GA'></div>

# astro-ph.GA [[Back]](#toc)

### [71] [Galactic echoes](https://arxiv.org/abs/2506.16512)
*Rimpei Chiba,Jupiter Ding,Chris Hamilton,Matthew W. Kunz,Scott Tremaine*

Main category: astro-ph.GA

TL;DR: The paper explores whether substructures like the Gaia 'Snail' in stellar phase space could be phase-space echoes, a phenomenon from plasma physics, and tests this in a Milky Way model.


<details>
  <summary>Details</summary>
Motivation: To understand if observed substructures in the Milky Way's phase space are echoes from multiple perturbations, not just a single event.

Method: Derived a galactic version of plasma echo theory using angle-action variables and tested it with idealized simulations, including diffusion effects.

Result: The Gaia Snail is unlikely a pure echo, but phase-space echoes are expected to be common in disc galaxies.

Conclusion: Phase-space echoes are a generic phenomenon in disc galaxies, though not the sole explanation for the Gaia Snail.

Abstract: Gaia has revealed a variety of substructures in the phase space of stars in
the Solar neighborhood, including the vertical `Snail' in $(z,v_z)$ space. Such
substructures are often interpreted as the incompletely phase-mixed response of
the disc stars to a single perturbation, such as an impulsive encounter with a
satellite galaxy. In this paper we consider the possibility that such
structures contain manifestations of phase space echoes. First established in
plasma physics in the 1960s, echoes arise when a collisionless system is
perturbed twice: the macroscopic responses to both perturbations mix to small
scales in phase space, whereupon they couple nonlinearly, producing a third
macroscopic `echo' response without the need for a third perturbation. We
derive the galactic analogue of the plasma echo theory using angle-action
variables and apply it to a one-dimensional model of vertical motion in the
Milky Way. We verify the predicted echo behavior using idealized test particle
simulations, both with and without the inclusion of diffusion through orbital
scattering off molecular clouds. While we conclude that the Gaia Snail itself
is unlikely a (pure) echo effect, the basic physics we uncover is sufficiently
generic that we expect phase-space echoes to be common in disc galaxies.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [72] [Hemodynamic Simulation in the Aortic Arch Under Anemic Diabetic and Healthy Blood Flow Conditions Using Computational Fluid Dynamics](https://arxiv.org/abs/2506.16763)
*Farzana Akter Tina,Hashnayne Ahmed,Hena Rani Biswas*

Main category: physics.flu-dyn

TL;DR: CFD simulations analyzed blood flow in the aortic arch under anemic, diabetic, and healthy conditions, revealing distinct hemodynamic impacts due to rheological differences.


<details>
  <summary>Details</summary>
Motivation: To understand how blood rheology and vessel geometry affect hemodynamics in different health conditions, aiding non-invasive vascular risk assessment.

Method: Computational fluid dynamics (CFD) simulations with a non-Newtonian Carreau viscosity model, analyzing velocity fields, pressure distributions, and wall shear stress (WSS).

Result: Anemic blood showed smooth, low-resistance flow with reduced WSS; diabetic blood had elevated viscosity, higher WSS, and flow separation; healthy cases maintained balanced hemodynamics.

Conclusion: The study links rheological properties to cardiovascular stress, advocating for CFD in risk assessment and future integration of patient-specific data for clinical relevance.

Abstract: This study investigates the hemodynamic behavior of blood flow in the aortic
arch across anemic, diabetic, and healthy conditions using computational fluid
dynamics (CFD) simulations with a non-Newtonian Carreau viscosity model.
Velocity fields, pressure distributions, and wall shear stress (WSS) patterns
were analyzed to assess the impact of blood rheology and vessel geometry.
Anemic blood, with low viscosity and hematocrit, produced smooth,
low-resistance flow with reduced WSS and pressure gradients, potentially
impairing perfusion. Diabetic blood exhibited elevated viscosity, leading to
increased flow resistance, higher WSS, and localized separation at arterial
branches -- conditions associated with vascular stress and remodeling. Healthy
cases showed balanced hemodynamic behavior with localized flow acceleration but
maintained physiological ranges. These findings highlight the mechanistic links
between rheological properties and cardiovascular stress, supporting the role
of CFD in non-invasive vascular risk assessment and motivating future
integration of patient-specific data and structural modeling for enhanced
clinical relevance.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [73] [Formation of multiple Black Holes from Cauchy data](https://arxiv.org/abs/2506.16117)
*Dawei Shen,Jingbo Wan*

Main category: gr-qc

TL;DR: A family of asymptotically flat Cauchy initial data for Einstein vacuum equations is constructed, showing no trapped surfaces initially but forming multiple causally independent trapped surfaces later, implying multiple black hole formation.


<details>
  <summary>Details</summary>
Motivation: To explore the formation of multiple black holes from initial data without trapped surfaces, challenging assumptions about black hole formation.

Method: Modifies a vacuum geometrostatic manifold with the Brill-Lindquist metric, replacing regions with dynamical spacetime slices, using Christodoulou's short-pulse framework and annular gluing.

Result: Demonstrates initial data without trapped surfaces can evolve into multiple black holes, supporting weak cosmic censorship.

Conclusion: The study provides insights into black hole formation dynamics and validates the annular gluing method for constructing such initial data.

Abstract: We construct a family of asymptotically flat Cauchy initial data for the
Einstein vacuum equations that contain no trapped surfaces, yet whose future
development admits multiple causally independent trapped surfaces. Assuming the
weak cosmic censorship conjecture, this implies the formation of multiple black
holes in finite time. The initial data are obtained by surgically modifying a
vacuum geometrostatic manifold equipped with the Brill-Lindquist metric: the
data agree exactly with the Brill-Lindquist metric outside a collection of
balls centered at its multiple poles, and each ball is replaced by a
constant-time slice of a well-prepared dynamical spacetime. The construction is
based on Christodoulou's short-pulse framework, a stability analysis in the
finite future of the short-pulse region, the geometry of geometrostatic
manifolds with small mass and large separation, and the obstruction-free
annular gluing method developed by Mao-Oh-Tao. The absence of trapped surfaces
in the initial data is verified via a standard mean curvature comparison
argument.

</details>


### [74] [Weak null singularity for the Einstein-Euler system](https://arxiv.org/abs/2506.16635)
*Yuefeng Song*

Main category: gr-qc

TL;DR: The paper studies weak null singularities in self-gravitating relativistic fluids, showing they persist and fluid variables extend continuously.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of relativistic fluids near weak null singularities, expected in black hole interiors.

Method: Analyzes the Einstein-Euler system for a perfect relativistic fluid near weak null singularities, leveraging the speed of sound being less than light speed.

Result: Weak null singularities persist in the presence of the fluid, and fluid variables extend continuously to the singularity.

Conclusion: The findings confirm the robustness of weak null singularities in relativistic fluids, extending prior vacuum results.

Abstract: We study the behavior of a self-gravitating perfect relativistic fluid
satisfying the Einstein-Euler system in the presence of a weak null terminal
spacetime singularity. This type of singularities is expected in the interior
of generic dynamical black holes. In the vacuum case, weak null singularities
have been constructed locally by Luk, where the metrics extend continuously to
the singularities while the Christoffel symbols fail to be square integrable in
any neighborhood of any point on the singular boundaries. We prove that this
type of singularities persists in the presence of a self-gravitating fluid.
Moreover, using the fact that the speed of sound is strictly less than the
speed of light, we prove that the fluid variables also extend continuously to
the singularity.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [75] [Leveraging Influence Functions for Resampling Data in Physics-Informed Neural Networks](https://arxiv.org/abs/2506.16443)
*Jonas R. Naujoks,Aleksander Krasowski,Moritz Weckbecker,Galip Ümit Yolcu,Thomas Wiegand,Sebastian Lapuschkin,Wojciech Samek,René P. Klausen*

Main category: cs.LG

TL;DR: The paper explores using influence functions from explainable AI (XAI) to improve training data sampling for physics-informed neural networks (PINNs), enhancing prediction accuracy.


<details>
  <summary>Details</summary>
Motivation: PINNs are valuable for solving PDEs, but their training relies on readily available data. The study aims to leverage XAI methods to optimize data sampling and improve PINN performance.

Method: The authors apply influence function-based sampling approaches to select training data points, targeting those with the most significant impact on the model.

Result: Targeted resampling using influence functions improves prediction accuracy in PINNs, demonstrating the practical utility of XAI methods in this context.

Conclusion: Influence function-based sampling enhances PINN training, showcasing the potential of XAI techniques in scientific machine learning.

Abstract: Physics-informed neural networks (PINNs) offer a powerful approach to solving
partial differential equations (PDEs), which are ubiquitous in the quantitative
sciences. Applied to both forward and inverse problems across various
scientific domains, PINNs have recently emerged as a valuable tool in the field
of scientific machine learning. A key aspect of their training is that the data
-- spatio-temporal points sampled from the PDE's input domain -- are readily
available. Influence functions, a tool from the field of explainable AI (XAI),
approximate the effect of individual training points on the model, enhancing
interpretability. In the present work, we explore the application of influence
function-based sampling approaches for the training data. Our results indicate
that such targeted resampling based on data attribution methods has the
potential to enhance prediction accuracy in physics-informed neural networks,
demonstrating a practical application of an XAI method in PINN training.

</details>


### [76] [Consistent Sampling and Simulation: Molecular Dynamics with Energy-Based Diffusion Models](https://arxiv.org/abs/2506.17139)
*Michael Plainer,Hao Wu,Leon Klein,Stephan Günnemann,Frank Noé*

Main category: cs.LG

TL;DR: Diffusion models trained on equilibrium molecular distributions show inconsistencies in generated samples and forces. A proposed energy-based model with Fokker-Planck regularization improves consistency and sampling.


<details>
  <summary>Details</summary>
Motivation: Address inconsistencies in diffusion models when used for coarse-grained molecular dynamics simulations, particularly at small timesteps.

Method: Introduce an energy-based diffusion model with a Fokker-Planck-derived regularization term to enforce consistency.

Result: Demonstrated effectiveness on toy systems and alanine dipeptide, achieving enhanced consistency and efficient sampling.

Conclusion: The proposed model provides a state-of-the-art transferable Boltzmann emulator for dipeptides, improving simulation reliability.

Abstract: Diffusion models have recently gained significant attention due to their
effectiveness in various scientific domains, including biochemistry. When
trained on equilibrium molecular distributions, diffusion models provide both:
a generative procedure to sample equilibrium conformations and associated
forces derived from the model's scores. However, using the forces for
coarse-grained molecular dynamics simulations uncovers inconsistencies in the
samples generated via classical diffusion inference and simulation, despite
both originating from the same model. Particularly at the small diffusion
timesteps required for simulations, diffusion models fail to satisfy the
Fokker-Planck equation, which governs how the score should evolve over time. We
interpret this deviation as an indication of the observed inconsistencies and
propose an energy-based diffusion model with a Fokker-Planck-derived
regularization term enforcing consistency. We demonstrate the effectiveness of
our approach on toy systems, alanine dipeptide, and introduce a
state-of-the-art transferable Boltzmann emulator for dipeptides that supports
simulation and demonstrates enhanced consistency and efficient sampling.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [77] [Extension of the CC($P$;$Q$) Formalism to the Electron Attachment and Ionization Potential Equation-of-Motion Coupled-Cluster Frameworks](https://arxiv.org/abs/2506.16320)
*Karthik Gururangan,Stephen H. Yuwono,A. Eugene DePrince III,Piotr Piecuch*

Main category: physics.chem-ph

TL;DR: The paper introduces EA/IP-CC(t;3) methods, combining EA/IP-EOMCC with CC(P;Q), to approximate high-level energetics for open-shell molecules with reduced computational effort and high accuracy.


<details>
  <summary>Details</summary>
Motivation: To develop efficient and accurate methods for describing electronic states of open-shell molecules, approximating high-level EA/IP-EOMCC energetics with lower computational cost.

Method: Combines EA/IP-EOMCC with CC(P;Q) formalism, focusing on active-orbital-based approaches (EA/IP-CC(t;3)) to approximate 3p-2h and 3h-2p excitations beyond CCSD.

Result: EA/IP-CC(t;3) achieves sub-millihartree accuracy relative to high-level methods while reducing computational effort and outperforming CR-EOMCC(2,3) counterparts.

Conclusion: The proposed EA/IP-CC(t;3) methods offer a computationally efficient and accurate alternative for studying electronic states of open-shell molecules.

Abstract: We combine the electron attachment (EA) and ionization potential (IP)
equation-of-motion (EOM) coupled-cluster (CC) approaches with the CC($P$;$Q$)
formalism. The resulting methodologies are used to describe the electronic
states of several open-shell molecules, with the goal of approximating
high-level EA/IP-EOMCC energetics corresponding to a full treatment of
3-particle-2-hole (3$p$-2$h$) and 3-hole-2-particle (3$h$-2$p$) excitations on
top of CC with singles and doubles (CCSD). We show that the
active-orbital-based EA/IP-EOMCC CC($P$;$Q$) approaches, abbreviated as
EA/IP-CC(t;3), achieve sub-millihartree accuracies relative to the parent
EA-EOMCCSD(3$p$-2$h$)/IP-EOMCCSD(3$h$-2$p$) data using reduced computational
effort, while improving upon their completely renormalized EA/IP-CR-EOMCC(2,3)
counterparts.

</details>


<div id='cond-mat.quant-gas'></div>

# cond-mat.quant-gas [[Back]](#toc)

### [78] [Single-shot thermometry of simulated Bose--Einstein condensates using artificial intelligence](https://arxiv.org/abs/2506.16925)
*Jack Griffiths,Steven A. Wrathmall,Simon A. Gardiner*

Main category: cond-mat.quant-gas

TL;DR: An AI approach using a convolutional neural network enables rapid, non-destructive estimation of thermodynamic parameters in ultracold Bose gases from single-shot density profiles, generalizing to unseen trap geometries and dynamic states.


<details>
  <summary>Details</summary>
Motivation: Conventional measurement techniques for thermodynamic parameters in ultracold Bose gases are destructive and prone to experimental uncertainties, necessitating a non-destructive, precise alternative.

Method: A convolutional neural network is trained on quasi-2D 'pancake' condensates in harmonic traps to estimate chemical potential and temperature from density profiles, demonstrating zero-shot generalization to other geometries and dynamic states.

Result: The model achieves rapid parameter extraction with high accuracy, even for toroidally trapped condensates and non-equilibrium states, with errors as low as a few nanokelvin.

Conclusion: Supervised learning can overcome traditional thermometry limitations, offering real-time analysis potential for quantum gas experiments, improving precision and workflow efficiency.

Abstract: Precise determination of thermodynamic parameters in ultracold Bose gases
remains challenging due to the destructive nature of conventional measurement
techniques and inherent experimental uncertainties. We demonstrate an
artificial intelligence approach for rapid, non-destructive estimation of the
chemical potential and temperature from single-shot, in situ imaged density
profiles of finite-temperature Bose gases. Our convolutional neural network is
trained exclusively on quasi-2D `pancake' condensates in harmonic trap
configurations. It achieves parameter extraction within fractions of a second.
The model also demonstrates zero-shot generalisation across both trap geometry
and thermalisation dynamics, successfully estimating thermodynamic parameters
for toroidally trapped condensates with errors of only a few nanokelvin despite
no prior exposure to such geometries during training, and maintaining
predictive accuracy during dynamic thermalisation processes after a relatively
brief evolution without explicit training on non-equilibrium states. These
results suggest that supervised learning can overcome traditional limitations
in ultracold atom thermometry, with extension to broader geometric
configurations, temperature ranges, and additional parameters potentially
enabling comprehensive real-time analysis of quantum gas experiments. Such
capabilities could significantly streamline experimental workflows whilst
improving measurement precision across a range of quantum fluid systems.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [79] [Frequently Used References For Atomic Data In X-ray Spectroscopy](https://arxiv.org/abs/2506.17106)
*N. Hell,G. V. Brown,M. E. Eckart,A. J. Fairchild,C. A. Kilbourne,M. A. Leutenegger,F. S. Porter,M. C. Witthoeft*

Main category: astro-ph.IM

TL;DR: A curated list of atomic physics references for X-ray spectroscopy, focusing on K-shell transitions, providing high-accuracy data for plasma diagnostics.


<details>
  <summary>Details</summary>
Motivation: To offer accurate atomic physics reference data for interpreting high-resolution spectra, especially for plasma diagnostics in X-ray spectroscopy.

Method: Compiles references and tables for physical constants, transition energies, fluorescence lines, and radiative branching ratios, emphasizing H-, He-, and Li-like ions.

Result: Provides high-accuracy reference energies for calibration and empirical models for neutral line shapes, aligning with XRISM/Resolve standards.

Conclusion: Serves as a resource for finding relevant references and formatted tables, with acknowledgment to original authors through citations.

Abstract: Accurate atomic physics reference data are a crucial requirement for analysis
and interpretation of observed spectra, even more so for observations with high
spectral resolution. This document provides a curated list of atomic physics
references frequently used for plasma diagnostics in X-ray spectroscopy,
outside of comprehensive plasma models that typically come with their own
underlying atomic databases. The list includes references to physical
constants, laboratory benchmarks, transition energies, position and line shapes
of neutral fluorescence lines, radiative branching ratios, and commonly used
notation for prominent transitions. Quick-look tables for transition energies
in H-, He-, and Li-like ions and line positions and shapes for fluorescence
lines in neutrals. The main focus is on K-shell transitions. For the H- and
He-like tables, we cite state-of-the art calculations that we consider
currently the best available reference energies, which are considered high
accuracy and thus typically used for energy scale calibration in laboratory
measurements. Omissions in these tables are due to the lack of availability in
the chosen references, and are not a statement about the relevance of these
lines. Due to their complex and highly source-dependent line shape, the atomic
data for neutrals is of lower accuracy than that for the highly charged ions,
and the best reference data for these line shapes typically consist of
empirical models derived from very high-resolution laboratory measurements. The
table for neutrals provided here is consistent with the reference used for the
energy gain scale calibration of XRISM/Resolve. This document is meant to serve
as a resource to help find relevant references and conveniently formatted
overview tables. When making use of the information found in these papers,
credit should be given to their original authors by citing the appropriate
references.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [80] [Graphics4Science: Computer Graphics for Scientific Impacts](https://arxiv.org/abs/2506.15786)
*Peter Yichen Chen,Minghao Guo,Hanspeter Pfister,Ming Lin,William Freeman,Qixing Huang,Han-Wei Shen,Wojciech Matusik*

Main category: cs.GR

TL;DR: The paper explores the relationship between computer graphics and science, emphasizing its role as a modeling language for scientific challenges.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between computer graphics and scientific communities, leveraging graphics methods for scientific discovery.

Method: Uses core graphics techniques like geometric reasoning and physical modeling to address scientific problems, especially in data-scarce scenarios.

Result: Highlights past and ongoing contributions of graphics to science and identifies open questions for future collaboration.

Conclusion: Encourages the graphics community to engage with science, tackle high-impact problems, and contribute to scientific discovery.

Abstract: Computer graphics, often associated with films, games, and visual effects,
has long been a powerful tool for addressing scientific challenges--from its
origins in 3D visualization for medical imaging to its role in modern
computational modeling and simulation. This course explores the deep and
evolving relationship between computer graphics and science, highlighting past
achievements, ongoing contributions, and open questions that remain. We show
how core methods, such as geometric reasoning and physical modeling, provide
inductive biases that help address challenges in both fields, especially in
data-scarce settings. To that end, we aim to reframe graphics as a modeling
language for science by bridging vocabulary gaps between the two communities.
Designed for both newcomers and experts, Graphics4Science invites the graphics
community to engage with science, tackle high-impact problems where graphics
expertise can make a difference, and contribute to the future of scientific
discovery. Additional details are available on the course website:
https://graphics4science.github.io

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [81] [A Comparative Analysis of Principal Component Analysis (PCA) and Singular Value Decomposition (SVD) as Dimensionality Reduction Techniques](https://arxiv.org/abs/2506.16663)
*Michael Gyimadu,Gregory Bell*

Main category: cs.CV

TL;DR: The paper compares PCA and SVD for dimensionality reduction in high-dimensional image data, focusing on interpretability, numerical stability, and matrix shape suitability, without empirical benchmarking.


<details>
  <summary>Details</summary>
Motivation: To provide a theoretical comparison of PCA and SVD for dimensionality reduction, aiding in algorithm selection without empirical testing.

Method: Derives PCA and SVD from first principles and evaluates them based on interpretability, numerical stability, and matrix shape suitability.

Result: Synthesizes guidelines for choosing between PCA and SVD, leveraging classical and recent numerical literature.

Conclusion: Highlights limitations and suggests future experimental work to validate theoretical findings.

Abstract: High-dimensional image data often require dimensionality reduction before
further analysis. This paper provides a purely analytical comparison of two
linear techniques-Principal Component Analysis (PCA) and Singular Value
Decomposition (SVD). After the derivation of each algorithm from first
principles, we assess their interpretability, numerical stability, and
suitability for differing matrix shapes. building on classical and recent
numerical literature, We synthesize rule-of-thumb guidelines for choosing one
out of the two algorithms without empirical benchmarking, building on classical
and recent numerical literature. Limitations and directions for future
experimental work are outlined at the end.

</details>


<div id='math.AG'></div>

# math.AG [[Back]](#toc)

### [82] [Complexity of sparse polynomial solving 3: Infinity](https://arxiv.org/abs/2506.17086)
*Gregorio Malajovich*

Main category: math.AG

TL;DR: The paper presents a homotopy algorithm for solving polynomial systems over toric varieties, avoiding spurious roots by leveraging local charts and bounding complexity by condition length.


<details>
  <summary>Details</summary>
Motivation: To solve polynomial systems with real or complex coefficients without introducing spurious or degenerate roots, which can occur in projective or complex spaces.

Method: A homotopy algorithm is locally defined on toric variety charts, with complexity bounded by the condition length (integral of the toric condition number).

Result: The method enables stable computations near 'toric infinity,' overcoming limitations of prior approaches.

Conclusion: Solving over toric varieties with the proposed algorithm avoids spurious roots and improves computational stability.

Abstract: A theory of numerical path-following in toric varieties was suggested in two
previous papers. The motivation is solving systems of polynomials with real or
complex coefficients. When those polynomials are not assumed 'dense', solving
them over projective space or complex space may introduce spurious, degenerate
roots or components. Spurious roots may be avoided by solving over toric
varieties.
  In this paper, a homotopy algorithm is locally defined on charts of the toric
variety. Its complexity is bounded linearly by the condition length, that is
the integral along the lifted path (coefficients and solution) of thetoric
condition number. Those charts allow for stable computations near "toric
infinity",which was not possible within the technology of the previous papers.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [83] [Quasi-Monte Carlo with one categorical variable](https://arxiv.org/abs/2506.16582)
*Valerie N. P. Ho,Art B. Owen,Zexin Pan*

Main category: stat.CO

TL;DR: RQMC estimation for multivariate integrals with finite-valued variables benefits from oversampling small mixture components and using power-of-2 subsample sizes.


<details>
  <summary>Details</summary>
Motivation: The study addresses RQMC estimation in cases where a variable has finite values, common in mixture distributions like importance sampling and transport maps.

Method: Analyzes RQMC integration error rates and proposes oversampling small mixture components and using power-of-2 subsample sizes for Sobol' points.

Result: Oversampling small components and power-of-2 subsampling improve RQMC estimation accuracy.

Conclusion: Optimal RQMC performance is achieved by strategic oversampling and structured subsampling.

Abstract: We study randomized quasi-Monte Carlo (RQMC) estimation of a multivariate
integral where one of the variables takes only a finite number of values. This
problem arises when the variable of integration is drawn from a mixture
distribution as is common in importance sampling and also arises in some recent
work on transport maps. We find that when integration error decreases at an
RQMC rate that it is then beneficial to oversample the smallest mixture
components instead of using a proportional allocation. We also find that for
the most accurate RQMC sampling methods, it is advantageous to arrange that our
$n=2^m$ randomized Sobol' points split into subsample sizes that are also
powers of~$2$.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [84] [Fast Converging Single Trace Quasi-local PMCHWT Equation for the Modelling of Composite Systems](https://arxiv.org/abs/2506.16376)
*Kristof Cools*

Main category: cs.CE

TL;DR: The paper introduces a single trace quasi-local PMCHWT equation for modeling scattering in composite systems with junctions, improving iterative solution efficiency and avoiding interior resonances.


<details>
  <summary>Details</summary>
Motivation: Existing methods for solving PMCHWT equations in systems with junctions either fail to apply Calderón preconditioning or require doubling degrees of freedom, leading to inefficiencies and approximate solutions.

Method: A single trace quasi-local PMCHWT equation is introduced, generalizing the classic PMCHWT. Its discretization is detailed, and numerical experiments validate its performance.

Result: The method shows slow iteration growth with mesh refinement, correctness, convergence, and efficiency. It avoids interior resonances.

Conclusion: The proposed method effectively addresses limitations of existing approaches, offering a robust solution for scattering problems in composite systems with junctions.

Abstract: The PMCHWT integral equation enables the modelling of scattering of
time-harmonic fields by penetrable, piecewise homogeneous, systems. They have
been generalised to include the modelling of composite systems that may contain
junctions, i.e. lines along which three or more materials meet. Linear systems
resulting upon discretisation of the PMCHWT are, because of their large
dimension, typically solved by Krylov iterative methods. The number of
iterations required for this solution critically depends on the eigenvalue
distribution of the system matrix. For systems that do not contain junction
lines, Calder\'on preconditioning, which was first applied to the electric
field integral equation, has been generalised to the PMCHWT equation. When
junctions are present, this approach cannot be applied. Alternative approaches,
such as the global multi-trace method, conceptually remove the junction lines
and as a result are amenable to Calder\'on preconditioning. This approach
entails a doubling of the degrees of freedom, and the solution that is produced
only approximately fulfils the continuity conditions at interfaces separating
domains. In this contribution, a single trace quasi-local PMCHWT equation is
introduced that requires a number of iterations for its solution that only
slowly increases as the mesh size tends to zero. The method is constructed as a
generalisation of the classic PMCHWT, and its discretisation is thoroughly
discussed. A comprehensive suite of numerical experiments demonstrates the
correctness, convergence behaviour, and efficiency of the method. The integral
equation is demonstrated to be free from interior resonances.

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [85] [A general relativistic magnetohydrodynamics extension to mesh-less schemes in the code GIZMO](https://arxiv.org/abs/2506.15775)
*Giacomo Fedrigo,Alessandro Lupi*

Main category: astro-ph.HE

TL;DR: A new mesh-less GRMHD scheme in GIZMO code is introduced, addressing the lack of general relativistic magnetic fields in existing simulations of AGN accretion disks.


<details>
  <summary>Details</summary>
Motivation: Accurate modeling of AGN dynamics requires general relativistic magnetic fields, which existing mesh-less schemes lack.

Method: A GRMHD scheme is developed within the mesh-less GIZMO framework, incorporating hyperbolic divergence cleaning for magnetic fields.

Result: The scheme is successfully benchmarked against relativistic MHD tests in various scenarios, including Minkowski and Schwarzchild/Kerr backgrounds.

Conclusion: This is the first GRMHD scheme in a mesh-free environment, enabling more efficient and accurate AGN simulations.

Abstract: The profound comprehension of the evolution and phenomenology of an Active
Galactic Nucleus requires an accurate exploration of the dynamics of the
magnetized gaseous disk surrounding the massive black hole in the centre. Many
numerical simulations have studied this environment using elaborate grid-based
codes, but in recent years, new mesh-less schemes have exhibited excellent
conservation properties and good accuracy at a more moderate computational
cost. Still, none implement general relativistic magnetic fields, a fundamental
ingredient to model an accretion disk around a massive black hole. We present
here a general relativistic magnetohydrodynamics (GRMHD) scheme working within
the mesh-less framework of the code \texttt{GIZMO}. We implement the hyperbolic
divergence cleaning procedure, consistently extended to general relativistic
effects, to keep the magnetic field divergence under safe levels. We benchmark
the scheme against various relativistic magnetohydrodynamics stress tests,
considering different dimensionalities and both a Minkowski or a
Schwarzchild/Kerr background. To date, this is the first GRMHD scheme working
in a mesh-free environment.

</details>


<div id='math.CA'></div>

# math.CA [[Back]](#toc)

### [86] [Heterotopic energy for Sobolev mappings](https://arxiv.org/abs/2506.16204)
*Antoine Detaille,Jean Van Schaftingen*

Main category: math.CA

TL;DR: The paper studies heterotopic energy, showing it's finite if mappings are homotopic on a codimension one skeleton, and decomposes it into Sobolev and disparity energies.


<details>
  <summary>Details</summary>
Motivation: To understand the conditions under which heterotopic energy is finite and its decomposition into simpler components.

Method: Analyze Sobolev mappings and their homotopy classes, using triangulation of the domain and disparity energy concepts.

Result: Heterotopic energy is finite iff mappings are homotopic on a codimension one skeleton, decomposable into Sobolev and disparity energies.

Conclusion: The framework extends to non-simply connected manifolds, providing insights into energy decomposition and homotopy conditions.

Abstract: We study the notion of heterotopic energy defined as the limit of Sobolev
energies of Sobolev mappings in a given homotopy class approximating almost
everywhere a given Sobolev mapping. We show that the heterotopic energy is
finite if and only if the mappings in the corresponding homotopy classes are
homotopic on a codimension one skeleton of a triangulation of the domain. When
this is the case, the heterotopic energy of a mapping is the sum of its Sobolev
energy and its disparity energy, defined as the minimum energy of a bubble to
pass between these homotopy classes. At the more technical level, we rely on a
framework that works when the target and domain manifolds are not simply
connected and there is no canonical isomorphism between homotopy groups with
different basepoints.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [87] [Microcanonical simulated annealing: Massively parallel Monte Carlo simulations with sporadic random-number generation](https://arxiv.org/abs/2506.16240)
*M. Bernaschi,L. A. Fernandez,I. González-Adalid Pemartín,E. Marinari,V. Martin-Mayor,G. Parisi,F. Ricci-Tersenghi,J. J. Ruiz-Lorenzo,D. Yllanes*

Main category: cond-mat.stat-mech

TL;DR: The paper introduces a microcanonical simulated annealing (mic.SA) method to reduce the computational burden of random-number generation in Monte Carlo simulations, demonstrating its effectiveness in parallel computation for spin-glass systems.


<details>
  <summary>Details</summary>
Motivation: Monte Carlo simulations, crucial for modeling complex systems in physics and other fields, are hindered by their high demand for random numbers, especially on advanced hardware.

Method: The authors propose a general-purpose mic.SA formalism, optimized for parallel computation, and test it on the 3D Ising spin glass, comparing results with standard simulations.

Result: The mic.SA method achieves compatible results with standard simulations in equilibrium conditions and shows scalable dynamics with simple time rescaling.

Conclusion: The mic.SA algorithm significantly reduces computational costs while maintaining accuracy, making it viable for large-scale simulations.

Abstract: Numerical simulations of models and theories that describe complex
experimental systems $\unicode{x2014}$in fields like high-energy and
condensed-matter physics$\unicode{x2014}$ are becoming increasingly important.
Examples include lattice gauge theories, which can describe, among others,
quantum chromodynamics (the Standard Model description of strong interactions
between elementary particles), and spin-glass systems. Beyond fundamental
research, these computational methods also find practical applications, among
many others, in optimization, finance, and complex biological problems.
However, Monte Carlo simulations, an important subcategory of these methods,
are plagued by a major drawback: they are extremely greedy for (pseudo) random
numbers. The total fraction of computer time dedicated to random-number
generation increases as the hardware grows more sophisticated, and can get
prohibitive for special-purpose computing platforms. We propose here a
general-purpose microcanonical simulated annealing (mic.SA) formalism that
dramatically reduces such a burden. The algorithm is fully adapted to a
massively parallel computation, as we show in the particularly demanding
benchmark of the three-dimensional Ising spin glass. We carry out very
stringent numerical tests of the new algorithm by comparing our results,
obtained on GPUs, with high-precision standard (i.e., random-number-greedy)
simulations performed on the Janus II custom-built supercomputer. In those
cases where thermal equilibrium is reachable (i.e., in the paramagnetic phase),
both simulations reach compatible values. More significantly, barring
short-time corrections, a simple time rescaling suffices to map the mic.SA
off-equilibrium dynamics onto the results obtained with standard simulations.

</details>


### [88] [Correcting systematic errors in the likelihood optimization of underdamped Langevin models of molecular dynamics trajectories](https://arxiv.org/abs/2506.16272)
*David Daniel Girardier,Hadrien Vroylandt,Sara Bonella,Fabio Pietrucci*

Main category: cond-mat.stat-mech

TL;DR: The paper addresses the gap in parameter inference for underdamped Langevin equations by proposing an analytical correction for spurious correlations in velocities, validated on benchmark and realistic systems.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the lack of methods for inferring parameters in underdamped Langevin equations, which include inertial effects and velocities, unlike the well-established overdamped case.

Method: The authors introduce an analytical correction for spurious correlations in velocities, using a likelihood-maximization algorithm that works with short, non-ergodic trajectories.

Result: The proposed method is tested on a benchmark case and a realistic system, demonstrating its accuracy and robustness.

Conclusion: This work enables the application of generalized Langevin equation inference to chemical reactions, filling a critical gap in the literature.

Abstract: Since Kramers' pioneering work in 1940, significant efforts have been devoted
to studying Langevin equations applied to physical and chemical reactions
projected onto few collective variables, with particular focus on the inference
of their parameters. While the inference for overdamped Langevin equations is
well-established and widely applied, a notable gap remains in the literature
for underdamped Langevin equations, which incorporate inertial effects and
velocities. This gap arises from the challenge of accessing velocities solely
through finite differences of positions, resulting in spurious correlations. In
this letter, we propose an analytical correction for these correlations,
specifically designed for a likelihood-maximization algorithm that exploits
short, non-ergodic trajectories that can be obtained at reasonable numerical
cost. The accuracy and robustness of our approach are tested across a benchmark
case and a realistic system. This work paves the way for applying generalized
Langevin equation inference to chemical reactions.

</details>


### [89] [Mean-field and Monte Carlo Analysis of Multi-Species Dynamics of agents](https://arxiv.org/abs/2506.16717)
*Eduardo Velasco Stock,Roberto da Silva,Sebastian Gonçalves*

Main category: cond-mat.stat-mech

TL;DR: The paper proposes a mean-field approximation for particle dynamics on a square lattice, validated with Monte Carlo simulations, and identifies a 'Gaussian-to-Gaussian' behavior in particle distributions.


<details>
  <summary>Details</summary>
Motivation: To model complex particle motion in environments like subway corridors using a mean-field approximation and Monte Carlo simulations.

Method: Combines mean-field approximation with Monte Carlo simulations under identical initial conditions, analyzing spatial distributions for different species and density regimes.

Result: Identifies a 'Gaussian-to-Gaussian' behavior where particle distributions transiently distort but return to Gaussian-like profiles in steady state.

Conclusion: The mean-field approximation aligns well with simulations, and the study reveals unique dynamics in particle interactions.

Abstract: We propose a mean-field (MF) approximation for the recurrence relation
governing the dynamics of $m$ species of particles on a square lattice, and we
simultaneously perform Monte Carlo (MC) simulations under identical initial
conditions to emulate the intricate motion observed in environments such as
subway corridors and scramble crossings in large cities. Each species moves
according to transition probabilities influenced by its respective static floor
field and the state of neighboring cells. To illustrate the methodology, we
analyze statistical fluctuations in the spatial distribution for $m = 1$, $m =
2$, and $m = 4$ and for different regimes of average density and biased
movement. A numerical comparison is conducted to determine the best agreement
between the MC simulations and the MF approximation considering a
renormalization exponent $\beta$ that optimizes the fit between methods.
Finally, we report a phenomenon we term "Gaussian-to-Gaussian" behavior, in
which an initially normal distribution of particles becomes distorted due to
interactions among same and opposing species, passes through a transient
regime, and eventually returns to a Gaussian-like profile in the steady state,
after multiple rounds of motion under periodic boundary conditions.

</details>


### [90] [Endoreversible Stirling cycles: plasma engines at maximal power](https://arxiv.org/abs/2506.16303)
*Gregory Behrendt,Sebastian Deffner*

Main category: cond-mat.stat-mech

TL;DR: Endoreversible Stirling engines with a one-component plasma achieve maximal power output at Curzon-Ahlborn efficiency due to the linear temperature and additive volume dependence of the caloric equation of state. This generalizes to other plasmas, unlike photonic engines, which show lower efficiency.


<details>
  <summary>Details</summary>
Motivation: To understand the efficiency limits of endoreversible Stirling engines using plasmas as working media and compare them with photonic engines.

Method: Analyzed endoreversible Stirling engines with a one-component plasma, focusing on the caloric equation of state's linear temperature and additive volume dependence.

Result: Maximal power output occurs at Curzon-Ahlborn efficiency for one-component plasmas, but photonic engines exhibit lower efficiency.

Conclusion: The findings generalize efficiency limits for plasmas beyond ideal gases, highlighting differences between plasma and photonic engines.

Abstract: Endoreversible engine cycles are a cornerstone of finite-time thermodynamics.
We show that endoreversible Stirling engines operating with a one-component
plasma as working medium run at maximal power output with the Curzon-Ahlborn
efficiency. As a main result, we elucidate that this is actually a consequence
of the fact that the caloric equation of state depends only linearly on
temperature and only additively on volume. In particular, neither the exact
form of the mechanical equation of state, nor the full fundamental relation are
required. Thus, our findings immediately generalize to a larger class of
working plasmas, far beyond simple ideal gases. In addition, we show that for
plasmas described by the photonic equation of state the efficiency is
significantly lower. This is in stark contrast to endoreversible Otto cycles,
for which photonic engines have an efficiency larger than the Curzon-Ahlborn
efficiency.

</details>


<div id='physics.acc-ph'></div>

# physics.acc-ph [[Back]](#toc)

### [91] [Theory of wakefield in a transversely inhomogeneous plasma waveguide](https://arxiv.org/abs/2506.16200)
*K. V. Galaydych,P. I. Markov,G. V. Sotnikov*

Main category: physics.acc-ph

TL;DR: Study of relativistic drive bunch generation of wakefields in a plasma-filled cylindrical waveguide with transverse inhomogeneity.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of wakefields in transversely inhomogeneous plasmas and their electromagnetic field components.

Method: Analytical derivation of field components (radial, axial electric, azimuthal magnetic) and investigation of dispersion and field topography.

Result: Determined amplitude distributions and frequency content of longitudinal and transverse wakefields.

Conclusion: The study provides insights into wakefield generation and electromagnetic field structures in inhomogeneous plasmas.

Abstract: Theoretical studies have been made into the relativistic drive bunch
generation of a wakefield in a cylindrical waveguide filled with a transversely
inhomogeneous plasma. According to the model used, the transversely
inhomogeneous plasma is considered as a combination of tubular plasma and the
plasma background of different density. Analytical expressions have been
derived for the excited radial and axial electric field components, and for the
azimuthal magnetic field component. The dispersion of the plasma waveguide
under study, as well as the topography of the electromagnetic field components
of the TM-eigenwaves, resonant with the bunch, have been investigated.
Longitudinal and transverse amplitude distribution structures of the axial and
radial wakefields have been determined. Spectrum analysis of the longitudinal
and transverse wakefields has been performed with the result that their
frequency content has been determined.

</details>


<div id='cs.MS'></div>

# cs.MS [[Back]](#toc)

### [92] [Transformations of Computational Meshes](https://arxiv.org/abs/2506.16341)
*Matthew G. Knepley*

Main category: cs.MS

TL;DR: A table-driven paradigm for mesh transformation simplifies complex mesh manipulations, improving performance and maintainability.


<details>
  <summary>Details</summary>
Motivation: Mesh manipulation in PDE simulations is often complex, error-prone, and hard to maintain, necessitating a simpler approach.

Method: A table-driven paradigm is introduced to execute diverse mesh transformations efficiently and in parallel.

Result: Implemented in PETSc, the method demonstrates performance and usability in practical experiments.

Conclusion: The proposed paradigm offers a scalable and maintainable solution for mesh transformations in simulations.

Abstract: Computational meshes, as a way to partition space, form the basis of much of
PDE simulation technology, for instance for the finite element and finite
volume discretization methods. In complex simulations, we are often driven to
modify an input mesh, for example, to refine, coarsen, extrude, change cell
types, or filter it. Mesh manipulation code can be voluminous, error-prone,
spread over many special cases, and hard to understand and maintain by
subsequent developers. We present a simple, table-driven paradigm for mesh
transformation which can execute a large variety of transformations in a
performant, parallel manner, along with experiments in the open source library
PETSc which can be run by the reader.

</details>
