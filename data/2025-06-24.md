<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 30]
- [math.AP](#math.AP) [Total: 22]
- [physics.comp-ph](#physics.comp-ph) [Total: 6]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 11]
- [quant-ph](#quant-ph) [Total: 1]
- [stat.ML](#stat.ML) [Total: 1]
- [astro-ph.SR](#astro-ph.SR) [Total: 2]
- [math-ph](#math-ph) [Total: 2]
- [math.OC](#math.OC) [Total: 2]
- [stat.CO](#stat.CO) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.DS](#cs.DS) [Total: 1]
- [math.PR](#math.PR) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 2]
- [physics.ins-det](#physics.ins-det) [Total: 1]
- [math.DG](#math.DG) [Total: 2]
- [math.CA](#math.CA) [Total: 1]
- [cs.DC](#cs.DC) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 2]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Fast solvers for the high-order FEM simplicial de Rham complex](https://arxiv.org/abs/2506.17406)
*Pablo D. Brubeck,Patrick E. Farrell,Robert C. Kirby,Charles Parker*

Main category: math.NA

TL;DR: New finite elements for Riesz maps of the de Rham complex on triangular/tetrahedral meshes achieve p-robust iterations with reduced computational cost (O(p^6) vs O(p^9)).


<details>
  <summary>Details</summary>
Motivation: To improve efficiency in solving Riesz maps of the de Rham complex by reducing computational complexity while maintaining accuracy.

Method: Uses novel basis functions for finite elements, orthogonal in two inner products, and a preconditioning strategy neglecting weak couplings. Combines with space decomposition on vertex/edge star patches.

Result: Achieves p-robust iterations with O(p^6) flops in 3D, improving over the naive O(p^9). Efficiently solves Hodge Laplacians with new preconditioners.

Conclusion: The method provides a computationally efficient and accurate solution for high-order Riesz maps and Hodge Laplacians in the de Rham complex.

Abstract: We present new finite elements for solving the Riesz maps of the de Rham
complex on triangular and tetrahedral meshes at high order. The finite elements
discretize the same spaces as usual, but with different basis functions, so
that the resulting matrices have desirable properties. These properties mean
that we can solve the Riesz maps to a given accuracy in a $p$-robust number of
iterations with $\mathcal{O}(p^6)$ flops in three dimensions, rather than the
na\"ive $\mathcal{O}(p^9)$ flops.
  The degrees of freedom build upon an idea of Demkowicz et al., and consist of
integral moments on an equilateral reference simplex with respect to a
numerically computed polynomial basis that is orthogonal in two different inner
products. As a result, the interior-interface and interior-interior couplings
are provably weak, and we devise a preconditioning strategy by neglecting them.
The combination of this approach with a space decomposition method on vertex
and edge star patches allows us to efficiently solve the canonical Riesz maps
at high order. We apply this to solving the Hodge Laplacians of the de Rham
complex with novel augmented Lagrangian preconditioners.

</details>


### [2] [Bounds-constrained finite element approximation of time-dependent partial differential equations](https://arxiv.org/abs/2506.17464)
*Robert C. Kirby,John D. Stephens*

Main category: math.NA

TL;DR: The paper extends variational inequality methods to enforce bounds constraints in collocation-type Runge-Kutta methods for time-dependent PDEs, ensuring high-order accuracy and uniform constraint satisfaction.


<details>
  <summary>Details</summary>
Motivation: Finite element methods lack guarantees for enforcing bounds constraints in PDEs, which are crucial for physical and numerical validity.

Method: The approach reformulates collocation schemes to incorporate variational inequalities, extending it to Runge-Kutta methods for time-dependent problems.

Result: High-order accuracy in space and time is achieved while uniformly enforcing bounds constraints, demonstrated via numerical examples.

Conclusion: The method successfully enforces constraints uniformly in time, offering a robust solution for bounded PDE problems.

Abstract: Finite element methods provide accurate and efficient methods for the
numerical solution of partial differential equations by means of restricting
variational problems to finite-dimensional approximating spaces. However, they
do not guarantee enforcement of bounds constraints inherent in the original
problem. Previous work enforces these bounds constraints by replacing the
variational equations with variational inequalities. We extend this approach to
collocation-type Runge-Kutta methods for time-dependent problems, obtaining
(formally) high order methods in both space and time. By using a novel
reformulation of the collocation scheme, we can guarantee that the bounds
constraints hold uniformly in time. Numerical examples for a model of
phytoplankton growth, the heat equation, and the Cahn-Hilliard system are
given.

</details>


### [3] [Operator Splitting Methods: Numerical Solutions of Ordinary Differential Equations via Separation of Variables](https://arxiv.org/abs/2506.17524)
*A. Banjara,I. AlJabea,T. Papamarkou,F. Neubrander*

Main category: math.NA

TL;DR: The paper uses linear semigroups from nonlinear flows to approximate solvable nonlinear ODEs, extending Trotter's splitting method to higher-order schemes with better error bounds.


<details>
  <summary>Details</summary>
Motivation: To leverage linear semigroup theory for approximating nonlinear ODEs, building on historical works and improving splitting methods.

Method: Analyzes nonlinear systems via the Koopman-Lie semigroup, decomposes the generator for operator splitting, and extends Trotter's scheme to higher orders.

Result: Numerical examples confirm the accuracy and efficiency of the proposed higher-order splitting methods.

Conclusion: The approach successfully applies linear semigroup theory to nonlinear ODEs, enhancing classical techniques with improved error bounds.

Abstract: This paper applies the concept of linear semigroups induced by nonlinear
flows, originally developed by Dorroh and Neuberger in the 1990s, to the
approximation of uniquely solvable initial value problems for nonlinear
ordinary differential equations. Building on a framework rooted in the earlier
works of Lie, Kowalewski, and Groebner, we analyze nonlinear systems through
the lens of the Koopman-Lie semigroup exp(tK), where K is the linear Lie
generator associated with the flow induced by the nonlinear differential
equation. A central feature of this approach is the decomposition K = K1 + ...
+ KN, which enables the use of operator splitting methods. We revisit the
foundational first-order splitting scheme introduced by H. F. Trotter in 1959
and extend it to higher-order schemes with improved error bounds. These
theoretical developments are supported by numerical examples that demonstrate
the accuracy and efficiency of the proposed methods, which are based entirely
on the classical separation of variables technique for solving ordinary
differential equations.

</details>


### [4] [Scattered point measurement-based regularization for backward problems for fractional wave equations](https://arxiv.org/abs/2506.17575)
*Dakang Cen,Zhiyuan Li,Wenlong Zhang*

Main category: math.NA

TL;DR: The paper focuses on reconstructing an unknown initial value from terminal data using Mittag-Leffler functions for stability, a regularization method for noisy measurements, and an iterative algorithm for optimal parameter selection.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of reconstructing initial values from terminal data, especially with noisy measurements, ensuring stability and accuracy.

Method: Uses Mittag-Leffler functions for stability analysis, introduces a regularization method for noisy data, and develops an iterative algorithm for optimal regularization.

Result: Proves stochastic convergence of the regularization method and demonstrates efficiency and accuracy through numerical experiments.

Conclusion: The proposed method effectively handles noisy measurements and provides accurate initial value reconstruction, validated by numerical results.

Abstract: In this work, we are devoted to the reconstruction of an unknown initial
value from the terminal data. The asymptotic and root-distribution properties
of Mittag-Leffler functions are used to establish stability of the backward
problem. Furthermore, we introduce a regularization method that effectively
handles scattered point measurements contaminated with stochastic noise.
Furthermore, we prove the stochastic convergence of our proposed regularization
and provide an iterative algorithm to find the optimal regularization
parameter. Finally, several numerical experiments are presented to demonstrate
the efficiency and accuracy of the algorithm.

</details>


### [5] [A priori error analysis of consistent PINNs for parabolic PDEs](https://arxiv.org/abs/2506.17614)
*Shiv Mishra,Arbaz Khan*

Main category: math.NA

TL;DR: The paper introduces a new analysis of collocation methods for parabolic PDEs, focusing on pointwise data and optimal recovery rates under Besov regularity. It proposes a consistent loss function for error control and demonstrates near-optimal recovery.


<details>
  <summary>Details</summary>
Motivation: To improve the recovery of solutions for parabolic PDEs using collocation methods by leveraging pointwise data and addressing approximation errors systematically.

Method: Develops a new consistent loss function incorporating interior, boundary, and initial data contributions, designed to reflect the PDE structure. Theoretical analysis and numerical experiments validate the approach.

Result: The proposed loss function yields near-optimal recovery under certain regularity and sampling conditions, supported by numerical evidence.

Conclusion: The method effectively controls approximation errors and achieves near-optimal recovery, with practical variants and numerical validation enhancing its applicability.

Abstract: We present a new a priori analysis of a class of collocation methods for
parabolic PDEs that rely only on pointwise data of force term, boundary data,
and initial data. Under Besov regularity assumptions, we characterize the
optimal recovery rate of the solution u based on sample complexity. We
establish error bounds by constructing a new consistent loss function that
effectively controls the approximation error. This loss incorporates
contributions from the interior, boundary, and initial data in a discretized
form and is designed to reflect the true PDE structure. Our theoretical results
demonstrate that minimizing this loss function yields near optimal recovery
under suitable conditions of regularity and sampling. Novel practical variants
of the loss function are discussed, and numerical experiments confirm the
effectiveness.

</details>


### [6] [A $C^0$ weak Galerkin method with preconditioning for constrained optimal control problems with general tracking](https://arxiv.org/abs/2506.17619)
*SeongHee Jeong,Seulip Lee,Kening Wang*

Main category: math.NA

TL;DR: A $C^0$ weak Galerkin method with an additive Schwarz preconditioner is proposed for solving optimal control problems with general tracking cost functionals and state constraints, addressing challenges like fourth-order variational inequalities and solution irregularity.


<details>
  <summary>Details</summary>
Motivation: Optimal control problems governed by PDEs with state constraints and tracking costs are challenging due to high-order variational inequalities and solution irregularity, necessitating robust numerical methods.

Method: The paper introduces a $C^0$-WG method using quadratic Lagrange elements for efficient assembly and parameter-free implementation, coupled with an additive Schwarz preconditioner to tackle ill-conditioned systems.

Result: Numerical experiments demonstrate the method's effectiveness and robustness for biharmonic and optimal control problems.

Conclusion: The $C^0$-WG method and additive Schwarz preconditioner provide an accurate and efficient solution for challenging optimal control problems.

Abstract: This paper presents a $C^0$ weak Galerkin ($C^0$-WG) method combined with an
additive Schwarz preconditioner for solving optimal control problems (OCPs)
governed by partial differential equations with general tracking cost
functionals and pointwise state constraints. These problems pose significant
analytical and numerical challenges due to the presence of fourth-order
variational inequalities and the reduced regularity of solutions. Our first
contribution is the design of a $C^0$-WG method based on globally continuous
quadratic Lagrange elements, enabling efficient elementwise stiffness matrix
assembly and parameter-free implementation while maintaining accuracy, as
supported by a rigorous error analysis. As a second contribution, we develop an
additive Schwarz preconditioner tailored to the $C^0$-WG method to improve
solver performance for the resulting ill-conditioned linear systems. Numerical
experiments confirm the effectiveness and robustness of the proposed method and
preconditioner for both biharmonic and optimal control problems.

</details>


### [7] [Local feature filtering for scalable and well-conditioned Random Feature Methods](https://arxiv.org/abs/2506.17626)
*Jan Willem van Beek,Victorita Dolean,Ben Moseley*

Main category: math.NA

TL;DR: The paper introduces a local filtering technique to address ill-conditioning in random feature methods (RFMs) for solving PDEs, improving computational efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: RFMs show promise for solving PDEs but suffer from ill-conditioned systems, hindering computational efficiency.

Method: A novel local filtering approach is proposed to eliminate nonexpressive features and construct an efficient preconditioner.

Result: Numerical experiments show the method rivals or outperforms existing preconditioning techniques in condition number and solver efficiency.

Conclusion: The technique enhances RFMs for PDE solving, offering improved computational performance.

Abstract: In recent years, machine learning has emerged as a powerful tool for solving
partial differential equations (PDEs), with randomized neural networks showing
remarkable potential. These networks, typically shallow, differ from
conventional approaches by randomizing all parameters except those in the final
layer. Notably, by integrating techniques inspired by domain decomposition,
they achieve improved local accuracy and computational efficiency. Following
\cite{chen:2022:BTM}, we refer to these approaches as \emph{random feature
methods} (RFM). Despite their promise, RFMs lead to highly ill-conditioned
least squares systems, posing a significant computational challenge. To address
this, we introduce a novel approach based on different strategies of local
filtering. This technique provides two key advantages: first, it enables the
selective elimination of nonexpressive features, reducing system size and
improving the condition number without sacrificing accuracy; second, it
facilitates the construction of an efficient preconditioner for the global
system. Our numerical experiments demonstrate that this strategy can rival or
even outperform existing preconditioning techniques, both in terms of condition
number and the efficiency of iterative solvers. These results underscore the
potential of our method as a powerful enhancement to machine learning-based PDE
solvers.

</details>


### [8] [Rank Inspired Neural Network for solving linear partial differential equations](https://arxiv.org/abs/2506.17654)
*Wentao Peng,Yunqing Huang,Nianyu Yi*

Main category: math.NA

TL;DR: RINN improves PIELM by adding a preconditioning stage to optimize basis function orthogonality, reducing initialization sensitivity and enhancing stability in solving PDEs.


<details>
  <summary>Details</summary>
Motivation: PIELM's random initialization leads to sensitivity issues in solving PDEs. RINN aims to address this by optimizing orthogonality for better stability and accuracy.

Method: RINN uses covariance-driven regularization to minimize off-diagonal elements of the covariance matrix, enforcing orthogonality. It involves a nonlinear optimization stage followed by linear least-squares for PDE constraints.

Result: RINN reduces performance variability and improves stability compared to PIELM, with consistently high accuracy across initializations.

Conclusion: RINN effectively addresses PIELM's initialization sensitivity, offering a more stable and accurate solution for PDEs.

Abstract: This paper proposes a rank inspired neural network (RINN) to tackle the
initialization sensitivity issue of physics informed extreme learning machines
(PIELM) when numerically solving partial differential equations (PDEs). Unlike
PIELM which randomly initializes the parameters of its hidden layers, RINN
incorporates a preconditioning stage. In this stage, covariance-driven
regularization is employed to optimize the orthogonality of the basis functions
generated by the last hidden layer. The key innovation lies in minimizing the
off-diagonal elements of the covariance matrix derived from the hidden-layer
output. By doing so, pairwise orthogonality constraints across collocation
points are enforced which effectively enhances both the numerical stability and
the approximation ability of the optimized function space.The RINN algorithm
unfolds in two sequential stages. First, it conducts a non-linear optimization
process to orthogonalize the basis functions. Subsequently, it solves the PDE
constraints using linear least-squares method. Extensive numerical experiments
demonstrate that RINN significantly reduces performance variability due to
parameter initialization compared to PIELM. Incorporating an early stopping
mechanism based on PDE loss further improves stability, ensuring consistently
high accuracy across diverse initialization settings.

</details>


### [9] [A meshless generalized finite difference method for solving the Stokes-Darcy coupled problem in static and moving systems](https://arxiv.org/abs/2506.17688)
*Yanan Xing,Haibiao Zheng*

Main category: math.NA

TL;DR: A meshless Generalized Finite Difference Method (GFDM) is proposed for solving Stokes-Darcy coupled problems with BJS interface conditions, demonstrating high accuracy, simplicity, and stability.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of Stokes-Darcy coupled problems with complex interfaces and BJS conditions, leveraging GFDM's advantages in handling derivative jumps.

Method: High order GFDM is applied to Stokes-Darcy problems, tested on closed interfaces with varying locations and BJS conditions. Neumann boundary conditions are used.

Result: GFDM shows high accuracy, convergence, and stability, even with large jumps and complex geometries. Four numerical examples validate its performance.

Conclusion: GFDM is effective for Stokes-Darcy problems, offering simplicity, accuracy, and robustness, especially in handling derivative jumps and complex interfaces.

Abstract: In this paper, a meshless Generalized Finite Difference Method (GFDM) is
proposed to deal with the Stokes-Darcy coupled problem with the
Beavers-Joseph-Saffman (BJS) interface conditions. Some high order GFDMs are
proposed to show the advantage of the high order GFDM for the Stokes-Darcy
coupled problem, which is that the high order method has high order accuracy
and the convergence order. Some Stokes-Darcy coupled problems with closed
interfaces, which has more complex geometric shape, are given to show the
advantage of the GFDM for the complex interface. The interface location has
been changed to show the influence of the interface location for the
Stokes-Darcy coupled problem. The BJS interface conditions has related to the
partial derivatives of unknown variables and the GFDM has advantage in dealing
with the interface conditions with the jump of derivatives. Four numerical
examples have been provided to verify the existence of the good performance of
the GFDM for the Stokes-Darcy coupled problems, including that the simplicity,
accuracy, and stability in static and moving systems. Especially, the GFDM has
the tolerance of the large jump. The Neaumann boundary condition is used in
numerical simulations.

</details>


### [10] [Numerical simulation of transient heat conduction with moving heat source using Physics Informed Neural Networks](https://arxiv.org/abs/2506.17726)
*Anirudh Kalyan,Sundararajan Natarajan*

Main category: math.NA

TL;DR: PINNs with transfer learning for heat transfer simulation with a moving source, reducing computational effort by dividing time intervals and reusing a single network.


<details>
  <summary>Details</summary>
Motivation: To efficiently simulate heat transfer with a moving source using PINNs while minimizing computational complexity.

Method: Divide time into intervals, train a single network sequentially using transfer learning, and reuse solutions as initial conditions.

Result: Accurate temperature distribution estimation, validated against finite element method results.

Conclusion: The framework successfully simulates large temporal intervals without network complexity, matching traditional methods.

Abstract: In this paper, the physics informed neural networks (PINNs) is employed for
the numerical simulation of heat transfer involving a moving source. To reduce
the computational effort, a new training method is proposed that uses a
continuous time-stepping through transfer learning. Within this, the time
interval is divided into smaller intervals and a single network is initialized.
On this single network each time interval is trained with the initial condition
for (n+1)th as the solution obtained at nth time increment. Thus, this
framework enables the computation of large temporal intervals without
increasing the complexity of the network itself. The proposed framework is used
to estimate the temperature distribution in a homogeneous medium with a moving
heat source. The results from the proposed framework is compared with
traditional finite element method and a good agreement is seen.

</details>


### [11] [Robust PDE discovery under sparse and highly noisy conditions via attention neural networks](https://arxiv.org/abs/2506.17908)
*Shilin Zhang,Yunqing Huang,Nianyu Yi,shihan Zhang*

Main category: math.NA

TL;DR: ANN-PYSR, a framework combining attention neural networks and PySR, efficiently discovers PDEs from noisy, sparse data, outperforming DLGA.


<details>
  <summary>Details</summary>
Motivation: To uncover predictive models of complex physical systems from experimental data, especially in noisy and sparse conditions.

Method: Integrates attention neural networks with PySR symbolic regression to identify governing PDEs.

Result: Successfully identifies PDEs in six benchmarks, outperforming DLGA in efficiency and robustness with high noise (up to 200%) and sparse data.

Conclusion: ANN-PYSR is highly practical for sparse, noisy environments where traditional methods fail.

Abstract: The discovery of partial differential equations (PDEs) from experimental data
holds great promise for uncovering predictive models of complex physical
systems. In this study, we introduce an efficient automatic model discovery
framework, ANN-PYSR, which integrates attention neural networks with the
state-of-the-art PySR symbolic regression library. Our approach successfully
identifies the governing PDE in six benchmark examples. Compared to the DLGA
framework, numerical experiments demonstrate ANN-PYSR can extract the
underlying dynamic model more efficiently and robustly from sparse, highly
noisy data (noise level up to 200%, 5000 sampling points). It indicates an
extensive variety of practical applications of ANN-PYSR, particularly in
conditions with sparse sensor networks and high noise levels, where traditional
methods frequently fail.

</details>


### [12] [Simultaneous recovery of corroded boundaries and admittance using the Kohn-Vogelius method](https://arxiv.org/abs/2506.17938)
*Moustapha Essahraoui,Elmehdi Cherrat,Lekbir Afraites,Julius Fergy Tiongson Rabago*

Main category: math.NA

TL;DR: The paper proposes a method to identify an unknown boundary portion and its Robin admittance coefficient using two sets of boundary Cauchy data, ensuring uniqueness through a cost function and gradient-based iterative scheme.


<details>
  <summary>Details</summary>
Motivation: The problem involves identifying an unknown boundary portion and its admittance coefficient, where single measurements fail to ensure uniqueness. The goal is to develop a reliable numerical method for simultaneous reconstruction.

Method: A cost function based on the energy-gap of two auxiliary problems is proposed. Variational derivatives are derived for both the boundary and admittance coefficient, enabling a gradient-based iterative scheme.

Result: Numerical experiments confirm the method's effectiveness in reconstructing the unknown boundary portion and admittance coefficient.

Conclusion: The proposed approach successfully addresses the identifiability issue and provides a practical numerical solution for simultaneous reconstruction.

Abstract: We address the problem of identifying an unknown portion $\Gamma$ of the
boundary of a $d$-dimensional ($d \in \{1, 2\}$) domain $\Omega$ and its
associated Robin admittance coefficient, using two sets of boundary Cauchy data
$(f, g)$--representing boundary temperature and heat flux--measured on the
accessible portion $\Sigma$ of the boundary. Identifiability results
\cite{Bacchelli2009,PaganiPierotti2009} indicate that a single measurement on
$\Sigma$ is insufficient to uniquely determine both $\Gamma$ and $\alpha$, but
two independent inputs yielding distinct solutions ensure the uniqueness of the
pair $\Gamma$ and $\alpha$. In this paper, we propose a cost function based on
the energy-gap of two auxiliary problems. We derive the variational derivatives
of this objective functional with respect to both the Robin boundary $\Gamma$
and the admittance coefficient $\alpha$. These derivatives are utilized to
develop a nonlinear gradient-based iterative scheme for the simultaneous
numerical reconstruction of $\Gamma$ and $\alpha$. Numerical experiments are
presented to demonstrate the effectiveness and practicality of the proposed
method.

</details>


### [13] [A residual a posteriori error estimate for the Stabilization-free Virtual Element Method](https://arxiv.org/abs/2506.17947)
*Stefano Berrone,Andrea Borio,Davide Fassino,Francesca Marcon*

Main category: math.NA

TL;DR: A posteriori error analysis of Stabilization-Free Virtual Element Methods for the 2D Poisson equation, showing equivalence between a defined error measure and standard residual error estimators.


<details>
  <summary>Details</summary>
Motivation: To analyze error measures in Stabilization-Free Virtual Element Methods, avoiding the limitations of stabilized methods.

Method: A posteriori error analysis and numerical experiments with various mesh types and diffusion term jumps.

Result: Confirmation of estimator robustness and expected behavior across different conditions.

Conclusion: The method provides reliable error estimation without stabilization, validated by numerical results.

Abstract: In this work, we present the a posteriori error analysis of
Stabilization-Free Virtual Element Methods for the 2D Poisson equation. The
abscence of a stabilizing bilinear form in the scheme allows to prove the
equivalence between a suitably defined error measure and standard residual
error estimators, which is not obtained in general for stabilized virtual
elements. Several numerical experiments are carried out, confirming the
expected behaviour of the estimator in the presence of different mesh types,
and robustness with respect to jumps of the diffusion term.

</details>


### [14] [A nodal basis for the $C^1$-$P_{33}$ finite elements on 5D simplex grids](https://arxiv.org/abs/2506.17961)
*Jun Hu,Shangyou Zhang*

Main category: math.NA

TL;DR: A nodal basis for a 5D $C^1$ finite element space with high polynomial degree (33) is constructed, ensuring specific continuity conditions across simplex components.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of constructing finite element spaces with high polynomial degrees and specific continuity requirements in higher dimensions.

Method: Develop a nodal basis for the 5D $C^1$ finite element space, enforcing $C^1$ continuity on 4D faces, $C^2$ on tetrahedra, $C^4$ on triangles, $C^8$ on edges, and $C^{16}$ at vertices.

Result: Successful construction of the basis, meeting the specified continuity conditions across all simplex components.

Conclusion: The method provides a robust framework for high-degree finite element spaces in 5D, with precise continuity control.

Abstract: We construct a nodal basis for the 5-dimensional $C^1$ finite element space
of polynomial degree $33$
  on simplex grids,
  where the finite element functions are $C^1$ on the 6 4D-simplex faces,
  $C^2$ on the 15 face-tetrahedra, $C^4$ on the 20 face-triangles,
  $C^8$ on the 15 edges, and $C^{16}$ at the 6 vertices, of a 5D simplex.

</details>


### [15] [Hybridizable Discontinuous Galerkin Methods for Thermo-Poroelastic Systems](https://arxiv.org/abs/2506.17978)
*Salim Meddahi*

Main category: math.NA

TL;DR: A high-order hybridizable discontinuous Galerkin (HDG) method is proposed for linear thermo-poroelasticity, ensuring energy conservation and validated through numerical experiments.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of modeling thermo-poroelastic wave propagation in heterogeneous media with a method that combines computational efficiency and accuracy.

Method: A first-order hyperbolic system is used, incorporating multiple state variables. The HDG discretization ensures energy consistency and exploits locality and static condensation.

Result: The method achieves theoretical convergence rates and effectively models thermo-poroelastic wave propagation, as confirmed by numerical experiments.

Conclusion: The proposed HDG formulation is robust, energy-conserving, and suitable for complex thermo-poroelastic problems in heterogeneous media.

Abstract: We propose a high-order hybridizable discontinuous Galerkin (HDG) formulation
for the fully dynamic, linear thermo-poroelasticity problem. The governing
equations are formulated as a first-order hyperbolic system incorporating solid
and fluid velocities, heat flux, effective stress, pore pressure, and
temperature as state variables. We establish well-posedness of the continuous
problem using semigroup theory and develop an energy-consistent HDG
discretization. The method exploits computational advantages of HDG-including
locality and static condensation-while maintaining energy conservation for the
coupled system. We establish an $hp$-convergence analysis and support it with
comprehensive numerical experiments, confirming the theoretical rates and
showcasing the method's effectiveness for thermo-poroelastic wave propagation
in heterogeneous media.

</details>


### [16] [Algorithms for pointwise and piecewise polynomial approximations to the trigonometric functions](https://arxiv.org/abs/2506.17992)
*Quan Le Phuong*

Main category: math.NA

TL;DR: A new, simplified approach to approximation algorithms, building on prior work, with computational and graphical examples using Maple.


<details>
  <summary>Details</summary>
Motivation: To improve and simplify existing approximation algorithms from previous research.

Method: Proposes a modified approach, demonstrated through computational and graphical examples using Maple procedures.

Result: Successful demonstration of the new approach with practical examples.

Conclusion: The proposed method is effective and simpler, validated by computational and graphical results.

Abstract: In this paper, we propose a new and simple approach to the approximation
algorithms that are modified and improved from our published results. The
computational and graphical examples are presented with the aid of Maple
procedures.

</details>


### [17] [Fast solution of a phase-field model of pitting corrosion](https://arxiv.org/abs/2506.18058)
*Gianluca Frasca-Caccia,Dajana Conte,Beatrice Paternoster*

Main category: math.NA

TL;DR: Proposes an efficient strategy for solving phase-field corrosion models using iterative IMEX methods, reducing computational time while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Excessive computational times in corrosion models limit their practical use, especially for predictive maintenance.

Method: Uses time-stepping IMEX methods on rectangular domains, extends to non-rectangular domains via iterative IMEX methods.

Result: Achieves comparable accuracy to existing methods with significantly reduced computational time.

Conclusion: The approach enables practical predictions on standard workstations, enhancing applicability.

Abstract: Excessive computational times represent a major challenge in the solution of
corrosion models, limiting their practical applicability, e.g., as a support to
predictive maintenance. In this paper, we propose an efficient strategy for
solving a phase-field model for metal corrosion. Based on the Kronecker
structure of the diffusion matrix in classical finite difference approximations
on rectangular domains, time-stepping IMEX methods are efficiently solve in
matrix form. However, when the domain is non-rectangular, the lack of the
Kronecker structure prevents the direct use of this matrix-based approach. To
address this issue, we reformulate the problem on an extended rectangular
domain and introduce suitable iterative IMEX methods. The convergence of the
iterations is analyzed, and numerical experiments show that the proposed
approach achieves accuracy comparable to existing methods, while significantly
reducing the computational time, to the point of allowing actual predictions on
standard workstations.

</details>


### [18] [Mixed virtual element methods for a stress-velocity-rotation formulation in viscoelasticity](https://arxiv.org/abs/2506.18168)
*Sarvesh Kumar,Utkarsh Rajput,Ricardo Ruiz-Baier*

Main category: math.NA

TL;DR: A new mixed virtual element method for viscoelasticity equations with weakly imposed stress symmetry, using the Zener model and Crank-Nicolson time discretization, is proposed. Unique solvability and optimal error estimates are proven, supported by numerical examples.


<details>
  <summary>Details</summary>
Motivation: To address the numerical approximation of viscoelasticity equations with weakly imposed stress symmetry, leveraging the Zener model for accurate modeling.

Method: Proposes a mixed virtual element formulation with additively decomposed stress, rotation tensor, and velocity as Lagrange multipliers. Uses Crank-Nicolson for time discretization and local projectors for solvability.

Result: Demonstrates unique solvability and optimal a priori error estimates for all variables. Numerical examples validate the formulation.

Conclusion: The proposed method effectively solves viscoelasticity equations with proven solvability and accuracy, supported by numerical validation.

Abstract: In this paper we propose a new mixed virtual element formulation for the
numerical approximation of viscoelasticity equations with weakly imposed stress
symmetry. The governing equations use the Zener model and are expressed in
terms of the principal unknowns of additively decomposed stress into elastic
and internal viscoelastic contributions, while the rotation tensor and velocity
act as Lagrange multipliers. The time discretisation uses Crank--Nicolson's
scheme. We demonstrate the unique solvability of both semi-discrete and
fully-discrete problems by leveraging the properties of suitable local
projectors. Moreover, we establish optimal a priori error estimates for all
variables that appear in the mixed formulation. To validate our theoretical
findings, we present several representative numerical examples that also
highlight the features of the proposed formulation.

</details>


### [19] [AE-PINNs: Attention-enhanced physics-informed neural networks for solving elliptic interface problems](https://arxiv.org/abs/2506.18332)
*Jiachun Zheng,Yunqing Huang,Nianyu Yi*

Main category: math.NA

TL;DR: AE-PINNs enhance physics-informed neural networks with an attention mechanism to solve elliptic interface equations, improving accuracy over existing methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of solving elliptic interface equations by leveraging the attention mechanism to better handle discontinuities across interfaces.

Method: Decomposes the solution into continuous and discontinuous components, using a fully connected neural network for the former and an interface-attention neural network for the latter, focusing on interface information.

Result: AE-PINNs achieve higher accuracy compared to PINNs, I-PINNs, and M-PINNs in numerical experiments.

Conclusion: The attention-enhanced approach effectively improves the accuracy of solving elliptic interface equations.

Abstract: Inspired by the attention mechanism, we develop an attention-enhanced
physics-informed neural networks (AE-PINNs) for solving elliptic interface
equations. In AE-PINNs, we decompose the solution into two complementary
components: a continuous component and a component with discontinuities across
the interface. The continuous component is approximated by a fully connected
neural network in the whole domain, while the discontinuous component is
approximated by an interface-attention neural network in each subdomain
separated by the interface. The interface-attention neural network adopts a
network structure similar to the attention mechanism to focus on the interface,
with its key extension is to introduce a neural network that transmits
interface information. Some numerical experiments have confirmed the
effectiveness of the AE-PINNs, demonstrating higher accuracy compared with
PINNs, I-PINNs and M-PINNs.

</details>


### [20] [Accuracy and componentwise accuracy in multilinear PageRank](https://arxiv.org/abs/2506.18356)
*Mehdi Najafi Kalyani,Federico Poloni*

Main category: math.NA

TL;DR: The paper analyzes the stability and accuracy of numerical algorithms for the multilinear PageRank problem, revealing improved stability bounds and accuracy for the minimal solution, with broader implications for quadratic vector equations and M-matrices.


<details>
  <summary>Details</summary>
Motivation: To understand the stability and accuracy of numerical solutions for the multilinear PageRank problem, particularly in handling perturbations and numerical errors, and to generalize findings to broader mathematical contexts.

Method: The study examines the multilinear PageRank problem, derives stability bounds, and analyzes the Newton method's accuracy. It also proposes subtraction-free algorithm modifications for componentwise stability.

Result: The solution is more stable than classical bounds suggest, with accuracy depending on a smaller quantity due to nonnegativity. The Newton method's limiting accuracy is also improved.

Conclusion: The findings provide better stability and accuracy bounds for the multilinear PageRank problem, with broader applications to quadratic vector equations and M-matrices.

Abstract: We study the stability with respect to perturbations and the accuracy of
numerical algorithms for computing solutions to the multilinear PageRank
problem $\mathbf{x} = (1-\alpha)\mathbf{v} + \alpha \mathcal{P} \mathbf{x}^2$.
Our results reveal that the solution can be more stable with respect to
perturbations and numerical errors with respect to the classical bounds for
nonlinear systems of equations (based on the norm of the Jacobian). In detail,
one can obtain bounds for the minimal solution which ignore the singularity of
the problem for $\alpha=1/2$, and one can show that the limiting accuracy of
the Newton method depends not on the norm of the Jacobian but on a quantity
that can be much smaller thanks to the nonnegativity structure of the equation.
For the minimal solution, we also suggest subtraction-free modifications to the
existing algorithms to achieve componentwise stability.
  Some of the theoretical results we obtain are interesting even outside the
scope of this problem: bounds for more general quadratic vector equations, and
a partial inverse for M-matrices which remains bounded when the matrix to
invert approaches singularity.

</details>


### [21] [A high-order, conservative and positivity-preserving intersection-based remapping method between meshes with isoparametric curvilinear cells](https://arxiv.org/abs/2506.18389)
*Nuo Lei,Juan Cheng,Chi-Wang Shu*

Main category: math.NA

TL;DR: A novel intersection-based remapping method for high-order curved-edge meshes in the ALE framework, using Weiler-Atherton clipping and WENO reconstruction for accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in transferring physical quantities between high-order curved-edge meshes while maintaining accuracy and efficiency.

Method: Leverages Weiler-Atherton clipping for intersection computation, integrates WENO reconstruction for accuracy, and applies a positivity-preserving limiter.

Result: Achieves high-order accuracy, strict conservation, non-oscillation, and positivity-preserving properties efficiently.

Conclusion: The method is efficient, scalable, and applicable to arbitrary high-order isoparametric curvilinear cells without performance loss.

Abstract: This paper presents a novel intersection-based remapping method for
isoparametric curvilinear meshes within the indirect arbitrary
Lagrangian-Eulerian (ALE) framework, addressing the challenges of transferring
physical quantities between high-order curved-edge meshes. Our method leverages
the Weiler-Atherton clipping algorithm to compute intersections between
curved-edge quadrangles, enabling robust handling of arbitrary order
isoparametric curves. By integrating multi-resolution weighted essentially
non-oscillatory (WENO) reconstruction, we achieve high-order accuracy while
suppressing numerical oscillations near discontinuities. A
positivity-preserving limiter is further applied to ensure physical quantities
such as density remain non-negative without compromising conservation or
accuracy. Notably, the computational cost of handling higher-order curved
meshes, such as cubic or even higher-degree parametric curves, does not
significantly increase compared to secondorder curved meshes. This ensures that
our method remains efficient and scalable, making it applicable to arbitrary
high-order isoparametric curvilinear cells without compromising performance.
Numerical experiments demonstrate that the proposed method achieves highorder
accuracy, strict conservation (with errors approaching machine precision),
essential non-oscillation and positivity-preserving.

</details>


### [22] [Stabilizing randomized GMRES through flexible GMRES](https://arxiv.org/abs/2506.18408)
*Stefan Güttel,John W. Pearson*

Main category: math.NA

TL;DR: Flexible GMRES is used as an outer wrapper for sketched GMRES, with a new residual bound enabling a practical, efficient, and robust randomized solver.


<details>
  <summary>Details</summary>
Motivation: To develop a solver that minimizes parameter tuning while maintaining efficiency and robustness in residual norm reduction.

Method: Uses flexible GMRES as an outer wrapper for sketched GMRES, leveraging a new residual bound for the preconditioner.

Result: A practical randomized solver is derived, requiring minimal tuning and ensuring non-increasing residual norms.

Conclusion: The approach provides an efficient and robust solution with reduced parameter tuning.

Abstract: We explore the use of flexible GMRES as an outer wrapper for sketched GMRES.
Building on a new bound for the residual of FGMRES in terms of the residual of
the preconditioner, we derive a practical randomized solver that requires very
little parameter tuning, while still being efficient and robust in the sense of
generating non-increasing residual norms.

</details>


### [23] [A Complete-Electrode-Model-Based Forward Approach for Transcranial Temporal Interference Stimulation with Linearization: Numerical Simulation Study](https://arxiv.org/abs/2506.18436)
*Santtu Söderholm,Maryam Samavaki,Sampsa Pursiainen*

Main category: math.NA

TL;DR: The study develops a mathematical model for transcranial temporal interference stimulation (tTIS) using the complete electrode model (CEM) and evaluates its accuracy and sensitivity.


<details>
  <summary>Details</summary>
Motivation: To create a realistic and adaptable simulation for tTIS, crucial for optimizing stimulation currents targeting specific brain regions.

Method: Uses CEM-based forward finite-element-method simulation and investigates a linearized CEM surrogate.

Result: CEM successfully modeled tTIS fields, with electrode impedance significantly affecting distribution. Linearized CEM matched the nonlinear model within error limits.

Conclusion: CEM is effective for tTIS simulation, with linearized CEM as a viable surrogate, both showing high sensitivity near focal regions.

Abstract: Background and Objective: Transcranial temporal interference stimulation
(tTIS) is a promising non-invasive brain stimulation technique in which
interference between electrical current fields extends the possibilities of
electrical brain stimulation. The objective of this study is to develop an
efficient mathematical tTIS forward modelling scheme that allows for realistic
and adaptable simulation and can be updated accurately when the stimulation
frequency is modified in one or more electrodes. Such a model is vital, for
example, in optimization processes that seek the best possible stimulation
currents to exhibit or inhibit a given brain region. This study aims to
establish and evaluate the complete electrode model (CEM), i.e., a set of
boundary conditions incorporating electrode impedance and contact patch, as a
forward finite-element-method-based simulation technique for tTIS and
investigate linearized CEM as a surrogate.
  Results: The CEM-based forward simulation successfully reproduced the
volumetric stimulating fields induced by tTIS. Sensitivity analysis showed that
variations in electrode impedance substantially affect the field distribution,
especially in regions where the interfering currents have nearly equal
amplitudes. The linearized CEM model closely matched the full nonlinear model
within a predefined peak signal-to-noise ratio (PSNR) threshold for relative
error. Both models exhibited the highest sensitivity near the focal region.

</details>


### [24] [Semi-discrete heat equations with variable coefficients and the parametrix method](https://arxiv.org/abs/2506.18649)
*Ulrik S. Fjordholm,Kenneth H. Karlsen,Peter H. C. Pang*

Main category: math.NA

TL;DR: A parametrix approach is used to solve semi-discrete heat equations with variable coefficients, leveraging Lorentzian estimates due to the absence of Gaussian estimates in this context.


<details>
  <summary>Details</summary>
Motivation: The lack of Gaussian estimates for semi-discrete heat equations necessitates an alternative approach to derive solutions and grid-size independent estimates.

Method: The paper employs a parametrix approach, using Lorentz (Cauchy) probability densities to handle iterated convolutions involving Bessel functions.

Result: Lorentzian estimates enable convergence of the parametrix method, providing solutions and grid-size independent estimates.

Conclusion: The parametrix approach with Lorentzian estimates successfully addresses the challenges of semi-discrete heat equations with variable coefficients.

Abstract: We develop a parametrix approach for constructing solutions and establishing
grid-size independent estimates for semi-discrete heat equations with variable
coefficients. While the classical continuous setting benefits from Gaussian
estimates of the constant coefficient heat kernel, such estimates are not
available in the semi-discrete context. To address this complication, we derive
estimates involving products of heavy-tailed Lorentz (also known as Cauchy)
probability densities. These Lorentzian estimates provide a sufficient handle
on certain iterated convolutions involving Bessel functions, enabling us to
achieve convergence of the parametrix approach.

</details>


### [25] [Shifted HSS preconditioners for the indefinite Helmholtz equation](https://arxiv.org/abs/2506.18694)
*Colin J Cotter,Kars Knook,Joshua Hope-Collins*

Main category: math.NA

TL;DR: A scalable preconditioning method for the Helmholtz equation using HSS iteration and multigrid, achieving O(k) wallclock time convergence.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of solving the indefinite Helmholtz equation efficiently with robustness in k and mesh size.

Method: Uses Hermitian Skew-Hermitian Splitting (HSS) iteration on a shifted operator, combined with multigrid for approximation.

Result: Proves k- and mesh-robustness with O(k) HSS iterations, achieving scalable O(k) wallclock time convergence.

Conclusion: Demonstrates a fully scalable method using standard multigrid components for the Helmholtz equation.

Abstract: We provide a preconditioning approach for the indefinite Helmholtz equation
discretised using finite elements, based upon a Hermitian Skew-Hermitian
Splitting (HSS) iteration applied to the shifted operator, and prove that the
preconditioner is k- and mesh-robust when O(k) HSS iterations are performed.
The HSS iterations involve solving a shifted operator that is suitable for
approximation by multigrid using standard smoothers and transfer operators,
leading to a fully scalable algorithm. We argue that the algorithm converges in
O(k) wallclock time when within the range of scalability of the multigrid. We
provide numerical results verifying our proofs and demonstrating this claim.
This establishes a scalable O(k) method using multigrid with entirely standard
components.

</details>


### [26] [On the computation of tensor functions under tensor-tensor multiplications with linear maps](https://arxiv.org/abs/2506.18713)
*Jeong-Hoon Ju,Susana Lopez-Moreno*

Main category: math.NA

TL;DR: The paper explores algebraic and non-algebraic tensor functions under tensor-tensor multiplication with linear maps, linking their computational complexity to matrix multiplication and introducing new tensor means and their properties.


<details>
  <summary>Details</summary>
Motivation: To understand the computational complexity of tensor functions and extend concepts like geometric and Wasserstein means to tensors under specific linear maps.

Method: Proves asymptotic exponent equivalence for algebraic tensor functions, defines tensor geometric and Wasserstein means, and introduces a pseudo-SVD for injective linear maps.

Result: Shows tensor geometric mean can be solved via a Riccati equation and lacks determinantal identity; applies pseudo-SVD for image compression.

Conclusion: The study bridges tensor computations with matrix multiplication complexity and extends mean concepts to tensors, revealing unique properties and applications.

Abstract: In this paper we study the computation of both algebraic and non-algebraic
tensor functions under the tensor-tensor multiplication with linear maps. In
the case of algebraic tensor functions, we prove that the asymptotic exponent
of both the tensor-tensor multiplication and the tensor polynomial evaluation
problem under this multiplication is the same as that of the matrix
multiplication, unless the linear map is injective. As for non-algebraic
functions, we define the tensor geometric mean and the tensor Wasserstein mean
for pseudo-positive-definite tensors under the tensor-tensor multiplication
with invertible linear maps, and we show that the tensor geometric mean can be
calculated by solving a specific Riccati tensor equation. Furthermore, we show
that the tensor geometric mean does not satisfy the resultantal (determinantal)
identity in general, which the matrix geometric mean always satisfies. Then we
define a pseudo-SVD for the injective linear map case and we apply it on image
data compression.

</details>


### [27] [DPG loss functions for learning parameter-to-solution maps by neural networks](https://arxiv.org/abs/2506.18773)
*Pablo Cortés Castillo,Wolfgang Dahmen,Jay Gopalakrishnan*

Main category: math.NA

TL;DR: The paper introduces residual-based loss functions for machine learning in PDE contexts, focusing on accuracy certification and robust performance, especially for high-contrast diffusion fields.


<details>
  <summary>Details</summary>
Motivation: To enhance prediction accuracy and robustness of deep neural network models for parameter-dependent PDEs, particularly in challenging scenarios like high-contrast diffusion.

Method: Uses variationally correct loss functions derived from ultraweak Discontinuous Petrov Galerkin (DPG) discretization, demonstrated on an elliptic PDE example.

Result: DPG loss functions outperform simpler least-squares losses in robustness and accuracy, especially for high-contrast diffusion parameters.

Conclusion: The proposed DPG-based loss functions are effective for a wide range of problems with stable DPG formulations, offering improved performance in challenging cases.

Abstract: We develop, analyze, and experimentally explore residual-based loss functions
for machine learning of parameter-to-solution maps in the context of
parameter-dependent families of partial differential equations (PDEs). Our
primary concern is on rigorous accuracy certification to enhance prediction
capability of resulting deep neural network reduced models. This is achieved by
the use of variationally correct loss functions. Through one specific example
of an elliptic PDE, details for establishing the variational correctness of a
loss function from an ultraweak Discontinuous Petrov Galerkin (DPG)
discretization are worked out. Despite the focus on the example, the proposed
concepts apply to a much wider scope of problems, namely problems for which
stable DPG formulations are available. The issue of {high-contrast} diffusion
fields and ensuing difficulties with degrading ellipticity are discussed. Both
numerical results and theoretical arguments illustrate that for high-contrast
diffusion parameters the proposed DPG loss functions deliver much more robust
performance than simpler least-squares losses.

</details>


### [28] [Optimal adaptive implicit time stepping](https://arxiv.org/abs/2506.18809)
*Michael Feischl,David Niederkofler*

Main category: math.NA

TL;DR: Proposes an adaptive time stepping algorithm with guaranteed optimal convergence, leveraging adaptive mesh refinement techniques.


<details>
  <summary>Details</summary>
Motivation: Despite widespread use, adaptive time stepping lacks a complete theoretical foundation for optimal convergence.

Method: Uses recent advances in adaptive mesh refinement to develop an algorithm ensuring optimal convergence, treating the time stepping scheme as a black box.

Result: The algorithm achieves the best possible error convergence relative to the number of time steps.

Conclusion: Provides a mathematically guaranteed optimal adaptive time stepping method, bridging theory and application.

Abstract: We revisit adaptive time stepping, one of the classical topics of numerical
analysis and computational engineering. While widely used in application and
subject of many theoretical works, a complete understanding is still missing.
Apart from special cases, there does not exist a complete theory that shows how
to choose the time steps such that convergence towards the exact solution is
guaranteed with the optimal convergence rate. In this work, we use recent
advances in adaptive mesh refinement to propose an adaptive time stepping
algorithm that is mathematically guaranteed to be optimal in the sense that it
achieves the best possible convergence of the error with respect to the number
of time steps, and it can be implemented using a time stepping scheme as a
black box.

</details>


### [29] [Unconditionally stable space-time isogeometric method for the linear Schrödinger equation](https://arxiv.org/abs/2506.18859)
*Matteo Ferrari,Sergio Gómez*

Main category: math.NA

TL;DR: A space-time isogeometric finite element method for the linear time-dependent Schrödinger equation is proposed, showing unconditional stability and conservation of mass and energy.


<details>
  <summary>Details</summary>
Motivation: To develop a stable and accurate numerical method for solving the time-dependent Schrödinger equation with spatially varying potential.

Method: Uses splines with maximal regularity in time for a space-time isogeometric finite element approach.

Result: The method is unconditionally stable, preserves mass and energy, and numerical experiments confirm theoretical convergence.

Conclusion: The method is effective and provides an alternative proof of stability for a related wave equation method.

Abstract: We propose and analyze a space-time isogeometric finite element method based
on splines with maximal regularity in time for the linear time-dependent
Schr\"odinger equation with a spatially varying potential. We investigate the
stability and conservation properties of the method, demonstrating that it
preserves both mass and energy at the final time, and it is unconditionally
stable. Numerical experiments confirm our theoretical findings and illustrate
the convergence behavior of the scheme. Incidentally, our analysis also
provides an alternative proof of unconditional stability of the
first-order-in-time isogeometric method for the wave equation proposed in (M.
Ferrari, S. Fraschini, G. Loli and I. Perugia (2025)), eliminating the need for
the numerical verifications required in the previous analysis.

</details>


### [30] [Convex-concave splitting for the Allen-Cahn equation leads to $\varepsilon^2$-slow movement of interfaces](https://arxiv.org/abs/2506.18869)
*Patrick Dondl,Akwum Onwunta,Ludwig Striet,Stephan Wojtowytsch*

Main category: math.NA

TL;DR: The paper analyzes a convex-concave splitting discretization of the Allen-Cahn equation, showing energy decrease for large time-steps and linking it to mean curvature flow.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of the Allen-Cahn equation under large time-steps and its connection to mean curvature flow.

Method: Analyzes the time-stepping scheme for various potentials, including extreme cases, and links it to thresholding approximation of mean curvature flow.

Result: The scheme's effective time step scales with the square of the phase-field parameter ε, and stability is achieved by freezing interfaces.

Conclusion: The method is stable and effective, with time-step limitations tied to ε, not geometry, and connects to mean curvature flow.

Abstract: The convex-concave splitting discretization of the Allen-Cahn is easy to
implement and guaranteed to be energy decreasing even for large time-steps. We
analyze the time-stepping scheme for a large class of potentials which includes
the standard potential as well as two extreme settings: Potentials with
quadratic convex part (uniform positive curvature), and potentials which are
concave between the potential wells and either linear or infinite outside
(highly concentrated curvature). In all three scenarios, the 'effective time
step size' of the scheme scales with the square of the small parameter
$\varepsilon$ governing the width of transition layers. A weaker 'slow motion'
result is proved under much more general assumptions. Thus, stability is
achieved by effectively 'freezing' the interfaces in place. The time step
limitation is not geometric in origin, but depends on the phase-field parameter
$\varepsilon$. Along the way, we establish a new link between an Allen-Cahn
type equation and a thresholding approximation of mean curvature flow.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [31] [Spectral asymptotics of pseudodifferential operators with discontinuous symbols](https://arxiv.org/abs/2506.17426)
*Alexey Derkach,Alexander V. Sobolev*

Main category: math.AP

TL;DR: The paper analyzes the discrete spectrum of self-adjoint Weyl pseudodifferential operators with discontinuous symbols, focusing on domains like polygons and sectors. It improves bounds on singular values and introduces a 'dual' symbol technique.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of singular values for pseudodifferential operators with discontinuous symbols, particularly in domains like polygons and sectors, and to refine existing bounds.

Method: The study reduces the problem to a smooth 'dual' symbol and derives new bounds for singular values of pseudodifferential operators with smooth symbols in $L^2(\mathbb R^d)$.

Result: For polygons, singular values decrease as $O(k^{-1}\log k)$, and for sectors, an asymptotic formula confirms the sharpness of the bound.

Conclusion: The 'dual' symbol technique effectively addresses the problem, providing sharper bounds and insights into the spectrum of such operators.

Abstract: We study discrete spectrum of self-adjoint Weyl pseudodifferential operators
with discontinuous symbols of the form $1_\Omega \phi$ where $1_\Omega$ is the
indicator of a domain in $\Omega\subset\mathbb R^2$, and $\phi\in
C^\infty_0(\mathbb R^2)$ is a real-valued function. It was known that in
general, the singular values $s_k$ of such an operator satisfy the bound $s_k =
O(k^{-3/4})$, $k = 1, 2, \dots$. We show that if $\Omega$ is a polygon, the
singular values decrease as $O(k^{-1}\log k)$. In the case where $\Omega$ is a
sector, we obtain an asymptotic formula which confirms the sharpness of the
above bound.
  Our main technical tool is the reduction to another symbol that we call
\textit{dual}, which is automatically smooth. To analyse the dual symbol we
find new bounds for singular values of pseudodifferential operators with smooth
symbols in $L^2(\mathbb R^d)$ for arbitrary dimension $d\ge 1$.

</details>


### [32] [On Vanishing Viscosity with Inflow, Outflow](https://arxiv.org/abs/2506.17430)
*Michael A. Gulas,James P. Kelliher*

Main category: math.AP

TL;DR: Convergence of Navier-Stokes to Euler solutions as viscosity vanishes, with nonzero tangential outflow.


<details>
  <summary>Details</summary>
Motivation: To extend the work of Temam and Wang (2002) by allowing nonzero tangential outflow in the boundary conditions.

Method: Extends the approach of Temam and Wang (2002) for inflow/outflow boundary conditions.

Result: Proves convergence of Navier-Stokes solutions to Euler solutions as viscosity vanishes.

Conclusion: The extension successfully generalizes prior results for more realistic boundary conditions.

Abstract: We establish convergence as the viscosity vanishes of solutions of the
Navier-Stokes equations to a solution of the Euler equations for inflow,
outflow boundary conditions. We extend the approach of Temam and Wang 2002,
allowing the tangential component on outflow to be nonzero.

</details>


### [33] [Evolution Equations on Manifolds with Conical Singularities](https://arxiv.org/abs/2506.17481)
*Elmar Schrohe*

Main category: math.AP

TL;DR: Analysis of nonlinear evolution equations on manifolds with conical singularities using maximal regularity techniques, focusing on challenges from singularities and extensions of the conic Laplacian.


<details>
  <summary>Details</summary>
Motivation: To address difficulties posed by singularities in nonlinear evolution equations on manifolds and ensure bounded $H_\infty$-calculus.

Method: Uses maximal regularity techniques and introduces technical tools for handling singularities.

Result: Demonstrates applications to porous medium equation, fractional porous medium equation, Yamabe flow, and Cahn-Hilliard equation.

Conclusion: Provides a framework for analyzing nonlinear evolution equations on singular manifolds, with practical applications in various equations.

Abstract: This is an introduction to the analysis of nonlinear evolution equations on
manifolds with conical singularities via maximal regularity techniques. We
address the specific difficulties due to the singularities, in particular the
choice of extensions of the conic Laplacian that guarantee the existence of a
bounded $H_\infty$-calculus. We introduce the relevant technical tools and
survey, as main examples, applications to the porous medium equation, the
fractional porous medium equation, the Yamabe flow, and the Cahn-Hilliard
equation.

</details>


### [34] [A new infinite-dimensional Linking theorem with application to a system of coupled Poisson equations](https://arxiv.org/abs/2506.17563)
*Ablanvi Songo,Fabrice Colin*

Main category: math.AP

TL;DR: A new infinite-dimensional linking theorem for strongly indefinite functionals is established using minimax techniques and the τ-topology, generalizing the classical linking theorem, with an application to solving a system of coupled Poisson equations.


<details>
  <summary>Details</summary>
Motivation: To extend the classical linking theorem to strongly indefinite functionals and apply it to practical problems like coupled Poisson equations.

Method: Uses minimax techniques and the τ-topology of Kryszewski and Szulkin to characterize critical values of functionals.

Result: A generalized linking theorem is proven, leading to the existence of a nontrivial solution for a system of coupled Poisson equations.

Conclusion: The new theorem successfully generalizes the classical linking theorem and provides a tool for solving strongly indefinite problems.

Abstract: Using the minimax technique from the critical point theory, which consists in
constructing or transforming a suitable class of applications such that a
critical value $c$ of a functional $f$ can be characterized as a minimax value
over this class, we establish a new natural infinite-dimensional linking
theorem for strongly indefinite functionals by using the $\tau-$topology of
Kryszewski and Szulkin. Our result is a generalization of the classical linking
theorem \cite[Theorem 2.21]{Wi}. As an application, we obtain the existence of
a nontrivial solution to a system of coupled Poisson equations.

</details>


### [35] [Lipschitz regularity of weakly coupled vectorial almost-minimizers for Alt-Caffarelli functionals in Orlicz spaces](https://arxiv.org/abs/2506.17616)
*Pedro Fellype Pontes,João Vitor da Silva,Minbo Yang*

Main category: math.AP

TL;DR: Almost-minimizers of an Alt-Caffarelli type functional exhibit optimal Lipschitz continuity and universal gradient estimates, extending prior results for non-linear free boundary problems.


<details>
  <summary>Details</summary>
Motivation: To generalize and extend regularity results for almost-minimizers of functionals with non-standard growth, applicable to vectorial and scalar cases in free boundary problems.

Method: Analyze almost-minimizers of a functional involving a Young function $G$ and a characteristic term, focusing on Lipschitz continuity and gradient estimates.

Result: Optimal Lipschitz continuity on compact subsets and universal gradient estimates for non-negative almost-minimizers.

Conclusion: The work provides new insights and methods for non-linear free boundary problems, extending and unifying prior results.

Abstract: For a fixed constant $\lambda > 0$ and a bounded Lipschitz domain $\Omega
\subset \mathbb{R}^n$ with $n \geq 2$, we establish that almost-minimizers
(functions satisfying a sort of variational inequality) of the Alt-Caffarelli
type functional \[ \mathcal{J}_G({\bf v};\Omega) \coloneqq \int_\Omega
\left(\sum_{i=1}^mG\big(|\nabla v_i(x)|\big) + \lambda \chi_{\{|{\bf
v}|>0\}}(x)\right) dx , \] where ${\bf v} = (v_1, \dots, v_m)$ and $m \in
\mathbb{N}$, exhibit optimal Lipschitz continuity on compact subsets of
$\Omega$, where $G$ is a Young function satisfying specific growth conditions.
Furthermore, we obtain universal gradient estimates for non-negative
almost-minimizers in the interior of non-coincidence sets.
%{\color{blue}Furthermore, under the additional convexity assumption on $G$, we
address the problem of boundary Lipschitz regularity for $v$ by adopting a
fundamentally different analytical approach.} Notably, this method also
provides an alternative proof of the optimal local Lipschitz regularity in the
domain's interior. Our work extends the recent regularity results for weakly
coupled vectorial almost-minimizers for the $p$-Laplacian addressed in
\cite{BFS24}, and even the scalar case treated in \cite{daSSV}, \cite{DiPFFV24}
and \cite{PelegTeix24}, thereby providing new insights and approaches
applicable to a variety of non-linear one or two-phase free boundary problems
with non-standard growth.

</details>


### [36] [Entropy decrease and emergence of order in collective dynamics](https://arxiv.org/abs/2506.17635)
*Eitan Tadmor*

Main category: math.AP

TL;DR: The paper studies hydrodynamic collective dynamics with velocity alignment, proving global regularity under certain conditions.


<details>
  <summary>Details</summary>
Motivation: To understand global existence and smoothness of solutions in Euler alignment systems, addressing open questions in higher dimensions.

Method: Two-step approach: (i) analyzing entropy decrease and mono-kinetic closure, and (ii) proving global regularity for initial conditions meeting a critical threshold.

Result: Global regularity is maintained for a class of initial conditions, linked to entropy decrease, applicable in any spatial dimension.

Conclusion: The study resolves the open question of global existence beyond two dimensions, highlighting the role of entropy in collective dynamics.

Abstract: We study the hydrodynamic description of collective dynamics driven by
velocity {\it alignment}. It is known that such Euler alignment systems must
flock towards a limiting ``flocking'' velocity, provided their solutions remain
globally smooth. To address this question of global existence we proceed in two
steps. (i) Entropy and closure. The system lacks a closure, reflecting lack of
detailed energy balance in collective dynamics. We discuss the decrease of
entropy and the asymptotic behavior towards a mono-kinetic closure; and (ii)
Mono-kinetic closure. We prove that global regularity persists for all time for
a large class of initial conditions satisfying a critical threshold condition,
which is intimately linked to the decrease of entropy. The result applies in
any number of spatial dimensions, thus addressing the open question of
existence beyond two dimensions.

</details>


### [37] [Simultaneous Identification of Coefficients and Source in a Subdiffusion Equation from One Passive Measurement](https://arxiv.org/abs/2506.17648)
*Maolin Deng,Ali Feizmohammadi,Bangti Jin,Yavar Kian*

Main category: math.AP

TL;DR: The paper focuses on detecting parameters in anomalous diffusion using a single passive measurement, including coefficients and a time-dependent source term in a time-fractional diffusion equation. It provides uniqueness results in 1D and multidimensional cases, supported by a reconstruction algorithm and simulations.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of identifying multiple parameters in anomalous diffusion from limited data, specifically a single passive measurement.

Method: Uses spectral representation of solutions, complex and harmonic analysis, and inverse spectral results for Sturm-Liouville operators.

Result: Achieves uniqueness results in one dimension and extends to multidimensional cases under symmetry assumptions.

Conclusion: The theoretical findings are validated by a practical reconstruction algorithm and numerical simulations, demonstrating feasibility.

Abstract: This article is devoted to the detection of parameters in anomalous diffusion
from a single passive measurement. More precisely, we consider the simultaneous
identification of coefficients as well as a time-dependent source term
appearing in a time-fractional diffusion equation from a single boundary or
internal passive measurement. We obtain several uniqueness results in dimension
one as well as a multidimensional extension under some symmetry assumptions.
Our analysis relies on spectral representation of solutions, complex and
harmonic analysis combined with some known inverse spectral results for
Sturm-Liouville operators. The theoretical results are complemented by a
corresponding reconstruction algorithm and numerical simulations.

</details>


### [38] [On Growth of Sobolev norms for cubic Schrödinger equation with harmonic potential in dimensions $d=2,3$](https://arxiv.org/abs/2506.17731)
*Yilin Song,Ruixiao Zhang,Jiqiang Zheng*

Main category: math.AP

TL;DR: The paper studies the growth of higher-order Sobolev norms for solutions to a defocusing cubic nonlinear Schrödinger equation with a harmonic potential in 2D and 3D. It improves bilinear Strichartz estimates and uses the upside-down I-method to show polynomial growth, addressing challenges in frequency interaction.


<details>
  <summary>Details</summary>
Motivation: Motivated by prior work (Planchon-Tzvetkov-Visciglia), the paper aims to remove the ε-loss in bilinear Strichartz estimates and extend results to higher dimensions.

Method: The authors establish bilinear Strichartz estimates and employ the upside-down I-method, focusing on controlling frequency interactions and explicit eigenfunction product interactions.

Result: The paper achieves polynomial growth bounds for Sobolev norms, improving prior results in 2D and providing new findings in 3D.

Conclusion: The study successfully addresses challenges in frequency interaction and extends results to higher dimensions, contributing to the understanding of nonlinear Schrödinger equations with harmonic potentials.

Abstract: In this article, we study the growth of higher-order Sobolev norms for
solutions to the defocusing cubic nonlinear Schr\"odinger equation with
harmonic potential in dimensions $d=2,3$, \begin{align}\label{PNLS}
\begin{cases}\tag{PNLS}
i\partial_tu-Hu=|u|^{2}u,&(t,x)\in\mathbb{R}\times\mathbb{R}^d,\\
u(0,x)=u_0(x), \end{cases} \end{align} where $H=-\Delta+|x|^2$. Motivated by
Planchon-Tzvetkov-Visciglia [Rev. Mat. Iberoam., 39 (2023), 1405-1436], we
first establish the bilinear Strichartz estimates, which removes the
$\varepsilon$-loss of Burq-Poiret-Thomann [Preprint, arXiv: 2304.10979]. To
show the polynomial growth of Sobolev norm, our proof relies on the upside-down
$I$-method associated to the harmonic oscillator. Due to the lack of Fourier
transform or expansion, we need to carefully control the freqeuncy interaction
of the type "high-high-low-low". To overcome this difficulty, we establish the
explicit interaction for products of eigenfunctions. Our bound covers the
result of Planchon-Tzvetkov-Visciglia [Rev. Mat. Iberoam., 39 (2023),
1405-1436] in dimension two and is new in dimension three.

</details>


### [39] [Diffusion-free boundary conditions for the Navier-Stokes equations](https://arxiv.org/abs/2506.17749)
*Emmanuel Dormy,David Gerard-Varet*

Main category: math.AP

TL;DR: The paper analyzes diffusion-free boundary conditions for the Navier-Stokes equations, showing their effectiveness in reducing diffusive effects in numerical studies of nearly inviscid solutions.


<details>
  <summary>Details</summary>
Motivation: To mathematically validate the diffusion-free boundary conditions introduced by Lin and Kerswell for inertial waves in rotating fluids, and to explore their impact on the well-posedness of the Navier-Stokes equations.

Method: The study uses mathematical analysis of the nonlinear Navier-Stokes equations in bounded domains (2D and 3D) with diffusion-free boundary conditions, including boundary layer analysis for vanishing viscosity.

Result: The boundary conditions ensure well-posedness (global for 2D, local for 3D) and reduce boundary layer flow amplitude to order ν, significantly lower than standard conditions.

Conclusion: Diffusion-free boundary conditions are analytically confirmed to minimize diffusive effects, making them suitable for numerical studies of nearly inviscid solutions.

Abstract: We provide a mathematical analysis of the `diffusion-free' boundary
conditions recently introduced by Lin and Kerswell for the numerical treatment
of inertial waves in a fluid contained in a rotating sphere. We consider here
the full setting of the nonlinear Navier-Stokes equation in a general bounded
domain $\Omega$ of $\mathbb{R}^d$, $d=2$ or $3$. We show that diffusion-free
boundary conditions $$ \Delta u \cdot \tau \vert_{\partial \Omega} = 0, \quad u
\cdot n\vert_{\partial \Omega} = 0 \quad \text{ when } d=2, $$ $$ \Delta u
\times n\vert_{\partial \Omega} = 0, \quad u \cdot n\vert_{\partial \Omega} = 0
\quad \text{ when } d=3, $$
  allow for a satisfactory well-posedness theory of the full Navier-Stokes
equations (global in time for $d=2$, local for $d=3$). Moreover, we perform a
boundary layer analysis in the limit of vanishing viscosity $\nu \rightarrow
0$. We establish that the amplitude of the boundary layer flow is in this case
of order $\nu$, i.e. much lower than in the case of standard Dirichlet or even
stress-free conditions. This confirms analytically that this choice of boundary
conditions may be used to reduce diffusive effects in numerical studies relying
on the Navier-Stokes equation to approach nearly inviscid solutions.

</details>


### [40] [Berezin-Li-Yau inequality for mixed local-nonlocal Dirichlet-Laplacian](https://arxiv.org/abs/2506.17780)
*Aidyn Kassymov,Berikbol T. Torebek*

Main category: math.AP

TL;DR: The paper analyzes eigenvalue problems for mixed local-nonlocal Laplacian operators, establishing Berezin-Li-Yau inequalities for different parameter cases.


<details>
  <summary>Details</summary>
Motivation: To study eigenvalue problems involving mixed local-nonlocal Laplacian operators and derive lower bounds for eigenvalue sums.

Method: The paper considers two cases: (1) positive coefficients for both local and nonlocal terms, and (2) a positive local coefficient and a negative nonlocal coefficient within a specific range. The Berezin-Li-Yau inequality is derived for both cases.

Result: For the first case, the inequality combines classical and fractional forms. For the second case, the inequality explicitly depends on the embedding constant.

Conclusion: The paper successfully extends the Berezin-Li-Yau inequality to mixed local-nonlocal Laplacian operators, covering both positive and negative nonlocal coefficients.

Abstract: In this paper, we consider an eigenvalue problem for mixed local-nonlocal
Laplacian
$$\mathcal{L}^{a,b}_{\Om}:=-a\Delta+b(-\Delta)^s,\,a>0,\,b\in\mathbb{R},\,s\in
(0,1),$$ with Dirichlet boundary conditions. First, the case $a>0$ and $b>0$ is
considered and the Berezin-Li-Yau inequality (lower bounds of the sum of
eigenvalues) is established. This inequality is characterised as the maximum of
the classical and fractional versions of the Berezin-Li-Yau inequality, and, in
particular, yields both the classical and fractional forms of the
Berezin-Li-Yau inequality. Next, we consider the case $a>0$ and
$-\frac{a}{C_E}<b<0$, where $C_E\geq 1$ is the constant of the continuous
embedding $H_{0}^{1}(\Om)\subset H_{0}^{s}(\Om)$. In this setting, we also
derive the Berezin-Li-Yau inequality, which explicitly depends on the constant
$C_E$.

</details>


### [41] [Low regularity well-posedness of nonlocal dispersive perturbations of Burgers' equation](https://arxiv.org/abs/2506.17801)
*Luc Molinet,Didier Pilod,Stéphane Vento*

Main category: math.AP

TL;DR: The paper analyzes the local and global well-posedness of a class of dispersive perturbations of Burgers' equations, including the low dispersion Benjamin-Ono equation, in Sobolev spaces. It provides regularity thresholds for well-posedness and derives a priori estimates.


<details>
  <summary>Details</summary>
Motivation: The study aims to understand the well-posedness of dispersive perturbations of Burgers' equations, which are important in fluid dynamics and mathematical physics, particularly for the Benjamin-Ono equation.

Method: The authors use analytical techniques to prove local well-posedness in Sobolev spaces $H^s(\mathbb K)$ for $s > s_{\alpha}$, with unconditional uniqueness for $s > \max\{\frac{1}{2}, s_{\alpha}\}$. They also derive a priori estimates for lower regularity thresholds.

Result: Local well-posedness is established for $s > s_{\alpha}$, and global well-posedness is deduced for $s > s_{\alpha}$ when $\alpha > \frac{2}{3}$, and in the energy space $H^{\frac{\alpha}{2}}(\mathbb K)$ when $\alpha > \frac{4}{5}$.

Conclusion: The results extend the understanding of well-posedness for dispersive perturbations of Burgers' equations, providing explicit regularity thresholds and implications for global solutions, particularly in the context of the Benjamin-Ono equation.

Abstract: We consider the Cauchy problem associated to a class of dispersive
perturbations of Burgers' equations, which contains the low dispersion
Benjamin-Ono equation, (also known as low dispersion fractional KdV equation),
$$ \partial_tu-D_x^{\alpha}\partial_xu=\partial_x(u^2) \, ,$$ and prove that it
is locally well-posed in $H^s(\mathbb K)$, $\mathbb K=\mathbb R$ or $\mathbb
T$, for $s>s_{\alpha}$, where \begin{equation*} s_\alpha=\begin{cases}
  1-\frac{3\alpha}4 & \text{for} \quad \frac23 \le \alpha \le 1;
  \frac 32(1-\alpha) & \text{for} \quad \frac13 \le \alpha \le \frac23;
  \frac 32-\frac{\alpha}{1-\alpha} & \text{for} \quad 0 < \alpha \le \frac13 .
  \end{cases} \end{equation*} The uniqueness is unconditional in $H^s(\mathbb
K)$ for $s>\max\{\frac12,s_{\alpha}\}$. Moreover, we obtain \emph{a priori}
estimates for the solutions at the lower regularity threshold
$s>\widetilde{s}_\alpha$ where \begin{equation*}
\widetilde{s}_\alpha=\begin{cases}
  \frac 12-\frac \alpha 4 & \text{for} \quad \frac23 \le \alpha \le 1;
  1-\alpha & \text{for} \quad \frac12 \le \alpha \le \frac23;
  \frac 32-\frac{\alpha}{1-\alpha} & \text{for} \quad 0 < \alpha \le \frac12 .
  \end{cases} \end{equation*} As a consequence of these results and of the
Hamiltonian structure of the equation, we deduce global well-posedness in
$H^s(\mathbb K)$ for $s>s_{\alpha}$ when $\alpha>\frac23$, and in the energy
space $H^{\frac{\alpha}2}(\mathbb K)$ when $\alpha>\frac45$.

</details>


### [42] [Structure of two-dimensional mod$(q)$ area-minimizing currents near flat singularities: the codimension one case](https://arxiv.org/abs/2506.17813)
*Anna Skorobogatova,Luca Spolaor,Salvatore Stuvard*

Main category: math.AP

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We obtain a fine structural result for two-dimensional mod$(q)$
area-minimizing currents of codimension one, close to flat singularities.
Precisely, we show that, locally around any such singularity, the current is a
$C^{1,\alpha}$-perturbation of the graph of a radially homogeneous special
multiple-valued function that arises from a superposition of homogeneous
harmonic polynomials. Additionally, as a preliminary step towards an analogous
result in arbitrary codimension, we prove in general that the set of flat
singularities of density $\frac{q}{2}$, where the current is ``genuinely
mod$(q)$", consists of isolated points.

</details>


### [43] [Global well-posedness of planar MHD system without heat conductivity](https://arxiv.org/abs/2506.18057)
*Jinkai Li,Mingjie Li*

Main category: math.AP

TL;DR: Global well-posedness of strong solutions for the planar MHD system with viscosity and resistivity, allowing density discontinuity and vacuum, is proven for large initial data.


<details>
  <summary>Details</summary>
Motivation: To address the Cauchy problem for the planar MHD system without heat conductivity, focusing on global solutions despite far field vacuum and large initial data.

Method: Utilizes a new coupling structure between velocity and magnetic field, avoiding the entropy-type energy inequality due to vacuum inconsistency.

Result: Establishes global well-posedness for strong solutions, accommodating density discontinuity and interior vacuum.

Conclusion: The study successfully overcomes challenges posed by vacuum and large data, introducing a novel coupling approach for dissipative estimates.

Abstract: In this paper, we consider the Cauchy problem to the planar
magnetohydrodynamics (MHD) system with both constant viscosity and constant
resistivity but without heat conductivity. Global well-posedness of strong
solutions in the presence of natural far field vacuum, due to the finiteness of
the mass, is established for any large initial data of suitable smoothness.
Density discontinuity and interior vacuum of either point-like or
piecewise-like are also allowed. Technically, the entropy-type energy
inequality, which although is commonly used as a basic tool in existing
literature on the planar MHD system, is not workable in the current paper, as
it is not consistent with the far field vacuum. Instead, besides making full
use of advantages of the effective viscous flux as in
\cite{LJK1DNONHEAT,LIJLIM2022,LIXINADV}, a new coupling structure, between the
longitudinal velocity $u$ and the transversal magnetic field $\bm h$, is
exploited to recover the dissipative estimate on $\bm h$.

</details>


### [44] [Ground states of the planar nonlinear Schrödinger--Newton system with a point interaction](https://arxiv.org/abs/2506.18202)
*Gustavo de Paula Ramos*

Main category: math.AP

TL;DR: The paper establishes conditions for ground states in a nonlinear Schrödinger-Newton system with point interaction and links critical points to standing waves.


<details>
  <summary>Details</summary>
Motivation: To analyze the existence of ground states in a specific nonlinear system and connect them to standing waves in an evolution problem.

Method: Uses the Laplacian of point interaction and constrained energy functional to derive conditions.

Result: Sufficient conditions for ground states are established, and critical points are linked to standing waves.

Conclusion: The study provides insights into the behavior of the system and its connection to standing waves.

Abstract: We establish sufficient conditions for the existence of ground states of the
following normalized nonlinear Schr\"odinger--Newton system with a point
interaction: \[ \begin{cases} - \Delta_\alpha u = w u + \beta u |u|^{p - 2}
&\text{on} ~ \mathbb{R}^2; \\ - \Delta w = 2 \pi |u|^2 &\text{on} ~
\mathbb{R}^2; \\ \|u\|_{L^2}^2 = c, \end{cases} \] where $p > 2$; $\alpha,
\beta \in \mathbb{R}$ and $- \Delta_\alpha$ denotes the Laplacian of point
interaction with scattering length $(- 2 \pi \alpha)^{- 1}$. Additionally, we
show that critical points of the corresponding constrained energy functional
are naturally associated with standing waves of the evolution problem \[
\mathrm{i} \psi' (t) = - \Delta_\alpha \psi (t) - (\log |\cdot| \ast |\psi
(t)|^2) \psi (t) - \beta \psi (t) |\psi (t)|^{p - 2}. \]

</details>


### [45] [Incompressible Euler limit from the Boltzmann equation with Maxwell reflection boundary condition in the half-space](https://arxiv.org/abs/2506.18420)
*Ning Jiang,Chao Wang,Yulong Wu,Zhifei Zhang*

Main category: math.AP

TL;DR: The paper justifies the incompressible Euler limit of the Boltzmann equation with Maxwell reflection boundary conditions, focusing on shear flows and overcoming derivative loss challenges.


<details>
  <summary>Details</summary>
Motivation: To rigorously validate the incompressible Euler limit for the Boltzmann equation in a half-space with general boundary conditions, addressing the complexity introduced by nonlinear Prandtl layers.

Method: Uses an analytic L²-L∞ framework to handle the loss of normal derivatives by converting it into tangential derivative loss, leveraging full conservation laws.

Result: Establishes uniform estimates for remainder equations, confirming the validity of the Euler limit for shear flows.

Conclusion: The approach successfully justifies the Euler limit for the Boltzmann equation in the specified boundary and flow conditions.

Abstract: In this paper, we rigorously justify the incompressible Euler limit of the
Boltzmann equation with general Maxwell reflection boundary condition in the
half-space. The accommodation coefficient $\alpha \in (0,1]$ is assumed to be
$O(1)$. Our construction of solutions includes the interior fluid part and
Knudsen-Prandtl coupled boundary layers. The corresponding solutions to the
nonlinear Euler and nonlinear Prandtl systems are taken to be shear flows. Due
to the presence of the nonlinear Prandtl layer, the remainder equation loses
one order normal derivative. The key technical novelty lies in employing the
full conservation laws to convert this loss of the normal derivative into the
loss of tangential spatial derivative, avoiding any loss of regularity in time.
By working within an analytic $L^2 \mbox{-} L^\infty$ framework, we establish
the uniform estimate on the remainder equations, thus justify the validity of
the incompressible Euler limit from the Boltzmann equation for the shear flow
case.

</details>


### [46] [Regularity of random attractor and fractal dimension of fractional stochastic Navier-Stokes equations on three-dimensional torus](https://arxiv.org/abs/2506.18480)
*Hui Liu,Chengfeng Sun,Jie Xin*

Main category: math.AP

TL;DR: The paper studies the asymptotic dynamics of fractional Navier-Stokes equations with additive white noise on a 3D torus, proving the existence of finite-dimensional random attractors under specific conditions.


<details>
  <summary>Details</summary>
Motivation: To understand the long-term behavior of fractional Navier-Stokes equations under stochastic perturbations, particularly focusing on the existence and properties of random attractors.

Method: The authors use indirect estimation and iterative methods to construct bounded absorbing sets and prove Lipschitz continuity, leading to asymptotic compactness.

Result: The system possesses finite-dimensional tempered random attractors in higher-order Sobolev spaces under given conditions on external forces and noise intensity.

Conclusion: The findings provide rigorous mathematical insights into the dynamics of stochastic fractional Navier-Stokes equations, confirming the existence of finite-dimensional attractors.

Abstract: In this paper we will study the asymptotic dynamics of fractional
Navier-Stokes (NS) equations with additive white noise on three-dimensional
torus $\mathbb T^3$. Under the conditions that the external forces $f(x)$
belong to the phase space $ H$ and the noise intensity function $h(x)$
satisfies $\|\nabla h\|_{L^\infty} < \sqrt \pi \nu \lambda_1^\frac{5}{4}$,
where $ \nu $ is the kinematic viscosity of the fluid and $\lambda_1$ is the
first eigenvalue of the Stokes operator, we shown that the random fractional
three-dimensional NS equations possess a tempered $(H,H^\frac{5}{2})$-random
attractor whose fractal dimension in $H^\frac{5}{2}$ is finite. This was proved
by establishing, first, an $H^\frac{5}{2}$ bounded absorbing set and, second, a
local $(H,H^\frac{5}{2})$-Lipschitz continuity in initial values from which the
$(H,H^\frac{5}{2})$-asymptotic compactness of the system follows. Since the
forces $f$ belong only to $H$, the $H^\frac{5}{2}$ bounded absorbing set was
constructed by an indirect approach of estimating the $H^\frac{5}{2}$-distance
between the solutions of the random fractional three-dimensional NS equations
and that of the corresponding deterministic equations. Furthermore, under the
conditions that the external forces $f(x)$ belong to the $ H^{k-\frac{5}{4}}$
and the noise intensity function $h(x)$ belong to $H^{k+\frac{5}{4}}$ for
$k\geq\frac{5}{2}$, we shown that the random fractional three-dimensional NS
equations possess a tempered $(H,H^k)$-random attractor whose fractal dimension
in $H^k$ is finite. This was proved by using iterative methods and
establishing, first, an $H^k$ bounded absorbing set and, second, a local
$(H,H^k)$-Lipschitz continuity in initial values from which the
$(H,H^k)$-asymptotic compactness of the system follows.

</details>


### [47] [Stationary gravitational modes on Kerr-anti-de Sitter spacetimes](https://arxiv.org/abs/2506.18524)
*Olivier Graf*

Main category: math.AP

TL;DR: The paper proves the existence of non-trivial stationary mode solutions for Teukolsky equations near the Hawking-Reall threshold in Kerr-adS black holes, using a novel WKB-based shooting method.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of solving Teukolsky equations for large azimuthal numbers, where variational methods fail.

Method: A shooting-type continuity argument combined with high-frequency WKB approximation.

Result: Existence of stationary mode solutions for large |m|, accumulating at the Hawking-Reall threshold.

Conclusion: The study provides a new approach to solving Teukolsky equations in extreme parameter regimes.

Abstract: We prove that, for any continuous path of Kerr-adS black hole parameters
crossing the Hawking-Reall threshold, for all azimuthal number $|m|$
sufficiently large, there exists black hole parameters on the path such that
the Teukolsky equations with conformal boundary conditions admit a non-trivial
regular stationary (\emph{i.e.} with frequency $\omega=m\omega_+$) mode
solution. When $|m|$ goes to infinity, we show that these black hole parameters
accumulate at the Hawking-Reall threshold. The major difficulty is that one
cannot construct solutions to the system of Teukolsky equations using
variational arguments as it is usually done for the classical wave equation. We
introduce a new scheme of proof based on a shooting-type continuity argument
and a high-frequency WKB approximation.

</details>


### [48] [On an Iterative Scheme for the Spinorial Yamabe Equation on Manifolds with Boundary](https://arxiv.org/abs/2506.18546)
*Eric Trébuchon*

Main category: math.AP

TL;DR: Existence and regularity of solutions for the spinorial Yamabe equation on compact manifolds with boundary, using iterative and bootstrapping methods.


<details>
  <summary>Details</summary>
Motivation: To address the existence and regularity of solutions for the spinorial Yamabe equation, particularly for compact manifolds with boundary, which is a conformal invariant problem.

Method: An iterative scheme for the inhomogeneous equation under small parameter assumptions, and bootstrapping methods to extend solution regularity.

Result: Solutions exist under small parameter conditions, with smoothness away from zero sets and boundary regularity under Shapiro--Lopatinski conditions.

Conclusion: The study successfully establishes solution existence and regularity for the spinorial Yamabe equation on bounded manifolds, with specific boundary conditions.

Abstract: We study the existence of solutions to the spinorial Yamabe equation -- that
is, the Euler--Lagrange equation associated with the conformal invariant
introduced by S. Raulot -- for compact manifolds with boundary. For the
inhomogeneous equation, we employ an iterative scheme to establish existence
under smallness assumptions on the relevant parameters. Using bootstrapping
methods, we extend the regularity of the solution to $C^\infty$ away from its
zero set in the interior, and up to the boundary in the case of
Shapiro--Lopatinski boundary conditions.

</details>


### [49] [Homogenization of magnetoelastic materials with rigid magnetic inclusions at small strains](https://arxiv.org/abs/2506.18585)
*Raffaele Grande,Stefan Krömer,Martin Kružík,Giuseppe Tomassetti*

Main category: math.AP

TL;DR: The paper studies homogenization of a linearly elastic magnetic material with rigid inclusions, deriving an effective magnetoelastic energy as the period approaches zero. It also compares alternative magnetic models linked by a Legendre-Fenchel transform.


<details>
  <summary>Details</summary>
Motivation: To understand the effective behavior of magnetoelastic materials with periodic rigid inclusions and explore equivalent magnetic models.

Method: Analyzes a periodic arrangement of the material, homogenizing it to derive an effective energy. Compares alternative models using a Legendre-Fenchel transform.

Result: Identifies an effective magnetoelastic energy for the homogenized material. Shows equivalence of alternative magnetic models.

Conclusion: The homogenization approach successfully captures the effective behavior, and alternative models provide equivalent insights.

Abstract: We investigate a homogenization problem for a linearly elastic magnetic
material that incorporates elastically rigid magnetic inclusions firmly bonded
to the matrix. By considering a periodic arrangement of this material, we
identify an effective magnetoelastic energy, obtained by homogenization when
the period approaches zero. For comparison, we also briefly discuss
alternative, essentially equivalent magnetic models naturally linked by a
Legendre-Fenchel transform of magnetic energy density where the elastic
deformation enters as a parameter.

</details>


### [50] [Regularity of a bulk-surface Cahn-Hilliard model driven by Leray velocity fields](https://arxiv.org/abs/2506.18617)
*Andrea Giorgini,Patrik Knopf,Jonas Stange*

Main category: math.AP

TL;DR: The paper extends well-posedness results for a convective bulk-surface Cahn-Hilliard system to more general velocity fields, including Leray-type fields, using a novel elliptic system theory.


<details>
  <summary>Details</summary>
Motivation: Existing well-posedness results for the system require high time regularity for velocity fields, which is restrictive for physical applications. The paper aims to relax these assumptions.

Method: The authors prove well-posedness of weak solutions under general regularity assumptions and establish strong solutions for Leray-type velocity fields using a new elliptic system theory.

Result: Well-posedness is achieved for weaker velocity field assumptions, and strong solutions are shown for physically relevant Leray-type fields.

Conclusion: The results broaden the applicability of the model to more realistic physical scenarios, supported by a new theoretical framework for singular nonlinearities.

Abstract: We consider a convective bulk-surface Cahn--Hilliard system with dynamic
boundary conditions and singular potentials. For this model, well-posedness
results concerning weak and strong solutions have already been established in
the literature. However, they require the prescribed velocity fields to belong
to function spaces with high time regularity. In this paper, we prove that the
well-posedness of weak solutions holds true under more general regularity
assumptions on the velocity fields. Next, via an alternative proof for higher
regularity, we show the well-posedness of strong solutions for velocity fields
of Leray type, which is a more relevant assumption for physical applications.
Our approach hinges upon a new well-posedness and regularity theory for a
bulk-surface elliptic system with singular nonlinearities, which may be of
independent interest.

</details>


### [51] [Stabilization of Quasilinear Parabolic Equations by Cubic Feedback at Boundary with Estimated Region of Attraction](https://arxiv.org/abs/2506.18634)
*Mohamed Camil Belhadjoudja,M Maghenem,E Witrant,M Krstic*

Main category: math.AP

TL;DR: The paper analyzes quasilinear parabolic PDEs with finite-time blow-up under null boundary conditions, proposing cubic feedback laws for boundary control to ensure stability and other properties.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of stabilizing PDEs with nonlinear state-dependent terms (diffusion, convection, reaction) and finite-time blow-up, using boundary feedback control.

Method: Cubic feedback laws applied at the boundary, leveraging boundary measurements, to estimate the region of attraction and ensure stability.

Result: Achieves L2 and H1 exponential stability, convergence of H2 and C1 norms, existence of classical solutions, and positivity preservation. The region of attraction can grow with diffusion.

Conclusion: The proposed cubic feedback framework effectively stabilizes nonlinear PDEs, with flexible boundary condition implementation and improved region of attraction estimates.

Abstract: For quasilinear parabolic partial differential equations (PDEs) that exhibit
finite-time blow up in open loop, i.e., under null boundary conditions, we
provide an estimate of the region of attraction under cubic feedback laws
applied at the boundary, using boundary measurements. We guarantee: 1-L 2 and H
1 exponential stability of the origin with an estimate of the region of
attraction. 2-Convergence of the H 2 and the C 1 norms of the solutions to
zero. 3-Existence and uniqueness of complete classical solutions. 4-Positivity
of the solutions starting from positive initial conditions. Unlike existing
approaches, our framework handles nonlinear state-dependent diffusion,
convection, and (destabilizing) reaction. The cubic terms are used to enlarge
our estimate of the region of attraction. The size of the region of attraction
is shown, in many cases, to grow unboundedly as diffusion increases. Finally,
our controllers can be implemented as Neumann, Dirichlet, or mixed-type
boundary conditions.

</details>


### [52] [Free boundary regularity and well-posedness of physical solutions to the supercooled Stefan problem](https://arxiv.org/abs/2506.18741)
*Sebastian Munoz*

Main category: math.AP

TL;DR: The paper analyzes the regularity and well-posedness of solutions to the supercooled Stefan problem, proving smoothness of the free boundary and addressing uniqueness questions.


<details>
  <summary>Details</summary>
Motivation: To resolve conjectures and open questions about the regularity and uniqueness of solutions to the supercooled Stefan problem, particularly regarding the behavior of the free boundary.

Method: Derives a weighted obstacle problem, uses it to analyze regularity and non-degeneracy, and classifies free boundary points. Also employs backward propagation of oscillation to control jump occurrences.

Result: The free boundary is $C^1$ in space and $C^{\infty}$ outside a countable set. Jumps in time cannot accumulate, and short-time uniqueness implies global uniqueness.

Conclusion: The study resolves open questions, proving global uniqueness and regularity for general initial data, advancing understanding of the supercooled Stefan problem.

Abstract: We study the regularity and well-posedness of physical solutions to the
supercooled Stefan problem. Assuming only that the initial temperature is
integrable, we prove that the free boundary, known to have jump discontinuities
as a function of the time variable, is $C^1$ as a function of the space
variable, and is $C^{\infty}$ outside of a closed, countable set, which we
describe explicitly. We also prove that, as conjectured in arXiv:1902.05174,
the set of positive times when a jump occurs cannot have accumulation points.
In addition, we prove that short-time uniqueness of physical solutions implies
global uniqueness, which allows us to obtain uniqueness for very general
initial data that fall outside the scope of the current well-posedness regime.
In particular, we answer two questions left open in arXiv:1811.12356,
arXiv:2302.13097, regarding the global uniqueness of solutions. We proceed by
deriving a weighted obstacle problem satisfied by the solutions, which we
exploit to establish regularity and non-degeneracy estimates and to classify
the free boundary points. We also establish a backward propagation of
oscillation property, which allows us to control the occurrence of future jumps
in terms of the past oscillation of the solution.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [53] [XtalOpt Version 14: Variable-Composition Crystal Structure Search for Functional Materials Through Pareto Optimization](https://arxiv.org/abs/2506.17246)
*Samad Hajinazar,Eva Zurek*

Main category: physics.comp-ph

TL;DR: XtalOpt version 14 introduces an enhanced multi-objective global optimization algorithm for crystal structure prediction, integrating ab initio methods and machine-learning potentials for efficient discovery of functional materials.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the search for novel crystal structures with variable composition, enabling efficient discovery of functional materials.

Method: The method involves integrating ab initio methods, classical potentials, and machine-learning potentials for structural relaxation, alongside Pareto optimization for multi-objective search.

Result: The result is an enhanced version of XtalOpt (version 14) with improved capabilities for ground state search and functional materials discovery.

Conclusion: XtalOpt version 14 offers advanced tools for crystal structure prediction, making it a valuable resource for materials science research.

Abstract: Version 14 of XtalOpt, evolutionary multi-objective global optimization
algorithm for crystal structure prediction, is now available for download from
its official website https://xtalopt.github.io. The new version of the code is
designed to perform ground state search for novel crystal structures with
variable composition by integrating a suite of ab initio methods alongside
classical and machine-learning potentials for structural relaxation. The
multi-objective search framework has been further enhanced through the
introduction of Pareto optimization, enabling efficient discovery of functional
materials. Here, we describe the implemented methodologies, provide detailed
instructions for their use, and present an overview of additional improvements
included in XtalOpt version 14.

</details>


### [54] [A Comprehensive Evaluation of the Bernoulli-Trial Collisions Families in Treating Rarefied Micro Flows and Hypersonic Flows](https://arxiv.org/abs/2506.17256)
*Ahmad Shoja-sani,Maryam Javani,Ehsan Roohi,Stefan Stefanov*

Main category: physics.comp-ph

TL;DR: The paper evaluates Bernoulli-trials (BT) family collision algorithms (SBT, GBT, SSBT, SGBT) in DSMC, showing they maintain collision frequency better than NTC and NN methods, with SGBT matching theory for velocity distribution moments.


<details>
  <summary>Details</summary>
Motivation: To comprehensively evaluate BT-based collision partner selection schemes in DSMC for rarefied gas dynamics, comparing their performance to standard methods.

Method: Analyzes SBT, GBT, SSBT, and SGBT algorithms, focusing on collision partner selection from particle lists, and tests them on standard problems like the BKW solution.

Result: BT-based algorithms outperform NTC and NN in maintaining collision frequency with fewer particles. SGBT matches theoretical predictions for velocity distribution moments.

Conclusion: BT-based schemes, especially SGBT, are effective for DSMC, offering accurate collision frequency and velocity distribution results.

Abstract: The collision process is essential to the Direct Simulation Monte Carlo
(DSMC) method, as it incorporates the fundamental principles of the Boltzmann
and Kac stochastic equations. A series of collision algorithms, known as the
Bernoulli-trials (BT) family schemes, have been proposed based on the Kac
stochastic equation. The impetus of this paper is to present a comprehensive
evaluation of different BT-based collision partner selection schemes, including
the simplified Bernoulli-trials (SBT), generalized Bernoulli-trials (GBT),
symmetrized and simplified Bernoulli-trials (SSBT), and newly proposed
symmetrized and generalized Bernoulli-trials (SGBT), in treating some standard
rarefied gas dynamics problems. In these algorithms, the first particle is
chosen in a specific order from the list of particles in the cell. For the SBT
and GBT, the second particle is chosen from the remaining particles in the list
placed after the first particle; however, for SSBT and SGBT, the second
particle is chosen from all particles in the list. This means that the SBT and
GBT select pairs from the upper triangle of the collision probability matrix,
while symmetrized algorithms such as SSBT and SGBT utilize the entire matrix.
The results show that the BT-based collision algorithms, compared to the
standard "No Time Counter (NTC)" and "Nearest Neighbor (NN)", successfully
maintain the collision frequency as the number of particles per cell decreases.
Simulation of the Bobylev-Krook-Wu (BKW) exact solution of the Boltzmann
equation indicates that, like the GBT, the SGBT algorithm produces the same
results as the theory for the average of the 4th moment of the velocity
distribution function (VDF).

</details>


### [55] [Refining Tc Prediction in Hydrides via Symbolic-Regression-Enhanced Electron-Localization-Function-Based Descriptors](https://arxiv.org/abs/2506.17456)
*Francesco Belli,Sean Torres,Julia Contreras-Garcìa,Eva Zurek*

Main category: physics.comp-ph

TL;DR: The paper improves the prediction of superconducting critical temperatures (Tc) in hydrogen-based materials by expanding the dataset and introducing a new descriptor, the molecularity index.


<details>
  <summary>Details</summary>
Motivation: To enhance the predictive power of the Electron Localization Function (ELF) descriptor for Tc in hydrogen-containing compounds, addressing limitations due to dataset size and compositional complexity.

Method: Compiled a larger dataset of 244 hydride superconductors, introduced the molecularity index, and applied symbolic regression to improve accuracy.

Result: The ELF-based descriptor's predictive power declines with complexity, but the molecularity index significantly enhances prediction accuracy.

Conclusion: The study provides a robust framework for screening hydride superconductors, aiding in accelerated discovery of new materials.

Abstract: Hydrogen-based materials are able to possess extremely high superconducting
critical temperatures, \tc s, due to hydrogen's low atomic mass and strong
electron-phonon interaction. Recently, a descriptor based on the Electron
Localization Function (ELF) has enabled the rapid estimation of the \tc\ of
hydrogen-containing compounds from electronic networking properties, but its
applicability has been limited by the small size and homogeneity of the
training dataset used. Herein, the model is re-examined compiling a publicly
available combined dataset of 244 binary and ternary hydride superconductors.
Our analysis shows that though ELF-based networking remains a valuable
descriptor, its predictive power declines with increasing compositional
complexity. However, by introducing the molecularity index, defined as the
highest value of the ELF at which two hydrogen atoms connect, and applying
symbolic regression, the accuracy of the predictions can be substantially
enhanced. These results establish a more robust framework for assessing
superconductivity in hydride materials, facilitating accelerated screening of
novel candidates through integration with crystal structure prediction methods
or high-throughput searches.

</details>


### [56] [JAX-LaB: A High-Performance, Differentiable, Lattice Boltzmann Library for Modeling Multiphase Fluid Dynamics in Geosciences and Engineering](https://arxiv.org/abs/2506.17713)
*Piyush Pradhan,Pierre Gentine,Shaina Kelly*

Main category: physics.comp-ph

TL;DR: JAX-LaB is a Python-based Lattice Boltzmann library for simulating multiphase flows in porous media, leveraging JAX for performance and scalability. It improves multiphase modeling and wetting control, validated through benchmarks and open-sourced.


<details>
  <summary>Details</summary>
Motivation: To provide a performant, hardware-agnostic tool for simulating complex multiphase flows in porous media, integrating with machine learning workflows.

Method: Uses the Shan-Chen pseudopotential method with an improved forcing scheme and virtual density for wetting. Built on JAX for scalability across CPUs/GPUs.

Result: Validated with benchmarks (Laplace's law, capillary rise) and demonstrated efficient performance scaling on GPUs.

Conclusion: JAX-LaB is a versatile, open-source library for accurate multiphase flow simulations, suitable for diverse applications.

Abstract: We present JAX-LaB, a differentiable, Python-based Lattice Boltzmann library
for simulating multiphase and multiphysics flows in hydrologic, geologic, and
engineered porous media. Built as an extension of the XLB library, JAX-LaB
utilizes JAX for computations and offers a performant, hardware-agnostic
implementation that integrates seamlessly with machine learning workflows and
scales efficiently across CPUs, GPUs, and distributed systems. Multiphase
interactions are modeled using the Shan-Chen pseudopotential method, which is
coupled with an equation of state and an improved forcing scheme to obtain
liquid-vapor densities that are consistent with Maxwell's construction,
enabling simulations of systems with very large density ratios while
maintaining minimal spurious currents. Wetting is handled using the "improved"
virtual density scheme, which allows precise control of contact angles and
eliminates non-physical films seen in other Shan-Chen wetting methods. We
validate the library through several analytical benchmarks, such as Laplace's
law, capillary rise, and cocurrent multicomponent flow, and demonstrate some
exemplary use cases for the library. We also report single- and multi-GPU
performance scaling of the library. The library is open-source under the Apache
license and available at https://github.com/piyush-ppradhan/JAX-LaB.

</details>


### [57] [Stochastic dynamics simulation of the focused electron beam induced deposition process](https://arxiv.org/abs/2506.18163)
*Ilia A. Solov'yov,Alexey Prosvetov,Gennady Sushko,Andrey V. Solov'yov*

Main category: physics.comp-ph

TL;DR: A new stochastic dynamics (SD) approach for multiscale modeling of FEBID was developed using MBN Explorer and MBN Studio, validated with experimental data.


<details>
  <summary>Details</summary>
Motivation: To provide atomistic insights into the complex FEBID process, including deposit composition and morphology.

Method: Combines stochastic dynamics (SD) with molecular dynamics (MD) simulations for parameter determination, applied to W(CO)$_6$ on SiO$_2$-H.

Result: The approach offers detailed insights into FEBID growth stages and matches experimental observations.

Conclusion: The multiscale method is promising for applications like 3D nanoprinting and can be further upgraded.

Abstract: This work reports on the development of a new approach to the multiscale
computational modelling of the focused electron beam-induced deposition
(FEBID), realised using the advanced software packages: MBN Explorer and MBN
Studio. Our approach is based on stochastic dynamics (SD), which describes the
probabilistic evolution of complex systems. The parameters for the new SD-based
FEBID model were determined using molecular dynamics (MD) simulations. A new
methodology was developed for this purpose and is described in detail. This
methodology can be applied to many other case studies of the dynamics of
complex systems. Our work focuses on the FEBID process involving W(CO)$_6$
precursor molecules deposited on a hydroxylated SiO$_2$-H substrate.
Simulations and a detailed analysis of a growing W-rich nanostructure were
performed. This new approach was shown to provide essential atomistic insights
into the complex FEBID process, including the elemental composition and
morphology of the deposit at each stage of growth. The derived results were
then compared with experimental observations and validated. The multiscale
methods developed in this study can be further upgraded and applied to
important technological developments, such as 3D nanoprinting.

</details>


### [58] [Quantifying Gibbs measures of disordered crystals up to the solid-liquid phase transition](https://arxiv.org/abs/2506.18190)
*Vladislav Efremkin,Julian Heske,Thomas D. Kühne,Emil Prodan*

Main category: physics.comp-ph

TL;DR: The paper addresses the challenge of quantifying the configuration space and Gibbs measure of thermally disordered condensed matter systems by using Voronoi cells to reconstruct the lattice, even when scrambled.


<details>
  <summary>Details</summary>
Motivation: To solve the Gibbs paradox and quantify the disordered states of condensed matter systems (solid, liquid, gas) without labeling atoms.

Method: Analyzing Voronoi cells of atoms to reconstruct the lattice, focusing on large facets that persist despite thermal fluctuations.

Result: In crystalline silicon, four large Voronoi facets exist with probability one up to the melting temperature, enabling lattice reconstruction.

Conclusion: Persistent Voronoi facets are a fundamental signature of the crystalline phase, crucial for defining the Gibbs measure in solids.

Abstract: Quantifying the configuration space and the Gibbs measure of thermally
disordered condensed matter systems has been a long standing problem. The
challenge is to avoid the Gibbs paradox, which forbids any ordering or labeling
of the atoms. Our key observation is that the lattice of a thermally disordered
condensed matter system, in either solid, liquid or gas phase, can be fully
reconstructed from the Voronoi cells of the atoms alone, even if these Voronoi
cells are disassembled and randomly scrambled. In the example of the
crystalline phase of silicon, the statistics of the Voronoi cells reveals the
existence of four, and only four, large facets that are present with
probability one for all temperatures up to the solid-liquid melting line. These
four largest facets, which separate nearest-neighboring atoms, can be also be
used to reconstruct the lattice of the crystal. Hence, their collection
supplies the optimal representation of the configuration of the crystal. We
conjecture that the existence of Voronoi facets that, despite their large
thermal fluctuations, survive with probability one up to the melting
temperature, is the fundamental signature of the crystalline solid phase and
therefore key to quantifying the Gibbs measure over the entire solid phase.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [59] [Modeling helium compression and enrichment in DIII-D edge plasmas using the SOLPS-ITER code](https://arxiv.org/abs/2506.17468)
*Rebecca Masline,Dennis Whyte*

Main category: physics.plasm-ph

TL;DR: The paper explores helium ash removal in fusion reactors, using SOLPS-ITER to model helium behavior in DIII-D tokamak discharges, and evaluates the Tritium Burn Efficiency (TBE) metric for reactor design.


<details>
  <summary>Details</summary>
Motivation: Helium accumulation in fusion reactors can degrade performance, but understanding its transport is limited. This study revisits helium dynamics to aid in designing future fusion plants.

Method: SOLPS-ITER is used to model helium-seeded discharges in the DIII-D tokamak, analyzing transport, recycling, and enrichment in the divertor.

Result: The study characterizes helium dynamics and assesses the Tritium Burn Efficiency (TBE) metric's viability for reactor design.

Conclusion: The work provides insights into helium transport and TBE's practicality, contributing to fusion reactor planning.

Abstract: Efficient removal of helium ash is a critical requirement for the operation
of fusion power plants, as its accumulation can dilute the core fuel and
degrade plasma performance. While past studies suggested that helium exhaust in
burning plasmas could be managed effectively through divertor optimization and
conventional cryopumping, a detailed understanding of helium behavior in the
edge and divertor plasma remains limited, as helium transport through the edge
plasma is complex and fundamentally different from other impurity species. With
the emergence of more sophisticated numerical modeling tools and renewed focus
on D-T burning plasmas, revisiting helium transport in current magnetic
confinement devices is necessary for planning and designing fusion pilot
plants. This study uses SOLPS-ITER to model a helium-seeded discharge from the
DIII-D tokamak, analyzing the transport, recycling, and enrichment of helium in
the divertor. In addition to characterizing helium dynamics, the results are
interpreted in terms of the Tritium Burn Efficiency (TBE), a recently proposed
metric linking helium exhaust fraction to tritium fuel utilization in
steady-state burning plasmas. By assessing the compatibility of TBE assumptions
with detailed edge plasma simulations, this work provides insight into the
practical viability of TBE as a reactor design and performance metric.

</details>


### [60] [A class of high-beta, large-aspect-ratio quasiaxisymmetric Palumbo-like configurations](https://arxiv.org/abs/2506.17528)
*Andrew Brown,Wrick Sengupta,Nikita Nikulsin,Amitava Bhattacharjee*

Main category: physics.plasm-ph

TL;DR: Exploration of high-beta, quasiaxisymmetric stellarator configurations using inverse coordinate methods and polynomial ansatz for flux functions.


<details>
  <summary>Details</summary>
Motivation: To study and expand the understanding of stellarator equilibria with specific geometric properties like triangularity and cusps.

Method: Inverse coordinate approach with quadratic and cubic polynomial ansatz for flux functions, solving nonlinear ODEs.

Result: Equilibria with positive/negative triangularity, cusps, and current singularities; higher-degree polynomials lead to overdetermination.

Conclusion: Quadratic and cubic ansatz are viable for modeling stellarator equilibria, but higher-degree polynomials are impractical.

Abstract: The space of high-beta, approximately quasiaxisymmetric, large-aspect-ratio
stellarator configurations is explored using an inverse coordinate approach and
a quadratic polynomial ansatz for the flux function, following the method of
Palumbo, extended by Hernandes and Clemente. This approach yields a system of
nonlinear ODEs that, when solved, give equilibria exhibiting positive or
negative triangularity, cusps, and (in an extreme limit) current singularities.
It is shown that a cubic ansatz may also be used, but that polynomials of
degree four or higher will lead to overdetermination.

</details>


### [61] [Particle-in-cell simulations of plasma wakefield formation in microwave waveguides](https://arxiv.org/abs/2506.17752)
*Jesús E. López,Eduardo A. Orozco*

Main category: physics.plasm-ph

TL;DR: The paper explores microwave-driven plasma acceleration as an alternative to conventional and laser-driven methods, using simulations to analyze wakefield formation in plasma waveguides.


<details>
  <summary>Details</summary>
Motivation: Conventional accelerators are limited by size and complexity, while laser-driven plasma accelerators require high-intensity systems. Microwave-driven plasma acceleration offers a more accessible solution.

Method: Three-dimensional particle-in-cell simulations were used to study electrostatic wakefields in rectangular plasma waveguides excited by short microwave pulses.

Result: The simulations provided insights into wakefield formation and structure, establishing a theoretical basis for evaluating microwave-driven plasma acceleration.

Conclusion: Microwave-driven plasma acceleration is a feasible and promising alternative, with potential applications in particle physics and other fields.

Abstract: The acceleration of charged particles is fundamental not only for
experimental studies in particle physics but also for applications in fields
such as semiconductor manufacturing and medical therapies. However,
conventional accelerators face limitations due to their large size, driven by
low acceleration gradients. Plasma-based accelerators have emerged as a
promising alternative, offering ultrahigh acceleration gradients, though their
implementation is often limited by the need for high-intensity, femtosecond
laser systems and sophisticated diagnostics. As a more accessible alternative,
the use of microwave pulses to excite plasma wakefields in waveguides filled
with low-density plasma has gained attention. In this study, we perform
three-dimensional particle-in-cell simulations to investigate the formation and
structure of electrostatic wakefields driven by short microwave pulses in
rectangular plasma waveguides. The results establish a theoretical basis for
evaluating the feasibility and potential applications of microwave-driven
plasma acceleration schemes.

</details>


### [62] [Observation of laser plasma accelerated electrons with transverse momentum spread below the thermal level](https://arxiv.org/abs/2506.18047)
*T. L. Steyn,A. Panchal,O. Vasilovici,S. Schöbel,P. Ufer,F. M. Herrmann,Y. -Y. Chang,I. Moulanier,M. Masckala,O. Khomyshyn,C. Ballage,M. LaBerge,F. Massimo,S. Dobosz Dufrénoy,U. Schramm,A. Irman,B. Cros*

Main category: physics.plasm-ph

TL;DR: The paper demonstrates controlled electron beam acceleration and shaping in laser-plasma accelerators using plasma density tailoring, achieving high-quality beams with low momentum spread and high brightness.


<details>
  <summary>Details</summary>
Motivation: To improve electron beam quality in laser-plasma accelerators by controlling dynamics through plasma density tailoring.

Method: Experimental observation and numerical simulations of electron beam acceleration and 3D shaping in a gas cell with tailored plasma density.

Result: Relativistic electron beams with 0.2 mec transverse momentum spread, 40 pC charge, 190 MeV energy, 1.9% energy spread, 0.54 mrad divergence, and brightness up to 8 pC/MeV/mrad.

Conclusion: Plasma density tailoring effectively reduces transverse momentum spread and enhances beam quality, supported by simulations showing dechirping in the plasma tail.

Abstract: Achieving high-quality electron beams from laser-plasma accelerators
critically relies on density tailoring to control electron dynamics during
injection, acceleration, and extraction. We report on the experimental
observation of electron beam acceleration and 3D shaping, in transverse
momentum and longitudinal phase space, controlled by plasma density tailoring
in a gas cell. It leads to relativistic electron beams with a transverse
momentum spread of 0.2 mec, below the thermal level imprinted by the process of
ionization injection in a laser-driven wakefield accelerator. These beams have
a charge of 40 pC at an energy of 190 MeV with 1.9% energy spread and an rms
divergence of 0.54 mrad, resulting in single-peak spectra with a brightness of
up to 8 pC/MeV/mrad. Using numerical simulations, we show that the decrease in
transverse momentum spread starts in the downramp and continues in a 10 mm long
plasma tail at the exit of the gas cell, where it is accompanied by energy
dechirping.

</details>


### [63] [Spatial, Spectral and Temporal Response of High Intensity Laser Plasma Mirrors](https://arxiv.org/abs/2506.18425)
*Sk Rakeeb,Animesh Sharma,Sagar Dam,Ameya Parab,Amit Lad,Yash. M. Ved,Amita Das,G. Ravindra Kumar*

Main category: physics.plasm-ph

TL;DR: The paper discusses plasma mirrors (PMs) and their behavior under ultra-intense laser pulses, highlighting challenges in applications like VUV/X-ray generation and particle acceleration. It presents a method for direct, in situ measurement of PM surface evolution and confirms findings with simulations.


<details>
  <summary>Details</summary>
Motivation: Plasma mirrors are crucial for manipulating intense laser pulses but suffer from nonlinear and dynamic modulations, complicating their use in applications. Understanding these effects is essential for improving their performance.

Method: The study uses simultaneous analysis of reflected light's wavefront, spectrum, and temporal profile to measure 3D plasma surface evolution during femtosecond laser irradiation, supported by 3D-PIC simulations.

Result: Measurements show surface deformations of a few hundred nanometers at relativistic intensities, matching simulations. PMs also alter pulse spectrum and temporal profile, introducing spatio-temporal couplings.

Conclusion: The findings provide insights into PM behavior under extreme conditions, aiding future optimization for applications like VUV/X-ray generation and particle acceleration.

Abstract: Plasma-based optics have emerged as a powerful platform for manipulating and
amplifying ultra-intense laser pulses. However, the inherently nonlinear and
dynamic nature of plasma leads to significant spatial, spectral, and temporal
modulations when driven at relativistic intensities. These modifications can
dramatically alter the structure of the reflected laser pulses, posing
challenges for their use in applications such as vacuum ultraviolet (VUV) and
X-ray generation, as well as relativistic particle acceleration. Comprehensive,
multidimensional diagnostics are essential to accurately characterize these
so-called `plasma mirrors' (PMs). We present a direct, \textit{in situ}
measurement of the three-dimensional plasma surface evolution during
femtosecond laser irradiation, achieved through simultaneous analysis of the
wavefront, spectrum, and temporal profile of the reflected light. Our
measurements reveal surface deformations on the order of a few hundred
nanometers at relativistic intensities, in agreement with three-dimensional
particle-in-cell (3D-PIC) simulations. Additionally, the PM induces substantial
modifications to the pulse spectrum and temporal profile, introducing
spatio-temporal couplings.

</details>


### [64] [Polarization ratios of turbulent Langmuir/$\mathcal{Z}$-mode waves generated by electron beams in magnetized solar wind plasmas](https://arxiv.org/abs/2506.18429)
*Francisco Javier Polanco-Rodríguez,Catherine Krafft,Philippe Savoini*

Main category: physics.plasm-ph

TL;DR: The study explores polarization ratios of turbulent Langmuir/Z-mode waves and emissions in magnetized plasmas, linking density fluctuations to increased polarization, with implications for solar wind analysis.


<details>
  <summary>Details</summary>
Motivation: To understand how beam-generated turbulent waves and emissions in plasmas, relevant to type III solar radio bursts, are influenced by density fluctuations and other parameters.

Method: Large-scale 2D/3V Particle-In-Cell simulations and statistical analysis of waveforms from virtual satellites, mimicking spacecraft data.

Result: Density fluctuations significantly increase polarization ratios, with linear mode conversion identified as the key process. Findings align with solar wind observations.

Conclusion: The study provides insights into wave behavior in plasmas and aids in estimating solar wind density fluctuations, supporting theoretical and observational research.

Abstract: The polarization ratios $F=|E_\perp|^2/|E|^2$ of beam-generated turbulent
Langmuir/$\mathcal{Z}$-mode ($\mathcal{LZ}$) waves and electromagnetic
emissions radiated at plasma frequency $\omega_p$ by such sources are studied
in weakly magnetized and randomly inhomogeneous plasmas owing to large-scale
and long-term 2D/3V Particle-In-Cell simulations with parameters relevant to
type III solar radio bursts. Statistical studies using waveforms recorded by
virtual satellites are performed to determine the distributions of polarization
ratios as a function of beam and plasma parameters. This efficient method,
which mimics waveform recording by spacecraft in the solar wind, leads to
results consistent with observations. Moreover, plasma random density
fluctuations $\delta n$ turn out to be the key factor responsible for the
increase in polarization ratios up to $F\simeq 1$. Indeed, it is demonstrated
that linear mode conversion at constant frequency of $\mathcal{LZ}$ waves
scattering on $\delta n$ is the most efficient and fast process to produce
large polarization ratios in randomly inhomogeneous plasmas, due to
electromagnetic slow extraordinary $\mathcal Z$-mode wave emission by
$\mathcal{LZ}$ wave turbulence. Results provide guidance to theoretical studies
and useful support to estimate the average level of density fluctuations
$\Delta N$ in solar wind plasmas.

</details>


### [65] [Simulating ultrarelativistic beam-plasma instabilities with a quasistatic particle-in-cell code](https://arxiv.org/abs/2506.18567)
*Q. Labro,X. Davoine,L. Gremillet,L. Bergé*

Main category: physics.plasm-ph

TL;DR: Quasistatic PIC codes like QuaSSis enable efficient study of relativistic beam-plasma instabilities, such as OTSI, by decoupling slow driver dynamics from fast plasma response. A new numerical scheme allows transversely periodic boundary conditions, and noise control is improved via macroparticle initialization.


<details>
  <summary>Details</summary>
Motivation: To extend the use of quasistatic PIC codes beyond laser/plasma wakefield accelerators to study relativistic beam-plasma instabilities, particularly OTSI, with improved computational efficiency.

Method: Developed a 2D quasistatic PIC code, QuaSSis, with a new numerical scheme for transversely periodic boundary conditions. Benchmarked against CALDER code and analytical OTSI model. Controlled numerical noise via macroparticle initialization.

Result: QuaSSis accurately models OTSI, demonstrating exponential growth from initial fluctuations. Noise control via macroparticle initialization is effective and computationally efficient.

Conclusion: Quasistatic PIC codes, enhanced by QuaSSis's capabilities, are viable for studying relativistic beam-plasma instabilities, offering significant computational savings and precise noise control.

Abstract: Quasistatic particle-in-cell (PIC) codes are increasingly employed to study
laser or plasma wakefield accelerators. By decoupling the slow dynamics of the
driver (a laser or ultrarelativistic particle beam) from the fast plasma
response, these codes can reduce the computational time by several orders of
magnitude compared to conventional PIC codes. In this work, we demonstrate that
quasistatic PIC codes can also be utilized to investigate relativistic
beam-plasma instabilities, with a focus on the oblique two-stream instability
(OTSI). For this purpose, we have developed a 2D quasistatic PIC code, QuaSSis,
based on a new numerical scheme that can handle transversely periodic boundary
conditions, a capability absent in previous quasistatic codes. The accuracy of
QuaSSis is benchmarked first against standard PIC simulations performed with
the CALDER code, and then against an analytical spatiotemporal model of the
OTSI. Physically, this instability grows exponentially from initial
fluctuations in the particle charge or current densities. Since the numerical
noise inherent to PIC simulations can mimic these fluctuations to some extent,
its control is crucial to seed the beam-plasma instability at the desired
amplitude. Common methods for tuning this noise involve modifying the
resolution or adding filters, but these can be computationally costly when
aiming at very low noise levels. Here, we show that this noise can be finely
controlled by properly initializing the positions and weights of the
macroparticles.

</details>


### [66] [External charged debris in a flowing plasma : charge fluctuation induced complexity](https://arxiv.org/abs/2506.18687)
*Bikramjit Joardar,Hitendra Sarkar,Madhurjya P. Bora*

Main category: physics.plasm-ph

TL;DR: The paper studies how a flowing electron-ion plasma reacts to embedded charged debris, focusing on charge fluctuations causing chaos and nonlinear Landau damping. Simulations and theory reveal differences in response based on debris charge and highlight damping effects near ion-acoustic speed.


<details>
  <summary>Details</summary>
Motivation: To understand the complex interactions between flowing plasma and charged debris, particularly how charge fluctuations trigger phenomena like chaos and nonlinear Landau damping, with potential applications in space debris management.

Method: Combined kinetic and fluid simulations to analyze plasma response to time-dependent debris charge, supported by a theoretical framework.

Result: Nonlinear response differs for positively vs. negatively charged debris. Charge fluctuations cause ion-acoustic wave damping near ion-acoustic speed via nonlinear Landau damping and wave-wave interactions.

Conclusion: The study provides key insights into debris-plasma interactions, useful for applications like space debris management.

Abstract: In this work, we investigate the response of a flowing e-i plasma to embedded
external charged debris, focusing on the periodic debris charge fluctuations
that can trigger complex phenomena such as chaos and nonlinear Landau damping.
We employ both kinetic and fluid simulations to analyse the plasma response to
the time-dependent debris charge. Our findings indicate that the nature of the
nonlinear response can be considerably different for fluctuating positively
charged external debris from negatively charged debris. The simulations show
that the debris charge fluctuation causes damping of the ion-acoustic wave as
the debris velocity nears the ion-acoustic speed through nonlinear Landau
damping and wave-wave interactions. We also present a theoretical framework to
support the simulation findings. Our findings provide critical insights into
debris-plasma interactions, which may be useful in applications involving space
debris management.

</details>


### [67] [Flux-driven turbulent transport using penalisation in the Hasegawa-Wakatani system](https://arxiv.org/abs/2506.18705)
*Pierre L. Guillon,Özgür D. Gürcan,Guilhem Dif-Pradalier,Yanick Sarazin,Nicolas Fedorczak*

Main category: physics.plasm-ph

TL;DR: P-FLARE, a pseudo-spectral code, simulates flux-driven turbulence/transport, showing profile relaxation and sandpile-like behavior in the modified Hasegawa-Wakatani system.


<details>
  <summary>Details</summary>
Motivation: To present initial numerical results from P-FLARE, a flexible code for studying flux-driven turbulence and transport in reduced fluid models.

Method: Uses pseudo-spectral formulation with penalisation for radial boundary conditions, applied to the modified Hasegawa-Wakatani system.

Result: Observed coupled spreading/profile relaxation, sandpile-like critical behavior, and profile stiffness due to varying zonal flow levels.

Conclusion: P-FLARE effectively models complex transport behaviors, demonstrating hysteresis and stiffness in the system.

Abstract: First numerical results from the newly-developed pseudo-spectral code P-FLARE
(Penalised FLux-driven Algorithm for REduced models) are presented. This
flux-driven turbulence/transport code uses a pseudo-spectral formulation with
the penalisation method in order to impose radial boundary conditions. Its
concise, flexible structure allows implementing various two dimensional reduced
fluid models in flux-driven formulation. Here, results from simulations of the
modified Hasegawa-Wakatani system are discussed, where particle transport and
zonal flow formation, together with profile relaxation, are studied. It is
shown that coupled spreading/profile relaxation that one obtains for this
system is consistent with a simple one dimensional model of coupled
spreading/transport equations. Then, the effect of a source of density is
investigated, which results in the observation of sandpile-like critical
behaviour. The model displays profile stiffness for certain parameters, with
very different input fluxes resulting in very similar mean density gradients.
This is due to different zonal flow levels around the critical value for the
control parameter (i.e. the ratio of the adiabaticity parameter to the mean
gradient) and the existence for this system of a hysteresis loop for the
transition from 2D turbulence to a zonal flow dominated state.

</details>


### [68] [Structure and dynamics of finite three-dimensional Yukawa clusters in complex plasmas : Newtonian versus Langevin Dynamics](https://arxiv.org/abs/2506.18820)
*Hirakjyoti Sarma,Nilakshi Das*

Main category: physics.plasm-ph

TL;DR: The paper investigates the structure and dynamics of a 3D dust cluster using Langevin Dynamics (LD) and frictionless Molecular Dynamics (fMD). Key findings include differences in inter-shell angular correlations and particle mobility between the two methods.


<details>
  <summary>Details</summary>
Motivation: To understand how different simulation methods (LD and fMD) affect the structural and dynamic properties of confined dust clusters.

Method: Used LD and fMD simulations to analyze static structure (Radial Distribution Function, C2P, angular correlation) and dynamics (Van-Hove function, MSD, trajectory analysis).

Result: Inter-shell angular correlations show sharp peaks in fMD but not in LD. fMD exhibits narrower Van-Hove function peaks, indicating less particle mobility. Rotational motion in fMD disappears with friction.

Conclusion: Friction in LD leads to faster relaxation of interparticle distances and angular separations, eliminating rotational motion seen in fMD.

Abstract: The structure and dynamics of a harmonically confined three dimensional
finite dust cluster are investigated via both Langevin Dynamics (LD) and
frictionless Molecular Dynamics (fMD) simulation. The static structure of the
system is analyzed through the Radial Distribution Function,
Center-two-particle correlation function(C2P) and angular correlation function.
The intra-shell angular correlation, Radial Distribution Function and C2P
remains largely unaffected by the dynamics employed. However, the inter-shell
angular correlation exhibits sharp peaks at irregular angular intervals in fMD
which are not seen in LD indicating strongly correlated motion of the two
spherical shells in the cluster in fMD. The single particle dynamics of the
cluster is characterized by the Van - Hove self autocorrelation function and
Mean Squared Displacement (MSD). Notably, the Van - Hove autocorrelation
function in fMD simulations exhibits narrower width and higher peak height as
compared to the LD simulations, suggesting greater particle mobility in LD.
Trajectory analysis reveals a rotational motion of the particles about a common
axis in fMD which disappears with progressively increasing friction
coefficient. We show that the disappearance of rotational motion with the
introduction of neutral friction in the dynamics is due to the much faster
relaxation of the interparticle distance as well as interparticle angular
separation in LD as compared to fMD.

</details>


### [69] [Wave Topology in Hall MHD](https://arxiv.org/abs/2506.18830)
*Alejandro Mesa Dame,Hong Qin,Eric Palmerduca,Yichen Fu*

Main category: physics.plasm-ph

TL;DR: HMHD extends MHD with the Hall effect, and this paper derives its complete wave spectrum, showing it matches ideal MHD's three branches but with added topological features like a Weyl point.


<details>
  <summary>Details</summary>
Motivation: A comprehensive description of HMHD eigenmodes was missing, despite its importance for plasma behavior at small scales.

Method: Derived the complete spectrum and eigenvectors of HMHD waves, analyzing their topological structure.

Result: HMHD's spectrum matches ideal MHD's three branches (slow, shear Alfvén, fast magnetosonic waves) but includes nontrivial topology (Weyl point, nonzero Chern numbers).

Conclusion: HMHD's wave spectrum is homotopic to ideal MHD's, with no additional branches, but features unique topological properties.

Abstract: Hall Magnetohydrodynamics (HMHD) extends ideal MHD by incorporating the Hall
effect via the induction equation, making it more accurate for describing
plasma behavior at length scales below the ion skin depth. Despite its
importance, a comprehensive description of the eigenmodes in HMHD has been
lacking. In this work, we derive the complete spectrum and eigenvectors of HMHD
waves and identify their underlying topological structure. We prove that the
HMHD wave spectrum is homotopic to that of ideal MHD, consisting of three
distinct branches: the slow magnetosonic-Hall waves, the shear Alfv\'en-Hall
waves, and the fast magnetosonic-Hall waves, which continuously reduce to their
ideal MHD counterparts in the limit of vanishing Hall parameter. Contrary to a
recent claim, we find that HMHD does not admit any additional wave branches
beyond those in ideal MHD. The key qualitative difference lies in the
topological nature of the HMHD wave structure: it exhibits nontrivial topology
characterized by a Weyl point-an isolated eigenmode degeneracy point-and
associated nonzero Chern numbers of the eigenmode bundles over a 2-sphere in
k-space surrounding the Weyl point.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [70] [Classical optimization algorithms for diagonalizing quantum Hamiltonians](https://arxiv.org/abs/2506.17883)
*Taehee Ko,Sangkook Choi,Hyowon Park,Xiantao Li*

Main category: quant-ph

TL;DR: The paper introduces classical optimization algorithms for Hamiltonian diagonalization, addressing limitations of existing methods and expanding the class of fast-forwardable Hamiltonians.


<details>
  <summary>Details</summary>
Motivation: Diagonalizing Hamiltonians is crucial for quantum simulations, but existing methods are limited in efficiency and applicability. The paper aims to overcome these limitations.

Method: The authors formulate a cost function penalizing off-diagonal terms and enforce unitarity via orthogonality constraints in the Pauli operator basis. They also introduce a randomized-coordinate variant for efficiency.

Result: The proposed algorithms achieve polynomial-time efficiency, avoid suboptimal points, and extend the class of fast-forwardable Hamiltonians to those generated by exponentially large Lie algebras.

Conclusion: The work broadens the scope of quantum-diagonalizable Hamiltonians and demonstrates practical effectiveness through examples and numerical experiments.

Abstract: Diagonalizing a Hamiltonian, which is essential for simulating its long-time
dynamics, is a key primitive in quantum computing and has been proven to yield
a quantum advantage for several specific families of Hamiltonians. Yet, despite
its importance, only a handful of diagonalization algorithms exist, and
correspondingly few families of fast-forwardable Hamiltonians have been
identified. This paper introduces classical optimization algorithms for
Hamiltonian diagonalization by formulating a cost function that penalizes
off-diagonal terms and enforces unitarity via an orthogonality constraint, both
expressed in the Pauli operator basis. We pinpoint a class of Hamiltonians that
highlights severe drawbacks of existing methods, including exponential
per-iteration cost, exponential circuit depth, or convergence to spurious
optima. Our approach overcomes these shortcomings, achieving polynomial-time
efficiency while provably avoiding suboptimal points. As a result, we broaden
the known realm of fast-forwardable systems, showing that
quantum-diagonalizable Hamiltonians extend to cases generated by exponentially
large Lie algebras. On the practical side, we also present a
randomized-coordinate variant that achieves a more efficient per-iteration cost
than the deterministic counterpart. We demonstrate the effectiveness of these
algorithms through explicit examples and numerical experiments.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [71] [Gaussian Processes and Reproducing Kernels: Connections and Equivalences](https://arxiv.org/abs/2506.17366)
*Motonobu Kanagawa,Philipp Hennig,Dino Sejdinovic,Bharath K. Sriperumbudur*

Main category: stat.ML

TL;DR: The monograph explores connections between Gaussian processes (probabilistic) and RKHS (non-probabilistic) methods, unifying their equivalences and bridging research communities.


<details>
  <summary>Details</summary>
Motivation: To review and establish connections between Gaussian processes and RKHS, widely used in machine learning, statistics, and numerical analysis.

Method: Analyzes fundamental topics like regression, interpolation, and numerical integration, and establishes a unifying perspective via Gaussian Hilbert space and RKHS equivalence.

Result: Demonstrates equivalences between the two approaches and provides a basis for bridging parallel developments in both research communities.

Conclusion: The monograph unifies Gaussian processes and RKHS methods, offering a foundation for further interdisciplinary research.

Abstract: This monograph studies the relations between two approaches using positive
definite kernels: probabilistic methods using Gaussian processes, and
non-probabilistic methods using reproducing kernel Hilbert spaces (RKHS). They
are widely studied and used in machine learning, statistics, and numerical
analysis. Connections and equivalences between them are reviewed for
fundamental topics such as regression, interpolation, numerical integration,
distributional discrepancies, and statistical dependence, as well as for sample
path properties of Gaussian processes. A unifying perspective for these
equivalences is established, based on the equivalence between the Gaussian
Hilbert space and the RKHS. The monograph serves as a basis to bridge many
other methods based on Gaussian processes and reproducing kernels, which are
developed in parallel by the two research communities.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [72] [Two Types of $1/f$ Range in Solar Wind Turbulence](https://arxiv.org/abs/2506.17523)
*Zesen Huang,Marco Velli,B. D. G. Chandran,Chen Shi,Yuliang Ding,Lorenzo Matteini,Kyung-Eun Choi*

Main category: astro-ph.SR

TL;DR: The paper investigates the origin of $1/f$ noise in solar wind turbulence, identifying two distinct types and analyzing their properties and behaviors under different solar wind conditions.


<details>
  <summary>Details</summary>
Motivation: The study aims to resolve the long-standing debate about the origin of $1/f$ noise in solar wind, leveraging recent Parker Solar Probe observations and systematic analysis of solar wind conditions.

Method: The researchers classify $1/f$ noise into fast/Alfvénic and slow/mixed types, analyze their properties using frequency-averaged fluctuation amplitudes, and examine solar cycle dependence using the OMNI-LRO dataset.

Result: The fast/Alfvénic type is intrinsic to Alfvénic turbulence, while the slow/mixed type resembles flicker noise. The study also reveals a relationship between the $1/f$ range and magnetic field correlation decline, along with unexpected resonance peaks.

Conclusion: The findings provide new insights into the origin and behavior of $1/f$ noise in solar wind turbulence, distinguishing between intrinsic and classical noise types and highlighting their solar cycle dependence.

Abstract: The $1/f$ noise is a ubiquitous phenomenon in natural systems. Since the
advent of space exploration, the $1/f$ range has been consistently observed in
\textit{in situ} solar wind measurements throughout the heliosphere, sparking
decades of debate regarding its origin. Recent Parker Solar Probe (PSP)
observations near the Alfv\'en surface have revealed a systematic absence of
the $1/f$ range in pristine solar wind, providing a unique opportunity to
investigate its origin in solar wind turbulence. Despite numerous observations
of the $1/f$ range at varying frequencies, no study has systematically examined
its properties across different solar wind conditions. Here, we identify two
distinct types of $1/f$ ranges in solar wind turbulence: the fast/Alfv\'enic
wind type and the slow/mixed wind type. The fast/Alfv\'enic type appears to be
an intrinsic feature of Alfv\'enic turbulence, while the slow/mixed type
resembles classical flicker noise. For the fast/Alfv\'enic type, we find a
near-perfect WKB evolution of the frequency-averaged fluctuation amplitude and
an intriguing migration pattern in frequency space. For the slow/mixed type, we
examine the solar cycle dependence of the $1/f$ noise using the OMNI-LRO
dataset spanning solar cycles 22 to 25. We also analyze the autocorrelation
function of the magnetic field vectors and identify a clear relationship
between the $1/f$ range and the decline in correlation, as well as unexpected
resonance peaks in the autocorrelation function.

</details>


### [73] [The effect of plasma-$β$ on the heating mechanisms during magnetic reconnection in partially ionized low solar atmosphere](https://arxiv.org/abs/2506.17579)
*Abdullah Zafar,Lei Ni,Kaifeng Kang,Guanchong Cheng,Jing Ye,Jun Lin,Ahmad Ali,Nadia Imtiaz*

Main category: astro-ph.SR

TL;DR: The study explores magnetic reconnection in the solar atmosphere, identifying compression heating as dominant in low plasma-β conditions, while partial ionization effects gain importance in weaker magnetic fields.


<details>
  <summary>Details</summary>
Motivation: To understand the dominant heating mechanisms in plasmas during magnetic reconnection under varying plasma-β conditions in the solar atmosphere.

Method: Numerical simulations of magnetic reconnection across different magnetic field strengths from the photosphere to the upper chromosphere.

Result: Lower plasma-β reconnection generates more plasmoids, enhancing turbulence and compression heating. Joule heating dominates at higher plasma-β, while ambipolar diffusion and viscous heating become significant in the upper chromosphere.

Conclusion: Compression heating is key in strong magnetic fields (e.g., active regions), while partial ionization effects are critical in weaker fields (e.g., quiet Sun events).

Abstract: We performed numerical simulations of magnetic reconnection with different
strength of magnetic fields from the solar photosphere to the upper
chromosphere. The main emphasis is to identify dominant mechanisms for heating
plasmas in the reconnection region under different plasma-$\beta$ conditions in
the partially ionized low solar atmosphere. The numerical results show that
more plasmoids are generated in a lower $\beta$ reconnection event. The
frequent coalescence of these plasmoids leads to a significant enhancement of
turbulence and compression heating, which becomes the dominant mechanism for
heating plasma in a lower plasma-$\beta$ reconnection process. The average
power density of the compression heating (Q$_{comp}$) decreases with increasing
initial plasma-$\beta$ as a power function: Q$_{comp} \sim \beta_{0}^{-a}$,
where the value $a$ is $1.9$ in the photosphere and decreases to about 1.29 in
the upper chromosphere. In the photosphere and lower chromosphere, the joule
heating contributed by electron-neutral collisions Q$_{en}=\eta_{en} J^2$
eventually dominates over the compression heating when the initial
plasma-$\beta$ is larger than the critical value $\beta_{0-critical} = 8$. In
the upper chromosphere, the ambipolar diffusion heating and the viscous heating
will become equally important as the compression heating when the initial
plasma-$\beta$ is larger than the critical value $\beta_{0-critical} = 0.5$.
These results indicate that the compression heating caused by turbulent
reconnection mediated with plasmoids is likely the major heating mechanism for
the small-scale reconnection events with stronger magnetic fields such as
active region EBs and UV bursts. However, the heating caused by the partial
ionization effects can not be ignored for those reconnection events with weaker
magnetic fields such as quiet Sun EBs and cold surges.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [74] [On the Relation of Exact Hydrodynamics to the Chapman--Enskog Series](https://arxiv.org/abs/2506.17441)
*Florian Kogelbauer,Ilya Karlin*

Main category: math-ph

TL;DR: The Chapman-Enskog series is locally equivalent to the exact spectral closure for slow kinetic eigenmodes at vanishing Knudsen number but diverges everywhere except at global equilibrium, while the exact spectral closure works globally for any Knudsen number.


<details>
  <summary>Details</summary>
Motivation: To compare the Chapman-Enskog series and exact spectral closure in hydrodynamics, highlighting their limitations and global applicability.

Method: Analyzing the equivalence and divergence of the Chapman-Enskog series versus the exact spectral closure for slow kinetic eigenmodes.

Result: Chapman-Enskog series diverges except at global equilibrium; exact spectral closure is valid globally for any Knudsen number.

Conclusion: The exact spectral closure is more robust and globally applicable than the Chapman-Enskog series, which has limited validity.

Abstract: We demonstrate that the Chapman--Enskog series is locally equivalent to the
exact spectral closure defined on slow kinetic eigenmodes in the limit of
vanishing Knudsen number. We further show that the Chapman--Enskog series
diverges everywhere expect at the global equilibrium for an explicit example,
while the exact spectrally closed hydrodynamics are defined globally for any
Knudsen number.

</details>


### [75] [Spectrum of the Laplacian in waveguide shaped surfaces](https://arxiv.org/abs/2506.18875)
*Diana C. S. Bello*

Main category: math-ph

TL;DR: The paper analyzes the essential spectrum of the Laplace operator on waveguide-shaped surfaces in 3D space, focusing on conditions for discrete eigenvalues and broken sheared waveguides.


<details>
  <summary>Details</summary>
Motivation: To understand the spectral properties of the Laplace operator on complex waveguide surfaces, which is crucial for applications in wave propagation and quantum mechanics.

Method: The study uses mathematical analysis of the Laplace operator on surfaces shaped like waveguides, considering the tangent vector's behavior at infinity and broken sheared configurations.

Result: The essential spectrum of the Laplace operator is determined, and conditions for the emergence of discrete eigenvalues are identified. The analysis is extended to broken sheared waveguides.

Conclusion: The findings provide insights into the spectral behavior of Laplace operators on waveguide surfaces, with implications for theoretical and applied physics.

Abstract: Let $-\Delta_{\cal S}$ be the Laplace operator in ${\cal S} \subset
\mathbb{R}^3$ on a waveguide shaped surfaces, i.e., ${\cal S}$ is built by
translating a closed curve in a constant direction along an unbounded spatial
curve. Under the condition that the tangent vector of the reference curve
admits a finite limit at infinity, we find the essential spectrum of
$-\Delta_{\cal S}$ and discuss conditions under which discrete eigenvalues
emerge. Furthermore, we analyze the Laplacian in the case of a broken sheared
waveguide shaped surface.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [76] [Convergent Proximal Multiblock ADMM for Nonconvex Dynamics-Constrained Optimization](https://arxiv.org/abs/2506.17405)
*Bowen Li,Ya-xiang Yuan*

Main category: math.OC

TL;DR: A provably convergent multiblock ADMM for nonconvex optimization with nonlinear dynamics constraints, addressing divergence in classical methods.


<details>
  <summary>Details</summary>
Motivation: To solve optimization problems arising from dynamics-constrained variational problems, particularly those with nonlinear ODEs/PDEs, where classical methods may diverge.

Method: Proximal ADMM applied to n-sum nonconvex problems with nonlinear constraints, leveraging problem structure and local conditions for convergence.

Result: Proves bounded sequences with KKT accumulation points, designs parameter selection, and shows fast convergence rates. Numerical tests confirm stability.

Conclusion: The proximal ADMM outperforms gradient-based methods in stability and convergence, with practical applications in data assimilation and stiff problems.

Abstract: This paper proposes a provably convergent multiblock ADMM for nonconvex
optimization with nonlinear dynamics constraints, overcoming the divergence
issue in classical extensions. We consider a class of optimization problems
that arise from discretization of dynamics-constrained variational problems
that are optimization problems for a functional constrained by time-dependent
ODEs or PDEs. This is a family of $n$-sum nonconvex optimization problems with
nonlinear constraints. We study the convergence properties of the proximal
alternating direction method of multipliers (proximal ADMM) applied to those
problems. Taking the advantage of the special problem structure, we show that
under local Lipschitz and local $L$-smooth conditions, the sequence generated
by the proximal ADMM is bounded and all accumulation points are KKT points.
Based on our analysis, we also design a procedure to determine the penalty
parameters $\rho_i$ and the proximal parameters $\eta_i$. We further prove that
among all the subsequences that converge, the fast one converges at the rate of
$o(1/k)$. The numerical experiments are performed on 4D variational data
assimilation problems and as the solver of implicit schemes for stiff problems.
The proposed proximal ADMM has more stable performance than gradient-based
methods. We discuss the implementation to solve the subproblems, a new way to
solve the implicit schemes, and the advantages of the proposed algorithm.

</details>


### [77] [Regular Tree Search for Simulation Optimization](https://arxiv.org/abs/2506.17696)
*Du-Yi Wang,Guo Liang,Guangwu Liu,Kun Zhang*

Main category: math.OC

TL;DR: Proposes Regular Tree Search, a random search algorithm combining adaptive sampling and recursive partitioning, to tackle non-convex simulation optimization problems. Uses UCT for tree search and proves global convergence under sub-Gaussian noise.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of solving non-convex simulation optimization problems efficiently.

Method: Integrates adaptive sampling with recursive partitioning using a tree structure. Employs UCT for tree search and refines sampling based on node depth.

Result: Proves global convergence under sub-Gaussian noise and demonstrates reliable identification of the global optimum in numerical experiments.

Conclusion: Regular Tree Search is effective for non-convex problems, providing accurate global optima estimates.

Abstract: Tackling simulation optimization problems with non-convex objective functions
remains a fundamental challenge in operations research. In this paper, we
propose a class of random search algorithms, called Regular Tree Search, which
integrates adaptive sampling with recursive partitioning of the search space.
The algorithm concentrates simulations on increasingly promising regions by
iteratively refining a tree structure. A tree search strategy guides sampling
decisions, while partitioning is triggered when the number of samples in a leaf
node exceeds a threshold that depends on its depth. Furthermore, a specific
tree search strategy, Upper Confidence Bounds applied to Trees (UCT), is
employed in the Regular Tree Search. We prove global convergence under
sub-Gaussian noise, based on assumptions involving the optimality gap, without
requiring continuity of the objective function. Numerical experiments confirm
that the algorithm reliably identifies the global optimum and provides accurate
estimates of its objective value.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [78] [Bayesian decomposition using Besov priors](https://arxiv.org/abs/2506.18846)
*Andreas Horst,Babak Maboudi Afkham,Yiqiu Dong,Jakob Lemvig*

Main category: stat.CO

TL;DR: The paper addresses Bayesian inverse problems with unknowns composed of smooth and piecewise constant components, proposing two prior models and Gibbs sampling for balanced reconstruction.


<details>
  <summary>Details</summary>
Motivation: Inverse problems often involve unknowns with mixed regularities (smooth and rough features), requiring tailored priors for accurate reconstruction.

Method: Two prior models are proposed: (i) Haar wavelet-based Besov prior + smoothing Besov prior, and (ii) hierarchical Gaussian prior on gradient + smoothing Besov prior. Hyperpriors are used for balanced inference via Gibbs sampling.

Result: Numerical tests on 1D/2D deconvolution show improved reconstruction quality over single-prior methods, with successful hyperparameter estimation.

Conclusion: The proposed methods effectively handle mixed-regularity unknowns, offering better reconstructions and balanced decompositions.

Abstract: In many inverse problems, the unknown is composed of multiple components with
different regularities, for example, in imaging problems, where the unknown can
have both rough and smooth features. We investigate linear Bayesian inverse
problems, where the unknown consists of two components: one smooth and one
piecewise constant. We model the unknown as a sum of two components and assign
individual priors on each component to impose the assumed behavior. We propose
and compare two prior models: (i) a combination of a Haar wavelet-based Besov
prior and a smoothing Besov prior, and (ii) a hierarchical Gaussian prior on
the gradient coupled with a smoothing Besov prior. To achieve a balanced
reconstruction, we place hyperpriors on the prior parameters and jointly infer
both the components and the hyperparameters. We propose Gibbs sampling schemes
for posterior inference in both prior models. We demonstrate the capabilities
of our approach on 1D and 2D deconvolution problems, where the unknown consists
of smooth parts with jumps. The numerical results indicate that our methods
improve the reconstruction quality compared to single-prior approaches and that
the prior parameters can be successfully estimated to yield a balanced
decomposition.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [79] [Conservative data-driven finite element formulation](https://arxiv.org/abs/2506.18206)
*Adriana Kuliková,Andrei G. Shvarts,Łukasz Kaczmarczyk,Chris J. Pearce*

Main category: cs.CE

TL;DR: A data-driven finite element framework using mixed formulation avoids material model bias, directly incorporating experimental data and enabling adaptive refinement and uncertainty quantification.


<details>
  <summary>Details</summary>
Motivation: To exploit experimental data fully and avoid bias from simplified material models in diffusion problems.

Method: Mixed finite element formulation satisfies conservation laws a priori, uses data directly, and relaxes regularity requirements for adaptive hp-refinement.

Result: Demonstrated in nonlinear heat transfer in nuclear graphite, showing non-uniqueness of solutions and uncertainty quantification.

Conclusion: The framework efficiently integrates data, avoids model fitting, and provides adaptive refinement and uncertainty analysis.

Abstract: This paper presents a new data-driven finite element framework derived with
mixed finite element formulation. The standard approach to diffusion problems
requires the solution of the mathematical equations that describe both the
conservation law and the constitutive relations, where the latter is
traditionally obtained after fitting experimental data to simplified material
models. To exploit all available information and avoid bias in the material
model, we follow a data-driven approach. While the conservation laws and
boundary conditions are satisfied by means of the finite element method, the
experimental data is used directly in the numerical simulations, avoiding the
need of fitting material model parameters. In order to satisfy the conservation
law a priori in the strong sense, we introduce a mixed finite element
formulation. This relaxes the regularity requirements on approximation spaces
while enforcing continuity of the normal flux component across all of the inner
boundaries. This weaker mixed formulation provides a posteriori error
indicators tailored for this data-driven approach, enabling adaptive
hp-refinement. The relaxed regularity of the approximation spaces makes it
easier to observe how the variation in the datasets results in the
non-uniqueness of the solution, which can be quantified to predict the
uncertainty of the results. The capabilities of the formulation are
demonstrated in an example of the nonlinear heat transfer in nuclear graphite
using synthetically generated material datasets.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [80] [Structural Optimal Jacobian Accumulation and Minimum Edge Count are NP-Complete Under Vertex Elimination](https://arxiv.org/abs/2506.17521)
*Matthias Bentert,Alex Crane,Pål Grønås Drange,Yosuke Mizutani,Blair D. Sullivan*

Main category: cs.DS

TL;DR: The paper resolves NP-completeness of two graph-theoretic problems in algorithmic differentiation and provides exact algorithms and data reduction rules.


<details>
  <summary>Details</summary>
Motivation: To address longstanding open questions about the computational complexity of Structural Optimal Jacobian Accumulation and Minimum Edge Count problems in algorithmic differentiation.

Method: The study uses vertex elimination operations and provides reductions to prove NP-completeness. It also offers exact algorithms with tight running times under the exponential time hypothesis.

Result: Both problems are proven NP-complete. Exact algorithms with $O^*(2^n)$-time are provided, and a data reduction rule for Structural Optimal Jacobian Accumulation is introduced.

Conclusion: The work resolves key complexity questions and provides practical algorithmic solutions for the studied problems.

Abstract: We study graph-theoretic formulations of two fundamental problems in
algorithmic differentiation. The first (Structural Optimal Jacobian
Accumulation) is that of computing a Jacobian while minimizing multiplications.
The second (Minimum Edge Count) is to find a minimum-size computational graph.
For both problems, we consider the vertex elimination operation. Our main
contribution is to show that both problems are NP-complete, thus resolving
longstanding open questions. In contrast to prior work, our reduction for
Structural Optimal Jacobian Accumulation does not rely on any assumptions about
the algebraic relationships between local partial derivatives; we allow these
values to be mutually independent. We also provide $O^*(2^n)$-time exact
algorithms for both problems, and show that under the exponential time
hypothesis these running times are essentially tight. Finally, we provide a
data reduction rule for Structural Optimal Jacobian Accumulation by showing
that false twins may always be eliminated consecutively.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [81] [Probabilistic approximation of fully nonlinear second-order PIDEs with convergence rates for the universal robust limit theorem](https://arxiv.org/abs/2506.18374)
*Lianzi Jiang,Mingshang Hu,Gechun Liang*

Main category: math.PR

TL;DR: A probabilistic approximation scheme for nonlinear PIDEs under G-expectation, with explicit error bounds and applications in robust limit theorems.


<details>
  <summary>Details</summary>
Motivation: Address the lack of numerical methods for nonlinear PIDEs with degenerate diffusion and non-separable uncertainty sets under Peng's G-expectation framework.

Method: Constructs a recursive, piecewise-constant approximation to the viscosity solution and derives explicit error bounds.

Result: Provides explicit convergence rates for robust limit theorems, unifying various theorems under sublinear expectations.

Conclusion: The scheme effectively approximates solutions and quantifies convergence, bridging gaps in existing literature.

Abstract: This paper develops a probabilistic approximation scheme for a class of
nonstandard, fully nonlinear second-order partial integro-differential
equations (PIDEs) arising from nonlinear L\'evy processes under Peng's
G-expectation framework. The PIDE involves a supremum over a set of
\(\alpha\)-stable L\'evy measures, potentially with degenerate diffusion and a
non-separable uncertainty set, which renders existing numerical results
inapplicable. We construct a recursive, piecewise-constant approximation to the
viscosity solution and derive explicit error bounds. A key application of our
analysis is the quantification of convergence rates for the universal robust
limit theorem under sublinear expectations, unifying Peng's robust central
limit theorem, laws of large numbers, and the \(\alpha\)-stable limit theorem
of Bayraktar and Munk, with explicit Berry--Esseen-type bounds.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [82] [LFR-PINO: A Layered Fourier Reduced Physics-Informed Neural Operator for Parametric PDEs](https://arxiv.org/abs/2506.17582)
*Jing Wang,Biao Chen,Hairun Xie,Rui Wang,Yifan Xia,Jifa Zhang,Hui Xu*

Main category: cs.LG

TL;DR: LFR-PINO introduces a layered hypernetwork and frequency-domain reduction to efficiently solve parametric PDEs, reducing errors by 22.8%-68.7% and memory usage by 28.6%-69.3%.


<details>
  <summary>Details</summary>
Motivation: Existing methods for solving parametric PDEs are either limited in expressiveness or computationally expensive due to high dimensionality.

Method: LFR-PINO uses a layered hypernetwork for specialized parameter generation and frequency-domain reduction to cut parameter count while preserving spectral features.

Result: LFR-PINO achieves significant error reduction (22.8%-68.7%) and memory savings (28.6%-69.3%) compared to baselines.

Conclusion: LFR-PINO balances computational efficiency and solution fidelity, offering a universal PDE solver with optional fine-tuning for precision.

Abstract: Physics-informed neural operators have emerged as a powerful paradigm for
solving parametric partial differential equations (PDEs), particularly in the
aerospace field, enabling the learning of solution operators that generalize
across parameter spaces. However, existing methods either suffer from limited
expressiveness due to fixed basis/coefficient designs, or face computational
challenges due to the high dimensionality of the parameter-to-weight mapping
space. We present LFR-PINO, a novel physics-informed neural operator that
introduces two key innovations: (1) a layered hypernetwork architecture that
enables specialized parameter generation for each network layer, and (2) a
frequency-domain reduction strategy that significantly reduces parameter count
while preserving essential spectral features. This design enables efficient
learning of a universal PDE solver through pre-training, capable of directly
handling new equations while allowing optional fine-tuning for enhanced
precision. The effectiveness of this approach is demonstrated through
comprehensive experiments on four representative PDE problems, where LFR-PINO
achieves 22.8%-68.7% error reduction compared to state-of-the-art baselines.
Notably, frequency-domain reduction strategy reduces memory usage by
28.6%-69.3% compared to Hyper-PINNs while maintaining solution accuracy,
striking an optimal balance between computational efficiency and solution
fidelity.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [83] [IRENE: a fluId layeR finitE-elemeNt softwarE](https://arxiv.org/abs/2506.17827)
*Dennis Wörthmuller,Gaetano Ferraro,Pierre Sens,Michele Castellana*

Main category: physics.flu-dyn

TL;DR: IRENE is a finite-element software library for simulating 2D viscous fluid layers in 3D space, handling various physical regimes and complex geometries.


<details>
  <summary>Details</summary>
Motivation: To address the need for versatile numerical tools capable of simulating fluid dynamics across multiple scales, including biological membranes and macroscopic air flows.

Method: The library employs finite-element methods to solve steady-state and dynamic problems, incorporating in-plane flows, out-of-plane deformations, surface tension, and elastic response.

Result: IRENE is validated against known analytical and numerical results, demonstrating its effectiveness in diverse scenarios.

Conclusion: IRENE provides a robust and flexible solution for simulating complex fluid dynamics in multi-scale and multi-physics environments.

Abstract: We present a finite-element software library, IRENE, for numerically solving
the steady state and dynamics of a two-dimensional viscous fluid layer embedded
in three-dimensional space, on multiple physical scales. The library is
designed to handle a wide range of physical regimes -- both low-Reynolds-number
and turbulent ones -- and geometries, capturing the coupling between in-plane
flows, out-of-plane deformations, surface tension, and elastic response. In
addition, the software can treat complex geometries, including those with
intra-layer obstacles or arbitrary layer boundaries. We validate IRENE against
known analytical and numerical results and demonstrate its capabilities through
examples relevant to biological membranes and macroscopic air flows.

</details>


### [84] [The mechanism of tornadogenesis from the perspective of vortex tubes](https://arxiv.org/abs/2506.18222)
*Peng Yue,Y. Charles Li,Jiamin Dang,Leigh Orf,Grace Yan*

Main category: physics.flu-dyn

TL;DR: A new theory explains tornadogenesis and lifespan using vortex tubes and pressure differences, linking tornado strength to pressure changes.


<details>
  <summary>Details</summary>
Motivation: To understand tornado formation and behavior through the lens of vortex dynamics and pressure differences.

Method: Analyzes tornadogenesis using Kelvin-Helmholtz Theorems, focusing on vorticity in squeezed vortex tubes due to pressure differences.

Result: Tornado strength varies with pressure differences; formation and decay are tied to these changes.

Conclusion: The theory provides a comprehensive explanation for tornado lifespan based on pressure-driven vorticity changes.

Abstract: In this paper, we propose a new theory on tornadogenesis from the perspective
of vortex tubes based on Kelvin-Helmholtz Theorems. When the pressure
difference between the lowest pressure line from the wall cloud down to the
ground and its surroundings is large enough, the increase of vorticity inside
the squeezed vortex tube can reach the tornado level, and thus a tornado is
born. When the pressure difference increases, the tornado strength increases.
When the pressure difference decreases, the tornado strength decreases. The
decay of tornadoes is caused by the decreasing pressure difference. This is our
theory of the entire tornado lifespan.

</details>


<div id='physics.ins-det'></div>

# physics.ins-det [[Back]](#toc)

### [85] [Full-spectrum modeling of mobile gamma-ray spectrometry systems in scattering media](https://arxiv.org/abs/2506.17820)
*David Breitenmoser,Alberto Stabilini,Sabine Mayer*

Main category: physics.ins-det

TL;DR: A generalized full-spectrum modeling framework for mobile gamma-ray spectrometry (MGRS) systems achieves near real-time template generation with a significant computational speedup and high accuracy.


<details>
  <summary>Details</summary>
Motivation: The need for accurate and efficient gamma-ray source localization in complex environments drives the development of a faster, platform-agnostic method.

Method: The framework uses dynamic, anisotropic instrument response functions (IRFs) for near real-time spectral template generation, benchmarked against Monte Carlo simulations.

Result: The method achieves a computational speedup of O(10^7) with median spectral deviations below 6%, maintaining high accuracy.

Conclusion: This framework enhances MGRS capabilities across various applications, including environmental monitoring and nuclear safeguards.

Abstract: Mobile gamma-ray spectrometry (MGRS) systems are essential for localizing,
identifying, and quantifying gamma-ray sources in complex environments.
Full-spectrum template matching offers the highest accuracy and sensitivity for
these tasks but is limited by the computational cost of generating the required
spectral templates. Here, we present a generalized full-spectrum modeling
framework for MGRS systems in scattering media, enabling near real-time
template generation through dynamic, anisotropic instrument response functions
(IRFs). Benchmarked against high-fidelity brute-force Monte Carlo simulations,
our method yields a computational speedup by a factor $\mathcal{O}(10^7)$,
while achieving comparable accuracy with median spectral deviations below 6%.
The presented methodology is platform-agnostic and applicable across marine,
terrestrial, and airborne domains, unlocking new capabilities for MGRS in a
variety of applications, such as environmental monitoring, geophysical
exploration, nuclear safeguards, and radiological emergency response.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [86] [An ansatz for constructing explicit solutions of Hessian equations](https://arxiv.org/abs/2506.17701)
*Chung-Jun Tsai,Mao-Pei Tsui,Mu-Tao Wang*

Main category: math.DG

TL;DR: The paper introduces a quadrics-based ansatz for solving complex and real Hessian equations, providing explicit solutions for dHYM/LYZ, Monge-Ampère, and J-equations. The method reduces PDEs to ODEs, solvable via abelian integrals, yielding entire solutions with detailed descriptions and some singularities.


<details>
  <summary>Details</summary>
Motivation: To address the lack of explicit solutions for complex and real Hessian equations, particularly for dHYM/LYZ, Monge-Ampère, and J-equations, by developing a systematic ansatz-based approach.

Method: A quadrics ansatz reduces the PDEs to second-order ODE systems with explicit first integrals, solvable via abelian integrals. The method is applied to construct solutions in dimensions like ℂ³ and ℝ³.

Result: Explicit solutions for dHYM/LYZ and special Lagrangian equations, including entire solutions of subcritical phases and singular solutions. Some solutions match known special Lagrangian submanifolds.

Conclusion: The ansatz provides a powerful tool for constructing explicit solutions to Hessian equations, with applications in complex and real geometry, including singular and entire solutions.

Abstract: We introduce a (variation of quadrics) ansatz for constructing explicit,
real-valued solutions to broad classes of complex Hessian equations on domains
in $\mathbb{C}^{n+1}$ and real Hessian equations on domains in
$\mathbb{R}^{n+1}$. In the complex setting, our method simultaneously addresses
the deformed Hermitian--Yang--Mills/Leung--Yau--Zaslow (dHYM/LYZ) equation, the
Monge--Amp\`ere equation, and the $J$-equation. Under this ansatz each PDE
reduces to a second-order system of ordinary differential equations admitting
explicit first integrals. These ODE systems integrate in closed form via
abelian integrals, producing wide families of explicit solutions together with
a detailed description. In particular, on $\mathbb{C}^3$, we construct entire
dHYM/LYZ solutions of arbitrary subcritical phase, and on $\mathbb{R}^3$ we
produce entire special Lagrangian solutions of arbitrary subcritical phase.
More generally, in any complex or real dimension, our ansatz yields entire
solutions of certain subcritical phases for both the dHYM/LYZ and special
Lagrangian equations. Some of these solutions develop singularities on compact
regions. In the special Lagrangian case we show that, after a natural extension
across the singular locus, these blow-up solutions coincide with previously
known complete special Lagrangian submanifolds obtained via a different ansatz.

</details>


### [87] [The Exponential of Skew-Symmetric Matrices: A Nearby Inverse and Efficient Computation of Derivatives](https://arxiv.org/abs/2506.18302)
*Zhifeng Deng,P. -A. Absil,Kyle A. Gallivan,Wen Huang*

Main category: math.DG

TL;DR: The paper characterizes the invertibility of the derivative of the skew-restricted matrix exponential, introduces a 'nearby logarithm' for efficient computation, and shows significant speed improvements over existing methods.


<details>
  <summary>Details</summary>
Motivation: The skew-symmetric matrix exponential has key applications in Lie groups and Riemannian geometry, but its derivative's invertibility and computational efficiency are underexplored.

Method: The authors characterize the invertibility of the derivative, construct a smooth inverse (nearby logarithm), and derive symbolic formulae for differentiation and inversion.

Result: The proposed formulae are up to 3.9x and 3.6x faster than state-of-the-art methods for differentiation and inversion, respectively.

Conclusion: The work provides efficient computational tools for the skew-restricted matrix exponential, with practical benefits for applications in geometry and optimization.

Abstract: The matrix exponential restricted to skew-symmetric matrices has numerous
applications, notably in view of its interpretation as the Lie group
exponential and Riemannian exponential for the special orthogonal group. We
characterize the invertibility of the derivative of the skew-restricted
exponential, thereby providing a simple expression of the tangent conjugate
locus of the orthogonal group. In view of the skew restriction, this
characterization differs from the classic result on the invertibility of the
derivative of the exponential of real matrices. Based on this characterization,
for every skew-symmetric matrix $A$ outside the (zero-measure) tangent
conjugate locus, we explicitly construct the domain and image of a smooth
inverse -- which we term \emph{nearby logarithm} -- of the skew-restricted
exponential around $A$. This nearby logarithm reduces to the classic principal
logarithm of special orthogonal matrices when $A=\mathbf{0}$. The symbolic
formulae for the differentiation and its inverse are derived and implemented
efficiently. The extensive numerical experiments show that the proposed
formulae are up to $3.9$-times and $3.6$-times faster than the current
state-of-the-art robust formulae for the differentiation and its inversion,
respectively.

</details>


<div id='math.CA'></div>

# math.CA [[Back]](#toc)

### [88] [Sharp $L^p$-estimates for wave equation on $ax+b$ groups](https://arxiv.org/abs/2506.17531)
*Yunxiang Wang,Lixin Yan*

Main category: math.CA

TL;DR: The paper establishes necessary and sufficient conditions for L^p norm estimates of solutions to a wave equation on a specific group, with endpoint results for certain parameters.


<details>
  <summary>Details</summary>
Motivation: To analyze the behavior of solutions to a wave equation on the group G = ℝ₊⋉ℝⁿ and derive precise L^p norm estimates under given conditions.

Method: The study uses the group's Riemannian symmetric space metric and Haar measure, focusing on the Laplacian L and solving the wave equation with initial conditions.

Result: The L^p norm of the solution u(t,·) is bounded if and only if the parameters α₀ and α₁ satisfy specific inequalities, with endpoint results provided.

Conclusion: The paper provides exact conditions for L^p norm estimates of wave equation solutions on G, extending prior work by Müller and Thiele.

Abstract: Let $G$ be the group $\mathbb{R}_+\ltimes \mathbb{R}^n$ endowed with
Riemannian symmetric space metric $d$ and the right Haar measure $\mathrm{d}
\rho$ which is of $ax+b$ type, and $L$ be the positive definite distinguished
left invariant Laplacian on $G$. Let $u=u(t,\cdot)$ be the solution of
$u_{tt}+Lu=0$ with initial conditions $u|_{t=0}=f$ and $u_t|_{t=0}=g$. In this
article we show that for a fixed $t \in{\mathbb R}$ and every $1<p<\infty$,
\begin{align*} \|u(t,\cdot)\|_{L^p(G)}\leq C_p\Big(
(1+|t|)^{2|1/p-1/2|}\|f\|_{L^p_{\alpha_0}(G)}+(1+|t|)\,\|g\|_{L^p_{\alpha_1}(G)}\Big)
\end{align*} if and only if \begin{align*} \alpha_0\geq n\left|{1\over p}-
{1\over2}\right| \quad \mbox{and} \quad \alpha_1\geq n\left|{1\over p}-
{1\over2}\right| -1. \end{align*} This gives an endpoint result for
$\alpha_0=n|1/p-1/2|$ and $\alpha_1=n|1/p-1/2|-1$ with $1<p<\infty$ in
Corollary 8.2, as pointed out in Remark 8.1 due to M\"{u}ller and Thiele
[Studia Math. \textbf{179} (2007)].

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [89] [Code Generation for Near-Roofline Finite Element Actions on GPUs from Symbolic Variational Forms](https://arxiv.org/abs/2506.17471)
*Kaushik Kulkarni,Andreas Klöckner*

Main category: cs.DC

TL;DR: A novel GPU parallelization strategy for FEM variational forms using UFL on simplex meshes, achieving over 50% roofline performance in 65% of test cases.


<details>
  <summary>Details</summary>
Motivation: To handle the diversity of computational workloads in FEM and achieve near-roofline performance on GPUs.

Method: Code transformations, scheduling candidates, heuristic cost model, and pruning strategy within the Firedrake framework.

Result: Achieves over 50% roofline performance in 65% of test cases on Nvidia GPUs (Titan V and Tesla K40c).

Conclusion: The strategy effectively balances latency-hiding and state space, demonstrating strong performance across diverse FEM applications.

Abstract: We present a novel parallelization strategy for evaluating Finite Element
Method (FEM) variational forms on GPUs, focusing on those that are expressible
through the Unified Form Language (UFL) on simplex meshes. We base our approach
on code transformations, wherein we construct a space of scheduling candidates
and rank them via a heuristic cost model to effectively handle the large
diversity of computational workloads that can be expressed in this way. We
present a design of a search space to which the cost model is applied, along
with an associated pruning strategy to limit the number of configurations that
need to be empirically evaluated. The goal of our design is to strike a balance
between the device's latency-hiding capabilities and the amount of state space,
a key factor in attaining near-roofline performance.
  To make our work widely available, we have prototyped our parallelization
strategy within the \textsc{Firedrake} framework, a UFL-based FEM solver. We
evaluate the performance of our parallelization scheme on two generations of
Nvidia GPUs, specifically the Titan V (Volta architecture) and Tesla K40c
(Kepler architecture), across a range of operators commonly used in
applications, including fluid dynamics, wave propagation, and structural
mechanics, in 2D and 3D geometries. Our results demonstrate that our proposed
algorithm achieves more than $50\%$ roofline performance in $65\%$ of the test
cases on both devices.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [90] [Resolving the Ti-V Phase Diagram Discrepancy with First-Principles Calculations and Bayesian Learning](https://arxiv.org/abs/2506.17719)
*Timofei Miryashkin,Olga Klimanova,Alexander Shapeev*

Main category: cond-mat.mtrl-sci

TL;DR: The study resolves the controversy over the Ti-V alloy's miscibility gap using an ab initio + machine-learning workflow, confirming the gap exists without oxygen contamination.


<details>
  <summary>Details</summary>
Motivation: Conflicting experiments about the Ti-V alloy's miscibility gap and the hypothesis of oxygen contamination motivated this research.

Method: An ab initio + machine-learning workflow combining Moment Tensor Potential and Bayesian thermodynamic inference was used.

Result: The study confirmed a BCC miscibility gap at T = 980 K and c = 0.67, independent of oxygen contamination.

Conclusion: The findings contradict CALPHAD reassessments and demonstrate the robustness of the computational approach.

Abstract: Conflicting experiments disagree on whether the titanium-vanadium (Ti-V)
binary alloy exhibits a body-centred cubic (BCC) miscibility gap or remains
completely soluble. A leading hypothesis attributes the miscibility gap to
oxygen contamination during alloy preparation. To resolve this controversy, we
use an ab initio + machine-learning workflow that couples an actively-trained
Moment Tensor Potential to Bayesian thermodynamic inference. Using this
workflow, we obtain Ti-V binary system across the entire composition range,
together with confidence intervals in the thermodynamic limit. The resulting
diagram reproduces all experimental features, demonstrating the robustness of
our approach, and clearly favors the variant with a BCC miscibility gap
terminating at T = 980 K and c = 0.67. Because oxygen was excluded from
simulations, the gap cannot be attributed to impurity effects, contradicting
recent CALPHAD reassessments.

</details>


### [91] [Intercalation-Altermagnet-Driven Ferrimagnetic-Ferroelastic Multiferroics and Anomalous and Spin Transport](https://arxiv.org/abs/2506.18531)
*Long Zhang,Yuxin Liu,Junfeng Ren,Guangqian Ding,Xiaotian Wang,Guangxin Ni,Guoying Gao,Zhenxiang Cheng*

Main category: cond-mat.mtrl-sci

TL;DR: Intercalation-driven altermagnets enhance spin splitting, multiferroic properties, and spin transport, demonstrated in V2Se2O bilayers with high efficiency and room-temperature functionality.


<details>
  <summary>Details</summary>
Motivation: To address challenges in altermagnets by improving electronic structures and transport functionalities for practical applications.

Method: Density functional theory, Wannier function analyses, Monte Carlo simulations, and non-equilibrium Green function methods applied to intercalated V2Se2O bilayers.

Result: Enhanced spin splitting, half-metallic behavior, high magnetoresistance (877%), and ultra-high thermal tunneling magnetoresistance (~12000%).

Conclusion: Intercalation-driven altermagnets offer multifunctional integration for advanced, miniaturized, room-temperature applications in spin transport.

Abstract: Spin splitting in emerging altermagnets is non-relativistic and
momentum-dependent, yet energy-independent and localized, posing challenges for
practical applications. Here, we propose a paradigm of intercalation-driven
altermagnets to attain ameliorative electronic structures, multiferroic
characteristics, and anomalous and spin transport functionalities. As a
representative system, we investigate electrochemistry- and self-intercalated
V2Se2O bilayers, building on the recently reported room-temperature K- and
Rb-intercalated V2Se2O family, utilizing density functional theory, Wannier
function analyses, Monte Carlo simulations, and non-equilibrium Green function
methods. Intercalation induces room-temperature intralayer ferrimagnetic and
interlayer ferromagnetic couplings (358 K for Li-intercalation and 773 K for
V-intercalation), ferroelasticity (~1 % signal intensity), in-plane uniaxial
magnetic anisotropy and metallization, while modifying the anomalous Hall
effect. Notably, Li- and V-intercalated V2Se2O bilayers exhibit enhanced spin
splitting and half-metallic behavior, respectively, yielding near-perfect spin
filtering efficiencies. Intercalation substantially boosts spin transport in
V2Se2O-based devices, enabling giant magnetoresistance (877 %), ultra-high
thermal tunneling magnetoresistance (~12000 %), and observable spin Seebeck and
temperature negative differential resistance effects. Such
intercalation-altermagnet-driven paradigm pioneers the expansion of
altermagnetic functionalities through multifunctional integration, offering
promising avenues for advanced, miniaturized, room-temperature utilization of
anomalous, electron, and spin transport.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [92] [Corner Topology Makes Woven Baskets into Stiff, yet Resilient Metamaterials](https://arxiv.org/abs/2506.18197)
*Guowei Wayne Tu,Evgueni T. Filipov*

Main category: cond-mat.soft

TL;DR: Basket weaving's 3D structures offer high resilience and stiffness, enabling modern engineering applications like robotics and metamaterials.


<details>
  <summary>Details</summary>
Motivation: To explore the underexplored mechanics and engineering potential of 3D woven structures, inspired by traditional basket weaving.

Method: Study corner topologies to convert 2D woven sheets into 3D metamaterials, analyzing stiffness and resilience under deformation.

Result: Woven structures match stiffness of continuous ones but excel in resilience, enabling elastic buckling and damage resistance.

Conclusion: Basket weaving principles can revolutionize design in automotive, robotics, and more, combining stiffness and resilience.

Abstract: Basket weaving is a traditional craft used to create practical
three-dimensional (3D) structures. While the geometry and aesthetics of baskets
have received considerable attention, the underlying mechanics and modern
engineering potential remain underexplored. This work shows that 3D woven
structures offer similar stiffness yet substantially higher resilience than
their non-woven continuous counterparts. We explore corner topologies that
serve as building blocks to convert 2D woven sheets into 3D metamaterials that
can carry compressive loads. Under small deformations, the woven corners
exhibit axial stiffness similar to continuous structures because the woven
ribbons are engaged with in-plane loads. Under large deformations, the woven
corners can be compressed repeatedly without plastic damage because ribbons can
undergo elastic local buckling. We present a modular platform to assemble woven
corners into complex spatial metamaterials and demonstrate applications
including damage-resilient robotic systems and metasurfaces with tailorable
deformation modes. Our results explain the historic appeal of basket weaving,
where readily available ribbons are crafted into 3D structures with comparable
stiffness yet far superior resilience to continuous systems. The modular
assembly of woven metamaterials can further revolutionize design of
next-generation automotive components, consumer devices, soft robots, and more
where both resilience and stiffness are essential.

</details>
