<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 14]
- [math.AP](#math.AP) [Total: 17]
- [physics.comp-ph](#physics.comp-ph) [Total: 3]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 1]
- [cs.LG](#cs.LG) [Total: 4]
- [math.CV](#math.CV) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [cs.LO](#cs.LO) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [quant-ph](#quant-ph) [Total: 4]
- [physics.acc-ph](#physics.acc-ph) [Total: 1]
- [math.DS](#math.DS) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Numerical analysis of scattered point measurement-based regularization for backward problems for fractional wave equations](https://arxiv.org/abs/2506.18948)
*Dakang Cen,Zhiyuan Li,Wenlong Zhang*

Main category: math.NA

TL;DR: A numerical framework for reconstructing initial values from terminal data in fractional wave equations, with regularization for noisy measurements and optimal error estimates.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of reconstructing unknown initial values from terminal data under lower regularity assumptions and noisy conditions.

Method: Develops a numerical framework on nonuniform time grids, introduces a regularization method for stochastic noise, and provides optimal error estimates.

Result: Optimal error estimates balance discretization errors, noise, and observation points, with an a priori choice of regularization parameters.

Conclusion: Numerical experiments confirm the algorithm's efficiency and accuracy in handling the problem.

Abstract: In this work, our aim is to reconstruct the unknown initial value from
terminal data. We develop a numerical framework on nonuniform time grids for
fractional wave equations under the lower regularity assumptions. Then, we
introduce a regularization method that effectively handles scattered point
measurements contaminated with stochastic noise. The optimal error estimates of
stochastic convergence not only balance discretization errors, the noise, and
the number of observation points, but also propose an a priori choice of
regularization parameters. Finally, several numerical experiments are presented
to demonstrate the efficiency and accuracy of the algorithm.

</details>


### [2] [Modular data assimilation for flow prediction](https://arxiv.org/abs/2506.19002)
*Aytekin Çıbık,Rui Fang,William Layton*

Main category: math.NA

TL;DR: The paper introduces a modular, 2-step data assimilation method inspired by Kalman filters, improving stability and predictability while reducing complexity.


<details>
  <summary>Details</summary>
Motivation: To address algorithmic inadequacies in standard nudging methods by developing a more stable and efficient modular approach.

Method: A 2-step algorithm with variants, combining implicit and explicit updates, and integrating assimilation with eddy viscosity models.

Result: The method enhances stability, reduces error, and increases predictability horizons, confirmed by numerical tests.

Conclusion: The modular 2-step method outperforms standard nudging, retaining mathematical robustness and practical effectiveness.

Abstract: This report develops several modular, 2-step realizations (inspired by Kalman
filter algorithms) of nudging-based data assimilation \begin{equation*}
\begin{array}{cc} Step{{{\text { }}}}1 & \begin{array}{c} \frac{\widetilde
{v}^{n+1}-v^{n}}{k}+v^{n}\cdot \nabla \widetilde {v}% ^{n+1}-\nu \triangle
\widetilde {v}^{n+1}+\nabla q^{n+1}=f(x){{{{\text { }}}}% } \\ \nabla \cdot
\widetilde {v}^{n+1}=0% \end{array} \\ Step{{{\text { }}}}2 &
\frac{v^{n+1}-\widetilde {v}^{n+1}}{k}-\chi I_{H}(u(t^{n+1})-v^{n+1})=0.%
\end{array}% \end{equation*} Several variants of this algorithm are developed.
Three main results are developed. The first is that if $I_{H}^{2}=I_{H}$, then
Step 2 can be rewritten as the explicit step \begin{equation*}
v^{n+1}=\widetilde {v}^{n+1}+\frac{k\chi }{1+k\chi }[I_{H}u(t^{n+1})-I_{H}%
\widetilde {v}^{n+1}]. \end{equation*} This means Step 2 has the greater
stability of an implicit update and the lesser complexity of an explicit
analysis step. The second is that the basic result of nudging (that for $H$
\textit{small enough} and $\chi $\ \textit{large enough} predictability
horizons are infinite) holds for one variant of the modular algorithm. The
third is that, for \textit{any} $H>0$ and \textit{any} $\chi>0$, one step of
the modular algorithm decreases the next step's error and \textit{increases}
(an estimate of) predictability horizons. A method synthesizing assimilation
with eddy viscosity models of turbulence is also presented. Numerical tests are
given, confirming the effectiveness of the modular assimilation algorithm. The
conclusion is that the modular, 2-step method overcomes many algorithmic
inadequacies of standard nudging methods and retains a robust mathematical
foundation.

</details>


### [3] [Stabilizing PDE--ML Coupled System](https://arxiv.org/abs/2506.19274)
*Saad Qadeer,Panos Stinis,Hui Wan*

Main category: math.NA

TL;DR: The paper addresses instabilities in machine-learnt PDE surrogates, proposes stabilization strategies, and explores accuracy improvements using the Mori--Zwanzig formalism.


<details>
  <summary>Details</summary>
Motivation: Instabilities in numerical solutions of machine-learnt PDE surrogates hinder their broader use, prompting the need for stabilization and accuracy enhancement.

Method: The study focuses on a viscous Burgers'-ML system, identifies instability causes, and prescribes stabilization strategies, followed by accuracy improvements via the Mori--Zwanzig formalism.

Result: Strategies to stabilize the coupled system are developed, and methods to enhance accuracy are explored.

Conclusion: The insights and methods from the prototype problem can aid in addressing instabilities in more complex PDE systems.

Abstract: A long-standing obstacle in the use of machine-learnt surrogates with larger
PDE systems is the onset of instabilities when solved numerically. Efforts
towards ameliorating these have mostly concentrated on improving the accuracy
of the surrogates or imbuing them with additional structure, and have garnered
limited success. In this article, we study a prototype problem and draw
insights that can help with more complex systems. In particular, we focus on a
viscous Burgers'-ML system and, after identifying the cause of the
instabilities, prescribe strategies to stabilize the coupled system. To improve
the accuracy of the stabilized system, we next explore methods based on the
Mori--Zwanzig formalism.

</details>


### [4] [Spectral approximation to fractional integral operator](https://arxiv.org/abs/2506.19332)
*Xiaolin Liu,Kuan Xu*

Main category: math.NA

TL;DR: A fast and stable method for constructing matrix approximations to fractional integral operators using Chebyshev fractional polynomials, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency and stability of approximating fractional integral operators for solving boundary value problems, initial value problems, and eigenvalue problems in fractional calculus.

Method: Utilizes a recurrence relation satisfied by fractional integrals of mapped Chebyshev polynomials.

Result: Demonstrates superior performance over existing methods and broad applicability in numerical examples.

Conclusion: The method is effective for various fractional calculus problems, including boundary value problems, initial value problems, and eigenvalue problems.

Abstract: We propose a fast and stable method for constructing matrix approximations to
fractional integral operators applied to series in the Chebyshev fractional
polynomials. This method utilizes a recurrence relation satisfied by the
fractional integrals of mapped Chebyshev polynomials and significantly
outperforms existing methods. Through numerical examples, we highlight the
broad applicability of these matrix approximations, including the solution of
boundary value problems for fractional integral and differential equations.
Additional applications include fractional differential equation initial value
problems and fractional eigenvalue problems.

</details>


### [5] [A High-Order Compact Hermite Difference Method for Double-Diffusive Convection](https://arxiv.org/abs/2506.19367)
*Jianqing Yang,Jianxian Qiu*

Main category: math.NA

TL;DR: A high-order compact finite difference Hermite scheme is proposed for double-diffusive convection, using flux splitting and Hermite interpolation for stability and accuracy. Numerical tests confirm its effectiveness.


<details>
  <summary>Details</summary>
Motivation: To develop a stable and accurate numerical method for simulating double-diffusive convection without requiring auxiliary derivative equations.

Method: Convective fluxes are split into positive/negative parts and discretized using compact Hermite differences. Diffusion fluxes use Hermite interpolation. Temporal discretization employs third-order Runge-Kutta.

Result: Numerical results match benchmarks and literature. The method is applied to steady/unsteady double-diffusive convection problems.

Conclusion: The proposed scheme is effective for double-diffusive convection, validated by numerical tests and preliminary applications.

Abstract: In this paper, a class of high-order compact finite difference Hermite scheme
is presented for the simulation of double-diffusive convection. To maintain
linear stability, the convective fluxes are split into positive and negative
parts, then the compact Hermite difference methods are used to discretize the
positive and negative fluxes, respectively. The diffusion fluxes of the
governing equations are directly approximated by a high-order finite difference
scheme based on the Hermite interpolation. The advantages of the proposed
schemes are that the derivative values of the solutions are directly solved by
the compact central difference scheme, and the auxiliary derivative equation is
no longer required. The third-order Runge-Kutta method is utilized for the
temporal discretization. Several numerical tests are presented to assess the
numerical capability of the newly proposed algorithm. The numerical results are
in great agreement with the benchmark solutions and some of the accurate
results available in the literature. Subsequently, we apply the algorithm to
solve steady and unsteady problems of double-diffusive convection and a
preliminary application to the double-diffusive convection for different
Raleigh numbers and aspect ratios is carried out.

</details>


### [6] [General-domain FC-based shock-dynamics solver II: Non-smooth domains, accuracy and parallel performance](https://arxiv.org/abs/2506.19370)
*Daniel V. Leibovici,Oscar P. Bruno*

Main category: math.NA

TL;DR: Part II extends FC-SDNN to non-smooth domains, introduces a parallel implementation, and tests it on 2D Euler equations, showing high accuracy and scalability.


<details>
  <summary>Details</summary>
Motivation: To enhance the FC-SDNN for non-smooth domains and demonstrate its parallel scalability and accuracy in handling shocks and high-speed flows.

Method: Extends FC-SDNN with localized smooth artificial viscosity, uses an ANN to detect discontinuities, and implements parallel processing for scalability.

Result: Achieves sharp shock resolution, minimal numerical dissipation, and high scalability in weak and strong scaling tests.

Conclusion: The method is accurate and scalable, effectively resolving shocks and discontinuities while maintaining smooth solutions in regular flow regions.

Abstract: This contribution Part II of a two-part series, extends the general-domain
FC-SDNN (Fourier Continuation Shock-Detecting Neural Network) introduces in
Part I to enable treatment of non-smooth domains, it introduces a parallel
implementation of the scheme with high-quality weak and strong scalability
properties, and it illustrates the overall methodology for a variety of tests
for the 2D Euler equations--including supersonic and hypersonic flows and
shocks past obstacles with corners. The results produces by the new methods are
compared to previous theoretical and experimental results, and the high
parallel scalability of the algorithm is demonstrated in both weak and strong
scaling cases. Thanks to its use of a localized yet smooth artificial viscosity
term--whose support is confined to regions near flow discontinuities identified
by an artificial neural network--the algorithm maintains minimal numerical
dissipation away from discontinuities. Overall, the method delivers accurate,
sharp resolution of shocks and contact discontinuities, while producing smooth
numerical solutions in regular flow regions--as evidences by the near-complete
absence of spurious oscillations in level-set contours, even under strong
shocks and high-speed flow conditions.

</details>


### [7] [Towards automated generation of fast and accurate algorithms for recursive matrix multiplication](https://arxiv.org/abs/2506.19405)
*Jean-Guillaume Dumas,Clément Pernet,Alexandre Sedoglavic*

Main category: math.NA

TL;DR: A strategy for generating fast and accurate non-commutative recursive matrix multiplication algorithms using norm bounds, heuristics, and sparsification.


<details>
  <summary>Details</summary>
Motivation: To improve the speed and accuracy of non-commutative recursive matrix multiplication algorithms by leveraging norm bounds and optimization techniques.

Method: 1. Unify and extend norm bounds for stability. 2. Minimize growth factor and operations. 3. Sparsify alternative basis to improve time complexity.

Result: Proposed a 2x2-matrix algorithm with 7 products, better accuracy, and improved time complexity. Demonstrated effectiveness on other algorithms like Smirnov's 3x3x6:40.

Conclusion: The strategy successfully balances speed and accuracy, offering practical improvements for recursive matrix multiplication.

Abstract: We propose a strategy for the generation of fast and accurate versions of
non-commutative recursive matrix multiplication algorithms. To generate these
algorithms, we consider matrix and tensor norm bounds governing the stability
and accuracy of numerical matrix multiplication. We start by a unification on
known max-norm bounds on matrix multiplication stability and then extend them
to further norms and more generally to recursive bilinear algorithms and the
alternative basis matrix multiplication algorithms. Then our strategy has three
phases. First, we reduce those bounds by minimizing a growth factor along the
orbits of the associated matrix multiplication tensor decomposition. Second, we
develop heuristics that minimize the number of operations required to realize a
bilinear formula, while further improving its accuracy. Third, we perform an
alternative basis sparsification that improves on the time complexity constant
and mostly preserves the overall accuracy. For instance this strategy allows us
to propose a non-commutative algorithm for multiplying 2x2-matrices using 7
coefficient products. This algorithm reaches simultaneously a better accuracy
in practice compared to previously known such fast ___2x2x2:7___ Strassen-like
algorithms and a time complexity bound with the best currently known leading
term (obtained via alternative basis sparsification). We also present detailed
results of our technique on other recursive matrix multiplication algorithms,
such as Smirnov's ___3x3x6:40___ family of algorithms.

</details>


### [8] [Sharp numerical approximation of the Hardy constant](https://arxiv.org/abs/2506.19422)
*Liviu I. Ignat,Enrique Zuazua*

Main category: math.NA

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study the $P_1$ finite element approximation of the best constant in the
classical Hardy inequality over bounded domains containing the origin in
$\mathbb{R}^N$, for $N \geq 3$.
  Despite the fact that this constant is not attained in the associated Sobolev
space $H^1$, our main result establishes an explicit, sharp, and
dimension-independent rate of convergence proportional to $1/|\log h|^2$.
  The analysis carefully combines an improved Hardy inequality involving a
reminder term with logarithmic weights, approximation estimates for Hardy-type
singular radial functions constituting minimizing sequences, properties of
piecewise linear and continuous finite elements, and weighted Sobolev space
techniques.
  We also consider other closely related spectral problems involving the
Laplacian with singular quadratic potentials obtaining sharp convergence rates.

</details>


### [9] [A Generalized Framework for Higher-Order Localized Orthogonal Decomposition Methods](https://arxiv.org/abs/2506.19462)
*Moritz Hauck,Alexei Lozinski,Roland Maier*

Main category: math.NA

TL;DR: A generalized framework for higher-order multiscale methods (Localized Orthogonal Decomposition) is introduced, accommodating conforming and nonconforming constraints with new localization strategies. Analyzed for linear elliptic problems, extended to Helmholtz and Gross-Pitaevskii equations, with numerical comparisons.


<details>
  <summary>Details</summary>
Motivation: To generalize and enhance the Localized Orthogonal Decomposition method for higher-order problems, accommodating diverse constraints and improving localization strategies.

Method: Reformulation to include conforming and nonconforming constraints, with new localization strategies. Analysis focuses on linear elliptic problems, with extensions to Helmholtz and Gross-Pitaevskii equations.

Result: Successful framework adaptation for higher-order problems, validated through numerical examples comparing conforming and nonconforming constraints.

Conclusion: The framework effectively generalizes higher-order multiscale methods, offering flexibility and insights into constraint handling and localization strategies.

Abstract: We introduce a generalized framework for studying higher-order versions of
the multiscale method known as Localized Orthogonal Decomposition. Through a
suitable reformulation, we are able to accommodate both conforming and
nonconforming constraints in the construction process. In particular, we offer
a new perspective on localization strategies. We fully analyze the strategy for
linear elliptic problems and discuss extensions to the Helmholtz equation and
the Gross--Pitaevskii eigenvalue problem. Numerical examples are presented that
particularly provide valuable comparisons between conforming and nonconforming
constraints.

</details>


### [10] [Anisotropic approximation on space-time domains](https://arxiv.org/abs/2506.19517)
*Pedro Morin,Cornelia Schneider,Nick Schneider*

Main category: math.NA

TL;DR: The paper explores anisotropic polynomial approximation in Lebesgue and Besov spaces, focusing on temporal and spatial smoothness moduli, and proves inequalities on Lipschitz cylinders. It also applies these findings to adaptive space-time finite element approximation.


<details>
  <summary>Details</summary>
Motivation: To advance understanding of anisotropic polynomial approximation in function spaces and its applications in finite element methods.

Method: Study of temporal and spatial moduli of smoothness, proving Jackson- and Whitney-type inequalities on Lipschitz cylinders.

Result: Direct estimate result for adaptive space-time finite element approximation in discontinuous settings.

Conclusion: The findings enhance theoretical foundations for anisotropic approximation and its practical use in adaptive finite element methods.

Abstract: We investigate anisotropic (piecewise) polynomial approximation of functions
in Lebesgue spaces as well as anisotropic Besov spaces. For this purpose we
study temporal and spacial moduli of smoothness and their properties. In
particular, we prove Jackson- and Whitney-type inequalities on Lipschitz
cylinders, i.e., space-time domains $I\times D$ with a finite interval $I$ and
a bounded Lipschitz domain $D\subset \R^d$, $d\in \N$. As an application, we
prove a direct estimate result for adaptive space-time finite element
approximation in the discontinuous setting.

</details>


### [11] [Sparse and low-rank approximations of parametric elliptic PDEs: the best of both worlds](https://arxiv.org/abs/2506.19584)
*Markus Bachmayr,Huqing Yang*

Main category: math.NA

TL;DR: A new method combines low-rank tensor approximation and sparse polynomial expansion to solve PDEs with infinitely many parameters, focusing on elliptic problems with short correlation lengths. An adaptive solver ensures quasi-optimal ranks and optimal convergence rates.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies in direct polynomial expansions for PDEs with infinitely many parameters, especially in elliptic problems with short correlation lengths.

Method: Combines low-rank tensor approximation in selected variables with sparse polynomial expansion in remaining parametric variables. Introduces an adaptive solver for quasi-optimal ranks and optimal convergence.

Result: The method achieves efficient approximation and optimal convergence rates, validated by numerical tests.

Conclusion: The proposed approach effectively solves challenging PDEs with infinitely many parameters, offering computational efficiency and accuracy.

Abstract: A new approximation format for solutions of partial differential equations
depending on infinitely many parameters is introduced. By combining low-rank
tensor approximation in a selected subset of variables with a sparse polynomial
expansion in the remaining parametric variables, it addresses in particular
classes of elliptic problems where a direct polynomial expansion is
inefficient, such as those arising from random diffusion coefficients with
short correlation length. A convergent adaptive solver is proposed and analyzed
that maintains quasi-optimal ranks of approximations and at the same time
yields optimal convergence rates of spatial discretizations without coarsening.
The results are illustrated by numerical tests.

</details>


### [12] [Hybrid high-order approximations of div-curl systems on domains with general topology](https://arxiv.org/abs/2506.19616)
*Jérémy Dalphin,Jean-Pierre Ducreux,Simon Lemaire,Silvano Pitassi*

Main category: math.NA

TL;DR: Hybrid polyhedral methods of arbitrary order are developed for approximating div-curl systems in 3D domains with non-trivial topology, focusing on magnetostatics. The approach relies on hybrid Weber inequalities for well-posedness and includes error analysis for regular solutions. Numerical validation addresses non-simply-connected domains.


<details>
  <summary>Details</summary>
Motivation: The study aims to address div-curl systems in magnetostatics, particularly in complex 3D domains with non-trivial topology, where traditional methods may falter.

Method: Hybrid polyhedral methods of arbitrary order are employed, leveraging hybrid Weber inequalities for discrete problem well-posedness. Error analysis is conducted for regular solutions.

Result: The method demonstrates effectiveness in approximating div-curl systems, validated through numerical experiments, including non-simply-connected domains.

Conclusion: The hybrid polyhedral approach is robust for div-curl systems in topologically complex domains, with potential applications in magnetostatics.

Abstract: We devise and analyze hybrid polyhedral methods of arbitrary order for the
approximation of div-curl systems on three-dimensional domains featuring
non-trivial topology. The div-curl systems we are interested in stem from
magnetostatics, and can either be first-order (field formulation) or
second-order (vector potential formulation). The well-posedness of the
resulting discrete problems essentially hinges on recently established,
topologically generic, hybrid versions of the (first and second) Weber
inequalities. Our error analysis covers the case of regular solutions.
Leveraging (co)homology computation techniques from the literature, we perform
an in-depth numerical assessment of our approach, covering, in particular, the
case of non-simply-connected domains.

</details>


### [13] [Fast Flexible LSQR with a Hybrid Variant for Efficient Large-Scale Regularization](https://arxiv.org/abs/2506.19666)
*Eva Mikušová,Iveta Hnětynková*

Main category: math.NA

TL;DR: The paper introduces Fast Flexible LSQR (FaFLSQR), a novel algorithm for solving large-scale ill-posed problems. It improves computational efficiency over existing methods like FCGLS and FLSQR by using a new bidiagonalization technique.


<details>
  <summary>Details</summary>
Motivation: Large-scale ill-posed problems with noise require efficient regularization methods. Existing Krylov subspace methods like LSQR and CGLS have limitations in computational cost and flexibility.

Method: The authors propose a Fast Flexible Golub-Kahan bidiagonalization method, combining one long-term and one short-term recurrence. This leads to the FaFLSQR algorithm, which is mathematically equivalent to FCGLS but more efficient.

Result: Numerical experiments show FaFLSQR outperforms FCGLS and FLSQR in computational efficiency while maintaining similar regularization capabilities.

Conclusion: FaFLSQR is a promising advancement for solving ill-posed problems, offering improved efficiency and flexibility compared to existing methods.

Abstract: A wide range of applications necessitates solving large-scale ill-posed
problems contaminated by noise. Krylov subspace regularization methods are
particularly advantageous in this context, as they rely solely on matrix-vector
multiplication. Among the most widely used techniques are LSQR and CGLS, both
of which can be extended with flexible preconditioning to enforce solution
properties such as nonnegativity or sparsity. Flexible LSQR (FLSQR) can also be
further combined with direct methods to create efficient hybrid approaches. The
Flexible Golub-Kahan bidiagonalization underlying FLSQR requires two long-term
recurrences. In this paper, we introduce a novel Fast Flexible Golub-Kahan
bidiagonalization method that employs one long-term and one short-term
recurrence. Using this, we develop the Fast Flexible LSQR (FaFLSQR) algorithm,
which offers comparable computational cost to FCGLS while also supporting
hybrid regularization like FLSQR. We analyze the properties of FaFLSQR and
prove its mathematical equivalence to FCGLS. Numerical experiments demonstrate
that in floating point arithmetic FaFLSQR outperforms both FCGLS and FLSQR in
terms of computational efficiency.

</details>


### [14] [Krylov and core transformation algorithms for an inverse eigenvalue problem to compute recurrences of multiple orthogonal polynomials](https://arxiv.org/abs/2506.19796)
*Amin Faghih,Michele Rinelli,Marc Van Barel,Raf Vandebril,Robbe Vermeiren*

Main category: math.NA

TL;DR: Algorithms for computing recurrence coefficients of multiple orthogonal polynomials are developed, reformulated as an inverse eigenvalue problem, and solved using two numerical approaches.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of computing recurrence coefficients for multiple orthogonal polynomials on the step-line efficiently and accurately.

Method: Two approaches: (1) a biorthogonal Lanczos process with multiple starting vectors linked to block Krylov subspaces, and (2) Gaussian eliminations on a diagonal matrix to construct a banded Hessenberg matrix.

Result: The algorithms are tested on ill-conditioned (Kravchuk, Hahn polynomials) and well-conditioned problems, analyzing their accuracy and stability.

Conclusion: The proposed methods effectively solve the inverse eigenvalue problem for recurrence coefficients, demonstrating practical utility in numerical linear algebra.

Abstract: In this paper, we develop algorithms for computing the recurrence
coefficients corresponding to multiple orthogonal polynomials on the step-line.
We reformulate the problem as an inverse eigenvalue problem, which can be
solved using numerical linear algebra techniques. We consider two approaches:
the first is based on the link with block Krylov subspaces and results in a
biorthogonal Lanczos process with multiple starting vectors; the second
consists of applying a sequence of Gaussian eliminations on a diagonal matrix
to construct the banded Hessenberg matrix containing the recurrence
coefficients. We analyze the accuracy and stability of the algorithms with
numerical experiments on the ill-conditioned inverse eigenvalue problemshave
related to Kravchuk and Hahn polynomials, as well as on other better
conditioned examples.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [15] [Asymptotic analysis and design of linear elastic shell lattice metamaterials](https://arxiv.org/abs/2506.18910)
*Di Zhang,Ligang Liu*

Main category: math.AP

TL;DR: The paper introduces asymptotic directional stiffness (ADS) to analyze shell lattice metamaterials, proves a convergence theorem for ADS, explains high bulk modulus in TPMS-based lattices, and proposes an optimization framework.


<details>
  <summary>Details</summary>
Motivation: To understand how the geometry of the middle surface influences the effective stiffness of shell lattice metamaterials and to explain the high bulk modulus in TPMS-based structures.

Method: Uses Ciarlet's shell theory, introduces ADS, proves a convergence theorem, and develops a triangular-mesh-based discretization and shape optimization framework.

Result: Rigorous characterization of ADS, explanation for high bulk modulus in TPMS-based lattices, and validation through numerical experiments.

Conclusion: The theory and optimization framework effectively explain and enhance the stiffness properties of shell lattice metamaterials.

Abstract: We present an asymptotic analysis of shell lattice metamaterials based on
Ciarlet's shell theory, introducing a new metric--asymptotic directional
stiffness (ADS)--to quantify how the geometry of the middle surface governs the
effective stiffness. We prove a convergence theorem that rigorously
characterizes ADS and establishes its upper bound, along with necessary and
sufficient condition for achieving it. As a key result, our theory provides the
first rigorous explanation for the high bulk modulus observed in Triply
Periodic Minimal Surfaces (TPMS)-based shell lattices. To optimize ADS on
general periodic surfaces, we propose a triangular-mesh-based discretization
and shape optimization framework. Numerical experiments validate the
theoretical findings and demonstrate the effectiveness of the optimization
under various design objectives. Our implementation is available at
https://github.com/lavenklau/minisurf.

</details>


### [16] [Weighted nonlocal area functionals without the triangle inequality](https://arxiv.org/abs/2506.19048)
*Serena Dipierro,Enrico Valdinoci,Mary Vaughan*

Main category: math.AP

TL;DR: The paper analyzes a weighted nonlocal area functional where coefficients violate the triangle inequality, showing energy reduction by introducing a thin strip of an intermediate phase and proving Γ-convergence to a local functional as the fractional parameter approaches 1.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of nonlocal area functionals when the triangle inequality is violated, particularly in multi-phase transitions.

Method: Introduces a thin strip of an intermediate phase to reduce energy and analyzes Γ-convergence as the fractional parameter approaches 1.

Result: Demonstrates energy reduction via the thin strip and proves Γ-convergence to a local area functional with modified weights.

Conclusion: The nonlocal functional remains lower semicontinuous even without the triangle inequality, differing from classical models.

Abstract: We consider a weighted nonlocal area functional in which the coefficients do
not satisfy the triangle inequality. In the context of three phase transitions,
this means that one of the weights is larger than the sum of the other two, say
$$\sigma_{-1,1} > \sigma_{-1,0} + \sigma_{0,1}.$$ We show that the energy can
be reduced by covering interfaces between phases $-1$ and $1$ with a thin strip
of phase $0$.
  Moreover, as the fractional parameter $s\nearrow1$, we prove that the
nonlocal energies $\Gamma$-converge to a local area functional with different
weights.
  The functional structure of this long-range interaction model is conceptually
different from its classical counterpart, since the functional remains lower
semicontinuous, even in the absence of the triangle inequality.

</details>


### [17] [Asymptotic estimates for solutions of inhomogeneous non-divergence diffusion equations with drifts](https://arxiv.org/abs/2506.19059)
*Luan Hoang,Akif Ibragimov*

Main category: math.AP

TL;DR: The paper analyzes the long-time dynamics of nonlinear processes modeled by diffusion-transport PDEs in non-divergence form with drifts, focusing on asymptotic estimates and solution convergence under specific conditions.


<details>
  <summary>Details</summary>
Motivation: To understand the asymptotic behavior of solutions to nonlinear diffusion-transport PDEs with drifts, especially under inhomogeneous boundary conditions and varying drift behaviors.

Method: The study starts with a reduced linear problem to derive asymptotic estimates. For the nonlinear case, convergence is proven under balancing conditions. Techniques include an inhomogeneous Landis-typed Growth Lemma and careful barrier function optimization.

Result: Asymptotic estimates for solutions are obtained, and convergence is proven for the nonlinear problem under specific conditions. The model's robustness is demonstrated.

Conclusion: The paper provides rigorous results on the long-time behavior of solutions, highlighting the model's robustness and the effectiveness of the applied methods.

Abstract: We study the long-time dynamics of the nonlinear processes modeled by
diffusion-transport partial differential equations in non-divergence form with
drifts. The solutions are subject to some inhomogeneous Dirichlet boundary
condition. Starting with the reduced linear problem, we obtain the asymptotic
estimates for the solutions, as time $t\to\infty$, depending on the asymptotic
behavior of the forcing term and boundary data. These are established in both
cases when the drifts are uniformly bounded, and unbounded as $t\to\infty$. For
the nonlinear problem, we prove the convergence of the solutions under suitable
conditions that balance the growth of the nonlinear term with the decay of the
data. To take advantage of the diffusion in the non-divergence form, we prove
an inhomogeneous version of the Landis-typed Growth Lemma and apply it to
successive time-intervals. At each time step, the center for the barrier
function is selected carefully to optimize the contracting factor. Our rigorous
results show the robustness of the model.

</details>


### [18] [A frequency function approach to quantitative unique continuation for elliptic equations](https://arxiv.org/abs/2506.19130)
*Blair Davey*

Main category: math.AP

TL;DR: The paper studies unique continuation properties of solutions to second-order elliptic equations, focusing on quantitative forms and global properties, using novel frequency function techniques.


<details>
  <summary>Details</summary>
Motivation: To understand and quantify the unique continuation properties of solutions to generalized Schrödinger equations, addressing gaps in existing methods.

Method: Uses frequency functions instead of traditional Carleman estimates to analyze solutions to equations of the form $-\nabla \cdot (A \nabla u) + W \cdot \nabla u + V u = 0$.

Result: Establishes quantitative strong unique continuation properties and global results related to Landis' conjecture.

Conclusion: Novel frequency function techniques provide effective alternatives to Carleman estimates for analyzing unique continuation properties.

Abstract: We investigate the quantitative unique continuation properties of solutions
to second order elliptic equations with bounded lower order terms. In
particular, we establish quantitative forms of the strong unique continuation
property for solutions to generalized Schr\"odinger equations of the form $-
\nabla \cdot (A \nabla u) + W \cdot \nabla u + V u = 0$, where we assume that
$A$ is bounded, elliptic, symmetric, and Lipschitz, while $W$ and $V$ belong to
$L^\infty$. We also study the global unique continuation properties of
solutions to these equations, establishing results that are related to Landis'
conjecture concerning the optimal rate of decay at infinity. Versions of the
theorems in this article have been previously proved using Carleman estimates,
but here we present novel proof techniques that rely on frequency functions.

</details>


### [19] [Boundary layer profiles of positive solutions for logistic equation with sublinear nonlinearity on the boundary](https://arxiv.org/abs/2506.19237)
*Kenichiro Umezu*

Main category: math.AP

TL;DR: Analysis of the logistic elliptic equation with sublinear Neumann boundary conditions, focusing on the asymptotic profile of its unique positive solution as the parameter μ→∞.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of the logistic elliptic equation under sublinear Neumann boundary conditions and how the solution evolves as the boundary parameter μ becomes very large.

Method: Uses sub- and super-solutions along with a comparison principle to study the equation and derive the asymptotic profile.

Result: Identifies the asymptotic profile of the unique positive solution as μ→∞.

Conclusion: The study provides insights into the behavior of solutions for the logistic elliptic equation under large boundary parameters, contributing to the understanding of such PDEs.

Abstract: In this paper, we consider the logistic elliptic equation $-\Delta u = u-
u^{p}$ in a smooth bounded domain $\Omega \subset \mathbb{R}^{N}$, $N\geq2$,
equipped with the sublinear Neumann boundary condition $\frac{\partial
u}{\partial \nu} = \mu u^{q}$ on $\partial \Omega$, where $0<q<1<p$, and
$\mu\geq0$ is a parameter. With sub- and super-solutions and a comparison
principle for the equation, we analyze the asymptotic profile of a unique
positive solution for the equation as $\mu \to \infty$.

</details>


### [20] [Logarithmic Laplacian on General Riemannian Manifolds](https://arxiv.org/abs/2506.19311)
*Rui Chen*

Main category: math.AP

TL;DR: The paper introduces a Bochner integral formula for the logarithmic Laplacian on complete Riemannian manifolds, unifying its definition across compact and noncompact settings. It derives explicit formulas under Ricci bounds, compares spectral and heat kernel definitions, and provides precise estimates on real hyperbolic space.


<details>
  <summary>Details</summary>
Motivation: To generalize the logarithmic Laplacian beyond Euclidean space and provide a unified framework for its definition and analysis on Riemannian manifolds.

Method: Uses a Bochner integral formula, derives pointwise expressions under Ricci lower bounds, compares spectral and heat kernel definitions, and applies heat kernel asymptotics on real hyperbolic space.

Result: Explicit integral formulas for the logarithmic Laplacian, comparison of definitions, and precise kernel estimates on hyperbolic space, including Lp continuity.

Conclusion: The work establishes a robust framework for the logarithmic Laplacian on manifolds, with applications in spectral theory and analysis.

Abstract: We introduce, for the first time, a Bochner integral formula for the
logarithmic Laplacian on any complete Riemannian manifold. This unified
framework recovers the classical pointwise expression on Euclidean space and
allows us to define logarithmic Laplacian in both compact and noncompact
settings. Under a Ricci lower bound, we derive explicit pointwise integral
formulas for logarithmic Laplacian, analogous to those for the fractional
Laplacian. We further compare spectral versus heat kernel definitions of both
fractional and logarithmic Laplacians, showing that their discrepancy is
governed by the mass loss function and hence by stochastic completeness.
Finally, on real hyperbolic space we exploit sharp heat kernel asymptotics to
obtain precise estimates for the fractional and logarithmic kernels, identify
the optimal pointwise domain for logarithmic Laplacian and establish its Lp
continuity.

</details>


### [21] [Formation and construction of large variational shock waves for 1-D $n\times n$ quasilinear hyperbolic conservation systems](https://arxiv.org/abs/2506.19313)
*Huicheng Yin,Wanqing Zhu*

Main category: math.AP

TL;DR: The paper studies the blowup mechanism and singularity behaviors in 1D quasilinear strictly hyperbolic systems with large variational initial data, focusing on shock wave formation.


<details>
  <summary>Details</summary>
Motivation: To understand the geometric blowup mechanism and singularity behaviors in 1D quasilinear hyperbolic systems with large initial data.

Method: Analyzes the blowup point and singularity behaviors for a 1D $n\times n$ strictly hyperbolic system, focusing on genuinely nonlinear eigenvalues.

Result: Details the formation of large variational shock waves from the blowup point under specific conditions.

Conclusion: The study provides insights into the blowup mechanism and shock wave formation in quasilinear hyperbolic systems.

Abstract: In the paper [Li Jun, Xu Gang, Yin Huicheng, On the blowup mechanism of
smooth solutions to 1D quasilinear strictly hyperbolic systems with large
variational initial data, Nonlinearity 38 (2025), No.2, 025016], for the 1-D
$n\times n$ ($n\geqslant 3$) strictly hyperbolic system
$\partial_tv+F(v)\partial_xv=0$ with some classes of large variational initial
data $v(x, 0)$, the geometric blowup mechanism and the detailed singularity
behaviours of $\partial_{x,t}v$ near the blowup point are studied when the
$n\times n$ matrix $F(v)$ admits at least one genuinely nonlinear eigenvalue.
In this paper, we focus on the formation and construction of a large
variational shock wave from the blowup point for 1-D $n\times n$ quasilinear
hyperbolic conservation law system $\partial_tu+\partial_xf(u)=0$ when some
smooth simple wave solution is generic non-degenerate before the formation of
singularity and the corresponding eigenvalue is genuinely nonlinear.

</details>


### [22] [An improvement toward global boundedness in a fully parabolic chemotaxis with singular sensitivity in any dimension](https://arxiv.org/abs/2506.19318)
*Minh Le*

Main category: math.AP

TL;DR: The paper improves the global boundedness condition for solutions of a chemotaxis system, extending the range of the chemotactic sensitivity parameter χ.


<details>
  <summary>Details</summary>
Motivation: To extend the known condition for global boundedness of solutions in a chemotaxis system beyond χ < √(2/n).

Method: Introduces an energy functional F_λ(u,v) with specific parameters p, r, and λ to analyze the system.

Result: Shows existence of χ₀ > √(2/n) such that for χ ∈ (0,χ₀), solutions remain globally bounded.

Conclusion: The study successfully broadens the parameter range for global boundedness in the chemotaxis system.

Abstract: We are concerned with the following chemotaxis system \begin{equation*}
  \begin{cases}
  u_t = \Delta u -\chi \nabla \cdot \left ( \frac{u}{v} \nabla v\right ),
  v_t = \Delta v -u+ v,
  \end{cases} \end{equation*} in an open bounded domain with smooth boundary
$\Omega \subset \mathbb{R}^n$ with $n \geq 3$. It is known that solutions are
globally bounded when $0<\chi< \sqrt{ \frac{2}{n}}$. In this paper, by
considering the following energy functional \begin{align*}
  F_\lambda(u,v)= \int_\Omega \frac{u^p}{v^r} + \lambda \int_\Omega
\frac{|\nabla v|^{2p}}{v^{p+r}} + \int_\Omega v^{p-r} \end{align*} for $p>1$,
$r= \frac{p-1}{2}$, and $\lambda>0$, we improve this result by showing that
there exists $\chi_0>\sqrt{\frac{2}{n}}$ such that for any $\chi \in
(0,\chi_0)$, solutions exist globally and remain bounded.

</details>


### [23] [From kinetic mixtures to compressible two-phase flow: A BGK-type model and rigorous derivation](https://arxiv.org/abs/2506.19321)
*Seung Yeon Cho,Young-Pil Choi,Byung-Hoon Hwang,Sihyun Song*

Main category: math.AP

TL;DR: A BGK-type kinetic model for binary gas mixtures is proposed, derived into compressible two-phase Euler equations, and validated numerically.


<details>
  <summary>Details</summary>
Motivation: To provide a kinetic formulation for compressible two-phase fluid dynamics with species-dependent adiabatic exponents.

Method: The model uses an entropy minimization problem for relaxation and derives Euler equations via Chapman--Enskog expansion. The Euler limit is justified using the relative entropy method.

Result: Quantitative convergence estimates are established, and numerical experiments confirm the model's asymptotic preserving property.

Conclusion: The BGK model effectively converges to the isentropic two-phase Euler system, validated by numerical results.

Abstract: We propose a BGK-type kinetic model for a binary gas mixture, designed to
serve as a kinetic formulation of compressible two-phase fluid dynamics. The
model features species-dependent adiabatic exponents, and the relaxation
operator is constructed by solving an entropy minimization problem under
moments constraints. Starting from this model, we derive the compressible
two-phase Euler equations via a formal Chapman--Enskog expansion and identify
dissipative corrections of Navier--Stokes type. We then rigorously justify the
Euler limit using the relative entropy method, establishing quantitative
convergence estimates under appropriate regularity assumptions. Finally, we
present numerical experiments based on an implicit-explicit Runge--Kutta
method, which confirm the asymptotic preserving property and demonstrate the
convergence from the BGK model to the isentropic two-phase Euler system in the
hydrodynamic regime.

</details>


### [24] [PDE methods for extracting normal vector fields and distance functions of shapes](https://arxiv.org/abs/2506.19323)
*Takahiro Hasebe,Jun Masamune,Hiroshi Teramoto,Takayuki Yamada*

Main category: math.AP

TL;DR: The paper reviews methods for extracting geometric features like normal vector fields and signed distance functions from PDEs, including elliptic and heat equations.


<details>
  <summary>Details</summary>
Motivation: To summarize and compare recent techniques for deriving geometric properties of shapes using PDEs.

Method: Discusses methods based on elliptic equations (Yamada's and a generalized Varadhan's) and the heat equation.

Result: Provides a consolidated overview of PDE-based approaches for extracting normal vector fields and signed distance functions.

Conclusion: The reviewed methods offer effective ways to analyze geometric features of shapes using PDEs.

Abstract: Partial differential equations can be used for extracting geometric features
of shapes. This article summarizes recent methods to extract the normal vector
field from an elliptic equation proposed by Yamada and from the heat equation,
and also a method to extract the (signed) distance function from an elliptic
equation that generalizes Varadhan's in 1967.

</details>


### [25] [On inverse problem for generalized Kerr-type nonlinearities](https://arxiv.org/abs/2506.19447)
*Pu-Zhao Kow,Rulin Kuan*

Main category: math.AP

TL;DR: The paper extends Ikehata's enclosure method to reconstruct unknown inclusions in semilinear elliptic equations with nonanalytic nonlinearities, using an approximate solution from the linearized equation.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of reconstructing shapes in semilinear elliptic equations with nonanalytic nonlinearities, which are common in models like nonlinear optics and dissipative media.

Method: Constructs an approximate solution based on the linearized equation to adapt the enclosure method for nonanalytic nonlinearities.

Result: The method works for a broad class of semilinear elliptic equations, including Kerr-type and Ginzburg-Landau-type nonlinearities.

Conclusion: The proposed approach successfully extends the enclosure method to handle nonanalytic nonlinearities in shape reconstruction.

Abstract: We study the inverse problem of reconstructing the shape of unknown
inclusions in semilinear elliptic equations with nonanalytic nonlinearities, by
extending Ikehata's enclosure method to accommodate such nonlinear effects. To
address the analytical challenges, we construct an approximate solution based
on the linearized equation, enabling the enclosure method to operate in this
setting. The proposed method applies to a broad class of semilinear elliptic
equations with non-analytic nonlinearities, including representative examples
such as the Kerr-type nonlinearity, which appears in models of nonlinear
optics, and the Ginzburg-Landau-type nonlinearity, which models light
propagation in nonlinear dissipative media.

</details>


### [26] [A Nonlinear Nonlocal Problem for the Caputo Fractional Subdiffusion Equation](https://arxiv.org/abs/2506.19516)
*Ravshan Ashurov,Rajapboy Saparboyev,Navbahor Nuraliyeva*

Main category: math.AP

TL;DR: The paper studies a time-fractional subdiffusion equation with a nonlinear nonlocal initial condition, proving existence and uniqueness of a solution using the Green function and Banach fixed point theorem.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of solving a fractional subdiffusion equation with a nonlocal initial condition implicitly dependent on the final state.

Method: Uses the Caputo fractional derivative, Green function representation, and Banach fixed point theorem to establish solvability.

Result: Existence and uniqueness of a regular solution are proven, with uniform estimates for the Green function and analysis of the Lipschitz constant's impact.

Conclusion: The problem is solvable under certain conditions, with the Lipschitz constant playing a key role in solution existence.

Abstract: In this paper, we study a time-fractional subdiffusion equation with a
nonlinear nonlocal initial condition involving the unknown solution at the
final time. The considered problem is formulated using the Caputo fractional
derivative of order \(0 < \alpha < 1\), along with homogeneous Dirichlet
boundary conditions. The nonlocal initial condition is of the form \( u(x,0) =
g(x, u(x,T)) \), where \(g\) is a nonlinear function satisfying a Lipschitz
condition. The main challenge arises from the implicit dependence on the
unknown final state. Using an explicit representation of the solution in terms
of the Green function and applying the Banach fixed point theorem, we establish
the existence and uniqueness of a regular solution. We also provide uniform
estimates for the Green function and analyze the influence of the Lipschitz
constant on solvability.

</details>


### [27] [Revisiting the blow-up criterion and the maximal existence time for solutions of the parabolic-elliptic Keller-Segel system in 2d-Euclidean space](https://arxiv.org/abs/2506.19582)
*Patrick Maheux,Vittoria Pierfelice*

Main category: math.AP

TL;DR: The paper revisits blow-up criteria for the 2D PKS system with a consumption term, extending results to broader initial conditions and improving time estimates for solution existence.


<details>
  <summary>Details</summary>
Motivation: To generalize blow-up results for the PKS system beyond traditional initial conditions and refine maximal existence time estimates.

Method: Analyzes a one-parameter differential inequality for the second moment evolution and constructs blow-up solutions for non-zero initial data with finite second moment.

Result: Established blow-up for supercritical mass (M > 8π) under global assumptions on initial data's second moment, with improved upper bounds for solution existence time.

Conclusion: The study broadens understanding of blow-up in the PKS system and provides sharper time estimates, applicable to a wider class of initial conditions.

Abstract: In this paper, we revisit the blow-up criteria for the simplest
parabolic-elliptic (PKS) system in the 2D Euclidean space, including a
consumption term. In the supercritical mass case M > 8pi, and under an
additional global assumption on the second moment (or variance) of the initial
data, we establish blow-up results for a broader class of initial conditions
than those traditionally considered. We also derive improved upper bounds for
the maximal existence time of (PKS) solutions on the plane. These time
estimates are obtained through a sharp analysis of a one-parameter differential
inequality governing the evolution of the second moment of the (PKS) system. As
a consequence, for any given non-negative (non-zero) initial datum n1 with
finite second moment, we construct blow-up solutions of the (PKS) system.

</details>


### [28] [Controllability of Boussinesq flows driven by finite-dimensional and physically localized forces](https://arxiv.org/abs/2506.19764)
*Manuel Rissel*

Main category: math.AP

TL;DR: The paper demonstrates approximate controllability of Boussinesq flows on a 2D torus using finite-dimensional, spatially localized controls, addressing a question by Agrachev.


<details>
  <summary>Details</summary>
Motivation: To explore controllability in fluid PDEs, specifically Boussinesq flows, with localized finite-dimensional controls, building on prior work for Navier-Stokes.

Method: Uses tailored convection equations with time-periodic Euler flows for transport and mixing, employing Coron's return method for temperature control and buoyant force for velocity.

Result: Achieves controllability with explicit control space dimensions for velocity and temperature, given specific geometric conditions on the control region.

Conclusion: Provides the first example of controllability for fluid PDEs with finite-dimensional, localized controls, extending previous results.

Abstract: We show approximate controllability of Boussinesq flows in $\mathbb{T}^2 =
\mathbb{R}^2 / 2\pi\mathbb{Z}^2$ driven by finite-dimensional controls that are
supported in any fixed region $\omega \subset \mathbb{T}^2$. This addresses a
Boussinesq version of a question by Agrachev and provides a first example of
fluid PDEs controllable in that sense. In particular, we add in this context to
results obtained for the Navier-Stokes system by Agrachev-Sarychev (Comm. Math.
Phys. 265, 2006), where the controls are finite-dimensional but not localized
in physical space, and Nersesyan-Rissel (Comm. Pure Appl. Math. 78, 2025),
where physically localized controls admit for special $\omega$ a degenerate but
not finite-dimensional structure.
  For our proof, we study controllability properties of tailored convection
equations governed by time-periodic degenerately forced Euler flows that
provide a twofold geometric mechanism: transport of information through
$\omega$ versus non-stationary mixing effects transferring energy from
low-dimensional sources to higher frequencies. The temperature is then
controlled by using Coron's return method, while the velocity is mainly driven
by the buoyant force.
  When $\omega$ contains two cuts of the torus and a closed square of
side-length $L$, our approach yields explicit control spaces for the velocity
and temperature of dimensions $2 + 18\lceil2\pi/L\rceil^2 +
8\lceil2\pi/L\rceil^4$ and $2 + 8\lceil2\pi/L\rceil^2$, respectively.

</details>


### [29] [Singular limits of anisotropic weak solutions to compressible magnetohydrodynamics](https://arxiv.org/abs/2506.19784)
*Nicolas Besse,Christophe Cheverry*

Main category: math.AP

TL;DR: The paper rigorously justifies the reduced magnetohydrodynamic (RMHD) model for strongly magnetized and anisotropic plasmas, showing weak solutions converge to RMHD solutions.


<details>
  <summary>Details</summary>
Motivation: To validate the RMHD model, widely used in fusion, space, and astrophysical plasmas, for strongly anisotropic and magnetized plasmas.

Method: Study global weak solutions of viscous, resistive, barotropic compressible magnetohydrodynamic equations, analyzing their asymptotic behavior in periodic and whole-space domains.

Result: Anisotropic weak solutions converge to RMHD solutions, with incompressibility perpendicular to the magnetic field and compressibility parallel to it.

Conclusion: The RMHD model is rigorously justified for anisotropic plasmas, with distinct behaviors in perpendicular and parallel directions to the magnetic field.

Abstract: The aim is to justify rigorously the so-called reduced magnetohydrodynamic
model (abbreviated as RMHD), which is widely used in fusion, space and
astrophysical plasmas. Motivated by physics, the focus is on plasmas that are
simultaneously strongly magnetized and anisotropic. We consider conducting
fluids that can be described by viscous and resistive barotropic compressible
magnetohydrodynamic equations. The purpose is to study the asymptotic behaviour
of global weak solutions, which do exist, for strongly anisotropic plasmas such
as the large aspect ratio framework. We prove that such anisotropic weak
solutions converge to the weak solutions of the RMHD equations. Rigorous
justification of this limit is performed both in a periodic domain and in the
whole space. It turns out that the resulting system is incompressible only in
the perpendicular direction to the external strong magnetic field, whereas it
involves compressible features in the parallel direction. In order to pass to
the singular limit in the perpendicular direction we exploit, among others,
tools elaborated for proving the low Mach number limit of compressible fluid
flows such as the introduction of a fast oscillatory unitary group associated
to the dynamics of transverse fast magnetosonic waves. In the parallel
direction, we bring out compactness arguments and particular cancellations
coming from the structure of our equations.

</details>


### [30] [Multiplicity results for mixed local-nonlocal variable exponent problem involving singular and superlinear term](https://arxiv.org/abs/2506.19793)
*Shammi Malhotra,Ambesh Kumar Pandey,K. Sreenadh*

Main category: math.AP

TL;DR: The paper investigates quasilinear elliptic equations with local and nonlocal operators, variable exponents, singular nonlinearities, and a parameter λ. Using variational methods and the Nehari manifold, it proves the existence of two distinct solutions and establishes L∞-bounds.


<details>
  <summary>Details</summary>
Motivation: To address the existence of multiple solutions for quasilinear elliptic equations with complex nonlinearities and variable exponents, leveraging variational techniques and the Nehari manifold.

Method: Variational methods are applied by restricting the energy functional to subsets of the Nehari manifold. The topological index and fibering maps are used to analyze the manifold's splitting property.

Result: Existence of two distinct solutions is proven, and L∞-bounds for the solutions are established.

Conclusion: The study successfully demonstrates the existence of multiple solutions for the given class of equations, with rigorous bounds on solution behavior.

Abstract: In this paper, we study a class of quasilinear elliptic equations involving
both local and nonlocal operators with variable exponents. The problem exhibits
singular nonlinearities along with a subcritical superlinear growth term and a
parameter $\lambda$. We study the existence of multiple solutions with the help
of variational methods by restricting the associated energy functional on
appropriate subsets of the Nehari manifold. Using the topological index and the
structure of the fibering maps, we analyse a key splitting property of the
associated Nehari manifold. This decomposition allows us to establish the
existence of two distinct solutions. Additionally, we establish the
$L^\infty$-bound for the solutions.

</details>


### [31] [Pattern formation and film rupture in a two-dimensional thermocapillary thin-film model of the Bénard-Marangoni problem](https://arxiv.org/abs/2506.19795)
*Stefano Böhmer,Bastian Hilder,Jonas Jansen*

Main category: math.AP

TL;DR: The paper analyzes 2D stationary square and hexagonal patterns in a thermocapillary thin-film model, showing instability at a critical Marangoni number and global bifurcation of patterns.


<details>
  <summary>Details</summary>
Motivation: To understand pattern formation and stability in thin-film models derived from the Bénard-Marangoni problem.

Method: Linear stability analysis and analytic global bifurcation theory are used to study patterns and their properties.

Result: Square and hexagonal patterns bifurcate from flat profiles, with global curves extending beyond local bifurcation. Film rupture occurs if Marangoni number is bounded.

Conclusion: The study provides insights into pattern formation and rupture in thin-film models, supported by theoretical and numerical evidence.

Abstract: We study two-dimensional, stationary square and hexagonal patterns in the
thermocapillary deformational thin-film model for the fluid height
$h$\begin{equation*}
  \partial_t h+\nabla\cdot\left(h^3\left(\nabla\Delta h-g\nabla
h\right)+M\frac{h^2}{(1+h)^2}\nabla h\right)=0,\quad t>0,\quad
x\in\mathbb{R}^2,
  \end{equation*} that can be formally derived from the B\'enard-Marangoni
problem via a long-wave approximation. Using a linear stability analysis, we
show that the flat surface profile corresponding to the pure conduction state
destabilises at a critical Marangoni number $M^*$ via a conserved long-wave
instability. For any fixed absolute wave number $k_0$, we find that square and
hexagonal patterns bifurcate from the flat surface profile at $M=M^* + 4k_0^2$.
Using analytic global bifurcation theory, we show that the local bifurcation
curves can be extended to global curves of square and hexagonal patterns with
constant absolute wave number and mass. We exclude that the global bifurcation
curves are closed loops through a global bifurcation in cones argument, which
also establishes nodal properties for the solutions. Furthermore, assuming that
the Marangoni number is uniformly bounded on the bifurcation branch, we prove
that solutions exhibit film rupture, that is, their minimal height tends to
zero. This assumption is substantiated by numerical experiments.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [32] [Optimized auxiliary functions for robust mitigation of finite-size errors in periodic hybrid density functional theory](https://arxiv.org/abs/2506.19157)
*Stephen Jon Quiton,Juan D. F. Pottecher,Xin Xing,Martin Head-Gordon,Lin Lin*

Main category: physics.comp-ph

TL;DR: The paper addresses finite size error (FSE) in periodic systems by improving the singularity subtraction (SS) method for long-range Coulomb interactions, achieving rapid convergence and high accuracy in hybrid DFT calculations.


<details>
  <summary>Details</summary>
Motivation: The dominant source of FSE in periodic systems is the long-range Coulomb interaction, which causes slow convergence in reciprocal space integrals. The SS method aims to mitigate this error.

Method: The study evaluates the SS method using a single Gaussian auxiliary function, optimizes its width via fitting, and proposes new auxiliary functions for better performance.

Result: The method achieves millihartree-level accuracy in hybrid DFT calculations for semiconductors and insulators, even with sparse k-meshes and large basis sets.

Conclusion: The improved SS method effectively reduces FSE, enabling accurate calculations at the thermodynamic limit with minimal computational cost.

Abstract: When calculating properties of periodic systems at the thermodynamic limit
(TDL), the dominant source of finite size error (FSE) arises from the
long-range Coulomb interaction, and can manifest as a slowly converging
quadrature error when approximating an integral in the reciprocal space by a
finite sum. The singularity subtraction (SS) method offers a systematic
approach for reducing this quadrature error and thus the FSE. In this work, we
first investigate the performance of the SS method in the simplest setting,
aiming at reducing the FSE in exact exchange calculations by subtracting the
Coulomb contribution with a single, adjustable Gaussian auxiliary function. We
demonstrate that a simple fitting method can robustly estimate the optimal
Gaussian width and leads to rapid convergence toward the TDL. Furthermore, we
suggest new forms of the auxiliary function, whose optimal parameters could
also be determined through least-squares fitting. For a range of semiconductors
and insulators, the proposed auxiliary functions achieve robust,
millihartree-level accuracy in hybrid density functional theory calculations,
including cases with sparse k-meshes and large basis sets.

</details>


### [33] [From Brownian dynamics to Poisson-Nernst-Planck equations: multi-resolution simulations of ions](https://arxiv.org/abs/2506.19738)
*Jinyuan Zhang,Radek Erban*

Main category: physics.comp-ph

TL;DR: A multi-resolution method combines microscopic Brownian dynamics and macroscopic Poisson-Nernst-Planck equations for simulating charged particles.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between detailed microscopic models and coarse macroscopic descriptions for ion dynamics.

Method: Derived macroscopic PDEs from Brownian dynamics, then developed a hybrid multi-resolution simulation approach.

Result: Demonstrated the strengths and limitations of microscopic, macroscopic, and hybrid methods using Na+ and Cl- ions.

Conclusion: The multi-resolution approach effectively balances detail and computational efficiency for ion simulations.

Abstract: Starting with a microscopic (individual-based) Brownian dynamics model of
charged particles (ions), its macroscopic description is derived as a system of
partial differential equations that govern the evolution of ion concentrations
in space and time. The macroscopic equations are obtained in the form of the
Poisson-Nernst-Planck system. A multi-resolution method for simulating charged
particles is then developed, combining the detailed Brownian dynamics model in
a part of the computational domain with coarser macroscopic equations in the
remainder. The strengths, limitations, and applicability of microscopic,
macroscopic, and multi-resolution simulation approaches are demonstrated
through an illustrative model comprising a system of Na$^+$ and Cl$^-$ ions.

</details>


### [34] [Efficient calculation of thermodynamic properties of baryon-rich QCD matter from heavy-ion transport models](https://arxiv.org/abs/2506.19766)
*Lipei Du*

Main category: physics.comp-ph

TL;DR: The MATRICS framework optimizes heavy-ion collision simulations by modular workflows, dynamic grid resolution, and particle clustering, enhancing computational efficiency and physical accuracy.


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency and accuracy of simulating heavy-ion collisions by addressing computational challenges in particle generation, grid construction, and tensor calculations.

Method: Modular workflows for parallel execution, dynamic grid resolution adjustment, and clustering particles into super-particles.

Result: Efficient calculation of energy-momentum tensor and charge currents, suitable for thermodynamic studies and hydrodynamic initial conditions.

Conclusion: MATRICS is a powerful tool for large-scale simulations, advancing the study of QCD matter at high baryon densities.

Abstract: This study presents the MATRICS framework (Modeling Aggregated Tensors for
Relativistic Ion Collision Simulations) that implements modular workflows to
enable parallel execution of particle generation, grid construction, and tensor
calculations for heavy-ion collisions. It introduces an efficient approach to
calculating the space-time distribution of the energy-momentum tensor and
charge currents from discrete particles generated by transport models. By
dynamically adjusting grid resolution based on particle density and clustering
particles into representative super-particles, MATRICS optimizes computational
efficiency while maintaining high physical accuracy. The framework can also
provide a thermodynamic background for electromagnetic thermal emission
calculations or serve as initial conditions for hydrodynamic evolution. It
offers a powerful tool for exploring the thermodynamic properties of QCD matter
at high baryon densities, making it well-suited for large-scale simulations in
heavy-ion collision studies.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [35] [Electromagnetic and Centrifugal Effects on Plasma Acceleration in the Magnetic Nozzle](https://arxiv.org/abs/2506.19647)
*A. I. Smolyakov,A. Sabo,S. I. Krasheninnikov,P. N. Yushmanov*

Main category: physics.plasm-ph

TL;DR: Analysis of plasma acceleration in magnetic nozzles, focusing on electromagnetic effects and centrifugal forces, showing transonic and trans-Alfvenic flow solutions.


<details>
  <summary>Details</summary>
Motivation: To understand plasma acceleration in magnetic nozzles and open mirrors, emphasizing electromagnetic effects and rotation.

Method: Analyzed intrinsic coupling of azimuthal rotation and magnetic field, derived stationary solutions for transonic and trans-Alfvenic flows.

Result: Demonstrated additional acceleration via energy conversion, with flow velocities exceeding Alfven velocity; stationary solutions are robust.

Conclusion: Stationary equilibrium flows are stable and converge in time-dependent simulations, validating MHD theory predictions.

Abstract: Plasma flow and acceleration in the converging-diverging magnetic field
configuration, such as magnetic nozzle in electric propulsion and open magnetic
mirrors for fusion applications are considered. This work analyses plasma
acceleration in the magnetic nozzle with an emphasis on the electromagnetic
effects and centrifugal forces due to plasma rotation. Intrinsic coupling of
the azimuthal rotation and azimuthal magnetic field is analyzed, and additional
plasma acceleration due to the conversion of the energy of the azimuthal
magnetic field and azimuthal rotation is demonstrated. For large expansion in
the diverging magnetic field plasma flow velocities may approach and exceed the
Alfven velocity. In these regimes, stationary solutions for the transonic and
trans-Alfvenic flows have been obtained that demonstrate the existence of the
unique regular solution passing through all critical points within the MHD
theory, i. e. the points where the plasma flow is equal to the signal
velocities of the MHD modes: slow and fast magnetohydrodynamic waves and Alfven
wave. The time-dependent initial value simulations show that stationary
equilibrium flows are robust and stable, so that time-dependent solutions
converge toward stationary solutions.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [36] [Deep Electromagnetic Structure Design Under Limited Evaluation Budgets](https://arxiv.org/abs/2506.19384)
*Shijian Zheng,Fangxiao Jin,Shuhai Zhang,Quan Xue,Mingkui Tan*

Main category: cs.LG

TL;DR: Progressive Quadtree-based Search (PQS) is introduced for efficient EMS design, reducing evaluation costs by 75-85% compared to generative methods.


<details>
  <summary>Details</summary>
Motivation: High-dimensional EMS design is challenging due to expensive evaluations and data-intensive methods. PQS aims to address these constraints.

Method: PQS uses a quadtree-based hierarchical representation for progressive search and a consistency-driven sample selection mechanism to balance exploration and exploitation.

Result: PQS outperforms baselines, achieving satisfactory designs under limited budgets and cutting evaluation costs significantly.

Conclusion: PQS is a practical solution for EMS design, saving time and resources while maintaining performance.

Abstract: Electromagnetic structure (EMS) design plays a critical role in developing
advanced antennas and materials, but remains challenging due to
high-dimensional design spaces and expensive evaluations. While existing
methods commonly employ high-quality predictors or generators to alleviate
evaluations, they are often data-intensive and struggle with real-world scale
and budget constraints. To address this, we propose a novel method called
Progressive Quadtree-based Search (PQS). Rather than exhaustively exploring the
high-dimensional space, PQS converts the conventional image-like layout into a
quadtree-based hierarchical representation, enabling a progressive search from
global patterns to local details. Furthermore, to lessen reliance on highly
accurate predictors, we introduce a consistency-driven sample selection
mechanism. This mechanism quantifies the reliability of predictions, balancing
exploitation and exploration when selecting candidate designs. We evaluate PQS
on two real-world engineering tasks, i.e., Dual-layer Frequency Selective
Surface and High-gain Antenna. Experimental results show that our method can
achieve satisfactory designs under limited computational budgets, outperforming
baseline methods. In particular, compared to generative approaches, it cuts
evaluation costs by 75-85%, effectively saving 20.27-38.80 days of product
designing cycle.

</details>


### [37] [Beyond Static Models: Hypernetworks for Adaptive and Generalizable Forecasting in Complex Parametric Dynamical Systems](https://arxiv.org/abs/2506.19609)
*Pantelis R. Vlachas,Konstantinos Vlachas,Eleni Chatzi*

Main category: cs.LG

TL;DR: PHLieNet introduces a framework to learn global mappings for handling parametric variability in dynamical systems, enabling smooth transitions and generalization across parameter regimes.


<details>
  <summary>Details</summary>
Motivation: Parametric variability in dynamical systems leads to diverse model behaviors, challenging generalization across parameter regimes.

Method: PHLieNet learns a global mapping from parameter space to a nonlinear embedding and maps this embedding to weights of a dynamics propagation network, using a hypernetwork to modulate a base network.

Result: PHLieNet outperforms state-of-the-art baselines in short-term forecasting and capturing long-term dynamical features, generalizing to unseen dynamics.

Conclusion: PHLieNet effectively addresses parametric variability, enabling unified modeling of dynamical systems across diverse parameterizations.

Abstract: Dynamical systems play a key role in modeling, forecasting, and
decision-making across a wide range of scientific domains. However, variations
in system parameters, also referred to as parametric variability, can lead to
drastically different model behavior and output, posing challenges for
constructing models that generalize across parameter regimes. In this work, we
introduce the Parametric Hypernetwork for Learning Interpolated Networks
(PHLieNet), a framework that simultaneously learns: (a) a global mapping from
the parameter space to a nonlinear embedding and (b) a mapping from the
inferred embedding to the weights of a dynamics propagation network. The
learned embedding serves as a latent representation that modulates a base
network, termed the hypernetwork, enabling it to generate the weights of a
target network responsible for forecasting the system's state evolution
conditioned on the previous time history. By interpolating in the space of
models rather than observations, PHLieNet facilitates smooth transitions across
parameterized system behaviors, enabling a unified model that captures the
dynamic behavior across a broad range of system parameterizations. The
performance of the proposed technique is validated in a series of dynamical
systems with respect to its ability to extrapolate in time and interpolate and
extrapolate in the parameter space, i.e., generalize to dynamics that were
unseen during training. In all cases, our approach outperforms or matches
state-of-the-art baselines in both short-term forecast accuracy and in
capturing long-term dynamical features, such as attractor statistics.

</details>


### [38] [On the algorithmic construction of deep ReLU networks](https://arxiv.org/abs/2506.19104)
*Daan Huybrechs*

Main category: cs.LG

TL;DR: The paper explores neural networks as algorithms, constructing exact solutions like sorting, and analyzes their recursive, parallel nature and limitations compared to conventional algorithms.


<details>
  <summary>Details</summary>
Motivation: Understanding what neural networks can represent mathematically, beyond training on data, by treating them as algorithms.

Method: Constructive programming of neural networks (e.g., exact sorting) and analyzing their properties, such as recursion, parallelism, and depth.

Result: Neural networks as algorithms are recursive and parallel, with deep networks outperforming shallow ones, but constrained by continuity and depth limits.

Conclusion: Constructive neural networks offer insights into their expressivity, showing potential for exact solutions but with inherent limitations.

Abstract: It is difficult to describe in mathematical terms what a neural network
trained on data represents. On the other hand, there is a growing mathematical
understanding of what neural networks are in principle capable of representing.
Feedforward neural networks using the ReLU activation function represent
continuous and piecewise linear functions and can approximate many others. The
study of their expressivity addresses the question: which ones? Contributing to
the available answers, we take the perspective of a neural network as an
algorithm. In this analogy, a neural network is programmed constructively,
rather than trained from data. An interesting example is a sorting algorithm:
we explicitly construct a neural network that sorts its inputs exactly, not
approximately, and that, in a sense, has optimal computational complexity if
the input dimension is large. Such constructed networks may have several
billion parameters. We construct and analyze several other examples, both
existing and new. We find that, in these examples, neural networks as
algorithms are typically recursive and parallel. Compared to conventional
algorithms, ReLU networks are restricted by having to be continuous. Moreover,
the depth of recursion is limited by the depth of the network, with deep
networks having superior properties over shallow ones.

</details>


### [39] [High precision PINNs in unbounded domains: application to singularity formulation in PDEs](https://arxiv.org/abs/2506.19243)
*Yixuan Wang,Ziming Liu,Zongyi Li,Anima Anandkumar,Thomas Y. Hou*

Main category: cs.LG

TL;DR: The paper explores high-precision training of PINNs for unbounded domains, focusing on singularity formation in PDEs. A modular approach is proposed, and results show improved precision and efficiency for 1D and 2D cases.


<details>
  <summary>Details</summary>
Motivation: To enhance the precision of PINNs for studying singularities in PDEs, particularly in unbounded domains, and to provide rigorous numerical solutions for such problems.

Method: A modularized approach involving choices of neural network ansatz, sampling strategy, and optimization algorithm, combined with computer-assisted proofs and PDE analysis.

Result: Achieved high-precision solutions for 1D Burgers equation and a 2D Boussinesq equation, with loss reduction compared to prior work.

Conclusion: The framework is effective for high-precision solutions in PDEs, with potential for extending to machine precision in higher dimensions.

Abstract: We investigate the high-precision training of Physics-Informed Neural
Networks (PINNs) in unbounded domains, with a special focus on applications to
singularity formulation in PDEs. We propose a modularized approach and study
the choices of neural network ansatz, sampling strategy, and optimization
algorithm. When combined with rigorous computer-assisted proofs and PDE
analysis, the numerical solutions identified by PINNs, provided they are of
high precision, can serve as a powerful tool for studying singularities in
PDEs. For 1D Burgers equation, our framework can lead to a solution with very
high precision, and for the 2D Boussinesq equation, which is directly related
to the singularity formulation in 3D Euler and Navier-Stokes equations, we
obtain a solution whose loss is $4$ digits smaller than that obtained in
\cite{wang2023asymptotic} with fewer training steps. We also discuss potential
directions for pushing towards machine precision for higher-dimensional
problems.

</details>


<div id='math.CV'></div>

# math.CV [[Back]](#toc)

### [40] [CR Paneitz operator and embeddability](https://arxiv.org/abs/2506.19215)
*Yuya Takeuchi*

Main category: math.CV

TL;DR: Survey of global embeddability of closed strictly pseudoconvex CR manifolds and the CR Paneitz operator.


<details>
  <summary>Details</summary>
Motivation: Explore connections between CR manifold embeddability and the CR Paneitz operator.

Method: Brief survey of recent developments.

Result: Highlights key findings linking embeddability and the operator.

Conclusion: The survey underscores the importance of the CR Paneitz operator in understanding CR manifold embeddability.

Abstract: In this article, we give a brief survey of recent developments on relations
between global embeddability of a closed strictly pseudoconvex CR manifold and
the CR Paneitz operator.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [41] [Operator Forces For Coarse-Grained Molecular Dynamics](https://arxiv.org/abs/2506.19628)
*Leon Klein,Atharva Kelkar,Aleksander Durumeric,Yaoyi Chen,Frank Noé*

Main category: physics.chem-ph

TL;DR: The paper introduces flow-based kernels to improve machine-learned coarse-graining (MLCG) force fields, reducing local distortions while maintaining global accuracy, using only configurational data.


<details>
  <summary>Details</summary>
Motivation: Traditional force matching in MLCG requires extensive labeled force data, which is often unavailable. Noise-based kernels address this but introduce local distortions.

Method: The authors propose normalizing flow-based kernels to refine MLCG force fields, eliminating the need for force labels and reducing distortions.

Result: Flow-based kernels successfully generate accurate CG forces from configurational samples, demonstrated on small proteins.

Conclusion: Flow-based kernels offer a robust solution for MLCG, enabling high-quality force fields without force labels and minimizing local errors.

Abstract: Coarse-grained (CG) molecular dynamics simulations extend the length and time
scale of atomistic simulations by replacing groups of correlated atoms with CG
beads. Machine-learned coarse-graining (MLCG) has recently emerged as a
promising approach to construct highly accurate force fields for CG molecular
dynamics. However, the calibration of MLCG force fields typically hinges on
force matching, which demands extensive reference atomistic trajectories with
corresponding force labels. In practice, atomistic forces are often not
recorded, making traditional force matching infeasible on pre-existing
datasets. Recently, noise-based kernels have been introduced to adapt force
matching to the low-data regime, including situations in which reference
atomistic forces are not present. While this approach produces force fields
which recapitulate slow collective motion, it introduces significant local
distortions due to the corrupting effects of the noise-based kernel. In this
work, we introduce more general kernels based on normalizing flows that
substantially reduce these local distortions while preserving global
conformational accuracy. We demonstrate our method on small proteins, showing
that flow-based kernels can generate high-quality CG forces solely from
configurational samples.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [42] [Time-Sensitive Importance Splitting](https://arxiv.org/abs/2506.19568)
*Gabriel Dengler,Carlos E. Budde,Laura Carnevali,Arnd Hartmanns*

Main category: cs.LO

TL;DR: The paper proposes a time-sensitive importance function for rare event simulation in non-Markovian models, addressing prior knowledge limitations.


<details>
  <summary>Details</summary>
Motivation: Current methods for rare event simulation in non-Markovian models are limited by the need for prior knowledge or timed behavior information.

Method: Extends importance splitting with a time-sensitive importance function using backwards reachability search, considering timer bounds to guide path generation.

Result: A prototype implementation in the Modest Toolset shows potential in estimating rare event probabilities for reliability engineering.

Conclusion: The approach effectively addresses limitations in rare event simulation for non-Markovian models.

Abstract: State-of-the-art methods for rare event simulation of non-Markovian models
face practical or theoretical limits if observing the event of interest
requires prior knowledge or information on the timed behavior of the system. In
this paper, we attack both limits by extending importance splitting with a
time-sensitive importance function. To this end, we perform backwards
reachability search from the target states, considering information about the
lower and upper bounds of the active timers in order to steer the generation of
paths towards the rare event. We have developed a prototype implementation of
the approach for input/output stochastic automata within the Modest Toolset.
Preliminary experiments show the potential of the approach in estimating rare
event probabilities for an example from reliability engineering.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [43] [Global regularity of the value function in a stopper vs. singular-controller game](https://arxiv.org/abs/2506.19129)
*Andrea Bovo,Alessandro Milazzo*

Main category: math.OC

TL;DR: The paper analyzes zero-sum stochastic games between a stopper and a singular-controller, focusing on the regularity of the value function and its implications for optimal strategies.


<details>
  <summary>Details</summary>
Motivation: To understand the regularity properties of the value function in stochastic games and their impact on free-boundaries and optimal strategies.

Method: The study uses parabolic variational inequalities with spatial-derivative and obstacle constraints, analyzing the value function's smoothness and discontinuities.

Result: The value function is $C^1$ in $[0,T)\times\mathcal{O}$, with continuous second-order derivatives except for a jump across the stopping boundary.

Conclusion: The regularity of the value function aids in exploring free-boundaries and optimal strategies, advancing the understanding of stochastic games.

Abstract: We study a class of zero-sum stochastic games between a stopper and a
singular-controller, previously considered in [Bovo and De Angelis (2025)]. The
underlying singularly-controlled dynamics takes values in
$\mathcal{O}\subseteq\mathbb{R}$. The problem is set on a finite time-horizon
and is connected to a parabolic variational inequality of min-max type with
spatial-derivative and obstacle constraints.
  We show that the value function of the problem is of class $C^1$ in the whole
domain $[0,T)\times\mathcal{O}$ and that the second-order spatial derivative
and the second-order mixed derivative are continuous everywhere except for a
(potential) jump across a non-decreasing curve (the stopping boundary of the
game). The latter discontinuity is a natural consequence of the partial
differential equation associated to the problem. Beyond its intrinsic
analytical value, such a regularity for the value function is a stepping stone
for further exploring the structure and properties of the free-boundaries of
the stochastic game, which in turn determine the optimal strategies of the
players.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [44] [Efficient Berry Phase Calculation via Adaptive Variational Quantum Computing Approach](https://arxiv.org/abs/2506.19150)
*Martin Mootz,Yong-Xin Yao*

Main category: quant-ph

TL;DR: A quantum computing method for calculating Berry phases in topological Hamiltonians using adaptive variational algorithms, tested on Fermi-Hubbard chains.


<details>
  <summary>Details</summary>
Motivation: To efficiently simulate Berry phases in topological materials, especially in strongly correlated systems, leveraging quantum computing.

Method: Uses cyclic adiabatic evolution and adaptive variational quantum algorithms for state preparation and evolution, optimizing circuit efficiency.

Result: Achieves precise Berry phase simulations with circuit depths up to 106 layers (noninteracting) and 279 layers (interacting), showing robustness across parameters.

Conclusion: Demonstrates the potential of adaptive variational quantum algorithms for quantum simulations of topological materials and geometric phases.

Abstract: We present a quantum computing approach for efficiently calculating the Berry
phase in topological Hamiltonians. Our method leverages cyclic adiabatic
evolution of the Hamiltonian and employs adaptive variational quantum
algorithms for state preparation and evolution, optimizing circuit efficiency
while maintaining high accuracy. We benchmark our approach on dimerized
Fermi-Hubbard chains with four sites, demonstrating precise Berry phase
simulations in both noninteracting and interacting regimes. Our results show
that circuit depths reach up to 106 layers for noninteracting systems and
increase to 279 layers for interacting systems due to added complexity.
Additionally, we demonstrate the robustness of our scheme across a wide range
of parameters governing adiabatic evolution and variational algorithm. These
findings highlight the potential of adaptive variational quantum algorithms for
advancing quantum simulations of topological materials and computing geometric
phases in strongly correlated systems.

</details>


### [45] [Dynamics of discrete spacetimes with Quantum-enhanced Markov Chain Monte Carlo](https://arxiv.org/abs/2506.19538)
*Stuart Ferguson,Arad Nasiri,Petros Wallden*

Main category: quant-ph

TL;DR: A quantum algorithm for sampling causal sets is introduced, offering a super-quadratic quantum advantage over classical methods by leveraging quantum-enhanced Markov chain Monte Carlo (QeMCMC) with constraints.


<details>
  <summary>Details</summary>
Motivation: To practically realize quantum computational advantages in Causal Set Theory, a discrete approach to quantum gravity, by improving classical sampling methods.

Method: Adapts QeMCMC with a constraint term in the Hamiltonian and derives a qubit Hamiltonian for the Benincasa-Dowker action to sample causal sets.

Result: Achieves super-quadratic quantum scaling advantage and shows greater potential than unconstrained QeMCMC under certain conditions.

Conclusion: The quantum algorithm significantly advances the practical application of quantum computing in Causal Set Theory, outperforming classical methods.

Abstract: Quantum algorithms offer the potential for significant computational
advantages; however, in many cases, it remains unclear how these advantages can
be practically realized. Causal Set Theory is a discrete, Lorentz-invariant
approach to quantum gravity which may be well positioned to benefit from
quantum computing. In this work, we introduce a quantum algorithm that
investigates the dynamics of causal sets by sampling the space of causal sets,
improving on classical methods. Our approach builds on the quantum-enhanced
Markov chain Monte Carlo technique developed by Layden et al. [Nature 619, 282
(2023)], adapting it to sample from the constrained spaces required for
application. This is done by adding a constraint term to the Hamiltonian of the
system. A qubit Hamiltonian representing the Benincasa-Dowker action (the
causal set equivalent of the Einstein-Hilbert action) is also derived and used
in the algorithm as the problem Hamiltonian. We achieve a super-quadratic
quantum scaling advantage and, under some conditions, demonstrate a greater
potential compared to classical approaches than previously observed in
unconstrained QeMCMC implementations.

</details>


### [46] [Numerical solution of quantum Landau-Lifshitz-Gilbert equation](https://arxiv.org/abs/2506.19594)
*Vahid Azimi-Mousolou,Davoud Mirzaei*

Main category: quant-ph

TL;DR: A quantum generalization of the classical LLG equation is proposed, and a robust numerical method is developed to simulate quantum many-body spin systems, revealing rich quantum behavior like entanglement.


<details>
  <summary>Details</summary>
Motivation: The classical LLG equation lacks quantum effects like entanglement, limiting its use for quantum phenomena. A quantum version is needed.

Method: A numerical methodology is developed for the quantum LLG equation, preserving its mathematical and physical properties, and applied to many-body quantum spin systems.

Result: The method demonstrates rich quantum behavior, including long-time entangled states, in topological quantum spin systems.

Conclusion: This approach enables reliable quantum magnetism simulations beyond classical approximations, promising new discoveries.

Abstract: The classical Landau-Lifshitz-Gilbert (LLG) equation has long served as a
cornerstone for modeling magnetization dynamics in magnetic systems, yet its
classical nature limits its applicability to inherently quantum phenomena such
as entanglement and nonlocal correlations. Inspired by the need to incorporate
quantum effects into spin dynamics, recently a quantum generalization of the
LLG equation is proposed [Phys. Rev. Lett. 133, 266704 (2024)] which captures
essential quantum behavior in many-body systems. In this work, we develop a
robust numerical methodology tailored to this quantum LLG framework that not
only handles the complexity of quantum many-body systems but also preserves the
intrinsic mathematical structures and physical properties dictated by the
equation. We apply the proposed method to a class of many-body quantum spin
systems, which host topological states of matter, and demonstrate rich quantum
behavior, including the emergence of long-time entangled states. This approach
opens a pathway toward reliable simulations of quantum magnetism beyond
classical approximations, potentially leading to new discoveries.

</details>


### [47] [Noncontextual Pauli Hamiltonians](https://arxiv.org/abs/2506.19778)
*Alexis Ralli,Tim Weaving,Peter J. Love*

Main category: quant-ph

TL;DR: The paper rigorously analyzes noncontextual Pauli Hamiltonians, showing they describe more physical interactions than diagonal Hamiltonians and admit efficient classical descriptions of eigenspaces.


<details>
  <summary>Details</summary>
Motivation: To explore noncontextual subtheories of quantum mechanics, which are fundamental and practically useful, especially in variational quantum algorithms.

Method: The study involves proving properties of noncontextual Pauli Hamiltonians, including their composition, eigenspace structure, and stabilizer rank scaling.

Result: Noncontextual Hamiltonians describe more interactions, have efficiently describable eigenspaces, and exhibit linear stabilizer rank scaling. Degeneracies in their eigenspectrum are also identified.

Conclusion: The work introduces a new class of efficiently simulatable states, advancing the field of noncontextual quantum subtheories.

Abstract: Contextuality is a key feature of quantum mechanics, and identification of
noncontextual subtheories of quantum mechanics is of both fundamental and
practical importance. Recently, noncontextual Pauli Hamiltonians have been
defined in the setting of variational quantum algorithms. In this work we
rigorously establish a number of properties of noncontextual Pauli
Hamiltonians. We prove that these Hamiltonians can be composed of more Pauli
operators than diagonal Hamiltonians. This establishes that noncontextual
Hamiltonians are able to describe a greater number of physical interactions. We
then show that the eigenspaces admit an efficient classical description. We
analyse the eigenspace of these Hamiltonians and prove that for every
eigenvalue there exists an associated eigenvector whose stabilizer rank scales
linearly with the number of qubits. We prove that further structure in these
Hamiltonians allow us to derive where degeneracies in the eigenspectrum can
arise. We thus open the field to a new class of efficiently simulatable states.

</details>


<div id='physics.acc-ph'></div>

# physics.acc-ph [[Back]](#toc)

### [48] [Laser Wakefield Acceleration Driven by a Discrete Flying Focus](https://arxiv.org/abs/2506.19824)
*Jacob R. Pierce,Kyle G. Miller,Fei Li,John P. Palastro,Warren B. Mori*

Main category: physics.acc-ph

TL;DR: The paper proposes a discrete flying focus method for laser wakefield acceleration (LWFA) to achieve higher energy gain in a single stage, reducing the need for multiple stages.


<details>
  <summary>Details</summary>
Motivation: Overcoming limitations of multi-stage LWFA, such as alignment issues and lower gradients, to enable TeV-scale lepton colliders.

Method: Using a sequence of laser pulses with staggered focal points and delays to drive a plasma wave, maintaining a near-constant accelerating gradient over extended distances.

Result: Simulations show a 150 J discrete flying focus can transfer 40 GeV per electron to a 50-pC beam in a single 30-cm stage.

Conclusion: The discrete flying focus offers a promising solution for high-energy LWFA, reducing complexity and improving efficiency.

Abstract: Laser wakefield acceleration (LWFA) may enable the next generation of
TeV-scale lepton colliders. Reaching such energies will likely require multiple
LWFA stages to overcome limitations on the energy gain achievable in a single
stage. The use of stages, however, introduces challenges such as alignment,
adiabatic matching between stages, and a lower average accelerating gradient.
Here, we propose a discrete flying focus that can deliver higher energy gain in
a single stage, thereby reducing the number of stages required for a target
energy. A sequence of laser pulses with staggered focal points and delays
drives a plasma wave in which an electron beam experiences a near-constant
accelerating gradient over distances beyond those attainable with a
conventional pulse. Simulations demonstrate that a discrete flying focus with a
total energy of 150 J can transfer 40 GeV per electron to a 50-pC beam in a
single 30-cm stage, corresponding to 50 dephasing lengths.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [49] [McGehee blowup for Lagrangian systems and instability of equilibria](https://arxiv.org/abs/2506.19135)
*J. M. Burgos*

Main category: math.DS

TL;DR: Total instability is generically proven in real analytic electromagnetic Lagrangian systems under weak magnetism, using McGehee blowup adaptation and new instability criteria.


<details>
  <summary>Details</summary>
Motivation: To demonstrate that total instability is a common feature in electromagnetic Lagrangian systems, especially under weak magnetism conditions.

Method: Adaptation of the McGehee blowup technique for these systems and introduction of new criteria for total instability.

Result: Total instability is shown to be generic in the specified systems, with new criteria provided for broader cases.

Conclusion: The findings confirm the prevalence of total instability in such systems and offer tools for analyzing instability in both generic and non-generic scenarios.

Abstract: We prove that total instability is a generic phenomenon in the real analytic
class of electromagnetic Lagrangian systems under a weak magnetism hypothesis.
The main object in the proof is an adaptation of the McGehee blowup for these
systems. Together with this result, new criteria for total instability are
introduced for both generic and non-generic cases.

</details>
