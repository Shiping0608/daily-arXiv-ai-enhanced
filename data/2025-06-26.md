<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 11]
- [math.AP](#math.AP) [Total: 12]
- [physics.comp-ph](#physics.comp-ph) [Total: 3]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 6]
- [physics.optics](#physics.optics) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [nlin.PS](#nlin.PS) [Total: 1]
- [nlin.CD](#nlin.CD) [Total: 1]
- [cs.AI](#cs.AI) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [nucl-ex](#nucl-ex) [Total: 1]
- [math.OC](#math.OC) [Total: 2]
- [math.FA](#math.FA) [Total: 2]
- [math.PR](#math.PR) [Total: 2]
- [quant-ph](#quant-ph) [Total: 1]
- [math.DG](#math.DG) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [A Hybrid High-Order Method for the Gross--Pitaevskii Eigenvalue Problem](https://arxiv.org/abs/2506.19944)
*Moritz Hauck,Yizhou Liang*

Main category: math.NA

TL;DR: A hybrid high-order method for approximating the ground state of the nonlinear Gross-Pitaevskii problem, providing optimal convergence rates and guaranteed lower energy bounds without post-processing.


<details>
  <summary>Details</summary>
Motivation: To improve accuracy and efficiency in approximating the ground state of the nonlinear Gross-Pitaevskii problem, avoiding the limitations of classical conforming methods.

Method: A hybrid high-order method is introduced, which directly provides lower energy bounds without post-processing.

Result: Optimal convergence rates for ground state, eigenvalue, and energy approximations, along with guaranteed lower energy bounds.

Conclusion: The proposed method offers more accurate and practical lower energy bounds compared to previous approaches.

Abstract: We introduce a hybrid high-order method for approximating the ground state of
the nonlinear Gross--Pitaevskii eigenvalue problem. Optimal convergence rates
are proved for the ground state approximation, as well as for the associated
eigenvalue and energy approximations. Unlike classical conforming methods,
which inherently provide upper bounds on the ground state energy, the proposed
approach gives rise to guaranteed and asymptotically exact lower energy bounds.
Importantly, and in contrast to previous works, they are obtained directly
without the need of post-processing, leading to more accurate guaranteed lower
energy bounds in practice.

</details>


### [2] [A parametric tensor ROM for the shallow water dam break problem](https://arxiv.org/abs/2506.20007)
*Md Rezwan Bin Mizan,Maxim Olshanskii,Ilya Timofeyev*

Main category: math.NA

TL;DR: A tensor reduced-order model (tROM) variant is developed for the parameterized shallow-water dam-break problem, outperforming traditional POD-ROMs by addressing challenges like slow Kolmogorov N-width decay and shock formation.


<details>
  <summary>Details</summary>
Motivation: Traditional POD-ROMs struggle with the shallow-water dam-break problem due to slow Kolmogorov N-width decay, shock formation, and loss of smooth parameter dependence, necessitating a more robust approach.

Method: The tROM uses low-rank tensor decomposition to build a parameter-to-solution map from high-fidelity snapshots and constructs localized reduced bases via local POD, with Chebyshev sampling near critical parameters.

Result: The tROM, especially its non-interpolatory variant, effectively captures parameter-dependent behavior and outperforms POD-ROMs, particularly in wet-bed cases where POD-ROMs fail to resolve shocks and exhibit oscillations.

Conclusion: The tROM approach is superior for the shallow-water dam-break problem, offering better resolution of shocks and parameter-dependent behavior compared to traditional POD-ROMs.

Abstract: We develop a variant of a tensor reduced-order model (tROM) for the
parameterized shallow-water dam-break problem. This hyperbolic system presents
multiple challenges for model reduction, including a slow decay of the
Kolmogorov $N$-width of the solution manifold, shock formation, and the loss of
smooth solution dependence on parameters. These issues limit the performance of
traditional Proper Orthogonal Decomposition based ROMs. Our tROM approach,
based on a low-rank tensor decomposition, builds a parameter-to-solution map
from high-fidelity snapshots and constructs localized reduced bases via a local
POD procedure. We apply this method to both dry-bed and wet-bed problem setups,
showing that the non-interpolatory variant of the tROM, combined with Chebyshev
sampling near critical parameter values, effectively captures
parameter-dependent behavior and significantly outperforms standard POD-ROMs.
This is especially evident in the wet-bed case, where POD-ROMs exhibit poor
resolution of shock waves and spurious oscillations.

</details>


### [3] [DefElement: an encyclopedia of finite element definitions](https://arxiv.org/abs/2506.20188)
*Matthew W. Scroggs,Pablo D. Brubeck,Joseph P. Dean,JÃ¸rgen S. Dokken,India Marsden*

Main category: math.NA

TL;DR: DefElement is an online encyclopedia for finite element definitions, aiding in verifying element implementations across libraries.


<details>
  <summary>Details</summary>
Motivation: To standardize and simplify the verification of finite element definitions and implementations across various libraries.

Method: Derived conditions for element equivalence and developed an algorithm to verify implementations.

Result: Implemented the verification algorithm, with results available on DefElement.

Conclusion: DefElement provides a reliable resource for verifying finite element definitions and implementations.

Abstract: DefElement is an online encyclopedia of finite element definitions that was
created and is maintained by the authors of this paper. DefElement aims to make
information about elements defined in the literature easily available in a
standard format. There are a number of open-source finite element libraries
available, and it can be difficult to check that an implementation of an
element in a library matches the element's definition in the literature or
implementation in another library, especially when many libraries include
variants of elements whose basis functions do not match exactly. In this paper,
we carefully derive conditions under which elements can be considered
equivalent and describe an algorithm that uses these conditions to verify that
two implementations of a finite element are indeed variants of the same
element. The results of scheduled runs of our implementation of this
verification algorithm are included in the information available on DefElement.

</details>


### [4] [A quasi-Grassmannian gradient flow model for eigenvalue problems](https://arxiv.org/abs/2506.20195)
*Shengyue Wang,Aihui Zhou*

Main category: math.NA

TL;DR: A quasi-Grassmannian gradient flow model is proposed for solving eigenvalue problems, ensuring asymptotic orthogonality without initial orthogonality, with exponential convergence and robustness.


<details>
  <summary>Details</summary>
Motivation: To efficiently compute many eigenpairs without explicit orthogonalization, enhancing robustness against numerical perturbations.

Method: Quasi-Grassmannian gradient flow model, ensuring asymptotic orthogonality and providing analytical solutions.

Result: Exponential convergence of gradient to zero and energy to minimum, with solutions converging to eigenvalue problem solutions.

Conclusion: The model eliminates explicit orthogonalization, improves robustness, and outperforms conventional methods.

Abstract: We propose a quasi-Grassmannian gradient flow model for eigenvalue problems
of linear operators, aiming to efficiently address many eigenpairs. Our model
inherently ensures asymptotic orthogonality: without the need for initial
orthogonality, the solution naturally evolves toward being orthogonal over
time. We establish the well-posedness of the model, and provide the analytical
representation of solutions. Through asymptotic analysis, we show that the
gradient converges exponentially to zero and that the energy decreases
exponentially to its minimum. This implies that the solution of the
quasi-Grassmannian gradient flow model converges to the solution of the
eigenvalue problems as time progresses. These properties not only eliminate the
need for explicit orthogonalization in numerical computation but also
significantly enhance robustness of the model, rendering it far more resilient
to numerical perturbations than conventional methods.

</details>


### [5] [Stochastic particle method with birth-death dynamics](https://arxiv.org/abs/2506.20201)
*Zhengyang Lei,Sihong Shao*

Main category: math.NA

TL;DR: A stochastic particle method (SPM) with active birth-death dynamics (SPM-birth-death) is introduced to improve efficiency in solving high-dimensional nonlinear PDEs, outperforming the original SPM in accuracy at the same computational cost.


<details>
  <summary>Details</summary>
Motivation: To address the curse of dimensionality in solving high-dimensional nonlinear PDEs and enhance the efficiency of the stochastic particle method (SPM).

Method: Introduces active birth-death dynamics in SPM, sampling new particles based on the nonlinear term and annihilating particles when exceeding a threshold.

Result: SPM-birth-death achieves smaller errors than the original SPM at the same computational cost, demonstrated on the Allen-Cahn equation.

Conclusion: SPM-birth-death is an effective improvement over SPM for solving high-dimensional nonlinear PDEs efficiently.

Abstract: In order to numerically solve high-dimensional nonlinear PDEs and alleviate
the curse of dimensionality, a stochastic particle method (SPM) has been
proposed to capture the relevant feature of the solution through the adaptive
evolution of particles [J. Comput. Phys. 527 (2025) 113818]. In this paper, we
introduce an active birth-death dynamics of particles to improve the efficiency
of SPM. The resulting method, dubbed SPM-birth-death, sample new particles
according to the nonlinear term and execute the annihilation strategy when the
number of particles exceeds a given threshold. Preliminary numerical
experiments on the Allen-Cahn equation demonstrate that SPM-birth-death can
achieve smaller errors at the same computational cost compared with the
original SPM.

</details>


### [6] [Low-order finite element complex with application to a fourth-order elliptic singular perturbation problem](https://arxiv.org/abs/2506.20240)
*Xuewei Cui,Xuehai Huang*

Main category: math.NA

TL;DR: A nonconforming finite element discretization for a smooth de Rham complex in 3D is proposed, applied to a fourth-order elliptic problem, achieving optimal convergence without extra stabilization.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of discretizing a generalized singularly perturbed Stokes-type equation, particularly handling boundary layers without additional stabilization.

Method: Uses a nonconforming finite element discretization involving $H^2$-nonconforming, tangentially continuous $H^1$-nonconforming, Raviart-Thomas, and piecewise constant spaces, with nodal interpolation for the NÃ©dÃ©lec element.

Result: The method achieves optimal convergence rates uniformly with respect to the perturbation parameter, even with strong boundary layers.

Conclusion: The proposed discretization provides a decoupled mixed finite element method that is effective for singular perturbation problems without needing extra stabilization.

Abstract: A low-order nonconforming finite element discretization of a smooth de Rham
complex starting from the $H^2$ space in three dimensions is proposed,
involving an $H^2$-nonconforming finite element space, a new tangentially
continuous $H^1$-nonconforming vector-valued finite element space, the
lowest-order Raviart-Thomas space, and piecewise constant functions. While
nonconforming for the smooth complex, the discretization conforms to the
classical de Rham complex. It is applied to develop a decoupled mixed finite
element method for a fourth-order elliptic singular perturbation problem,
focusing on the discretization of a generalized singularly perturbed
Stokes-type equation. In contrast to Nitsche's method, which requires
additional stabilization to handle boundary layers, the nodal interpolation
operator for the lowest-order N\'{e}d\'{e}lec element of the second kind is
introduced into the discrete bilinear forms. This modification yields a
decoupled mixed method that achieves optimal convergence rates uniformly with
respect to the perturbation parameter, even in the presence of strong boundary
layers, without requiring any additional stabilization.

</details>


### [7] [Deep random difference method for high dimensional quasilinear parabolic partial differential equations](https://arxiv.org/abs/2506.20308)
*Wei Cai,Shuixin Fang,Tao Zhou*

Main category: math.NA

TL;DR: A deep random difference method (DRDM) is proposed to solve high-dimensional PDEs efficiently by avoiding Hessian computation and enabling parallel loss function computation.


<details>
  <summary>Details</summary>
Motivation: High-dimensional PDEs are computationally intensive due to Hessian matrix calculations in deep learning methods.

Method: DRDM approximates convection-diffusion operators using first-order differences and deep neural networks, avoiding explicit Hessian computation. It integrates into a Galerkin framework and extends to HJB equations.

Result: DRDM achieves first-order accuracy in time discretization and efficiently solves quasilinear PDEs and HJB equations in very high dimensions.

Conclusion: DRDM offers a computationally efficient alternative to traditional methods by eliminating AD dependence and enabling parallelism, validated by numerical experiments.

Abstract: Solving high-dimensional parabolic partial differential equations (PDEs) with
deep learning methods is often computationally and memory intensive, primarily
due to the need for automatic differentiation (AD) to compute large Hessian
matrices in the PDE. In this work, we propose a deep random difference method
(DRDM) that addresses these issues by approximating the convection-diffusion
operator using only first-order differences and the solution by deep neural
networks, thus, avoiding explicit Hessian computation. When incorporated into a
Galerkin framework, the DRDM eliminates the need for pointwise evaluation of
expectations, resulting in efficient implementation. We further extend the
approach to Hamilton-Jacobi-Bellman (HJB) equations. Notably, the DRDM recovers
existing martingale deep learning methods for PDEs (Cai et al., 2024,
arXiv:2405.03169), without using the tools of stochastic calculus. The proposed
method offers two main advantages: it removes the dependence on AD for PDE
derivatives and enables parallel computation of the loss function in both time
and space. We provide rigorous error estimates for the DRDM in the linear case,
which shows a first order accuracy in $\Delta t$ used in the sampling of the
paths by the Euler-Maruyama scheme. Numerical experiments demonstrate that the
method can efficiently and accurately solve quasilinear parabolic PDEs and HJB
equations in dimensions up to $10^4$ and $10^5$, respectively.

</details>


### [8] [Solver Performance of Accelerated MoM for Connected Arrays](https://arxiv.org/abs/2506.20350)
*Harald Hultin,Lucas Ãkerstedt,B. L. G. Jonsson*

Main category: math.NA

TL;DR: Two accelerated solvers for large rectangular antenna arrays leverage multilevel Toeplitz structure to reduce computational and storage costs, outperforming conventional methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the computational complexity of simulating large rectangular arrays with equidistant spacing by exploiting their inherent matrix structure.

Method: 1. Iterative solver using multilevel fast Fourier transform. 2. Fast direct Toeplitz solver adapted to block-matrix structure. Both use a novel mesh-partitioning algorithm.

Result: Both methods significantly reduce time and storage compared to conventional solvers, with the direct solver achieving near machine epsilon accuracy.

Conclusion: The choice of solver depends on residual thresholds, element geometry, and frequency. Both accelerated methods outperform traditional approaches.

Abstract: Simulating and developing large rectangularly shaped arrays with equidistant
interspacing is challenging as the computational complexity grows quickly with
array size. However, the geometrical shape of the array, appropriately meshed,
leads to a multilevel Toeplitz structure in the RWG-based Method of Moment
impedance matrix representation that can be used to mitigate the increased
complexity. This paper develops, presents and compares two different
accelerated solvers that both utilize the matrix structure to determine antenna
properties. Both methods use a novel mesh-partitioning algorithm and its
associated data representation, reducing storage and computational costs. The
first solver is an iterative method based on multilevel fast Fourier transform
to accelerate matrix multiplications. The second solver approach is based on an
extension of a fast direct Toeplitz solver, adapted to a block-matrix
structure. This fast direct solver is demonstrated to have close to machine
epsilon accuracy. Both accelerated methods are evaluated on two different array
element types, for arrays with up to 900 elements. The results are compared
with conventional direct and iterative matrix solvers. Improvements are seen in
both the time and required storage to solve the problem. The choice of the most
efficient method depends on the residual thresholds in the iterative method,
geometry of the element and frequency. Two different preconditioners for the
iterative method are investigated to evaluate their performance. The two
accelerated methods vastly outperform regular matrix inversion methods.

</details>


### [9] [An adaptive scheme for the optimization of damping positions by decoupling controllability spaces in vibrational systems](https://arxiv.org/abs/2506.20372)
*Jennifer Przybilla,Matea Ugrica VukojeviÄ,Ninolsav Truhar,Peter Benner*

Main category: math.NA

TL;DR: A new reduction method is proposed to optimize damper positions in vibrational systems by minimizing computational costs through solving Lyapunov equations more efficiently.


<details>
  <summary>Details</summary>
Motivation: The high computational cost of solving Lyapunov equations for optimizing damper positions in vibrational systems motivates the need for a faster method.

Method: The algorithm generates a reduced basis for approximating the solution space of Lyapunov equations, adaptively expanding it for different damper positions, and introduces an error indicator for quality assessment.

Result: The method produces a reduced optimization problem with significantly lower computational cost, demonstrated by numerical examples.

Conclusion: The proposed reduction technique effectively accelerates the optimization process for damper positioning while maintaining accuracy.

Abstract: In this work, the problem of optimizing damper positions in vibrational
systems is investigated. The objective is to determine the positions of
external dampers in such a way that the influence of the input on the output is
minimized. The energy response serves as an optimization criterion, whose
computation involves solving Lyapunov equations. Hence, in order to find the
best positions, many of these equations need to be solved, and so the
minimization process can have a high computational cost.
  To accelerate the process of finding the optimal positions, we propose a new
reduction method. Our algorithm generates a basis spanning an approximation to
the solution space of the Lyapunov equations for all possible positions of the
dampers. We derive an adaptive scheme that generates the reduced solution space
by adding the subspaces of interest, and then we define the corresponding
reduced optimization problem that is solvable in a reasonable amount of time.
We decouple the solution spaces of the problem to obtain a space that
corresponds to the system without external dampers and serves as a starting
point for the reduction of the optimization problem. In addition, we derive
spaces corresponding to the different damper positions that are used to expand
the reduced basis if needed. To evaluate the quality of the basis, we introduce
an error indicator based on the space decomposition. Our new technique produces
a reduced optimization problem of significantly smaller dimension that is
faster to solve than the original problem, which we illustrate with some
numerical examples.

</details>


### [10] [A Taylor-Hood finite element method for the surface Stokes problem without penalization](https://arxiv.org/abs/2506.20419)
*Alan Demlow,Michael Neilan*

Main category: math.NA

TL;DR: The paper extends a tangentiality-conforming, $H^1$ nonconforming FEM for surface Stokes equations to Taylor-Hood elements, achieving optimal convergence with Gauss-Lobatto node placement.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of enforcing both tangentiality and $H^1$ conformity in FEM for surface Stokes equations, the work builds on a recent method avoiding penalization.

Method: Extends the MINI element approach to Taylor-Hood FEMs, using an auxiliary Piola transform for velocity degrees of freedom placed at Gauss-Lobatto nodes.

Result: Optimal-order convergence is achieved, with numerical experiments confirming the necessity of nonstandard node placement.

Conclusion: The proposed Taylor-Hood FEM effectively balances tangentiality and weak continuity, enabling optimal convergence without penalization.

Abstract: Finite element approximation of the velocity-pressure formulation of the
surfaces Stokes equations is challenging because it is typically not possible
to enforce both tangentiality and $H^1$ conformity of the velocity field. Most
previous works concerning finite element methods (FEMs) for these equations
thus have weakly enforced one of these two constraints by penalization or a
Lagrange multiplier formulation. Recently in [A tangential and penalty-free
finite element method for the surface Stokes problem, SINUM 62(1):248-272,
2024], the authors constructed a surface Stokes FEM based on the MINI element
which is tangentiality conforming and $H^1$ nonconforming, but possesses
sufficient weak continuity properties to circumvent the need for penalization.
The key to this method is construction of velocity degrees of freedom lying on
element edges and vertices using an auxiliary Piola transform. In this work we
extend this methodology to construct Taylor-Hood surface FEMs. The resulting
method is shown to achieve optimal-order convergence when the edge degrees of
freedom for the velocity spaced are placed at Gauss-Lobatto nodes. Numerical
experiments confirm that this nonstandard placement of nodes is necessary to
achieve optimal convergence orders.

</details>


### [11] [A Novel Homotopy Perturbation Sumudu Transform Method for Nonlinear Fractional PDEs: Applications and Comparative Analysis](https://arxiv.org/abs/2506.20457)
*Maryam Jalili*

Main category: math.NA

TL;DR: HPSTM combines Sumudu transform and homotopy perturbation to solve nonlinear FPDEs, offering faster convergence and high accuracy with low computational time.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of solving strongly nonlinear FPDEs efficiently and accurately.

Method: Hybrid approach using Sumudu transform and homotopy perturbation, validated against multiple methods.

Result: Achieves low absolute errors (e.g., $3.12 \times 10^{-3}$) and fast computational times (0.5s per example).

Conclusion: HPSTM is accurate and efficient for FPDEs but has limitations with high-order nonlinearities and multi-dimensional domains.

Abstract: This study introduces the Homotopy Perturbation Sumudu Transform Method
(HPSTM), a novel hybrid approach combining the Sumudu transform with homotopy
perturbation to solve nonlinear fractional partial differential equations
(FPDEs), including fractional porous medium, heat transfer, and Fisher
equations, using the Caputo fractional derivative. HPSTM leverages the
linearity-preserving properties of the Sumudu transform and the flexibility of
homotopy perturbation, achieving faster convergence than Laplace-HPM or
Elzaki-HPM for strongly nonlinear FPDEs. Series solutions yield absolute errors
as low as $3.12 \times 10^{-3}$ for $\alpha = 0.9$, with computational times
averaging 0.5 seconds per example using 5 series terms on standard hardware.
Solutions are validated against exact solutions, Adomian Decomposition Method
(ADM), radial basis function (RBF) meshless method, Variational Iteration
Method (VIM), Finite Difference Method (FDM), and a spectral method. Numerical
examples, sensitivity analysis, and graphical representations for $\alpha =
1.0, 0.9, 0.8, 0.7$ confirm HPSTM's accuracy, efficiency, and robustness.
Limitations include challenges with high-order nonlinearities and
multi-dimensional domains. HPSTM shows promise for applications in modeling
fluid flow in porous media, heat conduction in complex materials, and
biological population dynamics.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [12] [Normalized solutions for Choquard equations with critical nonlinearities on bounded domains](https://arxiv.org/abs/2506.19872)
*Ru Yan*

Main category: math.AP

TL;DR: The paper studies normalized solutions to a nonlinear SchrÃ¶dinger equation with nonlocal nonlinearities, proving the existence of two positive solutions: a ground state and a mountain pass solution.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore the existence of normalized solutions for a specific type of nonlinear SchrÃ¶dinger equation with nonlocal terms, which is relevant in mathematical physics and quantum mechanics.

Method: The authors analyze the equation using variational methods, focusing on the properties of the Riesz potential and the star-shaped domain.

Result: They establish the existence of two positive normalized solutions: one as a ground state and another as a mountain pass solution.

Conclusion: The work confirms the existence of multiple solutions for the given nonlinear SchrÃ¶dinger equation, contributing to the understanding of nonlocal interactions in such systems.

Abstract: The aim of this work is the study of the existence of normalized solutions to
the nonlinear Schr\"odinger equation with nonlocal nonlinearities:
\begin{equation}\nonumber \left\{\begin{aligned} &-\Delta u =\lambda
u+(I_\alpha*|u|^{2_\alpha^*})|u|^{2_\alpha^*-2}u+a(I_\alpha*|u|^p)|u|^{p-2}u,\
x\in\Omega,\\ &u>0\ \text {in}\ \Omega,\ u=0\ \text {on}\ \partial \Omega,\
\int _{\Omega}|u|^2dx=c, \end{aligned} \right. \end{equation} where $c>0,\
\alpha \in (0,N),\ \frac{N+\alpha+2}{N}<p<\frac{N+\alpha}{N-2}=2_\alpha^*,\
a\ge 0,\ \Omega \subset \mathbb{R}^N (N \ge 3)$ is smooth, bounded, star-shaped
and $I_\alpha$ is the Riesz potential. We prove the existence of two positive
normalized solutions, one of which is a ground state and the other is a
mountain pass solution.

</details>


### [13] [Logarithmic convexity of evolution equations and application to inverse problems](https://arxiv.org/abs/2506.19954)
*S. E. Chorfi*

Main category: math.AP

TL;DR: Review of logarithmic convexity in evolution equations, covering self-adjoint operators, analytic semigroups, and applications to inverse problems like initial data recovery. Extends to time-fractional equations and non-symmetric cases.


<details>
  <summary>Details</summary>
Motivation: To explore logarithmic convexity as a method for solving inverse and ill-posed problems in evolution equations, extending classical results to broader contexts.

Method: Analyzes classical self-adjoint operators, analytic semigroups, and provides explicit estimates. Applies results to Ornstein-Uhlenbeck equations and time-fractional cases.

Result: Demonstrates applicability of logarithmic convexity in diverse settings, including non-symmetric diffusion equations, with explicit estimates for inverse problems.

Conclusion: Logarithmic convexity is versatile for evolution equations, with potential extensions to non-symmetric cases and open problems remaining.

Abstract: We review some results on the logarithmic convexity for evolution equations,
a well-known method in inverse and ill-posed problems. We start with the
classical case of self-adjoint operators. Then, we analyze the case of analytic
semigroups. In this general case, we give an explicit estimate, which will be
used to study inverse problems for initial data recovery. We illustrate our
abstract result by an application to the Ornstein-Uhlenbeck equations. We
discuss both analytic and non-analytic semigroups. We conclude with recent
results for the time-fractional evolution equations with the Caputo derivative
of order $0<\alpha <1$. We start with symmetric evolution equations. Then, we
show that the results extend to the non-symmetric case for diffusion equations,
provided that a gradient vector field generates the drift coefficient. Finally,
some open problems will be mentioned.

</details>


### [14] [On solutions to Hardy-Sobolev equations on Riemannian manifolds](https://arxiv.org/abs/2506.20089)
*Guillermo Henry,Jimmy Petean*

Main category: math.AP

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Let $(M,g)$ be a closed Riemannian manifold of dimension at least $3$. Let
$S$ be the union of the focal submanifolds of an isoparametric function on
$(M,g)$. In this article we address the existence of solutions of the
Hardy-Sobolev type equation $\Delta_g
u+K(x)u=\frac{u^{q-1}}{\left(d_{S}(x)\right)^s}$, where $d_{S}(x)$ is the
distance from $x$ to $S$ and $q>2$. In particular, we will prove the existence
of infinite sign-changing solutions to the equation.

</details>


### [15] [Fundamental Solutions of the Logarithmic Laplacian: An Approach via the Division Problem](https://arxiv.org/abs/2506.20121)
*David Lee*

Main category: math.AP

TL;DR: An alternative method for the fundamental solution of the logarithmic Laplacian is presented, inspired by Malgrange and Ehrenpreis. It includes a variant of the Liouville theorem and addresses a conjecture about solutions in dimensions 1 and 2.


<details>
  <summary>Details</summary>
Motivation: To provide a new approach for the fundamental solution of the logarithmic Laplacian and clarify a conjecture by Chen and VÃ©ron.

Method: Modification of the classical division problem, inspired by Malgrange and Ehrenpreis's theory.

Result: A variant of the Liouville theorem for the logarithmic Laplacian and further insights into the conjecture for dimensions 1 and 2.

Conclusion: The alternative approach successfully addresses the problem and provides additional clarity on the conjecture.

Abstract: Existence of the fundamental solution of the logarithmic Laplacian (in
dimensions $d \geq 3$) was established by Huyuan Chen and Laurent V\'eron
(2024). In this note, we present an alternative approach, based on a
modification on the classical division problem. This is inspired by the theory
of fundamental solutions by Malgrange and Ehrenpreis. Moreover, we give a
variant of the Liouville theorem for the logarithmic Laplacian and give some
further clarification regarding a conjecture posed by Chen and V\'eron
regarding the behavior of solutions in dimensions 1 and 2.

</details>


### [16] [Axisymmetric self-similar solutions to the MHD equations without magnetic diffusion](https://arxiv.org/abs/2506.20131)
*Shaoheng Zhang*

Main category: math.AP

TL;DR: The paper analyzes axisymmetric self-similar solutions to stationary MHD equations without magnetic diffusion, focusing on cases where the magnetic field has only a swirl component. It finds trivial solutions in certain domains.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of axisymmetric self-similar solutions in MHD systems without magnetic diffusion, particularly in specific geometric settings like âÂ³â{0} and half-spaces.

Method: The study examines stationary MHD equations under no magnetic diffusion, with a focus on axisymmetric self-similar solutions where the magnetic field is purely swirl. It uses mathematical analysis to derive solutions.

Result: In âÂ³â{0}, the velocity field is a Landau solution and the magnetic field vanishes. In half-spaces with no-slip or Navier slip conditions, solutions are trivial.

Conclusion: The paper concludes that under the given conditions, axisymmetric self-similar solutions are either trivial or reduce to known solutions like Landau solutions.

Abstract: We study the axisymmetric self-similar solutions $(\mathbf{u},\mathbf{B})$ to
the stationary MHD equations without magnetic diffusion, where $\mathbf{B}$ has
only the swirl component. Our first result states that in
$\mathbb{R}^3\setminus\{0\}$, $\mathbf{u}$ is a Landau solution and
$\mathbf{B}=0$. Our second result proves the triviality of axisymmetric
self-similar solutions in the half-space $\mathbb{R}^3_+$ with the no-slip
boundary condition or the Navier slip boundary condition.

</details>


### [17] [Gamma-convergence of nonlocal energies for partitions](https://arxiv.org/abs/2506.20215)
*Thomas Gabard,Vincent Millot*

Main category: math.AP

TL;DR: Nonlocal functionals on partitions Gamma-converge to a local perimeter functional, with novel effects compared to fractional perimeter convergence.


<details>
  <summary>Details</summary>
Motivation: To explore the Gamma-convergence of nonlocal functionals to local perimeter functionals, even without triangular inequality constraints.

Method: Analyzing nonlocal functionals with generalized surface tension coefficients and their convergence properties.

Result: The functionals Gamma-converge to a local perimeter functional, revealing a relaxation process and novel effects.

Conclusion: This work extends understanding of Gamma-convergence beyond traditional constraints, highlighting new phenomena.

Abstract: We prove that certain nonlocal functionals defined on partitions made of
measurable sets Gamma-converge to a local functional modeled on the perimeter
in the sense of De Giorgi. Those nonlocal functionals involve generalized
surface tension coefficients, and are lower semicontinuous even if the
coefficients do not satisfy the triangular inequality. It implies a relaxation
process in the limit, and provides a novel effect compare to the known
gamma-convergence of the fractional perimeter towards the standard perimeter.

</details>


### [18] [Fractional multi-phase transitions and nonlocal minimal partitions](https://arxiv.org/abs/2506.20226)
*Thomas Gabard,Vincent Millot*

Main category: math.AP

TL;DR: Study of phase transition models with nonlocal energies, focusing on fractional elliptic equations and their convergence to nonlocal geometric energy critical points. Includes regularity analysis and detailed study for 3-partitions.


<details>
  <summary>Details</summary>
Motivation: To understand phase transitions involving nonlocal energies and analyze the behavior of solutions as a parameter tends to zero.

Method: Asymptotic analysis of fractional elliptic equations (Allen-Cahn type), convergence to critical points of nonlocal geometric energy, and regularity analysis for solutions.

Result: Solutions converge to critical points of nonlocal geometric energy. Regularity analysis performed, with detailed study for 3-partitions where triangular inequality reverses.

Conclusion: The study provides insights into nonlocal geometric energy and its behavior, especially in cases where traditional inequalities do not hold.

Abstract: This article is devoted to the study of certain models for phase transitions
involving nonlocal energies. A first part is concerned with to the asymptotic
analysis of a system of fractional elliptic equations of Allen-Cahn type as a
characteristic small parameter tends to zero. It is shown that solutions
converge to critical points of a nonlocal geometric energy defined over a class
of partitions of the domain. A regularity analysis for solutions of the
geometric problem is also performed, in the minimizing and non minimizing case.
The limiting geometric problem involves generalized surface tension
coefficients which might not satisfy the usual triangular inequality. A more
detailed regularity analysis for minimizers is performed for 3-partitions, in
particular in the case where one triangular inequality strictly holds in the
reverse sense.

</details>


### [19] [Log-concavity and anti-maximum principles for semilinear and linear elliptic equations](https://arxiv.org/abs/2506.20250)
*FranÃ§ois Hamel,Nikolai Nadirashvili*

Main category: math.AP

TL;DR: Existence and properties of positive solutions for semilinear elliptic equations in bounded domains are studied, focusing on solutions near the linear equation and log-concavity in convex domains.


<details>
  <summary>Details</summary>
Motivation: To understand the existence and qualitative properties (e.g., log-concavity) of positive solutions for semilinear elliptic equations under Dirichlet boundary conditions.

Method: Uses the maximum principle, anti-maximum principle, Schauder fixed point theorem, and a priori estimates to derive existence and convergence results.

Result: Existence of positive solutions near the linear equation and log-concavity in strictly convex domains are proven.

Conclusion: The paper establishes key properties of solutions for semilinear elliptic equations, leveraging classical tools like the maximum principle and Schauder theorem.

Abstract: This paper is concerned with existence and qualitative properties of positive
solutions of semilinear elliptic equations in bounded domains with Dirichlet
boundary conditions. We show the existence of positive solutions in the
vicinity of the linear equation and the log-concavity of the solutions when the
domain is strictly convex. We also review the standard results on the
log-concavity or the more general quasi-concavity of solutions of elliptic
equations. The existence and other convergence results especially rely on the
maximum principle, on a quantified version of the anti-maximum principle, on
the Schauder fixed point theorem, and on some a priori estimates.

</details>


### [20] [Spike layered solutions for elliptic systems on Riemannian Manifolds](https://arxiv.org/abs/2506.20300)
*Anusree R Kannoth,Bhakti Bhusan Manna*

Main category: math.AP

TL;DR: The paper studies a Hamiltonian system on a Riemannian manifold, analyzing the concentration behavior of least energy critical points as a parameter approaches zero.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of solutions to a Hamiltonian system under subcritical growth conditions and their concentration properties.

Method: Investigates a sequence of least energy critical points of the dual functional and analyzes their behavior as Îµâ0.

Result: The solutions exhibit point concentration at the point where the manifold's scalar curvature is maximized.

Conclusion: The study reveals a connection between solution concentration and geometric properties of the manifold.

Abstract: In this article, we study the following Hamiltonian system: \begin{equation*}
  \begin{cases}
  \begin{aligned}
  &-\varepsilon^{2}\Delta_{g} u +u = |v|^{q-1}v,
  &-\varepsilon^{2}\Delta_{g} v +v = |u|^{p-1}u && \text{ in } \mathcal{M},
  & \quad u,v >0 && \text{ in } \mathcal{M},
  \end{aligned}
  \end{cases} \end{equation*} where $\mathcal{M}$ is a smooth, compact and
connected Riemannian manifold of dimension $N\geq 3$ without boundary. The
exponents $p,q>1$ are assumed to lie below the critical hyperbola, ensuring
subcritical growth conditions. We investigate a sequence of least energy
critical points of the associated dual functional and analyze their
concentration behavior as $\varepsilon \to 0$. Our main result shows that the
sequence of solutions exhibits point concentration, with the concentration
occurring at a point where the scalar curvature of $\mathcal{M}$ attains its
maximum.

</details>


### [21] [On Hardy-Littlewood-Sobolev estimates for degenerate Laplacians](https://arxiv.org/abs/2506.20368)
*Pascal Auscher,Khalid Baadi*

Main category: math.AP

TL;DR: The paper extends norm inequalities for fractional powers of degenerate Laplacians, using weights in the Muckenhoupt class and reverse HÃ¶lder conditions, building on classical Riesz potential results.


<details>
  <summary>Details</summary>
Motivation: To generalize known results for classical Riesz potentials to degenerate Laplacians with specific weight conditions.

Method: The approach relies on size estimates for degenerate heat kernels and applies to broader weighted degenerate operators.

Result: Established norm inequalities for fractional powers of degenerate Laplacians under given weight conditions.

Conclusion: The method successfully extends classical results to degenerate cases and is applicable to more general weighted operators.

Abstract: We establish norm inequalities for fractional powers of degenerate
Laplacians, with degeneracy being determined by weights in the Muckenhoupt
class $A_2(\mathbb{R}^n)$, accompanied by specific additional reverse H\"older
assumptions. This extends the known results for classical Riesz potentials. The
approach is based on size estimates for the degenerate heat kernels. The
approach also applies to more general weighted degenerate operators.

</details>


### [22] [Generalized existence of extremizers for the sharp $p$-Sobolev inequality on Riemannian manifolds with nonnegative curvature](https://arxiv.org/abs/2506.20592)
*Francesco Nobili,Ivan Yuri Violo*

Main category: math.AP

TL;DR: The paper investigates extremizers for the p-Sobolev inequality on noncompact Riemannian manifolds with nonnegative curvature and Euclidean volume growth. It shows almost extremal functions resemble radial Euclidean bubbles under Ricci curvature bounds and vanish under sectional curvature bounds.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of extremizers for p-Sobolev inequalities in noncompact settings with curvature constraints, bridging analysis and geometry.

Method: Uses nonsmooth concentration compactness and Mosco-convergence for Cheeger energy, extended to all pâ(1,â).

Result: Almost extremal functions are close to radial Euclidean bubbles under Ricci bounds and nearly vanish under sectional bounds.

Conclusion: The study reveals the influence of curvature on extremizers, providing insights into their behavior in noncompact manifolds.

Abstract: We study the generalized existence of extremizers for the sharp $p$-Sobolev
inequality on noncompact Riemannian manifolds in connection with nonnegative
curvature and Euclidean volume growth assumptions. Assuming a nonnegative Ricci
curvature lower bound, we show that almost extremal functions are close in
gradient norm to radial Euclidean bubbles. In the case of nonnegative sectional
curvature lower bounds, we additionally deduce that vanishing is the only
possible behavior, in the sense that almost extremal functions are almost zero
globally. Our arguments rely on nonsmooth concentration compactness methods and
Mosco-convergence results for the Cheeger energy on noncompact varying spaces,
generalized to every exponent $p\in (1,\infty)$.

</details>


### [23] [Telegrapher's Generative Model via Kac Flows](https://arxiv.org/abs/2506.20641)
*Richard Duong,Jannis Chemseddine,Peter Friz,Gabriele Steidl*

Main category: math.AP

TL;DR: The paper introduces a novel flow-based generative model using the telegrapher's equation, offering advantages like Lipschitz continuity and bounded velocity over diffusion models.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of diffusion models by leveraging the telegrapher's equation for improved generative modeling.

Method: Proposes a Kac flow model based on the telegrapher's equation, extends it to multi-dimensions, and trains a neural network for velocity field approximation.

Result: Demonstrates superior performance over diffusion models in numerical experiments.

Conclusion: The Kac flow model provides a promising alternative to diffusion models with bounded velocity and Lipschitz continuity.

Abstract: We break the mold in flow-based generative modeling by proposing a new model
based on the damped wave equation, also known as telegrapher's equation.
Similar to the diffusion equation and Brownian motion, there is a Feynman-Kac
type relation between the telegrapher's equation and the stochastic Kac process
in 1D. The Kac flow evolves stepwise linearly in time, so that the probability
flow is Lipschitz continuous in the Wasserstein distance and, in contrast to
diffusion flows, the norm of the velocity is globally bounded. Furthermore, the
Kac model has the diffusion model as its asymptotic limit. We extend these
considerations to a multi-dimensional stochastic process which consists of
independent 1D Kac processes in each spatial component. We show that this
process gives rise to an absolutely continuous curve in the Wasserstein space
and compute the conditional velocity field starting in a Dirac point
analytically. Using the framework of flow matching we train a neural network
that approximates the velocity field and use it for sample generation. Our
numerical experiments demonstrate the advantages of our approach over diffusion
models.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [24] [Exploring the Capabilities of the Frontier Large Language Models for Nuclear Energy Research](https://arxiv.org/abs/2506.19863)
*Ahmed Almeldein,Mohammed Alnaggar,Rick Archibald,Tom Beck,Arpan Biswas,Rike Bostelmann,Wes Brewer,Chris Bryan,Christopher Calle,Cihangir Celik,Rajni Chahal,Jong Youl Choi,Arindam Chowdhury,Mark Cianciosa,Franklin Curtis,Gregory Davidson,Sebastian De Pascuale,Lisa Fassino,Ana Gainaru,Yashika Ghai,Luke Gibson,Qian Gong,Christopher Greulich,Scott Greenwood,Cory Hauck,Ehab Hassan,Rinkle Juneja,Soyoung Kang,Scott Klasky,Atul Kumar,Vineet Kumar,Paul Laiu,Calvin Lear,Yan-Ru Lin,Jono McConnell,Furkan Oz,Anant Raj,Pradeep Ramuhalli,Marie Romedenne,Samantha Sabatino,JosÃ© Salcedo-PÃ©rez,Nathan D. See,Arpan Sircar,Punam Thankur,Tim Younkin,Xiao-Ying Yu,Prashant Jain,Tom Evans,Prasanna Balaprakash*

Main category: physics.comp-ph

TL;DR: The workshop explored LLMs' role in nuclear energy research, showing their strengths in early-stage tasks but limitations in advanced applications, emphasizing expert-driven AI integration.


<details>
  <summary>Details</summary>
Motivation: To evaluate LLMs' potential in accelerating nuclear energy research, addressing challenges in fusion and fission.

Method: Interdisciplinary teams used LLMs like ChatGPT for tasks such as literature synthesis, code prototyping, and hypothesis generation, employing structured workflows.

Result: LLMs excelled in early-stage exploration and workflow design but struggled with advanced tasks like novel materials design and domain-specific details.

Conclusion: AI can complement nuclear research but requires expert validation, curated datasets, and specialized models for effective integration.

Abstract: The AI for Nuclear Energy workshop at Oak Ridge National Laboratory evaluated
the potential of Large Language Models (LLMs) to accelerate fusion and fission
research. Fourteen interdisciplinary teams explored diverse nuclear science
challenges using ChatGPT, Gemini, Claude, and other AI models over a single
day. Applications ranged from developing foundation models for fusion reactor
control to automating Monte Carlo simulations, predicting material degradation,
and designing experimental programs for advanced reactors. Teams employed
structured workflows combining prompt engineering, deep research capabilities,
and iterative refinement to generate hypotheses, prototype code, and research
strategies. Key findings demonstrate that LLMs excel at early-stage
exploration, literature synthesis, and workflow design, successfully
identifying research gaps and generating plausible experimental frameworks.
However, significant limitations emerged, including difficulties with novel
materials designs, advanced code generation for modeling and simulation, and
domain-specific details requiring expert validation. The successful outcomes
resulted from expert-driven prompt engineering and treating AI as a
complementary tool rather than a replacement for physics-based methods. The
workshop validated AI's potential to accelerate nuclear energy research through
rapid iteration and cross-disciplinary synthesis while highlighting the need
for curated nuclear-specific datasets, workflow automation, and specialized
model development. These results provide a roadmap for integrating AI tools
into nuclear science workflows, potentially reducing development cycles for
safer, more efficient nuclear energy systems while maintaining rigorous
scientific standards.

</details>


### [25] [STable AutoCorrelation Integral Estimator (STACIE): Robust and accurate transport properties from molecular dynamics simulations](https://arxiv.org/abs/2506.20438)
*GÃ¶zdenur Toraman,Dieter Fauconnier,Toon Verstraelen*

Main category: physics.comp-ph

TL;DR: STACIE is a Python package for robust, uncertainty-aware autocorrelation integral estimation from time-correlated data, applicable across scientific fields.


<details>
  <summary>Details</summary>
Motivation: To provide accurate transport property estimates from molecular dynamics simulations without manual hyperparameter tuning.

Method: Uses a novel algorithm to estimate autocorrelation integrals, validated with synthetic benchmark data.

Result: Demonstrated effectiveness by estimating ionic conductivity in a NaCl-water solution and validated with 15360 synthetic datasets.

Conclusion: STACIE is a reliable, open-source tool for autocorrelation integral estimation, available on GitHub and PyPI.

Abstract: STACIE (STable AutoCorrelation Integral Estimator) is a novel algorithm and
Python package that delivers robust, uncertainty-aware estimates of
autocorrelation integrals from time-correlated data. While its primary
application is deriving transport properties from equilibrium molecular
dynamics simulations, STACIE is equally applicable to time-correlated data in
other scientific fields. A key feature of STACIE is its ability to provide
robust and accurate estimates without requiring manual adjustment of
hyperparameters. Additionally, one can follow a simple protocol to prepare
sufficient simulation data to achieve a desired relative error of the transport
property. We demonstrate its application by estimating the ionic electrical
conductivity of a NaCl-water electrolyte solution. We also present a massive
synthetic benchmark dataset to rigorously validate STACIE, comprising 15360
sets of time-correlated inputs generated with diverse covariance kernels with
known autocorrelation integrals. STACIE is open source and available on GitHub
and PyPI, with comprehensive documentation and examples.

</details>


### [26] [Resolvent4py: a parallel Python package for analysis, model reduction and control of large-scale linear systems](https://arxiv.org/abs/2506.20539)
*Alberto Padovan,Vishal Anantharaman,Clarence W. Rowley,Blaine Vollmer,Tim Colonius,Daniel J. Bodony*

Main category: physics.comp-ph

TL;DR: resolvent4py is a parallel Python package for analyzing, reducing, and controlling large-scale linear systems, leveraging MPI-based parallelism for efficiency.


<details>
  <summary>Details</summary>
Motivation: To provide a user-friendly Python tool for handling large-scale linear systems in various fields like fluid mechanics, solid mechanics, and more.

Method: Uses MPI-based parallelism via mpi4py, petsc4py, and slepc4py for efficient computation.

Result: Enables streamlined and efficient Python code for solving complex problems in multiple domains.

Conclusion: resolvent4py offers a powerful, parallel solution for large-scale linear system analysis and control.

Abstract: In this paper, we present resolvent4py, a parallel Python package for the
analysis, model reduction and control of large-scale linear systems with
millions or billions of degrees of freedom. This package provides the user with
a friendly Python-like experience (akin to that of well-established libraries
such as numpy and scipy), while enabling MPI-based parallelism through mpi4py,
petsc4py and slepc4py. In turn, this allows for the development of streamlined
and efficient Python code that can be used to solve several problems in fluid
mechanics, solid mechanics, graph theory, molecular dynamics and several other
fields.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [27] [Particle Injection Problem in Magnetic Reconnection and Turbulence](https://arxiv.org/abs/2506.19938)
*Fan Guo,Omar French,Qile Zhang,Xiaocan Li,Jeongbhin Seo*

Main category: physics.plasm-ph

TL;DR: The paper reviews recent advances in understanding injection mechanisms for nonthermal particle acceleration in magnetically-dominated environments, emphasizing their role in energy partition and nonthermal particle fractions.


<details>
  <summary>Details</summary>
Motivation: To explore the importance of pre-acceleration (injection) in nonthermal particle acceleration and its impact on energy distribution between thermal and nonthermal particles.

Method: Review of recent studies and advances in injection mechanisms for populating nonthermal power-law spectra.

Result: Highlights the significance of injection processes in determining nonthermal particle fractions and energy partition.

Conclusion: Outlook for future studies and applications of injection models in understanding particle acceleration in magnetically-dominated environments.

Abstract: Magnetic reconnection and turbulence in magnetically-dominated environments
have been proposed as important nonthermal particle acceleration mechanisms
that generate high energy particles and associated emissions. While the
acceleration to high energy that produces the power-law energy distribution has
drawn strong interest, recent studies actively discuss pre-acceleration, or
injection, to a sufficient energy for a sustained and prolonged Fermi-like
acceleration. The injection process is important for determining the fraction
of nonthermal particles and energy partition between thermal and nonthermal
particles. We review recent advances in understanding the injection mechanisms
responsible for populating these nonthermal power-law spectra, and conclude
with an outlook for studies and applications of injection models.

</details>


### [28] [First experimental demonstration of plasma shape control in a tokamak through Model Predictive Control](https://arxiv.org/abs/2506.20096)
*Adriano Mele,Maria A. Topalova,Cristian Galperti,Stefano Coda,TCV team,Eurofusion Tokamak Exploitation Team*

Main category: physics.plasm-ph

TL;DR: A Model Predictive Controller (MPC) is proposed for plasma shape control in Tokamak Ã  Configuration Variable (TCV), combining linearized plasma models with a state-space description of the magnetic control system, solved via real-time Quadratic Programming (QP).


<details>
  <summary>Details</summary>
Motivation: To achieve precise plasma shape control in TCV while enforcing constraints on outputs, leveraging MPC for the first time in experimental tokamak testing.

Method: Coupling linearized plasma response models (from MEQ's fge code) with TCV's magnetic control system, formulating and solving a QP problem in real-time.

Result: Demonstrated effectiveness through simulations and experimental results, marking the first experimental test of MPC-based plasma shape control in a tokamak.

Conclusion: The MPC approach successfully controls plasma shape in TCV, validated by simulations and experiments, pioneering its real-world application in tokamaks.

Abstract: In this work, a Model Predictive Controller (MPC) is proposed to control the
plasma shape in the Tokamak \`a Configuration Variable (TCV). The proposed
controller relies on models obtained by coupling linearized plasma response
models, derived from the \texttt{fge} code of the Matlab EQuilibrium toolbox
(MEQ) suite, with a state-space description of the core TCV magnetic control
system. It optimizes the reference signals fed to this inner control loop in
order to achieve the desired plasma shape while also enforcing constraints on
the plant outputs. To this end, a suitable Quadratic Programming (QP) problem
is formulated and solved in real-time. The effectiveness of the proposed
controller is illustrated through a combination of simulations and experimental
results. To the best of our knowledge, this is the first time that a plasma
shape control solution based on MPC has been experimentally tested on a real
tokamak.

</details>


### [29] [Physics-Informed Machine Learning Approach to Modeling Line Emission from Helium-Containing Plasmas](https://arxiv.org/abs/2506.20117)
*Shin Kajita*

Main category: physics.plasm-ph

TL;DR: A hybrid neural network combining CRM- and experiment-based models improves electron temperature prediction in plasmas, with CRM-based models excelling in data-limited scenarios.


<details>
  <summary>Details</summary>
Motivation: To enhance the accuracy of measuring electron density and temperature in fusion-relevant plasmas using the helium I line intensity ratio method, leveraging machine learning alongside traditional collisional-radiative models.

Method: A hybrid neural network approach integrating CRM- and experiment-based models was developed and tested for predicting electron density and temperature.

Result: The ensemble model modestly improved prediction accuracy for electron temperature, while the CRM-based model outperformed others in data-limited scenarios.

Conclusion: The hybrid approach shows promise for improving plasma diagnostics, especially in situations with limited experimental data.

Abstract: The helium I line intensity ratio (LIR) method is used to measure the
electron density ($n_e$) and temperature ($T_e$) of fusion-relevant plasmas.
Although the collisional-radiative model (CRM) has been used to predict $n_e$
and $T_e$, recent studies have shown that machine learning approaches can
provide better measurements if a sufficient dataset for training is available.
This study investigates a hybrid neural network approach that combines CRM- and
experiment-based models. Although the CRM-based model alone exhibited negative
transfer in most cases, the ensemble model modestly improved the prediction
accuracy of $T_e$. Notably, in data-limited scenarios, the CRM-based model
outperformed the others for $T_e$ prediction, highlighting its potential for
applications with constrained diagnostic access.

</details>


### [30] [Tomography for Plasma Imaging: a Unifying Framework for Bayesian Inference](https://arxiv.org/abs/2506.20232)
*D. Hamm,C. Theiler,M. Simeoni,B. P. Duval,T. Debarre,L. Simons,J. R. Queralt*

Main category: physics.plasm-ph

TL;DR: The paper presents a Bayesian framework for sparse-view tomographic reconstructions in plasma imaging, unifying various inversion methods and enabling uncertainty quantification via a stochastic gradient flow algorithm.


<details>
  <summary>Details</summary>
Motivation: To provide a unifying perspective on sparse-view tomographic reconstructions in plasma diagnostics and address limitations of existing methods.

Method: A Bayesian framework combining likelihood (data modeling) and prior (profile properties) terms, with reconstructions obtained via a stochastic gradient flow algorithm.

Result: Demonstrated successful application to soft x-ray imaging at the TCV tokamak and validated on model phantoms, enabling principled statistical analysis.

Conclusion: The Bayesian approach offers credible reconstructions and uncertainty quantification, though inherent limitations of sparse-view tomography remain.

Abstract: Plasma diagnostics often employ computerized tomography to estimate
emissivity profiles from a finite, and often limited, number of line-integrated
measurements. Decades of algorithmic refinement have brought considerable
improvements, and led to a variety of employed solutions. These often feature
an underlying, common structure that is rarely acknowledged or investigated. In
this paper, we present a unifying perspective on sparse-view tomographic
reconstructions for plasma imaging, highlighting how many inversion approaches
reported in the literature can be naturally understood within a Bayesian
framework. In this setting, statistical modelling of acquired data leads to a
likelihood term, while the assumed properties of the profile to be
reconstructed are encoded within a prior term. Together, these terms yield the
posterior distribution, which models all the available information on the
profile to be reconstructed. We show how credible reconstructions, uncertainty
quantification and further statistical quantities of interest can be
efficiently obtained from noisy tomographic data by means of a stochastic
gradient flow algorithm targeting the posterior. This is demonstrated by
application to soft x-ray imaging at the TCV tokamak. We validate the proposed
imaging pipeline on a large dataset of generated model phantoms, showing how
posterior-based inference can be leveraged to perform principled statistical
analysis of quantities of interest. Finally, we address some of the inherent,
and thus remaining, limitations of sparse-view tomography. All the
computational routines used in this work are made available as open access
code.

</details>


### [31] [MHD simulation of tilt instability during the dynamic FRC magnetic compression process](https://arxiv.org/abs/2506.20443)
*Yiming Ma,Ping Zhu,Bo Rao,Haolong Li*

Main category: physics.plasm-ph

TL;DR: The paper investigates the tilt instability in FRCs during magnetic compression using MHD simulations, finding no dynamic stabilization. Toroidal flow reduces instability growth and delays distortion, enabling higher compression ratios.


<details>
  <summary>Details</summary>
Motivation: To understand the nonlinear evolution of tilt instability in FRCs during dynamic magnetic compression and explore stabilization methods like toroidal flow.

Method: MHD simulations using the NIMROD code to analyze tilt mode growth and effects of toroidal flow under varying compression field ramping rates.

Result: Tilt mode causes confinement loss; toroidal flow reduces growth rate and delays distortion, allowing compression ratios up to 5.3.

Conclusion: Toroidal flow stabilizes tilt instability but doesn't eliminate it, enabling higher compression before heating termination.

Abstract: The nonlinear evolution of the tilt instability in a field reversed
configuration (FRC) during the dynamic magnetic compression process has been
investigated using magnetohydrodynamic (MHD) simulations with the NIMROD code
[C. R. Sovinec \textit{et al.}, J. Comput. Phys. \textbf{195}, 355 (2004)]. The
tilt mode induces significant deformations in the linear growth phase and
results in complete confinement loss of the FRC in the nonlinear phase, with no
evidence of dynamic nonlinear stabilization. The growth rate of the tilt mode
increases with the compression field ramping rate and approaches an asymptotic
value. Toroidal flow can reduce both the growth rate and the nonlinear
saturation amplitude of the tilt mode. The stabilizing effect of the toroidal
rotation is enhanced with higher compression field ramping rates due to the
spontaneous toroidal field generation and increased flow shear during
compression. Although the tilt mode remains unstable with a toroidal rotation
Mach number close to 0.5, the onset of tilt distortion can be delayed, allowing
a magnetic compression ratio up to 5.3 before the compressional heating
terminates.

</details>


### [32] [Two-Loop Turbulent Helical Magnetohydrodynamics: Large-Scale Dynamo and Energy Spectrum](https://arxiv.org/abs/2506.20578)
*Michal HnatiÄ,TomÃ¡Å¡ LuÄivjanskÃ½,LukÃ¡Å¡ MiÅ¾iÅ¡in,Yurii Molotkov,Andrei Ovsiannikov*

Main category: physics.plasm-ph

TL;DR: The paper analyzes helical MHD turbulence, identifying an unstable term in the magnetic response function and proposing two stabilization methods: kinematic regime and turbulent dynamo regime. The latter leads to a steeper magnetic energy spectrum.


<details>
  <summary>Details</summary>
Motivation: To understand the destabilizing effects in helical MHD turbulence and explore mechanisms to stabilize the system, including spontaneous symmetry breaking.

Method: Two-loop field-theoretic analysis of incompressible helical MHD, examining loop diagrams and proposing stabilization via external parameters or symmetry breaking.

Result: Stabilization is achievable through kinematic or turbulent dynamo regimes, with the latter generating a nonzero magnetic field and altering the energy spectrum.

Conclusion: The study reveals stabilization mechanisms for helical MHD turbulence and demonstrates the impact of symmetry breaking on the magnetic energy spectrum.

Abstract: We present a two-loop field-theoretic analysis of incompressible helical
magnetohydrodynamics (MHD) in fully developed stationary turbulence. A key
feature of helical MHD is the appearance of an infrared-unstable ``mass-like''
term in the loop diagrams of the magnetic response function. Physically, this
term corresponds to the relevant perturbation of the Joule damping,
proportional to $\boldsymbol{\nabla} \times \boldsymbol{b}$ ($\boldsymbol{b} =$
magnetic field). Its presence destabilizes the trivial ground state $\langle
\boldsymbol{b} \rangle = 0$ and forces us to look for a mechanism for
stabilizing the system. We show that such stabilization can be achieved in two
ways: (i) by introducing into induction equation an external mass-like
parameter that precisely cancels these dangerous loop corrections (kinematic
regime), or (ii) via spontaneous breaking of the rotational symmetry, leading
to a new ground state with nonzero large-scale magnetic field (turbulent dynamo
regime). For the latter case, we study the two-loop correction to the
spontaneously generated magnetic field and demonstrate that Goldstone-like
corrections to Alfv\'en modes along with some other anisotropic structures
arise. Our results also confirm that the emergent mean magnetic field leads to
a steeper slope of the magnetic energy spectrum, $-11/3 + 2\gamma_{b\star}$
(with $\gamma_{b\star} = -0.1039 - 0.4202\rho^2$, for $|\rho| \leqslant 1$ as
the degree of helicity), compared to the Kolmogorov velocity spectrum of
$-11/3$, thereby breaking equipartition.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [33] [Nonadiabatic effect in high order harmonic generation revealed by a fully analytical method](https://arxiv.org/abs/2506.20257)
*Fengjian Sun,Pei Huang,Alexandra S. Landsman,Yanpeng Zhang,Liang-Wen Pi,Yuxi Fu*

Main category: physics.optics

TL;DR: A fully analytical method for high-order harmonic generation (HHG) is proposed, using strong-field approximation (SFA) and perturbation expansion. Third- and fifth-order expansions (TAE/FAE) are applied based on the Keldysh parameter, revealing nonadiabatic effects on HHG intensity.


<details>
  <summary>Details</summary>
Motivation: To analytically describe HHG and understand the role of nonadiabatic effects in electron dynamics.

Method: Based on SFA, the laser-induced dipole moment is expanded to third- (TAE) and fifth-order (FAE) with respect to the Keldysh parameter. TAE suits Î³â¤0.27, FAE for Î³â¤0.65.

Result: Higher-order terms capture nonadiabatic effects, while the zero-order term represents adiabatic effects. Nonadiabatic effects impact HHG intensity via electron dynamics.

Conclusion: The method provides insights into HHG by distinguishing adiabatic and nonadiabatic effects, with higher-order expansions extending applicability.

Abstract: We propose a fully analytical method for describing high-order harmonic
generation (HHG). This method is based on the strong-field approximation (SFA)
and utilizes the perturbation expansion method. Specifically, we expand the
laser-induced dipole moment to third-order analytical expansion (TAE) and
fifth-order expansion (FAE) with respect to the Keldysh parameter $\gamma$. The
TAE method is suitable for $\gamma\leqslant0.27$, while the FAE method can be
applied for $\gamma\leqslant 0.65$. We demonstrate that higher-order
perturbation terms capture the nonadiabatic effect, while the zero-order term
represents the adiabatic effect. Furthermore, we reveal that the nonadiabatic
effect influences HHG intensity by impacting electron dynamics.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [34] [Causal Operator Discovery in Partial Differential Equations via Counterfactual Physics-Informed Neural Networks](https://arxiv.org/abs/2506.20181)
*Ronald Katende*

Main category: cs.LG

TL;DR: A framework for discovering causal structure in PDEs using physics-informed neural networks and counterfactual perturbations, outperforming traditional methods in accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: To address limitations of classical methods like residual minimization or sparse regression in identifying causal structure in PDEs, by leveraging neural networks and functional interventions.

Method: Uses physics-informed neural networks and counterfactual perturbations, introducing causal sensitivity indices and structural deviation metrics to assess operator influence.

Result: Theoretically proves exact recovery of causal operator support under certain conditions and empirically validates the framework on synthetic and real-world datasets, outperforming standard methods.

Conclusion: The framework makes causal PDE discovery tractable and interpretable, grounded in structural causal models and variational residual analysis.

Abstract: We develop a principled framework for discovering causal structure in partial
differential equations (PDEs) using physics-informed neural networks and
counterfactual perturbations. Unlike classical residual minimization or sparse
regression methods, our approach quantifies operator-level necessity through
functional interventions on the governing dynamics. We introduce causal
sensitivity indices and structural deviation metrics to assess the influence of
candidate differential operators within neural surrogates. Theoretically, we
prove exact recovery of the causal operator support under restricted isometry
or mutual coherence conditions, with residual bounds guaranteeing
identifiability. Empirically, we validate the framework on both synthetic and
real-world datasets across climate dynamics, tumor diffusion, and ocean flows.
Our method consistently recovers governing operators even under noise,
redundancy, and data scarcity, outperforming standard PINNs and DeepONets in
structural fidelity. This work positions causal PDE discovery as a tractable
and interpretable inference task grounded in structural causal models and
variational residual analysis.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [35] [Reducing Self-Interaction Error in Transition-Metal Oxides with Different Exact-Exchange Fractions for Energy and Density](https://arxiv.org/abs/2506.20635)
*Harshan Reddy Gopidi,Ruiqi Zhang,Yanyong Wang,Abhirup Patra,Jianwei Sun,Adrienn Ruzsinszky,John P. Perdew,Pieremanuele Canepa*

Main category: cond-mat.mtrl-sci

TL;DR: The paper introduces r2SCANY@r2SCANX, a new method to improve the accuracy of DFT for strongly correlated materials, addressing functional and density-driven errors in r2SCAN.


<details>
  <summary>Details</summary>
Motivation: The limitations of r2SCAN in predicting properties of strongly correlated compounds due to functional and density-driven errors motivate the development of a more accurate method.

Method: The proposed r2SCANY@r2SCANX method uses different fractions of exact exchange (X and Y) to set electronic density and energy density functional approximation, reducing errors.

Result: r2SCANY@r2SCANX significantly improves predictions for 18 correlated oxides, outperforming DFT+U, and reduces O2 overbinding and uncertainties in oxidation energies and magnetic moments.

Conclusion: The method offers a computationally efficient and accurate alternative to hybrid functionals, enabling faster and more reliable simulations of strongly correlated materials.

Abstract: DFT is vital for materials discovery, and at the base of extensive molecular
and materials databases, chemical reaction predictions, and machine learning
potentials. The widespread use of DFT in chemistry and materials science aims
for "chemical accuracy," but this is limited by the unknown exchange and
correlation (XC) functional. A meta-GGA, the restored regularized strongly
constrained and appropriately normed, r r2SCAN XC functional, fulfils 17 exact
constraints of the XC energy. r2SCAN still appears inadequate at predicting
material properties of strongly correlated compounds. Inaccuracies of r2SCAN
arise from functional and density-driven errors, linked to the self-interaction
error. We introduce a new method, r2SCANY@r2SCANX, for simulating transition
metal oxides accurately. r2SCANY@r2SCANX utilizes different fractions of exact
exchange: X to set the electronic density, and Y to set the energy density
functional approximation. r2SCANY@r2SCANX addresses functional-driven and
density-driven inaccuracies. Using just one or two universal parameters,
r2SCANY@r2SCANX enhances the r2SCAN predictions of the properties of 18
correlated oxides, outperforming the highly parameterized DFT+U method. The O2
overbinding in r2SCAN (~0.3 eV/O2) reduces to just ~0.03 eV/O$_2$ with any X in
r2SCAN10@r2SCANX. Uncertainties for oxide oxidation energies and magnetic
moments are reduced by r2SCAN10@r2SCAN50, minimizing r2SCAN density-driven
errors. The computationally efficient r2SCAN10@r2SCAN is nearly as accurate as
the hybrid r2SCAN10 for oxidation energies. Thus, accurate energy differences
can be achieved by rate-limiting self-consistent iterations and geometry
optimizations with the efficient r2SCAN. Subsequently, expensive hybrid
functionals can be applied in a fast-to-execute single post-self-consistent
calculation, as in r2SCAN10@r2SCAN, which is 10 to 300 times faster than
r2SCAN10.

</details>


<div id='nlin.PS'></div>

# nlin.PS [[Back]](#toc)

### [36] [Synchronization of Dirac-Bianconi driven oscillators](https://arxiv.org/abs/2506.20163)
*Riccardo Muolo,IvÃ¡n LeÃ³n,Yuzuru Kato,Hiroya Nakao*

Main category: nlin.PS

TL;DR: The paper introduces a higher-order network theory approach for dynamical systems, focusing on interactions beyond nodes, such as links and higher-dimensional structures, using the Dirac-Bianconi operator to model oscillations and synchronization.


<details>
  <summary>Details</summary>
Motivation: Traditional network dynamics focus on nodes, neglecting group interactions and higher-dimensional structures. This work aims to address this gap by exploring topological signals and their interactions.

Method: The study uses the Dirac-Bianconi operator to couple topological signals (e.g., nodes and links) and applies the phase reduction method to analyze synchronization in oscillatory systems.

Result: The approach successfully models periodic oscillations and synchronization between Dirac-Bianconi driven oscillators, extending beyond node-based dynamics.

Conclusion: This framework provides a versatile tool for analyzing higher-order network dynamics and opens new avenues for studying oscillatory behaviors in complex systems.

Abstract: In dynamical systems on networks, one assigns the dynamics to nodes, which
are then coupled via links. This approach does not account for group
interactions and dynamics on links and other higher dimensional structures.
Higher-order network theory addresses this by considering variables defined on
nodes, links, triangles, and higher-order simplices, called topological signals
(or cochains). Moreover, topological signals of different dimensions can
interact through the Dirac-Bianconi operator, which allows coupling between
topological signals defined, for example, on nodes and links. Such interactions
can induce various dynamical behaviors, for example, periodic oscillations. The
oscillating system consists of topological signals on nodes and links whose
dynamics are driven by the Dirac-Bianconi coupling, hence, which we call it
Dirac-Bianconi driven oscillator. Using the phase reduction method, we obtain a
phase description of this system and apply it to the study of synchronization
between two such oscillators. This approach offers a way to analyze oscillatory
behaviors in higher-order networks beyond the node-based paradigm, while
providing a ductile modeling tool for node- and edge-signals.

</details>


<div id='nlin.CD'></div>

# nlin.CD [[Back]](#toc)

### [37] [Escape and transport in chaotic motion of charged particles in a magnetized plasma under the influence of two and three modes of drift waves](https://arxiv.org/abs/2506.20013)
*P. Haerter,E. D. Leonel,R. L. Viana*

Main category: nlin.CD

TL;DR: The study explores how two- and three-wave configurations impact particle escape and transport in tokamak edge plasmas, revealing enhanced escape rates with a third wave and transitions between anomalous and normal diffusion.


<details>
  <summary>Details</summary>
Motivation: To understand and control particle escape and transport in tokamak edge plasmas, which is crucial for optimizing confinement and heat load mitigation in divertors.

Method: A Hamiltonian model from drift-wave turbulence is used, analyzing test particle dynamics via PoincarÃ© maps, fractal escape basins, and entropy metrics.

Result: Introducing a third wave increases particle escape rates and homogenizes fluxes through stochastic scattering, while suppressing basin mixing. Transitions between anomalous and normal diffusion are observed.

Conclusion: Fractal structures and entropy-driven analysis suggest strategies to engineer transport properties, balancing chaos and order for optimized plasma confinement.

Abstract: This study investigates how two- and three-wave configurations govern
particle escape and transport in tokamak edge plasmas. Using a Hamiltonian
model derived from drift-wave turbulence, we analyze test particle dynamics
through Poincar\'e maps, fractal escape basins, and entropy metrics.
Introducing a third wave increases basin entropy, enhancing particle escape
rates while reducing basin boundary entropy, indicative of suppressed basin
mixing. Escape time analyses reveal resonant scattering disrupts coherent
transport pathways, linking fractal absorption patterns to heat load mitigation
in divertors. Characteristic transport is also analyzed and regimes transition
between anomalous $(\alpha > 1)$ and normal diffusion $(\alpha \approx 1)$,
two-wave systems sustain anomalous transport, while the third wave homogenizes
fluxes through stochastic scattering. Fractal structures in escape basins and
entropy-driven uncertainty quantification suggest strategies to engineer
transport properties, balancing chaos and order for optimized confinement.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [38] [AI Assistants to Enhance and Exploit the PETSc Knowledge Base](https://arxiv.org/abs/2506.20608)
*Barry Smith,Junchao Zhang,Hong Zhang,Lois Curfman McInnes,Murat Keceli,Archit Vasan,Satish Balay,Toby Isaac,Le Chen,Venkatram Vishwanath*

Main category: cs.AI

TL;DR: The paper discusses using LLMs to organize and utilize PETSc's fragmented knowledge base, enhancing accessibility and development workflows.


<details>
  <summary>Details</summary>
Motivation: PETSc's extensive but informal knowledge base is underutilized, hindering users and developers.

Method: The team built an LLM-powered system with RAG, reranking, and chatbots to integrate and activate PETSc's knowledge.

Result: Initial evaluations show improved access to PETSc-specific information and potential for scalable support.

Conclusion: The framework aims to evolve into a robust platform for scientific software, accelerating discovery.

Abstract: Generative AI, especially through large language models (LLMs), is
transforming how technical knowledge can be accessed, reused, and extended.
PETSc, a widely used numerical library for high-performance scientific
computing, has accumulated a rich but fragmented knowledge base over its three
decades of development, spanning source code, documentation, mailing lists,
GitLab issues, Discord conversations, technical papers, and more. Much of this
knowledge remains informal and inaccessible to users and new developers. To
activate and utilize this knowledge base more effectively, the PETSc team has
begun building an LLM-powered system that combines PETSc content with custom
LLM tools -- including retrieval-augmented generation (RAG), reranking
algorithms, and chatbots -- to assist users, support developers, and propose
updates to formal documentation. This paper presents initial experiences
designing and evaluating these tools, focusing on system architecture, using
RAG and reranking for PETSc-specific information, evaluation methodologies for
various LLMs and embedding models, and user interface design. Leveraging the
Argonne Leadership Computing Facility resources, we analyze how LLM responses
can enhance the development and use of numerical software, with an initial
focus on scalable Krylov solvers. Our goal is to establish an extensible
framework for knowledge-centered AI in scientific software, enabling scalable
support, enriched documentation, and enhanced workflows for research and
development. We conclude by outlining directions for expanding this system into
a robust, evolving platform that advances software ecosystems to accelerate
scientific discovery.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [39] [Simulating acoustically-actuated flows in complex microchannels using the volume penalization technique](https://arxiv.org/abs/2506.20034)
*Khemraj Gautam Kshetri,Amneet Pal Singh Bhalla,Nitesh Nama*

Main category: physics.flu-dyn

TL;DR: A volume penalization technique for simulating acoustically-actuated flows in complex microchannels is presented, segregating nonlinear responses into harmonic and time-averaged sub-problems with efficient solvers and novel force evaluation.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of simulating acoustically-actuated flows in geometrically complex domains, which traditionally rely on body-fitted methods, by proposing a volume penalization technique.

Method: Segregates the nonlinear response into harmonic (first-order) and time-averaged (second-order) sub-problems, using volume penalized systems with distinct solvers (MUMPS for first-order, iterative projection method for second-order). Includes a novel contour integration technique for acoustic radiation force.

Result: Demonstrates excellent agreement with body-fitted grid results, identifies optimal penalty factors and smearing widths, and shows solver scalability.

Conclusion: The volume penalization method is effective for acoustic streaming problems, offering a viable alternative to body-fitted methods.

Abstract: We present a volume penalization technique for simulating
acoustically-actuated flows in geometrically complex microchannels. Using a
perturbation approach, the nonlinear response of an acoustically-actuated
compressible Newtonian fluid moving over obstacles or flowing in a
geometrically complex domain is segregated into two sub-problems: a harmonic
first-order problem and a time-averaged second-order problem, where the latter
utilizes forcing terms and boundary conditions arising from the first-order
solution. This segregation results in two distinct volume penalized systems of
equations. The no-slip boundary condition at the fluid-solid interface is
enforced by prescribing a zero structure velocity for the first-order problem,
while spatially varying Stokes drift -- which depends on the gradient of the
first-order solution -- is prescribed as the structure velocity for the
second-order problem. The harmonic first-order system is solved via MUMPS
direct solver, whereas the steady state second-order system is solved
iteratively using a novel projection method-based preconditioner. The
preconditioned iterative solver for the second-order system is demonstrated to
be highly effective and scalable with respect to increasing penalty force and
grid resolution, respectively. A novel contour integration technique to
evaluate the acoustic radiation force on an immersed object is also proposed.
Through test cases featuring representative microfluidic geometries, we
demonstrate excellent agreement between the volume penalized and body-fitted
grid. We also identify suitable penalty factors and interfacial smearing widths
to accurately capture the first- and second-order solutions. These results
provide first-of-its-kind empirical evidence of the efficacy of the volume
penalization method for simulating acoustic streaming problems that have so far
been analyzed using body-fitted methods in literature.

</details>


<div id='nucl-ex'></div>

# nucl-ex [[Back]](#toc)

### [40] [Single-event neutron time-of-flight spectroscopy with a petawatt-laser-driven neutron source](https://arxiv.org/abs/2506.20026)
*M. A. MillÃ¡n-Callado,S. Scheuren,A. Alejo,J. Benlliure,R. Beyer,T. E. Cowan,B. FernÃ¡ndez,E. Griesmayer,A. R. Junghans,J. Kohl,F. Kroll,J. Metzkes-Ng,I. Prencipe,J. M. Quesada,M. Rehwald,C. RÃ¶del,T. RodrÃ­guez-GonzÃ¡lez,U. Schramm,M. Roth,R. Stefanikova,S. Urlass,C. Weiss,K. Zeil,T. Ziegler,C. Guerrero*

Main category: nucl-ex

TL;DR: Proof-of-concept experiment demonstrates stable laser-driven neutron production for fast neutron spectroscopy, achieving 6-7e7 neutrons/shot at 1 MeV+ energies over 200+ shots.


<details>
  <summary>Details</summary>
Motivation: Fast neutron-induced reactions are vital for nuclear research and applications, but traditional sources are declining. Laser-driven neutron sources (LDNSs) offer advantages like ultrashort pulses and high flux, but their stability and detector compatibility need validation.

Method: Conducted a pitcher-catcher experiment at the DRACO PW laser, producing neutrons stably over 200+ shots. Used a diamond detector for time-of-flight measurements, validated by Monte Carlo simulations.

Result: Achieved 6-7e7 neutrons/shot above 1 MeV, with consistent reaction rates. Demonstrated LDNSs' potential for high-flux, scalable neutron studies.

Conclusion: LDNSs are a promising platform for fast neutron research, especially for short-lived isotopes or high-flux requirements, supported by advancements in laser technology.

Abstract: Fast neutron-induced nuclear reactions are crucial for advancing our
understanding of fundamental nuclear processes, stellar nucleosynthesis, and
applications, including reactor safety, medical isotope production, and
materials research. With many research reactors being phased out, compact
accelerator-based neutron sources are becoming increasingly important.
Laser-driven neutron sources (LDNSs) offer unique advantages -- ultrashort
neutron pulsees for superior energy resolution, high per-pulse flux, and a
drastically reduced footprint. However, their use in single-event fast neutron
spectroscopy remains unproven, requiring stable multi-shot operation and
detectors capable of functioning in the extreme environment of petawatt-class
laser-plasma interactions. Here, we present a proof-of-concept experiment at
the DRACO~PW laser in a pitcher-catcher configuration, stably producing 6-7e7
neutrons/shot with energies above 1 MeV, over more than 200 shots delivered at
a shot-per-minute rate. Neutron time-of-flight measurements were performed
using a single-crystal diamond detector, which is located only 1.5 m away from
the source and capable of resolving individual neutron-induced reactions.
Observed reaction rates are consistent with Monte Carlo simulations inferred by
real-time diagnostics of accompanying gamma, ion, and electron fluxes. With the
recent advances in repetition rate, targetry, and ion acceleration efficiency,
this work establishes LDNSs as a promising, scalable platform for future fast
neutron-induced reaction studies, particularly for measurements involving
short-lived isotopes or requiring high instantaneous neutron flux.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [41] [Fast entropy-regularized SDP relaxations for permutation synchronization](https://arxiv.org/abs/2506.20191)
*Michael Lindsey,Yunpeng Shi*

Main category: math.OC

TL;DR: Fast randomized algorithms for solving SDP relaxations of partial permutation synchronization (PPS) improve multi-image matching, offering theoretical and practical advantages over traditional methods.


<details>
  <summary>Details</summary>
Motivation: PPS is critical for multi-image matching and 3D reconstruction, but standard relaxations suffer from optimizer non-uniqueness.

Method: Uses entropy-regularized SDP tailored to PPS structure, with randomized solvers and rounding procedures for combinatorial solutions.

Result: Achieves state-of-the-art performance in speed and accuracy on synthetic and real-world datasets.

Conclusion: Entropy-regularized SDP is superior for PPS, combining theoretical robustness and practical efficiency.

Abstract: We introduce fast randomized algorithms for solving semidefinite programming
(SDP) relaxations of the partial permutation synchronization (PPS) problem, a
core task in multi-image matching with significant relevance to 3D
reconstruction. Our methods build on recent advances in entropy-regularized
semidefinite programming and are tailored to the unique structure of PPS, in
which the unknowns are partial permutation matrices aligning sparse and noisy
pairwise correspondences across images. We prove that entropy regularization
resolves optimizer non-uniqueness in standard relaxations, and we develop a
randomized solver with nearly optimal scaling in the number of observed
correspondences. We also develop several rounding procedures for recovering
combinatorial solutions from the implicitly represented primal solution
variable, maintaining cycle consistency if desired without harming
computational scaling. We demonstrate that our approach achieves
state-of-the-art performance on synthetic and real-world datasets in terms of
speed and accuracy. Our results highlight PPS as a paradigmatic setting in
which entropy-regularized SDP admits both theoretical and practical advantages
over traditional low-rank or spectral techniques.

</details>


### [42] [First-order methods for stochastic and finite-sum convex optimization with deterministic constraints](https://arxiv.org/abs/2506.20630)
*Zhaosong Lu,Yifeng Xiao*

Main category: math.OC

TL;DR: The paper proposes stochastic first-order methods to find solutions with deterministic constraint bounds and expected optimality gaps, addressing limitations of existing methods that only ensure expected feasibility.


<details>
  <summary>Details</summary>
Motivation: Existing methods for stochastic convex optimization problems with deterministic constraints often provide solutions that are only expectedly feasible, which may not suffice in applications requiring near-certain constraint satisfaction.

Method: The authors introduce stochastic first-order methods, including an accelerated stochastic gradient (ASG) scheme and a modified variance-reduced ASG scheme, applied to quadratic penalty subproblems with carefully chosen parameters.

Result: The methods achieve first-order oracle complexity bounds for computing solutions with deterministic constraint violations and expected optimality gaps within a tolerance.

Conclusion: The proposed methods effectively address the need for nearly certain constraint satisfaction in stochastic optimization, offering practical improvements over existing approaches.

Abstract: In this paper, we study a class of stochastic and finite-sum convex
optimization problems with deterministic constraints. Existing methods
typically aim to find an $\epsilon$-$expectedly\ feasible\ stochastic\ optimal$
solution, in which the expected constraint violation and expected optimality
gap are both within a prescribed tolerance $\epsilon$. However, in many
practical applications, constraints must be nearly satisfied with certainty,
rendering such solutions potentially unsuitable due to the risk of substantial
violations. To address this issue, we propose stochastic first-order methods
for finding an $\epsilon$-$surely\ feasible\ stochastic\ optimal$
($\epsilon$-SFSO) solution, where the constraint violation is deterministically
bounded by $\epsilon$ and the expected optimality gap is at most $\epsilon$.
Our methods apply an accelerated stochastic gradient (ASG) scheme or a modified
variance-reduced ASG scheme $only\ once$ to a sequence of quadratic penalty
subproblems with appropriately chosen penalty parameters. We establish
first-order oracle complexity bounds for the proposed methods in computing an
$\epsilon$-SFSO solution. As a byproduct, we also derive first-order oracle
complexity results for sample average approximation method in computing an
$\epsilon$-SFSO solution of the stochastic optimization problem using our
proposed methods to solve the sample average problem.

</details>


<div id='math.FA'></div>

# math.FA [[Back]](#toc)

### [43] [A matrix-valued measure associated to the derivatives of a function of generalised bounded deformation](https://arxiv.org/abs/2506.19978)
*Gianni Dal Maso,Davide Donati*

Main category: math.FA

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We associate to every function $u\in GBD(\Omega)$ a measure $\mu_u$ with
values in the space of symmetric matrices, which generalises the distributional
symmetric gradient $Eu$ defined for functions of bounded deformation. We show
that this measure $\mu_u$ admits a decomposition as the sum of three mutually
singular matrix-valued measures $\mu^a_u$, $\mu^c_u$, and $\mu^j_u$, the
absolutely continuous part, the Cantor part, and the jump part, as in the case
of $BD(\Omega)$ functions. We then characterise the space $GSBD(\Omega)$,
originally defined only by slicing, as the space of functions $u\in
GBD(\Omega)$ such that $\mu^c_u=0$.

</details>


### [44] [On gradient descent-ascent flows in metric spaces](https://arxiv.org/abs/2506.20258)
*Noboru Isobe,Sho Shimoyama*

Main category: math.FA

TL;DR: The paper develops a theory for Gradient Descent-Ascent (GDA) flows in general metric spaces, focusing on Wasserstein spaces, proving existence, uniqueness, and stability under convex-concave assumptions.


<details>
  <summary>Details</summary>
Motivation: Extending GDA flows from Hilbert/Banach spaces to general metric spaces, particularly Wasserstein spaces, which was previously unexplored.

Method: Formulates GDA flows as solutions to evolution variational inequalities (EVIs), uses a minimizing-maximizing movement scheme, and proves results via a minimax theorem on metric spaces.

Result: Existence, uniqueness, stability, Î»-contraction, error estimates, regularization effects, and exponential decay of duality gap. For Wasserstein spaces, global existence and exponential convergence to saddle points.

Conclusion: The framework unifies and extends existing analyses, providing a metric-geometric perspective on GDA dynamics in nonlinear/non-smooth settings.

Abstract: Gradient descent-ascent (GDA) flows play a central role in finding saddle
points of bivariate functionals, with applications in optimization, game
theory, and robust control. While they are well-understood in Hilbert and
Banach spaces via maximal monotone operator theory, their extension to general
metric spaces, particularly Wasserstein spaces, has remained largely
unexplored. In this paper, we develop a mathematical theory of GDA flows on the
product of two complete metric spaces, formulating them as solutions to a
system of evolution variational inequalities (EVIs) driven by a proper, closed
functional $\phi$. Under mild convex-concave and regularity assumptions on
$\phi$, we prove the existence, uniqueness, and stability of the flows via a
novel minimizing-maximizing movement scheme and a minimax theorem on metric
spaces. We establish a $\lambda$-contraction property, derive a quantitative
error estimate for the discrete scheme, and demonstrate regularization effects
analogous to classical gradient flows. Moreover, we obtain an exponential decay
bound for the Nikaid\^o--Isoda duality gap along the flow. Focusing on
Wasserstein spaces over Hilbert spaces, we show the global existence in time
and the exponential convergence of the Wasserstein GDA flow to the unique
saddle point for strongly convex-concave functionals. Our framework unifies and
extends existing analyses, offering a metric-geometric perspective on GDA
dynamics in nonlinear and non-smooth settings.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [45] [Pivot probabilities and norm effects in Gaussian elimination for $Î²$-ensembles](https://arxiv.org/abs/2506.20470)
*Kenji Gunawan,John Peca-Medlin*

Main category: math.PR

TL;DR: Exact pivot probabilities in GEPP for 2x2 GUE matrices are derived, resolving prior discrepancies. Tridiagonal Î²-ensembles align with theory. Open question on norm choices is posed.


<details>
  <summary>Details</summary>
Motivation: To resolve discrepancies between theoretical predictions and empirical observations of pivot probabilities in GEPP for specific matrix ensembles.

Method: Analyze pivot probabilities in GEPP for 2x2 random matrices, focusing on GUE and Dumitriu-Edelman tridiagonal Î²-ensembles.

Result: Exact pivot probability for GUE matrices under LAPACK-style implementations is derived; tridiagonal Î²-ensembles match theoretical expectations.

Conclusion: The study resolves prior discrepancies and poses an open question on pivot behavior under alternative norms.

Abstract: We analyze pivot probabilities in Gaussian elimination with partial pivoting
(GEPP) for $2 \times 2$ random matrix ensembles. For GUE matrices, we resolve a
previously reported discrepancy between theoretical predictions and empirical
observations by deriving the exact pivot probability under standard
LAPACK-style implementations. We further show that Dumitriu-Edelman tridiagonal
$\beta$-ensembles agree with the earlier theoretical expectations. Finally, we
propose an open question on pivot behavior under alternative norm choices,
supported by empirical evidence.

</details>


### [46] [Parabolic Anderson Model in the Hyperbolic Space. Part II: Quenched Asymptotics](https://arxiv.org/abs/2506.20147)
*Xi Geng,Sheng Wang,Weijun Xu*

Main category: math.PR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We establish the exact quenched asymptotic growth of the solution to the
parabolic Anderson model (PAM) in the hyperbolic space with a regular,
stationary, time-independent Gaussian potential. More precisely, we show that
with probability one, the solution $u$ to PAM with constant initial data has
pointwise growth asymptotics \[ u(t,x)\sim e^{L^{*}t^{5/3}+o(t^{5/3})} \] as $t
\rightarrow +\infty$. Both the power $t^{5/3}$ on the exponential and the exact
value of $L^*$ are different from their counterparts in the Euclidean
situation. They are determined through an explicit optimisation procedure. Our
proof relies on certain fine localisation techniques, which also reveals a
stronger non-Euclidean localisation mechanism.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [47] [How to use quantum computers for biomolecular free energies](https://arxiv.org/abs/2506.20587)
*Jakob GÃ¼nther,Thomas Weymuth,Moritz Bensberg,Freek Witteveen,Matthew S. Teynor,F. Emil Thomasen,Valentina Sora,William Bro-JÃ¸rgensen,Raphael T. Husistein,Mihael Erakovic,Marek Miller,Leah Weisburn,Minsik Cho,Marco Eckhoff,Aram W. Harrow,Anders Krogh,Troy Van Voorhis,Kresten Lindorff-Larsen,Gemma Solomon,Markus Reiher,Matthias Christandl*

Main category: quant-ph

TL;DR: The paper introduces FreeQuantum, a computational pipeline combining quantum computing and machine learning for accurate free energy calculations in biochemical processes, demonstrated with a ruthenium-based anticancer drug.


<details>
  <summary>Details</summary>
Motivation: Free energy calculations are crucial for understanding biochemical processes, but current methods struggle with accuracy for large biomolecules. Quantum computing offers potential solutions but requires integration with classical techniques.

Method: A two-fold quantum embedding strategy links accurate quantum-mechanical data to biomolecular potential energy, using machine learning. Traditional quantum chemical methods and quantum computing are analyzed for scalability and accuracy.

Result: The approach is viable for molecular recognition, demonstrated with a ruthenium-based anticancer drug. Quantum computers, once meeting requirements, can enhance accuracy and efficiency.

Conclusion: FreeQuantum combines quantum computing's speedups with classical machine learning, enabling accurate modeling of large biochemical systems.

Abstract: Free energy calculations are at the heart of physics-based analyses of
biochemical processes. They allow us to quantify molecular recognition
mechanisms, which determine a wide range of biological phenomena from how cells
send and receive signals to how pharmaceutical compounds can be used to treat
diseases. Quantitative and predictive free energy calculations require
computational models that accurately capture both the varied and intricate
electronic interactions between molecules as well as the entropic contributions
from motions of these molecules and their aqueous environment. However,
accurate quantum-mechanical energies and forces can only be obtained for small
atomistic models, not for large biomacromolecules. Here, we demonstrate how to
consistently link accurate quantum-mechanical data obtained for substructures
to the overall potential energy of biomolecular complexes by machine learning
in an integrated algorithm. We do so using a two-fold quantum embedding
strategy where the innermost quantum cores are treated at a very high level of
accuracy. We demonstrate the viability of this approach for the molecular
recognition of a ruthenium-based anticancer drug by its protein target,
applying traditional quantum chemical methods. As such methods scale
unfavorable with system size, we analyze requirements for quantum computers to
provide highly accurate energies that impact the resulting free energies. Once
the requirements are met, our computational pipeline FreeQuantum is able to
make efficient use of the quantum computed energies, thereby enabling quantum
computing enhanced modeling of biochemical processes. This approach combines
the exponential speedups of quantum computers for simulating interacting
electrons with modern classical simulation techniques that incorporate machine
learning to model large molecules.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [48] [On geometry of $Q^{(2k)}_g$-curvature](https://arxiv.org/abs/2506.20165)
*Mingxiang Li,Juncheng Wei,Xingwang Xu*

Main category: math.DG

TL;DR: The paper studies the geometry of $Q$-curvature, focusing on a conformal metric with non-negative $nth$-order $Q$-curvature and scalar curvature. It shows non-negative Ricci curvature and polynomial growth rates for certain curvature functions, along with gap theorems for specific cases.


<details>
  <summary>Details</summary>
Motivation: To understand the geometric properties of $Q$-curvature, particularly under non-negativity assumptions, and to derive growth controls and gap theorems for curvature functions.

Method: Analyzes a conformal metric $g=e^{2u}|dx|^2$ on $\mathbb{R}^n$ with non-negative $Q$-curvature and scalar curvature, proving Ricci curvature non-negativity and polynomial growth bounds for symmetric functions of Ricci curvature and $Q$-curvature.

Result: Demonstrates non-negative Ricci curvature, polynomial growth rates for curvature functions, and gap theorems for $k=1$ or $2$.

Conclusion: The study provides insights into the behavior of $Q$-curvature and related geometric quantities under specific assumptions, with implications for understanding conformal metrics.

Abstract: The main purpose of current article is to study the geometry of
$Q$-curvature. For simplicity, we start with a very simple model: a complete
and conformal metric $g=e^{2u}|dx|^2$ on $\mathbb{R}^n$. We assume the metric
$g$ has non-negative $nth$-order $Q$-curvature and non-negative scalar
curvature. We show that the Ricci curvature is non-negative. We further assume
that the metric $g$ is not degenerate in the sense that the isoperimetric ratio
near the end is positive. With this extra assumption, we show that the growth
rate of $kth$ elementary symmetric function $\sigma_k(g)$ of Ricci curvature
over geodesic ball of radius $r$ is at most polynomial in $r$ with order $n-2k$
for all $1 \leq k \leq \frac{n-2}{2}$. Similarly, we are able to show that the
same growth control holds for $2kth$-order $Q$-curvature. Finally, we also
observe that for $k=1$ or $2$, the gap theorems for $Q^{(2k)}_g$ hold true.

</details>
