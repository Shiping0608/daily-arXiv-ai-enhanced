<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 18]
- [math.AP](#math.AP) [Total: 9]
- [physics.comp-ph](#physics.comp-ph) [Total: 3]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 6]
- [math-ph](#math-ph) [Total: 2]
- [cs.LG](#cs.LG) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [gr-qc](#gr-qc) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Inverse initial data reconstruction for Maxwell's equations via time-dimensional reduction method](https://arxiv.org/abs/2506.20777)
*Thuy T. Le,Cong B. Van,Trong D. Dang,Loc H. Nguyen*

Main category: math.NA

TL;DR: The paper addresses an inverse problem for the time-dependent Maxwell system, aiming to recover the initial electric field in a bounded domain using boundary measurements. A time-dimension reduction method and quasi-reversibility approach are proposed, validated by numerical experiments.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the need to recover the initial electric field in anisotropic media without requiring initial magnetic field or charge density data, addressing practical constraints.

Method: A time-dimension reduction approach projects the electric field onto a Legendre polynomial-exponential basis, transforming the problem into spatial systems. The quasi-reversibility method is used for reconstruction.

Result: Numerical experiments confirm the method's accuracy, even with 10% noise, demonstrating robustness for realistic inverse electromagnetic problems.

Conclusion: The proposed approach effectively solves the under-determined inverse problem, with theoretical and numerical validation supporting its applicability.

Abstract: We study an inverse problem for the time-dependent Maxwell system in an
inhomogeneous and anisotropic medium. The objective is to recover the initial
electric field $\mathbf{E}_0$ in a bounded domain $\Omega \subset
\mathbb{R}^3$, using boundary measurements of the electric field and its normal
derivative over a finite time interval. Informed by practical constraints, we
adopt an under-determined formulation of Maxwell's equations that avoids the
need for initial magnetic field data and charge density information. To address
this inverse problem, we develop a time-dimension reduction approach by
projecting the electric field onto a finite-dimensional Legendre
polynomial-exponential basis in time. This reformulates the original space-time
problem into a sequence of spatial systems for the projection coefficients. The
reconstruction is carried out using the quasi-reversibility method within a
minimum-norm framework, which accommodates the inherent non-uniqueness of the
under-determined setting. We prove a convergence theorem that ensures the
quasi-reversibility solution approximates the true solution as the noise and
regularization parameters vanish. Numerical experiments in a fully
three-dimensional setting validate the method's performance. The reconstructed
initial electric field remains accurate even with $10\%$ noise in the data,
demonstrating the robustness and applicability of the proposed approach to
realistic inverse electromagnetic problems.

</details>


### [2] [Boundary integral equation analysis for spheroidal suspensions](https://arxiv.org/abs/2506.20809)
*Leo Crowder,Tianyue Li,Eduardo Corona,Shravan Veerapaneni*

Main category: math.NA

TL;DR: A fast, spectrally accurate method for evaluating boundary integral operators (BIOs) on spheroid suspensions, combining spheroidal harmonics and the fast multipole method.


<details>
  <summary>Details</summary>
Motivation: To efficiently and accurately evaluate BIOs for dense, polydisperse suspensions of spheroids, applicable to Laplace and Stokes problems.

Method: Derives layer potential operators in spheroidal harmonic basis, uses analytical expressions for near-field, and accelerates far-field with the fast multipole method.

Result: Validated accuracy and efficiency for dense suspensions, handling hundreds of particles on a single processor.

Conclusion: The method is effective for BIO evaluation in particulate suspension flows, extending to both Laplace and Stokes problems.

Abstract: In this work, we provide a fast, spectrally accurate method for the
evaluation of boundary integral operators (BIOs) on a suspension of prolate and
oblate spheroids. We first derive formulas for the standard layer potential
operators for the Laplace equation applied to an expansion of the integral
densities in the appropriate spheroidal harmonic basis. These then lead to
analytical expressions in solid harmonics that allow spectrally accurate
evaluation of near-field particle interactions. Finally, a standard quadrature
scheme is used to evaluate smooth, far-field interactions; these are then
accelerated using the fast multipole method.
  Through a number of numerical test cases, we verify the accuracy and
efficiency of our BIO evaluation framework for dense, polydisperse suspensions
of spheroids. Through the use of standard formulas linking Stokes and Laplace
potentials, we show our scheme can be readily applied to problems involving
particulate suspension flows. For both Laplace and Stokes, our method allows us
to evaluate BIOs for suspensions up to hundreds of particles on a single
processor.

</details>


### [3] [Multicontinuum Homogenization for Poroelasticity Model](https://arxiv.org/abs/2506.20890)
*Dmitry Ammosov,Mohammed Al-Kobaisi,Yalchin Efendiev*

Main category: math.NA

TL;DR: The paper derives multicontinuum poroelasticity models using homogenization to address high-contrast properties in porous media, improving accuracy over standard methods.


<details>
  <summary>Details</summary>
Motivation: Standard homogenization fails for high-contrast poroelastic media due to lack of macroscopic parameters, necessitating multicontinuum approaches.

Method: Uses multicontinuum homogenization, coupled constraint cell problems, and fine-scale field expansions to derive generalized models.

Result: Numerical experiments show high accuracy of the proposed multicontinuum models for heterogeneous media.

Conclusion: The multicontinuum approach effectively addresses computational challenges in high-contrast poroelasticity, offering accurate solutions.

Abstract: In this paper, we derive multicontinuum poroelasticity models using the
multicontinuum homogenization method. Poroelasticity models are widely used in
many areas of science and engineering to describe coupled flow and mechanics
processes in porous media. However, in many applications, the properties of
poroelastic media possess high contrast, presenting serious computational
challenges. It is well known that standard homogenization approaches often fail
to give an accurate solution due to the lack of macroscopic parameters.
Multicontinuum approaches allow us to consider such cases by defining several
average states known as continua. In the field of poroelasticity,
multiple-network models arising from the multiple porous media theory are
representatives of these approaches. In this work, we extend previous findings
by deriving the generalized multicontinuum poroelasticity model. We apply the
recently developed multicontinuum homogenization method and provide a rigorous
derivation of multicontinuum equations. For this purpose, we formulate coupled
constraint cell problems in oversampled regions to consider different
homogenized effects. Then, we obtain a multicontinuum expansion of the
fine-scale fields and derive the multicontinuum model supposing the smoothness
of macroscopic variables. We present the most general version of equations and
the simplified ones based on our numerical experiments. Numerical results are
presented for different heterogeneous media cases and demonstrate the high
accuracy of our proposed multicontinuum models.

</details>


### [4] [Two-dimensional greedy randomized Kaczmarz methods for solving large-scale linear systems](https://arxiv.org/abs/2506.20940)
*Tao Li,Meng-Long Xiao,Xin-Fang Zhang*

Main category: math.NA

TL;DR: The paper introduces novel two-dimensional randomized Kaczmarz methods, including greedy and semi-randomized versions, for solving large-scale linear systems, proving convergence and demonstrating superior performance in computing time.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency of existing methods for solving large-scale linear systems, the authors propose improved randomized Kaczmarz methods with better convergence properties.

Method: The paper introduces two-dimensional randomized Kaczmarz methods, including greedy and semi-randomized versions, leveraging probability-based row selection and residual vector analysis.

Result: The proposed methods converge to the least-norm solution for consistent linear systems and outperform existing methods in computing time, as shown in numerical experiments.

Conclusion: The novel two-dimensional Kaczmarz methods offer efficient solutions for large-scale linear systems, with theoretical guarantees and practical advantages.

Abstract: In this paper, we consider a novel two-dimensional randomized Kaczmarz method
and its improved version with simple random sampling, which chooses two active
rows with probability proportional to the square of their cross-product-like
constant, for solving large-scale linear systems. From the greedy selection
strategy with grasping two larger entries of the residual vector at each
iteration, we then devise a two-dimensional greedy randomized Kaczmarz method.
To improve the above methods further, motivated by the semi-randomized Kaczmarz
method and Chebyshev's law of large numbers, we propose a two-dimensional
semi-randomized Kaczmarz method and its modified version with simple random
sampling, which is particularly advantageous for big data problems.
Theoretically, we prove that the proposed methods converge to the unique
least-norm solution of the consistent linear systems. Numerical results on some
practical applications illustrate the superiority of the proposed methods
compared with some existing ones in terms of computing time.

</details>


### [5] [An energy-stable parametric finite element method for the Willmore flow in three dimensions](https://arxiv.org/abs/2506.21025)
*Weizhu Bao,Yifei Li,Dongmin Wang*

Main category: math.NA

TL;DR: Novel energy-stable parametric finite element methods (ES-PFEM) are developed for Willmore flow and curvature-dependent geometric gradient flows, ensuring energy stability through new geometric identities and implicit schemes.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of maintaining energy stability and mesh quality in numerical simulations of curvature-dependent geometric flows, particularly for Willmore flow and Gauss curvature flow.

Method: Uses two novel geometric identities to derive a new variational formulation, combined with parametric finite element methods to create an implicit fully discrete scheme. Includes tangential velocity control for mesh quality improvement.

Result: The method successfully preserves energy dissipation and maintains good mesh quality in surface evolution under Willmore flow, demonstrated through extensive numerical experiments.

Conclusion: The ES-PFEM provides a robust framework for curvature-dependent flows, offering insights for general applications and improving mesh quality in PFEM.

Abstract: This work develops novel energy-stable parametric finite element methods
(ES-PFEM) for the Willmore flow and curvature-dependent geometric gradient
flows of surfaces in three dimensions. The key to achieving the energy
stability lies in the use of two novel geometric identities: (i) a reformulated
variational form of the normal velocity field, and (ii) incorporation of the
temporal evolution of the mean curvature into the governing equations. These
identities enable the derivation of a new variational formulation. By using the
parametric finite element method, an implicit fully discrete scheme is
subsequently developed, which maintains the energy dissipative property at the
fully discrete level. Based on the ES-PFEM, comprehensive insights into the
design of ES-PFEM for general curvature-dependent geometric gradient flows and
a new understanding of mesh quality improvement in PFEM are provided. In
particular, we develop the first PFEM for the Gauss curvature flow of surfaces.
Furthermore, a tangential velocity control methodology is applied to improve
the mesh quality and enhance the robustness of the proposed numerical method.
Extensive numerical experiments confirm that the proposed method preserves
energy dissipation properties and maintain good mesh quality in the surface
evolution under the Willmore flow.

</details>


### [6] [Entropy-stable in- and outflow boundary conditions for the compressible Navier-Stokes equations](https://arxiv.org/abs/2506.21065)
*Magnus Svärd,Anita Gjesteland*

Main category: math.NA

TL;DR: Proposed inflow/outflow boundary conditions for compressible Navier-Stokes equations, ensuring entropy, mass, and energy estimates. Demonstrated approximation with entropy-stable finite-volume schemes and validated robustness numerically.


<details>
  <summary>Details</summary>
Motivation: To develop boundary conditions for compressible Navier-Stokes equations that ensure physical consistency (entropy, mass, energy) and can be integrated with entropy-stable numerical schemes.

Method: Proposed boundary conditions, proved a priori estimates, and demonstrated approximation with entropy-stable finite-volume schemes. Applied to numerical computations for validation.

Result: Achieved robust boundary conditions with proven estimates for entropy, mass, and energy. Numerical tests confirmed the method's effectiveness.

Conclusion: The proposed boundary conditions are effective and compatible with entropy-stable schemes, validated by numerical robustness.

Abstract: We propose inflow and outflow boundary conditions for the compressible
Navier-Stokes equations and prove that they allow a priori estimates of the
entropy, mass and total energy. Furthermore, we demonstrate how to approximate
these boundary conditions in conjunction with an entropy-stable finite-volume
scheme. The method is also applicable to other types of entropy-stable schemes.
Finally, we carry out some numerical computations with the finite-volume scheme
and demonstrate their robustness.

</details>


### [7] [Inverse source problem with a posteriori interior measurements for space-time fractional diffusion equations](https://arxiv.org/abs/2506.21070)
*Kai Yu,Zhiyuan Li,Yikan Liu*

Main category: math.NA

TL;DR: The paper addresses an inverse source problem for space-time fractional diffusion equations using interior measurements, proving uniqueness and proposing a numerical reconstruction method with Tikhonov regularization and the Levenberg-Marquardt algorithm.


<details>
  <summary>Details</summary>
Motivation: To solve the inverse source problem for fractional diffusion equations, leveraging the memory effect of fractional derivatives and unique continuation properties.

Method: Reformulates the problem as an optimization task with Tikhonov regularization and employs the Levenberg-Marquardt method for numerical reconstruction from noisy data.

Result: Uniqueness is proven, and numerical examples demonstrate the algorithm's efficiency and accuracy.

Conclusion: The proposed method effectively reconstructs unknown sources in fractional diffusion equations, validated by numerical results.

Abstract: This paper investigates an inverse source problem for space-time fractional
diffusion equations from a posteriori interior measurements. The uniqueness
result is established by the memory effect of fractional derivatives and the
unique continuation property. For the numerical reconstruction, the inverse
problem is reformulated as an optimization problem with the Tikhonov
regularization. We use the Levenberg-Marquardt method to identity the unknown
source from noisy measurements. Finally, we give some numerical examples to
illustrate the efficiency and accuracy of the proposed algorithm.

</details>


### [8] [Robust space-time multiscale upscaling via multicontinuum homogenization for evolving perforated media](https://arxiv.org/abs/2506.21104)
*Wei Xie,Viet Ha Hoang,Yin Yang,Yunqing Huang*

Main category: math.NA

TL;DR: A multiscale modeling framework is developed for time-evolving perforated domains, using multicontinuum homogenization to derive effective macroscopic equations in shrinking domains.


<details>
  <summary>Details</summary>
Motivation: Time-evolving perforated domains in engineering and geoscientific applications pose computational challenges due to dynamic fine-scale geometries.

Method: The method uses multicontinuum homogenization, distinguishing continua by physical characteristics and coupling them via space-time local cell problems on representative volume elements.

Result: The upscaled system provides computable macroscopic coefficients, validated by numerical experiments for accuracy and efficiency.

Conclusion: The framework is robust, generalizable, and suitable for large-scale simulations of complex time-dependent problems.

Abstract: Time-evolving perforated domains arise in many engineering and geoscientific
applications, including reactive transport, particle deposition, and structural
degradation in porous media. Accurately capturing the macroscopic behavior of
such systems poses significant computational challenges due to the dynamic
fine-scale geometries. In this paper, we develop a robust and generalizable
multiscale modeling framework based on multicontinuum homogenization to derive
effective macroscopic equations in shrinking domains. The method distinguishes
multiple continua according to the physical characteristics (e.g., channel
widths), and couples them via space-time local cell problems formulated on
representative volume elements. These local problems incorporate temporal
derivatives and domain evolution, ensuring consistency with underlying
fine-scale dynamics. The resulting upscaled system yields computable
macroscopic coefficients and is suitable for large-scale simulations. Several
numerical experiments are presented to validate the accuracy, efficiency, and
potential applicability of the method to complex time-dependent engineering
problems.

</details>


### [9] [Robust and efficient pre-processing techniques for particle-based methods including dynamic boundary generation](https://arxiv.org/abs/2506.21206)
*Niklas S. Neher,Erik Faulhaber,Sven Berger,Christian Weißenfels,Gregor J. Gassner,Michael Schlottke-Lakemper*

Main category: math.NA

TL;DR: The paper introduces a preprocessing technique for generating high-quality particle distributions in 2D/3D geometries, optimized for SPH and other particle-based methods, ensuring stability and accuracy.


<details>
  <summary>Details</summary>
Motivation: Challenges in obtaining stable and accurate particle distributions for complex geometries in particle-based simulations motivate this work.

Method: The method involves generating a resolution-adaptive point cloud, creating a signed distance field, using a hierarchical winding number for segmentation, and relaxing particle positions with an SPH-inspired scheme.

Result: The technique produces robust, memory-efficient particle distributions that converge to exact geometries at higher resolutions.

Conclusion: The approach is meshless, easy to integrate, and effective for particle-based simulations, even with imperfect input geometries.

Abstract: Obtaining high-quality particle distributions for stable and accurate
particle-based simulations poses significant challenges, especially for complex
geometries. We introduce a preprocessing technique for 2D and 3D geometries,
optimized for smoothed particle hydrodynamics (SPH) and other particle-based
methods. Our pipeline begins with the generation of a resolution-adaptive point
cloud near the geometry's surface employing a face-based neighborhood search.
This point cloud forms the basis for a signed distance field, enabling
efficient, localized computations near surface regions. To create an initial
particle configuration, we apply a hierarchical winding number method for fast
and accurate inside-outside segmentation. Particle positions are then relaxed
using an SPH-inspired scheme, which also serves to pack boundary particles.
This ensures full kernel support and promotes isotropic distributions while
preserving the geometry interface. By leveraging the meshless nature of
particle-based methods, our approach does not require connectivity information
and is thus straightforward to integrate into existing particle-based
frameworks. It is robust to imperfect input geometries and memory-efficient
without compromising performance. Moreover, our experiments demonstrate that
with increasingly higher resolution, the resulting particle distribution
converges to the exact geometry.

</details>


### [10] [On the coordinate system-dependence of the accuracy of symplectic numerical methods](https://arxiv.org/abs/2506.21241)
*Donát M. Takács,Tamás Fülöp*

Main category: math.NA

TL;DR: The paper explores how coordinate transformations impact the accuracy of symplectic numerical methods in Hamiltonian systems, providing theoretical insights and numerical examples.


<details>
  <summary>Details</summary>
Motivation: Despite the mathematical understanding of symplectic methods, the practical effects of coordinate choices on simulation accuracy are understudied.

Method: The study derives conditions for non-invariance of modified Hamiltonians and explores order-compensating transformations to improve accuracy.

Result: Findings highlight the computational significance of coordinate choices and offer theoretical and numerical validation.

Conclusion: The work fills a gap by systematically analyzing coordinate effects on symplectic simulations, with practical implications for accuracy.

Abstract: Symplectic numerical methods have become a widely-used choice for the
accurate simulation of Hamiltonian systems in various fields, including
celestial mechanics, molecular dynamics and robotics. Even though their
characteristics are well-understood mathematically, relatively little attention
has been paid in general to the practical aspect of how the choice of
coordinates affects the accuracy of the numerical results, even though the
consequences can be computationally significant. The present article aims to
fill this gap by giving a systematic overview of how coordinate transformations
can influence the results of simulations performed using symplectic methods. We
give a derivation for the non-invariance of the modified Hamiltonian of
symplectic methods under coordinate transformations, as well as a sufficient
condition for the non-preservation of a first integral corresponding to a
cyclic coordinate for the symplectic Euler method. We also consider the
possibility of finding order-compensating coordinate transformations that
improve the order of accuracy of a numerical method. Various numerical examples
are presented throughout.

</details>


### [11] [Runge--Kutta generalized Convolution Quadrature for sectorial problems](https://arxiv.org/abs/2506.21242)
*Jing Guo,Maria Lopez-Fernandez*

Main category: math.NA

TL;DR: The paper explores the generalized convolution quadrature (gCQ) method, based on Runge-Kutta methods, for solving sectorial problems. It improves upon Lubich's original CQ by allowing variable steps and achieving high-order convergence under general time meshes, with efficient implementation.


<details>
  <summary>Details</summary>
Motivation: To address the suboptimal stability and convergence of Runge-Kutta based gCQ compared to uniform-step CQ, especially for sectorial problems, and to enable optimal convergence even with singular data.

Method: The study focuses on a special class of sectorial problems, proving high-order convergence for gCQ under general time meshes. It also describes a fast and memory-efficient implementation of the method.

Result: The gCQ achieves the same convergence order as the original CQ under similar regularity conditions, with optimal performance on graded meshes for singular data. Numerical experiments validate the theoretical findings.

Conclusion: The gCQ method is effective for sectorial problems, offering high-order convergence and efficient implementation, outperforming the original CQ in handling singular data.

Abstract: We study the application of the generalized convolution quadrature (gCQ)
based on Runge--Kutta methods to approximate the solution of an important class
of sectorial problems. The gCQ generalizes Lubich's original convolution
quadrature (CQ) to variable steps. High-order versions of the gCQ have been
developed in the last decade, relying on certain Runge--Kutta methods. The
Runge--Kutta based gCQ has been studied so far in a rather general setting,
which includes applications to boundary integral formulations of wave problems.
The available stability and convergence results for these new methods are
suboptimal compared to those known for the uniform-step CQ, both in terms of
convergence order and regularity requirements of the data. Here we focus on a
special class of sectorial problems and prove that in these important
applications it is possible to achieve the same order of convergence as for the
original CQ, under the same regularity hypotheses on the data, and for very
general time meshes. In the particular case of data with some known algebraic
type of singularity, we also show how to choose an optimally graded time mesh
to achieve convergence with maximal order, overcoming the well-known order
reduction of the original CQ in these situations. An important advantage of the
gCQ method is that it allows for a fast and memory-efficient implementation. We
describe how the fast and oblivious Runge--Kutta based gCQ can be implemented
and illustrate our theoretical results with several numerical experiments. The
codes implementing the examples in this article are available in [13].

</details>


### [12] [On Uniform Weighted Deep Polynomial approximation](https://arxiv.org/abs/2506.21306)
*Kingsley Yeon,Steven B. Damelin*

Main category: math.NA

TL;DR: The paper introduces weighted deep polynomial approximants for functions with asymmetric behavior, outperforming traditional methods like Taylor and Chebyshev approximations.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of polynomial approximations for non-smooth or singular functions, especially those with asymmetric growth and decay.

Method: Proposes a class of weighted deep polynomial approximants, combining learnable deep polynomials with one-sided weights to capture local non-smoothness and global growth.

Result: Numerically demonstrates superior performance over Taylor, Chebyshev, and standard deep polynomial approximants.

Conclusion: The framework effectively approximates asymmetric functions and is optimized using a stable graph-based parameterization strategy.

Abstract: It is a classical result in rational approximation theory that certain
non-smooth or singular functions, such as $|x|$ and $x^{1/p}$, can be
efficiently approximated using rational functions with root-exponential
convergence in terms of degrees of freedom \cite{Sta, GN}. In contrast,
polynomial approximations admit only algebraic convergence by Jackson's theorem
\cite{Lub2}. Recent work shows that composite polynomial architectures can
recover exponential approximation rates even without smoothness \cite{KY}. In
this work, we introduce and analyze a class of weighted deep polynomial
approximants tailored for functions with asymmetric behavior-growing unbounded
on one side and decaying on the other. By multiplying a learnable deep
polynomial with a one-sided weight, we capture both local non-smoothness and
global growth. We show numerically that this framework outperforms Taylor,
Chebyshev, and standard deep polynomial approximants, even when all use the
same number of parameters. To optimize these approximants in practice, we
propose a stable graph-based parameterization strategy building on \cite{Jar}.

</details>


### [13] [A Sampling-Based Adaptive Rank Approach to the Wigner-Poisson System](https://arxiv.org/abs/2506.21314)
*Andrew Christlieb,Sining Gong,Jing-Mei Qiu,Nanyi Zheng*

Main category: math.NA

TL;DR: A mass-conserving, adaptive-rank solver for the 1D1V Wigner-Poisson system is developed to study stopping power of α particles at NIF, achieving O(N) complexity while preserving mass and momentum accuracy.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenges of modeling warm dense electrons, which require quantum effects but neglect Pauli exclusion, using the Wigner-Poisson system.

Method: A second-order Strang splitting method is used, combining a full-rank solver with a structure-preserving Fourier update, followed by an adaptive-rank solver (SLAR scheme) for efficiency.

Result: The solver achieves O(N) complexity, preserves mass, and maintains momentum accuracy, with adaptive-rank simulations matching full-rank results.

Conclusion: The adaptive-rank method is promising for high-dimensional Wigner-Poisson simulations, enabling kinetic studies of stopping power in warm dense plasmas.

Abstract: We develop a mass-conserving, adaptive-rank solver for the 1D1V
Wigner-Poisson system. Our work is motivated by applications to the study of
the stopping power of $\alpha$ particles at the National Ignition Facility
(NIF). In this regime, electrons are in a warm dense state, requiring more than
a standard kinetic model. They are hot enough to neglect Pauli exclusion, yet
quantum enough to require accounting for uncertainty. The Wigner-Poisson system
captures these effects but presents challenges due to its nonlocal nature.
Based on a second-order Strang splitting method, we first design a full-rank
solver with a structure-preserving Fourier update that ensures the intermediate
solutions remain real-valued (up to machine precision), improving upon previous
methods. Simulations demonstrate that the solutions exhibit a low rank
structure for moderate to high dimensionless Planck constants ($H \ge 0.1$).
This observed low rank structure motivates the development of an adaptive-rank
solver, built on a Semi-Lagrangian adaptive-rank (SLAR) scheme for advection
and an adaptive-rank, structure-preserving Fourier update for the Wigner
integral terms, with a rigorous proof of structure-preserving property
provided. Our solver achieves $O(N)$ complexity in both storage and computation
time, while preserving mass and maintaining momentum accuracy up to the
truncation error. The adaptive rank simulations are visually indistinguishable
from the full-rank simulations in capturing solution structures. These results
highlight the potential of adaptive rank methods for high-dimensional
Wigner-Poisson simulations, paving the way toward fully kinetic studies of
stopping power in warm dense plasmas.

</details>


### [14] [A discontinuous in time Streamline Diffusion Virtual Element Method for Darcy-transport problem](https://arxiv.org/abs/2506.21326)
*R A Caraballo Diaz,F Dassi*

Main category: math.NA

TL;DR: Numerical study of reactive transport using advection-diffusion-reaction systems, employing Streamline Diffusion and Virtual Element Method with Discontinuous Galerkin for time. Error estimates derived via Gauss-Radau interpolation, validated by high-order numerical experiments.


<details>
  <summary>Details</summary>
Motivation: To investigate transport phenomena involving chemically reactive species, addressing the need for accurate numerical methods in such systems.

Method: Combines Streamline Diffusion for discretization, Virtual Element Method for velocity/concentration, and Discontinuous Galerkin for time. Error analysis uses Gauss-Radau interpolation.

Result: Derived abstract error estimates and demonstrated arbitrary-order accuracy in space and time through numerical experiments.

Conclusion: The proposed methods and theoretical framework effectively model reactive transport, validated by high-accuracy numerical results.

Abstract: We present a first numerical study of transport phenomena involving
chemically reactive species, modeled by advection-diffusion-reaction systems
with flow fields governed by Darcy's law. Among the various discretisation
approaches, we consider the Streamline Diffusion method. Both the velocity
field and the species concentrations are computed using the Virtual Element
Method using a Discontinuous Galerkin scheme for time. An abstract error
estimate has been derived using a special technique that utilizes Gauss-Radau
interpolation in conjunction with numerical integration. These theoretical
findings are supported by numerical experiments with arbitrary-order accuracy
in both space and time.

</details>


### [15] [Efficient parameter-robust preconditioners for linear poroelasticity and elasticity in the primal formulation](https://arxiv.org/abs/2506.21361)
*Weizhang Huang,Zhuoran Wang*

Main category: math.NA

TL;DR: The paper develops nonsingular preconditioners for large-scale saddle-point systems in poroelasticity, ensuring robust performance for locking cases without requiring exact Schur complement computations.


<details>
  <summary>Details</summary>
Motivation: Poroelasticity problems are critical in engineering and geophysics, but their discretization leads to large, singular systems needing efficient preconditioners for iterative solutions.

Method: The authors propose nonsingular preconditioners with eigenvalue clusters around 1, avoiding exact Schur complement computations. They use a locking-free weak Galerkin finite element method and implicit Euler scheme.

Result: The preconditioners show robustness against parameter variations, with eigenvalue cluster bounds independent of mesh size, time step, and locking parameters. Numerical results in 2D and 3D confirm effectiveness.

Conclusion: The developed preconditioners are effective and parameter-robust, offering a practical solution for large-scale poroelasticity problems.

Abstract: Poroelasticity problems play an important role in various engineering,
geophysical, and biological applications. Their full discretization results in
a large-scale saddle-point system at each time step that is becoming singular
for locking cases and needs effective preconditioners for its fast iterative
solution. Instead of constructing spectrally equivalent ones, we develop
nonsingular preconditioners so that the eigenvalues of the preconditioned
system consist of a cluster around $1$ and an outlier in the order of
$1/\lambda$, where $\lambda$ is a Lam\'{e} constant that is large for locking
cases. It is known that the convergence factor of GMRES is bounded by the
radius of the cluster for this type of systems. Both two- and three-field block
triangular Schur complement preconditioners are studied. Upper bounds of the
radius of the eigenvalue cluster for those systems are obtained and shown to be
related to the inf-sup condition but independent of mesh size, time step, and
locking parameters, which reflects the robustness of the preconditioners with
respect to parameter variations. Moreover, the developed preconditioners do not
need to compute the Schur complement and neither require exact inversion of
diagonal blocks except the leading one. A locking-free weak Galerkin finite
element method and the implicit Euler scheme are used for the discretization of
the governing equation. Both two- and three-dimensional numerical results are
presented to confirm the effectiveness and parameter-robustness of the
developed preconditioners.

</details>


### [16] [Optimal solutions employing an algebraic Variational Multiscale approach Part II: Application to Navier-Stokes](https://arxiv.org/abs/2506.21395)
*Suyash Shrestha,Marc Gerritsma,Gonzalo Rubio,Steven Hulshoff,Esteban Ferrer*

Main category: math.NA

TL;DR: A nonlinear extension of the VMS method for high-order discretization of the 2D incompressible Navier-Stokes equations, preserving accuracy and conservation properties.


<details>
  <summary>Details</summary>
Motivation: To generalize the VMS framework for steady linear problems to handle nonlinear multiscale problems, specifically the 2D Navier-Stokes equations.

Method: Uses an optimal projector and approximates fine-scale contributions via the Fine-Scale Greens' function of the symmetric operator.

Result: Numerical solutions closely approximate the optimal projection of the continuous solution, with confirmed robustness and accuracy.

Conclusion: The method is effective for nonlinear multiscale problems and maintains high-order accuracy and conservation properties.

Abstract: This work presents a nonlinear extension of the high-order discretisation
framework based on the Variational Multiscale (VMS) method previously
introduced for steady linear problems. Building on the concept of an optimal
projector defined via the symmetric part of the governing operator, we
generalise the formulation to treat the 2D incompressible Navier-Stokes
equations. The arroach maintains a clear separation between the resolved and
unresolved scales, with the fine-scale contributions approximated through the
approximate Fine-Scale Greens' function of the associated symmetric operator.
This enables a consistent variational treatment of the nonlinearity while
preserving high-order accuracy. We show that the method yields numerical
solutions that closely approximate the optimal projection of the
continuous/highly resolved solution and inherits desirable conservation
properties. Numerical results confirm the framework's robustness, accuracy, and
its potential for application to a broad class of nonlinear multiscale
problems.

</details>


### [17] [An adaptive dynamical low-rank optimizer for solving kinetic parameter identification inverse problems](https://arxiv.org/abs/2506.21405)
*Lena Baumann,Lukas Einkemmer,Christian Klingenberg,Jonas Kusch*

Main category: math.NA

TL;DR: A dynamical low-rank scheme is proposed for efficiently solving parameter identification in kinetic equations, reducing computational and memory costs while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: High computational and memory costs in solving parameter identification inverse problems for kinetic equations motivate the need for an efficient solution.

Method: The method involves PDE constrained optimization, adjoint equations via Lagrangian reformulation, B-spline approximation for the scattering coefficient, gradient descent, and dynamical low-rank approximation (DLRA) with rank-adaptive techniques.

Result: The DLRA scheme significantly reduces memory and computational costs while maintaining accuracy, validated by numerical results.

Conclusion: The proposed DLRA scheme is efficient and accurate for solving parameter identification problems in kinetic equations.

Abstract: The numerical solution of parameter identification inverse problems for
kinetic equations can exhibit high computational and memory costs. In this
paper, we propose a dynamical low-rank scheme for the reconstruction of the
scattering parameter in the radiative transfer equation from a number of
macroscopic time-independent measurements. We first work through the PDE
constrained optimization procedure in a continuous setting and derive the
adjoint equations using a Lagrangian reformulation. For the scattering
coefficient, a periodic B-spline approximation is introduced and a gradient
descent step for updating its coefficients is formulated. After the
discretization, a dynamical low-rank approximation (DLRA) is applied. We make
use of the rank-adaptive basis update & Galerkin integrator and a line search
approach for the adaptive refinement of the gradient descent step size and the
DLRA tolerance. We show that the proposed scheme significantly reduces both
memory and computational cost. Numerical results computed with different
initial conditions validate the accuracy and efficiency of the proposed DLRA
scheme compared to solutions computed with a full solver.

</details>


### [18] [An Iterative Methodology for Unitary Quantum Channel Search](https://arxiv.org/abs/2506.21455)
*Matthew M. Lin,Hao-Wei Huang,Bing-Ze Lu*

Main category: math.NA

TL;DR: Proposes an iterative algorithm using polar decomposition to approximate a quantum channel from input-output state pairs, reducing search space and identifying local minima.


<details>
  <summary>Details</summary>
Motivation: To efficiently approximate quantum channels with limited data by leveraging structured input-output pairs.

Method: Uses polar decomposition and iterative optimization to identify unitary matrices describing the channel, proving equivalence under phase differences.

Result: Optimal solution reduces search space; algorithm identifies critical points as local minima.

Conclusion: The method effectively approximates quantum channels with limited data and rigorous guarantees.

Abstract: In this paper, we propose an iterative algorithm using polar decomposition to
approximate a channel characterized by a single unitary matrix based on
input-output quantum state pairs. In limited data, we state and prove that the
optimal solution obtained from our method using one pair with a specific
structure will generate an equivalent class, significantly reducing the
dimension of the searching space. Furthermore, we prove that the unitary
matrices describing the same channel differ by a complex number with modulus 1.
We rigorously prove our proposed algorithm can ultimately identify a critical
point, which is also a local minimum of the established objective function.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [19] [The $\mathcal{M}$-Operator and Uniqueness of Nonlinear Kinetic Equations](https://arxiv.org/abs/2506.20775)
*Ricardo Alonso,Maria Pia Gualdani,Weiran Sun*

Main category: math.AP

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We introduce an $\mathcal{M}$-operator approach to establish the uniqueness
of continuous or bounded solutions for a broad class of Landau-type nonlinear
kinetic equations. The specific $\mathcal{M}$-operator, originally developed in
[3], acts as a negative fractional derivative in both spatial and velocity
variables and interacts in a controllable manner with the kinetic transport
operator. The novelty of this method is that it bypasses the need for bounds on
the derivatives of the solution - an assumption typically required in
uniqueness arguments for non-cutoff equations. As a result, the method enables
working with solutions with low regularity.

</details>


### [20] [Normalized solutions for the NLS equation with mixed fractional Laplacians and combined nonlinearities](https://arxiv.org/abs/2506.20943)
*Shubin Yu,Chen Yang,Chun-Lei Tang*

Main category: math.AP

TL;DR: The paper studies normalized solutions to a nonlinear Schrödinger equation with mixed fractional Laplacians and combined nonlinearities, proving existence of multiple solutions under specific conditions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to extend previous results on nonlinear Schrödinger equations by considering mixed fractional Laplacians and combined nonlinearities, addressing gaps in the literature.

Method: The authors analyze the problem using variational methods, focusing on fractional Sobolev subcritical or critical exponents. They prove existence of solutions via ground state and mountain pass techniques.

Result: For certain parameter ranges, the problem has at least two solutions: a ground state with negative energy and a mountain pass solution with positive energy. Ground states also exist for critical exponents.

Conclusion: The results generalize prior work by Chergui et al. and Luo et al., confirming the existence of solutions under broader conditions.

Abstract: We look for normalized solutions to the nonlinear Schr\"{o}dinger equation
with mixed fractional Laplacians and combined nonlinearities $$
\left\{\begin{array}{ll} (-\Delta)^{s_{1}} u+(-\Delta)^{s_{2}} u=\lambda u+\mu
|u|^{q-2}u+|u|^{p-2}u \ \text{in}\;{\mathbb{R}^{N}}, \\[0.1cm]
\int_{\mathbb{R}^{N}}|u|^2\mathrm dx=a^2, \end{array} \right. $$ where $N\geq
2,\;0<s_2<s_1<1, \mu>0$ and $\lambda\in\mathbb R$ appears as an unknown
Lagrange multiplier. We mainly focus on some special cases, including
fractional Sobolev subcritical or critical exponent. More precisely, for
$2<q<2+\frac{4s_2}{N}<2+\frac{4s_1}{N}<p<2_{s_1}^{\ast}:=\frac{2N}{N-2s_1}$, we
prove that the above problem has at least two solutions: a ground state with
negative energy and a solution of mountain pass type with positive energy. For
$2<q<2+\frac{4s_2}{N}$ and $p=2_{s_1}^{\ast}$, we also obtain the existence of
ground states. Our results extend some previous ones of Chergui et al. (Calc.
Var. Partial Differ. Equ., 2023) and Luo et al. (Adv. Nonlinear Stud., 2022).

</details>


### [21] [Asymptotic Analysis of Boundary Layer Solutions to Poisson-Boltzmann Type Equations in General Bounded Smooth Domains](https://arxiv.org/abs/2506.20953)
*Jhih-Hong Lyu,Tai-Chia Lin*

Main category: math.AP

TL;DR: The paper analyzes boundary layer solutions for Poisson-Boltzmann type equations in bounded domains, using asymptotic expansions and exponential decay estimates across three regions defined by distance from the boundary.


<details>
  <summary>Details</summary>
Motivation: To address the analytical challenges of nonlocal nonlinearity in charge-conserving PB equations and understand how domain geometry influences electrostatic interactions.

Method: Uses principal coordinate system, exponential-type estimates, and moving plane arguments to rigorously prove asymptotic expansions in three characteristic regions.

Result: Derives second-order asymptotic formulas in Region I and exponential decay estimates in Regions II and III, along with expansions for key physical quantities.

Conclusion: The study provides insights into how boundary geometry regulates electrostatic interactions, with rigorous proofs for boundary layer solutions in PB-type equations.

Abstract: We study the boundary layer solution to singular perturbation problems
involving Poisson-Boltzmann (PB) type equations with a small parameter
$\epsilon$ in general bounded smooth domains (including multiply connected
domains) under the Robin boundary condition. The PB type equations include the
classical PB, modified PB and charge-conserving PB (CCPB) equations, which are
mathematical models for the electric potential and ion distributions. The CCPB
equations present particular analytical challenges due to their nonlocal
nonlinearity introduced through integral terms enforcing charge conservation.
Using the principal coordinate system, exponential-type estimates and the
moving plane agruments, we rigorously prove asymptotic expansions of boundary
layer solutions throughout the whole domain. The solution domain is partitioned
into three characteristic regions based on the distance from the boundary:
Region I, where the distance from the boundary is at most $T\sqrt\epsilon$,
Region II, where the distance ranges between $T\sqrt\epsilon$ and
$\epsilon^\beta$, and Region III, where the distance is at least
$\epsilon^\beta$, for given parameters $T>0$ and $0<\beta<1/2$. In Region I, we
derive second-order asymptotic formulas explicitly incorporating the effects of
boundary mean curvature, while exponential decay estimates are established for
Regions II and III. Furthermore, we obtain asymptotic expansions for key
physical quantities, including the electric potential, electric field, total
ionic charge density and total ionic charge, revealing how domain geometry
regulates electrostatic interactions.

</details>


### [22] [Phase Transition in Non-isentropic Compressible Immiscible Two-Phase Flow with van der Waals Equation of State](https://arxiv.org/abs/2506.20955)
*Yazhou Chen,Yi Peng,Xiaoding Shi,Xiaoping Wang*

Main category: math.AP

TL;DR: The paper proves global well-posedness for the compressible non-isentropic Navier-Stokes/Allen-Cahn system with van der Waals equation of state and degenerate thermal conductivity, showing bounded physical quantities despite phase transitions.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenges of modeling immiscible two-phase flow with diffusive interfaces, particularly focusing on the non-monotonic pressure-density relationship in the van der Waals equation, which drives gas-liquid phase transitions.

Method: The authors develop a refined $L^2$-energy framework to analyze the one-dimensional Cauchy problem, proving existence and uniqueness of global strong solutions for non-vacuum and finite-temperature initial data without smallness restrictions.

Result: The results show that despite significant density fluctuations and phase transitions due to non-monotonic pressure, all physical quantities remain bounded over finite time intervals.

Conclusion: The study successfully establishes the global well-posedness of the system, demonstrating robustness in handling phase transitions without requiring small initial conditions.

Abstract: This study establishes the global well-posedness of the compressible
non-isentropic Navier-Stokes/Allen-Cahn system governed by the van der Waals
equation of state $p(\rho,\theta)=- a\rho^2+\frac{R\theta\rho}{1-b\rho}$ and
degenerate thermal conductivity $\kappa(\theta)=\tilde{\kappa}\theta^\beta$,
where $p$, $\rho$ and $\theta$ are the pressure, the density and the
temperature of the flow respectively, and $a,b,R,\tilde\kappa$ are positive
constants related to the physical properties of the flow.
Navier-Stokes/Allen-Cahn system models immiscible two-phase flow with diffusive
interfaces, where the non-monotonic pressure-density relationship in the van
der Waals equation drives gas-liquid phase transitions. By developing a refined
$L^2$-energy framework, we prove the existence and uniqueness of global strong
solutions to the one-dimensional Cauchy problem for non-vacuum and
finite-temperature initial data, without imposing smallness restrictions on the
initial conditions. The findings demonstrate that despite non-monotonic
pressure inducing substantial density fluctuations and triggering phase
transitions, all physical quantities remain bounded over finite time intervals.

</details>


### [23] [Shape sensitivity analysis of the heat equation and the Dirichlet-to-Neumann map](https://arxiv.org/abs/2506.21196)
*Matteo Dalla Riva,Paolo Luzzini,Paolo Musolino*

Main category: math.AP

TL;DR: The paper analyzes a Dirichlet problem for the heat equation in a domain with an interior hole, proving smoothness of solutions and their normal derivatives with respect to shape parameters.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by an inverse problem application, focusing on understanding how solutions and their derivatives depend on the shape of the domain.

Method: The authors analyze maps from shape parameters to solutions and their normal derivatives, proving smoothness and computing differentials.

Result: Both the solution and its normal derivative are shown to be smooth with respect to the shape parameter. The differential of the normal derivative on the exterior boundary is computed.

Conclusion: The results provide insights into the smooth dependence of solutions on domain shape, with implications for inverse problems.

Abstract: We study a Dirichlet problem for the heat equation in a domain containing an
interior hole. The domain has a fixed outer boundary and a variable inner
boundary determined by a diffeomorphism $\phi$. We analyze the maps that assign
to the infinite-dimensional shape parameter $\phi$ the corresponding solution
and its normal derivative, and we prove that both are smooth. Motivated by an
application to an inverse problem, we then compute the differential with
respect to $\phi$ of the normal derivative of the solution on the exterior
boundary.

</details>


### [24] [Solving Mean-Field Games with Monotonicity Methods in Banach Spaces?](https://arxiv.org/abs/2506.21212)
*Rita Ferreira,Diogo Gomes,Melih Ucer*

Main category: math.AP

TL;DR: A unified framework for proving the existence of solutions to stationary first-order mean-field games (MFGs) using monotone operators in Banach spaces, yielding strong solutions with low-order regularization.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of prior Hilbert-space approaches that relied on high-order regularization and produced weak solutions, aiming for a more direct theoretical path and suitability for modern numerical algorithms.

Method: Cast the MFG system as a variational inequality, using low-order $p$-Laplacian regularization to restore coercivity and Minty's method for limits. For minimal growth Hamiltonians, regularize via infimal convolution.

Result: Existence of strong solutions for models with power-growth and singular congestion, and weak solutions for minimal growth Hamiltonians, unifying and extending prior results.

Conclusion: The Banach-space framework provides a robust, unified approach for MFGs, avoiding high-order smoothing and enhancing theoretical and numerical applicability.

Abstract: This paper develops a unified framework for proving the existence of
solutions to stationary first-order mean-field games (MFGs) based on the theory
of monotone operators in Banach spaces. We cast the coupled MFG system as a
variational inequality, overcoming the limitations of prior Hilbert-space
approaches that relied on high-order regularization and typically yielded only
weak solutions in the monotone operator sense. In contrast, with our low-order
regularization, we obtain strong solutions.
  Our approach addresses the non-coercivity of the underlying MFG operator
through two key regularization strategies. First, by adding a low-order
$p$-Laplacian term, we restore coercivity, derive uniform a priori estimates,
and pass to the limit via Minty's method. This establishes, for the first time
via monotonicity methods, the existence of strong solutions for models with
both standard power-growth and singular congestion, with the latter requiring a
careful restriction of the operator's domain. Second, for Hamiltonians with
only minimal growth hypotheses, we regularize the Hamiltonian itself via
infimal convolution to prove the existence of weak solutions.
  Our Banach-space framework unifies and extends earlier existence results. By
avoiding high-order smoothing, it not only provides a more direct theoretical
path but is also ideally suited for modern numerical algorithms.

</details>


### [25] [Determination of the potential by a fixed angle scattering data](https://arxiv.org/abs/2506.21251)
*Suliang Si*

Main category: math.AP

TL;DR: A compactly supported potential is uniquely determined by far field patterns at a fixed angle using a new Carleman estimate.


<details>
  <summary>Details</summary>
Motivation: To address the uniqueness of determining a potential from far field data, leveraging Carleman estimates for inverse problems.

Method: Uses a new Carleman estimate and techniques from Bukhgeim and Klibanov for inverse problems.

Result: The potential is uniquely determined by far field patterns at a fixed angle.

Conclusion: The approach provides a unique determination of the potential, advancing inverse problem methodologies.

Abstract: In this paper, we show that a compactly supported potential is uniquely
determined by the far field pattern at a fixed angle. Our method is based on a
new Carleman estimate and the ideas introduced by Bukhgeim and Klibanov on the
use of Carleman estimates for inverse problems.

</details>


### [26] [Asymptotic stability of solutions to semilinear evolution equations in Banach spaces](https://arxiv.org/abs/2506.21437)
*Francesco Cellarosi,Anirban Dutta,Giusy Mazzone*

Main category: math.AP

TL;DR: The paper proves a linearization principle for nonlinear stability in semilinear parabolic evolution equations, showing global mild solutions near equilibria converge exponentially to equilibrium. Applied to fluid-filled heavy solids, weak solutions converge to steady states exponentially.


<details>
  <summary>Details</summary>
Motivation: To establish a general framework for nonlinear stability in semilinear parabolic equations and apply it to fluid-solid systems.

Method: Assumes equilibria form a finite-dimensional manifold, uses linearized operator properties, and proves exponential convergence for global mild solutions near equilibria.

Result: Global mild solutions near equilibria converge exponentially to equilibrium. Applied to fluid-filled solids, weak solutions converge to steady states exponentially.

Conclusion: The abstract framework successfully explains stability and convergence in semilinear parabolic systems and fluid-solid interactions.

Abstract: We prove a new linearization principle for the nonlinear stability of
solutions to semilinear evolution equations of parabolic type. We assume that
the set of equilibria forms a finite dimensional manifold of normally stable
and normally hyperbolic equilibria. In addition, we assume that the linearized
operator is the generator of an analytic semigroup (not necessarily stable). We
show that if a mild solution to our evolution equation exists globally in time
and remains ``close'' to the manifold of equilibria at all times, then the
solution must eventually converge to an equilibrium point at an exponential
rate.
  We apply our abstract results to the equations governing the motion of a
fluid-filled heavy solid. Under general assumptions on the physical
configuration and initial conditions, we show that weak solutions to the
governing equations eventually converge to a steady state with an exponential
rate. In particular, the fluid velocity relative to the solid converges to zero
as $t\to\infty$ in $H^{2\alpha}_p(\Omega)$ for each $p\in [1,\infty)$ and
$\alpha\in [0,1)$ as well as in $H^{2}_2(\Omega)$.

</details>


### [27] [A priori estimates of stable solutions of the general Hardy-Henon equation in the ball](https://arxiv.org/abs/2506.21515)
*J. Silverio Martinez-Baena,Salvador Villegas*

Main category: math.AP

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper is devoted to the study of semi-stable radial solutions $u\in
H^1(B_1)$ of $-\Delta u=\vert x\vert^\alpha f(u) \mbox{ in }
B_1\setminus\lbrace0\rbrace$, where $f\in C^1(\mathbb{R})$ is a general
nonlinearity, $\alpha>-2$ and $B_1$ is the unit ball of $\mathbb{R}^N$, $N>1$.
We establish the boudness of such solutions for dimensions $2\leq N<10+4\alpha$
and sharp pointwise estimates in the case $N\geq10+4\alpha$. In addition, we
provide, for this range of dimensions, a large family of semi-stable radially
decreasing unbounded $H^1(B_1)$ solutions.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [28] [On the rank-reduced relativistic coupled cluster method](https://arxiv.org/abs/2506.21133)
*Alexander V. Oleynichenko,Artem S. Rumiantsev,Andrei Zaitsevskii,Ephraim Eliav*

Main category: physics.comp-ph

TL;DR: The study explores Tucker decomposition efficiency in relativistic CCSD for systems like (AuCl)$_n$, Au$_n$, and YbCl$_2$, achieving 1 kJ/mol accuracy with high compression rates.


<details>
  <summary>Details</summary>
Motivation: To improve computational efficiency in relativistic CCSD for large systems with heavy atoms by reducing tensor ranks.

Method: Benchmark calculations on (AuCl)$_n$, Au$_n$, and YbCl$_2$ using Tucker decomposition and compression of amplitude tensors.

Result: Achieved 1 kJ/mol accuracy with high compression; only ~3% of doubles amplitudes were significant for YbCl$_7$.

Conclusion: Rank reduction in relativistic CCSD is feasible, promising for large heavy-atom systems, with preference for Goldstone diagrams.

Abstract: An efficiency of the Tucker decomposition of amplitude tensors within the
single-reference relativistic coupled cluster method with single and double
excitations (RCCSD) was studied in a series of benchmark calculations for
(AuCl)$_n$ chains, Au$_n$ clusters, and the cluster model of solid YbCl$_2$.
The 1 kJ/mol level of accuracy for correlation energy estimates of
moderate-size systems and typical reaction energies can be achieved with
relatively high compression rates of amplitude tensors via rejecting singular
values smaller than $\sim 10^{-4}$. For the most extensive system studied
(YbCl$_7$ cluster used for modeling of ytterbium center in ytterbium dichloride
crystal), only $\sim 3$% of compressed doubles amplitudes were shown to be
significant. Thus, the rank reduction for the relativistic CCSD theory
improving its computational scaling is feasible. The advantage (if not
necessity) of using the Goldstone diagrammatic technique rather than the
"antisymmetrized" Brandow one is underlined. The proposed approach is promising
for high-precision modeling of relatively large systems with heavy atoms.

</details>


### [29] [Anharmonic phonons via quantum thermal bath simulations](https://arxiv.org/abs/2506.21139)
*Taylor Baird,Rodolphe Vuilleumier,Sara Bonella*

Main category: physics.comp-ph

TL;DR: A novel method combines quantum correlators and the quantum thermal bath (QTB) to efficiently account for anharmonic and nuclear quantum effects in phonon calculations, tested on 1D systems and solid neon.


<details>
  <summary>Details</summary>
Motivation: Accurate simulation of phonon characteristics with anharmonic and nuclear quantum effects is computationally expensive. This work aims to reduce this cost.

Method: Uses quantum correlators for anharmonic phonon frequencies and the QTB method for nuclear quantum effects, applied to 1D systems and solid neon.

Result: The scheme is efficient and accurate, demonstrating its potential for practical applications.

Conclusion: The approach successfully mitigates computational costs while maintaining accuracy, with further exploration needed for broader applicability.

Abstract: Lattice vibrations within crystalline solids, or phonons, provide information
on a variety of important material characteristics, from thermal qualities to
optical properties and phase transition behaviour. When the material contains
light ions, or is subjected to sufficiently low temperatures and/or high
pressures, anharmonic and nuclear quantum effects (NQEs) may significantly
alter its phonon characteristics. Unfortunately, accurate inclusion of these
two effects within numerical simulations typically incurs a substantial
computational cost. In this work, we present a novel approach which promises to
mitigate this problem. The scheme leverages the recently introduced quantum
correlators approach for the extraction of anharmonic phonon frequencies from
molecular dynamics data. To account for NQEs without excessive increase of the
computational cost, we include nuclear quantum effects via the quantum thermal
bath (QTB) method. This is the first full exploration of the use of QTB for the
calculation of phonon dispersion relations. We demonstrate the noteworthy
efficiency and accuracy of the scheme, and analyze its upsides and drawbacks by
first considering 1-dimensional systems, and then the physically interesting
case of solid neon.

</details>


### [30] [Benchmarking and Parallelization of Electrostatic Particle-In-Cell for low-temperature Plasma Simulation by particle-thread Binding](https://arxiv.org/abs/2506.21524)
*Libn Varghese,Bhaskar Chaudhury,Miral Shah,Mainak Bandyopadhyay*

Main category: physics.comp-ph

TL;DR: A novel particle-thread binding strategy improves Charge Deposition (CD) scalability in PIC simulations, reducing private grids and maintaining performance.


<details>
  <summary>Details</summary>
Motivation: High computational costs and scalability issues in parallel PIC simulations, especially with CD as a bottleneck.

Method: Particle-thread binding strategy requiring only four private grids per node, ensuring grid accessibility and avoiding simultaneous particle access.

Result: Demonstrated scalability on shared and distributed memory systems (1000 cores) with minimal hardware dependency.

Conclusion: The method enhances CD performance and scalability while requiring minimal changes to existing PIC codes.

Abstract: The Particle-In-Cell (PIC) method for plasma simulation tracks particle phase
space information using particle and grid data structures. High computational
costs in 2D and 3D device-scale PIC simulations necessitate parallelization,
with the Charge Deposition (CD) subroutine often becoming a bottleneck due to
frequent particle-grid interactions. Conventional methods mitigate dependencies
by generating private grids for each core, but this approach faces scalability
issues. We propose a novel approach based on a particle-thread binding strategy
that requires only four private grids per node in distributed memory systems or
four private grids in shared memory systems, enhancing CD scalability and
performance while maintaining conventional data structures and requiring
minimal changes to existing PIC codes. This method ensures complete
accessibility of grid data structure for concurrent threads and avoids
simultaneous access to particles within the same cell using additional
functions and flags. Performance evaluations using a PIC benchmark for
low-temperature partially magnetized E x B discharge simulation on a shared
memory as well as a distributed memory system (1000 cores) demonstrate the
method's scalability, and additionally, we show the method has little hardware
dependency.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [31] [Control of pedestal-top electron density using RMP and gas puff at KSTAR](https://arxiv.org/abs/2506.20700)
*Minseok Kim,S. K. Kim,A. Rothstein,P. Steiner,K. Erickson,Y. H. Lee,H. Han,Sang-hee Hahn,J. W. Juhn,B. Kim,R. Shousha,C. S. Byun,J. Butt,ChangMin Shin,J. Hwang,Minsoo Cha,Hiro Farre,S. M. Yang,Q. Hu,D. Eldon,N. C. Logan,A. Jalalvand,E. Kolemen*

Main category: physics.plasm-ph

TL;DR: The paper presents a real-time controller for pedestal-top electron density in KSTAR experiments, using resonant magnetic perturbation and gas puff, achieving low error rates and dynamic target following.


<details>
  <summary>Details</summary>
Motivation: To enable precise and dynamic control of electron density in fusion experiments, improving scenario exploration and discharge stability.

Method: Uses a parametrized psi_N grid, a two-colored interferometer for density measurement, a multi-layer perceptron for fast reconstruction, and a proportional-integration controller with system-identified gains.

Result: Achieves median and average absolute percentage errors of 1.5% and 2.5%, respectively, and demonstrates dynamic target following and density reduction via pump-out under RMP.

Conclusion: The controller is effective for real-time density control, enabling dynamic scenario exploration and stable discharge maintenance in fusion experiments.

Abstract: We report the experimental results of controlling the pedestal-top electron
density by applying resonant magnetic perturbation with the in-vessel control
coils and the main gas puff in the 2024-2025 KSTAR experimental campaign. The
density is reconstructed using a parametrized psi_N grid and the five channels
of the line-averaged density measured by a two-colored interferometer. The
reconstruction procedure is accelerated by deploying a multi-layer perceptron
to run in about 120 microseconds and is fast enough for real-time control. A
proportional-integration controller is adopted, with the controller gains being
estimated from the system identification processes. The experimental results
show that the developed controller can follow a dynamic target while
exclusively using both actuators. The absolute percentage errors between the
electron density at psi_N=0.89 and the target are approximately 1.5% median and
a 2.5% average value. The developed controller can even lower the density by
using the pump-out mechanism under RMP, and it can follow a more dynamic target
than a single actuator controller. The developed controller will enable
experimental scenario exploration within a shot by dynamically setting the
density target or maintaining a constant electron density within a discharge.

</details>


### [32] [Global properties and stability of transonic plasma acceleration in the magnetic nozzle](https://arxiv.org/abs/2506.20880)
*N. Sheth,A. Smolyakov,J. Deguire,S. Pande,P. N. Yushmanov*

Main category: physics.plasm-ph

TL;DR: The paper demonstrates that transonic plasma acceleration in a magnetic nozzle follows a unique global solution defined by the magnetic field, validated by MHD simulations.


<details>
  <summary>Details</summary>
Motivation: To validate and extend the analytical solution for transonic plasma acceleration in a magnetic nozzle using MHD simulations.

Method: Axisymmetric 2D MHD simulations are used to compare with the analytically derived solution in the paraxial approximation.

Result: The analytical solution aligns well with simulations near the axis and can generalize to arbitrary magnetic surfaces. Shock-like transitions adjust the flow to the unique transonic profile.

Conclusion: The study confirms the robustness of the analytical solution, even with modified magnetic fields, and highlights the global adjustment mechanism in plasma acceleration.

Abstract: It is shown that transonic plasma acceleration in the converging-diverging
magnetic field (magnetic nozzle) follows the unique global solution which is
fully defined by the magnetic field. Such solution, which was analytically
obtained earlier in the paraxial approximation, is compared here with results
of the axisymmetric two-dimensional (rz) magnetohydrodynamics(MHD) simulations.
It is shown that analytical solution describes well the region near the axis
but also can be applied to arbitrary magnetic surfaces. The simulations with
different length of the nozzle and different boundary values for plasma
velocity show that the plasma flow switches to the unique transonic
acceleration profile via the shock-like transition in the velocity and pressure
profiles. The simulations with arbitrary (not vacuum) initial magnetic field
demonstrate the global adjustment of the magnetic field such that the transonic
acceleration velocity profile follows the analytic predictions with the
modified magnetic field.

</details>


### [33] [Nonlinear edge localized mode with impurity seeding in CFETR hybrid scenario](https://arxiv.org/abs/2506.20892)
*Shiyong Zeng,Ping Zhu*

Main category: physics.plasm-ph

TL;DR: The paper explores ELM mitigation in fusion plasma using impurity seeding, revealing its impact on pedestal pressure and ELM dynamics through MHD simulations.


<details>
  <summary>Details</summary>
Motivation: To understand the physics of ELM mitigation via impurity seeding, which is experimentally effective but poorly understood.

Method: Nonlinear magnetohydrodynamic (MHD) simulations to study ELM crash triggers and the effects of impurity seeding.

Result: Impurity seeding modifies pedestal pressure, triggering high-n ballooning instabilities. Key parameters (impurity density and seeding location) influence ELM crash timing and energy loss.

Conclusion: Impurity seeding's role in ELM dynamics is clarified, providing insights for better control in fusion plasma operations.

Abstract: A critical challenge for operating fusion burning plasma in high confinement
mode lies in mitigating damage caused by edge localized modes (ELMs). While
impurity seeding has been experimentally validated as a reliable and effective
ELM mitigation technique, its underlying physics remains insufficiently
understood and requires further clarification. Through nonlinear
magnetohydrodynamic (MHD) simulations, this work reproduces key features of
natural ELM crash and reveals its trigger mechanism. Impurity seeding
significantly affects nonlinear ELM dynamics by inducing local and global
modifications to the pedestal pressure profile, driving high-n ballooning mode
instabilities that govern ELM crash. Two critical control parameters --
impurity density level and poloidal seeding location -- are systematically
investigated, which play key roles in the ELM crash onset timing and the
resulting energy loss magnitude.

</details>


### [34] [Observation of Enhanced Core Impurity Transport in a Turbulence-Reduced Stellarator Plasma](https://arxiv.org/abs/2506.21141)
*Daniel Medina-Roque,Isabel García-Cortés,Naoki Tamura,Kieran J. McCarthy,Federico Nespoli,Kenji Tanaka,Mamoru Shoji,Suguru Masuzaki,Hisamichi Funaba,Chihiro Suzuki,Albert Mollen,Robert Lunsford,Katsumi Ida,Mikiro Yoshinuma,Motoshi Goto,Yasuko Kawamoto,Tomoko Kawate,Tokihiko Tokuzawa,Ichihiro Yamada*

Main category: physics.plasm-ph

TL;DR: Continuous Li-granule injection in high-density stellarator plasmas improves energy confinement by reducing turbulence but increases mid- and high-Z impurity transport. Simulations show neoclassical transport dominates for main plasma components, while classical transport is key for high-Z impurities.


<details>
  <summary>Details</summary>
Motivation: To investigate the impact of continuous Li-granule injection on plasma confinement and impurity transport in high-density stellarator plasmas.

Method: Experimental observation combined with simulations using the drift-kinetic transport code SFINCS.

Result: Li-granule injection improves energy confinement but increases high-Z impurity transport, with classical transport playing a dominant role for impurities.

Conclusion: Classical transport is crucial for enhancing high-Z impurity transport in high-density stellarator plasmas, achievable via Li-granule injection for real-time wall conditioning and performance improvement.

Abstract: An enhancement of core impurity transport is observed for the first time in a
high-density stellarator plasma with continuous lithium (Li) granule injection.
When Li-granules are dropped continuously into the plasma, energy confinement
is improved due to reduced turbulence. In parallel, the transport of mid- and
high-Z impurities is increased. Simulations with the drift-kinetic transport
code SFINCS for such plasmas show that the role of neoclassical transport
prevails for the main plasma components (electrons, ions, and Zavg = 3.5). In
contrast, the classical contribution is dominant in transporting high-Z
impurities. This study demonstrates experimentally, for the first time also,
that classical transport plays an essential role in enhancing the transport of
such impurities in high-density stellarator plasmas, a situation that is
achieved by continuous injection of Li-granules, which is effective for
real-time wall conditioning and plasma performance improvement

</details>


### [35] [Relativistic Oscillating Window Driven by an Intense Laguerre Gaussian Laser Pulse](https://arxiv.org/abs/2506.21407)
*Yao Meng,Runze Li,Longqing Yi*

Main category: physics.plasm-ph

TL;DR: Study of high-order harmonic generation via diffraction of Laguerre-Gaussian laser beams, revealing complex spin-orbital angular momentum interplay and new selection rules.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of light in nonlinear optics and enable controllable orbital angular momentum (OAM) in high-intensity UV/X-ray pulses for scientific applications.

Method: Analyzed 2D peripheral electron dynamics on the rim of a small aperture, derived a theory, and validated it with simulations.

Result: Linearly polarized drivers produce multiple Laguerre-Gaussian modes in harmonic beams, with mode count matching harmonic order. Theory predicts topological charges and intensities.

Conclusion: Provides fundamental insights into nonlinear optics and advances the generation of OAM-controlled UV/X-ray pulses for diverse scientific uses.

Abstract: High-order harmonic generation by the diffraction of an intense
Laguerre-Gaussian (LG) laser beam through a small aperture is studied. It is
found that the 2D peripheral electron dynamics on the rim can facilitate
complex interplay between the spin and orbital angular momentum interaction,
which leads to distinct selection rules for LG pulses with different
polarization states. In particular, when the driver is linearly polarized, the
harmonic beams no longer follow a simple orbital angular momentum conservation
rule. Instead, multiple LG modes with different topological charges are
produced in each harmonic beam, and the number of modes equals to the harmonic
order. A theory is derived and validated by simulations, which can predict the
harmonic topological charges as well as their relative intensities for LG
drivers with different polarization states. Our work provides fundamental
insight into the behavior of light in nonlinear optics, and paves the way
towards high-intensity UV or X-ray pulses carrying controllable OAM, that can
serve as versatile tools at frontiers of various scientific fields.

</details>


### [36] [Excitation of Giant Surface Waves During Laser Wake Field Acceleration](https://arxiv.org/abs/2506.21503)
*Travis Garrett,Christopher Pieronek,E. Rockafellow,Oliver Sale,Sahir Virani,J. E. Shrock,B. Miao,A. Sloss,Jennifer Elle,H. M. Milchberg*

Main category: physics.plasm-ph

TL;DR: High-intensity surface waves are detected during plasma waveguided laser wakefield acceleration, producing multi-GeV electron bunches and strong RF radiation.


<details>
  <summary>Details</summary>
Motivation: To understand and enhance wakefield acceleration by studying the excitation of surface waves and RF radiation in plasma waveguides.

Method: Combined laboratory measurements, particle-in-cell simulations, and analytic approximations to analyze the phenomenon.

Result: A 20 J laser pulse excites a 1 Joule, 400 GW broadband THz surface wave with a peak electric field of 35 GV/m.

Conclusion: The study reveals a strong correlation between laser pulses, surface waves, and RF radiation, providing insights for optimizing wakefield acceleration.

Abstract: We have detected the presence of very high intensity surface waves that are
excited during plasma waveguided laser wakefield acceleration. Wakefield
acceleration can be enchanced by the introduction of an ``all optical" plasma
waveguide that confines and guides a laser pulse at the optimal intensity over
long distances, producing quasimonoenergetic multi-GeV electron bunches.
However strong pulses of radio frequency radiation (RF) are also produced, and
particle in cell simulations show why: a continuous stream of multi-MeV
electrons are also ejected radially from the plasma due to nonlinear wave
breaking, and these excite and copropagate coherently with a giant cylindrical
Sommerfeld surface wave. Laboratory measurements, simulations, and analytic
approximations all converge on a 20 J laser pulse exciting a 1 Joule, 400 GW
broadband THz surface wave, with a peak electric field strength of 35 GV/m.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [37] [Hamiltonian formulation of the quasineutral Vlasov-Poisson system](https://arxiv.org/abs/2506.21415)
*J. W. Burby*

Main category: math-ph

TL;DR: The paper uses slow manifold reduction and Poisson-Dirac submanifolds to derive a Hamiltonian formulation for a quasineutral limit of the Vlasov-Poisson system, modeling plasma dynamics without fast Langmuir oscillations.


<details>
  <summary>Details</summary>
Motivation: To provide a Hamiltonian framework for quasineutral plasma dynamics, avoiding fast oscillations and ensuring incompressible flow.

Method: Applies slow manifold reduction and Poisson-Dirac submanifold theory to the Vlasov-Poisson system.

Result: A Hamiltonian structure for the quasineutral model, combining known Poisson brackets for incompressible fluids and kinetic equations.

Conclusion: The derived Hamiltonian formulation synthesizes existing theories and simplifies plasma dynamics modeling.

Abstract: Slow manifold reduction and the theory of Poisson-Dirac submanifolds are used
to deduce a Hamiltonian formulation for a quasineutral limit of the planar,
collisionless, magnetized Vlasov-Poisson system. Motion on the slow manifold
models plasma dynamics free of fast Langmuir oscillations. Preservation of
quasineutrality requires the bulk plasma flow is incompressible. The electric
field is determined by counterbalancing plasma stresses that would otherwise
produce compression. The Hamiltonian structure for the quasineutral model
synthesizes well-known Poisson brackets for incompressible fluids and
collisionless kinetic equations.

</details>


### [38] [Asymmetry of curl eigenfields solving Woltjer's variational problem](https://arxiv.org/abs/2506.21243)
*Daniel Peralta-Salas,David Perrella,David Pfefferlé*

Main category: math-ph

TL;DR: The paper shows that in certain toroidal domains in ℝ³, eigenfields for the first Ampèrian curl eigenvalue may or may not be symmetric, disproving the assumption that symmetric domains always yield symmetric eigenfields.


<details>
  <summary>Details</summary>
Motivation: To challenge the common belief that symmetric domains guarantee symmetric eigenfields for the lowest curl eigenvalue.

Method: Construction of rotationally symmetric toroidal domains and analysis of their eigenfields.

Result: Some domains have symmetric first eigenfields, while others do not, disproving the folk wisdom.

Conclusion: Minimizers of Woltjer's variational principle need not inherit the domain's rotational symmetry.

Abstract: We construct families of rotationally symmetric toroidal domains in $\mathbb
R^3$ for which the eigenfields associated to the first (positive) Amp\`erian
curl eigenvalue are symmetric, and others for which no first eigenfield is
symmetric. This implies, in particular, that minimizers of the celebrated
Woltjer's variational principle do not need to inherit the rotational symmetry
of the domain. This disproves the folk wisdom that the eigenfields
corresponding to the lowest curl eigenvalue must be symmetric if the domain is.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [39] [Stochastic and Non-local Closure Modeling for Nonlinear Dynamical Systems via Latent Score-based Generative Models](https://arxiv.org/abs/2506.20771)
*Xinghao Dong,Huchen Yang,Jin-Long Wu*

Main category: cs.LG

TL;DR: A latent score-based generative AI framework is proposed for learning stochastic, non-local closure models in nonlinear dynamical systems, reducing computational costs while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of modeling multiscale dynamical systems without clear scale separation, where resolving all scales is computationally expensive.

Method: Joint training of convolutional autoencoders with conditional diffusion models in latent spaces to reduce dimensionality and preserve physical characteristics.

Result: The framework achieves computational acceleration with comparable predictive accuracy to standard diffusion models in physical spaces.

Conclusion: The proposed method effectively balances computational efficiency and accuracy for closure modeling in complex systems.

Abstract: We propose a latent score-based generative AI framework for learning
stochastic, non-local closure models and constitutive laws in nonlinear
dynamical systems of computational mechanics. This work addresses a key
challenge of modeling complex multiscale dynamical systems without a clear
scale separation, for which numerically resolving all scales is prohibitively
expensive, e.g., for engineering turbulent flows. While classical closure
modeling methods leverage domain knowledge to approximate subgrid-scale
phenomena, their deterministic and local assumptions can be too restrictive in
regimes lacking a clear scale separation. Recent developments of
diffusion-based stochastic models have shown promise in the context of closure
modeling, but their prohibitive computational inference cost limits practical
applications for many real-world applications. This work addresses this
limitation by jointly training convolutional autoencoders with conditional
diffusion models in the latent spaces, significantly reducing the
dimensionality of the sampling process while preserving essential physical
characteristics. Numerical results demonstrate that the joint training approach
helps discover a proper latent space that not only guarantees small
reconstruction errors but also ensures good performance of the diffusion model
in the latent space. When integrated into numerical simulations, the proposed
stochastic modeling framework via latent conditional diffusion models achieves
significant computational acceleration while maintaining comparable predictive
accuracy to standard diffusion models in physical spaces.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [40] [Correlated reaction coordinate motion produces non-additive rate enhancement for electron and energy transfer in multi-acceptor structures](https://arxiv.org/abs/2506.21504)
*Hanggai Nuomin,Feng-Feng Song,Peng Zhang,David N. Beratan*

Main category: physics.chem-ph

TL;DR: Multi-acceptor systems show quantum interference effects, increasing ET rates beyond classical predictions due to acceptor-acceptor interactions.


<details>
  <summary>Details</summary>
Motivation: To understand why ET rates in multi-acceptor systems exceed simple additive predictions.

Method: Analyzed coupling interactions in multi-acceptor systems, focusing on free energy shifts, donor-acceptor coupling changes, and reaction-coordinate alterations.

Result: Found 4- to 5-fold ET rate increase due to acceptor-acceptor interactions, explaining experimental observations.

Conclusion: Multi-acceptor interactions can tailor ET/EnT kinetics, offering new design strategies for molecular systems.

Abstract: Molecular structures with multiple donor, bridge, or acceptor units can
display quantum interference effects that influence electron and energy
transfer (ET and EnT) rates. Recent experiments found a 4- to 5-fold increase
in ET rates for donor-acceptor structures with two acceptors compared to one.
This result is surprising: simple classical or quantum analysis suggests a
factor of two rate enhancement. We analyze the coupling interactions in
multiple acceptor systems and find that rate enhancements beyond additive
effects arise from acceptor-acceptor interactions that: 1) shift the reaction
free energy, 2) change the donor-acceptor couplings, and 3) alter the
reaction-coordinate motion. Consideration of these effects explains the
observed rates in multi-acceptor systems and suggests strategies to tailor
energy and electron transfer kinetics.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [41] [Linear stability of Kerr black holes in the full subextremal range](https://arxiv.org/abs/2506.21183)
*Dietrich Häfner,Peter Hintz,András Vasy*

Main category: gr-qc

TL;DR: Unconditional proof of linear stability for the Kerr family in the full subextremal range, extending earlier work with additional results on mode stability and zero energy behavior.


<details>
  <summary>Details</summary>
Motivation: To establish the linear stability of the Kerr family in the full subextremal range, building on prior work and addressing gaps.

Method: Extends the analytic proof from earlier work, incorporating mode stability results and zero energy behavior computations.

Result: Successfully proves the linear stability of the Kerr family unconditionally.

Conclusion: The study confirms the stability of the Kerr family in the subextremal range, leveraging new insights and prior findings.

Abstract: We prove, unconditionally, the linear stability of the Kerr family in the
full subextremal range. On an analytic level, our proof is the same as that of
our earlier paper in the slowly rotating case. The additional ingredients we
use are, firstly, the mode stability result proved by Andersson, Whiting, and
the first author and, secondly, computations related to the zero energy
behavior of the linearized gauge-fixed Einstein equation in work by the second
author.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [42] [A generalised framework for phase field-based modelling of coupled problems: application to thermo-mechanical fracture, hydraulic fracture, hydrogen embrittlement and corrosion](https://arxiv.org/abs/2506.20763)
*Y. Navidtehrani,C. Betegón,E. Martínez-Pañeda*

Main category: cs.CE

TL;DR: A novel phase field and multi-physics approach for coupled structural integrity problems, implemented in Abaqus via UMAT/UMATHT subroutines, validated with experimental and analytical data.


<details>
  <summary>Details</summary>
Motivation: To address coupled structural integrity problems (e.g., thermo-mechanical fracture, hydraulic fracture) with a versatile, generalised framework.

Method: Combines phase field and multi-physics modelling, leveraging the heat transfer equation for implementation in commercial finite element packages (e.g., Abaqus).

Result: Demonstrates excellent agreement with experimental data and existing solutions for 2D/3D problems.

Conclusion: The framework is effective for diverse engineering problems, with freely available subroutines for practical use.

Abstract: We present a novel, generalised formulation to treat coupled structural
integrity problems by combining phase field and multi-physics modelling. The
approach exploits the versatility of the heat transfer equation and is
therefore well suited to be adopted in commercial finite element packages,
requiring only integration point-level implementation. This aspect is
demonstrated here by implementing coupled, multi-variable phenomena through
simple \texttt{UMAT} and \texttt{UMATHT} subroutines in the finite element
package \texttt{Abaqus}. The generalised theoretical and computational
framework presented is particularised to four problems of engineering and
scientific relevance: thermo-mechanical fracture, hydraulic fracture,
hydrogen-assisted cracking and metallic corrosion. 2D and 3D problems are
considered. The results reveal a very good agreement with experimental data,
and existing numerical and analytical solutions.The user subroutines developed
are made freely available at https://mechmat.web.ox.ac.uk/codes.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [43] [Anisotropy-Induced Magnetic Field Generation in Bidimensional Materials](https://arxiv.org/abs/2506.21464)
*Diogo Simões,Hugo Terças,Jorge Ferreira*

Main category: cond-mat.mtrl-sci

TL;DR: Study of electromagnetic instability in 2D materials due to Fermi surface anisotropy, analyzing growth rates, confinement, and magnetic field generation.


<details>
  <summary>Details</summary>
Motivation: To understand how anisotropy in the Fermi surface of 2D materials leads to electromagnetic instabilities and their effects.

Method: Uses a kinetic model considering temperature, chemical potential, anisotropy ratio, and band structures (linear/quadratic). Analyzes growth rates in the linear regime and simulates behavior in saturation.

Result: Derived wavenumber-dependent growth rates, described confinement of unstable modes, and demonstrated structured magnetic field generation.

Conclusion: The study reveals how Fermi surface anisotropy drives electromagnetic instabilities in 2D materials, with implications for their behavior in practical applications.

Abstract: We investigate an electromagnetic instability in two-dimensional materials
arising from an anisotropy of the Fermi surface, utilizing a kinetic model
accounting for the effects of the values of temperature, chemical potential and
anisotropy ratio, as well as considering both linear and quadratic low-energy
band structures. The wavenumber-dependent growth-rate of these modes is derived
in the linear regime, and their confinement, contrasting with stable
electromagnetic waves in these systems, is described. The generation of
structured out-of-plane magnetic fields, as well as their behaviour in
saturation, is shown using fully kinetic and non-linear simulations.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [44] [Surrogate normal-forms for the numerical bifurcation and stability analysis of navier-stokes flows via machine learning](https://arxiv.org/abs/2506.21275)
*Alessandro Della Pia,Dimitrios G. Patsatzis,Gianluigi Rozza,Lucia Russo,Constantinos Siettos*

Main category: physics.flu-dyn

TL;DR: The paper presents a framework for constructing minimal-dimensional surrogate models from high-fidelity Navier-Stokes simulations using manifold learning and Gaussian Process Regression, enabling efficient bifurcation and stability analysis.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome computational challenges in analyzing high-dimensional spatio-temporal dynamics by deriving reduced-order models (ROMs) for efficient bifurcation and stability analysis.

Method: The method involves four steps: manifold learning to identify latent dimensions, constructing ROMs using Gaussian Process Regression, applying numerical bifurcation tools in latent space, and reconstructing results in the original space.

Result: The framework successfully identifies latent dimensionality and constructs surrogate models, enabling analysis of bifurcations (e.g., Andronov-Hopf and pitchfork) and limit cycles in canonical flows.

Conclusion: The proposed framework efficiently bridges high-fidelity simulations with reduced-order modeling, enabling complex bifurcation and stability analyses previously intractable in full space.

Abstract: Inspired by the Equation-Free multiscale modeling approach, we demonstrate
how the embed-learn-lift framework enables the construction of surrogate
normal-forms, namely minimal-dimensional reduced-order models (ROMs), from
high-fidelity Navier-Stokes simulations. These surrogate models are then used
for efficient and accurate bifurcation and stability analysis. The framework
proceeds in four steps. First, manifold learning reveals the intrinsic latent
dimension of the high-dimensional spatio-temporal Navier-Stokes dynamics across
parameter space. Second, we construct low-dimensional "normal-form" like ROMs
on this latent space using Gaussian Process Regression (GPR), capturing the
emergent dynamics. Third, using these models, we apply numerical bifurcation
tools to compute bifurcation diagrams and perform stability analysis in the
latent space. This includes tracing branches of limit cycles arising from
Andronov-Hopf bifurcations - tasks intractable in full space due to
computational cost. Finally, solving the pre-image problem allows
reconstruction of the bifurcation structure in the original high-dimensional
space. We demonstrate the methodology on two canonical flows: wake flow past an
infinite circular cylinder and planar sudden-expansion channel flow. These
exhibit Andronov-Hopf and pitchfork bifurcations, respectively, as Reynolds
number increases. Our method identifies the latent dimensionality and
constructs GPR-based surrogate normal-forms that enable the tracing and
stability analysis of bifurcating solutions, including limit cycles, their
period, and stability via Floquet multipliers.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [45] [Quantum Adaptive Search: A Hybrid Quantum-Classical Algorithm for Global Optimization of Multivariate Functions](https://arxiv.org/abs/2506.21124)
*G. Intoccia,U. Chirico,V. Schiano Di Cola,G. Pepe,S. Cuomo*

Main category: quant-ph

TL;DR: QAGS is a hybrid quantum-classical algorithm for global optimization, using adaptive quantum search and classical refinement to achieve high accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: To improve global optimization of multivariate functions by leveraging quantum computing for adaptive search and classical methods for refinement.

Method: Combines quantum state encoding for solution quality estimation with classical local optimization, dynamically narrowing the search space.

Result: QAGS outperforms classical methods in accuracy, time, and space complexity on benchmark functions.

Conclusion: QAGS is effective for global optimization, offering computational advantages over classical approaches.

Abstract: This work presents Quantum Adaptive Search (QAGS), a hybrid quantum-classical
algorithm for the global optimization of multivariate functions. The method
employs an adaptive mechanism that dynamically narrows the search space based
on a quantum-estimated probability distribution of the objective function. A
quantum state encodes information about solution quality through an appropriate
complex amplitude mapping, enabling the identification of the most promising
regions, and thus progressively tightening the search bounds; then a classical
optimizer performs local refinement of the solution. The analysis demonstrates
that QAGS ensures a contraction of the search space toward global optima, with
controlled computational complexity. The numerical results on the benchmark
functions show that, compared to the classical methods, QAGS achieves higher
accuracy while offering advantages in both time and space complexity.

</details>
