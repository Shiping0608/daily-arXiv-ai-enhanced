<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 25]
- [math.AP](#math.AP) [Total: 31]
- [physics.comp-ph](#physics.comp-ph) [Total: 11]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 7]
- [math.OC](#math.OC) [Total: 4]
- [math.PR](#math.PR) [Total: 2]
- [math.NT](#math.NT) [Total: 1]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [math.AG](#math.AG) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [math.CV](#math.CV) [Total: 1]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [physics.chem-ph](#physics.chem-ph) [Total: 2]
- [math.ST](#math.ST) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [math.FA](#math.FA) [Total: 2]
- [physics.space-ph](#physics.space-ph) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 3]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 1]
- [math.DG](#math.DG) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [The lightning method for the heat equation](https://arxiv.org/abs/2506.22576)
*Hunter L Croix,Alan E. Lindsay*

Main category: math.NA

TL;DR: A new method using the Lightning Method and Laplace transform solves the planar heat equation with spectral accuracy and root-exponential convergence.


<details>
  <summary>Details</summary>
Motivation: To address challenges in solving the planar heat equation, especially in domains with sharp corners and singularities.

Method: Combines the Lightning Method (using sums of polynomials/rational functions) with Laplace transform to solve a modified Helmholtz equation, followed by Talbot integration for numerical inversion.

Result: Achieves spectral accuracy and root-exponential convergence, validated against existing results and test problems.

Conclusion: The method is robust, adaptable to various geometries, and effective across different time intervals.

Abstract: This paper introduces a new method for solving the planar heat equation based
on the Lightning Method. The lightning method is a recent development in the
numerical solution of linear PDEs which expresses solutions using sums of
polynomials and rational functions, or more generally as sums of fundamental
solutions. The method is particularly well suited to handle domains with sharp
corners where solution singularities are present. Boundary conditions are
formed on a set of collocation points which is then solved as an overdetermined
linear system. The approach of the present work is to utilize the Laplace
transform to obtain a modified Helmholtz equation which is solved by an
application of the lightning method. The numerical inversion of the Laplace
transform is then performed by means of Talbot integration. Our validation of
the method against existing results and multiple challenging test problems
shows the method attains spectral accuracy with root-exponential convergence
while being robust across a wide range of time intervals and adaptable to a
variety of geometric scenarios.

</details>


### [2] [Error Estimates for the Arnoldi Approximation of a Matrix Square Root](https://arxiv.org/abs/2506.22615)
*James H. Adler,Xiaozhe Hu,Wenxiao Pan,Zhongqin Xue*

Main category: math.NA

TL;DR: The paper extends error analysis for approximating matrix square roots using the Arnoldi process to non-Hermitian matrices, refines bounds for large-scale problems, and validates results numerically.


<details>
  <summary>Details</summary>
Motivation: To generalize error estimates for matrix square root approximations beyond Hermitian matrices and adapt the method for large-scale applications.

Method: Derives an a priori error estimate using the Arnoldi process, reformulates error in terms of linear system solutions, and refines bounds for data-sparse approximations.

Result: Theoretical analysis provides reliable error bounds, validated by numerical experiments on diverse matrices and large-scale simulations.

Conclusion: The approach is effective for non-Hermitian matrices and scalable for large problems, as demonstrated by simulations.

Abstract: The Arnoldi process provides an efficient framework for approximating
functions of a matrix applied to a vector, i.e., of the form $f(M)\mathbf{b}$,
by repeated matrix-vector multiplications. In this paper, we derive an
\textit{a priori} error estimate for approximating the action of a matrix
square root using the Arnoldi process, where the integral representation of the
error is reformulated in terms of the error for solving the linear system
$M\mathbf{x}=\mathbf{b}$. The results extend the error analysis of the Lanczos
method for Hermitian matrices in [Chen et al., SIAM J. Matrix Anal. Appl.,
2022] to non-Hermitian cases. Furthermore, to make the method applicable to
large-scale problems, we assume that the matrices are preprocessed utilizing
data-sparse approximations preserving positive definiteness, and then establish
a refined error bound in this setting. The numerical results on matrices with
different structures demonstrate that our theoretical analysis yields a
reliable upper bound. Finally, simulations on large-scale matrices arising in
particulate suspensions validate the effectiveness and practicality of the
approach.

</details>


### [3] [A Class of Stochastic Runge-Kutta Methods for Stochastic Differential Equations Converging with Order 1 in $L^p$-Norm](https://arxiv.org/abs/2506.22657)
*Andreas Rößler*

Main category: math.NA

TL;DR: A new class of efficient stochastic Runge-Kutta (SRK) methods is developed for solving Itô and Stratonovich SDEs, requiring only two stages for order 1, applicable to non-commutative or commutative noise.


<details>
  <summary>Details</summary>
Motivation: To develop highly efficient SRK methods for SDEs with minimal computational cost and broad applicability.

Method: Proposes two-stage SRK methods for order 1, including variants for additive noise and drift-implicit schemes, with explicit order conditions.

Result: The methods are computationally efficient (linear cost in SDE and Wiener process dimensions) and achieve strong convergence of order 1 in L^p-norm (p ≥ 2).

Conclusion: Theoretical results are validated numerically, confirming the efficiency and convergence properties of the new SRK methods.

Abstract: For the approximation of solutions for It\^o and Stratonovich stochastic
differential equations (SDEs)a new class of efficient stochastic Runge-Kutta
(SRK) methods is developed. As the main novelty only two stages are necessary
for the proposed SRK methods of order 1 that can be applied to SDEs with
non-commutative or with commutative noise. In addition, a variant of the SRK
method for SDEs with additive noise is presented. All proposed SRK methods
cover also the case of drift-implicit schemes and general order conditions for
the coefficients are calculated explicitly. The new class of SRK methods is
highly efficient in the sense that it features computational cost depending
only linearly on the dimension of the SDE and on the dimension of the driving
Wiener process. For all proposed SRK methods strong convergence with order 1 in
$L^p$-norm for any $p \geq 2$ is proved. Moreover, sufficient conditions for
approximated iterated stochastic integrals are established such that
convergence with order 1 in $L^p$-norm is preserved if they are applied for the
SRK method. The presented theoretical results are confirmed by numerical
experiments.

</details>


### [4] [Hybrid Explicit-Implicit Predictor-Corrector Exponential Time-Differencing Multistep Padé Schemes for Semilinear Parabolic Equations with Time-Delay](https://arxiv.org/abs/2506.22664)
*Haishen Dai,Huan Lei*

Main category: math.NA

TL;DR: Proposes ETD-MS-Padé and ETD-IMS-Padé schemes for semilinear parabolic delay differential equations, improving upon the complexity of previous ETD-RK-Padé methods. A predictor-corrector scheme is introduced, showing better convergence than existing methods.


<details>
  <summary>Details</summary>
Motivation: To simplify and improve the numerical solution of semilinear parabolic delay differential equations, addressing the complexity and instability of previous methods.

Method: Combines ETD-based Adams multistep extrapolation with Padé approximation for exponential operators, forming ETD-MS-Padé and ETD-IMS-Padé schemes, and validates them through numerical experiments.

Result: The predictor-corrector scheme outperforms the EERK scheme in convergence, and the new methods are validated for arbitrary time orders.

Conclusion: The proposed schemes are efficient tools for solving parabolic differential equations, offering improved stability and convergence.

Abstract: In this paper, we propose and analyze ETD-Multistep-Pad\'{e}
(ETD-MS-Pad\'{e}) and ETD Implicit Multistep-Pad\'{e} (ETD-IMS-Pad\'{e}) for
semilinear parabolic delay differential equations with smooth solutions. In our
previous work [15], we proposed ETD-RK-Pad\'{e} scheme to compute high-order
numerical solutions for nonlinear parabolic reaction-diffusion equation with
constant time delay. However, the based ETD-RK numerical scheme in [15] is very
complex and the corresponding calculation program is also very complicated. We
propose in this paper ETD-MS-Pad\'{e} and ETD-IMS-Pad\'{e} schemes for the
solution of semilinear parabolic equations with delay. We synergize the
ETD-MS-Pad\'{e} with ETD-IMS-Pad\'{e} to construct efficient
predictor-corrector scheme. This new predictor-corrector scheme will become an
important tool for solving the numerical solutions of parabolic differential
equations. Remarkably, we also conducted experiments in Table$10$ to compare
the numerical results of the predictor-corrector scheme with the EERK scheme
proposed in paper [42]. The predictor-corrector scheme demonstrated better
convergence.
  The main idea is to employ an ETD-based Adams multistep extrapolation for the
time integration of the corresponding equation. To overcome the well-known
numerical instability associated with computing the exponential operator, we
utilize the Pad\'{e} approach to approximate this exponential operator. This
methodology leads to the development of the ETD-MS-Pad\'{e} and
ETD-IMS-Pad\'{e} schemes, applicable even for arbitrary time orders. We
validate the ETD-MS1,2,3,4-Pad\'{e} schemes and ETD-IMS2,3,4 schemes through
numerical experiments.

</details>


### [5] [A new sparsity promoting residual transform operator for Lasso regression](https://arxiv.org/abs/2506.22689)
*Yao Xiao,Anne Gelb,Aditya Viswanathan*

Main category: math.NA

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Lasso regression is a widely employed approach within the $\ell_1$
regularization framework used to promote sparsity and recover piecewise smooth
signals $f:[a,b) \rightarrow \mathbb{R}$ when the given observations are
obtained from noisy, blurred, and/or incomplete data environments. In choosing
the regularizing sparsity-promoting operator, it is assumed that the particular
type of variability of the underlying signal, for example, piecewise constant
or piecewise linear behavior across the entire domain, is both known and fixed.
Such an assumption is problematic in more general cases, e.g.~when a signal
exhibits piecewise oscillatory behavior with varying wavelengths and
magnitudes. To address the limitations of assuming a fixed (and typically low
order) variability when choosing a sparsity-promoting operator, this
investigation proposes a novel residual transform operator that can be used
within the Lasso regression formulation. In a nutshell, the idea is that for a
general piecewise smooth signal $f$, it is possible to design two operators
$\mathcal L_1$ and $\mathcal L_2$ such that $\mathcal L_1{\boldsymbol f}
\approx \mathcal L_2{\boldsymbol f}$, where ${\boldsymbol f} \in \mathbb{R}^n$
is a discretized approximation of $f$, but $\mathcal L_1 \not\approx \mathcal
L_2$. The corresponding residual transform operator, $\mathcal L = \mathcal
L_1- \mathcal L_2$, yields a result that (1) effectively reduces the
variability dependent error that occurs when applying either $\mathcal L_1$ or
$\mathcal L_2$ to ${\boldsymbol f}$, a property that holds even when $\mathcal
L_1{\boldsymbol f} \approx \mathcal L_2{\boldsymbol f}$ is not a good
approximation to the true sparse domain vector of ${\boldsymbol f}$, and (2)
does not require $\mathcal L_1$ or $\mathcal L_2$ to have prior information
regarding the variability of the underlying signal.

</details>


### [6] [A Novel Adaptive Low-Rank Matrix Approximation Method for Image Compression and Reconstruction](https://arxiv.org/abs/2506.22713)
*Weiwei Xu,Weijie Shen,Chang Liu,Zhigang Jia*

Main category: math.NA

TL;DR: EOD-ABE is a novel method for low-rank matrix approximation that automatically identifies the optimal rank, eliminating the need for manual rank estimation. It offers speed, accuracy, and robustness, making it useful for image compression and dimensionality reduction.


<details>
  <summary>Details</summary>
Motivation: Existing methods for low-rank matrix approximation require manual rank estimation or additional steps, which can be inefficient. EOD-ABE aims to automate this process for better efficiency and accuracy.

Method: EOD-ABE uses a randomized basis extraction mechanism to compute rank-revealing approximations without additional rank determination steps. It has a computational complexity of O(mnr).

Result: EOD-ABE outperforms state-of-the-art methods in speed, accuracy, and robustness, as demonstrated in experiments.

Conclusion: EOD-ABE is an efficient and powerful tool for low-rank matrix approximation, particularly in large-scale applications like image compression and hyperspectral image processing.

Abstract: Low-rank matrix approximation plays an important role in various applications
such as image processing, signal processing and data analysis. The existing
methods require a guess of the ranks of matrices that represent images or
involve additional costs to determine the ranks. A novel efficient orthogonal
decomposition with automatic basis extraction (EOD-ABE) is proposed to compute
the optimal low-rank matrix approximation with adaptive identification of the
optimal rank. By introducing a randomized basis extraction mechanism, EOD-ABE
eliminates the need for additional rank determination steps and can compute a
rank-revealing approximation to a low-rank matrix. With a computational
complexity of $O(mnr)$, where $m$ and $n$ are the dimensions of the matrix and
$r$ is its rank, EOD-ABE achieves significant speedups compared to the
state-of-the-art methods. Experimental results demonstrate the superior speed,
accuracy and robustness of EOD-ABE and indicate that EOD-ABE is a powerful tool
for fast image compression and reconstruction and hyperspectral image
dimensionality reduction in large-scale applications.

</details>


### [7] [Long-time error estimate and decay of finite element method to a generalized viscoelastic flow](https://arxiv.org/abs/2506.22782)
*Yingwen Guo,Yinnian He,Wenlin Qiu,Xiangcheng Zheng*

Main category: math.NA

TL;DR: The paper analyzes a viscoelastic flow model using finite element approximation, proving solution regularity, long-time decay, and numerical error estimates. A benchmark simulation validates the model.


<details>
  <summary>Details</summary>
Motivation: To generalize the Navier-Stokes equation and Oldroyd's model by introducing a tempered power-law memory kernel for viscoelastic flows.

Method: Develops a Volterra-Stokes projection for parabolic-type duality, proves regularity, and uses finite element approximation. Simulates a benchmark problem for validation.

Result: Proves solution regularity, long-time exponential decay, and provides error estimates. Benchmark shows model generality.

Conclusion: The proposed model effectively generalizes existing models, with proven theoretical properties and practical validation.

Abstract: This work analyzes the finite element approximation to a viscoelastic flow
model, which generalizes the Navier-Stokes equation and Oldroyd's model by
introducing the tempered power-law memory kernel. We prove regularity and
long-time exponential decay of the solutions, as well as a long-time
convolution-type Gr\"onwall inequality to support numerical analysis. A
Volterra-Stokes projection is developed and analyzed to facilitate the
parabolic-type duality argument, leading to the long-time error estimates and
exponential decay of velocity and pressure. A benchmark problem of planar
four-to-one contraction flow is simulated to substantiate the generality of the
proposed model in comparison with the Navier-Stokes equation and Oldroyd's
model.

</details>


### [8] [A Chimera domain decomposition method with weak Dirichlet-Robin coupling for finite element simulation of particulate flows](https://arxiv.org/abs/2506.22831)
*Raphael Münster,Otto Mierka,Dmitri Kuzmin,Stefan Turek*

Main category: math.NA

TL;DR: A multimesh finite element method for incompressible particulate flows is introduced, combining background and body-fitted submeshes for accurate force and torque calculations.


<details>
  <summary>Details</summary>
Motivation: To improve accuracy in simulating particulate flows by leveraging overlapping domain decomposition and coupling techniques.

Method: Uses overlapping meshes (background and body-fitted submeshes) with Robin and Dirichlet-type coupling for solving Navier-Stokes equations.

Result: Demonstrates significant accuracy gains in drag and lift approximations compared to fictitious boundary methods.

Conclusion: The proposed method enhances simulation accuracy for particulate flows, validated through numerical studies.

Abstract: We introduce a new multimesh finite element method for direct numerical
simulation of incompressible particulate flows. The proposed approach falls
into the category of overlapping domain decomposition / Chimera / overset grid
meshes. In addition to calculating the velocity and pressure of the fictitious
fluid on a fixed background mesh, we solve the incompressible Navier-Stokes
equations on body-fitted submeshes that are attached to moving particles. The
submesh velocity and pressure are used to calculate the hydrodynamic forces and
torques acting on the particles. The coupling with the background velocity and
pressure is enforced via (i) Robin-type boundary conditions for an
Arbitrary-Lagrangian-Eulerian (ALE) formulation of the submesh problems and
(ii) a Dirichlet-type distributed interior penalty term in the weak form of the
background mesh problem. The implementation of the weak Dirichlet-Robin
coupling is discussed in the context of discrete projection methods and finite
element discretizations. Detailed numerical studies are performed for standard
test problems involving fixed and moving immersed objects. A comparison of
Chimera results with those produced by fictitious boundary methods illustrates
significant gains in the accuracy of drag and lift approximations.

</details>


### [9] [A Dilation-based Seamless Multiscale Method For Elliptic Problems](https://arxiv.org/abs/2506.22912)
*Ziheng Chen,Björn Engquist*

Main category: math.NA

TL;DR: The paper introduces seamless methods for multiscale problems, extending them to numerical homogenization in multiple dimensions. It proves a 1D elliptic operator can be rewritten as a multiscale dynamical system and proposes a novel local dilation approach for higher dimensions, balancing accuracy and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: To reduce the reliance on scale separation in multiscale problems, enabling more efficient and accurate numerical methods for homogenization.

Method: Translates seamless methods from multiscale dynamical systems to numerical homogenization, proves a 1D elliptic operator can be rewritten as a multiscale system, and introduces a local dilation approach for higher dimensions.

Result: Develops error estimates and demonstrates promising numerical results for various examples.

Conclusion: The proposed methods provide a practical balance between accuracy and computational efficiency for multiscale problems without requiring full resolution.

Abstract: Many numerical methods for multiscale differential equations require a scale
separation between the larger and the smaller scales to achieve accuracy and
computational efficiency. In the area of multiscale dynamical systems,
so-called, seamless methods have been introduced to reduce the requirement of
scale separation. We will translate these methods to numerical homogenization
problems and extend the technique to multiple dimensions. The initial step is
to prove that a one-dimensional \sepia{second-order} elliptic operator with
oscillatory coefficients can be rewritten as a multiscale dynamical system.
Inspired by this, multiscale elliptic operators in higher dimensions are
approximated by a novel approach based on local dilation, which provides a
middle ground for balancing intractability and accuracy without the need for
full resolution. The dilation operator can be further generalized to preserve
important structures by properly decomposing the coefficient field. Error
estimates are developed and promising numerical results of different examples
are included.

</details>


### [10] [An approximation theory for Markov chain compression](https://arxiv.org/abs/2506.22918)
*Mark Fornace,Michael Lindsey*

Main category: math.NA

TL;DR: A framework for compressing reversible Markov chains with error control, using two compression schemes and validated by numerical experiments.


<details>
  <summary>Details</summary>
Motivation: To provide a method for compressing reversible Markov chains while rigorously controlling approximation errors.

Method: Two compression schemes: projective (using committor functions) and structure-preserving (induced Markov chain). Error bounds are derived using Nyström approximation.

Result: Theoretical bounds on recovery error are proven, and numerical experiments confirm scalability.

Conclusion: The framework effectively compresses Markov chains with controlled error, validated by scalable numerical results.

Abstract: We develop a framework for the compression of reversible Markov chains with
rigorous error control. Given a subset of selected states, we construct reduced
dynamics that can be lifted to an approximation of the full dynamics, and we
prove simple spectral and nuclear norm bounds on the recovery error in terms of
a suitably interpreted Nystr\"{o}m approximation error. We introduce two
compression schemes: a projective compression based on committor functions and
a structure-preserving compression defined in terms of an induced Markov chain
over the selected states. The Nystr\"{o}m error appearing in our bounds can be
controlled using recent results on column subset selection by nuclear
maximization. Numerical experiments validate our theory and demonstrate the
scalability of our approach.

</details>


### [11] [PML method for the time-domain stochastic acoustic wave equation and an inverse source problem](https://arxiv.org/abs/2506.23084)
*Hongxia Guo,Tianjiao Wang,Xiang Xu,Yue Zhao*

Main category: math.NA

TL;DR: The paper develops a time-domain PML method for the stochastic acoustic wave equation with white Gaussian noise, analyzes its well-posedness, and proposes an inverse random source problem solution with stability estimates.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of low regularity in random sources and ensure stability in solving the stochastic acoustic wave equation.

Method: Uses scattering theory for meromorphic continuation, constructs an approximate wave solution, and formulates a time-domain PML method. Also proposes a frequency-domain approach for inverse problems.

Result: Convergence of the PML method is proven, with explicit dependencies on parameters. A logarithmic stability estimate for the inverse problem is derived.

Conclusion: The PML method is effective for stochastic acoustic waves, and the stability estimate guides numerical scheme design for inverse problems.

Abstract: In this paper, we develop and analyze a time-domain perfectly matched layer
(PML) method for the stochastic acoustic wave equation driven by spatially
white additive Gaussian noise. We begin by establishing the well-posedness and
stability of the direct problem through a rigorous analysis of the associated
time-harmonic stochastic Helmholtz equation and the application of an abstract
Laplace transform inversion theorem. To address the low regularity of the
random source, we employ scattering theory to investigate the meromorphic
continuation of the Helmholtz resolvent defined on rough fields. Based on a
piecewise constant approximation of the white noise, we construct an
approximate wave solution and formulate a time-domain PML method. The
convergence of the PML method is established, with explicit dependence on the
PML layer's thickness and medium properties, as well as the piecewise constant
approximation of the white noise. In addition, we propose a frequency-domain
approach for solving the inverse random source problem using time-domain
boundary measurements. A logarithmic stability estimate is derived,
highlighting the ill-posedness of the inverse problem and offering guidance for
the design of effective numerical schemes.

</details>


### [12] [A residual driven multiscale method for Darcy's flow in perforated domains](https://arxiv.org/abs/2506.23093)
*Wei Xie,Shubin Fu,Yin Yang,Yunqing Huang*

Main category: math.NA

TL;DR: A residual-driven multiscale method for Darcy flow in perforated domains reduces computational cost by focusing on pressure and using adaptive enrichment.


<details>
  <summary>Details</summary>
Motivation: Direct simulations of Darcy flow in complex, heterogeneous perforated domains are computationally expensive.

Method: Velocity elimination simplifies the system to pressure-only. Offline and online basis functions are constructed within GMsFEM, with adaptive online enrichment using residuals.

Result: Numerical experiments show reduced computational cost and high accuracy, especially with adaptive online enrichment.

Conclusion: The method efficiently and accurately simulates Darcy flow in complex perforated domains.

Abstract: In this paper, we present a residual-driven multiscale method for simulating
Darcy flow in perforated domains, where complex geometries and highly
heterogeneous permeability make direct simulations computationally expensive.
To address this, we introduce a velocity elimination technique that
reformulates the mixed velocity-pressure system into a pressure-only
formulation, significantly reducing complexity by focusing on the dominant
pressure variable. Our method is developed within the Generalized Multiscale
Finite Element Method (GMsFEM) framework. For each coarse block, we construct
offline basis functions from local spectral problems that capture key geometric
and physical features. Online basis functions are then adaptively enriched
using residuals, allowing the method to incorporate global effects such as
source terms and boundary conditions, thereby improving accuracy. We provide
detailed error analysis demonstrating how the offline and online spaces
contribute to the accuracy and efficiency of the solution. Numerical
experiments confirm the method's effectiveness, showing substantial reductions
in computational cost while maintaining high accuracy, particularly through
adaptive online enrichment. These results highlight the method's potential for
efficient and accurate simulation of Darcy flow in complex, heterogeneous
perforated domains.

</details>


### [13] [An \textsf{AT1} phase-field framework for quasi-static anti-plane shear fracture: Unifying $ξ$-based adaptivity and nonlinear strain energy density function](https://arxiv.org/abs/2506.23249)
*Maria P. Fernando,S. M. Mallikarjunaiah*

Main category: math.NA

TL;DR: A novel AT1 phase-field framework for simulating quasi-static anti-plane shear fracture is introduced, combining local mesh adaptivity and nonlinear strain energy for improved computational efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: To address challenges in simulating fracture in nonlinearly constituted materials by unifying mesh adaptivity and nonlinear strain energy.

Method: Proposes a modified Francfort-Marigo energy functional with AT1 regularization, dynamically optimizing the damage zone length scale. Uses variational principles and conforming bilinear finite element method for discretization.

Result: Enhanced mesh adaptivity and robust convergence of numerical solutions, validated through simulations with calibrated penalty parameters.

Conclusion: The framework effectively improves computational efficiency and accuracy in fracture simulations for nonlinearly elastic materials.

Abstract: This work introduces a novel \textsf{AT1} phase-field framework for
simulating quasi-static anti-plane shear fracture in geometrically linear
elastic bodies. A key feature of this framework is the unification of
$\xi$-based local mesh adaptivity -- where $\xi$ represents the characteristic
length of the damage zone -- and an algebraically nonlinear strain energy
density function. A modified Francfort-Marigo energy functional, together with
its Ambrosio-Tortorelli-type regularization, is hereby proposed to address
challenges within the framework of nonlinearly constituted materials. We
dynamically optimize $\xi$ throughout the simulation, significantly enhancing
the computational efficiency and accuracy of numerically approximating the
local minimizers of the Ambrosio-Tortorelli (\textsf{AT1})-type phase-field
model. The proposed regularization for the total energy functional comprises
three distinct components: a nonlinear strain energy, an evolving surface
energy, and a linear-type regularization term dependent on the length scale of
the damage zone. Variational principles applied to this novel energy functional
yield a coupled system of governing second-order quasilinear partial
differential equations for the mechanics and phase-field variables. These
equations are subsequently discretized using the conforming bilinear finite
element method. The formulation is underpinned by four crucial parameters: two
are integral to the nonlinear strain energy function, while the other two serve
as penalty parameters. These penalty parameters are asymptotically calibrated
and rigorously utilized in the numerical simulations. Our results demonstrate
that this spatially adaptive approach leads to enhanced mesh adaptivity,
ensuring the robust convergence of the numerical solution.

</details>


### [14] [Data-Driven Self-Supervised Learning for the Discovery of Solution Singularity for Partial Differential Equations](https://arxiv.org/abs/2506.23344)
*Difeng Cai,Paulina Sepúlveda*

Main category: math.NA

TL;DR: The paper addresses singularity detection in scientific computing using a self-supervised learning framework to improve accuracy and reduce computational costs.


<details>
  <summary>Details</summary>
Motivation: Singularities in functions hinder numerical methods, especially when their locations are unknown. Detecting them is crucial for adaptive methods in PDEs and other applications.

Method: Proposes a self-supervised learning (SSL) framework with filtering methods (k-nearest neighbors and kernel density estimation) to estimate singularity locations from raw data.

Result: Numerical examples show the framework's effectiveness in handling input perturbations, label corruption, and various singularity types.

Conclusion: The SSL-based approach successfully detects singularities, offering a robust solution for data-driven singularity detection.

Abstract: The appearance of singularities in the function of interest constitutes a
fundamental challenge in scientific computing. It can significantly undermine
the effectiveness of numerical schemes for function approximation, numerical
integration, and the solution of partial differential equations (PDEs), etc.
The problem becomes more sophisticated if the location of the singularity is
unknown, which is often encountered in solving PDEs. Detecting the singularity
is therefore critical for developing efficient adaptive methods to reduce
computational costs in various applications. In this paper, we consider
singularity detection in a purely data-driven setting. Namely, the input only
contains given data, such as the vertex set from a mesh. To overcome the
limitation of the raw unlabeled data, we propose a self-supervised learning
(SSL) framework for estimating the location of the singularity. A key component
is a filtering procedure as the pretext task in SSL, where two filtering
methods are presented, based on $k$ nearest neighbors and kernel density
estimation, respectively. We provide numerical examples to illustrate the
potential pathological or inaccurate results due to the use of raw data without
filtering. Various experiments are presented to demonstrate the ability of the
proposed approach to deal with input perturbation, label corruption, and
different kinds of singularities such interior circle, boundary layer,
concentric semicircles, etc.

</details>


### [15] [A new family of a posteriori error estimates for non-conforming finite element methods leading to stabilization-free error bounds](https://arxiv.org/abs/2506.23381)
*T. Chaumont-Frelet*

Main category: math.NA

TL;DR: New a posteriori error estimators for non-conforming finite element methods for elliptic PDEs, offering efficiency without extra stabilization and polynomial-degree-robustness.


<details>
  <summary>Details</summary>
Motivation: Improve error estimation for non-conforming finite element discretizations of elliptic PDEs by avoiding extra stabilization terms and ensuring robustness across polynomial degrees.

Method: Novel reformulations of the Prager-Synge identity to derive residual-based and equilibrated estimators, with one providing guaranteed error bounds.

Result: Efficiency estimates without stabilization, optimal scaling in polynomial degree, and polynomial-degree-robustness for equilibrated estimators.

Conclusion: The proposed estimators enhance accuracy and robustness in error estimation for non-conforming finite element methods.

Abstract: We propose new a posteriori error estimators for non-conforming finite
element discretizations of second-order elliptic PDE problems. These estimators
are based on novel reformulations of the standard Prager-Synge identity, and
enable to prove efficiency estimates without extra stabilization terms in the
error measure for a large class of discretization schemes. We propose a
residual-based estimator for which the efficiency constant scales optimally in
polynomial degree, as well as two equilibrated estimators that are
polynomial-degree-robust. One of the two estimators further leads to guaranteed
error bounds.

</details>


### [16] [Fourth-order compact difference schemes for the one-dimensional Euler-Bernoulli beam equation with damping term](https://arxiv.org/abs/2506.23449)
*Wenjie Huang,Hao Wang,Shiquan Zhang,Qinyi Zhang*

Main category: math.NA

TL;DR: A compact finite difference method for the Euler-Bernoulli beam equation with damping terms achieves high accuracy (4th-order in space, 2nd-order in time) using minimal grid points and rigorous theoretical validation.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient and accurate numerical method for solving the Euler-Bernoulli beam equation with damping, addressing challenges in spatial and temporal discretization.

Method: Uses compact finite difference schemes for spatial discretization, reduces equation order via variable substitution, and applies the Crank-Nicolson scheme for temporal discretization.

Result: The method is proven consistent, stable, and convergent, with numerical experiments confirming its accuracy and efficiency.

Conclusion: The proposed method is effective for solving the Euler-Bernoulli beam equation with damping, offering high accuracy and computational efficiency.

Abstract: This paper proposes and analyzes a finite difference method based on compact
schemes for the Euler-Bernoulli beam equation with damping terms. The method
achieves fourth-order accuracy in space and second-order accuracy in time,
while requiring only three spatial grid points within a single compact stencil.
Spatial discretization is carried out using a compact finite difference scheme,
with a variable substitution technique employed to reduce the order of the
equation and effectively handle the damping terms. For the temporal
discretization, the Crank-Nicolson scheme is applied. The consistency,
stability, and convergence of the proposed method are rigorously proved.
Numerical experiments are presented to verify the theoretical results and
demonstrate the accuracy and efficiency of the method.

</details>


### [17] [On the convergence of iterative regularization method assisted by the graph Laplacian with early stopping](https://arxiv.org/abs/2506.23483)
*Harshit Bajpai,Gaurav Mittal,Ankik Kumar Giri*

Main category: math.NA

TL;DR: A data-assisted iterative regularization method (IRMGL+Ψ) combines classical techniques with a data-driven term via an iteratively updated graph Laplacian, showing robustness and stability, especially with the adjoint operator as the initial reconstructor.


<details>
  <summary>Details</summary>
Motivation: Addressing ill-posed inverse problems in Hilbert spaces by integrating data-driven regularization with classical iterative methods to improve solution accuracy and stability.

Method: The method starts with a preliminary solution, constructs an initial graph Laplacian, and iteratively refines both the solution and Laplacian. The discrepancy principle is justified as a stopping criterion.

Result: Stability and convergence are proven under standard assumptions. Numerical experiments show IRMGL+Adj outperforms other initial reconstructors (FBP, TV, Tik).

Conclusion: IRMGL+Ψ, particularly with the adjoint operator, offers a robust and stable solution for ill-posed inverse problems, validated by theoretical and experimental results.

Abstract: We present a data-assisted iterative regularization method for solving
ill-posed inverse problems in Hilbert space settings. The proposed approach,
termed \texttt{IRMGL+\(\Psi\)}, integrates classical iterative techniques with
a data-driven regularization term realized through an iteratively updated graph
Laplacian. Our method commences by computing a preliminary solution using any
suitable reconstruction method, which then serves as the basis for constructing
the initial graph Laplacian. The solution is subsequently refined through an
iterative process, where the graph Laplacian is simultaneously recalibrated at
each step to effectively capture the evolving structure of the solution. A key
innovation of this work lies in the formulation of this iterative scheme and
the rigorous justification of the classical discrepancy principle as a reliable
early stopping criterion specifically tailored to the proposed method. Under
standard assumptions, we establish stability and convergence results for the
scheme when the discrepancy principle is applied. Furthermore, we demonstrate
the robustness and effectiveness of our method through numerical experiments
utilizing four distinct initial reconstructors $\Psi$: the adjoint operator
(Adj), filtered back projection (FBP), total variation (TV) denoising, and
standard Tikhonov regularization (Tik). It is observed that \texttt{IRMGL+Adj}
demonstrates a distinct advantage over the other initializers, producing a
robust and stable approximate solution directly from a basic initial
reconstruction.

</details>


### [18] [Rectangular $C^1$-$Q_k$ Bell finite elements in two and three dimensions](https://arxiv.org/abs/2506.23702)
*Hongling Hu,Shangyou Zhang*

Main category: math.NA

TL;DR: The paper constructs a Bell-type $C^1$-$Q_k$ finite element on rectangular meshes in 2D and 3D, ensuring optimal convergence despite reduced space.


<details>
  <summary>Details</summary>
Motivation: To maintain optimal approximation order while reducing the space for $C^1$-$Q_k$ finite elements, addressing the need for efficient numerical methods.

Method: Constructs a Bell-type $C^1$-$Q_k$ element with normal derivatives as $Q_{k-1}$ polynomials for $k\ge 4$, and compares it with original elements.

Result: The new Bell-type element retains optimal convergence order despite space reduction, validated by numerical experiments.

Conclusion: The proposed $C^1$-$Q_k$ Bell finite element is efficient and maintains optimal performance, offering a practical alternative to original elements.

Abstract: Both the function and its normal derivative on the element boundary are $Q_k$
polynomials
  for the Bogner-Fox-Schmit $C^1$-$Q_k$ finite element functions.
Mathematically, to keep the optimal order of approximation, their spaces are
required to
  include $P_k$ and $P_{k-1}$ polynomials respectively. We construct a Bell
type $C^1$-$Q_k$ finite element on rectangular meshes in 2D and 3D,
  which has its normal derivative as a $Q_{k-1}$ polynomial on each face, for
$k\ge 4$. We show, with a big reduction of the space, the $C^1$-$Q_k$ Bell
  finite element retains the optimal order of convergence. Numerical
experiments are performed, comparing the new elements with the original
elements.

</details>


### [19] [Efficient Numerical Integration for Finite Element Trunk Spaces in 2D and 3D using Machine Learning: A new Optimisation Paradigm to Construct Application-Specific Quadrature Rules](https://arxiv.org/abs/2506.23741)
*Tomas Teijeiro,Pouria Behnoudfar,Jamie M. Taylor,David Pardo,Victor M. Calo*

Main category: math.NA

TL;DR: The paper proposes using reduced polynomial spaces (trunk spaces) to create more efficient quadrature rules for finite element methods, reducing computational overhead while maintaining exact integration.


<details>
  <summary>Details</summary>
Motivation: Standard tensor-product quadrature rules in finite element methods lead to unnecessarily large integration spaces, especially for high polynomial degrees or dimensions, causing computational inefficiency.

Method: The authors define trial and test spaces as 2D or 3D trunk spaces, form an integration space, and solve a non-convex optimization problem to construct exact quadrature rules with fewer points. A shallow neural network and random restarts are used to optimize point coordinates and weights.

Result: The method achieves machine-precision accuracy with up to 30% fewer points in 2D and 50% fewer in 3D compared to standard tensor-product quadrature.

Conclusion: Combining polynomial structure understanding with numerical optimization yields practical, efficient, and scalable quadrature rules for high-order finite element simulations.

Abstract: Finite element methods usually construct basis functions and quadrature rules
for multidimensional domains via tensor products of one-dimensional
counterparts. While straightforward, this approach results in integration
spaces larger than necessary, especially as the polynomial degree $p$ or the
spatial dimension increases, leading to considerable computational overhead.
This work starts from the hypothesis that reducing the dimensionality of the
polynomial space can lead to quadrature rules with fewer points and lower
computational cost, while preserving the exactness of numerical integration. We
use trunk spaces that exclude high-degree monomials that do not improve the
approximation quality of the discrete space. These reduced spaces retain
sufficient expressive power and allow us to construct smaller (more economical)
integration domains. Given a maximum degree $p$, we define trial and test
spaces $U$ and $V$ as 2D or 3D trunk spaces and form the integration space
$\mathcal{S} = U \otimes V$. We then construct exact quadrature rules by
solving a non-convex optimisation problem over the number of points $q$, their
coordinates, and weights. We use a shallow neural network with linear
activations to parametrise the rule, and a random restart strategy to mitigate
convergence to poor local minima. When necessary, we dynamically increase $q$
to achieve exact integration. Our construction reaches machine-precision
accuracy (errors below 1e-22) using significantly fewer points than standard
tensor-product Gaussian quadrature: up to 30\% reduction in 2D for $p \leq 10$,
and 50\% in 3D for $p \leq 6$. These results show that combining the
mathematical understanding of polynomial structure with numerical optimisation
can lead to a practical and extensible methodology for improving the
adaptiveness, efficiency, and scalability of quadrature rules for high-order
finite element simulations.

</details>


### [20] [Error analysis for a Finite Element Discretization of a radially symmetric harmonic map heat flow problem](https://arxiv.org/abs/2506.23748)
*Nam Anh Nguyen,Arnold Reusken*

Main category: math.NA

TL;DR: The paper analyzes the harmonic map heat flow problem using a radially symmetric approach, employing an $H^1$-conforming finite element method and semi-implicit Euler time stepping. It provides optimal error bounds and validates results numerically.


<details>
  <summary>Details</summary>
Motivation: To address the harmonic map heat flow problem in a radially symmetric case, ensuring accurate discretization and stability.

Method: Uses $H^1$-conforming finite elements in space and semi-implicit Euler time stepping, resulting in linear problems per time step.

Result: Achieves optimal order discretization error bounds, supported by discrete energy estimates and convexity properties.

Conclusion: The method is validated numerically, demonstrating its effectiveness for smooth solutions of the continuous problem.

Abstract: We consider the harmonic map heat flow problem for a radially symmetric case.
For discretization of this problem we apply a $H^1$-conforming finite element
method in space combined with a semi-implicit Euler time stepping. The
semi-implicit Euler method results in a linear problem in each time step. We
restrict to the regime of smooth solutions of the continuous problem and
present an error analysis of this discretization method. This results in
optimal order discretization error bounds. Key ingredients of the analysis are
a discrete energy estimate, that mimics the energy dissipation of the
continuous solution, and a convexity property that is essential for discrete
stability and for control of the linearization error. We also present numerical
results that validate the theoretical ones.

</details>


### [21] [Dimension and model reduction approaches for linear Bayesian inverse problems with rank-deficient prior covariances](https://arxiv.org/abs/2506.23892)
*Josie König,Elizabeth Qian,Melina A. Freitag*

Main category: math.NA

TL;DR: The paper proposes dimension and model reduction methods for linear Bayesian inverse problems with rank-deficient prior covariances, improving computational efficiency while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: High-dimensional Bayesian inverse problems are computationally expensive due to sampling and forward model evaluations. Low-dimensional structure in data can be exploited for efficiency.

Method: Dimension reduction restricts inference to low-dimensional subspaces informed by data. Model reduction replaces expensive high-dimensional forward models with cheaper reduced models.

Result: Theoretical guarantees and numerical experiments show the proposed methods are accurate and efficient.

Conclusion: The approaches effectively reduce computational costs in linear Bayesian inverse problems, particularly for inferring initial conditions in linear dynamical systems.

Abstract: Bayesian inverse problems use observed data to update a prior probability
distribution for an unknown state or parameter of a scientific system to a
posterior distribution conditioned on the data. In many applications, the
unknown parameter is high-dimensional, making computation of the posterior
expensive due to the need to sample in a high-dimensional space and the need to
evaluate an expensive high-dimensional forward model relating the unknown
parameter to the data. However, inverse problems often exhibit low-dimensional
structure due to the fact that the available data are only informative in a
low-dimensional subspace of the parameter space. Dimension reduction approaches
exploit this structure by restricting inference to the low-dimensional subspace
informed by the data, which can be sampled more efficiently. Further
computational cost reductions can be achieved by replacing expensive
high-dimensional forward models with cheaper lower-dimensional reduced models.
In this work, we propose new dimension and model reduction approaches for
linear Bayesian inverse problems with rank-deficient prior covariances, which
arise in many practical inference settings. The dimension reduction approach is
applicable to general linear Bayesian inverse problems whereas the model
reduction approaches are specific to the problem of inferring the initial
condition of a linear dynamical system. We provide theoretical approximation
guarantees as well as numerical experiments demonstrating the accuracy and
efficiency of the proposed approaches.

</details>


### [22] [Structure-preserving approximation of the non-isothermal Cahn-Hilliard system](https://arxiv.org/abs/2506.23933)
*Aaron Brunk,Maria Lukacova-Medvidova,Dennis Schumann*

Main category: math.NA

TL;DR: A structure-preserving approximation of the non-isothermal Cahn-Hilliard equation is proposed, using conforming finite elements and a mixed explicit-implicit temporal approach, ensuring mass and energy conservation and entropy production.


<details>
  <summary>Details</summary>
Motivation: To develop a numerical method that preserves structural properties (mass, energy conservation, entropy production) in the non-isothermal Cahn-Hilliard equation.

Method: Uses conforming finite elements for spatial discretization and a mixed explicit-implicit approach for temporal discretization, with a variational formulation based on the entropy equation.

Result: Analytical findings confirm the method's structure-preserving properties, supported by numerical tests and convergence analysis.

Conclusion: The proposed method effectively preserves structural properties and demonstrates robustness through numerical validation.

Abstract: We propose and analyze a structure-preserving approximation of the
non-isothermal Cahn-Hilliard equation using conforming finite elements for the
spatial discretization and a problem-specific mixed explicit-implicit approach
for the temporal discretization. To ensure the preservation of structural
properties, i.e. conservation of mass and internal energy as well as entropy
production, we introduce a suitable variational formulation for the continuous
problem, based on the entropy equation. Analytical findings are supported by
numerical tests, including convergence analysis.

</details>


### [23] [Explicit modified Euler approximations of the Aït-Sahalia type model with Poisson jumps](https://arxiv.org/abs/2506.23947)
*Yingsong Jiang,Ruishu Liu,Minhong Xu*

Main category: math.NA

TL;DR: The paper proposes a novel explicit Euler-type scheme for the Aït-Sahalia interest rate model with Poisson jumps, addressing challenges like drift blow-up and positivity preservation. It achieves a mean-square convergence rate of 1/2.


<details>
  <summary>Details</summary>
Motivation: The study aims to overcome difficulties in numerical schemes for the Aït-Sahalia model, including nonlinear drift/diffusion, positivity preservation, and Poisson jumps.

Method: The authors introduce implicitness in the drift term and modification functions in the recursion to create an explicit Euler-type scheme.

Result: The proposed scheme preserves positivity unconditionally and achieves a mean-square convergence rate of 1/2, confirmed by numerical experiments.

Conclusion: The new scheme is effective for the Aït-Sahalia model, handling its complexities while maintaining simplicity and theoretical guarantees.

Abstract: This paper focuses on mean-square approximations of a generalized
A\"it-Sahalia interest rate model with Poisson jumps. The main challenge in the
construction and analysis of time-discrete numerical schemes is caused by a
drift that blows up at the origin, highly nonlinear drift and diffusion
coefficients and positivity-preserving requirement. Due to the presence of the
Poisson jumps, additional difficulties arise in recovering the exact order
$1/2$ of convergence for the time-stepping schemes. By incorporating
implicitness in the term $\alpha_{-1}x^{-1} $ and introducing the modifications
functions $f_h$ and $g_h$ in the recursion, a novel explicit Euler-type scheme
is proposed, which is easy to implement and preserves the positivity of the
original model unconditionally, i.e., for any time step-size $h>0$. A
mean-square convergence rate of order $1/2$ is established for the proposed
scheme in both the non-critical and general critical cases. Finally, numerical
experiments are provided to confirm the theoretical findings.

</details>


### [24] [Full history recursive multilevel Picard approximations suffer from the curse of dimensionality for the Hamilton-Jacobi-Bellman equation of a stochastic control problem](https://arxiv.org/abs/2506.23969)
*Martin Hutzenthaler,Tuan Anh Nguyen*

Main category: math.NA

TL;DR: MLP approximations overcome dimensionality curse for semilinear heat equations but fail for a specific Hamilton-Jacobi-Bellman equation due to its norm properties.


<details>
  <summary>Details</summary>
Motivation: To investigate the applicability of MLP approximations for equations with nonlinearities in stochastic control theory, which differ in norm properties.

Method: Analyze MLP approximations for a Hamilton-Jacobi-Bellman equation with locally Lipschitz nonlinearities under the Euclidean norm.

Result: MLP approximations suffer from the curse of dimensionality for the studied equation.

Conclusion: The norm properties of nonlinearities critically impact the effectiveness of MLP approximations, limiting their applicability in certain cases.

Abstract: Full history recursive multilevel Picard (MLP) approximations have been
proved to overcome the curse of dimensionality in the numerical approximation
of semilinear heat equations with nonlinearities which are globally Lipschitz
continuous with respect to the maximum-norm. Nonlinearities in
Hamilton-Jacobi-Bellman equations in stochastic control theory, however, are
often (locally) Lipschitz continuous with respect to the standard Euclidean
norm. In this paper we prove the surprising fact that MLP approximations for
one such example equation suffer from the curse of dimensionality.

</details>


### [25] [Sparse grids vs. random points for high-dimensional polynomial approximation](https://arxiv.org/abs/2506.24054)
*Jakob Eggl,Elias Mindlberger,Mario Ullrich*

Main category: math.NA

TL;DR: Comparison of sparse grid interpolation (Smolyak's algorithm) and least squares with random points for polynomial approximation in high dimensions. LS matches SA in low dimensions and outperforms in high dimensions.


<details>
  <summary>Details</summary>
Motivation: Extending the analysis from Barthelmann et al. (2000) on high-dimensional polynomial interpolation, comparing SA and LS methods.

Method: Theoretical analysis and numerical experiments comparing SA and LS on benchmark functions in dimensions up to 100.

Result: LS matches SA accuracy in low dimensions with slight oversampling and outperforms SA in high dimensions.

Conclusion: LS is superior to SA for high-dimensional polynomial approximation, especially with oversampling.

Abstract: We study polynomial approximation on a $d$-cube, where $d$ is large, and
compare interpolation on sparse grids, aka Smolyak's algorithm (SA), with a
simple least squares method based on randomly generated points (LS) using
standard benchmark functions. Our main motivation is the influential paper
[Barthelmann, Novak, Ritter: High dimensional polynomial interpolation on
sparse grids, Adv. Comput. Math. 12, 2000]. We repeat and extend their
theoretical analysis and numerical experiments for SA and compare to LS in
dimensions up to 100. Our extensive experiments demonstrate that LS, even with
only slight oversampling, consistently matches the accuracy of SA in low
dimensions. In high dimensions, however, LS shows clear superiority.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [26] [Pullback dynamics for a semilinear heat equation with homogeneous Neumann boundary conditions on time-varying domains](https://arxiv.org/abs/2506.22585)
*Gleiciane S. Aragão,Flank D. M. Bezerra,Lucas G. Mendonça*

Main category: math.AP

TL;DR: The paper studies a non-autonomous semilinear heat equation on time-varying domains, using differential geometry to transform it into an equivalent problem on a fixed domain. It proves local and global solution existence, uniqueness, and introduces new results on pullback attractors.


<details>
  <summary>Details</summary>
Motivation: To address the lack of results on attractors for non-autonomous semilinear heat equations on time-varying domains with homogeneous Neumann boundary conditions.

Method: Differential geometry and coordinate transformations to equate the problem on time-varying domains to fixed domains, followed by analysis of solution existence, uniqueness, and attractors.

Result: Local and global existence and uniqueness of solutions, plus new findings on pullback attractors for such equations.

Conclusion: The study successfully extends understanding of non-autonomous semilinear heat equations on time-varying domains, particularly in terms of attractors.

Abstract: We are interested in studying a non-autonomous semilinear heat equation with
homogeneous Neumann boundary conditions on time-varying domains. Using a
differential geometry approach with coordinate transformations technique, we
will show that the non-autonomous problem on a time-varying domain is
equivalent, in some sense, to a non-autonomous problem on a fixed domain.
Furthermore, we intend to show the local existence and uniqueness of solutions
to this problem, as well as, to extend these solutions globally. Finally, we
will show the existence of pullback attractors. To the best of our knowledge,
results on attractors are new even for non-autonomous semilinear heat equations
with homogeneous Neumann boundary conditions on time-varying domains subject to
conditions with more restrictive assumptions

</details>


### [27] [Remarks on graph-like forward self-similar solutions to the surface diffusion flow equations](https://arxiv.org/abs/2506.22731)
*Yoshikazu Giga,Sho Katayama*

Main category: math.AP

TL;DR: The paper examines the existence and non-existence of graph-like forward self-similar solutions in planar surface diffusion equations.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior and properties of graph-like solutions in surface diffusion equations, which is crucial for modeling physical processes.

Method: The study likely involves mathematical analysis and proofs to determine conditions for existence and non-existence of such solutions.

Result: The paper clarifies under what conditions graph-like forward self-similar solutions exist or do not exist.

Conclusion: The findings provide insights into the nature of solutions in planar surface diffusion equations, contributing to theoretical understanding.

Abstract: We clarify existence and non-existence of graph-like forward self-similar
solutions to the planar surface diffusion equations.

</details>


### [28] [$L^p$-Logvinenko-Sereda sets and $L^p$-Carleson measures on compact manifolds](https://arxiv.org/abs/2506.22759)
*Xing Wang,Xiangjin Xu,Cheng Zhang*

Main category: math.AP

TL;DR: The paper extends geometric characterizations of $L^p$-Logvinenko-Sereda sets and $L^p$-Carleson measures to compact manifolds with or without boundary for $1<p<\infty$, and investigates these for eigenfunctions, providing partial solutions and conjectures for the standard sphere.


<details>
  <summary>Details</summary>
Motivation: To generalize and extend previous work on $L^p$-Logvinenko-Sereda sets and $L^p$-Carleson measures to broader settings, including manifolds with boundary, and to address an open problem raised by Ortega-Cerdà and Pridhnani.

Method: The authors characterize $L^p$-Logvinenko-Sereda sets and $L^p$-Carleson measures on compact manifolds, with or without boundary, and investigate these for eigenfunctions, particularly on the standard sphere.

Result: Complete characterization for $p > \frac{2m}{m-1}$ on the standard sphere, and partial results with conjectures for $p < \frac{2m}{m-1}$.

Conclusion: The work advances understanding of $L^p$-Logvinenko-Sereda sets and Carleson measures, offering new insights and partial solutions to an open problem.

Abstract: Marzo and Ortega-Cerd\`a gave geometric characterizations for
$L^p$-Logvinenko-Sereda sets on the standard sphere for all $1\le p<\infty$.
Later, Ortega-Cerd\`a and Pridhnani further investigated
$L^2$-Logvinenko-Sereda sets and $L^2$-Carleson measures on compact manifolds
without boundary. In this paper, we characterize $L^p$-Logvinenko-Sereda sets
and $L^p$-Carleson measures on compact manifolds with or without boundary for
all $1<p<\infty$. Furthermore, we investigate $L^p$-Logvinenko-Sereda sets and
$L^p$-Carleson measures for eigenfunctions on compact manifolds without
boundary, and we completely characterize them on the standard sphere $S^m$ for
$p > \frac{2m}{m-1}$. For the range $p < \frac{2m}{m-1}$, we conjecture that
$L^p$-Logvinenko-Sereda sets for eigenfunctions on the standard sphere $S^m$
are characterized by the tubular geometric control condition and we provide
some evidence. These results provide new progress on an open problem raised by
Ortega-Cerd\`a and Pridhnani.

</details>


### [29] [Subelliptic Random Walks on Riemannian Manifolds and Their Convergence to Equilibrium](https://arxiv.org/abs/2506.22869)
*Davide Tramontana*

Main category: math.AP

TL;DR: Study of convergence to equilibrium for an $(h,ho)$-subelliptic random walk on a Riemannian manifold using Fefferman-Phong technique and spectral theory.


<details>
  <summary>Details</summary>
Motivation: To understand the equilibrium behavior of subelliptic random walks on closed, connected Riemannian manifolds.

Method: Uses Fefferman-Phong technique to reduce the problem to a constant-coefficient operator, then leverages compactness and spectral theory.

Result: Convergence to equilibrium is proven for the constructed random walk.

Conclusion: The method successfully demonstrates convergence, linking local and global diffusion properties.

Abstract: The aim of this work is to study the convergence to equilibrium of an
$(h,\rho)$-subelliptic random walk on a closed, connected Riemannian manifold
$(M,g)$ associated with a subelliptic second-order differential operator $A$ on
$M$. In such a random walk, $h$ roughly represents the step size and $\rho$ the
speed at which it is carried out. To construct the random walk and prove the
convergence result, we employ a technique due to Fefferman and Phong, which
reduces the problem to the study of a constant-coefficient operator $\tilde{A}$
that is locally equivalent to our second-order subelliptic operator $A$, in the
sense that the diffusion generated by $\tilde{A}$ induces a local diffusion for
$A$. By using the compactness of $M$ this local diffusion can be lifted to a
global diffusion, and the convergence result is then obtained via the spectral
theory of the associated Markov operator.

</details>


### [30] [Local well-posedness of the equations governing the motion of a fluid-filled elastic solid](https://arxiv.org/abs/2506.22874)
*Giusy Mazzone*

Main category: math.AP

TL;DR: The paper proves the existence of a unique strong solution for a fluid-structure interaction problem involving a viscous incompressible fluid and an elastic solid.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of modeling and solving the interaction between a fluid and an elastic solid, where the solid's motion is not predefined.

Method: The Navier equations of linear elasticity describe the solid's motion, while the Navier-Stokes equations describe the fluid's motion. The study focuses on non-zero initial conditions for the solid and fluid.

Result: A unique strong solution is proven for the governing equations under specified initial conditions, including fluid velocity in $H^{5/2}$.

Conclusion: The paper successfully establishes the existence and uniqueness of solutions for the given fluid-structure interaction problem.

Abstract: We consider the fluid-structure interaction problem of a viscous
incompressible fluid contained in an elastic solid whose motion is not
prescribed. The equations governing the motion of the solid are given by the
Navier equations of linear elasticity, whereas the fluid motion is described by
the Navier-Stokes equations. We prove that the governing equations admit a
unique strong solution corresponding to non-zero initial data for the solid
initial displacement and velocity, and for a fluid initial velocity in
$H^{5/2}$.

</details>


### [31] [$W^{1,p}$ priori estimates for solutions of linear elliptic PDEs on subanalytic domains](https://arxiv.org/abs/2506.22913)
*Guillaume Valette*

Main category: math.AP

TL;DR: The paper establishes a priori estimates for solutions of second-order linear elliptic PDEs on subanalytic domains with singular boundaries, ensuring $W^{1,p}$ bounds for $p>2$.


<details>
  <summary>Details</summary>
Motivation: To understand how the geometry of singularities in the boundary of subanalytic domains affects the regularity of solutions to elliptic PDEs.

Method: Analyzes solutions of the equation $Lu=f$ on bounded subanalytic domains, focusing on conditions related to the tangent cones of boundary singularities.

Result: Proves $||u||_{W^{1,p}} \le C||f||_{L^2}$ for some $p>2$, where $p$ depends on the boundary's singularity geometry.

Conclusion: The geometry of boundary singularities determines the regularity of solutions, independent of the solution itself.

Abstract: We prove a priori estimates for solutions of order $2$ linear elliptic PDEs
in divergence form on subanalytic domains. More precisely, we study the
solutions of a strongly elliptic equation $Lu=f$, with $f\in
L^2(\mathcal{\Omega})$ and $Lu=div (A(x) \nabla u)$, and, given a bounded
subanalytic domain $\mathcal{\Omega}$, possibly admitting non metrically
conical singularities within its boundary, we provide explicit conditions on
the tangent cone of the singularities of the boundary which ensure that
$||u||_{ W^{1,p}(\mathcal{\Omega})}\le C||f||_{L^2(\mathcal{\Omega})}$, for
some $p>2$. The number $p$ depends on the geometry of the singularities of
$\delta \mathcal{\Omega}$, but not on $u$.

</details>


### [32] [Monotone Multispecies Flows](https://arxiv.org/abs/2506.22947)
*Lauren Conger,Franca Hoffmann,Eric Mazumdar,Lillian J. Ratliff*

Main category: math.AP

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We present a novel notion of $\lambda$-monotonicity for an $n$-species system
of partial differential equations governed by mass-preserving flow dynamics,
extending monotonicity in Banach spaces to the Wasserstein-2 metric space. We
show that monotonicity implies the existence of and convergence to a unique
steady state, convergence of the velocity fields and second moments, and
contraction in the Wasserstein-2 metric, at rates dependent on $\lambda$. In
the special setting of Wasserstein-2 gradient descent of different energies for
each species, we prove convergence to the unique Nash equilibrium of the
associated energies and delineate the relationship between monotonicity and
displacement convexity. This extends known zero-sum results in
infinite-dimensional game theory to the general-sum setting. We provide a
number of examples of monotone coupled gradient flow systems, including
cross-diffusion, gradient flows with potentials, nonlocal interaction, linear
and nonlinear diffusion, and min-max systems, and draw connections to a class
of mean-field games. Numerically, we demonstrate convergence of a four-player
economic model for service providers and strategic users competing in a market,
and a degenerately monotone game.

</details>


### [33] [Existence and Nonexistence of Extremals for Trudinger-Moser inequalities with $L^p$ type perturbation on any bounded planar domains](https://arxiv.org/abs/2506.23076)
*Lu Chen,Rou Jiang,Guozhen Lu,Maochun Zhu*

Main category: math.AP

TL;DR: The paper investigates perturbed Trudinger-Moser inequalities, identifying a threshold λ*(p) for attainability of S_Ω(λ,p) and showing full attainability for p>2.


<details>
  <summary>Details</summary>
Motivation: To understand how L^p-type perturbations affect extremals for critical Trudinger-Moser inequalities in 2D bounded domains, extending the Brezis-Nirenberg problem.

Method: Refined blow-up analysis and establishing a sharp Dirichlet energy expansion formula, along with a comparison principle for radial vs. non-radial solutions.

Result: For p∈[1,2], S_Ω(λ,p) is attainable if λ<λ*(p) but not if λ>λ*(p). For p>2, it's always attainable.

Conclusion: The study provides a complete characterization of extremals for perturbed Trudinger-Moser inequalities in 2D, addressing asymmetric challenges.

Abstract: In this study, we investigate the perturbed Trudinger-Moser inequalities as
follows:\[ S_\Omega(\lambda,p)=\sup_{u\in H_{0}^{1}(\Omega),\Vert\nabla u\Vert
_{L^{2}\left( \Omega\right) }\leq 1}\int_{\Omega}\left( e^{4\pi
u^{2}}-\lambda|u|^{p}\right) dx, \] where $1\leq p<\infty$ and $\Omega$ is a
bounded domain in $\mathbb{R}^2$. Our results demonstrate that there exists a
threshold $\lambda^{\ast}(p)>0$ such that $S_\Omega(\lambda,p)$ is attainable
if $\lambda<\lambda^{\ast}(p)$, but unattainable if $\lambda>\lambda^{\ast}(p)$
when $p\in[1,2]$. For $p>2$, however, we show that $S_\Omega(\lambda,p)$ is
always attainable for any $\lambda\in \mathbb{R}$. These results are achieved
through a refined blow-up analysis, which allow us to establish a sharp
Dirichlet energy expansion formula for sequences of solutions to the
corresponding Euler-Lagrange equations. The asymmetric nature of our problem
poses significant challenges to our analysis. To address these, we will
establish an appropriate comparison principle between radial and non-radial
solutions of the associated Euler-Lagrange equations. Our study establishes a
complete characterization of how $L^p$-type perturbations influence the
existence of extremals for critical Trudinger-Moser inequalities on any bounded
planar domains, this extends the classical Brezis-Nirenberg problem framework
to the two-dimensional settings.

</details>


### [34] [The Grothendieck duality and sparse minimizing in spaces of Sobolev solutions to elliptic systems](https://arxiv.org/abs/2506.23087)
*Alexander Shlapunov,Alexander Polkovnikov,Kseniya Gagelgans*

Main category: math.AP

TL;DR: The paper explores using Banach spaces and elliptic operators to solve variational inverse problems in neural networks and boundary value problems, proving sparse minimizers and a representer theorem with a fundamental solution as a kernel.


<details>
  <summary>Details</summary>
Motivation: To address variational inverse problems in neural networks and regularization of boundary value problems using Banach spaces and elliptic operators.

Method: Utilizes Banach spaces of solutions to elliptic operator $A$, kernel methods, and Grothendieck-type duality in Sobolev spaces to prove sparse minimizers and a representer theorem.

Result: Existence of sparse minimizers and a representer theorem using a fundamental solution of $A$ as a reproducing kernel.

Conclusion: The approach effectively addresses variational problems, with potential applications in infinite data scenarios and standard elliptic operators.

Abstract: We present an instructive example of using Banach spaces of solutions to
(linear, generally, non-scalar) elliptic operator $A$ to investigate
variational inverse problems related to neural networks and/or to
regularization of solutions to boundary value problems. More precisely,
inspired by kernel's method for optimization problems in locally convex spaces,
we prove the existence of the so-called sparse minimizers for the related
variational problem and produce a representer theorem where a suitable
fundamental solution of the operator $A$ is used as a reproducing kernel. The
Grothendieck type duality for the Sobolev spaces of solutions to elliptic
operator $A$ plays an essential role in the considerations. The case where the
number of data passes to infinity is also discussed. Some typical situations
related to the standard elliptic operators, the corresponding function spaces
and fundamental solutions are considered.

</details>


### [35] [Decay estimates for discrete bi-Laplace operators with potentials on the lattice $\mathbb{Z}$](https://arxiv.org/abs/2506.23119)
*Sisi Huang,Xiaohua Yao*

Main category: math.AP

TL;DR: The paper shows the discrete bi-Laplacian on ℤ matches the continuous decay rate of |t|^(-1/4). It also analyzes discrete bi-Schrödinger operators, establishes limiting absorption principles, and derives sharp decay estimates for different resonance types.


<details>
  <summary>Details</summary>
Motivation: To understand the decay properties of discrete bi-Laplacian and bi-Schrödinger operators, comparing them to their continuous counterparts and exploring resonance effects.

Method: The study involves establishing limiting absorption principles, deriving resolvent expansions near thresholds, and characterizing resonance types in weighted ℓ² spaces.

Result: Sharp decay estimates are proven for discrete bi-Laplacian and bi-Schrödinger operators, matching continuous decay rates. The discrete beam equation also exhibits specific decay behavior.

Conclusion: The discrete bi-Laplacian and bi-Schrödinger operators exhibit decay rates analogous to their continuous versions, with detailed resonance characterizations and decay estimates provided.

Abstract: It is known that the discrete Laplace operator $\Delta$ on the lattice
$\mathbb{Z}$ satisfies the following sharp time decay estimate:
$$\big\|e^{it\Delta}\big\|_{\ell^1\rightarrow\ell^{\infty}}\lesssim|t|^{-\frac{1}{3}},\quad
t\neq0,$$ which is slower than the usual $ O(|t|^{-\frac{1}{2}})$ decay in the
continuous case on $\mathbb{R}$. However, this paper shows that the discrete
bi-Laplacian $\Delta^2$ on $\mathbb{Z}$ actually exhibits the same sharp decay
estimate $|t|^{-\frac{1}{4}}$ as its continuous counterpart.
  In view of the free decay estimate, we further investigate the discrete
bi-Schr\"{o}dinger operators of the form $H=\Delta^2+V$ on the lattice space
$\ell^2(\mathbb{Z})$, where $V$ is a class of real-valued decaying potentials
on $\mathbb{Z}$. First, we establish the limiting absorption principle for $H$,
and then derive the full asymptotic expansions of the resolvent of $H$ near the
thresholds $0$ and $16$, including resonance cases. In particular, we provide a
complete characterizations of the different resonance types in
$\ell^2$-weighted spaces.
  Based on these results above, we establish the following sharp
$\ell^1-\ell^{\infty}$ decay estimates for all different resonances types of
$H$ under suitable decay conditions on $V$:
$$\big\|e^{-itH}P_{ac}(H)\big\|_{\ell^1\rightarrow\ell^{\infty}}\lesssim|t|^{-\frac{1}{4}},\quad
t\neq0,$$ where $P_{ac}(H)$ denotes the spectral projection onto the absolutely
continuous spectrum space of $H$. Additionally, the decay estimates for the
evolution flow of discrete beam equation are also derived: $$\|{\cos}(t\sqrt
H)P_{ac}(H)\|_{\ell^1\rightarrow\ell^{\infty}}+\Big\|\frac{{\sin}(t\sqrt
H)}{t\sqrt
H}P_{ac}(H)\Big\|_{\ell^1\rightarrow\ell^{\infty}}\lesssim|t|^{-\frac{1}{3}},\quad
t\neq0.$$

</details>


### [36] [Ionic KdV structure in weakly collisional plasmas](https://arxiv.org/abs/2506.23159)
*Renjun Duan,Zongguang Li,Dongcheng Yang,Tong Yang*

Main category: math.AP

TL;DR: The paper justifies the uniform convergence of Vlasov-Poisson-Landau (VPL) solutions to Korteweg-de Vries (KdV) solutions in weakly collisional plasmas under specific scaling conditions, using energy methods and velocity-weighted functionals.


<details>
  <summary>Details</summary>
Motivation: To capture ion acoustic solitons in plasmas and understand their dynamics under weak-collision and longwave limits.

Method: Combines cold-ions and longwave limits of the VPL system, using energy methods near local Maxwellians and velocity-weighted functionals to handle multi-parameter limits.

Result: Uniform convergence of VPL solutions to KdV solutions is proven, and global-in-time existence of solutions near equilibrium is established.

Conclusion: The approach successfully handles the multi-parameter limit problem and provides insights into soliton dynamics in plasmas.

Abstract: We consider the one-dimensional ions dynamics in weakly collisional plasmas
governed by the Vlasov-Poisson-Landau system under the Boltzmann relation with
the small collision frequency $\nu>0$. It is observed in physical experiments
that the interplay of nonlinearities and dispersion may lead to the formation
of ion acoustic solitons that are described by the Korteweg-de Vries equation.
In this paper, to capture the ionic KdV structure in the weak-collision regime,
we study the combined cold-ions limit and longwave limit of the rescaled VPL
system depending on a small scaling parameter $\epsilon>0$. The main goal is to
justify the uniform convergence of the VPL solutions to the KdV solutions over
any finite time interval as $\epsilon\to 0$ under restriction that
$\epsilon^{3/2}\lesssim \nu \lesssim \epsilon^{1/2}$. The proof is based on the
energy method near local Maxwellians for making use of the Euler-Poisson
dynamics under the longwave scaling. The KdV profiles, in particular including
both velocity field and electric potential, may have large amplitude, which
induces the cubic velocity growth. To overcome the $\epsilon$-singularity in
such multi-parameter limit problem, we design delicate velocity weighted energy
functional and dissipation rate functional in the framework of macro-micro
decomposition that is further incorporated with the Caflisch's decomposition.
As an application of our approach, the global-in-time existence of solutions
near global Maxwellians when the KdV profile is degenerate to a constant
equilibrium is also established under the same scaling with
$\epsilon^{3}\lesssim \nu \lesssim \epsilon^{5/2}$. For the proof, the velocity
weight is modified to depend on the solution itself, providing an extra quartic
dissipation so as to obtain the global dynamics for most singular Coulomb
potentials.

</details>


### [37] [Stability transitions of NLS action ground-states on metric graphs](https://arxiv.org/abs/2506.23166)
*Francisco Agostinho,Simão Correia,Hugo Tavares*

Main category: math.AP

TL;DR: The paper investigates orbital stability of action ground-states in the nonlinear Schrödinger equation on specific metric graphs, revealing stability transitions near the $L^2$-critical exponent.


<details>
  <summary>Details</summary>
Motivation: To understand the dynamical behavior and stability transitions of ground-states in nonlinear Schrödinger equations on metric graphs.

Method: Theoretical analysis of stability transitions near the $L^2$-critical exponent, complemented by asymptotic case studies and numerical simulations.

Result: Ground-states transition from stable to unstable and back to stable (or vice-versa) as frequency increases, with numerical validation.

Conclusion: The study reveals new dynamical features of the nonlinear Schrödinger equation, supported by theoretical and numerical evidence.

Abstract: We study the orbital stability of action ground-states of the nonlinear
Schr\"odinger equation over two particular cases of metric graphs, the
$\mathcal{T}$ and the tadpole graphs. We show the existence of stability
transitions near the $L^2$-critical exponent, a new dynamical feature of the
nonlinear Schr\"odinger equation. More precisely, as the frequency $\lambda$
increases, the action ground-state transitions from stable to unstable and then
back to stable (or vice-versa).
  This result is complemented with the stability analysis of ground-states in
the asymptotic cases of low/high frequency and weak/strong nonlinear
interaction. Finally, we present a numerical simulation of the stability of
action ground-states depending on the nonlinearity and the frequency parameter,
which validates the aforementioned theoretical results.

</details>


### [38] [Semiregular and strongly irregular boundary points for nonlocal Dirichlet problems](https://arxiv.org/abs/2506.23188)
*Anders Björn,Jana Björn,Minhyun Kim*

Main category: math.AP

TL;DR: The paper studies nonlocal nonlinear equations of fractional $(s,p)$-Laplacian type on $\mathbf{R}^n$, classifying irregular boundary points into semiregular and strongly irregular points, and characterizing their behavior.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of irregular boundary points in the Dirichlet problem for fractional $(s,p)$-Laplacian equations, distinguishing between semiregular and strongly irregular points.

Method: Uses the Kellogg property and a new removability result for solutions in the $V^{s,p}$ Sobolev space, extending to supersolutions with a right-hand side.

Result: Boundary points are classified into semiregular and strongly irregular, with distinct behaviors. Characterizations for both types are provided, and the dependence of semiregularity on $s$ and $p$ is explained.

Conclusion: The study provides a clear classification and characterization of irregular boundary points, enhancing understanding of their behavior in fractional $(s,p)$-Laplacian equations.

Abstract: In this paper we study nonlocal nonlinear equations of fractional
$(s,p)$-Laplacian type on $\mathbf{R}^n$. We show that the irregular boundary
points for the Dirichlet problem can be divided into two disjoint classes:
semiregular and strongly irregular boundary points, with very different
behaviour. Two fundamental tools needed to show this are the Kellogg property
(from our previous paper) and a new removability result for solutions in the
$V^{s,p}$ Sobolev type space, which we deduce more generally also for
supersolutions of equations with a right-hand side. Semiregular and strongly
irregular points are also characterized in various ways. Finally, it is
explained how semiregularity depends on $s$ and $p$.

</details>


### [39] [Global Calderón-Zygmund estimates for asymptotically convex fully nonlinear Grad-Mercier type equations](https://arxiv.org/abs/2506.23216)
*Yao Zhang,Xiaofeng Jin,Lingwei Ma,Zhenqiu Zhang*

Main category: math.AP

TL;DR: The paper addresses a Dirichlet problem for a fully nonlinear elliptic equation under asymptotic convexity conditions, proving existence and estimates for solutions using compactness and frozen techniques.


<details>
  <summary>Details</summary>
Motivation: The study aims to tackle the challenges posed by non-convexity in the operator and nonlocality in the nonhomogeneous term, seeking to establish rigorous mathematical solutions and estimates.

Method: The authors employ compactness methods and frozen techniques to prove the existence of $W^{2,p}$-viscosity solutions and derive global $W^{2,p}$ estimates.

Result: Key results include a Cordes-Nirenberg type continuous estimate up to the boundary and a global BMO estimate for second derivatives, refining Calderón-Zygmund estimates.

Conclusion: The paper successfully overcomes the non-convexity and nonlocality issues, providing robust solutions and refined estimates for the problem.

Abstract: In this paper, we consider the following Dirichlet problem for the fully
nonlinear elliptic equation of Grad-Mercier type under asymptotic convexity
conditions \begin{equation*}
  \left\{
  \begin{array}{ll}
  F(D^2u(x),Du(x),u(x),x)=g(|\{y\in \Omega:u(y)\ge u(x)\}|)+f(x) & \text{in }
\Omega,
  u=\psi &\text{on } \partial \Omega.
  \end{array}
  \right. \end{equation*} In order to overcome the non-convexity of the
operator $F$ and the nonlocality of the nonhomogeneous term $g$, we apply the
compactness methods and frozen technique to prove the existence of the
$W^{2,p}$-viscosity solutions and the global $W^{2,p}$ estimate. As an
application, we derive a Cordes-Nirenberg type continuous estimate up to
boundary. Furthermore, we establish a global BMO estimate for the second
derivatives of solutions by using an asymptotic approach, thereby refining the
borderline case of Calder\'{o}n-Zygmund estimates.

</details>


### [40] [Approximate Synchronization of Memristive Hopfield Neural Networks](https://arxiv.org/abs/2506.23279)
*Yuncheng You*

Main category: math.AP

TL;DR: The paper introduces approximate synchronization in memristive Hopfield neural networks, proving robust dissipative dynamics and exponential convergence under a threshold condition.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of asymptotic synchronization differences between artificial and biologically inspired neural networks due to dynamical weight updates and heterogeneous activations.

Method: Proposes and analyzes approximate synchronization in Hopfield neural networks with nonlinear memristors, using uniform estimates on interneuron differencing equations.

Result: Demonstrates robust dissipative dynamics, a sharp ultimate bound, and exponential convergence to approximate synchronization under a computable threshold condition.

Conclusion: The findings extend to memristive Hopfield networks with Hebbian learning, broadening applications in unsupervised train learning.

Abstract: Asymptotic synchronization is one of the essential differences between
artificial neural networks and biologically inspired neural networks due to
mismatches from dynamical update of weight parameters and heterogeneous
activations. In this paper a new concept of approximate synchronization is
proposed and investigated for Hopfield neural networks coupled with nonlinear
memristors. It is proved that global solution dynamics are robustly dissipative
and a sharp ultimate bound is acquired. Through \emph{a priori} uniform
estimates on the interneuron differencing equations, it is rigorously shown
that approximate synchronization to any prescribed small gap at an exponential
convergence rate of the memristive Hopfield neural networks occurs if an
explicitly computable threshold condition is satisfied by the interneuron
coupling strength coefficient. The main result is further extended to
memristive Hopfield neural networks with Hebbian learning rules for a broad
range of applications in unsupervised train learning.

</details>


### [41] [An analysis of the 2-D isentropic Euler Equations for a generalized polytropic gas law](https://arxiv.org/abs/2506.23327)
*Lauren M. M. Bonaldo,Talita Mello,Wladimir Neves*

Main category: math.AP

TL;DR: Analysis of compressible, isentropic Euler equations in 2D for a generalized polytropic gas law, focusing on subsonic rotational flows using self-similar variables and pseudo-velocities. A Bernoulli-type equation is derived, and quasi-potential flows are explored.


<details>
  <summary>Details</summary>
Motivation: To understand rotational flows in subsonic regimes for generalized polytropic gases using the Euler equations.

Method: Uses self-similar variables and pseudo-velocities to derive a Bernoulli-type equation and analyze rotational flows. Extends to perturbed models for quasi-potential flows.

Result: Established a self-similar system for rotational flows and insights into quasi-potential flows.

Conclusion: The study provides a framework for analyzing rotational flows and extends understanding to quasi-potential flows in perturbed models.

Abstract: In this paper we developed an analysis of the compressible, isentropic Euler
equations in two spatial dimensions for a generalized polytropic gas law. The
main focus is rotational flows in the subsonic regimes, described through the
framework of the Euler equations expressed in self-similar variables and
pseudo-velocities. A Bernoulli type equation is derived, serving as a
cornerstone for establishing a self-similar system tailored to rotational
flows. In the final section, the study extends to an analysis of a perturbed
model, introducing the concept of quasi-potential flows, offering insights into
their behavior and implications.

</details>


### [42] [Yudovich theory under geometric regularity for density-dependent incompressible fluids](https://arxiv.org/abs/2506.23365)
*Francesco Fanelli*

Main category: math.AP

TL;DR: The paper extends Yudovich theory to density-dependent incompressible Euler equations in 2D, proving stability and uniqueness for low-regularity solutions under a geometric control condition.


<details>
  <summary>Details</summary>
Motivation: To generalize Yudovich's results for homogeneous Euler equations to the non-homogeneous case, focusing on low-regularity initial data.

Method: Uses a priori control of the directional derivative of velocity along a specific vector field to construct and analyze solutions.

Result: Proves stability (convergence of approximate solutions) and uniqueness (under finiteness of the directional derivative) for Yudovich-type solutions.

Conclusion: The study successfully extends Yudovich theory to density-dependent Euler equations, improving uniqueness conditions for less smooth initial data.

Abstract: This paper focuses on the study of the density-dependent incompressible Euler
equations in space dimension $d=2$, for low regularity (\textsl{i.e.}
non-Lipschitz) initial data satisfying assumptions in spirit of the celebrated
Yudovich theory for the classical homogeneous Euler equations.
  We show that, under an \textsl{a priori} control of a non-linear geometric
quantity, namely the directional derivative $\partial_Xu$ of the fluid velocity
$u$ along the vector field $X:=\nabla^\perp\rho$, where $\rho$ is the fluid
density, low regularity solutions \textsl{\`a la Yudovich} can be constructed
also in the non-homogeneous setting. More precisely, we prove the following
facts:
  (i) \emph{stability}: given a sequence of smooth approximate solutions
enjoying a uniform control on the above mentioned geometric quantity, then (up
to an extraction) that sequence converges to a Yudovich-type solution of the
density-dependent incompressible Euler system; \\ (ii) \emph{uniqueness}: there
exists at most one Yudovich-type solution of the density-dependent
incompressible Euler equations such that $\partial_Xu$ remains finite; besides,
this statement improves previous uniqueness results for regular solutions,
inasmuch as it requires less smoothness on the initial data.

</details>


### [43] [Homogenization of an indefinite spectral problem arising in population genetics](https://arxiv.org/abs/2506.23378)
*Srinivasan Aiyappan,Aditi Chattaraj,Irina Pettersson*

Main category: math.AP

TL;DR: The paper analyzes the spectrum of a second-order self-adjoint elliptic operator in a thin cylinder with locally periodic coefficients and a sign-changing spectral density function. The asymptotic behavior of the spectrum is studied, focusing on the positive part when the average density is negative. A one-dimensional effective problem is derived, resembling a harmonic oscillator, and spectral convergence is proven.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by applications in population genetics, where spectral problems with sign-changing weight arise naturally.

Method: The authors investigate the asymptotic behavior of the spectrum as the cylinder's thickness approaches zero, deriving a one-dimensional effective spectral problem (a harmonic oscillator) and proving spectral convergence. A key auxiliary result involves the existence of a positive principal eigenvalue for an indefinite spectral problem with Neumann boundary conditions.

Result: The spectrum exhibits infinitely many positive and negative eigenvalues due to the sign-changing density function. The asymptotic behavior depends on the average density's sign, with the positive part of the spectrum converging to a one-dimensional harmonic oscillator when the average is negative.

Conclusion: The paper provides insights into the spectral behavior of operators in thin cylinders with sign-changing weights, linking it to a simplified one-dimensional problem. The findings have implications for population genetics and similar fields.

Abstract: We study an indefinite spectral problem for a second-order self-adjoint
elliptic operator in an asymptotically thin cylinder. The operator coefficients
and the spectral density function are assumed to be locally periodic in the
axial direction of the cylinder. The key assumption is that the spectral
density function changes sign, which leads to infinitely many both positive and
negative eigenvalues. The asymptotic behavior of the spectrum, as the thickness
of the rod tends to zero, depends essentially on the sign of the average of the
density function. We study the positive part of the spectrum in a specific case
when the local average is negative. We derive a one-dimensional effective
spectral problem that is a harmonic oscillator on the real line, and prove the
convergence of spectrum. A key auxiliary result is the existence of a positive
principal eigenvalue of an indefinite spectral problem with the Neumann
boundary condition on a periodicity cell. This study is motivated by
applications in population genetics where spectral problems with sign-changing
weight naturally appear.

</details>


### [44] [On Exponential Instability of an Inverse Problem for the Wave Equation](https://arxiv.org/abs/2506.23559)
*Leonard Busch,Matti Lassas,Lauri Oksanen,Mikko Salo*

Main category: math.AP

TL;DR: The paper examines the instability of recovering a potential $q$ from a source-to-solution map in a Euclidean space with an obstacle, showing exponential instability in certain cases.


<details>
  <summary>Details</summary>
Motivation: To understand the feasibility and stability of reconstructing the potential $q$ from limited measurements in the presence of an obstacle.

Method: Analyzes the source-to-solution operator for the wave equation with an obstacle, focusing on cases where measurements are restricted and $q$ lies in a 'shadow region.'

Result: Demonstrates that recovering $q$ is exponentially unstable when measurements are confined and $q$ is in the shadow region.

Conclusion: The study highlights the challenges in inverse problems involving obstacles, showing that stability deteriorates exponentially in certain scenarios.

Abstract: For a time-independent potential $q\in L^\infty$, consider the
source-to-solution operator that maps a source $f$ to the solution $u=u(t,x)$
of $(\Box+q)u=f$ in Euclidean space with an obstacle, where we impose on $u$
vanishing Cauchy data at $t=0$ and vanishing Dirichlet data at the boundary of
the obstacle. We study the inverse problem of recovering the potential $q$ from
this source-to-solution map restricted to some measurement domain. By giving an
example where measurements take place in some subset and the support of $q$
lies in the `shadow region' of the obstacle, we show that recovery of $q$ is
exponentially unstable.

</details>


### [45] [Three-dimensional structural stability of shock waves in elastodynamics](https://arxiv.org/abs/2506.23572)
*Artem Shafeev,Yuri Trakhinin*

Main category: math.AP

TL;DR: The paper analyzes the structural stability of shock waves in 3D elastodynamics, showing planar shocks are weakly stable and identifying conditions for uniform stability. Compressive shocks are uniformly stable for convex equations of state.


<details>
  <summary>Details</summary>
Motivation: To extend previous 2D analysis to 3D and understand the stability of shock waves in elastodynamics, focusing on the role of elastic forces.

Method: Uses equivalent formulations of Kreiss-Lopatinski conditions for 1-shocks and leverages the Agranovich-Majda-Osher block structure condition.

Result: Planar shock waves are weakly stable; uniform stability depends on specific conditions. Compressive shocks are uniformly stable for convex equations of state.

Conclusion: Elastic forces stabilize shock waves, confirming findings from 2D studies.

Abstract: We study the three-dimensional structural stability of shock waves for the
equations of elastodynamics governing isentropic flows of compressible inviscid
elastic materials. By nonlinear structural stability of a shock wave we mean
the local-in-time existence and uniqueness of the discontinuous shock front
solution to the hyperbolic system of elastodynamics. By using equivalent
formulations of the uniform and weak Kreiss-Lopatinski conditions for 1-shocks,
we show that planar shock waves in three-dimensional elastodynamics are always
at least weakly stable, and we find a condition necessary and sufficient for
their uniform stability. Since the system of elastodynamics satisfies the
Agranovich-Majda-Osher block structure condition, uniform stability implies
structural stability of corresponding nonplanar shock waves. We also show that,
as in isentropic gas dynamics, all compressive shock waves are uniformly stable
for convex equations of state. This paper is a natural continuation of the
previous two-dimensional analysis in [Morando A., Trakhinin Y., Trebeschi P.,
Math. Ann. 378 (2020), 1471-1504; Trakhinin Y., J. Hyperbolic Differ. Equ. 19
(2022), 157-173]. As in the two-dimensional case, we make the conclusion that
the elastic force plays stabilizing role for uniform stability.

</details>


### [46] [Constraint Maps: Insights and Related Themes](https://arxiv.org/abs/2506.23608)
*Alessio Figalli,André Guerra,Sunghan Kim,Henrik Shahgholian*

Main category: math.AP

TL;DR: The paper clarifies complex arguments about constraint maps, introduces new results like optimal geometric conditions for unique continuation, and proves gradient properties of minimizing maps.


<details>
  <summary>Details</summary>
Motivation: To provide a clear and accessible explanation of intricate arguments in constraint maps and contribute new findings.

Method: Builds on prior work, introduces optimal geometric conditions, and analyzes gradients of minimizing harmonic maps.

Result: Proves gradient of minimizing maps is an $A_\infty$-weight and ensures unique continuation. Also provides new geometric conditions.

Conclusion: Offers insights into constraint maps, presents new results, and suggests future research directions in harmonic maps and free boundary problems.

Abstract: This paper explores recent progress related to constraint maps. Building on
the exposition in [14], our goal is to provide a clear and accessible account
of some of the more intricate arguments behind the main results in this work.
Along the way, we include several new results of independent value. In
particular, we give optimal geometric conditions on the target manifold that
guarantee a unique continuation result for the projected image map. We also
prove that the gradient of a minimizing harmonic map (or, more generally, of a
minimizing constraint map) is an $A_\infty$-weight, and therefore satisfies a
strong form of the unique continuation principle. In addition, we outline
possible directions for future research and highlight several open problems
that may interest researchers working on free boundary problems and harmonic
maps.

</details>


### [47] [A doubly nonlinear elliptic problem with variable exponents, homogeneous Neumann conditions and generalized logistic source](https://arxiv.org/abs/2506.23660)
*Bogdan Maxim*

Main category: math.AP

TL;DR: Existence and uniqueness results for a doubly nonlinear elliptic problem, relaxing the locally Lipschitz condition on the source term, using variable exponent Lebesgue spaces.


<details>
  <summary>Details</summary>
Motivation: To solve the associated parabolic problem via Rothe's method under weaker assumptions than commonly used.

Method: Relies on the continuity of the Nemytskii operator between variable exponent Lebesgue spaces, dropping the locally Lipschitz condition.

Result: Proves existence and uniqueness for the doubly nonlinear elliptic problem.

Conclusion: The results are rigorously proved, though the paper is lengthy due to detailed proofs.

Abstract: The aim of this work is to prove existence and uniqueness results for a
doubly nonlinear elliptic problem that is essential for solving the associated
parabolic problem using Rothe's method (discretizing time). We work under very
weak assumptions, dropping the commonly used condition that the source term is
locally Lipschitz, which appears frequently in the literature. Instead, we rely
on the continuity of the Nemytskii operator between two Lebesgue spaces with
variable exponents. All results presented here are proved in full detail, which
makes the article lengthy.

</details>


### [48] [On Existence and Uniqueness of the Solution of a Two-Surfaces Contact Problem Using a Fixed Point Approach](https://arxiv.org/abs/2506.23763)
*Abdelkrim Atailia,Frekh Taallah*

Main category: math.AP

TL;DR: Proof of existence and uniqueness for a two-surfaces contact problem's weak solution using fixed point approach.


<details>
  <summary>Details</summary>
Motivation: To address the need for rigorous mathematical treatment of contact problems involving deformable surfaces with viscoplastic behavior.

Method: Modeled the contact problem with a general viscoplastic law, derived its variational formulation, and applied fixed point theory.

Result: Established an existence and uniqueness theorem for the weak form of the problem.

Conclusion: The work provides a theoretical foundation for solving such contact problems, confirming solution validity.

Abstract: In this work, we give the proof of the existence and uniqueness of the
solution to the weak form of a two-surfaces contact problem using fixed point
approach. We begin by modeling the evolution of a two deformable surfaces
contact problem with a general viscoplastic law, the contact is considered
frictionless and governed by the Signorini-type condition with an initial gap.
Then, we derive the variational formulation of the classical problem. Finally,
we conclude our work by establishing an existence and uniqueness theorem for
the weak form.

</details>


### [49] [Nonlinearity exponential stability for Lions-Feireisl's weak solutions to the three-dimensional Barotropic Compressible Navier-Stokes Equations with large Potential Force](https://arxiv.org/abs/2506.23807)
*Lingping Kang,Yanfang Peng,Chengfeng Xiong*

Main category: math.AP

TL;DR: The paper analyzes the large-time behavior of barotropic compressible Navier-Stokes equations with large external forces in 3D bounded domains, showing exponential decay to equilibrium for weak solutions.


<details>
  <summary>Details</summary>
Motivation: To extend previous results (like \cite{PSW}) to cases with large external forces, where the equilibrium density is non-constant.

Method: Constructs a Lyapunov functional and uses density integrability, with careful Taylor expansion analysis.

Result: Exponential decay to equilibrium is proven for finite-energy weak solutions.

Conclusion: The work extends existing results to large potential forces, addressing the challenge of non-constant equilibrium density.

Abstract: We consider the large-time behavior for the barotropic compressible
Navier-Stokes equations with large external force in 3D bounded domain. By
constructing a suitable Lyapunov functional and using the extra integrability
of the density, we state the exponentially decay-in-time to the equilibrium
state for Lions-Feireisl's finite-energy weak solutions. In addition, some
careful discussion on Taylor expansion also plays a crucial role in our
analysis. The main difficulty lies in the fact that the equilibrium state of
density is not a constant anymore induced by the large external force. It
should be noted that our result can be regarded as an extension of \cite{PSW}
to large potential forces case.

</details>


### [50] [A priori bounds and equicontinuity of orbits for the intermediate long wave equation](https://arxiv.org/abs/2506.23868)
*Benjamin Harrop-Griffiths,Rowan Killip,Monica Visan*

Main category: math.AP

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We prove uniform-in-time a priori $H^s$ bounds for solutions to the
intermediate long wave equation posed both on the line and on the circle,
covering the range $-\frac12<s\leq0$. Additionally, we prove that the set of
orbits emanating from a bounded and equicontinuous set in $H^s$ is also bounded
and equicontinuous in $H^s$. Our proof is based on the identification of a
suitable Lax pair formulation for the intermediate long wave equation.

</details>


### [51] [A variational view on constitutive laws in parabolic problems](https://arxiv.org/abs/2506.23910)
*Stefan Schiffer,Espen Xylander*

Main category: math.AP

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We consider a variational approach to solve parabolic problems by minimising
a functional over time and space. To achieve existence results we investigate
the notion of $\mathscr{A}$-quasiconvexity for non-homogeneous operators in
anisotropic spaces. The abstract theory is then applied to formulate a
variational solution concept for the non-Newtonian Navier--Stokes equations.

</details>


### [52] [On the convergence of the no-response test for the heat equation](https://arxiv.org/abs/2506.23948)
*Shiwei Sun,Gen Nakamura,Haibing Wang*

Main category: math.AP

TL;DR: The paper proves the convergence of the No-Response Test (NRT) for the heat equation, independent of its duality with the Range Test (RT), completing the theoretical study of both methods.


<details>
  <summary>Details</summary>
Motivation: Previous work established the duality between NRT and RT and proved RT's convergence but left NRT's convergence unaddressed. This paper fills that gap.

Method: The authors define an indicator function for a test domain and use the analytical extension property of heat equation solutions to prove convergence.

Result: The NRT converges if and only if the cavity is within the test domain, provided solutions cannot be analytically extended beyond the cavity's boundary.

Conclusion: The study completes the theoretical foundation for both NRT and RT, emphasizing the role of analytical extension properties in proving convergence.

Abstract: Domain sampling methods called the range test (RT) and no-response test
(NRT), and their duality are known for several inverse scattering problems and
an inverse boundary value problem for the Laplace operator (see Section 1 for
more details). In our previous work [21], we established the duality between
the NRT and RT, and demonstrated the convergence of the RT for the heat
equation. We also provided numerical studies for both methods. However, we did
not address the convergence for the NRT. As a continuation of this work, we
prove the convergence of the NRT without using the duality. Specifically,
assuming there exists a cavity $D$ inside a heat conductor $\Omega$, we define
an indicator function $I_{NRT}(G)$ for a prescribed test domain $G$, where
$\overline G\subset\Omega$ (i.e., $G\Subset\Omega$). By using the analytical
extension property of solutions to the heat equation with respect to the
spatial variables, we prove the convergence result given as $I_{NRT}(G)<\infty$
if and only if $\overline{D}\subset \overline{G}$, provided that the solution
to the heat equation cannot be analytically extended across the boundary of the
cavity. Thus, we complete the theoretical study of both methods. Here the
analytic extension of solutions does not require the property that the
solutions are real analytic with respect to the space variables. However, for
the proof of the mentioned convergence result, we fully use this property.

</details>


### [53] [Boundary Value Problems in graph Lipschitz domains in the plane with $A_{\infty}$-measures on the boundary](https://arxiv.org/abs/2506.23961)
*Fernando Ballesta-Yagüe,María J. Carro*

Main category: math.AP

TL;DR: The paper analyzes solvability of Dirichlet, Neumann, and Regularity problems for the Laplace equation in graph Lipschitz domains, focusing on $A_{\infty}$-measures and extending prior work.


<details>
  <summary>Details</summary>
Motivation: To extend and complement existing results on solvability of boundary value problems for the Laplace equation in specific domains, addressing gaps in the literature.

Method: Study $L^{p,1}$-solvability for Dirichlet, $L^p$-solvability for Neumann, and adapt results for Regularity problems, using $A_{\infty}$-measures and exploring endpoint cases.

Result: Established solvability ranges, conditions for Lorentz space and atomic Hardy space solvability, and a Sawyer-type inequality for the Neumann problem.

Conclusion: The work generalizes and refines solvability conditions for boundary value problems, highlighting differences from the arc-length case and providing new insights.

Abstract: We prove several results for the Dirichlet, Neumann and Regularity problems
for the Laplace equation in graph Lipschitz domains in the plane, considering
$A_{\infty}$-measures on the boundary. More specifically, we study the
$L^{p,1}$-solvability for the Dirichlet problem, complementing results of Kenig
(1980) and Carro and Ortiz-Caraballo (2018). Then, we study $L^p$-solvability
of the Neumann problem, obtaining a range of solvability which is empty in some
cases, a clear difference with the arc-length case. When it is not empty, it is
an interval, and we consider solvability at its endpoints, establishing
conditions for Lorentz space solvability when $p>1$ and atomic Hardy space
solvability when $p=1$. Solving the Lorentz endpoint leads us to a two-weight
Sawyer-type inequality, for which we give a sufficient condition. Finally, we
show how to adapt to the Regularity problem the results for the Neumann
problem.

</details>


### [54] [Geometric condition for the observability of electromagnetic Schrödinger operators on $\mathbb{T}^2$](https://arxiv.org/abs/2506.24049)
*Kévin Le Balc'h,Jingrui Niu,Chenmin Sun*

Main category: math.AP

TL;DR: The paper revisits the observability of the Schrödinger equation on a 2D torus, highlighting the impact of a magnetic potential compared to purely electric potentials.


<details>
  <summary>Details</summary>
Motivation: To understand how magnetic potentials introduce obstructions to observability, unlike purely electric potentials where any non-empty open set suffices.

Method: Establishes a geometric condition incorporating the magnetic potential, linking it to a geometric control condition for the magnetic field.

Result: A sufficient and almost necessary geometric condition for observability of electromagnetic Schrödinger operators is derived.

Conclusion: The presence of a magnetic potential requires additional geometric conditions for observability, unlike purely electric potentials.

Abstract: In this article we revisit the observability of the Schr\"odinger equation on
the two-dimensional torus. In contrast to the Schr\"odinger operator with a
purely electric potential, for which any non-empty open set guarantees
observability, the presence of a magnetic potential introduces an additional
obstruction. We establish a sufficient and almost necessary geometric condition
for the observability of electromagnetic Schr\"odinger operators. This
condition incorporates the magnetic potential, which can also be characterized
by a geometric control condition for the corresponding magnetic field.

</details>


### [55] [Evolution models with time-dependent coefficients in friction and viscoelastic damping terms](https://arxiv.org/abs/2506.24058)
*Halit Sevki Aslan,Michael Reissig*

Main category: math.AP

TL;DR: The paper analyzes decay estimates for solutions to a linear wave equation with time-dependent friction and viscoelastic damping, focusing on the interplay between damping terms.


<details>
  <summary>Details</summary>
Motivation: To understand how time-dependent coefficients in damping terms influence the qualitative behavior of solutions.

Method: Uses the WKB-method in extended phase space to classify damping mechanisms (frictional and viscoelastic).

Result: Derives decay estimates for higher order energy norms of solutions.

Conclusion: The interplay between damping terms significantly affects solution behavior, with decay estimates providing insights into their qualitative impact.

Abstract: We study the following Cauchy problem for the linear wave equation with both
time-dependent friction and time-dependent viscoelastic damping:
\begin{equation} \label{EqAbstract}\tag{$\ast$} \begin{cases} u_{tt}- \Delta u
+ b(t)u_t - g(t)\Delta u_t=0, &(t,x) \in (0,\infty) \times \mathbb{R}^n, \\
u(0,x)= u_0(x),\quad u_t(0,x)= u_1(x), &x \in \mathbb{R}^n. \end{cases}
\end{equation} Our goal is to derive decay estimates for higher order energy
norms of solutions to this problem. We focus on the interplay between the
time-dependent coefficients in both damping terms and their influence on the
qualitative behavior of solutions. The analysis is based on a classification of
the damping mechanisms, frictional damping $b(t)u_t$ and viscoelastic damping
$-g(t)\Delta u_t$ as well, and employs the WKB-method in the extended phase
space.

</details>


### [56] [The matrix weighted real-analytic double fibration transforms](https://arxiv.org/abs/2506.24067)
*Hiroyuki Chihara,Shubham R. Jathar,Jesse Railo*

Main category: math.AP

TL;DR: The paper demonstrates that the real-analytic matrix-weighted double fibration transform identifies the analytic wavefront set of vector-valued functions. It also proves injectivity of the matrix-weighted ray transform on 2D non-trapping real-analytic Riemannian manifolds with strictly convex boundaries and uniquely determines real-analytic Higgs fields from nonabelian ray transforms.


<details>
  <summary>Details</summary>
Motivation: The study aims to extend understanding of transforms and their injectivity properties in real-analytic settings, particularly for vector-valued functions and Higgs fields on Riemannian manifolds.

Method: The authors use the real-analytic matrix-weighted double fibration transform and apply it to analyze the analytic wavefront set. They then apply this to prove injectivity results for the matrix-weighted ray transform and nonabelian ray transform.

Result: Key results include the injectivity of the matrix-weighted ray transform on 2D non-trapping real-analytic Riemannian manifolds and the unique determination of real-analytic Higgs fields from nonabelian ray transforms.

Conclusion: The findings advance the theoretical framework for transforms in real-analytic contexts, with implications for geometric analysis and inverse problems.

Abstract: We show that the real-analytic matrix-weighted double fibration transform
determines the analytic wavefront set of a vector-valued function. We apply
this result to show that the matrix weighted ray transform is injective on a
two-dimensional, non-trapping, real-analytic Riemannian manifold with strictly
convex boundary. Additionally, we show that a real-analytic Higgs field can be
uniquely determined from the nonabelian ray transform on real-analytic
Riemannian manifolds of any dimension with a strictly convex boundary point.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [57] [Data-Driven Surrogate Modeling of DSMC Solutions Using Deep Neural Networks](https://arxiv.org/abs/2506.22453)
*Ehsan Roohi,Ahmad Shoja-sani*

Main category: physics.comp-ph

TL;DR: A DNN framework accelerates DSMC computations for rarefied-gas flows, maintaining high fidelity. Innovations include physical constraint injection, Fourier features for shocks, and expert-interpolation for Knudsen ranges.


<details>
  <summary>Details</summary>
Motivation: To reduce the computational time of DSMC for rarefied-gas flows while preserving physical accuracy.

Method: Uses a fully connected DNN trained on DSMC data, learnable Fourier features for shock waves, and a 'family-of-experts' strategy for varying Knudsen numbers.

Result: Achieves near-perfect agreement with DSMC for shock waves and less than 2% error for lid-driven cavity flows, reducing inference time to milliseconds.

Conclusion: The framework provides a general approach for rapid, accurate surrogate models applicable to non-equilibrium phenomena and design optimization.

Abstract: This study presents a deep neural network (DNN) framework that accelerates
Direct Simulation Monte Carlo (DSMC) computations for rarefied-gas flows, while
maintaining high physical fidelity. First, a fully connected deep neural
network is trained on high-quality DSMC data for seven temperatures (200-650 K)
to reproduce the Maxwell-Boltzmann speed distribution of argon. Injecting the
physical boundary point into the training set enforces the correct low-speed
limit. It reduces the mean-squared error to below 10^-5, thereby decreasing
inference time from tens of minutes per DSMC run to milliseconds. For
one-dimensional shock waves, a multi-output network equipped with learnable
Fourier features learns the complete profiles of density, velocity, and
temperature. Trained only on Mach numbers 1.4-1.9, it predicts a Mach 2 and 2.5
case with near-perfect agreement to DSMC, demonstrating robust out-of-training
generalization. In a lid-driven cavity, the large parametric spread in Knudsen
number is handled by a "family-of-experts" strategy: separate specialist models
are trained at discrete Knudsen (Kn) values, and log-space interpolation fuses
their outputs. This hybrid surrogate recovers the full 2-D velocity and
temperature fields at unseen Kn with less than 2% spatial error. Key
innovations include (i) explicit injection of physical constraints during data
preprocessing, (ii) learnable Fourier feature mapping to capture steep shock
gradients, and (iii) a modular expert-interpolation scheme to cover wide
Knudsen ranges. Together, they establish a general recipe for trustworthy,
rapid surrogate models that can be extended to non-equilibrium phenomena, gas
mixtures, and design optimization workflows

</details>


### [58] [Extending OpenMC Validation to Spent Fuel Canisters: A Criticality Benchmark Against MCNP](https://arxiv.org/abs/2506.22559)
*Javier Ruiz-Pineda,Jaime Romero-Barrientos,Francisco Molina,Marcelo Zambra,Franco López-Usquiano*

Main category: physics.comp-ph

TL;DR: OpenMC, an open-source Monte Carlo code, is benchmarked against MCNP for spent nuclear fuel storage scenarios, showing strong agreement in results.


<details>
  <summary>Details</summary>
Motivation: To validate OpenMC's performance in spent nuclear fuel storage applications, an area with limited literature coverage.

Method: Benchmarked OpenMC against MCNP for eleven KBS-3 disposal concept configurations, varying geometry, fuel composition, and environmental conditions. Evaluated k-eff and leakage fractions.

Result: Strong agreement between OpenMC and MCNP, with k-eff differences below 0.8% in dry storage. OpenMC captures inter-canister neutron interactions effectively.

Conclusion: The benchmark validates OpenMC's applicability to spent nuclear fuel storage, supporting its use in transport and disposal applications.

Abstract: OpenMC is an open-source Monte Carlo code with increasing relevance in
criticality safety and reactor physics applications. While its validation has
covered a broad range of systems, its performance in spent nuclear fuel storage
scenarios remains limited in the literature. This work benchmarks OpenMC
against MCNP for eleven configurations based on the KBS-3 disposal concept,
involving variations in geometry, fuel composition (fresh vs spent), and
environmental conditions (e.g., air, argon, flooding scenarios). Effective
multiplication factors (k-eff) and leakage fractions were evaluated for both
codes. Results show strong agreement, with code-to-code k-eff differences below
0.8% in dry storage conditions, and consistent trends across all cases.
Notably, OpenMC successfully captures inter-canister neutron interaction
effects under periodic boundary conditions, demonstrating its applicability to
dry storage configurations. This benchmark supports the extension of the
validation domain of OpenMC toward SNF transport and disposal applications.

</details>


### [59] [On the Structure of Carbon Nanotubes: Results from Computer-Assisted Proofs](https://arxiv.org/abs/2506.22614)
*Miguel Ayala,Rustum Choksi,Benedikt Wirth*

Main category: physics.comp-ph

TL;DR: A toolbox using computer-assisted proofs rigorously analyzes capped carbon nanotubes, measuring atomic distances and structural variations.


<details>
  <summary>Details</summary>
Motivation: To study the structure of capped carbon nanotubes with mathematical precision, quantifying oscillations and effects of interaction potentials.

Method: Model nanotubes as minimizers of an interatomic potential, using numerical simulations and validated computations.

Result: Rigorous measurements of diameter, bond lengths, and angles reveal oscillations near caps, differences in potentials, and effects of size/chirality.

Conclusion: Caps induce diameter oscillations, with spatial extent increasing for less smooth potentials, challenging monotonic equilibration assumptions.

Abstract: We present a toolbox based on computer-assisted proofs to rigorously study
the structure of capped carbon nanotubes. We model nanotubes as minimizers of
an interatomic potential. Numerical simulations and validated computations
produce rigorous mathematical results about atomic distances and structural
variations. In particular, we rigorously measure the diameter, bond lengths,
and bond angles of nanotubes and thereby precisely quantify oscillations near
the caps, differences between interaction potentials, and effects of nanotube
size or chirality. As an example, we observe that the caps induce diameter
oscillations along the tube (rather than a monotonous diameter equilibration)
with increasing spatial extent for less smooth interaction potentials.

</details>


### [60] [AirCANS: CFD 2D Mesh Optimisation-based Airfoil Classification and Assessment using Neural Networks](https://arxiv.org/abs/2506.22662)
*Lushun Fan,Yuqin Xia,Jun Li,Karl Jenkins*

Main category: physics.comp-ph

TL;DR: The study proposes AirCANS, a CNN-based framework for automating the classification and assessment of CFD airfoil mesh data, showing CNNs are effective for this task and can benefit the CFD industry.


<details>
  <summary>Details</summary>
Motivation: To automate and speed up the classification and assessment of CFD mesh data, particularly for airfoils, using neural networks.

Method: Developed AirCANS, a framework with a data loader and improved CNN structure tailored for CFD mesh data.

Result: CNNs are adaptable and effective for CFD airfoil mesh data, validating the hypothesis and suggesting potential industry benefits like faster solutions and refined meshes.

Conclusion: Neural networks like AirCANS can significantly improve efficiency in the CFD industry by automating mesh data tasks.

Abstract: This study explores the possibilities of automating the loading,
classification and assessment of Computational Fluid Dynamics (CFD) mesh data
by Convolutional Neural Networks (CNNs). The research aim is finding a feasible
way to quickly make classification and assessment on airfoil mesh data. For
this purpose, this study designed a new framework named CFD-based airfoil
Classification and Assessment Network (AirCANS) for CFD mesh data which
including the data loader and improved the CNN structure to achieve our target.
In our research, we found that CNNs are fully adaptable as well as
understandable to CFD airfoil mesh data structures, which suggests that our
hypothesis is successful and that neural networks can be used to have a greater
positive impact on the CFD industry, such as it can be used to refine the mesh
and accelerate the solution. This could allow CFD to spend much less time.

</details>


### [61] [Generating Moving Field Initial Conditions with Spatially Varying Boost](https://arxiv.org/abs/2506.23020)
*Siyang Ling*

Main category: physics.comp-ph

TL;DR: A novel algorithm, 'spatially varying boost,' generates dynamical field initial conditions with prescribed bulk velocities by applying local Lorentz boosts to non-moving initial data.


<details>
  <summary>Details</summary>
Motivation: To enable simulations of fields with ultra-relativistic velocities across various physics domains like cosmology and condensed matter.

Method: The algorithm performs local Lorentz boosts on initial field data to achieve desired velocity profiles, applicable to any field type or equation of motion.

Result: Demonstrated effectiveness in boosting Sine-Gordon solitons, generating relativistic Proca fields, and setting up Schrödinger-Poisson fields for dark matter simulations.

Conclusion: The algorithm is versatile and opens new possibilities for simulations in physics, particularly for ultra-relativistic scenarios.

Abstract: We introduce a novel class of algorithms, the ``spatially varying boost'',
for generating dynamical field initial conditions with prescribed bulk
velocities. Given (non-moving) initial field data, the algorithm generates new
initial data with the given velocity profile by performing local Lorentz
boosts. This algorithm is generic, with no restriction on the type of the
field, the equation of motion, and can endow fields with ultra-relativistic
velocities. This algorithm enables new simulations in different branches of
physics, including cosmology and condensed matter physics. For demonstration,
we used this algorithm to (1) boost two Sine-Gordon solitons to
ultra-relativistic speeds for subsequent collision, (2) generate a relativistic
transverse Proca field with random velocities, and (3) set up a spin-$1$
Schr\"{o}dinger-Poisson field with velocity and density perturbations
consistent with dark matter in matter dominated universe.

</details>


### [62] [Variational PINNs with tree-based integration and boundary element data in the modeling of multi-phase architected materials](https://arxiv.org/abs/2506.23357)
*Dimitrios C. Rodopoulos,Panos Pantidis,Nikolaos Karathanasopoulos*

Main category: physics.comp-ph

TL;DR: A VPINN framework for multiphase architected solids is developed, using a Petrov-Galerkin approach with neural networks and local polynomials. It includes adaptive tree-based integration and demonstrates accurate deformation field capture.


<details>
  <summary>Details</summary>
Motivation: To improve modeling of multiphase architected materials by combining neural networks with physics-informed methods for better accuracy and efficiency.

Method: VPINN methodology with Petrov-Galerkin approach, neural networks as trial functions, local polynomials as test functions, and adaptive tree-based integration. GBEM generates mechanical field data.

Result: VPINNs accurately capture deformation fields and handle material discontinuities with reduced computational cost, outperforming classical PINNs.

Conclusion: The VPINN framework is advantageous for modeling multiphase architected materials, enabling future development of physics-informed computational models for complex structures.

Abstract: The current contribution develops a Variational Physics-Informed Neural
Network (VPINN)-based framework for the analysis and design of multiphase
architected solids. The elaborated VPINN methodology is based on the
Petrov-Galerkin approach, with a deep neural network acting as trial function
and local polynomials as test functions. For the analysis, a Galerkin Boundary
Element Method (GBEM) scheme is developed to generate the mechanical field
data, employing solely domain boundary information. The VPINN methodology is
complemented by an adaptive, tree-based integration scheme for the evaluation
of the weak-form integrals. Different double-phase material architectures are
considered, with the VPINNs demonstrating their ability to capture the
deformation fields with considerable accuracy. Moreover, the performance
enhancement by the incorporation of additional semi-analytical information at
auxiliary internal points is analyzed. Tree-based integration schemes are shown
to be capable of robustly capturing inner material discontinuities upon
substantial computational cost reductions. The results suggest that the
proposed VPINN formulation offers comparative advantages in the modeling of
multiphase architected materials compared to classical PINN formulations. The
analysis paves the way for the development of variational physics-informed
computational models for the mechanical analysis of complex architected
multiphase materials and structures.

</details>


### [63] [Graph Neural Networks to Predict Coercivity of Hard Magnetic Microstructures](https://arxiv.org/abs/2506.23615)
*Heisam Moustafa,Alexander Kovacs,Johann Fischbacher,Markus Gusenbauer,Qais Ali,Leoni Breth,Thomas Schrefl,Harald Oezelt*

Main category: physics.comp-ph

TL;DR: A GNN is trained to predict coercivity in magnetic microstructures, evaluated for performance and uncertainty, and repurposed for predicting maximum energy product, with out-of-distribution predictions also explored.


<details>
  <summary>Details</summary>
Motivation: To speed up the search for rare-earth free permanent magnets by predicting magnetic properties of large multi-grain structures using GNNs.

Method: Train a GNN on magnetic simulation data to predict coercivity, evaluate its performance and uncertainty, reuse the architecture for maximum energy product prediction, and perform out-of-distribution predictions.

Result: The GNN successfully predicts coercivity and is adapted for other magnetic properties, with feature engineering aiding out-of-distribution predictions.

Conclusion: GNNs are effective for predicting magnetic properties, offering potential for accelerating material discovery.

Abstract: Graph neural networks (GNN) are a promising tool to predict magnetic
properties of large multi-grain structures, which can speed up the search for
rare-earth free permanent magnets. In this paper, we use our magnetic
simulation data to train a GNN to predict coercivity of hard magnetic
microstructures. We evaluate the performance of the trained GNN and quantify
its uncertainty. Subsequently, we reuse the GNN architecture for predicting the
maximum energy product. Out-of-distribution predictions of coercivity are also
performed, following feature engineering based on the observed dependence of
coercivity on system size.

</details>


### [64] [Efficient snap-to-contact computations for van der Waals interacting fibers](https://arxiv.org/abs/2506.23620)
*Aleksandar Borković,Michael H. Gfrerer,Roger A. Sauer,Benjamin Marussig*

Main category: physics.comp-ph

TL;DR: The paper explores van der Waals interactions between in-plane fibers using Lennard-Jones potential and coarse-grained modeling, with efficient computational methods for complex phenomena.


<details>
  <summary>Details</summary>
Motivation: To model and analyze van der Waals interactions between fibers efficiently, addressing computational challenges in small-scale phenomena.

Method: Uses Lennard-Jones potential and coarse-grained approach, splitting integrals into analytical and numerical parts, and employs Bernoulli-Euler beam theory with isogeometric finite elements.

Result: Demonstrates efficient modeling of nonlinear and dynamic snap-to-contact phenomena through refined computational techniques.

Conclusion: Coarse-graining and pre-integration of interaction potentials enable efficient simulation of complex small-scale fiber interactions.

Abstract: We consider van der Waals interactions between in-plane fibers, where the
computational model employs the Lennard-Jones potential and the coarse-grained
approach. The involved 6D integral over two interacting fibers is split into a
4D analytical pre-integration over cross sections and the remaining 2D
numerical integration along the fibers' axes. Two section-section interaction
laws are implemented, refined, and compared. Fibers are modeled using the
Bernoulli-Euler beam theory and spatially discretized with isogeometric finite
elements. We derive and solve the weak form of both quasi-static and dynamic
boundary value problems. Four numerical examples involving highly nonlinear and
dynamic snap-to-contact phenomena are scrutinized. We observe that the
coarse-graining and pre-integration of interaction potentials enable the
efficient modeling of complex phenomena at small length scales.

</details>


### [65] [Learning robust parameter inference and density reconstruction in flyer plate impact experiments](https://arxiv.org/abs/2506.23914)
*Evan Bell,Daniel A. Serino,Ben S. Southworth,Trevor Wilcox,Marc L. Klasky*

Main category: physics.comp-ph

TL;DR: The paper addresses the challenge of estimating material properties from radiographic observations in shock physics, proposing a machine learning approach to infer equation of state and crush model parameters from a combination of low and high impact velocity experiments.


<details>
  <summary>Details</summary>
Motivation: Traditional parameter estimation methods fail in shock physics experiments because radiography doesn't directly measure key variables like density. The study aims to overcome this limitation.

Method: A generative machine learning approach is introduced to produce posterior distributions of physical parameters from radiographs, using data from both low and high impact velocity experiments.

Result: The method accurately estimates parameters for simulations and enables physically admissible density reconstructions. It also shows robustness to noise and unseen physics.

Conclusion: The approach offers a breakthrough in estimating material properties from radiographic images, even with model mismatches.

Abstract: Estimating physical parameters or material properties from experimental
observations is a common objective in many areas of physics and material
science. In many experiments, especially in shock physics, radiography is the
primary means of observing the system of interest. However, radiography does
not provide direct access to key state variables, such as density, which
prevents the application of traditional parameter estimation approaches. Here
we focus on flyer plate impact experiments on porous materials, and resolving
the underlying parameterized equation of state (EoS) and crush porosity model
parameters given radiographic observation(s). We use machine learning as a tool
to demonstrate with high confidence that using only high impact velocity data
does not provide sufficient information to accurately infer both EoS and crush
model parameters, even with fully resolved density fields or a dynamic sequence
of images. We thus propose an observable data set consisting of low and high
impact velocity experiments/simulations that capture different regimes of
compaction and shock propagation, and proceed to introduce a generative machine
learning approach which produces a posterior distribution of physical
parameters directly from radiographs. We demonstrate the effectiveness of the
approach in estimating parameters from simulated flyer plate impact
experiments, and show that the obtained estimates of EoS and crush model
parameters can then be used in hydrodynamic simulations to obtain accurate and
physically admissible density reconstructions. Finally, we examine the
robustness of the approach to model mismatches, and find that the learned
approach can provide useful parameter estimates in the presence of
out-of-distribution radiographic noise and previously unseen physics, thereby
promoting a potential breakthrough in estimating material properties from
experimental radiographic images.

</details>


### [66] [A hybrid numerical algorithm based on the stochastic particle Shakhov and DSMC method](https://arxiv.org/abs/2506.23917)
*Hao Jin,Sha Liu,Sirui Yang,Junzhe Cao,Congshan Zhuo,Chengwen Zhong*

Main category: physics.comp-ph

TL;DR: The paper proposes a hybrid particle method combining stochastic particle Shakhov with DSMC, addressing computational inefficiency in multi-scale gas flow simulations by decomposing the collision operator based on timescales.


<details>
  <summary>Details</summary>
Motivation: The need for efficient simulation of multi-scale gas flows, where rarefied and continuum regimes coexist, due to limitations in DSMC's time steps and mesh sizes.

Method: Introduces a hybrid method coupling stochastic particle Shakhov with DSMC, decomposing the collision operator into sub-steps based on local timescales and relaxation time.

Result: Demonstrated validity and accuracy through benchmark cases like 1-D shock tube, 2-D hypersonic flow, and 3-D hypersonic flows.

Conclusion: The proposed method offers a rational and efficient approach for simulating multi-scale gas flows, overcoming DSMC's limitations.

Abstract: The Direct Simulation Monte Carlo (DSMC) method is widely employed for
simulating rarefied nonequilibrium gas flows. With advances in aerospace
engineering and micro/nano-scale technologies, gas flows exhibit the
coexistence of rarefied and continuum/near-continuum regimes, which calls for
larger time steps and coarser spatial grids for efficient numerical simulation.
However, the mesh sizes and time steps in DSMC are constrained by the
single-scale nature of the Boltzmann equation and the explicit treatment of
collision term following operator splitting. To overcome the resulting
computational inefficiency, the Time-Relaxed Monte Carlo (TRMC) method
introduces a suitable time discretization of the Boltzmann equation, allowing
for significantly larger time steps. Besides, domain decomposition methods
leverage the complementary strengths of continuum and particle-based
approaches, facilitating the efficient simulation of multi-scale gas flows.
However, in TRMC method, the physically accurate high-order terms are truncated
and approximated through convergence to a local Maxwellian distribution.
Meanwhile, the continuum breakdown criteria employed in hybrid methods are
either empirical or semi-empirical. Recently, a timescale-based decomposition
of the Boltzmann equation has been proposed to enable a more rational coupling
between DSMC and Navier-Stokes. Inspired by this strategy, a novel hybrid
particle method is proposed to couple the stochastic particle Shakhov with
DSMC, in which the collision operator is decomposed into two sub-steps based on
local observation timescale and the relaxation time. The validity and accuracy
of the proposed method are demonstrated through a series of benchmark cases,
including 1-D sod shock tube, 2-D hypersonic flow around cylinder and jet
expansion into the vacuum, 3-D hypersonic flows around sphere and X-38 like
vehicle in near-continuum flow regimes.

</details>


### [67] [Modified non-local damage model: resolving spurious damage evolution](https://arxiv.org/abs/2506.24099)
*Roshan Philip Saji,Panos Pantidis,Mostafa E. Mobasher*

Main category: physics.comp-ph

TL;DR: The paper introduces a modified non-local gradient damage model (MNLD) to address computational and generalizability limitations in existing damage prediction methods, ensuring fixed-width damage bands.


<details>
  <summary>Details</summary>
Motivation: Existing methods for predicting damage and fracture evolution are either computationally prohibitive or suffer from unrealistic damage-band widening, limiting their practical application.

Method: The MNLD model modifies the stress degradation function and forcing term in the Helmholtz free energy expression to ensure vanishing thermodynamic driving force and decay of the forcing term as damage progresses.

Result: Numerical validation shows the model reliably produces fixed-width damage bands, resolving the spurious widening issue in gradient damage models.

Conclusion: The MNLD model offers an effective, practical solution for engineering applications, overcoming long-standing challenges in gradient damage models.

Abstract: Accurate prediction of damage and fracture evolution is critical for the
safety design and preventive maintenance of engineering structures, however
existing computational methods face significant limitations. On one hand,
discrete damage and phase-field models are often computationally prohibitive
for real world applications and they are less generalizable across different
material classes. On the other hand, conventional gradient damage models which
are based on phenomenological laws, though more computationally efficient, they
suffer from unrealistic widening of the damage-band as damage progresses. This
paper presents a modified non-local gradient damage model (MNLD) that overcomes
these shortcomings by introducing modifications to the stress degradation
function and forcing term in the Helmholtz free energy expression. These two
modifications ensure that as damage approaches its maximum value, both the
thermodynamic damage driving force for damage vanishes and the evolution of the
forcing term decays. Consequently, the damage band retains a non-growing
constant width throughout its evolution. The proposed approach builds on
insights gained from two intermediate models, which addressed the necessary
conditions separately before integrating them into a unified formulation.
Numerical validation is performed on several 1D and 2D benchmark problems,
demonstrating that the proposed model can reliably produce fixed-width damage
bands. The proposed approach can be implemented within existing gradient
damage-based finite element frameworks with minimal implementation changes. The
results highlight the potential of this approach to resolve the decades-long
challenge of spurious widening in gradient damage models, offering an effective
and practical solution for engineering applications.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [68] [Probing the transition from classical to quantum radiation reaction in relativistic plasma](https://arxiv.org/abs/2506.22577)
*Haidar Al-Naseri,Gert Brodin*

Main category: physics.plasm-ph

TL;DR: The paper examines the shift from classical to quantum radiation reaction in a plasma under a circularly polarized field, highlighting differences due to the quantum χ-parameter and plasma conditions.


<details>
  <summary>Details</summary>
Motivation: To understand how quantum effects alter classical radiation reaction (Landau-Lifshitz model) in plasmas and how plasma parameters influence this transition.

Method: The study uses a self-consistent plasma current model under a circularly polarized field, analyzing wave energy loss, frequency up-conversion, and distribution function changes.

Result: Quantum effects diverge from classical predictions as the χ-parameter increases, with deviations also tied to plasma density and temperature.

Conclusion: The findings reveal the nuanced impact of quantum mechanics on radiation reaction, dependent on both χ-parameter and plasma properties, with broader implications for plasma physics.

Abstract: We study the transition from classical radiation reaction, described by the
Landau-Lifshitz model, to the quantum mechanical regime. The plasma is subject
to a circularly polarized field where the self-consistent plasma current is the
source of the electromagnetic field through Ampere's law. The radiation
reaction implies wave energy loss, frequency up-conversion, and a modified
distribution function. Increasing the value of the quantum $\chi$-parameter,
the quantum results gradually differ from the classical ones. Moreover, the
deviation between models also depends on the plasma parameters, including
density and temperature. We discuss the implications of our findings.

</details>


### [69] [Variational hydrodynamics of the classical Yukawa one-component plasma](https://arxiv.org/abs/2506.23006)
*Daniels Krimans,Hanno Kählert*

Main category: physics.plasm-ph

TL;DR: The paper extends a variational hydrodynamic approach to the Yukawa one-component plasma, deriving motion equations and conservation laws, and validating results with simulations.


<details>
  <summary>Details</summary>
Motivation: To generalize hydrodynamic equations for strongly coupled plasmas to finite length scales, incorporating pair distribution effects.

Method: A variational approach is used to derive equations of motion, momentum, and energy conservation laws, validated against thermodynamics and simulations.

Result: Excellent agreement with numerical data for sound speed and dispersion relations in weak to moderate screening, with discrepancies at strong screening.

Conclusion: The approach is effective for linear regimes and holds promise for nonlinear problems and other systems.

Abstract: We consider a recently developed variational approach to the hydrodynamics of
strongly coupled plasmas [D. Krimans and S. Putterman, Phys. Fluids 36, 037131
(2024)] and extend it to the Yukawa one-component plasma. This approach
generalizes the ordinary hydrodynamic equations to finite length scales by
explicitly including terms that depend on the pair distribution function. After
discussing the form of the Lagrangian, we derive equations of motion and
explicit formulas for the momentum and energy conservation laws. After
demonstrating consistency with thermodynamics, we consider the simpler linear
regime and the dispersion laws. By comparing the longitudinal speed of sound to
existing numerical data, we find excellent agreement in the weak to moderate
screening regimes, while discrepancies arise at strong screening. The
finite-wavelength behavior of the longitudinal dispersion relation also shows
excellent agreement with simulations across a wide range of coupling and
screening parameters, even when the wavelength is comparable to the average
interparticle spacing. In addition to the linear regime, our variational
approach has potential for application to nonlinear problems and other physical
systems.

</details>


### [70] [Investigation of resonant layer response in electron viscosity regime](https://arxiv.org/abs/2506.23530)
*Yeongsun Lee,Jace Waybright,Jong-Kyu Park*

Main category: physics.plasm-ph

TL;DR: Study refines previous work by including electron viscosity in Ohm's law, revealing resonant layer response in viscous plasmas.


<details>
  <summary>Details</summary>
Motivation: To explore the impact of electron viscosity on resonant layer response to magnetic perturbations.

Method: Included a curl element of electron viscosity in the generalized Ohm's law.

Result: Revealed a resonant layer response in the Electron Viscosity (EV) regime for slow, viscous plasmas.

Conclusion: Electron viscosity significantly affects resonant layer dynamics in plasmas.

Abstract: We present a supplementary study of previous work in Waybright and Park
[Phys. Plasmas 31, 022502 (2024)] which demonstrates a substantial effect of
electron viscosity on the resonant layer response to non-axisymmetric magnetic
perturbations. A main refinement is to include a curl element of electron
viscosity in the generalized Ohm's law. The refinement reveals a resonant layer
response in the Electron Viscosity (EV) regime corresponding to slowly rotating
and highly viscous plasmas.

</details>


### [71] [Alloharmonics in Burst Intensification by Singularity Emitting Radiation](https://arxiv.org/abs/2506.23548)
*K. Ogura,M. S. Pirozhkova,A. Sagisaka,T. Zh. Esirkepov,A. Ya. Faenov,T. A. Pikuz,H. Kotaki,Y. Hayashi,Y. Fukuda,J. K. Koga,S. V. Bulanov,H. Daido,N. Hasegawa,M. Ishino,M. Nishikino,M. Koike,T. Kawachi,H. Kiriyama,M. Kando,D. Neely,A. S. Pirozhkov*

Main category: physics.plasm-ph

TL;DR: BISER in underdense relativistic laser plasma produces coherent XUV and x-ray radiation with fine spectral fringes, explained by laser redshift and alloharmonic interference.


<details>
  <summary>Details</summary>
Motivation: To understand the fine spectral fringes in BISER spectra, which differ from traditional harmonic generation mechanisms.

Method: Analyzed high-resolution experimental BISER spectra in the XUV region, focusing on fringe separation and its causes.

Result: Fringe separations are due to laser frequency redshift and spectral interference of alloharmonics from different emission moments.

Conclusion: BISER's spectral fringes are uniquely explained by redshift and alloharmonic interference, distinguishing it from other harmonic generation methods.

Abstract: Burst Intensification by Singularity Emitting Radiation (BISER) in underdense
relativistic laser plasma is a bright source of coherent extreme ultraviolet
(XUV) and x-ray radiation. In contrast to all harmonic generation mechanisms,
high-resolution experimental BISER spectra in the XUV region contain spectral
fringes with separation much finer (down to 0.12 eV) than the initial driving
laser frequency (~1.5 eV). We show that these fringe separations result from
two main factors: laser frequency downshift (redshift) due to the
quasi-adiabatic energy loss to the plasma waves, and spectral interference of
different harmonic orders from different emission moments, i.e. alloharmonics
[Pirozhkova et al., arXiv:2306.01018]

</details>


### [72] [The interaction of turbulence, magnetic islands and zonal fields in fluid plasma models with cubic non-linearities](https://arxiv.org/abs/2506.23593)
*Daniele Villa,Nicolas Dubuit,Olivier Agullo,Xavier Garbet*

Main category: physics.plasm-ph

TL;DR: Magnetic islands from pressure-gradient-driven turbulence are common, interacting with zonal flows and currents, influencing system dynamics. Turbulence energizes large-scale structures indirectly. Zonal current slows island formation, while zonal flow aids energy transfer to larger scales. KBM instability drives the system, with islands forming early and coalescing over time.


<details>
  <summary>Details</summary>
Motivation: To understand the role of turbulence-driven magnetic islands and their interaction with zonal flows and currents in plasma dynamics.

Method: Analyzed interactions among turbulence, magnetic islands, zonal flow, and zonal current, focusing on KBM instability and tearing mode stability.

Result: Magnetic islands form early, coalesce to larger scales, and show unbounded growth. Zonal current slows island formation, while zonal flow aids energy transfer. Pressure profile flattening occurs far from resonance.

Conclusion: Turbulence-driven magnetic islands and their interactions with zonal structures are key to plasma dynamics, with implications for energy transfer and pressure profile evolution.

Abstract: It is shown that magnetic islands generated by pressure-gradient-driven
turbulence are common across a wide range of conditions. The interaction among
turbulence, magnetic island and other large scale structures (the zonal flow
and the zonal current), largely determines the dynamics of the system.
Turbulence takes a background role, providing energy to the large-scale
structures, without influencing their evolution directly. The growth of the
zonal current is linearly related to that of the magnetic island, while the
zonal flow has a strongly sheared region where the island has its maximum
radial extension. The zonal current is found to slow down the formation of
large-scale magnetic islands, while the zonal flow is needed to have the system
move its energy to larger and larger scales. The driving instability in the
system is the fluid Kinetic Ballooning Mode (KBM) instability at high beta,
while the tearing mode is kept stable. The formation of magnetic-island-like
structures at the spatial scale of the fluid KBM instability is observed quite
early in the non-linear phase for most cases studied, and a slow coalescence
process evolves the magnetic structures towards larger and larger scales. Cases
that did not show this coalescence process, nor the formation of the small
scale island-like structures, were seen to have narrower mode structures for
comparable instability growth rates, which was achieved by varying the magnetic
shear. The islands often end up exceeding the radial box size late in the
non-linear phase, showing unbounded growth. The impact on the pressure profile
of turbulence driven magnetic islands is not trivial, showing flattening of the
pressure profile only far from the resonance, where the zonal flow is weaker,
and the appearance of said flattening is slow, after the island has reached a
sufficiently large size, when compared with collisional time scales.

</details>


### [73] [Plasma Accelerator For Decaying Particle](https://arxiv.org/abs/2506.23668)
*Chiara Badiali,Rafael Almeida,Bernardo Malaca,Ricardo Fonseca,Thales Silva,Jorge Vieira*

Main category: physics.plasm-ph

TL;DR: A plasma wakefield acceleration scheme boosts subrelativistic particles to relativistic speeds over millimeter distances using a tailored plasma density and subluminal light pulse.


<details>
  <summary>Details</summary>
Motivation: To extend the dephasing length and efficiently accelerate particles from subrelativistic to relativistic velocities.

Method: A theoretical model generalizable across particle mass, initial velocity, and accelerating bucket, validated with particle-in-cell simulations using Joule-range laser drivers.

Result: Successful acceleration of particles to relativistic velocities within millimeter-scale distances, verified by simulations.

Conclusion: The scheme effectively extends dephasing length and accelerates particles efficiently, with potential for broader applications.

Abstract: We introduce a plasma wakefield acceleration scheme capable of boosting
initially subrelativistic particles to relativistic velocities within
millimeter-scale distances. A subluminal light pulse drives a wake whose
velocity is continuously matched to the beam speed through a tailored plasma
density, thereby extending the dephasing length. We develop a theoretical model
that is generalizable across particle mass, initial velocity, and the
particular accelerating bucket being used, and we verify its accuracy with
particle-in-cell simulations using laser drivers with energies in the Joule
range.

</details>


### [74] [High brightness multi-MeV photon source driven by a petawatt-scale laser wakefield accelerator](https://arxiv.org/abs/2506.23718)
*E. Gerstmayr,B. Kettle,M. J. V. Streeter,L. Tudor,O. J. Finlay,L. E. Bradley,R. Fitzgarrald,T. Foster,P. Gellersen,A. E. Gunn,O. Lawrence,P. P. Rajeev,B. K. Russell,D. R. Symes,C. D. Murphy,A. G. R. Thomas,C. P. Ridgers,G. Sarri,S. P. D. Mangles*

Main category: physics.plasm-ph

TL;DR: A petawatt laser-driven gamma source produces 1.2×10^9 photons above 1 MeV per pulse, surpassing previous sources by 100x, with peak brightness of 3.9×10^22 photons/mm²/mrad²/s/0.1%BW at 11 MeV.


<details>
  <summary>Details</summary>
Motivation: To demonstrate a high-brightness, multi-MeV gamma source for fundamental physics and applications like nuclear resonance fluorescence and transmutation.

Method: Inverse Compton scattering of a GeV electron beam (laser wakefield accelerated) and its back-reflected driving laser pulse.

Result: Achieved unprecedented photon yield and brightness, validated by a simple model of laser and electron properties.

Conclusion: The source shows great potential for fundamental physics and practical applications in nuclear studies.

Abstract: We present an experimental demonstration of a bright multi-MeV gamma source
driven by a petawatt laser. The source generates on average
$(1.2\pm0.6)\times10^9$ photons above 1 MeV per pulse, exceeding those of
previous all-optical sources by a hundred times, and reached a peak spectral
brightness of $(3.9 \pm 1.5)\times 10^{22}$ photons/mm$^2$/mrad$^2$/s/0.1%BW at
$\epsilon_\gamma\approx11$ MeV. The source was produced by inverse Compton
scattering of a laser wakefield accelerated GeV electron beam and its
back-reflected driving laser pulse, and is well described by a simple model of
the laser and electron properties at the collision point. Our results highlight
the promise of this source for fundamental physics studies, as well as for
applications of nuclear resonance fluorescence and nuclear transmutation.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [75] [On a result by Meshulam](https://arxiv.org/abs/2506.22553)
*Heinz H. Bauschke,Tran Thanh Tung*

Main category: math.OC

TL;DR: The paper extends Meshulam's 1996 result on bounded sequences from affine subspaces in Euclidean space to convex polyhedral subsets in general Hilbert space.


<details>
  <summary>Details</summary>
Motivation: To generalize Meshulam's bounded sequence theorem to broader settings, including convex polyhedral subsets and Hilbert spaces.

Method: Extends the original proof framework to handle convex polyhedral subsets and general Hilbert spaces, supported by illustrative examples.

Result: The sequences remain bounded under projections onto convex polyhedral subsets in Hilbert spaces, with examples demonstrating the sharpness of the results.

Conclusion: The paper successfully generalizes Meshulam's theorem, confirming boundedness in more complex and abstract settings.

Abstract: In 1996, Meshulam proved that every sequence generated by applying
projections onto affine subspaces, drawn from a finite collection in Euclidean
space, must be bounded.
  In this paper, we extend his result not only from affine subspaces to convex
polyhedral subsets, but also from Euclidean to general Hilbert space. Various
examples are provided to illustrate the sharpness of the results.

</details>


### [76] [Denoising Multi-Color QR Codes and Stiefel-Valued Data by Relaxed Regularizations](https://arxiv.org/abs/2506.22826)
*Robert Beinert,Jonas Bresch*

Main category: math.OC

TL;DR: The paper extends a convexification-based denoising approach to multi-binary and Stiefel-valued data, proposing TV- and Tikhonov-based models with synthetic experiments.


<details>
  <summary>Details</summary>
Motivation: To address denoising challenges in manifold-valued data, particularly for multi-binary (e.g., multi-color QR codes) and Stiefel-valued data (e.g., image/video recognition).

Method: Embed data in Euclidean space, encode manifolds via fixed-rank matrices, relax rank constraints for convexification, and apply TV/Tikhonov models.

Result: Proposed models are evaluated on synthetic experiments, demonstrating feasibility.

Conclusion: The approach successfully extends to new data types, offering efficient denoising solutions for manifold-valued data.

Abstract: The handling of manifold-valued data, for instance, plays a central role in
color restoration tasks relying on circle- or sphere-valued color models, in
the study of rotational or directional information related to the special
orthogonal group, and in Gaussian image processing, where the pixel statistics
are interpreted as values on the hyperbolic sheet. Especially, to denoise these
kind of data, there have been proposed several generalizations of total
variation (TV) and Tikhonov-type denoising models incorporating the underlying
manifolds. Recently, a novel, numerically efficient denoising approach has been
introduced, where the data are embedded in an Euclidean ambient space, the
non-convex manifolds are encoded by a series of positive semi-definite,
fixed-rank matrices, and the rank constraint is relaxed to obtain a
convexification that can be solved using standard algorithms from convex
analysis. The aim of the present paper is to extent this approach to new kinds
of data like multi-binary and Stiefel-valued data. Multi-binary data can, for
instance, be used to model multi-color QR codes whereas Stiefel-valued data
occur in image and video-based recognition. For both new data types, we propose
TV- and Tikhonov-based denoising modelstogether with easy-to-solve
convexification. All derived methods are evaluated on proof-of-concept,
synthetic experiments.

</details>


### [77] [Deep neural networks can provably solve Bellman equations for Markov decision processes without the curse of dimensionality](https://arxiv.org/abs/2506.22851)
*Arnulf Jentzen,Konrad Kleinberg,Thomas Kruse*

Main category: math.OC

TL;DR: The paper constructs deep neural network (DNN) approximations for Q-functions in Markov decision processes (MDPs) with infinite time horizon and finite control sets, showing polynomial growth in parameters for state space dimension and error tolerance.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of approximating Q-functions in MDPs, which are central to reinforcement learning, using DNNs with leaky ReLU activation.

Method: The authors use DNNs with leaky ReLU activation to approximate payoff functions and transition dynamics, leveraging the full-history recursive multilevel fixed-point (MLFP) scheme for proof.

Result: The Q-functions can be approximated in the L²-sense by DNNs with polynomial parameter growth in state space dimension and error tolerance.

Conclusion: DNNs with leaky ReLU activation are effective for approximating Q-functions in MDPs, with scalable parameter requirements.

Abstract: Discrete time stochastic optimal control problems and Markov decision
processes (MDPs) are fundamental models for sequential decision-making under
uncertainty and as such provide the mathematical framework underlying
reinforcement learning theory. A central tool for solving MDPs is the Bellman
equation and its solution, the so-called $Q$-function. In this article, we
construct deep neural network (DNN) approximations for $Q$-functions associated
to MDPs with infinite time horizon and finite control set $A$. More
specifically, we show that if the the payoff function and the random transition
dynamics of the MDP can be suitably approximated by DNNs with leaky rectified
linear unit (ReLU) activation, then the solutions $Q_d\colon \mathbb R^d\to
\mathbb R^{|A|}$, $d\in \mathbb{N}$, of the associated Bellman equations can
also be approximated in the $L^2$-sense by DNNs with leaky ReLU activation
whose numbers of parameters grow at most polynomially in both the dimension
$d\in \mathbb{N}$ of the state space and the reciprocal $1/\varepsilon$ of the
prescribed error $\varepsilon\in (0,1)$. Our proof relies on the recently
introduced full-history recursive multilevel fixed-point (MLFP) approximation
scheme.

</details>


### [78] [On the controllability of laminated beams with Venttsel-type boundary conditions](https://arxiv.org/abs/2506.22887)
*George J. Bautista,Roberto de A. Capistrano-Filho,Juan Límaco*

Main category: math.OC

TL;DR: The paper analyzes boundary controllability of a Timoshenko laminated beam with Venttsel-type boundary conditions using three boundary controls.


<details>
  <summary>Details</summary>
Motivation: To study controllability in a novel beam configuration with dynamic Venttsel-type boundary conditions.

Method: Derives an observability inequality for the adjoint system and uses the duality method (HUM) to solve the control problem.

Result: Controllability is established for the system with three boundary controls.

Conclusion: The work contributes to understanding controllability in systems with Venttsel-type boundary conditions.

Abstract: This paper examines the boundary controllability of a Timoshenko laminated
beam system subject to Venttsel-type boundary conditions. The study focuses on
a novel configuration in which three controls are applied solely at the
boundary of the beam. Controllability is established by deriving an appropriate
observability inequality for the corresponding adjoint system, which is then
employed within the framework of the duality method in the setup of the
classical Hilbert uniqueness method (HUM) to achieve the control problem. The
main contribution lies in the analysis of a system comprising three beams
governed by dynamic Venttsel-type boundary conditions, as introduced by
Venttsel in [Theory Probab. Appl., 4 (1959)].

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [79] [Shifted Composition IV: Underdamped Langevin and Numerical Discretizations with Partial Acceleration](https://arxiv.org/abs/2506.23062)
*Jason M. Altschuler,Sinho Chewi,Matthew S. Zhang*

Main category: math.PR

TL;DR: The paper introduces a coupling-based framework for analyzing underdamped Langevin dynamics (ULD) and its discretizations, establishing new Harnack inequalities and a local error framework for KL divergence analysis.


<details>
  <summary>Details</summary>
Motivation: The degeneracy of ULD requires new analysis approaches, and the paper aims to provide a user-friendly framework for analyzing ULD and its numerical discretizations.

Method: A coupling-based framework is developed to analyze ULD, including parabolic Harnack inequalities and a local error framework for KL divergence analysis.

Result: The framework enables ballistic acceleration for log-concave sampling and a $d^{1/3}$ iteration complexity guarantee for sampling in dimension $d$.

Conclusion: The new framework advances the analysis of ULD and its discretizations, offering practical tools for degenerate diffusions.

Abstract: Quantifying the convergence rate of the underdamped Langevin dynamics (ULD)
is a classical topic, in large part due to the possibility for
diffusive-to-ballistic speedups -- as was recently established for the
continuous-time dynamics via space-time Poincare inequalities. A central
challenge for analyzing ULD is that its degeneracy necessitates the development
of new analysis approaches, e.g., the theory of hypocoercivity. In this paper,
we give a new coupling-based framework for analyzing ULD and its numerical
discretizations. First, in the continuous-time setting, we use this framework
to establish new parabolic Harnack inequalities for ULD. These are the first
Harnack inequalities that decay to zero in contractive settings, thereby
reflecting the convergence properties of ULD in addition to just its regularity
properties.
  Second, we build upon these Harnack inequalities to develop a local error
framework for analyzing discretizations of ULD in KL divergence. This extends
our framework in part III from uniformly elliptic diffusions to degenerate
diffusions, and shares its virtues: the framework is user-friendly, applies to
sophisticated discretization schemes, and does not require contractivity.
Applying this framework to the randomized midpoint discretization of ULD
establishes (i) the first ballistic acceleration result for log-concave
sampling (i.e., sublinear dependence on the condition number), and (ii) the
first $d^{1/3}$ iteration complexity guarantee for sampling to constant total
variation error in dimension $d$.

</details>


### [80] [A notion of BSDE on the Wasserstein space and its applications to control problems and PDEs](https://arxiv.org/abs/2506.23177)
*Mao Fabrice Djete*

Main category: math.PR

TL;DR: The paper introduces a new class of BSDEs on the Wasserstein space, extending classical BSDEs to the mean-field setting, linking them to control problems and PDEs on probability measures.


<details>
  <summary>Details</summary>
Motivation: Classical BSDEs are inadequate for mean-field settings, motivating a new measure-dependent framework.

Method: Defines BSDEs on Wasserstein space, establishes correspondence with mean-field control and PDEs, and proves uniqueness via a comparison principle.

Result: Existence shown for linear/quadratic generators; uniqueness ensured.

Conclusion: Provides a probabilistic approach for control and analysis on probability measure spaces.

Abstract: We introduce a class of backward stochastic differential equations (BSDEs) on
the Wasserstein space of probability measures. This formulation extends the
classical correspondence between BSDEs, stochastic control, and partial
differential equations (PDEs) to the mean--field (McKean--Vlasov) setting,
where the dynamics depend on the law of the state process. The standard BSDE
framework becomes inadequate in this context, motivating a new definition in
terms of measure--dependent solutions.
  Under suitable assumptions, we demonstrate that this formulation is in
correspondence with both mean--field control problems and partial differential
equations defined on the Wasserstein space. A comparison principle is
established to ensure uniqueness, and existence results are obtained for
generators that are linear or quadratic in the $z$--variable. This framework
provides a probabilistic approach to control and analysis on the space of
probability measures.

</details>


<div id='math.NT'></div>

# math.NT [[Back]](#toc)

### [81] [A Rigorous Error Bound for the TG Kernel in Prime Counting](https://arxiv.org/abs/2506.22634)
*Bugra Kilictas,Faruk Alpay*

Main category: math.NT

TL;DR: The paper establishes rigorous error bounds for prime counting using a truncated Gaussian kernel, ensuring exact computation of π(x) without unproven hypotheses. It demonstrates efficiency for large-scale computations and connects analytic number theory with practical algorithms.


<details>
  <summary>Details</summary>
Motivation: The motivation is to bridge analytic number theory and practical computation by providing a reliable method for exact prime counting at large scales, avoiding reliance on unproven hypotheses.

Method: The method involves using a truncated Gaussian kernel with compact support and vanishing moments, combined with explicit error bounds for tail truncation, zero-sum truncation, and trivial zero contributions.

Result: For x with 10^8 decimal digits, the method achieves the error bound with ~1200 nontrivial zeta zeros, enabling fast computation. All constants are explicit for verifiability.

Conclusion: The work demonstrates how classical analytic techniques, when combined with modern computation, yield practical algorithms for theoretical problems, opening new research avenues in computational number theory.

Abstract: We establish rigorous error bounds for prime counting using a truncated
Gaussian (TG) kernel in the explicit formula framework. Our main theorem proves
that the approximation error remains globally below 1/2 for all sufficiently
large arguments, guaranteeing exact computation of {\pi}(x) through simple
rounding, without relying on unproven hypotheses.
  The TG kernel construction employs Gaussian-like test functions with compact
support, engineered with vanishing moments to eliminate main terms. For x with
10^8 decimal digits, we demonstrate that only ~1200 nontrivial zeta zeros
suffice to achieve the error bound, enabling computation in seconds on modern
hardware - a dramatic improvement over classical methods.
  Key contributions include: (1) Explicit tail truncation bounds using Taylor
remainder analysis, showing exponential decay; (2) Zero-sum truncation error
bounds via unconditional density estimates; (3) Rigorous treatment of trivial
zero contributions. All constants are made explicit, ensuring full
verifiability.
  The method bridges analytic number theory and practical computation, with
potential applications to record-breaking prime counting computations. We
discuss algorithmic implications including FFT-based arithmetic for ~330
million bit numbers. The framework's flexibility suggests connections to deeper
structures in prime distribution, particularly regarding optimized kernel
designs and the interplay between smoothing parameters {\alpha} and truncation
heights.
  This work exemplifies how classical analytic techniques, when carefully
implemented with modern computational perspectives, yield practical algorithms
for problems previously considered purely theoretical. The rigorous error
analysis ensures reliability even at astronomical scales, opening new avenues
for computational number theory research.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [82] [Protein Drift-Diffusion in Membranes with Non-equilibrium Fluctuations arising from Gradients in Concentration or Temperature](https://arxiv.org/abs/2506.22695)
*D. Jasuja,P. J. Atzberger*

Main category: cond-mat.soft

TL;DR: The paper explores protein dynamics in heterogeneous cell membranes using hybrid simulation methods to study non-equilibrium effects like concentration gradients and thermal sensing.


<details>
  <summary>Details</summary>
Motivation: To understand protein positioning, patterning, and thermal gradient sensing in cell membranes under non-equilibrium conditions.

Method: Develops stochastic hybrid continuum-discrete simulation methods based on non-equilibrium statistical mechanics to track protein dynamics, concentration fluctuations, and thermal exchanges.

Result: Provides self-consistent models for studying drift-diffusion dynamics of proteins and energy exchanges in membranes.

Conclusion: The methods are applicable to other biological systems and soft materials for studying non-equilibrium phenomena.

Abstract: We investigate proteins within heterogeneous cell membranes where
non-equilibrium phenomena arises from spatial variations in concentration and
temperature. We develop simulation methods building on non-equilibrium
statistical mechanics to obtain stochastic hybrid continuum-discrete
descriptions which track individual protein dynamics, spatially varying
concentration fluctuations, and thermal exchanges. We investigate biological
mechanisms for protein positioning and patterning within membranes and factors
in thermal gradient sensing. We also study the kinetics of Brownian motion of
particles with temperature variations within energy landscapes arising from
heterogeneous microstructures within membranes. The introduced approaches
provide self-consistent models for studying biophysical mechanisms involving
the drift-diffusion dynamics of individual proteins and energy exchanges and
fluctuations between the thermal and mechanical parts of the system. The
methods also can be used for studying related non-equilibrium effects in other
biological systems and soft materials.

</details>


<div id='math.AG'></div>

# math.AG [[Back]](#toc)

### [83] [Efficient Tensor Decomposition via Moment Matrix Extension](https://arxiv.org/abs/2506.22564)
*Bobby Shi,Julia Lindberg,Joe Kileel*

Main category: math.AG

TL;DR: The paper improves the efficiency of the moment matrix extension algorithm for symmetric tensor CP decomposition by focusing on regularity, reducing complexity, and enabling efficient decomposition for certain tensor classes.


<details>
  <summary>Details</summary>
Motivation: Recent work on efficient tensor decomposition algorithms inspired the study to enhance the moment matrix extension algorithm's efficiency under specific conditions.

Method: The paper analyzes the regularity of target decompositions to reduce algorithm complexity and shows that for certain tensors, decomposition can be reduced to solving linear systems. It also provides computer-assisted proofs for conjectures.

Result: For order-4 tensors, generic tensors of rank up to r=2n+1 can be decomposed efficiently, exceeding previous rank thresholds. Nonidentifiable tensors, like monomials, can also be decomposed efficiently.

Conclusion: The study advances tensor decomposition efficiency, particularly for symmetric tensors, and provides practical implementations and examples.

Abstract: Motivated by a flurry of recent work on efficient tensor decomposition
algorithms, we show that the celebrated moment matrix extension algorithm of
Brachat, Comon, Mourrain, and Tsigaridas for symmetric tensor canonical
polyadic (CP) decomposition can be made efficient under the right conditions.
We first show that the crucial property determining the complexity of the
algorithm is the regularity of a target decomposition. This allows us to reduce
the complexity of the vanilla algorithm, while also unifying results from
previous works. We then show that for tensors in $S^d\mathbb{C}^{n+1}$ with $d$
even, low enough regularity can reduce finding a symmetric tensor decomposition
to solving a system of linear equations. For order-$4$ tensors we prove that
generic tensors of rank up to $r=2n+1$ can be decomposed efficiently via moment
matrix extension, exceeding the rank threshold allowed by simultaneous
diagonalization. We then formulate a conjecture that states for generic
order-$4$ tensors of rank $r=O(n^2)$ the induced linear system is sufficient
for efficient tensor decomposition, matching the asymptotics of existing
algorithms and in fact improving the leading coefficient. Towards this
conjecture we give computer assisted proofs that the statement holds for $n=2,
\dots, 17$. Next we demonstrate that classes of nonidentifiable tensors can be
decomposed efficiently via the moment matrix extension algorithm, bypassing the
usual need for uniqueness of decomposition. Of particular interest is the class
of monomials, for which the extension algorithm is not only efficient but also
improves on existing theory by explicitly parameterizing the space of
decompositions. Code for implementations of the efficient algorithm for generic
tensors and monomials are provided, along with several numerical examples.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [84] [Differentiable Radar Ambiguity Functions: Mathematical Formulation and Computational Implementation](https://arxiv.org/abs/2506.22935)
*Marc Bara Iniesta*

Main category: eess.SP

TL;DR: The paper introduces GRAF, a differentiable radar ambiguity function framework, enabling gradient-based optimization and integration with modern machine learning for radar waveform design.


<details>
  <summary>Details</summary>
Motivation: Traditional radar ambiguity functions are non-differentiable, limiting their use with gradient-based methods and modern ML frameworks.

Method: GRAF reformulates the ambiguity function using Wirtinger calculus, parallelized FFTs, and numerical stability measures to ensure differentiability.

Result: GRAF enables gradient flow, compatibility with automatic differentiation, and supports applications like neural waveform generation and radar system optimization.

Conclusion: GRAF bridges classical radar theory with modern ML, providing a foundation for advanced radar waveform design.

Abstract: The ambiguity function is fundamental to radar waveform design,
characterizing range and Doppler resolution capabilities. However, its
traditional formulation involves non-differentiable operations, preventing
integration with gradient-based optimization methods and modern machine
learning frameworks. This paper presents the first complete mathematical
framework and computational implementation for differentiable radar ambiguity
functions. Our approach addresses the fundamental technical challenges that
have prevented the radar community from leveraging automatic differentiation:
proper handling of complex-valued gradients using Wirtinger calculus, efficient
computation through parallelized FFT operations, numerical stability throughout
cascaded operations, and composability with arbitrary differentiable
operations. We term this approach GRAF (Gradient-based Radar Ambiguity
Functions), which reformulates the ambiguity function computation to maintain
mathematical equivalence while enabling gradient flow through the entire
pipeline. The resulting implementation provides a general-purpose
differentiable ambiguity function compatible with modern automatic
differentiation frameworks, enabling new research directions including neural
network-based waveform generation with ambiguity constraints, end-to-end
optimization of radar systems, and integration of classical radar theory with
modern deep learning. We provide complete implementation details and
demonstrate computational efficiency suitable for practical applications. This
work establishes the mathematical and computational foundation for applying
modern machine learning techniques to radar waveform design, bridging classical
radar signal processing with automatic differentiation frameworks.

</details>


<div id='math.CV'></div>

# math.CV [[Back]](#toc)

### [85] [On the zero sets of harmonic polynomials](https://arxiv.org/abs/2506.24116)
*Ioann Vasilyev*

Main category: math.CV

TL;DR: The paper solves Problem 151 from the Scottish Book, constructs a nonzero harmonic function vanishing on cube edges, and extends prior results on harmonic polynomials.


<details>
  <summary>Details</summary>
Motivation: To address longstanding problems in harmonic analysis, particularly R. Wavre's 1936 problem, and explore harmonic functions with specific vanishing properties.

Method: Constructs harmonic functions and polynomials, using harmonic morphisms, to vanish on specific subvarieties like cube edges and subspaces.

Result: A nonzero harmonic function vanishing on cube edges is constructed, and new families of harmonic polynomials are developed, extending previous work.

Conclusion: The paper advances understanding of harmonic functions with prescribed vanishing sets, solving a classic problem and introducing new constructions.

Abstract: In this paper we consider nonzero harmonic functions vanishing on certain
subvarieties. Among other things, we give a positive solution to the Problem
151 from the Scottish Book posed by R. Wavre in 1936. In more detail, we
construct a nonzero harmonic function in the whole space that vanishes on the
edges of the unit cube. Moreover, using harmonic morphisms we construct certain
new nontrivial families of harmonic polynomials that vanish at the same set in
the unit ball in n-dimensional space for all n greater than or equal to 4. This
extends results of Logunov and Malinnikova. We also present some (presumably)
new results on harmonic functions in the space whose nodal sets are unions of
(affine) subspaces.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [86] [Sociophysics models inspired by the Ising model](https://arxiv.org/abs/2506.23837)
*Pratik Mullick,Parongama Sen*

Main category: physics.soc-ph

TL;DR: The paper reviews the adaptation of the Ising model to sociophysical systems, showing its effectiveness in modeling collective behaviors like opinion dynamics and epidemic spreading.


<details>
  <summary>Details</summary>
Motivation: To demonstrate the versatility of the Ising model in capturing collective phenomena in sociophysical systems.

Method: Review and analysis of Ising-like models applied to areas such as opinion dynamics, financial markets, and social segregation.

Result: The Ising model effectively captures key features of collective behavior, including phase transitions and consensus formation.

Conclusion: The Ising model remains highly relevant for analyzing complex social systems, with potential for future research in sociphysics.

Abstract: The Ising model, originally developed for understanding magnetic phase
transitions, has become a cornerstone in the study of collective phenomena
across diverse disciplines. In this review, we explore how Ising and Ising-like
models have been successfully adapted to sociophysical systems, where
binary-state agents mimic human decisions or opinions. By focusing on key areas
such as opinion dynamics, financial markets, social segregation, game theory,
language evolution, and epidemic spreading, we demonstrate how the models
describing these phenomena, inspired by the Ising model, capture essential
features of collective behavior, including phase transitions, consensus
formation, criticality, and metastability. In particular, we emphasize the role
of the dynamical rules of evolution in the different models that often converge
back to Ising-like universality. We end by outlining the future directions in
sociphysics research, highlighting the continued relevance of the Ising model
in the analysis of complex social systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [87] [BWLer: Barycentric Weight Layer Elucidates a Precision-Conditioning Tradeoff for PINNs](https://arxiv.org/abs/2506.23024)
*Jerry Liu,Yasa Baig,Denise Hui Jean Lee,Rajat Vadiraj Dwaraknath,Atri Rudra,Chris Ré*

Main category: cs.LG

TL;DR: The paper investigates precision limitations in Physics-informed neural networks (PINNs) and introduces the Barycentric Weight Layer (BWLer) to address these issues, achieving significant accuracy improvements in solving PDEs.


<details>
  <summary>Details</summary>
Motivation: PINNs struggle with machine-precision accuracy for PDEs, prompting an investigation into whether the issue stems from PDE ill-conditioning or MLP architecture limitations.

Method: The BWLer, a barycentric polynomial interpolation layer, is introduced to either augment or replace MLPs, separating solution representation from PDE loss derivatives. Spectral derivatives and preconditioning are used to manage the tradeoff between accuracy and PDE loss conditioning.

Result: BWLer improves RMSE by up to 30x for convection, 10x for reaction, and 1800x for wave equations. Explicit BWLer achieves near-machine-precision on some problems, outperforming standard PINNs.

Conclusion: BWLer combines PINN flexibility with classical solver precision, offering a practical solution for high-accuracy PDE solving.

Abstract: Physics-informed neural networks (PINNs) offer a flexible way to solve
partial differential equations (PDEs) with machine learning, yet they still
fall well short of the machine-precision accuracy many scientific tasks demand.
In this work, we investigate whether the precision ceiling comes from the
ill-conditioning of the PDEs or from the typical multi-layer perceptron (MLP)
architecture. We introduce the Barycentric Weight Layer (BWLer), which models
the PDE solution through barycentric polynomial interpolation. A BWLer can be
added on top of an existing MLP (a BWLer-hat) or replace it completely
(explicit BWLer), cleanly separating how we represent the solution from how we
take derivatives for the PDE loss. Using BWLer, we identify fundamental
precision limitations within the MLP: on a simple 1-D interpolation task, even
MLPs with O(1e5) parameters stall around 1e-8 RMSE -- about eight orders above
float64 machine precision -- before any PDE terms are added. In PDE learning,
adding a BWLer lifts this ceiling and exposes a tradeoff between achievable
accuracy and the conditioning of the PDE loss. For linear PDEs we fully
characterize this tradeoff with an explicit error decomposition and navigate it
during training with spectral derivatives and preconditioning. Across five
benchmark PDEs, adding a BWLer on top of an MLP improves RMSE by up to 30x for
convection, 10x for reaction, and 1800x for wave equations while remaining
compatible with first-order optimizers. Replacing the MLP entirely lets an
explicit BWLer reach near-machine-precision on convection, reaction, and wave
problems (up to 10 billion times better than prior results) and match the
performance of standard PINNs on stiff Burgers' and irregular-geometry Poisson
problems. Together, these findings point to a practical path for combining the
flexibility of PINNs with the precision of classical spectral solvers.

</details>


### [88] [Faster Diffusion Models via Higher-Order Approximation](https://arxiv.org/abs/2506.24042)
*Gen Li,Yuchen Zhou,Yuting Wei,Yuxin Chen*

Main category: cs.LG

TL;DR: A training-free sampling algorithm for diffusion models achieves provable acceleration without retraining, requiring fewer score function evaluations for accurate distribution approximation.


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency of diffusion models by reducing the computational cost of sampling without additional training or restrictive assumptions on data distributions.

Method: Proposes a principled, training-free algorithm inspired by high-order ODE solvers, using high-order Lagrange interpolation and successive refinement to approximate the probability flow ODE integral.

Result: The algorithm requires only O(d^{1+2/K} ε^{-1/K}) score function evaluations (up to log factor) for accurate approximation, robust to inexact score estimation.

Conclusion: The method provides a scalable and efficient solution for diffusion models, applicable to a broad class of distributions without smoothness or log-concavity assumptions.

Abstract: In this paper, we explore provable acceleration of diffusion models without
any additional retraining. Focusing on the task of approximating a target data
distribution in $\mathbb{R}^d$ to within $\varepsilon$ total-variation
distance, we propose a principled, training-free sampling algorithm that
requires only the order of
  $$ d^{1+2/K} \varepsilon^{-1/K} $$
  score function evaluations (up to log factor) in the presence of accurate
scores, where $K$ is an arbitrarily large fixed integer. This result applies to
a broad class of target data distributions, without the need for assumptions
such as smoothness or log-concavity. Our theory is robust vis-a-vis inexact
score estimation, degrading gracefully as the score estimation error increases
-- without demanding higher-order smoothness on the score estimates as assumed
in previous work. The proposed algorithm draws insight from high-order ODE
solvers, leveraging high-order Lagrange interpolation and successive refinement
to approximate the integral derived from the probability flow ODE.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [89] [Computing excited eigenstates using inexact Lanczos methods and tree tensor network states](https://arxiv.org/abs/2506.22574)
*Madhumita Rano,Henrik R. Larsson*

Main category: physics.chem-ph

TL;DR: The paper introduces an inexact Lanczos method combined with tree tensor network states (TTNS) to compute excited eigenstates in quantum many-body systems, demonstrating its effectiveness on three complex problems.


<details>
  <summary>Details</summary>
Motivation: Excited eigenstates are essential for understanding quantum many-body dynamics, but their computation remains challenging despite tensor network states being effective for ground states.

Method: The authors develop an inexact Lanczos method tailored for TTNS to efficiently compute excited states.

Result: The method successfully computes excited vibrational states for three challenging systems: acetonitrile, the Zundel ion, and the Eigen ion, showcasing its accuracy and applicability.

Conclusion: The proposed TTNS inexact Lanczos method is versatile and directly applicable to other quantum many-body systems.

Abstract: Excited eigenstates are crucial to understand the dynamics of quantum
many-body systems. Tensor network states are one of the workhorses to compute
ground states of many-body systems, yet the accurate computation of excited
eigenstates is still challenging. Here, we develop a combination of the inexact
Lanczos method, which aims at efficiently computing excited states, to tree
tensor network states (TTNSs). We demonstrate our approach by computing excited
vibrational states for three challenging problems: (1) 84 states in different
energy intervals of acetonitrile (12-dimensional), (2) Fermi resonance states
of the fluxional Zundel ion (15-dimensional), and (3) selected excited states
of the fluxional and very correlated Eigen ion (33-dimensional). The proposed
TTNS inexact Lanczos method is directly applicable to other quantum many-body
systems.

</details>


### [90] [Insights into Ionic Diffusion in C-S-H Gel Pore from MD Simulations: Spatial Distributions, Energy Barriers, and Structural Descriptor](https://arxiv.org/abs/2506.23089)
*Weiqiang Chen,Kai Gong*

Main category: physics.chem-ph

TL;DR: The paper investigates molecular transport in nanoconfined C-S-H pores using MD simulations, revealing spatial variations in diffusivity and activation energy, and introduces a structural descriptor (TCS) to predict mobility near interfaces.


<details>
  <summary>Details</summary>
Motivation: Understanding transport in nanoconfined environments is crucial for natural and engineering systems, like cementitious materials, but molecular-level mechanisms are unclear.

Method: MD simulations analyzed Na+, Cl-, and water diffusion in a 4 nm C-S-H pore channel across 300 K to 360 K, using spatially resolved and Arrhenius analyses.

Result: Diffusivity is suppressed near interfaces, recovering toward the pore center. A mechanistic transition from structure-controlled to hydrodynamics-controlled transport was observed, with TCS predicting mobility near interfaces.

Conclusion: Tailoring nanochannel structure and interfacial chemistry in cementitious gels could enhance durability by suppressing ionic ingress.

Abstract: Understanding transport behavior in nanoconfined environments is critical to
many natural and engineering systems, including cementitious materials, yet its
molecular-level mechanisms remain poorly understood. Here, molecular dynamics
(MD) simulations were used to investigate Na+, Cl-, and water diffusion inside
a 4 nm calcium-silicate-hydrate (C-S-H) pore channel over temperatures ranging
from 300 K to 360 K. Spatially resolved analysis revealed strong suppression of
diffusivity near the solid-liquid interface and gradual recovery toward the
pore center. Arrhenius analysis further quantified the spatial variation of
activation energy barriers and intrinsic mobilities across the pore channel,
showing distinct confinement effects. The spatially resolved structural
analysis uncovers a mechanistic transition from structure-controlled to
hydrodynamics-controlled transport regimes with increasing distance from the
pore surface. A structural descriptor, total coordination strength (TCS), was
introduced, providing a predictive link between local liquid structure and
molecular mobility within approximately 1 nm of the interface. Beyond 1 nm,
suppressed diffusivities were well captured by an exponential decay model based
on the Darcy-Brinkman framework. To the best of our knowledge, this is the
first MD study to comprehensively resolve the spatial heterogeneity of
transport, thermal kinetics, and structure within cementitious nanopores. These
findings deepen the fundamental understanding of nanoscale transport phenomena
and suggest that tailoring the nanochannel structure and interfacial chemistry
of cementitious gels, such as surface coordination environments, pore size
distributions, and adsorption sites, may offer a promising strategy to suppress
ionic ingress and enhance the durability of cement-based materials.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [91] [Lower bounds for trace estimation via Block Krylov and other methods](https://arxiv.org/abs/2506.22701)
*Shi Jie Yu*

Main category: math.ST

TL;DR: The paper explores theoretical bounds for estimating the trace of matrix functions using Hutchinson's method and Block Krylov techniques, linking Krylov steps to polynomial approximation degrees.


<details>
  <summary>Details</summary>
Motivation: To understand the theoretical limits and efficiency of trace estimation methods for matrix functions, particularly focusing on polynomial approximation and Krylov subspace techniques.

Method: Uses Hutchinson's method combined with Block Krylov techniques to approximate matrix-vector products and analyzes polynomial approximations for functions like $A^{-1/2}$ and $A^{-1}$.

Result: Derives upper bounds on Krylov steps for specific functions and lower limits on queries for trace estimation, especially for Wishart matrices.

Conclusion: The study connects Krylov method steps to polynomial approximation degrees, highlighting the relationship between computational cost and fundamental limits in approximation.

Abstract: This paper studies theoretical lower bounds for estimating the trace of a
matrix function, $\text{tr}(f(A))$, focusing on methods that use Hutchinson's
method along with Block Krylov techniques. These methods work by approximating
matrix-vector products like $f(A)V$ using a Block Krylov subspace. This is
closely related to approximating functions with polynomials. We derive
theoretical upper bounds on how many Krylov steps are needed for functions such
as $A^{-1/2}$ and $A^{-1}$ by analyzing the upper bounds from the polynomial
approximation of their scalar equivalent. In addition, we also develop lower
limits on the number of queries needed for trace estimation, specifically for
$\text{tr}(W^{-p})$ where $W$ is a Wishart matrix. Our study clarifies the
connection between the number of steps in Block Krylov methods and the degree
of the polynomial used for approximation. This links the total cost of trace
estimation to basic limits in polynomial approximation and how much information
is needed for the computation.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [92] [QMetric: Benchmarking Quantum Neural Networks Across Circuits, Features, and Training Dimensions](https://arxiv.org/abs/2506.23765)
*Silvie Illésová,Tomasz Rybotycki,Martin Beseda*

Main category: quant-ph

TL;DR: QMetric is a Python package for evaluating hybrid quantum-classical models with interpretable metrics like circuit fidelity and training stability.


<details>
  <summary>Details</summary>
Motivation: The need for tools to assess hybrid quantum-classical models beyond raw accuracy.

Method: QMetric provides metrics for circuit expressibility, feature representations, and training dynamics, integrating with Qiskit and PyTorch.

Result: Demonstrated via a case study on binary MNIST classification comparing classical and quantum-enhanced models.

Conclusion: QMetric offers a comprehensive toolkit for evaluating quantum models, with code and resources available on GitLab.

Abstract: As hybrid quantum-classical models gain traction in machine learning, there
is a growing need for tools that assess their effectiveness beyond raw
accuracy. We present QMetric, a Python package offering a suite of
interpretable metrics to evaluate quantum circuit expressibility, feature
representations, and training dynamics. QMetric quantifies key aspects such as
circuit fidelity, entanglement entropy, barren plateau risk, and training
stability. The package integrates with Qiskit and PyTorch, and is demonstrated
via a case study on binary MNIST classification comparing classical and
quantum-enhanced models. Code, plots, and a reproducible environment are
available on GitLab.

</details>


### [93] [High-Performance Contraction of Quantum Circuits for Riemannian Optimization](https://arxiv.org/abs/2506.23775)
*Fabian Putterer,Max M. Zumpe,Isabel Nha Minh Le,Qunsheng Huang,Christian B. Mendl*

Main category: quant-ph

TL;DR: The paper presents a method for optimizing quantum circuit gates using Riemannian optimization, avoiding large matrix storage by leveraging state vectors and HPC kernels.


<details>
  <summary>Details</summary>
Motivation: To efficiently approximate unitary time evolution in quantum circuits without the computational burden of large matrix storage.

Method: Uses Riemannian trust-region optimization, matrix-free algorithmic framework, and HPC-optimized kernels for gate operations and gradient/Hessian computations.

Result: Demonstrated nearly linear parallelization speed-up on the Fermi-Hubbard model with 16 sites and compared favorably with a matrix product operator-based approach.

Conclusion: The method is efficient and scalable, offering a practical solution for optimizing quantum circuits while conserving computational resources.

Abstract: This work focuses on optimizing the gates of a quantum circuit with a given
topology to approximate the unitary time evolution governed by a Hamiltonian.
Recognizing that unitary matrices form a mathematical manifold, we employ
Riemannian optimization methods -- specifically the Riemannian trust-region
algorithm -- which involves second derivative calculations with respect to the
gates. Our key technical contribution is a matrix-free algorithmic framework
that avoids the explicit construction and storage of large unitary matrices
acting on the whole Hilbert space. Instead, we evaluate all quantities as sums
over state vectors, assuming that these vectors can be stored in memory. We
develop HPC-optimized kernels for applying gates to state vectors and for the
gradient and Hessian computation. Further improvements are achieved by
exploiting sparsity structures due to Hamiltonian conservation laws, such as
parity conservation, and lattice translation invariance. We benchmark our
implementation on the Fermi-Hubbard model with up to 16 sites, demonstrating a
nearly linear parallelization speed-up with up to 112 CPU threads. Finally, we
compare our implementation with an alternative matrix product operator-based
approach.

</details>


<div id='math.FA'></div>

# math.FA [[Back]](#toc)

### [94] [Uncentered Fractional Maximal functions and mean oscillation spaces associated with dyadic Hausdorff content](https://arxiv.org/abs/2506.23206)
*Riju Basak,You-Wei Benson Chen,Prasun Roychowdhury*

Main category: math.FA

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study the action of uncentered fractional maximal functions on mean
oscillation spaces associated with the dyadic Hausdorff content
$\mathcal{H}_{\infty}^{\beta}$ with $0<\beta\leq n$. For $0 < \alpha < n$, we
refine existing results concerning the action of the Euclidean uncentered
fractional maximal function $\mathcal{M}_{\alpha}$ on the functions of bounded
mean oscillations (BMO) and vanishing mean oscillations (VMO). In addition, for
$0 < \beta_1 \leq \beta_2 \leq n$, we establish the boundedness of the
$\beta_2$-dimensional uncentered maximal function $\mathcal{M}^{\beta_2}$ on
the space $\text{BMO}^{\beta_1}(\mathbb{R}^n)$, where
$\text{BMO}^{\beta_1}(\mathbb{R}^n)$ denotes the mean oscillation space adapted
to the dyadic Hausdorff content $\mathcal{H}_{\infty}^{\beta_1}$ on
$\mathbb{R}^n$.

</details>


### [95] [Weighted inequalities involving two Hardy operators](https://arxiv.org/abs/2506.23324)
*Amiran Gogatishvili,Tugce Ünver*

Main category: math.FA

TL;DR: The paper establishes necessary and sufficient conditions on weights for a weighted integral inequality to hold, using a discretization method.


<details>
  <summary>Details</summary>
Motivation: To generalize and improve earlier results on weighted integral inequalities by removing restrictions and providing precise conditions.

Method: A recently developed discretization method is applied to analyze the inequality.

Result: Necessary and sufficient conditions on the weights are derived for the inequality to hold.

Conclusion: The method successfully overcomes limitations of previous approaches, providing a complete solution for the given inequality.

Abstract: We find necessary and sufficient conditions on weights $u_1, u_2, v_1, v_2$,
i.e. measurable, positive, and finite, a.e. on $(a,b)$, for which there exists
a positive constant $C$ such that for given $0 < p_1,q_1,p_2,q_2 <\infty$ the
inequality \begin{equation*} \begin{split} \bigg(\int_a^b \bigg(\int_a^t
f(s)^{p_2} v_2(s)^{p_2} ds\bigg)^{\frac{q_2}{p_2}} u_2(t)^{q_2} dt
\bigg)^{\frac{1}{q_2}}& \\ & \hspace{-3cm}\le C \bigg(\int_a^b \bigg(\int_a^t
f(s)^{p_1} v_1(s)^{p_1} ds\bigg)^{\frac{q_1}{p_1}} u_1(t)^{q_1} dt
\bigg)^{\frac{1}{q_1}} \end{split} \end{equation*} holds for every
non-negative, measurable function $f$ on $(a,b)$, where $0 \le a <b \le
\infty$. The proof is based on a recently developed discretization method that
enables us to overcome the restrictions of the earlier results.

</details>


<div id='physics.space-ph'></div>

# physics.space-ph [[Back]](#toc)

### [96] [The impacts of tropospheric gravity wave-generated MSTIDs on skywaves at middle latitude North American sector observed and modeled using SuperDARN HF radars](https://arxiv.org/abs/2506.23984)
*S. Chakraborty,P. A. Inchin,S. Debchoudhury,C. Heale,B. Bergsson,M. Zettergren,J. M. Ruohoniemi*

Main category: physics.space-ph

TL;DR: The paper studies HF radar response to gravity waves in the mid-latitude ionosphere, linking thunderstorm activity to MSTIDs and their impact on skywaves.


<details>
  <summary>Details</summary>
Motivation: To understand the impact of thunderstorm-driven gravity waves on ionospheric disturbances and HF radar observations.

Method: Combined SuperDARN HF radar observations with model simulations (MAGIC, GEMINI, PHaRLAP) to analyze GW-generated MSTIDs.

Result: Observed semi-concentric GWs created MSTIDs at F-region heights, with model simulations matching SuperDARN observations. MSTIDs may form plasma waveguides.

Conclusion: Thunderstorm-driven GWs significantly affect ionospheric HF radar observations, with MSTIDs potentially enabling long ducting of rays.

Abstract: Trans-ionospheric high frequency (HF: 3-30 MHz) response to gravity waves
(GWs) is studied in the middle-latitude ionosphere in relation to thunderstorm
activity. SuperDARN HF radar observations are compared against the model
simulations to quantify the impact of GW-generated MSTID (medium-scale
traveling ionospheric disturbances) activity on the skywaves traveling through
ionospheric F-region heights. The tropospheric thunderstorm-driven convective
source is modeled using MAGIC. The outputs are coupled with GEMINI to model
ionospheric plasma response, which is then used to model SuperDARN HF radar
observations using the PHaRLAP raytracing tool. Semi-concentric GWs were
observed at different atmospheric heights, creating MSTIDs at F-region heights.
PHaRLAP raytracing through the modeled ionosphere shows great qualitative
agreement with SuperDARN daytime ground scatter observations. Modeled rays show
possibilities of long ducting Pedersen rays, suggesting MSTID can create a
plasma waveguide to duct rays at the F-region height.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [97] [Scalable Bayesian Optimization for High-Dimensional Coarse-Grained Model Parameterization](https://arxiv.org/abs/2506.22533)
*Carlos A. Martins Junior,Daniela A. Damasceno,Keat Yung Hue,Caetano R. Miranda,Erich A. Müller,Rodrigo A. Vargas-Hernández*

Main category: cond-mat.mtrl-sci

TL;DR: Bayesian Optimization (BO) is successfully applied to optimize a high-dimensional coarse-grained (CG) model of Pebax-1657, achieving accurate reproduction of atomistic properties in fewer than 600 iterations.


<details>
  <summary>Details</summary>
Motivation: Traditional hybrid parameterization strategies for CG models limit joint optimization of all parameters, and BO is often considered unsuitable for high-dimensional problems. This study challenges that assumption.

Method: BO is extended to optimize a 41-parameter CG model of Pebax-1657, targeting density, radius of gyration, and glass transition temperature.

Result: The optimized CG model accurately reproduces key physical properties of its atomistic counterpart, converging in fewer than 600 iterations with consistent improvements.

Conclusion: BO is viable for high-dimensional CG model optimization, offering an efficient alternative to traditional methods.

Abstract: Coarse-grained (CG) force field models are extensively utilised in material
simulations due to their scalability. Traditionally, these models are
parameterized using hybrid strategies that integrate top-down and bottom-up
approaches; however, this combination restricts the capacity to jointly
optimize all parameters. While Bayesian Optimization (BO) has been explored as
an alternative search strategy for identifying optimal parameters, its
application has traditionally been limited to low-dimensional problems. This
has contributed to the perception that BO is unsuitable for more realistic CG
models, which often involve a large number of parameters. In this study, we
challenge this assumption by successfully extending BO to optimize a
high-dimensional CG model. Specifically, we show that a 41-parameter CG model
of Pebax-1657, a copolymer composed of alternating polyamide and polyether
segments, can be effectively parameterized using BO, resulting in a model that
accurately reproduces key physical properties of its atomistic counterpart. Our
optimization framework simultaneously targets density, radius of gyration, and
glass transition temperature. It achieves convergence in fewer than 600
iterations, resulting in a CG model that shows consistent improvements across
all three properties.

</details>


### [98] [Accelerated discovery and design of Fe-Co-Zr magnets with tunable magnetic anisotropy through machine learning and parallel computing](https://arxiv.org/abs/2506.22627)
*Weiyi Xia,Maxim Moraru,Ying Wai Li,Timothy Liao,James R. Chelikowsky,Cai-Zhuang Wang*

Main category: cond-mat.mtrl-sci

TL;DR: A machine learning-assisted framework identified new rare earth-free hard magnetic materials in the Fe-Co-Zr system, revealing stable and metastable phases with potential for optimization.


<details>
  <summary>Details</summary>
Motivation: To find sustainable alternatives to rare earth-containing magnets for energy and electronics.

Method: Combined crystal graph convolutional neural network (CGCNN) with first-principles calculations to explore Fe-Co-Zr compositions.

Result: Discovered 9 stable and 81 metastable phases, with Fe6Co17Zr6 optimized into Fe5Co18Zr6 (strong anisotropy) and Fe5Co16Zr6Mn4.

Conclusion: The approach efficiently identifies promising rare earth-free magnetic materials, enabling further optimization.

Abstract: Rare earth (RE)-free permanent magnets, as alternative substitutes for
RE-containing magnets for sustainable energy technologies and modern
electronics, have attracted considerable interest. We performed a comprehensive
search for new hard magnetic materials in the ternary Fe-Co-Zr space by
leveraging a scalable, machine learning-assisted materials discovery framework
running on GPU-enabled exascale computing resources. This framework integrates
crystal graph convolutional neural network (CGCNN) machine learning (ML) method
with first-principles calculations to efficiently navigate the vast
composition-structure space. The efficiency and accuracy of the ML approach
enable us to reveal 9 new thermodynamically stable ternary Fe-Co-Zr compounds
and 81 promising low-energy metastable phases with their formation energies
within 0.1 eV/atom above the convex hull. The predicted compounds span a wide
range of crystal symmetries and magnetic behaviors, providing a rich platform
for tuning functional properties. Based on the analysis of site-specific
magnetic properties, we show that the Fe6Co17Zr6 compound obtained from our ML
discovery can be further optimized by chemical doping. Chemical substitutions
lead to a ternary Fe5Co18Zr6 phase with a strong anisotropy of K1 = 1.1 MJ/m3,
and a stable quaternary magnetic Fe5Co16Zr6Mn4 compound.

</details>


### [99] [Half-metallicity and anomalous Slater-Pauling behaviour in half-Heusler CrMnSb](https://arxiv.org/abs/2506.23993)
*Himanshu Joshi,Shradhanjali Dewan,Lalrin Kima,Aldrin Lalremtluanga,Homnath Luitel,K. C. Bhamu,D. P. Rai*

Main category: cond-mat.mtrl-sci

TL;DR: CrMnSb, a half-Heusler with 18 valence electrons, defies the Slater-Pauling rule by exhibiting a half-metallic ferrimagnetic state instead of the expected nonmagnetic semiconductor behavior.


<details>
  <summary>Details</summary>
Motivation: To understand why CrMnSb deviates from the conventional Slater-Pauling semiconducting behavior despite having 18 valence electrons.

Method: Combination of density functional theory and Green's function-based multiple-scattering theory.

Result: CrMnSb shows a half-metallic, fully compensated ferrimagnetic ground state due to antiparallel alignment of Cr and Mn sublattices.

Conclusion: The anomaly arises from localized sublattice moments, enforcing ferrimagnetism despite the 18 valence electron count.

Abstract: This study provides a first-principles insight into half-Heusler CrMnSb to
understand its deviation from the conventional Slater-Pauling semiconducting
behavior. CrMnSb, having a valence electron count of 18, has been proposed to
exhibit compensated ferrimagnetic character instead of the expected nonmagnetic
semiconducting ground state. As half-Heusler systems with a valence electron
count of 18 are not known to exhibit magnetic ordering, we have investigated
the electronic and magnetic properties of CrMnSb using a combination of density
functional theory and Green's function-based multiple-scattering theory. We
show that, despite satisfying the 18 valence electron Slater-Pauling rule,
CrMnSb does not exhibit ground-state nonmagnetic semiconducting behavior.
Instead, it reveals a half-metallic, fully compensated ferrimagnetic ground
state. This anomaly originates from the presence of localized sublattice
moments, resulting from antiparallel alignment between Cr and Mn sublattices,
which enforces half-metallic ferrimagnetism despite its ideal 18 valence
electron count.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [100] [Seeding neural network quantum states with tensor network states](https://arxiv.org/abs/2506.23550)
*Ryui Kaneko,Shimpei Goto*

Main category: cond-mat.str-el

TL;DR: Efficient conversion of MPSs to restricted Boltzmann machine wave functions via CP decomposition, enabling polynomial-time ground-state calculations.


<details>
  <summary>Details</summary>
Motivation: To simplify and accelerate quantum many-body ground-state calculations by bridging MPSs and neural network quantum states.

Method: Uses CP decomposition of MPSs to generate initial neural network quantum states, tested on the transverse-field Ising model.

Result: Demonstrates efficient ground-state approximation with systematic improvement via higher CP decomposition ranks.

Conclusion: The method is effective for systems with complex nodal structures and offers scalable initial states for quantum calculations.

Abstract: We find an efficient approach to approximately convert matrix product states
(MPSs) into restricted Boltzmann machine wave functions consisting of a
multinomial hidden unit through a canonical polyadic (CP) decomposition of the
MPSs. This method allows us to generate well-behaved initial neural network
quantum states for quantum many-body ground-state calculations in polynomial
time of the number of variational parameters and systematically shorten the
distance between the initial states and the ground states with increasing the
rank of the CP decomposition. We demonstrate the efficiency of our method by
taking the transverse-field Ising model as an example and discuss possible
applications of our method to more general quantum many-body systems in which
the ground-state wave functions possess complex nodal structures.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [101] [The Dirichlet problem for the minimal surface system on smooth domains](https://arxiv.org/abs/2506.23245)
*Caiyan Li,Hengyu Zhou*

Main category: math.DG

TL;DR: Proposes a new assumption for maps involving small oscillation and $C^2$ norms, solves the Dirichlet problem for minimal surfaces via mean curvature flow under non-negative Ricci curvature, and extends results to exterior problems.


<details>
  <summary>Details</summary>
Motivation: To address the Dirichlet problem for minimal surfaces in domains with non-negative Ricci curvature, removing diameter restrictions.

Method: Uses mean curvature flow (MCF) with boundary, leveraging Bernstein-type theorems for self-shrinkers in full and half-spaces.

Result: Establishes long-time existence of MCF and solves the Dirichlet problem, including exterior cases.

Conclusion: The approach successfully extends minimal surface theory to broader geometric settings without diameter constraints.

Abstract: In this paper, we propose a new assumption (1.2) that involves a small
oscillation and $C^2$ norms for maps from smooth bounded domains into Euclidean
spaces. Furthermore, by assuming that the domain has non-negative Ricci
curvature, we establish the Dirichlet problem for the minimal surface system
via the mean curvature flow (MCF) with boundary. The long-time existence of
such flow is derived using Bernstein-type theorems of higher codimensional
self-shrinkers in the whole space and the half-space. Another novel aspect is
that our hypothesis imposes no restriction on the diameter of the domains,
which implies an existence result for an exterior Dirichlet problem of the
minimal surface system.

</details>
