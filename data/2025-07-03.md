<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 18]
- [math.AP](#math.AP) [Total: 23]
- [physics.comp-ph](#physics.comp-ph) [Total: 2]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 3]
- [cs.LG](#cs.LG) [Total: 1]
- [math.OC](#math.OC) [Total: 2]
- [math.PR](#math.PR) [Total: 1]
- [math-ph](#math-ph) [Total: 1]
- [stat.ML](#stat.ML) [Total: 1]
- [astro-ph.HE](#astro-ph.HE) [Total: 1]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [cond-mat.supr-con](#cond-mat.supr-con) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 1]
- [physics.class-ph](#physics.class-ph) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Computational Insights into Orthotropic Fracture: Crack-Tip Fields in Strain-Limiting Materials under Non-Uniform Loads](https://arxiv.org/abs/2507.01150)
*Saugata Ghosh,Dambaru Bhatta,S. M. Mallikarjunaiah*

Main category: math.NA

TL;DR: A finite element framework for analyzing crack-tip phenomena in transversely isotropic, strain-limiting elastic materials, mitigating non-physical strain singularities and advancing BVP formulation.


<details>
  <summary>Details</summary>
Motivation: To address non-physical strain singularities at crack tips and improve boundary value problem (BVP) formulation for anisotropic, strain-limiting elastic materials.

Method: Combines a nonlinear constitutive model with a numerical scheme using Picard-type linearization and continuous Galerkin finite element method.

Result: Demonstrates bounded strain magnitudes, lower crack-tip strain growth than stress growth, and concentrated strain-energy density at the crack tip.

Conclusion: The framework provides a robust basis for studying crack propagation, damage, and nucleation in anisotropic materials under diverse loading conditions.

Abstract: A finite element framework is presented for analyzing crack-tip phenomena in
transversely isotropic, strain-limiting elastic materials. Mechanical response
is characterized by an algebraically nonlinear constitutive model, relating
stress to linearized strain. Non-physical strain singularities at the crack
apex are mitigated, ensuring bounded strain magnitudes. This methodology
significantly advances boundary value problem (BVP) formulation, especially for
first-order approximate theories. For a transversely isotropic elastic solid
with a crack, the governing equilibrium equation, derived from linear momentum
balance and the nonlinear constitutive model, is reduced to a second-order,
vector-valued, quasilinear elliptic BVP. This BVP is solved using a robust
numerical scheme combining Picard-type linearization with a continuous Galerkin
finite element method for spatial discretization. Numerical results are
presented for various loading conditions, including uniform tension,
non-uniform slope, and parabolic loading, with two distinct material fiber
orientations. It is demonstrated that crack-tip strain growth is substantially
lower than stress growth. Nevertheless, strain-energy density is found to be
concentrated at the crack tip, consistent with linear elastic fracture
mechanics principles. The proposed framework provides a robust basis for
formulating physically meaningful, rigorous BVPs, critical for investigating
fundamental processes like crack propagation, damage, and nucleation in
anisotropic, strain-limiting elastic solids under diverse loading conditions.

</details>


### [2] [On the Intensity-based Inversion Method for Quantitative Quasi-Static Elastography](https://arxiv.org/abs/2507.01207)
*Ekaterina Sherina,Simon Hubmer*

Main category: math.NA

TL;DR: The paper introduces the Intensity-based Inversion Method (IIM) for estimating material parameters in quasi-static elastography, offering a one-step alternative to traditional two-step approaches. It combines image registration with model-based reconstruction for improved stability and noise resistance.


<details>
  <summary>Details</summary>
Motivation: To simplify and enhance the accuracy of material parameter estimation in elastography by avoiding the approximations and noise sensitivity of conventional two-step methods.

Method: The IIM directly computes material parameters from internal measurements using image registration and a regularized parameter reconstruction approach, bypassing intermediate steps.

Result: The IIM demonstrates improved stability against measurement noise and avoids derivative computations, with a full convergence analysis provided. Numerical examples validate its application in optical coherence elastography.

Conclusion: The IIM is a robust, one-step method for material parameter estimation in elastography, offering advantages over traditional approaches in terms of stability and computational efficiency.

Abstract: In this paper, we consider the intensity-based inversion method (IIM) for
quantitative material parameter estimation in quasi-static elastography. In
particular, we consider the problem of estimating the material parameters of a
given sample from two internal measurements, one obtained before and one after
applying some form of deformation. These internal measurements can be obtained
via any imaging modality of choice, for example ultrasound, optical coherence
or photo-acoustic tomography. Compared to two-step approaches to elastography,
which first estimate internal displacement fields or strains and then
reconstruct the material parameters from them, the IIM is a one-step approach
which computes the material parameters directly from the internal measurements.
To do so, the IIM combines image registration together with a model-based,
regularized parameter reconstruction approach. This combination has the
advantage of avoiding some approximations and derivative computations typically
found in two-step approaches, and results in the IIM being generally more
stable to measurement noise. In the paper, we provide a full convergence
analysis of the IIM within the framework of inverse problems, and detail its
application to linear elastography. Furthermore, we discuss the numerical
implementation of the IIM and provide numerical examples simulating an optical
coherence elastography (OCE) experiment.

</details>


### [3] [A fourth-order exponential time differencing scheme with real and distinct poles rational approximation for solving non-linear reaction-diffusion systems](https://arxiv.org/abs/2507.01245)
*Wisdom Kwame Attipoe,Andreas Kleefeld,Emmanuel Asante-Asamani*

Main category: math.NA

TL;DR: A fourth-order L-stable ETDRK4RDP scheme is developed for solving nonlinear reaction-diffusion systems with nonsmooth data, using RDP rational functions for efficient damping and parallelization.


<details>
  <summary>Details</summary>
Motivation: To address challenges in solving nonlinear reaction-diffusion systems with nonsmooth data, particularly spurious oscillations and computational efficiency.

Method: Approximates matrix exponentials in ETDRK4 with fourth-order L-acceptable RDP rational functions, ensuring damping of oscillations and enabling parallelization.

Result: Fourth-order accuracy verified for Dirichlet and Neumann boundary conditions; up to six times speedup in CPU time when parallelized.

Conclusion: ETDRK4RDP is more efficient than competing schemes, especially in parallel, making it suitable for nonsmooth reaction-diffusion problems.

Abstract: A fourth-order, L-stable, exponential time differencing Runge-Kutta type
scheme is developed to solve nonlinear systems of reaction diffusion equations
with nonsmooth data. The new scheme, ETDRK4RDP, is constructed by approximating
the matrix exponentials in the ETDRK4 scheme with a fourth order, L-acceptable,
non-Pad\'e rational function having real and distinct poles (RDP). Using RDP
rational functions to construct the scheme ensures efficient damping of
spurious oscillations arising from non-smooth initial and boundary conditions
and a straightforward parallelization. We verify empirically that the new
ETDRK4RDP scheme is fourth-order accurate for several reaction diffusion
systems with Dirichlet and Neumann boundary conditions and show it to be more
efficient than competing exponential time differencing schemes, especially when
implemented in parallel, with up to six times speedup in CPU time.

</details>


### [4] [Stability and error analysis of a new class of higher-order consistent splitting schemes for the Navier-Stokes equations](https://arxiv.org/abs/2507.01296)
*Fukeng Huang,Jie Shen*

Main category: math.NA

TL;DR: New decoupled splitting schemes for Navier-Stokes equations, using Taylor expansion with parameter β, achieve stability and optimal convergence rates for higher-order schemes.


<details>
  <summary>Details</summary>
Motivation: To develop stable, higher-order decoupled schemes for Navier-Stokes equations, addressing limitations of existing methods.

Method: Constructs schemes via Taylor expansion at tⁿ⁺ᵝ with β ≥ 1, selecting β=3,6,9 for second-, third-, and fourth-order schemes.

Result: Numerical solutions are bounded and achieve optimal convergence rates; new schemes are unconditionally stable unlike BDF-based ones.

Conclusion: First stable, higher-order decoupled schemes for Navier-Stokes, validated numerically for stability and convergence.

Abstract: A new class of fully decoupled consistent splitting schemes for the
Navier-Stokes equations are constructed and analyzed in this paper. The schemes
are based on the Taylor expansion at $t^{n+\beta}$ with $\beta\ge 1$ being a
free parameter. It is shown that by choosing {\color{black} $\beta= 3,
\,6,\,9$} respectively for the second-, third- and fourth-order schemes, their
numerical solutions are uniformed bounded in a strong norm, and admit optimal
global-in-time convergence rates in both 2D and 3D. {\color{black}These }
results are the first stability and convergence results for any fully
decoupled, higher than second-order schemes for the Navier-Stokes equations.
Numerical results are provided to show that the third- and fourth-order schemes
based on the usual BDF (i.e. $\beta=1$) are not unconditionally stable while
the new third- and fourth-order schemes with suitable $\beta$ are
unconditionally stable and lead to expected convergence rates.

</details>


### [5] [Asymptotic Preserving and Accurate scheme for Multiscale Poisson-Nernst-Planck (MPNP) system](https://arxiv.org/abs/2507.01402)
*Clarissa Astuto,Giovanni Russo*

Main category: math.NA

TL;DR: A two-species Multiscale model for a Poisson-Nernst-Planck system is proposed, focusing on correlated ion motion under a trap, extending prior work by including both positive and negative ions and addressing Coulomb interactions.


<details>
  <summary>Details</summary>
Motivation: To model surface traps with small attraction ranges and capture the correlated motion of both ion species, often neglected in traditional approaches.

Method: Extends a prior model by incorporating both ion species and uses boundary conditions derived from mass conservation and asymptotic analysis. Addresses Coulomb interactions via the Quasi-Neutral limit for small Debye lengths.

Result: The model simplifies to a diffusion equation for small Debye lengths but highlights challenges for non-negligible values.

Conclusion: The work advances understanding of ion dynamics under traps but notes computational challenges for non-negligible Debye lengths.

Abstract: In this paper, we propose and validate a two-species Multiscale model for a
Poisson-Nernst-Planck (PNP) system, focusing on the correlated motion of
positive and negative ions under the influence of a trap. Specifically, we aim
to model surface traps whose attraction range, of length delta, is much smaller
then the scale of the problem. The physical setup we refer to is an anchored
gas drop (bubble) surrounded by a diffusive flow of charged surfactants (ions).
When the diffusing surfactants reach the surface of the trap, the anions are
adsorbed. As in our previous works [11,6,9,4], the effect of the attractive
potential is replaced by a suitable boundary condition derived by mass
conservation and asymptotic analysis. The novelty of this work is the extension
of the model proposed in [11], now incorporating the influence of both carriers
- positive and negative ions - simultaneously, which is often neglected in
traditional approaches that treat ion species independently. In the second part
of the paper, we address the treatment of the Coulomb interaction between
carriers. When the Debye length lambda_D (proportional to a small parameter
epsilon) is very small, one can adopt the so-called Quasi-Neutral limit, which
significantly simplifies the system, reducing it to a diffusion equation for a
single carriers with effective diffusion coefficient [36,53]. This approach,
while simplifying the mathematical model, does not capture the effects of non
negligible values of epsilon. When the Debye length is small but not
negligible, it may be very expensive to capture the small deviation from the
Quasi-Neutral limit by standard methods in the literature. [...]

</details>


### [6] [An Optimal Least-Square Solver For Scaled Partial-Isometric Linear Systems](https://arxiv.org/abs/2507.01434)
*Suvendu Kar,Murugesan Venkatapathi*

Main category: math.NA

TL;DR: An efficient $O(mn)$ solver for $m \times n$ linear systems with scaled partial isometry, also applicable to block diagonal systems with distinct scaling factors.


<details>
  <summary>Details</summary>
Motivation: To address the need for efficient solvers for linear systems involving scaled partial isometries, especially in block diagonal contexts.

Method: A direct least-squares solver leveraging the properties of scaled partial isometries and block diagonal structures.

Result: The algorithm achieves $O(mn)$ complexity and is demonstrated through numerical experiments.

Conclusion: The proposed solver is efficient and versatile for specific linear systems, validated by experiments.

Abstract: We present an $O(mn)$ direct least-squares solver for $m \times n$ linear
systems with a scaled partial isometry. The proposed algorithm is also useful
when the system is block diagonal and each block is a scaled partial isometry
with distinct scaling factors. We also include numerical experiments as a
demonstration.

</details>


### [7] [A surface finite element scheme for a stochastic PDE on an evolving curve](https://arxiv.org/abs/2507.01527)
*Paola Pozzi,Björn Stinner*

Main category: math.NA

TL;DR: The paper presents an ESFEM method for advection and diffusion on a moving curve, including stochastic noise, with a variational solution and error analysis.


<details>
  <summary>Details</summary>
Motivation: To address advection and diffusion on a moving curve with stochastic noise, which disrupts classical differentiability.

Method: Uses a variational solution concept and fully discrete FEM discretization.

Result: Generalizes classical error estimates to the stochastic setting, supported by numerical simulations.

Conclusion: The proposed method effectively handles stochastic noise in advection-diffusion on moving curves.

Abstract: In this paper we consider an ESFEM method for the advection and diffusion of
a scalar quantity on a moving closed curve. The diffusion process is controlled
by a forcing term that may include a rough term (specifically a stochastic
noise) which in particular destroys the classical time differentiability
properties of the solution. We provide a suitable variational solution concept
and a fully discrete FEM discretization. Our error analysis appropriately
generalizes classical estimates to this weaker setting. We present some
numerical simulations that confirm our theoretical findings.

</details>


### [8] [Consistency of Learned Sparse Grid Quadrature Rules using NeuralODEs](https://arxiv.org/abs/2507.01533)
*Hanno Gottschalk,Emil Partow,Tobias J. Riedlinger*

Main category: math.NA

TL;DR: The paper proves the consistency of sparse grid quadrature for high-dimensional integration, using a learned transport map and Clenshaw-Curtis quadrature, with PAC-learning guarantees.


<details>
  <summary>Details</summary>
Motivation: To ensure accurate numerical integration of high-dimensional distributions by combining learned transport maps with sparse grid quadrature.

Method: 1. Learn a transport map to normalize the distribution. 2. Use Clenshaw-Curtis sparse grid quadrature for integration. 3. Decompose errors into quadrature and statistical components.

Result: All error terms are controllable under PAC-learning, ensuring high-probability approximation of the theoretical integral.

Conclusion: The method reliably approximates high-dimensional integrals with controlled errors, validated by PAC-learning theory.

Abstract: This paper provides a proof of the consistency of sparse grid quadrature for
numerical integration of high dimensional distributions. In a first step, a
transport map is learned that normalizes the distribution to a noise
distribution on the unit cube. This step is built on the statistical learning
theory of neural ordinary differential equations, which has been established
recently. Secondly, the composition of the generative map with the quantity of
interest is integrated numerically using the Clenshaw-Curtis sparse grid
quadrature. A decomposition of the total numerical error in quadrature error
and statistical error is provided. As main result it is proven in the framework
of empirical risk minimization that all error terms can be controlled in the
sense of PAC (probably approximately correct) learning and with high
probability the numerical integral approximates the theoretical value up to an
arbitrary small error in the limit where the data set size is growing and the
network capacity is increased adaptively.

</details>


### [9] [A mixed Petrov--Galerkin Cosserat rod finite element formulation](https://arxiv.org/abs/2507.01552)
*Marco Herrmann,Domenico Castello,Jonas Breuling,Idoia Cortes Garcia,Leopoldo Greco,Simon R. Eugster*

Main category: math.NA

TL;DR: A singularity-free, locking-free finite element method for Cosserat rods using quaternions and mixed Petrov-Galerkin formulation.


<details>
  <summary>Details</summary>
Motivation: To address singularities and locking in Cosserat rod analysis with a computationally efficient method.

Method: Uses nodal centerline positions and unit quaternions, Lagrangian interpolation, and a mixed variational approach with Hellinger-Reissner principle.

Result: Improved computational robustness and efficiency, reduced load steps and iterations.

Conclusion: The method effectively mitigates locking and enhances numerical performance for Cosserat rod analysis.

Abstract: This paper presents a total Lagrangian mixed Petrov--Galerkin finite element
formulation that provides a computationally efficient approach for analyzing
Cosserat rods that is free of singularities and locking. To achieve a
singularity-free orientation parametrization of the rod, the nodal kinematical
unknowns are defined as the nodal centerline positions and unit quaternions. We
apply Lagrangian interpolation to all nodal kinematic coordinates, and in
combination with a projection of non-unit quaternions, this leads to an
interpolation with orthonormal cross-section-fixed bases. To eliminate locking
effects such as shear locking, the variational Hellinger--Reissner principle is
applied, resulting in a mixed approach with additional fields composed of
resultant contact forces and moments. Since the mixed formulation contains the
constitutive law in compliance form, it naturally incorporates constrained
theories, such as the Kirchhoff--Love theory. This study specifically examines
the influence of the additional internal force fields on the numerical
performance, including locking mitigation and robustness. Using
well-established benchmark examples, the method demonstrates enhanced
computational robustness and efficiency, as evidenced by the reduction in
required load steps and iterations when applying the standard Newton--Raphson
method.

</details>


### [10] [A trust-region framework for optimization using Hermite kernel surrogate models](https://arxiv.org/abs/2507.01729)
*Sven Ullmann,Tobias Ehring,Robin Herkert,Bernard Haasdonk*

Main category: math.NA

TL;DR: A trust-region optimization framework using Hermite kernel surrogate models for computationally expensive functions, ensuring convergence and efficiency.


<details>
  <summary>Details</summary>
Motivation: Optimizing expensive-to-evaluate functions directly is impractical; surrogate models offer a viable alternative.

Method: Uses Hermite kernel interpolation for surrogate modeling within trust-regions, incorporating gradient information for precision.

Result: Proven convergence to stationary points and demonstrated efficiency gains over direct optimization in experiments.

Conclusion: The framework effectively balances accuracy and computational cost for medium- to high-dimensional problems.

Abstract: In this work, we present a trust-region optimization framework that employs
Hermite kernel surrogate models. The method targets optimization problems with
computationally demanding objective functions, for which direct optimization is
often impractical due to expensive function evaluations. To address these
challenges, we leverage a trust-region strategy, where the objective function
is approximated by an efficient surrogate model within a local neighborhood of
the current iterate. In particular, we construct the surrogate using Hermite
kernel interpolation and define the trust-region based on bounds for the
interpolation error. As mesh-free techniques, kernel-based methods are
naturally suited for medium- to high-dimensional problems. Furthermore, the
Hermite formulation incorporates gradient information, enabling precise
gradient estimates that are crucial for many optimization algorithms. We prove
that the proposed algorithm converges to a stationary point, and we demonstrate
its effectiveness through numerical experiments, which illustrate the
convergence behavior as well as the efficiency gains compared to direct
optimization.

</details>


### [11] [An energy-based discontinuous Galerkin method for the wave equation with nonsmooth solutions](https://arxiv.org/abs/2507.01736)
*Yangxin Fu,Yan Jiang,Siyang Wang*

Main category: math.NA

TL;DR: A stable, high-order accurate discontinuous Galerkin method for the second-order wave equation, handling nonsmooth solutions by combining energy-based DG with oscillation-free techniques.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of spurious oscillations near discontinuities in wave equations, ensuring stability and accuracy.

Method: Integration of energy-based discontinuous Galerkin method with oscillation-free techniques, supported by stability analysis and error estimates.

Result: Optimal convergence rates for smooth solutions and robust oscillation-free behavior for nonsmooth solutions, including nonlinear cases.

Conclusion: The method effectively balances accuracy and stability for wave equations, even with nonsmooth or nonlinear terms.

Abstract: We develop a stable and high-order accurate discontinuous Galerkin method for
the second order wave equation, specifically designed to handle nonsmooth
solutions. Our approach integrates the energy-based discontinuous Galerkin
method with the oscillation-free technique to effectively suppress spurious
oscillations near solution discontinuities. Both stability analysis and apriori
error estimates are established for common choices of numerical fluxes. We
present a series of numerical experiments to confirm the optimal convergence
rates for smooth solutions and its robustness in maintaining oscillation-free
behavior for nonsmooth solutions in wave equations without or with nonlinear
source terms.

</details>


### [12] [Global Energy Minimization for Simplex Mesh Optimization: A Radius Ratio Approach to Sliver Elimination](https://arxiv.org/abs/2507.01762)
*Dong Wang,Chunyu Chen,Huayi Wei*

Main category: math.NA

TL;DR: A novel method using a radius ratio energy function optimizes 3D simplex mesh quality by eliminating sliver elements, with a symmetric positive definite matrix accelerating the process.


<details>
  <summary>Details</summary>
Motivation: Sliver elements in 3D simplex meshes degrade simulation stability and accuracy, necessitating an effective optimization method.

Method: Proposes a radius ratio energy function, decomposing its gradient into a matrix-vector product, and uses the resulting symmetric positive definite matrix as a preconditioner.

Result: Effectively eliminates sliver elements and improves mesh quality, with faster optimization due to the preconditioner.

Conclusion: The method enhances mesh quality and accelerates optimization, proving advantageous for numerical simulations.

Abstract: The quality of simplex mesh is crucial for the stability and accuracy of
numerical simulations in finite element analysis and computational geometry.
However, the presence of sliver elements in 3D simplex mesh can severely impact
the results. This paper presents a novel method based on a radius ratio energy
function to optimize the quality of simplex mesh elements. This method can
effectively eliminate sliver elements, thereby enhancing mesh quality.The
gradient of the proposed energy function can be decomposed into a matrix-vector
product. With minor processing, the matrix becomes symmetric positive definite,
and this symmetric positive definite matrix can serve as a preconditioner to
significantly accelerate the optimization process. Experimental results
demonstrate that this method has significant advantages in eliminating sliver
elements and improving mesh quality.

</details>


### [13] [GPU-based complete search for nonlinear minimization subject to bounds](https://arxiv.org/abs/2507.01770)
*Guanglu Zhang,Qihang Shan,Jonathan Cagan*

Main category: math.NA

TL;DR: A GPU-based complete search method using interval analysis to guarantee the global minimum of nonlinear functions, validated on 10 benchmark functions up to 10,000 dimensions.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of enclosing the global minimum of high-dimensional nonlinear functions rigorously and efficiently, leveraging GPU computational power.

Method: Combines interval analysis with GPU parallelism, employing a single program, single data style and variable cycling to optimize performance.

Result: Successfully encloses the global minimum of 10 benchmark functions, including high-dimensional cases (up to 10,000D), outperforming existing methods.

Conclusion: The method is effective and efficient for high-dimensional global optimization, validated by rigorous benchmarks.

Abstract: This paper introduces a GPU-based complete search method to enclose the
global minimum of a nonlinear function subject to simple bounds on the
variables. Using interval analysis, coupled with the computational power and
architecture of GPU, the method iteratively rules out the regions in the search
domain where the global minimum cannot exist and leaves a finite set of regions
where the global minimum must exist. For effectiveness, because of the rigor of
interval analysis, the method is guaranteed to enclose the global minimum of
the nonlinear function even in the presence of rounding errors. For efficiency,
the method employs a novel GPU-based single program, single data parallel
programming style to circumvent major GPU performance bottlenecks, and a
variable cycling technique is also integrated into the method to reduce
computational cost when minimizing large-scale nonlinear functions. The method
is validated by minimizing 10 multimodal benchmark test functions with scalable
dimensions, including the well-known Ackley function, Griewank function, Levy
function, and Rastrigin function. These benchmark test functions represent
grand challenges of global optimization, and enclosing the guaranteed global
minimum of these benchmark test functions with more than 80 dimensions has not
been reported in the literature. Our method completely searches the feasible
domain and successfully encloses the guaranteed global minimum of these 10
benchmark test functions with up to 10,000 dimensions using only one GPU in a
reasonable computation time, far exceeding the reported results in the
literature due to the unique method design and implementation based on GPU
architecture.

</details>


### [14] [The inverse source problem of stochastic wave equation](https://arxiv.org/abs/2507.01789)
*Yunqing Huang,Shihan Zhang*

Main category: math.NA

TL;DR: A novel computational framework is developed to address the ill-posed inverse source problem for stochastic Helmholtz equations, focusing on non-Gaussian stochastic properties.


<details>
  <summary>Details</summary>
Motivation: The study aims to mitigate the inherent challenges of inverse source problems in stochastic wave equations, particularly those driven by finite-jump Lévy processes.

Method: The paper establishes the existence of a mild solution for the direct problem and investigates the well-posedness of the inverse problem. It develops a methodology to reconstruct unknown source terms using wave field data.

Result: The framework provides rigorous theoretical analysis and effective numerical schemes for solving inverse source problems in stochastic wave equations.

Conclusion: The work offers new perspectives for wave propagation inverse problems and has applications in seismic analysis and financial modeling.

Abstract: To address the ill-posedness of the inverse source problem for the
one-dimensional stochastic Helmholtz equations without attenuation, this study
develops a novel computational framework designed to mitigate this inherent
challenge at the numerical implementation level. For the stochastic wave
equation driven by a finite-jump L\'evy process (assuming that its jump
amplitude obeys a Gaussian distribution and the jump time interval obeys a
Poisson distribution), this paper firstly establish the existence of a mild
solution to its direct problem satisfying a particular stability estimate.
Building upon these theoretical foundations, we further investigate the
well-posedness of the inverse problem and develop a methodology to reconstruct
the unknown source terms $f$ and $g$ using the data of the wave field at the
final time point $u(x,T)$. This work not only provides rigorous theoretical
analysis and effective numerical schemes for solving inverse source problems in
these two specific classes of stochastic wave equations, but also offers new
perspectives and methodological approaches for addressing a broader range of
wave propagation inverse problems characterized by non-Gaussian stochastic
properties. The proposed framework demonstrates significant relevance for
characterizing physical phenomena influenced by jump-type stochastic
perturbations, offering promising applications in diverse domains including but
not limited to seismic wave propagation analysis and financial market
volatility modeling.

</details>


### [15] [Neural Entropy-stable conservative flux form neural networks for learning hyperbolic conservation laws](https://arxiv.org/abs/2507.01795)
*Lizuo Liu,Lu Zhang,Anne Gelb*

Main category: math.NA

TL;DR: A neural entropy-stable conservative flux form neural network (NESCFN) is proposed to learn hyperbolic conservation laws and entropy functions directly from data, without predefined discretization or governing equations.


<details>
  <summary>Details</summary>
Motivation: Existing neural network methods rely on prior knowledge of equations or fixed discretization. This work aims to remove such dependencies by embedding entropy-stable principles into learning.

Method: The NESCFN jointly learns numerical flux functions and corresponding entropy, ensuring conservation and entropy dissipation for stability.

Result: The method achieves stability and conservation over long time horizons and accurately captures shock propagation speeds without future-time data.

Conclusion: The NESCFN enables physically consistent, data-driven discovery of hyperbolic conservation laws, ensuring long-term stability and fidelity.

Abstract: We propose a neural entropy-stable conservative flux form neural network
(NESCFN) for learning hyperbolic conservation laws and their associated entropy
functions directly from solution trajectories, without requiring any predefined
numerical discretization. While recent neural network architectures have
successfully integrated classical numerical principles into learned models,
most rely on prior knowledge of the governing equations or assume a fixed
discretization. Our approach removes this dependency by embedding
entropy-stable design principles into the learning process itself, enabling the
discovery of physically consistent dynamics in a fully data-driven setting. By
jointly learning both the numerical flux function and a corresponding entropy,
the proposed method ensures conservation and entropy dissipation, critical for
long-term stability and fidelity in the system of hyperbolic conservation laws.
Numerical results demonstrate that the method achieves stability and
conservation over extended time horizons and accurately captures shock
propagation speeds, even without oracle access to future-time solution profiles
in the training data.

</details>


### [16] [Faber polynomials in a deltoid region and power iteration momentum methods](https://arxiv.org/abs/2507.01885)
*Peter Cowal,Nicholas F. Marshall,Sara Pollock*

Main category: math.NA

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We consider a region in the complex plane enclosed by a deltoid curve
inscribed in the unit circle, and define a family of polynomials $P_n$ that
satisfy the same recurrence relation as the Faber polynomials for this region.
We use this family of polynomials to give a constructive proof that $z^n$ is
approximately a polynomial of degree $\sim\sqrt{n}$ within the deltoid region.
Moreover, we show that $|P_n| \le 1$ in this deltoid region, and that, if $|z|
= 1+\varepsilon$, then the magnitude $|P_n(z)|$ is at least
$\frac{1}{3}(1+\sqrt{\varepsilon})^n$, for all $\varepsilon > 0$. We illustrate
our polynomial approximation theory with an application to iterative linear
algebra. In particular, we construct a higher-order momentum-based method that
accelerates the power iteration for certain matrices with complex eigenvalues.
We show how the method can be run dynamically when the two dominant eigenvalues
are real and positive.

</details>


### [17] [PDE-Constrained High-Order Mesh Optimization](https://arxiv.org/abs/2507.01917)
*Tzanio Kolev,Boyan Lazarov,Ketan Mittal,Mathias Schmidt,Vladimir Tomov*

Main category: math.NA

TL;DR: A novel framework for PDE-constrained $r$-adaptivity of high-order meshes optimizes mesh movement by combining mesh quality and PDE solution accuracy, achieving up to 10x error reduction.


<details>
  <summary>Details</summary>
Motivation: To improve high-order mesh adaptation by integrating mesh quality, PDE solution accuracy, and robust gradient regularization.

Method: Formulates mesh movement as an optimization problem using a convex combination of mesh quality and PDE accuracy metrics, with adjoint sensitivity analysis and gradient regularization.

Result: Demonstrates up to 10x error reduction for Poisson and linear elasto-static problems.

Conclusion: The framework is general, applicable to various PDEs, mesh types, and dimensions, and enhances both mesh quality and solution accuracy.

Abstract: We present a novel framework for PDE-constrained $r$-adaptivity of high-order
meshes. The proposed method formulates mesh movement as an optimization
problem, with an objective function defined as a convex combination of a mesh
quality metric and a measure of the accuracy of the PDE solution obtained via
finite element discretization. The proposed formulation achieves optimized,
well-defined high-order meshes by integrating mesh quality control, PDE
solution accuracy, and robust gradient regularization. We adopt the
Target-Matrix Optimization Paradigm to control geometric properties across the
mesh, independent of the PDE of interest. To incorporate the accuracy of the
PDE solution, we introduce error measures that control the finite element
discretization error. The implicit dependence of these error measures on the
mesh nodal positions is accurately captured by adjoint sensitivity analysis.
Additionally, a convolution-based gradient regularization strategy is used to
ensure stable and effective adaptation of high-order meshes. We demonstrate
that the proposed framework can improve mesh quality and reduce the error by up
to 10 times for the solution of Poisson and linear elasto-static problems. The
approach is general with respect to the dimensionality, the order of the mesh,
the types of mesh elements, and can be applied to any PDE that admits
well-defined adjoint operators.

</details>


### [18] [Parallel-in-Time Preconditioning for Time-Dependent Variational Mean Field Games](https://arxiv.org/abs/2507.01958)
*Heidi Wolles Ljósheim,Dante Kalise,John W. Pearson,Francisco J. Silva*

Main category: math.NA

TL;DR: The paper presents a numerical method for solving time-dependent variational mean field games using finite differences and the Chambolle-Pock algorithm, enhanced by parallel-in-time preconditioners for efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of solving time-dependent variational mean field game systems with local couplings, particularly the computational inefficiency and ill-conditioned linear systems.

Method: Finite difference discretization and the Chambolle-Pock primal-dual algorithm, with parallel-in-time preconditioners based on diagonalization techniques using discrete Fourier transforms.

Result: Improved performance and parallel scalability, with robustness across various viscosities and efficient solvers for ill-conditioned systems.

Conclusion: The proposed approach effectively enhances computational efficiency and scalability for solving variational mean field game systems.

Abstract: We study the numerical approximation of a time-dependent variational mean
field game system with local couplings and either periodic or Neumann boundary
conditions. Following a variational approach, we employ a finite difference
discretization and solve the resulting finite-dimensional optimization problem
using the Chambolle--Pock primal--dual algorithm. As this involves computing
proximal operators and solving ill-conditioned linear systems at each
iteration, we propose a general class of parallel-in-time preconditioners based
on diagonalization techniques using discrete Fourier transforms. These enable
efficient, scalable iterative solvers with robustness across a wide range of
viscosities. We further develop fast solvers for the resulting ill-conditioned
systems arising at each time step, using exact recursive schemes for structured
grids while allowing for other geometries. Numerical experiments confirm the
improved performance and parallel scalability of our approach.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [19] [Weak solutions for coupled reaction-diffusion systems with pattern formation by a stochastic fixed point theorem](https://arxiv.org/abs/2507.01159)
*Erika Hausenblas,Michael A. Högele,Tesfalem A. Tegegn*

Main category: math.AP

TL;DR: The paper studies the existence of weak solutions for a stochastic reaction-diffusion system with nonlinear coupling and fractional Laplacian operators, using a fixed-point theorem approach.


<details>
  <summary>Details</summary>
Motivation: To understand the complex behaviors (e.g., multiple steady states, oscillations) in chemical and biochemical reactions, modeled by stochastic reaction-diffusion equations.

Method: The authors employ a Schauder-Tychonoff type fixed-point theorem to analyze controlled versions of the laws of an infinite-dimensional Ornstein-Uhlenbeck system.

Result: Existence of a probabilistic weak solution for the given stochastic system is proven.

Conclusion: The study successfully demonstrates the existence of solutions for the stochastic system, contributing to the understanding of nonlinear reaction-diffusion models with stochastic perturbations.

Abstract: Chemical and biochemical reactions can exhibit surprisingly different
behaviours, ranging from multiple steady-state solutions to oscillatory
solutions and chaotic behaviours. These types of systems are often modelled by
a system of reaction-diffusion equations coupled by a nonlinearity. In the
article, we study the existence of stochastically perturbed equations of this
type. In particular, we show the existence of a probabilitic weak solution of
the following stochastic system \begin{align*} \dot {u} & = r_1\,\Delta u+
a_1\, u + b_1 -c_1\, u\cdot v^q+\sigma_1\, g_1(u)\circ \dot W_1, \\ \dot{v} & =
r_2 \,A v + a_2\, v + b_2 +c_2\, u\cdot v^q + \sigma_2\, g_2(v)\circ \dot W_2,
\end{align*} where $r_i,b_i,c_i, \sigma_i>0$, $a_i\in\mathbb{R}$, and $g_i$ are
linear, $i=1,2$, and the exponent $q\geq 1$. The operator
$A=-(-\Delta)^{\aleph/2}$ is a fractional power of the Laplacian, $1<\aleph
\le2$. The main result is obtained by a Schauder-Tychonoff type fixed point
theorem for the controlled versions of the laws of the respective (infinite
dimensional) Ornstein-Uhlenbeck system, from which we infer the existence of a
weak solution of the coupled system.

</details>


### [20] [Existence of wave operators for Zakharov-Kuznetsov equation in two space dimensions](https://arxiv.org/abs/2507.01288)
*Jun-ichi Segata*

Main category: math.AP

TL;DR: Study of long-term behavior of solutions to the 2D Zakharov-Kuznetsov equation, focusing on scattering to free solutions and wave operator existence.


<details>
  <summary>Details</summary>
Motivation: To understand the asymptotic behavior of solutions to the Zakharov-Kuznetsov equation and establish the existence of wave operators.

Method: Uses the space-time resonance method developed by Gustafson-Nakanishi-Tsai and Germain-Masmoudi-Shatah to construct small global solutions.

Result: Constructed small global solutions that scatter to free solutions, proving the existence of wave operators.

Conclusion: The study successfully demonstrates the scattering behavior and existence of wave operators for the Zakharov-Kuznetsov equation.

Abstract: In this paper, we study long time behavior of solution to the two dimensional
Zakharov-Kuznetsov equation in the framework of the final state problem. We
construct a small global solution to the Zakharov-Kuznetsov equation which
scatters to a given free solution. From this result, we have the existence of
wave operators for the Zakharov-Kuznetsov equation. The proof is based on the
space-time resonance method developed by Gustafson-Nakanishi-Tsai and
Germain-Masmoudi-Shatah etc.

</details>


### [21] [Physical Space Proof of Bilinear Estimates and Applications to Nonlinear Dispersive Equations (II)](https://arxiv.org/abs/2507.01317)
*Xinfeng Hu,Li Tu,Yi Zhou*

Main category: math.AP

TL;DR: The paper reproduces optimal local well-posedness results for the Zakharov system in 2D and 3D, using a bilinear estimate and a new div-curl lemma. A key addition is a Strichartz estimate with mixed spatial integrability.


<details>
  <summary>Details</summary>
Motivation: To extend and confirm the optimal local well-posedness results for the Zakharov system in higher dimensions, building on prior work.

Method: Uses a bilinear estimate and a new div-curl lemma in physical space, with a novel Strichartz estimate involving mixed spatial integrability.

Result: Reproduces and confirms the optimal local well-posedness results for the Zakharov system in 2D and 3D.

Conclusion: The study successfully replicates prior results with a new methodological twist, enhancing understanding of the Zakharov system's well-posedness.

Abstract: We study the Zakharov system in two and three spatial dimensions, reproducing
the optimal local well-posedness results from Bejenaru-Herr-Holmer-Tataru [2]
and Bejenaru-Herr [1]. The main tools are similar to [16], based on a bilinear
estimate, which is proved in a physical space approach by a new type of
div-curl lemma. The new ingredient of our proof is a Strichartz estimate with
mixed spatial integrability.

</details>


### [22] [Suleimanov-Talanov self-focusing and the hierarchy of the focusing nonlinear Schrödinger equation](https://arxiv.org/abs/2507.01393)
*Robert J. Buckingham,Robert M. Jenkins,Peter D. Miller*

Main category: math.AP

TL;DR: The paper studies self-focusing wave packets using the semiclassical nonlinear Schrödinger equation, validating dispersive regularization of Talanov blowup and extending results to higher-order equations.


<details>
  <summary>Details</summary>
Motivation: To understand and rigorously validate the dispersive regularization of Talanov's blowup solutions in nonlinear wave dynamics.

Method: Uses semiclassical soliton ensembles to approximate Talanov solutions, analyzing their behavior as the semiclassical parameter tends to zero.

Result: Establishes dispersive saturation of Talanov blowup and extends findings to higher-order equations, identifying new initial data for extreme focusing.

Conclusion: Generic perturbations of Talanov data lead to different singularities, with dispersive regularization involving Painlevé-I and Peregrine breather solutions.

Abstract: We study the self-focusing of wave packets from the point of view of the
semiclassical focusing nonlinear Schr\"odinger equation. A type of finite-time
collapse/blowup of the solution of the associated dispersionless limit was
investigated by Talanov in the 1960s, and recently Suleimanov identified a
special solution of the dispersive problem that formally regularizes the blowup
and is related to the hierarchy of the Painlev\'e-III equation. In this paper
we approximate the Talanov solutions in the full dispersive equation using a
semiclassical soliton ensemble, a sequence of exact reflectionless solutions
for a corresponding sequence of values of the semiclassical parameter epsilon
tending to zero, approximating the Talanov initial data more and more
accurately in the limit as epsilon tends to zero. In this setting, we
rigorously establish the validity of the dispersive saturation of the Talanov
blowup obtained by Suleimanov. We extend the result to the full hierarchy of
higher focusing nonlinear Schr\"odinger equations, exhibiting new
generalizations of the Talanov initial data that produce such dispersively
regularized extreme focusing in both mixed and pure flows. We also argue that
generic perturbations of the Talanov initial data lead to a different
singularity of the dispersionless limit, namely a gradient catastrophe for
which the dispersive regularization is instead based on the tritronqu\'ee
solution of the Painlev\'e-I equation and the Peregrine breather solution which
appears near points in space time corresponding to the poles of the former
transcendental function as shown by Bertola and Tovbis.

</details>


### [23] [Global Existence and Incompressible Limit for Compressible Navier-Stokes Equations with Large Bulk Viscosity Coefficient and Large Initial Data](https://arxiv.org/abs/2507.01432)
*Qinghao Lei,Chengfeng Xiong*

Main category: math.AP

TL;DR: Global existence and exponential decay of solutions to 2D compressible Navier-Stokes equations with large bulk viscosity, converging to incompressible solutions as viscosity tends to infinity.


<details>
  <summary>Details</summary>
Motivation: To study the behavior of compressible Navier-Stokes equations with vacuum and large bulk viscosity, and their convergence to incompressible solutions.

Method: Introduce time-layers to avoid restrictions on initial velocity divergence, and estimate the $L^\infty$ norm of the effective viscous flux using a time-partitioning approach.

Result: Global existence and exponential decay of weak, strong, and classical solutions, with convergence to incompressible solutions as bulk viscosity increases.

Conclusion: Large bulk viscosity ensures stability and convergence to incompressible solutions, even without divergence-free initial velocity.

Abstract: For periodic initial data with the density allowing vacuum, we establish the
global existence and exponential decay of weak, strong and classical solutions
to the two-dimensional(2D) compressible Navier-Stokes equations when the bulk
viscosity coefficient is sufficiently large, without any extra restrictions on
initial velocity divergence. Moreover, we demonstrate that when the bulk
viscosity coefficient tends to infinity, these solutions converge to solutions
to the inhomogeneous incompressible Navier-Stokes equations. For the
incompressible limit of weak solutions, our results hold even without requiring
the initial velocity field to be divergence-free. Our results are established
by introducing time-layers to avoid imposing restrictions on the initial
velocity divergence, along with estimates of $L^\infty$ norm of the effective
viscous flux $G$ via a time-partitioning approach based on Gagliardo-Nirenberg
inequality.

</details>


### [24] [Infinite concentration and oscillation estimates for supercritical semilinear elliptic equations in discs. I](https://arxiv.org/abs/2507.01475)
*Daisuke Naimen*

Main category: math.AP

TL;DR: The paper extends previous results to general supercritical semilinear elliptic equations, analyzing infinite concentration and oscillation phenomena in blow-up solutions.


<details>
  <summary>Details</summary>
Motivation: To generalize and extend prior findings on supercritical semilinear elliptic equations, particularly focusing on infinite concentration and oscillation behaviors.

Method: Uses scaling and pointwise techniques to analyze blow-up solutions, classifying concentration behaviors and describing limit profiles, energy, and positions via Liouville equations.

Result: Identifies two types of supercritical behaviors, including a new one for multiple exponential growth, and provides detailed descriptions of concentration phenomena.

Conclusion: The study reveals new insights into supercritical behaviors and sets the stage for further analysis of oscillation phenomena in the second part.

Abstract: In our series of papers, we establish infinite concentration and oscillation
estimates for supercritical semilinear elliptic equations in discs. Especially,
we extend the previous result by the author (N. arXiv:2404.01634) to the
general supercritical case. Our growth condition is related to the one
introduced by Dupaign-Farina (J. Eur. Math. Soc. 12: 855--882, 2010) and admits
two types of supercritical nonlinearities, the Trudinger-Moser type growth
$e^{u^p}$ with $p>2$ and the multiple exponential one
$\exp{(\cdots(\exp{(u^m)})\cdots)}$ with $m>0$. In this first part, we carry
out the analysis of infinite concentration phenomena on any blow-up solutions.
As a result, we classify all the infinite concentration behaviors into two
types which in particular shows a new behavior for the multiple exponential
case. More precisely, we detect an infinite sequence of concentrating parts on
any blow-up solutions via the scaling and pointwise techniques. The precise
description of the limit profile, energy, and position of each concentration is
given via the Liouville equation with the two types of the energy recurrence
formulas. This leads us to observe the two types of supercritical behaviors.
The behavior for the latter growth is new and can be understood as the limit
case of the former one. Our concentration estimates lead to the analysis of
infinite oscillation phenomena, including infinite oscillations of bifurcation
diagrams, discussed in the second part.

</details>


### [25] [Infinite concentration and oscillation estimates for supercritical semilinear elliptic equations in discs. II](https://arxiv.org/abs/2507.01488)
*Daisuke Naimen*

Main category: math.AP

TL;DR: The paper studies infinite oscillation phenomena in supercritical semilinear elliptic equations in discs, building on prior work on infinite concentration. It describes asymptotic shapes of blow-up solutions and provides conditions for infinite oscillations in bifurcation diagrams, applied to specific nonlinearities.


<details>
  <summary>Details</summary>
Motivation: To extend the analysis of infinite concentration to infinite oscillation phenomena in supercritical elliptic equations, aiming to describe asymptotic behaviors and ensure the existence of infinitely many solutions.

Method: Builds on concentration estimates from prior work to study oscillation phenomena, analyzing asymptotic shapes of blow-up solutions and deriving conditions for infinite oscillations.

Result: Precise description of asymptotic shapes of blow-up solutions and a sufficient condition for infinite oscillations in bifurcation diagrams, applied to Trudinger-Moser and multiple exponential nonlinearities.

Conclusion: The study successfully extends the analysis to oscillation phenomena, providing insights into asymptotic behaviors and conditions for infinite solutions in supercritical elliptic equations.

Abstract: This paper is the latter part of our series concerning the infinite
concentration and oscillation phenomena on supercritical semilinear elliptic
equations in discs. Our supercritical setting admits two types of
nonlinearities, the Trudinger-Moser type $e^{u^p}$ with $p>2$ and the multiple
exponential one $\exp{(\cdots(\exp{(u^m)}))}$ with $m>0$. In the first part, we
accomplished the analysis of infinite concentration phenomena on any blow-up
solutions. In this second part, we proceed to the study of infinite oscillation
phenomena based on the concentration estimates obtained in the first part. As a
result, we provide a precise description of the asymptotic shapes of the graphs
of blow-up solutions near the origin. This allows us to show several infinite
oscillation behaviors around singular solutions with a suitable asymptotic
behavior. In particular, we arrive at a natural sufficient condition for
infinite oscillations of bifurcation diagrams which ensure the existence of
infinitely many solutions. We successfully apply this condition to certain
classes of nonlinearities including the two types mentioned above.

</details>


### [26] [Global existence for a fractionally damped nonlinear Jordan--Moore--Gibson--Thompson equation](https://arxiv.org/abs/2507.01538)
*Mostafa Meliani,Belkacem Said-Houari*

Main category: math.AP

TL;DR: Global existence of solutions for the fractionally damped JMGT model with quadratic gradient nonlinearity is proven under minimal assumptions on the nonlocal damping kernel.


<details>
  <summary>Details</summary>
Motivation: Address the open question of global existence for the fractionally damped JMGT model, which arises in nonlinear acoustics with thermal relaxation laws.

Method: Tailored analysis exploiting the nonlinearity structure and weak damping from nonlocal kernels, handling non-integrable kernels.

Result: Global existence of solutions is established for the fractionally damped JMGT model.

Conclusion: The work resolves a key open problem in nonlinear acoustics by proving global existence under minimal damping assumptions.

Abstract: In nonlinear acoustics, higher-order-in-time equations arise when taking into
account a class of thermal relaxation laws in the modeling of sound wave
propagation. In the literature, these families of equations came to be known as
Jordan--Moore--Gibson--Thompson (JMGT) models. In this work, we show the global
existence of solutions relying only on minimal assumptions on the nonlocal
damping kernel. In particular, our result covers the until-now open question of
global existence of solutions for the fractionally damped JMGT model with
quadratic gradient nonlinearity. The factional damping setting forces us to
work with non-integrable kernels, which require a tailored approach in the
analysis to control. This approach relies on exploiting the specific
nonlinearity structure combined with a weak damping provided by the nonlocality
kernel.

</details>


### [27] [A note on the honeycomb optimality among periodic convex tilings](https://arxiv.org/abs/2507.01566)
*Annalisa Cesaroni,Ilaria Fragalà,Matteo Novaga*

Main category: math.AP

TL;DR: The hexagonal honeycomb is proven optimal among convex periodic tessellations under specific functional conditions.


<details>
  <summary>Details</summary>
Motivation: To identify the optimal tessellation shape for given cost and geometric constraints.

Method: Analysis using lower semicontinuous cost functionals and Steiner symmetrization.

Result: Hexagonal honeycomb is optimal under the specified conditions.

Conclusion: The hexagonal honeycomb is the best choice for convex periodic tessellations under the given functional constraints.

Abstract: We show that the hexagonal honeycomb is optimal among convex periodic
tessellations of the plane, provided the cost functional is lower
semicontinuous with respect to the Hausdorff convergence, and decreasing under
Steiner symmetrization.

</details>


### [28] [Existence of normalized solutions to nonlinear Schrödinger equations on lattice graphs](https://arxiv.org/abs/2507.01591)
*Zhentao He,Chao Ji,Yifan Tao*

Main category: math.AP

TL;DR: The paper studies the existence of global minimizers for a functional on lattice graphs, identifying an excitation threshold m* that classifies the problem into three cases (L²-subcritical, critical, supercritical) and separates existence/nonexistence of minimizers.


<details>
  <summary>Details</summary>
Motivation: To understand the conditions under which global minimizers exist for a given functional constrained on a specific set in lattice graphs, and to classify the problem based on the excitation threshold m*.

Method: Uses discrete Schwarz rearrangement and analyzes the functional I(u) constrained on the set S_m, deriving conditions for the existence of minimizers and classifying the problem into three cases based on m*.

Result: Identifies an excitation threshold m* that determines when the infimum of I(u) is negative and separates the existence/nonexistence of global minimizers for the constrained problem.

Conclusion: The classification into L²-subcritical, critical, and supercritical cases, along with the role of m*, provides a nearly optimal framework for understanding the existence of global minimizers in this context.

Abstract: In this paper, using a discrete Schwarz rearrangement on lattice graphs
developed in \cite{DSR}, we study the existence of global minimizers for the
following functional $I:H^1\left(\mathbb{Z}^N\right)\to \R$, $$I(u)=\frac{1}{2}
\int_{\mathbb{Z}^N}|\nabla u|^2 \,d\mu-\int_{\mathbb{Z}^N} F(u)\, d\mu,$$
constrained on $S_m:=\left\{u \in H^1\left(\mathbb{Z}^N\right)
\mid\|u\|_{\ell^2\left(\mathbb{Z}^N\right)}^2=m\right\}$, where $N \geq 2$,
$m>0$ is prescribed, $f \in C(\mathbb{R}, \mathbb{R})$ satisfying some
technical assumptions and $F(t):=\int_0^t f(\tau) \,d\tau$. We prove the
following minimization problem $$ \inf_{u \in S_m} I(u) $$
  has an excitation threshold $m^*\in [0,+\infty]$ such that \begin{equation*}
  \inf_{u \in S_m} I(u)<0 \quad \text{if and only if } m>m^*. \end{equation*}
Based primarily on $m^* \in (0,+\infty)$ or $m^*=0$, we classify the problem
into three different cases: $L^2$-subcritical, $L^2$-critical and
$L^2$-supercritical. Moreover, for all three cases, under assumptions that we
believe to be nearly optimal, we show that $m^*$ also separates the existence
and nonexistence of global minimizers for $I(u)$ constrained on $S_{m}$.

</details>


### [29] [The Klein-Gordon equation on asymptotically Minkowski spacetimes: the Feynman propagator](https://arxiv.org/abs/2507.01600)
*Dean Baskin,Moritz Doll,Jesse Gell-Redman*

Main category: math.AP

TL;DR: The paper develops a theory for Feynman propagators in the massive Klein-Gordon equation with asymptotically static perturbations, using microlocal analysis and Sobolev spaces.


<details>
  <summary>Details</summary>
Motivation: To extend understanding of Feynman propagators in perturbed Klein-Gordon equations, leveraging microlocal techniques for global analysis.

Method: Uses Vasy's 3sc-calculus for propagation of singularities estimates and Sobolev spaces to analyze the Feynman propagator's properties.

Result: Proves global spacetime mapping properties and microlocal Hadamard condition for the Feynman propagator, showing it as an inverse in specific Sobolev spaces.

Conclusion: The Feynman propagator is rigorously analyzed and shown to satisfy key microlocal and global properties in the given framework.

Abstract: We develop a theory of Feynman propagators for the massive Klein--Gordon
equation with asymptotically static perturbations. Building on our previous
work on the causal propagators, we employ a framework based on propagation of
singularities estimates in Vasy's 3sc-calculus. We combine these estimates to
prove global spacetime mapping properties for the Feynman propagator, and to
show that it satisfies a microlocal Hadamard condition.
  We show that the Feynman propagator can be realized as the inverse of a
mapping between appropriate $L^2$-based Sobolev spaces with additional
regularity near the asymptotic sources of the Hamiltonian flow, realized as a
family of radial points on a compactified spacetime.

</details>


### [30] [A Thermodynamically Consistent Free Boundary Model for Two-Phase Flows in an Evolving Domain with Moving Contact Lines, Variable Contact Angles and Bulk-Surface Interaction](https://arxiv.org/abs/2507.01618)
*Patrik Knopf,Yadong Liu*

Main category: math.AP

TL;DR: A thermodynamically consistent model for two-phase flow in evolving domains, incorporating bulk-surface interactions, phase-fields, and generalized Navier slip conditions.


<details>
  <summary>Details</summary>
Motivation: To accurately describe the dynamics of two-phase flows in evolving domains, including material transfer and variable contact angles.

Method: Derived from local mass balance and energy dissipation laws, using a bulk-surface convective Cahn--Hilliard equation and generalized Navier slip boundary conditions.

Result: The model generalizes previous approaches, allowing for dynamic boundary conditions and material transfer between bulk and surface.

Conclusion: The framework provides a comprehensive description of two-phase flows in evolving domains, unifying and extending existing models.

Abstract: We derive a thermodynamically consistent model, which describes the time
evolution of a two-phase flow in an evolving domain. The movement of the free
boundary of the domain is driven by the velocity field of the mixture in the
bulk, which is determined by a Navier--Stokes equation. In order to take
interactions between bulk and boundary into account, we further consider two
materials on the boundary, which may be the same or different materials as
those in the bulk. The bulk and the surface materials are represented by
respective phase-fields, whose time evolution is described by a bulk-surface
convective Cahn--Hilliard equation. This approach allows for a transfer of
material between bulk and surface as well as variable contact angles between
the diffuse interface in the bulk and the boundary of the domain. To provide a
more accurate description of the corresponding contact line motion, we include
a generalized Navier slip boundary condition on the velocity field. We derive
our model from scratch by considering local mass balance and energy dissipation
laws. Finally, the derivation is completed via the Langrange multiplier
approach. We further show that our model generalizes previous models from the
literature, which can be recovered from our system by either dropping the
dynamic boundary conditions or assuming a static boundary of the domain.

</details>


### [31] [Approximate Peregrine Solitons in Dispersive Nonlinear Wave Equations](https://arxiv.org/abs/2507.01632)
*Guido Schneider,Nils Thorin*

Main category: math.AP

TL;DR: Extends NLS approximation validity from Sobolev spaces to mixed spaces, enabling rogue wave dynamics analysis.


<details>
  <summary>Details</summary>
Motivation: To broaden the applicability of NLS approximation results for studying rogue waves in complex systems.

Method: Extends existing results from $H^s(\mathbb{R})$ to mixed spaces $u = v + w$ where $v \in H_{{per}}^s$ and $w \in H^s(\mathbb{R})$.

Result: Enables use of Peregrine solution for analyzing freak/rogue waves in more complicated systems.

Conclusion: The extension enhances the study of rogue waves in diverse settings.

Abstract: The purpose of this short note is to explain how the existing results on the
validity of the NLS approximation can be extended from Sobolev spaces
$H^s(\mathbb{R})$ to the spaces of functions $u = v + w$ where $v \in
H_{{per}}^s$ and $w \in H^s(\mathbb{R})$. This allows us to use the Peregrine
solution of the NLS equation to find freak or rogue wave dynamics in more
complicated systems.

</details>


### [32] [On the Vanishing Viscosity Limit for Inhomogeneous Incompressible Navier-Stokes Equations on Bounded Domains](https://arxiv.org/abs/2507.01642)
*Jens Schröder,Emil Wiedemann*

Main category: math.AP

TL;DR: The paper extends Kato's criterion for the vanishing viscosity limit to inhomogeneous incompressible Navier-Stokes equations, showing convergence to Euler equations under specific density assumptions.


<details>
  <summary>Details</summary>
Motivation: To generalize Kato's criterion for homogeneous Navier-Stokes equations to the inhomogeneous case, addressing convergence in energy space.

Method: Uses a new relative energy functional to analyze the vanishing viscosity limit and energy dissipation in a boundary layer.

Result: Convergence to Euler equations occurs if and only if energy dissipation vanishes in a viscosity-proportional boundary layer.

Conclusion: The study successfully extends Kato's criterion to inhomogeneous flows, providing a new tool (relative energy functional) for analysis.

Abstract: In this paper we study the vanishing viscosity limit for the inhomogeneous
incompressible Navier-Stokes equations on bounded domains with no-slip boundary
condition in two or three space dimensions. We show that, under suitable
assumptions on the density, we can establish the convergence in energy space of
Leray-Hopf type solutions of the Navier-Stokes equation to a smooth solution of
the Euler equations if and only if the energy dissipation vanishes on a
boundary layer with thickness proportional to the viscosity. This extends
Kato's criterion for homogeneous Navier-Stokes equations to the inhomogeneous
case. We use a new relative energy functional in our proof.

</details>


### [33] [The half-space problem of evaporation and condensation for polyatomic gases and entropy inequalities](https://arxiv.org/abs/2507.01647)
*Niclas Bernhoff,Stephane Brull,Eddie Wadbro*

Main category: math.AP

TL;DR: The study analyzes the steady Boltzmann equation for a polyatomic gas in a half-space, deriving relations between boundary and far-end macroscopic parameters using conservation laws and entropy. Numerical exploration shows qualitative similarities but quantitative differences based on gas properties.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of a polyatomic gas in a half-space under inflow boundary conditions and its transition to equilibrium, focusing on evaporation or condensation processes.

Method: The study uses the steady Boltzmann equation, conservation laws, and entropy inequality to derive relations between boundary and far-end macroscopic parameters, validated numerically.

Result: Relations between boundary and far-end parameters depend on the Mach number, indicating evaporation or condensation. Numerical results show qualitative consistency but quantitative variations with gas properties.

Conclusion: The study provides insights into gas behavior in a half-space, highlighting the role of macroscopic parameters and internal degrees of freedom in evaporation/condensation processes.

Abstract: This study investigates the steady Boltzmann equation in one spatial variable
for a polyatomic single-component gas in a half-space. Inflow boundary
conditions are assumed at the half-space boundary, where particles entering the
half-space are distributed as a Maxwellian, an equilibrium distribution
characterized by macroscopic parameters of the boundary. At the far end, the
gas tends to an equilibrium distribution, which is also Maxwellian. Using
conservation laws and an entropy inequality, we derive relations between the
macroscopic parameters of the boundary and at infinity required for the
existence of solutions. The relations vary depending on the sign of the Mach
number at infinity, which dictates whether evaporation or condensation takes
place at the interface between the gas and the condensed phase. We explore the
obtained relations numerically. This investigation reveals that, although the
relations are qualitatively similar for various internal degrees of freedom of
the gas, clear quantitative differences exist.

</details>


### [34] [Conformal Green functions and Yamabe metrics of Sobolev regularity](https://arxiv.org/abs/2507.01674)
*Rodrigo Avalos,Albachiara Cogo,Andoni Royo Abrego*

Main category: math.AP

TL;DR: Resolution of the Yamabe problem for rough metrics on 3-manifolds using elliptic theory and blow-up analysis.


<details>
  <summary>Details</summary>
Motivation: To extend the solution of the Yamabe problem to Riemannian metrics of Sobolev class $W^{2,q}$ with $q > 3$ on closed 3-manifolds.

Method: Develop elliptic theory for the conformal Laplacian, analyze existence and regularity, and perform delicate blow-up analysis for its Green function.

Result: Full resolution of the Yamabe problem for specified rough metrics, with broader analytical work applicable to dimensions $n \geq 3$.

Conclusion: The work provides a comprehensive solution to the Yamabe problem for rough metrics and offers tools of independent interest for higher dimensions.

Abstract: We provide a full resolution of the Yamabe problem on closed 3-manifolds for
Riemannian metrics of Sobolev class $W^{2,q}$ with $q > 3$. This requires
developing an elliptic theory for the conformal Laplacian for rough metrics and
establishing existence, regularity and a delicate blow-up analysis for its
Green function. Most of the analytical work is carried out in dimensions $n
\geq 3$ and for $W^{2,q}$ Riemannian metrics with $q>\tfrac{n}{2}$ and should
be of independent interest.

</details>


### [35] [Sharp remainder of the $L^{p}$-Poincaré inequality for Baouendi-Grushin vector fields](https://arxiv.org/abs/2507.01681)
*Kuralay Apseit,Nurgissa Yessirkegenov,Amir Zhangirbayev*

Main category: math.AP

TL;DR: Sharp remainder formula for Poincaré inequality for Baouendi-Grushin vector fields in L^p for complex-valued functions, with applications to nonlinear porous medium equations.


<details>
  <summary>Details</summary>
Motivation: To extend and refine the Poincaré inequality for Baouendi-Grushin vector fields, particularly in L^p spaces, and apply it to nonlinear problems.

Method: Establish a sharp remainder formula, recover known results in special cases, and estimate the remainder term for specific p and n ranges.

Result: Derived estimates for the remainder term and applied them to prove finite-time blow-up and global existence for solutions of a nonlinear porous medium equation.

Conclusion: The results generalize previous findings and provide new insights into the behavior of solutions to degenerate nonlinear operators.

Abstract: In this paper, we establish a sharp remainder formula for the Poincar\'e
inequality for Baouendi-Grushin vector fields in the setting of $L^{p}$ for
complex-valued functions. In special cases, we recover previously known
results. Additionally, we provide estimates of the remainder term for $p\geq2$
and $1<p<2\leq n<\infty$. As an application, we obtain a blow-up in finite time
and global existence of the positive solutions to the initial-boundary value
problem of the doubly nonlinear porous medium equation involving a degenerate
nonlinear operator $\Delta_{\gamma,p}$.

</details>


### [36] [A note on the uniqueness properties of solutions for the Schrödinger-Korteweg de Vries system](https://arxiv.org/abs/2507.01733)
*Eddye Bustamante,José Jiménez Urrea,Jorge Mejía*

Main category: math.AP

TL;DR: The paper proves uniqueness of solutions for a coupled Schrödinger-Korteweg-de Vries system under specific decay conditions at infinity and matching at two distinct times.


<details>
  <summary>Details</summary>
Motivation: To establish conditions under which solutions of the coupled system are unique, ensuring robustness in modeling wave interactions.

Method: Analyzes smooth solutions of the system with decay conditions, comparing solutions at two times to prove uniqueness.

Result: If solutions match in a weighted Sobolev space at two times, they are identical.

Conclusion: The uniqueness of solutions is guaranteed under the given decay and matching conditions.

Abstract: In this work we prove that if $(u_i,v_i)$, $i=1,2$, are smooth enough
solutions of the coupled Schr\"odinger-Korteweg-de Vries system \begin{align*}
\left. \begin{array}{rl} i u_t+\partial_x^2 u &\hspace{-2mm}=\beta uv - |u|^2
u,\\ \partial_t v + \partial_x^3 v &\hspace{-2mm}=\gamma \partial_x
|u|^2-\frac12\partial_x (v^2) \end{array} \right\} \end{align*} with
appropriate decay at infinity such that at two different times $t_0=0$ and
$t_1=1$ satisfy that
$$u_1(0)-u_2(0),u_1(1)-u_2(1),v_1(0)-v_2(0),v_1(1)-v_2(1)\in
H^1(e^{ax^{2}}dx),$$ for $a>0$ big enough, then $u_1=u_2$ and $v_1=v_2$.
  (Let us recall that $f\in H^1(e^{ax^{2}} dx)$ iff $f\in L^2(e^{ax^{2}}dx)$
and $\partial_x f\in L^2(e^{ax^{2}}dx)$).

</details>


### [37] [Existence and concentration of nontrivial solutions for quasilinear Schrödinger equation with indefinite potential](https://arxiv.org/abs/2507.01748)
*Lifeng Yin,Xiaoqi Liu,Yongyong Li*

Main category: math.AP

TL;DR: The paper studies the quasilinear Schrödinger equation with an indefinite potential, proving existence and multiplicity of solutions using variational methods and analyzing their behavior as a parameter approaches zero.


<details>
  <summary>Details</summary>
Motivation: To address the existence and multiplicity of solutions for the quasilinear Schrödinger equation with an indefinite potential, which has applications in physics and engineering.

Method: Combines local linking argument, Morse theory, Moser iteration, and the symmetric Mountain Pass Theorem for odd nonlinearities.

Result: Existence of a nontrivial solution and, for odd nonlinearities, an unbounded sequence of solutions. Analyzes solution concentration as a parameter approaches zero.

Conclusion: The methods successfully establish solution existence and multiplicity, with insights into their behavior under parameter variation.

Abstract: This paper is concerned with the quasilinear Schr\"{o}dinger equation
\begin{align*} -\Delta u+V(x)u+\frac{k}{2}\Delta(u^2)u=f(u)\quad
\text{in}~~\mathbb{R}^N\text{,} \end{align*} where $N\geq 3$, $k>0$, $V\in
C(\R)$ is an indefinite potential. Under structural conditions on the potential
$V$ and the nonlinearity $f$, we establish the existence of a nontrivial
solution through a combination of a local linking argument, Morse theory, and
the Moser iteration. Moreover, if $f$ is odd, we obtain an unbounded sequence
of nontrivial solutions via the symmetric Mountain Pass Theorem. Additionally,
as $k\rightarrow0$, we analyze the concentration behavior of nontrivial
solutions.

</details>


### [38] [Schauder-type estimates and applications](https://arxiv.org/abs/2507.01818)
*Satyanad Kichenassamy*

Main category: math.AP

TL;DR: Schauder estimates are fundamental tools in elliptic PDE theory, providing Hölder regularity for solutions with Hölder continuous data, analogous to Cauchy's theory in function theory.


<details>
  <summary>Details</summary>
Motivation: To generalize derivative estimates of analytic functions and provide a priori bounds for solutions of elliptic PDEs, influencing fields like nonlinear diffusion and differential geometry.

Method: The chapter presents complete proofs of commonly used theorems in Schauder estimates, focusing on Hölder regularity for elliptic problems.

Result: Schauder estimates offer wide-ranging generalizations of derivative bounds, enabling solutions to elliptic PDEs through a priori estimates.

Conclusion: Schauder theory is pivotal in modern PDE theory, equating solving PDEs to obtaining a priori bounds, with applications across multiple fields.

Abstract: The Schauder estimates are among the oldest and most useful tools in the
modern theory of elliptic partial differential equations (PDEs). Their
influence may be felt in practically all applications of the theory of elliptic
boundary-value problems, that is, in fields such as nonlinear diffusion,
potential theory, field theory or differential geometry and its applications.
Schauder estimates give H\"older regularity estimates for solutions of elliptic
problems with H\"older continuous data; they may be thought of as wide-ranging
generalizations of estimates of derivatives of an analytic function in the
interior of its domain of analyticity and play a role comparable to that of
Cauchy's theory in function theory. They may be viewed as converses to the
mean-value theorem: a bound on the solution gives a bound on its derivatives.
Schauder theory has strongly contributed to the modern idea that solving a PDE
is equivalent to obtaining an a priori bound that is, trying to estimate a
solution before any solution has been constructed. The chapter presents the
complete proofs of the most commonly used theorems used in actual applications
of the estimates.

</details>


### [39] [Muckenhoupt-weighted $L_q(L_p)$ boundedness for time-space fractional nonlocal operators](https://arxiv.org/abs/2507.01890)
*Yong Zhen Yang,Yong Zhou*

Main category: math.AP

TL;DR: The paper establishes weighted $L_q(L_p)$ estimates for solutions to a fractional evolution equation using harmonic analysis techniques.


<details>
  <summary>Details</summary>
Motivation: To extend and complement prior work on fractional evolution equations by providing weighted estimates for solutions.

Method: Uses harmonic analysis tools like Fefferman-Stein inequality, Hardy-Littlewood maximal estimates, and sharp function estimates in weighted $L_q(L_p)$ spaces.

Result: Proves an inequality bounding the weighted norm of the solution in terms of the weighted norm of the forcing term.

Conclusion: The results generalize previous findings and contribute to the understanding of fractional evolution equations in weighted spaces.

Abstract: Based on the $\phi(\Delta)$-type operator studied by Kim
\cite[\emph{Adv.~Math.}]{Kim2}, where $\phi$ is a Bernstein function, we
establish weighted $L_{q}(L_{p})$ estimates for solutions to the following
fractional evolution equation: $$ \partial_{t}^{\alpha}w(t,x) =
\phi(\Delta)w(t,x) + h(t,x), \quad t > 0, \; x \in \mathbb{R}^{d}, $$ where
$\partial_{t}^{\alpha}$ denotes the Caputo derivative of $0 < \alpha < 1$. To
be specific, for all $1 < p, q < \infty$, we demonstrate that $$
\int_{0}^{\infty} \left( \int_{\mathbb{R}^{d}} \left| \phi(\Delta)w \right|^{p}
\mu_{1}(x) \, dx \right)^{\frac{q}{p}} \mu_{2}(t) \, dt \leq C
\int_{0}^{\infty} \left( \int_{\mathbb{R}^{d}} |h|^{p} \mu_{1}(x) \, dx
\right)^{\frac{q}{p}} \mu_{2}(t) \, dt, $$ where $\mu_{1}(x) \in
A_{p}(\mathbb{R}^{d})$ and $\mu_{2}(t) \in A_{q}(\mathbb{R})$ are
\emph{Muckenhoupt} weights.~Our proof relies on harmonic analysis techniques,
using fundamental tools including the \emph{Fefferman-Stein} inequality and
\emph{Hardy-Littlewood} maximal estimates in weighted $L_q(L_p)$ spaces, and
\emph{sharp function} estimates for solution operators. In particular, our
results extend the work of Han and Kim (2020, J. Differ. Equ.,269:3515-3550)
and complement the work of Dong (2023, Calc. Var. Partial Differ. Equ., 62:96).

</details>


### [40] [Local Hölder Regularity for Quasilinear Elliptic Equations with Mixed Local-Nonlocal Operators, Variable Exponents, and Weights](https://arxiv.org/abs/2507.01899)
*Juan Pablo Alcon Apaza*

Main category: math.AP

TL;DR: The paper proves local boundedness and Hölder continuity of weak solutions to a mixed local-nonlocal equation with variable exponents and weights.


<details>
  <summary>Details</summary>
Motivation: To extend regularity results for weak solutions to equations combining local and nonlocal operators with variable exponents and weights.

Method: Adapts the De Giorgi-Nash-Moser theory to a mixed local-nonlocal framework with variable exponents and weights.

Result: Establishes local boundedness and local Hölder continuity of weak solutions.

Conclusion: The adapted De Giorgi-Nash-Moser theory successfully extends regularity results to this complex framework.

Abstract: We establish local boundedness and local H\"older continuity of weak
solutions to the following prototype problem:
  $$ -\operatorname{div}\left(|x|^{-2 \beta}|\nabla u|^{\mathbf{q}-2} \nabla
u\right)+(-\Delta)_{p(\cdot, \cdot), \beta}^{s(\cdot, \cdot)} u=0 \quad \text {
in } \quad \Omega, $$
  where $\Omega \subset \mathbb{R}^n, n \geq 2$, is a bounded domain. The
nonlocal operator is defined by
  $$ (-\Delta)_{p(\cdot, \cdot), \beta}^{s(\cdot, \cdot)} u(x):=\mathrm{P} .
\mathrm{V} . \int_{\Omega} \frac{|u(x)-u(y)|^{p(x,
y)-2}(u(x)-u(y))}{|x-y|^{n+s(x, y) p(x, y)}} \frac{1}{|x|^\beta|y|^\beta}
\mathrm{d} y $$
  Here, $p: \Omega \times \Omega \rightarrow(1, \infty)$ and $s: \Omega \times
\Omega \rightarrow(0,1)$ are measurable functions,
$\mathbf{q}:=\operatorname{ess}_{\Omega \times \Omega} p$, and $0 \leq
\beta<n$. Our approach is analytic and relies on an adaptation of the De
Giorgi-Nash-Moser theory to a mixed local-nonlocal framework with variable
exponents and weights.

</details>


### [41] [Initial boundary value problem for a system derived from Eulerian droplet model for air particle flow](https://arxiv.org/abs/2507.01920)
*Kayyunnapara Divya Joseph*

Main category: math.AP

TL;DR: Existence of weak asymptotic solutions for a non-strictly hyperbolic system derived from an Eulerian droplet model, using vanishing viscosity and Hopf-Cole transformation. Explicit weak solutions derived for BV initial data.


<details>
  <summary>Details</summary>
Motivation: To address the initial boundary value problem for a non-strictly hyperbolic system from an Eulerian droplet model, focusing on weak solutions.

Method: Vanishing viscosity regularization, Hopf-Cole transformation, and Hopf-Lax formula for BV initial data.

Result: Existence of weak asymptotic solutions for bounded measurable initial velocity and integrable volume fraction. Explicit weak solutions for BV initial data.

Conclusion: The methods successfully provide weak solutions for the system, with explicit formulas for BV initial data.

Abstract: In this work, we study the initial boundary value problem for a non-strictly
hyperbolic $2\times2$ system of equations in the quarter plane $x>0,t>0$ which
is derived from Eulerian droplet model for air particle flow for velocity and
volume fraction. We show the existence of weak asymptotic solutions to the
initial value problem to the system using a regularisation, by a vanishing
viscosity method when the initial velocity is bounded measurable, the initial
volume fraction is integrable and the boundary data are bounded measurable.
Here we use a generalization of the Hopf-Cole transformation. We also derive an
explicit formula for the weak solution when the initial data are functions of
bounded variation, the boundary datas are bounded and locally in the class of
Lipschitz continuous functions. This construction involves the Hopf-Lax formula
for the boundary value problem for the Burgers equation and the product of a
bounded variation function with derivative of another bounded variation
function using non-conservative Volpert product.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [42] [An Algorithm for Automated Extraction of Resonance Parameters from the Stabilization Method](https://arxiv.org/abs/2507.01382)
*Johanna Langner,Anjan Sadhukhan,Jayanta K. Saha,Henryk A. Witek*

Main category: physics.comp-ph

TL;DR: The paper introduces \dosmax, a Python-based algorithm automating the stabilization method for analyzing resonance states, demonstrated using helium's doubly-excited states.


<details>
  <summary>Details</summary>
Motivation: The stabilization method for resonance analysis is labor-intensive and requires numerical precision, prompting the need for an automated, efficient solution.

Method: The authors developed \dosmax, an open-access Python code, to automate the stabilization diagram analysis while ensuring high numerical accuracy.

Result: \dosmax successfully analyzed helium's doubly-excited resonance states, proving its accuracy and efficiency.

Conclusion: The algorithm is versatile and applicable to various atomic, molecular, and nuclear resonance systems.

Abstract: The application of the stabilization method [A.~U.\ Hazi and H.~S.\ Taylor,
Phys.~Rev.~A {\bf 1}, 1109 (1970)]) to extract accurate energy and lifetimes of
resonance states is challenging: The process requires labor-intensive numerical
manipulation of a large number of eigenvalues of a parameter-dependent
Hamiltonian matrix, followed by a fitting procedure. In this article, we
present \dosmax, an efficient algorithm implemented as an open-access
\texttt{Python} code, which offers full automation of the stabilization diagram
analysis in a user-friendly environment while maintaining high numerical
precision of the computed resonance characteristics. As a test case, we use
\dosmax to analyze the natural parity doubly-excited resonance states
(${}^{1}\textnormal{S}^{\textnormal{e}}$,
${}^{3}\textnormal{S}^{\textnormal{e}}$,
${}^{1}\textnormal{P}^{\textnormal{o}}$, and
${}^{3}\textnormal{P}^{\textnormal{o}}$) of helium, demonstrating the accuracy
and efficiency of the developed methodology. The presented algorithm is
applicable to a wide range of resonances in atomic, molecular, and nuclear
systems.

</details>


### [43] [Improved energies and wave function accuracy with Weighted Variational Monte Carlo](https://arxiv.org/abs/2507.01905)
*Huan Zhang,Robert J. Webber,Michael Lindsey,Timothy C. Berkelbach,Jonathan Weare*

Main category: physics.comp-ph

TL;DR: The paper introduces a weighted VMC method to improve wavefunction accuracy in probability tails, reducing errors significantly compared to traditional VMC.


<details>
  <summary>Details</summary>
Motivation: Traditional VMC methods optimize wave functions only in peak probability regions, leaving tails uncontrolled, which limits accuracy.

Method: The paper interprets VMC as a gradient flow followed by a projection step, allowing arbitrary probability distributions to prioritize accuracy in specific regions.

Result: Weighted VMC reduces ground state energy error by a factor of 2 and local energy errors by factors of 10^2--10^4 in the antiferromagnetic Heisenberg model.

Conclusion: The weighted VMC method significantly improves accuracy in probability tails, demonstrating its effectiveness for variational Monte Carlo applications.

Abstract: Neural network parametrizations have increasingly been used to represent the
ground and excited states in variational Monte Carlo (VMC) with promising
results. However, traditional VMC methods only optimize the wave function in
regions of peak probability. The wave function is uncontrolled in the tails of
the probability distribution, which can limit the accuracy of the trained
wavefunction approximation. To improve the approximation accuracy in the
probability tails, this paper interprets VMC as a gradient flow in the space of
wave functions, followed by a projection step. From this perspective, arbitrary
probability distributions can be used in the projection step, allowing the user
to prioritize accuracy in different regions of state space. Motivated by this
theoretical perspective, the paper tests a new weighted VMC method on the
antiferromagnetic Heisenberg model for a periodic spin chain. Compared to
traditional VMC, weighted VMC reduces the error in the ground state energy by a
factor of 2 and it reduces the errors in the local energies away from the mode
by large factors of $10^2$--$10^4$.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [44] [Quadruple Beltrami field structures in electron-positron multi-ion plasma](https://arxiv.org/abs/2507.01283)
*Farhat Saleem,M. Iqbal,Usman Shazad*

Main category: physics.plasm-ph

TL;DR: The paper derives and analyzes a quadruple Beltrami (QB) equilibrium state in a four-component plasma, revealing multiscale vortex formation and controllable magnetofluid coupling.


<details>
  <summary>Details</summary>
Motivation: To explore the relaxed state of a four-component plasma (electrons, positrons, H+ ions, O-2 ions) and its potential for multiscale self-organized structures.

Method: Derived a QB equilibrium state as a superposition of four single Beltrami fields, analyzing its properties and behavior.

Result: The QB state enables four distinct vortices and robust magnetofluid coupling, with control over multiscale structures and paramagnetic/diamagnetic behavior via helicities and densities.

Conclusion: The QB state offers a versatile framework for controlling plasma dynamics and multiscale vortex formation in complex plasmas.

Abstract: A quadruple Beltrami (QB) equilibrium state for a four-component plasma that
consists of inertial electrons, positrons, lighter positive (H+) ions and
heavier negative ions (O-2) is derived and investigated. The QB relaxed state
is a linear superposition of four distinct single Beltrami fields and provides
the possibility of the formation of four self-organized vortices of different
length scales. In addition, robust magnetofluid coupling characterizes this
non-force-free state. The analysis of the QB state also shows that by adjusting
the generalized helicities and densities of plasma species, the formation of
multiscale structures as well as the paramagnetic and diamagnetic behavior of
the relaxed state can be controlled.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [45] [LOMS.cz: A computational platform for high-throughput Classical and Combinatorial Judd-Ofelt analysis and rare-earth spectroscopy](https://arxiv.org/abs/2507.01218)
*Jan Hrabovský,Petr Vařák,Robin Kryštůfek*

Main category: cond-mat.mtrl-sci

TL;DR: LOMS.cz is an open-source platform standardizing Judd-Ofelt calculations for rare-earth spectroscopy, featuring automated computations, an experimental database, and a novel C-JO algorithm.


<details>
  <summary>Details</summary>
Motivation: The lack of standardized computational methodologies for Judd-Ofelt calculations in rare-earth spectroscopy hinders precise and reproducible parameter determination.

Method: LOMS integrates automated JO parameter computation, a dynamic experimental database, and a C-JO algorithm for optimal absorption band combinations.

Result: The platform enables rapid screening of spectroscopic parameters and reliable prediction of optical properties.

Conclusion: LOMS.cz provides a standardized tool for accelerating the discovery of rare-earth-based photonic materials.

Abstract: We present LOMS.cz (Luminescence, Optical and Magneto-optical Software), an
open-source computational platform that addresses the long-standing challenge
of standardizing Judd-Ofelt (JO) calculations in rare-earth spectroscopy.
Despite JO theory's six-decade history as the fundamental framework for
understanding $4f\leftrightarrow4f$ transitions, the field lacks standardized
computational methodologies for precise and reproducible parameter
determination. LOMS integrates three key innovations: (1) automated computation
of JO parameters, transition probabilities, branching ratios, and theoretical
radiative lifetimes, (2) a dynamically expanding database of experimentally
validated parameters enabling direct comparison between computed and empirical
results, and (3) a novel Combinatorial JO (C-JO) analysis algorithm that
systematically identifies optimal absorption band combinations to ensure
reliable parameter extraction. As a proof-of-concept, we demonstrate how this
computational framework enables rapid screening of spectroscopic parameters,
allowing researchers to predict optical properties with enhanced reliability.
By combining automated analysis with experimental validation through its
integrated database, LOMS.cz establishes a standardized platform for
accelerating the discovery and optimization of rare-earth-based photonic and
optoelectronic materials.

</details>


### [46] [Electronic structure and defect properties of Bi-doped GaN: origins of photoluminescence and optical absorption](https://arxiv.org/abs/2507.01346)
*Yujie Liu,Ishtiaque Ahmed Navid,Zetian Mi,Emmanouil Kioupakis*

Main category: cond-mat.mtrl-sci

TL;DR: The paper explores defect-related optical phenomena in Bi-incorporated GaN using hybrid density functional theory, revealing the role of bismuth defects and their complexes in absorption and emission.


<details>
  <summary>Details</summary>
Motivation: To address the lack of theoretical insights into defect-related optical absorption and emission in extreme lattice-mismatched III-V nitrides like Bi-doped GaN.

Method: Hybrid density functional theory is applied to study substitutional bismuth defects (Bi_Ga and Bi_N) and their complexes with native vacancies in GaN.

Result: The study identifies charge-compensated defect complexes (Bi_N + V_Ga)^(3±) as key to stabilizing anionic bismuth, explaining observed absorption peaks (~1.11 eV, ~3.17 eV) and band-edge emissions (~2.0 eV, ~2.5 eV).

Conclusion: The findings clarify defect-level physics in Bi-doped GaN and offer practical guidelines for controlling Bi incorporation, aiding experimental efforts.

Abstract: Extreme lattice-mismatched III-V nitrides, such as Bi-incorporated GaN, have
been realized experimentally thanks to recent advances in epitaxial growth and
characterization techniques. However, theoretical insights into defect-related
optical absorption and emission phenomena in these materials remain scarce.
Here, we apply hybrid density functional theory to systematically explore the
role of substitutional bismuth atoms on both cationic $\mathrm{Bi_{Ga}}$ and
anionic $\mathrm{Bi_{N}}$ sites in Bi-incorporated GaN, as well as their
complexes with native vacancies. Our calculations reveal that the
charge-compensated defect complexes $(\mathrm{Bi_{N}} + \mathrm{V_{Ga}})^{3-}$
and $(\mathrm{Bi_{N}} + \mathrm{V_{Ga}})^{3+}$ stabilize anionic bismuth
incorporation, accounting for the experimentally observed absorption peaks at
~1.11 eV and ~3.17 eV. We further uncover the origins of the reported band-edge
emissions near 2.0 eV and 2.5 eV by examining various charge states of
$\mathrm{Bi_{Ga}}$ and $\mathrm{Bi_{N}}$ centers. Our findings elucidate the
defect-level physics of Bi-doped GaN and provide practical guidelines for
controlling the incorporation of Bi into GaN.

</details>


### [47] [Quantum Anharmonic Effects in Hydrogen-Bond Symmetrization of High-Pressure Ice](https://arxiv.org/abs/2507.01452)
*Qi Zhang,Lei Wang*

Main category: cond-mat.mtrl-sci

TL;DR: Nuclear quantum effects in hydrogen influence ice phase stability, lowering the transition pressure from classical predictions. The Perdew-Burke-Ernzerhof functional underestimates this effect, while neural canonical transformation reveals temperature-independent transition pressures and bond softening under pressure.


<details>
  <summary>Details</summary>
Motivation: To understand the role of nuclear quantum effects in hydrogen on the phase stability of water ice, particularly the transition from ice VIII to ice X under high pressure.

Method: Uses quantum Monte Carlo calculations and neural canonical transformation (NCT) to analyze hydrogen-bond symmetrization and anharmonicity, comparing results from different functionals.

Result: Quantum effects reduce the transition pressure from 100 GPa to 60 GPa. The Perdew-Burke-Ernzerhof functional underestimates this pressure by over 10 GPa. NCT shows temperature-independent transition and bond softening under pressure.

Conclusion: Nuclear quantum effects are crucial for accurate predictions of ice phase transitions, with NCT providing deeper insights into hydrogen bond behavior under pressure.

Abstract: The nuclear quantum effects of hydrogen play a significant role in
determining the phase stability of water ice. Hydrogen-bond symmetrization
occurs as hydrogen atoms tunnel in a double-well potential, ultimately
occupying the midpoint between oxygen atoms and transforming ice VIII into ice
X under high pressure. Quantum fluctuations lower this transition from
classical predictions of over 100 GPa to 60 GPa. We reveal that the
Perdew-Burke-Ernzerhof functional underestimates the hydrogen double-well
barrier, thus resulting in a transition pressure over 10 GPa lower than the
strongly constrained and appropriately normed functional, which is validated
against quantum Monte Carlo calculations. Nuclear quantum anharmonicity,
treated via neural canonical transformation (NCT), reveals that this transition
pressure is temperature-independent and observes a 2 GPa reduction when
comparing the non-Gaussian flow models based wavefunction compared to the
self-consistent harmonic approximation. Despite increasing pressure typically
shortens chemical bonds and hardens phonon modes, NCT calculations reveal that
hydrogen bond softens hydrogen-oxygen stretching in ice VIII upon pressure.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [48] [Tensor Decomposition Networks for Fast Machine Learning Interatomic Potential Computations](https://arxiv.org/abs/2507.01131)
*Yuchao Lin,Cong Fu,Zachary Krueger,Haiyang Yu,Maho Nakata,Jianwen Xie,Emine Kucukbenli,Xiaofeng Qian,Shuiwang Ji*

Main category: cs.LG

TL;DR: TDNs replace expensive CG tensor products in SO(3)-equivariant networks with low-rank decompositions, reducing computational cost while maintaining equivariance and universality.


<details>
  <summary>Details</summary>
Motivation: To accelerate the computationally expensive Clebsch-Gordan (CG) tensor products in SO(3)-equivariant networks for MLIPs.

Method: Develop Tensor Decomposition Networks (TDNs) using low-rank decompositions (e.g., CP decomposition) and path-weight sharing to reduce parameters and complexity.

Result: TDNs achieve competitive performance on datasets like PubChemQCR, OC20, and OC22, with significant computational speedup (O(L^4) vs. O(L^6)).

Conclusion: TDNs offer an efficient, plug-and-play alternative for tensor products in existing networks, balancing performance and computational cost.

Abstract: $\rm{SO}(3)$-equivariant networks are the dominant models for machine
learning interatomic potentials (MLIPs). The key operation of such networks is
the Clebsch-Gordan (CG) tensor product, which is computationally expensive. To
accelerate the computation, we develop tensor decomposition networks (TDNs) as
a class of approximately equivariant networks whose CG tensor products are
replaced by low-rank tensor decompositions, such as the CANDECOMP/PARAFAC (CP)
decomposition. With the CP decomposition, we prove (i) a uniform bound on the
induced error of $\rm{SO}(3)$-equivariance, and (ii) the universality of
approximating any equivariant bilinear map. To further reduce the number of
parameters, we propose path-weight sharing that ties all multiplicity-space
weights across the $O(L^3)$ CG paths into a single path without compromising
equivariance, where $L$ is the maximum angular degree. The resulting layer acts
as a plug-and-play replacement for tensor products in existing networks, and
the computational complexity of tensor products is reduced from $O(L^6)$ to
$O(L^4)$. We evaluate TDNs on PubChemQCR, a newly curated molecular relaxation
dataset containing 105 million DFT-calculated snapshots. We also use existing
datasets, including OC20, and OC22. Results show that TDNs achieve competitive
performance with dramatic speedup in computations.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [49] [Randomized subspace correction methods for convex optimization](https://arxiv.org/abs/2507.01415)
*Boou Jiang,Jongho Park,Jinchao Xu*

Main category: math.OC

TL;DR: A framework for randomized subspace correction methods in convex optimization, unifying domain decomposition, multigrid, and block coordinate descent, with convergence analysis under various assumptions.


<details>
  <summary>Details</summary>
Motivation: To generalize and unify existing algorithms for convex optimization, addressing limitations like nonoverlapping decompositions and restricted problem settings.

Method: Introduces a framework for randomized subspace correction, allowing arbitrary space decompositions, inexact local solvers, and problems with limited smoothness or convexity.

Result: Provides convergence rate analysis under minimal to practical assumptions, extending applicability to diverse problems.

Conclusion: The framework is broadly applicable, enhancing flexibility and generality in solving convex optimization problems across multiple domains.

Abstract: This paper introduces an abstract framework for randomized subspace
correction methods for convex optimization, which unifies and generalizes a
broad class of existing algorithms, including domain decomposition, multigrid,
and block coordinate descent methods. We provide a convergence rate analysis
ranging from minimal assumptions to more practical settings, such as sharpness
and strong convexity. While most existing studies on block coordinate descent
methods focus on nonoverlapping decompositions and smooth or strongly convex
problems, our framework extends to more general settings involving arbitrary
space decompositions, inexact local solvers, and problems with limited
smoothness or convexity. The proposed framework is broadly applicable to convex
optimization problems arising in areas such as nonlinear partial differential
equations, imaging, and data science.

</details>


### [50] [A first-order method for nonconvex-nonconcave minimax problems under a local Kurdyka-Łojasiewicz condition](https://arxiv.org/abs/2507.01932)
*Zhaosong Lu,Xiangyuan Wang*

Main category: math.OC

TL;DR: The paper addresses nonconvex-nonconcave minimax problems with a local KL condition, proposing an inexact proximal gradient method for solving them and providing complexity guarantees.


<details>
  <summary>Details</summary>
Motivation: The study aims to tackle practical scenarios where global KL or PL conditions are too restrictive, focusing on local KL conditions that introduce analytical challenges.

Method: An inexact proximal gradient method is developed, leveraging the local Hölder smoothness of the maximal function and solving KL-structured subproblems for gradient computation.

Result: The method achieves complexity guarantees for computing approximate stationary points under mild assumptions.

Conclusion: The approach accommodates broader practical scenarios by addressing the challenges of local KL conditions, offering a viable solution for such minimax problems.

Abstract: We study a class of nonconvex-nonconcave minimax problems in which the inner
maximization problem satisfies a local Kurdyka-{\L}ojasiewicz (KL) condition
that may vary with the outer minimization variable. In contrast to the global
KL or Polyak-{\L}ojasiewicz (PL) conditions commonly assumed in the literature
-- which are significantly stronger and often too restrictive in practice --
this local KL condition accommodates a broader range of practical scenarios.
However, it also introduces new analytical challenges. In particular, as an
optimization algorithm progresses toward a stationary point of the problem, the
region over which the KL condition holds may shrink, resulting in a more
intricate and potentially ill-conditioned landscape. To address this challenge,
we show that the associated maximal function is locally H\"older smooth.
Leveraging this key property, we develop an inexact proximal gradient method
for solving the minimax problem, where the inexact gradient of the maximal
function is computed by applying a proximal gradient method to a KL-structured
subproblem. Under mild assumptions, we establish complexity guarantees for
computing an approximate stationary point of the minimax problem.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [51] [Martingales and Path-Dependent PDEs via Evolutionary Semigroups](https://arxiv.org/abs/2507.01845)
*Robert Denk,Markus Kunze,Michael Kupper*

Main category: math.PR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this article, we develop a semigroup-theoretic framework for the analytic
characterisation of martingales with path-dependent terminal conditions. Our
main result establishes that a measurable adapted process of the form \[ V(t) -
\int_0^t\Psi(s)\, ds \] is a martingale with respect to an expectation operator
$\mathbb{E}$ if and only if a time-shifted version of $V$ is a mild solution of
a final value problem involving a path-dependent differential operator that is
intrinsically connected to $\mathbb{E}$. We prove existence and uniqueness of
strong and mild solutions for such final value problems with measurable
terminal conditions using the concept of evolutionary semigroups. To
characterise the compensator $\Psi$, we introduce the notion of
$\mathbb{E}$-derivative of $V$, which in special cases coincides with Dupire's
time derivative. We also compare our findings to path-dependent partial
differential equations in terms of Dupire derivatives such as the
path-dependent heat equation.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [52] [Reconstruction of the observable universe from the integrated Sachs-Wolfe effect](https://arxiv.org/abs/2507.01399)
*Julianne Chung,Yiran Wang*

Main category: math-ph

TL;DR: The paper explores using a tomographic approach to infer gravitational perturbations from the ISW effect in the CMB, akin to medical X-ray CT reconstruction.


<details>
  <summary>Details</summary>
Motivation: To determine if gravitational perturbations can be inferred from the ISW effect observed near Earth.

Method: Develops mathematical analysis for stable inversion of the X-ray transform in cosmology and tests reconstruction methods numerically.

Result: Demonstrates feasibility and potential of the tomography method for cosmological applications.

Conclusion: The tomographic approach shows promise for analyzing the ISW effect and gravitational perturbations.

Abstract: The integrated Sachs-Wolfe (ISW) effect is a property of the Cosmic Microwave
Background (CMB), in which photons from the CMB are gravitationally redshifted,
causing the anisotropies in the CMB. An intriguing question is whether one can
infer the gravitational perturbations from the ISW effect observed near the
Earth. In this work, we address the question using a tomographic reconstruction
approach, similar to X-ray CT reconstruction in medical imaging. We develop the
mathematical analysis for the stable inversion of the X-ray transform in the
cosmological setting. In addition, we provide a numerical study of
reconstruction methods, thereby demonstrating the feasibility and potential of
the tomography method.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [53] [A generative modeling / Physics-Informed Neural Network approach to random differential equations](https://arxiv.org/abs/2507.01687)
*Georgios Arampatzis,Stylianos Katsarakis,Charalambos Makridakis*

Main category: stat.ML

TL;DR: The paper introduces a method to enhance Physics-Informed Neural Networks (PINNs) by integrating probabilistic frameworks for better uncertainty modeling in complex systems.


<details>
  <summary>Details</summary>
Motivation: To improve uncertainty quantification in computational science by combining SciML and UQ techniques.

Method: Incorporates generative modeling with PINNs to systematically control uncertainty while preserving predictive accuracy.

Result: Demonstrated effectiveness through applications to random differential equations and random PDEs.

Conclusion: The approach successfully advances PINNs by enabling robust uncertainty modeling in complex systems.

Abstract: The integration of Scientific Machine Learning (SciML) techniques with
uncertainty quantification (UQ) represents a rapidly evolving frontier in
computational science. This work advances Physics-Informed Neural Networks
(PINNs) by incorporating probabilistic frameworks to effectively model
uncertainty in complex systems. Our approach enhances the representation of
uncertainty in forward problems by combining generative modeling techniques
with PINNs. This integration enables in a systematic fashion uncertainty
control while maintaining the predictive accuracy of the model. We demonstrate
the utility of this method through applications to random differential
equations and random partial differential equations (PDEs).

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [54] [Spectral Learning of Magnetized Plasma Dynamics: A Neural Operator Application](https://arxiv.org/abs/2507.01388)
*Roberta Duarte,Rodrigo Nemmen,Reinaldo Santos-Lima*

Main category: astro-ph.HE

TL;DR: FNOs effectively model magnetized turbulence in 2-D MHD, outperforming UNet and finite-volume solvers in accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: Explore the efficacy of Fourier neural operators (FNOs) for magnetized turbulence, a largely unexplored area.

Method: Train an FNO surrogate on the 2-D Orszag-Tang vortex across viscosities and magnetic diffusivities.

Result: Achieves low error rates, reproduces energy spectra, and offers 25x speed-up over finite-volume solvers.

Conclusion: FNOs provide a practical, efficient tool for rapid MHD parameter sweeps.

Abstract: Fourier neural operators (FNOs) provide a mesh-independent way to learn
solution operators for partial differential equations, yet their efficacy for
magnetized turbulence is largely unexplored. Here we train an FNO surrogate for
the 2-D Orszag-Tang vortex, a canonical non-ideal magnetohydrodynamic (MHD)
benchmark, across an ensemble of viscosities and magnetic diffusivities. On
unseen parameter settings the model achieves a mean-squared error of $\approx 6
\times 10^{-3}$ in velocity and $\approx 10^{-3}$ in magnetic field, reproduces
energy spectra and dissipation rates within $96\%$ accuracy, and retains
temporal coherence over long timescales. Spectral analysis shows accurate
recovery of large- and intermediate-scale structures, with degradation at the
smallest resolved scales due to Fourier-mode truncation. Relative to a UNet
baseline the FNO cuts error by $97\%$, and compared with a high-order
finite-volume solver it delivers a $25\times$ inference speed-up, offering a
practical path to rapid parameter sweeps in MHD simulations.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [55] [Coupled Fire-Cloud Simulations Reveal Mechanisms of Pyrocumulonimbus Clouds](https://arxiv.org/abs/2507.01237)
*Qing Wang,Cenk Gazen,Matthias Ihme,Robert Carver,Jeffrey B. Parker,Tapio Schneider,Sheide Chammas,Yi-Fan Chen,John Anderson*

Main category: physics.ao-ph

TL;DR: Fuel moisture acts as an energy sink in pyroCb events, suppressing fire intensity and cloud development. A feedback loop (SAFIR) intensifies fires via downdrafts, with weak upper-level winds favoring rapid fire growth.


<details>
  <summary>Details</summary>
Motivation: Understanding pyroCb firestorms is critical due to their unpredictable threats. Current models lack accurate fire-atmosphere coupling, and the role of fuel moisture is disputed.

Method: A fine-scale cloud model coupled with a fire model incorporating combustion physics was used to simulate pyroCb-fire interactions.

Result: Fuel moisture is an energy sink, not a source, and the SAFIR mechanism intensifies fires via downdrafts. Weak upper-level winds enhance fire growth.

Conclusion: The study provides a new framework for pyroCb behavior, improving prediction of these extreme events.

Abstract: Pyrocumulonimbus (pyroCb) firestorms occur when the updraft from a wildfire
forms a precipitating cloud. These clouds create unpredictable winds, posing
threats to life and property. The coupled fire-atmosphere interactions that
govern the behavior of pyroCb are incompletely understood, with previous
simulations either treating the fire as static or using semi-empirical rate of
spread models. It is disputed whether fuel moisture serves as an energy source
or sink for pyroCB events. We use a fine-scale cloud model coupled with a fire
model with combustion physics to simulate and analyze the co-evolution of
pyroCb and its associated fire. Here we show fuel moisture acts as an energy
sink attenuating fire intensity and suppressing pyroCb development, and we also
identify and quantify a positive feedback loop, the self-amplifying
fire-induced recirculation (SAFIR) mechanism, where precipitation-induced
downdrafts intensify the parent fire. Our results demonstrate fuel moisture is
an insignificant source of moisture for the pyroCb, contradicting previous
forecasting guidance. Our analysis of the SAFIR mechanism shows weak
upper-level winds favor a pyroCb downdraft close to the parent fire, creating
conditions for rapid intensification. These findings provide a new mechanistic
framework for understanding pyroCb behavior, offering insight towards improved
prediction of these extreme events.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [56] [More sophisticated is not always better: comparison of similarity measures for unsupervised learning of pathways in biomolecular simulations](https://arxiv.org/abs/2507.01725)
*Miriam Jäger,Steffen Wolf*

Main category: cond-mat.soft

TL;DR: The paper evaluates four similarity measures (Euclidean, Wasserstein, Procrustes, and dynamical time warping) for clustering molecular simulation trajectories, finding Euclidean and Wasserstein distances most effective in different systems.


<details>
  <summary>Details</summary>
Motivation: To identify effective similarity measures for clustering trajectories in molecular simulations, particularly for ligand unbinding pathways.

Method: Four similarity measures (Euclidean, Wasserstein, Procrustes, dynamical time warping) were tested on trajectory data from biased simulations (targeted MD and steered MD) in two systems: streptavidin-biotin and A2a receptor-inhibitor.

Result: Wasserstein distances performed best in the streptavidin-biotin system, while Euclidean distances sufficed for the A2a receptor-inhibitor system. Both measures were computationally efficient.

Conclusion: The choice of similarity measure depends on system complexity, with simpler measures like Euclidean distances often sufficient for meaningful clustering.

Abstract: Finding process pathways in molecular simulations such as the unbinding paths
of small molecule ligands from their binding sites at protein targets in a set
of trajectories via unsupervised learning approaches requires the definition of
a suitable similarity measure between trajectories. We here evaluate the
performance of four such measures with varying degree of sophistication, i.e.,
Euclidean and Wasserstein distances, Procrustes analysis and dynamical time
warping, when analyzing trajectory data from two different biased simulation
driving protocols in the form of constant velocity constraint targeted MD and
steered MD. In a streptavidin-biotin benchmark system with known ground truth
clusters, Wasserstein distances yielded the best clustering performance,
closely followed by Euclidean distances, both being the most computationally
efficient similarity measures. In a more complex A2a receptor-inhibitor system,
however, the simplest measure, i.e., Euclidean distances, was sufficient to
reveal meaningful and interpretable clusters.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [57] [Simulation and analysis of turbulent flame and its effect on the wall of aero engine combustor](https://arxiv.org/abs/2507.01892)
*A. Mokhtari,A. Abdallah-elhirtsi,F. Larbi,R. Renane,R. Allouche*

Main category: physics.flu-dyn

TL;DR: Simulation of turbulent flame behavior in an aeronautical combustion chamber using Ansys-Fluent, focusing on flame structure, thermodynamic parameters, and thermal expansion analysis.


<details>
  <summary>Details</summary>
Motivation: To analyze flame structure and thermodynamic behavior in the ALLISON-T56 turboprop combustion chamber under specific pressure and temperature conditions.

Method: Numerical simulation using Ansys-Fluent with the K-e turbulence model and Ansys-Workbench for geometry. Transient temperature, thermal expansion, and stresses are analyzed.

Result: Results compared with literature to validate findings on flame behavior and thermodynamic parameters.

Conclusion: The study successfully simulates and analyzes turbulent flame behavior, providing insights into combustion chamber performance and material stresses.

Abstract: The main objective of this study is to simulate the behavior of the reactive
flow of the turbulent flame in aeronautical combustion chamber of the
ALLISON-T56 turboprop, and contribute to the analysis of flame structure and
determine for given pressure and temperature of fresh gas the behavior of the
thermodynamic parameters of combustion. The numerical approach is based on the
resolution of basic equations of turbulent combustion using Ansys-Fluent code
where the turbulence model K-e is chosen, the geometry of the combustion
chamber is made using Ansys-workbench software. Thereafter, we simulate the
transient temperature field through the wall of a tubular combustion chamber,
and the characterization of the thermal expansion, the thermoelastic stresses
and strains with the physical properties of refractory materials. The obtained
results are then compared with the results of the scientific literature

</details>


<div id='cond-mat.supr-con'></div>

# cond-mat.supr-con [[Back]](#toc)

### [58] [Electron-phonon vertex correction effect in superconducting H3S](https://arxiv.org/abs/2507.01897)
*Shashi B. Mishra,Hitoshi Mori,Elena R. Margine*

Main category: cond-mat.supr-con

TL;DR: The paper extends the Migdal-Eliashberg formalism by including vertex corrections to electron-phonon interactions, showing significant impact on H3S's Tc but negligible effect on Pb, validating non-adiabatic corrections for high-Tc hydrides.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of the adiabatic assumption in the Migdal-Eliashberg formalism for phonon-mediated superconductivity, especially in systems like H3S where it breaks down.

Method: Incorporated first-order vertex corrections to electron-phonon interactions within the Eliashberg formalism and used first-principles calculations for H3S and Pb.

Result: Vertex corrections significantly affect H3S's Tc, aligning with experiments, while having negligible impact on Pb, confirming the adiabatic approximation's validity for it.

Conclusion: Non-adiabatic corrections are crucial for high-Tc hydrides, and the study provides a reliable framework for predicting superconductivity across regimes.

Abstract: The Migdal-Eliashberg (ME) formalism provides a reliable framework for
describing phonon-mediated superconductivity in the adiabatic regime, where the
electronic Fermi energy exceeds the characteristic phonon energy. In this work,
we go beyond this limit by incorporating first-order vertex corrections to the
electron-phonon (e-ph) interaction within the Eliashberg formalism and assess
their impact on the superconducting properties of H3S and Pb using
first-principles calculations. For H3S, where the adiabatic assumption breaks
down, we find that vertex corrections to the e-ph coupling are substantial.
When combined with phonon anharmonicity and the energy dependence of the
electronic density of states, the predicted critical temperature (Tc) is in
very good agreement with experimental observations. In contrast, for elemental
Pb, where the adiabatic approximation remains valid, vertex corrections have a
negligible effect, and the calculated Tc and superconducting gap closely match
the predictions of the standard ME formalism. These findings demonstrate the
importance of non-adiabatic corrections in strongly coupled high-Tc hydrides
and establish a robust first-principles framework for accurately predicting
superconducting properties across different regimes.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [59] [HPC-AI Coupling Methodology for Scientific Applications](https://arxiv.org/abs/2507.01025)
*Yutong Lu,Dan Huang,Pin Chen*

Main category: cs.CE

TL;DR: The paper explores coupling HPC and AI (HPC-AI) in scientific applications, introducing three coupling patterns (surrogate, directive, coordinate) and demonstrating their effectiveness in materials science.


<details>
  <summary>Details</summary>
Motivation: To address challenges like high computational intensity in HPC applications by leveraging AI-driven approaches.

Method: Proposes three coupling patterns (surrogate, directive, coordinate) and validates them through case studies in materials science.

Result: Demonstrates performance improvements and technical challenges, highlighting the effectiveness of HPC-AI coupling.

Conclusion: The coupling patterns are broadly applicable across scientific domains, offering guidance for future HPC-AI integration.

Abstract: Artificial intelligence (AI) technologies have fundamentally transformed
numerical-based high-performance computing (HPC) applications with data-driven
approaches and endeavored to address existing challenges, e.g. high
computational intensity, in various scientific domains. In this study, we
explore the scenarios of coupling HPC and AI (HPC-AI) in the context of
emerging scientific applications, presenting a novel methodology that
incorporates three patterns of coupling: surrogate, directive, and coordinate.
Each pattern exemplifies a distinct coupling strategy, AI-driven prerequisite,
and typical HPC-AI ensembles. Through case studies in materials science, we
demonstrate the application and effectiveness of these patterns. The study
highlights technical challenges, performance improvements, and implementation
details, providing insight into promising perspectives of HPC-AI coupling. The
proposed coupling patterns are applicable not only to materials science but
also to other scientific domains, offering valuable guidance for future HPC-AI
ensembles in scientific discovery.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [60] [Extracting the singularity of the logarithmic partition function](https://arxiv.org/abs/2506.16111)
*Zhe Wang,Yi-Ming Ding,Zenan Liu,Zheng Yan*

Main category: cond-mat.str-el

TL;DR: The paper proposes using Renyi thermal entropy (RTE) and its derivative (DRTE) to analyze quantum phase transitions, overcoming limitations of traditional methods in detecting weakly first-order transitions.


<details>
  <summary>Details</summary>
Motivation: Traditional methods struggle to distinguish weakly first-order phase transitions from deconfined quantum critical points due to precision requirements.

Method: The authors eliminate the leading volume-law term in the logarithmic partition function by computing RTE and DRTE, deriving scaling relations for these quantities.

Result: RTE and DRTE successfully identify phase transition points and critical exponents, even detecting weakly first-order transitions in systems like the $JQ$ model.

Conclusion: The study introduces a robust framework for analyzing quantum phase transitions, particularly effective for weakly first-order cases.

Abstract: In principle, the logarithm of the partition function and its derivatives can
be used to rigorously determine the nature of phase transitions. However, in
practice, this approach is often ineffective in numerical calculations, since
resolving the singularities of the logarithmic partition function requires
extremely high precision data especially when distinguishing between weakly
first order phase transitions and deconfined quantum critical points. Based on
the finite size scaling hypothesis, we propose that the singular contribution
to the logarithmic partition function in quantum phase transitions can be
extracted by eliminating the leading volume-law term. This is achieved by
computing the Renyi thermal entropy (RTE) and its derivative (DRTE). We have
derived the scaling relations for RTE and DRTE, which successfully determine
the phase transition points and critical exponents through data collapse in
various quantum many body systems. Notably, beyond their utility in locating
quantum critical points, we find that DRTE is remarkably effective in detecting
weakly first order phase transitions, such as in $ JQ $ models, which were
initially thought to be continuous deconfined quantum phase transitions. The
discontinuity of these transitions is challenging to detect using traditional
local physical quantities. This work provides a new and powerful paradigm for
studying quantum many body phase transitions.

</details>


<div id='physics.class-ph'></div>

# physics.class-ph [[Back]](#toc)

### [61] [Thermodynamically extended symplectic numerical simulation of viscoelastic, thermal expansion and heat conduction phenomena in solids](https://arxiv.org/abs/2211.12120)
*Donát M. Takács,Áron Pozsár,Tamás Fülöp*

Main category: physics.class-ph

TL;DR: The paper introduces a symplectic numerical scheme for reversible dynamical systems, extending it to irreversible scenarios like viscoelastic wave propagation and heat conduction. It emphasizes minimizing dissipation and dispersion errors and leverages thermodynamics for reliable long-term solutions.


<details>
  <summary>Details</summary>
Motivation: To develop reliable numerical schemes for irreversible systems (e.g., viscoelastic wave propagation, heat conduction) while minimizing artificial errors that could obscure real physical phenomena.

Method: A symplectic numerical scheme is extended to irreversible systems, incorporating continuum thermodynamics principles (flux balances, constitutive relationships, entropy production) for accuracy and supervision.

Result: The scheme demonstrates long-term reliability for 1D viscoelastic wave propagation with heat conduction, validated using thermodynamics-based monitoring.

Conclusion: Thermodynamics-based numerical schemes are effective for irreversible systems, ensuring minimal artificial errors and reliable solutions.

Abstract: Symplectic numerical schemes for reversible dynamical systems predict the
solution reliably over large times as well, and are a good starting point for
extension to schemes for simulating irreversible situations like viscoelastic
wave propagation and heat conduction coupled via thermal expansion occuring in
rocks, plastics, biological samples etc. Dissipation error (artificial
nonpreservation of energies and amplitudes) of the numerical solution should be
as small as possible since it should not be confused with the real dissipation
occuring in the irreversible system. In addition, the other well-known
numerical artefact, dispersion error (artificial oscillations emerging at sharp
changes), should also be minimal to avoid confusion with the true wavy
behaviour. The continuum thermodynamical aspects (respect for balances with
fluxes, systematic constitutive relationships between intensive quantities and
fluxes, the second law of thermodynamics with positive definite entropy
production, and the spacetime-based kinematic viewpoint) prove valuable for
obtaining such extended schemes and for monitoring the solutions. Generalizing
earlier works in this direction, here, we establish and investigate such a
numerical scheme for one-dimensional viscoelastic wave propagation in the
presence of heat conduction coupled via thermal expansion, demonstrating
long-term reliability and the applicability of thermodynamics-based quantities
in supervising the quality of the solution.

</details>
