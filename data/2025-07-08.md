<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 31]
- [math.AP](#math.AP) [Total: 52]
- [physics.comp-ph](#physics.comp-ph) [Total: 8]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 15]
- [math.CO](#math.CO) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 2]
- [cs.DC](#cs.DC) [Total: 1]
- [math.ST](#math.ST) [Total: 1]
- [astro-ph.GA](#astro-ph.GA) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [math-ph](#math-ph) [Total: 3]
- [physics.atom-ph](#physics.atom-ph) [Total: 1]
- [cond-mat.mes-hall](#cond-mat.mes-hall) [Total: 2]
- [math.CA](#math.CA) [Total: 2]
- [stat.ML](#stat.ML) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [astro-ph.HE](#astro-ph.HE) [Total: 1]
- [math.PR](#math.PR) [Total: 3]
- [math.SG](#math.SG) [Total: 1]
- [q-bio.PE](#q-bio.PE) [Total: 1]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 3]
- [cs.LG](#cs.LG) [Total: 5]
- [cs.RO](#cs.RO) [Total: 1]
- [math.DG](#math.DG) [Total: 2]
- [math.SP](#math.SP) [Total: 2]
- [astro-ph.CO](#astro-ph.CO) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 4]
- [cs.MS](#cs.MS) [Total: 1]
- [math.FA](#math.FA) [Total: 1]
- [astro-ph.EP](#astro-ph.EP) [Total: 2]
- [cs.NE](#cs.NE) [Total: 1]
- [physics.ins-det](#physics.ins-det) [Total: 1]
- [stat.ME](#stat.ME) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [A PINNs approach for the computation of eigenvalues in elliptic problems](https://arxiv.org/abs/2507.03126)
*Julian Fernandez Bonder,Ariel M. Salort*

Main category: math.NA

TL;DR: A deep learning method for computing eigenvalues of elliptic problems, dimension-independent and adaptable to nonlinear cases.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional eigenvalue computation methods, especially their dependence on space dimension and sequential computation of eigenvalues.

Method: Utilizes deep learning techniques to compute eigenvalues without prior computation of lower ones, adaptable to nonlinear problems.

Result: Proposes a flexible, dimension-independent approach for eigenvalue computation.

Conclusion: The method offers a promising alternative for eigenvalue problems, especially in high dimensions or nonlinear cases.

Abstract: In this paper, we propose a method for computing eigenvalues of elliptic
problems using Deep Learning techniques. A key feature of our approach is that
it is independent of the space dimension and can compute arbitrary eigenvalues
without requiring the prior computation of lower ones. Moreover, the method can
be easily adapted to handle nonlinear eigenvalue problems.

</details>


### [2] [Parallel multilevel methods for solving the Darcy--Forchheimer model based on a nearly semicoercive formulation](https://arxiv.org/abs/2507.03192)
*Jongho Park,S. Majid Hassanizadeh*

Main category: math.NA

TL;DR: The paper analyzes parallel multilevel methods for solving the Darcy-Forchheimer model, reformulating it as a convex optimization problem and demonstrating robustness and effectiveness through numerical results.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of high-velocity fluid flow in porous media, the paper aims to develop efficient parallel multilevel methods for the Darcy-Forchheimer model.

Method: The model is reformulated as a nearly semicoercive convex optimization problem using the augmented Lagrangian method. A parallel multilevel method with subspace correction and backtracking line search is proposed.

Result: The method shows robustness against problem nature and system size, with numerical results confirming its effectiveness and superiority.

Conclusion: The proposed parallel multilevel method is validated as an efficient solution for the Darcy-Forchheimer model, supported by theoretical and numerical evidence.

Abstract: High-velocity fluid flow through porous media is modeled by prescribing a
nonlinear relationship between the flow rate and the pressure gradient, called
Darcy--Forchheimer equation. This paper is concerned with the analysis of
parallel multilevel methods for solving the Darcy--Forchheimer model. We begin
by reformulating the Darcy--Forchheimer model as a nearly semicoercive convex
optimization problem via the augmented Lagrangian method. Building on this
formulation, we develop a parallel multilevel method within the framework of
subspace correction for nearly semicoercive convex problems. The proposed
method exhibits robustness with respect to both the nearly semicoercive nature
of the problem and the size of the discretized system. To further enhance
convergence, we incorporate a backtracking line search scheme. Numerical
results validate the theoretical findings and demonstrate the effectiveness and
superiority of the proposed approach.

</details>


### [3] [Weak Form Scientific Machine Learning: Test Function Construction for System Identification](https://arxiv.org/abs/2507.03206)
*April Tran,David Bortz*

Main category: math.NA

TL;DR: A novel method for constructing single-scale-local test functions in WSciML is introduced, optimizing support size for minimal integration error without needing model parameters, showing improved computational efficiency over multi-scale-global methods.


<details>
  <summary>Details</summary>
Motivation: To enhance noise robustness in WSciML by addressing the challenge of wisely choosing compactly supported test functions for system identification.

Method: A data-driven approach to construct single-scale-local test functions by numerically approximating integration error and identifying optimal support size.

Result: The method consistently selects supports with minimal parameter estimation error across various models, noise levels, and temporal resolutions.

Conclusion: The proposed single-scale-local method outperforms multi-scale-global test functions in computational efficiency and accuracy.

Abstract: Weak form Scientific Machine Learning (WSciML) is a recently developed
framework for data-driven modeling and scientific discovery. It leverages the
weak form of equation error residuals to provide enhanced noise robustness in
system identification via convolving model equations with test functions,
reformulating the problem to avoid direct differentiation of data. The
performance, however, relies on wisely choosing a set of compactly supported
test functions.
  In this work, we mathematically motivate a novel data-driven method for
constructing Single-scale-Local reference functions for creating the set of
test functions. Our approach numerically approximates the integration error
introduced by the quadrature and identifies the support size for which the
error is minimal, without requiring access to the model parameter values.
Through numerical experiments across various models, noise levels, and temporal
resolutions, we demonstrate that the selected supports consistently align with
regions of minimal parameter estimation error. We also compare the proposed
method against the strategy for constructing Multi-scale-Global (and
orthogonal) test functions introduced in our prior work, demonstrating the
improved computational efficiency.

</details>


### [4] [An ensemble Kalman approach to randomized maximum likelihood estimation](https://arxiv.org/abs/2507.03207)
*Pavlos Stavrinides,Elizabeth Qian*

Main category: math.NA

TL;DR: Proposes ensemble Kalman randomized maximum likelihood estimation, a derivative-free method for Bayesian inverse problems, with linear analysis showing exponential convergence and posterior sampling. Addresses computational cost via model reduction.


<details>
  <summary>Details</summary>
Motivation: To develop a derivative-free method for generating approximate samples from posterior distributions in Bayesian inverse problems, addressing computational challenges.

Method: Evolves an ensemble to solve randomly perturbed optimization problems, with linear analysis and model reduction for tractability.

Result: Ensemble members converge exponentially to estimators; method samples from the posterior. Model reduction ensures tractability and near-optimality.

Conclusion: The method is effective for Bayesian inverse problems, with theoretical and numerical validation of its performance and efficiency.

Abstract: This work proposes ensemble Kalman randomized maximum likelihood estimation,
a new derivative-free method for performing randomized maximum likelihood
estimation, which is a method that can be used to generate approximate samples
from posterior distributions in Bayesian inverse problems. The new method has
connections to ensemble Kalman inversion and works by evolving an ensemble so
that each ensemble member solves an instance of a randomly perturbed
optimization problem. Linear analysis demonstrates that ensemble members
converge exponentially fast to randomized maximum likelihood estimators and,
furthermore, that the new method produces samples from the Bayesian posterior
when applied to a suitably regularized optimization problem. The method
requires that the forward operator, relating the unknown parameter to the data,
be evaluated once per iteration per ensemble member, which can be prohibitively
expensive when the forward model requires the evolution of a high-dimensional
dynamical system. We propose a strategy for making the proposed method
tractable in this setting based on a balanced truncation model reduction method
tailored to the Bayesian smoothing problem. Theoretical results show
near-optimality of this model reduction approach via convergence to an optimal
approximation of the posterior covariance as a low-rank update to the prior
covariance. Numerical experiments verify theoretical results and illustrate
computational acceleration through model reduction.

</details>


### [5] [Structured Backward Errors of Sparse Generalized Saddle Point Problems with Hermitian Block Matrices](https://arxiv.org/abs/2507.03335)
*Sk. Safique Ahmad,Pinki Khatun*

Main category: math.NA

TL;DR: The paper derives structured backward errors (BE) for generalized saddle point problems (GSPP) while preserving sparsity and Hermitian structures, constructs optimal backward perturbations, and validates results numerically.


<details>
  <summary>Details</summary>
Motivation: To analyze and ensure the reliability of structured BEs in GSPP, including cases where sparsity is not preserved, and to assess backward stability of numerical methods.

Method: Derives structured BEs for GSPP, constructs optimal backward perturbation matrices, and performs numerical experiments.

Result: Demonstrates reliability of structured BEs and optimal perturbations, and uses them to assess numerical method stability.

Conclusion: The structured BEs and optimal perturbations are reliable tools for analyzing backward stability in GSPP numerical solutions.

Abstract: In this paper, we derive the structured backward error (BE) for a class of
generalized saddle point problems (GSPP) by preserving the sparsity pattern and
Hermitian structures of the block matrices. Additionally, we construct the
optimal backward perturbation matrices for which the structured BE is achieved.
Our analysis also examines the structured BE in cases where the sparsity
pattern is not maintained. Through numerical experiments, we demonstrate the
reliability of the derived structured BEs and the corresponding optimal
backward perturbations. Additionally, the derived structured BEs are used to
assess the strong backward stability of numerical methods for solving the GSPP.

</details>


### [6] [On the non-convexity issue in the radial Calderón problem](https://arxiv.org/abs/2507.03379)
*Giovanni S. Alberti,Romain Petit,Clarice Poon*

Main category: math.NA

TL;DR: The paper evaluates a convex optimization method for the Calderón problem, comparing it to nonlinear least squares. The convex method performs poorly for larger problems, while least squares solvers are faster and more accurate. The study also disproves the existence of local minima in a specific case.


<details>
  <summary>Details</summary>
Motivation: To assess the computational viability of a convex optimization approach for the Calderón problem, challenging the belief that least squares methods are nonconvex and problematic.

Method: Implementation and comparison of convex reconstruction and least squares methods in a piecewise constant, radial conductivity setting. Theoretical analysis of nonconvexity in specific cases.

Result: The convex method is only viable for very small problems and is outperformed by least squares solvers. No local minima exist in a two-unknown, noise-free case.

Conclusion: The convex approach is limited in practice, while least squares methods are more efficient and accurate. Theoretical insights clarify nonconvexity misconceptions.

Abstract: A classical approach to the Calder\'on problem is to estimate the unknown
conductivity by solving a nonlinear least-squares problem. It is generally
believed that it leads to a nonconvex optimization problem which is riddled
with bad local minimums. This has motivated the development of reconstruction
methods based on convex optimization, one recent contribution being the
nonlinear convex semidefinite programming approach of Harrach (2023). In this
work, we investigate the computational viability of this convex approach in a
simple setting where the conductivities are piecewise constant and radial. We
implement this convex reconstruction method and compare it extensively to the
least squares approach. Our experiments suggest that this convex programming
approach only allows to accurately estimate the unknown for problems with a
very small size. Moreover, surprisingly, it is consistently outperformed by
Newton-type least squares solvers, which are also faster and require less
measurements. We revisit the issue of nonconvexity in this piecewise constant
radial setting and prove that, contrary to previous claims, there are no local
minimums in the case of two scalar unknowns with no measurement noise. We also
provide a partial proof of this result in the general setting which holds under
a numerically verifiable assumption.

</details>


### [7] [Generalized UGK scheme in the diffusive](https://arxiv.org/abs/2507.03385)
*Julien Mathiaud,Luc Mieussens,Nicolas Crouseilles*

Main category: math.NA

TL;DR: The paper extends the unified gas kinetic scheme (UGKS) to linear kinetic models with non-isotropic scattering and Fokker-Planck models, demonstrating its asymptotic preserving property in both diffusive and free transport regimes.


<details>
  <summary>Details</summary>
Motivation: To address multiscale challenges in rarefied gas dynamics and radiative transfer, the UGKS is extended to more complex kinetic models.

Method: The UGKS is applied to linear kinetic models with non-isotropic scattering and Fokker-Planck models, focusing on their behavior in optically thick and free transport limits.

Result: Numerical experiments confirm the UGKS's effectiveness and its asymptotic preserving property in both regimes.

Conclusion: The UGKS is successfully extended to new kinetic models, maintaining its multiscale capabilities and asymptotic preserving properties.

Abstract: The unified gas kinetic scheme (UGKS) was initially designed to address
multiscale challenges in rarefied gas dynamics and then extended to radiative
transfert theory, as described by BGK like relaxation models. In this work, we
extend its application to linear kinetic models with non isotropic scattering
collision operators, as well as Fokker-Planck models . These problems typically
exhibit a fully diffusive nature in the optically thick limit (corresponding to
a small Knudsen number). It still leads to an asymptotic preserving (AP)
property not only in this diffusive regime but also in the free transport
limit. A series of numerical experiments confirm the effectiveness of the
approach.

</details>


### [8] [Elliptic interface problem approximated by CutFEM: I. Conservative flux recovery and numerical validation of adaptive mesh refinement](https://arxiv.org/abs/2507.03492)
*Daniela Capatina,Aimene Gouasmi,Cuiyu He*

Main category: math.NA

TL;DR: The paper presents a method for solving elliptic interface problems with discontinuous diffusion coefficients using CutFEM, focusing on conservative flux reconstruction and a posteriori error estimation.


<details>
  <summary>Details</summary>
Motivation: To address challenges in solving elliptic interface problems on unfitted meshes, particularly with discontinuous diffusion coefficients, by developing a robust and efficient error estimation method.

Method: A hybrid mixed formulation with locally computable Lagrange multipliers is introduced, and flux is reconstructed in the immersed Raviart-Thomas space. A new a posteriori error estimator with volume and interface terms is proposed.

Result: The method demonstrates robust reliability and local efficiency, validated through numerical experiments.

Conclusion: The proposed approach effectively handles elliptic interface problems on unfitted meshes, providing accurate error estimation and practical utility.

Abstract: We study an elliptic interface problem with discontinuous diffusion
coefficients on unfitted meshes using the CutFEM method. Our main contribution
is the reconstruction of conservative fluxes from the CutFEM solution and their
use in a posteriori error estimation. We introduce a hybrid mixed formulation
with locally computable Lagrange multipliers and reconstruct the flux in the
immersed Raviart-Thomas space. Based on this, we propose a new a posteriori
error estimator that includes both volume and interface terms. We state its
robust reliability and local efficiency, and validate the approach through
numerical experiments.

</details>


### [9] [PINN-DG: Residual neural network methods trained with Finite Elements](https://arxiv.org/abs/2507.03521)
*Georgios Grekas,Charalambos G. Makridakis,Tristan Pryer*

Main category: math.NA

TL;DR: A new PINN method integrates neural networks with discontinuous Galerkin finite elements, improving efficiency and robustness for PDEs.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of standard collocation-based PINNs, particularly costly gradient computations and instability for elliptic PDEs.

Method: Combines neural networks with discontinuous Galerkin finite element methods, using finite element interpolation and integration for the loss functional.

Result: Theoretical convergence analysis and numerical experiments show improved efficiency and robustness.

Conclusion: The proposed method offers a stable and efficient alternative to traditional PINNs for solving PDEs.

Abstract: Over the past few years, neural network methods have evolved in various
directions for approximating partial differential equations (PDEs). A promising
new development is the integration of neural networks with classical numerical
techniques such as finite elements and finite differences. In this paper, we
introduce a new class of Physics-Informed Neural Networks (PINNs) trained using
discontinuous Galerkin finite element methods. Unlike standard
collocation-based PINNs that rely on pointwise gradient evaluations and Monte
Carlo quadrature, our approach computes the loss functional using finite
element interpolation and integration. This avoids costly pointwise derivative
computations, particularly advantageous for elliptic PDEs requiring
second-order derivatives, and inherits key stability and accuracy benefits from
the finite element framework. We present a convergence analysis based on
variational arguments and support our theoretical findings with numerical
experiments that demonstrate improved efficiency and robustness.

</details>


### [10] [Bilinear Quadratic Output Systems and Balanced Truncation](https://arxiv.org/abs/2507.03684)
*Heike Faßbender,Serkan Gugercin,Till Peters*

Main category: math.NA

TL;DR: The paper develops primal-dual formulations and Gramians for bilinear dynamical systems with quadratic outputs, establishes conditions for their existence and uniqueness, and introduces a balanced truncation framework for computational efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the growing interest in dynamical systems with quadratic outputs, focusing on bilinear systems as a special case.

Method: Develops primal-dual formulations, defines system Gramians, derives generalized Lyapunov equations, and constructs a balanced truncation framework.

Result: Conditions for Gramian existence and uniqueness are established, and the framework is validated through numerical examples.

Conclusion: The proposed approach provides a computationally efficient method for analyzing bilinear systems with quadratic outputs.

Abstract: Dynamical systems with quadratic outputs have recently attracted significant
attention. In this paper, we consider bilinear dynamical systems, a special
class of weakly nonlinear systems, with a quadratic output. We develop various
primal-dual formulations for these systems and define the corresponding system
Gramians. Conditions for the existence and uniqueness of these Gramians are
established, and the generalized Lyapunov equations they satisfy are derived.
Using these Gramians and their truncated versions, which are computationally
more efficient, we construct a balanced truncation framework for bilinear
systems with quadratic outputs. The proposed approach is demonstrated through
two numerical examples.

</details>


### [11] [Noise-robust multi-fidelity surrogate modelling for parametric partial differential equations](https://arxiv.org/abs/2507.03691)
*Benjamin M. Kent,Lorenzo Tamellini,Matteo Giacomini,Antonio Huerta*

Main category: math.NA

TL;DR: Proposes an improved Multi-Index Stochastic Collocation (MISC) method to detect and ignore solver noise in multi-fidelity surrogate models for parametric PDEs, enhancing robustness and accuracy.


<details>
  <summary>Details</summary>
Motivation: Numerical noise in low-fidelity PDE evaluations can degrade surrogate quality, causing overfitting and limiting downstream applications like uncertainty quantification.

Method: The improved MISC monitors spectral decay to detect noise, halting use of noisy fidelities and focusing on informative ones.

Result: Validated on advection-diffusion and Navier-Stokes problems, the method shows accuracy and robustness, even with under-resolved meshes.

Conclusion: The approach effectively mitigates noise impact, improving surrogate reliability for practical applications.

Abstract: We address the challenge of constructing noise-robust surrogate models for
quantities of interest (QoIs) arising from parametric partial differential
equations (PDEs), using multi-fidelity collocation techniques; specifically,
the Multi-Index Stochastic Collocation (MISC). In practical scenarios, the PDE
evaluations used to build a response surface are often corrupted by numerical
noise, especially for the low-fidelity models. This noise, which may originate
from loose solver tolerances, coarse discretisations, or transient effects, can
lead to overfitting in MISC, degrading surrogate quality through nonphysical
oscillations and loss of convergence, thereby limiting its utility in
downstream tasks like uncertainty quantification, optimisation, and control. To
correct this behaviour, we propose an improved version of MISC that can
automatically detect the presence of solver noise during the surrogate model
construction and then ignore the exhausted fidelities. Our approach monitors
the spectral decay of the surrogate at each iteration, identifying stagnation
in the coefficient spectrum that signals the onset of noise. Once detected, the
algorithm selectively halts the use of noisy fidelities, focusing computational
resources on those fidelities that still provide meaningful information. The
effectiveness of this approach is numerically validated on two challenging test
cases: a parabolic advection--diffusion PDE with uncertain coefficients, and a
parametric turbulent incompressible Navier--Stokes problem. The results
showcase the accuracy and robustness of the resulting multi-fidelity surrogate
and its capability to extract relevant information, even from under-resolved
meshes not suitable for reliable single-fidelity computations.

</details>


### [12] [Adjoint-based A Posteriori Error Analysis for Semi-explicit Index-1 and Hessenberg Index-2 Differential-Algebraic Equations](https://arxiv.org/abs/2507.03712)
*Jehanzeb Chaudhry,Owen Lewis,Md Al Amin Molla*

Main category: math.NA

TL;DR: The paper develops adjoint-based methods for error estimation in temporal discretization of specific DAEs, focusing on semi-explicit index-1 and Hessenberg index-2 types. It quantifies errors in QoIs and compares two analysis approaches: direct DAE adjoint and ODE-converted adjoint. Numerical results show accurate error estimation.


<details>
  <summary>Details</summary>
Motivation: To improve error estimation for temporal discretization of DAEs, particularly for semi-explicit index-1 and Hessenberg index-2 types, by focusing on QoIs.

Method: Two adjoint-based analyses: one directly for DAEs and another converting DAEs to ODEs first. Error representations for various QoIs are derived.

Result: Numerical results demonstrate high accuracy in error estimation for nonlinear, non-autonomous DAEs, and discretized PDAEs.

Conclusion: The adjoint-based techniques effectively estimate errors in QoIs for the studied DAEs, with numerical validation supporting their accuracy.

Abstract: In this work we develop and analyze adjoint-based analyses for \textit{a
posteriori} error estimation for the temporal discretization of
differential-algebraic equations (DAEs) of special type: semi-explicit index-1
and Hessenberg index-2. Our technique quantifies the error in a Quantity of
Interest (QoI), which is defined as a bounded linear functional of the solution
of a DAE. We derive representations for errors various trypes of QoIs
(depending on the entire time interval, final time, algebraic variables,
differential variables etc.). We form two analyses: one that defines the
adjoint to the DAE system, and one that first converts the DAE to an ODE system
and then applies classical \textit{a posteriori} analysis techniques. A number
of examples are presented, including nonlinear and non-autonomous DAEs, as well
as discretized partial differential-algebraic equations (PDAEs). Numerical
results indicate a high degree of accuracy in the error estimation.

</details>


### [13] [A discontinuous Galerkin pressure correction scheme for the Oldroyd model of order one](https://arxiv.org/abs/2507.03909)
*Pratyay Mondal,Rajen Kumar Sinha*

Main category: math.NA

TL;DR: A discontinuous Galerkin pressure correction scheme for the Oldroyd model is analyzed, proving existence, uniqueness, stability, and optimal error bounds for velocity and pressure. Numerical experiments confirm convergence rates.


<details>
  <summary>Details</summary>
Motivation: To address the need for accurate numerical methods for the Oldroyd model, ensuring stability and optimal convergence in discontinuous discrete spaces.

Method: Develops a discontinuous Galerkin pressure correction scheme, proving its properties theoretically and validating with numerical experiments.

Result: Existence, uniqueness, and stability of the discrete solution are proven. Optimal error bounds for velocity and pressure are derived, with numerical confirmation.

Conclusion: The scheme is effective for the Oldroyd model, with proven theoretical properties and practical validation through experiments.

Abstract: We develop and analyze a discontinuous Galerkin pressure correction scheme
for the Oldroyd model of order one. The existence and uniqueness of the
discrete solution as well as the consistency of the scheme are proved. The
stability of the discrete velocity and pressure are established. We derive
optimal $\textit{a priori}$ error bounds for the fully discrete velocity in the
discontinuous discrete space. In addition, an improved error estimate for the
velocity is derived in the $L^2$ norm which is optimal with respect to space
and time. Furthermore, the error bound for the pressure is obtained via the
estimates of discrete time derivative of the velocity. Finally, numerical
experiments confirm the optimal convergence rates.

</details>


### [14] [On the discrete Poincaré inequality for B-schemes of 1D Fokker-Planck equations in full space](https://arxiv.org/abs/2507.03941)
*Lei Li,Jian-Guo Liu,Zhen Wang*

Main category: math.NA

TL;DR: Two approaches (Gamma-calculus and Lyapunov function) are proposed to derive the discrete Poincaré inequality for B-schemes in the 1D Fokker-Planck equation, ensuring exponential convergence to equilibrium.


<details>
  <summary>Details</summary>
Motivation: To analyze the properties of spatially discretized Fokker-Planck equations and ensure stability and convergence in discrete settings.

Method: 1. Gamma-calculus to validate the Bakry-Émery criterion discretely. 2. Lyapunov function method to extend local inequalities to full space.

Result: Exponential convergence to equilibrium for discrete schemes is achieved.

Conclusion: The methods are effective for 1D and hold potential for extension to higher dimensions.

Abstract: In this paper, we propose two approaches to derive the discrete Poincar\'e
inequality for the B-schemes, a family of finite volume discretization schemes,
for the one-dimensional Fokker-Planck equation in full space. We study the
properties of the spatially discretized Fokker-Planck equation in the viewpoint
of a continuous-time Markov chain. The first approach is based on
Gamma-calculus, through which we show that the Bakry-\'Emery criterion still
holds in the discrete setting. The second approach employs the Lyapunov
function method, allowing us to extend a local discrete Poincar\'e inequality
to the full space. The assumptions required for both approaches are roughly
comparable with some minor differences. These methods have the potential to be
extended to higher dimensions. As a result, we obtain exponential convergence
to equilibrium for the discrete schemes by applying the discrete Poincar\'e
inequality.

</details>


### [15] [Mixed FEM for coupled unsteady fluid flow problems with $p$-type Brinkman-Forchheimer framework and its application for reverse-osmosis desalination](https://arxiv.org/abs/2507.03974)
*Zeinab Gharibi,Mostafa Abbaszadeh,Mehdi Dehghan*

Main category: math.NA

TL;DR: The paper presents a mixed finite element method for solving nonstationary Brinkman-Forchheimer equations in Banach spaces, applied to reverse osmosis. It ensures well-posedness and stability, with numerical validation.


<details>
  <summary>Details</summary>
Motivation: To address coupled fluid flow and transport problems in reverse osmosis, requiring robust numerical methods for nonlinear boundary conditions.

Method: Uses a mixed formulation with Raviart-Thomas elements, piecewise constants, and linear elements. A fixed-point approach ensures well-posedness.

Result: Proves stability and optimal convergence rates, confirmed by numerical results.

Conclusion: The method is effective for modeling reverse osmosis, with theoretical and numerical validation.

Abstract: This work analyzes a fully discrete mixed finite element method in a Banach
space framework for solving nonstationary coupled fluid flow problems modeled
by the Brinkman-Forchheimer equations, with applications to reverse osmosis.
The model couples unsteady $p$-type convective Brinkman-Forchheimer and
transport equations with nonlinear boundary conditions across a semi-permeable
membrane. A mixed formulation is used for the fluid equation
(pseudostress-velocity) and for the transport equation (concentration, its
gradient, and a Lagrange multiplier from the membrane condition). The
continuous problem is reformulated in Banach spaces as a fixed-point problem,
enabling a well-posedness analysis via differential-algebraic system theory.
Spatial discretization employs lowest-order Raviart-Thomas elements for fluxes
and piecewise constants for primal variables, while linear elements are used
for the Lagrange multiplier. A fully discrete Galerkin scheme with backward
Euler time-stepping is proposed. Its well-posedness and stability are proven
using a fixed-point argument, and optimal convergence rates are established.
Numerical results confirm the theoretical error estimates and demonstrate the
method's effectiveness.

</details>


### [16] [Exploring Exponential Runge-Kutta Methods: A Survey](https://arxiv.org/abs/2507.04024)
*Alessia andò,Nicolò Cangiotti,Mattia Sensi*

Main category: math.NA

TL;DR: A survey on exponential Runge-Kutta methods for numerical integration, blending classical and modern approaches.


<details>
  <summary>Details</summary>
Motivation: To bridge classical Runge-Kutta methods and exponential integrators, providing historical context and accessibility.

Method: Historical analysis and illustrative examples to explain the topic.

Result: Comprehensive overview of exponential Runge-Kutta methods.

Conclusion: The survey successfully synthesizes and demystifies these methods for a wide audience.

Abstract: In this survey, we provide an in-depth investigation of exponential
Runge-Kutta methods for the numerical integration of initial-value problems.
These methods offer a valuable synthesis between classical Runge-Kutta methods,
introduced more than a century ago, and exponential integrators, which date
back to the 1960s. This manuscript presents both a historical analysis of the
development of these methods up to the present day and several examples aimed
at making the topic accessible to a broad audience.

</details>


### [17] [Remarkable upper bounds for the interpolation error constants on the triangles](https://arxiv.org/abs/2507.04032)
*Kenta Kobayashi*

Main category: math.NA

TL;DR: Sharp upper bounds for interpolation error constants on triangles are introduced, proven via numerical verification and asymptotic analysis.


<details>
  <summary>Details</summary>
Motivation: To analyze interpolation errors, especially in the Finite Element Method, by providing precise and simple formulas.

Method: Numerical verification and asymptotic analysis are used to prove the bounds.

Result: Sharp and simple upper bounds for interpolation error constants are established.

Conclusion: The study not only provides valuable bounds but also demonstrates the application of numerical verification methods, with potential for broader use in norm inequalities.

Abstract: We introduce remarkable upper bounds for the interpolation error constants on
triangles, which are sharp and given by simple formulas. These constants are
crucial in analyzing interpolation errors, particularly those associated with
the Finite Element Method. In this study, we proved boundness via the numerical
verification method and asymptotic analysis. This study is also essential in
that it demonstrates a valuable application of the numerical verification
method. The proof process of this study may be applied to the proof of various
other norm inequalities.

</details>


### [18] [Physics-informed neural networks and neural operators for a study of EUV electromagnetic wave diffraction from a lithography mask](https://arxiv.org/abs/2507.04153)
*Vasiliy A. Es'kin,Egor V. Ivanov*

Main category: math.NA

TL;DR: A hybrid Waveguide Neural Operator (WGNO) combines waveguide methods with neural networks to efficiently solve EUV wave diffraction, achieving top accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: To accelerate lithography mask design by improving computational efficiency for EUV wave diffraction problems.

Method: Introduces WGNO, replacing the most expensive part of waveguide methods with a neural network.

Result: WGNO achieves state-of-the-art accuracy and fast inference in 2D and 3D mask simulations.

Conclusion: WGNO offers an efficient solution for lithography mask design workflows.

Abstract: Physics-informed neural networks (PINNs) and neural operators (NOs) for
solving the problem of diffraction of Extreme Ultraviolet (EUV) electromagnetic
waves from a mask are presented. A novel hybrid Waveguide Neural Operator
(WGNO) is introduced, which is based on a waveguide method with its most
computationally expensive part replaced by a neural network. Numerical
experiments on realistic 2D and 3D masks show that the WGNO achieves
state-of-the-art accuracy and inference time, providing a highly efficient
solution for accelerating the design workflows of lithography masks.

</details>


### [19] [Generalized Rellich's lemmas, uniqueness theorem and inside-out duality for scattering poles](https://arxiv.org/abs/2507.04242)
*Xiaodong Liu,Jiguang Sun,Lei Zhang*

Main category: math.NA

TL;DR: The paper proves generalized Rellich's lemmas for scattered fields with complex wavenumbers, establishes uniqueness for inverse scattering problems, and introduces inside-out duality to identify scattering poles without prior obstacle knowledge.


<details>
  <summary>Details</summary>
Motivation: To study scattering poles, which are crucial for understanding wave phenomena, and to advance inverse scattering problem solutions.

Method: Proving generalized Rellich's lemmas, using inside-out duality, and applying the linear sampling method to interior problems.

Result: Uniqueness results for inverse scattering problems and identification of exterior Dirichlet/Neumann poles without obstacle knowledge.

Conclusion: Theoretical results are validated numerically, demonstrating practical applicability.

Abstract: Scattering poles correspond to non-trivial scattered fields in the absence of
incident waves and play a crucial role in the study of wave phenomena. These
poles are complex wavenumbers with negative imaginary parts. In this paper, we
prove two generalized Rellich's lemmas for scattered fields associated with
complex wavenumbers. These lemmas are then used to establish uniqueness results
for inverse scattering problems. We further explore the inside-out duality,
which characterizes scattering poles through the linear sampling method applied
to interior scattering problems. Notably, we demonstrate that exterior
Dirichlet/Neumann poles can be identified without prior knowledge of the actual
sound-soft or sound-hard obstacles. Numerical examples are provided to validate
the theoretical results.

</details>


### [20] [Adaptive Multiquadratic Radial Basis Function-based Explicit Runge--Kutta Methods](https://arxiv.org/abs/2507.04261)
*Rajesh Yadav,Deepak Kumar Yadav,Alpesh Kumar*

Main category: math.NA

TL;DR: The paper introduces an adaptive multiquadric RBF-based method to enhance explicit Runge-Kutta (RK) methods, achieving higher convergence and improved local accuracy by optimizing shape functions.


<details>
  <summary>Details</summary>
Motivation: To improve the order of convergence and local accuracy of classical RK methods for solving initial value problems.

Method: An adaptive multiquadric radial basis function (RBF) approach is used to optimize free parameters (shape functions) by minimizing local truncation errors. Convergence and stability are analyzed.

Result: The proposed methods outperform classical RK methods in accuracy and convergence, as validated by numerical examples, tables, and figures.

Conclusion: The adaptive RBF-based RK methods are superior to classical RK methods, offering higher convergence and better local accuracy.

Abstract: Runge--Kutta (RK) methods are widely used techniques for solving a class of
initial value problems. In this article, we introduce an adaptive
multiquadratic (MQ) radial basis function (RBF)-based method to develop
enhanced explicit RK methods. These methods achieve a higher order of
convergence than the corresponding classical RK methods. To improve the local
convergence of the numerical solution, we optimize the free parameters (shape
functions) involved in the RBFs by forcing the local truncation errors to
vanish. We also present a convergence and stability analysis of the proposed
methods. To demonstrate the advantages of these methods in terms of accuracy
and convergence, we consider several numerical examples and compare the
performance of our methods with that of the classical RK methods. The Tables
and Figures presented in this article clearly validate the superiority of the
proposed methods.

</details>


### [21] [The median trick does not help for fully nested scrambling](https://arxiv.org/abs/2507.04297)
*Takashi Goda,Kosuke Suzuki*

Main category: math.NA

TL;DR: The paper investigates variance equivalence in randomized quasi-Monte Carlo methods, showing that median estimators behave differently under linear and fully nested scrambling.


<details>
  <summary>Details</summary>
Motivation: To explore whether the variance equivalence observed in average estimators extends to median estimators in randomized quasi-Monte Carlo methods.

Method: Comparison of median estimators using linear scrambling and fully nested scrambling for numerical integration.

Result: Median estimators with linear scrambling achieve faster convergence for smooth integrands, unlike those with fully nested scrambling.

Conclusion: The variance equivalence in average estimators does not hold for median estimators, highlighting a difference in performance between scrambling methods.

Abstract: In randomized quasi-Monte Carlo methods for numerical integration, average
estimators based on digital nets with fully nested and linear scrambling are
known to exhibit the same variance. In this note, we show that this equivalence
does not extend to the median estimators. Specifically, while the median
estimator with linear scrambling can achieve faster convergence for smooth
integrands, the median estimator with fully nested scrambling does not exhibit
this advantage.

</details>


### [22] [Entropy stable high-order discontinuous Galerkin spectral-element methods on curvilinear, hybrid meshes](https://arxiv.org/abs/2507.04334)
*Jens Keim,Anna Schwarz,Patrick Kopper,Marcel Blind,Christian Rohde,Andrea Beck*

Main category: math.NA

TL;DR: The paper presents an entropy-stable discontinuous Galerkin spectral element method (DGSEM) for hyperbolic-parabolic PDEs on complex grids, extending it to diverse element shapes using collapsed coordinate transformations and generalized SBP operators.


<details>
  <summary>Details</summary>
Motivation: High-order methods like DG are unstable for underresolved flows; entropy-stable formulations are needed for robustness on heterogeneous, curvilinear grids with diverse element shapes.

Method: Uses collapsed coordinate transformations, Legendre-Gauss quadrature nodes, generalized SBP operators, and entropy-projected variables. Extends to hyperbolic-parabolic problems via lifting and employs modal degrees of freedom for efficiency.

Result: The method achieves free-stream preservation, polynomial/grid convergence, and entropy conservation/stability, validated with the flow around the common research model.

Conclusion: The proposed DGSEM is robust, efficient, and applicable to real-world problems, advancing entropy-stable high-order methods for complex geometries.

Abstract: Hyperbolic-parabolic partial differential equations are widely used for the
modeling of complex, multiscale problems. High-order methods such as the
discontinuous Galerkin (DG) scheme are attractive candidates for their
numerical approximation. However, high-order methods are prone to instabilities
in the presence of underresolved flow features. A popular counter measure to
stabilize DG methods is the use of entropy-stable formulations based on
summation-by-parts (SBP) operators. The present paper aims to construct a
robust and efficient entropy-stable discontinuous Galerkin spectral element
method (DGSEM) of arbitrary order on heterogeneous, curvilinear grids composed
of triangular and quadrilateral elements or hexahedral, prismatic, tetrahedral
and pyramid elements. To the author's knowledge, with the exception of
hexahedral and quadrilateral elements, entropy-stable DGSE operators have been
constructed exclusively for tetrahedral and triangular meshes. The extension of
the DGSEM to more complex element shapes is achieved by means of a collapsed
coordinate transformation. Legendre--Gauss quadrature nodes are employed as
collocation points in conjunction with a generalized SBP operator and
entropy-projected variables. The purely hyperbolic operator is extended to
hyperbolic-parabolic problems by the use of a lifting procedure. To circumvent
the penalizing time step restriction imposed by the collapsing, modal rather
than nodal degrees of freedom are evolved in time, thereby relying on a
memory-efficient weight-adjusted approximation to the inverse of the mass
matrix. Essential properties of the proposed numerical scheme including
free-stream preservation, polynomial and grid convergence as well as entropy
conservation / stability are verified. Finally, with the flow around the common
research model, the applicability of the presented method to real-world
problems is demonstrated.

</details>


### [23] [Energy-conserving Kansa methods for Hamiltonian wave equations](https://arxiv.org/abs/2507.04361)
*Xiaobin Li,Meng Chen,Zhengjie Sun,Leevan Ling,Siqing Li*

Main category: math.NA

TL;DR: A fast meshfree solver for Hamiltonian wave equations ensures energy conservation via the Kansa method and a quadratic constraint, with a Newton-based iterative solver for efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of maintaining energy conservation in Hamiltonian wave equations while improving computational efficiency.

Method: Uses the Kansa method (kernel-based collocation) with time-stepping and a quadratic constraint, solved via a fast Newton-based iterative approach.

Result: Demonstrates competitive performance against traditional methods, conserving energy and accelerating computation.

Conclusion: The solver is effective for Hamiltonian wave equations and has potential for broader EC-constrained optimization problems.

Abstract: We introduce a fast, constrained meshfree solver designed specifically to
inherit energy conservation (EC) in second-order time-dependent Hamiltonian
wave equations. For discretization, we adopt the Kansa method, also known as
the kernel-based collocation method, combined with time-stepping. This approach
ensures that the critical structural feature of energy conservation is
maintained over time by embedding a quadratic constraint into the definition of
the numerical solution. To address the computational challenges posed by the
nonlinearity in the Hamiltonian wave equations and the EC constraint, we
propose a fast iterative solver based on the Newton method with successive
linearization. This novel solver significantly accelerates the computation,
making the method highly effective for practical applications. Numerical
comparisons with the traditional secant methods highlight the competitive
performance of our scheme. These results demonstrate that our method not only
conserves the energy but also offers a promising new direction for solving
Hamiltonian wave equations more efficiently. While we focus on the Kansa method
and corresponding convergence theories in this study, the proposed solver is
based solely on linear algebra techniques and has the potential to be applied
to EC constrained optimization problems arising from other PDE discretization
methods.

</details>


### [24] [When do World Models Successfully Learn Dynamical Systems?](https://arxiv.org/abs/2507.04898)
*Edmund Ross,Claudia Drygala,Leonhard Schwarz,Samir Kaiser,Francesca di Mare,Tobias Breiten,Hanno Gottschalk*

Main category: math.NA

TL;DR: The paper explores using compact latent representations ('World Models') to simulate physical systems, validating the approach with progressively complex models on various datasets.


<details>
  <summary>Details</summary>
Motivation: To understand why projecting time slices into a low-dimensional space ('Tokenization') effectively learns physics datasets and to characterize when dynamics admit reconstruction.

Method: Develops models from least-squares regression to full-scale GANs, testing on datasets like heat/wave equations, Kuramoto-Sivashinsky, and CFD.

Result: The model successfully recreates complex flows, such as the 2D Kármán vortex street.

Conclusion: The framework effectively simulates physical systems using compact latent representations and tokenization.

Abstract: In this work, we explore the use of compact latent representations with
learned time dynamics ('World Models') to simulate physical systems. Drawing on
concepts from control theory, we propose a theoretical framework that explains
why projecting time slices into a low-dimensional space and then concatenating
to form a history ('Tokenization') is so effective at learning physics
datasets, and characterise when exactly the underlying dynamics admit a
reconstruction mapping from the history of previous tokenized frames to the
next. To validate these claims, we develop a sequence of models with increasing
complexity, starting with least-squares regression and progressing through
simple linear layers, shallow adversarial learners, and ultimately full-scale
generative adversarial networks (GANs). We evaluate these models on a variety
of datasets, including modified forms of the heat and wave equations, the
chaotic regime 2D Kuramoto-Sivashinsky equation, and a challenging
computational fluid dynamics (CFD) dataset of a 2D K\'arm\'an vortex street
around a fixed cylinder, where our model is successfully able to recreate the
flow.

</details>


### [25] [Theoretical analysis and numerical solution to a vector equation $Ax-\|x\|_1x=b$](https://arxiv.org/abs/2507.04971)
*Yuezhi Wang,Gwi Soo Kim,Jie Meng*

Main category: math.NA

TL;DR: The paper investigates the vector equation $Ax-\|x\|_1x=b$, proving existence and uniqueness of a nonnegative solution. It proposes and analyzes fixed-point and Newton iterations, along with a structure-preserving doubling algorithm, showing linear convergence. Numerical experiments validate the methods.


<details>
  <summary>Details</summary>
Motivation: The study aims to explore the theoretical and computational properties of the vector equation $Ax-\|x\|_1x=b$, focusing on nonnegative solutions and efficient algorithms for solving it.

Method: The paper uses fixed-point iterations (including a relaxed version), Newton iteration, and a structure-preserving doubling algorithm to solve the equation. Theoretical analysis includes proving linear convergence.

Result: Existence and uniqueness of a nonnegative solution are established. The doubling algorithm converges linearly with rate 1/2. Numerical experiments confirm the methods' effectiveness.

Conclusion: The proposed algorithms are effective for solving the vector equation, with theoretical guarantees and practical validation through experiments.

Abstract: Theoretical and computational properties of a vector equation $Ax-\|x\|_1x=b$
are investigated, where $A$ is an invertible $M$-matrix and $b$ is a
nonnegative vector. Existence and uniqueness of a nonnegative solution is
proved. Fixed-point iterations, including a relaxed fixed-point iteration and
Newton iteration, are proposed and analyzed.
  A structure-preserving doubling algorithm is proved to be applicable in
computing the required solution, the convergence is at least linear with rate
1/2. Numerical experiments are performed to demonstrate the effectiveness of
the proposed algorithms.

</details>


### [26] [Efficient implicit-explicit sparse stochastic method for high dimensional semi-linear nonlocal diffusion equations](https://arxiv.org/abs/2507.04973)
*Changtao Sheng,Bihao Su,Chenglong Xu*

Main category: math.NA

TL;DR: A sparse grid-based Monte Carlo method is introduced for solving high-dimensional semi-linear nonlocal diffusion equations, addressing computational complexity and hypersingular kernels with a scalable and stable approach.


<details>
  <summary>Details</summary>
Motivation: To tackle the computational challenges of high-dimensional semi-linear nonlocal diffusion equations, including the curse of dimensionality and dense linear systems from nonlocal operators.

Method: An implicit-explicit scheme using sparse grid interpolation and a tailored sampling strategy for hypersingular kernels, based on the nonlinear Feynman-Kac representation.

Result: The method shows robustness and accuracy, validated by numerical experiments in up to 100 dimensions, with unconditional stability.

Conclusion: The proposed approach effectively solves high-dimensional nonlocal diffusion problems, offering scalability and stability without discretization constraints.

Abstract: In this paper, we present a sparse grid-based Monte Carlo method for solving
high-dimensional semi-linear nonlocal diffusion equations with volume
constraints. The nonlocal model is governed by a class of semi-linear partial
integro-differential equations (PIDEs), in which the operator captures both
local convection-diffusion and nonlocal diffusion effects, as revealed by its
limiting behavior with respect to the interaction radius. To overcome the
bottleneck of computational complexity caused by the curse of dimensionality
and the dense linear systems arising from nonlocal operators, we propose a
novel implicit-explicit scheme based on a direct approximation of the nonlinear
Feynman-Kac representation. The incorporation of sparse grid interpolation
significantly enhances the algorithm's scalability and enables its application
to problems in high dimensions. To further address the challenges posed by
hypersingular kernels, we design a sampling strategy tailored to their singular
structure, which ensures accurate and stable treatment of the nonlocal
operators within the probabilistic framework. Notably, the proposed method
inherits unconditional stability from the underlying stochastic representation,
without imposing constraints on the temporal and spatial discretization scales.
A rigorous error analysis is provided to establish the convergence of the
proposed scheme. Extensive numerical experiments, including some non-radial
solutions in up to 100 dimensions, are presented to validate the robustness and
accuracy of the proposed method.

</details>


### [27] [Paired Explicit Relaxation Runge-Kutta Methods: Entropy-Conservative and Entropy-Stable High-Order Optimized Multirate Time Integration](https://arxiv.org/abs/2507.04991)
*Daniel Doehring,Hendrik Ranocha,Manuel Torrilhon*

Main category: math.NA

TL;DR: Novel entropy-conservative/stable multirate Runge-Kutta methods (P-ERRK) with relaxation are introduced, optimized up to fourth-order, and validated for conservation laws. They outperform standalone methods by up to four times in computational efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the need for efficient and stable numerical methods for conservation laws and PDEs, particularly for problems with scale separation like non-uniform meshes.

Method: Develops Paired Explicit Relaxation Runge-Kutta (P-ERRK) methods, combining them with high-order discontinuous Galerkin spectral element methods on unstructured meshes.

Result: P-ERRK methods show improved nonlinear stability and computational efficiency, outperforming standalone methods by factors up to four.

Conclusion: P-ERRK methods are effective for a range of problems, offering robustness and efficiency, with publicly available reproducibility.

Abstract: We present novel entropy-conservative and entropy-stable multirate
Runge-Kutta methods based on Paired Explicit Runge-Kutta (P-ERK) with
relaxation for conservation laws and related systems of partial differential
equations. Optimized schemes up to fourth-order of accuracy are derived and
validated in terms of order of consistency, conservation of linear invariants,
and entropy conservation/stability. We demonstrate the effectiveness of these
P-ERRK methods when combined with a high-order, entropy-conservative/stable
discontinuous Galerkin spectral element method on unstructured meshes. The
Paired Explicit Relaxation Runge-Kutta methods(P-ERRK) are readily implemented
for partitioned semidiscretizations arising from problems with equation-based
scale separation such as non-uniform meshes. We highlight that the relaxation
approach acts as a time-limiting technique which improves the nonlinear
stability and thus robustness of the multirate schemes. The P-ERRK methods are
applied to a range of problems, ranging from compressible Euler over
compressible Navier-Stokes to the visco-resistive magnetohydrodynamics
equations in two and three spatial dimensions. For each test case, we compare
computational load and runtime to standalone relaxed Runge-Kutta methods which
are outperformed by factors up to four. All results can be reproduced using a
publicly available repository.

</details>


### [28] [Fourier Spectral Method for Nonlocal Equations on Bounded Domains](https://arxiv.org/abs/2507.05034)
*Ilyas Mustapha,Bacim Alali,Nathan Albin*

Main category: math.NA

TL;DR: Efficient spectral solvers for nonlocal equations on bounded domains reduce computational cost to O(N log N) using Fourier space and 2D-FC algorithm.


<details>
  <summary>Details</summary>
Motivation: To address the computational challenges of nonlocal equations on bounded domains by leveraging spectral methods and Fourier transformations.

Method: Extends spectral solvers for periodic domains using the 2D-FC algorithm, applied to nonlocal Poisson and diffusion equations.

Result: Demonstrates reduced computational cost (O(N log N)) and evaluates performance on 2D nonlocal equations, exploring solution regularity.

Conclusion: Proposed solvers are efficient and accurate, though solution regularity in bounded domains remains an open question.

Abstract: This work introduces efficient and accurate spectral solvers for nonlocal
equations on bounded domains. These spectral solvers exploit the fact that
integration in the nonlocal formulation transforms into multiplication in
Fourier space and that nonlocality is decoupled from the grid size. As a
result, the computational cost is reduced to $O(N\log N)$ for an $N$-point
discretization grid. Our approach extends the spectral solvers developed by
Alali and Albin (2020) for periodic domains by incorporating the
two-dimensional Fourier continuation (2D-FC) algorithm introduced by Bruno and
Paul (2022). We evaluate the performance of the proposed methods on
two-dimensional nonlocal Poisson and nonlocal diffusion equations defined on
bounded domains. While the regularity of solutions to these equations in
bounded settings remains an open problem, we conduct numerical experiments to
explore this issue, particularly focusing on studying discontinuities.

</details>


### [29] [Cubic spline functions revisited](https://arxiv.org/abs/2507.05083)
*Florian Jarre*

Main category: math.NA

TL;DR: A new cubic interpolating spline (Q-spline) achieves a fourth-order asymptotically optimal error bound without derivative data, outperforming existing methods like the not-a-knot spline. A modified not-a-knot spline is also introduced, showing comparable performance to Q-spline in numerical tests.


<details>
  <summary>Details</summary>
Motivation: To improve error bounds for cubic spline interpolation when only function values are available, addressing limitations of existing methods like the not-a-knot spline.

Method: Derives a fourth-order error bound for Q-spline and modifies the not-a-knot spline's end conditions to enhance performance. Numerical examples validate the approach.

Result: Q-spline shows stronger error bounds than not-a-knot spline. The modified not-a-knot spline performs comparably to Q-spline, especially with small mesh sizes.

Conclusion: The Q-spline and modified not-a-knot spline offer improved interpolation accuracy, with Q-spline being near optimal and the revised not-a-knot spline being a practical alternative.

Abstract: In this paper a fourth order asymptotically optimal error bound for a new
cubic interpolating spline function, denoted by Q-spline, is derived for the
case that only function values at given points are used but not any derivative
information. The bound seems to be stronger than earlier error bounds for cubic
spline interpolation in such setting such as the not-a-knot spline. A brief
analysis of the conditioning of the end conditions of cubic spline
interpolation leads to a modification of the not-a-knot spline, and some
numerical examples suggest that the interpolation error of this revised
not-a-knot spline generally is comparable to the near optimal Q-spline and
lower than for the not-a-knot spline when the mesh size is small.

</details>


### [30] [Model order reduction techniques for the stochastic finite volume method](https://arxiv.org/abs/2507.05091)
*Ray Qu,Jesse Chan,Svetlana Tokareva*

Main category: math.NA

TL;DR: The paper proposes combining the SFV method with ROM and Q-DEIM hyper-reduction to reduce computational costs in high-dimensional UQ problems.


<details>
  <summary>Details</summary>
Motivation: The SFV method's high computational cost in high-dimensional stochastic spaces motivates the need for efficiency improvements.

Method: Incorporates interpolation-based ROM and Q-DEIM hyper-reduction to reduce stochastic integral costs in SFV.

Result: Numerical experiments show reduced computational cost and memory requirements.

Conclusion: The combined approach effectively addresses the curse of dimensionality in SFV for UQ.

Abstract: The stochastic finite volume method (SFV method) is a high-order accurate
method for uncertainty quantification (UQ) in hyperbolic conservation laws.
However, the computational cost of SFV method increases for high-dimensional
stochastic parameter spaces due to the curse of dimensionality. To address this
challenge, we incorporate interpolation-based reduced order modeling (ROM)
techniques that reduce the cost of computing stochastic integrals in SFV
method. Further efficiency gains are achieved through a Q-DEIM hyper-reduction
method. Numerical experiments suggest that this approach can lower both
computational cost and memory requirements for high-dimensional stochastic
parameter spaces.

</details>


### [31] [A 3D Machine Learning based Volume Of Fluid scheme without explicit interface reconstruction](https://arxiv.org/abs/2507.05218)
*Moreno Pintore,Bruno Després*

Main category: math.NA

TL;DR: A machine-learning-based Volume of Fluid method for 3D multi-material flow simulation, using a neural network for flux fraction computation without explicit interface reconstruction.


<details>
  <summary>Details</summary>
Motivation: To improve simulation of multi-material flows by leveraging machine learning for efficient and accurate flux fraction computation.

Method: Uses a trained neural network on synthetic data to compute flux fractions, avoiding explicit interface reconstruction. Includes strategies for efficiency and physical constraint satisfaction.

Result: Demonstrates numerical convergence with better rates than reference schemes in advection equation tests.

Conclusion: The method is effective for multi-material flow simulation, offering improved performance over traditional schemes.

Abstract: We present a machine-learning based Volume Of Fluid method to simulate
multi-material flows on three-dimensional domains. One of the novelties of the
method is that the flux fraction is computed by evaluating a previously trained
neural network and without explicitly reconstructing any local interface
approximating the exact one. The network is trained on a purely synthetic
dataset generated by randomly sampling numerous local interfaces and which can
be adapted to improve the scheme on less regular interfaces when needed.
Several strategies to ensure the efficiency of the method and the satisfaction
of physical constraints and properties are suggested and formalized. Numerical
results on the advection equation are provided to show the performance of the
method. We observe numerical convergence as the size of the mesh tends to zero
$h=1/N_h\searrow 0$, with a better rate than two reference schemes.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [32] [The focusing energy-critical nonlinear Schrödinger system with power-type growth nonlinearities in the radial case](https://arxiv.org/abs/2507.03096)
*Luiz Gustavo Farah,Maicon Hespanha*

Main category: math.AP

TL;DR: The paper explores scattering vs. blow-up in a coupled nonlinear Schrödinger system for dimensions 3-5, using variational and concentration-compactness methods.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior (scattering or blow-up) of solutions in a coupled nonlinear Schrödinger system with energy-critical power-type nonlinearities.

Method: Uses concentration-compactness and variational techniques for ground states, convexity arguments for blow-up, and concentration-compactness/rigidity for scattering.

Result: Demonstrates the dichotomy: solutions either scatter or blow up in finite time.

Conclusion: The study confirms the scattering vs. blow-up dichotomy in the radial case for the given system.

Abstract: This work is concerned with a coupled system of focusing nonlinear
Schr\"odinger equations involving general power-type nonlinearities in the
energy-critical setting for dimensions $3\leq d\leq 5$ in the radial setting.
Our aim is to demonstrate the scattering versus blow-up dichotomy in the radial
case. To achieve this, we first prove the existence of ground state solutions
using the concentration-compactness method combined with variational
techniques. We then establish finite-time blow-up through a convexity argument
and prove scattering by applying the concentration-compactness and rigidity
method.

</details>


### [33] [Global existence of weak solutions to a two-phase diffuse interface model of ferrofluids dynamics](https://arxiv.org/abs/2507.03099)
*Samuel Lanthaler,Franziska Weber*

Main category: math.AP

TL;DR: A two-phase diffuse interface model for ferrofluids is developed, coupling Navier-Stokes, magnetization, magnetostatics, and Cahn-Hilliard equations. The model satisfies an energy balance even in singular limits. Global weak solutions are proven, and convergence to a quasi-equilibrium system is rigorously shown.


<details>
  <summary>Details</summary>
Motivation: To model the dynamics of ferrofluids, capturing their fluid and magnetic properties, while ensuring a meaningful energy balance in singular limits.

Method: Derives a coupled PDE system (Navier-Stokes, magnetization, magnetostatics, Cahn-Hilliard) and proves global weak solutions via approximation steps. Also studies relaxation to quasi-equilibrium.

Result: Existence of global weak solutions is proven, and rigorous convergence to a quasi-equilibrium system is demonstrated.

Conclusion: The model successfully captures ferrofluid dynamics with a robust energy balance, and the analysis provides theoretical guarantees for its behavior.

Abstract: Ferrofluids are a class of materials that exhibit both fluid and magnetic
properties. We consider a two-phase diffuse interface model for the dynamics of
ferrofluids on a bounded domain. One phase is assumed to be magnetic, the other
phase can be magnetic or non-magnetic. We derive a coupled system of partial
differential equations consisting of the incompressible Navier-Stokes
equations, an evolution equation for the magnetization, the magnetostatics
equations for the magnetic field and the Cahn-Hilliard equations for the
evolution of the phase field variable, which are all coupled through various
source terms and parameters. In contrast to similar models in the literature,
the system in this work formally satisfies an energy balance which remains
meaningful even in singular limits such as a limit of zero relaxation time.
However, the formal derivation of this balance requires a delicate cancellation
of several highly non-linear terms, making it challenging to ensure similar
cancellations for approximating systems. Our first main result is to prove the
existence of global weak solutions for our ferrofluid system based on a
carefully constructed sequence of approximation steps. Additionally, we also
study the relaxation towards the quasi-equilibrium, in which case the
magnetization equation degenerates to a linear relation between the magnetic
field and the magnetization. As our second main result, we prove the rigorous
convergence to this limiting system.

</details>


### [34] [Unconditional wave decay in dimension two](https://arxiv.org/abs/2507.03140)
*T. J. Christiansen,K. Datchev,P. Morales,M. Yang*

Main category: math.AP

TL;DR: Extends Burq's logarithmic decay rate to general compactly supported scatterers in 2D, removing the need for regular spectrum at zero.


<details>
  <summary>Details</summary>
Motivation: To generalize Burq's decay rate to broader cases, including smooth obstacles with variable boundary conditions.

Method: Uses recent results on low-frequency expansions to eliminate the regularity requirement at zero.

Result: Achieves logarithmic decay for general compactly supported scatterers in two dimensions.

Conclusion: The approach successfully broadens applicability, covering more complex scatterers.

Abstract: We extend Burq's logarithmic decay rate [Bur98] to general compactly
supported scatterers in dimension two. The main novelty is using recent results
on low-frequency expansions to remove the requirement that the spectrum be
regular at zero. This allows us to include, among other examples, arbitrary
smooth obstacles with variable boundary conditions.

</details>


### [35] [Existence of weak solution of 3D ferrohydrodynamic equations with transport noise: Bloch-Torrey regularisation](https://arxiv.org/abs/2507.03388)
*Aristide Ndongmo Ngana,Paul Razafimandimby*

Main category: math.AP

TL;DR: The paper proves the global existence of a probabilistic weak solution for a stochastic ferrohydrodynamic system using Galerkin and compactness methods.


<details>
  <summary>Details</summary>
Motivation: To extend existing results on weak solutions of stochastic MHD, Navier-Stokes, and Bloch-type equations to a coupled system involving ferrofluids.

Method: Combination of Galerkin method and compactness method to analyze the stochastic system.

Result: Global existence of a probabilistic weak solution for the coupled system in a 3D bounded domain.

Conclusion: The work generalizes previous results on stochastic fluid and magnetization equations.

Abstract: In this article, we consider a stochastic ferrohydrodynamic system which
describes the Bloch-Torrey regularization of the motion of an electrically
conducting ferrofluids driven by transport noise filling a 3D bounded domain
with a smooth boundary. We prove the global existence of a probabilistic weak
solution of the stochastic system by making use of the combination of Galerkin
method and compactness method. The system under study is basically a coupling
of the Navier-Stokes equations with internal rotation, the Maxwell equations
and the ferromagnetization equations. Thus, our result can be seen as a modest
generalization of the existing results on the global existence of weak
solutions of stochastic Magnetohydrodynamic (MHD), Navier-Stokes and Bloch type
equations on 3D bounded domain.

</details>


### [36] [From random matrices to systems of particles in interaction](https://arxiv.org/abs/2507.03400)
*Valentin Pesce*

Main category: math.AP

TL;DR: Introduction to random matrices for non-specialists, linking them to interacting particle systems via eigenvalue laws, Dyson Brownian motion, and large deviations.


<details>
  <summary>Details</summary>
Motivation: To introduce random matrices to non-specialists and explore their connection with interacting particle systems.

Method: Recalls general random matrix theory, focuses on Dyson Brownian motion, and studies singular interactions and mean field limits.

Result: Large deviations for eigenvalues of random matrices, viewed as log gases in 1D or 2D.

Conclusion: The paper bridges random matrices and particle systems, providing insights via eigenvalue analysis and large deviations.

Abstract: The goal of these expository notes is to give an introduction to random
matrices for non-specialist of this topic focusing on the link between random
matrices and systems of particles in interaction. We first recall some general
results about the random matrix theory that create a link between random
matrices and systems of particles through the knowledge of the law of the
eigenvalues of certain random matrices models. We next focus on a continuous in
time approach of random matrices called the Dyson Brownian motion. We detail
some general methods to study the existence of system of particles in singular
interaction and the existence of a mean field limit for these systems of
particles. Finally, we present the main result of large deviations when
studying the eigenvalues of random matrices. This method is based on the fact
that the eigenvalues of certain models of random matrices can be viewed as log
gases in dimension 1 or 2.

</details>


### [37] [Normalised solutions for $p$-Laplacian equations with $L^p$-supercritical growth](https://arxiv.org/abs/2507.03429)
*Raj Narayan Dhara,Matteo Rizzi*

Main category: math.AP

TL;DR: The paper finds normalized solutions for a nonlinear PDE involving the p-Laplacian and a potential, in the mass supercritical and Sobolev subcritical case, for small enough mass. It also proves compactness of embeddings for radial functions.


<details>
  <summary>Details</summary>
Motivation: To address the existence of normalized solutions for a specific PDE with a potential, particularly in the mass supercritical and Sobolev subcritical regime, which is a challenging and relevant problem in analysis.

Method: The authors use variational methods to find solutions and analyze the compactness of embeddings for radial functions in Sobolev spaces.

Result: Existence of normalized solutions is proven for small enough mass, and compactness of the embedding for radial functions is established.

Conclusion: The results contribute to understanding solutions for nonlinear PDEs with potentials and provide insights into the behavior of radial functions in Sobolev spaces.

Abstract: For $N\ge 3$ and $2<p<N$, we find normalised solutions to the equation
\begin{align*} -\Delta_p u+(1+V(x))|u|^{p-2}u+\lambda
u&=|u|^{q-2}u\qquad\text{in $\mathbb{R}^N$}\\ \|u\|_2&=\rho \end{align*} in the
mass supercritical and Sobolev subcritical case, that is
$q\in(p\frac{N+2}{N},\frac{Np}{N-p})$, at least if $\rho>0$ is small enough.
The function $V\in L^{N/p}(\mathbb{R}^N)$, which plays the role of potential,
is assumed to be non-positive and vanishing at infinity. Moreover, we will
prove the compactness of the embedding of the space of radial functions
$W^{1,p}_{rad}(\mathbb{R}^N)\subset L^q(\mathbb{R}^N)$ for $p\in(1,N)$ and
$q\in(p\frac{N+2}{N},\frac{Np}{N-p})$.

</details>


### [38] [Long-time behaviour and bifurcation analysis of a two-species aggregation-diffusion system on the torus](https://arxiv.org/abs/2507.03431)
*José Carrillo,Yurij Salmaniw*

Main category: math.AP

TL;DR: The paper studies stationary states in nonlocal aggregation-diffusion equations, extending results for scalar cases and analyzing two-species systems rigorously. It highlights applications in cell-cell adhesion.


<details>
  <summary>Details</summary>
Motivation: To understand existence, stability, and bifurcation in nonlocal aggregation-diffusion models, with applications in biological processes like cell sorting.

Method: Extends scalar case results under bounded variation; uses Crandall & Rabinowitz bifurcation theory for two-species systems, classifying solution branches and stability.

Result: Shows stable segregation patterns in cell-cell adhesion, even with purely attractive interactions.

Conclusion: The framework provides rigorous analysis of bifurcation and stability, with practical implications for biological modeling.

Abstract: We investigate stationary states, including their existence and stability, in
a class of nonlocal aggregation-diffusion equations with linear diffusion and
symmetric nonlocal interactions. For the scalar case, we extend previous
results by showing that key model features, such as existence, regularity,
bifurcation structure, and stability exchange, continue to hold under a mere
bounded variation hypothesis. For the corresponding two-species system, we
carry out a fully rigorous bifurcation analysis using the bifurcation theory of
Crandall & Rabinowitz. This framework allows us to classify all solution
branches from homogeneous states, with particular attention given to those
arising from the self-interaction strength and the cross-interaction strength,
as well as the stability of the branch at a point of critical stability. The
analysis relies on an equivalent classification of solutions through fixed
points of a nonlinear map, followed by a careful derivation of Fr\'echet
derivatives up to third order. An interesting application to cell-cell adhesion
arises from our analysis, yielding stable segregation patterns that appear at
the onset of cell sorting in a modelling regime where all interactions are
purely attractive.

</details>


### [39] [Wavelet based solutions to the Poisson and the Helmholtz equations on the $n$-dimensional unit sphere](https://arxiv.org/abs/2507.03451)
*Ilona Iglewska-Nowak,Piotr Stefaniak*

Main category: math.AP

TL;DR: A method for solving PDEs on the n-dimensional unit sphere using wavelet transforms is presented, with explicit solutions for Poisson and Helmholtz equations, including closed-form Green's functions for special cases.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of solving partial differential equations (PDEs) on the n-dimensional unit sphere by leveraging wavelet-based methods.

Method: Utilizes the continuous wavelet transform derived from approximate identities to solve PDEs, providing explicit analytical solutions for Poisson and Helmholtz equations.

Result: Derives explicit solutions for the Poisson equation and Helmholtz equations, including closed-form generalized Green's functions for specific cases.

Conclusion: The method successfully solves PDEs on the n-sphere, offering analytical solutions and Green's functions, demonstrating its effectiveness for such problems.

Abstract: We present a method of solving partial differential equations on the
$n$-dimensional unit sphere using methods based on the continuous wavelet
transform derived from approximate identities. We give an explicit analytical
solution to the Poisson equation and to the Helmholtz equations. For the first
one and for some special values of the parameter in the latter one, we derive a
closed formula for the generalized Green function.

</details>


### [40] [Green function to the biharmonic equation on $n$-dimensional sphere](https://arxiv.org/abs/2507.03455)
*Ilona Iglewska-Nowak*

Main category: math.AP

TL;DR: The paper analyzes biharmonic equations on the n-dimensional unit sphere, deriving the Green function as a series of Gegenbauer polynomials and providing explicit representations for certain parameter sets.


<details>
  <summary>Details</summary>
Motivation: To solve biharmonic equations on the n-dimensional unit sphere by deriving the Green function, which is essential for understanding the behavior of such equations in spherical geometry.

Method: The Green function is expressed as a series of Gegenbauer polynomials, and explicit representations are derived for specific parameter sets.

Result: Explicit forms of the Green function are obtained for certain parameter configurations, facilitating solutions to the biharmonic equations on the sphere.

Conclusion: The derived explicit representations of the Green function provide a practical tool for solving biharmonic equations on the n-dimensional unit sphere.

Abstract: Homogeneous and inhomogeneous biharmonic equation are considered on the
$n$-dimensional unit sphere. The Green function is given as a series of
Gegenbauer polynomials. In the paper, explicit representations of the Green
function are found for some sets of parameters.

</details>


### [41] [On a phenotype-structured phase-field model of nutrient-limited tumour growth](https://arxiv.org/abs/2507.03467)
*Tommaso Lorenzi,Giulia Pozzi,Andrea Signori*

Main category: math.AP

TL;DR: The paper extends phase-field tumor growth models by incorporating phenotypic heterogeneity and evolution, addressing previous limitations of assuming identical cells.


<details>
  <summary>Details</summary>
Motivation: Existing phase-field tumor models assume identical cells, ignoring phenotypic heterogeneity and evolution. This paper aims to address these limitations.

Method: A phenotype-structured phase-field model of nutrient-limited tumor growth is formulated, with well-posedness established under general assumptions.

Result: Numerical solutions demonstrate spatiotemporal and evolutionary cell dynamics, showcasing the model's key features.

Conclusion: The paper concludes with an overview of future research directions in modeling and analysis.

Abstract: Phase-field models of tumour growth have proved useful as theoretical tools
to investigate cancer invasion. A key implicit assumption underlying
mathematical models of this type which have so far been proposed, though, is
that cells in the tumour are identical. This assumption ignores both the fact
that cells in the same tumour may express different characteristics to
different extents, exhibiting heterogeneous phenotypes, and the fact that cells
may undergo phenotypic changes, with their characteristics evolving over time.
To address such a limitation, in this paper we incorporate inter-cellular
phenotypic heterogeneity and the evolution of cell phenotypes into the
phase-field modelling framework. This is achieved by formulating a
phenotype-structured phase-field model of nutrient-limited tumour growth. For
this model, we first establish a well-posedness result under general
assumptions on the model functions, which encompass a wide spectrum of
biologically relevant scenarios. We then present a sample of numerical
solutions to showcase key features of spatiotemporal and evolutionary cell
dynamics predicted by the model. We conclude with a brief overview of modelling
and analytical research perspectives.

</details>


### [42] [On least energy solutions for a nonlinear Schrödinger system with $K$-wise interaction](https://arxiv.org/abs/2507.03480)
*Lorenzo Giaretto,Nicola Soave*

Main category: math.AP

TL;DR: The paper studies minimal energy solutions for a weakly coupled system with K-wise interaction, analyzing existence, properties, and asymptotic behavior under attractive and repulsive cases.


<details>
  <summary>Details</summary>
Motivation: To understand the existence and behavior of minimal energy solutions in a weakly coupled system with multi-component interaction, extending beyond pairwise models.

Method: Analyzes the system using variational methods, considering both attractive (β>0) and repulsive (β<0) cases, and studies radial solutions under strong competition (β→−∞).

Result: Establishes sufficient conditions for least energy solutions, demonstrates partial segregation phenomena in the repulsive case, differing from pairwise models.

Conclusion: The work provides insights into multi-component interaction systems, highlighting unique segregation behaviors and expanding understanding beyond pairwise interactions.

Abstract: In this paper we establish existence and properties of minimal energy
solutions for the weakly coupled system $$ \begin{cases}
  -\Delta u_i + \lambda_i u_i = \mu_i|u_i|^{Kq-2}u_i +
\beta|u_i|^{q-2}u_i\prod_{j\neq i}|u_j|^q & \text{in }\mathbb{R}^d, \qquad
  u_i \in H^1(\mathbb{R}^d), \end{cases}\qquad i=1,\dots, K, $$ characterized
by $K$-wise interaction (namely the interaction term involves the product of
all the components). We consider both attractive ($\beta>0$) and repulsive
cases ($\beta<0$), and we give sufficient conditions on $\beta$ in order to
have least energy fully non-trivial solutions, if necessary under a radial
constraint. We also study the asymptotic behavior of least energy fully
non-trivial radial solutions in the limit of strong competition $\beta \to
-\infty$, showing partial segregation phenomena which differ substantially from
those arising in pairwise interaction models.

</details>


### [43] [Higher differentiability for solutions to stationary $p$-Stokes systems under sub-quadratic growth conditions](https://arxiv.org/abs/2507.03499)
*Anna Cavagnoli*

Main category: math.AP

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We consider weak solutions $(u,\pi):\mathbb{R}^n\supset\Omega\to\
\mathbb{R}^n\times\ \mathbb{R}$ to stationary $p$-Stokes systems of the type \[
\begin{cases} -\mathrm{div} (a(\mathcal{E} u))+\nabla\pi=f \\
\mathrm{div}(u)=0, \end{cases} \] in $\Omega,$ where the function $a(\xi)$
satisfies $p$-growth conditions in $\xi$. By $\mathcal{E} u$ we denote the
symmetric part of the gradient $Du$. In this setting, we establish results on
the fractional higher differentiability of both the symmetric part of the
gradient $ D u$ and of the pressure $\pi$.

</details>


### [44] [Global weak solutions with higher regularity to the compressible isentropic Navier-Stokes equations in the half-plane under a slip boundary condition](https://arxiv.org/abs/2507.03505)
*Shuai Wang,Guochun Wu,Xin Zhong*

Main category: math.AP

TL;DR: Global existence of weak solutions for compressible isentropic Navier-Stokes equations with vacuum in the half-plane under large bulk viscosity and slip boundary conditions.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of proving global existence for weak solutions with arbitrarily large initial data and stronger regularity than previous results.

Method: Uses a Desjardins-type logarithmic interpolation inequality and new techniques based on the effective viscous flux.

Result: Demonstrates global existence of weak solutions under specified conditions, with improved regularity.

Conclusion: The study provides a robust framework for analyzing weak solutions in compressible fluid dynamics with large initial data.

Abstract: We prove the global existence of weak solutions to the compressible
isentropic Navier-Stokes equations with vacuum in the half-plane under a slip
boundary condition provided the bulk viscosity coefficient is properly large.
In particular, the initial data can be arbitrarily large and the regularity of
solutions is stronger than Lions-Feireisl's weak solutions with general
finite-energy initial data. Our method relies on a Desjardins-type logarithmic
interpolation inequality and some new techniques based on the effective viscous
flux.

</details>


### [45] [Incompressible limit for the 3D compressible FENE dumbbell model](https://arxiv.org/abs/2507.03533)
*Jincheng Gao,Jiahong Wu,Zheng-an Yao,Ruijia Yu*

Main category: math.AP

TL;DR: The paper studies the global-in-time incompressible limit of the compressible FENE dumbbell model on T^3, driven by large volume viscosity, using time-weighted a priori estimates and a novel momentum-based approach.


<details>
  <summary>Details</summary>
Motivation: To understand the incompressible limit of the compressible FENE dumbbell model, especially the challenges posed by large volume viscosity suppressing high-frequency decay.

Method: Develops time-weighted a priori estimates and introduces a momentum-based estimate to address weakened dissipation and derive uniform-in-time decay estimates.

Result: Demonstrates a time-decreasing convergence rate toward the incompressible limit by leveraging enhanced decay of the incompressible momentum component.

Conclusion: The approach successfully closes a priori estimates and establishes the desired incompressible limit with a time-decreasing convergence rate.

Abstract: In this work, we study the global-in-time incompressible limit of the
compressible FENE dumbbell model on the three-dimensional torus T^3, where the
incompressible limit is driven by large volume viscosity. To establish this
limit, we develop time-weighted a priori estimates that yield decay rates for
strong solutions. A key challenge arises from the fact that increasing the
volume viscosity suppresses the decay of high-frequency components, thereby
weakening the dissipation of the density and complicating the derivation of
uniform-in-time decay estimates. To overcome this difficulty, we introduce a
novel momentum-based estimate and show that the incompressible component of the
momentum decays faster in time than the velocity itself. Exploiting this
enhanced decay, we successfully close the a priori estimates and establish a
time-decreasing convergence rate toward the incompressible limit.

</details>


### [46] [A priori bounds for stochastic porous media equations via regularity structures](https://arxiv.org/abs/2507.03575)
*Markus Tempelmayr,Hendrik Weber*

Main category: math.AP

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We prove a priori bounds for solutions of singular stochastic porous media
equations with multiplicative noise in their natural $L^1$-based regularity
class. We consider the first singular regime, i.e.~noise of space-time
regularity $\alpha-2$ for $\alpha\in(2/3,1)$, and prove modelledness of the
solution in the sense of regularity structures with respect to the solution of
the corresponding linear stochastic heat equation. The proof relies on the
kinetic formulation of the equation and a novel renormalized energy inequality.
A careful analysis allows to balance the degeneracy of the diffusion
coefficient against sufficiently strong damping of the multiplicative noise for
small values of the solution.

</details>


### [47] [Steklov vs. Steklov: A Fourth-Order Affair Related to the Babuška Paradox](https://arxiv.org/abs/2507.03677)
*Francesco Ferraresso,Pier Domenico Lamberti*

Main category: math.AP

TL;DR: The paper explores two fourth-order Steklov problems, revealing a Babuška paradox in their approximations on convex domains using convex polygons. It extends continuity results for eigenvalues and connects findings to variants of the Babuška paradox and a degeneration result.


<details>
  <summary>Details</summary>
Motivation: To investigate the behavior of eigenvalues in fourth-order Steklov problems on convex domains and uncover paradoxes in their approximations.

Method: Analyzes domain perturbation effects on eigenvalues, focusing on convex domains and polygons. Examines a nonlocal, second-order problem for harmonic functions.

Result: Demonstrates continuous dependence of eigenvalues on domain perturbations in convex domains, extending known results for the first eigenvalue.

Conclusion: The study links eigenvalue continuity to the Babuška paradox and Maz'ya-Nazarov's degeneration result, providing insights into approximation challenges in convex domains.

Abstract: We discuss two fourth-order Steklov problems and highlight a Babu\v{s}ka
paradox appearing in their approximations on convex domains via sequences of
convex polygons. To do so, we prove that the eigenvalues of one of the two
problems depend with continuity upon domain perturbation in the class of convex
domains, extending a result known in the literature for the first eigenvalue.
This is obtained by examining in detail a nonlocal, second-order problem for
harmonic functions introduced by Ferrero, Gazzola, and Weth. We further review
how this result is connected to diverse variants of the classical Babu\v{s}ka
paradox for the hinged plate and to a degeneration result by Maz'ya and
Nazarov.

</details>


### [48] [Attractor of the limiting Navier--Stokes--Voigt system in $\mathbb R^4$](https://arxiv.org/abs/2507.03686)
*Alexei Ilyin,Varga Kalantarov,Sergey Zelik*

Main category: math.AP

TL;DR: Analysis of the Navier-Stokes-Voigt system in 4D space, focusing on attractor theory due to its simplicity and elegance in this dimension.


<details>
  <summary>Details</summary>
Motivation: Despite lacking physical justification for 4D, the attractor theory is uniquely simple and elegant in this dimension, unlike others.

Method: Develops theory for well-posedness, dissipativity, global attractor existence, and dimension estimates.

Result: The system exhibits a well-defined attractor in 4D with tractable properties.

Conclusion: The 4D case offers a mathematically elegant framework for attractor theory, distinct from other dimensions.

Abstract: The Navier--Stokes--Voigt system in the whole four-dimensional space is
considered. Although we do not know any physical reasons to consider this
system in space dimension four, the attractors theory for this case becomes
especially simple and elegant and nothing similar happens when the space
dimension is different than four. These notes are devoted to developing this
theory, including well-posedness, dissipativity, existence of a global
attractor and estimates for its dimension.

</details>


### [49] [Time-asymptotic self-similarity of the damped compressible Euler equations in parabolic scaling variables](https://arxiv.org/abs/2507.03688)
*Thomas Eiter,Stefanie Schindler*

Main category: math.AP

TL;DR: The paper analyzes the long-term behavior of solutions to the compressible Euler equations with frictional damping, showing convergence to self-similar solutions of the porous medium equation and Darcy's law, with explicit convergence rates.


<details>
  <summary>Details</summary>
Motivation: To understand the asymptotic behavior of compressible Euler equations with damping, especially how density and momentum evolve over time in different spatial dimensions.

Method: Transform the system into parabolic scaling variables, derive a relative entropy inequality, and analyze weak solutions (1D) and energy-variational solutions (multi-D).

Result: Density converges to a self-similar solution of the porous medium equation, and momentum follows Darcy's law, with convergence rates depending on the limit profile's flatness.

Conclusion: The study provides insights into the long-time dynamics of damped compressible Euler equations, with explicit convergence results for both 1D and multi-D cases.

Abstract: We study the long-time behavior of solutions to the compressible Euler
equations with frictional damping in the whole space, where we prescribe
direction-dependent values for the density at spatial infinity. To this end, we
transform the system into parabolic scaling variables and derive a relative
entropy inequality, which allows to conclude the convergence of the density
towards a self-similar solution to the porous medium equation while the
associated limit momentum is governed by Darcy's law. Moreover, we obtain
convergence rates that explicitly depend on the flatness of the limit profile.
While we focus on weak solutions in the one-dimensional case, we extend our
results to energy-variational solutions in the multi-dimensional setting.

</details>


### [50] [Régularité du rayon hyperbolique](https://arxiv.org/abs/2507.03717)
*Satyanad Kichenassamy*

Main category: math.AP

TL;DR: The paper proves that the hyperbolic radius of the maximal solution to a specific PDE is smooth up to the boundary.


<details>
  <summary>Details</summary>
Motivation: To understand the regularity of solutions to the PDE Δu = 4exp(2u) near the boundary.

Method: Uses new Schauder estimates for Fuchsian elliptic equations.

Result: The hyperbolic radius v = exp(-u) is of class C^{2+α} up to the boundary.

Conclusion: The maximal solution's hyperbolic radius exhibits smooth behavior at the boundary.

Abstract: Let $\Omega\subset\mathbb R^2$ be a bounded domain of class $C^{2+\alpha}$,
$0<\alpha<1$. We show that if $u$ is the maximal solution of $\Delta u =
4\exp(2u)$, which tends to $+\infty$ as $(x,y)\to\partial\Omega$, then the
hyperbolic radius $v=\exp(-u)$ is of class $C^{2+\alpha}$ up to the boundary.
The proof relies on new Schauder estimates for Fuchsian elliptic equations.

</details>


### [51] [Second order estimates for a free boundary phase transition](https://arxiv.org/abs/2507.03810)
*Jingeon An*

Main category: math.AP

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: It is well known that minimizers of the Allen-Cahn-type functional
  \[
  J_\epsilon(u):=\int_\Omega\frac{\epsilon|\nabla
u|^2}{2}+\frac{W(u)}{\epsilon},
  \]
  where $W$ is a double-well potential, resemble minimal surfaces in the sense
that their level sets converge to a minimal surface as $\epsilon\rightarrow 0$.
In this work, we consider the indicator potential $W(t)=\chi_{(-1,1)}(t)$,
which leads to the Bernoulli-type free-boundary problem
  \[
  \begin{cases}
  \Delta u=0&\quad\textrm{ in }\{|u|<1\}
  |\nabla u|=\epsilon^{-1}&\textrm{ on }\partial \{|u|<1\}.
  \end{cases}
  \]
  We provide a short proof that the transition layers are uniformly
$C^{2,\alpha}$ regular, up to the free boundary. In addition to the uniform
$C^{2,\alpha}$ estimate, we also obtain improved interior $C^\alpha$ mean
curvature bound that decays in algebraic rate of $\epsilon$. We develop a
framework to interpret the family of level surfaces as a geometric flow, where
the notion of the time coincides with the level. This results in an simple
elliptic equation for the surface velocity $\sigma=1/|\nabla u|$, from which
uniform estimates readily follow.

</details>


### [52] [Existence and uniqueness of solutions of degenerate elliptic equations with lower order terms](https://arxiv.org/abs/2507.03818)
*Seyma Cetin,David Cruz-Uribe,Feyza Elif Dal,Scott Rodney,Yusuf Zeren*

Main category: math.AP

TL;DR: Existence and uniqueness of solutions to a degenerate elliptic Dirichlet problem under weak hypotheses and degenerate Sobolev/Poincaré inequalities, with trade-offs between Sobolev inequality strength and coefficient integrability.


<details>
  <summary>Details</summary>
Motivation: To generalize existing results on degenerate elliptic equations by relaxing assumptions and exploring the interplay between Sobolev inequality strength and coefficient integrability.

Method: Analyze a Dirichlet problem with a degenerate linear second-order elliptic operator, assuming weak hypotheses and degenerate Sobolev/Poincaré inequalities.

Result: Proved existence and uniqueness of solutions, showing flexibility in assumptions (weaker Sobolev inequalities with stronger coefficient integrability).

Conclusion: The results generalize prior work on degenerate elliptic equations, offering broader applicability under relaxed conditions.

Abstract: We prove the existence and uniqueness of solutions to a Dirichlet problem
  \[ \begin{cases} Lu = f + v^{-1}\text{Div}(v{\bf e} h), & x \in \Omega;
  u = 0, & x \in \partial \Omega,
  \end{cases}\]
  where $L$ is a degenerate, linear, second order elliptic operator with lower
order terms. We assume very weak hypotheses, in terms of the coefficients of
the equation, and we also assume the existence of degenerate Sobolev and
Poincar\'e inequalities. One notable feature of our result is that we show that
we can assume significantly weaker versions of the Sobolev inequality if we in
turn assume stronger integrability conditions on the coefficients. Our theorems
generalize a number of results in the literature on degenerate elliptic
equations.

</details>


### [53] [On the Regularity of Navier-Stokes Equations in Critical Space](https://arxiv.org/abs/2507.03881)
*Shiyang Xiong,Liqun Zhang*

Main category: math.AP

TL;DR: The paper proves smoothness and non-blowup of solutions to the Navier-Stokes equations in critical scaling-invariant spaces.


<details>
  <summary>Details</summary>
Motivation: To establish regularity conditions for weak solutions of the Navier-Stokes equations in critical spaces, ensuring smoothness and preventing blowup.

Method: Analyzes weak solutions in scaling-invariant spaces $L_t^{\infty}L_{x_3}^{p_1}L_{x_h}^{p_2}(Q_T)$ with specific constraints on $p_1$ and $p_2$.

Result: If $u(x,t)$ lies in the specified spaces, it is smooth in $Q_T$ and does not blow up at $t=0$.

Conclusion: The results provide regularity criteria for Navier-Stokes solutions, particularly in $L_t^{\infty}L_{x_3}^{\infty}L_{x_h}^{2}(Q_T)$.

Abstract: This paper focuses on the regularity of the Navier-Stokes equations in
critical space. Let $ u(x,t) $ and $ p(x,t) $ denote suitable weak solution of
the Navier-Stokes equations in $Q_T=\mathbb{R}^3\times(-T, 0)$. We prove that
if $u(x,t)$ is in the scaling invariant spaces
$L_t^{\infty}L_{x_3}^{p_1}L_{x_h}^{p_2}(Q_T)$ , where $
\frac{1}{p_1}+\frac{2}{p_2}=1 $ , $p_1\geq 2$ and $ x_h = (x_1, x_2) $ , then $
u $ is a smooth solution in $ Q_T $ and doesn't blow up at $ t = 0 $. In
particular, if $ u(x,t) \in L_t^{\infty}L_{x_3}^{\infty}L_{x_h}^{2}(Q_T)$, then
$u(x,t)$ is a smooth solution in $ Q_T $ and regular up to $ t = 0 $.

</details>


### [54] [Sharp convergence rate of Schrödinger operator along curves](https://arxiv.org/abs/2507.03883)
*Meng Wang,Zhichao Wang*

Main category: math.AP

TL;DR: Analysis of convergence rates for Schrödinger operators along curves, with sharp results up to endpoints.


<details>
  <summary>Details</summary>
Motivation: To understand the convergence behavior of Schrödinger operators in Sobolev spaces, particularly for functions in $H^{s}(\mathbb{R}^{d})$.

Method: Study the operator $U_{\gamma}f(x,t)$ and analyze its convergence rates.

Result: Sharp convergence rates are derived, except at the endpoints.

Conclusion: The study provides precise convergence insights for Schrödinger operators along curves, with limitations at endpoints.

Abstract: We analyze convergence rate of Schr\"odinger operator along curves
$U_{\gamma}f(x,t)$, where $f \in H^{s}\left(\mathbb{R}^{d}\right)$. All results
are sharp up to the endpoints.

</details>


### [55] [Sharp pointwise convergence of Schrödinger operator with complex time along curves](https://arxiv.org/abs/2507.03891)
*Binyu Wang,Zhichao Wang*

Main category: math.AP

TL;DR: Study of almost everywhere convergence of Schrödinger operators with complex time along curves, including fractional cases, with sharp results up to endpoints.


<details>
  <summary>Details</summary>
Motivation: To explore and establish convergence properties of Schrödinger operators in complex time settings, particularly along curves, and extend these findings to fractional cases.

Method: Analyzing the convergence of Schrödinger operators with complex time along curves, including fractional scenarios.

Result: Sharp convergence results are obtained, valid up to the endpoints.

Conclusion: The study provides rigorous and sharp convergence results for Schrödinger operators with complex time, extending to fractional cases, with implications for mathematical analysis.

Abstract: In this paper, we study the almost everywhere convergence results of
Schr\"odinger operator with complex time along curves. We also consider the
fractional cases. All results are sharp up to the endpoints.

</details>


### [56] [Supersonic Euler-Poisson flows with nonzero vorticity in convergent nozzles](https://arxiv.org/abs/2507.03896)
*Yuanyuan Xing,Zihao Zhang*

Main category: math.AP

TL;DR: The paper proves the existence and stability of supersonic flows in a convergent nozzle under electric potential effects, using a deformation-curl-Poisson decomposition and linearized system analysis.


<details>
  <summary>Details</summary>
Motivation: To understand the interplay between electric potential and nozzle geometry in stabilizing supersonic flows with vorticity.

Method: Reformulates the Euler-Poisson system into a deformation-curl-Poisson system, uses transport equations, and analyzes a linearized hyperbolic-elliptic coupled system with boundary perturbations.

Result: Demonstrates that electric fields can counteract nozzle geometry effects, stabilizing supersonic flows.

Conclusion: The electric field force stabilizes compressible flows in convergent nozzles, preserving key physical features.

Abstract: This paper concerns supersonic flows with nonzero vorticity governed by the
steady Euler-Poisson system, under the coupled effects of the electric
potential and the geometry of a convergent nozzle. By the coordinate rotation,
the existence of radially symmetric supersonic flows is proved. We then
establish the structural stability of these background supersonic flows under
multi-dimensional perturbations of the boundary conditions. One of the crucial
ingredients of the analysis is the reformulation of the steady Euler-Poisson
system into a deformation-curl-Poisson system and several transport equations
via the deformation-curl-Poisson decomposition. Another one is to obtain the
well-posedness of the boundary value problem for the associated linearized
hyperbolic-elliptic coupled system, which is achieved through a delicate choice
of multiplier to derive a priori estimates. The result indicates that the
electric field force in compressible flows can counteract the geometric effects
of the convergent nozzle and thereby stabilize key physical features of the
flow.

</details>


### [57] [Enhanced dissipation for the N-Dimensional elastic FENE dumbbell model near an equilibrium](https://arxiv.org/abs/2507.03962)
*Zheng-an Yao,Ruijia Yu*

Main category: math.AP

TL;DR: The paper analyzes the stability and decay of perturbations in the FENE dumbbell model without velocity dissipation, using enhanced dissipation mechanisms to achieve global stability and decay rates.


<details>
  <summary>Details</summary>
Motivation: The lack of velocity dissipation makes the problem challenging, as the Euler equation is unstable without coupling. The study aims to understand stability and decay in this context.

Method: Investigates two types of enhanced dissipation (equilibrium and coupling/interaction) and combines them with the Fourier splitting method.

Result: Establishes global stability in $H^s$-type Sobolev norms and derives decay rates matching those of models with velocity dissipation.

Conclusion: Enhanced dissipation plays a critical role in achieving stability and decay in the FENE dumbbell model without velocity dissipation.

Abstract: This paper studies the $N$-dimensional FENE dumbbell model without velocity
dissipation, focusing on the stability and decay of perturbations near the
steady solution $(0,\psi_\infty)$. Due to the lack of velocity dissipation, the
above problems are highly challenging. In fact, without coupling, the
corresponding $N$-dimensional Euler equation is well known to be unstable. To
overcome this difficulty, we investigate two types of enhanced dissipation
associated with the system governing the perturbations near this steady state,
one due to the equilibrium and one due to the coupling and interaction. These
enhanced dissipation mechanisms enable us to establish the global stability in
the $H^s$-type Sobolev norms. Also, we highlight the critical role of enhanced
dissipation in the decay estimates of the elastic FENE dumbbell model. By
combining this property with the Fourier splitting method, we derive the decay
rate which is identical to that of the general FENE dumbbell with velocity
dissipation.

</details>


### [58] [A new proof of a Liouville theorem for the one dimensional Gross-Pitaevskii equation](https://arxiv.org/abs/2507.03966)
*Michał Kowalczyk,Yvan Martel*

Main category: math.AP

TL;DR: An alternate proof of Liouville theorems for black and dark solitons in the Gross-Pitaevskii equation is provided using a factorization identity for the linearized operator.


<details>
  <summary>Details</summary>
Motivation: To simplify the spectral analysis and provide a more straightforward proof of the asymptotic stability of solitons.

Method: Uses a factorization identity for the linearized operator to trivialize spectral analysis.

Result: Demonstrates an alternative proof for the Liouville theorems, confirming the stability of solitons.

Conclusion: The factorization identity offers a simpler approach to proving soliton stability in the Gross-Pitaevskii equation.

Abstract: The asymptotic stability of the black and dark solitons of the
one-dimensional Gross-Pitaevskii equation was proved by B\'ethuel, Gravejat and
Smets (Ann. Sci. \'Ec. Norm. Sup\'er. 48 (2015)) and Gravejat and Smets (Proc.
Lond. Math. Soc. 111 (2015)), using a rigidity property in the vicinity of
solitons. We provide an alternate proof of the Liouville theorems in the above
articles using a factorization identity for the linearized operator which
trivializes the spectral analysis.

</details>


### [59] [Three bound states with prescribed angular momentum to the cubic-quintic NLS equations in $\mathbb{R}^{3}$](https://arxiv.org/abs/2507.04057)
*Shuai Yao,Juntao Sun*

Main category: math.AP

TL;DR: The paper explores bound states with specific angular momentum and mass for cubic-quintic NLS in 3D, identifying three solutions: local minimizer, mountain pass type, and global minimizer. A new minimax method links these solutions and compares their energies.


<details>
  <summary>Details</summary>
Motivation: To address the existence and properties of bound states in nonlinear Schrödinger equations with cubic-quintic nonlinearity, particularly focusing on solutions with prescribed angular momentum and mass.

Method: Uses a minimax method to construct a mountain pass path, linking the three solutions (local minimizer, mountain pass type, global minimizer) and comparing their energy levels.

Result: Demonstrates the existence of three distinct solutions for the double-constrained problem and establishes their geometric and energetic relationships.

Conclusion: This work provides the first analysis of three solutions for such problems, with the method also applicable to single-constraint cases.

Abstract: In this paper, we investigate bound states with prescribed angular momentum
and mass for the nonlinear Schr\"{o}dinger equations (NLS) with the
cubic-quintic nonlinearity in dimensions three. We demonstrate that there exist
three solutions for the double constrained problem: a local minimizer, a
mountain pass type solution, and a global minimizer. Moreover, by means of the
minimax method, we construct a new mountain pass path and further obtain the
geometric link among the three solutions as well as a comparison of their
energy levels. This seems to be the first paper concerning three solutions,
with the method also being applicable to the single constraint problem.

</details>


### [60] [Global existence for full compressible Navier-Stokes equations around the Couette flow with a temperature gradient in an infinite channel](https://arxiv.org/abs/2507.04088)
*Tuowei Chen,Qiangchang Ju*

Main category: math.AP

TL;DR: Global strong solutions exist for 2D compressible Navier-Stokes equations between moving and stationary walls with small temperature differences, low Reynolds and Mach numbers. Low Mach limit is shown for equal wall temperatures.


<details>
  <summary>Details</summary>
Motivation: To study the behavior of compressible Navier-Stokes equations in a specific setup with moving and stationary walls and temperature differences.

Method: Analyze the equations under close-to-Couette flow initial conditions, low Reynolds and Mach numbers, and small temperature differences.

Result: Existence of global strong solutions under specified conditions; low Mach number limit for equal wall temperatures.

Conclusion: The study confirms stability and solvability under controlled conditions, with implications for fluid dynamics in similar setups.

Abstract: This paper is concerned with the two-dimensional full compressible
Navier-Stokes equations between two infinite parallel isothermal walls, where
the upper wall is moving with a horizontal velocity, while the lower wall is
stationary, and there allows a temperature difference between the two walls. It
is shown that if the initial state is close to the Couette flow with a
temperature gradient, then the global strong solutions exist, provided that the
Reynolds and Mach numbers are low and the temperature difference between the
two walls is small. The low Mach number limit of the global strong solutions is
also shown for the case that both walls maintain the same temperature.

</details>


### [61] [Regularity of velocity averages in kinetic equations with heterogeneity](https://arxiv.org/abs/2507.04102)
*Marko Erceg,Kenneth H. Karlsen,Darko Mitrović*

Main category: math.AP

TL;DR: The paper establishes regularity estimates for kinetic equations with spatially heterogeneous drift, proving solutions belong to fractional Sobolev spaces. It extends known results for homogeneous settings to general heterogeneous cases.


<details>
  <summary>Details</summary>
Motivation: Recent progress in kinetic equations has shown compactness for weak solutions under non-degeneracy conditions, but quantitative regularity for heterogeneous settings remains unexplored.

Method: The study introduces a quantitative non-degeneracy condition for x-dependent drift vectors and proves regularity in fractional Sobolev spaces for solutions.

Result: Solutions map to fractional Sobolev spaces W_loc^(β,r) for some β∈(0,1) and r≥1, generalizing previous homogeneous results.

Conclusion: This work provides the first quantitative regularity estimate for heterogeneous kinetic equations, with applications to entropy solutions of conservation laws.

Abstract: This study investigates the regularity of kinetic equations with spatial
heterogeneity. Recent progress has shown that velocity averages of weak
solutions $h$ in $L^p$ ($p>1$) are strongly $L^1_{\text{loc}}$ compact under
the natural non-degeneracy condition. We establish regularity estimates for
equations with an $\boldsymbol{x}$-dependent drift vector $\mathfrak{f} =
\mathfrak{f}(\boldsymbol{x}, \boldsymbol{\lambda})$, which satisfies a
quantitative version of the non-degeneracy condition. We prove that
$(t,\boldsymbol{x}) \mapsto \int \rho(\boldsymbol{\lambda})
h(t,\boldsymbol{x},\boldsymbol{\lambda})\, d\boldsymbol{\lambda}$, for any
sufficiently regular $\rho(\cdot)$, belongs to the fractional Sobolev space
$W_{\text{loc}}^{\beta,r}$, for some regularity $\beta\in (0,1)$ and
integrability $r \geq 1$ exponents. While such estimates have long been known
for $\boldsymbol{x}$-independent drift vectors
$\mathfrak{f}=\mathfrak{f}(\boldsymbol{\lambda})$, this is the first
quantitative regularity estimate in a general heterogeneous setting. As an
application, we obtain a regularity estimate for entropy solutions to
heterogeneous conservation laws with nonlinear flux and $L^\infty$ initial
data.

</details>


### [62] [Maximal Double-Exponential Growth for the Euler Equation on the Half-Plane](https://arxiv.org/abs/2507.04198)
*Andrej Zlatos*

Main category: math.AP

TL;DR: Smooth solutions to the Euler equation on the half-plane can show double-exponential growth of vorticity gradients, with the maximal growth rate determined and solutions constructed to saturate it.


<details>
  <summary>Details</summary>
Motivation: To explore the behavior of vorticity gradients in smooth solutions to the Euler equation on unbounded or 2D domains, where such results were previously lacking.

Method: Analyzed smooth solutions to the Euler equation on the half-plane, determining the maximal growth rate of vorticity gradients and constructing solutions that achieve this rate.

Result: Demonstrated double-exponential growth of vorticity gradients, identified the maximal growth rate, and provided solutions that saturate this rate.

Conclusion: This work provides the first results on double-exponential growth of vorticity gradients for the Euler equation on an unbounded or 2D domain.

Abstract: We show that smooth solutions to the Euler equation on the half-plane can
exhibit double-exponential growth of their vorticity gradients. We also
determine the maximal possible growth rate and construct solutions that
saturate it. These appear to be the first such results on an unbounded resp.
any 2D domain.

</details>


### [63] [Existence of normalized solutions to nonlinear Schrödinger equations with potential on lattice graphs](https://arxiv.org/abs/2507.04204)
*Weiqi Guan*

Main category: math.AP

TL;DR: The paper studies ground state normalized solutions for a Schrödinger equation on a lattice, identifying a threshold for solution existence based on potential and nonlinearity conditions.


<details>
  <summary>Details</summary>
Motivation: To understand the conditions under which normalized solutions exist for the Schrödinger equation on a discrete lattice, particularly focusing on the role of potential and nonlinearity.

Method: Analyzes the equation with trapping or well potential and Berestycki-Lions type nonlinearity, using threshold analysis to determine solution existence.

Result: A threshold α is identified: no solutions exist for a < α, while solutions exist for a > α. Positivity of α depends on the nonlinearity's mass behavior near zero.

Conclusion: The study provides clear conditions for the existence of normalized solutions, linking the threshold α to the nonlinearity's properties near zero.

Abstract: We study the existence of ground state normalized solution of the following
Schr\"{o}dinger equation: \begin{equation*} \begin{cases} -\Delta
u+V(x)u+\lambda u=f(x,u), & x\in\mathbb{Z}^d \\ \Vert u\Vert_2^2=a \end{cases}
\end{equation*}
  where $V(x)$ is trapping potential or well potential, $f(x,u)$ satisfies
Berestycki-Lions type condition and other suitable conditions. We show that
there always exists a threshold $\alpha\in[0,\infty)$ such that there do not
exist ground state normalized solutions for $a\in (0,\alpha)$, and there exists
a ground state normalized solution for $a\in(\alpha,\infty)$. Furthermore, we
prove sufficient conditions for the positivity of $\alpha$ that $\alpha=0$ if
$f(x,u)$ is mass-subcritical near 0, and $\alpha>0$ if $f(x,u)$ is
mass-critical or mass-supercritical near 0.

</details>


### [64] [Existence of radially symmetric stationary solutions for viscous and Heat-conductive ideal Gas](https://arxiv.org/abs/2507.04234)
*Itsukoo Hashimoto,Akitaka Matsumura*

Main category: math.AP

TL;DR: Existence and uniqueness of radially symmetric stationary solutions for compressible viscous/heat-conductive fluid on an unbounded domain, with decay rate estimates.


<details>
  <summary>Details</summary>
Motivation: To study stationary solutions for compressible fluids under prescribed boundary and far-field conditions.

Method: Analyze radially symmetric solutions for inflow/outflow problems near the far-field state.

Result: Unique existence proven for small neighborhoods; algebraic decay rates estimated.

Conclusion: Stationary solutions exist uniquely and decay algebraically toward the far field.

Abstract: We consider the existence of radially symmetric stationary solutions of the
compressible viscous and heat-conductive polytropic ideal fluid on the
unbounded exterior domain of a sphere where the boundary and far-field
conditions are prescribed. The unique existence of the stationary solution is
shown for both inflow and outflow problems in a suitably small neighborhood of
the far-field state. Estimates of the algebraic decay rate toward the far field
state are also obtained.

</details>


### [65] [Existence Theory for a class of semilinear mixed local and nonlocal equations involving variable singularities and singular measures](https://arxiv.org/abs/2507.04260)
*Sanjit Biswas,Prashanta Garain*

Main category: math.AP

TL;DR: Existence of weak solutions for mixed local-nonlocal problems with singular nonlinearities and measure-valued data, including novel treatment of variable singular exponents.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of solving mixed local-nonlocal problems with singular nonlinearities and measure-valued data, particularly focusing on singular measure data, which is unexplored even for constant exponents.

Method: Analysis of weak solutions for problems with variable singular exponents and measure-valued data, including singular and non-singular measures.

Result: Established existence of weak solutions for the described class of problems, highlighting the novelty of handling singular measure data.

Conclusion: The study successfully extends the understanding of weak solutions in mixed local-nonlocal problems, particularly with singular measure data, offering new insights for variable exponents.

Abstract: This article establishes the existence of weak solutions for a class of mixed
local-nonlocal problems with pure and perturbed singular nonlinearities. A key
novelty is the treatment of variable singular exponents alongside
measure-valued data. Notably, both source terms may be measures, with the
singular component modeled by both a singular and non-singular measure. Our
main focus is on the singular measure data, which appears to be new, even for
constant exponents.

</details>


### [66] [Fractional Infinity Laplacian with Obstacle](https://arxiv.org/abs/2507.04328)
*Samer Dweik,Ahmad Sabra*

Main category: math.AP

TL;DR: The paper addresses the obstacle problem for the fractional infinity Laplacian with a nonhomogeneous term, proving existence and Hölder regularity of solutions under specific conditions.


<details>
  <summary>Details</summary>
Motivation: To solve the obstacle problem involving the fractional infinity Laplacian with a nonhomogeneous term, ensuring solutions exist and exhibit Hölder continuity.

Method: Approximation of the nonhomogeneous term by a sequence of functions, using Perron's method to prove solution existence, and deriving uniform Hölder estimates.

Result: Existence of a Hölderian solution is proven, with the solution converging from the approximated sequence.

Conclusion: The method successfully solves the obstacle problem, providing a Hölder-continuous solution under given assumptions.

Abstract: This paper deals with the obstacle problem for the fractional infinity
Laplacian with nonhomogeneous term $f(u)$, where $f:\mathbb{R}^+ \mapsto
\mathbb{R}^+$: $$\begin{cases} L[u]=f(u) &\qquad in \{u>0\}\\ u \geq 0 &\qquad
in\, \Omega\\ u=g &\qquad on\, \partial \Omega\end{cases},$$ with
$$L[u](x)=\sup_{y\in \Omega,\,y\neq
x}\dfrac{u(y)-u(x)}{|y-x|^{\alpha}}+\inf_{y\in \Omega,\,y\neq x}
\dfrac{u(y)-u(x)}{|y-x|^\alpha},\qquad 0<\alpha<1.$$ Under the assumptions that
$f$ is a continuous and monotone function and that the boundary datum $g$ is in
$C^{0,\beta}(\partial\Omega)$ for some $0<\beta<\alpha$, we prove existence of
a solution $u$ to this problem. Moreover, this solution $u$ is
$\beta-$H\"olderian on $\overline{\Omega}$. Our proof is based on an
approximation of $f$ by an appropriate sequence of functions $f_\varepsilon$
where we prove using Perron's method the existence of solutions
$u_\varepsilon$, for every $\varepsilon>0$. Then, we show some uniform H\"older
estimates on $u_\varepsilon$ that guarantee that $u_\varepsilon \rightarrow u$
where this limit function $u$ turns out to be a solution to our obstacle
problem.

</details>


### [67] [Local/global well-posedness analysis of time-space fractional Schrödinger equation on $\mathbb{R}^{d}$](https://arxiv.org/abs/2507.04411)
*Yong Zhen Yang,Yong Zhou*

Main category: math.AP

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Based on the $\phi(-\Delta)$-type operator studied by Kim \cite[\emph{Adv.
Math.}]{Kim2}, where $\phi$ is the Bernstein function, this paper investigates
a class of nonlinear time-space fractional Schr\"{o}dinger equations that
exhibit nonlocal effects in both time and space. The time part is derived from
the model proposed by Narahari Achar, and the space part is a
$\phi(-\Delta)$-type operator. Due to nonlocal effects, this invalidates the
classical Strichartz estimate. Combining the asymptotic behavior of
Mittag-Leffler functions, H\"{o}rmander multiplier theory and other methods of
harmonic analysis, we establish the Gagliardo-Nirenberg inequality in the
$\phi$-Triebel-Lizorkin space studied by Mikulevi\v{c}ius \cite[\emph{Potential
Anal.}]{Mikulevicius} and obtain some Sobolev estimates for the solution
operator, thus establishing the global/local well-posedness of the equations in
some Banach space. In particular, our results are complementary to those of Su
\cite[\emph{J. Math. Anal. Appl.}]{Su}, and the methods are quite different.

</details>


### [68] [Non-convergence of the principal eigenvalue of elliptic operators for large advection](https://arxiv.org/abs/2507.04418)
*Xueli Bai,Zhi-An Wang,Xin Xu,Kexin Zhang,Maolin Zhou*

Main category: math.AP

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper investigates the limit of the principal eigenvalue $\lambda(s)$ as
$s\to+\infty$ for the following elliptic equation
  \begin{align*}
-\Delta\varphi(x)-2s\mathbf{v}\cdot\nabla\varphi(x)+c(x)\varphi(x)=\lambda(s)\varphi(x),
\quad x\in \Omega
  \end{align*} in a bounded domain $\Omega\subset \mathbb{R}^d (d\geq 1)$ with
the Neumann boundary condition. Previous studies have shown that under certain
conditions on $\mathbf{v}$, $\lambda(s)$ converges as $s\to\infty$ (including
cases where $\lim\limits_{s \to\infty }\lambda(s)=\pm\infty$). This work
constructs an example such that $\lambda(s)$ is divergent as $s\to+\infty$.
This seems to be the first rigorous result demonstrating the non-convergence of
the principal eigenvalue for second-order linear elliptic operators with some
strong advection. As an application, we demonstrate that for the classical
advection-reaction-diffusion model with advective velocity field
$\mathbf{v}=\nabla m$, where $m$ is a potential function with infinite
oscillations, the principal eigenvalue changes sign infinitely often along a
subsequence of $s\to\infty$. This leads to solution behaviors that differ
significantly from those observed when $m$ is non-oscillatory.

</details>


### [69] [On the well-posedness of time-space fractional Schrödinger equation on $\mathbb{R}^{d}$](https://arxiv.org/abs/2507.04433)
*Yong Zhen Yang,Yong Zhou*

Main category: math.AP

TL;DR: The paper analyzes the well-posedness of time-space fractional Schrödinger equations, addressing derivative loss and lack of semigroup structure. It uses harmonic analysis to establish dispersive estimates and proves local/global well-posedness in specific cases.


<details>
  <summary>Details</summary>
Motivation: To address the challenges posed by fractional Schrödinger equations, such as derivative loss and non-semigroup structure, which make classical methods inapplicable.

Method: Uses harmonic analysis tools (smoothing effect theory, real interpolation, Van der Corput lemma) to derive dispersive estimates for the solution operator.

Result: Proves local and global well-posedness for cases β<2 (1D) and β>2 (higher dimensions) in Sobolev and Lorentz-type spaces. Also shows existence of self-similar solutions.

Conclusion: The work extends understanding of nonlocal evolution equations, highlighting the role of fractional derivatives and dispersive properties in quantum mechanics.

Abstract: This paper considers the well-posedness of a class of time-space fractional
Schr\"{o}dinger equations introduced by Naber. In contrast to the classical
Schr\"{o}dinger equation, the solution operator here exhibits derivative loss
and lacks the structure of a semigroup, which makes the classical Strichartz
estimates inapplicable. By using harmonic analysis tools -- including the
smoothing effect theory of Kenig and Ponce for Korteweg-de Vries equations
\cite[\emph{Commun.~Pure Appl.~Math.}]{Kenig}, real interpolation techniques,
and the Van der Corput lemma -- we establish novel dispersive estimates for the
solution operator. These estimates generalize Ponce's regularity results
\cite[\emph{J.~Funct.~Anal.}]{Ponce} for oscillatory integrals and enable us to
address the derivative loss in the Schr\"{o}dinger kernel. For the cases
$\beta<2$~(in one space dimension) and $\beta>2$~(in higher dimensions), we
prove local and global well-posedness in Sobolev and Lorentz-type spaces,
respectively. Additionally, we analyze the asymptotic behavior of solutions and
demonstrate the existence of self-similar solutions under homogeneous initial
data. The results highlight the interplay between fractional derivatives,
dispersive properties, and nonlinear dynamics, extending the understanding of
nonlocal evolution equations in quantum mechanics and related fields.

</details>


### [70] [Characterising buried objects in metal detection](https://arxiv.org/abs/2507.04450)
*P. D. Ledger,W. R. B. Lionheart*

Main category: math.AP

TL;DR: A new asymptotic model for detecting buried conductive objects accounts for soil conductivity and permeability, improving accuracy over traditional non-conductive soil assumptions.


<details>
  <summary>Details</summary>
Motivation: Existing models ignore soil conductivity and permeability, which can affect detection accuracy, especially at higher frequencies.

Method: Develops an asymptotic expansion for the perturbed magnetic field, incorporating soil properties, and introduces a magnetic polarizability tensor for object identification.

Result: The new model provides more accurate results by considering ground conditions, with conditions for computing tensor coefficients independently of soil properties.

Conclusion: Accounting for soil properties enhances detection accuracy, offering a better approach for identifying buried conductive objects.

Abstract: Current mathematical models for identifying highly conducting buried objects
in metal detection assume that the soil is non-conducting and has the same
permeability as free space. However, although the electrical conductivity of
soil is low, it is not negligible and depends on factors such as soil type and
salinity. Moreover, the magnetic permeability of soil varies with its iron
content and is often described as uncooperative. Depending on the ground
conditions, these soil's properties can influence the induced voltages in the
measurement coils of metal detectors and becomes of increasing importance as
the frequency of the exciting current source is increased. In this work, we
develop a new asymptotic expansion for the perturbed magnetic field due to the
presence of a highly conducting magnetic buried object as its size tends to
zero, which takes account of the ground conditions. The leading order term of
this expansion can be expressed in terms of a complex symmetric rank-2 magnetic
polarizability tensor, which characterises the object, can assist in its
identification, and we provide conditions under which this tensor's
coefficients can be computed independently of the ground conditions. We
demonstrate the improved accuracy of our new result, which takes account of the
ground conditions, over the situation where the soil's conductivity and
permeability are not considered.

</details>


### [71] [Non-radial minimizers for Hardy--Sobolev inequalities in non-convex cones](https://arxiv.org/abs/2507.04470)
*A. I. Nazarov,N. V. Rastegaev*

Main category: math.AP

TL;DR: Symmetry breaking in Neumann problems driven by $p$-Laplacian in non-convex cones, leading to non-radial minimizers and multiple solutions.


<details>
  <summary>Details</summary>
Motivation: Investigate symmetry breaking in Neumann problems arising from Hardy-Sobolev inequalities, extending prior work on Sobolev inequalities.

Method: Analyze radial solutions (Talenti-Bliss functions) and prove non-radiality under specific conditions on the first Neumann eigenvalue of the Beltrami-Laplace operator.

Result: Radial solutions are not extremal; minimizers must be non-radial, yielding multiple solutions for the Neumann problem.

Conclusion: Symmetry breaking occurs under certain eigenvalue conditions, ensuring non-radial minimizers and multiple solutions.

Abstract: The symmetry breaking is obtained for Neumann problems driven by
$p$-Laplacian in certain non-convex cones. These problems are generated by the
Hardy--Sobolev inequalities. In the case of the Sobolev inequality for the
ordinary Laplacian this problem was investigated in (Ciraolo, Pacella, Polvara,
2024).
  Such problems have obvious radial solutions -- Talenti--Bliss type functions
of $|x|$. However, under a certain restriction on the first Neumann eigenvalue
$\lambda_1(D)$ of the Beltrami--Laplace operator on the spherical cross-section
$D$ of the cone we prove this radial solution cannot be an extremal function,
therefore minimizer must be non-radial. This leads to multiple solutions for
the corresponding Neumann problem.

</details>


### [72] [Existence and multiplicity of normalized solutions to a large class of elliptic equations on bounded domains with general boundary conditions](https://arxiv.org/abs/2507.04624)
*Claudianor O. Alves,Zhentao He,Chao Ji*

Main category: math.AP

TL;DR: The paper studies the existence and multiplicity of normalized solutions for a nonlinear Schrödinger equation using a perturbation method, with applications to other related equations.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of finding normalized solutions for nonlinear Schrödinger equations under specific boundary conditions and explore broader applications.

Method: Adapts the perturbation method to analyze the equation with prescribed constraints and boundary conditions.

Result: Demonstrates the existence and multiplicity of solutions, with potential extensions to other equations like those with critical exponential growth or magnetic fields.

Conclusion: The approach is versatile, offering insights into normalized solutions and broader applicability to related nonlinear problems.

Abstract: In this paper, by adapting the perturbation method, we study the existence
and multiplicity of normalized solutions for the following nonlinear
Schr\"odinger equation
  $$
  \left\{
  \begin{array}{ll}
  -\Delta u = \lambda u + f(u)\quad & \text{in } \Omega,
  \mathcal{B}_{\alpha,\zeta,\gamma}u = 0 & \text{on } \partial \Omega,
  \int_{\Omega} |u|^2\,dx = \mu,
  \end{array}
  \right.
  \leqno{(P)^\mu_{\alpha,\zeta,\gamma}}
  $$
  where $\Omega \subset \mathbb{R}^N$ ($N \geq 1$) is a smooth bounded domain,
$\mu>0$ is prescribed, $\lambda \in \mathbb{R}$ is a part of the unknown which
appears as a Lagrange multiplier, $f,g:\mathbb{R} \to \mathbb{R}$ are
continuous functions satisfying some technical conditions. The boundary
operator $\mathcal{B}_{\alpha,\zeta,\gamma}$ is defined by
  $$
  \mathcal{B}_{\alpha,\zeta,\gamma}u=\alpha u+\zeta \frac{\partial u}{\partial
\eta }-\gamma g(u),
  $$ where $\alpha,\zeta,\gamma \in \{0,1\}$ and $\eta$ denotes the outward
unit normal on $\partial\Omega$. Moreover, we highlight several further
applications of our approach, including the nonlinear Schr\"{o}dinger equations
with critical exponential growth in $\mathbb{R}^{2}$, the nonlinear
Schr\"{o}dinger equations with magnetic fields, the biharmonic equations, and
the Choquard equations, among others.

</details>


### [73] [Multi-vortices and lower bounds on the attractor dimensions for 2D Navier--Stokes equations](https://arxiv.org/abs/2507.04645)
*Anna Kostianko,Alexei Ilyin,Dominic Stone,Sergey Zelik*

Main category: math.AP

TL;DR: A new method for estimating lower bounds of attractor dimensions in hydrodynamic equations, applied to 2D Navier-Stokes equations, yielding sharp results where none existed before.


<details>
  <summary>Details</summary>
Motivation: To provide lower bounds for attractor dimensions in hydrodynamic equations, addressing gaps in existing knowledge, particularly for bounded domains and the whole plane.

Method: Uses multi-vortex structures (Vishik vortices) as analogues to Kolmogorov flows, avoiding reliance on traditional methods.

Result: Achieves sharp lower bounds for the whole plane and similar bounds for bounded domains, matching known estimates for tori and spheres.

Conclusion: The method is versatile, replicating known results and applicable to various hydrodynamic equations, filling a critical gap in the field.

Abstract: We present a principally new method, which is not based on the Kolmogorov
flows, for obtaining the lower bounds for the attractors dimensions of the
equations related with hydrodynamics and apply it to the classical 2D
Navier--Stokes equations in a bounded domain as well as for the Navier--Stokes
equations with Ekman damping in the whole plane. In the case of bounded
domains, we give the lower bounds, which are similar to the well-known estimate
on a torus and sphere and in the case of the whole plane our estimate is sharp.
Note that no lower bounds for these two cases were known before. \par We
suggest to use the so-called multi-vortex, which consists of a well-separated
Vishik vortices (i.e., spectrally unstable localized in space flows constructed
by M.M. Vishik), as the analogue of the Kolmogorov flows. Note also that this
method reproduces the known result on the torus and that it is applicable to
many other equations of hydrodynamics.

</details>


### [74] [Perimeter on a manifold, with applications to partial differential equations](https://arxiv.org/abs/2507.04740)
*Satyanad Kichenassamy*

Main category: math.AP

TL;DR: The paper generalizes the notion of perimeter for subsets of Riemannian manifolds, links it to heat kernel regularization, and applies it to solve a quasilinear elliptic problem using spherical symmetrization.


<details>
  <summary>Details</summary>
Motivation: To extend the concept of perimeter from Euclidean space to Riemannian manifolds and apply it to problems where traditional symmetrization methods fail.

Method: Generalize perimeter using heat kernel regularization, prove related inequalities, and introduce spherical symmetrization for quasilinear elliptic problems.

Result: The perimeter is shown as a limit of heat kernel regularization, with generalizations of isoperimetric inequality and Fleming-Rishel formula. The method successfully solves a previously intractable problem.

Conclusion: The generalized perimeter and spherical symmetrization provide powerful tools for analyzing problems in Riemannian manifolds and beyond.

Abstract: The perimeter of a measurable subset of $\mathbb R^N$ is the total variation
of its characteristic function. We generalize this notion to a subset $E$ of a
closed Riemannian manifold. We show that the perimeter of $E$ is the limit of
the hear kernel regularization of its characteristic function. A generalization
of the isoperimetric inequality and of the Fleming-Rishel formula follow. These
results are applied to a quasilinear elliptic problem in $\mathbb R^N$ for
which the usual symmetrization methods fail. It will be tackled successfully by
introducing a symmetrization method on the sphere.

</details>


### [75] [Global strong solution of the 3D compressible liquid crystal flows with density-dependent viscosity and large velocity](https://arxiv.org/abs/2507.04760)
*Jiaxu Li,Yu Mei,Rong Zhang*

Main category: math.AP

TL;DR: Global strong solution for 3D compressible liquid crystal flows with large initial density and small initial director derivative.


<details>
  <summary>Details</summary>
Motivation: Address the Cauchy problem for 3D compressible liquid crystal flows with density-dependent viscosity, focusing on conditions for global strong solutions.

Method: Analyze the system with viscosity coefficients as power functions of density (power > 1), assuming large initial density and small initial director derivative.

Result: Proves existence of a unique global strong solution under the specified conditions, without requiring small velocity.

Conclusion: First result for global strong solutions in 3D compressible liquid crystal flows without small velocity constraints.

Abstract: This paper concerns the Cauchy problem of three-dimensional compressible
liquid crystal flows with density-dependent viscosity. When the viscosity
coefficients $\mu_1(\rho),\mu_2(\rho)$ are power functions of the density with
the power larger than $1$, it is proved that the system exists a unique global
strong solution as long as the initial density is sufficiently large and
$L^3$-norm of the derivative of the initial director is sufficiently small.
This is the first result concerning the global strong solution for
three-dimensional compressible liquid crystal flows without smallness of
velocity.

</details>


### [76] [Direct reconstruction of general elastic inclusions](https://arxiv.org/abs/2507.04831)
*Sarah Eberle-Blick,Henrik Garde,Nuutti Hyvönen*

Main category: math.AP

TL;DR: The paper extends the monotonicity method for reconstructing inclusions in linear elasticity, allowing for both positive and negative inclusions with finite or extreme contrast.


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing methods that only handle inclusions with finite contrast (either positive or negative) by generalizing the approach to include extreme cases like infinitely stiff or perfectly elastic materials.

Method: The monotonicity method is used, a direct reconstruction technique, now generalized to handle mixed inclusions (positive/negative) and extreme contrasts.

Result: The method is proven to work for inclusions of any type (positive/negative, finite/extreme contrast), expanding its applicability.

Conclusion: The generalized monotonicity method successfully broadens the scope of reconstructing inclusions in linear elasticity, accommodating diverse material properties.

Abstract: The inverse problem of linear elasticity is to determine the Lam\'e
parameters, which characterize the mechanical properties of a domain, from
pairs of pressure activations and the resulting displacements on its boundary.
This work considers the specific problem of reconstructing inclusions that
manifest themselves as deviations from the background Lam\'e parameters.
  The monotonicity method is a direct reconstruction method that has previously
been considered for domains only containing positive (or negative) inclusions
with finite contrast. That is, all inclusions have previously been assumed to
correspond to a finite increase (or decrease) in both Lam\'e parameters
compared to their background values. We prove the general outer approach of the
monotonicity method that simultaneously allows positive and negative
inclusions, of both finite and extreme contrast; the latter refers to either
infinitely stiff or perfectly elastic materials.

</details>


### [77] [Existence of solutions with prescribed frequency for the perturbed Schrödinger-Bopp-Podolsky system in bounded domains](https://arxiv.org/abs/2507.04914)
*Danilo Gregorin Afonso,Bruno Mascaro*

Main category: math.AP

TL;DR: The paper proves the existence of infinitely many solutions for the Schrödinger-Bopp-Podolsky system under Dirichlet boundary conditions, using a variational approach and a symmetric Mountain Pass theorem.


<details>
  <summary>Details</summary>
Motivation: To address the solvability of the Schrödinger-Bopp-Podolsky system with prescribed frequencies and arbitrary boundary conditions, leveraging nonlinear perturbations.

Method: Variational approach with a symmetric variant of the Mountain Pass theorem.

Result: Infinitely many solutions exist for the system under the given conditions.

Conclusion: The study successfully demonstrates the existence of multiple solutions, expanding the understanding of such systems.

Abstract: In this paper, we show that the Schr\"odinger-Bopp-Podolsky system with
Dirichlet boundary conditions in a bounded domain possesses infinitely many
solutions of prescribed frequency, for any set of (continuous) boundary
conditions, provided that the Schr\"odinger equation is perturbed with a
suitable nonlinearity. Our approach is variational, and our proof is based on a
symmetric variant of the Mountain Pass theorem.

</details>


### [78] [Irregular double-phase evolution problem: existence and global regularity](https://arxiv.org/abs/2507.04924)
*Rakesh Arora,Sergey Shmarev*

Main category: math.AP

TL;DR: The paper studies the existence and regularity of solutions to a double-phase evolution equation with irregular coefficients and variable exponents, under specific balance and integrability conditions.


<details>
  <summary>Details</summary>
Motivation: To address the challenges posed by non-differentiable coefficients and variable exponents in double-phase problems, ensuring solution existence and regularity.

Method: The solution is derived as the limit of classical solutions to regularized problems, leveraging balance conditions and integrability assumptions.

Result: Existence of a solution is proven, with additional properties like higher integrability and second-order regularity.

Conclusion: The study successfully establishes the existence and regularity of solutions under the given conditions, contributing to the understanding of double-phase evolution equations.

Abstract: We investigate the homogeneous Dirichlet problem for the irregular
double-phase evolution equation \[ u_t-\operatorname{div} \left( a(z)|\nabla
u|^{p(z)-2} \nabla u + b(z)|\nabla u|^{q(z)-2} \nabla u\right)=f(z),\quad
z=(x,t)\in Q_T:=\Omega\times (0,T),
  \] where $\Omega \subset \mathbb{R}^N$, $N \geq 2$ is a bounded domain,
$T>0$, The non-differentiable coefficients $a(z)$, $b(z)$, the free term $f$,
and the variable exponents $p$, $q$ are given functions. The coefficients $a$
and $b$ are nonnegative, bounded, satisfy the inequality \[ a(z)+b(z)\geq
\alpha \quad \text{in} \ Q_T, \quad \text{and} \quad |\nabla a|, |\nabla b|,
a_t, b_t \in L^d(Q_T) \] for some constant $\alpha>0$, and with $d>2$ depending
on $\sup p(z)$, $\sup q(z)$, $N$, and the regularity of initial data $u(x,0)$.
The free term $f$ and initial data $u(x,0)$ satisfy \[ f\in L^\sigma(Q_T) \
\text{with} \ \sigma>2 \quad \text{and} \quad |\nabla u(x,0)|\in L^{r}(\Omega)
\ \text{with} \ r\geq \max \bigg\{2,\sup_{Q_T}p(z),\sup_{Q_T}q(z)\bigg\}. \]
The variable exponents $p,q \in C^{0,1}(\overline{Q}_T)$ satisfy the balance
condition \[ \frac{2N}{N+2} < p(z), q(z)< +\infty \ \text{in} \ \overline Q_T
\quad \text{and} \quad \max\limits_{\overline Q_T}|p(z)-q(z)|< \dfrac{2}{N+2}.
\] Under the above assumptions, we establish the existence of a solution, which
is obtained as the limit of classical solutions to a family of regularized
problems and preserves initial temporal integrability: \[
  |\nabla u(\cdot, t)| \in L^r(\Omega) \ \text{for a.e.} \ t \in (0,T), \]
gains global higher integrability: \[ |\nabla u|^{\min\{p(z), q(z)\} + s +r}
\in L^1(Q_T) \ \text{for any} \ s \in \left(0, \frac{4}{N+2}\right), \] and
attains second-order regularity: \[ a(z) |\nabla u|^{\frac{p+r-2}{2}}+b(z)
|\nabla u|^{\frac{q+r-2}{2}}\in L^2(0,T;W^{1,2}(\Omega)). \]

</details>


### [79] [Note on injectivity in second-gradient Nonlinear Elasticity](https://arxiv.org/abs/2507.04938)
*Stanislav Hencl,Kaushik Mohanta*

Main category: math.AP

TL;DR: The paper proves that planar mappings in $W^{2,q}$ with certain conditions on the Jacobian determinant are homeomorphisms if they match a boundary homeomorphism. The condition $(1-\frac{1}{q})a \geq 1$ is sharp. It also provides a sharp result on the measure of the zero Jacobian set's projection.


<details>
  <summary>Details</summary>
Motivation: To establish conditions under which Sobolev mappings are homeomorphisms, particularly focusing on the Jacobian determinant's role and boundary behavior.

Method: Analyzes planar mappings in the Sobolev space $W^{2,q}$ with constraints on the Jacobian determinant and boundary agreement. Uses sharp conditions to derive results.

Result: Mappings in $W^{2,q}$ with $|J_f|^{-a} \in L^1$ and boundary agreement are homeomorphisms. The condition $(1-\frac{1}{q})a \geq 1$ is optimal. A sharp measure result for zero Jacobian projections is also given.

Conclusion: The paper provides sharp conditions ensuring homeomorphic properties of Sobolev mappings and extends understanding of Jacobian determinant behavior in higher dimensions.

Abstract: Let $q>1$, $(1-\frac{1}{q})a\geq 1$ and let $\Omega\subset \mathbb{R}^2$ be
Lipschitz domain. We show that planar mappings in the second order Sobolev
space $f\in W^{2,q}(\Omega,\mathbb{R}^2)$ with $|J_f|^{-a}\in L^1(\Omega)$ are
homeomorphism if they agree with a homeomorphism on the boundary. The condition
$(1-\frac{1}{q})a\geq 1$ is sharp. We also have a new sharp result about the
$\mathcal{H}^{n-1}$ measure of the projection of the set $\{J_f=0\}$ in
$\mathbb{R}^n$.

</details>


### [80] [Notes on $L^2$-estimates in linear elliptic equations with general coefficients](https://arxiv.org/abs/2507.04940)
*Haesung Lee*

Main category: math.AP

TL;DR: The paper presents an explicit $L^2$-estimate for weak solutions of linear elliptic equations, bounding the $L^2$-norm of the solution by a computable constant multiple of the $L^2$-norm of the source term.


<details>
  <summary>Details</summary>
Motivation: To provide explicit and computable error bounds for weak solutions of elliptic equations, addressing limitations of classical compactness-based methods.

Method: Uses a divergence-free transformation method to derive explicit constants, ensuring robustness even without zero-order terms.

Result: The $L^2$-estimate is robust, and the constant decreases with increasing diffusion coefficient or zero-order term.

Conclusion: The results offer rigorous foundations for applications like a posteriori error estimates in PINNs, where explicit bounds are crucial.

Abstract: This paper establishes an explicit $L^2$-estimate for weak solutions $u$ to
linear elliptic equations in divergence form with general coefficients and
external source term $f$, stating that the $L^2$-norm of $u$ over $U$ is
bounded by a constant multiple of the $L^2$-norm of $f$ over $U$. In contrast
to classical approaches based on compactness arguments, the proposed method,
which employs a divergence-free transformation method, provides a computable
and explicit constant $C>0$. The $L^2$-estimate remains robust even when there
is no zero-order term, and the analysis further demonstrates that the constant
$C>0$ decreases as the diffusion coefficient or the zero-order term increases.
These quantitative results provide a rigorous foundation for applications such
as a posteriori error estimates in Physics-Informed Neural Networks (PINNs),
where explicit error bounds are essential.

</details>


### [81] [The Cauchy Problem for Symmetric Hyperbolic Systems with Nonlocal Potentials](https://arxiv.org/abs/2507.05004)
*Felix Finster,Simone Murro,Gabriel Schmid*

Main category: math.AP

TL;DR: The paper studies the initial value problem for symmetric hyperbolic systems on globally hyperbolic Lorentzian manifolds with nonlocal potentials, proving well-posedness under certain conditions and providing counterexamples for failure cases. Applications include Maxwell's equations and the Dirac equation.


<details>
  <summary>Details</summary>
Motivation: To rigorously analyze the Cauchy problem for symmetric hyperbolic systems with nonlocal potentials, with applications in physics, particularly for semiclassical Einstein equations.

Method: The study involves analyzing the well-posedness of the Cauchy problem under bounded retarded potentials and short-range non-retarded potentials, with additional decay conditions. A counterexample is provided for failure cases.

Result: Well-posedness is proven for retarded potentials, while strong solutions exist for non-retarded potentials under specific decay and bound conditions. A counterexample shows failure when bounds are too large.

Conclusion: The paper establishes foundational results for the Cauchy problem in symmetric hyperbolic systems with nonlocal potentials, with implications for physical models like Maxwell's and Dirac equations, and aims to extend this to semiclassical Einstein equations.

Abstract: In this paper, we investigate the initial value problem for symmetric
hyperbolic systems on globally hyperbolic Lorentzian manifolds with potentials
that are both nonlocal in time and space. When the potential is retarded and
uniformly bounded in time, we establish well-posedness of the Cauchy problem on
a time strip, proving existence, uniqueness, and regularity of solutions. If
the potential is not retarded but has only short time range, we show that
strong solutions still exist, under the additional assumptions that the uniform
bound in time is sufficiently small compared to the range in time and that its
kernel decays sufficiently fast in time with respect to the zero-order terms of
the system. Furthermore, we present a counterexample demonstrating that when
the uniform bound is too large compared to the time range, solutions may fail
to exist. As an application, we discuss Maxwell's equations in linear
dispersive media on ultrastatic spacetimes, as well as the Dirac equation with
nonlocal potential naturally arising in the theory of causal fermion systems.
Our paper aims to represent the starting point for a rigorous study for the
Cauchy problem for the semiclassical Einstein equations.

</details>


### [82] [Asymmetric Self-similar Spiral Solutions of 2-D Incomressible Euler Equations](https://arxiv.org/abs/2507.05059)
*Hyungjun Choi*

Main category: math.AP

TL;DR: Nonradial, self-similar solutions for 2D incompressible Euler equations without rotational symmetry are constructed, extending prior algebraic spiral solutions.


<details>
  <summary>Details</summary>
Motivation: To generalize existing solutions (Elling and Shao-Wei-Zhang's algebraic spirals) by removing the rotational symmetry constraint.

Method: Construction of nonradial, self-similar solutions for the 2D incompressible Euler equations.

Result: Successful generalization of prior solutions, demonstrating existence of nonradial, self-similar solutions.

Conclusion: The work extends the understanding of Euler equation solutions by introducing nonradial, self-similar cases.

Abstract: We construct nonradial, self-similar solutions to the two-dimensional
incompressible Euler equations in the absence of rotational symmetry. This
generalizes the algebraic spiral solutions of Elling and Shao-Wei-Zhang.

</details>


### [83] [The Landau equation and Fisher information](https://arxiv.org/abs/2507.05167)
*Nestor Guillen,Luis Silvestre*

Main category: math.AP

TL;DR: The paper rules out blow-up in the Landau equation with Coulomb potential by showing the Fisher information does not increase over time, using a novel 'lifted equation' method.


<details>
  <summary>Details</summary>
Motivation: To address the problem of blow-up in the Landau equation with Coulomb potential by exploring the behavior of the Fisher information over time.

Method: Introduces a 'lifted equation,' an auxiliary linear equation in double the variables, which encodes the nonlinear collision operator. For the Landau equation, this reduces to a family of heat equations on the sphere.

Result: Blow-up is ruled out due to the non-increasing nature of the Fisher information, proven via the lifted equation.

Conclusion: The approach provides a new way to analyze blow-up in kinetic equations, connecting Fisher information and Bakry-Emery theory.

Abstract: In this expository note (submitted to Notices of the AMS) we present the
ideas used in our recent work ruling out blow up for the Landau equation with
Coulomb potential. Blow up is ruled out by the discovery that the Fisher
information is not increasing in time along a solution. This monotonicity is
established by means of a new ``lifted equation'' which is an auxiliary linear
equation in double the number of variables that encodes the nonlinear nonlocal
collision operator. For the Landau equation in particular this lifted equation
amounts to a family of heat equations over the sphere. Some background on
kinetic equations and the Fisher information, and connections to Bakry-Emery
theory is also discussed.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [84] [Acceleration of the CASINO quantum Monte Carlo software using graphics processing units and OpenACC](https://arxiv.org/abs/2507.02888)
*B. Thorpe,M. J. Smith,P. J. Hasnip,N. D. Drummond*

Main category: physics.comp-ph

TL;DR: Quantum Monte Carlo calculations using CASINO software are accelerated with GPUs and OpenACC, achieving speedups of up to 2.5x for simulations with hundreds of particles.


<details>
  <summary>Details</summary>
Motivation: To enhance the efficiency of quantum Monte Carlo calculations by leveraging GPU acceleration and OpenACC for computationally intensive tasks like Ewald summation and orbital evaluation.

Method: Utilizes GPUs and OpenACC to offload tasks such as Ewald summation, Jastrow correlation factor evaluation, and blip basis set orbital calculations.

Result: Achieves significant speedups (up to 2.5x) for simulations with hundreds of particles, with minimal accuracy loss when using single-precision arithmetic.

Conclusion: GPU acceleration with OpenACC effectively speeds up quantum Monte Carlo calculations, particularly for large-scale simulations, without compromising accuracy.

Abstract: We describe how quantum Monte Carlo calculations using the CASINO software
can be accelerated using graphics processing units (GPUs) and OpenACC. In
particular we consider offloading Ewald summation, the evaluation of long-range
two-body terms in the Jastrow correlation factor, and the evaluation of
orbitals in a blip basis set. We present results for three- and two-dimensional
homogeneous electron gases and ab initio simulations of bulk materials, showing
that significant speedups of up to a factor of 2.5 can be achieved by the use
of GPUs when several hundred particles are included in the simulations. The use
of single-precision arithmetic can improve the speedup further without
significant detriment to the accuracy of the calculations.

</details>


### [85] [Estimating Free Parameters in Stochastic Oscillatory Models Using a Weighted Cost Function](https://arxiv.org/abs/2507.03200)
*Joseph M. Marcinik,Dzmitry Vaido,Dolores Bozovic*

Main category: physics.comp-ph

TL;DR: A novel cost function combining power spectral density, analytic signal, and position crossings is developed to estimate parameters in stochastic oscillatory systems, validated on test data and applied to a biophysical auditory model.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of parameter estimation in stochastic oscillatory systems by capturing distinct oscillatory characteristics like amplitude, frequency, and shape.

Method: Develop a cost function incorporating power spectral density, analytic signal, and position crossings, weighted for different oscillatory traits, and minimize it using differential evolution.

Result: Successful parameter estimation in two stochastic systems, validated by recovering known parameters from test data, and applied to a biophysical auditory model.

Conclusion: The study presents a general methodology for fitting stochastic oscillatory systems, demonstrated through validation and application.

Abstract: In this study, we estimate parameters in stochastic oscillatory systems by
developing a novel cost function. This function incorporates power spectral
density, analytic signal, and position crossings, each weighted to capture
distinct oscillatory characteristics such as amplitude, frequency, and shape.
By minimizing this cost via differential evolution, we estimate parameters in
two stochastic systems given measured datasets. We validate this procedure by
recovering known parameters from a test dataset. We then apply it to a
biophysical model for auditory mechanics. Thus, we establish a general
methodology for fitting stochastic oscillatory systems.

</details>


### [86] [Real-time prediction of plasma instabilities with sparse-grid-accelerated optimized dynamic mode decomposition](https://arxiv.org/abs/2507.03245)
*Kevin Gill,Ionut-Gabriel Farcas,Silke Glas,Benjamin J. Faber*

Main category: physics.comp-ph

TL;DR: The paper explores efficient training of parametric data-driven reduced-order models (ROMs) using sparse grid interpolation with (L)-Leja points for high-dimensional input spaces, applied to gyrokinetic plasma simulations.


<details>
  <summary>Details</summary>
Motivation: Standard training methods for ROMs are computationally expensive due to high-dimensional input spaces, limiting their use in many-query tasks like design optimization and uncertainty quantification.

Method: The study employs sparse grid interpolation with (L)-Leja points and optimized dynamic mode decomposition (optDMD) to construct parametric ROMs for gyrokinetic simulations.

Result: The method successfully constructs accurate ROMs with only 28 high-fidelity simulations, demonstrating effectiveness in both benchmark and real-world scenarios.

Conclusion: Sparse grid-based parametric ROMs show promise for enabling computationally intensive tasks in fusion research and other applications.

Abstract: Parametric data-driven reduced-order models (ROMs) that embed dependencies in
a large number of input parameters are crucial for enabling many-query tasks in
large-scale problems. These tasks, including design optimization, control, and
uncertainty quantification, are essential for developing digital twins in
real-world applications. However, standard training data generation methods are
computationally prohibitive due to the curse of dimensionality, as their cost
scales exponentially with the number of inputs.This paper investigates
efficient training of parametric data-driven ROMs using sparse grid
interpolation with (L)-Leja points, specifically targeting scenarios with
higher-dimensional input parameter spaces. (L)-Leja points are nested and
exhibit slow growth, resulting in sparse grids with low cardinality in
low-to-medium dimensional settings, making them ideal for large-scale,
computationally expensive problems. Focusing on gyrokinetic simulations of
plasma micro-instabilities in fusion experiments as a representative real-world
application, we construct parametric ROMs for the full 5D gyrokinetic
distribution function via optimized dynamic mode decomposition (optDMD) and
sparse grids based on (L)-Leja points. We perform detailed experiments in two
scenarios: First, the Cyclone Base Case benchmark assesses optDMD ROM
prediction capabilities beyond training time horizons and across variations in
the binormal wave number. Second, for a real-world electron temperature
gradient driven micro-instability simulation featuring six input parameters, we
demonstrate that an accurate parametric optDMD ROM can be constructed at a cost
of only $28$ high-fidelity gyrokinetic simulations thanks to sparse grids. In
the broader context of fusion research, these results demonstrate the potential
of sparse grid-based parametric ROMs to enable otherwise intractable many-query
tasks.

</details>


### [87] [Sequential Neural Operator Transformer for High-Fidelity Surrogates of Time-Dependent Non-linear Partial Differential Equations](https://arxiv.org/abs/2507.03272)
*Qibang Liu,Seid Koric*

Main category: physics.comp-ph

TL;DR: The paper introduces the Sequential Neural Operator Transformer (S-NOT) to improve accuracy and robustness in predicting solutions for highly nonlinear, time-dependent PDEs, outperforming S-DeepONet (S-DON) in benchmarks.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of existing surrogate models in handling highly nonlinear responses from sequential input functions in PDE solutions.

Method: Combines GRUs with transformer self-attention to better capture dependencies in sequential inputs and spatial query points.

Result: S-NOT achieves higher accuracy than S-DON on challenging datasets, including outliers, demonstrating robustness.

Conclusion: S-NOT is effective for accelerating computational frameworks in scientific and engineering applications involving nonlinear PDEs.

Abstract: Partial differential equations (PDEs) are fundamental to modeling complex and
nonlinear physical phenomena, but their numerical solution often requires
significant computational resources, particularly when a large number of
forward full solution evaluations are necessary, such as in design,
optimization, sensitivity analysis, and uncertainty quantification. Recent
progress in operator learning has enabled surrogate models that efficiently
predict full PDE solution fields; however, these models often struggle with
accuracy and robustness when faced with highly nonlinear responses driven by
sequential input functions. To address these challenges, we propose the
Sequential Neural Operator Transformer (S-NOT), a architecture that combines
gated recurrent units (GRUs) with the self-attention mechanism of transformers
to address time-dependent,nonlinear PDEs. Unlike S-DeepONet (S-DON), which uses
a dot product to merge encoded outputs from the branch and trunk sub-networks,
S-NOT leverages attention to better capture intricate dependencies between
sequential inputs and spatial query points. We benchmark S-NOT on three
challenging datasets from real-world applications with plastic and
thermo-viscoplastic highly nonlinear material responses: multiphysics steel
solidification, a 3D lug specimen, and a dogbone specimen under temporal and
path-dependent loadings. The results show that S-NOT consistently achieves a
higher prediction accuracy than S-DON even for data outliers, demonstrating its
accuracy and robustness for drastically accelerating computational frameworks
in scientific and engineering applications.

</details>


### [88] [General synthetic iterative scheme for multiscale radiative transfer in the finite-volume framework](https://arxiv.org/abs/2507.03975)
*Kaiyuan Wang,Yanbing Zhang,Qi Li,Lei Wu*

Main category: physics.comp-ph

TL;DR: A new synthetic iterative scheme is introduced for efficient and accurate radiative transfer simulation, combining macroscopic and transport effects, with adaptive gradient approximation for opacity discontinuity.


<details>
  <summary>Details</summary>
Motivation: Efficient and accurate simulation of radiative transfer is challenging, requiring methods that handle optically thick media and opacity discontinuities.

Method: The scheme uses a macroscopic synthetic equation combining diffusion limit and transport effects, with an adaptive least square method for gradient approximation.

Result: Significant speed-up in optically thick problems and insights into Knudsen layer resolution in the Tophat problem.

Conclusion: The method is effective for radiative transfer simulation, offering speed and accuracy while addressing key challenges.

Abstract: Achieving efficient and accurate simulation of the radiative transfer has
long been a research challenge. Here we introduce the general synthetic
iterative scheme as an easy-to-implement approach to address this issue. First,
a macroscopic synthetic equation, which combines the asymptotic equation at the
diffusion limit and the "high-order terms" extracted from the transport
equation to account for transport effects, is introduced to accelerate the
simulation of the radiative transfer equation. Second, the asymptotic
preserving property is directly provided by the macroscopic process,
eliminating the need for fine spatial discretization in optically thick media,
as well as the need for consistency enforcement. Third, to address the issue of
opacity discontinuity in the finite volume method, an adaptive least square
method for gradient approximation is proposed. Numerical results on several
canonical tests demonstrate that, in optically thick problems, our method
achieves significant speed-up over the conventional iterative schemes. Finally,
with our newly developed method, we reveal the importance of resolving the
Knudsen layer in the initial stage of Tophat problem, while in steady-state the
Knudsen layer can be under-resolved.

</details>


### [89] [High-Fidelity Modelling of the Molten Salt Fast Reactor](https://arxiv.org/abs/2507.04129)
*Maximiliano Dalinger,Elia Merzari,Saya Lee,Casey Emler*

Main category: physics.comp-ph

TL;DR: The paper develops a multiphysics model for the Molten Salt Fast Reactor (MSFR) using Cardinal, integrating neutronic-thermal hydraulic feedback and precursor transport, with limitations in OpenMC for DNP feedback.


<details>
  <summary>Details</summary>
Motivation: To address the complex phenomenology of the MSFR, which requires multiphysics modeling due to its liquid fuel acting as coolant.

Method: Uses Cardinal with OpenMC for neutronic equations and NekRS for mass, momentum, energy, and precursor transport, employing a RANS k-t turbulence model.

Result: The model shows reasonable behavior for temperature, heat source, velocity, DNPs, and DHPs, but lacks DNP feedback due to OpenMC limitations.

Conclusion: Future work aims to include DNP feedback in OpenMC to enhance the model.

Abstract: The Molten Salt Fast Reactor (MSFR) is one of the six GEN-IV reactor designs.
In the MSFR, the liquid fuel is the coolant, which moves throughout the primary
circuit. This complex phenomenology requires multiphysics modeling. In the
present paper, a model of the MSFR is developed in the multiphysics code
Cardinal, considering neutronic-thermal hydraulic feedback and the transport of
delayed neutron precursors (DNPs) and decay heat precursors (DHPs). OpenMC is
used to solve neutronic equations, and NekRS is used to solve mass, momentum,
energy, DNPs, and DHPs distribution. A RANS k-t turbulence model is used in
NekRS. DNPs and DHPs are modeled using a convective-diffusion equation with
modified source terms considering radioactive decay. Cardinal results showed a
reasonable behavior for temperature, heat source, velocity, DNPs, and DHPs.
However, the current limitations in OpenMC do not allow the modification of
delayed neutron source locations. Ongoing efforts look to include this feature
in future work to introduce DNP feedback in OpenMC.

</details>


### [90] [RePlaChem: A dimensionality reduction library for plasma chemical mechanisms](https://arxiv.org/abs/2507.04401)
*Z. Nikolaou,E. Morais,S. Van Rompaey,C. Anastassiou,A. Bogaerts,V. Vavourakis*

Main category: physics.comp-ph

TL;DR: RePlaChem is a software library for reducing large-scale plasma chemical mechanisms into smaller skeletal ones, enabling faster simulations and analysis of dominant reactions.


<details>
  <summary>Details</summary>
Motivation: To address the computational expense of large-scale plasma chemical simulations by simplifying mechanisms without significant accuracy loss.

Method: The library parses mechanisms in ZDPlasKin-compatible format, applies reduction algorithms, and generates skeletal mechanisms for seamless solver integration.

Result: Successfully reduced a methane-hydrogen plasma mechanism (77 species, 4404 reactions) with minimal accuracy loss.

Conclusion: RePlaChem effectively accelerates simulations and aids in analyzing reaction physics, validated by its application to a complex mechanism.

Abstract: In this work, we present RePlaChem, a software library for reducing detailed
large-scale plasma chemical mechanisms to smaller skeletal ones. The library
parses a plasma chemical mechanism in the well-established format compatible
with the software ZDPlasKin, runs the reduction algorithm, and generates
automatically a skeletal chemical mechanism. This feature allows the seamless
implementation of the skeletal chemistry in well-known solvers including
ZDPlasKin. In turn, the reduction of the chemistry accelerates otherwise
computationally expensive numerical simulations. Furthermore, RePlaChem can be
used as an analysis tool to shed light into the underlying reaction physics by
identifying the dominant species and associated reactions. In order to validate
and demonstrate its capabilities, RePlaChem is used to substantially reduce a
large-scale methane-hydrogen plasma chemical mechanism with 77 species and 4404
reactions at various orders with a relatively small loss of accuracy. GitHub:
https: // github. com/ znikolaou/ RePlaChem. git

</details>


### [91] [Reduced SIGMA Basis Sets: a new family of SIGMA basis sets for molecular calculations](https://arxiv.org/abs/2507.04975)
*Ignacio Ema,Jesús San-Fabián,Guillermo Ramírez,Rafael López,José Manuel García-de-la-Vega*

Main category: physics.comp-ph

TL;DR: Reduced SIGMA basis sets, sharing Dunning basis sets' composition, improve performance by reducing linear dependencies in large systems, enhancing convergence and lowering costs.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies in large systems by reducing linear dependencies and improving computational performance.

Method: Introducing reduced SIGMA basis sets, derived from Gaussian-type SIGMA basis sets, and testing them preliminarily.

Result: Enhanced performance in large systems through reduced linear dependencies, better convergence, and lower computational costs.

Conclusion: Reduced SIGMA basis sets offer a promising approach for efficient computations in large systems.

Abstract: A new family of Gaussian-type SIGMA basis sets, termed reduced SIGMA basis
sets, is introduced and preliminarily tested. Sharing the same composition as
Dunning basis sets, they enhance performance by reducing linear dependencies in
large systems, thereby improving convergence and lowering computational costs
for such systems.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [92] [Heat-flux Instabilities of Regularized Kappa Distributed Strahl Electrons Resolved with ALPS](https://arxiv.org/abs/2507.03084)
*Dustin L. Schröder,Marian Lazar,Rodrigo A. López,Horst Fichtner*

Main category: physics.plasm-ph

TL;DR: The paper investigates heat-flux instabilities in the solar wind using regularized Kappa distributions (RKDs) for electron strahl, resolving them with the ALPS solver. It identifies whistler and firehose instabilities, showing Maxwellian models can misrepresent effects compared to RKDs.


<details>
  <summary>Details</summary>
Motivation: To better understand and model the heat-flux instabilities in the solar wind, which are regulated by suprathermal electron populations like the electron strahl, using a more accurate distribution (RKDs).

Method: Uses regularized Kappa distributions (RKDs) to characterize electron strahl and resolves instabilities with the ALPS solver, comparing results with Maxwellian and standard Kappa models.

Result: Identifies whistler and firehose heat-flux instabilities, showing Maxwellian models can overrate or underestimate effects, while standard Kappa models underestimate firehose growth rates.

Conclusion: RKDs provide a more accurate representation of heat-flux instabilities, highlighting limitations of idealized Maxwellian and standard Kappa models.

Abstract: The fluid behavior of the solar wind is affected by the heat flux carried by
the suprathermal electron populations, especially the electron strahl (or beam)
that propagates along the magnetic field. In turn, the electron strahl cannot
be stable, and in the absence of collisions, its properties are regulated
mainly by self-generated instabilities. This paper approaches the description
of these heat-flux instabilities in a novel manner using regularized Kappa
distributions (RKDs) to characterize the electron strahl. RKDs conform to the
velocity distributions with suprathermal tails observed in situ, and at the
same time allow for consistent macromodeling, based on their singularity-free
moments. In contrast, the complexity of RKD models makes the analytical kinetic
formalism complicated and still inaccessible, and therefore, here heat-flux
instabilities are resolved using the advanced solver ALPS. Two primary types of
instabilities emerge depending on plasma conditions: the whistler and firehose
heat-flux instabilities. The solver is successfully tested for the first time
for such instabilities by comparison with previous results for standard
distributions, such as Maxwellian and Kappa. Moreover, the new RKD results show
that idealized Maxwellian models can overrate or underestimate the effects of
these instabilities, and also show differences from those obtained for the
standard Kappa, which, for instance, underestimate the firehose heat-flux
growth rates.

</details>


### [93] [Multi-messenger dynamic imaging of laser-driven shocks in water using a plasma wakefield accelerator](https://arxiv.org/abs/2507.03179)
*Mario D. Balcazar,Hai-En Tsai,Tobias Ostermayr,Paul T. Campbell,Qiang Chen,Cary Colgan,Gillis M. Dyer,Zachary Eisentraut,Eric Esarey,Cameron G. R. Geddes,Benjamin Greenwood,Anthony Gonsalves,Sahel Hakimi,Robert Jacob,Brendan Kettle,Paul King,Karl Krushelnick,Nuno Lemos,Eva Los,Yong Ma,Stuart P. D. Mangles,John Nees,Isabella M. Pagano,Carl Schroeder,Raspberry Simpson,Alexander G. R. Thomas,Matthew Trantham,Jeroen van Tilborg,Anthony Vazquez,Carolyn C. Kuranz*

Main category: physics.plasm-ph

TL;DR: A dual-probe platform using X-rays and electron beams at 1 Hz tracks plasma dynamics in laser-driven fusion, revealing shock compression and electromagnetic fields not captured by traditional methods.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of traditional diagnostics in studying dense matter hydrodynamics for laser-driven fusion, by providing high-repetition-rate, multi-messenger insights.

Method: A dual-probe system combines ultrafast X-rays and relativistic electron beams to interrogate a laser-heated water target, enabling synchronized tracking of plasma evolution.

Result: Betatron X-rays show shock compression, while electron beams detect evolving electromagnetic fields, revealing charge separation and ion differentiation.

Conclusion: Combining both probes offers complementary insights, emphasizing the need for hybrid models to predict fusion-relevant plasma behavior accurately.

Abstract: Understanding dense matter hydrodynamics is critical for predicting plasma
behavior in environments relevant to laser-driven inertial confinement fusion.
Traditional diagnostic sources face limitations in brightness, spatiotemporal
resolution, and inability to detect relevant electromagnetic fields. In this
work, we present a dual-probe, multi-messenger laser wakefield accelerator
platform combining ultrafast X-rays and relativistic electron beams at 1 Hz, to
interrogate a free-flowing water target in vacuum, heated by an intense 200 ps
laser pulse. This scheme enables high-repetition-rate tracking of the
interaction evolution using both particle types. Betatron X-rays reveal a
cylindrically symmetric shock compression morphology assisted by low-density
vapor, resembling foam-layer-assisted fusion targets. The synchronized electron
beam detects time-evolving electromagnetic fields, uncovering charge separation
and ion species differentiation during plasma expansion - phenomena not
captured by photons or hydrodynamic simulations. We show that combining both
probes provides complementary insights spanning kinetic to hydrodynamic
regimes, highlighting the need for hybrid physics models to accurately predict
fusion-relevant plasma behavior

</details>


### [94] [Numerical investigation of the effect of high voltage frequency on the density of RONS species in the air atmospheric pressure gas discharge](https://arxiv.org/abs/2507.03396)
*Fariborz Momtazzadeh,Farshad Sohbatzadeh,Hamed Soltani Ahmadi,Ramin Mehrabifard*

Main category: physics.plasm-ph

TL;DR: The study explores how frequency changes in dielectric barrier discharges affect active species density, revealing trends in RONS and O densities, with implications for optimizing plasma applications.


<details>
  <summary>Details</summary>
Motivation: To understand the impact of voltage frequency on plasma characteristics, particularly active species density, for improved practical applications.

Method: Simulated 87 reactions in COMSOL Multiphysics, analyzing species density, electric field, and waveforms under varying frequencies (500 Hz to 500 kHz) at 2 kV.

Result: Higher frequencies decrease RONS density but increase O density, with electron temperature distribution changes. Results align with prior studies.

Conclusion: Findings aid in optimizing plasma parameters for diverse applications, enhancing therapeutic outcomes.

Abstract: In the last few decades, studies in various fields of plasma technology have
expanded and its application in different processes has increased. Therefore,
the achievement of a desirable and practical plasma with specific
characteristics is of particular importance. The frequency of the applied
voltage is one of the important factors that play a role in the physical and
chemical characteristics. In this research, changes in the density of active
species produced in an electrical discharge using a dielectric barrier and air
working gas have been investigated from a frequency of 500 Hz to 500 kHz, and
by applying a constant voltage of 2 kV, have been investigated. For this
purpose, 87 different reactions with specific collision cross-sections were
defined in COMSOL Multiphysics. Other parameters, including current-voltage
waveform, electric field, and species densitywere evaluated. The results show
that under completely identical conditions, the electron temperature
distribution changes with increasing applied frequency, and the density of
reactive oxygen and nitrogen species RONS decreases, but O shows an increasing
trend. It should be noted that the simulation results are in good agreement
with previous experimental and simulation reports. These results offer valuable
insights into optimizing plasma parameters for different applications,
potentially resulting in better treatment outcomes across a range of
therapeutic domains.

</details>


### [95] [Nonlinear Mixing of Waves in a Yukawa One Component Plasma](https://arxiv.org/abs/2507.03645)
*Ajaz Mir,Farida Batool,Sanat Tiwari,Abhijit Sen*

Main category: physics.plasm-ph

TL;DR: Nonlinear wave mixing in a Yukawa plasma is studied via simulations, revealing three-wave mixing as the dominant mechanism, aligning with prior fKdV model findings.


<details>
  <summary>Details</summary>
Motivation: To explore nonlinear wave interactions in a Yukawa plasma and validate the fKdV model's accuracy for dusty plasmas.

Method: Two-dimensional classical Langevin molecular dynamics simulations and bispectral analysis.

Result: Three-wave mixing dominates nonlinear interactions, matching fKdV model predictions.

Conclusion: The fKdV model effectively captures weakly nonlinear dynamics in dusty plasmas.

Abstract: The phenomenon of nonlinear wave mixing is investigated in a Yukawa
one-component plasma using two-dimensional classical Langevin molecular
dynamics simulations. The wave spectrum indicates that nonlinear interactions
between the excited modes are primarily governed by a three-wave mixing
mechanism, as confirmed by bispectral analysis. In particular, the mixing
characteristics observed in the simulations closely resemble those reported in
previous numerical studies of the forced Korteweg-de Vries (fKdV) model [ Phys.
Plasmas 29, 032303 (2022)]. This similarity further validates the applicability
of the fKdV fluid model in capturing the weakly nonlinear dynamics of dusty
plasmas with reasonable accuracy.

</details>


### [96] [Data-Driven Approach to Model the Influence of Magnetic Geometry in the Confinement of Fusion Devices](https://arxiv.org/abs/2507.03776)
*R. Laia,R. Jorge,G. Abreu*

Main category: physics.plasm-ph

TL;DR: The paper explores how stellarator geometry impacts omnigenity (alpha particle confinement) by analyzing quasisymmetric and quasi-isodynamic stellarators. It uses a database, supervised autoencoders, and machine learning models to predict and optimize design parameters.


<details>
  <summary>Details</summary>
Motivation: To understand how stellarator geometry influences omnigenity, a key factor for fusion energy device performance, and to optimize design parameters for better confinement.

Method: A database of stellarator configurations is analyzed using correlations, dimensionality reduction, and supervised autoencoders. Classification and regression models (LightGBM, LightGBM LSS, neural networks) predict solver convergence and omnigenity metrics.

Result: The study identifies key geometric parameters affecting omnigenity and demonstrates the effectiveness of machine learning in optimizing stellarator designs.

Conclusion: Machine learning tools can effectively analyze and predict stellarator performance, aiding in the design of fusion devices with improved omnigenity.

Abstract: The design of fusion energy devices involves a balance between competing
performance metrics to achieve an energy gain. In stellarators, the geometry is
very flexible and involves a large number of free parameters. These can be
optimized to achieve good performance. One of the main optimization targets is
omnigenity, that is, the confinement of alpha particles stemming from the
fusion reactions. In this work, two classes of omnigenous stellarators are
studied, namely quasisymmetric and quasi-isodynamic stellarators. The goal is
to determine the influence of the geometry on omnigenity, which can lead to
greater insight into the design space of stellarators. For this purpose, a
database of stellarator configurations is created and analyzed for
correlations, pair-wise distributions, and dimensionality reduction using a
supervised autoencoder framework. Then, a classification model is trained on
this database to predict the convergence of numerical solvers. Finally, two
regression models, LightGBM and its probabilistic version, LightGBM LSS, as
well as a feed-forward neural network, are trained to predict quasisymmetry and
quasi-isodynamiticity and find the design parameters that most influence
omnigenity.

</details>


### [97] [Laser Amplification in $e^{-}$-$μ^{-}$-ion Plasmas](https://arxiv.org/abs/2507.04301)
*Y. Chen,R. Ou,H. Wang,S. J. Chen,Y. X. Zhong,Y. G. Chen,S. Tan,Y. X. Li,C. Y. Zheng,Z. J. Liu,L. H. Cao,M. M. Zhang,D. P. Feng,W. J. Zuo,C. Z. Xiao*

Main category: physics.plasm-ph

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We investigate laser amplification in $e^{-}$-$\mu^{-}$-ion plasmas, where
negative muons partially replace electrons. Theoretical results reveal a hybrid
plasma wave, called $\mu$-wave that exhibits ion-acoustic behavior in
long-wavelength regime and Langmuir-like behavior in short-wavelength regime.
Besides, the Landau damping of $\mu$-wave is smaller than that of Langmuir
wave. Particle-in-cell (PIC) simulations confirm the theoretical results of
instabilities in$e^{-}$-$\mu^{-}$-ion plasmas. The $\mu$-wave enables efficient
laser amplification by suppressing pump-driven spontaneous instabilities
through enhanced Landau damping of Langmuir waves. Compared to Raman
amplification, $\mu$-wave amplification can maintain the Gaussian waveform of
the seed laser, avoiding pulse splitting. Compared to strongcoupling Brillouin
amplification, $\mu$-wave amplification exhibits weaker filamentation
instability. Our theoretical model can be generalized to other plasma systems
containing two species of negatively charged particles, such as two-temperature
electron plasmas and negative-ion plasma. These findings establish
$e^{-}$-$\mu^{-}$-ion plasma as a promising medium for advanced laser
amplification schemes.

</details>


### [98] [From Filamentation to Stratification: Instability Dynamics in Scissors-Shaped Relativistic Beam-Plasma System](https://arxiv.org/abs/2507.04336)
*Xu Liu,Dong Wu,Jie Zhang*

Main category: physics.plasm-ph

TL;DR: The paper introduces a scissors-shaped configuration for counter-streaming systems, revealing a stratification mode instead of filamentation, leading to passive instability control via geometric design.


<details>
  <summary>Details</summary>
Motivation: To address the broken cylindrical symmetry in intersecting fast electron beams and explore its impact on beam-plasma instabilities.

Method: Theoretical analysis and particle-in-cell simulations of the scissors-shaped configuration.

Result: The system exhibits a stratification mode, rapidly quenched by magnetic reconnection, resulting in significantly lower magnetic energy.

Conclusion: The study establishes a new principle for passive instability control through geometric configuration, applicable in astrophysics and inertial confinement fusion.

Abstract: Counter-streaming systems are a canonical model for beam-plasma
instabilities, such as the filamentation instability, which is critical in high
energy density physics. However, scenarios involving intersecting fast electron
beams break the cylindrical symmetry inherent to such systems. Here, we
introduce the scissors-shaped configuration, a fundamental
multi-velocity-component system that captures this broken symmetry. Through
theoretical analysis and particle-in-cell simulations, we reveal a dramatic
shift in the instability dynamics: the system undergoes a stratification mode
instead of filamentation. This mode is rapidly quenched by magnetic
reconnection, leading to a quasi-stable state with magnetic energy two orders
of magnitude lower than in the counter-streaming case. This discovery
establishes a new principle of passive instability control via geometric
configuration, offering a new perspective on beam-plasma interactions in
astrophysics and inertial confinement fusion. The underlying physics is
verifiable in upcoming multi-laser experiments.

</details>


### [99] [Fluid approach of current-driven Langmuir waves associated with type III radiation and whistler waves: Relevance to PSP and other solar wind observations](https://arxiv.org/abs/2507.04498)
*Konrad Sauer,Kaijun Liu*

Main category: physics.plasm-ph

TL;DR: The paper presents a theoretical model for electron-ion plasma, describing current-driven Langmuir waves converting to type III radiation and possibly exciting whistler waves, differing from classical approaches by avoiding instabilities and wave coalescence.


<details>
  <summary>Details</summary>
Motivation: To propose a simpler mechanism for wave conversion in plasmas without relying on instabilities or wave coalescence, addressing limitations of classical models.

Method: A linearized system of fluid-Maxwell equations is used to model mode coupling between Langmuir and electromagnetic waves, allowing parameter variation for diverse experimental conditions.

Result: The model aligns with satellite observations, including Parker Solar Probe data, and is validated by kinetic PIC simulations for the uncompensated strahl case.

Conclusion: The fluid model offers a straightforward and versatile approach to understanding wave conversion in plasmas, supported by empirical and simulation evidence.

Abstract: A theoretical model on the basis of fluid-Maxwell equations for an
electron-ion plasma is presented which describes the conversion of
current-driven Langmuir waves into type III radiation whereby simultaneously an
excitation of whistler waves may occur. In contrast to the classical approach
of Ginzburg and Zhelezniakov (1958) after which beam-excited Langmuir waves in
a two-step process are converted in electromagnetic radiation, the presented
mechanism works without any instability and wave coalescence. Rather the
electric field oscillations at the electron plasma frequency can be triggered
by different realisations of the driving current, e.g. by the (uncompensated)
net current of the strahl at t=0 in a core-strahl plasma or by given current
variations which may represent different situations in space, as shocks,
magnetic switch-backs etc.. A linearized system of equations is used to
describe the mode coupling occurring at oblique propagation between the mostly
electrostatic Langmuir wave and the adjacent electromagnetic left-hand
polarized (L) wave. The simplicity of the fluid model allows without great
effort the parameters of the current profiles to be varied and thus to simulate
a wide range of possible experimental conditions. Measurements of Langmuir
waves, type III radiation and whistler waves on board various satellites in the
solar wind, and in particular some of the recent results of the Parker Solar
Probe are interpreted in the light of the theoretical model presented. For the
case of the uncompensated strahl, the fluid approach is confirmed by fully
kinetic PIC simulations. One comparison is shown in the Appendix.

</details>


### [100] [PIC simulation of current-driven solar type III radiation and whistler waves in an electron core-strahl plasma: Relevance to PSP and other space observations](https://arxiv.org/abs/2507.04506)
*Konrad Sauer,Kaijun Liu*

Main category: physics.plasm-ph

TL;DR: The paper demonstrates electron current oscillations generating type III radiation and whistler waves without classical plasma emission, using PIC simulations and satellite data.


<details>
  <summary>Details</summary>
Motivation: To challenge the classical plasma emission theory by showing an alternative mechanism for wave generation via current-driven oscillations.

Method: PIC simulations of electron-core-strahl plasma without initial current compensation, analyzing Langmuir waves, type III radiation, and whistler waves.

Result: Electromagnetic fields with amplitude oscillations arise, generating fundamental and second harmonic radiation, supported by satellite observations.

Conclusion: The study provides evidence for a non-classical wave generation mechanism, aligning with satellite data and revisiting earlier simulations.

Abstract: The aim of the paper is to demonstrate that electron current oscillations may
generate electromagnetic waves as type III radiation and whistler waves without
the involvement of the classical plasma emission via the coalescence of waves.
PIC simulation results of an electron-core-strahl plasma without initial
current compensation are presented which describe the conversion of
current-driven Langmuir oscillations/waves into type III radiation whereby
simultaneously whistler waves are excited. In contrast to the classical
approach of Ginzburg and Zhelezniakov (1958) after which beam-excited Langmuir
waves in a two-step process are converted in electromagnetic radiation, any
instability is suppressed by selecting a low strahl velocity. Rather electric
field oscillations at the electron plasma frequency are triggered by the
initially non-compensated current of the strahl. The arising electromagnetic
fields exhibit amplitude oscillations which are caused by the superposition of
the two wave modes of mixed polarisation at the point of mode coupling. This
basic mechanism of wave generation and transformation has already been
described in earlier papers using simple fluid models. It is also the topic of
the companion paper. Besides the fundamental electromagnetic radiation, the
second harmonic of nearly the same intensity has been obtained which is an
indication for nonlinear currents. Measurements of Langmuir waves, type III
radiation and whistler waves on board various satellites in the solar wind, in
particular Parker Solar Probe (PSP) observations are analysed in the light of
our results. Interpretations of earlier PIC simulations are critically
reviewed.

</details>


### [101] [Coherent synchrotron radiation by excitation of surface plasmon polariton on near-critical solid microtube surface](https://arxiv.org/abs/2507.04561)
*Bifeng Lei,Hao Zhang,Daniel Seipt,Alexandre Bonatto,Bin Qiao,Javier Resta-Lopez,Guoxing Xia,Carsten Welsch*

Main category: physics.plasm-ph

TL;DR: A method using surface plasmon polaritons (SPPs) on a microtube to generate coherent synchrotron radiation (CSR) with enhanced coherence and intensity.


<details>
  <summary>Details</summary>
Motivation: To develop powerful ultrashort light sources by leveraging CSR, which is crucial for advanced light applications.

Method: Resonant excitation of SPPs on a near-critical-density microtube by a high-intensity, circularly polarized laser pulse, creating rotating electromagnetic fields that accelerate electrons to emit CSR.

Result: Enhanced CSR coherence and intensity, with X-rays showing up to two orders of magnitude improvement in coherence over incoherent emission.

Conclusion: This scheme efficiently generates high-coherence CSR, promising for advanced light source development.

Abstract: Coherent synchrotron radiation (CSR) is vital for developing powerful
ultrashort light sources. We introduce a CSR generation mechanism using surface
plasmon polaritons (SPPs) resonantly excited on a solid, near-critical-density
microtube. A high-intensity, circularly polarised laser pulse, propagating
along the microtube axis, efficiently couples the cylindrical SPP modes. This
process creates azimuthally structured, rotating electromagnetic fields. These
rotating fields subsequently confine, modulate, and directly accelerate surface
electrons, causing them to emit CSR in the Valilov-Cherenkov angle. We further
demonstrate that by improving the azimuthal symmetry, the helical modulation
enables CSR emission across all azimuthal directions, significantly enhancing
radiation intensity even when full coherence is imperfect. The harmonics can be
well isolated for a high charge beam. Our full 3D Particle-in-Cell simulations
indicate this scheme can generate X-rays with coherence enhanced by up to two
orders of magnitude compared to incoherent emission.

</details>


### [102] [Compressed Ultrafast Photography of Plasmas Formed from Laser Breakdown of Dense Gases Reveals that Internal Processes Dominate Evolution at Early Times](https://arxiv.org/abs/2507.04608)
*Peng Wang,Yogeshwar Nath Mishra,Seth Pree,Lihong V. Wang,Dag Hanstorp,John P. Koulakis,Daniels Krimans,Seth Putterman*

Main category: physics.plasm-ph

TL;DR: The paper applies compressed ultrafast photography (CUP) to study plasma dynamics in argon and xenon at high pressures, revealing insights into temperature, opacity, and plasma behavior, including unexpected contraction and strong coupling of particles.


<details>
  <summary>Details</summary>
Motivation: To understand plasma dynamics, temperature, opacity, and heat flow in high-pressure gases using advanced imaging techniques.

Method: Enhanced CUP with spatial encoding, temporal shearing, and a new constraint from spatially integrated signals. Measurements through RGB and broad-band filters.

Result: Plasmas in dense gases contract instead of expanding, exhibit strong particle coupling, and show cooling rates unexplained by conventional models.

Conclusion: The isolated plasma system is ideal for studying dense plasma equations of state and hydrodynamics, with findings challenging existing models.

Abstract: Compressed ultrafast photography (CUP) is applied to laser breakdown in argon
and xenon under pressures up to 40atm to obtain 2D images of the plasma
dynamics of single events with a spatial resolution of 250x100 pixels and an
equivalent frame rate of 500 GHz. Light emission as a function of position and
time is measured through red, green, blue, and broad-band filters. The
spatially encoded and temporally sheared image normally used in CUP is now
enhanced by the introduction of a constraint given by a spatially integrated
and temporally sheared unencoded signal. The data yield insights into the
temperature, opacity, the plasma formation process, and heat flow within the
plasma and to the surrounding ambient gas. Contours of constant emission
indicate that plasmas formed from sufficiently dense gas contract rather than
expand despite having a temperature of a few eV. Plasmas formed from relatively
low pressure gases such as 7atm argon can radiate with emissivity near unity.
Modeling transport and opacity as arising from inverse Bremsstrahlung requires
a degree of ionization that strongly exceeds expectations based on Saha's
equation even as customarily modified to include density and screening.
According to this model, both electrons and ions are strongly coupled with a
plasma coefficient >1. During the first few nanoseconds after formation,
Stefan-Boltzmann radiation and thermal conduction to ambient gas are too weak
to explain the observed cooling rates, suggesting that transport within the
plasma dominates its evolution. Yet, thermal conduction within the plasma
itself is also small as indicated by the persistence of thermal inhomogeneities
for far longer timescales. The fact that plasma is isolated from the
surroundings makes it an excellent system for the study of the equation of
state and hydrodynamics of such dense plasmas via the systems and techniques
described.

</details>


### [103] [Hexagonal boron nitride thin film synthesis with a ns-pulsed MHCD: in-situ plasma diagnostics and post-growth film characterization](https://arxiv.org/abs/2507.04759)
*Belkacem Menacer,Dimitrios Stefas,Nikolaos Chazapis,Claudia Lazzaroni,Kristaq Gazeli,Vianney Mille*

Main category: physics.plasm-ph

TL;DR: h-BN is deposited on Si via PECVD using a novel MHCD with AlN dielectric to avoid contamination. Multi-diagnostic tools confirm successful synthesis but reveal film inhomogeneities due to plasma and cooling issues.


<details>
  <summary>Details</summary>
Motivation: To improve h-BN film quality by addressing contamination from atomic oxygen and exploring scalable synthesis methods.

Method: PECVD with MHCD using AlN dielectric, characterized by Raman, SEM, AFM, XPS, OES, and ICCD.

Result: Successful h-BN synthesis with E2g phonon mode at 1366 cm⁻¹, 33 nm thickness, but inhomogeneities in stoichiometry and morphology.

Conclusion: MHCD-driven PECVD shows promise for scalable h-BN synthesis, but further optimization is needed for ideal film properties.

Abstract: Hexagonal boron nitride (h-BN) is deposited on Si <100> wafer ($\approx$20
cm2) via Plasma Enhanced Chemical Vapor Deposition (PECVD) using a ns-pulsed
N2/Ar Micro Hollow Cathode Discharge (MHCD) as a microplasma source. For the
first time, aluminum nitride (AIN) is employed as the dielectric material in
the MHCD to mitigate film contamination by atomic oxygen, an issue previously
observed with conventional Al 2O3 dielectrics. A comprehensive multi-diagnostic
approach is followed to characterize the deposited h-BN, including Raman
spectroscopy, scanning electron microscopy (SEM), atomic force microscopy
(AFM), and X-ray photoelectron microscopy (XPS). In parallel, in-situ
diagnostics such as optical emission spectroscopy (OES) and intensified CCD
(ICCD) imaging are used to monitor plasma properties, including emission
profiles, gas temperature and discharge morphology. Raman spectra reveal the
E2g phonon mode of h-BN around 1366 cm_____, confirming successful synthesis.
SEM imaging reveals an almost complete surface coverage by the film, with
localized delamination. This is probably due to an uneven resistive heating of
the Si wafer, rapid post-deposition cooling ($\sim$13 K/min) and ambient
exposure. AFM analysis indicates an average thickness of about 33 nm after 90
minutes of deposition ($\sim$22 nm/h deposition rate). XPS measurements reveal
an average B/N atomic ratio of $\sim$1.5 along the wafer diameter. Deviations
from ideal film properties (e.g., stoichiometric unity, uniform morphology) are
attributed to plasma-induced inhomogeneities (such as non-uniform species flux
and temperature gradients) among other factors (e.g., ambient exposure
post-deposition), which affect nitrogen and boron incorporation and localized
film properties. Despite these challenges, the MHCD-driven PECVD process
demonstrates strong potential for scalable h-BN synthesis, with further
optimization of the reactor design, plasma conditions, and gas chemistry
required to grow ideal films.

</details>


### [104] [Quantifying Resolution Limits in Pedestal Profile Measurements with Gaussian Process Regression](https://arxiv.org/abs/2507.05067)
*Norman M. Cao,David R. Hatch,Craig Michoski,Todd A. Oliver,David Eldon,Andrew Oakleigh Nelson,Matthew Waller*

Main category: physics.plasm-ph

TL;DR: The paper uses Gaussian Process Regression (GPR) to quantify resolution limits for inferring plasma profiles in fusion pedestals, introducing metrics to balance over-fitting and over-regularization.


<details>
  <summary>Details</summary>
Motivation: Edge transport barriers (pedestals) in fusion plasmas are hard to diagnose due to steep pressure gradients. The work aims to improve profile inference from experimental data.

Method: GPR is applied to infer differentiable profiles, with metrics like an effective cutoff frequency and an information-theoretic measure (N_eff) introduced to evaluate credibility.

Result: The metrics provide a systematic way to assess trade-offs in GPR, demonstrated on DIII-D tokamak data for pedestal profiles.

Conclusion: The tools offer practical guidance for GPR use in fusion research, applicable beyond pedestals to any profile inference problem.

Abstract: Edge transport barriers (ETBs) in magnetically confined fusion plasmas,
commonly known as pedestals, play a crucial role in achieving high confinement
plasmas. However, their defining characteristic, a steep rise in plasma
pressure over short length scales, makes them challenging to diagnose
experimentally. In this work, we use Gaussian Process Regression (GPR) to
develop first-principles metrics for quantifying the spatiotemporal resolution
limits of inferring differentiable profiles of temperature, pressure, or other
quantities from experimental measurements. Although we focus on pedestals, the
methods are fully general and can be applied to any setting involving the
inference of profiles from discrete measurements. First, we establish a
correspondence between GPR and low-pass filtering, giving an explicit
expression for the effective `cutoff frequency' associated with smoothing
incurred by GPR. Second, we introduce a novel information-theoretic metric,
\(N_{eff}\), which measures the effective number of data points contributing to
the inferred value of a profile or its derivative. These metrics enable a
quantitative assessment of the trade-off between `over-fitting' and
`over-regularization', providing both practitioners and consumers of GPR with a
systematic way to evaluate the credibility of inferred profiles. We apply these
tools to develop practical advice for using GPR in both time-independent and
time-dependent settings, and demonstrate their usage on inferring pedestal
profiles using measurements from the DIII-D tokamak.

</details>


### [105] [Dequantized particle algorithm for the nonlinear Vlasov-Poisson system](https://arxiv.org/abs/2507.05151)
*Hong Qin,Michael Q. May,Jacob Molina*

Main category: physics.plasm-ph

TL;DR: A dequantization algorithm for the Vlasov-Poisson system, derived from quantum theory, provides efficient VP system approximation in 3D space.


<details>
  <summary>Details</summary>
Motivation: To develop a compact, efficient representation of the Vlasov-Poisson system by dequantizing quantum theory, avoiding conventional 6D phase space methods.

Method: Derives a finite-dimensional dequantized system from second-quantized Hamiltonian, preserving structure of Schrödinger-Poisson equations via Wigner/Husimi transforms.

Result: Efficiently approximates VP system with 97 particles, demonstrated via classical nonlinear two-stream instability simulation.

Conclusion: The algorithm is efficient, accurate, and conserves properties, serving as a foundation for quantum-inspired classical algorithms in plasma dynamics.

Abstract: We present a dequantization algorithm for the Vlasov--Poisson (VP) system,
termed the dequantized particle algorithm, by systematically dequantizing the
underlying many-body quantum theory. Starting from the second-quantized
Hamiltonian description, we derive a finite-dimensional dequantized system and
show that it furnishes a structure-preserving discretization of the
Schr\"odinger--Poisson (SP) equations. Through the Wigner or Husimi
transformations, this discretization provides an efficient approximation of the
VP system when quantum effects are negligible. Unlike conventional
structure-preserving algorithms formulated in 6D phase space, this dequantized
particle algorithm operates in 3D configuration space, potentially offering
more compact and efficient representations of physical information under
appropriate conditions. A numerical example of the classical nonlinear
two-stream instability, simulated using merely 97 dequantized particles,
demonstrates the efficiency, accuracy, and conservation properties of the
algorithm and confirms its potential as a foundation for developing quantum and
quantum-inspired classical algorithms for kinetic plasma dynamics.

</details>


### [106] [Bootstrap Current Modeling in M3D-C1](https://arxiv.org/abs/2507.05166)
*Saurabh Saxena,Nathaniel Ferraro,Mike F. Martin,Adelle M. Wright*

Main category: physics.plasm-ph

TL;DR: The paper enhances M3D-C1's capability to model bootstrap current in quasisymmetric stellarators and tokamaks using analytical frameworks, validated against neoclassical codes.


<details>
  <summary>Details</summary>
Motivation: Accurate modeling of bootstrap current is crucial for understanding MHD equilibrium and stability in magnetically confined plasmas.

Method: Implemented self-consistent physics models (generalized Sauter and revised Sauter-like) in M3D-C1, benchmarked against neoclassical codes (NEO, XGCa, SFINCS).

Result: Excellent agreement with benchmarks, enabling accurate neoclassical current calculations in axisymmetric and quasisymmetric configurations.

Conclusion: The improvements provide a more accurate plasma behavior representation, with a workflow for evaluating neoclassical transport in arbitrary equilibria.

Abstract: Bootstrap current plays a crucial role in the equilibrium of magnetically
confined plasmas, particularly in quasisymmetric (QS) stellarators and in
tokamaks, where it can represent bulk of the electric current density. Accurate
modeling of this current is essential for understanding the magnetohydrodynamic
(MHD) equilibrium and stability of these configurations. This study expands the
modeling capabilities of M3D-C1, an extended-MHD code, by implementing
self-consistent physics models for bootstrap current. It employs two analytical
frameworks: a generalized Sauter model (Sauter et al. (1999)), and a revised
Sauter-like model (Redl et al. (2021)). The isomorphism described by Landreman
et al. (2022) is employed to apply these models to quasisymmetric stellarators.
The implementation in M3D-C1 is benchmarked against neoclassical codes,
including NEO, XGCa, and SFINCS, showing excellent agreement. These
improvements allow M3D-C1 to self-consistently calculate the neoclassical
contributions to plasma current in axisymmetric and quasisymmetric
configurations, providing a more accurate representation of the plasma behavior
in these configurations. A workflow for evaluating the neoclassical transport
using SFINCS with arbitrary toroidal equilibria calculated using M3D-C1 is also
presented. This workflow enables a quantitative evaluation of the error in the
Sauter-like model in cases that deviate from axi- or quasi-symmetry (e.g.,
through the development of an MHD instability).

</details>


<div id='math.CO'></div>

# math.CO [[Back]](#toc)

### [107] [Spectrahedral relaxations of Eulerian rigidly convex sets](https://arxiv.org/abs/2507.03800)
*Alejandro González Nevado*

Main category: math.CO

TL;DR: The paper generalizes Eulerian polynomials to a multivariate setting, focusing on rigidly convex sets (RCSs) and their spectrahedral approximations. The method uses monic symmetric linear matrix polynomials (MSLMPs) and shows high accuracy, especially along the diagonal, improving bounds for extreme roots of univariate Eulerian polynomials.


<details>
  <summary>Details</summary>
Motivation: To extend Eulerian polynomials to a multivariate context and analyze the accuracy of spectrahedral relaxations for approximating rigidly convex sets defined by these polynomials.

Method: Translate restrictions of hyperbolic and stable polynomials to a real zero setting, then use MSLMPs to construct spectrahedral relaxations for RCSs. Analyze approximations by measuring behavior along the diagonal.

Result: The spectrahedral method provides highly accurate approximations, especially along the diagonal, and improves bounds for extreme roots of univariate Eulerian polynomials.

Conclusion: The relaxation-based spectrahedral method is highly accurate for approximating RCSs defined by multivariate Eulerian polynomials, particularly near the diagonal.

Abstract: We study a generalization of Eulerian polynomials to the multivariate setting
introduced by Br\"and\'en. Although initially these polynomials were introduced
using the language of hyperbolic and stable polynomials, we manage to translate
some restrictions of these polynomials to our real zero setting. Once we are in
this setting, we focus our attention on the rigidly convex sets (RCSs) defined
by these polynomials. In particular, we study the corresponding rigidly convex
sets looking at spectrahedral relaxations constructed through the use of monic
symmetric linear matrix polynomials (MSLMPs) of small size and depending
polynomially (actually just cubically) on the coefficients of the corresponding
polynomials. We analyze how good are the obtained spectrahedral approximations
to these rigidly convex sets. We do this analysis by measuring the behavior
along the diagonal, where we precisely recover the original univariate Eulerian
polynomials. Thus we conclude that, measuring through the diagonal, our
relaxation-based spectrahedral method for approximation of the rigidly convex
sets defined by multivariate Eulerian polynomials is highly accurate. In
particular, we see that this relaxation-based spectrahedral method for
approximation of the rigidly convex sets defined by multivariate Eulerian
polynomials provides bounds for the extreme roots of the corresponding
univariate Eulerian polynomials that are better than these already found in the
literature. All in all, this tells us that, at least close to the diagonal, the
global outer approximation to the rigidly convex sets provided by this
relaxation-based spectrahedral method is itself highly accurate.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [108] [Invariance of quantum scattering rate coefficients to anisotropy of atom-molecule interactions](https://arxiv.org/abs/2507.03801)
*Xuyang Guo,Kirk W. Madison,James L. Booth,Roman V. Krems*

Main category: physics.chem-ph

TL;DR: Quantum scattering calculations for atom-molecule systems can be simplified by ignoring interaction anisotropy, achieving 1% accuracy in rate coefficients.


<details>
  <summary>Details</summary>
Motivation: To reduce computational complexity in quantum scattering calculations for strongly interacting molecular systems.

Method: Numerical solutions of a single differential equation, validated with Gaussian process regression.

Result: Total scattering rate coefficients for Rb-H$_2$ and Rb-N$_2$ are insensitive to anisotropy, enabling simplified calculations.

Conclusion: Ignoring interaction anisotropy simplifies quantum scattering calculations without significant loss of accuracy.

Abstract: Quantum scattering calculations for strongly interacting molecular systems
are computationally demanding due to the large number of molecular states
coupled by the anisotropy of atom - molecule interactions. We demonstrate that
thermal rate coefficients for total (elastic + inelastic) atom - molecule
scattering are insensitive to the interaction anisotropy of the underlying
potential energy surface. In particular, we show that the rate coefficients for
Rb-H$_2$ and Rb-N$_2$ scattering at room temperature can be computed to 1%
accuracy with anisotropy set to zero, reducing the complexity of coupled
channel quantum scattering calculations to numerical solutions of a single
differential equation. Our numerical calculations and statistical analysis
based on Gaussian process regression elucidate the origin and limitations of
the invariance of the total scattering rate coefficients to changes in atom -
molecule interaction anisotropy.

</details>


### [109] [Understanding Reaction Mechanisms from Start to Finish](https://arxiv.org/abs/2507.04052)
*Rik S. Breebaart,Gianmarco Lazzeri,Roberto Covino,Peter G. Bolhuis*

Main category: physics.chem-ph

TL;DR: An iterative path sampling strategy is introduced to compute the committor function for high-dimensional systems, combining transition interface sampling with neural network training for accurate results.


<details>
  <summary>Details</summary>
Motivation: To accurately map transition paths in complex molecular systems (e.g., protein folding, ligand unbinding) by solving the challenge of calculating the committor function for high-dimensional, nonlinear systems.

Method: An iterative approach using initial guesses for isocommittor interfaces, transition interface sampling, path ensemble reweighting, and neural network training to refine the committor model.

Result: The method successfully computes the committor function, demonstrated on a 2D potential and a host-guest unbinding process in explicit solvent.

Conclusion: The approach effectively resolves the circular problem in enhanced sampling, providing a scalable solution for high-dimensional systems and enabling mechanistic insights.

Abstract: Understanding mechanisms of rare but important events in complex molecular
systems, such as protein folding or ligand (un)binding, requires accurately
mapping transition paths from an initial to a final state. The committor is the
ideal reaction coordinate for this purpose, but calculating it for
high-dimensional, nonlinear systems has long been considered intractable. Here,
we introduce an iterative path sampling strategy for computing the committor
function for systems with high free energy barriers. We start with an initial
guess to define isocommittor interfaces for transition interface sampling. The
resulting path ensemble is then reweighted and used to train a neural network,
yielding a more accurate committor model. This process is repeated until
convergence, effectively solving the long-standing circular problem in enhanced
sampling where a good reaction coordinate is needed to generate efficient
sampling, and vice-versa. The final, converged committor model can be
interrogated to extract mechanistic insights. We demonstrate the power of our
method on a benchmark 2D potential and a more complex host-guest (un)binding
process in explicit solvent.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [110] [RAPTOR: Practical Numerical Profiling of Scientific Applications](https://arxiv.org/abs/2507.04647)
*Faveo Hoerold,Ivan R. Ivanov,Akash Dhruv,William S. Moses,Anshu Dubey,Mohamed Wahib,Jens Domke*

Main category: cs.DC

TL;DR: RAPTOR is a numerical profiling tool designed to help scientists identify code regions where reducing precision (e.g., from FP64 to FP16) is feasible, using LLVM for transparent precision adjustments.


<details>
  <summary>Details</summary>
Motivation: The rise of low-precision units in HPC architectures, driven by AI, complicates precision choices for scientists, necessitating tools to evaluate and adapt code precision without trivial fixes.

Method: RAPTOR leverages LLVM to replace high-precision computations with low-precision units or emulate custom precision, offering a user-friendly approach to profile and analyze numerical requirements.

Result: Demonstrated with four real-world multi-physics Flash-X applications, RAPTOR effectively identifies feasible precision reductions.

Conclusion: RAPTOR provides a practical solution for scientists to adapt to modern architectures by enabling precision profiling and optimization.

Abstract: The proliferation of low-precision units in modern high-performance
architectures increasingly burdens domain scientists. Historically, the choice
in HPC was easy: can we get away with 32 bit floating-point operations and
lower bandwidth requirements, or is FP64 necessary? Driven by Artificial
Intelligence, vendors introduced novel low-precision units for vector and
tensor operations, and FP64 capabilities stagnate or are reduced. This is
forcing scientists to re-evaluate their codes, but a trivial search-and-replace
approach to go from FP64 to FP16 will not suffice. We introduce RAPTOR: a
numerical profiling tool to guide scientists in their search for code regions
where precision lowering is feasible. Using LLVM, we transparently replace
high-precision computations using low-precision units, or emulate a
user-defined precision. RAPTOR is a novel, feature-rich approach -- with focus
on ease of use -- to change, profile, and reason about numerical requirements
and instabilities, which we demonstrate with four real-world multi-physics
Flash-X applications.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [111] [Scale Dilation Dynamics in Flexible Bandwidth Needlet Constructions](https://arxiv.org/abs/2507.05075)
*Claudio Durastanti*

Main category: math.ST

TL;DR: The paper explores the impact of dilation sequences in flexible bandwidth needlets on the sphere, analyzing their asymptotic regimes (shrinking, stable, spreading) and their effects on localization, redundancy, and scalability.


<details>
  <summary>Details</summary>
Motivation: To understand how dilation sequences influence the geometry and spectral properties of needlet systems, aiding in their design for analyzing random fields.

Method: Examines asymptotic regimes of dilation sequences (shrinking, stable, spreading) and their effects on needlet properties like localization and spectral coverage.

Result: Characterizes the trade-offs between localization, redundancy, and scalability in needlet systems, particularly for random field analysis.

Conclusion: The study clarifies design trade-offs in needlet systems, emphasizing their asymptotic uncorrelation properties for random fields.

Abstract: Flexible bandwidth needlets offer a versatile multiscale framework for
analyzing functions on the sphere. A key element in their construction is the
dilation sequence, which controls how the multipole consecutive scales are
spaced and overlapped. At any resolution level, this sequence determines the
center positions of the needlet weight functions and influences their
localization in the spatial domain and spectral concentration properties by
means of the relative bandwidth ratio. In this paper, we explore the different
asymptotic regimes that arise when the dilation sequence exhibits shrinking,
stable (standard), or spreading behavior. Moreover, we assume the dilation
sequence grows regularly enough to ensure well-defined asymptotic properties.
For each regime, we characterize the impact on the geometry of the center
scales and the shape of the multipole windows, with particular attention to
their overlap structure and spectral coverage. These insights help to clarify
the trade-offs between localization, redundancy, and scalability in the design
of needlet-type systems, particularly in relation to the study of the
asymptotic uncorrelation of needlet coefficients when applied to random fields.

</details>


<div id='astro-ph.GA'></div>

# astro-ph.GA [[Back]](#toc)

### [112] [A COMPASS to Model Comparison and Simulation-Based Inference in Galactic Chemical Evolution](https://arxiv.org/abs/2507.05060)
*Berkay Gunes,Sven Buder,Tobias Buck*

Main category: astro-ph.GA

TL;DR: COMPASS is a simulation-based inference framework combining diffusion models and transformers for parameter estimation and Bayesian model comparison in Galactic Chemical Evolution models.


<details>
  <summary>Details</summary>
Motivation: To address challenges in analyzing high-dimensional, incomplete, and variable-size stellar abundance datasets and to robustly constrain uncertain physics in astrophysical simulators.

Method: Combines score-based diffusion models with transformer architectures to jointly perform parameter estimation and Bayesian model comparison across competing GCE models.

Result: Strongly favors specific nucleosynthetic yield tables, achieving near-unity cumulative posterior probability, and infers key astrophysical parameters consistent with prior studies.

Conclusion: Modern simulation-based inference methods like COMPASS can robustly constrain uncertain physics in astrophysics and enable principled model selection for complex data.

Abstract: We present \texttt{COMPASS}, a novel simulation-based inference framework
that combines score-based diffusion models with transformer architectures to
jointly perform parameter estimation and Bayesian model comparison across
competing Galactic Chemical Evolution (GCE) models. \texttt{COMPASS} handles
high-dimensional, incomplete, and variable-size stellar abundance datasets. %
Applied to high-precision elemental abundance measurements, \texttt{COMPASS}
evaluates 40 combinations of nucleosynthetic yield tables. The model strongly
favours Asymptotic Giant Branch yields from NuGrid and core-collapse SN yields
used in the IllustrisTNG simulation, achieving near-unity cumulative posterior
probability. Using the preferred model, we infer a steep high-mass IMF slope
and an elevated Supernova\,Ia normalization, consistent with prior solar
neighbourhood studies but now derived from fully amortized Bayesian inference.
% Our results demonstrate that modern SBI methods can robustly constrain
uncertain physics in astrophysical simulators and enable principled model
selection when analysing complex, simulation-based data.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [113] [Modular multiscale approach to modelling high-harmonic generation in gases](https://arxiv.org/abs/2507.04115)
*Jan Vábek,Tadeáš Němec,Stefan Skupin,Fabrice Catoire*

Main category: physics.optics

TL;DR: A modular simulation toolbox for high-harmonic generation in gases, featuring IR-pulse propagation, microscopic response solving, and far-field harmonic calculation, with user-friendly interfaces and tutorials.


<details>
  <summary>Details</summary>
Motivation: To provide a practical and flexible tool for studying high-harmonic generation in gases, catering to both local and HPC environments.

Method: The toolbox includes a computational pipeline for IR-pulse propagation, microscopic response solving (1D-TDSE), and far-field harmonic calculation (diffraction-integral). It offers interfaces and tutorials for ease of use.

Result: A user-oriented simulation toolbox with modular design, stand-alone capabilities, and support for HPC deployment.

Conclusion: The toolbox facilitates efficient and accessible study of high-harmonic generation, with potential for broader applications in research and education.

Abstract: We present a modular user-oriented simulation toolbox for studying
highharmonic generation in gases. The first release consists of the
computational pipeline to 1) compute the unidirectional IR-pulse propagation
incylindrical symmetry, 2) solve the microscopic responses in the whole
macroscopic volume using a 1D-TDSE solver, 3) obtain the far-field harmonic
field using a diffraction-integral approach. The code comes with interfaces and
tutorials, based on practical laboratory conditions, to facilitate the usage
and deployment of the code both locally and in HPC-clusters. Additionally, the
modules are designed to work as stand-alone applications as well, e.g., 1D-TDSE
is available through Pythonic interface.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [114] [Perturbed Toroidal Vortices Display Internal Simply Connected Topology](https://arxiv.org/abs/2507.04596)
*Taosif Ahsan,Samuel A. Cohen,Alan H. Glasser*

Main category: math-ph

TL;DR: Perturbed zero-helicity vortices exhibit simply connected, crescent-shaped interiors, with toroidal and simply connected topologies in fluid, magnetic, and plasma systems.


<details>
  <summary>Details</summary>
Motivation: To explore the topology of perturbed zero-helicity vortices and understand their implications for fluid, magnetic, and plasma systems.

Method: Analytical study of flux surfaces in fluid and magnetic vortices, and numerical examination of particle trajectories in plasma confinement.

Result: Simply connected topology with crescent-shaped boundaries emerges under small perturbations. Field lines remain closed under odd-parity perturbations.

Conclusion: The work reveals new topological features in perturbed vortices, with implications for plasma confinement and fluid dynamics.

Abstract: This work shows that the interiors of perturbed zero-helicity vortices
display simply connected topology with a crescent-shaped boundary. Flux
surfaces in fluid and magnetic vortices were explored analytically, while
particle trajectories in the context of plasma confinement were examined
numerically, demonstrating the existence of both toroidal and simply connected
topologies. This new topology appears for perturbations in a broad class, with
amplitudes and spatial variance allowed to be arbitrarily small. A corollary of
this work proves the closedness of field lines under odd-parity perturbations
of zero-helicity vortices.

</details>


### [115] [On an analogy between the Wiener--Hopf formulations of discrete and continuous diffraction problems](https://arxiv.org/abs/2507.04979)
*A. I. Korolkov,R. C. Assier,A. V. Kisil*

Main category: math-ph

TL;DR: Unifies the framework for deriving Wiener-Hopf equations in discrete and continuous wave diffraction problems using discrete Green's identity and normal derivative.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between discrete and continuous formulations of wave diffraction problems for easier analysis and solution.

Method: Uses discrete Green's identity and a discrete normal derivative to create a formal analogy between discrete and continuous Wiener-Hopf equations.

Result: Demonstrates the analogy's validity through 2D canonical diffraction problems and extends it to 3D problems.

Conclusion: The unified framework simplifies transitioning between discrete and continuous formulations, enhancing problem-solving efficiency.

Abstract: This article is dedicated to unifying the framework used to derive the
Wiener--Hopf equations arising from some discrete and continuous wave
diffraction problems.The main tools are the discrete Green's identity and the
appropriate notion of discrete normal derivative. The resulting formal analogy
between the Wiener--Hopf equations allows one to effortlessly move between the
discrete and continuous formulations. The validity of this novel analogy is
illustrated through several famous two-dimensional canonical diffraction
problems and extended to three-dimensional problems.

</details>


### [116] [Ballistic Transport for Discrete Multi-Dimensional Schrödinger Operators With Decaying Potential](https://arxiv.org/abs/2507.04988)
*David Damanik,Zhiyan Zhao*

Main category: math-ph

TL;DR: The paper proves ballistic transport for the discrete Schrödinger operator with decaying potentials, showing the weighted ℓ²-norm grows at rate tʳ for initial states in the absolutely continuous subspace.


<details>
  <summary>Details</summary>
Motivation: To extend classical results on ballistic transport for the free Laplacian to a broader class of decaying potentials, addressing the behavior of unitary evolution in quantum systems.

Method: Uses commutator methods, refined Mourre estimates, compactness arguments, and localized spectral projections to derive quantitative lower bounds on transport.

Result: The unitary evolution e⁻ⁱᵗᴴ exhibits ballistic transport, with the weighted ℓ²-norm growing as tʳ for suitable initial states.

Conclusion: The study generalizes ballistic transport results to perturbed operators with decaying potentials, leveraging spectral analysis and commutator techniques.

Abstract: We consider the discrete Schr\"odinger operator $H = -\Delta + V$ on
$\ell^2(\mathbb{Z}^d)$ with a decaying potential, in arbitrary lattice
dimension $d\in\mathbb{N}^*$, where $\Delta$ is the standard discrete Laplacian
and $V_n = o(|n|^{-1})$ as $|n| \to \infty$. We prove that the unitary
evolution $e^{-i tH}$ exhibits ballistic transport in the sense that, for any
$r > 0$, the weighted $\ell^2-$norm $$\|e^{-i
tH}u\|_r:=\left(\sum_{n\in\mathbb{Z}^d} (1+|n|^2)^{r} |(e^{-i
tH}u)_n|^2\right)^\frac12 $$ grows at rate $\simeq t^r$ as $t\to \infty$,
provided that the initial state $u$ is in the absolutely continuous subspace
and satisfies $\|u\|_r<\infty$.
  The proof relies on commutator methods and a refined Mourre estimate, which
yields quantitative lower bounds on transport for operators with purely
absolutely continuous spectrum over appropriate spectral intervals. Compactness
arguments and localized spectral projections are used to extend the result to
perturbed operators, extending the classical result for the free Laplacian to a
broader class of decaying potentials.

</details>


<div id='physics.atom-ph'></div>

# physics.atom-ph [[Back]](#toc)

### [117] [Orbital distortion and parabolic channel effects transform minima in molecular ionization probabilities into maxima](https://arxiv.org/abs/2507.04096)
*Imam S. Wahyutama,Denawakage D. Jayasinghe,Francois Mauger,Kenneth Lopata,Kenneth J. Schafer*

Main category: physics.atom-ph

TL;DR: The paper explores how orbital distortion and parabolic channel effects alter molecular ionization rate curves, independent of excited-state contributions, using OE-WFAT(1) for simulations and improving computational efficiency.


<details>
  <summary>Details</summary>
Motivation: To understand deviations in ionization rate curves from the ionized orbital shape at higher field strengths, beyond the commonly cited excited-state effects.

Method: Uses one-electron weak-field asymptotic theory with first-order correction (OE-WFAT(1)) in integral representation, later reformulated into a partial-wave expansion for efficiency.

Result: Demonstrates that orbital distortion and parabolic channel effects can transform local minima into maxima in ionization rates, as shown in CH$_3$Br.

Conclusion: Orbital distortion and parabolic channel effects significantly influence ionization rates, and the reformulated OE-WFAT(1) method enhances computational efficiency.

Abstract: In the tunneling regime and at sufficiently low field amplitudes, the shape
of orientation-dependent molecular ionization rate curves usually resembles the
shape of the ionized orbital. As the ionizing field strength increases, the
shape of the ionization rate can deviate from this pattern. The oft-cited
explanation is that the increasing contribution of excited states relative to
the ground state modifies the distribution. In this paper, we show that orbital
distortion and parabolic channel effects, which are independent of
excited-state effects, can also significantly modify the angular dependence of
the yields of widely studied molecules where excited state effects are
negligible. For example, we find that in CH$_3$Br, the interplay between
orbital distortion and parabolic channel effects transforms a local minimum in
the orientation-dependent ionization rate to a local maximum as the ionizing
field strength increases. To simulate orbital distortion and parabolic channel
effects, we use the one-electron weak-field asymptotic theory including the
first-order correction (OE-WFAT(1)) in the integral representation. Since
OE-WFAT(1) incurs expensive computations when the number of orientation angles
is large, we also reformulate the original OE-WFAT(1) algorithm into a
partial-wave expansion form, which greatly enhances the efficiency of the
method.

</details>


<div id='cond-mat.mes-hall'></div>

# cond-mat.mes-hall [[Back]](#toc)

### [118] [Electrostatics in semiconducting devices II : Solving the Helmholtz equation](https://arxiv.org/abs/2507.03131)
*Antonio Lacerda-Santos,Xavier Waintal*

Main category: cond-mat.mes-hall

TL;DR: The paper introduces a robust method for solving self-consistent quantum-electrostatic problems by mapping them to a Non-Linear Helmholtz (NLH) equation, ensuring provable convergence and fast results.


<details>
  <summary>Details</summary>
Motivation: Addressing the capricious convergence of iterative schemes in mean field problems, especially under strong non-linearities like electron gas depletion or large magnetic fields.

Method: Mapping the problem to a NLH equation, constructing a convex functional for provable convergence, and iteratively updating to reach the exact solution.

Result: Empirical evidence shows convergence in one or two iterations, providing a robust and fast solution for quantum nanoelectronic devices.

Conclusion: The proposed algorithms offer a precise, efficient, and reliable approach for studying electrostatics in quantum nanoelectronics.

Abstract: The convergence of iterative schemes to achieve self-consistency in mean
field problems such as the Schr\"odinger-Poisson equation is notoriously
capricious. It is particularly difficult in regimes where the non-linearities
are strong such as when an electron gas in partially depleted or in presence of
a large magnetic field. Here, we address this problem by mapping the
self-consistent quantum-electrostatic problem onto a Non-Linear Helmoltz (NLH)
equation at the cost of a small error. The NLH equation is a generalization of
the Thomas-Fermi approximation. We show that one can build iterative schemes
that are provably convergent by constructing a convex functional whose minimum
is the seeked solution of the NLH problem. In a second step, the approximation
is lifted and the exact solution of the initial problem found by iteratively
updating the NLH problem until convergence. We show empirically that
convergence is achieved in a handfull, typically one or two, iterations. Our
set of algorithms provide a robust, precise and fast scheme for studying the
effect of electrostatics in quantum nanoelectronic devices.

</details>


### [119] [Quantum transport in nitrogen-doped nanoporous graphenes](https://arxiv.org/abs/2507.04892)
*Gaetano Calogero,Isaac Alcón,Alan E. Anaya Morales,Nick Papior,Pol Febrer,Aron W. Cummings,Miguel Pruneda,Stephan Roche,Mads Brandbyge*

Main category: cond-mat.mes-hall

TL;DR: The paper studies the quantum transport properties of hybrid nanoporous graphenes (hNPGs) using Green's functions simulations, revealing lateral carrier spreading and proposing designs for controlled transport.


<details>
  <summary>Details</summary>
Motivation: Understanding the transport properties of hNPGs is crucial for applications like nanoelectronics and biosensing, but little is known about them.

Method: Green's functions simulations are used to analyze quantum transport in hNPGs, including carrier spreading and confinement.

Result: Carriers spread laterally through specific GNR types, and a proposed design enables directed electric signals with sub-nanometer precision over a micrometer.

Conclusion: The study provides insights into hNPG transport and offers design strategies for controlled charge transport, with potential applications in nanocircuitry.

Abstract: Bottom-up on-surface synthesized nanoporous graphenes (NPGs), realized as 2D
arrays of laterally covalently bonded pi-conjugated graphene nanoribbons
(GNRs), are a family of carbon nanomaterials which are receiving increasing
attention for nanoelectronics and biosensing. Recently, a so-called hybrid-NPG
(hNPG) was synthesized, featuring an alternating sequence of doped and
non-doped GNRs, resulting in a band staggering effect in its electronic
structure. Such a feature is appealing for photo-catalysis, photovoltaics and
even carbon nanocircuitry. However, to date, little is known about the
transport properties of hNPG and its derivatives, which is key for most
applications. Here, via Green's functions simulations, we study the quantum
transport properties of hNPGs. We find that injected carriers in hNPG spread
laterally through a number of GNRs, though such spreading may take place
exclusively through GNRs of one type (doped or non-doped). We propose a simple
model to discern the key parameters determining the electronic propagation in
hNPGs and explore alternative hNPG designs to control the spreading/confinement
and anisotropy of charge transport in these systems. For one such design, we
find that it is possible to send directed electric signals with sub-nanometer
precision for as long as one micrometer - a result first reported for any NPG.

</details>


<div id='math.CA'></div>

# math.CA [[Back]](#toc)

### [120] [Carnot-Carathéodory Balls on Manifolds with Boundary](https://arxiv.org/abs/2507.03501)
*Brian Street*

Main category: math.CA

TL;DR: Generalization of Carnot-Carathéodory ball theory to manifolds with boundary, introducing scaling maps for non-characteristic boundaries.


<details>
  <summary>Details</summary>
Motivation: Extend the quantitative study of Carnot-Carathéodory balls and Hörman-der vector fields to manifolds with boundary, enabling analysis of maximally subelliptic boundary value problems.

Method: Introduce scaling maps adapted to Carnot-Carathéodory balls and Hörman-der vector fields, applicable to both interior and non-characteristic boundary regions.

Result: First step in a series to study maximally subelliptic boundary value problems, laying groundwork for future research.

Conclusion: This work extends prior theory to manifolds with boundary, providing tools for analyzing subelliptic PDEs in broader contexts.

Abstract: Nagel, Stein, and Wainger introduced a detailed quantitative study of
Carnot--Carath\'eodory balls on a smooth manifold without boundary. Most
importantly, they introduced scaling maps adapted to Carnot--Carath\'eodory
balls and H\"ormander vector fields. Their work was extended by many authors
and has since become a key tool in the study of the interior theory of
subelliptic PDEs; in particular, the study of maximally subelliptic PDEs. We
introduce a generalization of this quantitative theory to manifolds with
boundary, where we have scaling maps both on the interior and on the part of
the boundary which is non-characteristic with respect to the vector fields.
This is the first paper in a forthcoming series devoted to studying maximally
subelliptic boundary value problems.

</details>


### [121] [One sided orthogonal polynomials and a pointwise convergence result for $SU(2)$-valued nonlinear Fourier series](https://arxiv.org/abs/2507.05124)
*Michel Alexis,Gevorg Mnatsakanyan,Christoph Thiele*

Main category: math.CA

TL;DR: The paper connects $SU(2)$-valued nonlinear Fourier series with orthogonal polynomials for complex measures on the unit circle, proving convergence results and relating polynomial behavior to zeros and local parameters.


<details>
  <summary>Details</summary>
Motivation: To explore the relationship between $SU(2)$-valued nonlinear Fourier series and orthogonal polynomials, aiming to simplify and generalize existing results in the $SU(2)$ case compared to $SU(1,1)$.

Method: The study involves analyzing reproducing kernels, pointwise convergence of polynomial products, and behavior of zeros and local parameters. It also examines lacunary sequences for convergence proofs.

Result: The paper shows universality-type convergence for reproducing kernels and proves almost everywhere convergence under specific conditions on the nonlinear Fourier series.

Conclusion: The $SU(2)$ case yields simpler results than $SU(1,1)$, with convergence established under boundedness and outer function conditions.

Abstract: We elaborate on a connection between the $SU(2)$-valued nonlinear Fourier
series and sequences of left and right orthogonal polynomials for complex
measures on the unit circle. We show a convergence result for the associated
reproducing kernel. This is a universality type result in the vein of
Mate-Nevai-Totik, which turns out to be much simpler in the $SU(2)$ case than
in the $SU(1,1)$ case. We then relate a.e. pointwise convergence of the product
of left and right polynomials and their squares with both behavior of their
zeros as well as behavior of some local parameters for these polynomials. We
conclude by proving almost everywhere convergence along lacunary sequences of
the functional $(a_n ^* +b_n)(a_n - b_n ^*)$ of the partial $SU(2)$-valued
nonlinear Fourier series $(a_n, b_n)$ under the assumption that the nonlinear
Fourier series $(a,b)$ itself satisfies both $\|b\|_{L^{\infty} (\mathbb{T})} <
2^{- \frac 1 2}$ and $a^*$ is outer.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [122] [Model selection for stochastic dynamics: a parsimonious and principled approach](https://arxiv.org/abs/2507.04121)
*Andonis Gerardos*

Main category: stat.ML

TL;DR: PASTIS is a new information criterion for selecting the simplest correct SDE/SPDE models from noisy data, outperforming AIC, BIC, CV, and SINDy. It includes robust variants for handling large sampling intervals and measurement noise.


<details>
  <summary>Details</summary>
Motivation: Standard information criteria (AIC, BIC) are limited in selecting the simplest correct model from large candidate libraries for SDEs/SPDEs.

Method: Introduces PASTIS, derived from extreme value theory, with a penalty term incorporating library size, model parameters, and significance threshold. Robust variants (PASTIS-Δt, PASTIS-σ) address noisy data.

Result: PASTIS outperforms AIC, BIC, CV, and SINDy in model identification and predictive capability, validated on benchmark systems.

Conclusion: PASTIS offers a statistically grounded, practical framework for discovering simple stochastic models, even with imperfect data.

Abstract: This thesis focuses on the discovery of stochastic differential equations
(SDEs) and stochastic partial differential equations (SPDEs) from noisy and
discrete time series. A major challenge is selecting the simplest possible
correct model from vast libraries of candidate models, where standard
information criteria (AIC, BIC) are often limited.
  We introduce PASTIS (Parsimonious Stochastic Inference), a new information
criterion derived from extreme value theory. Its penalty term, $n_\mathcal{B}
\ln(n_0/p)$, explicitly incorporates the size of the initial library of
candidate parameters ($n_0$), the number of parameters in the considered model
($n_\mathcal{B}$), and a significance threshold ($p$). This significance
threshold represents the probability of selecting a model containing more
parameters than necessary when comparing many models.
  Benchmarks on various systems (Lorenz, Ornstein-Uhlenbeck, Lotka-Volterra for
SDEs; Gray-Scott for SPDEs) demonstrate that PASTIS outperforms AIC, BIC,
cross-validation (CV), and SINDy (a competing method) in terms of exact model
identification and predictive capability.
  Furthermore, real-world data can be subject to large sampling intervals
($\Delta t$) or measurement noise ($\sigma$), which can impair model learning
and selection capabilities. To address this, we have developed robust variants
of PASTIS, PASTIS-$\Delta t$ and PASTIS-$\sigma$, thus extending the
applicability of the approach to imperfect experimental data.
  PASTIS thus provides a statistically grounded, validated, and practical
methodological framework for discovering simple models for processes with
stochastic dynamics.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [123] [Relaxation and stability analysis of a third-order multiclass traffic flow model](https://arxiv.org/abs/2507.04135)
*Stephan Gerster,Giuseppe Visconti*

Main category: math.OC

TL;DR: A third-order hyperbolic traffic model is proposed, introducing dynamic hesitation and relaxation terms to address hysteresis effects in driver behavior.


<details>
  <summary>Details</summary>
Motivation: The challenge in macroscopic traffic modeling lies in closure relations, especially specifying hesitation in second-order models like Aw-Rascle-Zhang.

Method: Starting from a microscopic formulation, the model relaxes standard assumptions by evolving hesitation dynamically and introducing relaxation terms.

Result: The model incorporates hysteresis effects, showing drivers respond differently to acceleration/deceleration under identical conditions, and connects to existing models like Aw-Rascle-Zhang.

Conclusion: The proposed third-order model enhances traffic flow modeling by dynamically addressing hesitation and linking to established frameworks.

Abstract: Traffic flow modeling spans a wide range of mathematical approaches, from
microscopic descriptions of individual vehicle dynamics to macroscopic models
based on aggregate quantities. A fundamental challenge in macroscopic modeling
lies in the closure relations, particularly in the specification of a traffic
hesitation function in second-order models like Aw-Rascle-Zhang. In this work,
we propose a third-order hyperbolic traffic model in which the hesitation
evolves as a driver-dependent dynamic quantity. Starting from a microscopic
formulation, we relax the standard assumption by introducing an evolution law
for the hesitation. This extension allows to incorporate hysteresis effects,
modeling the fact that drivers respond differently when accelerating or
decelerating, even under identical local traffic conditions. Furthermore,
various relaxation terms are introduced. These allow us to establish relations
to the Aw-Rascle-Zhang model and other traffic flow models.

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [124] [New Molecular Dynamics Methods for Simulating Neutron Star Crusts with Superfluid Vortices](https://arxiv.org/abs/2507.04527)
*M. E. Caplan,N. T. Smith*

Main category: astro-ph.HE

TL;DR: Study of vortex-crust interaction in neutron stars using molecular dynamics reveals lattice entrainment when pinning and Coulomb forces are comparable.


<details>
  <summary>Details</summary>
Motivation: Understanding the unpinning of superfluid vortices during spin glitches in neutron stars.

Method: New molecular dynamics approach to analyze crust response to a rigid vortex.

Result: Observation of lattice entrainment when vortex pinning and Coulomb forces are similar, impacting crust elasticity.

Conclusion: Findings suggest implications for the elastic evolution of neutron star crusts.

Abstract: Superfluid vortices in neutron star crusts are thought to be pinned to the
lattice of nuclei in the crust. The unpinning of superfluid vortices in spin
glitches therefore motivates us to study the vortex-crust interaction
explicitly with molecular dynamics. In this work, we present a new molecular
dynamics methods to characterize the response of the crust to a rigid vortex.
When vortex pinning forces and nearest neighbor Coulomb forces are comparable,
we observe a qualitatively new phenomena of lattice entrainment with
implications for the elastic evolution of the crust.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [125] [Divergence-Kernel method for scores of random systems](https://arxiv.org/abs/2507.04035)
*Angxiu Ni*

Main category: math.PR

TL;DR: A divergence-kernel formula for scores in random dynamical systems is derived, applicable to multiplicative noise without requiring hyperbolicity. Special cases and a Monte-Carlo algorithm are presented.


<details>
  <summary>Details</summary>
Motivation: To provide a general formula for scores in random dynamical systems, including multiplicative noise, and to enable practical computation via a Monte-Carlo method.

Method: Derive a divergence-kernel formula, extend it to continuous-time SDEs, and develop a pathwise Monte-Carlo algorithm for scores.

Result: A versatile formula for scores, validated on the 40-dimensional Lorenz 96 system with multiplicative noise.

Conclusion: The derived formula and algorithm offer a robust and general approach for computing scores in random dynamical systems.

Abstract: We derive the divergence-kernel formula for the scores of random dynamical
systems, then formally pass to the continuous-time limit of SDEs. Our formula
works for multiplicative noise systems over any period of time; it does not
require hyperbolicity. We also consider several special cases: (1) for additive
noise, we give a pure kernel formula; (2) for short-time, we give a pure
divergence formula; (3) we give a formula which does not involve scores of the
initial distribution. Based on the new formula, we derive a pathwise
Monte-Carlo algorithm for scores, and demonstrate it on the 40-dimensional
Lorenz 96 system with multiplicative noise.

</details>


### [126] [A Discretization Scheme for BSDEs with Random Time Horizon](https://arxiv.org/abs/2507.04882)
*Frank T. Seifried,Maximilian Würschmidt*

Main category: math.PR

TL;DR: The paper analyzes the backward Euler approximation for BSDEs with Lipschitz generators and random time horizons, deriving strong error bounds. It applies these to decoupled FBSDEs on bounded domains.


<details>
  <summary>Details</summary>
Motivation: To extend and refine error bounds for numerical approximations of BSDEs with random time horizons, improving accuracy in practical applications like FBSDEs.

Method: Uses backward Euler approximation for BSDEs, derives error bounds, and applies Euler-Maruyama scheme for diffusion approximation.

Result: Strong error bounds are derived, scaled by exponential of maximal terminal time, and refined for FBSDEs on bounded domains.

Conclusion: The method provides accurate error bounds for BSDEs with random horizons, applicable to FBSDEs, enhancing numerical approximation reliability.

Abstract: We analyze a natural extension of the backward Euler approximation for a
class of BSDEs with Lipschitz generators and random (unbounded) time horizons.
We derive strong error bounds in terms of the underlying stepsize; the distance
between the continuous terminal time and a discrete-time approximation; the
distance between the terminal condition and a respective approximation; and an
integrated distance depending on an approximation of the time component of the
generator - all are scaled by the exponential of the maximal terminal time. As
application we consider decoupled FBSDEs on bounded domains. We use an
Euler-Maruyama scheme to approximate the diffusion and further refine our error
bounds to only depend on the distance of the exit times.

</details>


### [127] [Approximation of the Lévy-driven stochastic heat equation on the sphere](https://arxiv.org/abs/2507.05005)
*Annika Lang,Andrea Papini,Verena Schwarz*

Main category: math.PR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The stochastic heat equation on the sphere driven by additive L\'evy random
field is approximated by a spectral method in space and forward and backward
Euler-Maruyama schemes in time, in analogy to the Wiener case. New regularity
results are proven for the stochastic heat equation. The spectral approximation
is based on a truncation of the series expansion with respect to the spherical
harmonic functions. To do so, we restrict to square-integrable random field and
optimal strong convergence rates for a given regularity of the initial
condition and two different settings of regularity for the driving noise are
derived for the Euler-Maruyama methods. Besides strong convergence, convergence
of the expectation and second moment is shown. Weak rates for the spectral
approximation are discussed. Numerical simulations confirm the theoretical
results.

</details>


<div id='math.SG'></div>

# math.SG [[Back]](#toc)

### [128] [Hilbert manifold structures on path spaces](https://arxiv.org/abs/2507.03782)
*Urs Frauenfelder,Joa Weber*

Main category: math.SG

TL;DR: The paper explores whether path spaces on two-level manifolds in Floer theory can form Hilbert manifolds, introducing 'tameness' as a key structure to achieve this.


<details>
  <summary>Details</summary>
Motivation: To generalize Floer homology construction by addressing the challenge of defining exponential maps on two-level manifolds.

Method: Introduces 'tame maps' and 'tame two-level manifolds,' proving their properties to define charts on path spaces.

Result: Path spaces on tame two-level manifolds are shown to have a Hilbert manifold structure.

Conclusion: The concept of tameness enables a viable framework for constructing Floer homology in a generalized setting.

Abstract: In Floer theory one has to deal with two-level manifolds like for instance
the space of $W^{2,2}$ loops and the space of $W^{1,2}$ loops. Gradient flow
lines in Floer theory are then trajectories in a two-level manifold. Inspired
by our endeavor to find a general setup to construct Floer homology we
therefore address in this paper the question if the space of paths on a
two-level manifold has itself the structure of a Hilbert manifold. In view of
the two topologies on a two-level manifold it is unclear how to define the
exponential map on a general two-level manifold. We therefore study a different
approach how to define charts on path spaces of two-level manifolds. To make
this approach work we need an additional structure on a two-level manifold
which we refer to as tameness. We introduce the notion of tame maps and show
that the composition of tame is tame again. Therefore it makes sense to
introduce the notion of a tame two-level manifold. The main result of this
paper shows that the path spaces on tame two-level manifolds have the structure
of a Hilbert manifold.

</details>


<div id='q-bio.PE'></div>

# q-bio.PE [[Back]](#toc)

### [129] [Noise Reinstates Collapsed Populations: Stochastic Reversal of Deterministic Extinction](https://arxiv.org/abs/2507.03954)
*Vinesh Vijayan,B Priyadharshini,R Sathish Kumar,G Janaki*

Main category: q-bio.PE

TL;DR: Stochastic noise can prevent population collapse by reversing deterministic tipping points, offering a stabilizing effect in complex systems.


<details>
  <summary>Details</summary>
Motivation: To challenge the conventional view that environmental noise drives populations toward extinction by demonstrating its potential to prevent collapse.

Method: A hybrid model combining logistic growth with density-triggered sigmoidal collapse, analyzing phase space topology under noise.

Result: Noise disrupts deterministic extinction trajectories, enabling back-transitions to viable states and inducing metastability.

Conclusion: Natural fluctuations can stabilize systems, suggesting a re-evaluation of strategies to leverage stochasticity in preventing collapse.

Abstract: Conventional wisdom suggests that environmental noise drives populations
toward extinction. In contrast, we report a paradoxical phenomenon in which
stochasticity reverses a deterministic tipping point, thereby preventing
collapse. Using a hybrid model that integrates logistic growth with a
density-triggered sigmoidal collapse, we uncover a striking reversal:
deterministic fragility on one side, and stochastic rescue under weak noise on
the other. Our analysis demonstrates that noise disrupts the convergence of
deterministic trajectories toward extinction by altering the phase space
topology, enabling back-transitions to viable states. This mechanism gives rise
to noise-induced metastability and reveals a form of stochastic robustness not
captured by deterministic models. These findings suggest that natural
fluctuations can serve as a stabilizing force in complex systems, offering a
compelling counter-narrative to classical models in ecology, epidemiology, and
beyond. We advocate for a re-evaluation of stabilization strategies,
emphasizing the constructive role of stochasticity in averting population
collapse.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [130] [Kappa distributions in the language of superstatistics](https://arxiv.org/abs/2507.03757)
*Sergio Davis,Biswajit Bora,Cristian Pavez,Leopoldo Soto*

Main category: cond-mat.stat-mech

TL;DR: The paper reviews the derivation of kappa distributions in collisionless plasmas using superstatistics, an alternative to Tsallis' non-extensive statistics, and explores its implications for correlations, temperature, and entropy.


<details>
  <summary>Details</summary>
Motivation: To provide an alternative framework (superstatistics) for understanding kappa distributions in collisionless plasmas, addressing limitations of non-extensive statistics.

Method: Review and derivation of multi-particle and single-particle kappa distributions within the superstatistics framework, including computation of expectation values.

Result: Demonstrates the utility of superstatistics for kappa distributions and discusses its implications for correlations, temperature, and entropy in plasmas.

Conclusion: Superstatistics offers a viable alternative to non-extensive statistics for analyzing kappa distributions, with broader implications for plasma physics.

Abstract: The kappa distribution of velocities appears routinely in the study of
collisionless plasmas present in Earth's magnetosphere, the solar wind among
other contexts where particles are unable to reach thermal equilibrium.
Originally justified through the use of Tsallis' non-extensive statistics,
nowadays there are alternative frameworks that provide insight into these
distributions, such as superstatistics. In this work we review the derivation
of the multi-particle and single-particle kappa distributions for collisionless
plasmas within the framework of superstatistics, as an alternative to the use
of non-extensive statistics. We also show the utility of the superstatistical
framework in the computation of expectation values under kappa distributions.
Some consequences of the superstatistical formalism regarding correlations,
temperature and entropy of kappa-distributed plasmas are also discussed.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [131] [AFLOW4: heading toward disorder](https://arxiv.org/abs/2507.03422)
*Simon Divilov,Hagen Eckert,Scott D. Thiel,Sean D. Griesemer,Rico Friedrich,Nicholas H. Anderson,Michael J. Mehl,David Hicks,Marco Esters,Nico Hotz,Xiomara Campilongo,Arrigo Calzolari,Stefano Curtarolo*

Main category: cond-mat.mtrl-sci

TL;DR: AFLOW4 is an upgraded toolkit for studying high-entropy disordered materials, featuring new modules, faster calculations, and improved usability.


<details>
  <summary>Details</summary>
Motivation: To enhance the AFLOW toolkit for better research on high-entropy materials by introducing innovative features and improving existing ones.

Method: Includes the Soliquidy module for Euclidean transport cost analysis, dielectric function calculations, and improved features like prototype identification and convex hull calculations.

Result: AFLOW4 offers faster and more accurate analysis of high-entropy materials, with human-readable data export for easier integration into workflows.

Conclusion: AFLOW4 advances research capabilities for high-entropy materials with its new and improved features.

Abstract: AFLOW4 is the latest iteration of the AFLOW toolkit, specifically tailored to
study high-entropy disordered materials. This upgrade includes innovative
features like the Soliquidy module, based on the Euclidean transport cost
between disordered and ordered material states. AFLOW4 can calculate dielectric
functions to understand optical and electronic properties of disordered
ceramics. The newly introduced human-readable data export feature ensures the
uncomplicated incorporation of AFLOW4 in diverse automated workflows. Features
relevant to high-entropy research, like prototype identification, partial
occupation method, convex hull calculation, and enthalpy corrections based on
local atomic environments, have been improved and exhibit substantial speed-up.
Together, these enhancements represent a step forward for AFLOW as a valuable
tool for research of high-entropy materials.

</details>


### [132] [The occupation dependent DFT-1/2 method](https://arxiv.org/abs/2507.04804)
*Shengxin Yang,Jiangzhen Shi,Kan-Hao Xue,Jun-Hui Yuan,Xiangshui Miao*

Main category: cond-mat.mtrl-sci

TL;DR: The paper introduces an occupation-dependent DFT-1/2 method to address band gap under-estimation in DFT without increasing computational load, improving accuracy for challenging semiconductors.


<details>
  <summary>Details</summary>
Motivation: Rectify band gap under-estimation in DFT while maintaining computational efficiency, overcoming limitations of existing methods like DFT-1/2 and shell DFT-1/2.

Method: Proposes occupation-dependent DFT-1/2, avoiding self-energy potential disturbance in conduction bands, and introduces shell DFT+$A$-1/2 with downscaled atomic self-energy potential.

Result: The method works for difficult cases like Li$_2$O$_2$, Cu$_2$O, and 2D semiconductors, yielding accurate band edges for monolayer MoS$_2$ compared to hybrid functional approaches.

Conclusion: The occupation-dependent DFT-1/2 method effectively corrects band gaps for complex semiconductors while maintaining computational efficiency.

Abstract: There has been a high demand in rectifying the band gap under-estimation
problem in density functional theory (DFT), while keeping the computational
load at the same level as local density approximation. DFT-1/2 and shell
DFT-1/2 are useful attempts, as they correct the spurious electron
self-interaction through the application of self-energy potentials, which pull
down the valence band. Nevertheless, the self-energy potential inevitably
disturbs the conduction band, and these two methods fail for semiconductors
whose hole and electron are entangled in the same shell-like regions. In this
work, we introduce the occupation-dependent DFT-1/2 method, where conduction
band states are not subject to the additional self-energy potential
disturbance. This methodology works for difficult cases such as
$\text{Li}_2\text{O}_2$, $\text{Cu}_2\text{O}$ and two-dimensional
semiconductors. Using a shell-like region for the self-energy potential, and
allowing for downscaling of the atomic self-energy potential (with an $A$ < 1
factor), the occupation-dependent shell DFT+$A$-1/2 method yields more accurate
conduction band and valence band edge levels for monolayer $\text{MoS}_2$,
compared with the computationally demanding hybrid functional approach.

</details>


### [133] [Atomistic study of dislocation formation during Ge epitaxy on Si](https://arxiv.org/abs/2507.05082)
*Luis Martín-Encinar,Luis A. Marqués,Iván Santos,Lourdes Pelaz*

Main category: cond-mat.mtrl-sci

TL;DR: Classical MD simulations study dislocation formation in Ge/Si epitaxial growth, revealing stress release via amorphous regions and dislocation half-loops influenced by surface morphology.


<details>
  <summary>Details</summary>
Motivation: To understand dislocation formation during Ge epitaxial growth on Si from an atomistic perspective.

Method: Classical molecular dynamics simulations at 900-1000 K with deposition rates of 10^8 monolayers per second.

Result: Dislocations form in low-density amorphous regions under surface valleys, releasing stress and smoothing the film. Dislocation half-loops propagate, influenced by surface morphology.

Conclusion: Simulations accurately describe film features and reveal the role of surface morphology in dislocation propagation.

Abstract: We performed classical molecular dynamics simulations to investigate, from an
atomistic point of view, the formation of dislocations during the epitaxial
growth of Ge on Si. We show that simulations at 900 and 1000 K with deposition
rates of 10$^8$ monolayers per second provide a good compromise between
computational cost and accuracy. In these conditions, the ratio between the Ge
deposition rate and the ad-atom jump rate is analogous to that of
out-of-equilibrium experiments. In addition, the main features of the grown
film (intermixing, critical film thickness, dislocation typology, and surface
morphology) are well described. Our simulations reveal that dislocations
originate in low-density amorphous regions that form under valleys of the rough
Ge film surface. Atoms are squeezed out of these regions to the surface,
releasing the stress accumulated in the film and smoothing its surface.
Amorphous regions grow until atoms begin to rearrange in dislocation half-loops
that propagate throughout the Ge film. The threading arm ends of the
dislocation half-loops move along the surface following valleys and avoiding
islands. The film surface morphology affects the propagation path of the
dislocation half-loops and the resulting dislocation network.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [134] [Physics Augmented Machine Learning Discovery of Composition-Dependent Constitutive Laws for 3D Printed Digital Materials](https://arxiv.org/abs/2507.02991)
*Steven Yang,Michal Levin,Govinda Anantha Padmanabha,Miriam Borshevsky,Ohad Cohen,D. Thomas Seidl,Reese E. Jones,Nikolaos Bouklas,Noy Cohen*

Main category: cs.LG

TL;DR: The paper explores the mechanical behavior of multi-material 3D printed digital materials, combining experimental and computational methods to model their nonlinear, rate-dependent responses using a physics-augmented neural network.


<details>
  <summary>Details</summary>
Motivation: To understand and predict the composition-dependent mechanical properties of digital materials fabricated via multi-material 3D printing, enabling tunable material design.

Method: Experimental characterization of five material formulations under tension and torsion, combined with a physics-augmented neural network (PANN) model integrating a pICNN for hyperelasticity and QLV for viscoelasticity.

Result: The PANN model accurately predicts nonlinear, rate-dependent behavior for interpolated compositions, demonstrating high predictive accuracy.

Conclusion: The proposed framework offers a scalable solution for automated constitutive modeling in multi-material 3D printing, enhancing material design capabilities.

Abstract: Multi-material 3D printing, particularly through polymer jetting, enables the
fabrication of digital materials by mixing distinct photopolymers at the micron
scale within a single build to create a composite with tunable mechanical
properties. This work presents an integrated experimental and computational
investigation into the composition-dependent mechanical behavior of 3D printed
digital materials. We experimentally characterize five formulations, combining
soft and rigid UV-cured polymers under uniaxial tension and torsion across
three strain and twist rates. The results reveal nonlinear and rate-dependent
responses that strongly depend on composition. To model this behavior, we
develop a physics-augmented neural network (PANN) that combines a partially
input convex neural network (pICNN) for learning the composition-dependent
hyperelastic strain energy function with a quasi-linear viscoelastic (QLV)
formulation for time-dependent response. The pICNN ensures convexity with
respect to strain invariants while allowing non-convex dependence on
composition. To enhance interpretability, we apply $L_0$ sparsification. For
the time-dependent response, we introduce a multilayer perceptron (MLP) to
predict viscoelastic relaxation parameters from composition. The proposed model
accurately captures the nonlinear, rate-dependent behavior of 3D printed
digital materials in both uniaxial tension and torsion, achieving high
predictive accuracy for interpolated material compositions. This approach
provides a scalable framework for automated, composition-aware constitutive
model discovery for multi-material 3D printing.

</details>


### [135] [Regulation Compliant AI for Fusion: Real-Time Image Analysis-Based Control of Divertor Detachment in Tokamaks](https://arxiv.org/abs/2507.02897)
*Nathaniel Chen,Cheolsik Byun,Azarakash Jalalvand,Sangkyeun Kim,Andrew Rothstein,Filippo Scotti,Steve Allen,David Eldon,Keith Erickson,Egemen Kolemen*

Main category: cs.LG

TL;DR: AI-based linear and interpretable control system for divertor detachment in fusion reactors, achieving 2% accuracy.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of regulatory compliance for AI in fusion control by developing an interpretable system.

Method: Implemented a real-time AI-enabled linear control system using DIII-D lower divertor camera and D2 gas for feedback control.

Result: Achieved a mean absolute difference of 2% from the target for divertor detachment and reattachment.

Conclusion: The framework is extendable to other image-based diagnostics, ensuring regulatory compliance for future fusion reactors.

Abstract: While artificial intelligence (AI) has been promising for fusion control, its
inherent black-box nature will make compliant implementation in regulatory
environments a challenge. This study implements and validates a real-time AI
enabled linear and interpretable control system for successful divertor
detachment control with the DIII-D lower divertor camera. Using D2 gas, we
demonstrate feedback divertor detachment control with a mean absolute
difference of 2% from the target for both detachment and reattachment. This
automatic training and linear processing framework can be extended to any image
based diagnostic for regulatory compliant controller necessary for future
fusion reactors.

</details>


### [136] [Enhanced accuracy through ensembling of randomly initialized auto-regressive models for time-dependent PDEs](https://arxiv.org/abs/2507.03863)
*Ishan Khurjekar,Indrashish Saha,Lori Graham-Brady,Somdatta Goswami*

Main category: cs.LG

TL;DR: A deep ensemble framework is proposed to reduce error accumulation in ML surrogates for PDE-driven systems, improving long-term prediction accuracy.


<details>
  <summary>Details</summary>
Motivation: Autoregressive ML models for PDE systems suffer from error accumulation, limiting their long-term accuracy.

Method: Multiple ML surrogate models with random initializations are trained in parallel and aggregated during inference.

Result: The framework reduces error accumulation in three PDE systems, enabling faster and accurate predictions.

Conclusion: Ensemble methods offer robust, efficient alternatives to traditional numerical solvers for PDE systems.

Abstract: Systems governed by partial differential equations (PDEs) require
computationally intensive numerical solvers to predict spatiotemporal field
evolution. While machine learning (ML) surrogates offer faster solutions,
autoregressive inference with ML models suffer from error accumulation over
successive predictions, limiting their long-term accuracy. We propose a deep
ensemble framework to address this challenge, where multiple ML surrogate
models with random weight initializations are trained in parallel and
aggregated during inference. This approach leverages the diversity of model
predictions to mitigate error propagation while retaining the autoregressive
strategies ability to capture the system's time dependent relations. We
validate the framework on three PDE-driven dynamical systems - stress evolution
in heterogeneous microstructures, Gray-Scott reaction-diffusion, and
planetary-scale shallow water system - demonstrating consistent reduction in
error accumulation over time compared to individual models. Critically, the
method requires only a few time steps as input, enabling full trajectory
predictions with inference times significantly faster than numerical solvers.
Our results highlight the robustness of ensemble methods in diverse physical
systems and their potential as efficient and accurate alternatives to
traditional solvers. The codes for this work are available on GitHub
(https://github.com/Graham-Brady-Research-Group/AutoregressiveEnsemble_SpatioTemporal_Evolution).

</details>


### [137] [Neural-Network solver of ideal MHD equilibria](https://arxiv.org/abs/2507.03119)
*Timo Thun,Andrea Merlo,Rory Conlin,Dario Panici,Daniel Böckenhoff*

Main category: cs.LG

TL;DR: A novel method using neural networks to compute 3D Magnetohydrodynamic equilibria achieves competitive or better results than conventional solvers.


<details>
  <summary>Details</summary>
Motivation: To improve the computation of 3D Magnetohydrodynamic equilibria by leveraging neural networks for potentially better performance and broader applicability.

Method: Parametrize Fourier modes with neural networks and minimize the global force residual using first-order optimizers.

Result: Neural networks achieve competitive or lower residuals than existing codes, with potential for further improvements.

Conclusion: Neural networks show promise for solving equilibria and modeling continuous distributions, offering a new computational approach.

Abstract: We present a novel approach to compute three-dimensional Magnetohydrodynamic
equilibria by parametrizing Fourier modes with artificial neural networks and
compare it to equilibria computed by conventional solvers. The full nonlinear
global force residual across the volume in real space is then minimized with
first order optimizers. Already,we observe competitive computational cost to
arrive at the same minimum residuals computed by existing codes. With increased
computational cost,lower minima of the residual are achieved by the neural
networks,establishing a new lower bound for the force residual. We use
minimally complex neural networks,and we expect significant improvements for
solving not only single equilibria with neural networks,but also for computing
neural network models valid over continuous distributions of equilibria.

</details>


### [138] [An explicit formulation of the learned noise predictor $ε_θ({\bf x}_t, t)$ via the forward-process noise $ε_{t}$ in denoising diffusion probabilistic models (DDPMs)](https://arxiv.org/abs/2507.04203)
*KiHyun Yun*

Main category: cs.LG

TL;DR: The paper derives an explicit formulation of the learned noise predictor in DDPMs, linking it to forward-process noise, and provides a rigorous proof of a fundamental equality in diffusion models.


<details>
  <summary>Details</summary>
Motivation: To clarify the relationship between the learned noise predictor and forward-process noise in DDPMs, and to provide a deeper theoretical understanding of diffusion models.

Method: Derives an explicit formulation of the noise predictor in terms of forward-process noise and presents a rigorous proof of a key equality.

Result: The paper shows how forward-process noise contributes to the learned predictor and offers new theoretical insights into diffusion models.

Conclusion: The findings enhance the theoretical foundation of diffusion models, clarifying the origin of a fundamental equality and its implications.

Abstract: In denoising diffusion probabilistic models (DDPMs), the learned noise
predictor $ \epsilon_{\theta} ( {\bf x}_t , t)$ is trained to approximate the
forward-process noise $\epsilon_t$. The equality $\nabla_{{\bf x}_t} \log
q({\bf x}_t) = -\frac 1 {\sqrt {1- {\bar \alpha}_t} } \epsilon_{\theta} ( {\bf
x}_t , t)$ plays a fundamental role in both theoretical analyses and
algorithmic design, and thus is frequently employed across diffusion-based
generative models. In this paper, an explicit formulation of $
\epsilon_{\theta} ( {\bf x}_t , t)$ in terms of the forward-process noise
$\epsilon_t$ is derived. This result show how the forward-process noise
$\epsilon_t$ contributes to the learned predictor $ \epsilon_{\theta} ( {\bf
x}_t , t)$. Furthermore, based on this formulation, we present a novel and
mathematically rigorous proof of the fundamental equality above, clarifying its
origin and providing new theoretical insight into the structure of diffusion
models.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [139] [Closed-Form Robustness Bounds for Second-Order Pruning of Neural Controller Policies](https://arxiv.org/abs/2507.02953)
*Maksym Shamrai*

Main category: cs.RO

TL;DR: The paper analyzes the robustness of second-order pruning methods in deep neural policies for control systems, providing closed-form bounds to ensure stability and safety post-pruning.


<details>
  <summary>Details</summary>
Motivation: Deep neural policies face memory and real-time constraints on embedded microcontrollers, and the impact of pruning on control stability and safety is unclear.

Method: The study uses a mathematical framework to analyze pruning effects on nonlinear discrete-time control systems, deriving closed-form bounds for control errors.

Result: The derived bounds allow pre-deployment assessment of pruning impacts, linking network compression with performance guarantees.

Conclusion: The work bridges deep-learning tools and robustness needs in safety-critical autonomous systems.

Abstract: Deep neural policies have unlocked agile flight for quadcopters, adaptive
grasping for manipulators, and reliable navigation for ground robots, yet their
millions of weights conflict with the tight memory and real-time constraints of
embedded microcontrollers. Second-order pruning methods, such as Optimal Brain
Damage (OBD) and its variants, including Optimal Brain Surgeon (OBS) and the
recent SparseGPT, compress networks in a single pass by leveraging the local
Hessian, achieving far higher sparsity than magnitude thresholding. Despite
their success in vision and language, the consequences of such weight removal
on closed-loop stability, tracking accuracy, and safety have remained unclear.
We present the first mathematically rigorous robustness analysis of
second-order pruning in nonlinear discrete-time control. The system evolves
under a continuous transition map, while the controller is an $L$-layer
multilayer perceptron with ReLU-type activations that are globally 1-Lipschitz.
Pruning the weight matrix of layer $k$ replaces $W_k$ with $W_k+\delta W_k$,
producing the perturbed parameter vector $\widehat{\Theta}=\Theta+\delta\Theta$
and the pruned policy $\pi(\cdot;\widehat{\Theta})$. For every input state
$s\in X$ we derive the closed-form inequality $
\|\pi(s;\Theta)-\pi(s;\widehat{\Theta})\|_2 \le C_k(s)\,\|\delta W_k\|_2, $
where the constant $C_k(s)$ depends only on unpruned spectral norms and biases,
and can be evaluated in closed form from a single forward pass. The derived
bounds specify, prior to field deployment, the maximal admissible pruning
magnitude compatible with a prescribed control-error threshold. By linking
second-order network compression with closed-loop performance guarantees, our
work narrows a crucial gap between modern deep-learning tooling and the
robustness demands of safety-critical autonomous systems.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [140] [On a parabolic curvature lower bound generalizing Ricci flows](https://arxiv.org/abs/2507.05032)
*Marco Flaim,Erik Hupp*

Main category: math.DG

TL;DR: The paper explores the limitations of extending Perelman's monotonicity results to super Ricci flows and introduces a condition by Buzano that preserves these properties. It provides characterizations using optimal transport and connects them to Ricci nonnegativity in space-time.


<details>
  <summary>Details</summary>
Motivation: To understand why Perelman's monotonicity functionals fail for super Ricci flows and to find a generalized condition that retains these properties, linking it to Ricci nonnegativity and optimal transport.

Method: The study uses optimal transport techniques, Bochner inequalities, gradient estimates, Wasserstein contraction, and Evolutionary Variational Inequalities (EVIs) to characterize Buzano's condition.

Result: The paper proves equivalences between Buzano's condition and various analytical properties, including Bochner inequalities and gradient estimates, some of which are new even for general Ricci flows.

Conclusion: The findings suggest a unified approach to analyzing Ricci nonnegative manifolds and Ricci flows, with potential extensions to more singular settings due to the minimal reliance on tensor calculus or Jacobi fields.

Abstract: Optimal transport plays a major role in the study of manifolds with Ricci
curvature bounded below. Some results in this setting have been extended to
super Ricci flows, revealing a unified approach to analysis on Ricci
nonnegative manifolds and Ricci flows. However we observe that the monotonicity
of Perelman's functionals ($\mathcal{F}$, $\mathcal{W}$, reduced volume), which
hold true for Ricci flows and Ricci nonnegative manifolds, cannot be strictly
generalized to super Ricci flows. In 2010 Buzano introduced a condition which
still generalizes Ricci flows and Ricci nonnegative manifolds, and on which
Perelman's monotonicities do hold. We provide characterizations of this
condition using optimal transport and understand it heuristically as Ricci
nonnegativity of the space-time. This interpretation is consistent with its
equivalence to Ricci nonnegativity on Perelman's infinite dimensional manifold.
  More precisely, we prove that for smooth evolutions of Riemannian manifolds,
this condition is equivalent to a Bochner inequality (resembling Perelman's
Harnack inequality but for the forward heat flow), a gradient estimate for the
heat flow, a Wasserstein contraction along the adjoint heat flow, the convexity
of a modified entropy along Wasserstein geodesics, and an Evolutionary
Variational Inequality (EVI). The optimal transport statements use Perelman's
$L$ distance as cost, as first studied on Ricci flows by Topping and by Lott.
We also consider dimensionally improved and weighted versions of these
conditions. The dimensional Bochner inequality and all gradient estimates for
the forward heat equation, along with the EVIs, appear to be new even for
general Ricci flows, and are related to the Hamiltonian perspective on the $L$
distance. Most of our proofs do not use tensor calculus or Jacobi fields,
suggesting the possibility of future extensions to more singular settings.

</details>


### [141] [Biharmonic Steklov problems with Neumann boundary conditions and spectral inequalities on differential forms](https://arxiv.org/abs/2507.05049)
*Rodolphe Abou Assali*

Main category: math.DG

TL;DR: The paper introduces three biharmonic Steklov problems on differential forms with Neumann boundary conditions, proves their ellipticity, and analyzes their discrete spectra and eigenvalue estimates.


<details>
  <summary>Details</summary>
Motivation: To extend the understanding of biharmonic Steklov problems to differential forms with Neumann boundary conditions and establish their spectral properties.

Method: Introduces three biharmonic Steklov problems, proves their ellipticity, derives discrete spectra, and provides variational characterizations for eigenvalues. Also establishes Kuttler-Sigillito inequalities.

Result: Existence of discrete spectra for the problems and eigenvalue estimates linking them to standard Steklov, Dirichlet, Neumann, and biharmonic Steklov problems with Dirichlet conditions.

Conclusion: The study successfully extends eigenvalue analysis to biharmonic Steklov problems on differential forms with Neumann boundary conditions, providing foundational results and inequalities.

Abstract: We introduce three biharmonic Steklov problems on differential forms with
Neumann boundary conditions and show that they are elliptic. We prove the
existence of a discrete spectrum for each of those problems and give associated
variational characterizations for their eigenvalues. We establish eigenvalue
estimates known as Kuttler-Sigillito inequalities, relating the eigenvalues of
these problems with the eigenvalues of the Steklov, Dirichlet and Neumann
standard problems as well as the biharmonic Steklov problem with Dirichlet
boundary conditions on differential forms.

</details>


<div id='math.SP'></div>

# math.SP [[Back]](#toc)

### [142] [Pólya's conjecture up to $ε$-loss and quantitative estimates for the remainder of Weyl's law](https://arxiv.org/abs/2507.04307)
*Renjin Jiang,Fanghua Lin*

Main category: math.SP

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Let $\Omega\subset\mathbb{R}^n$ be a bounded Lipschitz domain. For any
$\epsilon\in (0,1)$ we show that for any Dirichlet eigenvalue
$\lambda_k(\Omega)>\Lambda(\epsilon,\Omega)$, it holds \begin{align*} k&\le
(1+\epsilon)\frac{|\Omega|\omega(n)}{(2\pi)^n}\lambda_k(\Omega)^{n/2},
\end{align*} where $\Lambda(\epsilon,\Omega)$ is given explicitly. This reduces
the $\epsilon$-loss version of P\'olya's conjecture to a computational problem.
This estimate is based on a uniform estimate on the remainder of the Weyl law
on Lipschitz domains, which appears to be the first quantitative estimate for
the remainder of Weyl's law since Weyl's seminal work in the year 1911. We also
provide in all dimensions $n\ge 2$, a class of domains that may even have
rather irregular shapes or boundaries but satisfy P\'olya's conjecture.

</details>


### [143] [On the nodal domain count under metric perturbations, Courant sharpness and boundary intersections](https://arxiv.org/abs/2507.04928)
*Mayukh Mukherjee,Saikat Maji,Soumyajit Saha*

Main category: math.SP

TL;DR: The paper explores how metric perturbations on closed surfaces affect nodal domain counts, showing local maxima for non-generic metrics and applications in Courant sharp metrics.


<details>
  <summary>Details</summary>
Motivation: To understand the impact of metric changes on nodal domains of Laplace eigenfunctions and explore applications in boundary nodal-data problems.

Method: Investigates metric perturbations, focusing on localized changes and their effects on nodal domain counts.

Result: Nodal domain count is locally maximized for non-generic metrics, with applications to Courant sharp metrics.

Conclusion: Metric perturbations significantly influence nodal domains, with non-generic metrics playing a key role in maximizing counts.

Abstract: In this note, we investigate the effects of metric perturbations of closed
surfaces on the number of nodal domains. In particular, we show that the nodal
domain count for Laplace eigenfunctions is (locally) maximised for non-generic
metrics. We also investigate localised perturbations and showcase some
interesting applications regarding Courant sharp metrics on closed surfaces and
associated boundary nodal-data prescription problems.

</details>


<div id='astro-ph.CO'></div>

# astro-ph.CO [[Back]](#toc)

### [144] [On the interpretation of XRISM X-ray measurements of turbulence in the intracluster medium: a comparison with cosmological simulations](https://arxiv.org/abs/2507.04727)
*F. Vazza,G. Brunetti*

Main category: astro-ph.CO

TL;DR: The study reconciles XRISM observations of turbulence in the Coma cluster with simulations, showing compatibility without needing steeper turbulence spectra.


<details>
  <summary>Details</summary>
Motivation: To address the apparent tension between XRISM observations of turbulence in the Coma cluster and classical fluid simulations.

Method: High-resolution simulation of a Coma-like cluster, analyzing turbulence properties and comparing to XRISM data.

Result: Simulated Kolmogorov-like turbulence matches XRISM observations, resolving the tension.

Conclusion: Current observations don't require steeper turbulence spectra; biases in observations and simulations explain discrepancies.

Abstract: We investigate whether the properties of turbulent gas motions recently
measured via X-ray spectroscopy in the Coma cluster of galaxies by XRISM are in
tension with the "classical" fluid picture of the intracluster medium on large
scales, as produced by a typical high-resolution cosmological simulation. We
use a high-resolution simulation of Coma-like cluster of galaxies and show that
the Kolmogorov-like turbulence measured in the simulation yields to velocity
structure functions and line-width that fully compatible with those measured by
the XRISM observation of Coma. These results highlight the combined biases
driven by the inhomogeneity of turbulence in the intracluster medium and by the
X-ray weighting in observations, and appear to release the tension between the
XRISM data and current numerical simulations, showing that a turbulent spectrum
much steeper than Kolmogorov is not required by current observational data.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [145] [Efficient Quantum Access Model for Sparse Structured Matrices using Linear Combination of Things](https://arxiv.org/abs/2507.03714)
*Abeynaya Gnanasekaran,Amit Surana*

Main category: quant-ph

TL;DR: A novel LCU decomposition method for structured sparse matrices using an alternate basis (sigma basis) reduces terms logarithmically with matrix size, enabling efficient quantum circuits for VQAs and fault-tolerant algorithms.


<details>
  <summary>Details</summary>
Motivation: Structured sparse matrices from PDEs are common in science and engineering. Conventional LCU decompositions using Pauli basis can scale poorly (quadratically).

Method: Proposes sigma basis for LCU decomposition, leveraging sparsity and structure. Uses unitary completion for quantum circuits and block encoding for fault-tolerant algorithms.

Result: Achieves logarithmic scaling of terms with matrix size, demonstrated numerically on PDE examples.

Conclusion: The sigma basis offers a more efficient LCU decomposition, enhancing quantum algorithms for PDEs and other applications.

Abstract: We develop a novel approach for Linear combination of unitaries (LCU) type
decomposition for structured sparse matrices. Such matrices frequently arise
during numerical solution of partial differential equations which are
ubiquitous in science and engineering. LCU is a versatile quantum algorithmic
primitive that plays an important role in context of both variational quantum
algorithms (VQA) and fully fault-tolerant ones, and has been applied to a
diverse range of problems. Conventionally, Pauli basis is used for LCU
decomposition, which however in worst case can result in number of LCU terms
that scale quadratically with respect to the matrix size. We show that by using
an alternate basis one can better exploit the sparsity and underlying structure
of matrix leading to number of tensor product terms which scale only
logarithmically with respect to the matrix size. We develop numerical and
semi-analytical approaches for computing sigma basis decomposition for an
arbitrary matrix. Given this new basis is comprised of non-unitary operators,
we employ the concept of unitary completion to design efficient quantum
circuits for evaluation of the expectation values of operators composed of
tensor product of elements from sigma basis which can be used for cost function
evaluation in VQAs. We also develop an approach for block encoding of arbitrary
operator given its decomposition in sigma basis which could be used in variety
of fully fault-tolerant algorithms. We compare our approach with other related
concepts in the literature including unitary dilation and provide numerical
illustrations on several PDE examples.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [146] [Super-resolution of turbulent velocity fields in two-way coupled particle-laden flows](https://arxiv.org/abs/2507.03567)
*Ali Shamooni,Ruyue Cheng,Thorsten Zirwes,Hesam Tofighian,Oliver T. Stein,Andreas Kronenburg*

Main category: physics.flu-dyn

TL;DR: A deep learning framework using cGANs for super-resolution reconstruction of velocity fields in particle-laden turbulent flows, validated with DNS data.


<details>
  <summary>Details</summary>
Motivation: Accurate reconstruction of high-resolution velocity fields in complex particle-laden turbulent flows is challenging, requiring advanced methods.

Method: Uses cGANs with explicit conditioning on physical parameters (e.g., particle mass density) and leverages DNS datasets for training/testing.

Result: The model accurately reconstructs high-frequency details modulated by particles and generalizes well across unseen flow regimes.

Conclusion: The framework demonstrates strong predictive capabilities and generalization for super-resolution in particle-laden flows.

Abstract: This paper introduces a deep learning-based super-resolution (SR) framework
specifically developed for accurately reconstructing high-resolution velocity
fields in two-way coupled particle-laden turbulent flows. Leveraging
conditional generative adversarial networks (cGANs), the generator network
architecture incorporates explicit conditioning on physical parameters, such as
effective particle mass density and subgrid kinetic energy, while the
discriminator network is conditioned on low-resolution data as well as
high-frequency content of the input data. High-fidelity direct numerical
simulation (DNS) datasets, covering a range of particle Stokes numbers,
particle mass loadings, and carrier gas turbulence regimes, including forced-
and decaying-turbulence, serve as training and testing datasets. Extensive
validation studies, including detailed analyses of energy spectra, probability
density functions (PDFs), vorticity distributions, and wavelet-based
decomposition demonstrate the model's accuracy and generalization capabilities
across different particle parameters. The results show that the network
utilizes particle data, mainly in the reconstruction of high-frequency details
modulated by particles. Additionally, systematic assessment of the model's
performance in capturing previously unseen flow regimes further validates its
predictive capabilities.

</details>


### [147] [Burnett-level multi-relaxation-time central-moment discrete Boltzmann modeling of reactive flows](https://arxiv.org/abs/2507.03877)
*Qingbin Wu,Chuandong Lin,Huilin Lai*

Main category: physics.flu-dyn

TL;DR: A multi-relaxation-time central-moment discrete Boltzmann method (CDBM) is developed for compressible reactive flows, validated through simulations of various scenarios.


<details>
  <summary>Details</summary>
Motivation: To address complex reactive flows with hydrodynamic and thermodynamic nonequilibrium effects, applicable in research and engineering.

Method: Develops a CDBM framework with a unified Boltzmann equation, matrix inversion for collision/reaction terms, and a 2D 25-velocity model.

Result: Validated via simulations (thermal Couette flow, chemical reactions, detonation waves), showing recovery of Burnett equations with tunable parameters.

Conclusion: The CDBM is a versatile tool for simulating complex reactive flows with nonequilibrium effects.

Abstract: A multi-relaxation-time central-moment discrete Boltzmann method (CDBM) is
developed for compressible reactive flows, incorporating the effects of
chemical reactions. The Chapman--Enskog multiscale analysis demonstrates that
the model recovers the Burnett equations in the hydrodynamic limit, with
tunable specific heat ratios and Prandtl numbers. Within the CDBM framework, a
unified Boltzmann equation governs the evolution of hydrodynamic variables,
thermodynamic quantities, and higher-order central moments. The collision and
reaction term are consistently computed via matrix inversion method. A
two-dimensional twenty-five discrete velocities, exhibiting favorable spatial
symmetry, is constructed and employed. The model is validated through
simulations of the thermal Couette flow, homogeneous chemical reaction, steady
detonation wave, and collision of two detonation waves. This work presents a
versatile numerical simulation tool capable of addressing complex reactive
flows characterized by hydrodynamic and thermodynamic nonequilibrium effects,
applicable to both scientific research and engineering practice.

</details>


### [148] [A novel parallelizable convergence accelerating method: Pointwise Frequency Damping](https://arxiv.org/abs/2507.04873)
*Zikun Liu,Xukun Wang,Yilang Liu,Weiwei Zhang*

Main category: physics.flu-dyn

TL;DR: A novel data-driven method accelerates steady-state flow solvers by predicting asymptotic limits for each parameter using historical data, ensuring identical serial/parallel results and achieving speedups of 2.5-4x.


<details>
  <summary>Details</summary>
Motivation: To improve efficiency in steady-state flow field solvers by leveraging historical parameter data for faster convergence without compromising accuracy.

Method: Predicts asymptotic limits for parameters using historical data, derives a discretized algebraic formula, and ensures parallel compatibility.

Result: Achieves speedup factors of 2.5-4x in tests, from simple inviscid flows to complex 3D transonic flows, with minimal overhead.

Conclusion: The method is highly effective for large-scale industrial mesh computations due to its efficiency, parallel compatibility, and low overhead.

Abstract: This paper proposes a novel class of data-driven acceleration methods for
steady-state flow field solvers. The core innovation lies in predicting and
assigning the asymptotic limit value for each parameter during iterations based
on its own historical data, rather than processing and assigning the entire
flow field at once. This approach fundamentally guarantees identical results
between serial and parallel computations. Subsequently, a formula for
representing the asymptotic limit based on historical data is derived and
discretized, yielding a purely algebraic expression.Furthermore, the
applicability scope of the method is discussed, along with the underlying
reasons for its acceleration capability. A quantitative expression for
estimating the speedup ratio is also provided. Extensive validation cases were
tested, ranging from the simplest inviscid airfoil flow to complex
three-dimensional viscous transonic cruise flow around a aircraft, and solving
asymmetric linear systems via GMRES. These tests consistently demonstrate
significant acceleration effects with speedup factors ranging from 2.5 to 4.
Combined with the near-zero computational overhead of the purely algebraic
formulation during the solving process and the inherently parallel-compatible
pointwise prediction principle, the results strongly indicate that this method
is highly suitable for large-scale industrial mesh computations.

</details>


### [149] [Accuracy and stability of the accelerated multi-direct-forcing immersed boundary method](https://arxiv.org/abs/2507.04986)
*Kosuke Suzuki,Emmanouil Falagkaris,Timm Krüger,Takaji Inamuro*

Main category: physics.flu-dyn

TL;DR: The study analyzes the numerical stability and accuracy of the accelerated multidirect-forcing immersed boundary method, identifying a critical parameter for stability and an optimal acceleration parameter for minimizing no-slip condition errors.


<details>
  <summary>Details</summary>
Motivation: To address numerical instability in the multi-direct-forcing immersed boundary method and improve accuracy in moving boundary problems.

Method: Analysis of discretized equations of body motion and numerical simulations to identify stability parameters and optimal acceleration.

Result: Identified a critical stability parameter and an optimal acceleration parameter independent of boundary details, shape, or dimensionality.

Conclusion: Provides guidelines for stable simulations of moving boundary problems with optimal no-slip condition accuracy.

Abstract: The multi-direct-forcing immersed boundary method allows for high accuracy of
the no-slip condition in moving-particle problems but suffers from numerical
instability if simulation parameters are not carefully chosen. This study
investigates the numerical accuracy and stability of the accelerated
multidirect-forcing immersed boundary method. An analysis of the discretized
equations of body motion in moving boundary problems identifies a critical
parameter that solely determines the numerical stability for the body motion.
Additionally, numerical simulations reveal the optimal acceleration parameter
that minimizes the error in the no-slip condition and is independent of details
of the boundary discretisation, the boundary shape, and spatial dimensionality.
This study provides a guideline for establishing numerically stable simulations
of moving boundary problems at optimal accuracy of the no-slip condition.

</details>


<div id='cs.MS'></div>

# cs.MS [[Back]](#toc)

### [150] [Memory- and compute-optimized geometric multigrid GMGPolar for curvilinear coordinate representations -- Applications to fusion plasma](https://arxiv.org/abs/2507.03812)
*Julian Litz,Philippe Leleux,Carola Kruse,Joscha Gedicke,Martin J. Kühn*

Main category: cs.MS

TL;DR: The paper presents a refactored, object-oriented version of GMGPolar, a multigrid solver for the 2D gyrokinetic Poisson equation, introducing two matrix-free implementations with improved memory efficiency and speed.


<details>
  <summary>Details</summary>
Motivation: Tokamak fusion reactors require efficient numerical experiments due to high costs and time constraints of physical experiments. GMGPolar's optimization is crucial for plasma physics understanding and reactor design.

Method: The paper refactors GMGPolar into an object-oriented version with two matrix-free implementations, using the Sherman-Morrison formula for cyclic tridiagonal systems and reordering for cache optimization.

Result: The Give approach reduces memory and achieves 4-7x speedups, while the Take approach attains 16-18x speedups for typical test cases.

Conclusion: The refactored GMGPolar significantly improves efficiency and speed, supporting tokamak reactor design and plasma physics research.

Abstract: Tokamak fusion reactors are actively studied as a means of realizing energy
production from plasma fusion. However, due to the substantial cost and time
required to construct fusion reactors and run physical experiments, numerical
experiments are indispensable for understanding plasma physics inside tokamaks,
supporting the design and engineering phase, and optimizing future reactor
designs. Geometric multigrid methods are optimal solvers for many problems that
arise from the discretization of partial differential equations. It has been
shown that the multigrid solver GMGPolar solves the 2D gyrokinetic Poisson
equation in linear complexity and with only small memory requirements compared
to other state-of-the-art solvers. In this paper, we present a completely
refactored and object-oriented version of GMGPolar which offers two different
matrix-free implementations. Among other things, we leverage the
Sherman-Morrison formula to solve cyclic tridiagonal systems from circular line
solvers without additional fill-in and we apply reordering to optimize cache
access of circular and radial smoothing operations. With the Give approach,
memory requirements are further reduced and speedups of four to seven are
obtained for usual test cases. For the Take approach, speedups of 16 to 18 can
be attained.

</details>


<div id='math.FA'></div>

# math.FA [[Back]](#toc)

### [151] [Kernels of trace operators via fine continuity](https://arxiv.org/abs/2507.04536)
*Michael Hinz,Simon N. Chandler-Wilde,David P. Hewett*

Main category: math.FA

TL;DR: The paper analyzes traces of fractional Sobolev spaces on closed subsets of ℝⁿ, showing quasi-continuous functions vanish quasi-everywhere if they vanish μ-almost everywhere. It characterizes the trace operator's kernel and extends results to domains with measure density conditions.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of traces of fractional Sobolev spaces on closed subsets and generalize results without requiring doubling conditions on measures.

Method: Uses localized upper density conditions on measures to analyze quasi-continuous representatives and characterize the trace operator's kernel.

Result: Quasi-continuous functions vanish quasi-everywhere iff they vanish μ-almost everywhere. The trace operator's kernel is the closure of smooth functions outside Γ.

Conclusion: The findings generalize trace characterizations for fractional Sobolev spaces, applicable even for sets with varying Hausdorff dimensions.

Abstract: We study traces of elements of fractional Sobolev spaces
$H_p^\alpha(\mathbb{R}^n)$ on closed subsets $\Gamma$ of $\mathbb{R}^n$, given
as the supports of suitable measures $\mu$. We prove that if these measures
satisfy localized upper density conditions, then quasi continuous
representatives vanish quasi everywhere on $\Gamma$ if and only if they vanish
$\mu$-almost everywhere on $\Gamma$. We use this result to characterize the
kernel of the trace operator mapping from $H_p^\alpha(\mathbb{R}^n)$ into the
space of $\mu$-equivalence classes of functions on $\Gamma$ as the closure of
$C_c^\infty(\mathbb{R}^n\setminus \Gamma)$ in $H_p^\alpha(\mathbb{R}^n)$. The
measures do not have to satisfy a doubling condition. In particular, the set
$\Gamma$ may be a finite union of closed sets having different Hausdorff
dimensions. We provide corresponding results for fractional Sobolev spaces
$H_p^\alpha(\Omega)$ on domains $\Omega\subset \mathbb{R}^n$ satisfying the
measure density condition.

</details>


<div id='astro-ph.EP'></div>

# astro-ph.EP [[Back]](#toc)

### [152] [Optimization of Transfers linking Ballistic Captures to Earth-Moon Periodic Orbit Families](https://arxiv.org/abs/2507.04739)
*Lorenzo Anoè,Roberto Armellin,Jack Yarndley,Thomas Caleb,Stéphanie Lizy-Destrez*

Main category: astro-ph.EP

TL;DR: A unified framework optimizes bi-impulsive transfers to periodic orbits in the Earth-Moon system using high-order polynomial expansions of CR3BP dynamics, validated for lunar missions.


<details>
  <summary>Details</summary>
Motivation: Addressing the need for efficient transfers to periodic orbits for NASA's Artemis and CNSA's Chang'e programs.

Method: Develops a high-order polynomial expansion of CR3BP dynamics for optimizing bi-impulsive transfers and parameterizes orbit families for rapid targeting.

Result: Demonstrates low-cost solutions for transfers to planar and spatial periodic orbits, including NRHOs.

Conclusion: The method provides reliable initial guesses for mission-specific refinements and enriches low-energy transfer databases.

Abstract: The design of transfers to periodic orbits in the Earth--Moon system has
regained prominence with NASA's Artemis and CNSA's Chang'e programs. This work
addresses the problem of linking ballistic capture trajectories - exploiting
multi-body dynamics for temporary lunar orbit insertion - with bounded periodic
motion described in the circular restricted three-body problem (CR3BP). A
unified framework is developed for optimizing bi-impulsive transfers to
families of periodic orbits via a high-order polynomial expansion of the CR3BP
dynamics. That same expansion underlies a continuous 'abacus' parameterization
of orbit families, enabling rapid targeting and analytic sensitivity. Transfers
to planar periodic-orbit families (Lyapunov L1 and L2, and distant retrograde
orbits) are addressed first, followed by extension to spatial families, such as
butterfly and halo L1/L2 orbits, with an emphasis towards Near-Rectilinear Halo
Orbits (NRHOs). Numerical results demonstrate low-cost solutions and validate
the method's adaptability for the design of lunar missions. The optimized
trajectories can inform an established low-energy transfer database, enriching
it with detailed cost profiles that reflect both transfer feasibility and
underlying dynamical relationships to specific periodic-orbit families.
Finally, the proposed transfers provide reliable initial guesses for rapid
refinement, readily adaptable for further optimization across mission-specific
needs.

</details>


### [153] [On the convergence of N-body simulations of the Solar System](https://arxiv.org/abs/2507.04987)
*Hanno Rein,Garett Brown,Mei Kanda*

Main category: astro-ph.EP

TL;DR: The paper investigates the minimum timestep required for accurate long-term simulations of the Solar System, finding that timesteps up to 32 days yield reliable results.


<details>
  <summary>Details</summary>
Motivation: No systematic study exists on timestep convergence for long-term planetary simulations, raising concerns about the reliability of existing results.

Method: Numerical experiments are conducted to test timestep effects on Solar System simulations over 5 Gyr, focusing on secular frequencies and instability rates.

Result: Timesteps up to 32 days (a third of Mercury's orbital period) produce physical results, with chaotic diffusion outweighing numerical errors.

Conclusion: The study confirms the reliability of most existing simulations and provides guidelines for efficient and trustworthy long-term integrations.

Abstract: Most direct N-body integrations of planetary systems use a symplectic
integrator with a fixed timestep. A large timestep is desirable because
simulations run fast. However, simulations yield unphysical results if the
timestep is too large. Surprisingly, no systematic convergence study has been
performed on long (Gyr) timescales. In this paper we present numerical
experiments to determine the minimum timestep one has to use in long-term
integrations of the Solar System in order to recover the system's fundamental
secular frequencies and instability rate. We find that timesteps of up to 32
days, i.e. a third of Mercury's orbital period, yield physical results in 5 Gyr
integrations. We argue that the chaotic diffusion that drives the Solar
System's long-term evolution dominates over numerical diffusion and timestep
resonances. Our results bolster confidence that most simulations in the
literature are indeed converged and provide guidance on how to run time and
energy efficient simulations while making sure results can be trusted.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [154] [A Novel Hybrid Grey Wolf Differential Evolution Algorithm](https://arxiv.org/abs/2507.03022)
*Ioannis D. Bougas,Pavlos Doanis,Maria S. Papadopoulou,Achilles D. Boursianis,Sotirios P. Sotiroudis,Zaharias D. Zaharis,George Koudouridis,Panagiotis Sarigiannidis,Mohammad Abdul Matint,George Karagiannidis,Sotirios K. Goudos*

Main category: cs.NE

TL;DR: A hybrid algorithm (GWO-DE) combining Grey Wolf Optimizer (GWO) and Differential Evolution (DE) variants is introduced and tested on benchmark functions, showing good performance and solution quality.


<details>
  <summary>Details</summary>
Motivation: To leverage the strengths of GWO (nature-inspired swarm intelligence) and DE (evolutionary computation) for improved global optimization.

Method: Hybridization of GWO with two DE variants, forming the GWO-DE algorithm, tested on numerical benchmark functions.

Result: Comparative study shows satisfactory performance and solution quality.

Conclusion: The GWO-DE algorithm is effective for global optimization, combining the strengths of GWO and DE.

Abstract: Grey wolf optimizer (GWO) is a nature-inspired stochastic meta-heuristic of
the swarm intelligence field that mimics the hunting behavior of grey wolves.
Differential evolution (DE) is a popular stochastic algorithm of the
evolutionary computation field that is well suited for global optimization. In
this part, we introduce a new algorithm based on the hybridization of GWO and
two DE variants, namely the GWO-DE algorithm. We evaluate the new algorithm by
applying various numerical benchmark functions. The numerical results of the
comparative study are quite satisfactory in terms of performance and solution
quality.

</details>


<div id='physics.ins-det'></div>

# physics.ins-det [[Back]](#toc)

### [155] [Background in Low Earth Orbiting Cherenkov Detectors, and Mitigation Strategies](https://arxiv.org/abs/2507.03838)
*Christopher S. W. Davis,Fan Lei,Keith Ryden,Clive Dyer,Giovanni Santin,Piers Jiggens,Melanie Heil*

Main category: physics.ins-det

TL;DR: Simulations show Cherenkov detectors in low Earth orbit experience varying count rates due to factors like location, date, and solar events. They effectively gather data on trapped particles and spectral info during GLEs, with coincidence methods reducing background noise, though some trapped particle counts persist in the SAA.


<details>
  <summary>Details</summary>
Motivation: To characterize Cherenkov detector count rates in low Earth orbit and evaluate their effectiveness for measuring specific particle components amidst background noise.

Method: GRAS/Geant4 simulations were used to model count rates for a simple Cherenkov detector in low Earth orbit, analyzing factors like orbit location, date, and solar events. Coincidence methods were tested for background reduction.

Result: Count rates vary significantly with orbit conditions. The detector effectively measures trapped particles and GLEs, with coincidence methods reducing background but not eliminating all trapped particle counts in the SAA.

Conclusion: Cherenkov detectors are versatile for space particle measurements, though background reduction methods like coincidence are effective but not perfect, especially in regions like the SAA.

Abstract: Cherenkov detectors have been used in space missions for many decades, and
for a variety of purposes, including for example, for Galactic Cosmic Ray (GCR)
and Solar Energetic Particle (SEP) measurements. Cherenkov detectors are
sensitive to many types of particles that are present in the environment of
space, including gamma rays, trapped particles and cosmic particles, and each
particle component acts as essentially a background when trying to view another
specific particle component. In this research, GRAS/Geant4 simulations were
performed to characterise the count rates that a simple Cherenkov detector
design would experience in a low Earth orbit, and we find that Cherenkov count
rates due to most particle components vary significantly depending on many
different factors, including the location in the orbit, the date of the orbit,
whether or not the detector is within the van Allen belts, and whether or not a
solar particle event is occurring. We find that a small Cherenkov detector is
readily able to gather detailed data on both trapped particles and spectral
information during Ground-Level Enhancements. We also investigate the use of
coincidence as a method to remove count rates due to trapped particles and
delta electrons, finding that this method is generally very effective for
resolving count rates due to GLEs amongst intense trapped particle
environments, but that some Cherenkov count rates due to trapped particles are
still observed in the simulated south Atlantic anomaly region.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [156] [Sequential multiple importance sampling for high-dimensional Bayesian inference](https://arxiv.org/abs/2507.05114)
*Li Binbin,He Xiao,Liao Zihan*

Main category: stat.ME

TL;DR: SeMIS is a new algorithm for high-dimensional Bayesian inference, improving evidence estimation and posterior sampling by using a softly truncated prior and enabling better mode mixing.


<details>
  <summary>Details</summary>
Motivation: To address challenges in high-dimensional Bayesian inference, especially for multimodal distributions, by improving mode mixing and evidence estimation.

Method: Sequential multiple importance sampling (SeMIS) with a softly truncated prior as an intermediate proposal, combining importance-resampling for posterior samples.

Result: Outperforms SuS and aBUS in evidence estimation (lower bias/variance) and posterior sampling (higher effective sample sizes, closer to true posterior). Validated in a high-dimensional finite element model.

Conclusion: SeMIS advances Bayesian computation for complex posteriors and offers robust uncertainty quantification for civil engineering, enhancing structural health monitoring.

Abstract: This paper introduces a sequential multiple importance sampling (SeMIS)
algorithm for high-dimensional Bayesian inference. The method estimates
Bayesian evidence using all generated samples from each proposal distribution
while obtaining posterior samples through an importance-resampling scheme. A
key innovation of SeMIS is the use of a softly truncated prior distribution as
the intermediate proposal, providing a new way bridging prior and posterior
distributions. By enabling samples from high-likelihood regions to traverse
low-probability zones, SeMIS enhances mode mixing in challenging inference
problems. Comparative evaluations against subset simulation (SuS) and adaptive
Bayesian updating with structural reliability methods (aBUS) demonstrate that
SeMIS achieves superior performance in evidence estimation (lower bias and
variance) and posterior sampling (higher effective sample sizes and closer
approximation to the true posterior), particularly for multimodal
distributions. The efficacy of SeMIS is further validated in a high-dimensional
finite element model updating application, where it successfully localizes
structural damages by quantifying stiffness loss. The proposed algorithm not
only advances Bayesian computation for complex posterior distributions but also
provides a robust tool for uncertainty quantification in civil engineering
systems, offering new possibilities for probabilistic structural health
monitoring.

</details>
