<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 13]
- [math.AP](#math.AP) [Total: 10]
- [physics.comp-ph](#physics.comp-ph) [Total: 2]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 5]
- [math.OC](#math.OC) [Total: 1]
- [cond-mat.soft](#cond-mat.soft) [Total: 2]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 5]
- [hep-ph](#hep-ph) [Total: 1]
- [math.CA](#math.CA) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [econ.GN](#econ.GN) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Multi-patch/multiple-scattering frequency-time hybrid solver for interior and exterior wave equation problems](https://arxiv.org/abs/2507.05725)
*Shuai Pan,Gang Bao,Tao Yin,Oscar P. Bruno*

Main category: math.NA

TL;DR: The paper introduces an enhanced FTH-MS solver for wave scattering in 2D, improving geometric flexibility, parallelization, and efficiency by decomposing problems into open-arc subproblems.


<details>
  <summary>Details</summary>
Motivation: To extend the original FTH-MS algorithm for greater geometric flexibility, parallelization, and efficiency in solving wave scattering problems.

Method: Partitions domain boundaries into overlapping arcs, decomposing the problem into subproblems solved via Helmholtz frequency-domain boundary integral equations.

Result: The method enables parallelization, reduces iterations, and handles long-duration signals with spectral accuracy.

Conclusion: The enhanced FTH-MS solver offers improved performance and versatility for wave scattering problems in 2D.

Abstract: This paper proposes a new multiple-scattering frequency-time hybrid (FTH-MS)
integral equation solver for problems of wave scattering by obstacles in two
dimensional space, including interior problems in closed cavities and problems
exterior to a set of disconnected open or closed scattering obstacles. The
multiple-scattering FTH-MS method is based on a partition of the domain
boundary into a user-prescribed set of overlapping open arcs, along with a
corresponding sequence of multiple-scattering problems that effectively
decompose the interior problem into a series of open-arc wave equation
subproblems. The new strategy provides a significant extension of the original
FTH-MS algorithm originally presented in [22], in that (1) By allowing for use
of an arbitrary of number of component arcs, and not just two as in the
previous contribution, the new approach affords (1a) A significantly increased
geometric flexibility, as well as, (1b) The use of partitions for which each
open arc leads to small numbers of iterations if iterative linear-algebra
solvers are employed; and, (2) It facilitates parallelization -- as the
subproblem solutions that are needed at each multiple scattering step can be
evaluated in an embarrassingly parallel fashion. Utilizing a
suitably-implemented Fourier transformation, each sub-problem is reduced to a
Helmholtz frequency-domain problem that is tackled via a uniquely-solvable
boundary integral equation. Similar FTH-MS methods are also presented for
problems exterior to a number of bounded obstacles. All of the algorithms
considered incorporate the previously introduced ``time-windowing and
recentering'' methodology (that enables both treatment of incident signals of
long duration and long time simulation), as well as a high-frequency Fourier
transform algorithm that delivers numerically dispersionless,
spectrally-accurate time evolution for arbitrarily long times.

</details>


### [2] [A direct PinT algorithm for higher-order nonlinear equations](https://arxiv.org/abs/2507.05743)
*Shun-Zhi Zhong,Yong-Liang Zhao*

Main category: math.NA

TL;DR: A direct time-parallel algorithm for solving time-dependent differential equations (order 1-3) is proposed, using diagonalization of the time discretization matrix B, with explicit formulas for V and V⁻¹, and a fast spectral decomposition algorithm for computational speedup.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of traditional time-stepping methods by solving higher-order evolution equations directly via an all-at-once system.

Method: Diagonalize the time discretization matrix B, derive explicit formulas for eigenvector matrix V and its inverse, and develop a fast spectral decomposition algorithm.

Result: The proposed fast algorithm achieves significant computational speedup, validated by numerical experiments.

Conclusion: The direct time-parallel algorithm with spectral decomposition offers an efficient alternative to traditional methods for solving time-dependent differential equations.

Abstract: This paper mainly studies a direct time-parallel algorithm for solving
time-dependent differential equations of order 1 to 3. Different from the
traditional time-stepping approach, we directly solve the all-at-once system
from higher-order evolution equations by diagonalization the time
discretization matrix $B$. Based on the connection between the characteristic
equation and Chebyshev polynomials, we give explicit formulas for the
eigenvector matrix $V$ of $B$ and its inverse $V^{-1}$ , and prove that
$cond_2\left( V \right) =\mathcal{O} \left( n^3 \right)$, where $n$ is the
number of time steps. A fast algorithm $B$ designed by exploring the structure
of the spectral decomposition of $B$. Numerical experiments were performed to
validate the acceleration performance of the fast spectral decomposition
algorithm. The results show that the proposed fast algorithm achieves
significant computational speedup.

</details>


### [3] [Regularized boundary integral equation methods for open-arc scattering problems in thermoelasticity](https://arxiv.org/abs/2507.05747)
*Yixuan X. Kong,José Pinto,Tao Yin*

Main category: math.NA

TL;DR: Novel boundary integral equation solvers for thermoelastic scattering by open-arcs with four boundary conditions are developed, leveraging Calderón formulas and regularization to improve efficiency.


<details>
  <summary>Details</summary>
Motivation: To address thermoelastic scattering problems with varying boundary conditions on open-arcs, requiring efficient and accurate numerical solvers.

Method: Uses Calderón formulas and regularized boundary integral equations to handle edge singularities, reducing singular integrals to weakly-singular forms for numerical efficiency.

Result: The proposed solvers demonstrate accuracy and efficiency in numerical examples, reducing iteration counts for solving discretized systems.

Conclusion: The methodology effectively solves thermoelastic scattering problems on open-arcs, validated by numerical results.

Abstract: This paper devotes to developing novel boundary integral equation (BIE)
solvers for the problem of thermoelastic scattering by open-arcs with four
different boundary conditions in two dimensions. The proposed methodology is
inspired by the Calder\'on formulas, whose eigenvalues are shown to accumulate
at particular points depending only on Lam\'e parameters, satisfied by the
thermoelastic boundary integral operators (BIOs) on both closed- and
open-surfaces. Regularized BIEs in terms of weighted BIOs on open-arc that
explicitly exhibits the edge singularity behavior, depending on the types of
boundary conditions, of the unknown potentials are constructed to effectively
reduce the required iteration number to solve the corresponding discretized
linear systems. We implement the new formulations utilizing regularizations of
singular integrals, which reduces the strongly- and hyper-singular integrals
into weakly-singular integrals. Combined with spectrally accurate quadrature
rules, numerical examples are presented to illustrate the accuracy and
efficiency of the proposed solvers.

</details>


### [4] [On the detection of medium inhomogeneity by contrast agent: wave scattering models and numerical implementations](https://arxiv.org/abs/2507.05773)
*Zhe Wang,Ahcene Ghandriche,Jijun Liu*

Main category: math.NA

TL;DR: The paper develops an efficient method for wave scattering and inverse scattering in an inhomogeneous medium with a small droplet, using Lippmann-Schwinger equations and far-field patterns to reconstruct the medium's properties.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of reconstructing inhomogeneous media properties by leveraging wave scattering and far-field patterns, especially with small embedded droplets.

Method: Uses Lippmann-Schwinger integral equations for scattering waves, approximates far-field patterns, and employs the dual reciprocity method for numerical reconstruction.

Result: The method successfully reconstructs the bulk modulus function in 3D space using simulation data, validated by numerical implementations.

Conclusion: The proposed scheme effectively handles the ill-posed inverse problem and provides a practical approach for inhomogeneity reconstruction.

Abstract: We consider the wave scattering and inverse scattering in an inhomogeneous
medium embedded a homogeneous droplet with a small size, which is modeled by a
constant mass density and a small bulk modulus. Based on the Lippmann-Schwinger
integral equation for scattering wave in inhomogeneous medium, we firstly
develop an efficient approximate scheme for computing the scattered wave as
well as its far-field pattern for any droplet located in the inhomogeneous
background medium. By establishing the approximate relation between the
far-field patterns of the scattered wave before and after the injection of a
droplet, the scattered wave of the inhomogeneous medium after injecting the
droplet is represented by a measurable far-field patterns, and consequently the
inhomogeneity of the medium can be reconstructed from the Helmholtz equation.
Finally, the reconstruction process in terms of the dual reciprocity method is
proposed to realize the numerical algorithm for recovering the bulk modulus
function inside a bounded domain in three dimensional space, by moving the
droplet inside the bounded domain. Numerical implementations are given using
the simulation data of the far-field pattern to show the validity of the
reconstruction scheme, based on the mollification scheme for dealing with the
ill-posedness of this inverse problem.

</details>


### [5] [A nonsmooth extension of the Brezzi-Rappaz-Raviart approximation theorem via metric regularity techniques and applications to nonlinear PDEs](https://arxiv.org/abs/2507.05774)
*Jules Berry,Olivier Ley,Francisco José Silva*

Main category: math.NA

TL;DR: Generalization of the Brezzi-Rappaz-Raviart theorem for nonlinear PDEs with Lipschitz regularity, enabling quasi-optimal error estimates for finite element approximations.


<details>
  <summary>Details</summary>
Motivation: To extend the applicability of the Brezzi-Rappaz-Raviart theorem to nonlinearities with only Lipschitz regularity, beyond the previously required differentiability.

Method: Utilizes metrically regular mappings from variational analysis to handle nonlinearities with Lipschitz regularity.

Result: Quasi-optimal error estimates for finite element approximations of viscous Hamilton-Jacobi equations and second-order mean field games systems.

Conclusion: The generalized theorem broadens the scope of nonlinear PDEs that can be analyzed, particularly for problems with less smooth nonlinearities.

Abstract: We generalize the Brezzi-Rappaz-Raviart approximation theorem, which allows
to obtain existence and a priori error estimates for approximations of
solutions to some nonlinear partial differential equations. Our contribution
lies in the fact that we typically allow for nonlinearities having merely
Lipschitz regularity, while previous results required some form of
differentiability. This is achieved by making use of the theory of metrically
regular mappings, developed in the context of variational analysis. We apply
this generalization to derive some quasi-optimal error estimates for finite
element approximations to solutions of viscous Hamilton-Jacobi equations and
second order mean field games systems.

</details>


### [6] [A generalized Hessian-based error estimator for an IPDG formulation of the biharmonic problem in two dimensions](https://arxiv.org/abs/2507.05776)
*Théophile Chaumont-Frelet,Joscha Gedicke,Lorenzo Mascotto*

Main category: math.NA

TL;DR: A novel error estimator for a 2D biharmonic problem using a discontinuous Galerkin method, splitting error into conformity and nonconformity terms, proven reliable and efficient without DG stabilization.


<details>
  <summary>Details</summary>
Motivation: To address the need for a reliable and efficient error estimator for biharmonic problems without relying on DG stabilization.

Method: Uses a symmetric interior penalty discontinuous Galerkin method and a split error measure based on a generalized Hessian.

Result: The proposed error estimator is reliable, efficient, and outperforms the standard DG residual error estimator.

Conclusion: The new estimator is validated numerically, confirming its theoretical reliability and efficiency.

Abstract: We consider a two dimensional biharmonic problem and its discretization by
means of a symmetric interior penalty discontinuous Galerkin method. Based on
the ``div-div'' complex, a novel split of an error measure based on a
generalized Hessian into two terms measuring the conformity and nonconformity
of the scheme is proven. This splitting is the departing point for the design
of a new reliable and efficient error estimator, which does not involve any DG
stabilization. Such an error estimator can be bounded from above by the
standard DG residual error estimator. Numerical results assess the theoretical
predictions, including the efficiency of the proposed estimator.

</details>


### [7] [A relaxation scheme for the equations of isentropic gas dynamics on a network with jump transmission conditions](https://arxiv.org/abs/2507.05779)
*Magali Ribot,Roberto Natalini,Maya Briani*

Main category: math.NA

TL;DR: A new relaxation-type numerical scheme for approximating Euler equations of isentropic gas dynamics on network arcs, with a solver for subsonic and supersonic cases, proven consistency, and demonstrated performance.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of approximating Euler equations on network arcs with mass conservation and density jump conditions at junctions.

Method: Introduces a relaxation-type numerical scheme and a new solver for handling subsonic and supersonic cases, with proven consistency.

Result: Numerical tests confirm the solver's good performance compared to other solvers.

Conclusion: The proposed scheme and solver effectively approximate Euler equations on networks, with validated consistency and performance.

Abstract: In this paper we propose a new numerical scheme of relaxation type to
approximate the Euler equations of isentropic gas dynamics on the arcs of a
network. At the junction mass conservation and a jump transmission condition on
the density are given, and a new solver is introduced to deal with both
subsonic and supersonic cases. Consistency properties of the solver are proven
and numerical tests are displayed to show its good performance also with
respect to other possible solvers.

</details>


### [8] [The Neural Approximated Virtual Element Method for Elasticity Problems](https://arxiv.org/abs/2507.05786)
*Stefano Berrone,Moreno Pintore,Gioana Teora*

Main category: math.NA

TL;DR: The paper introduces the Neural Approximated Virtual Element Method (NAVEM), a hybrid approach combining Finite Element Method, Virtual Element Method, and deep neural networks to solve elasticity problems.


<details>
  <summary>Details</summary>
Motivation: To simplify the discretization of elasticity problems, especially non-linear ones, by eliminating the need for stabilization or projection operators used in traditional Virtual Element Methods.

Method: NAVEM approximates virtual basis functions element-wise using neural networks, merging classical FEM and VEM concepts with deep learning.

Result: Numerical tests on linear and non-linear elasticity problems show the method's effectiveness, particularly in simplifying non-linear problem handling.

Conclusion: NAVEM offers a simpler discretization approach for elasticity problems, leveraging neural networks to enhance traditional methods.

Abstract: We present the Neural Approximated Virtual Element Method to numerically
solve elasticity problems. This hybrid technique combines classical concepts
from the Finite Element Method and the Virtual Element Method with recent
advances in deep neural networks. Specifically, it is a polygonal method in
which the virtual basis functions are element-wise approximated by a neural
network, eliminating the need for stabilization or projection operators typical
of the standard virtual element method. We present the discrete formulation of
the problem and provide numerical tests on both linear and non-linear
elasticity problems, demonstrating the advantages of having a simple
discretization, particularly in handling non-linearities.

</details>


### [9] [Weak Galerkin Methods for the Brinkman Equations](https://arxiv.org/abs/2507.05953)
*Chunmei Wang,Shangyou Zhang*

Main category: math.NA

TL;DR: A novel weak Galerkin (WG) finite element method is introduced for solving the Brinkman equations, unifying Stokes and Darcy regimes with proven stability and accuracy.


<details>
  <summary>Details</summary>
Motivation: The Brinkman model describes fluid flow in heterogeneous porous media, requiring a robust numerical method to handle its dual Stokes-Darcy nature.

Method: The WG finite element method is developed, with a discrete inf-sup condition and optimal-order error estimates proven. Numerical experiments validate the approach.

Result: The method accurately captures both Stokes- and Darcy-dominated regimes, with numerical experiments confirming theoretical accuracy and stability.

Conclusion: The WG method is effective for the Brinkman equations, offering a unified solution for complex fluid flow in porous media.

Abstract: This paper introduces a novel weak Galerkin (WG) finite element method for
the numerical solution of the Brinkman equations. The Brinkman model, which
seamlessly integrates characteristics of both the Stokes and Darcy equations,
is employed to describe fluid flow in multiphysics contexts, particularly
within heterogeneous porous media exhibiting spatially variable permeability.
The proposed WG method offers a unified and robust approach capable of
accurately capturing both Stokes- and Darcy-dominated regimes. A discrete
inf-sup condition is established, and optimal-order error estimates are
rigorously proven for the WG finite element solutions. Furthermore, a series of
numerical experiments is performed to corroborate the theoretical analysis,
demonstrating the method's accuracy and stability in addressing the
complexities inherent in the Brinkman equations.

</details>


### [10] [A posteriori analysis of neural network approximations](https://arxiv.org/abs/2507.06017)
*Thomas Führer,Sergio Rojas*

Main category: math.NA

TL;DR: The paper studies a posteriori error estimates in finite element analysis, showing the error is equivalent to two residuals, one computable and the other estimable. It applies to neural networks and optimization, validated with numerical experiments.


<details>
  <summary>Details</summary>
Motivation: To provide reliable error estimates for approximations in finite element analysis, even when the approximation isn't generated by finite element methods, and extend applicability to neural networks.

Method: The error is decomposed into two residuals: one computable via projection and another estimable with an upper bound. The approach is validated using a second-order elliptic PDE and numerical experiments.

Result: The error is shown to be equivalent to the sum of two residuals, with practical computability and reliable estimation in many scenarios.

Conclusion: The method offers versatile error estimation for approximations, including neural networks, and can be integrated into optimization for error control.

Abstract: In a general setting, we study a posteriori estimates used in finite element
analysis to measure the error between a solution and its approximation. The
latter is not necessarily generated by a finite element method. We show that
the error is equivalent to the sum of two residuals provided that the
underlying variational formulation is well posed. The first contribution is the
projection of the residual to a finite-dimensional space and is therefore
computable, while the second one can be reliably estimated by a computable
upper bound in many practical scenarios. Assuming sufficiently accurate
quadrature, our findings can be used to estimate the error of, e.g., neural
network outputs. Two important applications can be considered during
optimization: first, the estimators are used to monitor the error in each
solver step, or, second, the two estimators are included in the loss
functional, and therefore provide control over the error. As a model problem,
we consider a second-order elliptic partial differential equation and discuss
different variational formulations thereof, including several options to
include boundary conditions in the estimators. Various numerical experiments
are presented to validate our findings.

</details>


### [11] [Fredholm Neural Networks for forward and inverse problems in elliptic PDEs](https://arxiv.org/abs/2507.06038)
*Kyriakos Georgiou,Constantinos Siettos,Athanasios N. Yannacopoulos*

Main category: math.NA

TL;DR: The paper extends Fredholm Neural Networks (FNNs) to solve forward and inverse problems for elliptic PDEs, introducing Potential Fredholm Neural Networks (PFNNs) for accuracy and explainability.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of solving linear and semi-linear elliptic PDEs using neural networks while ensuring accuracy and explainability.

Method: Proposes PFNNs, a DNN-based iterative scheme derived from fixed-point iterations for elliptic PDEs, with layers and parameters computed explainably.

Result: Achieves high accuracy (near machine-precision on boundaries) and provides explicit error bounds tied to network architecture.

Conclusion: PFNNs offer a robust, explainable solution for elliptic PDEs, validated through forward and inverse problem tests.

Abstract: Building on our previous work introducing Fredholm Neural Networks (Fredholm
NNs/ FNNs) for solving integral equations, we extend the framework to tackle
forward and inverse problems for linear and semi-linear elliptic partial
differential equations. The proposed scheme consists of a deep neural network
(DNN) which is designed to represent the iterative process of fixed-point
iterations for the solution of elliptic PDEs using the boundary integral method
within the framework of potential theory. The number of layers, weights, biases
and hyperparameters are computed in an explainable manner based on the
iterative scheme, and we therefore refer to this as the Potential Fredholm
Neural Network (PFNN). We show that this approach ensures both accuracy and
explainability, achieving small errors in the interior of the domain, and near
machine-precision on the boundary. We provide a constructive proof for the
consistency of the scheme and provide explicit error bounds for both the
interior and boundary of the domain, reflected in the layers of the PFNN. These
error bounds depend on the approximation of the boundary function and the
integral discretization scheme, both of which directly correspond to components
of the Fredholm NN architecture. In this way, we provide an explainable scheme
that explicitly respects the boundary conditions. We assess the performance of
the proposed scheme for the solution of both the forward and inverse problem
for linear and semi-linear elliptic PDEs in two dimensions.

</details>


### [12] [Learning-Enhanced Variational Regularization for Electrical Impedance Tomography via \Calderon's Method](https://arxiv.org/abs/2507.06114)
*Kai Li,Kwancheol Shin,Zhi Zhou*

Main category: math.NA

TL;DR: A deep learning-based method is proposed to solve the ill-posed 2D EIT problem using Calderon's method, incorporating a priori information for regularization, achieving accurate reconstructions even in high-contrast cases.


<details>
  <summary>Details</summary>
Motivation: The severe ill-posedness and nonlinearity of the EIT inverse problem require effective regularization strategies incorporating a priori information.

Method: A deep learning approach captures a priori shape/location information of the conductivity contrast, used to construct a regularization functional solved via Gauss-Newton.

Result: Numerical experiments show accurate reconstructions, even for high contrasts, with strong generalization. Stability and convergence analysis highlight the importance of a priori support information.

Conclusion: The proposed method effectively addresses the EIT inverse problem by leveraging deep learning for regularization, demonstrating robustness and accuracy.

Abstract: This paper aims to numerically solve the two-dimensional electrical impedance
tomography (EIT) with Cauchy data. This inverse problem is highly challenging
due to its severe ill-posed nature and strong nonlinearity, which necessitates
appropriate regularization strategies. Choosing a regularization approach that
effectively incorporates the \textit{a priori} information of the conductivity
distribution (or its contrast) is therefore essential. In this work, we propose
a deep learning-based method to capture the \textit{a priori} information about
the shape and location of the unknown contrast using \Calderon's method. The
learned \textit{a priori} information is then used to construct the
regularization functional of the variational regularization method for solving
the inverse problem. The resulting regularized variational problem for EIT
reconstruction is then solved using the Gauss-Newton method. Extensive
numerical experiments demonstrate that the proposed inversion algorithm
achieves accurate reconstruction results, even in high-contrast cases, and
exhibits strong generalization capabilities. Additionally, some stability and
convergence analysis of the variational regularization method underscores the
importance of incorporating \textit{a priori} information about the support of
the unknown contrast.

</details>


### [13] [Conservative approximation-based feedforward neural network for WENO schemes](https://arxiv.org/abs/2507.06190)
*Kwanghyuk Park,Jiaxi Gu,Jae-Hun Jung*

Main category: math.NA

TL;DR: A feedforward neural network replaces the classical WENO weighting procedure for hyperbolic conservation laws, achieving high accuracy and outperforming WENO3-Z.


<details>
  <summary>Details</summary>
Motivation: To improve the accuracy and generalization of WENO schemes by leveraging neural networks for conservative approximation.

Method: A supervised learning approach trains a neural network to replace WENO weighting, using a new dataset for one-dimensional conservative approximation and a symmetric-balancing loss function.

Result: The resulting WENO3-CADNNs outperform WENO3-Z and match WENO5-JS accuracy across benchmarks.

Conclusion: Neural networks can effectively enhance WENO schemes, offering robust generalization and superior performance.

Abstract: In this work, we present the feedforward neural network based on the
conservative approximation to the derivative from point values, for the
weighted essentially non-oscillatory (WENO) schemes in solving hyperbolic
conservation laws. The feedforward neural network, whose inputs are point
values from the three-point stencil and outputs are two nonlinear weights,
takes the place of the classical WENO weighting procedure. For the training
phase, we employ the supervised learning and create a new labeled dataset for
one-dimensional conservative approximation, where we construct a numerical flux
function from the given point values such that the flux difference approximates
the derivative to high-order accuracy. The symmetric-balancing term is
introduced for the loss function so that it propels the neural network to match
the conservative approximation to the derivative and satisfy the symmetric
property that WENO3-JS and WENO3-Z have in common. The consequent WENO schemes,
WENO3-CADNNs, demonstrate robust generalization across various benchmark
scenarios and resolutions, where they outperform WENO3-Z and achieve accuracy
comparable to WENO5-JS.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [14] [Boundary regularity of optimal transport maps on convex domains](https://arxiv.org/abs/2507.05395)
*Tristan C. Collins,Freid Tong*

Main category: math.AP

TL;DR: The paper analyzes the regularity of optimal transport maps between convex domains with quadratic cost, focusing on boundary regularity and degenerate densities.


<details>
  <summary>Details</summary>
Motivation: To understand the regularity properties of optimal transport maps, especially near boundaries and for degenerate densities, which is relevant in Kähler geometry.

Method: Uses a monotonicity formula for optimal transport maps on convex domains to study homogeneity of blow-ups and characterizes regularity based on boundary geometry.

Result: Proves $C^{1,1-\varepsilon}$-regularity for nondegenerate densities and $C^{2,\alpha}$-regularity for smoother boundaries. Also characterizes pointwise $C^{1,1}$-regularity for planar polytopes.

Conclusion: The study provides insights into boundary regularity and introduces a new tool (monotonicity formula) for analyzing optimal transport maps, with applications in geometry.

Abstract: We study the regularity of optimal transport maps between convex domains with
quadratic cost. For nondegenerate $C^{\alpha}$-densities, we prove $C^{1,
1-\varepsilon}$-regularity of the potentials up to the boundary. If in addition
the boundary is $C^{1, \alpha}$, we improve this to $C^{2, \alpha}$-regularity.
We also investigate pointwise $C^{1, 1}$-regularity at boundary points. We
obtain a complete characterization of pointwise $C^{1, 1}$-regularity for
planar polytopes in terms of the geometry of tangent cones. Furthermore, we
study the regularity of optimal transport maps with degenerate densities on
cones, which arise from recent developments in K\"ahler geometry. The main new
technical tool we introduce is a monotonicity formula for optimal transport
maps on convex domains which characterizes the homogeneity of blow-ups.

</details>


### [15] [Blow-up results for Inhomogeneous fourth-order nonlinear Schrödinger Equation](https://arxiv.org/abs/2507.05518)
*Renzo Scarpelli,Maicon Hespanha*

Main category: math.AP

TL;DR: Analysis of blow-up phenomena in the $H^2$ norm for inhomogeneous biharmonic Schrödinger equations, distinguishing radial and non-radial solutions in negative and positive energy cases.


<details>
  <summary>Details</summary>
Motivation: To understand the blow-up behavior of solutions in different energy scenarios (negative and positive) and solution types (radial and non-radial).

Method: Convexity methods and virial identities are used to analyze the blow-up phenomena.

Result: Blow-up phenomena are characterized for both negative and positive energy cases, with distinctions between radial and non-radial solutions.

Conclusion: The study provides insights into the blow-up behavior of solutions under varying energy conditions and solution types, using convexity and virial techniques.

Abstract: In this paper, we investigate the blow-up phenomenon of the $H^2$ norm of
solutions to the inhomogeneous biharmonic Schrodinger equation in two distinct
scenarios. First, we consider the case of negative energy, analyzing separately
the cases of radial and non-radial solutions. Then, we examine the positive
energy case, where the energy is below that of the ground state and the
``kinetic'' energy exceeds the corresponding value for the ground state, again
distinguishing between radial and non-radial solutions. Our approach is based
on convexity methods, employing virial identities in the analysis.

</details>


### [16] [Existence and Uniqueness for Double-Phase Poisson Equations with Variable Growth](https://arxiv.org/abs/2507.05553)
*Mohamed Khamsi,Osvaldo Mendez*

Main category: math.AP

TL;DR: The paper analyzes nonlinear elliptic problems with a double-phase operator and variable exponents, proving existence and uniqueness of weak solutions under minimal assumptions.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by modeling heterogeneous materials undergoing phase transitions, where spatially varying ellipticity arises due to a combination of growth conditions and a measurable weight function.

Method: The authors work within modular function spaces, establishing uniform convexity of the modular associated with the gradient term, enabling a variational approach.

Result: Existence and uniqueness of weak solutions are proven under natural and minimal assumptions on the variable exponents and weight function.

Conclusion: The variational framework and structural properties of the modular allow for rigorous treatment of the problem, yielding robust results for applications in material modeling.

Abstract: We study a class of nonlinear elliptic problems driven by a double-phase
operator with variable exponents, arising in the modeling of heterogeneous
materials undergoing phase transitions. The associated Poisson problem features
a combination of two distinct growth conditions, modulated by a measurable
weight function \( \mu \), leading to spatially varying ellipticity. Working
within the framework of modular function spaces, we establish the uniform
convexity of the modular associated with the gradient term. This structural
property enables a purely variational treatment of the problem. As a
consequence, we prove existence and uniqueness of weak solutions under natural
and minimal assumptions on the variable exponents and the weight.

</details>


### [17] [Global weak solutions with higher regularity to the compressible isentropic Navier-Stokes equations in 2D bounded domains involving large initial data and vacuum](https://arxiv.org/abs/2507.05608)
*Shuai Wang,Guochun Wu,Xin Zhong*

Main category: math.AP

TL;DR: Extends previous work on weak solutions for compressible Navier-Stokes equations to 2D bounded convex domains with Navier-slip boundary.


<details>
  <summary>Details</summary>
Motivation: To generalize the existence of weak solutions to more complex domains and boundary conditions.

Method: Uses effective viscous flux and a Desjardins-type logarithmic interpolation inequality to handle boundary challenges.

Result: Establishes global existence of weak solutions with higher regularity in 2D bounded convex domains.

Conclusion: The approach successfully addresses boundary-related difficulties, extending previous results.

Abstract: This paper is a continuation of our previous work (arXiv:2507.03505), where
the global existence of weak solutions with higher regularity to the
compressible isentropic Navier-Stokes equations in the half-plane involving
large initial data and vacuum was established. We extend such a result to the
case of two-dimensional bounded simply connected convex domains under a
Navier-slip boundary condition. To overcome difficulties brought by boundary,
some new estimates based on the effective viscous flux and a Desjardins-type
logarithmic interpolation inequality play crucial roles.

</details>


### [18] [Lipschitz regularity for anisotropic fully nonlinear equations with nonstandard growth](https://arxiv.org/abs/2507.05712)
*Sun-Sig Byun,Hongsoo Kim*

Main category: math.AP

TL;DR: Interior Lipschitz regularity for anisotropic fully nonlinear equations with nonstandard growth is proven without growth exponent restrictions, using an anisotropic Ishii-Lions method.


<details>
  <summary>Details</summary>
Motivation: To extend regularity results for anisotropic fully nonlinear equations to cases with nonstandard growth, without limiting the gap between growth exponents.

Method: An anisotropic variant of the Ishii-Lions method is employed to establish regularity.

Result: Interior Lipschitz regularity is achieved for solutions, generalizing previous divergence-form results to non-divergence settings.

Conclusion: The work provides a viscosity analogue of existing divergence-form theory, broadening applicability to non-divergence scenarios.

Abstract: We establish interior Lipschitz regularity for solutions to anisotropic fully
nonlinear equations with nonstandard growth, without imposing any restriction
on the gap between the highest and lowest growth exponents. Our proof is based
on an anisotropic variant of the seminal Ishii Lions method. Our result
furnishes a viscosity analogue of the divergence-form theory in [Bousquet20],
adapted to the non-divergence setting.

</details>


### [19] [The Logarithmic Laplacian on General Graphs](https://arxiv.org/abs/2507.05936)
*Rui Chen,Wendi Xu*

Main category: math.AP

TL;DR: The paper presents a Bochner integral representation of the logarithmic Laplacian on weighted graphs, derives its pointwise form, and analyzes its properties, including bounds for the log kernel and Fourier multipliers.


<details>
  <summary>Details</summary>
Motivation: To provide a rigorous mathematical framework for the logarithmic Laplacian on weighted graphs and analyze its behavior, filling a gap in existing literature.

Method: Uses Bochner integral representation, assumes stochastic completeness, and employs Fourier analysis to derive properties like bounds and asymptotics.

Result: Establishes sharp bounds for the log kernel, proves unboundedness on l2, computes Fourier multipliers, and derives exact heat kernel behavior.

Conclusion: The work provides a comprehensive analysis of the logarithmic Laplacian on weighted graphs, offering new insights and tools for further research.

Abstract: We are the first to present a Bochner integral representation of the
logarithmic Laplacian on weighted graphs and to derive, under the assumption of
stochastic completeness, its explicit pointwise form. For weighted lattice
graphs with uniformly positive vertex measures, we establish sharp upper and
lower bounds for the associated log kernel, prove that the logarithmic
Laplacian fails to be bounded on l2, and offer an alternative derivation of the
above formula. Moreover, we compute the Fourier multipliers of both the
fractional and logarithmic Laplacians, and derive the exact large time behavior
and off diagonal asymptotics of their heat kernels, including all sharp
asymptotic constants.

</details>


### [20] [Traveling waves for highly degenerate and singular reaction-diffusion-advection equations with discontinuous coefficients](https://arxiv.org/abs/2507.06027)
*Umberto Guarnotta,Cristina Marcelli*

Main category: math.AP

TL;DR: The paper provides conditions for the existence or non-existence of traveling wave solutions in a quasi-linear reaction-diffusion-convection equation with discontinuous coefficients, and characterizes admissible wave speeds under additional hypotheses.


<details>
  <summary>Details</summary>
Motivation: To address the challenges posed by highly degenerate or singular equations with discontinuous coefficients, and to understand the behavior of traveling wave solutions in such systems.

Method: The study furnishes sufficient conditions for the existence or non-existence of traveling wave solutions and characterizes the set of admissible wave speeds under an additional hypothesis on the convection term.

Result: A double-sided bound is derived to estimate the minimum wave speed, providing a clear characterization of admissible wave speeds.

Conclusion: The findings offer rigorous conditions and bounds for traveling wave solutions in complex, degenerate, or singular reaction-diffusion-convection equations, enhancing theoretical understanding.

Abstract: Sufficient conditions for either existence or non-existence of traveling wave
solutions for a general quasi-linear reaction-diffusion-convection equation,
possibly highly degenerate or singular, with discontinuous coefficients are
furnished. Under an additional hypothesis on the convection term, the set of
admissible wave speeds is characterized in terms of the minimum wave speed,
which is estimated through a double-sided bound.

</details>


### [21] [Local boundedness for solutions of a class of non-uniformly elliptic anisotropic problems](https://arxiv.org/abs/2507.06054)
*Stefano Biagi,Giovanni Cupini,Elvira Mascolo*

Main category: math.AP

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We consider a class of {energy integrals}, associated to nonlinear and
non-uniformly elliptic equations, with integrands $f(x,u,\xi)$ satisfying
anisotropic $p_i,q$-growth conditions of the form $$ \sum_{i=1}^n \lambda_i
(x)|\xi_i|^{p_i}\le {f}(x,u,\xi)\le \mu (x)\left\{|\xi|^{q} +
|u|^{\gamma}+1\right\} $$ for some exponents $\gamma\ge q\geq p_i>1$, and
non-negative functions $\lambda_i,\mu$ subject to suitable summability
assumptions. We prove the local boundedness of scalar local quasi-minimizers of
such integrals.

</details>


### [22] [Generic mean curvature flow with obstacles](https://arxiv.org/abs/2507.06150)
*Tim Laux,Keisuke Takasao*

Main category: math.AP

TL;DR: The paper studies the obstacle problem in mean curvature flow using a singular perturbation method, extending prior work to show unique solutions and distributional solutions for generic level sets.


<details>
  <summary>Details</summary>
Motivation: To address the obstacle problem in mean curvature flow by combining geometric vanishing-viscosity approximation with a singular perturbation, building on earlier work by Evans, Spruck, Ullrich, and others.

Method: A singular perturbation is added to the geometric vanishing-viscosity approximation to penalize constraint violations, and the limit is analyzed.

Result: The level set formulation yields unique solutions (up to fattening), and generic level sets are shown to be distributional solutions of the obstacle problem.

Conclusion: The approach successfully extends prior work, providing a framework for solving the obstacle problem in mean curvature flow with unique and distributional solutions.

Abstract: We study the obstacle problem associated to mean curvature flow. We add to
the geometric vanishing-viscosity approximation of Evans and Spruck a singular
perturbation that penalizes the violation of the constraint, and pass to the
limit. The resulting level set formulation has unique solutions - up to
fattening. Extending the work of Evans and Spruck and a work by Ullrich and one
of the authors, we show that generic level sets of this flow are distributional
solutions of the obstacle problem.

</details>


### [23] [Notes on $L^2$-estimates in linear elliptic equations with general coefficients](https://arxiv.org/abs/2507.04940)
*Haesung Lee*

Main category: math.AP

TL;DR: The paper provides an explicit $L^2$-estimate for weak solutions of linear elliptic equations, bounding the solution's norm by the source term's norm. The method avoids compactness arguments, uses a divergence-free transformation, and yields a computable constant. Results are robust and applicable to error estimates in PINNs.


<details>
  <summary>Details</summary>
Motivation: To derive explicit and computable bounds for weak solutions of elliptic equations, addressing limitations of classical methods and supporting applications like error estimation in PINNs.

Method: A divergence-free transformation method is used to establish explicit $L^2$-estimates, avoiding compactness arguments and providing a computable constant.

Result: The $L^2$-norm of the solution is bounded by the source term's norm, with robustness in cases without zero-order terms. The constant decreases with larger diffusion coefficients or zero-order terms.

Conclusion: The explicit estimates and computable constants enhance theoretical rigor and practical utility, particularly for applications like error bounds in PINNs.

Abstract: This paper establishes an explicit $L^2$-estimate for weak solutions $u$ to
linear elliptic equations in divergence form with general coefficients and
external source term $f$, stating that the $L^2$-norm of $u$ over $U$ is
bounded by a constant multiple of the $L^2$-norm of $f$ over $U$. In contrast
to classical approaches based on compactness arguments, the proposed method,
which employs a divergence-free transformation method, provides a computable
and explicit constant $C>0$. The $L^2$-estimate remains robust even when there
is no zero-order term, and the analysis further demonstrates that the constant
$C>0$ decreases as the diffusion coefficient or the zero-order term increases.
These quantitative results provide a rigorous foundation for applications such
as a posteriori error estimates in Physics-Informed Neural Networks (PINNs),
where explicit error bounds are essential.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [24] [A family of conservative axisymmetric contact SPH schemes for impact engineering applications](https://arxiv.org/abs/2507.05927)
*G. D. Rublev*

Main category: physics.comp-ph

TL;DR: A family of conservative schemes for axisymmetric CSPH improves accuracy and stability in modeling multi-material flows. Verification includes test problems and comparisons with experiments.


<details>
  <summary>Details</summary>
Motivation: To enhance accuracy and stability in modeling complex multi-material flows of compressible media using axisymmetric CSPH.

Method: Introduces conservative schemes, including MUSCL reconstruction and kernel gradient correction, verified on test problems like Sod's cylindrical test and Taylor bar test.

Result: Demonstrates conservative properties and successful simulations, such as air shock wave weakening by a sand barrier, matching experimental results.

Conclusion: The proposed schemes are effective for axisymmetric CSPH, validated by tests and practical simulations.

Abstract: A family of conservative schemes for the axisymmetric contact smoothed
particle hydrodynamics (CSPH) method, which ensure the accuracy and stability
in modeling of complex multi-material flows of compressible media, is
introduced. Among these schemes, the most convenient ones are considered.
Simulations with the proposed schemes may be also improved by embedding of
MUSCL reconstruction into a numerical scheme, or by correcting the kernel
gradient as was proposed earlier for the Cartesian case.
  Verification of the proposed method is performed on several test problems:
Sod's cylindrical test, Taylor bar test, and Sedov's point explosion. The
conservative properties of the scheme are demonstrated. Finally, a set of
simulations on air shock wave weakening by a breakaway sand barrier is
performed and compared to experimental results.

</details>


### [25] [Drag modelling for flows through assemblies of spherical particles with machine learning: A comparison of approaches](https://arxiv.org/abs/2507.05983)
*Julia Reuter,Hani Elmestikawy,Sanaz Mostaghim,Berend van Wachem*

Main category: physics.comp-ph

TL;DR: The paper explores using genetic programming (GP) to create interpretable drag models for particle assemblies, extending its application to higher particle Reynolds numbers by training a graph neural network (GNN) on PR-DNS data.


<details>
  <summary>Details</summary>
Motivation: To address the black-box nature of neural networks in deterministic drag models and develop interpretable alternatives.

Method: Uses GNN to learn pairwise particle interactions from PR-DNS data, then applies GP to derive symbolic expressions from these interactions.

Result: Symbolic models from GP are simpler but slightly less accurate than the GNN.

Conclusion: GP shows promise for creating interpretable drag models, though with a trade-off in accuracy compared to GNN.

Abstract: Drag forces on particles in random assemblies can be accurately estimated
through particle-resolved direct numerical simulations (PR-DNS). Despite its
limited applicability to relatively small assemblies, data obtained from PR-DNS
has been the driving force for the development of drag closures for much more
affordable simulation frameworks, such as Eulerian-Lagrangian point particle
methods. Recently, more effort has been invested in the development of
deterministic drag models that account for the effect of the structure of the
particle assembly. Current successful deterministic models are mainly black-box
neural networks which: 1) Assume pairwise superposition of the neighbours'
effect on the drag, and 2) Are trained on PR-DNS data for a wide range of
particle concentrations and flow regimes. To alleviate the black-box nature of
neural networks, we use genetic programming (GP) to develop interpretable
models. In our previous research, this has been proven successful in the Stokes
regime. In the current contribution, we extend the application of GP to higher
particle Reynolds number regimes. This is done by training a graph neural
network (GNN) on the PR-DNS data to learn the pairwise interactions among the
particles that constitute the drag variation. The significance of the input
features of the GNN is assessed via a feature permutation approach. Then, the
estimated pairwise interactions as extracted from the GNN are fed to a GP
algorithm, which searches for symbolic expressions that fit the input data. A
comparison between the trained GNN model and the resulting symbolic expressions
is presented, to assess whether the symbolic expression can capture the
underlying patterns learnt by the GNN. The comparison demonstrates the
potential of GP in finding relatively simple symbolic models. At the same time,
the accuracy of the symbolic models slightly fall behind the GNN.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [26] [Statistical inference of anomalous thermal transport with uncertainty quantification for interpretive 2-D SOL models](https://arxiv.org/abs/2507.05413)
*Yichen Fu,Ben Dudson,Xiao Chen,Maxim Umansky,Filippo Scotti,Tom Rognlien,Anthony Leonard*

Main category: physics.plasm-ph

TL;DR: A Bayesian optimization workflow for inferring anomalous cross-field transport coefficients in boundary plasma simulations is developed, tested on synthetic and experimental data, and validated for accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of inferring anomalous transport coefficients in boundary plasmas using fluid models, which is critical for understanding plasma behavior.

Method: Bayesian optimization with parallelized sampling and integrated uncertainty quantification is used to infer transport coefficients by maximizing their posterior probability distribution.

Result: The workflow successfully infers diffusivity and its uncertainty, matching 1-D measurements with 2-D profiles in H-, L-, and I-mode discharges.

Conclusion: The method is effective and scalable, with future work aimed at incorporating more complex models and analyzing larger experimental datasets.

Abstract: The critical task of inferring anomalous cross-field transport coefficients
is addressed in simulations of boundary plasmas with fluid models. A workflow
for parameter inference in the UEDGE fluid code is developed using Bayesian
optimization with parallelized sampling and integrated uncertainty
quantification. In this workflow, transport coefficients are inferred by
maximizing their posterior probability distribution, which is generally
multidimensional and non-Gaussian. Uncertainty quantification is integrated
throughout the optimization within the Bayesian framework that combines
diagnostic uncertainties and model limitations. As a concrete example, we infer
the anomalous electron thermal diffusivity $\chi_\perp$ from an interpretive
2-D model describing electron heat transport in the conduction-limited region
with radiative power loss. The workflow is first benchmarked against synthetic
data and then tested on H-, L-, and I-mode discharges to match their midplane
temperature and divertor heat flux profiles. We demonstrate that the workflow
efficiently infers diffusivity and its associated uncertainty, generating 2-D
profiles that match 1-D measurements. Future efforts will focus on
incorporating more complicated fluid models and analyzing transport
coefficients inferred from a large database of experimental results.

</details>


### [27] [Absolute constraints on the magnetic field evolution in tokamak power plants](https://arxiv.org/abs/2507.05456)
*Allen H Boozer*

Main category: physics.plasm-ph

TL;DR: The paper discusses constraints in tokamak power plants, focusing on pulse length, downtime between pulses, and current profile evolution, using a magnetic field evolution equation derived from Faraday's Law. It emphasizes the need for detailed simulations to validate feasibility and avoid limitations, with examples from CFS and Tokamak Energy projects.


<details>
  <summary>Details</summary>
Motivation: The study aims to address critical operational constraints in tokamak power plants, such as pulse duration and disruptive states, to ensure feasibility and efficiency in fusion power generation.

Method: The paper uses a simple evolution equation for the magnetic field derived from Faraday's Law to analyze constraints and validate detailed simulations.

Result: The evolution equation highlights methods to avoid operational limitations, with ongoing projects like SPARC and STEP aiming to test these findings.

Conclusion: Feasible simulations and experimental prototypes like SPARC and STEP are crucial for understanding and mitigating constraints in tokamak power plants.

Abstract: Issues of tokamak power plants, such as maximum possible length of a pulse of
fusion power, the required time between fusion pulses, and the evolution of the
current profile towards disruptive states, are tightly constrained by a simple
evolution equation for the magnetic field. This equation follows from Faraday's
Law and the mathematical relation between the magnetic and the electric fields.
The validity of detailed simulations is tested by consistency with Faraday's
Law, not the other way around. The evolution equation suggests methods of
avoiding the limitations of these constraints. Commonwealth Fusion Systems
(CFS) is proceeding on a fast timescale to operate a fusion power plant -- even
writing contracts on the power to be produced by their ARC tokamak. This makes
it important to determine the issues that should be studied by SPARC, which is
the tokamak that CFS is building to address issues before ARC is built. The
STEP prototype of a spherical tokamak power plant of Tokamak Energy is on a
similar fast timescale. Feasible simulations from startup to shutdown for
prototype plasmas could greatly clarify these constraints and the feasibility
of their avoidance.

</details>


### [28] [Influence of Conical Wire Array Geometry on Flow and Temperature Profiles Measured via Thomson Scattering and Optical Techniques](https://arxiv.org/abs/2507.05652)
*Luisa Izquierdo,Felipe Veloso,Miguel Escalona,Vicente Valenzuela-Villaseca,Gonzalo Avaria,Julio Valenzuela*

Main category: physics.plasm-ph

TL;DR: Study of conical wire arrays' opening angles on plasma jet properties reveals geometry affects jet velocity but not electron/ion temperatures or density profiles.


<details>
  <summary>Details</summary>
Motivation: To investigate how conical wire array geometry influences plasma jet properties under high-current conditions.

Method: Used conical wire arrays with varying opening angles on a 400kA generator, analyzed jets with moiré schlieren deflectometry, visible spectroscopy, and optical Thomson scattering.

Result: Electron temperatures (8-17 eV) increase axially, ion temperatures decrease (35-20 eV), and electron density peaks at ~4×10¹⁸ cm⁻³. Cone angle affects jet velocity (125 km/s for 40°, 98 km/s for 20°).

Conclusion: Cone geometry controls jet velocity but not other plasma properties, offering a reproducible method for velocity manipulation.

Abstract: Conical wire arrays with different opening angles are used as load of a
400kA, 1kA/ns generator. The differences in opening angle allow the study of
the influence of the array geometry on the jet properties. The characterization
of the jets is performed using a combination of advanced diagnostic techniques,
including moir\'e schlieren deflectometry, visible self-emission spectroscopy,
and optical Thomson scattering. The results reveal that, under the experimental
conditions, the plasma jets exhibit electron temperatures ranging from $8$ to
$17$ eV, increasing along the axial direction. In contrast, the ion temperature
decreases from approximately $35$ eV near the base of the jet to about $20$ eV
at higher axial positions. The electron density profile peaks at $\sim 4 \times
10^{18}$ cm$^{-3}$ in the central lower region of the jet and decreases with
height exponentially with a characteristic lenght $L_n = $2.86 mm. This
behavior is reproducible and independent of the conical array geometry.
However, the cone opening angle significantly affect the jet propagation
velocity, with larger opening angles producing higher axial velocities
($V_{\phi=40^\circ} \approx 125\pm3$ km/s, $V_{\phi=20^\circ} \approx 98\pm5$
km/s), demonstrating that the cone geometry provides effective control over the
jet propagation velocity.

</details>


### [29] [Analysis of RF Sheath-Driven Tungsten Erosion at RF Antenna in the WEST Tokamak](https://arxiv.org/abs/2507.06059)
*A. Kumar,W. Tierens,T. Younkin,C. Johnson,C. Klepper,A. Diaw,J. Lore,A. Grosjean,G. Urbanczyk,J. Hillairet,P. Tamain,L. Colas,C. Guillemaut,D. Current,S. Shiraiwa,N. Bertelli,the WEST Team*

Main category: physics.plasm-ph

TL;DR: The study uses the STRIPE framework to analyze tungsten erosion in the WEST tokamak, revealing RF sheath effects and predicting erosion patterns with reasonable accuracy.


<details>
  <summary>Details</summary>
Motivation: To understand and predict tungsten erosion at RF antenna structures in tokamaks, particularly under ohmic and ICRH conditions, for improved reactor-scale antenna design.

Method: STRIPE integrates SolEdge3x, COMSOL, RustBCA, and GITR to model plasma backgrounds, RF sheath potentials, sputtering yields, and impurity transport.

Result: The framework predicts a 30-fold increase in tungsten erosion from ohmic to ICRH phases, with observed poloidal and toroidal asymmetries due to RF sheath effects.

Conclusion: STRIPE demonstrates predictive capability and is suitable for reactor-scale antenna design, though sensitivity to sheath geometry and plasma resolution requires further refinement.

Abstract: This study applies the newly developed STRIPE (Simulated Transport of RF
Impurity Production and Emission) framework to interpret tungsten (W) erosion
at RF antenna structures in the WEST tokamak. STRIPE integrates SolEdge3x for
edge plasma backgrounds, COMSOL for 3D RF sheath potentials, RustBCA for
sputtering yields, and GITR for impurity transport and ion energy-angle
distributions. In contrast to prior work by Kumar et al. 2025 Nucl. Fusion 65,
076039, which focused on framework validation for WEST ICRH discharge 57877,
the present study provides a spatially resolved analysis of gross W erosion at
both Q2 antenna limiters under ohmic and ICRH conditions. Using 2D SolEdge3x
profiles in COMSOL, STRIPE captures rectified sheath potentials exceeding 300
V, leading to strong upper-limiter localization. Both poloidal and toroidal
asymmetries are observed and attributed to RF sheath effects, with modeled
erosion patterns deviating from experiment - highlighting sensitivity to sheath
geometry and plasma resolution. High-charge-state oxygen ions (O6+-O8+)
dominate erosion, while D+ contributes negligibly. A plasma composition of 1
percent oxygen and 98 percent deuterium is assumed. STRIPE predicts a 30-fold
increase in gross W erosion from ohmic to ICRH phases, consistent with W-I
400.9 nm brightness measurements. Agreement within 5 percent (ohmic) and 30
percent (ICRH) demonstrates predictive capability and supports STRIPE's
application in reactor-scale antenna design.

</details>


### [30] [Simulation of surface x-ray emission from the ASTERICS ECR ion source](https://arxiv.org/abs/2507.06074)
*Thomas Thuillier,Andrea Cernuschi,Benjamin Cheymol*

Main category: physics.plasm-ph

TL;DR: The study investigates bremsstrahlung x-ray emission from plasma electrons in the ASTERICS ion source, analyzing electron behavior, x-ray directionality, and shielding solutions to reduce radiation dose.


<details>
  <summary>Details</summary>
Motivation: Understanding the x-ray emission and electron behavior in the ASTERICS ion source to improve shielding and safety measures.

Method: Simulation using two codes to study electron distribution, x-ray emission, and dose mapping, followed by shielding design and parametric temperature analysis.

Result: Electrons exhibit anisotropic high-energy tails, broad angular impact, and significant x-ray emission. Shielding reduces dose to safe levels. A 380 keV electron temperature matches experimental heat deposition.

Conclusion: The study provides insights into electron behavior and x-ray emission, offering effective shielding solutions and highlighting the need to explain high electron temperatures.

Abstract: The bremsstrahlung x-ray emission induced by the impact of plasma electrons
de-confined on the chamber wall of the ASTERICS electron cyclotron resonance
ion source is investigated through a suite of two simulation codes. The
electron high energy temperature distribution tail at the wall is found to be
anisotropic and increases with Bmin. The electrons impinge the walls with broad
angular distribution peaking at angles ranging between 5-25{\deg} with respect
to the surface, which has consequences on the x-ray emission directionality and
on the yield of electrons bouncing back toward the plasma, reaching up to 50%.
The x-ray dose is mapped inside and around the ion source for Bmin = 0.8 T and
an electron temperature artificially increased to 120 keV to dimension with
margin the cave shielding. The dose without shielding reaches 100 $\mu$Sv/h per
kW of impacting electrons at 5 m. A set of internal and external shielding is
presented to attenuate this dose and reduce it to less than 1 $\mu$Sv/h per kW
of electrons. A parametric electron distribution temperature study with Fluka
indicates that the deposition of 1 W of heat in the superconducting cold mass
per kW of plasma electrons, as reported experimentally, is obtained when the
temperature is set to 380 keV. The mechanism required to provide such a high
electron temperature is unclear, but previous experiments on several ion
sources reported an x-ray spectral temperature 3 to 4 times higher radially,
compatible with this simulation result.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [31] [A Generalized $\ell_1$-Merit Function SQP Method Using Function Approximations with Tunable Accuracy](https://arxiv.org/abs/2507.06199)
*Dane S. Grundvig,Matthias Heinkenschloss*

Main category: math.OC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper develops a generalization of the line-search sequential quadratic
programming (SQP) algorithm with $\ell_1$-merit function that uses objective
and constraint function approximations with tunable accuracy to solve smooth
equality-constrained optimization problems. The evaluation of objective and
constraint functions and their gradients is potentially computationally
expensive, but it is assumed that one can construct effective, computationally
inexpensive models of these functions. This paper specifies how these models
can be used to generate new iterates. At each iteration, the models have to
satisfy function error and relative gradient error tolerances determined by the
algorithm based on its progress. Moreover, bounds for the model errors are used
to explore regions where the combined objective function and constraint models
are sufficiently accurate. The algorithm has the same first-order global
convergence properties as a line-search SQP algorithm with $\ell_1$-merit
function, but only uses objective and constraint function models and the model
error bounds. The algorithm is applied to a discretized boundary control
problem in which the evaluation of the objective and constraint functions
requires the solution of the Boussinesq partial differential equation (PDE).
The models are constructed from projection-based reduced-order models of the
Boussinesq PDE.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [32] [On void formation during the simulated tensile testing of polymer-filler particle composites](https://arxiv.org/abs/2507.05547)
*John J. Karnes,Supun S. Mohottalalage,Amitesh Maiti,Andrew P. Saab,Todd H. Weisgraber*

Main category: cond-mat.soft

TL;DR: Simulations study how filler particles (FP) affect polymer composites, focusing on void formation during tensile testing and its dependence on interaction potentials.


<details>
  <summary>Details</summary>
Motivation: To understand how varying formulation parameters (FP radius, volume fraction, temperature, interaction potentials) influences mechanical properties and void formation in polymer composites.

Method: Molecular dynamics simulations with a coarse-grained, bead-spring force field, varying parameters and analyzing uniaxial extension and void formation.

Result: Mechanical reinforcement or weakening depends on interaction potentials, with void morphology linked to these potentials.

Conclusion: Findings provide insights for designing polymer composites by tailoring interaction potentials to control mechanical properties and void formation.

Abstract: We simulate a series of model polymer composites, composed of linear polymer
strands and spherical, monodisperse filler particles (FP). These molecular
dynamics simulations implement a coarse-grained, bead-spring force field and we
vary several formulation parameters to study their respective influences on
material properties. These parameters include FP radius, FP volume fraction,
temperature, and the polymer-polymer, FP-FP, and polymer-FP interaction
potentials. Uniaxial extension of the simulation cells allows direct comparison
of mechanical reinforcement (or weakening) provided by the FP. We focus on the
formation of microscopic voids during simulated tensile testing of glassy
polymer composites and quantify how the characteristic spatial and
morphological arrangement of these voids is a function of interaction
potentials used in the simulation. We discuss the implications of these
findings in the context of polymer composite design and formulation.

</details>


### [33] [Bridging Machine Learning and Glassy Dynamics Theory for Predictive Polymer Modeling](https://arxiv.org/abs/2507.06194)
*Anh D. Phan,Ngo T. Que,Nguyen T. T. Duyen,Phan Thanh Viet,Quach K. Quang,Baicheng Mei*

Main category: cond-mat.soft

TL;DR: A machine learning-based approach simplifies thermal mapping for predicting polymer glassy dynamics, enhancing computational efficiency and applicability, especially for polymers with limited data.


<details>
  <summary>Details</summary>
Motivation: The challenge of predicting glassy dynamics in polymers, especially for chemically complex or new designs, due to the need for detailed thermodynamic data.

Method: Integrates machine learning-predicted Tg with a simplified thermal mapping using an effective thermal expansion coefficient.

Result: Quantitatively accurate predictions of relaxation dynamics across diverse polymers, resolving discrepancies in low-Tg polymers.

Conclusion: The approach offers a generalizable, efficient method for predictive modeling, aiding theory-guided materials discovery.

Abstract: Understanding and predicting the glassy dynamics of polymers remain
fundamental challenges in soft matter physics. While the Elastically Collective
Nonlinear Langevin Equation (ECNLE) theory has been successful in describing
relaxation dynamics, its practical application to polymers depends on a thermal
mapping to connect theory with experiment, which in turn requires detailed
thermodynamic data. Such data may not be available for chemically complex or
newly designed polymers. In this work, we propose a simple approach that
integrates machine learning-predicted glass transition temperatures (Tg) with a
simplified thermal mapping based on an effective thermal expansion coefficient
to overcome these limitations. This approach can provide quantitatively
accurate predictions of relaxation dynamics across a broad range of polymers.
Rather than replacing the original thermal mapping, our method complements it
by trading formal rigor for computational efficiency and broader applicability
in high-throughput screening and materials with limited available data.
Moreover, we introduce a physically motivated modification to the thermal
mapping that resolves discrepancies in the description of low-Tg polymers. Our
results establish a generalizable approach for predictive modeling of glassy
polymer dynamics and point toward new directions for theory-guided materials
discovery.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [34] [MBFormer: A General Transformer-based Learning Paradigm for Many-body Interactions in Real Materials](https://arxiv.org/abs/2507.05480)
*Bowen Hou,Xian Xu,Jinyuan Wu,Diana Y. Qiu*

Main category: cond-mat.mtrl-sci

TL;DR: MBFormer, a transformer-based model, predicts many-body quantum properties from mean-field inputs, achieving low error (0.1-0.2 eV) on quasiparticle and exciton energies.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of solving the quantum many-body problem for excited-state properties in materials science.

Method: Propose MBFormer, a symmetry-aware, grid-free transformer model using attention to capture many-body correlations from mean-field inputs.

Result: Achieves state-of-the-art performance with MAE of 0.1-0.2 eV on GW-BSE predictions for 2D materials.

Conclusion: MBFormer provides an end-to-end platform for many-body predictions, serving as a foundation model in computational materials science.

Abstract: Recently, radical progress in machine learning (ML) has revolutionized
computational materials science, enabling unprecedentedly rapid materials
discovery and property prediction, but the quantum many-body problem -- which
is the key to understanding excited-state properties, ranging from transport to
optics -- remains challenging due to the complexity of the nonlocal and
energy-dependent interactions. Here, we propose a symmetry-aware, grid-free,
transformer-based model, MBFormer, that is designed to learn the entire
many-body hierarchy directly from mean-field inputs, exploiting the attention
mechanism to accurately capture many-body correlations between mean-field
states. As proof of principle, we demonstrate the capability of MBFormer in
predicting results based on the GW plus Bethe Salpeter equation (GW-BSE)
formalism, including quasiparticle energies, exciton energies, exciton
oscillator strengths, and exciton wavefunction distribution. Our model is
trained on a dataset of 721 two-dimensional materials from the C2DB database,
achieving state-of-the-art performance with a low prediction mean absolute
error (MAE) on the order of 0.1-0.2 eV for state-level quasiparticle and
exciton energies across different materials. Moreover, we show explicitly that
the attention mechanism plays a crucial role in capturing many-body
correlations. Our framework provides an end-to-end platform from ground states
to general many-body prediction in real materials, which could serve as a
foundation model for computational materials science.

</details>


### [35] [MP-ALOE: An r2SCAN dataset for universal machine learning interatomic potentials](https://arxiv.org/abs/2507.05559)
*Matthew C. Kuner,Aaron D. Kaplan,Kristin A. Persson,Mark Asta,Daryl C. Chrzan*

Main category: cond-mat.mtrl-sci

TL;DR: MP-ALOE is a dataset of 1 million DFT calculations using r2SCAN, covering 89 elements, created via active learning. It benchmarks well for ML potentials in predicting thermochemical properties, forces, and stability under extreme conditions.


<details>
  <summary>Details</summary>
Motivation: To provide a large, accurate dataset (MP-ALOE) for training machine learning interatomic potentials, addressing the need for off-equilibrium structures and diverse elemental coverage.

Method: Active learning was used to create MP-ALOE, a dataset of DFT calculations with r2SCAN. A machine learning potential was trained and tested on benchmarks like thermochemical properties, forces, and extreme conditions.

Result: MP-ALOE performs well across benchmarks, including predicting equilibrium and off-equilibrium properties, and maintaining stability under extreme deformations and conditions.

Conclusion: MP-ALOE is a robust, publicly available dataset for training ML potentials, demonstrating strong performance in diverse scenarios.

Abstract: We present MP-ALOE, a dataset of nearly 1 million DFT calculations using the
accurate r2SCAN meta-generalized gradient approximation. Covering 89 elements,
MP-ALOE was created using active learning and primarily consists of
off-equilibrium structures. We benchmark a machine learning interatomic
potential trained on MP-ALOE, and evaluate its performance on a series of
benchmarks, including predicting the thermochemical properties of equilibrium
structures; predicting forces of far-from-equilibrium structures; maintaining
physical soundness under static extreme deformations; and molecular dynamic
stability under extreme temperatures and pressures. MP-ALOE shows strong
performance on all of these benchmarks, and is made public for the broader
community to utilize.

</details>


### [36] [Uncovering coupled ionic-polaronic dynamics and interfacial enhancement in Li$_x$FePO$_4$](https://arxiv.org/abs/2507.05626)
*Fengyu Xie,Yuxiang Gao,Ruoyu Wang,Zhicheng Zhong*

Main category: cond-mat.mtrl-sci

TL;DR: The paper develops a machine-learned force field (MLFF) to study coupled ionic-polaronic dynamics in Li$_x$FePO$_4$, revealing fast polaron flips and their correlation with Li-ion configurations, especially at phase boundaries.


<details>
  <summary>Details</summary>
Motivation: Understanding coupled ionic-polaronic dynamics is key for optimizing battery materials, but it's challenging due to complex interactions between Li-ion configurations, polaron ordering, and lattice vibrations.

Method: A fine-tuned machine-learned force field (MLFF) is developed for Li$_x$FePO$_4$ to simulate coupled ion-polaron behavior.

Result: Simulations show picosecond-scale polaron flips, much faster than Li-ion migration, with strong correlation to Li configurations. Polaron fluctuations are enhanced at phase boundaries, suggesting interfacial electronic conduction.

Conclusion: Fine-tuned MLFFs can resolve complex coupled transport and provide insights into ionic-polaronic dynamics in battery cathodes.

Abstract: Understanding and controlling coupled ionic-polaronic dynamics is crucial for
optimizing electrochemical performance in battery materials. However, studying
such coupled dynamics remains challenging due to the intricate interplay
between Li-ion configurations, polaron charge ordering, and lattice vibrations.
Here, we develop a fine-tuned machine-learned force field (MLFF) for
Li$_x$FePO$_4$ that captures coupled ion-polaron behavior. Our simulations
reveal picosecond-scale polaron flips occurring orders of magnitude faster than
Li-ion migration, featuring strong correlation to Li configurations. Notably,
polaron charge fluctuations are further enhanced at Li-rich/Li-poor phase
boundaries, suggesting a potential interfacial electronic conduction mechanism.
These results demonstrate the capability of fine-tuned MLFFs to resolve complex
coupled transport and provide insight into emergent ionic-polaronic dynamics in
multivalent battery cathodes.

</details>


### [37] [Revisiting the configurations of hydrogen impurities in SrTiO3: Insights from first-principles local vibration mode calculations](https://arxiv.org/abs/2507.05752)
*Cai Zenghua,Ma Chunlan*

Main category: cond-mat.mtrl-sci

TL;DR: The study investigates hydrogen impurity configurations in SrTiO3 (STO) using first-principles calculations, identifying specific complexes that align with experimental infrared absorption bands.


<details>
  <summary>Details</summary>
Motivation: To clarify ambiguous configurations of hydrogen impurities in STO and understand their vibrational properties.

Method: First-principles local vibration mode calculations with hybrid exchange correlation functional (0.2 exact exchange fraction) to analyze Hi, Hi complexes, and cation vacancy complexes.

Result: Hi alone does not match dominant absorption bands; VSr-Hi and VSr-2Hi align with main bands, while VTi-2Hi corresponds to additional bands.

Conclusion: Hydrogen-related complexes are crucial for STO's electronic properties, and accurate exchange correlation functionals are essential for reliable predictions.

Abstract: The specific configurations of hydrogen impurities in SrTiO3 (STO) are still
ambiguous. In this study, we systematically investigate the configurations and
vibrational properties of hy-drogen impurities in cubic STO using
first-principles local vibration mode calculations. Em-ploying the appropriate
hybrid exchange correlation functional with the fraction of exact ex-change
setting to 0.2, we revisit the interstitial hydrogen (Hi), Hi complexes (2Hi),
and various intrinsic cation vacancy complexes with Hi, including VSr-Hi,
VSr-2Hi, VTi-Hi, and VTi-2Hi. Comparison of the computed vibrational
frequencies with experimental infrared absorption bands reveals that Hi, with a
frequency of 3277 cm-1, is unlikely to account for the dominant absorption
bands near 3500 cm-1. Instead, strontium vacancy complexes with interstitial
hydro-gen (VSr-Hi and VSr-2Hi) exhibit vibrational frequencies that align with
the main absorption bands, whereas titanium vacancy complex with two
interstitial hydrogen (VTi-2Hi) corresponds to additional absorption bands
around 3300 cm-1. These findings provide insights into the im-portance of
hydrogen-related complexes in governing the electronic properties of STO, and
meanwhile underscore the necessity of employing accurate exchange correlation
functionals for reliable theoretical predictions of vibrational properties.

</details>


### [38] [Reference compositions for bismuth telluride thermoelectric materials for low-temperature power generation](https://arxiv.org/abs/2507.06101)
*Nirma Kumari,Jaywan Chung,Seunghyun Oh,Jeongin Jang,Jongho Park,Ji Hui Son,SuDong Park,Byungki Ryu*

Main category: cond-mat.mtrl-sci

TL;DR: The paper focuses on developing reference BiTe-based thermoelectric materials using data-driven analysis to address reproducibility and reliability issues, aiming to standardize TE technology for industrial use.


<details>
  <summary>Details</summary>
Motivation: Despite the potential of thermoelectric technology, industrialization is limited due to reproducibility and reliability concerns with BiTe-based alloys. The study aims to establish reliable reference materials to accelerate industrial adoption.

Method: Data-driven analysis of Starrydata2 identified frequently studied BiTe compositions (Bi0.46Sb1.54Te3 and Bi2Te2.7Se0.3). These were synthesized using hot pressing and spark-plasma sintering, and their properties were evaluated.

Result: The synthesized materials matched the median of reported data, confirming their representativeness. A module using these materials achieved 2.51 W power output and 3.58% efficiency at a 120 K temperature difference.

Conclusion: The study proposes standardized BiTe-based reference materials to support TE technology's industrial integration, backed by transparent data and validated benchmarks.

Abstract: Thermoelectric (TE) technology enables direct heat-to-electricity conversion
and is gaining attention as a clean, fuel-saving, and carbon-neutral solution
for industrial, automotive, and marine applications. Despite nearly a century
of research, apart from successes in deep-space power sources and solid-state
cooling modules, the industrialization and commercialization of TE power
generation remain limited. Since the new millennium, nanostructured bulk
materials have accelerated the discovery of new TE systems. However, due to
limited access to high-temperature heat sources, energy harvesting still relies
almost exclusively on BiTe-based alloys, which are the only system operating
stably near room temperature. Although many BiTe-based compositions have been
proposed, concerns over reproducibility, reliability, and lifetime continue to
hinder industrial adoption. Here, we aim to develop reference BiTe-based
thermoelectric materials through data-driven analysis of Starrydata2, the
world's largest thermoelectric database. We identify Bi0.46Sb1.54Te3 and
Bi2Te2.7Se0.3 as the most frequently studied ternary compositions. These were
synthesized using hot pressing and spark-plasma sintering. Thermoelectric
properties were evaluated with respect to the processing method and measurement
direction. The results align closely with the median of reported data,
confirming the representativeness of the selected compositions. We propose
these as reference BiTe materials, accompanied by transparent data and
validated benchmarks. Their use can support the standardization of TE legs and
modules while accelerating performance evaluation and industrial integration.
We further estimated the performance of a thermoelectric module made from the
reference composition, which gives the power output of over 2.51 W and an
efficiency of 3.58% at a temperature difference of 120 K.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [39] [evortran: a modern Fortran package for genetic algorithms with applications from LHC data fitting to LISA signal reconstruction](https://arxiv.org/abs/2507.06082)
*Thomas Biekötter*

Main category: hep-ph

TL;DR: evortran is a high-performance Fortran library for genetic algorithms and evolutionary optimization, designed for flexibility, efficiency, and ease of use in physics and other applications.


<details>
  <summary>Details</summary>
Motivation: The library addresses the need for efficient and adaptable tools for derivative-free optimization, complex searches, and data fitting in high-energy physics and beyond.

Method: evortran provides customizable selection, crossover, mutation, and elitism strategies, supporting various abstraction levels from individual operations to full evolutionary cycles and population migration.

Result: The library demonstrates effectiveness in benchmark applications, including physics scenarios like Higgs sector analysis and gravitational wave reconstruction.

Conclusion: evortran is a versatile and powerful tool for evolutionary optimization, validated by its performance in realistic, data-driven applications.

Abstract: evortran is a modern Fortran library designed for high-performance genetic
algorithms and evolutionary optimization. evortran can be used to tackle a wide
range of problems in high-energy physics and beyond, such as derivative-free
parameter optimization, complex search taks, parameter scans and fitting
experimental data under the presence of instrumental noise. The library is
built as an fpm package with flexibility and efficiency in mind, while also
offering a simple installation process, user interface and integration into
existing Fortran programs. evortran offers a variety of selection, crossover,
mutation and elitism strategies, with which users can tailor an evolutionary
algorithm to their specific needs. evortran supports different abstraction
levels: from operating directly on individuals and populations, to running full
evolutionary cycles, and even enabling migration between independently evolving
populations to enhance convergence and maintain diversity. In this paper, we
present the functionality of the evortran library, demonstrate its capabilities
with example benchmark applications, and compare its performance with existing
genetic algorithm frameworks. As physics-motivated applications, we use
evortran to confront extended Higgs sectors with LHC data and to reconstruct
gravitational wave spectra and the underlying physical parameters from LISA
mock data, demonstrating its effectiveness in realistic, data-driven scenarios.

</details>


<div id='math.CA'></div>

# math.CA [[Back]](#toc)

### [40] [Approximate direct and inverse scattering for the AKNS system](https://arxiv.org/abs/2507.05525)
*Vladislav V. Kravchenko*

Main category: math.CA

TL;DR: The paper presents a new method for solving direct and inverse scattering problems for the AKNS system using power series representations of Jost solutions and recurrent integration procedures.


<details>
  <summary>Details</summary>
Motivation: To develop a simple and efficient numerical method for solving scattering problems in the AKNS system.

Method: Uses power series representations of Jost solutions in terms of a transformed spectral parameter, with recurrent integration for coefficients. Direct problem involves computing coefficients and locating zeros; inverse problem involves solving linear algebraic equations.

Result: The method is illustrated with numerical examples, showing effectiveness for both direct and inverse scattering problems.

Conclusion: The approach provides a straightforward and efficient numerical solution for AKNS scattering problems.

Abstract: We study the direct and inverse scattering problems for the AKNS
(Ablowitz-Kaup-Newell-Segur) system. New representations for the Jost solutions
are obtained in the form of the power series in terms of a transformed spectral
parameter. In terms of that parameter, the Jost solutions are convergent power
series in corresponding unit disks. For the coefficients of the series simple
recurrent integration procedures are devised. Solution of the direct scattering
problem reduces to computing the coefficients and locating zeros of
corresponding analytic functions in the interior of the unit disk. Solution of
the inverse scattering problem reduces to the solution of two systems of linear
algebraic equations for the power series coefficients, while the potentials are
recovered from the first coefficients. The overall approach leads to a simple
and efficient method for the numerical solution of both direct and inverse
scattering problems, which is illustrated by numerical examples.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [41] [Physics-Informed Graph Neural Networks to Reconstruct Local Fields Considering Finite Strain Hyperelasticity](https://arxiv.org/abs/2507.05291)
*Manuel Ricardo Guevara Garban,Yves Chemisky,Étienne Prulière,Michaël Clément*

Main category: cs.LG

TL;DR: P-DivGNN is a physics-informed GNN for reconstructing micro-scale stress fields using macro-scale data, achieving computational efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Accurate local stress field prediction is crucial for fracture analysis and fatigue criteria, but traditional methods like FE simulations are computationally expensive.

Method: Uses a graph representation of periodic micro-structures with a message-passing GNN, incorporating physical constraints and periodic boundary conditions.

Result: Achieves accurate stress field reconstruction and significant computational speed-ups, especially in non-linear hyperelastic cases.

Conclusion: P-DivGNN is efficient and accurate, making it suitable for large-scale applications.

Abstract: We propose a physics-informed machine learning framework called P-DivGNN to
reconstruct local stress fields at the micro-scale, in the context of
multi-scale simulation given a periodic micro-structure mesh and mean,
macro-scale, stress values. This method is based in representing a periodic
micro-structure as a graph, combined with a message passing graph neural
network. We are able to retrieve local stress field distributions, providing
average stress values produced by a mean field reduced order model (ROM) or
Finite Element (FE) simulation at the macro-scale. The prediction of local
stress fields are of utmost importance considering fracture analysis or the
definition of local fatigue criteria. Our model incorporates physical
constraints during training to constraint local stress field equilibrium state
and employs a periodic graph representation to enforce periodic boundary
conditions. The benefits of the proposed physics-informed GNN are evaluated
considering linear and non linear hyperelastic responses applied to varying
geometries. In the non-linear hyperelastic case, the proposed method achieves
significant computational speed-ups compared to FE simulation, making it
particularly attractive for large-scale applications.

</details>


### [42] [Aliasing in Convnets: A Frame-Theoretic Perspective](https://arxiv.org/abs/2507.06152)
*Daniel Haider,Vincent Lostanlen,Martin Ehler,Nicki Holighaus,Peter Balazs*

Main category: cs.LG

TL;DR: The paper analyzes aliasing in convolutional layers, proposes frame-theoretic methods to ensure Parseval stability, and derives efficient optimization objectives to suppress aliasing.


<details>
  <summary>Details</summary>
Motivation: Aliasing in convolutional layers affects numerical stability and generalization, but lacks a general analysis in the context of Parseval stability.

Method: A frame-theoretic approach is adapted for 1D kernels to characterize aliasing and derive stability bounds. Two efficient optimization objectives are proposed to suppress aliasing.

Result: Practical estimates for stability bounds and closed-form expressions for aliasing effects in random kernels are derived.

Conclusion: The study provides insights into aliasing behavior and offers practical methods to ensure Parseval stability in convolutional layers.

Abstract: Using a stride in a convolutional layer inherently introduces aliasing, which
has implications for numerical stability and statistical generalization. While
techniques such as the parametrizations via paraunitary systems have been used
to promote orthogonal convolution and thus ensure Parseval stability, a general
analysis of aliasing and its effects on the stability has not been done in this
context. In this article, we adapt a frame-theoretic approach to describe
aliasing in convolutional layers with 1D kernels, leading to practical
estimates for stability bounds and characterizations of Parseval stability,
that are tailored to take short kernel sizes into account. From this, we derive
two computationally very efficient optimization objectives that promote
Parseval stability via systematically suppressing aliasing. Finally, for layers
with random kernels, we derive closed-form expressions for the expected value
and variance of the terms that describe the aliasing effects, revealing
fundamental insights into the aliasing behavior at initialization.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [43] [Increasing Systemic Resilience to Socioeconomic Challenges: Modeling the Dynamics of Liquidity Flows and Systemic Risks Using Navier-Stokes Equations](https://arxiv.org/abs/2507.05287)
*Davit Gondauri*

Main category: econ.GN

TL;DR: The paper introduces a novel mathematical model based on Navier-Stokes equations to assess liquidity flows and systemic risks, outperforming traditional models like CAPM and VaR.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of traditional economic models in capturing real market fluctuations and extreme events, emphasizing the need for systemic resilience and liquidity management.

Method: Develops a model incorporating 13 macroeconomic parameters, using econometric testing, Fourier analysis, stochastic simulation, and AI-based calibration. Validated with Georgian data (2010-2024).

Result: The model effectively describes liquidity dynamics, systemic risk, and extreme scenarios, providing a robust tool for crisis prediction and policy planning.

Conclusion: The proposed model offers a significant advancement in quantitative assessment and forecasting of financial systems, with practical applications for policy and risk management.

Abstract: Modern economic systems face unprecedented socioeconomic challenges, making
systemic resilience and effective liquidity flow management essential.
Traditional models such as CAPM, VaR, and GARCH often fail to reflect real
market fluctuations and extreme events. This study develops and validates an
innovative mathematical model based on the Navier-Stokes equations, aimed at
the quantitative assessment, forecasting, and simulation of liquidity flows and
systemic risks. The model incorporates 13 macroeconomic and financial
parameters, including liquidity velocity, market pressure, internal stress,
stochastic fluctuations, and risk premiums, all based on real data and formally
included in the modified equation. The methodology employs econometric testing,
Fourier analysis, stochastic simulation, and AI-based calibration to enable
dynamic testing and forecasting. Simulation-based sensitivity analysis
evaluates the impact of parameter changes on financial balance. The model is
empirically tested using Georgian macroeconomic and financial data from
2010-2024, including GDP, inflation, the Gini index, CDS spreads, and LCR
metrics. Results show that the model effectively describes liquidity dynamics,
systemic risk, and extreme scenarios, while also offering a robust framework
for multifactorial analysis, crisis prediction, and countercyclical policy
planning.

</details>
