<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 14]
- [math.AP](#math.AP) [Total: 17]
- [physics.comp-ph](#physics.comp-ph) [Total: 3]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 5]
- [physics.data-an](#physics.data-an) [Total: 1]
- [quant-ph](#quant-ph) [Total: 3]
- [cs.DS](#cs.DS) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [math.OC](#math.OC) [Total: 3]
- [cs.LG](#cs.LG) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [physics.space-ph](#physics.space-ph) [Total: 2]
- [math.DG](#math.DG) [Total: 1]
- [math-ph](#math-ph) [Total: 1]
- [astro-ph.SR](#astro-ph.SR) [Total: 2]
- [astro-ph.HE](#astro-ph.HE) [Total: 2]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [State-based approach to the numerical solution of Dirichlet boundary optimal control problems for the Laplace equation](https://arxiv.org/abs/2507.11646)
*Ulrich Langer,Richard LÃ¶scher,Olaf Steinbach,Huidong Yang*

Main category: math.NA

TL;DR: The paper explores Dirichlet boundary control for the Laplace equation, focusing on control in $H^{1/2}(\partial \Omega)$ and its regularization. It derives optimal mesh size and regularization parameter relationships and includes numerical examples.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of Dirichlet boundary control in the Laplace equation, particularly in balancing control cost and approximation accuracy.

Method: Uses $H^{1/2}(\partial \Omega)$ norm for control cost and regularization, derives finite element error estimates, and considers box constraints.

Result: Establishes an optimal relationship between mesh size $h$ and regularization parameter $\varrho$, validated by numerical examples.

Conclusion: The study provides a framework for efficient solver design and demonstrates practical applicability through numerical results, including constrained cases.

Abstract: We investigate the Dirichlet boundary control of the Laplace equation,
considering the control in $H^{1/2}(\partial \Omega)$, which is the natural
space for Dirichlet data when the state belongs to $H^1(\Omega)$. The cost of
the control is measured in the $H^{1/2}(\partial \Omega)$ norm that also plays
the role of the regularization term. We discuss regularization and finite
element error estimates enabling us to derive an optimal relation between the
finite element mesh size $h$ and the regularization parameter $\varrho$,
balancing the energy cost for the control and the accuracy of the approximation
of the desired state. This relationship is also crucial in designing efficient
solvers. We also discuss additional box constraints imposed on the control and
the state. Our theoretical findings are complemented by numerical examples,
including one example with box constraints.

</details>


### [2] [Discontinuous Galerkin approximation for a Stokes-Brinkman-type formulation for the eigenvalue problem in porous media](https://arxiv.org/abs/2507.11695)
*Felipe Lepe,Gonzalo Rivera,Jesus Vellojin*

Main category: math.NA

TL;DR: A family of discontinuous Galerkin methods for approximating eigenvalues and eigenfunctions of Stokes-Brinkman problems is introduced, with stability, convergence, and error analysis provided.


<details>
  <summary>Details</summary>
Motivation: To develop and analyze discontinuous Galerkin methods for Stokes-Brinkman eigenvalue problems, focusing on stability and accuracy.

Method: Interior penalty strategy-based discontinuous Galerkin methods, analyzed using non-compact operators theory for convergence and error estimates.

Result: Stability of the discrete scheme is proven, and computational analysis shows the influence of stabilization parameters on spectrum approximation.

Conclusion: The method is effective for approximating Stokes-Brinkman eigenvalues, with stabilization parameters playing a key role in accuracy.

Abstract: We introduce a family of discontinuous Galerkin methods to approximate the
eigenvalues and eigenfunctions of a Stokes-Brinkman type of problem based in
the interior penalty strategy. Under the standard assumptions on the meshes and
a suitable norm, we prove the stability of the discrete scheme. Due to the
non-conforming nature of the method, we use the well-known non-compact
operators theory to derive convergence and error estimates for the method. We
present an exhaustive computational analysis where we compute the spectrum with
different stabilization parameters with the aim of study its influence when the
spectrum is approximated.

</details>


### [3] [Norm-Stabilized Imaginary-Time Evolution via Feedback Control](https://arxiv.org/abs/2507.11700)
*Stylianos Savva*

Main category: math.NA

TL;DR: A norm-stabilized ITE scheme for the 1D NLSE is introduced, eliminating the need for explicit renormalization by using an adaptive feedback term.


<details>
  <summary>Details</summary>
Motivation: Traditional ITE solvers require disruptive renormalization; this work aims to provide a more flexible and stable alternative.

Method: An adaptive feedback term mu(tau) is used to continuously stabilize the evolution, avoiding external normalization.

Result: The method effectively preserves norm and converges to soliton solutions, with demonstrated accuracy against analytical solutions.

Conclusion: The framework is extendable to higher dimensions, with future work planned for 2D/3D systems and multi-soliton scenarios.

Abstract: We present a norm-stabilized imaginary-time evolution (ITE) scheme for the
one-dimensional nonlinear Schrodinger equation (NLSE). Traditional ITE solvers
often require explicit renormalization of the wavefunction after each step to
preserve norm, which can be disruptive and algorithmically inflexible. We
propose an alternative approach in which the evolution is continuously
stabilized using an adaptive feedback term mu(tau), proportional to the time
derivative of the wavefunction norm. This results in a self-regulating flow
that requires no external normalization while preserving convergence toward
soliton solutions. We demonstrate the method's effectiveness by comparing the
final wavefunction profiles and L2 errors against analytical solutions and
baseline methods without feedback. Although this work focuses on the 1D case,
the framework is designed to extend naturally to higher dimensions. Future work
will explore the behavior of the feedback mechanism in 2D and 3D systems,
multi-soliton scenarios, and external potentials.

</details>


### [4] [Acceleration methods for fixed point iterations](https://arxiv.org/abs/2507.11746)
*Yousef Saad*

Main category: math.NA

TL;DR: The paper reviews acceleration techniques for improving the convergence of sequences in scientific computing, focusing on methods like Anderson Acceleration for fixed-point iterations.


<details>
  <summary>Details</summary>
Motivation: Slow convergence in iterative procedures motivates the need for acceleration techniques to efficiently reach limits in scientific computing.

Method: The paper discusses extrapolation methods (e.g., Aitken's Delta-squared) and fixed-point accelerators (e.g., Anderson Acceleration), utilizing iterates and mappings.

Result: Acceleration techniques, particularly fixed-point accelerators like Anderson Acceleration, have proven effective in physics and other fields.

Conclusion: The paper highlights the success of fixed-point accelerators, such as Anderson Acceleration, in improving convergence for iterative sequences.

Abstract: A pervasive approach in scientific computing is to express the solution to a
given problem as the limit of a sequence of vectors or other mathematical
objects. In many situations these sequences are generated by slowly converging
iterative procedures and this led practitioners to seek faster alternatives to
reach the limit. ``Acceleration techniques'' comprise a broad array of methods
specifically designed with this goal in mind. They started as a means of
improving the convergence of general scalar sequences by various forms of
``extrapolation to the limit'', i.e., by extrapolating the most recent iterates
to the limit via linear combinations. Extrapolation methods of this type, the
best known example of which is Aitken's Delta-squared process, require only the
sequence of vectors as input. However, limiting methods to only use the
iterates is too restrictive. Accelerating sequences generated by fixed-point
iterations by utilizing both the iterates and the fixed-point mapping itself
has proven highly successful across various areas of physics. A notable example
of these Fixed-Point accelerators (FP-Accelerators) is a method developed by D.
Anderson in 1965 and now widely known as Anderson Acceleration (AA).
Furthermore, Quasi-Newton and Inexact Newton methods can also be placed in this
category as well. This paper presents an overview of these methods -- with an
emphasis on those, such as AA, that are geared toward accelerating fixed point
iterations.

</details>


### [5] [A quasi-interpolation operator yielding fully computable error bounds](https://arxiv.org/abs/2507.11819)
*T. Chaumont-Frelet,M. Vohralik*

Main category: math.NA

TL;DR: A quasi-interpolation operator for Sobolev spaces is designed, offering computable approximation constants, optimal error estimates, and local definition without requiring additional regularity.


<details>
  <summary>Details</summary>
Motivation: To create a versatile interpolation operator for finite element subspaces that works under minimal assumptions and provides computable error bounds.

Method: The operator is constructed using potential reconstruction from a posteriori error analysis, defined locally in vertex patches, and supports arbitrary polynomial degrees and dimensions.

Result: The operator achieves optimal error estimates in both $H^1$ seminorm and $L^2$ norm, with sharp and stable certified overestimation factors in numerical tests.

Conclusion: The proposed quasi-interpolation operator is robust, efficient, and broadly applicable, providing reliable error control and convergence rates.

Abstract: We design a quasi-interpolation operator from the Sobolev space
$H^1_0(\Omega)$ to its finite-dimensional finite element subspace formed by
piecewise polynomials on a simplicial mesh with a computable approximation
constant. The operator 1) is defined on the entire $H^1_0(\Omega)$, no
additional regularity is needed; 2) allows for an arbitrary polynomial degree;
3) works in any space dimension; 4) is defined locally, in vertex patches of
mesh elements; 5) yields optimal estimates for both the $H^1$ seminorm and the
$L^2$ norm error; 6) gives a computable constant for both the $H^1$ seminorm
and the $L^2$ norm error; 7) leads to the equivalence of global-best and
local-best errors; 8) possesses the projection property. Its construction
follows the so-called potential reconstruction from a posteriori error
analysis. Numerical experiments illustrate that our quasi-interpolation
operator systematically gives the correct convergence rates in both the $H^1$
seminorm and the $L^2$ norm and its certified overestimation factor is rather
sharp and stable in all tested situations.

</details>


### [6] [Analysis of a fast fully discrete finite element method for fractional viscoelastic wave propagation](https://arxiv.org/abs/2507.11822)
*Hao Yuan,Xiaoping Xie*

Main category: math.NA

TL;DR: Numerical analysis of a fractional viscoelastic wave propagation model, extending fractional Maxwell and Zener models, with theoretical and numerical validation.


<details>
  <summary>Details</summary>
Motivation: To generalize fractional Maxwell and Zener models for wave propagation and analyze their numerical solutions.

Method: Convert the model to an integro-differential equation, prove solution properties, develop finite element schemes, and use SOE approximation for discretization.

Result: Existence, uniqueness, and regularity of solutions are established; error estimates for numerical schemes are derived.

Conclusion: Theoretical results are validated through numerical examples, confirming the effectiveness of the proposed methods.

Abstract: This paper is devoted to a numerical analysis of a fractional viscoelastic
wave propagation model that generalizes the fractional Maxwell model and the
fractional Zener model. First, we convert the model problem into a velocity
type integro-differential equation and establish existence, uniqueness and
regularity of its solution. Then we consider a conforming
linear/bilinear/trilinear finite element semi-discrete scheme and a fast scheme
of backward Euler full discretization with a sum-of-exponentials (SOE)
approximation for the convolution integral, and derive error estimates for the
semi-discrete and fully discrete schemes. Finally, we provide several numerical
examples to verify the theoretical results.

</details>


### [7] [Automatic reproducing kernel and regularization for learning convolution kernels](https://arxiv.org/abs/2507.11944)
*Haibo Li,Fei Lu*

Main category: math.NA

TL;DR: The paper introduces a data-adaptive RKHS (DA-RKHS) for learning convolution kernels, eliminating manual kernel selection and outperforming standard methods.


<details>
  <summary>Details</summary>
Motivation: The challenge of selecting proper reproducing kernels for learning convolution kernels in operators from data, especially with varying operators and data.

Method: Proposes DA-RKHS with automatic basis functions, developing Tikhonov, iterative, and hybrid algorithms for scalable solutions.

Result: Numerical experiments show DA-RKHS outperforms standard ridge regression and Gaussian process methods with preselected kernels.

Conclusion: DA-RKHS provides an effective, automatic regularization approach for kernel learning in convolution operators.

Abstract: Learning convolution kernels in operators from data arises in numerous
applications and represents an ill-posed inverse problem of broad interest.
With scant prior information, kernel methods offer a natural nonparametric
approach with regularization. However, a major challenge is to select a proper
reproducing kernel, especially as operators and data vary. We show that the
input data and convolution operator themselves induce an automatic,
data-adaptive RKHS (DA-RKHS), obviating manual kernel selection. In particular,
when the observation data is discrete and finite, there is a finite set of
automatic basis functions sufficient to represent the estimators in the
DA-RKHS, including the minimal-norm least-squares, Tikhonov, and
conjugate-gradient estimators. We develop both Tikhonov and scalable iterative
and hybrid algorithms using the automatic basis functions. Numerical
experiments on integral, nonlocal, and aggregation operators confirm that our
automatic RKHS regularization consistently outperforms standard ridge
regression and Gaussian process methods with preselected kernels.

</details>


### [8] [Structured First-Layer Initialization Pre-Training Techniques to Accelerate Training Process Based on $\varepsilon$-Rank](https://arxiv.org/abs/2507.11962)
*Tao Tang,Jiang Yang,Yuxiang Zhao,Quanhui Zhu*

Main category: math.NA

TL;DR: Proposes a structured first-layer initialization (SFLI) method to enhance neural feature diversity at initialization, improving training efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Slow feature representation formation in early training stages of deep neural networks for scientific computing.

Method: SFLI pre-training constructs Îµ-linearly independent neurons in the input layer, compatible with various activations and architectures.

Result: Significantly improves initial Îµ-rank, accelerates convergence, mitigates spectral bias, and enhances prediction accuracy.

Conclusion: SFLI is a simple, effective method to boost training performance with minimal code changes.

Abstract: Training deep neural networks for scientific computing remains
computationally expensive due to the slow formation of diverse feature
representations in early training stages. Recent studies identify a staircase
phenomenon in training dynamics, where loss decreases are closely correlated
with increases in $\varepsilon$-rank, reflecting the effective number of
linearly independent neuron functions. Motivated by this observation, this work
proposes a structured first-layer initialization (SFLI) pre-training method to
enhance the diversity of neural features at initialization by constructing
$\varepsilon$-linearly independent neurons in the input layer. We present
systematic initialization schemes compatible with various activation functions
and integrate the strategy into multiple neural architectures, including
modified multi-layer perceptrons and physics-informed residual adaptive
networks. Extensive numerical experiments on function approximation and PDE
benchmarks, demonstrate that SFLI significantly improves the initial
$\varepsilon$-rank, accelerates convergence, mitigates spectral bias, and
enhances prediction accuracy. With the help of SILP, we only need to add one
line of code to conventional existing algorithms.

</details>


### [9] [The Arrow-Hurwicz iteration for virtual element discretizations of the incompressible Navier-Stokes equations](https://arxiv.org/abs/2507.12036)
*Binbin Du,Shenxiang Cheng,Yue Yu,Chuanjun Chen*

Main category: math.NA

TL;DR: Analysis of Arrow-Hurwicz iteration for incompressible Navier-Stokes equations using a divergence-free mixed virtual element method, showing geometric convergence and mesh-independent contraction.


<details>
  <summary>Details</summary>
Motivation: To rigorously analyze and validate the convergence and performance of the Arrow-Hurwicz iteration method for solving incompressible Navier-Stokes equations.

Method: Divergence-free mixed virtual element method combined with Arrow-Hurwicz iteration.

Result: Geometric convergence with mesh-independent contraction factor, validated by numerical experiments.

Conclusion: The method is effective and computationally efficient for solving the incompressible Navier-Stokes equations.

Abstract: This article presents a detailed analysis of the Arrow-Hurwicz iteration
applied to the solution of the incompressible Navier-Stokes equations,
discretized by a divergence-free mixed virtual element method. Under a set of
appropriate assumptions, it is rigorously demonstrated that the method exhibits
geometric convergence, with a contraction factor that remains independent of
the mesh sizes. A series of numerical experiments are conducted to validate the
theoretical findings and to assess the computational performance of the
proposed method.

</details>


### [10] [A Hybrid High-Order method for the power-law Brinkman problem robust in all regimes](https://arxiv.org/abs/2507.12140)
*Daniel CastaÃ±Ã³n Quiroz,Daniele A. Di Pietro,JÃ©rÃ´me Droniou,Marwa Salah*

Main category: math.NA

TL;DR: A new Hybrid High-Order method for the Brinkman problem with power-law viscosity is proposed, supporting general meshes and arbitrary approximation orders, robust across all regimes.


<details>
  <summary>Details</summary>
Motivation: To address the Brinkman problem for fluids with power-law viscosity, ensuring robustness from pure Stokes to pure Darcy regimes.

Method: Hybrid High-Order method with general meshes and arbitrary approximation orders, using a dimensionless number to distinguish Stokes- and Darcy-dominated elements.

Result: Robust error estimates and pre-asymptotic convergence orders are achieved, validated by numerical experiments.

Conclusion: The method is effective and versatile, demonstrated by theoretical and numerical results.

Abstract: In this work we propose and analyze a new Hybrid High-Order method for the
Brinkman problem for fluids with power-law viscosity. The proposed method
supports general meshes and arbitrary approximation orders and is robust in all
regimes, from pure (power-law) Stokes to pure Darcy. Robustness is reflected by
error estimates that distinguish the contributions from Stokes- and
Darcy-dominated elements as identified by an appropriate dimensionless number,
and that additionally account for pre-asymptotic orders of convergence.
Theoretical results are illustrated by a complete panel of numerical
experiments.

</details>


### [11] [Optimal Spectral Approximation in the Overlaps for Generalized Finite Element Methods](https://arxiv.org/abs/2507.12226)
*Christian Alber,Peter Bastian,Moritz Hauck,Robert Scheichl*

Main category: math.NA

TL;DR: A generalized finite element method for solving elliptic PDEs with rough coefficients, using local eigenvalue problems on rings to reduce computational cost and improve efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the computational challenges of solving second-order elliptic PDEs with rough coefficients by reducing problem size and bandwidth of system matrices.

Method: Uses local approximation spaces derived from eigenvalue problems on rings around subdomain boundaries, reducing computational cost compared to solving on whole subdomains.

Result: Proves nearly exponential decay of local and overall approximation errors, and demonstrates effectiveness as a preconditioner with optimal convergence.

Conclusion: The method is efficient, reduces computational cost, and is robust for rough coefficients, supported by numerical experiments.

Abstract: In this paper, we study a generalized finite element method for solving
second-order elliptic partial differential equations with rough coefficients.
The method uses local approximation spaces computed by solving eigenvalue
problems on rings around the boundary of local subdomains. Compared to the
corresponding method that solves eigenvalue problems on the whole subdomains,
the problem size and the bandwidth of the resulting system matrices are
substantially reduced, resulting in faster spectral computations. We prove a
nearly exponential a priori decay result for the local approximation errors of
the proposed method, which implies the nearly exponential decay of the overall
approximation error of the method. The proposed method can also be used as a
preconditioner, and only a slight adaptation of our theory is necessary to
prove the optimal convergence of the preconditioned iteration. Numerical
experiments are presented to support the effectiveness of the proposed method
and to investigate its coefficient robustness.

</details>


### [12] [The iterated Golub-Kahan-Tikhonov method](https://arxiv.org/abs/2507.12307)
*Davide Bianchi,Marco Donatelli,Davide FurchÃ¬,Lothar Reichel*

Main category: math.NA

TL;DR: The paper introduces an iterated Golub-Kahan-Tikhonov method for solving ill-posed problems, offering better accuracy than non-iterated and Arnoldi-Tikhonov methods, with a new regularization parameter choice.


<details>
  <summary>Details</summary>
Motivation: To improve solution accuracy for large linear discrete ill-posed problems, especially when the matrix is far from symmetric.

Method: Discretizes an ill-posed operator equation, applies iterated Golub-Kahan-Tikhonov, and introduces a new regularization parameter selection.

Result: Produces more accurate solutions than standard non-iterated and Arnoldi-Tikhonov methods.

Conclusion: The iterated Golub-Kahan-Tikhonov method with the new parameter choice is superior for solving such problems.

Abstract: The Golub-Kahan-Tikhonov method is a popular solution technique for large
linear discrete ill-posed problems. This method first applies partial
Golub-Kahan bidiagonalization to reduce the size of the given problem and then
uses Tikhonov regularization to compute a meaningful approximate solution of
the reduced problem. It is well known that iterated variants of this method
often yield approximate solutions of higher quality than the standard
non-iterated method. Moreover, it produces more accurate computed solutions
than the Arnoldi method when the matrix that defines the linear discrete
ill-posed problem is far from symmetric.
  This paper starts with an ill-posed operator equation in infinite-dimensional
Hilbert space, discretizes the equation, and then applies the iterated
Golub-Kahan-Tikhonov method to the solution of the latter problem. An error
analysis that addresses all discretization and approximation errors is
provided. Additionally, a new approach for choosing the regularization
parameter is described. This solution scheme produces more accurate approximate
solutions than the standard (non-iterated) Golub-Kahan-Tikhonov method and the
iterated Arnoldi-Tikhonov method.

</details>


### [13] [A bound-preserving and conservative enriched Galerkin method for elliptic problems](https://arxiv.org/abs/2507.12338)
*Gabriel R. Barrenechea,Philip L. Lederer,Andreas Rupp*

Main category: math.NA

TL;DR: A locally conservative enriched Galerkin scheme is proposed to respect the discrete maximum principle for elliptic problems, using over-penalization and a splitting approach to avoid ill-conditioning.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of maintaining the discrete maximum principle in elliptic problems while ensuring optimal convergence.

Method: Uses over-penalization of jumps in the discrete solution and introduces a splitting approach to separate discontinuous and continuous solution parts.

Result: Proves existence of discrete solutions and provides optimal error estimates, validated numerically.

Conclusion: The scheme effectively balances conservation, convergence, and conditioning issues in elliptic problems.

Abstract: We propose a locally conservative enriched Galerkin scheme that respects the
discrete maximum principle of an elliptic problem. To this end, we use a
substantial over-penalization of the discrete solution's jumps to obtain
optimal convergence. To avoid the ill-conditioning issues that arise in
over-penalized schemes, we introduce an involved splitting approach that
separates the system of equations for the discontinuous solution part from the
system of equations for the continuous solution part, yielding well-behaved
subproblems. We prove the existence of discrete solutions and optimal error
estimates, which are validated numerically.

</details>


### [14] [Refinement of the theory and convergence of the Sinc convolution](https://arxiv.org/abs/2507.12406)
*Tomoaki Okayama*

Main category: math.NA

TL;DR: The paper refines the theory of Sinc convolution, resolves open problems, and improves its convergence rate by replacing the single-exponential transformation with a double-exponential one.


<details>
  <summary>Details</summary>
Motivation: To address unresolved theoretical issues in Sinc convolution and enhance its convergence rate.

Method: Refinement of Sinc convolution theory and replacement of the single-exponential transformation with a double-exponential transformation.

Result: Improved convergence rate compared to Stenger's original formula, validated theoretically and numerically.

Conclusion: The study successfully resolves theoretical gaps and enhances the efficiency of Sinc convolution.

Abstract: The Sinc convolution is an approximate formula for indefinite convolutions
proposed by F. Stenger. The formula was derived based on the Sinc indefinite
integration formula combined with the single-exponential transformation.
Although its efficiency has been confirmed in variety of areas, there remain
some open problems in its theory. The first contribution of this study is to
resolve those problems by refinement of the theory of the Sinc convolution.
This contribution includes a partial resolution of Stenger's conjecture. The
second contribution of this study is to improve the convergence rate by
replacement of the single-exponential transformation with the
double-exponential transformation. In both theoretical and numerical ways, this
study also shows that the convergence rate of the new formula is improved
compared to Stenger's formula.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [15] [A non-Gaussian Hardy-type Equation in Fractional Time](https://arxiv.org/abs/2507.11743)
*Soveny SolÃ­s,Vicente Vergara*

Main category: math.AP

TL;DR: Study of a non-Gaussian Hardy equation with Osgood-type non-linearity, incorporating fractional time derivatives for the first time. Existence of local/global solutions and blow-up results are established.


<details>
  <summary>Details</summary>
Motivation: To explore the behavior of solutions in a non-Gaussian Hardy equation with fractional time derivatives, addressing gaps in prior research.

Method: Combines properties of fundamental solutions with non-Gaussian process parameters, using the Banach contraction mapping principle.

Result: Optimal asymptotic estimates, existence of solutions, and blow-up results are derived. A critical exponent for solution existence is identified.

Conclusion: The study provides new insights into non-Gaussian Hardy equations with fractional derivatives, including solution existence and blow-up criteria.

Abstract: A non-Gaussian Hardy equation is studied with a non-linearity of Osgood-type
growth. A fractional derivative in time is incorporated for the first time in
an research of this type. Existence of local and global solutions are
established by combining properties of the fundamental solutions together with
the parameters of the non-Gaussian process, leading to optimal asymptotic
estimates. Additional properties of the fundamental solutions and instantaneous
blow-up results are found. The Banach contraction mapping principle is
particularly exploited. It is also defined a critical exponent for existence
and non-existence of solutions together with a judicious choice of the initial
data.

</details>


### [16] [Existence of nested polygonal vortex patches for the generalized SQG equation](https://arxiv.org/abs/2507.11748)
*Edison Cuba,Lucas C. F. Ferreira*

Main category: math.AP

TL;DR: The paper constructs co-rotating nested polygonal vortex patch solutions for the SQG and gSQG equations, providing insights into rotating structures in singular regimes.


<details>
  <summary>Details</summary>
Motivation: To understand the evolution of patch-type structures in the singular regime of the SQG and gSQG equations.

Method: Uses an implicit function argument to construct families of co-rotating nested polygonal vortex patch solutions under a nondegeneracy condition.

Result: Precise asymptotic descriptions of evolving patch boundaries, contributing to the understanding of coherent rotating structures.

Conclusion: The study advances knowledge of rotating structures in active scalar equations with singular velocity coupling.

Abstract: This paper investigates time-periodic solutions of both the surface
quasi-geostrophic (SQG) equation and its generalized form (gSQG) within the
more singular regime, focusing on the evolution of patch-type structures.
Assuming the underlying point vortex equilibrium satisfies a natural
nondegeneracy condition, we employ an implicit function argument to construct
families of co-rotating nested polygonal vortex patch solutions. These
configurations provide precise asymptotic descriptions of the geometry of the
evolving patch boundaries. Our results contribute to the broader understanding
of coherent rotating structures arising in active scalar equations with
singular velocity coupling.

</details>


### [17] [Initial traces and solvability of porous medium equation with power nonlinearity](https://arxiv.org/abs/2507.11826)
*Kazuhiro Ishige,Nobuhito Miyake,Ryuichi Sato*

Main category: math.AP

TL;DR: Study of qualitative properties and solvability conditions for the porous medium equation with power nonlinearity, focusing on initial traces and optimal singularities.


<details>
  <summary>Details</summary>
Motivation: To understand the conditions under which solutions to the porous medium equation with power nonlinearity exist, particularly regarding initial traces and data singularities.

Method: Analysis using uniformly local Morrey spaces and their variations to derive necessary and sufficient conditions for solvability.

Result: Established sharp sufficient conditions for solution existence and identified optimal singularities of initial data.

Conclusion: The study provides clear criteria for solvability and optimal initial data conditions for the Cauchy problem in the porous medium equation.

Abstract: In this paper we study qualitative properties of initial traces of solutions
to the porous medium equation with power nonlinearity, and obtain necessary
conditions for the existence of solutions to the corresponding Cauchy problem.
Furthermore, we establish sharp sufficient conditions for the existence of
solutions to the Cauchy problem using uniformly local Morrey spaces and their
variations, and identify the optimal singularities of the initial data for the
solvability of the Cauchy problem.

</details>


### [18] [Least total curvature solutions to steady Euler system and monotone solutions to semilinear equations in a strip](https://arxiv.org/abs/2507.11837)
*Changfeng Gui,David Ruiz,Chunjing Xie,Huan Xu*

Main category: math.AP

TL;DR: The paper establishes steady solutions (least total curvature solutions) for the incompressible Euler system in a strip, complementing known results. It uses a minimization method to find monotone heteroclinic solutions for a semilinear elliptic PDE, also constructing stable solutions with non-convex superlevel sets.


<details>
  <summary>Details</summary>
Motivation: To extend the understanding of steady solutions in the incompressible Euler system and address a generalized problem regarding semilinear elliptic PDEs.

Method: A minimization procedure is employed to identify monotone heteroclinic solutions for a semilinear elliptic PDE, enabling construction of stable solutions with non-convex superlevel sets.

Result: Existence of least total curvature solutions is proven, and stable solutions to semilinear elliptic PDEs with non-convex superlevel sets are constructed.

Conclusion: The work provides new steady solutions for the Euler system and answers a generalized problem negatively, demonstrating the method's effectiveness.

Abstract: This paper focuses on establishing the existence of a class of steady
solutions, termed least total curvature solutions, to the incompressible Euler
system in a strip. The solutions obtained in this paper complement the least
total curvature solutions already known. Our approach employs a minimization
procedure to identify a monotone heteroclinic solution for a conveniently
chosen semilinear elliptic PDE. This method also enables us to construct
positive and monotone (and consequently stable) solutions to semilinear
elliptic PDEs with non-convex superlevel sets in a strip domain. This can be
regarded as a negative answer to a generalized problem raised in [27].

</details>


### [19] [A 2-complex containing Sobolev spaces of matrix fields](https://arxiv.org/abs/2507.11869)
*Jay Gopalakrishnan,Kaibo Hu,Joachim SchÃ¶berl*

Main category: math.AP

TL;DR: The paper introduces new Sobolev spaces for matrix fields using 2-complexes, analyzing their interrelationships, weak derivatives, and stable decompositions, with applications in finite elements and variational formulations.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the need to understand weak second-order derivatives of matrix fields and their applications in finite element methods and variational problems.

Method: The paper generalizes complexes to 2-complexes, defines Sobolev spaces for matrix fields, and analyzes their properties, including stable decompositions and duality relationships.

Result: Stable decompositions of the Sobolev spaces are established, and duality relationships between spaces with and without boundary conditions are discovered.

Conclusion: The findings have implications for weak well-posed variational formulations and finite element methods, providing a foundation for further theoretical and applied research.

Abstract: Using a generalization of complexes, called 2-complexes, this paper defines
and analyzes new Sobolev spaces of matrix fields and their interrelationships
within a commuting diagram. These spaces have very weak second-order
derivatives. An example is the space of matrix fields of square-integrable
components whose row-wise divergence followed by yet another divergence
operation yield a function in a standard negative-order Sobolev space. Similar
spaces where the double divergence is replaced by a curl composed with
divergence, or a double curl operator (the incompatibility operator), are also
studied. Stable decompositions of such spaces in terms of more regular
component functions (which are continuous in natural norms) are established.
Appropriately ordering such Sobolev spaces with and without boundary conditions
(in a weak sense), we discover duality relationships between them. Motivation
to study such Sobolev spaces, from a finite element perspective and
implications for weak well-posed variational formulations are pointed out.

</details>


### [20] [On the convergence to the Navier-Stokes-Maxwell system with solenoidal Ohm's law](https://arxiv.org/abs/2507.11881)
*Zihua Guo,Zeng Zhang*

Main category: math.AP

TL;DR: The paper rigorously proves the asymptotic limit of the incompressible Navier-Stokes-Maxwell system with solenoidal Ohm's law as the momentum transfer coefficient tends to zero, using frequency envelope techniques.


<details>
  <summary>Details</summary>
Motivation: To establish the rigorous limit of the two-fluid incompressible Navier-Stokes-Maxwell system without losing regularity.

Method: Uses the idea of frequency envelope to analyze the system.

Result: Successfully proves the limit without regularity loss.

Conclusion: The frequency envelope method effectively captures the asymptotic behavior of the system.

Abstract: The incompressible Navier-Stokes-Maxwell system with solenoidal Ohm's law can
be viewed as as the asymptotic limit of the two-fluid incompressible
Navier-Stokes-Maxwell system as the momentum transfer coefficient tends to zero
(see [1], Ars\'enio, Ibrahim and Masmoudi, Arch. Ration. Mech. Anal., 2015). We
prove this limit rigorously without loss of regularity by using the idea of
frequency envelope.

</details>


### [21] [A curvature flow approach to dorsal closure modelling](https://arxiv.org/abs/2507.12088)
*Shuhui He,Ben Whale,Glen Wheeler,Valentina-Mira Wheeler*

Main category: math.AP

TL;DR: A curvature-based model for dorsal closure in drosophila embryos is proposed, with global existence and convergence proven. A numerical scheme is also presented and validated.


<details>
  <summary>Details</summary>
Motivation: To mathematically model and analyze dorsal closure in embryonic drosophila, a key biological process.

Method: Combines maximum-principle and integral-estimates for analysis, and introduces a numerical approximation scheme.

Result: Global existence and convergence are proven for initial data mimicking dorsal closure. Numerical simulations validate the model.

Conclusion: The model and numerical scheme effectively capture dorsal closure dynamics, with potential applications in developmental biology.

Abstract: In this paper we propose and study a curvature-based mathematical model for
dorsal closure in embryonic drosophila. Using an analysis that mixes
maximum-principle and integral-estimates, we establish global existence and
convergence for data that mimics the initial geometry of a dorsal closure
event. Further, we present a numerical approximation scheme for the flow,
establishing stability, consistence, and convergence. We also give sample
simulations of the flow with initial configurations that include experimentally
observed data.

</details>


### [22] [Forward and inverse problems for a mixed-type equation with the Caputo fractional derivative and Dezin-type non-local condition](https://arxiv.org/abs/2507.12129)
*Ravshan Ashurov,Umida Dusanova,Navbahor Nuraliyeva*

Main category: math.AP

TL;DR: The paper studies a mixed-type PDE with Caputo fractional derivative for t>0 and classical parabolic equation for t<0, analyzing forward and inverse problems with Dezin-type conditions.


<details>
  <summary>Details</summary>
Motivation: To investigate solvability and uniqueness of solutions for mixed-type PDEs with non-local boundary conditions, focusing on forward and inverse problems.

Method: Uses the Fourier method for the forward problem to establish existence and uniqueness, and analyzes solvability dependency on parameter Î». For the inverse problem, assumes separable right-hand side and proves uniqueness under constant sign condition.

Result: Existence and uniqueness of solutions are proven for both forward and inverse problems under specific conditions.

Conclusion: The study provides rigorous analysis of solvability for mixed-type PDEs with non-local conditions, contributing to understanding such equations in applied mathematics.

Abstract: This work is dedicated to the study of a mixed-type partial differential
equation involving a Caputo fractional derivative in the time domain $t > 0$
and a classical parabolic equation in the domain $t < 0$, along with Dezin-type
non-local boundary and gluing conditions. The forward and inverse problems are
studied in detail. For the forward problem, the existence and uniqueness of
solutions are established using the Fourier method, under appropriate
assumptions on the initial data and the right-hand side. We also analyze the
dependency of solvability on the parameter $\lambda$, from the Dezin-type
condition. For the inverse problem, where the right-hand side is separable as
$F(x,t) = f(x)g(t)$ (the unknown function is $f(x)$), the existence and
uniqueness of a solution are proven under a certain condition on the function
$g(t)$ (a constant sign is sufficient).

</details>


### [23] [Phase-field modelling of cohesive fracture. Part I: $Î$-convergence results](https://arxiv.org/abs/2507.12169)
*Roberto Alessi,Francesco Colasanto,Matteo Focardi*

Main category: math.AP

TL;DR: The paper establishes a cohesive phase-field model for fracture, proving Î-convergence in 1D, unifying various models and extending prior work.


<details>
  <summary>Details</summary>
Motivation: To create a unified framework for phase-field modeling of cohesive fracture, addressing gaps in existing literature.

Method: Proving Î-convergence in a 1D setting for a broad class of phase-field energies, extending prior results and validating other approaches.

Result: The model generalizes the Ambrosio-Tortorelli approximation, unifying variational fracture problems.

Conclusion: This work lays the foundation for a cohesive phase-field framework, with Parts II and III further developing and validating the theory.

Abstract: The main aim of this three-part work is to provide a unified consistent
framework for the phase-field modeling of cohesive fracture.
  In this first paper we establish the mathematical foundation of a cohesive
phase-field model by proving a $\Gamma$-convergence result in a one-dimensional
setting. Specifically, we consider a broad class of phase-field energies,
encompassing different models present in the literature, thereby both extending
the results in \cite{ContiFocardiIurlano2016} and providing an analytical
validation of all the other approaches. Additionally, by modifying the
functional scaling, we demonstrate that our formulation also generalizes the
Ambrosio-Tortorelli approximation for brittle fracture, therefore laying the
groundwork for a unified framework for variational fracture problems.
  The Part~II paper presents a systematic procedure for constructing
phase-field models that reproduce prescribed cohesive laws, whereas the
Part~III paper validates the theoretical results with applied examples.

</details>


### [24] [Phase-field modelling of cohesive fracture. Part II: Reconstruction of the cohesive law](https://arxiv.org/abs/2507.12172)
*Roberto Alessi,Francesco Colasanto,Matteo Focardi*

Main category: math.AP

TL;DR: A systematic method for constructing phase-field models to reproduce cohesive fracture laws, building on prior theoretical work.


<details>
  <summary>Details</summary>
Motivation: To provide a unified framework for phase-field modeling of cohesive fracture, extending theoretical foundations from earlier work.

Method: Systematic procedure involving selection of degradation functions or damage potentials to derive phase-field models with identical cohesive behavior but differing localized evolution.

Result: Multiple phase-field models tailored to specific cohesive responses, validated through examples.

Conclusion: The methodology offers a flexible and rigorous approach for cohesive fracture modeling, with further engineering validation in a subsequent paper.

Abstract: This is the second paper of a three-part work the main aim of which is to
provide a unified consistent framework for the phase-field modelling of
cohesive fracture. Building on the theoretical foundations of the first paper,
where {$\Gamma$-convergence} results have been derived, this second paper
presents a systematic procedure for constructing phase-field models that
reproduce prescribed cohesive laws. By either selecting the degradation
function and determining the damage potential or vice versa, we enable the
derivation of multiple phase-field models that exhibit the same cohesive
fracture behavior but differ in their localized phase-field evolution. This
methodology provides a flexible and rigorous strategy for tailoring phase-field
models to specific cohesive responses, as shown by the several examples worked
out. The mechanical responses associated with these examples, highlighting
their features and validating the theoretical results, are investigated in the
third paper from a more engineering-oriented and applied perspective.

</details>


### [25] [On a fractional semilinear Neumann problem arising in Chemotaxis](https://arxiv.org/abs/2507.12181)
*Eleonora Cinti,Matteo Talluri*

Main category: math.AP

TL;DR: The paper extends results for a fractional Laplacian problem in chemotaxis, proving existence and properties of non-constant solutions for small diffusion and constant solutions for large diffusion.


<details>
  <summary>Details</summary>
Motivation: To generalize findings for a fractional Laplacian problem in chemotaxis, specifically for any fractional power sâ(0,1), beyond the previously studied case of s=1/2.

Method: The study uses a semilinear and nonlocal Neumann problem, analyzing stationary configurations of the Keller-Segel model with nonlocal diffusion.

Result: Existence and qualitative properties of non-constant solutions are shown for small diffusion (Îµ), while solutions are necessarily constant for large Îµ.

Conclusion: The work successfully extends prior results to any fractional power sâ(0,1), providing insights into the behavior of solutions under varying diffusion parameters.

Abstract: We study a semilinear and nonlocal Neumann problem, which is the fractional
analogue of the problem considered by Lin--Ni--Takagi in the '80s. The model
under consideration arises in the description of stationary configurations of
the Keller--Segel model for chemotaxis, when a nonlocal diffusion for the
concentration of the chemical is considered. In particular, we extend to any
fractional power $s\in (0,1)$ of the Laplacian (with homogeneous Neumann
boundary conditions) the results obtained in [20] for $s=1/2$. We prove
existence and some qualitative properties of non--constant solutions when the
diffusion parameter $\varepsilon$ is small enough, and on the other hand, we
show that for $\varepsilon$ large enough any solution must be necessarily
constant.

</details>


### [26] [Optimal Hamilton-type gradient estimates for the heat equation on noncompact manifolds](https://arxiv.org/abs/2507.12190)
*Loth Damagui Chabi,Philippe Souplet*

Main category: math.AP

TL;DR: The paper presents improved localized and global noncompact versions of Hamilton's gradient estimate for heat equation solutions on Riemannian manifolds with Ricci curvature bounds.


<details>
  <summary>Details</summary>
Motivation: To enhance previous gradient estimates for heat equation solutions on manifolds with Ricci curvature constraints, aiming for optimality and broader applicability.

Method: Derives localized and global noncompact versions of Hamilton's gradient estimate, focusing on manifolds with Ricci curvature bounded below.

Result: Achieves essentially optimal estimates, improving prior work, and introduces a sharp local pseudo-Harnack inequality and spatial modulus of continuity estimates.

Conclusion: The new estimates significantly advance the field, offering sharper tools for analyzing heat equation solutions on such manifolds.

Abstract: We derive localized and global noncompact versions of Hamilton's gradient
estimate
  for positive solutions to the heat equation on Riemannian manifolds with
Ricci curvature bounded below.
  Our estimates are essentially optimal and significantly improve on all
previous estimates of this type.
  As applications, we derive a new
  and sharp, space only, local pseudo-Harnack inequality, as well as estimates
of the spatial modulus of continuity of solutions.

</details>


### [27] [Classification of entire and ancient solutions of the diffusive Hamilton-Jacobi equation](https://arxiv.org/abs/2507.12214)
*Loth Damagui Chabi,Philippe Souplet*

Main category: math.AP

TL;DR: The paper studies Liouville-type classification and symmetry properties for solutions to the diffusive Hamilton-Jacobi equation in full space and half-space, addressing long-standing open problems and providing optimal results.


<details>
  <summary>Details</summary>
Motivation: The research is motivated by the role of Liouville-type rigidity in nonlinear elliptic and parabolic problems, particularly in the context of the BCN conjecture about one-dimensionality of solutions in a half-space.

Method: The study employs integral estimates, a translation-compactness procedure, comparison arguments, and new Bernstein and Li-Yau type estimates.

Result: Key findings include: (1) ancient solutions in full space with sublinear growth are constant; (2) entire solutions in half-space for p>2 are stationary and one-dimensional; (3) nonstationary ancient solutions exist in half-space for p>1, but stationarity and one-dimensionality are recovered under polynomial growth restrictions.

Conclusion: The paper resolves long-standing problems, provides sharp classification results, and introduces new optimal estimates, advancing understanding of qualitative behavior in nonlinear PDEs.

Abstract: Consider the diffusive HJ eq. with Dirichlet conditions, which arises in
stochastic control as well as in KPZ type models of surface growth. It is known
that, for $p>2$ and suitably large, smooth initial data, the sol. undergoes
finite time gradient blowup on the boundary. On the other hand, Liouville type
rigidity or classif. ppties play a central role in the study of qualitative
behavior in nonlinear elliptic and parabolic problems, and notably appear in
the famous BCN conjecture about one-dimensionality of solutions in a
half-space. With this motivation, we study the Liouville type classif. and
symmetry ppties for entire and ancient sol. in $\R^n$ and in a half-space with
Dirichlet B.C.
  - First, we show that any ancient sol. in $\R^n$ with sublinear upper growth
at infinity is necessarily constant. This result is {\it optimal}, in view of
explicit examples and solves a long standing open problem.
  - Next we turn to the half-space problem for $p>2$ and we completely classify
entire solutions: any entire sol. is stationary and one-dimensional. The
assumption is sharp in view of explicit examples for $p=2$.
  - Then we show that the situation is also completely different for ancient
sol. in a half-space: there exist nonstationary ancient sol. for all $p>1$.
  Nevertheless, we show that any ancient sol. is necessarily positive, and that
stationarity and one-dimensionality are recovered provided a -- close to
optimal -- polynomial growth restriction is imposed on the sol.
  - In addition we establish new and optimal, local estimates of Bernstein and
Li-Yau type.
  The proofs of the Liouville and classif. results are delicate, based on
integral estimates, a translation-compactness procedure and comparison
arguments, combined with our Bernstein and Li-Yau type estimates.

</details>


### [28] [Mixed local and nonlocal laplacian without standard critical exponent for Lane-Emden equation](https://arxiv.org/abs/2507.12258)
*BegoÃ±a Barrios,Leandro M. Del Pezzo,Alexander Quaas*

Main category: math.AP

TL;DR: The paper studies a mixed elliptic equation combining local and nonlocal Laplacian operators with a power-type nonlinearity, showing existence of solutions for exponents near the critical local Sobolev exponent.


<details>
  <summary>Details</summary>
Motivation: To explore the behavior of solutions in a mixed elliptic equation involving both local and nonlocal operators, and to understand the absence of a traditional critical exponent due to their interaction.

Method: A Lyapunov-Schmidt type reduction method is used to establish the existence of solutions for exponents slightly below the critical local Sobolev exponent.

Result: Solutions exist for exponents close to but below the critical local Sobolev exponent, demonstrating the failure of traditional critical exponent duality for this mixed operator.

Conclusion: The mixed Lane-Emden-Fowler equation lacks a traditional critical exponent, highlighting the unique interplay between local and nonlocal operators.

Abstract: In this paper, we investigate a mixed elliptic equation involving both local
and nonlocal Laplacian operators, with a power-type nonlinearity. Specifically,
we consider a Lane-Emden type equation of the form \[-\Delta u + (-\Delta)^s u
= u^p,\quad\mbox{ in }\mathbb{R}^n.\] where the operator combines the classical
Laplacian and the fractional Laplacian. We establish the existence of solutions
for exponents slightly below the critical local Sobolev exponent, that is, for
$p < \frac{n+2}{n-2}$, with $p$ close to $\frac{n+2}{n-2}$.
  Our results show that, due to the interaction between the local and nonlocal
operators, this mixed Lane-Emden-Fowler equation does not admit a critical
exponent in the traditional sense. The existence proof is carried out using a
Lyapunov-Schmidt type reduction method and, as far as we know, provide the
first example of an elliptic operator for which the duality between critical
exponents fails.

</details>


### [29] [A non-linear damping structure and global stability of wave-Klein-Gordon coupled system in $\RR^{3+1}$](https://arxiv.org/abs/2507.12285)
*Yue Ma,Weidong Zhang*

Main category: math.AP

TL;DR: Global existence of solutions for wave-Klein-Gordon systems in 3+1D spacetime is proven under specific nonlinear constraints.


<details>
  <summary>Details</summary>
Motivation: To address the global solvability of coupled wave-Klein-Gordon systems with certain nonlinearities.

Method: Uses a bootstrap argument with hyperboloidal foliation and vector field methods.

Result: Constraints on nonlinear terms induce damping, ensuring global solution existence.

Conclusion: The study successfully proves global existence under specific conditions, leveraging damping effects.

Abstract: This paper establishes the global existence of solutions for a class of
wave-Klein-Gordon coupled systems with specific nonlinearities in
3+1-dimensional Minkowski spacetime. The study demonstrates that imposing
certain constraints on the coefficients of these specific nonlinear terms
induces a damping effect within the system, which is crucial for proving the
global existence of solutions. The proof is conducted within the framework of a
bootstrap argument, primarily employing the hyperboloidal foliation method and
the vector field method.

</details>


### [30] [Regularizing Effect for a Nonlocal Maxwell-SchrÃ¶dinger System](https://arxiv.org/abs/2507.12294)
*LuÃ­s Henrique de Miranda,Ayana Pinheiro de Castro Santana*

Main category: math.AP

TL;DR: Existence and regularity of weak solutions for a coupled PDE system with non-monotone terms are proven under certain conditions.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of solving a system of partial differential equations with non-monotone terms and prove regularity of solutions.

Method: Analyze the system using weak solutions, Sobolev spaces, and Lebesgue regularity under specific conditions on the functions involved.

Result: Existence and improved Sobolev and Lebesgue regularity of solutions are established.

Conclusion: The paper successfully demonstrates regularity for solutions of the given system under appropriate conditions.

Abstract: In this paper we prove existence and regularity of weak solutions for the
following system
  \begin{align*}
  \begin{cases}
  &-\mbox{div}\Bigg(\bigg(\|\nabla u\|^{p}_{L^{p}}+\|\nabla
v\|^{p}_{L^{p}}\bigg)|\nabla u|^{p-2}\nabla u\Bigg) + g(x,u,v)=f \ \ \
\mbox{in} \ \Omega;
  &-\mbox{div}\Bigg(\bigg(\|\nabla u\|^{p}_{L^{p}}+\|\nabla
v\|^{p}_{L^{p}}\bigg)|\nabla v|^{p-2}\nabla v\Bigg) = h(x,u,v) \ \ \ \
\mbox{in} \ \Omega;
  &u=v=0 \ \mbox{on} \ \partial\Omega.
  \end{cases}
  \end{align*}
  where $\Omega$ is an open bounded subset of $\mathbb{R}^N$, $N>2$, $f\in
L^m(\Omega)$, where $m>1$ and $g$, $h$ are two Carath\'eodory functions, which
may be non monotone. We prove that under appropriate conditions on $g$ and $h$,
there is gain of Sobolev and Lebesgue regularity for the solutions of this
system.

</details>


### [31] [Blow-up Solutions to Parabolic Differential Inequality with $p$-Laplacian on Locally Finite Graphs](https://arxiv.org/abs/2507.12303)
*Wenyuan Ma,Liang Zhao*

Main category: math.AP

TL;DR: Study of blow-up behavior in semilinear parabolic inequalities with p-Laplacian on graphs, showing existence under certain growth rates of the nonlinear source.


<details>
  <summary>Details</summary>
Motivation: To understand the conditions under which blow-up solutions exist for semilinear parabolic inequalities on graphs, extending classical PDE results to discrete settings.

Method: Extends the comparison principle and analyzes the relationship between initial values and blow-up solutions under varying growth rates of the nonlinear source.

Result: Proves that blow-up solutions exist when the nonlinear source's growth rate exceeds linear growth, given suitable initial conditions.

Conclusion: The findings generalize blow-up behavior to graph-based settings, providing insights into the role of nonlinearity and initial conditions.

Abstract: In this paper, we study blow up behavior of the semilinear parabolic
inequality with $p$-Laplacian operator and nonlinear source $u_t - \Delta_p u
\geq \sigma(x, t)f(u)$ on a locally finite connected weighted graph $G = (V,
E)$. We extend the comparison principle and thereby establish the relationship
between the initial value and the existence of blow-up solutions to the problem
under different growth rates of $f$. We prove that when the growth rate of $f$
exceeds linear growth, blow-up solutions exist under appropriate initial
conditions.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [32] [An improved argument principle root-search method for modes of slab waveguides, optical fibers, and spheres](https://arxiv.org/abs/2507.11752)
*S. Rao,P. Y. Chen,T. Grossinger,Y. Sivan*

Main category: physics.comp-ph

TL;DR: Updated root-search method for transcendental equations, globally convergent and capable of locating all complex roots in a specified domain, with extensions for slab waveguides, sphere resonances, and step-index fibers.


<details>
  <summary>Details</summary>
Motivation: To improve the reliability and applicability of root-search methods for transcendental equations, especially in challenging parameter regimes.

Method: Based on Cauchy's residue theorem, with enhancements like improved contour selection and non-dimensional search variables. Extended to identify leaky modes as propagation constant eigenvalue modes.

Result: Successfully locates all complex roots, reveals a discontinuity across the light line in dispersion plots for leaky modes.

Conclusion: The updated method is robust, reliable, and versatile, applicable to various problems including waveguide dispersion and resonance analysis.

Abstract: We update our root-search method for transcendental equations. Our method is
globally convergent and is guaranteed to locate all complex roots within a
specified search domain, since it is based on Cauchy's residue theorem. We
extend the implementation to treat the dispersion relations of slab waveguides
and the resonances of a sphere, in addition to step-index fibers. We also
implement other improvements, such as to the contour selection procedure and
using non-dimensional search variables, to ensure the method remains reliable
even in challenging parameter regimes. We also extend the algorithm to identify
leaky modes in terms of propagation constant eigenvalue modes, revealing, to
the first time to our knowledge, a discontinuity across the light line in the
dispersion plot.

</details>


### [33] [Fragment size density estimator for shrinkage-induced fracture based on a physics-informed neural network](https://arxiv.org/abs/2507.11799)
*Shin-ichi Ito*

Main category: physics.comp-ph

TL;DR: A neural network solver for integro-differential equations reduces computational costs by mapping inputs to probability density functions, validated for efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the high computational costs of numerically solving integro-differential equations for shrinkage-induced fragmentation.

Method: Uses a neural network to directly map input parameters to probability density functions, bypassing numerical solutions.

Result: Achieves computational efficiency and accuracy comparable to or better than finite difference schemes, validated on synthetic data.

Conclusion: Establishes a foundation for data-driven inverse analysis of fragmentation and suggests broader applicability beyond pre-specified models.

Abstract: This paper presents a neural network (NN)-based solver for an
integro-differential equation that models shrinkage-induced fragmentation. The
proposed method directly maps input parameters to the corresponding probability
density function without numerically solving the governing equation, thereby
significantly reducing computational costs. Specifically, it enables efficient
evaluation of the density function in Monte Carlo simulations while maintaining
accuracy comparable to or even exceeding that of conventional finite difference
schemes. Validatation on synthetic data demonstrates both the method's
computational efficiency and predictive reliability. This study establishes a
foundation for the data-driven inverse analysis of fragmentation and suggests
the potential for extending the framework beyond pre-specified model
structures.

</details>


### [34] [Modal Analysis of Multimode Waveguides Based on Large Step Size AdaMax from Far-Field Amplitudes](https://arxiv.org/abs/2507.12299)
*Jingtong Li,Dongting Huang,Minhui Xiong,Mingzhi Li*

Main category: physics.comp-ph

TL;DR: The paper introduces a method using the AdaMax optimizer for accurate and efficient modal analysis of multimode waveguides, overcoming limitations of current approaches.


<details>
  <summary>Details</summary>
Motivation: Current modal analysis methods are limited by low accuracy, poor adaptability, and high computational costs due to hardware and experimental constraints.

Method: The AdaMax optimizer with a large-step-size strategy is used under a power-normalization constraint to analyze modal power and relative-phase distributions from far-field amplitude measurements.

Result: The method achieves high accuracy and robustness under noise (SNRs 20-120 dB), works for rectangular and circular waveguides, and improves accuracy and computational cost over comparable methods.

Conclusion: This novel method offers a robust and efficient solution for modal analysis with broad application potential.

Abstract: Optimizing multimode waveguide performance depends on modal analysis;
however, current approaches focus predominantly on modal power distribution
and, limited by experimental hardware and conditions, exhibit low accuracy,
poor adaptability, and high computational cost. In this work, under a
power-normalization constraint, we employ the AdaMax optimizer with a
large-step-size strategy to perform modal analysis of multimode waveguides from
far-field amplitude measurements. Our method retrieves both the modal power
distribution and the modal relative-phase distribution, and we elucidate how
twin-image ambiguity limits the capability to analyze modal relative-phase
distributions. Experimental results demonstrate that the proposed method
performs well for both rectangular and circular waveguides, maintaining high
accuracy and robustness under noise with signal-to-noise ratios (SNRs) ranging
from 20 to 120 dB, and achieving substantial improvements in accuracy and
computational cost over comparable methods. This method provides a novel
solution for modal analysis with broad application potential.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [35] [Ultrafast ps--TALIF and streak camera diagnostics of atomic hydrogen in a helium microplasma jet](https://arxiv.org/abs/2507.11586)
*Dimitrios Stefas,Yanis Agha,Laurent Invernizzi,JoÃ£o Santos Sousa,Swaminathan Prasanna,Joachim Franzke,Charalambos Anastassiou,Guillaume Lombardi,Kristaq Gazeli*

Main category: physics.plasm-ph

TL;DR: The paper combines picosecond two-photon absorption laser induced fluorescence (ps-TALIF) with a streak camera to measure atomic hydrogen's effective lifetime and density in a helium microplasma jet, achieving high temporal resolution and sensitivity.


<details>
  <summary>Details</summary>
Motivation: To improve temporal resolution and sensitivity for probing atomic hydrogen in atmospheric-pressure microplasmas, addressing limitations of conventional nanosecond diagnostics.

Method: Uses ps-TALIF and a streak camera for high-resolution measurements, calibrates atomic densities with krypton gas, and assesses physical mechanisms like photoionization.

Result: Measured effective lifetimes as small as 50 ps and densities down to 10^14 cm^-3, with findings showing dependence on helium flow rate and axial distance.

Conclusion: The ps-TALIF and streak camera combination is highly effective for probing reactive species in microplasmas, though density uncertainties remain due to unmeasured cross-section ratios.

Abstract: Picosecond two-photon absorption laser induced fluorescence (ps--TALIF) is
combined with a streak camera ($\sim$1 ps highest temporal resolution) to probe
the effective lifetime ($\tau_{eff_{H(n=3)}}$) and absolute density ($N_H$) of
atomic hydrogen in the effluent of a helium atmospheric-pressure microplasma
jet ($\mu$APPJ). This approach allows for improved temporal resolution compared
to conventional nanosecond diagnostics, enabling measurements of
$\tau_{eff_{H(n=3)}}$ as small as 50 ps, the corresponding $N_H$ reaching down
to $10^{14} cm^{-3}$. Atomic densities are calibrated by performing identical
ps--TALIF measurements in krypton gas contained in a custom-built low-pressure
cuvette. Physical mechanisms that may be involved in the TALIF scheme such as
photoionisation and/or stimulated emission are also assessed, ensuring reliable
studies. The determined $\tau_{eff_{H(n=3)}}$ and $N_H$ increase with the
helium flow rate ($Q_{He}$) in the range $Q_{He}=0.3-1$ slm. Both quantities
are maximized near the exit nozzle of the $\mu$APPJ, obtaining values below 400
ps and $6x10^{14} cm^{-3}$, respectively. As the axial distance increases,
$\tau_{eff_{H(n=3)}}$ declines with a rate of $\approx$65 ps/mm for $Q_{He}=1$
slm which is about 3 times smaller than for $Q_{He}=0.3$ slm. These findings
reveal a strong correlation between the experimentally-measured
$\tau_{eff_{H(n=3)}}$ and the local air entrainment into the jet as indicated
by their comparison with calculated effective lifetimes based on published
quenching rates. Furthermore, operating the $\mu$APPJ in the burst mode allows
for the estimation of the residence time ($t_{res}$) of ground state H--atoms
in the helium gas channel, which is larger for $Q_{He}=0.3$ slm ($t_{res}
\approx 1.2 ms$) compared to $Q_{He}=1$ slm ($t_{res} \approx 0.5 ms$). H-atoms
consumption in the gas channel can be affected by diffusion and mechanisms
involving neutral ground-state and metastable species among others.
Furthermore, based on an error propagation analysis, density uncertainty as
high as 64% (depending on the operating condition) is revealed. This mainly
originates in the ratio of the two-photon absorption cross sections of Kr and H
atoms ( $\sigma^{(2)}Kr$ / $\sigma^{(2)}H$) which has not yet been measured in
the picosecond regime. Nevertheless, the combined utilisation of ps--TALIF and
streak camera diagnostics demonstrates high sensitivity and temporal resolution
for directly probing key reactive species in atmospheric pressure microplasmas.

</details>


### [36] [Analytic neutron wall loading from spin-polarized fusion in axisymmetric geometries](https://arxiv.org/abs/2507.11758)
*Jacob A. Schwartz*

Main category: physics.plasm-ph

TL;DR: Formulas for neutron wall load (NWL) in axisymmetric tori with spin-polarized nuclei are presented, aiding fusion machine design and optimization.


<details>
  <summary>Details</summary>
Motivation: To understand and optimize the effects of spin-polarization on neutron wall loading in fusion reactors.

Method: Derived formulas for NWL from filamentary ring sources with arbitrary fuel polarizations and magnetic field directions, excluding neutron scattering.

Result: Quickly evaluable, differentiable formulas enabling uniform NWL optimization for various polarization modes.

Conclusion: The formulas provide practical tools for fusion reactor design and optimization, particularly for spin-polarized fuel scenarios.

Abstract: Spin-polarized nuclei can be used to enhance the fusion reaction rate,
preferentially direct neutrons and alpha particles, or do some combination of
these. In this paper we present formulas for the neutron wall load (NWL) on the
walls of an axisymmetric torus with a convex cross section from filamentary
ring sources with arbitrary fuel polarizations and arbitrary local magnetic
field directions. While the NWL does not include neutron scattering, which
would require a more sophisticated treatment, these formulas can be evaluated
very quickly. This can help a fusion machine designer build intuition about the
effects of spin-polarization on neutron wall loading. The formulas are also
fully differentiable, so they could be included in a fusion systems
optimization code. As an example, we present sets of first wall shapes
optimized to have a uniform NWL for each of the main polarization modes.

</details>


### [37] [Scaling of thin wire cylindrical compression after 100 fs Joule surface heating with material, diameter and laser energy](https://arxiv.org/abs/2507.12109)
*L. Yang,M. -L. Herbert,C. BÃ¤htz,V. Bouffetier,E. Brambrink,T. Dornheim,N. Fefeu,T. Gawne,S. GÃ¶de,J. Hagemann,H. HÃ¶eppner,L. G. Huang,O. S. Humphries,T. Kluge,D. Kraus,J. LÃ¼tgert,J. -P. Naedler,M. Nakatsutsumi,A. Pelka,T. R. Preston,C. Qu,S. V. Rahul,R. Redmer,M. Rehwald,L. Randolph,J. J. Santos,M. Å mÃ­d,U. Schramm,J. -P. Schwinkendorf,M. Vescovi,U. Zastrau,K. Zeil,A. Laso Garcia,T. Toncian,T. E. Cowan*

Main category: physics.plasm-ph

TL;DR: Experimental validation of return-current-driven implosion scaling in micrometer-sized wires using XFEL imaging and simulations, revealing dependencies on wire diameter, material, and laser energy.


<details>
  <summary>Details</summary>
Motivation: To systematically validate and refine scaling laws for return-current-driven implosion in high-energy-density physics and inertial fusion research.

Method: Used XFEL-based imaging with sub-micrometer spatial and femtosecond temporal resolution, supported by hydrodynamic and particle-in-cell simulations.

Result: Identified precise dependencies of return current density on wire diameter, material, and laser energy, with deviations from simple theoretical predictions due to electron escape dynamics.

Conclusion: The study refines and confirms scaling laws, aiding predictive modeling in high-energy-density physics and inertial fusion research.

Abstract: We present the first systematic experimental validation of
return-current-driven implosion scaling in micrometer-sized wires irradiated by
femtosecond laser pulses. Employing XFEL-based imaging with sub-micrometer
spatial and femtosecond temporal resolution, supported by hydrodynamic and
particle-in-cell simulations, we reveal how return current density depends
precisely on wire diameter, material properties, and incident laser energy. We
identify deviations from simple theoretical predictions due to geometrically
influenced electron escape dynamics. These results refine and confirm the
scaling laws essential for predictive modeling in high-energy-density physics
and inertial fusion research.

</details>


### [38] [Gauss' Separation Algorithm for the Magnetic Field Decomposition of the SPARC PRD simulation](https://arxiv.org/abs/2507.12287)
*Gregorio L Trevisan,Ryan M Sweeney,Robert S Granetz*

Main category: physics.plasm-ph

TL;DR: Gauss' Separation Algorithm (GSA) is applied to magnetic field computations in fusion experiments, enabling decomposition of internal and external field contributions, with promising applications for plasma equilibrium studies.


<details>
  <summary>Details</summary>
Motivation: To improve the reconstruction of plasma equilibria by separating internal and external magnetic field contributions, especially during complex phases like ramp-ups in fusion experiments.

Method: Application of GSA to simulations of the SPARC Primary Reference Discharge, leveraging the Virtual-Casing Principle for field decomposition.

Result: Demonstrated successful separation of magnetic field contributions in all phases of the shot, including ramp-ups, without specific modeling assumptions.

Conclusion: GSA offers a promising tool for studying plasma behavior in fusion experiments, with potential to enhance accuracy and speed in equilibrium computations.

Abstract: A magnetic separation algorithm originally introduced by Gauss has been
revisited in recent years with application to magnetically-confined fusion
experiments. The main result offered by Gauss' Separation Algorithm~(GSA) is a
magnetic field computation that enables the decomposition of the contributions
due to sources internal and external to a bounded volume through the
generalized Virtual-Casing Principle. The present work applies GSA to
simulations of the SPARC Primary Reference Discharge and demonstrates its
separation capability on all phases of the shot, including the ramp-up during
which significant currents on the passive structures often complicate the
reconstruction of plasma equilibria. Such early results suggest a promising new
way to study ramp-ups, disruptions, and vertical displacement events without
any specific modeling assumption. Envisaged applications include reading
magnetic data from experimental diagnostics and producing a new set of signals
without external contributions to be used by downstream equilibrium codes for
improved accuracy or speed.

</details>


### [39] [Ultra-strong Quantum Squeezing Mediated by Plasma Waves](https://arxiv.org/abs/2507.12288)
*Kenan Qu,Nathaniel J. Fisch*

Main category: physics.plasm-ph

TL;DR: Quantum squeezing via plasma waves achieves ultra-strong squeezing with high pump intensities, enabling precision measurements beyond standard limits.


<details>
  <summary>Details</summary>
Motivation: Overcome the limitations of conventional solid-state media in achieving strong quantum squeezing due to ionization thresholds.

Method: Use plasma waves and two copropagating pump beams with a frequency difference matching twice the plasma frequency to generate quantum-correlated photon pairs via phonon-mediated four-wave mixing.

Result: Achieves ultra-strong squeezing with high pump intensities (10^16 Wcm^-2) and remarkable thermal noise tolerance, even with large thermal phonon numbers.

Conclusion: Plasma-based squeezing enables ultrahigh photon numbers, expanding applications in strong-field scenarios from optical to X-ray wavelengths.

Abstract: Quantum squeezed states enable precision measurements beyond the standard
quantum limit, but conventional solid-state media fundamentally limit pump
intensities to the ionization threshold. We demonstrate that plasma waves can
mediate ultra-strong two-mode squeezing through stimulated Raman scattering,
achieving up to ultrastrong squeezing using $10^{16}{Wcm^{-2}}$ pump lasers.
Employing two copropagating pump beams with frequency difference matching twice
the plasma frequency, we generate quantum-correlated photon pairs through
phonon-mediated four-wave mixing. The process exhibits remarkable thermal noise
tolerance, allowing strong squeezing even with large thermal phonon numbers.
This plasma-based approach produces squeezed states with ultrahigh photon
numbers, opening new possibilities for strong-field applications across optical
to X-ray wavelengths.

</details>


<div id='physics.data-an'></div>

# physics.data-an [[Back]](#toc)

### [40] [Neural Network-Guided Symbolic Regression for Interpretable Descriptor Discovery in Perovskite Catalysts](https://arxiv.org/abs/2507.12404)
*Yeming Xian,Xiaoming Wang,Yanfa Yan*

Main category: physics.data-an

TL;DR: A two-phase framework combining neural networks, feature importance analysis, and symbolic regression improves interpretable descriptor discovery for OER activity in oxide perovskites, achieving high accuracy with physical interpretability.


<details>
  <summary>Details</summary>
Motivation: To find accurate and interpretable descriptors for OER activity in oxide perovskites, overcoming limitations of symbolic regression with high-dimensional inputs and small datasets.

Method: Phase I: Uses a small dataset and symbolic regression to refine the known descriptor. Phase II: Expands features, reduces dimensionality, and identifies key descriptors like LUMO energy.

Result: Improved accuracy (training MAE 22.1 meV, validation MAE 20.6 meV) with interpretable descriptors (Î¼/t, Î¼/RA, LUMO energy).

Conclusion: NN-guided symbolic regression balances accuracy and interpretability, proving useful for materials informatics in data-scarce scenarios.

Abstract: Understanding and predicting the activity of oxide perovskite catalysts for
the oxygen evolution reaction (OER) requires descriptors that are both accurate
and physically interpretable. While symbolic regression (SR) offers a path to
discover such formulas, its performance degrades with high-dimensional inputs
and small datasets. We present a two-phase framework that combines neural
networks (NN), feature importance analysis, and symbolic regression (SR) to
discover interpretable descriptors for OER activity in oxide perovskites. In
Phase I, using a small dataset and seven structural features, we reproduce and
improve the known {\mu}/t descriptor by engineering composite features and
applying symbolic regression, achieving training and validation MAEs of 22.8
and 20.8 meV, respectively. In Phase II, we expand to 164 features, reduce
dimensionality, and identify LUMO energy as a key electronic descriptor. A
final formula using {\mu}/t, {\mu}/RA, and LUMO energy achieves improved
accuracy (training and validation MAEs of 22.1 and 20.6 meV) with strong
physical interpretability. Our results demonstrate that NN-guided symbolic
regression enables accurate, interpretable, and physically meaningful
descriptor discovery in data-scarce regimes, indicating interpretability need
not sacrifice accuracy for materials informatics.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [41] [Surrogate Quantum Circuit Design for the Lattice Boltzmann Collision Operator](https://arxiv.org/abs/2507.12256)
*Monica LÄcÄtuÅ,Matthias MÃ¶ller*

Main category: quant-ph

TL;DR: The paper introduces a surrogate quantum circuit (SQC) to approximate the BGK collision operator in quantum lattice Boltzmann methods, addressing challenges in quantum CFD simulations.


<details>
  <summary>Details</summary>
Motivation: Traditional CFD struggles with high Reynolds number flows, prompting interest in quantum algorithms for potential speed-ups. The nonlinear collision step in QLBM remains a challenge.

Method: A four-qubit SQC is trained to mimic the BGK collision operator, ensuring physical properties like conservation laws and equivariance. It avoids ancilla qubits and post-selection.

Result: The SQC requires only 2,430 gates, has a depth independent of grid resolution, and accurately simulates benchmark flows like Taylor Green vortex decay and lid-driven cavity.

Conclusion: The SQC framework successfully addresses the collision step challenge in QLBM, enabling efficient quantum CFD simulations with validated accuracy.

Abstract: Direct numerical simulation of turbulent flows at high Reynolds numbers
remains a major challenge for traditional computational fluid dynamics (CFD)
tools running on classical computer hardware. This has motivated growing
interest in quantum algorithms for CFD to enable flow simulations on quantum
computers. The reason being that these computers are expected to deliver
potential speed-ups for certain problems. One promising quantum CFD approach is
a fully quantum implementation of the lattice Boltzmann method called QLBM.
Although efficient quantum routines are now available for the streaming step,
implementing the nonlinear, irreversible collision step with a low depth
circuit that avoids additional ancilla qubits, probabilistic post-selection and
repeated executions remains a significant challenge. In this study, we address
this challenge by introducing a framework for learning a surrogate quantum
circuit (SQC) that approximates the full Bhatnagar Gross Krook (BGK) collision
operator for the D2Q9 lattice. The four qubit circuit is trained to respect the
physical properties of the BGK collision operator, including mass and momentum
conservation, D8 equivariance and scale equivariance. When compiled to the gate
set used by IBM Heron processor under the assumption of full qubit
connectivity, the 15 block SQC requires only 2,430 native gates and uses
neither ancilla qubits nor post-selection or repeated executions. Moreover, its
depth is independent of the grid resolution, as collision is a local operation
that can exploit quantum parallelism to its full extent. We validate the SQC on
two benchmark flows, the Taylor Green vortex decay and the lid driven cavity,
demonstrating that it accurately captures vortex dissipation and flow
recirculation.

</details>


### [42] [Jenga-Krotov algorithm: Efficient compilation of multi-qubit gates for exchange-only qubits](https://arxiv.org/abs/2507.12448)
*Jiahao Wu,Guanjie He,Wenyuan Zhuo,Quan Fu,Xin Wang*

Main category: quant-ph

TL;DR: The paper introduces Jenga-Krotov (JK), a gradient-based optimization algorithm, to streamline multi-qubit gate operations like the Toffoli gate in exchange-only (EO) qubit systems, reducing complexity and errors.


<details>
  <summary>Details</summary>
Motivation: The challenge of synthesizing efficient multi-qubit operations in EO qubit systems, where conventional methods are too slow and error-prone, drives the need for optimized solutions.

Method: The JK algorithm is developed to optimize gate sequences, specifically targeting the Toffoli gate, by reducing the number of exchange unitaries and time steps while maintaining fidelity.

Result: JK reduces the Toffoli gate's exchange unitaries from 216 to 92 and time steps from 162 to 50, with significantly lower error rates under noise.

Conclusion: JK is a scalable and effective method for multi-qubit gate synthesis in EO architectures, advancing semiconductor-based quantum computing.

Abstract: Exchange-only (EO) qubits, implemented in triple-quantum-dot systems, offer a
compelling platform for scalable semiconductor-based quantum computing by
enabling universal control through purely exchange interactions. While
high-fidelity single- and two-qubit gates have been demonstrated, the synthesis
of efficient multi-qubit operations -- such as the Toffoli gate -- remains a
key bottleneck. Conventional gate decompositions into elementary operations
lead to prohibitively long and error-prone pulse sequences, limiting practical
deployment. In this work, we introduce a gradient-based optimization algorithm,
Jenga-Krotov (JK), tailored to discover compact, high-fidelity EO gate
sequences. Applying JK to the Toffoli gate, we reduce the number of required
exchange unitaries from 216 (in standard decomposition) to 92, and compress the
time steps required from 162 to 50, all while maintaining target fidelity.
Under realistic noise, the accumulated gate error from our optimized sequence
is an order of magnitude lower than that of conventional approaches. These
results demonstrate that the JK algorithm is a general and scalable strategy
for multi-qubit gate synthesis in EO architectures, potentially facilitating
realization of multi-qubit algorithms on semiconductor platforms.

</details>


### [43] [Adiabatic Cooling of Planar Motion in a Penning Trap Ion Crystal to Sub-Millikelvin Temperatures](https://arxiv.org/abs/2507.12429)
*Wes Johnson,Bryce Bullock,Athreya Shankar,John Zaris,John J. Bollinger,Scott E. Parker*

Main category: quant-ph

TL;DR: Dynamic tuning of rotation frequency in a Penning trap enhances cooling of low-frequency planar modes in ion crystals, enabling sub-millikelvin temperatures for quantum information applications.


<details>
  <summary>Details</summary>
Motivation: The low-frequency planar modes of ion crystals in Penning traps are not efficiently cooled by laser cooling, limiting their utility in quantum information processing.

Method: Numerical simulations demonstrate that nonlinear mode coupling can be dynamically tuned by adiabatically changing the rotation frequency of the ion crystal.

Result: This technique achieves sub-millikelvin temperatures for low-frequency planar modes, improving spectral resolution of drumhead modes.

Conclusion: Adiabatic tuning of rotation frequency enhances cooling and spectral resolution, advancing quantum information processing capabilities.

Abstract: Two-dimensional planar ion crystals in a Penning trap are a platform for
quantum information science experiments. However, the low-frequency planar
modes of these crystals are not efficiently cooled by laser cooling, which can
limit the utility of the drumhead modes for quantum information processing.
Recently, it has been shown that nonlinear mode coupling can enhance the
cooling of the low-frequency planar modes. Here, we demonstrate in numerical
simulations that this coupling can be dynamically tuned by adiabatically
changing the rotation frequency of the ion crystal during experiments.
Furthermore, we show that this technique can, in addition, produce lower
temperatures for the low-frequency planar modes via an adiabatic cooling
process. This result allows cooling of the planar modes to sub-millikelvin
temperatures, resulting in improved spectral resolution of the drumhead modes
at experimentally relevant rotation frequencies, which is crucial for quantum
information processing applications.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [44] [Approaching Optimality for Solving Dense Linear Systems with Low-Rank Structure](https://arxiv.org/abs/2507.11724)
*MichaÅ DereziÅski,Aaron Sidford*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We provide new high-accuracy randomized algorithms for solving linear systems
and regression problems that are well-conditioned except for $k$ large singular
values. For solving such $d \times d$ positive definite system our algorithms
succeed whp. and run in time $\tilde O(d^2 + k^\omega)$. For solving such
regression problems in a matrix $\mathbf{A} \in \mathbb{R}^{n \times d}$ our
methods succeed whp. and run in time $\tilde O(\mathrm{nnz}(\mathbf{A}) + d^2 +
k^\omega)$ where $\omega$ is the matrix multiplication exponent and
$\mathrm{nnz}(\mathbf{A})$ is the number of non-zeros in $\mathbf{A}$. Our
methods nearly-match a natural complexity limit under dense inputs for these
problems and improve upon a trade-off in prior approaches that obtain running
times of either $\tilde O(d^{2.065}+k^\omega)$ or $\tilde O(d^2 +
dk^{\omega-1})$ for $d\times d$ systems. Moreover, we show how to obtain these
running times even under the weaker assumption that all but $k$ of the singular
values have a suitably bounded generalized mean. Consequently, we give the
first nearly-linear time algorithm for computing a multiplicative approximation
to the nuclear norm of an arbitrary dense matrix. Our algorithms are built on
three general recursive preconditioning frameworks, where matrix sketching and
low-rank update formulas are carefully tailored to the problems' structure.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [45] [Terahertz Wave Generation in Two-Dimensional MXenes under Femtosecond Pulsed Laser Illumination](https://arxiv.org/abs/2507.11656)
*A. A. Molavi Choobini,A. Chimeh*

Main category: physics.optics

TL;DR: Study explores terahertz wave generation in MXene layers using near-infrared femtosecond laser pulses.


<details>
  <summary>Details</summary>
Motivation: To investigate efficient terahertz wave generation in 2D MXene layers.

Method: Numerical study with near-infrared femtosecond laser excitation.

Result: Feasibility of efficient terahertz wave generation demonstrated.

Conclusion: MXene layers show promise for terahertz wave applications.

Abstract: A comprehensive numerical study has been conducted to investigate the
feasibility of efficient terahertz wave generation in two-dimensional MXene
layers excited by near-infrared femtosecond laser pulses.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [46] [An augmented Lagrangian method for strongly regular minimizers in a class of convex composite optimization problems](https://arxiv.org/abs/2507.12040)
*Chengjing Wang,Peipei Tang*

Main category: math.OC

TL;DR: The paper studies convex composite optimization, establishes equivalence between primal/dual conditions, and uses ALM with theoretical guarantees and numerical validation.


<details>
  <summary>Details</summary>
Motivation: To analyze convex composite optimization problems and provide theoretical foundations for efficient algorithms.

Method: Characterizes equivalence between primal/dual conditions, employs ALM, and derives theoretical guarantees.

Result: Establishes equivalences for perturbation analysis and ALM performance, validated numerically.

Conclusion: Theoretical results support efficient algorithm design, demonstrated by successful application to von Neumann entropy optimization.

Abstract: In this paper, we study a class of convex composite optimization problems. We
begin by characterizing the equivalence between the primal/dual strong
second-order sufficient condition and the dual/primal nondegeneracy condition.
Building on this foundation, we derive a specific set of equivalent conditions
for the perturbation analysis of the problem. Furthermore, we employ the
augmented Lagrangian method (ALM) to solve the problem and provide theoretical
guarantees for its performance. Specifically, we establish the equivalence
between the primal/dual second-order sufficient condition and the dual/primal
strict Robinson constraint qualification, as well as the equivalence between
the dual nondegeneracy condition and the nonsingularity of Clarke's generalized
Jacobian for the ALM subproblem. These theoretical results form a solid
foundation for designing efficient algorithms. Finally, we apply the ALM to the
von Neumann entropy optimization problem and present numerical experiments to
demonstrate the algorithm's effectiveness.

</details>


### [47] [Linearization-Based Feedback Stabilization of McKean-Vlasov PDEs](https://arxiv.org/abs/2507.12411)
*Dante Kalise,Lucas M. Moschen,Grigorios A. Pavliotis*

Main category: math.OC

TL;DR: The paper studies feedback stabilization of the McKean-Vlasov PDE on the torus using a time-dependent control potential, achieving local exponential stabilization and demonstrating effectiveness through numerical experiments.


<details>
  <summary>Details</summary>
Motivation: To steer the dynamics toward a prescribed stationary distribution or accelerate convergence to it, addressing challenges in control and stabilization of PDEs.

Method: Reformulates the controlled PDE in a weighted-projected space, applies the ground-state transform for a Schrodinger-type operator, and uses spectral analysis, the Hautus test, and Riccati-based feedback laws.

Result: Local exponential stabilization is rigorously proven, and numerical experiments show convergence speed-ups and stabilization of unstable equilibria.

Conclusion: The proposed control strategy is effective for stabilizing and accelerating convergence in the McKean-Vlasov PDE, validated by theoretical proofs and numerical examples.

Abstract: We study the feedback stabilization of the McKean-Vlasov PDE on the torus.
Our goal is to steer the dynamics toward a prescribed stationary distribution
or accelerate convergence to it using a time-dependent control potential. We
reformulate the controlled PDE in a weighted-projected space and apply the
ground-state transform to obtain a Schrodinger-type operator. The resulting
operator framework enables spectral analysis, verification of the
infinite-dimensional Hautus test, and the construction of Riccati-based
feedback laws. We rigorously prove local exponential stabilization via maximal
regularity arguments and nonlinear estimates. Numerical experiments on
well-studied models (the noisy Kuramoto model for synchronization, the O(2)
spin model in a magnetic field, and the Gaussian/von Mises attractive
interaction potential) showcase the effectiveness of our control strategy,
demonstrating convergence speed-ups and stabilization of otherwise unstable
equilibria.

</details>


### [48] [Convergence of drift-diffusion PDEs arising as Wasserstein gradient flows of convex functions](https://arxiv.org/abs/2507.12385)
*LÃ©naÃ¯c Chizat,Maria Colombo,Xavier FernÃ¡ndez-Real*

Main category: math.OC

TL;DR: The paper analyzes the convergence of drift-diffusion PDEs in Wasserstein gradient flows for linearly convex functions, showing improved convergence rates under certain conditions.


<details>
  <summary>Details</summary>
Motivation: To understand global convergence in non-displacement convex settings and explore interactions between Wasserstein geometry and linear convexity.

Method: Analyze drift-diffusion PDEs as gradient flows, proving convergence rates (e.g., O(1/t)) under convexity and coercivity, with faster rates for strongly convex objectives.

Result: Quantitative convergence guarantees for mean-field Langevin dynamics, enabling applications in optimization over probability measures, including entropy-regularized nonconvex problems.

Conclusion: The framework extends convergence guarantees and supports new applications, such as trajectory inference and Fisher Information regularization.

Abstract: We study the quantitative convergence of drift-diffusion PDEs that arise as
Wasserstein gradient flows of linearly convex functions over the space of
probability measures on ${\mathbb R}^d$. In this setting, the objective is in
general not displacement convex, so it is not clear a priori whether global
convergence even holds. Still, our analysis reveals that diffusion {allows} a
favorable interaction between Wasserstein geometry and linear convexity,
leading to a general quantitative convergence theory, analogous to that of
gradient flows in convex settings in the Euclidean space.
  Specifically, we prove that if the objective is convex and suitably coercive,
the suboptimality gap decreases at a rate $O(1/t)$. This improves to a rate
faster than any polynomial -- or even exponential in compact settings -- when
the objective is strongly convex relative to the entropy.
  Our results extend the range of mean-field Langevin dynamics that enjoy
quantitative convergence guarantees, and enable new applications to
optimization over the space of probability measures. To illustrate this, we
show quantitative convergence results for the minimization of
entropy-regularized nonconvex problems, we propose and study an
\emph{approximate Fisher Information} regularization covered by our setting,
and we apply our results to an estimator for trajectory inference which
involves the minimization of the relative entropy with respect to the Wiener
measure in path space.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [49] [RiemannLoRA: A Unified Riemannian Framework for Ambiguity-Free LoRA Optimization](https://arxiv.org/abs/2507.12142)
*Vladimir Bogachev,Vladimir Aletov,Alexander Molozhavenko,Denis Bobkov,Vera Soboleva,Aibek Alanov,Maxim Rakhuba*

Main category: cs.LG

TL;DR: RiemannLoRA improves LoRA by treating LoRA matrices as a smooth manifold, addressing initialization and overparametrization challenges for better performance.


<details>
  <summary>Details</summary>
Motivation: Challenges in LoRA include suboptimal initialization and overparametrization, which hinder efficiency and performance.

Method: Proposes RiemannLoRA, treating LoRA matrices as a smooth manifold to remove overparametrization and optimize initialization via Riemannian optimization.

Result: RiemannLoRA outperforms standard LoRA and its variants in convergence speed and final performance on LLMs and diffusion models.

Conclusion: RiemannLoRA provides a unified, efficient solution to LoRA's challenges, enhancing fine-tuning of large models.

Abstract: Low-Rank Adaptation (LoRA) has become a widely adopted standard for
parameter-efficient fine-tuning of large language models (LLMs), significantly
reducing memory and computational demands. However, challenges remain,
including finding optimal initialization strategies or mitigating
overparametrization in low-rank matrix factorization. In this work, we propose
a novel approach that addresses both of the challenges simultaneously within a
unified framework. Our method treats a set of fixed-rank LoRA matrices as a
smooth manifold. Considering adapters as elements on this manifold removes
overparametrization, while determining the direction of the fastest loss
decrease along the manifold provides initialization. Special care is taken to
obtain numerically stable and computationally efficient implementation of our
method, using best practices from numerical linear algebra and Riemannian
optimization. Experimental results on LLM and diffusion model architectures
demonstrate that RiemannLoRA consistently improves both convergence speed and
final performance over standard LoRA and its state-of-the-art modifications.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [50] [Simultaneous observations of multiple ELVES and SPRITES at the Pierre Auger Observatory](https://arxiv.org/abs/2507.11641)
*Roberto Mussa*

Main category: astro-ph.IM

TL;DR: The Pierre Auger Observatory has expanded its capabilities to study atmospheric light transients like ELVES, HALOS, and SPRITES using upgraded cameras (TLECAMs) and a Python-based DBSCAN algorithm for automated detection.


<details>
  <summary>Details</summary>
Motivation: To enhance the study of atmospheric light transients (e.g., ELVES, HALOS, SPRITES) by improving detection methods and expanding observational tools.

Method: Extended readout trace lengths to 0.9 ms, installed TLECAMs for higher resolution and longer integration times, and developed a Python algorithm using DBSCAN for automated SPRITE detection.

Result: First simultaneous observations of SPRITES and ELVES using TLECAMs and FD, along with efficient data acquisition without FD triggers.

Conclusion: The upgrades and new algorithm significantly improve the study of atmospheric light transients, enabling more detailed and automated observations.

Abstract: Since 2014, the Pierre Auger Observatory has exploited a dedicated trigger
and its very high time resolution to study ELVES and harvest record samples of
multiple ELVES using the Fluorescence Detector (FD). In 2017, after extending
the readout of trace lengths to 0.9 ms, we started observing other types of
light transients from the base of the ionosphere, such as HALOS, which deserved
further investigation. In December 2023 and April 2024, we installed two
additional cameras (TLECAMs), which allow us to perform simultaneous detection
of these transients with higher space resolution and longer integration times.
Here, we present our first simultaneous observations of SPRITES and ELVES by
both TLECAMs and FD. Furthermore, we describe the Python algorithm based on
DBSCAN to automatically detect SPRITES in the videos recorded by our TLECAMs
and acquire data efficiently without needing the FD trigger.

</details>


<div id='physics.space-ph'></div>

# physics.space-ph [[Back]](#toc)

### [51] [The Origin of Structure in the Auroral Ionosphere](https://arxiv.org/abs/2507.11755)
*Magnus F Ivarsen,Kaili Song,Luca Spogli,Jean-Pierre St-Maurice,Brian Pitzel,Saif Marei,Devin R Huyghebaert,Satoshi Kasahara,Kunihiro Keika,Yoshizumi Miyoshi,Yoichi Kazama,Shiang-Yu Wang,Ayako Matsuoka,Iku Shinohara,Shoichiro Yokota,P. T. Jayachandran,Glenn C Hussey*

Main category: physics.space-ph

TL;DR: Turbulence in the auroral ionosphere is not caused by local instabilities but by field-aligned currents and precipitating particles, forming a self-organized dissipative system.


<details>
  <summary>Details</summary>
Motivation: To resolve the paradox of turbulence thriving in the conductive auroral ionosphere despite conditions that should suppress plasma instabilities.

Method: Analysis using a novel composite power spectrum from coherent radar and GPS data.

Result: Turbulence arises from field-aligned currents and is governed by precipitating particles, not local instabilities.

Conclusion: The auroral ionosphere is a driven, emergent system of self-organized dissipative structures.

Abstract: The auroral ionosphere presents a paradox: turbulence often thrives in a
conductive layer that should suppress the growth rates of plasma instabilities.
Using a novel composite power spectrum from coherent radar and GPS data, we
show that turbulence on a wide range of scales stem not from local
instabilities. Instead, its spectrum is a structural imprint of field-aligned
electrical currents, and its dissipative character is governed by the flux of
precipitating particles. Our measurements provide a clear geophysical example
of a driven, emergent system of self-organized dissipative structures.

</details>


### [52] [Multi-directional investigations on quiet time suprathermal ions measured by ASPEX-STEPS on-board Aditya L1](https://arxiv.org/abs/2507.11952)
*Aakash Gupta,Dibyendu Chakrabarty,Santosh Vadawale,Aveek Sarkar,Bijoy Dalal,Shiv Kumar Goyal,Jacob Sebastian,P. Janardhan,Nandita Srivastava,M. Shanmugam,Neeraj Kumar Tiwari,Aaditya Sarda,Piyush Sharma,Anil Bhardwaj,Prashant Kumar,Manan S. Shah,Bhas Bapat,Pranav R. Adhyaru,Arpit R. Patel,Hitesh Kumar Adalja,Abhishek Kumar,Tinkal Ladiya,Sushil Kumar,Nishant Singh,Deepak Kumar Painkra,Abhishek J. Verma,Swaroop Banerjee,K. P. Subramanian,M. B. Dadhania*

Main category: physics.space-ph

TL;DR: The study analyzes quiet-time suprathermal ions in the interplanetary medium using data from STEPS sensors on Aditya L1 and ACE's ULEIS, revealing isotropic distribution and contributions from past solar events.


<details>
  <summary>Details</summary>
Motivation: To understand the origin, acceleration, and anisotropy of suprathermal ions during quiet periods in the solar wind.

Method: Derived spectral indices from STEPS sensors and analyzed elemental abundance ratios from ACE's ULEIS during quiet intervals in 2024.

Result: Spectral indices (~2.0) indicate isotropic distribution; abundance ratios suggest contributions from past solar flares and CMEs.

Conclusion: Quiet-time suprathermal ions are isotropically distributed and likely include remnants from previous solar events.

Abstract: The origin, acceleration and anisotropy of suprathermal ions in the
interplanetary medium during quiet periods have remained poorly understood
issues in solar wind physics. To address these aspects, we derive the spectral
indices for the quiet time suprathermal ions based on the measurements by the
four directionally separated sensors that are part of the Supra-Thermal and
Energetic Particle Spectrometer (STEPS) of Aditya Solar Wind Particle
EXperiment (ASPEX) on-board Aditya L1 spacecraft. Three out of four STEPS
sensors Parker Spiral (PS), Inter-Mediate (IM), Earth Pointing (EP) are in one
plane (nearly aligned with the ecliptic plane) while the fourth sensor North
Pointing (NP) is in a mutually orthogonal plane. The energy ranges covered by
the PS, IM, EP and NP sensors are 0.36-1.32 MeV, 0.14-1.22 MeV, 0.39-1.33 MeV
and 0.12-1.23 MeV respectively. The quiet intervals are identified during
January November, 2024 and the derived spectral indices (differential
directional flux versus energy) are found to be in the range of 2.0 for all
directions in the time scale of a few days revealing isotropic nature of their
distribution. Further analysis of elemental abundance ratios (3He/4He, Fe/O,
and C/O) during the same quiet intervals obtained from the Ultra-Low Energy
Isotope Spectrometer (ULEIS) on board the Advanced Composition Explorer (ACE)
spacecraft suggests possible contributions from the leftover ions from the
previous impulsive (Solar flares) and gradual events (CMEs) in the quiet time
suprathermal ion pool.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [53] [Inverse curvature flows for capillary hypersurfaces in the unit ball](https://arxiv.org/abs/2507.12097)
*Shujing Pan,Bo Yang*

Main category: math.DG

TL;DR: Study of inverse curvature flows for convex capillary hypersurfaces in a Euclidean ball, proving existence and convergence, with applications to Alexandrov-Fenchel inequalities.


<details>
  <summary>Details</summary>
Motivation: To explore inverse curvature flows for capillary hypersurfaces and their implications for geometric inequalities.

Method: Analyze strictly convex, capillary hypersurfaces in the unit Euclidean ball and establish existence and convergence for inverse curvature flows.

Result: Proved existence and convergence for the flows, leading to Alexandrov-Fenchel inequalities for weakly convex hypersurfaces with free boundary.

Conclusion: The study advances understanding of geometric flows and inequalities, with potential applications in differential geometry.

Abstract: In this paper, we study inverse curvature flows for strictly convex,
capillary hypersurfaces in the unit Euclidean ball. We establish the existence
and convergence results for a class of such flows. As an application, we derive
a family of Alexandrov Fenchel inequalities for weakly convex hypersurfaces
with free boundary.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [54] [Pseudodifferential Weyl calculus on vector bundles](https://arxiv.org/abs/2507.11965)
*Lars Andersson,Benjamin Moser,Marius A. Oancea,Claudio F. Paganini,Gabriel Schmid*

Main category: math-ph

TL;DR: A geometric framework for Weyl quantization on pseudo-Riemannian manifolds is developed, extending results to curved geometries and providing explicit computations for physically significant operators.


<details>
  <summary>Details</summary>
Motivation: To extend Weyl quantization to curved geometries and establish a foundation for quantum field theory on curved spacetimes and semiclassical analysis.

Method: Constructs a star product and computes its semiclassical expansion up to third order, ensuring a one-to-one correspondence between self-adjoint symbols and operators.

Result: Explicit computations of Weyl symbols for Dirac, Maxwell, Yang-Mills, and Einstein operators, and analysis of the Moyal equation for Wigner functions.

Conclusion: The framework supports future developments in quantum field theory on curved spacetimes, semiclassical analysis, and chiral kinetic theory.

Abstract: We develop a geometric framework for Weyl quantization on pseudo-Riemannian
manifolds, in which pseudodifferential operators act on sections of vector
bundles equipped with arbitrary connections. We construct the associated star
product and compute its semiclassical expansion up to third order in the
expansion parameter. A central feature of our approach is a one-to-one
correspondence between formally self-adjoint symbols and formally self-adjoint
operators, extending known results from flat space to curved geometries. In
addition, we analyze the Moyal equation satisfied by the Wigner function in
this setting and provide explicit computations of Weyl symbols for several
physically significant operators, including the Dirac, Maxwell, linearized
Yang-Mills, and linearized Einstein operators. Our results lay the foundation
for future developments in quantum field theory on curved spacetimes,
semiclassical analysis, and chiral kinetic theory.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [55] [Subspace Approximation to the Focused Transport Equation. II. The Modified Form](https://arxiv.org/abs/2507.11801)
*B. Klippenstein,A. Shalchi*

Main category: astro-ph.SR

TL;DR: The paper compares two forms of the focused transport equation, highlighting the conservative form's norm conservation and lack of pitch-angle isotropization. It employs a subspace method for analytical and numerical solutions, emphasizing efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of the standard focused transport equation (non-conservation of norm) and explore the conservative form's properties.

Method: Uses the subspace method for solving the equation, including a two-dimensional subspace approximation for analytical treatment and numerical evaluation of matrix exponentials for higher dimensions.

Result: The semi-numerical approach is faster than traditional solvers, making it highly practical.

Conclusion: The conservative form of the focused transport equation, solved via the subspace method, offers efficient and accurate solutions, though it lacks pitch-angle isotropization.

Abstract: The transport of energetic particles in a spatially varying magnetic field is
described by the focused transport equation. In the past two versions of this
equation were investigated. The more commonly used standard form described a
pitch-angle isotropization process but does not conserve the norm. In the
current paper we consider the focused transport equation in conservative form
also called modified focused transport equation. This equation conserves the
norm but does not describe pitch-angle isotropization. We use the previously
developed subspace method to solve the focused transport equation analytically
and numerically. For a pure analytical treatment we employ the two-dimensional
subspace approximation. Furthermore, we consider a higher dimensionality for
which one needs to evaluate occurring matrix exponentials numerically. This
type of semi-numerical approach is much faster than traditional solvers and,
therefore, it is very useful.

</details>


### [56] [Can plasmoid-mediated reconnection occur in collisionless systems?](https://arxiv.org/abs/2507.11904)
*Keita Akutagawa,Shinsuke Imada,Munehito Shoda*

Main category: astro-ph.SR

TL;DR: The paper investigates plasmoid formation in collisionless magnetic reconnection, finding it depends on mass ratio but doesn't enhance reconnection rates.


<details>
  <summary>Details</summary>
Motivation: To understand why reconnection rates are consistently fast in astrophysical phenomena like solar flares.

Method: Uses 2.5D particle-in-cell simulations to study plasmoid formation in collisionless systems and compares with resistive MHD simulations.

Result: Secondary plasmoids form in collisionless systems but not with realistic mass ratios, and they don't enhance reconnection rates.

Conclusion: Plasmoid-mediated reconnection may not explain fast rates in collisionless systems, though results are limited to smaller scales like Earth's magnetotail.

Abstract: Magnetic reconnection is a process that converts magnetic energy into plasma
energy by changing the magnetic field line topology. The outstanding question
is why the reconnection rate is $\mathcal{O}(0.01 - 0.1)$ in many astrophysical
phenomena, for example solar flares and terrestrial substorms. Previous studies
have shown two ideas of Hall reconnection and plasmoid instability. However,
there is no consensus on which process is the reason for the fast reconnection.
In this paper, we discuss the formation of secondary plasmoids in \rewrite{2D
antiparallel collisionless reconnection} using 2.5-dimensional particle-in-cell
simulations and discuss whether plasmoid-mediated reconnection occur in
collisionless systems by comparing with plasmoid instability in resistive MHD
simulations. We find that in collisionless systems secondary plasmoids can
indeed form. However, the mass ratio has a strong effect on the formation of
secondary plasmoids, and it indicates that secondary plasmoids do not emerge
using realistic ion-electron mass ratio ($m_i/m_e = 1836$). Furthermore, we
find that there is no enhancement of the reconnection rate due to the secondary
plasmoid in the collisionless system, as discussed in the plasmoid-mediated
reconnection. Although our simulation $\mathcal{O}(100\lambda_i)$ box is not
large enough to discuss astrophysical phenomena such as solar flares, it can
reflect a relatively small plasma system such as the Earth's magnetotail.

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [57] [A Hybrid Algorithm for Drift-Kinetic Particle Dynamics within General Relativistic Magnetohydrodynamics Simulations of Black Holes Accretion Flows](https://arxiv.org/abs/2507.11616)
*Tyler Trent,Dimitrios Psaltis,Feryal Ãzel*

Main category: astro-ph.HE

TL;DR: A hybrid numerical algorithm using covariant guiding center formalism is developed for stable simulation of charged particles in GRMHD backgrounds, addressing scale separation in black hole accretion flows.


<details>
  <summary>Details</summary>
Motivation: The need to model weakly collisional astrophysical plasmas in relativistic spacetimes, like black hole accretion flows, where kinetic effects are crucial but fully kinetic simulations are impractical due to scale separation.

Method: A covariant guiding center formalism is used to develop a hybrid numerical algorithm, incorporating a semi-implicit integrator for stable particle trajectory evolution in GRMHD backgrounds.

Result: The method is applied to GRMHD simulations of black hole accretion flows, showing accuracy and efficiency across various physical conditions.

Conclusion: The proposed hybrid algorithm effectively addresses the challenges of simulating charged particles in extreme astrophysical environments, offering a practical solution for kinetic modeling in GRMHD contexts.

Abstract: Astrophysical plasmas in relativistic spacetimes, such as black hole
accretion flows, are often weakly collisional and require kinetic modeling to
capture non-local transport and particle acceleration. However, the extreme
scale separation between microscopic and macroscopic processes limits the
feasibility of fully kinetic simulations. A covariant guiding center formalism
has recently been derived to address this challenge in curved spacetimes. We
present a new hybrid numerical algorithm based on this formalism, which evolves
the trajectories of charged particles over macroscopic timescales in GRMHD
backgrounds. To address numerical instabilities in the equations of motion, we
develop a semi-implicit integrator that ensures stable evolution in
strong-field environments. We apply our method to GRMHD simulations of black
hole accretion flows, demonstrating its accuracy and efficiency across a range
of physical conditions.

</details>


### [58] [Constraining shear modulus of polycrystalline neutron star crust: Hashin-Shtrikman variational approach](https://arxiv.org/abs/2507.12266)
*Nikita A. Zemlyakov,Andrey I. Chugunov*

Main category: astro-ph.HE

TL;DR: The paper analyzes the elastic properties of neutron star crusts, focusing on shear modulus constraints using the Hashin-Shtrikman approach, and suggests it should be lower than the Voigt estimate.


<details>
  <summary>Details</summary>
Motivation: Understanding neutron star crust elasticity is crucial for modeling phenomena like glitches and gravitational waves, but current assumptions lack precision due to unknown crystallite properties.

Method: The study assumes local isotropy and uses the Hashin-Shtrikman variational approach to constrain the shear modulus, analyzing one- and two-component crust models.

Result: The effective shear modulus is found to be lower than the Voigt estimate, providing a refined constraint for astrophysical models.

Conclusion: The Hashin-Shtrikman approach offers improved constraints on neutron star crust elasticity, challenging traditional Voigt-based estimates.

Abstract: The elastic properties of the neutron star crust are thought to play a
crucial role in various phenomena of neutron stars (glitches, oscillations,
gravitational wave emission) and should be described quantitatively to model
these phenomena. The fundamental problem of this description is associated with
the polycrystalline nature of the crust: similar to terrestrial materials, the
elastic moduli, strictly speaking, depend on the shape and orientation of
crystallites, but for the crust, they are unknown. As a result, some
assumptions are generally required to predict the elastic properties or
constrain their possible range. In this paper, we follow the commonly believed
assumption that the crust is (locally) isotropic, which allows us to describe
elastic properties by two (effective) parameters: bulk and shear moduli. The
bulk modulus is well determined by the Voigt-Reuss bounds, and we constrain the
shear modulus by applying, for the first time in astrophysics of compact stars,
the variational Hashin-Shtrikman approach, based on the additional assumption
that there are no correlations in the orientation of crystallites. We analyse
the Hashin-Shtrikman bounds for the one-component crust taking into account the
electron screening and the motion of the nuclei, and for two-component static
crystals. In particular, we demonstrate that within applied assumptions the
effective shear modulus should be lower than the Voigt estimate, typically
applied in the astrophysical literature.

</details>
