<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 12]
- [math.AP](#math.AP) [Total: 19]
- [physics.comp-ph](#physics.comp-ph) [Total: 4]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 3]
- [cs.LG](#cs.LG) [Total: 2]
- [math.CV](#math.CV) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [astro-ph.SR](#astro-ph.SR) [Total: 1]
- [math.OC](#math.OC) [Total: 4]
- [math.DS](#math.DS) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [astro-ph.HE](#astro-ph.HE) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 2]
- [math.DG](#math.DG) [Total: 3]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Keep the beat going: Automatic drum transcription with momentum](https://arxiv.org/abs/2507.12596)
*Alisha L. Foster,Robert J. Webber*

Main category: math.NA

TL;DR: This paper compares two optimization methods (multiplicative update rule vs projected gradient descent with momentum) for automatic drum transcription using nonnegative matrix factorization, finding that projected gradient descent with momentum achieves better accuracy and convergence guarantees.


<details>
  <summary>Details</summary>
Motivation: To find a simple and interpretable approach for automatic drum transcription by comparing different optimization methods for nonnegative matrix factorization, addressing the need for effective drum transcription algorithms with theoretical guarantees.

Method: The paper uses partially fixed nonnegative matrix factorization on magnitude spectrograms and compares two optimization approaches: multiplicative update rule and projected gradient descent with momentum. Both methods are analyzed for time complexity and empirical performance.

Result: Projected gradient descent with momentum outperforms the multiplicative update rule in terms of accuracy for fixed runtime when tested on ENST-Drums dataset and original recordings. The method also provides stronger theoretical convergence guarantees.

Conclusion: Projected gradient descent with momentum is the superior optimization method for drum transcription tasks using nonnegative matrix factorization, offering both better empirical performance and stronger theoretical foundations compared to multiplicative update rules.

Abstract: A simple, interpretable way to perform automatic drum transcription is by
factoring the magnitude spectrogram of a recorded musical piece using a
partially fixed nonnegative matrix factorization. There are two natural ways to
optimize the nonnegative matrix factorization, including a multiplicative
update rule and projected gradient descent with momentum. The methods differ in
their empirical accuracies and theoretical convergence guarantees. This paper
summarizes the methods and their time complexities, and it applies the methods
to the ENST-Drums data set and an original recording from the author's band,
evaluating the empirical accuracy with respect to ground-truth drum
annotations. The results indicate that projected gradient descent with momentum
leads to higher accuracy for a fixed runtime, and it satisfies stronger
convergence guarantees.

</details>


### [2] [A Unified Framework for Efficient Kernel and Polynomial Interpolation](https://arxiv.org/abs/2507.12629)
*M. Belianovich,G. E. Fasshauer,A. Narayan,V. Shankar*

Main category: math.NA

TL;DR: A unified interpolation scheme combining compactly-supported kernels and polynomials, with efficient numerical methods for computation and application to manifolds.


<details>
  <summary>Details</summary>
Motivation: To generalize and improve interpolation by combining kernel and polynomial methods, enabling broader applications including manifolds.

Method: Combines compactly-supported kernels and polynomials, using specialized numerical linear algebra for efficiency.

Result: The unified interpolant outperforms polynomial least squares in numerical experiments on Euclidean domains and manifolds.

Conclusion: The proposed framework is effective and versatile, offering superior performance over traditional methods.

Abstract: We present a unified interpolation scheme that combines compactly-supported
positive-definite kernels and multivariate polynomials. This unified framework
generalizes interpolation with compactly-supported kernels and also classical
polynomial least squares approximation. To facilitate the efficient use of this
unified interpolation scheme, we present specialized numerical linear algebra
procedures that leverage standard matrix factorizations. These procedures allow
for efficient computation and storage of the unified interpolant. We also
present a modification to the numerical linear algebra that allows us to
generalize the application of the unified framework to target functions on
manifolds with and without boundary. Our numerical experiments on both
Euclidean domains and manifolds indicate that the unified interpolant is
superior to polynomial least

</details>


### [3] [Partitioned Conservative, Variable Step, Second-Order Method for Magneto-hydrodynamics In Elsässer Variables](https://arxiv.org/abs/2507.12700)
*Zhen Yao,Catalin Trenchea,Wenlong Pei*

Main category: math.NA

TL;DR: A symplectic, second-order algorithm for MHD in Elsässer variables is proposed, reducing computational cost via parallel subproblem solving and ensuring linear convergence. It conserves key quantities and achieves second-order accuracy, with adaptive time stepping balancing accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the computational challenges of solving the MHD system efficiently while maintaining accuracy and conserving key physical quantities like energy and helicity.

Method: The algorithm partitions the MHD system into two smaller subproblems solved in parallel, uses a symplectic, second-order approach, and employs adaptive time stepping based on local truncation error.

Result: The method conserves energy, cross-helicity, and magnetic helicity unconditionally, achieves second-order accuracy, and demonstrates efficiency through numerical tests.

Conclusion: The proposed algorithm is effective for MHD simulations, offering computational efficiency, accuracy, and conservation properties, with adaptive time stepping enhancing performance.

Abstract: Magnetohydrodynamics (MHD) describes the interaction between electrically
conducting fluids and electromagnetic fields. We propose and analyze a
symplectic, second-order algorithm for the evolutionary MHD system in
Els\"asser variables. We reduce the computational cost of the iterative
non-linear solver, at each time step, by partitioning the coupled system into
two subproblems of half size, solved in parallel. We prove that the iterations
converge linearly, under a time step restriction similar to the one required in
the full space-time error analysis. The variable step algorithm unconditionally
conserves the energy, cross-helicity and magnetic helicity, and numerical
solutions are second-order accurate in the $L^{2}$ and $H^{1}$-norms. The time
adaptive mechanism, based on a local truncation error criterion, helps the
variable step algorithm balance accuracy and time efficiency. Several numerical
tests support the theoretical findings and verify the advantage of time
adaptivity.

</details>


### [4] [DPNO: A Dual Path Architecture For Neural Operator](https://arxiv.org/abs/2507.12719)
*Yichen Wang,Wenlian Lu*

Main category: math.NA

TL;DR: A novel dual-path architecture enhances neural operators for solving PDEs, outperforming traditional stacking methods by 30% in some cases.


<details>
  <summary>Details</summary>
Motivation: Single operator blocks are limited in performance, and traditional stacking methods are not parameter-efficient.

Method: Proposes a dual-path architecture inspired by ResNet and DenseNet for parallel processing.

Result: Achieves a 30% improvement on standard test cases and shows versatility with DeepONet and FNO.

Conclusion: The dual-path architecture offers a promising direction for neural operator design, improving feature extraction and solution approximation.

Abstract: Neural operators have emerged as a powerful tool for solving partial
differential equations (PDEs) and other complex scientific computing tasks.
However, the performance of single operator block is often limited, thus often
requiring composition of basic operator blocks to achieve better per-formance.
The traditional way of composition is staking those blocks like feedforward
neural networks, which may not be very economic considering
parameter-efficiency tradeoff. In this pa-per, we propose a novel dual path
architecture that significantly enhances the capabilities of basic neural
operators. The basic operator block is organized in parallel two paths which
are similar with ResNet and DenseNet. By introducing this parallel processing
mechanism, our architecture shows a more powerful feature extraction and
solution approximation ability compared with the original model. We demonstrate
the effectiveness of our approach through extensive numerical experi-ments on a
variety of PDE problems, including the Burgers' equation, Darcy Flow Equation
and the 2d Navier-Stokes equation. The experimental results indicate that on
certain standard test cas-es, our model achieves a relative improvement of over
30% compared to the basic model. We also apply this structure on two standard
neural operators (DeepONet and FNO) selected from different paradigms, which
suggests that the proposed architecture has excellent versatility and offering
a promising direction for neural operator structure design.

</details>


### [5] [Quasi-optimality of the Crouzeix-Raviart FEM for p-Laplace-type problems](https://arxiv.org/abs/2507.12742)
*Johannes Storn*

Main category: math.NA

TL;DR: The paper verifies quasi-optimality of the Crouzeix-Raviart FEM for nonlinear $p$-Laplace problems, showing bounded error relative to best-approximation and data oscillation. It also provides a localized a priori error estimate for conforming Lagrange FEM.


<details>
  <summary>Details</summary>
Motivation: To establish quasi-optimality and error bounds for the Crouzeix-Raviart FEM in nonlinear $p$-Laplace problems, and to derive new localized error estimates for conforming FEM.

Method: Analysis of the Crouzeix-Raviart FEM using quasi-norms, bounding error by best-approximation and data oscillation terms.

Result: Quasi-optimality is proven for the Crouzeix-Raviart FEM, with error bounded by best-approximation and oscillation. A novel localized error estimate for conforming FEM is also derived.

Conclusion: The Crouzeix-Raviart FEM is quasi-optimal for $p$-Laplace problems, and the work provides additional insights into error estimation for conforming FEM.

Abstract: We verify quasi-optimality of the Crouzeix-Raviart FEM for nonlinear problems
of $p$-Laplace type. More precisely, we show that the error of the
Crouzeix-Raviart FEM with respect to a quasi-norm is bounded from above by a
uniformly bounded constant times the best-approximation error plus a data
oscillation term. As a byproduct, we verify a novel more localized a priori
error estimate for the conforming lowest-order Lagrange FEM.

</details>


### [6] [Analysis of Langevin midpoint methods using an anticipative Girsanov theorem](https://arxiv.org/abs/2507.12791)
*Matthew S. Zhang*

Main category: math.NA

TL;DR: A new method for analyzing midpoint discretizations of SDEs in MCMC is introduced, improving regularity and cross-regularity results and providing a query complexity bound for sampling accuracy.


<details>
  <summary>Details</summary>
Motivation: To enhance the analysis of midpoint discretizations in MCMC methods for sampling from target measures, addressing limitations in current literature.

Method: Uses Malliavin calculus to estimate Radon-Nikodym derivatives for non-adapted processes, applied to midpoint discretizations.

Result: Improved regularity and cross-regularity results, with a query complexity bound of O~(κ^(5/4)d^(1/4)/ε^(1/2)) for ε^2-accurate sampling.

Conclusion: The method advances the theoretical understanding and efficiency of midpoint discretizations in MCMC sampling.

Abstract: We introduce a new method for analyzing midpoint discretizations of
stochastic differential equations (SDEs), which are frequently used in Markov
chain Monte Carlo (MCMC) methods for sampling from a target measure $\pi
\propto \exp(-V)$. Borrowing techniques from Malliavin calculus, we compute
estimates for the Radon-Nikodym derivative for processes on $L^2([0, T);
\mathbb{R}^d)$ which may anticipate the Brownian motion, in the sense that they
may not be adapted to the filtration at the same time. Applying these to
various popular midpoint discretizations, we are able to improve the regularity
and cross-regularity results in the literature on sampling methods. We also
obtain a query complexity bound of $\widetilde{O}(\frac{\kappa^{5/4}
d^{1/4}}{\varepsilon^{1/2}})$ for obtaining a $\varepsilon^2$-accurate sample
in $\mathsf{KL}$ divergence, under log-concavity and strong smoothness
assumptions for $\nabla^2 V$.

</details>


### [7] [Adaptive feature capture method for solving partial differential equations with low regularity solutions](https://arxiv.org/abs/2507.12941)
*Yangtao Deng,Qiaolin He,Xiaoping Wang*

Main category: math.NA

TL;DR: AFCM is a machine learning framework that adaptively redistributes neurons and collocation points in high-gradient regions to solve PDEs with low-regularity solutions, outperforming traditional and deep-learning methods.


<details>
  <summary>Details</summary>
Motivation: Traditional numerical methods struggle with PDEs in complex geometries, while mesh-free deep-learning approaches lack adaptive resolution for steep gradients or singularities.

Method: AFCM uses gradient norms to guide adaptive redistribution of neurons and collocation points, enhancing local resolution without extra computational cost.

Result: AFCM accurately resolves low-regularity PDEs in complex geometries, outperforming existing methods.

Conclusion: AFCM bridges adaptive mesh refinement and randomized neural networks, offering a scalable solution for challenging PDEs.

Abstract: Partial differential equations (PDEs) with low-regularity solutions pose
significant challenges for traditional numerical methods, particularly in
complex geometries where mesh generation and adaptive refinement become
computationally expensive. While deep-learning-based approaches, such as
Physics-Informed Neural Networks (PINNs) and the Random Feature Method (RFM),
offer mesh-free alternatives, they often lack adaptive resolution in critical
regions, limiting their accuracy for solutions with steep gradients or
singularities. In this work, we propose the Adaptive Feature Capture Method
(AFCM), a novel machine learning framework that adaptively redistributes
neurons and collocation points in high-gradient regions to enhance local
expressive power. Inspired by adaptive moving mesh techniques, AFCM employs the
gradient norm of an approximate solution as a monitor function to guide the
reinitialization of feature function parameters. This ensures that partition
hyperplanes and collocation points cluster where they are most needed,
achieving higher resolution without increasing computational overhead. The AFCM
extends the capabilities of RFM to handle PDEs with near-singular solutions
while preserving its mesh-free efficiency. Numerical experiments demonstrate
the method's effectiveness in accurately resolving low-regularity problems,
even in complex geometries. By bridging the gap between adaptive mesh
refinement and randomized neural networks, AFCM offers a robust and scalable
approach for solving challenging PDEs in scientific and engineering
applications.

</details>


### [8] [High Performance Parallel Solvers for the time-harmonic Maxwell Equations](https://arxiv.org/abs/2507.13066)
*Elise Fressart,Sébastien Dubois,Loïc Gouarin,Marc Massot,Michel Nowak,Nicole Spillane*

Main category: math.NA

TL;DR: Comparison of four preconditioners for solving large-scale time-harmonic Maxwell equations, with preliminary results favoring Hiptmair-Xu and Block Low-Rank methods.


<details>
  <summary>Details</summary>
Motivation: The numerical solution of time-harmonic Maxwell equations is challenging due to their non-Hermitian and non-semi-definite nature, necessitating effective preconditioners.

Method: Four preconditioners (sparse approximate inverse, Restricted Additive Schwarz, Hiptmair-Xu, and Block Low-Rank) are compared using PETSc, MUMPS, and hypre, alongside standard LU factorization. Performance is evaluated based on mesh size, CPU cores, wavelength, and domain size.

Result: Preliminary results indicate better performance for the Hiptmair-Xu and Block Low-Rank preconditioners.

Conclusion: The Hiptmair-Xu and Block Low-Rank preconditioners show promise for solving time-harmonic Maxwell equations, though the study is ongoing.

Abstract: We consider the numerical solution of large scale time-harmonic Maxwell
equations. To this day, this problem remains difficult, in particular because
the equations are neither Hermitian nor semi-definite. Our approach is to
compare different strategies for solving this set of equations with
preconditioners that are available either in PETSc, MUMPS, or in hypre. Four
different preconditioners are considered. The first is the sparse approximate
inverse, which is often applied to electromagnetic problems. The second is
Restricted Additive Schwarz, a domain decomposition preconditioner. The third
is the Hiptmair-Xu preconditioner which is tailored to the positive Maxwell
equations, a nearby problem. The final preconditioner is MUMPS's Block Low-Rank
method, a compressed block procedure. We also compare the performance of this
method to the standard LU factorization technique, which is a direct solver.
Performance with respect to the mesh size, the number of CPU cores, the
wavelength and the physical size of the domain are considered. This work in
progress yields temporary conclusions in favour of the Hiptmair-Xu and the
Block Low-Rank preconditioners.

</details>


### [9] [Stability of lattice Boltzmann schemes for initial boundary value problems in raw formulation](https://arxiv.org/abs/2507.13108)
*Thomas Bellotti*

Main category: math.NA

TL;DR: The paper analyzes the stability of 1D linear scalar lattice Boltzmann schemes for hyperbolic equations, focusing on boundary data effects without transforming to scalar forms. It introduces strong stability notions for schemes with specific stencil properties and validates findings numerically.


<details>
  <summary>Details</summary>
Motivation: To address stability challenges in lattice Boltzmann schemes for hyperbolic equations, especially near boundaries, without relying on scalar transformations.

Method: The study uses the raw algorithm on multiple unknowns, introduces strong stability concepts, and examines three schemes with various boundary conditions.

Result: Theoretical stability-instability results are derived for schemes with specific stencil properties, supported by numerical simulations.

Conclusion: The approach successfully analyzes stability for lattice Boltzmann schemes with boundary conditions, providing insights without scalar transformations.

Abstract: We study the stability of one-dimensional linear scalar lattice Boltzmann
schemes for hyperbolic equations with respect to boundary data. Our approach is
based on the original raw algorithm on several unknowns, thereby avoiding the
need for a transformation into an equivalent scalar formulation-a challenging
process in presence of boundaries. To address different behaviors exhibited by
the numerical scheme, we introduce appropriate notions of strong stability.
They account for the potential absence of a continuous extension of the stable
vector bundle associated with the bulk scheme on the unit circle for certain
components. Rather than developing a general theory, complicated by the fact
that discrete boundaries in lattice Boltzmann schemes are inherently
characteristic, we focus on strong stability-instability for methods whose
characteristic equations have stencils of breadth one to the left. In this
context, we study three representative schemes. These are endowed with various
boundary conditions drawn from the literature, and our theoretical results are
supported by numerical simulations.

</details>


### [10] [Generalized Scattering Matrix Framework for Modeling Implantable Antennas in Multilayered Spherical Media](https://arxiv.org/abs/2507.13119)
*Chenbo Shi,Xin Gu,Shichen Liang,Jin Pan*

Main category: math.NA

TL;DR: A unified framework for analyzing antennas in spherically stratified media, combining free-space GSM with extended SSOs for efficient modeling.


<details>
  <summary>Details</summary>
Motivation: Addresses the need for efficient analysis of antennas in complex environments like biomedical implants and radome-enclosed systems.

Method: Decouples antenna and medium modeling using GSM and SSOs, avoiding re-simulation for material variations.

Result: Shows excellent agreement with full-wave and DGF-based solutions, proving accuracy and scalability.

Conclusion: The framework is efficient, accurate, and scalable, with provided code for adoption and development.

Abstract: This paper presents a unified and efficient framework for analyzing antennas
embedded in spherically stratified media -- a model broadly applicable to
implantable antennas in biomedical systems and radome-enclosed antennas in
engineering applications. The proposed method decouples the modeling of the
antenna and its surrounding medium by combining the antenna's free-space
generalized scattering matrix (GSM) with a set of extended spherical scattering
operators (SSOs) that rigorously capture the electromagnetic interactions with
multilayered spherical environments. This decoupling enables rapid reevaluation
under arbitrary material variations without re-simulating the antenna, offering
substantial computational advantages over traditional dyadic Green's function
(DGF)-based MoM approaches. The framework supports a wide range of spherical
media, including radially inhomogeneous and uniaxially anisotropic layers.
Extensive case studies demonstrate excellent agreement with full-wave and
DGF-based solutions, confirming the method's accuracy, generality, and
scalability. Code implementations are provided to facilitate adoption and
future development.

</details>


### [11] [On the efficiency of a posteriori error estimators for parabolic partial differential equations in the energy norm](https://arxiv.org/abs/2507.13188)
*Iain Smears*

Main category: math.NA

TL;DR: The paper analyzes the efficiency of a posteriori error estimators for the heat equation, showing dependence on the choice of norm and numerical solution representation.


<details>
  <summary>Details</summary>
Motivation: To understand how the efficiency of error estimators varies with different norms and numerical solution definitions.

Method: Uses an implicit Euler method in time and conforming finite element method in space, comparing continuous piecewise affine-in-time and piecewise constant-in-time reconstructions.

Result: Demonstrates that estimator efficiency depends on both the norm and the numerical solution's definition.

Conclusion: Highlights the importance of norm and solution representation choice in error estimator efficiency.

Abstract: For the model problem of the heat equation discretized by an implicit Euler
method in time and a conforming finite element method in space, we prove the
efficiency of a posteriori error estimators with respect to the energy norm of
the error, when considering the numerical solution as the average between the
usual continuous piecewise affine-in-time and piecewise constant-in-time
reconstructions. This illustrates how the efficiency of the estimators is not
only possibly dependent on the choice of norm, but also on the choice of notion
of numerical solution.

</details>


### [12] [Well-balanced path-conservative discontinuous Galerkin methods with equilibrium preserving space for shallow water linearized moment equations](https://arxiv.org/abs/2507.13284)
*Ruilin Fan,Julian Koellermeier,Yinhua Xia,Yan Xu,Jiahui Zhang*

Main category: math.NA

TL;DR: High-order, well-balanced, path-conservative DG methods for SWLME preserve still and moving water equilibria, addressing non-conservative terms and complex steady states.


<details>
  <summary>Details</summary>
Motivation: To model vertical momentum transfer and complex velocity profiles in shallow water systems more accurately than multi-layer approaches, while overcoming numerical challenges from non-conservative terms and steady states.

Method: Develops path-conservative DG schemes using DLM theory, reformulates equations for still water, and extends DG for moving water by transforming variables and using linear segment paths.

Result: Achieves exact equilibrium preservation and high-order accuracy in scenarios with vertical velocity variations and complex topographies.

Conclusion: The proposed methods effectively balance flux gradients, non-conservative terms, and source terms, preserving equilibria and maintaining accuracy.

Abstract: This paper presents high-order, well-balanced, path-conservative
discontinuous Galerkin (DG) methods for the shallow water linearized moment
equations (SWLME), designed to preserve both still and moving water equilibrium
states. Unlike the multi-layer shallow water equations, which model vertical
velocity variations using multiple distinct layers, the SWLME employs a
polynomial expansion of velocity profiles with up to $N$ moments. This approach
enables a more detailed representation of vertical momentum transfer and
complex velocity profiles while retaining hyperbolicity. However, the presence
of non-conservative terms and complex steady-state structures introduces
significant numerical challenges. Addressing these challenges, we develop
path-conservative DG schemes grounded in the Dal Maso-LeFloch-Murat (DLM)
theory for non-conservative products. Our method balances flux gradients,
non-conservative terms, and source terms through equilibrium-preserving spaces.
For the still water equilibrium, we reformulate the equations into a
quasilinear form that eliminates source terms, inherently preserving steady
states. For the moving water equilibrium, we extend the DG method by
transforming conservative variables into equilibrium variables and employing
linear segment paths. Theoretical analysis and numerical experiments
demonstrate that the proposed methods achieve exact equilibrium preservation
while maintaining high-order accuracy, even in scenarios with vertical velocity
variations and complex topographies.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [13] [Uniform dispersive estimates for the semi-classical Hartree equation with long-range interaction](https://arxiv.org/abs/2507.12577)
*Sonae Hadama*

Main category: math.AP

TL;DR: The paper analyzes the Hartree equation in the semi-classical regime, proving optimal decay rates for small-data solutions uniformly with respect to the semi-classical parameter ℏ. It also provides a new proof for modified scattering in long-range nonlinear Schrödinger equations.


<details>
  <summary>Details</summary>
Motivation: To extend existing results on the Hartree equation by ensuring uniformity with respect to the semi-classical parameter ℏ and to offer a new proof for modified scattering in related nonlinear equations.

Method: The proof involves boundedness of modified wave operators, an L¹–L∞ dispersive estimate for the modified propagator, and commutator estimates for density operators, combined with a bootstrap argument.

Result: The density function of small-data solutions decays at the optimal rate, with conditions and bounds independent of ℏ.

Conclusion: The paper successfully achieves uniformity in ℏ and provides a novel proof for modified scattering, leveraging key analytical tools.

Abstract: In this paper, we consider the Hartree equation with smooth but long-range
interaction in the semi-classical regime, in three-dimensional space. We show
that the density function of small-data solution decays at the optimal rate.
When the semi-classical parameter $\hbar \in (0,1]$ is fixed, our result is
essentially covered by the recent work by Nguyen and You [arXiv:2408.15860];
however, the novelty of this paper is the uniformity with respect to $\hbar$.
Namely, both smallness condition for initial data and bounds for the solution
are independent of $\hbar$. Moreover, the argument in this paper provides a new
proof of the modified scattering for the long-range nonlinear Schr\"{o}dinger
equation with a Hartree type nonlinearity. Our proof relies on three main
ingredients. First, we prove the boundedness of finite-time wave operators
modified by phase corrections. Second, we show an $L^1$--$L^\infty$ dispersive
estimate for the modified propagator. Third, we give various kinds of
commutator estimates for density operators. By combining them, we can apply the
usual bootstrap argument to obtain the main result.

</details>


### [14] [Boundary Feedback and Observer Synthesis for a Class of Nonlinear Parabolic--Elliptic PDE Systems](https://arxiv.org/abs/2507.12615)
*Kamal Fenza,Moussa Labbadi,Mohamed Ouzahra*

Main category: math.AP

TL;DR: The paper studies stabilization of a coupled parabolic-elliptic PDE system with nonlinear terms, using backstepping for control and observer design, ensuring exponential stability.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of stabilizing coupled PDE systems with nonlinearities, which are common in engineering and physics.

Method: Rigorous backstepping design for explicit boundary control and exponentially convergent observers from partial measurements.

Result: Several theorems confirm exponential stability and well-posedness of the nonlinear closed-loop system.

Conclusion: The proposed method effectively stabilizes the coupled PDE system, with proven theoretical guarantees.

Abstract: This paper investigates the stabilization of a coupled system comprising a
parabolic PDE and an elliptic PDE with nonlinear terms. A rigorous backstepping
design provides an explicit boundary control law and exponentially convergent
observers from partial boundary measurements. Several theorems ensure
exponential stability and well-posedness of the nonlinear closed-loop system.

</details>


### [15] [Semi-classical limit of quantum scattering states for the nonlinear Hartree equation](https://arxiv.org/abs/2507.12627)
*Sonae Hadama,Younghun Hong*

Main category: math.AP

TL;DR: The paper studies quantum particle dynamics in the semi-classical regime, showing dispersion bounds and scattering for small-data solutions of the nonlinear Hartree equation, with results independent of the Planck constant. It also proves convergence of quantum states to classical states in the semi-classical limit and establishes small-data scattering for the Vlasov equation.


<details>
  <summary>Details</summary>
Motivation: To understand the long-time behavior of quantum particles in the semi-classical regime and bridge the gap between quantum and classical dynamics.

Method: Uses the nonlinear Hartree equation with short-range potentials, analyzes dispersion bounds, and employs Wigner transforms to study the semi-classical limit. A new uniform dispersion estimate for the free Schrödinger flow is introduced.

Result: Small-data solutions scatter, and quantum states converge to classical states in the semi-classical limit. Small-data scattering for the Vlasov equation is established without regularity assumptions.

Conclusion: The work provides a rigorous link between quantum and classical scattering dynamics, with applications to singular potentials.

Abstract: This article concerns the long-time dynamics of quantum particles in the
semi-classical regime. First, we show that for the nonlinear Hartree equation
with short-range interaction potential, small-data solutions obey dispersion
bounds and they scatter, where the smallness conditions and the bounds are
independent of the small parameter $\hbar\in(0,1]$ representing the reduced
Planck constant. Then, taking the semi-classical limit $\hbar\to0$, we prove
that the Wigner transforms of such quantum scattering states converge weakly-*
to the corresponding classical scattering states for the Vlasov equation. As a
direct consequence, we establish small-data scattering for the Vlasov equation
without assuming regularity on initial data. Our analysis is based on a new
uniform dispersion estimate for the free Schr\"odinger flow, which is simple
but crucial to include singular interaction potentials such as inverse
power-law potential $\frac{1}{|x|^a}$ with $1<a<\frac{5}{3}$.

</details>


### [16] [Asymptotically sharp stability of Sobolev inequalities on the Heisenberg group with dimension-dependent constants](https://arxiv.org/abs/2507.12725)
*Lu Chen,Guozhen Lu,Hanli Tang,Bohan Wang*

Main category: math.AP

TL;DR: The paper establishes optimal stability bounds for Sobolev and Hardy-Littlewood-Sobolev inequalities on the Heisenberg group using CR Yamabe flow, avoiding rearrangement techniques.


<details>
  <summary>Details</summary>
Motivation: To address the lack of rearrangement inequality in the CR setting, which complicates deriving optimal stability bounds for Sobolev inequality.

Method: Uses bispherical harmonics and orthogonality for local stability on the CR sphere, then employs CR Yamabe flow for global stability on the Heisenberg group.

Result: Achieves optimal stability for Sobolev and HLS inequalities with dimension-dependent constants.

Conclusion: The rearrangement-free approach is versatile and applicable to fractional Sobolev and HLS inequalities on the Heisenberg group.

Abstract: In this paper, we are concerned with the optimal asymptotic lower bound for
the stability of Sobolev inequality on the Heisenberg group. We first establish
the optimal local stability of Sobolev inequality on the CR sphere through
bispherical harmonics and complicated orthogonality technique ( see Lemma 3.1).
The loss of rearrangement inequality in the CR setting makes it impossible to
use any rearrangement flow technique (either differential rearrangement flow or
integral rearrangement flow) to derive the optimal stability of Sobolev
inequality on the CR sphere from corresponding optimal local stability. To
circumvent this, we will use the CR Yamabe flow to establish the optimal
stability of Sobolev inequality on the Heisenberg group with the
dimension-dependent constants (see Theorem 1.1). As an application, we also
establish the optimal stability of the Hardy-Littlewood-Sobolev (HLS)
inequality for special conformal index with the dimension-dependent constants
(see Theorem 1.3). Our approach is rearrangement-free and can be used to study
the optimal stability problem for fractional Sobolev inequality or HLS
inequality on the Heisenberg group once the corresponding continuous flow is
established.

</details>


### [17] [Analysis of a parabolic-hyperbolic hybrid population model: an integrated semigroup approach](https://arxiv.org/abs/2507.12833)
*Qihua Huang,Minglong Wang,Yixiang Wu*

Main category: math.AP

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper is concerned with the global dynamics of a hybrid
parabolic-hyperbolic model describing populations with distinct dispersal and
sedentary stages. We first establish the global well-posedness of solutions,
prove a comparison principle, and demonstrate the asymptotic smoothness of the
solution semiflow. Through the spectral analysis of the linearized system, we
derive and characterize the net reproductive rate $\mathcal{R}_{0}$.
Furthermore, an explicit relationship between $\mathcal{R}_{0}$ and the
principal eigenvalue of the linearized system is analyzed. Under appropriate
monotonicity assumptions, we show that $\mathcal{R}_{0}$ serves as a threshold
parameter that completely determines the stability of steady states of the
system. More precisely, when $\mathcal{R}_{0}<1$, the trivial equilibrium is
globally asymptotical stable, while when $\mathcal{R}_{0}>1$, the system is
uniformly persistent and there is a positive equilibrium which is unique and
globally asymptotical stable.

</details>


### [18] [Polyharmonic Nonlinear Scalar Field Equations](https://arxiv.org/abs/2507.12962)
*Alessandro Cannone,Silvia Cingolani,Jarosław Mederski*

Main category: math.AP

TL;DR: Existence of ground state solutions for polyharmonic nonlinear equations with subcritical growth, overcoming analytical challenges and introducing a new logarithmic Sobolev inequality.


<details>
  <summary>Details</summary>
Motivation: Inspired by Berestycki and Lions, the study addresses the challenges posed by higher-order operators in polyharmonic equations.

Method: Analyzes the polyharmonic nonlinear equation $(-\Delta)^m u=g(u)$ with subcritical growth assumptions.

Result: Demonstrates existence of ground state solutions and establishes a new polyharmonic logarithmic Sobolev inequality.

Conclusion: The work successfully tackles analytical challenges in higher-order operators and contributes a novel inequality to the field.

Abstract: In this paper, we present a result on the existence of ground state solutions
for the polyharmonic nonlinear equation $(-\Delta)^m u=g(u)$, assuming that $g$
has a general subcritical growth at infinity, inspired by Berestycki and Lions
\cite{BerestyckiLions}. In comparison with the biharmonic case studied in
\cite{Med-Siem}, the presence of a higher-order operator gives rise to several
analytical challenges, which are overcome in the present work. Furthermore, we
establish a new polyharmonic logarithmic Sobolev inequality.

</details>


### [19] [Symmetrization on the sphere and applications](https://arxiv.org/abs/2507.13027)
*Satyanad Kichenassamy*

Main category: math.AP

TL;DR: A new symmetrization method for mappings on the n-sphere is introduced, applied to estimate solutions of p-Laplacian quasilinear elliptic PDEs with Dirac measures. The case p=n is solved via conformal transformation, while other cases (1<p<n, p>n) are briefly discussed.


<details>
  <summary>Details</summary>
Motivation: To develop a symmetrization technique for solving quasilinear elliptic PDEs with Dirac measures, particularly focusing on the p-Laplacian case.

Method: Introduces a new symmetrization method for mappings on the n-sphere, using conformal transformation for the p=n case and referencing other works for 1<p<n and p>n.

Result: The method provides estimates for solutions of the PDEs, with detailed results for p=n and references for other cases.

Conclusion: The symmetrization technique is effective for solving p-Laplacian PDEs, with p=n being the primary focus and other cases addressed elsewhere.

Abstract: We introduce a new method of symmetrization of mappings on the $n$-sphere
($n\geq 2$). They are applied to estimate solutions of quasilinear elliptic
partial differential equations of $p$-Laplacian type, with combinations of
Dirac measures on the right-hand side. The case $p=n$ is reduced to a problem
on the sphere, using a conformal transformation. The cases when $1 < p < n $
and $p > $n are considered more briefly, full details being available in other
papers of the author.

</details>


### [20] [Exponential convergence for ultrafast diffusion equations with log-concave weights](https://arxiv.org/abs/2507.13060)
*Max Fathi,Mikaela Iacobelli*

Main category: math.AP

TL;DR: The paper analyzes the asymptotic behavior of a weighted ultrafast diffusion PDE on the real line, proving exponential convergence to equilibrium under log-concave and log-Lipschitz weight conditions.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the gradient flow approach to the quantization of measures, as introduced in prior work [11].

Method: The authors study the asymptotic behavior of a weighted ultrafast diffusion PDE on the real line, focusing on log-concave and log-Lipschitz weights.

Result: They prove exponential convergence to equilibrium, extending results beyond the compact setting of previous research [22].

Conclusion: The findings demonstrate significant progress in understanding the behavior of such PDEs, particularly in non-compact settings.

Abstract: We study the asymptotic behavior of a weighted ultrafast diffusion PDE on the
real line, with a log-concave and log-lipschitz weight, and prove exponential
convergence to equilibrium. This result goes beyond the compact setting studied
in [22]. This equation is motivated by the gradient flow approach to the
problem of quantization of measures introduced in [11].

</details>


### [21] [Nonlinear smoothing implies improved lower bounds on the radius of spatial analyticity for nonlinear dispersive equations](https://arxiv.org/abs/2507.13083)
*Mikaela Baldasso,Simão Correia*

Main category: math.AP

TL;DR: Improved lower bounds on the decay rate of the uniform radius of analyticity for nonlinear dispersive equations, achieving σ(T)≳T^(-1/2-ε).


<details>
  <summary>Details</summary>
Motivation: To enhance understanding of the decay rate of analyticity in nonlinear dispersive equations, focusing on the generalized KdV and nonlinear Schrödinger equations.

Method: Developed a strategy involving nonlinear smoothing estimates with specific derivative distributions, applied to defocusing generalized KdV and nonlinear Schrödinger equations.

Result: Achieved a lower bound σ(T)≳T^(-1/2-ε) for any ε>0, surpassing existing literature results.

Conclusion: The proposed method successfully improves lower bounds on analyticity decay rates, offering advancements for both equations studied.

Abstract: We provide a roadmap to establish improved lower bounds on the decay rate of
the uniform radius of analyticity $\sigma(T)$ for a given nonlinear dispersive
equation, reducing the problem to the derivation of nonlinear smoothing
estimates with a specific distribution of extra derivatives. We apply this
strategy for both the defocusing generalized KdV and the nonlinear
Schr\"odinger equations with odd pure-power nonlinearity. For both equations,
we reach the lower bound $\sigma(T)\gtrsim T^{-\frac{1}{2}-\epsilon}$, for any
$\epsilon>0$, thus improving all available results in the current literature.

</details>


### [22] [A New Framework for Unidimensional Structures Based on Generalised Continua](https://arxiv.org/abs/2507.13098)
*Mewen Crespo,Casale Guy,Loïc Le Marrec,Patrizio Neff*

Main category: math.AP

TL;DR: The paper introduces a family of beam models based on a 3D higher-order elasticity framework, exploring three kinematic regimes (holonomic, semi-holonomic, non-holonomic) to capture material defects in beams.


<details>
  <summary>Details</summary>
Motivation: To systematically explore varying levels of kinematic constraints in beam models, from classical elasticity to fully relaxed models, and unify dislocations and disclinations.

Method: Incorporates three kinematic fields (u, P, N) and derives simplified ODE systems for specific cases like traction and bending.

Result: The framework hierarchically reduces to higher-order Euler-Bernoulli and Timoshenko models, with the non-holonomic case unifying dislocations and disclinations.

Conclusion: The proposed framework effectively captures material defects in beam structures, with practical applicability demonstrated through simplified cases.

Abstract: The present work introduces a family of beam models derived from a
three-dimensional higher-order elasticity framework. By incorporating three
kinematic fields - the macroscopic displacement u, the micro-distortion tensor
P, and the third-order tensor N - the study systematically explores three
regimes: holonomic, semi-holonomic, and non-holonomic. These regimes correspond
to varying levels of kinematic constraints, ranging from classical elasticity
to a fully relaxed model. The holonomic case reduces to a higher-order
Euler--Bernoulli beam model, while the semi-holonomic case generalises the
Timoshenko beam model. The non-holonomic case provides a unified framework that
naturally incorporates both dislocations and disclinations. Furthermore, the
holonomic and semi-holonomic models are shown to emerge as singular limits of
the non-holonomic model by increasing specific penalty coefficients. Simplified
ordinary differential equation systems are derived for specific cases, such as
pure traction and bending, illustrating the practical applicability of the
models. The results highlight the hierarchical structure of the proposed
framework and its ability to capture material defects in beam-like structures.

</details>


### [23] [Normalized solutions of coupled Sobolev critical Schrodinger equations with mass subcritical couplings](https://arxiv.org/abs/2507.13163)
*Zhang Jianjun,Zhong Xuexiu,Zhou Jinfang*

Main category: math.AP

TL;DR: The paper studies positive solutions to coupled Sobolev critical Schrödinger equations with mass constraints, focusing on the mass mixed case. It shows the existence of two positive solutions for small ν and analyzes their asymptotic behavior as ν approaches zero.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address a Soave's type open problem raised by Bartsch et al., focusing on qualitative properties of solutions under specific conditions.

Method: The method involves analyzing the system of equations with mass constraints, using variational techniques to identify solutions (local minimizer and mountain pass solution) for small ν.

Result: For small ν, the system admits two positive solutions. As ν→0, the asymptotic behavior of these solutions is analyzed.

Conclusion: The results affirmatively answer the open problem, demonstrating the existence and behavior of solutions in the mass mixed case.

Abstract: We are concerned with qualitative properties of positive solutions to the
following coupled Sobolev critical Schr\"odinger equations $$ \begin{cases}
-\Delta u+\lambda_1 u=\mu_1|u|^{2^*-2}u+\nu\alpha |u|^{\alpha-2}|v|^{\beta}u
~\hbox{in}~ \R^N,\\ -\Delta v+\lambda_2 v=\mu_2|v|^{2^*-2}v+\nu\beta
|u|^{\alpha}|v|^{\beta-2}v ~\hbox{in}~ \R^N \end{cases} $$ subject to the mass
constraints $\int_{\mathbb{R}^N}|u|^2 \ud x=a^2$ and $\int_{\mathbb{R}^N}|v|^2
\ud x=b^2$, where, $a>0,\,b>0,\,N=3,4$ and $2^*:=\frac{2N}{N-2}$ is the Sobolev
critical exponent. The main purpose of this paper is focused on the mass mixed
case, i. e., $ \alpha>1,\beta>1,\alpha+\beta<2+\frac{4}{N}$. For some suitable
small $\nu>0$, we show that the above system admits two positive solutions, one
of which is a local minimizer, and another one is a mountain pass solution.
Moreover, as $\nu\to0^+$, asymptotic behaviors of solutions are also
considered. Our result gives an affirmative answer to a Soave's type open
problem raised by Bartsch {\it et al.} (Calc. Var. Partial Differential
Equations 62(1), Paper No. 9, 34, 2023).

</details>


### [24] [Robin Green Function Estimates and a Model of Mammalian Lungs](https://arxiv.org/abs/2507.13168)
*Guy David,Stefano Decio,Max Engelstein,Marcel Filoche,Svitlana Mayboroda,Marco Michetti*

Main category: math.AP

TL;DR: The paper analyzes the Green function with Robin boundary conditions, revealing transitions between Dirichlet-like and Neumann-like behaviors, and provides bounds on harmonic measure, confirming a phase transition in flow behavior.


<details>
  <summary>Details</summary>
Motivation: To understand the properties of the Green function under Robin boundary conditions and its implications for harmonic measure and flow behavior, particularly in biological contexts like mammalian lungs.

Method: The study establishes delicate properties of the Green function, focusing on transitions between Dirichlet-like and Neumann-like behaviors, and quantifies bounds on harmonic measure.

Result: Sharp bounds on harmonic measure are derived, and the phase transition in total flow behavior, previously conjectured in physics literature, is proven.

Conclusion: The findings elucidate the Green function's behavior under Robin boundary conditions and confirm a phase transition in flow, with potential applications in biological systems like lungs.

Abstract: The present paper establishes delicate properties of the Green function with
Robin boundary conditions, in particular, elucidating the nature of the passage
between the Dirichlet-like and Neumann-like behavior. This yields sharp
quantifiable bounds on the corresponding harmonic measure and proves the phase
transition in the behavior of the total flow earlier conjectured in physics
literature in concert with the efficacy of mammalian lungs.

</details>


### [25] [Multiple normalized solutions for two coupled Gross-Pitaevskii equations with attractive interactions and mass constriants](https://arxiv.org/abs/2507.13172)
*Zhang Jianjun,Zhong Xuexiu,Zhou Jinfang*

Main category: math.AP

TL;DR: The paper studies a system of coupled Gross-Pitaevskii equations in two-component Bose-Einstein condensates, focusing on solutions with constrained mass. It identifies conditions for two positive solutions, including a local minimizer and a mountain pass solution.


<details>
  <summary>Details</summary>
Motivation: The research is motivated by the need to understand the behavior of two-component Bose-Einstein condensates, particularly under attractive interactions and mass constraints.

Method: The system is analyzed using variational methods to seek critical points of the associated functional, with constraints on the mass.

Result: For specific parameter ranges, the system admits two positive solutions: a local minimizer and a mountain pass solution.

Conclusion: The findings provide insights into the existence and nature of solutions in such systems, contributing to the broader understanding of Bose-Einstein condensates.

Abstract: We are concerned with the following system of two coupled time-independent
Gross-Pitaevskii equations $$ \begin{cases} -\Delta u+\lambda_1
u=\mu_1|u|^{p-2}u+\nu\alpha |u|^{\alpha-2}|v|^{\beta}u ~\hbox{in}~ \R^N,\\
-\Delta v+\lambda_2 v=\mu_2|v|^{q-2}v+\nu\beta |u|^{\alpha}|v|^{\beta-2}v
~\hbox{in}~ \R^N, \end{cases} $$ which arises in two-components Bose-Einstein
condensates and involve attractive Sobolev subcritical or critical
interactions, i. e., $\nu>0$ and $\alpha+\beta\leq 2^*$. This system is
employed by seeking critical points of the associated variational functional
with the constrained mass below $$\int_{\mathbb{R}^N}|u|^2 {\rm d}x=a, \quad
\int_{\mathbb{R}^N}|v|^2 {\rm d}x=b.$$ In the mass mixed case, i. e.,
$2<p<2+\frac{4}{N}<q<2^*$, for some suitable $a,b,\nu$ and $\beta$, the system
above admits two positive solutions. In particular, in the case
$\alpha+\beta<2^*$, using variational methods on the $L^2$-ball, two positive
solutions are obtained, one of which is a local minimizer and the second one is
a mountain pass solution.

</details>


### [26] [Pointwise convergence to initial data of heat and Poisson equations in Modulation Spaces](https://arxiv.org/abs/2507.13220)
*Divyang G. Bhimani,Rupak K. Dalai*

Main category: math.AP

TL;DR: The paper studies weighted modulation spaces where the heat and Poisson semigroups converge pointwise to initial data as time approaches zero, and proves the Hardy-Littlewood maximal operator's action on these spaces.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of heat and Poisson semigroups on weighted modulation spaces and their convergence properties, as well as the role of the Hardy-Littlewood maximal operator in these spaces.

Method: Analyzes the pointwise convergence of semigroups (heat and Poisson) for the Laplacian and Hermite operator, and investigates the operation of the Hardy-Littlewood maximal operator on modulation spaces.

Result: Establishes conditions under which semigroups converge pointwise to initial data and demonstrates the Hardy-Littlewood maximal operator's applicability in certain modulation spaces.

Conclusion: The findings provide insights into semigroup convergence and the utility of the Hardy-Littlewood maximal operator in modulation spaces, with potential independent significance.

Abstract: We characterize weighted modulation spaces (data space) for which the heat
semigroup $e^{-tL}f$ converges pointwise to the initial data $f$ as time $t$
tends to zero. Here $L$ stands for the standard Laplacian $-\Delta $ or Hermite
operator $H=-\Delta +|x|^2$ on the whole domain. Similar result also holds for
Poisson semigroup $e^{-t\sqrt{L}}f.$ We also prove that the Hardy-Littlewood
maximal operator operates on certain modulation spaces. This may be of
independent interest.

</details>


### [27] [Rigidity for the heat equation with density on Riemannian manifolds through a conformal change](https://arxiv.org/abs/2507.13230)
*Alexander Grigor'yan,Giulia Meglioli,Alberto Roncoroni*

Main category: math.AP

TL;DR: The paper studies uniqueness of solutions to the heat equation on weighted Riemannian manifolds, focusing on conditions for solutions to vanish in weighted Lebesgue spaces.


<details>
  <summary>Details</summary>
Motivation: To identify conditions ensuring solutions vanish in weighted spaces, addressing cases for $p > 1$ and $p = 1$.

Method: Uses a conformal metric transformation to reduce the problem to a standard heat equation on a weighted manifold, with explicit counterexamples for validation.

Result: Provides sufficient conditions for solution uniqueness, demonstrating optimality of assumptions on the density function.

Conclusion: The method and assumptions are validated, offering insights into solution behavior in weighted spaces.

Abstract: We investigate uniqueness of solution to the heat equation with a density
$\rho$ on complete, non-compact weighted Riemannian manifolds of infinite
volume. Our main goal is to identify sufficient conditions under which the
solution $u$ vanishes identically, assuming that $u$ belongs to a certain
weighted Lebesgue space with exponential or polynomial weight, $L^p_{\phi}$. We
distinguish between the cases $p > 1$ and $p = 1$ which required stronger
assumptions on the manifold and the density function $\rho$. We develop a
unified method based on a conformal transformation of the metric, which allows
us to reduce the problem to a standard heat equation on a suitably weighted
manifold. In addition, we construct explicit counterexamples on model manifolds
which demonstrate optimality of our assumptions on the density $\rho$.

</details>


### [28] [The Snapshot Problem for the Euler-Poisson-Darboux Equation](https://arxiv.org/abs/2507.13257)
*Fulton Gonzalez,Jue Wang,Jens Christensen,Tomoyuki Kakehi*

Main category: math.AP

TL;DR: The paper explores existence and uniqueness conditions for solutions to the generalized Euler-Poisson-Darboux equation with a two-snapshot problem, revealing connections to Liouville-like numbers and Bessel functions.


<details>
  <summary>Details</summary>
Motivation: To understand the conditions under which solutions exist for the generalized EPD equation given two specific snapshots, and to explore the properties of associated Liouville-like numbers.

Method: Analyzes the generalized EPD equation with complex parameter α, focusing on solutions for fixed radii and smooth functions, linking results to Bessel functions and Liouville-like numbers.

Result: Identifies conditions for solution existence and uniqueness, uncovering Liouville-like numbers related to Bessel functions and studying their properties.

Conclusion: The study provides insights into the generalized EPD equation's solutions and introduces new connections to Liouville-like numbers and Bessel functions.

Abstract: The generalized Euler-Poisson-Darboux (EPD) equation with complex parameter
$\alpha$ is given by $$ \Delta_x u=\frac{\partial^2 u}{\partial
t^2}+\frac{n-1+2\alpha}{t}\,\frac{\partial u}{\partial t}, $$ where $u(x,t)\in
\mathscr E(\mathbb R^n\times \mathbb R)$, with $u$ even in $t$. For $\alpha=0$
and $\alpha=1$ the solution $u(x,t)$ represents a mean value over spheres and
balls, respectively, of radius $|t|$ in $\mathbb R^n$. In this paper we
consider existence and uniqueness results for the following two-snapshot
problem: for fixed positive real numbers $r$ and $s$ and smooth functions $f$
and $g$ on $\mathbb R^n$, what are the conditions under which there is a
solution $u(x,t)$ to the generalized EPD equation such that $u(x,r)=f(x)$ and
$u(x,s)=g(x)$? The answer leads to a discovery of Liouville-like numbers
related to Bessel functions, and we also study the properties of such numbers.

</details>


### [29] [Homogenization of nonlocal exchange energies in micromagnetics](https://arxiv.org/abs/2507.13262)
*Rossella Giorgio,Leon Happ,Hidde Schönberger*

Main category: math.AP

TL;DR: The paper analyzes the homogenization of nonlocal micromagnetic functionals with symmetric and antisymmetric exchange contributions, constrained to the unit sphere. It identifies the Γ-limit of these energies, leading to an effective local functional.


<details>
  <summary>Details</summary>
Motivation: To understand the asymptotic behavior of nonlocal micromagnetic energies under vanishing interaction range and heterogeneities, while respecting the unit sphere constraint.

Method: Uses a tailored two-scale convergence approach, focusing on specific oscillation directions, and analyzes nonlocal difference quotients.

Result: Derives an effective local functional via a tangentially constrained nonlocal cell problem, with microscopic oscillations constrained to the tangent space.

Conclusion: The study successfully captures the homogenization limit of nonlocal micromagnetic energies, providing insights into their local behavior under constraints.

Abstract: We study the homogenization of nonlocal micromagnetic functionals
incorporating both symmetric and antisymmetric exchange contributions under the
physical constraint that the magnetization field takes values in the unit
sphere. Assuming that the nonlocal interaction range and the scale of
heterogeneities vanish simultaneously, we capture the asymptotic behavior of
the nonlocal energies by identifying their $\Gamma$-limit, leading to an
effective local functional expressed through a tangentially constrained
nonlocal cell problem. Our proof builds upon a tailored notion of two-scale
convergence, which takes into account oscillations only in specific directions.
It enables us to describe the two-scale limit of suitable nonlocal difference
quotients, yielding a nonlocal analog of the classical limit decomposition
result for gradient fields. To deal with the manifold constraint of the
magnetization, we additionally prove that the microscopic oscillations in the
two-scale limit are constrained to lie in the tangent space of the sphere.

</details>


### [30] [A hierarchy of blood vessel models, Part I: 3D-1D to 1D](https://arxiv.org/abs/2507.13316)
*Laurel Ohm,Sarah Strikwerda*

Main category: math.AP

TL;DR: The paper proposes and analyzes models for blood perfusion in tissue around a thin vessel, focusing on convergence between 3D-3D, 3D-1D, and 1D methods. Part I shows well-posedness of a 3D-1D model and its convergence to a 1D Green's function model. Part II extends this to a 3D-3D Darcy-Stokes system.


<details>
  <summary>Details</summary>
Motivation: To rigorously establish convergence among hierarchical models (3D-3D, 3D-1D, 1D) for blood perfusion, ensuring accuracy and consistency across scales.

Method: Proposes a 3D-1D Darcy-Poiseuille system with specific boundary conditions, analyzes its well-posedness, and derives a 1D integrodifferential equation. Uses a priori bounds to show convergence rates.

Result: The 1D model converges to the 3D-1D solution with a rate proportional to ε^(1/2)|logε|. Part II extends convergence to the 3D-3D Darcy-Stokes system.

Conclusion: The study successfully links hierarchical models for blood perfusion, providing rigorous convergence results and validating the use of simplified models.

Abstract: We propose and analyze a family of models describing blood perfusion through
a tissue surrounding a thin blood vessel. Our goal is to rigorously establish
convergence results among 3D-3D Darcy--Stokes, 3D-1D Darcy--Poiseuille, and 1D
Green's function methods commonly used to model this process. In Part I, we
propose a 3D-1D Darcy--Poiseuille system where the coupling across the
permeable vessel surface involves an angle-averaged Neumann boundary condition
coupled with a geometrically constrained Robin boundary condition. We show that
this model is well-posed and moreover limits to a 1D Green's function model as
the maximum vessel radius $\epsilon\to 0$. In the 1D model, the exterior blood
pressure is given by an explicit Green's function expression involving the
interior blood pressure. The interior pressure satisfies a novel 1D
integrodifferential equation in which the integral term incorporates the
effects of the exterior pressure and the vessel geometry. Much of this paper is
devoted to analyzing this integrodifferential equation. Using the \emph{a
priori} bounds obtained here, we show that the solution to the 1D model
converges to the 3D-1D solution with a rate proportional to
$\epsilon^{1/2}|\log\epsilon|$. In Part II [Ohm \& Strikwerda, arXiv preprint
July 2025], we rely on the 1D estimates to show that both the 1D and 3D-1D
models converge to a coupled 3D-3D Darcy-Stokes system as $\epsilon\to 0$,
thereby establishing a convergence chain among all hierarchy levels.

</details>


### [31] [A hierarchy of blood vessel models, Part II: 3D-3D to 3D-1D and 1D](https://arxiv.org/abs/2507.13330)
*Laurel Ohm,Sarah Strikwerda*

Main category: math.AP

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We propose and analyze a hierarchy of three models of blood perfusion through
a tissue surrounding a thin arteriole or venule. Our goal is to rigorously link
3D-3D Darcy--Stokes, 3D-1D Darcy--Poiseuille, and 1D Green's function methods
commonly used to model this process. Here in Part II, we consider the most
detailed level, a 3D-3D Darcy-Stokes system coupled across the permeable vessel
surface by mass conservation and pressure/stress balance conditions. We derive
a convergence result between the 3D-3D model and both the 3D-1D
Darcy--Poiseuille model and 1D Green's function model proposed in Part I [Ohm
\& Strikwerda, arXiv preprint July 2025] at a rate proportional to
$\epsilon^{1/6}|\log\epsilon|$, where $\epsilon$ is the maximum vessel radius.
The rate is limited by the inclusion of a degenerate endpoint where the vessel
radius vanishes, i.e. becomes indistinguishable from a capillary. Key to our
proof are \emph{a priori} estimates for the 1D integrodifferential model
obtained in Part I.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [32] [Shape optimization of metastable states](https://arxiv.org/abs/2507.12575)
*Noé Blassel,Tony Lelièvre,Gabriel Stoltz*

Main category: physics.comp-ph

TL;DR: The paper proposes a new method for defining metastable states in molecular simulations, addressing limitations of energy-based definitions by optimizing a local timescale metric linked to accelerated dynamics algorithms.


<details>
  <summary>Details</summary>
Motivation: Standard definitions of metastable states, based on energy minimization, often fail when entropic effects or thermal fluctuations dominate. A more robust approach is needed.

Method: The approach involves shape-optimization of a local timescale metric, using analytic expressions for Dirichlet eigenvalues and a local ascent algorithm. Two tractability methods for high-dimensional systems are proposed.

Result: The method is validated on a biomolecular system, showing significant improvement over conventional definitions.

Conclusion: The proposed approach provides a more effective way to define metastable states, especially in complex systems where traditional methods fall short.

Abstract: The definition of metastable states is an ubiquitous task in the design and
analysis of molecular simulation, and is a crucial input in a variety of
acceleration methods for the sampling of long configurational trajectories.
  Although standard definitions based on local energy minimization procedures
can sometimes be used, these definitions are typically suboptimal, or entirely
inadequate when entropic effects are significant, or when the lowest energy
barriers are quickly overcome by thermal fluctuations.
  In this work, we propose an approach to the definition of metastable states,
based on the shape-optimization of a local separation of timescale metric
directly linked to the efficiency of a class of accelerated molecular dynamics
algorithms.
  To realize this approach, we derive analytic expressions for shape-variations
of Dirichlet eigenvalues for a class of operators associated with reversible
elliptic diffusions, and use them to construct a local ascent algorithm,
explicitly treating the case of multiple eigenvalues.
  We propose two methods to make our method tractable in high-dimensional
systems: one based on dynamical coarse-graining, the other on recently obtained
low-temperature shape-sensitive spectral asymptotics.
  We validate our method on a benchmark biomolecular system, showcasing a
significant improvement over conventional definitions of metastable states.

</details>


### [33] [TinyDEM: Minimal open granular DEM code with sliding, rolling and twisting friction](https://arxiv.org/abs/2507.12610)
*Roman Vetter*

Main category: physics.comp-ph

TL;DR: TinyDEM is a lightweight 3D DEM solver for simulating granular particles, designed for simplicity and accessibility in C++11.


<details>
  <summary>Details</summary>
Motivation: To provide a compact, standalone DEM solver for granular particle dynamics, avoiding complex programming concepts.

Method: Uses explicit solving of Newton's damped equations for translations and rotations, quaternions for orientation, and models inelastic, frictional collisions with torque exchange. Includes particle-mesh collision for complex geometries.

Result: A simple, parallelized (OpenMP) C++11 program under BSD license, suitable for entry-level DEM simulations or foundational use.

Conclusion: TinyDEM is an accessible, efficient tool for DEM simulations, serving as a starting point or base for advanced models.

Abstract: This article introduces TinyDEM, a lightweight implementation of a
full-fledged discrete element method (DEM) solver in 3D. Newton's damped
equations of motion are solved explicitly for translations and rotations of a
polydisperse ensemble of dry, soft, granular spherical particles, using
quaternions to represent their orientation in space without gimbal lock.
Particle collisions are modeled as inelastic and frictional, including full
exchange of torque. With a general particle-mesh collision routine, complex
rigid geometries can be simulated. TinyDEM is designed to be a compact
standalone program written in simple C++11, devoid of explicit pointer
arithmetics and advanced concepts such as manual memory management or
polymorphism. It is parallelized with OpenMP and published freely under the
3-clause BSD license. TinyDEM can serve as an entry point into classical DEM
simulations or as a foundation for more complex models of particle dynamics.

</details>


### [34] [Equalized Hyperspin Machine](https://arxiv.org/abs/2507.12940)
*Marcello Calvanese Strinati,Claudio Conti*

Main category: physics.comp-ph

TL;DR: The paper introduces a method to equalize hyperspin amplitudes in a hyperspin machine, improving its performance in simulating spin models.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a mechanism for equal hyperspin amplitudes in the hyperspin machine, which is crucial for reliable simulation of spin models.

Method: Uses an additional network of oscillators (equalizers) with antisymmetric nonlinear coupling to equalize hyperspin amplitudes.

Result: The equalized hyperspin machine achieves significantly lower spin energy and reduced sensitivity to system parameters.

Conclusion: The method enhances the hyperspin machine's performance, making it a competitive spin Hamiltonian minimizer and enabling further improvements with annealing protocols.

Abstract: The reliable simulation of spin models is of critical importance to tackle
complex optimization problems that are intractable on conventional computing
machines. The recently introduced hyperspin machine, which is a network of
linearly and nonlinearly coupled parametric oscillators, provides a versatile
simulator of general classical vector spin models in arbitrary dimension,
finding the minimum of the simulated spin Hamiltonian and implementing novel
annealing algorithms. In the hyperspin machine, oscillators evolve in time
minimizing a cost function that must resemble the desired spin Hamiltonian in
order for the system to reliably simulate the target spin model. This condition
is met if the hyperspin amplitudes are equal in the steady state. Currently, no
mechanism to enforce equal amplitudes exists. Here, we bridge this gap and
introduce a method to simulate the hyperspin machine with equalized amplitudes
in the steady state. We employ an additional network of oscillators (named
equalizers) that connect to the hyperspin machine via an antisymmetric
nonlinear coupling and equalize the hyperspin amplitudes. We demonstrate the
performance of such an equalized hyperspin machine by large-scale numerical
simulations up to $10000$ hyperspins. Compared to the hyperspin machine without
equalization, we find that the equalized hyperspin machine (i) Reaches orders
of magnitude lower spin energy, and (ii) Its performance is significantly less
sensitive to the system parameters. The equalized hyperspin machine offers a
competitive spin Hamiltonian minimizer and opens the possibility to combine
amplitude equalization with complex annealing protocols to further boost the
performance of spin machines.

</details>


### [35] [A variationally consistent and asymptotically convergent phase-field model for solute precipitation and dissolution](https://arxiv.org/abs/2507.13270)
*Andrea Lamperti,Laura De Lorenzis*

Main category: physics.comp-ph

TL;DR: A novel phase-field model for solute precipitation and dissolution in liquid solutions is proposed, derived variationally from a free energy functional, and validated through numerical examples.


<details>
  <summary>Details</summary>
Motivation: To address solute precipitation and dissolution with a more consistent and accurate model than previous studies.

Method: Derives coupled governing equations (Allen-Cahn and Cahn-Hilliard) from a Modica-Mortola free energy functional, assesses convergence via matched asymptotic expansions, and uses finite element discretization for numerical examples.

Result: The model converges to the sharp-interface model and provides a novel reaction rate expression. Numerical examples in 2D and 3D demonstrate its effectiveness.

Conclusion: The proposed phase-field model is a robust and consistent approach for studying solute precipitation and dissolution, validated by theoretical and numerical results.

Abstract: We propose a novel phase-field model for solute precipitation and dissolution
in liquid solutions. Unlike in previous studies with similar scope, in our
model the two non-linear coupled governing equations of the problem, which
deliver the solute ion concentration and the phase-field variable, are derived
in a variationally consistent way starting from a free energy functional of
Modica-Mortola type. The phase-field variable is assumed to follow the
non-conservative Allen-Cahn evolution law, whereas the solute ion concentration
obeys the conservative Cahn-Hilliard equation. We also assess the convergence
of the new model to the corresponding sharp-interface model via the method of
matched asymptotic expansions, and derive a novel expression of the reaction
rate of the sharp-interface model. Through a finite element discretization, we
present several numerical examples in two and three dimensions.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [36] [Augmented Lagrangian methods produce cutting-edge magnetic coils for stellarator fusion reactors](https://arxiv.org/abs/2507.12681)
*Pedro F. Gil,Alan A. Kaptanoglu,Eve V. Stenson*

Main category: physics.plasm-ph

TL;DR: An augmented Lagrangian approach improves stellarator coil optimization, outperforming previous methods and achieving better physics and engineering results.


<details>
  <summary>Details</summary>
Motivation: Current methods for designing stellarator coils struggle with nonconvex optimization and inefficiency, leading to suboptimal solutions.

Method: The paper introduces an augmented Lagrangian approach to optimize coil designs, addressing the ill-posed nature of the problem.

Result: The method produces superior coil solutions for various stellarators, outperforming existing designs, including those of W7-X and HSX.

Conclusion: The approach is effective and versatile, demonstrating improved performance across diverse stellarator configurations.

Abstract: Finding feasible coils for stellarator fusion devices is a crux of realizing
this concept for future power plants. The coils must reproduce the physics
properties of the target plasma while satisfying myriad engineering
constraints. Current efforts struggle to navigate the highly nonconvex
optimization landscape and result in suboptimal stellarator coils, and/or they
end up spending considerable resources scanning the continuous parameter space.
In this work, we present an augmented Lagrangian approach to tackle the
ill-posed problem of coil optimization. Our approach yields solutions that
out-perform previous Pareto-optimal work and display excellent physics and
engineering properties. We conclude by illustrating its effectiveness and
versatility by generating coils for five stellarators with very different
symmetries and magnetic field shaping. In all cases, we find coil solutions
that in various ways outperform published coil sets, including built and
operating reactors such as Wendelstein 7-X (W7-X) and the Helically Symmetric
eXperiment (HSX).

</details>


### [37] [Early Prediction of Current Quench Events in the ADITYA Tokamak using Transformer based Data Driven Models](https://arxiv.org/abs/2507.12797)
*Jyoti Agarwal,Bhaskar Chaudhury,Jaykumar Navadiya,Shrichand Jakhar,Manika Sharma*

Main category: physics.plasm-ph

TL;DR: A transformer-based deep learning model is proposed for early current-quench prediction in tokamak plasmas, outperforming LSTM baselines with high recall and robustness up to 8 ms lead time.


<details>
  <summary>Details</summary>
Motivation: Disruptions in tokamak plasmas threaten system integrity, necessitating accurate early prediction for effective mitigation.

Method: Transformer-based deep learning models are applied to multivariate time series data from ADITYA tokamak diagnostics.

Result: The transformer model achieves recall above 0.9 up to 8-10 ms prediction threshold, outperforming LSTM and demonstrating robustness.

Conclusion: The approach is promising for real-time disruption avoidance in short-pulse tokamaks, marking the first transformer application to ADITYA data.

Abstract: Disruptions in tokamak plasmas, marked by sudden thermal and current
quenches, pose serious threats to plasma-facing components and system
integrity. Accurate early prediction, with sufficient lead time before
disruption onset, is vital to enable effective mitigation strategies. This
study presents a novel data-driven approach for predicting early current
quench, a key precursor to disruptions, using transformer-based deep learning
models, applied to ADITYA tokamak diagnostic data. Using multivariate time
series data, the transformer model outperforms LSTM baselines across various
data distributions and prediction thresholds. The transformer model achieves
better recall, maintaining values above 0.9 even up to a prediction threshold
of 8-10 ms, significantly outperforming LSTM in this critical metric. The
proposed approach remains robust up to an 8 ms lead time, offering practical
feasibility for disruption mitigation in ADITYA tokamak. In addition, a
comprehensive data diversity analysis and bias sensitivity study underscore the
generalization of the model. This work marks the first application of
transformer architectures to ADITYA tokamak data for early current-quench
prediction, establishing a promising foundation for real time disruption
avoidance in short-pulse tokamaks.

</details>


### [38] [Introduction to Stability and Turbulent Transport in Magnetic Confinement Fusion Plasmas](https://arxiv.org/abs/2507.13144)
*J. F. Parisi*

Main category: physics.plasm-ph

TL;DR: An introductory tutorial on stability and turbulent transport in magnetic confinement fusion plasmas, covering key concepts, models, and practical implications for newcomers.


<details>
  <summary>Details</summary>
Motivation: To provide an accessible guide for researchers new to the field of magnetic confinement fusion plasmas, addressing foundational principles and practical challenges.

Method: The tutorial discusses key concepts and models related to stability and turbulent transport, likely through theoretical explanations and examples.

Result: Researchers gain foundational knowledge and insights into the challenges and opportunities in the field.

Conclusion: The tutorial serves as a valuable resource for newcomers, bridging the gap between theory and practical applications in fusion plasma research.

Abstract: This tutorial provides an accessible introduction to the principles of
stability and turbulent transport in magnetic confinement fusion plasmas. Key
concepts, models, and practical implications are discussed to guide researchers
new to the field. Some challenges and opportunities are discussed.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [39] [Improving physics-informed neural network extrapolation via transfer learning and adaptive activation functions](https://arxiv.org/abs/2507.12659)
*Athanasios Papastathopoulos-Katsaros,Alexandra Stavrianidi,Zhandong Liu*

Main category: cs.LG

TL;DR: The paper introduces a transfer learning method and adaptive activation functions to enhance the extrapolation performance of Physics-Informed Neural Networks (PINNs), reducing errors significantly without added computational cost.


<details>
  <summary>Details</summary>
Motivation: PINNs struggle with poor extrapolation and sensitivity to activation functions, limiting their practical utility in scientific and engineering applications.

Method: The authors propose a transfer learning approach within an extended training domain and introduce an adaptive activation function combining standard ones.

Result: Experiments show a 40% reduction in relative L2 error and 50% reduction in mean absolute error in extrapolation tasks.

Conclusion: The method improves PINNs' robustness and accuracy for extrapolation, making them more reliable for real-world applications.

Abstract: Physics-Informed Neural Networks (PINNs) are deep learning models that
incorporate the governing physical laws of a system into the learning process,
making them well-suited for solving complex scientific and engineering
problems. Recently, PINNs have gained widespread attention as a powerful
framework for combining physical principles with data-driven modeling to
improve prediction accuracy. Despite their successes, however, PINNs often
exhibit poor extrapolation performance outside the training domain and are
highly sensitive to the choice of activation functions (AFs). In this paper, we
introduce a transfer learning (TL) method to improve the extrapolation
capability of PINNs. Our approach applies transfer learning (TL) within an
extended training domain, using only a small number of carefully selected
collocation points. Additionally, we propose an adaptive AF that takes the form
of a linear combination of standard AFs, which improves both the robustness and
accuracy of the model. Through a series of experiments, we demonstrate that our
method achieves an average of 40% reduction in relative L2 error and an average
of 50% reduction in mean absolute error in the extrapolation domain, all
without a significant increase in computational cost. The code is available at
https://github.com/LiuzLab/PINN-extrapolation .

</details>


### [40] [RONOM: Reduced-Order Neural Operator Modeling](https://arxiv.org/abs/2507.12814)
*Sven Dummer,Dongwei Ye,Christoph Brune*

Main category: cs.LG

TL;DR: The paper introduces RONOM, a framework combining reduced-order modeling (ROM) and neural operator learning, to address computational challenges in solving time-dependent PDEs. It provides error bounds and demonstrates superior performance in generalization and robustness.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between ROM's fixed discretization and neural operators' lack of error quantification, enabling flexible and accurate PDE solutions.

Method: Proposes RONOM, integrating ROM and neural operators, with theoretical error bounds and numerical validation.

Result: RONOM matches neural operators in generalization and excels in spatial super-resolution and robustness, with insights into temporal super-resolution.

Conclusion: RONOM effectively combines ROM and operator learning, offering improved flexibility and performance for PDE solutions.

Abstract: Time-dependent partial differential equations are ubiquitous in physics-based
modeling, but they remain computationally intensive in many-query scenarios,
such as real-time forecasting, optimal control, and uncertainty quantification.
Reduced-order modeling (ROM) addresses these challenges by constructing a
low-dimensional surrogate model but relies on a fixed discretization, which
limits flexibility across varying meshes during evaluation. Operator learning
approaches, such as neural operators, offer an alternative by parameterizing
mappings between infinite-dimensional function spaces, enabling adaptation to
data across different resolutions. Whereas ROM provides rigorous numerical
error estimates, neural operator learning largely focuses on discretization
convergence and invariance without quantifying the error between the
infinite-dimensional and the discretized operators. This work introduces the
reduced-order neural operator modeling (RONOM) framework, which bridges
concepts from ROM and operator learning. We establish a discretization error
bound analogous to those in ROM, and get insights into RONOM's discretization
convergence and discretization robustness. Moreover, two numerical examples are
presented that compare RONOM to existing neural operators for solving partial
differential equations. The results demonstrate that RONOM using standard
vector-to-vector neural networks achieves comparable performance in input
generalization and superior performance in both spatial super-resolution and
discretization robustness, while also offering novel insights into temporal
super-resolution scenarios.

</details>


<div id='math.CV'></div>

# math.CV [[Back]](#toc)

### [41] [An Iterative Approach to the Complex Monge-Ampère Eigenvalue Problem](https://arxiv.org/abs/2507.13273)
*Ahmed Zeriahi*

Main category: math.CV

TL;DR: An iterative method for solving the Dirichlet complex Monge-Ampère eigenvalue problem on pseudoconvex domains in ℂⁿ, inspired by a real Monge-Ampère approach.


<details>
  <summary>Details</summary>
Motivation: To extend the iterative solution technique from real to complex Monge-Ampère problems, addressing eigenvalue challenges in pseudoconvex domains.

Method: Iterative approximation inspired by Abedin and Kitagawa's real Monge-Ampère approach, adapted for complex settings.

Result: Demonstrates feasibility of iterative solutions for complex Monge-Ampère eigenvalue problems.

Conclusion: The approach successfully bridges real and complex Monge-Ampère problems, offering a practical solution method.

Abstract: We present an iterative approach to approximate the solution to the Dirichlet
complex Monge-Amp\`ere eigenvalue problem on a bounded strictly pseudoconvex
domain in $\C^n$. This approach is inspired by a similar approach initiated by
F. Abedin, J. Kitagawa who considered the real Monge-Amp\`ere operator on a
strictly convex domain in $\R^N$.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [42] [Unraveling Self-Similar Energy Transfer Dynamics: a Case Study for 1D Burgers System](https://arxiv.org/abs/2507.12764)
*Pritpal Matharu,Bartosz Protas,Tsuyoshi Yoneda*

Main category: physics.flu-dyn

TL;DR: The paper explores self-similar energy cascades in turbulence using the 1D viscous Burgers equation, solved via PDE-constrained optimization, identifying viscous and inertial solution families.


<details>
  <summary>Details</summary>
Motivation: To systematically find self-similar flow evolutions consistent with Kolmogorov's turbulence theory, starting with a simplified model.

Method: PDE-constrained optimization with adjoint-based gradient methods to solve the nonconvex problem.

Result: Two solution families (viscous and inertial) were found; inertial solutions exhibit self-similarity only at low viscosity.

Conclusion: The approach is promising for extending to 3D turbulence, demonstrating proof of concept for systematic self-similar flow analysis.

Abstract: In this work we consider the problem of constructing flow evolutions leading
to a self-similar energy cascade consistent with Kolmogorov's statistical
theory of turbulence. As a first step in this direction, we focus on the
one-dimensional viscous Burgers equation as a toy model. Its solutions
exhibiting self-similar behavior, in a precisely-defined sense, are found by
framing this problems in terms of PDE-constrained optimization. The main
physical parameters are the time window over which self-similar behavior is
sought (equal to approximately one eddy turnover time), viscosity (inversely
proportional to the ``Reynolds number") and an integer parameter characterizing
the distance in the Fourier space over which self-similar interactions occur.
Local solutions to this nonconvex PDE optimization problems are obtained with a
state-of-the-art adjoint-based gradient method. Two distinct families of
solutions, termed viscous and inertial, are identified and are distinguished
primarily by the behavior of enstrophy which, respectively, uniformly decays
and grows in the two cases. The physically meaningful and appropriately
self-similar inertial solutions are found only when a sufficiently small
viscosity is considered. These flows achieve the self-similar behaviour by a
uniform steepening of the wave fronts present in the solutions. The methodology
proposed and the results obtained represent an encouraging proof of concept for
this approach to be employed to systematically search for self-similar flow
evolutions in the context of three-dimensional turbulence.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [43] [Assessing the economic benefits of space weather mitigation investment decisions: Evidence from Aotearoa New Zealand](https://arxiv.org/abs/2507.12495)
*Edward J. Oughton,Andrew Renton,Daniel Mac Marnus,Craig J. Rodger*

Main category: physics.geo-ph

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Space weather events pose a growing threat to modern economies, yet their
macroeconomic consequences still remain underexplored. This study presents the
first dedicated economic assessment of geomagnetic storm impacts on Aotearoa
New Zealand, quantifying potential GDP losses across seven disruption and
mitigation scenarios due to an extreme coronal mass ejection (CME). The primary
focus is upon the damaging impacts of geomagnetically induced currents (GICs)
on the electrical power transmission network. The goal is to support
decision-making around space weather mitigation investments by providing a
first-order approximation of their potential economic benefits. We find that in
the absence of mitigation, a severe but realistic storm could result in up to
NZ\$8.36 billion in lost GDP, with more than half stemming from cascading
supply chain effects. Yet, even less severe scenarios incur losses exceeding
NZ\$3 billion. Importantly, research-led operational strategies, such as
optimized switching and islanding, can avoid up to NZ\$370 million in losses
for as little as NZ\$500,000 in expenditure, delivering a benefit-cost ratio of
740 to 1. Moreover, physical protections such as GIC blocking devices further
reduce disruption to as low as NZ\$1.12 billion, with avoided GDP losses up to
NZ\$2.3 billion, and benefit-cost returns up to 80 to 1. When also
acknowledging unmodelled impacts, including multi-billion losses in capital
equipment and long-term revenue, the economic rationale for pre-emptive
mitigation becomes even more pertinent. Future research needs to integrate the
modelling of capital and revenue losses for strategically important industrial
facilities.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [44] [Evidence for an Inverse Cascade of Magnetic Helicity in the Inner Heliosphere](https://arxiv.org/abs/2507.13213)
*Masatomi Iizawa,Yasuhito Narita,Tommaso Alberti,Stuart D. Bale,Axel Brandenburg,Abraham C. -L. Chian,Horia Comişel,Shuichi Matsukiyo,Nobumitsu Yokoi*

Main category: astro-ph.SR

TL;DR: Analysis of Parker Solar Probe data confirms an inverse cascade in solar wind turbulence from the Sun to Mercury's orbit, challenging traditional views and revealing a radial sign change in magnetic helicity density.


<details>
  <summary>Details</summary>
Motivation: To understand the direction of solar wind turbulence and challenge the conventional hypothesis of random magnetic helicity density in the inner heliosphere.

Method: Analyzed magnetic helicity density spectra from Parker Solar Probe data across 500+ heliocentric distances.

Result: Confirmed a persistent inverse cascade and a radial sign change in spectral magnetic helicity density, with frequency decreasing logarithmically with distance.

Conclusion: Provides new insights into solar wind turbulence evolution in the inner heliosphere, contradicting prior assumptions.

Abstract: To elucidate the cascade direction of the solar wind turbulence, we analyzed
magnetic helicity density spectra from the Parker Solar Probe data across more
than 500 heliocentric distances. For the first time, we confirmed a persistent
inverse cascade extending from the Sun to Mercury's orbital vicinity. This
finding challenges the conventional hypothesis that the magnetic helicity
density within the inner heliosphere is random. Furthermore, our analysis
revealed a radial sign change of the spectral magnetic helicity density at a
frequency whose value decreases logarithmically with distance. These results
provide new insights into the evolution of solar wind turbulence in the inner
heliosphere.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [45] [On the factorization of matrices into products of positive definite ones](https://arxiv.org/abs/2507.12560)
*Mahmoud Abdelgalil,Tryphon T. Georgiou*

Main category: math.OC

TL;DR: The paper revisits Ballantine's matrix factorization result, offering a new constructive approach using optimal mass transport theory to solve a control problem.


<details>
  <summary>Details</summary>
Motivation: To address the strong controllability of linear systems via state feedback and highlight the realization of rotations through irrotational motions.

Method: Constructive approach based on optimal mass transport theory, linking rotations of Gaussian distributions to optimal transport maps.

Result: Provides a new method for factorizing matrices into positive definite factors, solving the control problem.

Conclusion: The work extends Ballantine's result and demonstrates practical applications in control engineering.

Abstract: The present work revisits and provides a new approach on a result by Charles
Ballantine, on the factorization of a square matrix with positive determinant
into a product of positive definite factors. {\em Ballantine-type}
factorizations, that bound the number of positive definite factors, proved
central in solving a basic, yet elusive control problem--the strong
controllability of a linear system via control in the form of state feedback.
Ballantine's result transcends control engineering, and highlights the little
appreciated fact that rotations can be realized by the successive application
of irrotational motions. Our approach is constructive and is based on the
theory of optimal mass transport, specifically, it relates successive rotations
of Gaussian distributions to corresponding optimal transport maps that
constitute the sought factors.

</details>


### [46] [Tensor-Tensor Products, Group Representations, and Semidefinite Programming](https://arxiv.org/abs/2507.12729)
*Alex Dunbar,Elizabeth Newman*

Main category: math.OC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The $\star_M$-family of tensor-tensor products is a framework which
generalizes many properties from linear algebra to third order tensors. Here,
we investigate positive semidefiniteness and semidefinite programming under the
$\star_M$-product. Critical to our investigation is a connection between the
choice of matrix M in the $\star_M$-product and the representation theory of an
underlying group action. Using this framework, third order tensors equipped
with the $\star_M$-product are a natural setting for the study of invariant
semidefinite programs. As applications of the M-SDP framework, we provide a
characterization of certain nonnegative quadratic forms and solve low-rank
tensor completion problems.

</details>


### [47] [Unsupervised Ground Metric Learning](https://arxiv.org/abs/2507.13094)
*Janis Auffenberg,Jonas Bresch,Oleh Melnyk,Gabriele Steidl*

Main category: math.OC

TL;DR: The paper addresses unsupervised metric learning by simultaneously learning optimal transport cost matrices and exploring alternative distances like Mahalanobis-like and graph Laplacians. It proposes a stochastic algorithm with proven linear convergence and simplifies the problem for linear algebra methods.


<details>
  <summary>Details</summary>
Motivation: The challenge of data classification without labeled samples necessitates learning appropriate distances between features, motivating the exploration of unsupervised metric learning.

Method: The paper examines algorithms for learning optimal transport cost matrices, proposes a stochastic random function iteration algorithm, and explores alternative distances (Mahalanobis-like and graph Laplacians).

Result: The stochastic algorithm converges linearly despite non-paracontractive operators. Alternative distances simplify the problem, allowing linear algebra methods.

Conclusion: The work advances unsupervised metric learning by introducing efficient algorithms and flexible distance metrics, broadening applicability.

Abstract: Data classification without access to labeled samples remains a challenging
problem. It usually depends on an appropriately chosen distance between
features, a topic addressed in metric learning. Recently, Huizing, Cantini and
Peyr\'e proposed to simultaneously learn optimal transport (OT) cost matrices
between samples and features of the dataset. This leads to the task of finding
positive eigenvectors of a certain nonlinear function that maps cost matrices
to OT distances. Having this basic idea in mind, we consider both the
algorithmic and the modeling part of unsupervised metric learning. First, we
examine appropriate algorithms and their convergence. In particular, we propose
to use the stochastic random function iteration algorithm and prove that it
converges linearly for our setting, although our operators are not
paracontractive as it was required for convergence so far. Second, we ask the
natural question if the OT distance can be replaced by other distances. We show
how Mahalanobis-like distances fit into our considerations. Further, we examine
an approach via graph Laplacians. In contrast to the previous settings, we have
just to deal with linear functions in the wanted matrices here, so that simple
algorithms from linear algebra can be applied.

</details>


### [48] [Optimal regularity up to the boundary for Plateau-quasi-minimizers](https://arxiv.org/abs/2507.13189)
*Eve Machefert*

Main category: math.OC

TL;DR: The paper analyzes the regularity of quasi-minimal sets with boundary conditions, proving optimal regularity characterized by bi-John domains with Ahlfors regular boundaries.


<details>
  <summary>Details</summary>
Motivation: To understand the regularity properties of quasi-minimal sets, particularly in the context of Plateau's problem in co-dimension one.

Method: Investigates Ahlfors regularity and uniform rectifiability of quasi-minimal sets, extending analysis to boundary conditions.

Result: Establishes optimal regularity for Plateau-quasi-minimizers, characterized by bi-John domains with Ahlfors regular boundaries.

Conclusion: The study provides a comprehensive regularity framework for quasi-minimal sets with boundary conditions, advancing understanding in geometric measure theory.

Abstract: We study the regularity of quasi-minimal sets (in the sense of David and
Semmes) with a boundary condition, which can be interpreted as quasi-minimizers
of Plateau's problem in co-dimension one. For these Plateau-quasi-minimizers,
we establish the optimal regularity, which is a characterization by bi-John
domains with Ahlfors regular boundaries. This requires to investigate the
Ahlfors regularity and also the uniform rectifiability of those sets, up to the
boundary.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [49] [N Bugs on a Circle](https://arxiv.org/abs/2507.13333)
*Josh Briley,Bryan Quaife*

Main category: math.DS

TL;DR: A generalization of the 'Four Bugs on a Square' problem is analyzed, where bugs move on a unit circle's perimeter, leading to three steady states: coalescence, antipodal clusters, or infinite chase cycles. Stability and probabilities for these states are studied.


<details>
  <summary>Details</summary>
Motivation: To explore how constraining bugs to a circle's perimeter alters their long-term behavior compared to unrestricted pursuit problems, revealing richer dynamics.

Method: Analyze steady states (coalescence, antipodal clusters, infinite chase) and their stability. Use exact analytical expressions for N ≤ 4 and Monte Carlo simulations for larger N.

Result: Three steady states emerge. For N ≤ 4, exact probabilities are derived; for larger N, coalescence probability follows an inverse square root relationship.

Conclusion: Restricting bugs to a circle's perimeter introduces new dynamical behaviors, fundamentally changing long-term outcomes compared to classic pursuit problems.

Abstract: We describe and analyze a generalization of the classic ``Four Bugs on a
Square'' cyclic pursuit problem. Instead of allowing the bugs to spiral towards
one another, we constrain $N$ bugs to the perimeter of the unit circle.
Depending on their configuration, each bug moves either clockwise or
counterclockwise with a constant angular speed, or remains stationary. Unlike
the original problem where bugs always coalesce, this generalization produces
three possible steady states: all bugs coalescing to a single point, clusters
of bugs located at two antipodal points, or bugs entering a stable infinite
chase cycle where they never meet. We analyze the stability of these steady
states and calculate the probability that randomly initialized bugs reach each
state. For $N \leq 4$, we derive exact analytical expressions for these
probabilities. For larger values, we employ Monte Carlo simulations to estimate
the probability of coalescing, finding it approximately follows an inverse
square root relationship with the number of bugs. This generalization reveals
rich dynamical behaviors that are absent in the classic problem. Our analysis
provides insight into how restricting the bugs to the circle's perimeter
fundamentally alters the long-term behavior of pursuing agents compared to
unrestricted pursuit problems.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [50] [Inverse Physics-informed neural networks procedure for detecting noise in open quantum systems](https://arxiv.org/abs/2507.12552)
*Gubio G. de Lima,Iann Cunha,Leonardo Kleber Castelano*

Main category: quant-ph

TL;DR: The paper introduces PINNverse, a machine learning framework for identifying Hamiltonian parameters and decay rates in open quantum systems, demonstrating its effectiveness with two-qubit simulations.


<details>
  <summary>Details</summary>
Motivation: Accurate quantum system characterization is crucial for NISQ technologies, but traditional methods are inefficient. Machine learning offers scalable alternatives.

Method: Extends the PINNverse framework to open quantum systems using Lindblad master equations, training neural networks on coherent and dissipative dynamics.

Result: Numerical simulations show PINNverse effectively identifies system parameters and is resilient to noise, offering scalability.

Conclusion: PINNverse is a promising tool for quantum system identification, with applications in quantum control and error mitigation.

Abstract: Accurate characterization of quantum systems is essential for the development
of quantum technologies, particularly in the noisy intermediate-scale quantum
(NISQ) era. While traditional methods for Hamiltonian learning and noise
characterization often require extensive measurements and scale poorly with
system size, machine learning approaches offer promising alternatives. In this
work, we extend the inverse physics-informed neural network (referred to as
PINNverse) framework to open quantum systems governed by Lindblad master
equations. By incorporating both coherent and dissipative dynamics into the
neural network training, our method enables simultaneous identification of
Hamiltonian parameters and decay rates from noisy experimental data. We
demonstrate the effectiveness and robustness of the approach through numerical
simulations of two-qubit open systems. Our results show that PINNverse provides
a scalable and noise-resilient framework for quantum system identification,
with potential applications in quantum control and error mitigation.

</details>


### [51] [State transfer analysis for linear spin chains with non-uniform on-site energies](https://arxiv.org/abs/2507.13261)
*Chad C. Nelmes,Irene D'Amico,Timothy P. Spiller*

Main category: quant-ph

TL;DR: Analysis of perfect and quasi-perfect state transfer in linear spin chains with non-uniform on-site energies, focusing on coupling uniformity and its statistical variations.


<details>
  <summary>Details</summary>
Motivation: To explore high-fidelity state transfer for distributed quantum information processing, emphasizing the benefits of coupling uniformity in physical implementations.

Method: Study linear spin chains with non-uniform on-site energies, relating coupling uniformity to a particle in a discrete potential analogue. Analyze statistical variations in couplings and on-site energies as chain length increases.

Result: Findings highlight the relationship between coupling uniformity and state transfer fidelity, with insights into statistical variations for longer chains.

Conclusion: The study provides a framework for optimizing state transfer in quantum systems by leveraging coupling uniformity and understanding statistical effects.

Abstract: High fidelity state transfer is an important ingredient of distributed
quantum information processing. We present and analyse results on perfect and
quasi-perfect state transfer with linear spin chains incorporating non-uniform
on-site energies. The motivation is maintenance of coupling uniformity, which
could be beneficial for some physical implementations. We relate this coupling
uniformity to a particle in a discrete potential analogue. Our analysis further
considers the statistical variation in couplings and on-site energies, as a
function of increasing chain site number N.

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [52] [Three-dimensional Dynamics of Strongly Magnetized Ion-Electron Relativistic Reconnection](https://arxiv.org/abs/2507.12509)
*Fabio Bacchini,Gregory R. Werner,Camille Granier,Jesse Vos*

Main category: astro-ph.HE

TL;DR: 3D simulations of semirelativistic magnetic reconnection reveal that high magnetization reduces efficiency of energy dissipation and particle acceleration, contrary to expectations.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of collisionless magnetic reconnection in semirelativistic regimes, particularly in astrophysical contexts like black holes and neutron stars.

Method: Employ 3D simulations with realistic proton-to-electron mass ratio, scanning a range of ion magnetization, including unprecedentedly large domain sizes.

Result: Transition to ultrarelativistic regimes shows 3D effects (drift-kink and flux-rope kink dynamics) reduce magnetic-energy dissipation and nonthermal particle acceleration efficiency.

Conclusion: Findings challenge general expectations for 3D relativistic reconnection and have implications for astrophysical scenarios.

Abstract: We present 3D simulations of semirelativistic collisionless magnetic
reconnection, where upstream ions are subrelativistic while electrons are
ultrarelativistic. We employ the realistic proton-to-electron mass ratio and
explore a range of upstream ion magnetization spanning two orders of magnitude,
with our highest-magnetization run achieving unprecedentedly large domain
sizes. Through a parameter scan, we find that as the system transitions from
mildly to trans- and ultrarelativistic regimes the qualitative behavior of
reconnection becomes strongly influenced by 3D effects mediated by drift-kink
and flux-rope kink dynamics. As a result, magnetic-energy dissipation at high
magnetizations, and the subsequent nonthermal particle acceleration, can become
less efficient, contrary to general expectations for 3D relativistic
reconnection. Our results have important implications for understanding
reconnection in magnetized astrophysical scenarios, such as the surroundings of
black holes and neutron stars.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [53] [Accelerated free energy estimation in ab initio path integral Monte Carlo simulations](https://arxiv.org/abs/2507.12960)
*Pontus Svensson,Fotios Kalkavouras,Uwe Hernandez Acosta,Zhandos A. Moldabekov,Panagiotis Tolias,Jan Vorberger,Tobias Dornheim*

Main category: physics.chem-ph

TL;DR: The paper introduces a method to speed up free energy estimation in path integral Monte Carlo simulations using an artificial reference system and a ξ-extrapolation technique, achieving significant efficiency gains and accuracy for large electron systems.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the efficiency and accuracy of free energy estimation in quantum systems, particularly for large particle numbers, addressing challenges like the fermion sign problem.

Method: The method involves using an intermediate artificial reference system (spherically averaged Ewald interaction) and a ξ-extrapolation technique to accelerate calculations and mitigate the fermion sign problem.

Result: The approach achieved up to 18 times faster free energy estimation than traditional methods and enabled accurate calculations for 1000 electrons with minimal errors.

Conclusion: The combined techniques offer a practical solution for modeling systems in planetary and fusion research with moderate quantum degeneracy.

Abstract: We present a methodology for accelerating the estimation of the free energy
from path integral Monte Carlo simulations by considering an intermediate
artificial reference system where interactions are inexpensive to evaluate
numerically. Using the spherically averaged Ewald interaction as this
intermediate reference system for the uniform electron gas, the interaction
contribution for the free energy was evaluated up to 18 times faster than the
Ewald-only method. Furthermore, a $\xi$-extrapolation technique was tested and
applied to alleviate the fermion sign problem and to resolve the sign for large
particle numbers. Combining these two techniques enabled the evaluation of the
free energy for a system of 1000 electrons, where both finite-size and
statistical errors are below chemical accuracy. The general procedure can be
applied to systems relevant for planetary and inertial confinement fusion
modeling with low to moderate levels of quantum degeneracy.

</details>


### [54] [$GW$+2SOSEX self-energy made positive semi-definite](https://arxiv.org/abs/2507.13293)
*Fabien Bruneval,Arno Förster,Yaroslav Pavlyukh*

Main category: physics.chem-ph

TL;DR: The paper introduces a positive semi-definite extension to the $GW$+2SOSEX self-energy, named $GW$+2SOSEX-psd, ensuring accurate quasiparticle energy predictions.


<details>
  <summary>Details</summary>
Motivation: Vertex corrections beyond the $GW$ approximation often fail to meet the positive semi-definiteness condition, necessitating a reliable solution.

Method: The study cancels bare energy poles in the $G3W2$ self-energy and formulates the $GW$+2SOSEX-psd approximation.

Result: The proposed self-energy is shown to be positive semi-definite and accurate for predicting quasiparticle energies in molecular examples.

Conclusion: The $GW$+2SOSEX-psd approximation successfully addresses the positive semi-definiteness issue while maintaining accuracy.

Abstract: The formulation of vertex corrections beyond the $GW$ approximation within
the framework of perturbation theory is a subtle and challenging task, which
accounts for the wide variety of schemes proposed over the years. Exact
self-energies are required to satisfy the mathematical condition of positive
semi-definiteness. The $GW$ self-energy fulfills this property, but the vast
majority of the vertex-corrected self-energy approximations do not. In this
study, we devise a positive semi-definite extension to the $GW$+2SOSEX
self-energy that we name $GW$+2SOSEX-psd. To reach this goal, we demonstrate
the cancellation of the bare energy poles that are contained in the fully
dynamic second-order in $W$ self-energy ($G3W2$). We then demonstrate on
molecular examples the correct positive semi-definiteness of the proposed
self-energy approximation and its good accuracy in predicting accurate
quasiparticle energies for valence and core states.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [55] [Search for Z/2 eigenfunctions on the sphere using machine learning](https://arxiv.org/abs/2507.13122)
*Andriy Haydys,Willem Adriaan Salm*

Main category: math.DG

TL;DR: Machine learning identifies Z/2 eigenfunctions on the 2-sphere using a multivalued neural network implemented in JAX.


<details>
  <summary>Details</summary>
Motivation: To explore and identify Z/2 eigenfunctions on the 2-sphere, leveraging machine learning for computational efficiency.

Method: Developed a multivalued feedforward deep neural network using JAX, tested with fixed and movable branch points.

Result: Found Z/2 eigenfunctions for branch points fixed at tetrahedron and cube vertices, and movable points forming a squashed tetrahedron.

Conclusion: Machine learning successfully identified eigenfunctions, demonstrating adaptability in optimizing branch point positions.

Abstract: We use machine learning to search for examples of Z/2 eigenfunctions on the
2-sphere. For this we created a multivalued version of a feedforward deep
neural network, and we implemented it using the JAX library. We found Z/2
eigenfunctions for three cases: In the first two cases we fixed the branch
points at the vertices of a tetrahedron and at a cube respectively. In a third
case, we allowed the AI to move the branch points around and, in the end, it
positioned the branch points at the vertices of a squashed tetrahedron.

</details>


### [56] [Integral gradient estimates on a closed surface](https://arxiv.org/abs/2507.12790)
*Yuxiang Li,Rongze Sun*

Main category: math.DG

TL;DR: The paper establishes $L^p$ gradient estimates for solutions to a PDE on a Riemann surface, independent of the metric choice, especially near the moduli space boundary.


<details>
  <summary>Details</summary>
Motivation: To analyze solutions of a PDE on a Riemann surface with a signed Radon measure, particularly when the complex structure nears the moduli space boundary.

Method: Transform the metric to $g' = e^{2u} g$ and use its bounded integral curvature and quadratic area bound condition to derive gradient estimates.

Result: Gradient estimates for $g'$ in local conformal coordinates, leading to $L^p$ estimates for the gradient of $u$.

Conclusion: The method successfully provides metric-independent $L^p$ gradient estimates for the solution $u$.

Abstract: Let $(\Sigma, g)$ be a closed Riemann surface, and let $u$ be a weak solution
to equation \[ - \Delta_g u = \mu, \] where $\mu$ is a signed Radon measure. We
aim to establish $L^p$ estimates for the gradient of $u$ that are independent
of the choice of the metric $g$. This is particularly relevant when the complex
structure approaches the boundary of the moduli space. To this end, we consider
the metric $g' = e^{2u} g$ as a metric of bounded integral curvature. This
metric satisfies a so-called quadratic area bound condition, which allows us to
derive gradient estimates for $g'$ in local conformal coordinates. From these
estimates, we obtain the desired estimates for the gradient of $u$.

</details>


### [57] [On the Nature of Stationary Integral Varifolds near Multiplicity 2 Planes](https://arxiv.org/abs/2507.13148)
*Spencer Becker-Kahn,Paul Minter,Neshan Wickramasekera*

Main category: math.DG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study stationary integral $n$-varifolds $V$ in the unit ball
$B_1(0)\subset\mathbb{R}^{n+k}$. Allard's regularity theorem establishes the
existence of $\epsilon = \epsilon(n,k)\in (0,1)$ for which if $V$ is
$\epsilon$-close (as varifolds) to the plane $P_0 = \{0\}^k\times\mathbb{R}^n$
with multiplicity 1 then, in $B_{1/2}(0)$, $V$ is represented by a single
$C^{1,\alpha}$ minimal graph. However, when instead $P_0$ occurs with
multiplicity $Q\in \{2,3,\dotsc\}$, simple examples show that this conclusion,
now as a multi-valued graph, may fail, even if $V$ corresponds to an
area-minimising rectifiable current.
  In the present work we investigate the structure of such $V$ which are close
to planes with multiplicity $Q>1$, focusing primarily on the case $Q=2$. We
show that an $\epsilon$-regularity theorem holds when $V$ is close, as a
varifold, to $P_0$ with multiplicity $2$, provided $V$ satisfies a certain
topological structural condition on the part of its support where the density
of $V$ is $<2$. The conclusion then is that, in $B_{1/2}(0)$, $V$ is
represented by the graph of a Lipschitz $2$-valued function over $P_0$ with
small Lipschitz constant; in fact, the function is $C^{1,\alpha}$ in a precise
generalised sense, and satisfies estimates, implying that all tangent cones at
singular points in $B_{1/2}(0)$ are unique and comprised of stationary unions
of $4$ half-planes (which may form a union of two distinct planes or a single
multiplicity $2$ plane). The theorem does not require any additional assumption
on the part of $V$ with density $\geq 2$ (which a priori may be a relatively
large set in $\mathcal{H}^n$-measure with high topological complexity).
  As a corollary, we show that our $\epsilon$-regularity theorem applies
unconditionally to stationary $2$-valued Lipschitz graphs with arbitrary
Lipschitz constant, yielding improved regularity and uniform a priori
estimates.

</details>
