<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 12]
- [math.AP](#math.AP) [Total: 19]
- [physics.comp-ph](#physics.comp-ph) [Total: 4]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 3]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 2]
- [math.DS](#math.DS) [Total: 1]
- [math.CV](#math.CV) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [astro-ph.SR](#astro-ph.SR) [Total: 1]
- [math.DG](#math.DG) [Total: 3]
- [astro-ph.HE](#astro-ph.HE) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [math.OC](#math.OC) [Total: 4]
- [cs.LG](#cs.LG) [Total: 2]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Keep the beat going: Automatic drum transcription with momentum](https://arxiv.org/abs/2507.12596)
*Alisha L. Foster,Robert J. Webber*

Main category: math.NA

TL;DR: This paper compares two optimization methods (multiplicative update rule vs. projected gradient descent with momentum) for automatic drum transcription using nonnegative matrix factorization, finding that projected gradient descent achieves better accuracy and convergence guarantees.


<details>
  <summary>Details</summary>
Motivation: To develop a simple and interpretable approach for automatic drum transcription by comparing different optimization methods for nonnegative matrix factorization, addressing the need for effective drum pattern recognition in musical recordings.

Method: The paper uses partially fixed nonnegative matrix factorization to factor magnitude spectrograms of musical recordings, comparing two optimization approaches: multiplicative update rule and projected gradient descent with momentum. The methods are evaluated on time complexity and empirical accuracy.

Result: Projected gradient descent with momentum outperforms the multiplicative update rule in terms of accuracy for fixed runtime when tested on the ENST-Drums dataset and original band recordings. The projected gradient method also provides stronger theoretical convergence guarantees.

Conclusion: Projected gradient descent with momentum is superior to multiplicative update rules for automatic drum transcription tasks, offering both better empirical performance and stronger theoretical foundations for convergence in nonnegative matrix factorization applications.

Abstract: A simple, interpretable way to perform automatic drum transcription is by
factoring the magnitude spectrogram of a recorded musical piece using a
partially fixed nonnegative matrix factorization. There are two natural ways to
optimize the nonnegative matrix factorization, including a multiplicative
update rule and projected gradient descent with momentum. The methods differ in
their empirical accuracies and theoretical convergence guarantees. This paper
summarizes the methods and their time complexities, and it applies the methods
to the ENST-Drums data set and an original recording from the author's band,
evaluating the empirical accuracy with respect to ground-truth drum
annotations. The results indicate that projected gradient descent with momentum
leads to higher accuracy for a fixed runtime, and it satisfies stronger
convergence guarantees.

</details>


### [2] [A Unified Framework for Efficient Kernel and Polynomial Interpolation](https://arxiv.org/abs/2507.12629)
*M. Belianovich,G. E. Fasshauer,A. Narayan,V. Shankar*

Main category: math.NA

TL;DR: A unified interpolation scheme combining compactly-supported kernels and polynomials, with efficient numerical methods for computation and application on manifolds.


<details>
  <summary>Details</summary>
Motivation: To generalize interpolation with compactly-supported kernels and polynomial least squares, enabling broader applications.

Method: Combines compactly-supported kernels and polynomials, using specialized numerical linear algebra for efficiency.

Result: The unified interpolant outperforms polynomial least squares in numerical experiments on Euclidean domains and manifolds.

Conclusion: The framework is effective and versatile, with potential for further applications.

Abstract: We present a unified interpolation scheme that combines compactly-supported
positive-definite kernels and multivariate polynomials. This unified framework
generalizes interpolation with compactly-supported kernels and also classical
polynomial least squares approximation. To facilitate the efficient use of this
unified interpolation scheme, we present specialized numerical linear algebra
procedures that leverage standard matrix factorizations. These procedures allow
for efficient computation and storage of the unified interpolant. We also
present a modification to the numerical linear algebra that allows us to
generalize the application of the unified framework to target functions on
manifolds with and without boundary. Our numerical experiments on both
Euclidean domains and manifolds indicate that the unified interpolant is
superior to polynomial least

</details>


### [3] [Partitioned Conservative, Variable Step, Second-Order Method for Magneto-hydrodynamics In Elsässer Variables](https://arxiv.org/abs/2507.12700)
*Zhen Yao,Catalin Trenchea,Wenlong Pei*

Main category: math.NA

TL;DR: A symplectic, second-order algorithm for MHD in Elsässer variables is proposed, reducing computational cost via parallel subproblem solving and ensuring linear convergence. It conserves key quantities and achieves second-order accuracy, with adaptive time stepping balancing efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the computational challenges of solving the MHD system efficiently while maintaining accuracy and conserving physical quantities like energy and helicity.

Method: Partitioning the MHD system into smaller subproblems solved in parallel, using a symplectic, second-order algorithm with adaptive time stepping based on truncation error.

Result: Linear convergence under time step restrictions, conservation of energy and helicities, second-order accuracy in L² and H¹ norms, and improved efficiency with adaptivity.

Conclusion: The proposed algorithm effectively balances computational efficiency and accuracy, validated by numerical tests.

Abstract: Magnetohydrodynamics (MHD) describes the interaction between electrically
conducting fluids and electromagnetic fields. We propose and analyze a
symplectic, second-order algorithm for the evolutionary MHD system in
Els\"asser variables. We reduce the computational cost of the iterative
non-linear solver, at each time step, by partitioning the coupled system into
two subproblems of half size, solved in parallel. We prove that the iterations
converge linearly, under a time step restriction similar to the one required in
the full space-time error analysis. The variable step algorithm unconditionally
conserves the energy, cross-helicity and magnetic helicity, and numerical
solutions are second-order accurate in the $L^{2}$ and $H^{1}$-norms. The time
adaptive mechanism, based on a local truncation error criterion, helps the
variable step algorithm balance accuracy and time efficiency. Several numerical
tests support the theoretical findings and verify the advantage of time
adaptivity.

</details>


### [4] [DPNO: A Dual Path Architecture For Neural Operator](https://arxiv.org/abs/2507.12719)
*Yichen Wang,Wenlian Lu*

Main category: math.NA

TL;DR: The paper introduces a dual-path architecture for neural operators, improving performance over traditional stacking methods by 30% on PDE tasks like Burgers' and Navier-Stokes equations.


<details>
  <summary>Details</summary>
Motivation: Single neural operator blocks are limited; stacking them traditionally is inefficient. A dual-path architecture is proposed to enhance feature extraction and solution approximation.

Method: A dual-path architecture, inspired by ResNet and DenseNet, organizes basic operator blocks in parallel for better performance.

Result: The model achieves a 30% improvement on standard PDE test cases and shows versatility with DeepONet and FNO.

Conclusion: The dual-path architecture is effective and versatile, offering a promising direction for neural operator design.

Abstract: Neural operators have emerged as a powerful tool for solving partial
differential equations (PDEs) and other complex scientific computing tasks.
However, the performance of single operator block is often limited, thus often
requiring composition of basic operator blocks to achieve better per-formance.
The traditional way of composition is staking those blocks like feedforward
neural networks, which may not be very economic considering
parameter-efficiency tradeoff. In this pa-per, we propose a novel dual path
architecture that significantly enhances the capabilities of basic neural
operators. The basic operator block is organized in parallel two paths which
are similar with ResNet and DenseNet. By introducing this parallel processing
mechanism, our architecture shows a more powerful feature extraction and
solution approximation ability compared with the original model. We demonstrate
the effectiveness of our approach through extensive numerical experi-ments on a
variety of PDE problems, including the Burgers' equation, Darcy Flow Equation
and the 2d Navier-Stokes equation. The experimental results indicate that on
certain standard test cas-es, our model achieves a relative improvement of over
30% compared to the basic model. We also apply this structure on two standard
neural operators (DeepONet and FNO) selected from different paradigms, which
suggests that the proposed architecture has excellent versatility and offering
a promising direction for neural operator structure design.

</details>


### [5] [Quasi-optimality of the Crouzeix-Raviart FEM for p-Laplace-type problems](https://arxiv.org/abs/2507.12742)
*Johannes Storn*

Main category: math.NA

TL;DR: The paper proves quasi-optimality of the Crouzeix-Raviart FEM for nonlinear $p$-Laplace problems, showing error bounds and a localized a priori estimate for the conforming Lagrange FEM.


<details>
  <summary>Details</summary>
Motivation: To establish error bounds and quasi-optimality for the Crouzeix-Raviart FEM in nonlinear $p$-Laplace problems, and derive a localized a priori estimate for the conforming Lagrange FEM.

Method: Analyzes the Crouzeix-Raviart FEM's error in a quasi-norm, comparing it to the best-approximation error and a data oscillation term.

Result: The error is bounded by a constant times the best-approximation error plus a data oscillation term. A localized a priori estimate for the conforming Lagrange FEM is also derived.

Conclusion: The Crouzeix-Raviart FEM is quasi-optimal for $p$-Laplace problems, and a new localized a priori estimate enhances understanding of the conforming Lagrange FEM.

Abstract: We verify quasi-optimality of the Crouzeix-Raviart FEM for nonlinear problems
of $p$-Laplace type. More precisely, we show that the error of the
Crouzeix-Raviart FEM with respect to a quasi-norm is bounded from above by a
uniformly bounded constant times the best-approximation error plus a data
oscillation term. As a byproduct, we verify a novel more localized a priori
error estimate for the conforming lowest-order Lagrange FEM.

</details>


### [6] [Analysis of Langevin midpoint methods using an anticipative Girsanov theorem](https://arxiv.org/abs/2507.12791)
*Matthew S. Zhang*

Main category: math.NA

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We introduce a new method for analyzing midpoint discretizations of
stochastic differential equations (SDEs), which are frequently used in Markov
chain Monte Carlo (MCMC) methods for sampling from a target measure $\pi
\propto \exp(-V)$. Borrowing techniques from Malliavin calculus, we compute
estimates for the Radon-Nikodym derivative for processes on $L^2([0, T);
\mathbb{R}^d)$ which may anticipate the Brownian motion, in the sense that they
may not be adapted to the filtration at the same time. Applying these to
various popular midpoint discretizations, we are able to improve the regularity
and cross-regularity results in the literature on sampling methods. We also
obtain a query complexity bound of $\widetilde{O}(\frac{\kappa^{5/4}
d^{1/4}}{\varepsilon^{1/2}})$ for obtaining a $\varepsilon^2$-accurate sample
in $\mathsf{KL}$ divergence, under log-concavity and strong smoothness
assumptions for $\nabla^2 V$.

</details>


### [7] [Adaptive feature capture method for solving partial differential equations with low regularity solutions](https://arxiv.org/abs/2507.12941)
*Yangtao Deng,Qiaolin He,Xiaoping Wang*

Main category: math.NA

TL;DR: The paper introduces the Adaptive Feature Capture Method (AFCM), a machine learning framework that adaptively redistributes neurons and collocation points in high-gradient regions to improve accuracy for PDEs with low-regularity solutions.


<details>
  <summary>Details</summary>
Motivation: Traditional numerical methods struggle with low-regularity PDEs in complex geometries, while existing deep-learning approaches lack adaptive resolution for steep gradients or singularities.

Method: AFCM uses gradient norms to guide reinitialization of feature function parameters, clustering partition hyperplanes and collocation points in critical regions.

Result: Numerical experiments show AFCM effectively resolves low-regularity problems in complex geometries without increasing computational overhead.

Conclusion: AFCM bridges adaptive mesh refinement and randomized neural networks, offering a scalable solution for challenging PDEs.

Abstract: Partial differential equations (PDEs) with low-regularity solutions pose
significant challenges for traditional numerical methods, particularly in
complex geometries where mesh generation and adaptive refinement become
computationally expensive. While deep-learning-based approaches, such as
Physics-Informed Neural Networks (PINNs) and the Random Feature Method (RFM),
offer mesh-free alternatives, they often lack adaptive resolution in critical
regions, limiting their accuracy for solutions with steep gradients or
singularities. In this work, we propose the Adaptive Feature Capture Method
(AFCM), a novel machine learning framework that adaptively redistributes
neurons and collocation points in high-gradient regions to enhance local
expressive power. Inspired by adaptive moving mesh techniques, AFCM employs the
gradient norm of an approximate solution as a monitor function to guide the
reinitialization of feature function parameters. This ensures that partition
hyperplanes and collocation points cluster where they are most needed,
achieving higher resolution without increasing computational overhead. The AFCM
extends the capabilities of RFM to handle PDEs with near-singular solutions
while preserving its mesh-free efficiency. Numerical experiments demonstrate
the method's effectiveness in accurately resolving low-regularity problems,
even in complex geometries. By bridging the gap between adaptive mesh
refinement and randomized neural networks, AFCM offers a robust and scalable
approach for solving challenging PDEs in scientific and engineering
applications.

</details>


### [8] [High Performance Parallel Solvers for the time-harmonic Maxwell Equations](https://arxiv.org/abs/2507.13066)
*Elise Fressart,Sébastien Dubois,Loïc Gouarin,Marc Massot,Michel Nowak,Nicole Spillane*

Main category: math.NA

TL;DR: Comparison of four preconditioners for solving large-scale time-harmonic Maxwell equations, favoring Hiptmair-Xu and Block Low-Rank methods.


<details>
  <summary>Details</summary>
Motivation: The challenge of solving time-harmonic Maxwell equations, which are neither Hermitian nor semi-definite, drives the need for effective preconditioners.

Method: Evaluated four preconditioners: sparse approximate inverse, Restricted Additive Schwarz, Hiptmair-Xu, and MUMPS's Block Low-Rank, comparing performance against standard LU factorization.

Result: Preliminary results favor Hiptmair-Xu and Block Low-Rank preconditioners based on mesh size, CPU cores, wavelength, and domain size.

Conclusion: Hiptmair-Xu and Block Low-Rank preconditioners show promise for solving time-harmonic Maxwell equations, though work is ongoing.

Abstract: We consider the numerical solution of large scale time-harmonic Maxwell
equations. To this day, this problem remains difficult, in particular because
the equations are neither Hermitian nor semi-definite. Our approach is to
compare different strategies for solving this set of equations with
preconditioners that are available either in PETSc, MUMPS, or in hypre. Four
different preconditioners are considered. The first is the sparse approximate
inverse, which is often applied to electromagnetic problems. The second is
Restricted Additive Schwarz, a domain decomposition preconditioner. The third
is the Hiptmair-Xu preconditioner which is tailored to the positive Maxwell
equations, a nearby problem. The final preconditioner is MUMPS's Block Low-Rank
method, a compressed block procedure. We also compare the performance of this
method to the standard LU factorization technique, which is a direct solver.
Performance with respect to the mesh size, the number of CPU cores, the
wavelength and the physical size of the domain are considered. This work in
progress yields temporary conclusions in favour of the Hiptmair-Xu and the
Block Low-Rank preconditioners.

</details>


### [9] [Stability of lattice Boltzmann schemes for initial boundary value problems in raw formulation](https://arxiv.org/abs/2507.13108)
*Thomas Bellotti*

Main category: math.NA

TL;DR: The paper analyzes the stability of 1D linear scalar lattice Boltzmann schemes for hyperbolic equations, focusing on boundary data effects without transforming to scalar form. It introduces strong stability notions for schemes with specific stencil properties.


<details>
  <summary>Details</summary>
Motivation: To study stability of lattice Boltzmann schemes for hyperbolic equations with boundary data, avoiding complex transformations and addressing unique behaviors.

Method: The approach uses the raw algorithm for multiple unknowns, introduces strong stability notions, and focuses on schemes with left-breadth-one stencils. Three representative schemes with various boundary conditions are analyzed.

Result: Theoretical stability-instability results are derived for specific schemes and validated through numerical simulations.

Conclusion: The study provides insights into stability for lattice Boltzmann schemes with boundary conditions, supported by theoretical and numerical evidence.

Abstract: We study the stability of one-dimensional linear scalar lattice Boltzmann
schemes for hyperbolic equations with respect to boundary data. Our approach is
based on the original raw algorithm on several unknowns, thereby avoiding the
need for a transformation into an equivalent scalar formulation-a challenging
process in presence of boundaries. To address different behaviors exhibited by
the numerical scheme, we introduce appropriate notions of strong stability.
They account for the potential absence of a continuous extension of the stable
vector bundle associated with the bulk scheme on the unit circle for certain
components. Rather than developing a general theory, complicated by the fact
that discrete boundaries in lattice Boltzmann schemes are inherently
characteristic, we focus on strong stability-instability for methods whose
characteristic equations have stencils of breadth one to the left. In this
context, we study three representative schemes. These are endowed with various
boundary conditions drawn from the literature, and our theoretical results are
supported by numerical simulations.

</details>


### [10] [Generalized Scattering Matrix Framework for Modeling Implantable Antennas in Multilayered Spherical Media](https://arxiv.org/abs/2507.13119)
*Chenbo Shi,Xin Gu,Shichen Liang,Jin Pan*

Main category: math.NA

TL;DR: A unified framework for analyzing antennas in spherically stratified media, combining free-space GSM with extended SSOs for efficient and accurate modeling.


<details>
  <summary>Details</summary>
Motivation: To address the computational inefficiency of traditional DGF-based MoM methods for antennas in multilayered spherical environments, common in biomedical and engineering applications.

Method: Decouples antenna and medium modeling using free-space GSM and extended SSOs, enabling rapid reevaluation without re-simulation.

Result: Demonstrates accuracy and generality through case studies, matching full-wave and DGF-based solutions.

Conclusion: The framework offers computational efficiency, scalability, and broad applicability, with provided code for adoption.

Abstract: This paper presents a unified and efficient framework for analyzing antennas
embedded in spherically stratified media -- a model broadly applicable to
implantable antennas in biomedical systems and radome-enclosed antennas in
engineering applications. The proposed method decouples the modeling of the
antenna and its surrounding medium by combining the antenna's free-space
generalized scattering matrix (GSM) with a set of extended spherical scattering
operators (SSOs) that rigorously capture the electromagnetic interactions with
multilayered spherical environments. This decoupling enables rapid reevaluation
under arbitrary material variations without re-simulating the antenna, offering
substantial computational advantages over traditional dyadic Green's function
(DGF)-based MoM approaches. The framework supports a wide range of spherical
media, including radially inhomogeneous and uniaxially anisotropic layers.
Extensive case studies demonstrate excellent agreement with full-wave and
DGF-based solutions, confirming the method's accuracy, generality, and
scalability. Code implementations are provided to facilitate adoption and
future development.

</details>


### [11] [On the efficiency of a posteriori error estimators for parabolic partial differential equations in the energy norm](https://arxiv.org/abs/2507.13188)
*Iain Smears*

Main category: math.NA

TL;DR: The paper analyzes the efficiency of a posteriori error estimators for the heat equation, showing dependence on the choice of norm and numerical solution representation.


<details>
  <summary>Details</summary>
Motivation: To understand how the efficiency of error estimators varies with different norms and numerical solution definitions in the context of the heat equation.

Method: Uses an implicit Euler method for time discretization and a conforming finite element method in space, comparing error estimators for different numerical solution representations.

Result: Demonstrates that the efficiency of error estimators depends on both the norm and the definition of the numerical solution.

Conclusion: Highlights the importance of norm and numerical solution choice in assessing estimator efficiency for the heat equation.

Abstract: For the model problem of the heat equation discretized by an implicit Euler
method in time and a conforming finite element method in space, we prove the
efficiency of a posteriori error estimators with respect to the energy norm of
the error, when considering the numerical solution as the average between the
usual continuous piecewise affine-in-time and piecewise constant-in-time
reconstructions. This illustrates how the efficiency of the estimators is not
only possibly dependent on the choice of norm, but also on the choice of notion
of numerical solution.

</details>


### [12] [Well-balanced path-conservative discontinuous Galerkin methods with equilibrium preserving space for shallow water linearized moment equations](https://arxiv.org/abs/2507.13284)
*Ruilin Fan,Julian Koellermeier,Yinhua Xia,Yan Xu,Jiahui Zhang*

Main category: math.NA

TL;DR: High-order, well-balanced, path-conservative DG methods for SWLME preserve still and moving water equilibria, addressing challenges of non-conservative terms and complex steady states.


<details>
  <summary>Details</summary>
Motivation: The SWLME offers detailed vertical momentum transfer modeling but introduces numerical challenges due to non-conservative terms and complex steady states.

Method: Develops path-conservative DG schemes using DLM theory, balancing flux gradients, non-conservative terms, and sources. Reformulates equations for still water and extends DG for moving water equilibrium.

Result: Theoretical and numerical results show exact equilibrium preservation and high-order accuracy in complex scenarios.

Conclusion: The proposed methods effectively handle SWLME challenges, preserving equilibria and maintaining accuracy.

Abstract: This paper presents high-order, well-balanced, path-conservative
discontinuous Galerkin (DG) methods for the shallow water linearized moment
equations (SWLME), designed to preserve both still and moving water equilibrium
states. Unlike the multi-layer shallow water equations, which model vertical
velocity variations using multiple distinct layers, the SWLME employs a
polynomial expansion of velocity profiles with up to $N$ moments. This approach
enables a more detailed representation of vertical momentum transfer and
complex velocity profiles while retaining hyperbolicity. However, the presence
of non-conservative terms and complex steady-state structures introduces
significant numerical challenges. Addressing these challenges, we develop
path-conservative DG schemes grounded in the Dal Maso-LeFloch-Murat (DLM)
theory for non-conservative products. Our method balances flux gradients,
non-conservative terms, and source terms through equilibrium-preserving spaces.
For the still water equilibrium, we reformulate the equations into a
quasilinear form that eliminates source terms, inherently preserving steady
states. For the moving water equilibrium, we extend the DG method by
transforming conservative variables into equilibrium variables and employing
linear segment paths. Theoretical analysis and numerical experiments
demonstrate that the proposed methods achieve exact equilibrium preservation
while maintaining high-order accuracy, even in scenarios with vertical velocity
variations and complex topographies.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [13] [Uniform dispersive estimates for the semi-classical Hartree equation with long-range interaction](https://arxiv.org/abs/2507.12577)
*Sonae Hadama*

Main category: math.AP

TL;DR: The paper analyzes the Hartree equation in 3D with long-range interactions, showing optimal decay of the density function for small-data solutions, uniformly in the semi-classical parameter ℏ. It also provides a new proof for modified scattering in a related nonlinear Schrödinger equation.


<details>
  <summary>Details</summary>
Motivation: To extend existing results on the Hartree equation by ensuring uniformity with respect to the semi-classical parameter ℏ and to provide a novel proof for modified scattering in a related nonlinear Schrödinger equation.

Method: The proof involves boundedness of modified wave operators, an L¹–L∞ dispersive estimate for the modified propagator, and commutator estimates for density operators, combined with a bootstrap argument.

Result: The density function decays optimally for small-data solutions, with conditions and bounds independent of ℏ.

Conclusion: The paper achieves uniformity in ℏ and offers a new proof for modified scattering, advancing understanding of long-range interactions in the semi-classical regime.

Abstract: In this paper, we consider the Hartree equation with smooth but long-range
interaction in the semi-classical regime, in three-dimensional space. We show
that the density function of small-data solution decays at the optimal rate.
When the semi-classical parameter $\hbar \in (0,1]$ is fixed, our result is
essentially covered by the recent work by Nguyen and You [arXiv:2408.15860];
however, the novelty of this paper is the uniformity with respect to $\hbar$.
Namely, both smallness condition for initial data and bounds for the solution
are independent of $\hbar$. Moreover, the argument in this paper provides a new
proof of the modified scattering for the long-range nonlinear Schr\"{o}dinger
equation with a Hartree type nonlinearity. Our proof relies on three main
ingredients. First, we prove the boundedness of finite-time wave operators
modified by phase corrections. Second, we show an $L^1$--$L^\infty$ dispersive
estimate for the modified propagator. Third, we give various kinds of
commutator estimates for density operators. By combining them, we can apply the
usual bootstrap argument to obtain the main result.

</details>


### [14] [Boundary Feedback and Observer Synthesis for a Class of Nonlinear Parabolic--Elliptic PDE Systems](https://arxiv.org/abs/2507.12615)
*Kamal Fenza,Moussa Labbadi,Mohamed Ouzahra*

Main category: math.AP

TL;DR: The paper studies stabilization of a coupled parabolic-elliptic PDE system with nonlinearities, using backstepping for boundary control and observer design, ensuring exponential stability.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of stabilizing coupled PDE systems with nonlinear terms, which are common in engineering and physics.

Method: Rigorous backstepping design for explicit boundary control and observers from partial measurements.

Result: Theorems prove exponential stability and well-posedness of the nonlinear closed-loop system.

Conclusion: The proposed method effectively stabilizes the coupled PDE system, with theoretical guarantees.

Abstract: This paper investigates the stabilization of a coupled system comprising a
parabolic PDE and an elliptic PDE with nonlinear terms. A rigorous backstepping
design provides an explicit boundary control law and exponentially convergent
observers from partial boundary measurements. Several theorems ensure
exponential stability and well-posedness of the nonlinear closed-loop system.

</details>


### [15] [Semi-classical limit of quantum scattering states for the nonlinear Hartree equation](https://arxiv.org/abs/2507.12627)
*Sonae Hadama,Younghun Hong*

Main category: math.AP

TL;DR: The paper analyzes the long-time dynamics of quantum particles in the semi-classical regime, showing dispersion bounds and scattering for small-data solutions of the nonlinear Hartree equation. It also connects quantum and classical scattering states via the Wigner transform.


<details>
  <summary>Details</summary>
Motivation: To understand the transition from quantum to classical dynamics in the semi-classical regime and establish scattering results for singular interaction potentials.

Method: Uses uniform dispersion estimates for the free Schrödinger flow and analyzes the Wigner transform to connect quantum and classical scattering states.

Result: Small-data solutions scatter, and Wigner transforms of quantum states converge to classical states. Small-data scattering for the Vlasov equation is established without regularity assumptions.

Conclusion: The study bridges quantum and classical dynamics, providing insights into scattering behavior for singular potentials and the semi-classical limit.

Abstract: This article concerns the long-time dynamics of quantum particles in the
semi-classical regime. First, we show that for the nonlinear Hartree equation
with short-range interaction potential, small-data solutions obey dispersion
bounds and they scatter, where the smallness conditions and the bounds are
independent of the small parameter $\hbar\in(0,1]$ representing the reduced
Planck constant. Then, taking the semi-classical limit $\hbar\to0$, we prove
that the Wigner transforms of such quantum scattering states converge weakly-*
to the corresponding classical scattering states for the Vlasov equation. As a
direct consequence, we establish small-data scattering for the Vlasov equation
without assuming regularity on initial data. Our analysis is based on a new
uniform dispersion estimate for the free Schr\"odinger flow, which is simple
but crucial to include singular interaction potentials such as inverse
power-law potential $\frac{1}{|x|^a}$ with $1<a<\frac{5}{3}$.

</details>


### [16] [Asymptotically sharp stability of Sobolev inequalities on the Heisenberg group with dimension-dependent constants](https://arxiv.org/abs/2507.12725)
*Lu Chen,Guozhen Lu,Hanli Tang,Bohan Wang*

Main category: math.AP

TL;DR: The paper establishes optimal stability bounds for Sobolev and Hardy-Littlewood-Sobolev inequalities on the Heisenberg group using CR Yamabe flow, avoiding rearrangement techniques.


<details>
  <summary>Details</summary>
Motivation: To address the lack of rearrangement inequality in the CR setting, which complicates deriving optimal stability for Sobolev inequalities.

Method: Uses bispherical harmonics, orthogonality techniques, and CR Yamabe flow to achieve optimal stability without rearrangement.

Result: Proves optimal stability for Sobolev and HLS inequalities on the Heisenberg group with dimension-dependent constants.

Conclusion: The rearrangement-free approach is versatile and applicable to fractional Sobolev and HLS inequalities once suitable flows are established.

Abstract: In this paper, we are concerned with the optimal asymptotic lower bound for
the stability of Sobolev inequality on the Heisenberg group. We first establish
the optimal local stability of Sobolev inequality on the CR sphere through
bispherical harmonics and complicated orthogonality technique ( see Lemma 3.1).
The loss of rearrangement inequality in the CR setting makes it impossible to
use any rearrangement flow technique (either differential rearrangement flow or
integral rearrangement flow) to derive the optimal stability of Sobolev
inequality on the CR sphere from corresponding optimal local stability. To
circumvent this, we will use the CR Yamabe flow to establish the optimal
stability of Sobolev inequality on the Heisenberg group with the
dimension-dependent constants (see Theorem 1.1). As an application, we also
establish the optimal stability of the Hardy-Littlewood-Sobolev (HLS)
inequality for special conformal index with the dimension-dependent constants
(see Theorem 1.3). Our approach is rearrangement-free and can be used to study
the optimal stability problem for fractional Sobolev inequality or HLS
inequality on the Heisenberg group once the corresponding continuous flow is
established.

</details>


### [17] [Analysis of a parabolic-hyperbolic hybrid population model: an integrated semigroup approach](https://arxiv.org/abs/2507.12833)
*Qihua Huang,Minglong Wang,Yixiang Wu*

Main category: math.AP

TL;DR: A hybrid parabolic-hyperbolic model for populations with distinct dispersal and sedentary stages is analyzed. The study establishes global well-posedness, a comparison principle, and asymptotic smoothness. The net reproductive rate $\mathcal{R}_{0}$ is derived and linked to system stability.


<details>
  <summary>Details</summary>
Motivation: To understand the global dynamics of populations with distinct dispersal and sedentary stages, focusing on stability and persistence.

Method: Established global well-posedness, proved a comparison principle, analyzed spectral properties of the linearized system, and derived the net reproductive rate $\mathcal{R}_{0}$.

Result: $\mathcal{R}_{0}$ determines system stability: trivial equilibrium is stable if $\mathcal{R}_{0}<1$, while $\mathcal{R}_{0}>1$ leads to uniform persistence and a unique, stable positive equilibrium.

Conclusion: The net reproductive rate $\mathcal{R}_{0}$ is a critical threshold for population dynamics in the hybrid model, governing stability and persistence.

Abstract: This paper is concerned with the global dynamics of a hybrid
parabolic-hyperbolic model describing populations with distinct dispersal and
sedentary stages. We first establish the global well-posedness of solutions,
prove a comparison principle, and demonstrate the asymptotic smoothness of the
solution semiflow. Through the spectral analysis of the linearized system, we
derive and characterize the net reproductive rate $\mathcal{R}_{0}$.
Furthermore, an explicit relationship between $\mathcal{R}_{0}$ and the
principal eigenvalue of the linearized system is analyzed. Under appropriate
monotonicity assumptions, we show that $\mathcal{R}_{0}$ serves as a threshold
parameter that completely determines the stability of steady states of the
system. More precisely, when $\mathcal{R}_{0}<1$, the trivial equilibrium is
globally asymptotical stable, while when $\mathcal{R}_{0}>1$, the system is
uniformly persistent and there is a positive equilibrium which is unique and
globally asymptotical stable.

</details>


### [18] [Polyharmonic Nonlinear Scalar Field Equations](https://arxiv.org/abs/2507.12962)
*Alessandro Cannone,Silvia Cingolani,Jarosław Mederski*

Main category: math.AP

TL;DR: Existence of ground state solutions for polyharmonic nonlinear equations with subcritical growth, overcoming analytical challenges and introducing a new polyharmonic logarithmic Sobolev inequality.


<details>
  <summary>Details</summary>
Motivation: Inspired by Berestycki and Lions, the study extends results to higher-order operators, addressing gaps in the biharmonic case.

Method: Analysis of the polyharmonic nonlinear equation $(-\Delta)^m u=g(u)$ with subcritical growth assumptions.

Result: Existence of ground state solutions and establishment of a new polyharmonic logarithmic Sobolev inequality.

Conclusion: The work successfully tackles analytical challenges in higher-order operators and contributes new inequalities to the field.

Abstract: In this paper, we present a result on the existence of ground state solutions
for the polyharmonic nonlinear equation $(-\Delta)^m u=g(u)$, assuming that $g$
has a general subcritical growth at infinity, inspired by Berestycki and Lions
\cite{BerestyckiLions}. In comparison with the biharmonic case studied in
\cite{Med-Siem}, the presence of a higher-order operator gives rise to several
analytical challenges, which are overcome in the present work. Furthermore, we
establish a new polyharmonic logarithmic Sobolev inequality.

</details>


### [19] [Symmetrization on the sphere and applications](https://arxiv.org/abs/2507.13027)
*Satyanad Kichenassamy*

Main category: math.AP

TL;DR: A new symmetrization method for mappings on the n-sphere is introduced, applied to estimate solutions of quasilinear elliptic PDEs of p-Laplacian type with Dirac measures. The case p=n is reduced to a spherical problem via conformal transformation, while other cases (1<p<n, p>n) are briefly discussed.


<details>
  <summary>Details</summary>
Motivation: To develop a symmetrization technique for solving quasilinear elliptic PDEs with Dirac measures, leveraging spherical geometry for the p=n case.

Method: Introduces a symmetrization method for mappings on the n-sphere, applies it to p-Laplacian PDEs, and uses conformal transformation for the p=n case. Other cases are referenced.

Result: The method provides estimates for solutions of the PDEs, with detailed results for p=n and references for other cases.

Conclusion: The symmetrization technique is effective for p=n, with potential extensions to other cases as discussed in related works.

Abstract: We introduce a new method of symmetrization of mappings on the $n$-sphere
($n\geq 2$). They are applied to estimate solutions of quasilinear elliptic
partial differential equations of $p$-Laplacian type, with combinations of
Dirac measures on the right-hand side. The case $p=n$ is reduced to a problem
on the sphere, using a conformal transformation. The cases when $1 < p < n $
and $p > $n are considered more briefly, full details being available in other
papers of the author.

</details>


### [20] [Exponential convergence for ultrafast diffusion equations with log-concave weights](https://arxiv.org/abs/2507.13060)
*Max Fathi,Mikaela Iacobelli*

Main category: math.AP

TL;DR: Exponential convergence to equilibrium is proven for a weighted ultrafast diffusion PDE on the real line with specific weight conditions.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the gradient flow approach to measure quantization.

Method: Analysis of the asymptotic behavior of the PDE with log-concave and log-Lipschitz weights.

Result: Exponential convergence to equilibrium is established, extending beyond compact settings.

Conclusion: The findings advance understanding of ultrafast diffusion and its applications in measure quantization.

Abstract: We study the asymptotic behavior of a weighted ultrafast diffusion PDE on the
real line, with a log-concave and log-lipschitz weight, and prove exponential
convergence to equilibrium. This result goes beyond the compact setting studied
in [22]. This equation is motivated by the gradient flow approach to the
problem of quantization of measures introduced in [11].

</details>


### [21] [Nonlinear smoothing implies improved lower bounds on the radius of spatial analyticity for nonlinear dispersive equations](https://arxiv.org/abs/2507.13083)
*Mikaela Baldasso,Simão Correia*

Main category: math.AP

TL;DR: Improved lower bounds for the decay rate of the uniform radius of analyticity in nonlinear dispersive equations, achieving σ(T)≳T−1/2−ϵ.


<details>
  <summary>Details</summary>
Motivation: To enhance understanding of the decay rate of analyticity in nonlinear dispersive equations, focusing on generalized KdV and nonlinear Schrödinger equations.

Method: Developed a strategy involving nonlinear smoothing estimates with specific derivative distributions to derive improved bounds.

Result: Achieved the lower bound σ(T)≳T−1/2−ϵ for both equations, surpassing existing literature.

Conclusion: The method successfully improves decay rate bounds, advancing the field of nonlinear dispersive equations.

Abstract: We provide a roadmap to establish improved lower bounds on the decay rate of
the uniform radius of analyticity $\sigma(T)$ for a given nonlinear dispersive
equation, reducing the problem to the derivation of nonlinear smoothing
estimates with a specific distribution of extra derivatives. We apply this
strategy for both the defocusing generalized KdV and the nonlinear
Schr\"odinger equations with odd pure-power nonlinearity. For both equations,
we reach the lower bound $\sigma(T)\gtrsim T^{-\frac{1}{2}-\epsilon}$, for any
$\epsilon>0$, thus improving all available results in the current literature.

</details>


### [22] [A New Framework for Unidimensional Structures Based on Generalised Continua](https://arxiv.org/abs/2507.13098)
*Mewen Crespo,Casale Guy,Loïc Le Marrec,Patrizio Neff*

Main category: math.AP

TL;DR: A family of beam models is derived from a 3D higher-order elasticity framework, incorporating three kinematic fields to explore holonomic, semi-holonomic, and non-holonomic regimes, capturing material defects in beams.


<details>
  <summary>Details</summary>
Motivation: To systematically explore varying kinematic constraints in beam models, from classical elasticity to a fully relaxed framework, and unify dislocations and disclinations.

Method: Incorporates three kinematic fields (u, P, N) to derive models for holonomic, semi-holonomic, and non-holonomic regimes, with simplified ODE systems for practical cases.

Result: The holonomic case reduces to a higher-order Euler-Bernoulli beam, the semi-holonomic generalizes Timoshenko, and the non-holonomic unifies dislocations and disclinations.

Conclusion: The framework hierarchically captures material defects in beams, with holonomic and semi-holonomic models emerging as singular limits of the non-holonomic case.

Abstract: The present work introduces a family of beam models derived from a
three-dimensional higher-order elasticity framework. By incorporating three
kinematic fields - the macroscopic displacement u, the micro-distortion tensor
P, and the third-order tensor N - the study systematically explores three
regimes: holonomic, semi-holonomic, and non-holonomic. These regimes correspond
to varying levels of kinematic constraints, ranging from classical elasticity
to a fully relaxed model. The holonomic case reduces to a higher-order
Euler--Bernoulli beam model, while the semi-holonomic case generalises the
Timoshenko beam model. The non-holonomic case provides a unified framework that
naturally incorporates both dislocations and disclinations. Furthermore, the
holonomic and semi-holonomic models are shown to emerge as singular limits of
the non-holonomic model by increasing specific penalty coefficients. Simplified
ordinary differential equation systems are derived for specific cases, such as
pure traction and bending, illustrating the practical applicability of the
models. The results highlight the hierarchical structure of the proposed
framework and its ability to capture material defects in beam-like structures.

</details>


### [23] [Normalized solutions of coupled Sobolev critical Schrodinger equations with mass subcritical couplings](https://arxiv.org/abs/2507.13163)
*Zhang Jianjun,Zhong Xuexiu,Zhou Jinfang*

Main category: math.AP

TL;DR: The paper studies coupled Sobolev critical Schrödinger equations with mass constraints, proving the existence of two positive solutions for small ν and analyzing their asymptotic behavior as ν→0.


<details>
  <summary>Details</summary>
Motivation: The research addresses a Soave's type open problem regarding the existence and behavior of solutions in the mass mixed case for coupled Schrödinger equations.

Method: The authors analyze the system using variational methods, focusing on the mass mixed case with specific constraints on α and β.

Result: For small ν>0, the system admits two positive solutions: a local minimizer and a mountain pass solution. Asymptotic behaviors of these solutions are also examined as ν→0.

Conclusion: The findings affirmatively resolve the open problem posed by Bartsch et al., providing insights into the qualitative properties of solutions in the studied system.

Abstract: We are concerned with qualitative properties of positive solutions to the
following coupled Sobolev critical Schr\"odinger equations $$ \begin{cases}
-\Delta u+\lambda_1 u=\mu_1|u|^{2^*-2}u+\nu\alpha |u|^{\alpha-2}|v|^{\beta}u
~\hbox{in}~ \R^N,\\ -\Delta v+\lambda_2 v=\mu_2|v|^{2^*-2}v+\nu\beta
|u|^{\alpha}|v|^{\beta-2}v ~\hbox{in}~ \R^N \end{cases} $$ subject to the mass
constraints $\int_{\mathbb{R}^N}|u|^2 \ud x=a^2$ and $\int_{\mathbb{R}^N}|v|^2
\ud x=b^2$, where, $a>0,\,b>0,\,N=3,4$ and $2^*:=\frac{2N}{N-2}$ is the Sobolev
critical exponent. The main purpose of this paper is focused on the mass mixed
case, i. e., $ \alpha>1,\beta>1,\alpha+\beta<2+\frac{4}{N}$. For some suitable
small $\nu>0$, we show that the above system admits two positive solutions, one
of which is a local minimizer, and another one is a mountain pass solution.
Moreover, as $\nu\to0^+$, asymptotic behaviors of solutions are also
considered. Our result gives an affirmative answer to a Soave's type open
problem raised by Bartsch {\it et al.} (Calc. Var. Partial Differential
Equations 62(1), Paper No. 9, 34, 2023).

</details>


### [24] [Robin Green Function Estimates and a Model of Mammalian Lungs](https://arxiv.org/abs/2507.13168)
*Guy David,Stefano Decio,Max Engelstein,Marcel Filoche,Svitlana Mayboroda,Marco Michetti*

Main category: math.AP

TL;DR: The paper analyzes the Green function's properties under Robin boundary conditions, revealing transitions between Dirichlet-like and Neumann-like behaviors, with implications for harmonic measure bounds and lung efficiency.


<details>
  <summary>Details</summary>
Motivation: To understand the transition between Dirichlet-like and Neumann-like behaviors in the Green function under Robin boundary conditions, and its applications in physics and biology, such as mammalian lung efficiency.

Method: The study establishes properties of the Green function with Robin boundary conditions, focusing on transitions and quantifiable bounds on harmonic measures.

Result: Sharp bounds on harmonic measures are derived, and a phase transition in total flow behavior, previously conjectured in physics, is proven.

Conclusion: The findings confirm the conjectured phase transition and provide insights into the efficiency of mammalian lungs, linking mathematical analysis to biological applications.

Abstract: The present paper establishes delicate properties of the Green function with
Robin boundary conditions, in particular, elucidating the nature of the passage
between the Dirichlet-like and Neumann-like behavior. This yields sharp
quantifiable bounds on the corresponding harmonic measure and proves the phase
transition in the behavior of the total flow earlier conjectured in physics
literature in concert with the efficacy of mammalian lungs.

</details>


### [25] [Multiple normalized solutions for two coupled Gross-Pitaevskii equations with attractive interactions and mass constriants](https://arxiv.org/abs/2507.13172)
*Zhang Jianjun,Zhong Xuexiu,Zhou Jinfang*

Main category: math.AP

TL;DR: The paper studies a system of coupled Gross-Pitaevskii equations in two-component Bose-Einstein condensates, focusing on solutions under mass constraints.


<details>
  <summary>Details</summary>
Motivation: To analyze the existence of solutions in a system modeling two-component Bose-Einstein condensates with attractive interactions.

Method: Variational methods are used to seek critical points of the associated functional under mass constraints.

Result: Two positive solutions are found in the mass mixed case, including a local minimizer and a mountain pass solution.

Conclusion: The system admits multiple solutions under specific conditions, demonstrating the effectiveness of variational approaches.

Abstract: We are concerned with the following system of two coupled time-independent
Gross-Pitaevskii equations $$ \begin{cases} -\Delta u+\lambda_1
u=\mu_1|u|^{p-2}u+\nu\alpha |u|^{\alpha-2}|v|^{\beta}u ~\hbox{in}~ \R^N,\\
-\Delta v+\lambda_2 v=\mu_2|v|^{q-2}v+\nu\beta |u|^{\alpha}|v|^{\beta-2}v
~\hbox{in}~ \R^N, \end{cases} $$ which arises in two-components Bose-Einstein
condensates and involve attractive Sobolev subcritical or critical
interactions, i. e., $\nu>0$ and $\alpha+\beta\leq 2^*$. This system is
employed by seeking critical points of the associated variational functional
with the constrained mass below $$\int_{\mathbb{R}^N}|u|^2 {\rm d}x=a, \quad
\int_{\mathbb{R}^N}|v|^2 {\rm d}x=b.$$ In the mass mixed case, i. e.,
$2<p<2+\frac{4}{N}<q<2^*$, for some suitable $a,b,\nu$ and $\beta$, the system
above admits two positive solutions. In particular, in the case
$\alpha+\beta<2^*$, using variational methods on the $L^2$-ball, two positive
solutions are obtained, one of which is a local minimizer and the second one is
a mountain pass solution.

</details>


### [26] [Pointwise convergence to initial data of heat and Poisson equations in Modulation Spaces](https://arxiv.org/abs/2507.13220)
*Divyang G. Bhimani,Rupak K. Dalai*

Main category: math.AP

TL;DR: The paper analyzes weighted modulation spaces for pointwise convergence of heat and Poisson semigroups to initial data, and the operation of the Hardy-Littlewood maximal operator on these spaces.


<details>
  <summary>Details</summary>
Motivation: To understand the conditions under which heat and Poisson semigroups converge pointwise to initial data in weighted modulation spaces, and to explore the behavior of the Hardy-Littlewood maximal operator in these spaces.

Method: The study focuses on the standard Laplacian and Hermite operator, examining pointwise convergence of semigroups and the operation of the Hardy-Littlewood maximal operator on modulation spaces.

Result: Pointwise convergence is characterized for heat and Poisson semigroups, and the Hardy-Littlewood maximal operator is shown to operate on certain modulation spaces.

Conclusion: The findings provide insights into the behavior of semigroups and maximal operators in weighted modulation spaces, with potential independent interest in the latter.

Abstract: We characterize weighted modulation spaces (data space) for which the heat
semigroup $e^{-tL}f$ converges pointwise to the initial data $f$ as time $t$
tends to zero. Here $L$ stands for the standard Laplacian $-\Delta $ or Hermite
operator $H=-\Delta +|x|^2$ on the whole domain. Similar result also holds for
Poisson semigroup $e^{-t\sqrt{L}}f.$ We also prove that the Hardy-Littlewood
maximal operator operates on certain modulation spaces. This may be of
independent interest.

</details>


### [27] [Rigidity for the heat equation with density on Riemannian manifolds through a conformal change](https://arxiv.org/abs/2507.13230)
*Alexander Grigor'yan,Giulia Meglioli,Alberto Roncoroni*

Main category: math.AP

TL;DR: The paper studies uniqueness conditions for solutions to the heat equation on weighted Riemannian manifolds, focusing on vanishing solutions in specific weighted Lebesgue spaces.


<details>
  <summary>Details</summary>
Motivation: To determine when solutions to the heat equation vanish, given certain conditions on the manifold and density function.

Method: Uses a conformal transformation to reduce the problem to a standard heat equation on a weighted manifold, with explicit counterexamples for validation.

Result: Identifies sufficient conditions for vanishing solutions, distinguishing between cases where $p > 1$ and $p = 1$.

Conclusion: The method and assumptions are validated by counterexamples, showing optimality of the density conditions.

Abstract: We investigate uniqueness of solution to the heat equation with a density
$\rho$ on complete, non-compact weighted Riemannian manifolds of infinite
volume. Our main goal is to identify sufficient conditions under which the
solution $u$ vanishes identically, assuming that $u$ belongs to a certain
weighted Lebesgue space with exponential or polynomial weight, $L^p_{\phi}$. We
distinguish between the cases $p > 1$ and $p = 1$ which required stronger
assumptions on the manifold and the density function $\rho$. We develop a
unified method based on a conformal transformation of the metric, which allows
us to reduce the problem to a standard heat equation on a suitably weighted
manifold. In addition, we construct explicit counterexamples on model manifolds
which demonstrate optimality of our assumptions on the density $\rho$.

</details>


### [28] [The Snapshot Problem for the Euler-Poisson-Darboux Equation](https://arxiv.org/abs/2507.13257)
*Fulton Gonzalez,Jue Wang,Jens Christensen,Tomoyuki Kakehi*

Main category: math.AP

TL;DR: The paper studies the generalized Euler-Poisson-Darboux (EPD) equation with complex parameter α, focusing on existence and uniqueness for a two-snapshot problem involving smooth functions. It connects solutions to Liouville-like numbers related to Bessel functions.


<details>
  <summary>Details</summary>
Motivation: To understand conditions for solutions to the generalized EPD equation given two boundary conditions, and explore the role of Liouville-like numbers in this context.

Method: Analyzes the generalized EPD equation, examines the two-snapshot problem for smooth functions, and investigates properties of derived Liouville-like numbers.

Result: Identifies conditions for solution existence and uniqueness, linking them to Liouville-like numbers associated with Bessel functions.

Conclusion: The study reveals connections between the generalized EPD equation, boundary conditions, and Liouville-like numbers, enriching understanding of such equations.

Abstract: The generalized Euler-Poisson-Darboux (EPD) equation with complex parameter
$\alpha$ is given by $$ \Delta_x u=\frac{\partial^2 u}{\partial
t^2}+\frac{n-1+2\alpha}{t}\,\frac{\partial u}{\partial t}, $$ where $u(x,t)\in
\mathscr E(\mathbb R^n\times \mathbb R)$, with $u$ even in $t$. For $\alpha=0$
and $\alpha=1$ the solution $u(x,t)$ represents a mean value over spheres and
balls, respectively, of radius $|t|$ in $\mathbb R^n$. In this paper we
consider existence and uniqueness results for the following two-snapshot
problem: for fixed positive real numbers $r$ and $s$ and smooth functions $f$
and $g$ on $\mathbb R^n$, what are the conditions under which there is a
solution $u(x,t)$ to the generalized EPD equation such that $u(x,r)=f(x)$ and
$u(x,s)=g(x)$? The answer leads to a discovery of Liouville-like numbers
related to Bessel functions, and we also study the properties of such numbers.

</details>


### [29] [Homogenization of nonlocal exchange energies in micromagnetics](https://arxiv.org/abs/2507.13262)
*Rossella Giorgio,Leon Happ,Hidde Schönberger*

Main category: math.AP

TL;DR: The paper studies homogenization of nonlocal micromagnetic functionals with symmetric and antisymmetric exchange contributions, identifying their Γ-limit as an effective local functional.


<details>
  <summary>Details</summary>
Motivation: To understand the asymptotic behavior of nonlocal energies under vanishing interaction range and heterogeneities, while respecting the unit sphere constraint on magnetization.

Method: Uses a tailored two-scale convergence approach to capture oscillations in specific directions and analyzes nonlocal difference quotients.

Result: Derives an effective local functional via a tangentially constrained nonlocal cell problem, with microscopic oscillations constrained to the tangent space.

Conclusion: The study successfully links nonlocal micromagnetic functionals to their local counterparts, providing insights into homogenization under geometric constraints.

Abstract: We study the homogenization of nonlocal micromagnetic functionals
incorporating both symmetric and antisymmetric exchange contributions under the
physical constraint that the magnetization field takes values in the unit
sphere. Assuming that the nonlocal interaction range and the scale of
heterogeneities vanish simultaneously, we capture the asymptotic behavior of
the nonlocal energies by identifying their $\Gamma$-limit, leading to an
effective local functional expressed through a tangentially constrained
nonlocal cell problem. Our proof builds upon a tailored notion of two-scale
convergence, which takes into account oscillations only in specific directions.
It enables us to describe the two-scale limit of suitable nonlocal difference
quotients, yielding a nonlocal analog of the classical limit decomposition
result for gradient fields. To deal with the manifold constraint of the
magnetization, we additionally prove that the microscopic oscillations in the
two-scale limit are constrained to lie in the tangent space of the sphere.

</details>


### [30] [A hierarchy of blood vessel models, Part I: 3D-1D to 1D](https://arxiv.org/abs/2507.13316)
*Laurel Ohm,Sarah Strikwerda*

Main category: math.AP

TL;DR: The paper analyzes blood perfusion models, focusing on convergence between 3D-3D, 3D-1D, and 1D methods. Part I shows a 3D-1D model converges to a 1D Green's function model with a rate of ε^(1/2)|logε|. Part II extends this to a 3D-3D Darcy-Stokes system.


<details>
  <summary>Details</summary>
Motivation: To rigorously establish convergence results among different modeling approaches for blood perfusion in tissues around thin vessels.

Method: Proposes a 3D-1D Darcy-Poiseuille system with specific boundary conditions, analyzes its well-posedness, and derives a 1D integrodifferential equation. Uses a priori bounds to show convergence rates.

Result: The 1D model converges to the 3D-1D solution with a rate proportional to ε^(1/2)|logε|. Part II extends this to 3D-3D Darcy-Stokes convergence.

Conclusion: The paper establishes a convergence chain across modeling hierarchies, validating the use of simplified models for blood perfusion analysis.

Abstract: We propose and analyze a family of models describing blood perfusion through
a tissue surrounding a thin blood vessel. Our goal is to rigorously establish
convergence results among 3D-3D Darcy--Stokes, 3D-1D Darcy--Poiseuille, and 1D
Green's function methods commonly used to model this process. In Part I, we
propose a 3D-1D Darcy--Poiseuille system where the coupling across the
permeable vessel surface involves an angle-averaged Neumann boundary condition
coupled with a geometrically constrained Robin boundary condition. We show that
this model is well-posed and moreover limits to a 1D Green's function model as
the maximum vessel radius $\epsilon\to 0$. In the 1D model, the exterior blood
pressure is given by an explicit Green's function expression involving the
interior blood pressure. The interior pressure satisfies a novel 1D
integrodifferential equation in which the integral term incorporates the
effects of the exterior pressure and the vessel geometry. Much of this paper is
devoted to analyzing this integrodifferential equation. Using the \emph{a
priori} bounds obtained here, we show that the solution to the 1D model
converges to the 3D-1D solution with a rate proportional to
$\epsilon^{1/2}|\log\epsilon|$. In Part II [Ohm \& Strikwerda, arXiv preprint
July 2025], we rely on the 1D estimates to show that both the 1D and 3D-1D
models converge to a coupled 3D-3D Darcy-Stokes system as $\epsilon\to 0$,
thereby establishing a convergence chain among all hierarchy levels.

</details>


### [31] [A hierarchy of blood vessel models, Part II: 3D-3D to 3D-1D and 1D](https://arxiv.org/abs/2507.13330)
*Laurel Ohm,Sarah Strikwerda*

Main category: math.AP

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We propose and analyze a hierarchy of three models of blood perfusion through
a tissue surrounding a thin arteriole or venule. Our goal is to rigorously link
3D-3D Darcy--Stokes, 3D-1D Darcy--Poiseuille, and 1D Green's function methods
commonly used to model this process. Here in Part II, we consider the most
detailed level, a 3D-3D Darcy-Stokes system coupled across the permeable vessel
surface by mass conservation and pressure/stress balance conditions. We derive
a convergence result between the 3D-3D model and both the 3D-1D
Darcy--Poiseuille model and 1D Green's function model proposed in Part I [Ohm
\& Strikwerda, arXiv preprint July 2025] at a rate proportional to
$\epsilon^{1/6}|\log\epsilon|$, where $\epsilon$ is the maximum vessel radius.
The rate is limited by the inclusion of a degenerate endpoint where the vessel
radius vanishes, i.e. becomes indistinguishable from a capillary. Key to our
proof are \emph{a priori} estimates for the 1D integrodifferential model
obtained in Part I.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [32] [Shape optimization of metastable states](https://arxiv.org/abs/2507.12575)
*Noé Blassel,Tony Lelièvre,Gabriel Stoltz*

Main category: physics.comp-ph

TL;DR: The paper proposes a new method for defining metastable states in molecular simulations, improving upon traditional energy-based approaches by optimizing a local timescale separation metric.


<details>
  <summary>Details</summary>
Motivation: Traditional definitions of metastable states based on energy minimization are often inadequate, especially when entropic effects or thermal fluctuations dominate.

Method: The approach involves shape-optimization of a local timescale separation metric, using analytic expressions for Dirichlet eigenvalues and a local ascent algorithm. Two tractability methods for high-dimensional systems are introduced: dynamical coarse-graining and low-temperature spectral asymptotics.

Result: The method is validated on a biomolecular system, showing significant improvement over conventional definitions.

Conclusion: The proposed approach provides a more effective way to define metastable states, particularly in complex systems where traditional methods fail.

Abstract: The definition of metastable states is an ubiquitous task in the design and
analysis of molecular simulation, and is a crucial input in a variety of
acceleration methods for the sampling of long configurational trajectories.
  Although standard definitions based on local energy minimization procedures
can sometimes be used, these definitions are typically suboptimal, or entirely
inadequate when entropic effects are significant, or when the lowest energy
barriers are quickly overcome by thermal fluctuations.
  In this work, we propose an approach to the definition of metastable states,
based on the shape-optimization of a local separation of timescale metric
directly linked to the efficiency of a class of accelerated molecular dynamics
algorithms.
  To realize this approach, we derive analytic expressions for shape-variations
of Dirichlet eigenvalues for a class of operators associated with reversible
elliptic diffusions, and use them to construct a local ascent algorithm,
explicitly treating the case of multiple eigenvalues.
  We propose two methods to make our method tractable in high-dimensional
systems: one based on dynamical coarse-graining, the other on recently obtained
low-temperature shape-sensitive spectral asymptotics.
  We validate our method on a benchmark biomolecular system, showcasing a
significant improvement over conventional definitions of metastable states.

</details>


### [33] [TinyDEM: Minimal open granular DEM code with sliding, rolling and twisting friction](https://arxiv.org/abs/2507.12610)
*Roman Vetter*

Main category: physics.comp-ph

TL;DR: TinyDEM is a lightweight 3D DEM solver for simulating granular particles with inelastic, frictional collisions, designed as a simple C++11 program with OpenMP parallelization.


<details>
  <summary>Details</summary>
Motivation: To provide a compact, accessible DEM solver for granular particle simulations, avoiding complex programming concepts.

Method: Uses explicit solving of Newton's damped equations for translations/rotations, quaternions for orientation, and includes particle-mesh collisions for complex geometries.

Result: A standalone, parallelized DEM solver (TinyDEM) written in simple C++11, released under BSD license.

Conclusion: TinyDEM serves as an entry point for DEM simulations or a foundation for advanced particle dynamics models.

Abstract: This article introduces TinyDEM, a lightweight implementation of a
full-fledged discrete element method (DEM) solver in 3D. Newton's damped
equations of motion are solved explicitly for translations and rotations of a
polydisperse ensemble of dry, soft, granular spherical particles, using
quaternions to represent their orientation in space without gimbal lock.
Particle collisions are modeled as inelastic and frictional, including full
exchange of torque. With a general particle-mesh collision routine, complex
rigid geometries can be simulated. TinyDEM is designed to be a compact
standalone program written in simple C++11, devoid of explicit pointer
arithmetics and advanced concepts such as manual memory management or
polymorphism. It is parallelized with OpenMP and published freely under the
3-clause BSD license. TinyDEM can serve as an entry point into classical DEM
simulations or as a foundation for more complex models of particle dynamics.

</details>


### [34] [Equalized Hyperspin Machine](https://arxiv.org/abs/2507.12940)
*Marcello Calvanese Strinati,Claudio Conti*

Main category: physics.comp-ph

TL;DR: The paper introduces a method to equalize hyperspin amplitudes in a hyperspin machine, improving its performance in simulating spin models.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a mechanism for equal hyperspin amplitudes in hyperspin machines, which is critical for reliable simulation of spin models.

Method: Uses an additional network of oscillators (equalizers) with antisymmetric nonlinear coupling to equalize hyperspin amplitudes.

Result: The equalized hyperspin machine achieves significantly lower spin energy and reduced sensitivity to system parameters.

Conclusion: The equalized hyperspin machine enhances spin Hamiltonian minimization and enables advanced annealing protocols for better performance.

Abstract: The reliable simulation of spin models is of critical importance to tackle
complex optimization problems that are intractable on conventional computing
machines. The recently introduced hyperspin machine, which is a network of
linearly and nonlinearly coupled parametric oscillators, provides a versatile
simulator of general classical vector spin models in arbitrary dimension,
finding the minimum of the simulated spin Hamiltonian and implementing novel
annealing algorithms. In the hyperspin machine, oscillators evolve in time
minimizing a cost function that must resemble the desired spin Hamiltonian in
order for the system to reliably simulate the target spin model. This condition
is met if the hyperspin amplitudes are equal in the steady state. Currently, no
mechanism to enforce equal amplitudes exists. Here, we bridge this gap and
introduce a method to simulate the hyperspin machine with equalized amplitudes
in the steady state. We employ an additional network of oscillators (named
equalizers) that connect to the hyperspin machine via an antisymmetric
nonlinear coupling and equalize the hyperspin amplitudes. We demonstrate the
performance of such an equalized hyperspin machine by large-scale numerical
simulations up to $10000$ hyperspins. Compared to the hyperspin machine without
equalization, we find that the equalized hyperspin machine (i) Reaches orders
of magnitude lower spin energy, and (ii) Its performance is significantly less
sensitive to the system parameters. The equalized hyperspin machine offers a
competitive spin Hamiltonian minimizer and opens the possibility to combine
amplitude equalization with complex annealing protocols to further boost the
performance of spin machines.

</details>


### [35] [A variationally consistent and asymptotically convergent phase-field model for solute precipitation and dissolution](https://arxiv.org/abs/2507.13270)
*Andrea Lamperti,Laura De Lorenzis*

Main category: physics.comp-ph

TL;DR: A novel phase-field model for solute precipitation and dissolution is proposed, derived variationally from a free energy functional, and validated through asymptotic convergence and numerical examples.


<details>
  <summary>Details</summary>
Motivation: To improve upon previous models by deriving governing equations variationally and ensuring consistency with sharp-interface models.

Method: Derives coupled Allen-Cahn and Cahn-Hilliard equations from a Modica-Mortola free energy functional, validates via matched asymptotic expansions, and tests numerically with finite elements.

Result: The model converges to the sharp-interface limit and provides a new reaction rate expression, demonstrated through 2D and 3D simulations.

Conclusion: The proposed model is consistent, convergent, and effective for simulating solute precipitation and dissolution.

Abstract: We propose a novel phase-field model for solute precipitation and dissolution
in liquid solutions. Unlike in previous studies with similar scope, in our
model the two non-linear coupled governing equations of the problem, which
deliver the solute ion concentration and the phase-field variable, are derived
in a variationally consistent way starting from a free energy functional of
Modica-Mortola type. The phase-field variable is assumed to follow the
non-conservative Allen-Cahn evolution law, whereas the solute ion concentration
obeys the conservative Cahn-Hilliard equation. We also assess the convergence
of the new model to the corresponding sharp-interface model via the method of
matched asymptotic expansions, and derive a novel expression of the reaction
rate of the sharp-interface model. Through a finite element discretization, we
present several numerical examples in two and three dimensions.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [36] [Augmented Lagrangian methods produce cutting-edge magnetic coils for stellarator fusion reactors](https://arxiv.org/abs/2507.12681)
*Pedro F. Gil,Alan A. Kaptanoglu,Eve V. Stenson*

Main category: physics.plasm-ph

TL;DR: An augmented Lagrangian approach improves stellarator coil optimization, outperforming previous methods and producing superior physics and engineering results.


<details>
  <summary>Details</summary>
Motivation: Current methods for designing stellarator coils struggle with nonconvex optimization and resource-intensive parameter scans, leading to suboptimal solutions.

Method: The paper introduces an augmented Lagrangian approach to address the ill-posed problem of coil optimization.

Result: The method outperforms prior Pareto-optimal work, generating coils with excellent physics and engineering properties for five diverse stellarators.

Conclusion: The approach is effective and versatile, producing superior coil solutions for existing reactors like W7-X and HSX.

Abstract: Finding feasible coils for stellarator fusion devices is a crux of realizing
this concept for future power plants. The coils must reproduce the physics
properties of the target plasma while satisfying myriad engineering
constraints. Current efforts struggle to navigate the highly nonconvex
optimization landscape and result in suboptimal stellarator coils, and/or they
end up spending considerable resources scanning the continuous parameter space.
In this work, we present an augmented Lagrangian approach to tackle the
ill-posed problem of coil optimization. Our approach yields solutions that
out-perform previous Pareto-optimal work and display excellent physics and
engineering properties. We conclude by illustrating its effectiveness and
versatility by generating coils for five stellarators with very different
symmetries and magnetic field shaping. In all cases, we find coil solutions
that in various ways outperform published coil sets, including built and
operating reactors such as Wendelstein 7-X (W7-X) and the Helically Symmetric
eXperiment (HSX).

</details>


### [37] [Early Prediction of Current Quench Events in the ADITYA Tokamak using Transformer based Data Driven Models](https://arxiv.org/abs/2507.12797)
*Jyoti Agarwal,Bhaskar Chaudhury,Jaykumar Navadiya,Shrichand Jakhar,Manika Sharma*

Main category: physics.plasm-ph

TL;DR: A transformer-based deep learning model is proposed for early current-quench prediction in tokamak plasmas, outperforming LSTM baselines and showing robustness up to 8 ms lead time.


<details>
  <summary>Details</summary>
Motivation: Disruptions in tokamak plasmas threaten system integrity, necessitating accurate early prediction for effective mitigation.

Method: A transformer-based deep learning model is applied to multivariate time series data from ADITYA tokamak diagnostics.

Result: The transformer model achieves high recall (above 0.9) and outperforms LSTM, especially at 8-10 ms prediction thresholds.

Conclusion: The approach is robust and generalizable, offering a promising foundation for real-time disruption avoidance in tokamaks.

Abstract: Disruptions in tokamak plasmas, marked by sudden thermal and current
quenches, pose serious threats to plasma-facing components and system
integrity. Accurate early prediction, with sufficient lead time before
disruption onset, is vital to enable effective mitigation strategies. This
study presents a novel data-driven approach for predicting early current
quench, a key precursor to disruptions, using transformer-based deep learning
models, applied to ADITYA tokamak diagnostic data. Using multivariate time
series data, the transformer model outperforms LSTM baselines across various
data distributions and prediction thresholds. The transformer model achieves
better recall, maintaining values above 0.9 even up to a prediction threshold
of 8-10 ms, significantly outperforming LSTM in this critical metric. The
proposed approach remains robust up to an 8 ms lead time, offering practical
feasibility for disruption mitigation in ADITYA tokamak. In addition, a
comprehensive data diversity analysis and bias sensitivity study underscore the
generalization of the model. This work marks the first application of
transformer architectures to ADITYA tokamak data for early current-quench
prediction, establishing a promising foundation for real time disruption
avoidance in short-pulse tokamaks.

</details>


### [38] [Introduction to Stability and Turbulent Transport in Magnetic Confinement Fusion Plasmas](https://arxiv.org/abs/2507.13144)
*J. F. Parisi*

Main category: physics.plasm-ph

TL;DR: A tutorial introducing stability and turbulent transport in fusion plasmas, targeting newcomers with key concepts, models, and practical insights.


<details>
  <summary>Details</summary>
Motivation: To provide an accessible guide for new researchers in magnetic confinement fusion plasmas, covering foundational principles and practical implications.

Method: Discusses key concepts and models related to stability and turbulent transport, along with challenges and opportunities in the field.

Result: A clear introduction to the topic, aiding newcomers in understanding the complexities of fusion plasmas.

Conclusion: The tutorial successfully bridges knowledge gaps for new researchers, highlighting both challenges and opportunities in the field.

Abstract: This tutorial provides an accessible introduction to the principles of
stability and turbulent transport in magnetic confinement fusion plasmas. Key
concepts, models, and practical implications are discussed to guide researchers
new to the field. Some challenges and opportunities are discussed.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [39] [Assessing the economic benefits of space weather mitigation investment decisions: Evidence from Aotearoa New Zealand](https://arxiv.org/abs/2507.12495)
*Edward J. Oughton,Andrew Renton,Daniel Mac Marnus,Craig J. Rodger*

Main category: physics.geo-ph

TL;DR: The study assesses the economic impact of geomagnetic storms on New Zealand, highlighting potential GDP losses and the benefits of mitigation strategies like optimized switching and GIC blocking devices.


<details>
  <summary>Details</summary>
Motivation: To quantify the macroeconomic consequences of space weather events, particularly geomagnetic storms, on New Zealand's economy and support decision-making for mitigation investments.

Method: The study evaluates GDP losses across seven disruption and mitigation scenarios due to an extreme coronal mass ejection (CME), focusing on geomagnetically induced currents (GICs) in the power transmission network.

Result: Without mitigation, a severe storm could cause NZ$8.36 billion in GDP losses, with cascading supply chain effects contributing over half. Mitigation strategies like optimized switching and GIC blocking devices can significantly reduce losses, with benefit-cost ratios as high as 740:1 and 80:1, respectively.

Conclusion: The economic rationale for pre-emptive mitigation is strong, given the high potential losses. Future research should integrate capital and revenue loss modeling for industrial facilities.

Abstract: Space weather events pose a growing threat to modern economies, yet their
macroeconomic consequences still remain underexplored. This study presents the
first dedicated economic assessment of geomagnetic storm impacts on Aotearoa
New Zealand, quantifying potential GDP losses across seven disruption and
mitigation scenarios due to an extreme coronal mass ejection (CME). The primary
focus is upon the damaging impacts of geomagnetically induced currents (GICs)
on the electrical power transmission network. The goal is to support
decision-making around space weather mitigation investments by providing a
first-order approximation of their potential economic benefits. We find that in
the absence of mitigation, a severe but realistic storm could result in up to
NZ\$8.36 billion in lost GDP, with more than half stemming from cascading
supply chain effects. Yet, even less severe scenarios incur losses exceeding
NZ\$3 billion. Importantly, research-led operational strategies, such as
optimized switching and islanding, can avoid up to NZ\$370 million in losses
for as little as NZ\$500,000 in expenditure, delivering a benefit-cost ratio of
740 to 1. Moreover, physical protections such as GIC blocking devices further
reduce disruption to as low as NZ\$1.12 billion, with avoided GDP losses up to
NZ\$2.3 billion, and benefit-cost returns up to 80 to 1. When also
acknowledging unmodelled impacts, including multi-billion losses in capital
equipment and long-term revenue, the economic rationale for pre-emptive
mitigation becomes even more pertinent. Future research needs to integrate the
modelling of capital and revenue losses for strategically important industrial
facilities.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [40] [Accelerated free energy estimation in ab initio path integral Monte Carlo simulations](https://arxiv.org/abs/2507.12960)
*Pontus Svensson,Fotios Kalkavouras,Uwe Hernandez Acosta,Zhandos A. Moldabekov,Panagiotis Tolias,Jan Vorberger,Tobias Dornheim*

Main category: physics.chem-ph

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We present a methodology for accelerating the estimation of the free energy
from path integral Monte Carlo simulations by considering an intermediate
artificial reference system where interactions are inexpensive to evaluate
numerically. Using the spherically averaged Ewald interaction as this
intermediate reference system for the uniform electron gas, the interaction
contribution for the free energy was evaluated up to 18 times faster than the
Ewald-only method. Furthermore, a $\xi$-extrapolation technique was tested and
applied to alleviate the fermion sign problem and to resolve the sign for large
particle numbers. Combining these two techniques enabled the evaluation of the
free energy for a system of 1000 electrons, where both finite-size and
statistical errors are below chemical accuracy. The general procedure can be
applied to systems relevant for planetary and inertial confinement fusion
modeling with low to moderate levels of quantum degeneracy.

</details>


### [41] [$GW$+2SOSEX self-energy made positive semi-definite](https://arxiv.org/abs/2507.13293)
*Fabien Bruneval,Arno Förster,Yaroslav Pavlyukh*

Main category: physics.chem-ph

TL;DR: The paper introduces $GW$+2SOSEX-psd, a positive semi-definite extension to the $GW$+2SOSEX self-energy, addressing the challenge of vertex corrections in perturbation theory.


<details>
  <summary>Details</summary>
Motivation: Vertex corrections beyond the $GW$ approximation often fail to satisfy the positive semi-definiteness condition, a problem this study aims to solve.

Method: The authors cancel bare energy poles in the $G3W2$ self-energy and propose the $GW$+2SOSEX-psd approximation.

Result: The proposed self-energy is shown to be positive semi-definite and accurate for predicting quasiparticle energies in valence and core states.

Conclusion: The $GW$+2SOSEX-psd method successfully addresses the positive semi-definiteness issue while maintaining accuracy in energy predictions.

Abstract: The formulation of vertex corrections beyond the $GW$ approximation within
the framework of perturbation theory is a subtle and challenging task, which
accounts for the wide variety of schemes proposed over the years. Exact
self-energies are required to satisfy the mathematical condition of positive
semi-definiteness. The $GW$ self-energy fulfills this property, but the vast
majority of the vertex-corrected self-energy approximations do not. In this
study, we devise a positive semi-definite extension to the $GW$+2SOSEX
self-energy that we name $GW$+2SOSEX-psd. To reach this goal, we demonstrate
the cancellation of the bare energy poles that are contained in the fully
dynamic second-order in $W$ self-energy ($G3W2$). We then demonstrate on
molecular examples the correct positive semi-definiteness of the proposed
self-energy approximation and its good accuracy in predicting accurate
quasiparticle energies for valence and core states.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [42] [N Bugs on a Circle](https://arxiv.org/abs/2507.13333)
*Josh Briley,Bryan Quaife*

Main category: math.DS

TL;DR: A generalization of the 'Four Bugs on a Square' problem is analyzed, where bugs move on a unit circle's perimeter, leading to three steady states: coalescence, antipodal clusters, or stable chase cycles. Stability and probabilities are studied, with exact results for N ≤ 4 and simulations for larger N.


<details>
  <summary>Details</summary>
Motivation: To explore how restricting bugs to a circle's perimeter alters their long-term behavior compared to unrestricted pursuit problems, revealing richer dynamics.

Method: Analyze steady states (coalescence, antipodal clusters, chase cycles), their stability, and probabilities using exact expressions for N ≤ 4 and Monte Carlo simulations for larger N.

Result: Three steady states emerge; coalescence probability follows an inverse square root relationship with N. Exact probabilities derived for N ≤ 4.

Conclusion: Restricting bugs to a circle's perimeter introduces complex dynamics absent in classic problems, providing insights into constrained pursuit behavior.

Abstract: We describe and analyze a generalization of the classic ``Four Bugs on a
Square'' cyclic pursuit problem. Instead of allowing the bugs to spiral towards
one another, we constrain $N$ bugs to the perimeter of the unit circle.
Depending on their configuration, each bug moves either clockwise or
counterclockwise with a constant angular speed, or remains stationary. Unlike
the original problem where bugs always coalesce, this generalization produces
three possible steady states: all bugs coalescing to a single point, clusters
of bugs located at two antipodal points, or bugs entering a stable infinite
chase cycle where they never meet. We analyze the stability of these steady
states and calculate the probability that randomly initialized bugs reach each
state. For $N \leq 4$, we derive exact analytical expressions for these
probabilities. For larger values, we employ Monte Carlo simulations to estimate
the probability of coalescing, finding it approximately follows an inverse
square root relationship with the number of bugs. This generalization reveals
rich dynamical behaviors that are absent in the classic problem. Our analysis
provides insight into how restricting the bugs to the circle's perimeter
fundamentally alters the long-term behavior of pursuing agents compared to
unrestricted pursuit problems.

</details>


<div id='math.CV'></div>

# math.CV [[Back]](#toc)

### [43] [An Iterative Approach to the Complex Monge-Ampère Eigenvalue Problem](https://arxiv.org/abs/2507.13273)
*Ahmed Zeriahi*

Main category: math.CV

TL;DR: An iterative method for solving the Dirichlet complex Monge-Ampère eigenvalue problem on bounded strictly pseudoconvex domains in ℂⁿ, inspired by a real Monge-Ampère approach.


<details>
  <summary>Details</summary>
Motivation: To extend the iterative solution method from real to complex Monge-Ampère eigenvalue problems.

Method: Iterative approach inspired by Abedin and Kitagawa's work on real Monge-Ampère operators.

Result: Proposes a method to approximate solutions for the complex Monge-Ampère eigenvalue problem.

Conclusion: The approach bridges real and complex Monge-Ampère problems, offering a potential solution framework.

Abstract: We present an iterative approach to approximate the solution to the Dirichlet
complex Monge-Amp\`ere eigenvalue problem on a bounded strictly pseudoconvex
domain in $\C^n$. This approach is inspired by a similar approach initiated by
F. Abedin, J. Kitagawa who considered the real Monge-Amp\`ere operator on a
strictly convex domain in $\R^N$.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [44] [Unraveling Self-Similar Energy Transfer Dynamics: a Case Study for 1D Burgers System](https://arxiv.org/abs/2507.12764)
*Pritpal Matharu,Bartosz Protas,Tsuyoshi Yoneda*

Main category: physics.flu-dyn

TL;DR: The paper explores self-similar energy cascades in turbulence using the 1D viscous Burgers equation, solved via PDE-constrained optimization. Two solution families (viscous and inertial) are identified, with inertial solutions requiring small viscosity for self-similarity.


<details>
  <summary>Details</summary>
Motivation: To systematically find self-similar flow evolutions consistent with Kolmogorov's turbulence theory, starting with a simpler 1D model.

Method: PDE-constrained optimization with adjoint-based gradient methods to solve the nonconvex problem, focusing on parameters like time window, viscosity, and Fourier space interactions.

Result: Identified viscous (decaying enstrophy) and inertial (growing enstrophy) solutions; inertial solutions achieve self-similarity via wave front steepening under small viscosity.

Conclusion: The approach is a promising proof of concept for extending to 3D turbulence studies.

Abstract: In this work we consider the problem of constructing flow evolutions leading
to a self-similar energy cascade consistent with Kolmogorov's statistical
theory of turbulence. As a first step in this direction, we focus on the
one-dimensional viscous Burgers equation as a toy model. Its solutions
exhibiting self-similar behavior, in a precisely-defined sense, are found by
framing this problems in terms of PDE-constrained optimization. The main
physical parameters are the time window over which self-similar behavior is
sought (equal to approximately one eddy turnover time), viscosity (inversely
proportional to the ``Reynolds number") and an integer parameter characterizing
the distance in the Fourier space over which self-similar interactions occur.
Local solutions to this nonconvex PDE optimization problems are obtained with a
state-of-the-art adjoint-based gradient method. Two distinct families of
solutions, termed viscous and inertial, are identified and are distinguished
primarily by the behavior of enstrophy which, respectively, uniformly decays
and grows in the two cases. The physically meaningful and appropriately
self-similar inertial solutions are found only when a sufficiently small
viscosity is considered. These flows achieve the self-similar behaviour by a
uniform steepening of the wave fronts present in the solutions. The methodology
proposed and the results obtained represent an encouraging proof of concept for
this approach to be employed to systematically search for self-similar flow
evolutions in the context of three-dimensional turbulence.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [45] [Evidence for an Inverse Cascade of Magnetic Helicity in the Inner Heliosphere](https://arxiv.org/abs/2507.13213)
*Masatomi Iizawa,Yasuhito Narita,Tommaso Alberti,Stuart D. Bale,Axel Brandenburg,Abraham C. -L. Chian,Horia Comişel,Shuichi Matsukiyo,Nobumitsu Yokoi*

Main category: astro-ph.SR

TL;DR: Analysis of Parker Solar Probe data confirms a persistent inverse cascade in solar wind turbulence from the Sun to Mercury, challenging traditional views and revealing a radial sign change in magnetic helicity density.


<details>
  <summary>Details</summary>
Motivation: To understand the direction and behavior of solar wind turbulence in the inner heliosphere, particularly the cascade of magnetic helicity density.

Method: Analyzed magnetic helicity density spectra from Parker Solar Probe data across over 500 heliocentric distances.

Result: Confirmed an inverse cascade extending to Mercury's orbit and identified a radial sign change in spectral magnetic helicity density.

Conclusion: The findings challenge conventional hypotheses and provide new insights into solar wind turbulence evolution.

Abstract: To elucidate the cascade direction of the solar wind turbulence, we analyzed
magnetic helicity density spectra from the Parker Solar Probe data across more
than 500 heliocentric distances. For the first time, we confirmed a persistent
inverse cascade extending from the Sun to Mercury's orbital vicinity. This
finding challenges the conventional hypothesis that the magnetic helicity
density within the inner heliosphere is random. Furthermore, our analysis
revealed a radial sign change of the spectral magnetic helicity density at a
frequency whose value decreases logarithmically with distance. These results
provide new insights into the evolution of solar wind turbulence in the inner
heliosphere.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [46] [Search for Z/2 eigenfunctions on the sphere using machine learning](https://arxiv.org/abs/2507.13122)
*Andriy Haydys,Willem Adriaan Salm*

Main category: math.DG

TL;DR: Machine learning is used to find Z/2 eigenfunctions on the 2-sphere using a multivalued neural network implemented in JAX. Branch points were fixed at tetrahedron and cube vertices, and AI positioned them at a squashed tetrahedron.


<details>
  <summary>Details</summary>
Motivation: To explore and identify Z/2 eigenfunctions on the 2-sphere using machine learning techniques.

Method: A multivalued feedforward deep neural network was created and implemented using the JAX library. Branch points were fixed or allowed to move.

Result: Z/2 eigenfunctions were found for three cases: fixed branch points at tetrahedron and cube vertices, and AI-positioned points at a squashed tetrahedron.

Conclusion: Machine learning successfully identified Z/2 eigenfunctions, demonstrating the potential of AI in solving geometric problems.

Abstract: We use machine learning to search for examples of Z/2 eigenfunctions on the
2-sphere. For this we created a multivalued version of a feedforward deep
neural network, and we implemented it using the JAX library. We found Z/2
eigenfunctions for three cases: In the first two cases we fixed the branch
points at the vertices of a tetrahedron and at a cube respectively. In a third
case, we allowed the AI to move the branch points around and, in the end, it
positioned the branch points at the vertices of a squashed tetrahedron.

</details>


### [47] [Integral gradient estimates on a closed surface](https://arxiv.org/abs/2507.12790)
*Yuxiang Li,Rongze Sun*

Main category: math.DG

TL;DR: The paper establishes $L^p$ gradient estimates for weak solutions to a PDE on a Riemann surface, independent of the metric choice, especially near the moduli space boundary.


<details>
  <summary>Details</summary>
Motivation: To derive metric-independent gradient estimates for solutions to a PDE on a Riemann surface, particularly as the complex structure approaches the moduli space boundary.

Method: Transform the metric $g' = e^{2u} g$ into a bounded integral curvature metric, use a quadratic area bound condition, and derive gradient estimates in local conformal coordinates.

Result: Gradient estimates for $g'$ are obtained, leading to the desired $L^p$ estimates for the gradient of $u$.

Conclusion: The method successfully provides metric-independent $L^p$ gradient estimates for weak solutions, applicable near the moduli space boundary.

Abstract: Let $(\Sigma, g)$ be a closed Riemann surface, and let $u$ be a weak solution
to equation \[ - \Delta_g u = \mu, \] where $\mu$ is a signed Radon measure. We
aim to establish $L^p$ estimates for the gradient of $u$ that are independent
of the choice of the metric $g$. This is particularly relevant when the complex
structure approaches the boundary of the moduli space. To this end, we consider
the metric $g' = e^{2u} g$ as a metric of bounded integral curvature. This
metric satisfies a so-called quadratic area bound condition, which allows us to
derive gradient estimates for $g'$ in local conformal coordinates. From these
estimates, we obtain the desired estimates for the gradient of $u$.

</details>


### [48] [On the Nature of Stationary Integral Varifolds near Multiplicity 2 Planes](https://arxiv.org/abs/2507.13148)
*Spencer Becker-Kahn,Paul Minter,Neshan Wickramasekera*

Main category: math.DG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study stationary integral $n$-varifolds $V$ in the unit ball
$B_1(0)\subset\mathbb{R}^{n+k}$. Allard's regularity theorem establishes the
existence of $\epsilon = \epsilon(n,k)\in (0,1)$ for which if $V$ is
$\epsilon$-close (as varifolds) to the plane $P_0 = \{0\}^k\times\mathbb{R}^n$
with multiplicity 1 then, in $B_{1/2}(0)$, $V$ is represented by a single
$C^{1,\alpha}$ minimal graph. However, when instead $P_0$ occurs with
multiplicity $Q\in \{2,3,\dotsc\}$, simple examples show that this conclusion,
now as a multi-valued graph, may fail, even if $V$ corresponds to an
area-minimising rectifiable current.
  In the present work we investigate the structure of such $V$ which are close
to planes with multiplicity $Q>1$, focusing primarily on the case $Q=2$. We
show that an $\epsilon$-regularity theorem holds when $V$ is close, as a
varifold, to $P_0$ with multiplicity $2$, provided $V$ satisfies a certain
topological structural condition on the part of its support where the density
of $V$ is $<2$. The conclusion then is that, in $B_{1/2}(0)$, $V$ is
represented by the graph of a Lipschitz $2$-valued function over $P_0$ with
small Lipschitz constant; in fact, the function is $C^{1,\alpha}$ in a precise
generalised sense, and satisfies estimates, implying that all tangent cones at
singular points in $B_{1/2}(0)$ are unique and comprised of stationary unions
of $4$ half-planes (which may form a union of two distinct planes or a single
multiplicity $2$ plane). The theorem does not require any additional assumption
on the part of $V$ with density $\geq 2$ (which a priori may be a relatively
large set in $\mathcal{H}^n$-measure with high topological complexity).
  As a corollary, we show that our $\epsilon$-regularity theorem applies
unconditionally to stationary $2$-valued Lipschitz graphs with arbitrary
Lipschitz constant, yielding improved regularity and uniform a priori
estimates.

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [49] [Three-dimensional Dynamics of Strongly Magnetized Ion-Electron Relativistic Reconnection](https://arxiv.org/abs/2507.12509)
*Fabio Bacchini,Gregory R. Werner,Camille Granier,Jesse Vos*

Main category: astro-ph.HE

TL;DR: 3D simulations of semirelativistic collisionless magnetic reconnection reveal that 3D effects, like drift-kink and flux-rope kink dynamics, reduce magnetic-energy dissipation and nonthermal particle acceleration efficiency at high magnetizations, contrary to expectations.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of magnetic reconnection in semirelativistic regimes, especially in astrophysical contexts like black holes and neutron stars.

Method: Employed 3D simulations with realistic proton-to-electron mass ratios and varied upstream ion magnetization across two orders of magnitude.

Result: At high magnetizations, 3D effects dominate, reducing the efficiency of magnetic-energy dissipation and nonthermal particle acceleration.

Conclusion: The findings challenge general expectations for 3D relativistic reconnection and are significant for astrophysical scenarios.

Abstract: We present 3D simulations of semirelativistic collisionless magnetic
reconnection, where upstream ions are subrelativistic while electrons are
ultrarelativistic. We employ the realistic proton-to-electron mass ratio and
explore a range of upstream ion magnetization spanning two orders of magnitude,
with our highest-magnetization run achieving unprecedentedly large domain
sizes. Through a parameter scan, we find that as the system transitions from
mildly to trans- and ultrarelativistic regimes the qualitative behavior of
reconnection becomes strongly influenced by 3D effects mediated by drift-kink
and flux-rope kink dynamics. As a result, magnetic-energy dissipation at high
magnetizations, and the subsequent nonthermal particle acceleration, can become
less efficient, contrary to general expectations for 3D relativistic
reconnection. Our results have important implications for understanding
reconnection in magnetized astrophysical scenarios, such as the surroundings of
black holes and neutron stars.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [50] [Inverse Physics-informed neural networks procedure for detecting noise in open quantum systems](https://arxiv.org/abs/2507.12552)
*Gubio G. de Lima,Iann Cunha,Leonardo Kleber Castelano*

Main category: quant-ph

TL;DR: The paper introduces PINNverse, a machine learning framework for identifying Hamiltonian parameters and decay rates in open quantum systems, demonstrating its effectiveness with two-qubit simulations.


<details>
  <summary>Details</summary>
Motivation: Accurate quantum system characterization is crucial for NISQ technologies, but traditional methods are inefficient. Machine learning offers a scalable alternative.

Method: Extends the PINNverse framework to open quantum systems, incorporating coherent and dissipative dynamics in neural network training for parameter identification.

Result: Numerical simulations show PINNverse is effective and robust for quantum system identification, even with noisy data.

Conclusion: PINNverse provides a scalable, noise-resilient solution for quantum system identification, with potential uses in quantum control and error mitigation.

Abstract: Accurate characterization of quantum systems is essential for the development
of quantum technologies, particularly in the noisy intermediate-scale quantum
(NISQ) era. While traditional methods for Hamiltonian learning and noise
characterization often require extensive measurements and scale poorly with
system size, machine learning approaches offer promising alternatives. In this
work, we extend the inverse physics-informed neural network (referred to as
PINNverse) framework to open quantum systems governed by Lindblad master
equations. By incorporating both coherent and dissipative dynamics into the
neural network training, our method enables simultaneous identification of
Hamiltonian parameters and decay rates from noisy experimental data. We
demonstrate the effectiveness and robustness of the approach through numerical
simulations of two-qubit open systems. Our results show that PINNverse provides
a scalable and noise-resilient framework for quantum system identification,
with potential applications in quantum control and error mitigation.

</details>


### [51] [State transfer analysis for linear spin chains with non-uniform on-site energies](https://arxiv.org/abs/2507.13261)
*Chad C. Nelmes,Irene D'Amico,Timothy P. Spiller*

Main category: quant-ph

TL;DR: Analysis of perfect and quasi-perfect state transfer in linear spin chains with non-uniform on-site energies, focusing on coupling uniformity and its implications for quantum information processing.


<details>
  <summary>Details</summary>
Motivation: To maintain coupling uniformity in distributed quantum information processing, which is beneficial for certain physical implementations.

Method: Study of linear spin chains with non-uniform on-site energies, relating coupling uniformity to a particle in a discrete potential analogue. Statistical analysis of variations in couplings and on-site energies with increasing chain site number N.

Result: Findings on perfect and quasi-perfect state transfer, with insights into the impact of coupling uniformity and statistical variations.

Conclusion: The study provides valuable insights into achieving high fidelity state transfer in quantum systems, emphasizing the role of coupling uniformity and non-uniform on-site energies.

Abstract: High fidelity state transfer is an important ingredient of distributed
quantum information processing. We present and analyse results on perfect and
quasi-perfect state transfer with linear spin chains incorporating non-uniform
on-site energies. The motivation is maintenance of coupling uniformity, which
could be beneficial for some physical implementations. We relate this coupling
uniformity to a particle in a discrete potential analogue. Our analysis further
considers the statistical variation in couplings and on-site energies, as a
function of increasing chain site number N.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [52] [On the factorization of matrices into products of positive definite ones](https://arxiv.org/abs/2507.12560)
*Mahmoud Abdelgalil,Tryphon T. Georgiou*

Main category: math.OC

TL;DR: A new constructive approach for Ballantine-type matrix factorizations using optimal mass transport theory.


<details>
  <summary>Details</summary>
Motivation: To address the strong controllability problem in linear systems via state feedback and highlight the role of irrotational motions in realizing rotations.

Method: Constructive approach based on optimal mass transport theory, linking rotations of Gaussian distributions to optimal transport maps.

Result: Provides a method to factorize square matrices with positive determinants into positive definite factors.

Conclusion: The work advances Ballantine's result, offering a practical solution for control problems and demonstrating the utility of optimal transport theory.

Abstract: The present work revisits and provides a new approach on a result by Charles
Ballantine, on the factorization of a square matrix with positive determinant
into a product of positive definite factors. {\em Ballantine-type}
factorizations, that bound the number of positive definite factors, proved
central in solving a basic, yet elusive control problem--the strong
controllability of a linear system via control in the form of state feedback.
Ballantine's result transcends control engineering, and highlights the little
appreciated fact that rotations can be realized by the successive application
of irrotational motions. Our approach is constructive and is based on the
theory of optimal mass transport, specifically, it relates successive rotations
of Gaussian distributions to corresponding optimal transport maps that
constitute the sought factors.

</details>


### [53] [Tensor-Tensor Products, Group Representations, and Semidefinite Programming](https://arxiv.org/abs/2507.12729)
*Alex Dunbar,Elizabeth Newman*

Main category: math.OC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The $\star_M$-family of tensor-tensor products is a framework which
generalizes many properties from linear algebra to third order tensors. Here,
we investigate positive semidefiniteness and semidefinite programming under the
$\star_M$-product. Critical to our investigation is a connection between the
choice of matrix M in the $\star_M$-product and the representation theory of an
underlying group action. Using this framework, third order tensors equipped
with the $\star_M$-product are a natural setting for the study of invariant
semidefinite programs. As applications of the M-SDP framework, we provide a
characterization of certain nonnegative quadratic forms and solve low-rank
tensor completion problems.

</details>


### [54] [Unsupervised Ground Metric Learning](https://arxiv.org/abs/2507.13094)
*Janis Auffenberg,Jonas Bresch,Oleh Melnyk,Gabriele Steidl*

Main category: math.OC

TL;DR: The paper addresses unsupervised metric learning for data classification without labeled samples, focusing on optimal transport (OT) cost matrices and alternative distances like Mahalanobis-like and graph Laplacians. It proposes a stochastic random function iteration algorithm with proven linear convergence and explores simpler linear algebra methods for certain cases.


<details>
  <summary>Details</summary>
Motivation: The challenge of classifying data without labeled samples and the need for effective distance metrics in unsupervised learning motivate this work.

Method: The paper examines algorithms for learning OT cost matrices, proposes a stochastic random function iteration algorithm, and explores alternative distances (Mahalanobis-like, graph Laplacians).

Result: The proposed algorithm converges linearly for OT-based metric learning, and simpler linear algebra methods are viable for graph Laplacian-based approaches.

Conclusion: The work advances unsupervised metric learning by providing efficient algorithms and flexible distance metrics, broadening applicability beyond OT.

Abstract: Data classification without access to labeled samples remains a challenging
problem. It usually depends on an appropriately chosen distance between
features, a topic addressed in metric learning. Recently, Huizing, Cantini and
Peyr\'e proposed to simultaneously learn optimal transport (OT) cost matrices
between samples and features of the dataset. This leads to the task of finding
positive eigenvectors of a certain nonlinear function that maps cost matrices
to OT distances. Having this basic idea in mind, we consider both the
algorithmic and the modeling part of unsupervised metric learning. First, we
examine appropriate algorithms and their convergence. In particular, we propose
to use the stochastic random function iteration algorithm and prove that it
converges linearly for our setting, although our operators are not
paracontractive as it was required for convergence so far. Second, we ask the
natural question if the OT distance can be replaced by other distances. We show
how Mahalanobis-like distances fit into our considerations. Further, we examine
an approach via graph Laplacians. In contrast to the previous settings, we have
just to deal with linear functions in the wanted matrices here, so that simple
algorithms from linear algebra can be applied.

</details>


### [55] [Optimal regularity up to the boundary for Plateau-quasi-minimizers](https://arxiv.org/abs/2507.13189)
*Eve Machefert*

Main category: math.OC

TL;DR: The paper analyzes the regularity of quasi-minimal sets with boundary conditions, proving optimal regularity via bi-John domains and Ahlfors regular boundaries.


<details>
  <summary>Details</summary>
Motivation: To understand the regularity properties of quasi-minimal sets, particularly in the context of Plateau's problem in co-dimension one.

Method: Investigates Ahlfors regularity and uniform rectifiability of quasi-minimal sets, including boundary analysis.

Result: Establishes optimal regularity for Plateau-quasi-minimizers, characterized by bi-John domains with Ahlfors regular boundaries.

Conclusion: The study provides a comprehensive regularity characterization for quasi-minimal sets with boundary conditions.

Abstract: We study the regularity of quasi-minimal sets (in the sense of David and
Semmes) with a boundary condition, which can be interpreted as quasi-minimizers
of Plateau's problem in co-dimension one. For these Plateau-quasi-minimizers,
we establish the optimal regularity, which is a characterization by bi-John
domains with Ahlfors regular boundaries. This requires to investigate the
Ahlfors regularity and also the uniform rectifiability of those sets, up to the
boundary.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [56] [Improving physics-informed neural network extrapolation via transfer learning and adaptive activation functions](https://arxiv.org/abs/2507.12659)
*Athanasios Papastathopoulos-Katsaros,Alexandra Stavrianidi,Zhandong Liu*

Main category: cs.LG

TL;DR: A transfer learning method and adaptive activation function improve PINNs' extrapolation performance, reducing errors by 40-50% without added computational cost.


<details>
  <summary>Details</summary>
Motivation: PINNs struggle with extrapolation and sensitivity to activation functions, limiting their broader application.

Method: Proposes transfer learning in an extended domain and an adaptive activation function combining standard AFs.

Result: Achieves 40% reduction in relative L2 error and 50% reduction in mean absolute error in extrapolation.

Conclusion: The method enhances PINNs' robustness and accuracy for extrapolation tasks efficiently.

Abstract: Physics-Informed Neural Networks (PINNs) are deep learning models that
incorporate the governing physical laws of a system into the learning process,
making them well-suited for solving complex scientific and engineering
problems. Recently, PINNs have gained widespread attention as a powerful
framework for combining physical principles with data-driven modeling to
improve prediction accuracy. Despite their successes, however, PINNs often
exhibit poor extrapolation performance outside the training domain and are
highly sensitive to the choice of activation functions (AFs). In this paper, we
introduce a transfer learning (TL) method to improve the extrapolation
capability of PINNs. Our approach applies transfer learning (TL) within an
extended training domain, using only a small number of carefully selected
collocation points. Additionally, we propose an adaptive AF that takes the form
of a linear combination of standard AFs, which improves both the robustness and
accuracy of the model. Through a series of experiments, we demonstrate that our
method achieves an average of 40% reduction in relative L2 error and an average
of 50% reduction in mean absolute error in the extrapolation domain, all
without a significant increase in computational cost. The code is available at
https://github.com/LiuzLab/PINN-extrapolation .

</details>


### [57] [RONOM: Reduced-Order Neural Operator Modeling](https://arxiv.org/abs/2507.12814)
*Sven Dummer,Dongwei Ye,Christoph Brune*

Main category: cs.LG

TL;DR: The paper introduces RONOM, a framework combining reduced-order modeling (ROM) and neural operators, to address computational challenges in solving time-dependent PDEs. It provides error bounds and demonstrates superior performance in generalization and robustness.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between ROM's fixed discretization limitations and neural operators' lack of error quantification, enabling flexible and accurate PDE solutions.

Method: Introduces the RONOM framework, blending ROM and neural operators, with theoretical error bounds and numerical validation.

Result: RONOM matches neural operators in generalization and excels in spatial super-resolution and robustness, with insights into temporal super-resolution.

Conclusion: RONOM effectively combines ROM and operator learning, offering a robust, flexible, and theoretically grounded approach for PDE solutions.

Abstract: Time-dependent partial differential equations are ubiquitous in physics-based
modeling, but they remain computationally intensive in many-query scenarios,
such as real-time forecasting, optimal control, and uncertainty quantification.
Reduced-order modeling (ROM) addresses these challenges by constructing a
low-dimensional surrogate model but relies on a fixed discretization, which
limits flexibility across varying meshes during evaluation. Operator learning
approaches, such as neural operators, offer an alternative by parameterizing
mappings between infinite-dimensional function spaces, enabling adaptation to
data across different resolutions. Whereas ROM provides rigorous numerical
error estimates, neural operator learning largely focuses on discretization
convergence and invariance without quantifying the error between the
infinite-dimensional and the discretized operators. This work introduces the
reduced-order neural operator modeling (RONOM) framework, which bridges
concepts from ROM and operator learning. We establish a discretization error
bound analogous to those in ROM, and get insights into RONOM's discretization
convergence and discretization robustness. Moreover, two numerical examples are
presented that compare RONOM to existing neural operators for solving partial
differential equations. The results demonstrate that RONOM using standard
vector-to-vector neural networks achieves comparable performance in input
generalization and superior performance in both spatial super-resolution and
discretization robustness, while also offering novel insights into temporal
super-resolution scenarios.

</details>
