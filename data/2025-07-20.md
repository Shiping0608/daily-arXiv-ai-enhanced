<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 12]
- [math.AP](#math.AP) [Total: 19]
- [physics.comp-ph](#physics.comp-ph) [Total: 4]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 3]
- [cs.LG](#cs.LG) [Total: 2]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [math.DS](#math.DS) [Total: 1]
- [math.DG](#math.DG) [Total: 3]
- [math.CV](#math.CV) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 2]
- [astro-ph.HE](#astro-ph.HE) [Total: 1]
- [astro-ph.SR](#astro-ph.SR) [Total: 1]
- [math.OC](#math.OC) [Total: 4]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Keep the beat going: Automatic drum transcription with momentum](https://arxiv.org/abs/2507.12596)
*Alisha L. Foster,Robert J. Webber*

Main category: math.NA

TL;DR: This paper compares two optimization methods (multiplicative update rule vs projected gradient descent with momentum) for automatic drum transcription using nonnegative matrix factorization, finding that projected gradient descent with momentum achieves better accuracy and convergence guarantees.


<details>
  <summary>Details</summary>
Motivation: To develop a simple and interpretable approach for automatic drum transcription by comparing different optimization methods for nonnegative matrix factorization, addressing the need for effective drum track analysis in recorded music.

Method: The paper uses partially fixed nonnegative matrix factorization to factor magnitude spectrograms of musical recordings, implementing and comparing two optimization approaches: multiplicative update rule and projected gradient descent with momentum.

Result: Projected gradient descent with momentum demonstrated higher empirical accuracy compared to multiplicative update rule when tested on the ENST-Drums dataset and original recordings, while also providing better theoretical convergence guarantees within fixed runtime constraints.

Conclusion: Projected gradient descent with momentum is the superior optimization method for drum transcription tasks using nonnegative matrix factorization, offering both better practical performance and stronger theoretical foundations for convergence.

Abstract: A simple, interpretable way to perform automatic drum transcription is by
factoring the magnitude spectrogram of a recorded musical piece using a
partially fixed nonnegative matrix factorization. There are two natural ways to
optimize the nonnegative matrix factorization, including a multiplicative
update rule and projected gradient descent with momentum. The methods differ in
their empirical accuracies and theoretical convergence guarantees. This paper
summarizes the methods and their time complexities, and it applies the methods
to the ENST-Drums data set and an original recording from the author's band,
evaluating the empirical accuracy with respect to ground-truth drum
annotations. The results indicate that projected gradient descent with momentum
leads to higher accuracy for a fixed runtime, and it satisfies stronger
convergence guarantees.

</details>


### [2] [A Unified Framework for Efficient Kernel and Polynomial Interpolation](https://arxiv.org/abs/2507.12629)
*M. Belianovich,G. E. Fasshauer,A. Narayan,V. Shankar*

Main category: math.NA

TL;DR: A unified interpolation scheme combining compactly-supported kernels and polynomials, with efficient numerical methods for computation and application to manifolds.


<details>
  <summary>Details</summary>
Motivation: To generalize interpolation with compactly-supported kernels and polynomial least squares, enabling broader applications.

Method: Combines compactly-supported kernels and polynomials, using specialized numerical linear algebra for efficiency and extending to manifolds.

Result: The unified interpolant outperforms polynomial least squares in numerical experiments on Euclidean domains and manifolds.

Conclusion: The framework offers a superior, efficient interpolation method applicable to diverse domains, including manifolds.

Abstract: We present a unified interpolation scheme that combines compactly-supported
positive-definite kernels and multivariate polynomials. This unified framework
generalizes interpolation with compactly-supported kernels and also classical
polynomial least squares approximation. To facilitate the efficient use of this
unified interpolation scheme, we present specialized numerical linear algebra
procedures that leverage standard matrix factorizations. These procedures allow
for efficient computation and storage of the unified interpolant. We also
present a modification to the numerical linear algebra that allows us to
generalize the application of the unified framework to target functions on
manifolds with and without boundary. Our numerical experiments on both
Euclidean domains and manifolds indicate that the unified interpolant is
superior to polynomial least

</details>


### [3] [Partitioned Conservative, Variable Step, Second-Order Method for Magneto-hydrodynamics In Elsässer Variables](https://arxiv.org/abs/2507.12700)
*Zhen Yao,Catalin Trenchea,Wenlong Pei*

Main category: math.NA

TL;DR: A symplectic, second-order algorithm for MHD in Elsässer variables is proposed, reducing computational cost via parallel subproblem solving, with proven linear convergence and conservation properties.


<details>
  <summary>Details</summary>
Motivation: To efficiently solve the evolutionary MHD system while conserving key physical quantities like energy and helicity.

Method: Partitioning the system into smaller subproblems solved in parallel, with a symplectic, second-order algorithm and adaptive time stepping.

Result: Linear convergence under time step restrictions, conservation of energy and helicity, and second-order accuracy in L² and H¹ norms.

Conclusion: The algorithm balances accuracy and efficiency, validated by numerical tests.

Abstract: Magnetohydrodynamics (MHD) describes the interaction between electrically
conducting fluids and electromagnetic fields. We propose and analyze a
symplectic, second-order algorithm for the evolutionary MHD system in
Els\"asser variables. We reduce the computational cost of the iterative
non-linear solver, at each time step, by partitioning the coupled system into
two subproblems of half size, solved in parallel. We prove that the iterations
converge linearly, under a time step restriction similar to the one required in
the full space-time error analysis. The variable step algorithm unconditionally
conserves the energy, cross-helicity and magnetic helicity, and numerical
solutions are second-order accurate in the $L^{2}$ and $H^{1}$-norms. The time
adaptive mechanism, based on a local truncation error criterion, helps the
variable step algorithm balance accuracy and time efficiency. Several numerical
tests support the theoretical findings and verify the advantage of time
adaptivity.

</details>


### [4] [DPNO: A Dual Path Architecture For Neural Operator](https://arxiv.org/abs/2507.12719)
*Yichen Wang,Wenlian Lu*

Main category: math.NA

TL;DR: A dual-path architecture enhances neural operators for PDEs, improving performance by 30% over basic models and showing versatility across paradigms.


<details>
  <summary>Details</summary>
Motivation: Single operator blocks are limited, and traditional stacking methods are inefficient. A more effective architecture is needed.

Method: Proposes a dual-path architecture inspired by ResNet and DenseNet, organizing operator blocks in parallel for better feature extraction.

Result: Achieves 30% improvement on standard PDE test cases and works well with DeepONet and FNO.

Conclusion: The dual-path architecture is versatile and promising for neural operator design.

Abstract: Neural operators have emerged as a powerful tool for solving partial
differential equations (PDEs) and other complex scientific computing tasks.
However, the performance of single operator block is often limited, thus often
requiring composition of basic operator blocks to achieve better per-formance.
The traditional way of composition is staking those blocks like feedforward
neural networks, which may not be very economic considering
parameter-efficiency tradeoff. In this pa-per, we propose a novel dual path
architecture that significantly enhances the capabilities of basic neural
operators. The basic operator block is organized in parallel two paths which
are similar with ResNet and DenseNet. By introducing this parallel processing
mechanism, our architecture shows a more powerful feature extraction and
solution approximation ability compared with the original model. We demonstrate
the effectiveness of our approach through extensive numerical experi-ments on a
variety of PDE problems, including the Burgers' equation, Darcy Flow Equation
and the 2d Navier-Stokes equation. The experimental results indicate that on
certain standard test cas-es, our model achieves a relative improvement of over
30% compared to the basic model. We also apply this structure on two standard
neural operators (DeepONet and FNO) selected from different paradigms, which
suggests that the proposed architecture has excellent versatility and offering
a promising direction for neural operator structure design.

</details>


### [5] [Quasi-optimality of the Crouzeix-Raviart FEM for p-Laplace-type problems](https://arxiv.org/abs/2507.12742)
*Johannes Storn*

Main category: math.NA

TL;DR: The paper proves quasi-optimality of the Crouzeix-Raviart FEM for nonlinear $p$-Laplace problems, showing bounded error relative to best-approximation and data oscillation. It also introduces a localized a priori error estimate for the conforming Lagrange FEM.


<details>
  <summary>Details</summary>
Motivation: To establish quasi-optimality for the Crouzeix-Raviart FEM in nonlinear $p$-Laplace problems and provide improved error estimates.

Method: Analyzes the error of the Crouzeix-Raviart FEM using a quasi-norm, comparing it to the best-approximation error and data oscillation. Also derives a localized a priori error estimate for the conforming Lagrange FEM.

Result: The error of the Crouzeix-Raviart FEM is bounded by a constant times the best-approximation error plus data oscillation. A novel localized error estimate for the conforming Lagrange FEM is also verified.

Conclusion: The Crouzeix-Raviart FEM is quasi-optimal for $p$-Laplace problems, and the work provides additional insights into error estimation for FEM methods.

Abstract: We verify quasi-optimality of the Crouzeix-Raviart FEM for nonlinear problems
of $p$-Laplace type. More precisely, we show that the error of the
Crouzeix-Raviart FEM with respect to a quasi-norm is bounded from above by a
uniformly bounded constant times the best-approximation error plus a data
oscillation term. As a byproduct, we verify a novel more localized a priori
error estimate for the conforming lowest-order Lagrange FEM.

</details>


### [6] [Analysis of Langevin midpoint methods using an anticipative Girsanov theorem](https://arxiv.org/abs/2507.12791)
*Matthew S. Zhang*

Main category: math.NA

TL;DR: A new method for analyzing midpoint discretizations of SDEs in MCMC is introduced, improving regularity and cross-regularity results and providing a query complexity bound for sampling accuracy.


<details>
  <summary>Details</summary>
Motivation: To enhance the analysis of midpoint discretizations in MCMC methods for sampling from target measures, addressing limitations in current literature.

Method: Uses Malliavin calculus to estimate Radon-Nikodym derivatives for non-adapted processes on $L^2([0, T); \mathbb{R}^d)$, applied to midpoint discretizations.

Result: Improved regularity and cross-regularity results, with a query complexity bound of $\widetilde{O}(\frac{\kappa^{5/4} d^{1/4}}{\varepsilon^{1/2}})$ for $\varepsilon^2$-accurate samples under log-concavity and smoothness assumptions.

Conclusion: The method advances the theoretical understanding and practical efficiency of midpoint discretizations in MCMC sampling.

Abstract: We introduce a new method for analyzing midpoint discretizations of
stochastic differential equations (SDEs), which are frequently used in Markov
chain Monte Carlo (MCMC) methods for sampling from a target measure $\pi
\propto \exp(-V)$. Borrowing techniques from Malliavin calculus, we compute
estimates for the Radon-Nikodym derivative for processes on $L^2([0, T);
\mathbb{R}^d)$ which may anticipate the Brownian motion, in the sense that they
may not be adapted to the filtration at the same time. Applying these to
various popular midpoint discretizations, we are able to improve the regularity
and cross-regularity results in the literature on sampling methods. We also
obtain a query complexity bound of $\widetilde{O}(\frac{\kappa^{5/4}
d^{1/4}}{\varepsilon^{1/2}})$ for obtaining a $\varepsilon^2$-accurate sample
in $\mathsf{KL}$ divergence, under log-concavity and strong smoothness
assumptions for $\nabla^2 V$.

</details>


### [7] [Adaptive feature capture method for solving partial differential equations with low regularity solutions](https://arxiv.org/abs/2507.12941)
*Yangtao Deng,Qiaolin He,Xiaoping Wang*

Main category: math.NA

TL;DR: The paper introduces the Adaptive Feature Capture Method (AFCM), a machine learning framework that adaptively redistributes neurons and collocation points in high-gradient regions to improve accuracy for PDEs with low-regularity solutions.


<details>
  <summary>Details</summary>
Motivation: Traditional numerical methods struggle with PDEs in complex geometries, and existing deep-learning approaches lack adaptive resolution for solutions with steep gradients or singularities.

Method: AFCM uses gradient norms to guide adaptive redistribution of neurons and collocation points, inspired by adaptive moving mesh techniques.

Result: Numerical experiments show AFCM effectively resolves low-regularity problems in complex geometries without increasing computational overhead.

Conclusion: AFCM bridges adaptive mesh refinement and randomized neural networks, offering a robust and scalable solution for challenging PDEs.

Abstract: Partial differential equations (PDEs) with low-regularity solutions pose
significant challenges for traditional numerical methods, particularly in
complex geometries where mesh generation and adaptive refinement become
computationally expensive. While deep-learning-based approaches, such as
Physics-Informed Neural Networks (PINNs) and the Random Feature Method (RFM),
offer mesh-free alternatives, they often lack adaptive resolution in critical
regions, limiting their accuracy for solutions with steep gradients or
singularities. In this work, we propose the Adaptive Feature Capture Method
(AFCM), a novel machine learning framework that adaptively redistributes
neurons and collocation points in high-gradient regions to enhance local
expressive power. Inspired by adaptive moving mesh techniques, AFCM employs the
gradient norm of an approximate solution as a monitor function to guide the
reinitialization of feature function parameters. This ensures that partition
hyperplanes and collocation points cluster where they are most needed,
achieving higher resolution without increasing computational overhead. The AFCM
extends the capabilities of RFM to handle PDEs with near-singular solutions
while preserving its mesh-free efficiency. Numerical experiments demonstrate
the method's effectiveness in accurately resolving low-regularity problems,
even in complex geometries. By bridging the gap between adaptive mesh
refinement and randomized neural networks, AFCM offers a robust and scalable
approach for solving challenging PDEs in scientific and engineering
applications.

</details>


### [8] [High Performance Parallel Solvers for the time-harmonic Maxwell Equations](https://arxiv.org/abs/2507.13066)
*Elise Fressart,Sébastien Dubois,Loïc Gouarin,Marc Massot,Michel Nowak,Nicole Spillane*

Main category: math.NA

TL;DR: Comparison of four preconditioners for solving large-scale time-harmonic Maxwell equations, favoring Hiptmair-Xu and Block Low-Rank methods.


<details>
  <summary>Details</summary>
Motivation: The challenge of solving time-harmonic Maxwell equations due to their non-Hermitian and non-semi-definite nature.

Method: Evaluating four preconditioners (sparse approximate inverse, Restricted Additive Schwarz, Hiptmair-Xu, Block Low-Rank) and comparing them to LU factorization.

Result: Preliminary results favor Hiptmair-Xu and Block Low-Rank preconditioners.

Conclusion: Hiptmair-Xu and Block Low-Rank show promise for solving time-harmonic Maxwell equations.

Abstract: We consider the numerical solution of large scale time-harmonic Maxwell
equations. To this day, this problem remains difficult, in particular because
the equations are neither Hermitian nor semi-definite. Our approach is to
compare different strategies for solving this set of equations with
preconditioners that are available either in PETSc, MUMPS, or in hypre. Four
different preconditioners are considered. The first is the sparse approximate
inverse, which is often applied to electromagnetic problems. The second is
Restricted Additive Schwarz, a domain decomposition preconditioner. The third
is the Hiptmair-Xu preconditioner which is tailored to the positive Maxwell
equations, a nearby problem. The final preconditioner is MUMPS's Block Low-Rank
method, a compressed block procedure. We also compare the performance of this
method to the standard LU factorization technique, which is a direct solver.
Performance with respect to the mesh size, the number of CPU cores, the
wavelength and the physical size of the domain are considered. This work in
progress yields temporary conclusions in favour of the Hiptmair-Xu and the
Block Low-Rank preconditioners.

</details>


### [9] [Stability of lattice Boltzmann schemes for initial boundary value problems in raw formulation](https://arxiv.org/abs/2507.13108)
*Thomas Bellotti*

Main category: math.NA

TL;DR: The paper analyzes the stability of 1D linear scalar lattice Boltzmann schemes for hyperbolic equations with boundary data, introducing strong stability notions and testing three representative schemes with various boundary conditions.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of stability analysis for lattice Boltzmann schemes with boundary data without transforming them into scalar formulations, which is difficult near boundaries.

Method: The study uses the original raw algorithm on multiple unknowns and introduces strong stability notions. It focuses on schemes with characteristic equations of breadth one to the left, testing three representative schemes with boundary conditions.

Result: Theoretical analysis and numerical simulations demonstrate the stability-instability of the schemes under various boundary conditions.

Conclusion: The paper provides insights into the stability of lattice Boltzmann schemes with boundary data, supported by numerical evidence, without requiring scalar transformations.

Abstract: We study the stability of one-dimensional linear scalar lattice Boltzmann
schemes for hyperbolic equations with respect to boundary data. Our approach is
based on the original raw algorithm on several unknowns, thereby avoiding the
need for a transformation into an equivalent scalar formulation-a challenging
process in presence of boundaries. To address different behaviors exhibited by
the numerical scheme, we introduce appropriate notions of strong stability.
They account for the potential absence of a continuous extension of the stable
vector bundle associated with the bulk scheme on the unit circle for certain
components. Rather than developing a general theory, complicated by the fact
that discrete boundaries in lattice Boltzmann schemes are inherently
characteristic, we focus on strong stability-instability for methods whose
characteristic equations have stencils of breadth one to the left. In this
context, we study three representative schemes. These are endowed with various
boundary conditions drawn from the literature, and our theoretical results are
supported by numerical simulations.

</details>


### [10] [Generalized Scattering Matrix Framework for Modeling Implantable Antennas in Multilayered Spherical Media](https://arxiv.org/abs/2507.13119)
*Chenbo Shi,Xin Gu,Shichen Liang,Jin Pan*

Main category: math.NA

TL;DR: A unified framework for analyzing antennas in spherically stratified media, combining free-space GSM with extended SSOs for efficient modeling.


<details>
  <summary>Details</summary>
Motivation: Addressing the need for efficient analysis of antennas in complex spherical environments, such as biomedical implants or radome-enclosed systems, without computational overhead.

Method: Decouples antenna and medium modeling using GSM and SSOs, enabling rapid reevaluation under material variations without re-simulation.

Result: Demonstrates accuracy and scalability through case studies, matching full-wave and DGF-based solutions.

Conclusion: The framework offers computational efficiency, generality, and ease of adoption for spherical media analysis.

Abstract: This paper presents a unified and efficient framework for analyzing antennas
embedded in spherically stratified media -- a model broadly applicable to
implantable antennas in biomedical systems and radome-enclosed antennas in
engineering applications. The proposed method decouples the modeling of the
antenna and its surrounding medium by combining the antenna's free-space
generalized scattering matrix (GSM) with a set of extended spherical scattering
operators (SSOs) that rigorously capture the electromagnetic interactions with
multilayered spherical environments. This decoupling enables rapid reevaluation
under arbitrary material variations without re-simulating the antenna, offering
substantial computational advantages over traditional dyadic Green's function
(DGF)-based MoM approaches. The framework supports a wide range of spherical
media, including radially inhomogeneous and uniaxially anisotropic layers.
Extensive case studies demonstrate excellent agreement with full-wave and
DGF-based solutions, confirming the method's accuracy, generality, and
scalability. Code implementations are provided to facilitate adoption and
future development.

</details>


### [11] [On the efficiency of a posteriori error estimators for parabolic partial differential equations in the energy norm](https://arxiv.org/abs/2507.13188)
*Iain Smears*

Main category: math.NA

TL;DR: The paper analyzes the efficiency of a posteriori error estimators for the heat equation, showing dependency on the choice of norm and numerical solution representation.


<details>
  <summary>Details</summary>
Motivation: To understand how the efficiency of error estimators is influenced by the choice of norm and the definition of the numerical solution.

Method: Uses an implicit Euler method for time discretization and a conforming finite element method in space, comparing error estimators for different numerical solution representations.

Result: Demonstrates that the efficiency of error estimators depends on both the norm and the numerical solution's definition.

Conclusion: The choice of norm and numerical solution representation significantly impacts the efficiency of a posteriori error estimators.

Abstract: For the model problem of the heat equation discretized by an implicit Euler
method in time and a conforming finite element method in space, we prove the
efficiency of a posteriori error estimators with respect to the energy norm of
the error, when considering the numerical solution as the average between the
usual continuous piecewise affine-in-time and piecewise constant-in-time
reconstructions. This illustrates how the efficiency of the estimators is not
only possibly dependent on the choice of norm, but also on the choice of notion
of numerical solution.

</details>


### [12] [Well-balanced path-conservative discontinuous Galerkin methods with equilibrium preserving space for shallow water linearized moment equations](https://arxiv.org/abs/2507.13284)
*Ruilin Fan,Julian Koellermeier,Yinhua Xia,Yan Xu,Jiahui Zhang*

Main category: math.NA

TL;DR: High-order, well-balanced DG methods for SWLME preserve still and moving water equilibria, addressing non-conservative terms and complex steady states.


<details>
  <summary>Details</summary>
Motivation: To accurately model vertical momentum transfer and complex velocity profiles in shallow water systems while preserving equilibrium states.

Method: Path-conservative DG schemes based on DLM theory, reformulating equations for still water and extending DG for moving water.

Result: Exact equilibrium preservation and high-order accuracy in scenarios with vertical velocity variations and complex topographies.

Conclusion: The proposed methods effectively balance flux gradients, non-conservative terms, and source terms, achieving robust numerical performance.

Abstract: This paper presents high-order, well-balanced, path-conservative
discontinuous Galerkin (DG) methods for the shallow water linearized moment
equations (SWLME), designed to preserve both still and moving water equilibrium
states. Unlike the multi-layer shallow water equations, which model vertical
velocity variations using multiple distinct layers, the SWLME employs a
polynomial expansion of velocity profiles with up to $N$ moments. This approach
enables a more detailed representation of vertical momentum transfer and
complex velocity profiles while retaining hyperbolicity. However, the presence
of non-conservative terms and complex steady-state structures introduces
significant numerical challenges. Addressing these challenges, we develop
path-conservative DG schemes grounded in the Dal Maso-LeFloch-Murat (DLM)
theory for non-conservative products. Our method balances flux gradients,
non-conservative terms, and source terms through equilibrium-preserving spaces.
For the still water equilibrium, we reformulate the equations into a
quasilinear form that eliminates source terms, inherently preserving steady
states. For the moving water equilibrium, we extend the DG method by
transforming conservative variables into equilibrium variables and employing
linear segment paths. Theoretical analysis and numerical experiments
demonstrate that the proposed methods achieve exact equilibrium preservation
while maintaining high-order accuracy, even in scenarios with vertical velocity
variations and complex topographies.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [13] [Uniform dispersive estimates for the semi-classical Hartree equation with long-range interaction](https://arxiv.org/abs/2507.12577)
*Sonae Hadama*

Main category: math.AP

TL;DR: The paper analyzes the Hartree equation in 3D with long-range interaction, showing optimal decay of the density function for small-data solutions, uniformly in the semi-classical parameter ℏ. It also provides a new proof for modified scattering in a related nonlinear Schrödinger equation.


<details>
  <summary>Details</summary>
Motivation: To extend existing results on the Hartree equation by ensuring uniformity with respect to the semi-classical parameter ℏ and to provide a new proof for modified scattering in a related equation.

Method: The proof involves boundedness of modified wave operators, an L¹–L∞ dispersive estimate for the modified propagator, and commutator estimates for density operators, combined via a bootstrap argument.

Result: The density function decays optimally for small-data solutions, with conditions and bounds independent of ℏ.

Conclusion: The paper achieves uniformity in ℏ and offers a novel proof for modified scattering, leveraging key analytical tools.

Abstract: In this paper, we consider the Hartree equation with smooth but long-range
interaction in the semi-classical regime, in three-dimensional space. We show
that the density function of small-data solution decays at the optimal rate.
When the semi-classical parameter $\hbar \in (0,1]$ is fixed, our result is
essentially covered by the recent work by Nguyen and You [arXiv:2408.15860];
however, the novelty of this paper is the uniformity with respect to $\hbar$.
Namely, both smallness condition for initial data and bounds for the solution
are independent of $\hbar$. Moreover, the argument in this paper provides a new
proof of the modified scattering for the long-range nonlinear Schr\"{o}dinger
equation with a Hartree type nonlinearity. Our proof relies on three main
ingredients. First, we prove the boundedness of finite-time wave operators
modified by phase corrections. Second, we show an $L^1$--$L^\infty$ dispersive
estimate for the modified propagator. Third, we give various kinds of
commutator estimates for density operators. By combining them, we can apply the
usual bootstrap argument to obtain the main result.

</details>


### [14] [Boundary Feedback and Observer Synthesis for a Class of Nonlinear Parabolic--Elliptic PDE Systems](https://arxiv.org/abs/2507.12615)
*Kamal Fenza,Moussa Labbadi,Mohamed Ouzahra*

Main category: math.AP

TL;DR: The paper studies stabilization of a coupled parabolic-elliptic PDE system with nonlinear terms, using backstepping for boundary control and observer design.


<details>
  <summary>Details</summary>
Motivation: To address the stabilization challenge for coupled nonlinear PDE systems, ensuring stability and well-posedness.

Method: Rigorous backstepping design for explicit boundary control and exponentially convergent observers from partial measurements.

Result: Theorems prove exponential stability and well-posedness of the closed-loop system.

Conclusion: The proposed method effectively stabilizes the coupled nonlinear PDE system with guaranteed stability.

Abstract: This paper investigates the stabilization of a coupled system comprising a
parabolic PDE and an elliptic PDE with nonlinear terms. A rigorous backstepping
design provides an explicit boundary control law and exponentially convergent
observers from partial boundary measurements. Several theorems ensure
exponential stability and well-posedness of the nonlinear closed-loop system.

</details>


### [15] [Semi-classical limit of quantum scattering states for the nonlinear Hartree equation](https://arxiv.org/abs/2507.12627)
*Sonae Hadama,Younghun Hong*

Main category: math.AP

TL;DR: The paper analyzes quantum particle dynamics in the semi-classical regime, showing dispersion bounds and scattering for small-data solutions of the nonlinear Hartree equation. It also proves convergence of quantum states to classical states in the limit and establishes small-data scattering for the Vlasov equation.


<details>
  <summary>Details</summary>
Motivation: To understand the long-time behavior of quantum particles in the semi-classical regime and bridge the gap between quantum and classical dynamics.

Method: Uses uniform dispersion estimates for the free Schrödinger flow and analyzes the nonlinear Hartree equation and Vlasov equation.

Result: Small-data solutions scatter, and quantum states converge to classical states in the semi-classical limit. Small-data scattering for the Vlasov equation is established.

Conclusion: The study successfully links quantum and classical dynamics, providing insights into scattering behavior and the semi-classical limit.

Abstract: This article concerns the long-time dynamics of quantum particles in the
semi-classical regime. First, we show that for the nonlinear Hartree equation
with short-range interaction potential, small-data solutions obey dispersion
bounds and they scatter, where the smallness conditions and the bounds are
independent of the small parameter $\hbar\in(0,1]$ representing the reduced
Planck constant. Then, taking the semi-classical limit $\hbar\to0$, we prove
that the Wigner transforms of such quantum scattering states converge weakly-*
to the corresponding classical scattering states for the Vlasov equation. As a
direct consequence, we establish small-data scattering for the Vlasov equation
without assuming regularity on initial data. Our analysis is based on a new
uniform dispersion estimate for the free Schr\"odinger flow, which is simple
but crucial to include singular interaction potentials such as inverse
power-law potential $\frac{1}{|x|^a}$ with $1<a<\frac{5}{3}$.

</details>


### [16] [Asymptotically sharp stability of Sobolev inequalities on the Heisenberg group with dimension-dependent constants](https://arxiv.org/abs/2507.12725)
*Lu Chen,Guozhen Lu,Hanli Tang,Bohan Wang*

Main category: math.AP

TL;DR: The paper establishes optimal stability bounds for Sobolev and Hardy-Littlewood-Sobolev (HLS) inequalities on the Heisenberg group using CR Yamabe flow, avoiding rearrangement techniques.


<details>
  <summary>Details</summary>
Motivation: To address the lack of rearrangement inequality in the CR setting, which complicates deriving optimal stability bounds for Sobolev and HLS inequalities.

Method: Uses bispherical harmonics and orthogonality for local stability on the CR sphere, then applies CR Yamabe flow for global stability on the Heisenberg group.

Result: Achieves optimal stability for Sobolev and HLS inequalities with dimension-dependent constants.

Conclusion: The rearrangement-free approach is versatile and applicable to fractional inequalities once suitable flows are established.

Abstract: In this paper, we are concerned with the optimal asymptotic lower bound for
the stability of Sobolev inequality on the Heisenberg group. We first establish
the optimal local stability of Sobolev inequality on the CR sphere through
bispherical harmonics and complicated orthogonality technique ( see Lemma 3.1).
The loss of rearrangement inequality in the CR setting makes it impossible to
use any rearrangement flow technique (either differential rearrangement flow or
integral rearrangement flow) to derive the optimal stability of Sobolev
inequality on the CR sphere from corresponding optimal local stability. To
circumvent this, we will use the CR Yamabe flow to establish the optimal
stability of Sobolev inequality on the Heisenberg group with the
dimension-dependent constants (see Theorem 1.1). As an application, we also
establish the optimal stability of the Hardy-Littlewood-Sobolev (HLS)
inequality for special conformal index with the dimension-dependent constants
(see Theorem 1.3). Our approach is rearrangement-free and can be used to study
the optimal stability problem for fractional Sobolev inequality or HLS
inequality on the Heisenberg group once the corresponding continuous flow is
established.

</details>


### [17] [Analysis of a parabolic-hyperbolic hybrid population model: an integrated semigroup approach](https://arxiv.org/abs/2507.12833)
*Qihua Huang,Minglong Wang,Yixiang Wu*

Main category: math.AP

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper is concerned with the global dynamics of a hybrid
parabolic-hyperbolic model describing populations with distinct dispersal and
sedentary stages. We first establish the global well-posedness of solutions,
prove a comparison principle, and demonstrate the asymptotic smoothness of the
solution semiflow. Through the spectral analysis of the linearized system, we
derive and characterize the net reproductive rate $\mathcal{R}_{0}$.
Furthermore, an explicit relationship between $\mathcal{R}_{0}$ and the
principal eigenvalue of the linearized system is analyzed. Under appropriate
monotonicity assumptions, we show that $\mathcal{R}_{0}$ serves as a threshold
parameter that completely determines the stability of steady states of the
system. More precisely, when $\mathcal{R}_{0}<1$, the trivial equilibrium is
globally asymptotical stable, while when $\mathcal{R}_{0}>1$, the system is
uniformly persistent and there is a positive equilibrium which is unique and
globally asymptotical stable.

</details>


### [18] [Polyharmonic Nonlinear Scalar Field Equations](https://arxiv.org/abs/2507.12962)
*Alessandro Cannone,Silvia Cingolani,Jarosław Mederski*

Main category: math.AP

TL;DR: Existence of ground state solutions for polyharmonic nonlinear equations with subcritical growth, overcoming analytical challenges and introducing a new polyharmonic logarithmic Sobolev inequality.


<details>
  <summary>Details</summary>
Motivation: Inspired by Berestycki and Lions, the study addresses the challenges posed by higher-order operators in polyharmonic equations.

Method: Analyzes the polyharmonic nonlinear equation $(-\Delta)^m u=g(u)$ with subcritical growth assumptions.

Result: Demonstrates existence of ground state solutions and establishes a new polyharmonic logarithmic Sobolev inequality.

Conclusion: Successfully addresses analytical challenges and extends results to higher-order polyharmonic equations.

Abstract: In this paper, we present a result on the existence of ground state solutions
for the polyharmonic nonlinear equation $(-\Delta)^m u=g(u)$, assuming that $g$
has a general subcritical growth at infinity, inspired by Berestycki and Lions
\cite{BerestyckiLions}. In comparison with the biharmonic case studied in
\cite{Med-Siem}, the presence of a higher-order operator gives rise to several
analytical challenges, which are overcome in the present work. Furthermore, we
establish a new polyharmonic logarithmic Sobolev inequality.

</details>


### [19] [Symmetrization on the sphere and applications](https://arxiv.org/abs/2507.13027)
*Satyanad Kichenassamy*

Main category: math.AP

TL;DR: A new symmetrization method for mappings on the n-sphere is introduced, applied to estimate solutions of p-Laplacian type quasilinear elliptic PDEs with Dirac measures. The case p=n is solved via a conformal transformation, while other cases are briefly discussed.


<details>
  <summary>Details</summary>
Motivation: To develop a symmetrization technique for solving quasilinear elliptic PDEs with Dirac measures, particularly focusing on the p-Laplacian type.

Method: Introduces a symmetrization method for mappings on the n-sphere, uses conformal transformation for p=n, and references other papers for 1<p<n and p>n.

Result: The method provides estimates for solutions of the PDEs, with detailed results for p=n and references for other cases.

Conclusion: The symmetrization method is effective for solving p-Laplacian type PDEs, with p=n being the primary focus and other cases addressed elsewhere.

Abstract: We introduce a new method of symmetrization of mappings on the $n$-sphere
($n\geq 2$). They are applied to estimate solutions of quasilinear elliptic
partial differential equations of $p$-Laplacian type, with combinations of
Dirac measures on the right-hand side. The case $p=n$ is reduced to a problem
on the sphere, using a conformal transformation. The cases when $1 < p < n $
and $p > $n are considered more briefly, full details being available in other
papers of the author.

</details>


### [20] [Exponential convergence for ultrafast diffusion equations with log-concave weights](https://arxiv.org/abs/2507.13060)
*Max Fathi,Mikaela Iacobelli*

Main category: math.AP

TL;DR: The paper analyzes the asymptotic behavior of a weighted ultrafast diffusion PDE on the real line, proving exponential convergence to equilibrium, extending results beyond compact settings.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the gradient flow approach to the problem of measure quantization.

Method: The authors study a weighted ultrafast diffusion PDE with log-concave and log-Lipschitz weights.

Result: They prove exponential convergence to equilibrium for the PDE.

Conclusion: The work extends previous results to non-compact settings, providing new insights into the behavior of such PDEs.

Abstract: We study the asymptotic behavior of a weighted ultrafast diffusion PDE on the
real line, with a log-concave and log-lipschitz weight, and prove exponential
convergence to equilibrium. This result goes beyond the compact setting studied
in [22]. This equation is motivated by the gradient flow approach to the
problem of quantization of measures introduced in [11].

</details>


### [21] [Nonlinear smoothing implies improved lower bounds on the radius of spatial analyticity for nonlinear dispersive equations](https://arxiv.org/abs/2507.13083)
*Mikaela Baldasso,Simão Correia*

Main category: math.AP

TL;DR: Improved lower bounds for decay rate of uniform radius of analyticity in nonlinear dispersive equations.


<details>
  <summary>Details</summary>
Motivation: To enhance understanding of decay rates in nonlinear dispersive equations, focusing on analyticity.

Method: Derived nonlinear smoothing estimates with specific derivative distribution, applied to defocusing generalized KdV and nonlinear Schrödinger equations.

Result: Achieved lower bound σ(T)≳T^(-1/2-ϵ) for any ϵ>0, surpassing existing literature.

Conclusion: The method effectively improves decay rate bounds, advancing current knowledge in the field.

Abstract: We provide a roadmap to establish improved lower bounds on the decay rate of
the uniform radius of analyticity $\sigma(T)$ for a given nonlinear dispersive
equation, reducing the problem to the derivation of nonlinear smoothing
estimates with a specific distribution of extra derivatives. We apply this
strategy for both the defocusing generalized KdV and the nonlinear
Schr\"odinger equations with odd pure-power nonlinearity. For both equations,
we reach the lower bound $\sigma(T)\gtrsim T^{-\frac{1}{2}-\epsilon}$, for any
$\epsilon>0$, thus improving all available results in the current literature.

</details>


### [22] [A New Framework for Unidimensional Structures Based on Generalised Continua](https://arxiv.org/abs/2507.13098)
*Mewen Crespo,Casale Guy,Loïc Le Marrec,Patrizio Neff*

Main category: math.AP

TL;DR: A family of beam models is derived from a 3D higher-order elasticity framework, incorporating three kinematic fields to explore holonomic, semi-holonomic, and non-holonomic regimes, capturing material defects in beams.


<details>
  <summary>Details</summary>
Motivation: To systematically explore varying kinematic constraints in beam models, from classical elasticity to a fully relaxed model, and unify dislocations and disclinations.

Method: Incorporates three kinematic fields (u, P, N) to derive models for holonomic, semi-holonomic, and non-holonomic regimes, with simplified ODE systems for practical cases.

Result: The framework hierarchically captures material defects, with holonomic and semi-holonomic models emerging as limits of the non-holonomic model.

Conclusion: The proposed framework provides a unified and hierarchical approach to modeling beam-like structures with material defects.

Abstract: The present work introduces a family of beam models derived from a
three-dimensional higher-order elasticity framework. By incorporating three
kinematic fields - the macroscopic displacement u, the micro-distortion tensor
P, and the third-order tensor N - the study systematically explores three
regimes: holonomic, semi-holonomic, and non-holonomic. These regimes correspond
to varying levels of kinematic constraints, ranging from classical elasticity
to a fully relaxed model. The holonomic case reduces to a higher-order
Euler--Bernoulli beam model, while the semi-holonomic case generalises the
Timoshenko beam model. The non-holonomic case provides a unified framework that
naturally incorporates both dislocations and disclinations. Furthermore, the
holonomic and semi-holonomic models are shown to emerge as singular limits of
the non-holonomic model by increasing specific penalty coefficients. Simplified
ordinary differential equation systems are derived for specific cases, such as
pure traction and bending, illustrating the practical applicability of the
models. The results highlight the hierarchical structure of the proposed
framework and its ability to capture material defects in beam-like structures.

</details>


### [23] [Normalized solutions of coupled Sobolev critical Schrodinger equations with mass subcritical couplings](https://arxiv.org/abs/2507.13163)
*Zhang Jianjun,Zhong Xuexiu,Zhou Jinfang*

Main category: math.AP

TL;DR: The paper studies positive solutions to coupled Sobolev critical Schrödinger equations with mass constraints, focusing on the mass mixed case. It shows existence of two solutions and their asymptotic behavior as ν→0.


<details>
  <summary>Details</summary>
Motivation: To address an open problem by Bartsch et al. regarding the existence and properties of solutions in the mass mixed case for coupled Schrödinger equations.

Method: Analysis of the system under mass constraints, using variational methods to identify local minimizers and mountain pass solutions.

Result: For small ν>0, two positive solutions exist: a local minimizer and a mountain pass solution. Asymptotic behavior as ν→0 is also analyzed.

Conclusion: The results affirmatively answer the open problem, demonstrating the existence and properties of solutions in the specified case.

Abstract: We are concerned with qualitative properties of positive solutions to the
following coupled Sobolev critical Schr\"odinger equations $$ \begin{cases}
-\Delta u+\lambda_1 u=\mu_1|u|^{2^*-2}u+\nu\alpha |u|^{\alpha-2}|v|^{\beta}u
~\hbox{in}~ \R^N,\\ -\Delta v+\lambda_2 v=\mu_2|v|^{2^*-2}v+\nu\beta
|u|^{\alpha}|v|^{\beta-2}v ~\hbox{in}~ \R^N \end{cases} $$ subject to the mass
constraints $\int_{\mathbb{R}^N}|u|^2 \ud x=a^2$ and $\int_{\mathbb{R}^N}|v|^2
\ud x=b^2$, where, $a>0,\,b>0,\,N=3,4$ and $2^*:=\frac{2N}{N-2}$ is the Sobolev
critical exponent. The main purpose of this paper is focused on the mass mixed
case, i. e., $ \alpha>1,\beta>1,\alpha+\beta<2+\frac{4}{N}$. For some suitable
small $\nu>0$, we show that the above system admits two positive solutions, one
of which is a local minimizer, and another one is a mountain pass solution.
Moreover, as $\nu\to0^+$, asymptotic behaviors of solutions are also
considered. Our result gives an affirmative answer to a Soave's type open
problem raised by Bartsch {\it et al.} (Calc. Var. Partial Differential
Equations 62(1), Paper No. 9, 34, 2023).

</details>


### [24] [Robin Green Function Estimates and a Model of Mammalian Lungs](https://arxiv.org/abs/2507.13168)
*Guy David,Stefano Decio,Max Engelstein,Marcel Filoche,Svitlana Mayboroda,Marco Michetti*

Main category: math.AP

TL;DR: The paper analyzes the Green function under Robin boundary conditions, revealing transitions between Dirichlet-like and Neumann-like behaviors, with implications for harmonic measure bounds and lung efficiency.


<details>
  <summary>Details</summary>
Motivation: To understand the transition between Dirichlet-like and Neumann-like behaviors in the Green function and its impact on harmonic measure and flow dynamics, inspired by mammalian lung efficiency.

Method: Establishes properties of the Green function with Robin boundary conditions, focusing on transitions and quantifiable bounds.

Result: Sharp bounds on harmonic measure are derived, and a phase transition in total flow behavior is proven, aligning with earlier physics conjectures.

Conclusion: The study clarifies the Green function's behavior under Robin conditions, validating conjectures about flow dynamics and its relevance to mammalian lung efficiency.

Abstract: The present paper establishes delicate properties of the Green function with
Robin boundary conditions, in particular, elucidating the nature of the passage
between the Dirichlet-like and Neumann-like behavior. This yields sharp
quantifiable bounds on the corresponding harmonic measure and proves the phase
transition in the behavior of the total flow earlier conjectured in physics
literature in concert with the efficacy of mammalian lungs.

</details>


### [25] [Multiple normalized solutions for two coupled Gross-Pitaevskii equations with attractive interactions and mass constriants](https://arxiv.org/abs/2507.13172)
*Zhang Jianjun,Zhong Xuexiu,Zhou Jinfang*

Main category: math.AP

TL;DR: The paper studies a system of coupled Gross-Pitaevskii equations modeling two-component Bose-Einstein condensates, focusing on solutions under mass constraints and attractive interactions.


<details>
  <summary>Details</summary>
Motivation: To analyze the existence of solutions for the system, particularly in the context of Bose-Einstein condensates, where understanding such solutions is crucial for physical applications.

Method: Variational methods are employed to seek critical points of the associated functional under mass constraints, with a focus on the mass mixed case.

Result: Two positive solutions are found for suitable parameters, including a local minimizer and a mountain pass solution.

Conclusion: The study confirms the existence of multiple solutions under specific conditions, contributing to the understanding of coupled systems in Bose-Einstein condensates.

Abstract: We are concerned with the following system of two coupled time-independent
Gross-Pitaevskii equations $$ \begin{cases} -\Delta u+\lambda_1
u=\mu_1|u|^{p-2}u+\nu\alpha |u|^{\alpha-2}|v|^{\beta}u ~\hbox{in}~ \R^N,\\
-\Delta v+\lambda_2 v=\mu_2|v|^{q-2}v+\nu\beta |u|^{\alpha}|v|^{\beta-2}v
~\hbox{in}~ \R^N, \end{cases} $$ which arises in two-components Bose-Einstein
condensates and involve attractive Sobolev subcritical or critical
interactions, i. e., $\nu>0$ and $\alpha+\beta\leq 2^*$. This system is
employed by seeking critical points of the associated variational functional
with the constrained mass below $$\int_{\mathbb{R}^N}|u|^2 {\rm d}x=a, \quad
\int_{\mathbb{R}^N}|v|^2 {\rm d}x=b.$$ In the mass mixed case, i. e.,
$2<p<2+\frac{4}{N}<q<2^*$, for some suitable $a,b,\nu$ and $\beta$, the system
above admits two positive solutions. In particular, in the case
$\alpha+\beta<2^*$, using variational methods on the $L^2$-ball, two positive
solutions are obtained, one of which is a local minimizer and the second one is
a mountain pass solution.

</details>


### [26] [Pointwise convergence to initial data of heat and Poisson equations in Modulation Spaces](https://arxiv.org/abs/2507.13220)
*Divyang G. Bhimani,Rupak K. Dalai*

Main category: math.AP

TL;DR: The paper analyzes weighted modulation spaces to study pointwise convergence of heat and Poisson semigroups to initial data, and explores the Hardy-Littlewood maximal operator's behavior on these spaces.


<details>
  <summary>Details</summary>
Motivation: To understand the conditions under which heat and Poisson semigroups converge pointwise to initial data in weighted modulation spaces, and to investigate the role of the Hardy-Littlewood maximal operator in these spaces.

Method: The study uses the standard Laplacian and Hermite operator on the whole domain, analyzing the convergence of semigroups and the operation of the Hardy-Littlewood maximal operator.

Result: Pointwise convergence of heat and Poisson semigroups to initial data is characterized, and the Hardy-Littlewood maximal operator is shown to operate on certain modulation spaces.

Conclusion: The findings provide insights into the behavior of semigroups in weighted modulation spaces and highlight the independent significance of the Hardy-Littlewood maximal operator's operation.

Abstract: We characterize weighted modulation spaces (data space) for which the heat
semigroup $e^{-tL}f$ converges pointwise to the initial data $f$ as time $t$
tends to zero. Here $L$ stands for the standard Laplacian $-\Delta $ or Hermite
operator $H=-\Delta +|x|^2$ on the whole domain. Similar result also holds for
Poisson semigroup $e^{-t\sqrt{L}}f.$ We also prove that the Hardy-Littlewood
maximal operator operates on certain modulation spaces. This may be of
independent interest.

</details>


### [27] [Rigidity for the heat equation with density on Riemannian manifolds through a conformal change](https://arxiv.org/abs/2507.13230)
*Alexander Grigor'yan,Giulia Meglioli,Alberto Roncoroni*

Main category: math.AP

TL;DR: The paper explores conditions for uniqueness of solutions to the heat equation on non-compact weighted Riemannian manifolds, focusing on vanishing solutions in weighted Lebesgue spaces.


<details>
  <summary>Details</summary>
Motivation: To determine when solutions to the heat equation vanish identically under specific weighted conditions on infinite-volume manifolds.

Method: Uses a unified approach via conformal metric transformation to reduce the problem to a standard heat equation on a weighted manifold.

Result: Identifies sufficient conditions for solution uniqueness, with explicit counterexamples showing assumption optimality.

Conclusion: The method and results provide clarity on solution behavior under varying assumptions, validated by counterexamples.

Abstract: We investigate uniqueness of solution to the heat equation with a density
$\rho$ on complete, non-compact weighted Riemannian manifolds of infinite
volume. Our main goal is to identify sufficient conditions under which the
solution $u$ vanishes identically, assuming that $u$ belongs to a certain
weighted Lebesgue space with exponential or polynomial weight, $L^p_{\phi}$. We
distinguish between the cases $p > 1$ and $p = 1$ which required stronger
assumptions on the manifold and the density function $\rho$. We develop a
unified method based on a conformal transformation of the metric, which allows
us to reduce the problem to a standard heat equation on a suitably weighted
manifold. In addition, we construct explicit counterexamples on model manifolds
which demonstrate optimality of our assumptions on the density $\rho$.

</details>


### [28] [The Snapshot Problem for the Euler-Poisson-Darboux Equation](https://arxiv.org/abs/2507.13257)
*Fulton Gonzalez,Jue Wang,Jens Christensen,Tomoyuki Kakehi*

Main category: math.AP

TL;DR: The paper studies the generalized Euler-Poisson-Darboux (EPD) equation with a complex parameter, focusing on existence and uniqueness for a two-snapshot problem involving smooth functions. It connects solutions to Liouville-like numbers related to Bessel functions.


<details>
  <summary>Details</summary>
Motivation: To understand conditions for solutions to the generalized EPD equation given two snapshots (u(x,r)=f(x) and u(x,s)=g(x)), and explore connections to Liouville-like numbers.

Method: Analyzes the generalized EPD equation, derives conditions for existence and uniqueness of solutions for the two-snapshot problem, and investigates properties of related Liouville-like numbers.

Result: Identifies conditions for solutions and discovers Liouville-like numbers linked to Bessel functions, studying their properties.

Conclusion: The work provides insights into the generalized EPD equation and uncovers new connections between its solutions and Liouville-like numbers, enriching mathematical understanding.

Abstract: The generalized Euler-Poisson-Darboux (EPD) equation with complex parameter
$\alpha$ is given by $$ \Delta_x u=\frac{\partial^2 u}{\partial
t^2}+\frac{n-1+2\alpha}{t}\,\frac{\partial u}{\partial t}, $$ where $u(x,t)\in
\mathscr E(\mathbb R^n\times \mathbb R)$, with $u$ even in $t$. For $\alpha=0$
and $\alpha=1$ the solution $u(x,t)$ represents a mean value over spheres and
balls, respectively, of radius $|t|$ in $\mathbb R^n$. In this paper we
consider existence and uniqueness results for the following two-snapshot
problem: for fixed positive real numbers $r$ and $s$ and smooth functions $f$
and $g$ on $\mathbb R^n$, what are the conditions under which there is a
solution $u(x,t)$ to the generalized EPD equation such that $u(x,r)=f(x)$ and
$u(x,s)=g(x)$? The answer leads to a discovery of Liouville-like numbers
related to Bessel functions, and we also study the properties of such numbers.

</details>


### [29] [Homogenization of nonlocal exchange energies in micromagnetics](https://arxiv.org/abs/2507.13262)
*Rossella Giorgio,Leon Happ,Hidde Schönberger*

Main category: math.AP

TL;DR: The paper analyzes the homogenization of nonlocal micromagnetic functionals with symmetric and antisymmetric exchange contributions, constrained to the unit sphere. It identifies the Γ-limit of these energies, leading to an effective local functional via a nonlocal cell problem.


<details>
  <summary>Details</summary>
Motivation: To understand the asymptotic behavior of nonlocal micromagnetic energies under vanishing interaction range and heterogeneities, while respecting the unit sphere constraint.

Method: Uses a tailored two-scale convergence approach to capture oscillations in specific directions and describes the two-scale limit of nonlocal difference quotients.

Result: Derives an effective local functional through a tangentially constrained nonlocal cell problem, with microscopic oscillations constrained to the tangent space of the sphere.

Conclusion: The study successfully homogenizes nonlocal micromagnetic functionals, providing insights into their local behavior under constraints.

Abstract: We study the homogenization of nonlocal micromagnetic functionals
incorporating both symmetric and antisymmetric exchange contributions under the
physical constraint that the magnetization field takes values in the unit
sphere. Assuming that the nonlocal interaction range and the scale of
heterogeneities vanish simultaneously, we capture the asymptotic behavior of
the nonlocal energies by identifying their $\Gamma$-limit, leading to an
effective local functional expressed through a tangentially constrained
nonlocal cell problem. Our proof builds upon a tailored notion of two-scale
convergence, which takes into account oscillations only in specific directions.
It enables us to describe the two-scale limit of suitable nonlocal difference
quotients, yielding a nonlocal analog of the classical limit decomposition
result for gradient fields. To deal with the manifold constraint of the
magnetization, we additionally prove that the microscopic oscillations in the
two-scale limit are constrained to lie in the tangent space of the sphere.

</details>


### [30] [A hierarchy of blood vessel models, Part I: 3D-1D to 1D](https://arxiv.org/abs/2507.13316)
*Laurel Ohm,Sarah Strikwerda*

Main category: math.AP

TL;DR: The paper proposes and analyzes models for blood perfusion around a thin vessel, focusing on convergence between 3D-3D, 3D-1D, and 1D methods. Part I introduces a 3D-1D system, proves its well-posedness, and shows convergence to a 1D Green's function model as vessel radius shrinks. Part II extends this to convergence with a 3D-3D system.


<details>
  <summary>Details</summary>
Motivation: To rigorously establish convergence results among different modeling approaches (3D-3D, 3D-1D, 1D) for blood perfusion in tissues, ensuring accuracy and reliability across scales.

Method: Part I: Proposes a 3D-1D Darcy-Poiseuille system with specific boundary conditions, analyzes its well-posedness, and derives a 1D Green's function model. Part II: Uses 1D estimates to show convergence to a 3D-3D Darcy-Stokes system.

Result: The 3D-1D model is well-posed and converges to the 1D model with a rate proportional to ε^(1/2)|logε|. The 1D model's solution involves a novel integrodifferential equation. Part II shows convergence of 1D and 3D-1D models to 3D-3D.

Conclusion: The study successfully links hierarchical models for blood perfusion, proving convergence between 3D-3D, 3D-1D, and 1D methods, with implications for multi-scale modeling accuracy.

Abstract: We propose and analyze a family of models describing blood perfusion through
a tissue surrounding a thin blood vessel. Our goal is to rigorously establish
convergence results among 3D-3D Darcy--Stokes, 3D-1D Darcy--Poiseuille, and 1D
Green's function methods commonly used to model this process. In Part I, we
propose a 3D-1D Darcy--Poiseuille system where the coupling across the
permeable vessel surface involves an angle-averaged Neumann boundary condition
coupled with a geometrically constrained Robin boundary condition. We show that
this model is well-posed and moreover limits to a 1D Green's function model as
the maximum vessel radius $\epsilon\to 0$. In the 1D model, the exterior blood
pressure is given by an explicit Green's function expression involving the
interior blood pressure. The interior pressure satisfies a novel 1D
integrodifferential equation in which the integral term incorporates the
effects of the exterior pressure and the vessel geometry. Much of this paper is
devoted to analyzing this integrodifferential equation. Using the \emph{a
priori} bounds obtained here, we show that the solution to the 1D model
converges to the 3D-1D solution with a rate proportional to
$\epsilon^{1/2}|\log\epsilon|$. In Part II [Ohm \& Strikwerda, arXiv preprint
July 2025], we rely on the 1D estimates to show that both the 1D and 3D-1D
models converge to a coupled 3D-3D Darcy-Stokes system as $\epsilon\to 0$,
thereby establishing a convergence chain among all hierarchy levels.

</details>


### [31] [A hierarchy of blood vessel models, Part II: 3D-3D to 3D-1D and 1D](https://arxiv.org/abs/2507.13330)
*Laurel Ohm,Sarah Strikwerda*

Main category: math.AP

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We propose and analyze a hierarchy of three models of blood perfusion through
a tissue surrounding a thin arteriole or venule. Our goal is to rigorously link
3D-3D Darcy--Stokes, 3D-1D Darcy--Poiseuille, and 1D Green's function methods
commonly used to model this process. Here in Part II, we consider the most
detailed level, a 3D-3D Darcy-Stokes system coupled across the permeable vessel
surface by mass conservation and pressure/stress balance conditions. We derive
a convergence result between the 3D-3D model and both the 3D-1D
Darcy--Poiseuille model and 1D Green's function model proposed in Part I [Ohm
\& Strikwerda, arXiv preprint July 2025] at a rate proportional to
$\epsilon^{1/6}|\log\epsilon|$, where $\epsilon$ is the maximum vessel radius.
The rate is limited by the inclusion of a degenerate endpoint where the vessel
radius vanishes, i.e. becomes indistinguishable from a capillary. Key to our
proof are \emph{a priori} estimates for the 1D integrodifferential model
obtained in Part I.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [32] [Shape optimization of metastable states](https://arxiv.org/abs/2507.12575)
*Noé Blassel,Tony Lelièvre,Gabriel Stoltz*

Main category: physics.comp-ph

TL;DR: Proposes a method to define metastable states in molecular simulations using shape-optimization of a timescale metric, improving over conventional energy-based definitions.


<details>
  <summary>Details</summary>
Motivation: Standard definitions of metastable states based on energy minimization are often inadequate, especially when entropic effects or thermal fluctuations dominate.

Method: Uses shape-optimization of a local timescale metric, deriving analytic expressions for Dirichlet eigenvalues and constructing a local ascent algorithm. Also introduces tractability methods for high-dimensional systems.

Result: Validated on a biomolecular system, showing significant improvement over conventional metastable state definitions.

Conclusion: The proposed approach offers a more effective way to define metastable states, particularly in complex systems.

Abstract: The definition of metastable states is an ubiquitous task in the design and
analysis of molecular simulation, and is a crucial input in a variety of
acceleration methods for the sampling of long configurational trajectories.
  Although standard definitions based on local energy minimization procedures
can sometimes be used, these definitions are typically suboptimal, or entirely
inadequate when entropic effects are significant, or when the lowest energy
barriers are quickly overcome by thermal fluctuations.
  In this work, we propose an approach to the definition of metastable states,
based on the shape-optimization of a local separation of timescale metric
directly linked to the efficiency of a class of accelerated molecular dynamics
algorithms.
  To realize this approach, we derive analytic expressions for shape-variations
of Dirichlet eigenvalues for a class of operators associated with reversible
elliptic diffusions, and use them to construct a local ascent algorithm,
explicitly treating the case of multiple eigenvalues.
  We propose two methods to make our method tractable in high-dimensional
systems: one based on dynamical coarse-graining, the other on recently obtained
low-temperature shape-sensitive spectral asymptotics.
  We validate our method on a benchmark biomolecular system, showcasing a
significant improvement over conventional definitions of metastable states.

</details>


### [33] [TinyDEM: Minimal open granular DEM code with sliding, rolling and twisting friction](https://arxiv.org/abs/2507.12610)
*Roman Vetter*

Main category: physics.comp-ph

TL;DR: TinyDEM is a lightweight 3D DEM solver for simulating granular particles with inelastic, frictional collisions and complex rigid geometries, written in simple C++11 and parallelized with OpenMP.


<details>
  <summary>Details</summary>
Motivation: To provide a compact, standalone DEM solver for granular particle dynamics, accessible for beginners and extensible for advanced models.

Method: Explicitly solves Newton's damped equations for translations and rotations of polydisperse spherical particles using quaternions, with particle-mesh collision handling.

Result: A functional, parallelized DEM solver (TinyDEM) written in simple C++11, free under the BSD license.

Conclusion: TinyDEM is a practical tool for classical DEM simulations and a foundation for more complex particle dynamics models.

Abstract: This article introduces TinyDEM, a lightweight implementation of a
full-fledged discrete element method (DEM) solver in 3D. Newton's damped
equations of motion are solved explicitly for translations and rotations of a
polydisperse ensemble of dry, soft, granular spherical particles, using
quaternions to represent their orientation in space without gimbal lock.
Particle collisions are modeled as inelastic and frictional, including full
exchange of torque. With a general particle-mesh collision routine, complex
rigid geometries can be simulated. TinyDEM is designed to be a compact
standalone program written in simple C++11, devoid of explicit pointer
arithmetics and advanced concepts such as manual memory management or
polymorphism. It is parallelized with OpenMP and published freely under the
3-clause BSD license. TinyDEM can serve as an entry point into classical DEM
simulations or as a foundation for more complex models of particle dynamics.

</details>


### [34] [Equalized Hyperspin Machine](https://arxiv.org/abs/2507.12940)
*Marcello Calvanese Strinati,Claudio Conti*

Main category: physics.comp-ph

TL;DR: The paper introduces a method to equalize hyperspin amplitudes in a hyperspin machine, improving its performance as a spin Hamiltonian minimizer.


<details>
  <summary>Details</summary>
Motivation: Reliable simulation of spin models is crucial for solving complex optimization problems, but existing hyperspin machines lack a mechanism to enforce equal amplitudes, limiting their effectiveness.

Method: An additional network of oscillators (equalizers) with antisymmetric nonlinear coupling is introduced to equalize hyperspin amplitudes in the steady state.

Result: Large-scale simulations show the equalized hyperspin machine achieves significantly lower spin energy and reduced sensitivity to system parameters.

Conclusion: The equalized hyperspin machine enhances performance and enables integration with advanced annealing protocols for further improvements.

Abstract: The reliable simulation of spin models is of critical importance to tackle
complex optimization problems that are intractable on conventional computing
machines. The recently introduced hyperspin machine, which is a network of
linearly and nonlinearly coupled parametric oscillators, provides a versatile
simulator of general classical vector spin models in arbitrary dimension,
finding the minimum of the simulated spin Hamiltonian and implementing novel
annealing algorithms. In the hyperspin machine, oscillators evolve in time
minimizing a cost function that must resemble the desired spin Hamiltonian in
order for the system to reliably simulate the target spin model. This condition
is met if the hyperspin amplitudes are equal in the steady state. Currently, no
mechanism to enforce equal amplitudes exists. Here, we bridge this gap and
introduce a method to simulate the hyperspin machine with equalized amplitudes
in the steady state. We employ an additional network of oscillators (named
equalizers) that connect to the hyperspin machine via an antisymmetric
nonlinear coupling and equalize the hyperspin amplitudes. We demonstrate the
performance of such an equalized hyperspin machine by large-scale numerical
simulations up to $10000$ hyperspins. Compared to the hyperspin machine without
equalization, we find that the equalized hyperspin machine (i) Reaches orders
of magnitude lower spin energy, and (ii) Its performance is significantly less
sensitive to the system parameters. The equalized hyperspin machine offers a
competitive spin Hamiltonian minimizer and opens the possibility to combine
amplitude equalization with complex annealing protocols to further boost the
performance of spin machines.

</details>


### [35] [A variationally consistent and asymptotically convergent phase-field model for solute precipitation and dissolution](https://arxiv.org/abs/2507.13270)
*Andrea Lamperti,Laura De Lorenzis*

Main category: physics.comp-ph

TL;DR: A novel phase-field model for solute precipitation and dissolution is proposed, derived variationally from a free energy functional, and validated through numerical examples.


<details>
  <summary>Details</summary>
Motivation: To improve upon previous models by deriving governing equations in a variationally consistent way and ensuring convergence to sharp-interface models.

Method: Derives solute ion concentration and phase-field variable from a Modica-Mortola free energy functional, using Allen-Cahn and Cahn-Hilliard equations, and validates via matched asymptotic expansions and finite element discretization.

Result: The model converges to the sharp-interface model, and a novel reaction rate expression is derived. Numerical examples in 2D and 3D demonstrate its effectiveness.

Conclusion: The proposed model is robust and validated, offering a consistent framework for studying solute precipitation and dissolution.

Abstract: We propose a novel phase-field model for solute precipitation and dissolution
in liquid solutions. Unlike in previous studies with similar scope, in our
model the two non-linear coupled governing equations of the problem, which
deliver the solute ion concentration and the phase-field variable, are derived
in a variationally consistent way starting from a free energy functional of
Modica-Mortola type. The phase-field variable is assumed to follow the
non-conservative Allen-Cahn evolution law, whereas the solute ion concentration
obeys the conservative Cahn-Hilliard equation. We also assess the convergence
of the new model to the corresponding sharp-interface model via the method of
matched asymptotic expansions, and derive a novel expression of the reaction
rate of the sharp-interface model. Through a finite element discretization, we
present several numerical examples in two and three dimensions.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [36] [Augmented Lagrangian methods produce cutting-edge magnetic coils for stellarator fusion reactors](https://arxiv.org/abs/2507.12681)
*Pedro F. Gil,Alan A. Kaptanoglu,Eve V. Stenson*

Main category: physics.plasm-ph

TL;DR: An augmented Lagrangian approach improves stellarator coil optimization, outperforming previous methods and generating superior coils for various stellarators.


<details>
  <summary>Details</summary>
Motivation: Current methods for designing stellarator coils struggle with nonconvex optimization and resource-intensive parameter scans, leading to suboptimal results.

Method: The paper introduces an augmented Lagrangian approach to address the ill-posed problem of coil optimization.

Result: The method produces coils that outperform previous Pareto-optimal solutions and exhibit excellent physics and engineering properties, even for diverse stellarators like W7-X and HSX.

Conclusion: The approach is effective and versatile, demonstrating superior performance in generating optimized coils for multiple stellarator designs.

Abstract: Finding feasible coils for stellarator fusion devices is a crux of realizing
this concept for future power plants. The coils must reproduce the physics
properties of the target plasma while satisfying myriad engineering
constraints. Current efforts struggle to navigate the highly nonconvex
optimization landscape and result in suboptimal stellarator coils, and/or they
end up spending considerable resources scanning the continuous parameter space.
In this work, we present an augmented Lagrangian approach to tackle the
ill-posed problem of coil optimization. Our approach yields solutions that
out-perform previous Pareto-optimal work and display excellent physics and
engineering properties. We conclude by illustrating its effectiveness and
versatility by generating coils for five stellarators with very different
symmetries and magnetic field shaping. In all cases, we find coil solutions
that in various ways outperform published coil sets, including built and
operating reactors such as Wendelstein 7-X (W7-X) and the Helically Symmetric
eXperiment (HSX).

</details>


### [37] [Early Prediction of Current Quench Events in the ADITYA Tokamak using Transformer based Data Driven Models](https://arxiv.org/abs/2507.12797)
*Jyoti Agarwal,Bhaskar Chaudhury,Jaykumar Navadiya,Shrichand Jakhar,Manika Sharma*

Main category: physics.plasm-ph

TL;DR: A transformer-based deep learning model predicts early current quench in tokamak plasmas, outperforming LSTM with high recall and robustness up to 8 ms lead time.


<details>
  <summary>Details</summary>
Motivation: Disruptions in tokamak plasmas threaten system integrity, necessitating early prediction for effective mitigation.

Method: Uses transformer-based deep learning on ADITYA tokamak diagnostic data, analyzing multivariate time series.

Result: Transformer model achieves recall above 0.9 up to 8-10 ms, outperforming LSTM and showing robustness.

Conclusion: The approach is promising for real-time disruption avoidance in short-pulse tokamaks.

Abstract: Disruptions in tokamak plasmas, marked by sudden thermal and current
quenches, pose serious threats to plasma-facing components and system
integrity. Accurate early prediction, with sufficient lead time before
disruption onset, is vital to enable effective mitigation strategies. This
study presents a novel data-driven approach for predicting early current
quench, a key precursor to disruptions, using transformer-based deep learning
models, applied to ADITYA tokamak diagnostic data. Using multivariate time
series data, the transformer model outperforms LSTM baselines across various
data distributions and prediction thresholds. The transformer model achieves
better recall, maintaining values above 0.9 even up to a prediction threshold
of 8-10 ms, significantly outperforming LSTM in this critical metric. The
proposed approach remains robust up to an 8 ms lead time, offering practical
feasibility for disruption mitigation in ADITYA tokamak. In addition, a
comprehensive data diversity analysis and bias sensitivity study underscore the
generalization of the model. This work marks the first application of
transformer architectures to ADITYA tokamak data for early current-quench
prediction, establishing a promising foundation for real time disruption
avoidance in short-pulse tokamaks.

</details>


### [38] [Introduction to Stability and Turbulent Transport in Magnetic Confinement Fusion Plasmas](https://arxiv.org/abs/2507.13144)
*J. F. Parisi*

Main category: physics.plasm-ph

TL;DR: An introductory tutorial on stability and turbulent transport in magnetic confinement fusion plasmas, covering key concepts, models, and practical implications for newcomers.


<details>
  <summary>Details</summary>
Motivation: To provide an accessible guide for researchers new to the field of magnetic confinement fusion plasmas, addressing key principles and challenges.

Method: Discussion of key concepts, models, and practical implications, along with challenges and opportunities in the field.

Result: A clear introduction to stability and turbulent transport in fusion plasmas, aiding new researchers.

Conclusion: The tutorial successfully bridges knowledge gaps for newcomers, highlighting both challenges and opportunities in the field.

Abstract: This tutorial provides an accessible introduction to the principles of
stability and turbulent transport in magnetic confinement fusion plasmas. Key
concepts, models, and practical implications are discussed to guide researchers
new to the field. Some challenges and opportunities are discussed.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [39] [Improving physics-informed neural network extrapolation via transfer learning and adaptive activation functions](https://arxiv.org/abs/2507.12659)
*Athanasios Papastathopoulos-Katsaros,Alexandra Stavrianidi,Zhandong Liu*

Main category: cs.LG

TL;DR: The paper introduces a transfer learning method and an adaptive activation function to enhance the extrapolation performance of Physics-Informed Neural Networks (PINNs), achieving significant error reductions without added computational cost.


<details>
  <summary>Details</summary>
Motivation: PINNs struggle with extrapolation outside the training domain and sensitivity to activation functions, limiting their practical utility.

Method: The authors propose a transfer learning approach within an extended training domain and introduce an adaptive activation function combining standard ones.

Result: Experiments show a 40% reduction in relative L2 error and 50% reduction in mean absolute error in extrapolation.

Conclusion: The method improves PINNs' robustness and accuracy for extrapolation, with code made publicly available.

Abstract: Physics-Informed Neural Networks (PINNs) are deep learning models that
incorporate the governing physical laws of a system into the learning process,
making them well-suited for solving complex scientific and engineering
problems. Recently, PINNs have gained widespread attention as a powerful
framework for combining physical principles with data-driven modeling to
improve prediction accuracy. Despite their successes, however, PINNs often
exhibit poor extrapolation performance outside the training domain and are
highly sensitive to the choice of activation functions (AFs). In this paper, we
introduce a transfer learning (TL) method to improve the extrapolation
capability of PINNs. Our approach applies transfer learning (TL) within an
extended training domain, using only a small number of carefully selected
collocation points. Additionally, we propose an adaptive AF that takes the form
of a linear combination of standard AFs, which improves both the robustness and
accuracy of the model. Through a series of experiments, we demonstrate that our
method achieves an average of 40% reduction in relative L2 error and an average
of 50% reduction in mean absolute error in the extrapolation domain, all
without a significant increase in computational cost. The code is available at
https://github.com/LiuzLab/PINN-extrapolation .

</details>


### [40] [RONOM: Reduced-Order Neural Operator Modeling](https://arxiv.org/abs/2507.12814)
*Sven Dummer,Dongwei Ye,Christoph Brune*

Main category: cs.LG

TL;DR: The paper introduces RONOM, a framework combining reduced-order modeling (ROM) and neural operators, to address computational challenges in solving time-dependent PDEs. It provides error bounds and demonstrates superior performance in generalization and robustness.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between ROM's fixed discretization and neural operators' flexibility, enabling better adaptation to varying meshes and quantifying discretization errors.

Method: The RONOM framework integrates ROM and neural operators, establishing error bounds and testing it on PDEs with numerical examples.

Result: RONOM matches neural operators in generalization and outperforms them in spatial super-resolution and robustness, with insights into temporal super-resolution.

Conclusion: RONOM successfully combines ROM and operator learning, offering rigorous error analysis and improved performance for PDE solutions.

Abstract: Time-dependent partial differential equations are ubiquitous in physics-based
modeling, but they remain computationally intensive in many-query scenarios,
such as real-time forecasting, optimal control, and uncertainty quantification.
Reduced-order modeling (ROM) addresses these challenges by constructing a
low-dimensional surrogate model but relies on a fixed discretization, which
limits flexibility across varying meshes during evaluation. Operator learning
approaches, such as neural operators, offer an alternative by parameterizing
mappings between infinite-dimensional function spaces, enabling adaptation to
data across different resolutions. Whereas ROM provides rigorous numerical
error estimates, neural operator learning largely focuses on discretization
convergence and invariance without quantifying the error between the
infinite-dimensional and the discretized operators. This work introduces the
reduced-order neural operator modeling (RONOM) framework, which bridges
concepts from ROM and operator learning. We establish a discretization error
bound analogous to those in ROM, and get insights into RONOM's discretization
convergence and discretization robustness. Moreover, two numerical examples are
presented that compare RONOM to existing neural operators for solving partial
differential equations. The results demonstrate that RONOM using standard
vector-to-vector neural networks achieves comparable performance in input
generalization and superior performance in both spatial super-resolution and
discretization robustness, while also offering novel insights into temporal
super-resolution scenarios.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [41] [Assessing the economic benefits of space weather mitigation investment decisions: Evidence from Aotearoa New Zealand](https://arxiv.org/abs/2507.12495)
*Edward J. Oughton,Andrew Renton,Daniel Mac Marnus,Craig J. Rodger*

Main category: physics.geo-ph

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Space weather events pose a growing threat to modern economies, yet their
macroeconomic consequences still remain underexplored. This study presents the
first dedicated economic assessment of geomagnetic storm impacts on Aotearoa
New Zealand, quantifying potential GDP losses across seven disruption and
mitigation scenarios due to an extreme coronal mass ejection (CME). The primary
focus is upon the damaging impacts of geomagnetically induced currents (GICs)
on the electrical power transmission network. The goal is to support
decision-making around space weather mitigation investments by providing a
first-order approximation of their potential economic benefits. We find that in
the absence of mitigation, a severe but realistic storm could result in up to
NZ\$8.36 billion in lost GDP, with more than half stemming from cascading
supply chain effects. Yet, even less severe scenarios incur losses exceeding
NZ\$3 billion. Importantly, research-led operational strategies, such as
optimized switching and islanding, can avoid up to NZ\$370 million in losses
for as little as NZ\$500,000 in expenditure, delivering a benefit-cost ratio of
740 to 1. Moreover, physical protections such as GIC blocking devices further
reduce disruption to as low as NZ\$1.12 billion, with avoided GDP losses up to
NZ\$2.3 billion, and benefit-cost returns up to 80 to 1. When also
acknowledging unmodelled impacts, including multi-billion losses in capital
equipment and long-term revenue, the economic rationale for pre-emptive
mitigation becomes even more pertinent. Future research needs to integrate the
modelling of capital and revenue losses for strategically important industrial
facilities.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [42] [N Bugs on a Circle](https://arxiv.org/abs/2507.13333)
*Josh Briley,Bryan Quaife*

Main category: math.DS

TL;DR: A generalization of the 'Four Bugs on a Square' problem confines bugs to a unit circle's perimeter, leading to three steady states: coalescence, antipodal clusters, or stable chase cycles. Stability and probabilities are analyzed, with exact results for N ≤ 4 and simulations for larger N.


<details>
  <summary>Details</summary>
Motivation: To explore how restricting bugs to a circle's perimeter alters their long-term behavior compared to unrestricted pursuit, revealing richer dynamics.

Method: Analyze steady states (coalescence, antipodal clusters, chase cycles) and their stability. Use exact analytical expressions for N ≤ 4 and Monte Carlo simulations for larger N.

Result: Three steady states emerge. For N ≤ 4, exact probabilities are derived; for larger N, coalescence probability follows an inverse square root relationship.

Conclusion: Constraining bugs to a circle introduces complex dynamics absent in classic pursuit problems, with implications for understanding agent behavior under spatial restrictions.

Abstract: We describe and analyze a generalization of the classic ``Four Bugs on a
Square'' cyclic pursuit problem. Instead of allowing the bugs to spiral towards
one another, we constrain $N$ bugs to the perimeter of the unit circle.
Depending on their configuration, each bug moves either clockwise or
counterclockwise with a constant angular speed, or remains stationary. Unlike
the original problem where bugs always coalesce, this generalization produces
three possible steady states: all bugs coalescing to a single point, clusters
of bugs located at two antipodal points, or bugs entering a stable infinite
chase cycle where they never meet. We analyze the stability of these steady
states and calculate the probability that randomly initialized bugs reach each
state. For $N \leq 4$, we derive exact analytical expressions for these
probabilities. For larger values, we employ Monte Carlo simulations to estimate
the probability of coalescing, finding it approximately follows an inverse
square root relationship with the number of bugs. This generalization reveals
rich dynamical behaviors that are absent in the classic problem. Our analysis
provides insight into how restricting the bugs to the circle's perimeter
fundamentally alters the long-term behavior of pursuing agents compared to
unrestricted pursuit problems.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [43] [Search for Z/2 eigenfunctions on the sphere using machine learning](https://arxiv.org/abs/2507.13122)
*Andriy Haydys,Willem Adriaan Salm*

Main category: math.DG

TL;DR: Machine learning identifies Z/2 eigenfunctions on the 2-sphere using a multivalued neural network, with branch points fixed at tetrahedron and cube vertices, and optimized to a squashed tetrahedron.


<details>
  <summary>Details</summary>
Motivation: To explore and identify Z/2 eigenfunctions on the 2-sphere using machine learning techniques.

Method: Developed a multivalued feedforward deep neural network using JAX, testing fixed branch points (tetrahedron, cube) and optimizing their positions.

Result: Found Z/2 eigenfunctions for fixed branch points (tetrahedron, cube) and optimized positions (squashed tetrahedron).

Conclusion: Machine learning successfully identified Z/2 eigenfunctions, demonstrating adaptability in optimizing branch point positions.

Abstract: We use machine learning to search for examples of Z/2 eigenfunctions on the
2-sphere. For this we created a multivalued version of a feedforward deep
neural network, and we implemented it using the JAX library. We found Z/2
eigenfunctions for three cases: In the first two cases we fixed the branch
points at the vertices of a tetrahedron and at a cube respectively. In a third
case, we allowed the AI to move the branch points around and, in the end, it
positioned the branch points at the vertices of a squashed tetrahedron.

</details>


### [44] [Integral gradient estimates on a closed surface](https://arxiv.org/abs/2507.12790)
*Yuxiang Li,Rongze Sun*

Main category: math.DG

TL;DR: The paper establishes $L^p$ gradient estimates for solutions to a PDE on Riemann surfaces, independent of the metric choice, especially near the moduli space boundary.


<details>
  <summary>Details</summary>
Motivation: To analyze weak solutions of a PDE on Riemann surfaces and derive gradient estimates that hold regardless of the metric, particularly in cases where the complex structure approaches the moduli space boundary.

Method: Transform the metric $g' = e^{2u} g$ into one of bounded integral curvature, apply a quadratic area bound condition, and derive gradient estimates in local conformal coordinates.

Result: Gradient estimates for $g'$ are obtained, leading to the desired $L^p$ estimates for the gradient of $u$.

Conclusion: The method successfully provides metric-independent $L^p$ gradient estimates for solutions to the PDE, applicable near the moduli space boundary.

Abstract: Let $(\Sigma, g)$ be a closed Riemann surface, and let $u$ be a weak solution
to equation \[ - \Delta_g u = \mu, \] where $\mu$ is a signed Radon measure. We
aim to establish $L^p$ estimates for the gradient of $u$ that are independent
of the choice of the metric $g$. This is particularly relevant when the complex
structure approaches the boundary of the moduli space. To this end, we consider
the metric $g' = e^{2u} g$ as a metric of bounded integral curvature. This
metric satisfies a so-called quadratic area bound condition, which allows us to
derive gradient estimates for $g'$ in local conformal coordinates. From these
estimates, we obtain the desired estimates for the gradient of $u$.

</details>


### [45] [On the Nature of Stationary Integral Varifolds near Multiplicity 2 Planes](https://arxiv.org/abs/2507.13148)
*Spencer Becker-Kahn,Paul Minter,Neshan Wickramasekera*

Main category: math.DG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study stationary integral $n$-varifolds $V$ in the unit ball
$B_1(0)\subset\mathbb{R}^{n+k}$. Allard's regularity theorem establishes the
existence of $\epsilon = \epsilon(n,k)\in (0,1)$ for which if $V$ is
$\epsilon$-close (as varifolds) to the plane $P_0 = \{0\}^k\times\mathbb{R}^n$
with multiplicity 1 then, in $B_{1/2}(0)$, $V$ is represented by a single
$C^{1,\alpha}$ minimal graph. However, when instead $P_0$ occurs with
multiplicity $Q\in \{2,3,\dotsc\}$, simple examples show that this conclusion,
now as a multi-valued graph, may fail, even if $V$ corresponds to an
area-minimising rectifiable current.
  In the present work we investigate the structure of such $V$ which are close
to planes with multiplicity $Q>1$, focusing primarily on the case $Q=2$. We
show that an $\epsilon$-regularity theorem holds when $V$ is close, as a
varifold, to $P_0$ with multiplicity $2$, provided $V$ satisfies a certain
topological structural condition on the part of its support where the density
of $V$ is $<2$. The conclusion then is that, in $B_{1/2}(0)$, $V$ is
represented by the graph of a Lipschitz $2$-valued function over $P_0$ with
small Lipschitz constant; in fact, the function is $C^{1,\alpha}$ in a precise
generalised sense, and satisfies estimates, implying that all tangent cones at
singular points in $B_{1/2}(0)$ are unique and comprised of stationary unions
of $4$ half-planes (which may form a union of two distinct planes or a single
multiplicity $2$ plane). The theorem does not require any additional assumption
on the part of $V$ with density $\geq 2$ (which a priori may be a relatively
large set in $\mathcal{H}^n$-measure with high topological complexity).
  As a corollary, we show that our $\epsilon$-regularity theorem applies
unconditionally to stationary $2$-valued Lipschitz graphs with arbitrary
Lipschitz constant, yielding improved regularity and uniform a priori
estimates.

</details>


<div id='math.CV'></div>

# math.CV [[Back]](#toc)

### [46] [An Iterative Approach to the Complex Monge-Ampère Eigenvalue Problem](https://arxiv.org/abs/2507.13273)
*Ahmed Zeriahi*

Main category: math.CV

TL;DR: An iterative method for solving the Dirichlet complex Monge-Ampère eigenvalue problem on pseudoconvex domains in ℂⁿ, inspired by a real-domain approach.


<details>
  <summary>Details</summary>
Motivation: To extend the iterative solution technique from real Monge-Ampère problems to complex domains, addressing eigenvalue challenges.

Method: Iterative approximation inspired by Abedin and Kitagawa's real-domain method, applied to strictly pseudoconvex domains in ℂⁿ.

Result: Proposes a viable iterative approach for solving the complex Monge-Ampère eigenvalue problem.

Conclusion: The method bridges real and complex Monge-Ampère problems, offering a new tool for eigenvalue solutions in complex domains.

Abstract: We present an iterative approach to approximate the solution to the Dirichlet
complex Monge-Amp\`ere eigenvalue problem on a bounded strictly pseudoconvex
domain in $\C^n$. This approach is inspired by a similar approach initiated by
F. Abedin, J. Kitagawa who considered the real Monge-Amp\`ere operator on a
strictly convex domain in $\R^N$.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [47] [Accelerated free energy estimation in ab initio path integral Monte Carlo simulations](https://arxiv.org/abs/2507.12960)
*Pontus Svensson,Fotios Kalkavouras,Uwe Hernandez Acosta,Zhandos A. Moldabekov,Panagiotis Tolias,Jan Vorberger,Tobias Dornheim*

Main category: physics.chem-ph

TL;DR: A method accelerates free energy estimation in path integral Monte Carlo simulations using an artificial reference system, achieving 18x speedup and addressing the fermion sign problem for large systems.


<details>
  <summary>Details</summary>
Motivation: To improve computational efficiency and accuracy in estimating free energy for systems like the uniform electron gas, relevant for fusion modeling.

Method: Uses an intermediate artificial reference system (spherically averaged Ewald interaction) and a ξ-extrapolation technique to address the fermion sign problem.

Result: Achieved 18x faster free energy evaluation and accurate results for 1000-electron systems with errors below chemical accuracy.

Conclusion: The method is effective for systems with low to moderate quantum degeneracy, applicable to planetary and fusion modeling.

Abstract: We present a methodology for accelerating the estimation of the free energy
from path integral Monte Carlo simulations by considering an intermediate
artificial reference system where interactions are inexpensive to evaluate
numerically. Using the spherically averaged Ewald interaction as this
intermediate reference system for the uniform electron gas, the interaction
contribution for the free energy was evaluated up to 18 times faster than the
Ewald-only method. Furthermore, a $\xi$-extrapolation technique was tested and
applied to alleviate the fermion sign problem and to resolve the sign for large
particle numbers. Combining these two techniques enabled the evaluation of the
free energy for a system of 1000 electrons, where both finite-size and
statistical errors are below chemical accuracy. The general procedure can be
applied to systems relevant for planetary and inertial confinement fusion
modeling with low to moderate levels of quantum degeneracy.

</details>


### [48] [$GW$+2SOSEX self-energy made positive semi-definite](https://arxiv.org/abs/2507.13293)
*Fabien Bruneval,Arno Förster,Yaroslav Pavlyukh*

Main category: physics.chem-ph

TL;DR: The paper introduces a positive semi-definite extension to the $GW$+2SOSEX self-energy, named $GW$+2SOSEX-psd, addressing the challenge of vertex corrections in perturbation theory.


<details>
  <summary>Details</summary>
Motivation: Vertex corrections beyond the $GW$ approximation are complex, and many proposed schemes fail to satisfy the positive semi-definiteness condition. This study aims to address this gap.

Method: The authors devise the $GW$+2SOSEX-psd self-energy by demonstrating the cancellation of bare energy poles in the $G3W2$ self-energy.

Result: The proposed self-energy approximation is shown to be positive semi-definite and accurate in predicting quasiparticle energies for valence and core states in molecular examples.

Conclusion: The $GW$+2SOSEX-psd self-energy provides a viable solution for vertex corrections, ensuring positive semi-definiteness and accuracy in energy predictions.

Abstract: The formulation of vertex corrections beyond the $GW$ approximation within
the framework of perturbation theory is a subtle and challenging task, which
accounts for the wide variety of schemes proposed over the years. Exact
self-energies are required to satisfy the mathematical condition of positive
semi-definiteness. The $GW$ self-energy fulfills this property, but the vast
majority of the vertex-corrected self-energy approximations do not. In this
study, we devise a positive semi-definite extension to the $GW$+2SOSEX
self-energy that we name $GW$+2SOSEX-psd. To reach this goal, we demonstrate
the cancellation of the bare energy poles that are contained in the fully
dynamic second-order in $W$ self-energy ($G3W2$). We then demonstrate on
molecular examples the correct positive semi-definiteness of the proposed
self-energy approximation and its good accuracy in predicting accurate
quasiparticle energies for valence and core states.

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [49] [Three-dimensional Dynamics of Strongly Magnetized Ion-Electron Relativistic Reconnection](https://arxiv.org/abs/2507.12509)
*Fabio Bacchini,Gregory R. Werner,Camille Granier,Jesse Vos*

Main category: astro-ph.HE

TL;DR: 3D simulations of semirelativistic collisionless magnetic reconnection show that 3D effects like drift-kink and flux-rope kink dynamics reduce magnetic-energy dissipation and nonthermal particle acceleration efficiency at high magnetizations, contrary to expectations.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of magnetic reconnection in semirelativistic regimes, especially in astrophysical contexts like black holes and neutron stars.

Method: Employed 3D simulations with realistic proton-to-electron mass ratios and varied upstream ion magnetization over two orders of magnitude.

Result: At high magnetizations, 3D effects reduce the efficiency of magnetic-energy dissipation and particle acceleration.

Conclusion: The findings challenge general expectations for 3D relativistic reconnection and are significant for astrophysical applications.

Abstract: We present 3D simulations of semirelativistic collisionless magnetic
reconnection, where upstream ions are subrelativistic while electrons are
ultrarelativistic. We employ the realistic proton-to-electron mass ratio and
explore a range of upstream ion magnetization spanning two orders of magnitude,
with our highest-magnetization run achieving unprecedentedly large domain
sizes. Through a parameter scan, we find that as the system transitions from
mildly to trans- and ultrarelativistic regimes the qualitative behavior of
reconnection becomes strongly influenced by 3D effects mediated by drift-kink
and flux-rope kink dynamics. As a result, magnetic-energy dissipation at high
magnetizations, and the subsequent nonthermal particle acceleration, can become
less efficient, contrary to general expectations for 3D relativistic
reconnection. Our results have important implications for understanding
reconnection in magnetized astrophysical scenarios, such as the surroundings of
black holes and neutron stars.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [50] [Evidence for an Inverse Cascade of Magnetic Helicity in the Inner Heliosphere](https://arxiv.org/abs/2507.13213)
*Masatomi Iizawa,Yasuhito Narita,Tommaso Alberti,Stuart D. Bale,Axel Brandenburg,Abraham C. -L. Chian,Horia Comişel,Shuichi Matsukiyo,Nobumitsu Yokoi*

Main category: astro-ph.SR

TL;DR: Analysis of Parker Solar Probe data reveals a persistent inverse cascade of solar wind turbulence from the Sun to Mercury's orbit, challenging traditional views of random magnetic helicity density in the inner heliosphere.


<details>
  <summary>Details</summary>
Motivation: To understand the direction and behavior of solar wind turbulence, particularly the magnetic helicity density, in the inner heliosphere.

Method: Analyzed magnetic helicity density spectra from Parker Solar Probe data across over 500 heliocentric distances.

Result: Confirmed an inverse cascade extending to Mercury's orbit and identified a radial sign change of spectral magnetic helicity density with logarithmic distance dependence.

Conclusion: The findings challenge conventional hypotheses and offer new insights into solar wind turbulence evolution in the inner heliosphere.

Abstract: To elucidate the cascade direction of the solar wind turbulence, we analyzed
magnetic helicity density spectra from the Parker Solar Probe data across more
than 500 heliocentric distances. For the first time, we confirmed a persistent
inverse cascade extending from the Sun to Mercury's orbital vicinity. This
finding challenges the conventional hypothesis that the magnetic helicity
density within the inner heliosphere is random. Furthermore, our analysis
revealed a radial sign change of the spectral magnetic helicity density at a
frequency whose value decreases logarithmically with distance. These results
provide new insights into the evolution of solar wind turbulence in the inner
heliosphere.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [51] [On the factorization of matrices into products of positive definite ones](https://arxiv.org/abs/2507.12560)
*Mahmoud Abdelgalil,Tryphon T. Georgiou*

Main category: math.OC

TL;DR: A new constructive approach to Ballantine's matrix factorization using optimal mass transport theory, linking rotations of Gaussian distributions to positive definite factors.


<details>
  <summary>Details</summary>
Motivation: To address the strong controllability problem in linear systems via state feedback and highlight the realization of rotations through irrotational motions.

Method: Uses optimal mass transport theory to relate rotations of Gaussian distributions to positive definite matrix factors.

Result: Provides a constructive method for factorizing matrices with positive determinant into positive definite factors.

Conclusion: The approach connects control theory with optimal transport, offering new insights into matrix factorization and system controllability.

Abstract: The present work revisits and provides a new approach on a result by Charles
Ballantine, on the factorization of a square matrix with positive determinant
into a product of positive definite factors. {\em Ballantine-type}
factorizations, that bound the number of positive definite factors, proved
central in solving a basic, yet elusive control problem--the strong
controllability of a linear system via control in the form of state feedback.
Ballantine's result transcends control engineering, and highlights the little
appreciated fact that rotations can be realized by the successive application
of irrotational motions. Our approach is constructive and is based on the
theory of optimal mass transport, specifically, it relates successive rotations
of Gaussian distributions to corresponding optimal transport maps that
constitute the sought factors.

</details>


### [52] [Tensor-Tensor Products, Group Representations, and Semidefinite Programming](https://arxiv.org/abs/2507.12729)
*Alex Dunbar,Elizabeth Newman*

Main category: math.OC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The $\star_M$-family of tensor-tensor products is a framework which
generalizes many properties from linear algebra to third order tensors. Here,
we investigate positive semidefiniteness and semidefinite programming under the
$\star_M$-product. Critical to our investigation is a connection between the
choice of matrix M in the $\star_M$-product and the representation theory of an
underlying group action. Using this framework, third order tensors equipped
with the $\star_M$-product are a natural setting for the study of invariant
semidefinite programs. As applications of the M-SDP framework, we provide a
characterization of certain nonnegative quadratic forms and solve low-rank
tensor completion problems.

</details>


### [53] [Unsupervised Ground Metric Learning](https://arxiv.org/abs/2507.13094)
*Janis Auffenberg,Jonas Bresch,Oleh Melnyk,Gabriele Steidl*

Main category: math.OC

TL;DR: The paper addresses unsupervised metric learning by learning optimal transport cost matrices and explores alternative distances like Mahalanobis and graph Laplacians, with algorithmic convergence proofs.


<details>
  <summary>Details</summary>
Motivation: Data classification without labeled samples is challenging, requiring effective distance metrics between features.

Method: Simultaneously learn optimal transport cost matrices, propose stochastic random function iteration algorithm, and explore Mahalanobis-like distances and graph Laplacians.

Result: Proposed algorithm converges linearly; Mahalanobis and graph Laplacian distances are viable alternatives.

Conclusion: Unsupervised metric learning is feasible with OT and other distances, supported by efficient algorithms.

Abstract: Data classification without access to labeled samples remains a challenging
problem. It usually depends on an appropriately chosen distance between
features, a topic addressed in metric learning. Recently, Huizing, Cantini and
Peyr\'e proposed to simultaneously learn optimal transport (OT) cost matrices
between samples and features of the dataset. This leads to the task of finding
positive eigenvectors of a certain nonlinear function that maps cost matrices
to OT distances. Having this basic idea in mind, we consider both the
algorithmic and the modeling part of unsupervised metric learning. First, we
examine appropriate algorithms and their convergence. In particular, we propose
to use the stochastic random function iteration algorithm and prove that it
converges linearly for our setting, although our operators are not
paracontractive as it was required for convergence so far. Second, we ask the
natural question if the OT distance can be replaced by other distances. We show
how Mahalanobis-like distances fit into our considerations. Further, we examine
an approach via graph Laplacians. In contrast to the previous settings, we have
just to deal with linear functions in the wanted matrices here, so that simple
algorithms from linear algebra can be applied.

</details>


### [54] [Optimal regularity up to the boundary for Plateau-quasi-minimizers](https://arxiv.org/abs/2507.13189)
*Eve Machefert*

Main category: math.OC

TL;DR: The paper analyzes the regularity of quasi-minimal sets with boundary conditions, proving optimal regularity characterized by bi-John domains with Ahlfors regular boundaries.


<details>
  <summary>Details</summary>
Motivation: To understand the regularity properties of quasi-minimal sets, particularly in the context of Plateau's problem in co-dimension one.

Method: Investigates Ahlfors regularity and uniform rectifiability of quasi-minimal sets, including boundary analysis.

Result: Establishes optimal regularity for Plateau-quasi-minimizers, characterized by bi-John domains with Ahlfors regular boundaries.

Conclusion: The study provides a comprehensive regularity characterization for quasi-minimal sets with boundary conditions, advancing understanding in geometric measure theory.

Abstract: We study the regularity of quasi-minimal sets (in the sense of David and
Semmes) with a boundary condition, which can be interpreted as quasi-minimizers
of Plateau's problem in co-dimension one. For these Plateau-quasi-minimizers,
we establish the optimal regularity, which is a characterization by bi-John
domains with Ahlfors regular boundaries. This requires to investigate the
Ahlfors regularity and also the uniform rectifiability of those sets, up to the
boundary.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [55] [Unraveling Self-Similar Energy Transfer Dynamics: a Case Study for 1D Burgers System](https://arxiv.org/abs/2507.12764)
*Pritpal Matharu,Bartosz Protas,Tsuyoshi Yoneda*

Main category: physics.flu-dyn

TL;DR: The paper explores constructing self-similar energy cascades in turbulence using the 1D viscous Burgers equation, solved via PDE-constrained optimization. Two solution types (viscous and inertial) are identified, with inertial solutions requiring small viscosity for self-similarity.


<details>
  <summary>Details</summary>
Motivation: To develop a systematic approach for finding self-similar flow evolutions in turbulence, starting with a simplified model (Burgers equation).

Method: PDE-constrained optimization with an adjoint-based gradient method to solve the nonconvex problem.

Result: Identified two solution families (viscous and inertial), with inertial solutions achieving self-similarity through wave front steepening under small viscosity.

Conclusion: The approach is a promising proof of concept for extending to 3D turbulence.

Abstract: In this work we consider the problem of constructing flow evolutions leading
to a self-similar energy cascade consistent with Kolmogorov's statistical
theory of turbulence. As a first step in this direction, we focus on the
one-dimensional viscous Burgers equation as a toy model. Its solutions
exhibiting self-similar behavior, in a precisely-defined sense, are found by
framing this problems in terms of PDE-constrained optimization. The main
physical parameters are the time window over which self-similar behavior is
sought (equal to approximately one eddy turnover time), viscosity (inversely
proportional to the ``Reynolds number") and an integer parameter characterizing
the distance in the Fourier space over which self-similar interactions occur.
Local solutions to this nonconvex PDE optimization problems are obtained with a
state-of-the-art adjoint-based gradient method. Two distinct families of
solutions, termed viscous and inertial, are identified and are distinguished
primarily by the behavior of enstrophy which, respectively, uniformly decays
and grows in the two cases. The physically meaningful and appropriately
self-similar inertial solutions are found only when a sufficiently small
viscosity is considered. These flows achieve the self-similar behaviour by a
uniform steepening of the wave fronts present in the solutions. The methodology
proposed and the results obtained represent an encouraging proof of concept for
this approach to be employed to systematically search for self-similar flow
evolutions in the context of three-dimensional turbulence.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [56] [Inverse Physics-informed neural networks procedure for detecting noise in open quantum systems](https://arxiv.org/abs/2507.12552)
*Gubio G. de Lima,Iann Cunha,Leonardo Kleber Castelano*

Main category: quant-ph

TL;DR: The paper introduces PINNverse, a machine learning framework for identifying Hamiltonian parameters and decay rates in open quantum systems, demonstrating its effectiveness with two-qubit simulations.


<details>
  <summary>Details</summary>
Motivation: Accurate quantum system characterization is crucial for NISQ technologies, but traditional methods are inefficient. Machine learning offers scalable alternatives.

Method: Extends the PINNverse framework to open quantum systems using Lindblad master equations, training neural networks on coherent and dissipative dynamics.

Result: Numerical simulations show PINNverse effectively identifies Hamiltonian parameters and decay rates, proving scalable and noise-resilient.

Conclusion: PINNverse is a promising tool for quantum system identification, with applications in quantum control and error mitigation.

Abstract: Accurate characterization of quantum systems is essential for the development
of quantum technologies, particularly in the noisy intermediate-scale quantum
(NISQ) era. While traditional methods for Hamiltonian learning and noise
characterization often require extensive measurements and scale poorly with
system size, machine learning approaches offer promising alternatives. In this
work, we extend the inverse physics-informed neural network (referred to as
PINNverse) framework to open quantum systems governed by Lindblad master
equations. By incorporating both coherent and dissipative dynamics into the
neural network training, our method enables simultaneous identification of
Hamiltonian parameters and decay rates from noisy experimental data. We
demonstrate the effectiveness and robustness of the approach through numerical
simulations of two-qubit open systems. Our results show that PINNverse provides
a scalable and noise-resilient framework for quantum system identification,
with potential applications in quantum control and error mitigation.

</details>


### [57] [State transfer analysis for linear spin chains with non-uniform on-site energies](https://arxiv.org/abs/2507.13261)
*Chad C. Nelmes,Irene D'Amico,Timothy P. Spiller*

Main category: quant-ph

TL;DR: Analysis of perfect and quasi-perfect state transfer in linear spin chains with non-uniform on-site energies, focusing on coupling uniformity and its implications for quantum information processing.


<details>
  <summary>Details</summary>
Motivation: To maintain coupling uniformity in distributed quantum information processing, which could benefit certain physical implementations.

Method: Study of linear spin chains with non-uniform on-site energies, relating coupling uniformity to a particle in a discrete potential analogue. Statistical analysis of variations in couplings and on-site energies as chain length increases.

Result: Demonstrates the feasibility of high fidelity state transfer under non-uniform conditions, with insights into statistical variations.

Conclusion: Non-uniform on-site energies can support high fidelity state transfer, offering practical advantages for quantum implementations.

Abstract: High fidelity state transfer is an important ingredient of distributed
quantum information processing. We present and analyse results on perfect and
quasi-perfect state transfer with linear spin chains incorporating non-uniform
on-site energies. The motivation is maintenance of coupling uniformity, which
could be beneficial for some physical implementations. We relate this coupling
uniformity to a particle in a discrete potential analogue. Our analysis further
considers the statistical variation in couplings and on-site energies, as a
function of increasing chain site number N.

</details>
