<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 10]
- [math.AP](#math.AP) [Total: 15]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 6]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [physics.space-ph](#physics.space-ph) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [math-ph](#math-ph) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [math.OC](#math.OC) [Total: 2]
- [cs.LG](#cs.LG) [Total: 1]
- [cs.CV](#cs.CV) [Total: 1]
- [astro-ph.HE](#astro-ph.HE) [Total: 2]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Multiresolution local smoothness detection in non-uniformly sampled multivariate signals](https://arxiv.org/abs/2507.13480)
*Sara Avesani,Gianluca Giacchi,Michael Multerer*

Main category: math.NA

TL;DR: A linear-time algorithm for detecting local regularity in non-uniformly sampled multivariate signals using samplet transform, linking coefficient decay to signal regularity.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of detecting regularity in higher-dimensional and scattered data, where traditional wavelets fall short.

Method: Uses the fast samplet transform, a distributional wavelet transform for scattered data, to quantify regularity in microlocal spaces.

Result: Connects samplet coefficient decay to pointwise regularity, with applications in H\"older and Sobolev-Slobodeckij spaces. Demonstrates effectiveness in 1D, 2D, and 3D signals.

Conclusion: Samplets outperform traditional wavelets for regularity detection in higher-dimensional and scattered data, validated by numerical studies.

Abstract: Inspired by edge detection based on the decay behavior of wavelet
coefficients, we introduce a (near) linear-time algorithm for detecting the
local regularity in non-uniformly sampled multivariate signals. Our approach
quantifies regularity within the framework of microlocal spaces introduced by
Jaffard. The central tool in our analysis is the fast samplet transform, a
distributional wavelet transform tailored to scattered data. We establish a
connection between the decay of samplet coefficients and the pointwise
regularity of multivariate signals. As a by product, we derive decay estimates
for functions belonging to classical H\"older spaces and Sobolev-Slobodeckij
spaces. While traditional wavelets are effective for regularity detection in
low-dimensional structured data, samplets demonstrate robust performance even
for higher dimensional and scattered data. To illustrate our theoretical
findings, we present extensive numerical studies detecting local regularity of
one-, two- and three-dimensional signals, ranging from non-uniformly sampled
time series over image segmentation to edge detection in point clouds.

</details>


### [2] [A priori error analysis of the proximal Galerkin method](https://arxiv.org/abs/2507.13516)
*Brendan Keith,Rami Masri,Marius Zeinhofer*

Main category: math.NA

TL;DR: The paper introduces the first abstract a priori error analysis of the proximal Galerkin (PG) method, a finite element method for variational problems with inequality constraints, demonstrating its advantages and optimal convergence rates for specific problems.


<details>
  <summary>Details</summary>
Motivation: To provide a general framework for analyzing the convergence and error estimates of PG methods, which are advantageous for solving variational problems with inequality constraints.

Method: The paper employs an abstract a priori error analysis framework to evaluate PG methods, applying it to the obstacle and Signorini problems using various finite element subspaces.

Result: Optimal convergence rates are demonstrated for the obstacle and Signorini problems, validating the effectiveness of the PG method.

Conclusion: The PG method is a robust approach for variational problems with inequality constraints, with the presented framework enabling rigorous error analysis and convergence proofs.

Abstract: The proximal Galerkin (PG) method is a finite element method for solving
variational problems with inequality constraints. It has several advantages,
including constraint-preserving approximations and mesh independence. This
paper presents the first abstract a priori error analysis of PG methods,
providing a general framework to establish convergence and error estimates. As
applications of the framework, we demonstrate optimal convergence rates for
both the obstacle and Signorini problems using various finite element
subspaces.

</details>


### [3] [Quantifying Ocular Surface Changes with Contact Lens Wear](https://arxiv.org/abs/2507.13589)
*Lucia Carichino,Kara L. Maki,David S. Ross,Riley K. Supple,Evan Rysdam*

Main category: math.NA

TL;DR: A mathematical model coupling contact lens and open eye interactions is developed, predicting ocular deformations and stresses, with validation against experimental data.


<details>
  <summary>Details</summary>
Motivation: Address the lack of research and clinical measurement challenges in understanding mechanical interactions between contact lenses and the ocular surface, which cause discomfort for many users.

Method: A non-linear coupling model between the contact lens and the eye is created, considering suction pressure, lens dynamics, and varying material properties and shapes for both lens and eye.

Result: Higher ocular deformations and stresses are predicted at the eye's center and limbal/scleral region, influenced by lens stiffness and shape.

Conclusion: The model provides insights into contact lens-induced ocular stress, aiding future research and lens design to improve comfort.

Abstract: Over 140 million people worldwide and over 45 million people in the United
states wear contact lenses; it is estimated 12%-27.4% contact lens users stop
wearing them due to discomfort. Contact lens mechanical interactions with the
ocular surface have been found to affect the ocular surface. The mechanical
interactions between the contact lens and the eye are difficult to measure and
calculate in the clinical setting, and the research in this field is limited.
This paper presents the first mathematical model that couples the interaction
between the contact lens and the open eye, where the contact lens
configuration, the contact lens suction pressure, and the deformed ocular shape
are all emergent properties of the model. The non-linear coupling between the
contact lens and the eye is achieved assuming the the suction pressure under
the lens is applied directly to the ocular surface, neglecting the post-lens
tear film layer. The contact lens dynamics is modeled using a previous
published model. We consider a homogeneous and a heterogeneous linear elastic
eye model, different ocular shapes, different lens shapes and lens thickness
profiles, and extract lens deformation, lens suction pressure profiles, and
ocular deformations and stresses for all the scenarios considered. The model
predicts higher ocular deformations and stresses at the center of the eye and
in the limbal/scleral region. Accounting for a heterogeneous material eye
parameters increases such deformations and stresses. The ocular displacements
and stresses increase non-linearly as we increase the stiffness of the contact
lens. Inserting a steeper contact lens on the eye results in a reduction of the
ocular displacement at the center of the eye and a larger displacement at the
edge of the contact lens. The model predictions are compared to experimental
data and previously developed mathematical models.

</details>


### [4] [Interpolation in Polynomial Spaces of p-Degree](https://arxiv.org/abs/2507.13640)
*Phil-Alexander Hofmann,Damar Wicaksono,Michael Hecht*

Main category: math.NA

TL;DR: The paper analyzes the Fast Newton Transform (FNT) algorithm for multivariate Newton interpolation in specific downward closed polynomial spaces, showing improved complexity over tensor product spaces.


<details>
  <summary>Details</summary>
Motivation: To explore the efficiency and applicability of the FNT algorithm in downward closed polynomial spaces, particularly for sensitivity analysis.

Method: The FNT algorithm is analyzed within the family of downward closed sets $A_{m,n,p}$, achieving a time complexity of $\mathcal{O}(|A_{m,n,p}|mn)$.

Result: The FNT outperforms tensor product spaces with a super-exponential complexity improvement factor $\rho_{m,n,p}$ when $m \lesssim n^p$.

Conclusion: The FNT is efficient for multivariate interpolation in specific polynomial spaces and useful for sensitivity analysis.

Abstract: We recently introduced the Fast Newton Transform (FNT), an algorithm for
performing multivariate Newton interpolation in downward closed polynomial
spaces of spatial dimension $m$. In this work, we analyze the FNT in the
context of a specific family of downward closed sets $A_{m,n,p}$, defined as
all multi-indices with $\ell^p$ norm less than $n$ with $p \in [0,\infty]$.
These sets induce the downward closed polynomial space $\Pi_{m,n,p}$, within
which the FNT algorithm achieves a time complexity of
$\mathcal{O}(|A_{m,n,p}|mn)$. We show that this setting, compared to tensor
product spaces, yields an improvement in complexity by a factor $\rho_{m,n,p}$,
which decays super exponentially with increasing spatial dimension when $m
\lesssim n^p$. Additionally, we demonstrate the construction of the
hierarchical scheme employed by the FNT and showcase its performance to compute
activity scores in sensitivity analysis.

</details>


### [5] [Multiphysics embedding localized orthogonal decomposition for thermomechanical coupling problems](https://arxiv.org/abs/2507.13644)
*Yuzhou Nan,Yajun Wang,Changqing Ye,Xiaofei Guan*

Main category: math.NA

TL;DR: A novel ME-LOD method is proposed for thermomechanical coupling problems, outperforming traditional LOD in accuracy for heterogeneous media.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in multiscale modeling of multiphysics coupling in highly heterogeneous media.

Method: ME-LOD method unifies construction for displacement and temperature, ensuring operator stability and computational efficiency.

Result: Numerical experiments show ME-LOD outperforms traditional LOD in accuracy, especially with material property contrasts.

Conclusion: ME-LOD provides a systematic, efficient, and accurate approach for multiphysics coupling problems.

Abstract: Multiscale modeling and analysis of multiphysics coupling processes in highly
heterogeneous media present significant challenges. In this paper, we propose a
novel multiphysics embedding localized orthogonal decomposition (ME-LOD) method
for solving thermomechanical coupling problems, which also provides a
systematic approach to address intricate coupling effects in multiphysical
systems. Unlike the standard localized orthogonal decomposition (LOD) method
that constructs separate multiscale spaces for each physical field, the
proposed method features a unified construction for both displacement and
temperature. Compared to the standard LOD method, our approach achieves
operator stability reconstruction through orthogonalization while preserving
computational efficiency. Several numerical experiments demonstrate that the
ME-LOD method outperforms the traditional LOD method in accuracy, particularly
in cases with significant contrasts in material properties.

</details>


### [6] [Pass-efficient Randomized Algorithms for Low-rank Approximation of Quaternion Matrices](https://arxiv.org/abs/2507.13731)
*Salman Ahmadi-Asl,Malihe Nobakht Kooshkghazi,Valentin Leplat*

Main category: math.NA

TL;DR: Proposes pass-efficient randomized algorithms for low-rank approximation of quaternion matrices, allowing trade-offs between pass budget and accuracy. Includes arbitrary-pass methods and a block Krylov extension, with exponential error decay bounds. Validated via experiments in applications like compression and deep learning.


<details>
  <summary>Details</summary>
Motivation: Existing methods for low-rank approximation of quaternion matrices ignore pass efficiency, which is crucial in modern computing due to communication costs.

Method: Develops a suite of pass-efficient randomized algorithms, including arbitrary-pass methods and a block Krylov subspace extension, with spectral norm error bounds.

Result: The algorithms show exponential decay in approximation error with passes, validated in applications like compression, matrix completion, and deep learning.

Conclusion: The proposed framework effectively balances pass efficiency and accuracy, demonstrating practical utility in diverse applications.

Abstract: Randomized algorithms for low-rank approximation of quaternion matrices have
gained increasing attention in recent years. However, existing methods overlook
pass efficiency, the ability to limit the number of passes over the input
matrix-which is critical in modern computing environments dominated by
communication costs. We address this gap by proposing a suite of pass-efficient
randomized algorithms that let users directly trade pass budget for
approximation accuracy. Our contributions include: (i) a family of
arbitrary-pass randomized algorithms for low-rank approximation of quaternion
matrices that operate under a user-specified number of matrix views, and (ii) a
pass-efficient extension of block Krylov subspace methods that accelerates
convergence for matrices with slowly decaying spectra. Furthermore, we
establish spectral norm error bounds showing that the expected approximation
error decays exponentially with the number of passes. Finally, we validate our
framework through extensive numerical experiments and demonstrate its practical
relevance across multiple applications, including quaternionic data
compression, matrix completion, image super-resolution, and deep learning.

</details>


### [7] [Newton's method for nonlinear mappings into vector bundles Part II: Application to variational problems](https://arxiv.org/abs/2507.13836)
*Laura Weigl,Ronny Bergmann,Anton Schiela*

Main category: math.NA

TL;DR: The paper explores solving variational equations on manifolds using Newton's method, incorporating differential geometric tools and an affine covariant damping strategy, with numerical results provided.


<details>
  <summary>Details</summary>
Motivation: To address variational equations on manifolds, which are expressed as root-finding problems in infinite-dimensional settings, by leveraging Newton's method.

Method: Utilizes differential geometric tools to implement Newton's method, enhanced with an affine covariant damping strategy, and applies it to variational problems.

Result: Demonstrates the application of Newton's method to variational problems, supported by numerical results.

Conclusion: The approach effectively solves variational equations on manifolds, showcasing the utility of Newton's method in this geometric context.

Abstract: We consider the solution of variational equations on manifolds by Newton's
method. These problems can be expressed as root finding problems for mappings
from infinite dimensional manifolds into dual vector bundles. We derive the
differential geometric tools needed for the realization of Newton's method,
equipped with an affine covariant damping strategy. We apply Newton's method to
a couple of variational problems and show numerical results.

</details>


### [8] [A stochastic column-block gradient descent method for solving nonlinear systems of equations](https://arxiv.org/abs/2507.13855)
*Naiyu Jiang,Wendi Bao,Lili Xing,Weiguo Li*

Main category: math.NA

TL;DR: A new stochastic column-block gradient descent method for nonlinear systems with optimal step size and proven convergence.


<details>
  <summary>Details</summary>
Motivation: To improve solving nonlinear systems by introducing a more efficient gradient descent method.

Method: Stochastic column-block gradient descent with an optimal step size derived from an optimization problem.

Result: The method outperforms existing ones, with a proven upper bound for convergence rate.

Conclusion: The proposed method is effective and superior to current solutions for nonlinear systems.

Abstract: In this paper, we propose a new stochastic column-block gradient descent
method for solving nonlinear systems of equations. It has a descent direction
and holds an approximately optimal step size obtained through an optimization
problem. We provide a thorough convergence analysis, and derive an upper bound
for the convergence rate of the new method. Numerical experiments demonstrate
that the proposed method outperforms the existing ones.

</details>


### [9] [Deep Micro Solvers for Rough-Wall Stokes Flow in a Heterogeneous Multiscale Method](https://arxiv.org/abs/2507.13902)
*Emanuel Ström,Anna-Karin Tornberg,Ozan Öktem*

Main category: math.NA

TL;DR: A learned precomputation method for HMM in rough-wall Stokes flow uses a Fourier neural operator to approximate local flow averages, reducing computational cost while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: To efficiently compute effective slip lengths in rough-wall Stokes flow without solving local problems classically.

Method: Uses a Fourier neural operator to map local wall geometry to Riesz representors for flow averages, trained independently of boundary conditions.

Result: Bounded training loss ensures bounded macroscopic flow error; method performs stably across roughness scales.

Conclusion: The learned precomputation matches classical accuracy in HMM solutions but with significantly lower computational cost.

Abstract: We propose a learned precomputation for the heterogeneous multiscale method
(HMM) for rough-wall Stokes flow. A Fourier neural operator is used to
approximate local averages over microscopic subsets of the flow, which allows
to compute an effective slip length of the fluid away from the roughness. The
network is designed to map from the local wall geometry to the Riesz
representors for the corresponding local flow averages. With such a
parameterisation, the network only depends on the local wall geometry and as
such can be trained independent of boundary conditions. We perform a detailed
theoretical analysis of the statistical error propagation, and prove that under
suitable regularity and scaling assumptions, a bounded training loss leads to a
bounded error in the resulting macroscopic flow. We then demonstrate on a
family of test problems that the learned precomputation performs stably with
respect to the scale of the roughness. The accuracy in the HMM solution for the
macroscopic flow is comparable to when the local (micro) problems are solved
using a classical approach, while the computational cost of solving the micro
problems is significantly reduced.

</details>


### [10] [Convergence rates of curved boundary element methods for the 3D Laplace and Helmholtz equations](https://arxiv.org/abs/2507.13955)
*Luiz Maltez Faria,Pierre Marchand,Hadrien Montanelli*

Main category: math.NA

TL;DR: Improved convergence rates for 3D Laplace and Helmholtz equations using curved boundary element methods, with numerical validation up to fourth-order elements.


<details>
  <summary>Details</summary>
Motivation: To enhance the accuracy and efficiency of solving 3D Laplace and Helmholtz equations by addressing consistency errors in curved boundary element methods.

Method: Analysis of perturbed bilinear and sesquilinear forms, validated with numerical experiments using curved triangular elements up to fourth order.

Result: Demonstrated improved convergence rates for smooth geometry and data in 3D.

Conclusion: The method effectively reduces consistency errors, enhancing convergence rates for practical applications.

Abstract: We establish improved convergence rates for curved boundary element methods
applied to the three-dimensional (3D) Laplace and Helmholtz equations with
smooth geometry and data. Our analysis relies on a precise analysis of the
consistency errors introduced by the perturbed bilinear and sesquilinear forms.
We illustrate our results with numerical experiments in 3D based on basis
functions and curved triangular elements up to order four.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [11] [Functional calculus on weighted Sobolev spaces for the Laplacian on rough domains](https://arxiv.org/abs/2507.13478)
*Nick Lindemulder,Emiel Lorist,Floris Roodenburg,Mark Veraar*

Main category: math.AP

TL;DR: The paper analyzes the Laplace operator under Dirichlet or Neumann boundary conditions, proving it has a bounded $H^{\infty}$-functional calculus on weighted Sobolev spaces with boundary-distance weights. It applies to $C^{1,\lambda}$-domains, showing a trade-off between domain regularity and weight exponent. Key result: maximal regularity for the heat equation, extending well-posedness to minimally smooth domains.


<details>
  <summary>Details</summary>
Motivation: To extend the well-posedness theory for parabolic equations to domains with minimal smoothness, where classical methods fail, by leveraging weighted Sobolev spaces and functional calculus.

Method: Study the Laplace operator under Dirichlet/Neumann conditions on $C^{1,\lambda}$-domains, using weighted Sobolev spaces with boundary-distance weights and $H^{\infty}$-functional calculus.

Result: The operator admits a bounded $H^{\infty}$-functional calculus, enabling maximal regularity for the heat equation on minimally smooth domains.

Conclusion: The approach compensates for low domain regularity with adjusted weights, extending parabolic equation theory to less smooth domains.

Abstract: We study the Laplace operator on domains subject to Dirichlet or Neumann
boundary conditions. We show that these operators admit a bounded
$H^{\infty}$-functional calculus on weighted Sobolev spaces, where the weights
are powers of the distance to the boundary. Our analysis applies to bounded
$C^{1,\lambda}$-domains with $\lambda\in[0,1]$, revealing a crucial trade-off:
lower domain regularity can be compensated by enlarging the weight exponent. As
a primary consequence, we establish maximal regularity for the corresponding
heat equation. This extends the well-posedness theory for parabolic equations
to domains with minimal smoothness, where classical methods are inapplicable.

</details>


### [12] [The Cauchy problem for the integrable RZQ equation](https://arxiv.org/abs/2507.13535)
*John Holmes,Ryan C. Thompson*

Main category: math.AP

TL;DR: The paper analyzes the RZQ equation, a fifth-order Camassa-Holm type equation, proving its well-posedness in Sobolev spaces for s>7/2 and ill-posedness for s<7/2.


<details>
  <summary>Details</summary>
Motivation: To study the integrability and well-posedness of the newly derived RZQ equation, comparing it to the Camassa-Holm equation.

Method: Mathematical analysis of the RZQ equation's initial value problem, focusing on Sobolev spaces and Hadamard well-posedness.

Result: The RZQ equation is well-posed for s>7/2, with a non-uniformly continuous data-to-solution map, and ill-posed for s<7/2.

Conclusion: The RZQ equation exhibits distinct properties compared to the Camassa-Holm equation, with specific well-posedness conditions.

Abstract: In this paper we study the new, integrable, fifth order Camassa-Holm (CH)
type equation derived by Reyes, Zhu, and Qiao, which we call the RZQ equation.
The $m$-form of this equation possesses a striking similarity to the m-form of
the CH equation. However, unlike the CH equation, the nonlocal form of this
equation cannot be interpreted as a nonlocal perturbation of Burgers' equation.
  We prove that the initial value problem corresponding to the RZQ equation is
well-posed in the sense of Hadamard, in Sobolev spaces $H^s$, $s>7/2$. We
further show that the data-to-solution map is not uniformly continuous. We also
show that the RZQ equation is ill-posed in $H^s$ for $s<7/2$ in the
non-periodic case.

</details>


### [13] [Local and global solvability of the Grushin heat equation with mixed nonlinear memory and reaction terms](https://arxiv.org/abs/2507.13547)
*Ahmad Z. Fino,Arlucio Viana*

Main category: math.AP

TL;DR: Study of solvability for a heat equation with the Grushin operator and nonlinear reaction terms, including a memory component. Local well-posedness and global existence conditions are derived.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of parabolic equations with degenerate operators and complex nonlinear interactions, particularly the influence of memory terms on solvability.

Method: Established local well-posedness in Lebesgue and continuous function spaces, applied comparison principles to derive global existence conditions.

Result: Sufficient conditions for global solvability were found, highlighting the impact of the memory term.

Conclusion: The work advances understanding of degenerate parabolic equations with nonlinear and memory effects.

Abstract: In this work, we investigate the solvability of a heat equation involving the
Grushin operator. The equation is perturbed by two nonlinear reaction terms,
one of which includes a memory component, introducing nonlocal effects in time.
We first establish local well-posedness results in both Lebesgue spaces and in
the space of continuous functions vanishing at infinity. Furthermore, we
develop and apply comparison principles that allow us to derive sufficient
conditions for the global existence of solutions, depending on the relative
strength and nature of the nonlinearities involved. In particular, we highlight
how the presence of the memory term influences global solvability. Our results
contribute to the understanding of parabolic equations with degenerate
operators and complex nonlinear interactions.

</details>


### [14] [Hölder stability of an inverse spectral problem for the magnetic Schrödinger operator on a simple manifold](https://arxiv.org/abs/2507.13619)
*Boya Liu,Hadrian Quan,Teemu Saksala,Lili Yan*

Main category: math.AP

TL;DR: The paper demonstrates stable recovery of electric and magnetic potentials from boundary spectral data on a Riemannian manifold, using a two-part proof involving hyperbolic equations and geometric optics solutions.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the inverse problem of recovering potentials in the magnetic Schrödinger operator from boundary spectral data, which has applications in quantum mechanics and imaging.

Method: The proof involves two steps: (1) deriving boundary spectral data from the Dirichlet-to-Neumann map of a hyperbolic equation, and (2) using geometric optics solutions to reduce the problem to inverting the geodesic ray transform.

Result: The potentials can be recovered Hölder stably from the boundary spectral data, which includes eigenvalues and Neumann traces of Dirichlet eigenfunctions.

Conclusion: The paper provides a stable method for recovering potentials in the magnetic Schrödinger operator, leveraging hyperbolic equations and geometric optics.

Abstract: We show that on a simple Riemannian manifold, the electric potential and the
solenoidal part of the magnetic potential appearing in the magnetic
Schr\"odinger operator can be recovered H\"older stably from the boundary
spectral data. This data contains the eigenvalues and the Neumann traces of the
corresponding sequence of Dirichlet eigenfunctions of the operator. Our proof
contains two parts, which we present in the reverse order. (1) We show that the
boundary spectral data can be stably obtained from the Dirichlet-to-Neumann map
associated with the respective initial boundary value problem for a hyperbolic
equation, whose leading order terms are a priori known. (2) We construct
geometric optics solutions to the hyperbolic equation, which reduce the stable
recovery of the lower order terms to the stable inversion of the geodesic ray
transform of one-forms and functions.

</details>


### [15] [Global well-posedness for the Keller--Segel--Navier--Stokes system with nonlinear boundary conditions](https://arxiv.org/abs/2507.13684)
*Taiki Takeuchi,Keiichi Watanabe*

Main category: math.AP

TL;DR: The paper studies the Keller--Segel--Navier--Stokes system with nonlinear boundary conditions, proving global strong solutions' existence, uniqueness, and stability under small data assumptions.


<details>
  <summary>Details</summary>
Motivation: To address the lack of clarity in previous works regarding boundary conditions and to extend the analysis to rotational chemotactic fluxes.

Method: Uses maximal regularity for linear heat and Stokes systems, with a new theorem for inhomogeneous Neumann boundary conditions, avoiding limiting procedures.

Result: Existence, uniqueness, Lipschitz continuity, and asymptotic stability of solutions, with non-negativity via a new maximum principle.

Conclusion: The direct approach ensures boundary conditions are satisfied, improving upon prior contributions and providing classical solutions under regularity.

Abstract: In this paper, we consider the Keller--Segel--Navier--Stokes system with
nonlinear boundary conditions in a bounded smooth (and not necessarily convex)
domain $\Omega \subset \mathbb{R}^N$, $N \ge 2$, where the chemotactic
sensitivity $S$ is assumed to have values in $\mathbb{R}^{N \times N}$ which
accounts for rotational fluxes. In contrast to the case where $S$ is a
scalar-valued function (or $S$ is the identity matrix), in our system, the
normal derivative for the density $n$ of the cell is given as the product of
the unknown functions, i.e., the function $n$ satisfies the nonlinear boundary
condition. We show the existence and uniqueness of global strong solutions to
the system under the smallness assumptions of given data, where the Lipschitz
continuity of the solution mapping and the asymptotic stability of the solution
are also shown. The proof is based on maximal regularity results for the linear
heat equation and the Stokes system, where we establish a new maximal
regularity theorem for some linear heat equation with an inhomogeneous Neumann
boundary condition. Since we develop a direct approach to construct the
solutions (i.e., without considering limiting procedure in certain regularized
problem with homogeneous linear boundary conditions), our solutions indeed
satisfy the boundary conditions, which was not addressed clearly in the
previous contributions by Cao and Lankeit (2016) as well as Yu, Wang, and Zheng
(2018). Under suitable regularity conditions on given data, the solution may be
understood in a classical sense and, as a by-product, the non-negativity result
is proved via the maximum principle of a new type.

</details>


### [16] [Optimal boundary regularity for mixed local and nonlocal equations](https://arxiv.org/abs/2507.13711)
*Nicola Abatangelo,Elisa Affili,Matteo Cozzi*

Main category: math.AP

TL;DR: Sharp boundary regularity estimates for solutions to elliptic equations combining Laplacian and nonlocal operators, using weighted Hölder spaces and fixed-point arguments.


<details>
  <summary>Details</summary>
Motivation: To address the regularity of solutions to elliptic equations involving both local (Laplacian) and nonlocal (fractional Laplacian-like) operators.

Method: Uses weighted Hölder spaces, regularity estimates for the Laplacian, and a fixed-point argument.

Result: Provides sharp boundary regularity estimates, with optimality demonstrated via an explicit counterexample.

Conclusion: The approach successfully derives and validates sharp regularity estimates for mixed local-nonlocal elliptic equations.

Abstract: We provide sharp boundary regularity estimates for solutions to elliptic
equations driven by an integro-differential operator obtained as the sum of a
Laplacian with a nonlocal operator generalizing a fractional Laplacian.
  Our approach makes use of weighted H\"older spaces as well as regularity
estimates for the Laplacian in this context and a fixed-point argument.
  We show the optimality of the obtained estimates by means of a counterexample
that we have striven to keep as explicit as possible.

</details>


### [17] [Fractional Dirichlet problems with an overdetermined nonlocal Neumann condition](https://arxiv.org/abs/2507.13715)
*Michele Gatti,Julian Scheuer,Tobias Weth*

Main category: math.AP

TL;DR: The paper proves that a convex set with constant nonlocal normal derivative on a parallel external surface must be a ball, and extends this to fractional Laplacian problems.


<details>
  <summary>Details</summary>
Motivation: To explore symmetry and approximate symmetry in overdetermined problems for fractional torsion equations, generalizing results to broader contexts.

Method: Analyzes the fractional torsion equation in bounded sets, using nonlocal normal derivatives and assumptions like positive reach or convexity.

Result: Shows that under given conditions, the set must be a ball, and provides stability analysis under varying assumptions.

Conclusion: Extends symmetry results to fractional Laplacian problems, confirming robustness under convexity and other conditions.

Abstract: We investigate symmetry and quantitative approximate symmetry for an
overdetermined problem related to the fractional torsion equation in a regular
open, bounded set $\Omega \subseteq \mathbb{R}^n$. Specifically, we show that
if $\overline{\Omega}$ has positive reach and the nonlocal normal derivative
introduced in (Dipierro, Ros-Oton, Valdinoci, Rev. Mat. Iberoam. 33 (2017), no.
2, 377-416) is constant on an external surface parallel and sufficiently close
to $\partial \Omega$, then $\Omega$ must be a ball. Remarkably, this conclusion
remains valid under the sole assumption that $\Omega$ is convex. Moreover, we
analyze the quantitative stability of this result under two distinct sets of
assumptions on $\Omega$. Finally, we extend our analysis to a broader class of
overdetermined Dirichlet problems involving the fractional Laplacian.

</details>


### [18] [The periodic KdV with control on space-time measurable sets](https://arxiv.org/abs/2507.13740)
*Jingrui Niu,Ming Wang,Shengquan Xiang*

Main category: math.AP

TL;DR: Local exact controllability of the KdV equation on torus around equilibrium states is proven, using a novel observability inequality strategy.


<details>
  <summary>Details</summary>
Motivation: To address controllability of the KdV equation on torus, especially around equilibrium states, with spatial and temporal control regions of positive measure.

Method: A novel strategy for proving observability inequalities on space-time measurable sets.

Result: Local exact controllability is established for the KdV equation on torus.

Conclusion: The method is broadly applicable to dispersive equations on torus.

Abstract: In this paper, we establish the local exact controllability of the KdV
equation on torus around equilibrium states, where both the spatial control
region and the temporal control region are sets of positive measure. The proof
is based on a novel strategy for proving observability inequalities on
space-time measurable sets. This approach is applicable to a broad class of
dispersive equations on torus.

</details>


### [19] [Individual-Based Foundation of SIR-Type Epidemic Models: mean-field limit and large time behaviour](https://arxiv.org/abs/2507.13947)
*Giorgio Martalò,Giuseppe Toscani,Mattia Zanella*

Main category: math.AP

TL;DR: A kinetic framework models epidemic spread using Boltzmann-type equations for SIR compartments, linking microscopic interactions to macroscopic SIR-type dynamics. The system's equilibrium is analyzed via Fokker-Planck approximations and energy distance.


<details>
  <summary>Details</summary>
Motivation: To bridge kinetic theory and classical epidemic models by deriving macroscopic SIR dynamics from microscopic interaction rules.

Method: Uses Boltzmann-type equations for binary interactions between susceptible and infectious individuals, with linear redistribution for recovery and reinfection. Approximates dynamics via Fokker-Planck equations in the grazing collision regime.

Result: The model recovers SIR-type dynamics with reinfection, and the system's equilibrium is proven to converge in a Sobolev space using energy distance.

Conclusion: The framework connects kinetic theory with epidemic models, revealing the dissipative structure and equilibrium stability of the dynamics.

Abstract: We introduce a kinetic framework for modeling the time evolution of the
statistical distributions of the population densities in the three compartments
of susceptible, infectious, and recovered individuals, under epidemic spreading
driven by susceptible-infectious interactions. The model is based on a system
of Boltzmann-type equations describing binary interactions between susceptible
and infectious individuals, supplemented with linear redistribution operators
that account for recovery and reinfection dynamics. The mean values of the
kinetic system recover a SIR-type model with reinfection, where the macroscopic
parameters are explicitly derived from the underlying microscopic interaction
rules. In the grazing collision regime, the Boltzmann system can be
approximated by a system of coupled Fokker-Planck equations. This limit allows
for a more tractable analysis of the dynamics, including the large-time
behavior of the population densities. In this context, we rigorously prove the
convergence to equilibrium of the resulting mean-field system in a suitable
Sobolev space by means of the so-called energy distance. The analysis reveals
the dissipative structure of the dynamics and the role of the interaction terms
in driving the system toward a stable equilibrium configuration. These results
provide a multi-scale perspective connecting kinetic theory with classical
epidemic models.

</details>


### [20] [Scattering from analytic and piecewise analytic inhomogeneities](https://arxiv.org/abs/2507.13986)
*Narek Hovsepyan,Michael S. Vogelius*

Main category: math.AP

TL;DR: A technique for analyzing scattering of linear Helmholtz operator waves in 2D from regular inhomogeneities is developed and applied to examples.


<details>
  <summary>Details</summary>
Motivation: To understand and predict scattering behavior of incident waves from regular inhomogeneities in two dimensions.

Method: Develop a technique to analyze scattering for the linear Helmholtz operator, then apply it to specific examples.

Result: The technique successfully determines scattering for given incident waves from regular inhomogeneities.

Conclusion: The developed method is effective for studying scattering in 2D and can be applied to various examples.

Abstract: We study scattering for the linear Helmholtz operator in two dimensions and
develop a technique, which can be used to ascertain scattering of a given
incident wave from very regular inhomogeneities. This technique is then applied
to a number of interesting examples.

</details>


### [21] [On the non-uniqueness of locally minimizing clusters via singular cones](https://arxiv.org/abs/2507.13995)
*Lia Bronsard,Robin Neumayer,Michael Novack,Anna Skorobogatova*

Main category: math.AP

TL;DR: The paper constructs partitions of ℝⁿ into three sets that minimize interfacial area under volume-preserving variations and blow down to singular cones, proving non-uniqueness of the standard lens cluster in dimensions ≥8.


<details>
  <summary>Details</summary>
Motivation: To explore minimal interfacial area partitions and their behavior at infinity, addressing uniqueness questions in higher dimensions.

Method: Constructs partitions of ℝⁿ into three sets, analyzes their interfacial area minimization under compactly supported volume-preserving variations, and studies their blow-down behavior.

Result: Proves non-uniqueness of the standard lens cluster in dimensions starting from 8.

Conclusion: The study demonstrates the existence of non-unique minimal partitions in higher dimensions, challenging previous assumptions.

Abstract: We construct partitions of $\mathbb{R}^n$ into three sets
$\{\mathscr{X}(1),\mathscr{X}(2),\mathscr{X}(3)\}$ that locally minimize
interfacial area among compactly supported volume preserving variations and
that blow down at infinity to singular area-minimizing cones. As a consequence,
we prove the non-uniqueness of the standard lens cluster in a large number of
dimensions starting from $8$.

</details>


### [22] [Multiplicity of dead core solutions in indefinite elliptic problems](https://arxiv.org/abs/2507.14016)
*Vladimir Bobkov,Humberto Ramos Quoirin*

Main category: math.AP

TL;DR: The paper studies nonnegative solutions of indefinite elliptic problems with dead core phenomena, focusing on subhomogeneous problems. It proves multiple dead core solutions for large μ and extends results to other problem classes.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of nonnegative solutions in indefinite elliptic problems, particularly the dead core phenomenon, and to explore multiplicity and uniqueness of solutions.

Method: Analyzes the subhomogeneous problem using dead core formation properties and uniqueness features to derive exact multiplicity results for large μ.

Result: Multiple nonnegative dead core solutions are obtained for large μ, with an exact multiplicity result describing the solution set. The findings are extended to other problem classes.

Conclusion: The study provides a precise description of nonnegative solutions for large μ and generalizes the multiplicity results to broader problem types.

Abstract: We investigate nonnegative solutions of indefinite elliptic problems which
enjoy the dead core phenomenon. Our model is the subhomogeneous problem $$
-\Delta_p u = (a^+(x) - \mu a^-(x))|u|^{q-2}u, \quad u \in W_0^{1,p}(\Omega),
$$ where $\Omega$ is a bounded domain in $\mathbb{R}^N$, $1<q<p$, and $\mu >
0$. Thanks to a dead core formation property, we obtain multiple nonnegative
dead core solutions of this problem for $\mu$ large enough. This assertion, in
combination with a uniqueness feature, yields an exact multiplicity result,
which gives a precise description of the nonnegative solutions set of this
problem for large values of $\mu$. We also extend the multiplicity result to
other classes of problems.

</details>


### [23] [Well posedness and propagation of chaos for multi-agent models with strategies and diffusive effects](https://arxiv.org/abs/2507.14058)
*Alessandro Baldi,Marco Morandotti*

Main category: math.AP

TL;DR: A multi-agent model with strategies and diffusive effects is analyzed, proving well-posedness and convergence to a McKean-Vlasov equation as agent count increases.


<details>
  <summary>Details</summary>
Motivation: To model and analyze the behavior of multi-agent systems where agents have strategies and are influenced by spatial diffusion and interactions.

Method: Proposes a microscopic state description for agents (spatial position and probability measure) and governs evolution via non-local interactions and stochastic effects.

Result: Proves well-posedness of the multi-agent system and a McKean-Vlasov equation, and shows convergence (propagation of chaos) as agent count grows.

Conclusion: The model effectively links microscopic agent interactions to macroscopic behavior, validated by theoretical convergence results.

Abstract: A multi-agent model for individuals endowed with strategies and subject to
diffusive effects is proposed. The microscopic state of each agent is described
by a spatial position and a probability measure, interpreted as a mixed
strategy, over a compact metric space. The evolution is governed by a non-local
interaction mechanism and by stochastic effects acting on the spatial component
of the state. The well-posedness of the multi-agent system and that of a
certain McKean--Vlasov stochastic differential equation are proved. Eventually,
a propagation of chaos result is obtained, which guarantees that the former
model converges to the latter as the number of agents goes to infinity.

</details>


### [24] [A variational approach to the emergence of domain structures in magnetostrictive solids](https://arxiv.org/abs/2507.14091)
*Marco Bresciani,Manuel Friedrich*

Main category: math.AP

TL;DR: A variational model for magnetoelastic solids in large-strain settings is analyzed, focusing on magnetization and deformation. The study identifies a small-strain model for domain structures and magnetostriction using Γ-convergence.


<details>
  <summary>Details</summary>
Motivation: To understand the interplay between magnetization and deformation in magnetoelastic solids, especially in large-strain scenarios, and to derive a simplified model for small-strain settings.

Method: Simultaneous linearization of deformation and sharp-interface limit of magnetization using Γ-convergence, incorporating magnetic fields and uniaxial material properties.

Result: A variational model for small-strain magnetostriction and domain structure formation is derived, with ensured regularity of minimizers for uniaxial materials.

Conclusion: The study successfully bridges large-strain and small-strain behaviors in magnetoelastic solids, providing a framework for analyzing domain structures and magnetostriction.

Abstract: We consider a variational model for magnetoelastic solids in the large-strain
setting with the magnetization field defined on the unknown deformed
configuration. Through a simultaneous linearization of the deformation and
sharp-interface limit of the magnetization with respect to the easy axes,
performed in terms of $\Gamma$-convergence, we identify a variational model
describing the formation of domain structures and accounting for
magnetostriction in the small-strain setting. Our analysis incorporates the
effect of magnetic fields and, for uniaxial materials, ensures the regularity
of minimizers of the effective energy.

</details>


### [25] [Existence of a non-standard isoperimetric triple partition](https://arxiv.org/abs/2507.14112)
*Matteo Novaga,Emanuele Paolini,Vincenzo Maria Tortorelli*

Main category: math.AP

TL;DR: Existence of an isoperimetric 3-partition in ℝ⁸ with one finite and two infinite volumes, asymptotic to a singular minimal cone.


<details>
  <summary>Details</summary>
Motivation: To explore geometric partitioning in higher dimensions, particularly ℝ⁸, and understand asymptotic behavior relative to singular minimal cones.

Method: Mathematical analysis of isoperimetric partitions, focusing on properties like volume and asymptotic convergence to singular minimal cones.

Result: Demonstrated the existence of such a partition, highlighting the interplay between finite and infinite volumes in ℝ⁸.

Conclusion: The study provides insights into geometric partitioning in high-dimensional spaces and their asymptotic properties.

Abstract: We show existence of a isoperimetric $3$-partition of $\mathbb R^8$, with one
set of finite volume and two of infinite volume, which is asymptotic to a
singular minimal cone.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [26] [On the time integration for phase field modeling of grain growth in additive manufacturing](https://arxiv.org/abs/2507.13492)
*Chaoqian Yuan,Chinnapat Panwisawas,Ye Lu*

Main category: physics.comp-ph

TL;DR: The paper proposes stabilized time integration algorithms to accelerate phase field simulations in additive manufacturing by allowing larger time steps, ensuring stability and energy efficiency.


<details>
  <summary>Details</summary>
Motivation: Phase field simulations are computationally expensive due to small time steps needed for microstructure evolution in rapid solidification.

Method: Developed a stabilized time integration formulation and analyzed energy stability for a phase field model of rapid solidification.

Result: Achieved stable simulations with time steps two orders-of-magnitude larger than conventional methods, validated with 2D/3D simulations for 316L stainless steel.

Conclusion: The method enables efficient large-scale phase field modeling for additive manufacturing.

Abstract: Phase field simulations play a key role in the understanding of
microstructure evolution in additive manufacturing. However, they have been
found extremely computationally expensive. One of the reasons is the small time
step requirement to resolve the complex microstructure evolution during the
rapid solidification process. This paper investigates the possibility of using
a class of stabilized time integration algorithms to accelerate such phase
field simulations by increasing the time steps. The specific time integration
formulation and theoretical analysis on energy stability were developed, based
on a phase field model dedicated to simulating rapid solidification in additive
manufacturing. The numerical results confirmed that the proposed method can
ensure the numerical stability and a decreasing energy requirement for the
phase field simulations with at least two orders-of-magnitude larger time steps
over conventional explicit methods. 2D and 3D phase field simulations have been
conducted with relevant physical and kinetic parameters for 316L stainless
steels. This work provides a numerical framework for efficient phase field
simulations and open numerous opportunities for large scale phase field
modeling.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [27] [Scalable Chrysopoeia via $(n, 2n)$ Reactions Driven by Deuterium-Tritium Fusion Neutrons](https://arxiv.org/abs/2507.13461)
*Adam Rutkowski,Jake Harter,Jason Parisi*

Main category: physics.plasm-ph

TL;DR: A scalable method to synthesize stable gold from mercury using neutron reactions in a fusion blanket, enhancing fusion power plant economics.


<details>
  <summary>Details</summary>
Motivation: To find an economically viable method for large-scale gold production, leveraging fusion technology.

Method: Utilizes $(n, 2n)$ reactions in a neutron multiplier layer of a fusion blanket, driven by fast neutrons from deuterium-tritium fusion.

Result: Simulations show a tokamak can produce gold at 2 t/GW_th/yr, doubling fusion plant revenue.

Conclusion: This method improves fusion energy's economic viability by enabling gold production without hindering electricity generation.

Abstract: A scalable approach for chrysopoeia -- the transmutation of base metals into
gold -- has been pursued for millennia. While there have been small-scale
demonstrations in particle accelerators and proposals involving thermal neutron
capture, no economically attractive approach has yet been identified. We show a
new scalable method to synthesize stable gold (\ce{^{197}Au}) from the abundant
mercury isotope $\mathrm{^{198}Hg}$ using $(n, 2n)$ reactions in a specialized
neutron multiplier layer of a fusion blanket. Reactions are driven by fast
$14\, \mathrm{MeV}$ neutrons provided by a deuterium-tritium fusion plasma,
which are uniquely capable of enabling the desired reaction pathway at scale.
Crucially, the scheme identified here does not negatively impact electricity
production, and is also compatible with the challenging tritium breeding
requirements of fusion power plant design because $(n, 2n)$ reactions of
$\mathrm{^{198}Hg}$ drive both transmutation and neutron multiplication. Using
neutronics simulations, we demonstrate a tokamak with a blanket configuration
that can produce \ mathrm{^{197}Au} at a rate of about \SI{2}{t/GW_{th}/yr}.
Implementation of this concept allows fusion power plants to double the revenue
generated by the system, dramatically enhancing the economic viability of
fusion energy.

</details>


### [28] [A novel scheme for measuring the growth of Alfven wave parametric decay instability using counter-propagating waves](https://arxiv.org/abs/2507.13590)
*Feiyu Li,Seth Dorfman,Xiangrong Fu*

Main category: physics.plasm-ph

TL;DR: A novel experimental scheme is proposed to directly measure the parametric decay instability (PDI) of Alfven waves, validated via hybrid simulations and theoretical predictions.


<details>
  <summary>Details</summary>
Motivation: The lack of direct experimental measurements of PDI in Alfven waves, despite its fundamental importance in plasma physics, motivates this study.

Method: The scheme uses a large pump Alfven wave and a small counter-propagating seed wave, with frequency matching, to quantify PDI growth via damping reduction.

Result: Hybrid simulations confirm energy transfer from pump to seed reduces damping, serving as a reliable PDI proxy, with no excitation threshold.

Conclusion: This method is experimentally viable and critical for validating PDI theories in laboratory settings.

Abstract: The parametric decay instability (PDI) of Alfven waves -- where a pump Alfven
wave decays into a backward-propagating child Alfven wave and a forward ion
acoustic wave -- is a fundamental nonlinear wave-wave interaction and holds
significant implications for space and laboratory plasmas. However, to date
there has been no direct experimental measurement of PDI. Here, we propose a
novel and experimentally viable scheme to quantify the growth of Alfven wave
PDI on a linear device using a large pump Alfven wave and a small
counter-propagating seed Alfven wave, with the seed wave frequency tuned to
match the backward Alfven wave generated by standard PDI. Using hybrid
simulations, we show that energy transfer from the pump to the seed reduces the
latter's spatial damping. By comparing seed wave amplitudes with and without
the pump wave, this damping reduction can be used as a direct and reliable
proxy for PDI growth. The method is validated in our simulations across a range
of plasma and wave parameters and agrees well with theoretical predictions.
Notably, the scheme exhibits no threshold for PDI excitation and is, in
principle, readily implementable under current laboratory conditions. This
scheme is a critical step toward solving the challenge of experimentally
accessing Alfven wave PDI and provides an elegant method that may be used to
validate fundamental theories of parametric instabilities in controlled
laboratory settings.

</details>


### [29] [A Generalized Multinodal Model for Plasma Particle and Energy Transport](https://arxiv.org/abs/2507.13627)
*Zefang Liu,Weston M. Stacey*

Main category: physics.plasm-ph

TL;DR: A generalized multinodal model for simulating particle and energy transport in toroidal plasma configurations, offering flexibility for coupling with core-edge or core-pedestal simulations.


<details>
  <summary>Details</summary>
Motivation: To support burning plasma analysis and reactor-scale modeling by providing a flexible and modular framework for radial transport dynamics.

Method: Derives nodal balance equations by volume-averaging continuity and energy conservation equations across toroidal shell nodes, using linear diffusion laws for particle density and temperature.

Result: Effective particle and energy transport times are derived, enabling efficient implementation in reduced-order or integrated plasma simulations.

Conclusion: The model is compatible with data-driven approaches and facilitates modular implementation for plasma simulations.

Abstract: We present a generalized multinodal model for simulating particle and energy
transport in toroidal plasma configurations, developed to support burning
plasma analysis and reactor-scale modeling. Unlike fixed-node models, this
formulation allows an arbitrary number of nodes, offering increased flexibility
for coupling with core-edge or core-pedestal simulations. The model derives
nodal balance equations for each plasma species by volume-averaging the
continuity and energy conservation equations across toroidal shell nodes.
Particle and energy transport terms are expressed in terms of internodal
fluxes, linked to radial gradients via linear diffusion laws for particle
density and temperature, respectively. The resulting transport contributions
are characterized through effective particle and energy transport times,
derived explicitly in terms of nodal geometry and diffusivities. This
generalized framework facilitates efficient, modular implementation of radial
transport dynamics in reduced-order or integrated plasma simulations, and is
compatible with data-driven approaches such as NeuralPlasmaODE for model
calibration and inference from experimental data.

</details>


### [30] [Energy exchange between electrons and ions driven by ITG-TEM turbulence](https://arxiv.org/abs/2507.13692)
*T. Kato,H. Sugama,T. -H. Watanabe*

Main category: physics.plasm-ph

TL;DR: The paper investigates energy exchange between electrons and ions in ITG-TEM turbulence, revealing opposite energy transfer directions compared to ITG turbulence. It introduces a new method for predicting energy exchange.


<details>
  <summary>Details</summary>
Motivation: To understand the energy exchange mechanisms in ITG-TEM turbulence and develop predictive models for such processes.

Method: Gyrokinetic simulations are used to analyze energy exchange, focusing on parallel heating and perpendicular cooling. A quasilinear model and a new correlation-based method are tested.

Result: TEM turbulence transfers energy from electrons to ions, opposite to ITG turbulence. The new correlation method proves valid for predicting energy exchange.

Conclusion: The study clarifies energy transfer directions in ITG-TEM turbulence and validates a novel predictive method, enhancing understanding of turbulent energy dynamics.

Abstract: In this study, the energy exchange between electrons and ions in ITG TEM
turbulence is investigated using gyrokinetic simulations. The energy exchange
in TEM turbulence is primarily composed of the cooling of electrons associated
with perpendicular drift and the heating of ions moving parallel to magnetic
field lines. TEM turbulence facilitates energy transfer from electrons to ions,
which is opposite to the direction observed in ITG turbulence. In mixed ITG TEM
turbulence, the relative magnitudes of parallel heating and perpendicular
cooling for each species determine the overall direction and magnitude of
energy exchange. From the viewpoint of entropy balance, it is further confirmed
that energy flows from the species with larger entropy production, caused by
particle and heat fluxes, to the other species in ITG TEM turbulence. The
predictability of turbulent energy exchange in ITG-TEM turbulence by the
quasilinear model is examined. In addition, an alternative method based on the
correlation between energy flux and energy exchange is developed, and its
validity is demonstrated.

</details>


### [31] [Linear mode conversion theory of radio emission from turbulent solar wind plasmas](https://arxiv.org/abs/2507.13856)
*Catherine Krafft,Alexandr Volokitin*

Main category: physics.plasm-ph

TL;DR: The paper introduces a model for linear interactions between upper-hybrid wave turbulence and density fluctuations in solar wind plasma, covering processes like reflection, refraction, and wave coupling. It provides analytical and numerical solutions for electromagnetic field evolution and radiation rates, validated by weak turbulence theory.


<details>
  <summary>Details</summary>
Motivation: To understand and model the complex linear interactions between upper-hybrid waves and density fluctuations in solar wind plasma, including their impact on electromagnetic emissions.

Method: Develops a theoretical and numerical model, solves compact equations for electromagnetic fields analytically and numerically, and validates results using weak turbulence theory extended to inhomogeneous plasmas.

Result: Derives scaling laws for radiation rates based on plasma parameters like magnetization and electron thermal velocity, validating the model's hypotheses.

Conclusion: The work offers a new approach to analyze electromagnetic emissions in solar wind plasmas, linking wave and density turbulence spectra to radiation efficiency.

Abstract: This work presents a new theoretical and numerical model describing all
possible linear interactions between upper-hybrid wave turbulence and random
density fluctuations in a solar wind plasma; not only linear processes as wave
reflection, refraction, scattering, tunneling, trapping, or mode conversion at
constant frequency are taken into account, but also linear wave coupling,
interferences between scattered waves, etc. Compact equations describing the
time evolution of electromagnetic fields radiated in the $\mathcal{O}$,
$\mathcal{X}$ and $\mathcal{Z}$ modes by the current due to transformations of
upper-hybrid waves on density fluctuations, as well as the dispersion and
polarization properties of the modes, are determined analytically and solved
numerically, providing the time variations of electromagnetic energies and
corresponding radiation rates. Jointly, on the basis of these numerical results
that validate theoretical hypotheses, analytical calculations are conducted in
the framework of weak turbulence theory extended to randomly inhomogeneous
plasmas, that recover the main physical conclusions stated using the new model.
The dependencies of radiation rates on plasma parameters as the magnetization,
the electron thermal velocity and the average level of random density
fluctuations are determined in the form of scaling laws. This work opens a new
way to analyze the efficiency of electromagnetic emissions at plasma frequency
by realistic wave and density turbulence spectra interacting in solar wind
plasmas.

</details>


### [32] [Novel techniques of imaging interferometry analysis to study gas and plasma density for laser-plasma experiments](https://arxiv.org/abs/2507.13907)
*F. Filippi,M. Cipriani,S. Mastrostefano,M. Scisciò,F. Consoli*

Main category: physics.plasm-ph

TL;DR: The paper discusses the development of machine learning-based techniques for analyzing interferograms to measure gas and plasma density in high-repetition-rate laser-plasma experiments.


<details>
  <summary>Details</summary>
Motivation: The increasing demand for precise plasma characterization in high-repetition-rate experiments necessitates non-perturbative, efficient diagnostics like interferometry, which currently relies on manual analysis prone to errors.

Method: The authors are developing novel machine learning routines for interferogram analysis, including architectures and data acquisition methods for training and testing. Preliminary results use synthetic data.

Result: Preliminary results with synthetic data are presented, though the study is ongoing.

Conclusion: The goal is to create a fast, operator-independent diagnostic tool for plasma sources in high-repetition-rate experiments.

Abstract: Laser-plasma based experiments are always more demanding about the plasma
characteristics which need to be generated during the interaction. This is
valid for laser-plasma acceleration as well as for inertial confinement fusion
experiments. Most of these experiments are moving toward high repetition rate
operation regimes, making even more demanding the requests on the plasma
sources and the diagnostics to be implemented.\\ Interferometry is one of the
most used methods to characterize these sources, since it allows for
non-perturbative, single-shot measurements either of the neutral gas or the
plasma density. The design of the interferometric setup is non-trivial and
needs to be shaped on the actual conditions of the experiment. Similarly, the
analysis of the raw data is a complex task, prone to many sources of error and
dependent on the manual inputs.\\ In this work, we will present the techniques
we are developing for the analysis of the interferograms to measure both the
gas and plasma density. We will show the methods, the progress and the problems
we encountered in the development of novel routines of analysis based on
machine learning. The architectures and the methods to obtain data used for
training and testing them will be introduced. The study is ongoing and
preliminary results with synthetic data will be presented. The goal is to set
up a fast and operator independent diagnostic for the feedback of plasma
sources toward high repetition rate experiments.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [33] [Improving structure search with hyperspatial optimization and TETRIS seeding](https://arxiv.org/abs/2507.13791)
*Daviti Gochitashvili,Maxwell Meyers,Cindy Wang,Aleksey N. Kolmogorov*

Main category: cond-mat.mtrl-sci

TL;DR: The study extends the GOSH method to neural network potentials, tests its performance on nanoparticles and solids, and finds modest improvements for clusters but significant advantages for nanoalloys. Controlled bias in starting configurations proves more impactful.


<details>
  <summary>Details</summary>
Motivation: To improve global structure prediction by extending the GOSH method to more accurate neural network potentials and testing its efficiency on diverse systems.

Method: Extends GOSH to Behler-Parrinello NN potentials, integrates local minimization, and tests on nanoparticles, solids, and alloys. Uses TETRIS-inspired packing for better starting configurations.

Result: Four-dimensional optimization helps nanoalloys but offers modest gains for clusters. Controlled bias in starting configurations enhances search efficiency.

Conclusion: GOSH with NN potentials shows promise, but starting configuration bias is more impactful for global structure searches.

Abstract: Advanced structure prediction methods developed over the past decades include
an unorthodox strategy of allowing atoms to displace into extra dimensions. A
recently implemented global optimization of structures from hyperspace (GOSH)
has shown promise in accelerating the identification of global minima on
potential energy surfaces defined by simple interatomic models. In this study,
we extend the GOSH formalism to more accurate Behler-Parrinello neural network
(NN) potentials, make it compatible with efficient local minimization
algorithms, and test its performance on nanoparticles and crystalline solids.
For clusters modeled with NN potentials, four-dimensional optimization offers
fairly modest improvement in navigating geometric relaxation pathways and
incurs increased computational cost largely offsetting the benefit, but it
provides a significant advantage in facilitating atom swaps in nanoalloys. In
comparison, the introduction of a moderate, controlled bias for generating more
physically sensible starting configurations, achieved via TETRIS-inspired
packing of atomic blocks, has a more direct impact on the efficiency of global
structure searches. The benchmarked systems are Lennard-Jones clusters, Au or
Cu-Pd-Ag nanoparticles and binary Sn alloys described by NN potentials, and
compounds with covalent B or BC frameworks modeled with density functional
theory

</details>


<div id='physics.space-ph'></div>

# physics.space-ph [[Back]](#toc)

### [34] [Occurrence of Non-Stationarity at Earth's Quasi-Perpendicular Bow Shock](https://arxiv.org/abs/2507.13817)
*Ajay Lotekar,Yuri V. Khotyaintsev,Daniel B. Graham,Andrew Dimmock,Andreas Johlander,Ahmad Lalti*

Main category: physics.space-ph

TL;DR: The paper analyzes non-stationary behavior in Earth's quasi-perpendicular bow shock, finding ion phase-space holes in 65% of cases, peaking at 70% for high Mach numbers.


<details>
  <summary>Details</summary>
Motivation: To understand the non-stationary behavior of collisionless shocks and the role of ion phase-space holes in shock self-reformation.

Method: Statistical analysis of 521 crossings of Earth's quasi-perpendicular bow shock to detect ion phase-space holes.

Result: Phase-space holes appear in 65% of cases, with a peak occurrence of 70% for shocks with high Alfvén Mach numbers.

Conclusion: Earth's quasi-perpendicular bow shock is predominantly non-stationary, with phase-space holes as a key signature.

Abstract: Collisionless shocks can exhibit non-stationary behavior even under steady
upstream conditions, forming a complex transition region. Ion phase-space
holes, linked to shock self-reformation and surface ripples, are a signature of
this non-stationarity. We statistically analyze their occurrence using 521
crossings of Earth's quasi-perpendicular bow shock. Phase-space holes appear in
65% of cases, though the actual rate may be higher as the holes may not be
resolved during fast shock crossings. The occurrence rate peaks at 70% for
shocks with Alfv\'en Mach numbers $M_A>7$. These findings suggest that Earth's
quasi-perpendicular bow shock is predominantly non-stationary.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [35] [Graph Neural Network Surrogates for Contacting Deformable Bodies with Necessary and Sufficient Contact Detection](https://arxiv.org/abs/2507.13459)
*Vijay K. Dubey,Collin E. Haese,Osman Gültekin,David Dalton,Manuel K. Rausch,Jan N. Fuhg*

Main category: cs.CE

TL;DR: A graph neural network (GNN) architecture is introduced for surrogate modeling of nonlinear boundary value problems involving soft deformable body contact, offering improved generalization and inference speedups, albeit with high training costs.


<details>
  <summary>Details</summary>
Motivation: Existing surrogate models for contact problems are limited to rigid bodies or simple soft-rigid interactions, lacking sufficient conditions for soft deformable body contact.

Method: The proposed GNN uses continuous collision detection and incorporates sufficient conditions for soft body contact, tested on benchmarks including bioprosthetic aortic valve mechanics.

Result: The method shows better generalization with added contact terms in the loss function and achieves up to a thousand-fold inference speedup, though training is computationally expensive.

Conclusion: The GNN framework effectively addresses soft deformable body contact with significant inference speedups, but the high training cost presents a trade-off.

Abstract: Surrogate models for the rapid inference of nonlinear boundary value problems
in mechanics are helpful in a broad range of engineering applications. However,
effective surrogate modeling of applications involving the contact of
deformable bodies, especially in the context of varying geometries, is still an
open issue. In particular, existing methods are confined to rigid body contact
or, at best, contact between rigid and soft objects with well-defined contact
planes. Furthermore, they employ contact or collision detection filters that
serve as a rapid test but use only the necessary and not sufficient conditions
for detection. In this work, we present a graph neural network architecture
that utilizes continuous collision detection and, for the first time,
incorporates sufficient conditions designed for contact between soft deformable
bodies. We test its performance on two benchmarks, including a problem in soft
tissue mechanics of predicting the closed state of a bioprosthetic aortic
valve. We find a regularizing effect on adding additional contact terms to the
loss function, leading to better generalization of the network. These benefits
hold for simple contact at similar planes and element normal angles, and
complex contact at differing planes and element normal angles. We also
demonstrate that the framework can handle varying reference geometries.
However, such benefits come with high computational costs during training,
resulting in a trade-off that may not always be favorable. We quantify the
training cost and the resulting inference speedups on various hardware
architectures. Importantly, our graph neural network implementation results in
up to a thousand-fold speedup for our benchmark problems at inference.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [36] [Liquid Drop Model for Nuclear Matter in the Low Density Limit](https://arxiv.org/abs/2507.14012)
*Rupert L. Frank,Mathieu Lewin,Robert Seiringer*

Main category: math-ph

TL;DR: The paper analyzes the ground state energy in a liquid drop model with a positive background, proving a two-term asymptotics in the dilute limit and linking optimal configurations to the Jellium problem.


<details>
  <summary>Details</summary>
Motivation: To rigorously derive the ground state energy per unit volume in the dilute limit and validate the expected droplet configurations.

Method: Using the liquid drop model with a positive background density, the study employs thermodynamic limit analysis to derive energy asymptotics.

Result: Optimal configurations consist of unit-sized droplets arranged as per Jellium problem minimizers, confirming the gnocchi phase in astrophysics.

Conclusion: The work provides the first rigorous derivation of the gnocchi phase, linking droplet configurations to the Jellium problem.

Abstract: We consider the liquid drop model with a positive background density in the
thermodynamic limit. We prove a two-term asymptotics for the ground state
energy per unit volume in the dilute limit. Our proof justifies the expectation
that optimal configurations consist of droplets of unit size that arrange
themselves according to minimizers for the Jellium problem for point particles.
In particular, we provide the first rigorous derivation of what is known as the
gnocchi phase in astrophysics.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [37] [Quantum Wave Atom Transforms](https://arxiv.org/abs/2507.10739)
*Marianna Podzorova,Yi-Kai Liu*

Main category: quant-ph

TL;DR: A quantum algorithm for wavelet packet transforms is introduced, outperforming classical methods in efficiency and applicability.


<details>
  <summary>Details</summary>
Motivation: To leverage quantum computing for wavelet and wave atom transforms, enabling faster and more versatile solutions for partial differential equations.

Method: The algorithm uses an efficient representation for a broader class of tree structures, achieving polynomial gate complexity.

Result: The quantum implementation has $O(\mathrm{poly}(n))$ gate complexity, compared to classical $O(n 2^n)$ operations, and enhances quantum PDE solvers.

Conclusion: This quantum algorithm expands the scope of wavelet transforms and improves computational efficiency for PDE solutions.

Abstract: This paper constructs the first quantum algorithm for wavelet packet
transforms with a tree structure, sometimes called wave atom transforms.
Classically, wave atoms are used to construct sparse representations of
differential operators, which enable fast numerical algorithms for partial
differential equations. Compared to previous work, our quantum algorithm can
implement a larger class of wavelet and wave atom transforms, by using an
efficient representation for a larger class of possible tree structures. Our
quantum implementation has $O(\mathrm{poly}(n))$ gate complexity for the
transform of dimension $2^n$, while classical implementations have $O(n 2^n)$
floating point operations. The result can be used to improve existing quantum
algorithms for solving hyperbolic partial differential equations.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [38] [Expansive Natural Neural Gradient Flows for Energy Minimization](https://arxiv.org/abs/2507.13475)
*Wolfgang Dahmen,Wuchen Li,Yuankai Teng,Zhu Wang*

Main category: math.OC

TL;DR: The paper introduces expansive gradient dynamics in deep neural network-induced mapping spaces, focusing on minimizing energy functionals in a Hilbert space setting. It proposes a projection gradient descent method and an expanding strategy for neural networks, demonstrating efficacy in supervised learning and inverse problems.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop tools for minimizing energy functionals in abstract Hilbert spaces, applicable to PDE-based inverse problems and supervised learning, by leveraging neural network structures.

Method: The method involves a Hilbert space metric in diffeomorphism mapping spaces, a projection gradient descent approach, and an expanding strategy for neural networks to align natural gradient directions with ideal Hilbert space gradients.

Result: The strategy proves effective for simple model problems in supervised learning, model reduction, and inverse problems, emphasizing the importance of neural flow matrices based on Hilbert space inner products.

Conclusion: The paper concludes with a promising approach for gradient dynamics in neural networks, with broader implications for future detailed analysis.

Abstract: This paper develops expansive gradient dynamics in deep neural
network-induced mapping spaces. Specifically, we generate tools and concepts
for minimizing a class of energy functionals in an abstract Hilbert space
setting covering a wide scope of applications such as PDEs-based inverse
problems and supervised learning. The approach hinges on a Hilbert space metric
in the full diffeomorphism mapping space, which could be viewed as a
generalized Wasserstein-2 metric. We then study a projection gradient descent
method within deep neural network parameterized sets. More importantly, we
develop an adaptation and expanding strategy to step-by-step enlarge the deep
neural network structures. In particular, the expansion mechanism aims to
enhance the alignment of the neural manifold induced natural gradient direction
as well as possible with the ideal Hilbert space gradient descent direction
leveraging the fact that we can evaluate projections of the Hilbert space
gradient. We demonstrate the efficacy of the proposed strategy for several
simple model problems for energies arising in the context of supervised
learning, model reduction, or inverse problems. In particular, we highlight the
importance of assembling the neural flow matrix based on the inner product for
the ambient Hilbert space. The actual algorithms are the simplest
specifications of a broader spectrum based on a correspondingly wider
discussion, postponing a detailed analysis to forthcoming work.

</details>


### [39] [Gradient descent avoids strict saddles with a simple line-search method too](https://arxiv.org/abs/2507.13804)
*Andreea-Alexandra Muşat,Nicolas Boumal*

Main category: math.OC

TL;DR: The paper guarantees avoidance of strict saddle points for gradient descent (GD) with a modified Armijo backtracking line-search, even with large initial step sizes, and extends this to Riemannian GD (RGD) under analytic retraction.


<details>
  <summary>Details</summary>
Motivation: Existing guarantees for GD with line-search methods lacked avoidance of strict saddle points, especially without globally Lipschitz gradients. The paper addresses this gap.

Method: A modified Armijo backtracking line-search method is proposed for GD, and the analysis is extended to RGD with real analytic retraction.

Result: The method ensures avoidance of strict saddle points for GD and RGD, even with large initial step sizes, without requiring globally Lipschitz gradients.

Conclusion: The paper provides theoretical guarantees for GD and RGD with line-search methods, improving upon previous limitations and extending applicability.

Abstract: It is known that gradient descent (GD) on a $C^2$ cost function generically
avoids strict saddle points when using a small, constant step size. However, no
such guarantee existed for GD with a line-search method. We provide one for a
modified version of the standard Armijo backtracking method with generic,
arbitrarily large initial step size. In contrast to previous works, our
analysis does not require a globally Lipschitz gradient.
  We extend this to the Riemannian setting (RGD), assuming the retraction is
real analytic (though the cost function still only needs to be $C^2$). In
closing, we also improve guarantees for RGD with a constant step size in some
scenarios.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [40] [On-the-Fly Fine-Tuning of Foundational Neural Network Potentials: A Bayesian Neural Network Approach](https://arxiv.org/abs/2507.13805)
*Tim Rensmeyer,Denis Kramer,Oliver Niggemann*

Main category: cs.LG

TL;DR: The paper proposes a Bayesian neural network-based fine-tuning approach for machine learning force fields, enabling automated on-the-fly learning to reduce training data needs and detect rare events.


<details>
  <summary>Details</summary>
Motivation: The computational burden of generating diverse training datasets for interatomic force fields, especially for rare events or large configuration spaces, motivates the need for efficient fine-tuning methods.

Method: The authors introduce a Bayesian neural network method for fine-tuning foundation models, coupled with an on-the-fly workflow to automate training dataset creation and uncertainty quantification.

Result: The approach maintains pre-specified accuracy, reduces training data requirements, and detects rare events like transition states, sampling them more frequently.

Conclusion: The proposed method offers a practical solution for fine-tuning foundation models in force field applications, addressing challenges in uncertainty quantification and rare event detection.

Abstract: Due to the computational complexity of evaluating interatomic forces from
first principles, the creation of interatomic machine learning force fields has
become a highly active field of research. However, the generation of training
datasets of sufficient size and sample diversity itself comes with a
computational burden that can make this approach impractical for modeling rare
events or systems with a large configuration space. Fine-tuning foundation
models that have been pre-trained on large-scale material or molecular
databases offers a promising opportunity to reduce the amount of training data
necessary to reach a desired level of accuracy. However, even if this approach
requires less training data overall, creating a suitable training dataset can
still be a very challenging problem, especially for systems with rare events
and for end-users who don't have an extensive background in machine learning.
In on-the-fly learning, the creation of a training dataset can be largely
automated by using model uncertainty during the simulation to decide if the
model is accurate enough or if a structure should be recalculated with
classical methods and used to update the model. A key challenge for applying
this form of active learning to the fine-tuning of foundation models is how to
assess the uncertainty of those models during the fine-tuning process, even
though most foundation models lack any form of uncertainty quantification. In
this paper, we overcome this challenge by introducing a fine-tuning approach
based on Bayesian neural network methods and a subsequent on-the-fly workflow
that automatically fine-tunes the model while maintaining a pre-specified
accuracy and can detect rare events such as transition states and sample them
at an increased rate relative to their occurrence.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [41] [Localized FNO for Spatiotemporal Hemodynamic Upsampling in Aneurysm MRI](https://arxiv.org/abs/2507.13789)
*Kyriakos Flouris,Moritz Halter,Yolanne Y. R. Lee,Samuel Castonguay,Luuk Jacobs,Pietro Dirix,Jonathan Nestmann,Sebastian Kozerke,Ender Konukoglu*

Main category: cs.CV

TL;DR: LoFNO enhances hemodynamic analysis by improving spatiotemporal resolution and predicting WSS directly from clinical data, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Low spatiotemporal resolution and signal-to-noise ratio in magnetic resonance flow imaging limit its diagnostic utility for aneurysm rupture prediction.

Method: Proposes LoFNO, a 3D architecture integrating Laplacian eigenvectors for geometric priors and EDSR for upsampling, enhancing resolution and WSS prediction.

Result: LoFNO outperforms interpolation and other deep learning methods in velocity and WSS predictions.

Conclusion: LoFNO enables more precise cerebrovascular diagnostics by improving hemodynamic analysis.

Abstract: Hemodynamic analysis is essential for predicting aneurysm rupture and guiding
treatment. While magnetic resonance flow imaging enables time-resolved
volumetric blood velocity measurements, its low spatiotemporal resolution and
signal-to-noise ratio limit its diagnostic utility. To address this, we propose
the Localized Fourier Neural Operator (LoFNO), a novel 3D architecture that
enhances both spatial and temporal resolution with the ability to predict wall
shear stress (WSS) directly from clinical imaging data. LoFNO integrates
Laplacian eigenvectors as geometric priors for improved structural awareness on
irregular, unseen geometries and employs an Enhanced Deep Super-Resolution
Network (EDSR) layer for robust upsampling. By combining geometric priors with
neural operator frameworks, LoFNO de-noises and spatiotemporally upsamples flow
data, achieving superior velocity and WSS predictions compared to interpolation
and alternative deep learning methods, enabling more precise cerebrovascular
diagnostics.

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [42] [The role of three-dimensional effects on ion injection and acceleration in perpendicular shocks](https://arxiv.org/abs/2507.13436)
*Luca Orusa,Damiano Caprioli,Lorenzo Sironi,Anatoly Spitkovsky*

Main category: astro-ph.HE

TL;DR: The paper investigates particle acceleration at non-relativistic perpendicular shocks using 2D and 3D hybrid simulations, highlighting the importance of 3D for capturing ion injection and magnetic turbulence effects.


<details>
  <summary>Details</summary>
Motivation: To understand the conditions enabling particle acceleration at non-relativistic collisionless shocks, crucial for explaining cosmic ray origins.

Method: Uses 2D and 3D hybrid simulations (kinetic ions, fluid electrons) to study shock drift acceleration, focusing on Mach number dependence and ion injection.

Result: Efficient acceleration occurs only in 3D due to downstream magnetic turbulence 'porosity,' which affects ion injection. High resolution is needed to resolve small-scale turbulence.

Conclusion: High-resolution 3D simulations are essential for accurately modeling particle acceleration at perpendicular shocks.

Abstract: Understanding the conditions that enable particle acceleration at
non-relativistic collisionless shocks is essential to unveil the origin of
cosmic rays. We employ 2D and 3D hybrid simulations (with kinetic ions and
fluid electrons) to explore particle acceleration and magnetic field
amplification in non-relativistic perpendicular shocks, focusing on the role of
shock drift acceleration and its dependence on the shock Mach number. We
perform an analysis of the ion injection process and demonstrate why efficient
acceleration is only observed in 3D. In particular, we show that ion injection
critically depends on the "porosity" of the magnetic turbulence in the
downstream region near the shock, a property describing how easily the
post-shock region allows particles to traverse it and return upstream without
being trapped. This effect can only be properly captured in 3D. Additionally,
we explore the impact of numerical resolution on ion energization, highlighting
how resolving small-scale turbulence -- on scales below the thermal ion
gyroradius -- is essential for accurately modeling particle injection. Overall,
our results emphasize the necessity of high-resolution 3D simulations to
capture the fundamental microphysics driving particle acceleration at
perpendicular shocks.

</details>


### [43] [Reconnection-driven Flares in M87*: Proton-synchrotron Powered GeV Emission](https://arxiv.org/abs/2507.14002)
*Hayk Hakobyan,Amir Levinson,Lorenzo Sironi,Alexander Philippov,Bart Ripperda*

Main category: astro-ph.HE

TL;DR: The paper explores proton synchrotron radiation as the source of GeV emission in black hole accretion flows, complementing pair synchrotron and inverse Compton mechanisms.


<details>
  <summary>Details</summary>
Motivation: To explain the GeV emission observed by Fermi in M87 flares, which cannot be accounted for by electron-positron pairs alone.

Method: Combines analytic estimates with 3D radiative particle-in-cell simulations of pair-proton plasmas.

Result: Protons, though fewer, dominate the energy budget and produce 5-20% of total dissipation as GeV emission via synchrotron radiation.

Conclusion: Proton synchrotron radiation is key to explaining GeV emission, while pairs dominate MeV and TeV emissions.

Abstract: Magnetic reconnection in current layers that form intermittently in
radiatively inefficient accretion flows onto black holes is a promising
mechanism for particle acceleration and high-energy emission. It has been
recently proposed that such layers, arising during flux eruption events, can
power the rapid TeV flares observed from the core of M87. In this scenario,
inverse Compton scattering of soft radiation from the accretion flow by
energetic electron-positron pairs produced near the reconnection layer was
suggested as the primary emission mechanism. However, detailed calculations
show that radiation from pairs alone cannot account for the GeV emission
detected by the Fermi observatory. In this work, we combine analytic estimates
with 3D radiative particle-in-cell simulations of pair-proton plasmas to show
that the GeV emission can be naturally explained by synchrotron radiation from
protons accelerated in the current sheet. Although the exact proton content of
the layer is uncertain, our model remains robust across a broad range of
proton-to-pair number density ratios. While protons are subdominant in number
compared to pairs, our simulations demonstrate that they can be accelerated
more efficiently, leading to a self-regulated steady state in which protons
dominate the energy budget. Ultimately, proton synchrotron emission accounts
for approximately 5% to 20% of the total dissipation power. The majority is
radiated as MeV photons via pair synchrotron emission, with a smaller fraction
emitted as TeV photons through inverse Compton scattering.

</details>
