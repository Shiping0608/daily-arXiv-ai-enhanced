<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 13]
- [math.AP](#math.AP) [Total: 23]
- [physics.comp-ph](#physics.comp-ph) [Total: 2]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 3]
- [cond-mat.supr-con](#cond-mat.supr-con) [Total: 1]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [cs.ET](#cs.ET) [Total: 1]
- [astro-ph.SR](#astro-ph.SR) [Total: 1]
- [eess.IV](#eess.IV) [Total: 1]
- [math.DS](#math.DS) [Total: 1]
- [cond-mat.soft](#cond-mat.soft) [Total: 2]
- [math.DG](#math.DG) [Total: 1]
- [math.OC](#math.OC) [Total: 2]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [astro-ph.CO](#astro-ph.CO) [Total: 1]
- [math-ph](#math-ph) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Structure-preserving deflation of critical eigenvalues in quadratic eigenvalue problems associated with damped mass-spring systems](https://arxiv.org/abs/2507.16024)
*Rafikul Alam,Volker Mehrmann,Ninoslav Truhar*

Main category: math.NA

TL;DR: This paper presents structure-preserving deflation strategies for critical eigenvalues (infinite, zero, and imaginary axis eigenvalues) in quadratic matrix polynomials from damped mass-spring systems, using trimmed structure-preserving linearization to improve numerical stability analysis.


<details>
  <summary>Details</summary>
Motivation: Critical eigenvalues (∞, 0, and those on the imaginary axis) lie on the boundary of stable eigenvalue regions in damped mass-spring systems, making numerical computations and robust stability analysis challenging. There is a need for efficient methods to handle these problematic eigenvalues before computing other eigenvalues and eigenvectors.

Method: The authors develop structure-preserving deflation strategies that use trimmed structure-preserving linearization to project the quadratic matrix polynomial to a lower dimensional subspace. This approach deflates critical eigenvalues while maintaining the underlying mathematical structure of the system. They also analyze the impact of parametric damping matrices on purely imaginary eigenvalues.

Result: The paper successfully demonstrates deflation strategies for critical eigenvalues in quadratic matrix polynomials, with specific applications to hyperbolic problems. The methods preserve the structural properties of the original system while enabling more stable numerical computations of the remaining eigenvalues and eigenvectors.

Conclusion: Structure-preserving deflation via trimmed linearization provides an effective approach for handling critical eigenvalues in damped mass-spring systems, improving both numerical stability and computational efficiency for eigenvalue analysis. The method is particularly valuable for hyperbolic problems and systems with parametric damping.

Abstract: For a quadratic matrix polynomial associated with a damped mass-spring system
there are three types of critical eigenvalues, the eigenvalues $\infty$ and $0$
and the eigenvalues on the imaginary axis. All these are on the boundary of the
set of (robustly) stable eigenvalues. For numerical methods, but also for
(robust) stability analysis, it is desirable to deflate such eigenvalues by
projecting the matrix polynomial to a lower dimensional subspace before
computing the other eigenvalues and eigenvectors. We describe
structure-preserving deflation strategies that deflate these eigenvalues via a
trimmed structure-preserving linearization. We employ these results for the
special case of hyperbolic problems. We also analyze the effect of a (possibly
low rank) parametric damping matrix on purely imaginary eigenvalues.

</details>


### [2] [Diff-ANO: Towards Fast High-Resolution Ultrasound Computed Tomography via Conditional Consistency Models and Adjoint Neural Operators](https://arxiv.org/abs/2507.16344)
*Xiang Cao,Qiaoqiao Ding,Xinliang Liu,Lei Zhang,Xiaoqun Zhang*

Main category: math.NA

TL;DR: This paper introduces Diff-ANO, a framework that combines conditional consistency models with adjoint neural operators to solve Ultrasound Computed Tomography (USCT) inverse problems more efficiently by replacing traditional PDE solvers with neural surrogates and enabling few-step diffusion sampling.


<details>
  <summary>Details</summary>
Motivation: Traditional USCT approaches using diffusion generative priors face three key challenges: complex PDE-constrained gradient computation, discretization errors, and computational imbalance between neural networks and numerical PDE solvers. These limitations hinder effective integration of diffusion priors for regularization in nonlinear inverse USCT problems.

Method: The method introduces two main innovations: (1) a conditional consistency model that learns self-consistent mappings from diffusion trajectories for measurement-conditional few-step sampling, and (2) an adjoint operator learning module that replaces traditional PDE solvers with neural operator surrogates for efficient gradient computation. Additionally, they propose batch-based Convergent Born Series (BCBS) for memory-efficient online generation of neural operator training pairs.

Result: Comprehensive experiments show that Diff-ANO significantly improves both computational efficiency and reconstruction quality compared to traditional approaches, with particularly strong performance in challenging sparse-view and partial-view measurement scenarios.

Conclusion: Diff-ANO successfully addresses the fundamental challenges of integrating diffusion generative priors in USCT by combining conditional consistency models with adjoint neural operators, resulting in improved efficiency and reconstruction quality, especially for limited measurement scenarios.

Abstract: Ultrasound Computed Tomography (USCT) constitutes a nonlinear inverse problem
with inherent ill-posedness that can benefit from regularization through
diffusion generative priors. However, traditional approaches for solving
Helmholtz equation-constrained USCT face three fundamental challenges when
integrating these priors: PDE-constrained gradient computation,
discretization-induced approximation errors, and computational imbalance
between neural networks and numerical PDE solvers. In this work, we introduce
\textbf{Diff-ANO} (\textbf{Diff}usion-based Models with \textbf{A}djoint
\textbf{N}eural \textbf{O}perators), a novel framework that combines
conditional consistency models with adjoint operator learning to address these
limitations. Our two key innovations include: (1) a \textit{conditional
consistency model} that enables measurement-conditional few-step sampling by
directly learning a self-consistent mapping from diffusion trajectories, and
(2) an \textit{adjoint operator learning} module that replaces traditional PDE
solvers with neural operator surrogates for efficient adjoint-based gradient
computation. To enable practical deployment, we introduce the batch-based
Convergent Born Series (BCBS)--a memory-efficient strategy for online
generation of neural operator training pairs. Comprehensive experiments
demonstrate that Diff-ANO significantly improves both computational efficiency
and reconstruction quality, especially under sparse-view and partial-view
measurement scenarios.

</details>


### [3] [Neural Network Acceleration of Iterative Methods for Nonlinear Schrödinger Eigenvalue Problems](https://arxiv.org/abs/2507.16349)
*Daniel Peterseim,Jan-F. Pietschmann,Jonas Püschel,Kilian Ruess*

Main category: math.NA

TL;DR: A neural network-based approach is developed to accelerate iterative methods for solving nonlinear Schrödinger eigenvalue problems, demonstrating significant speedup over classical solvers in quantum mechanics applications like rotating Bose-Einstein condensates.


<details>
  <summary>Details</summary>
Motivation: Conventional solvers for nonlinear eigenvector problems in quantum mechanics suffer from slow convergence, particularly in extreme parameter regimes such as rotating Bose-Einstein condensate problems, necessitating more efficient computational approaches.

Method: The approach employs neural networks to predict and refine solution trajectories for nonlinear Schrödinger eigenvalue problems, utilizing knowledge gained from previous simulations to enhance both convergence speed and solution accuracy.

Result: Numerical experiments show significant speed-up compared to classical iterative solvers when applied to nonlinear Schrödinger eigenvalue problems, while also revealing both the capabilities and limitations of the neural network-based approach.

Conclusion: Neural networks can effectively accelerate iterative methods for solving nonlinear Schrödinger eigenvalue problems by leveraging prior simulation knowledge, offering a promising computational tool for quantum mechanics applications despite having certain limitations.

Abstract: We present a novel approach to accelerate iterative methods to solve
nonlinear Schr\"odinger eigenvalue problems using neural networks. Nonlinear
eigenvector problems are fundamental in quantum mechanics and other fields, yet
conventional solvers often suffer from slow convergence in extreme parameter
regimes, as exemplified by the rotating Bose- Einstein condensate (BEC)
problem. Our method uses a neural network to predict and refine solution
trajectories, leveraging knowledge from previous simulations to improve
convergence speed and accuracy. Numerical experiments demonstrate significant
speed-up over classical solvers, highlighting both the strengths and
limitations of the approach.

</details>


### [4] [Entropic approximations of the semigeostrophic shallow water equations](https://arxiv.org/abs/2507.16415)
*Jean-David Benamou,Colin J. Cotter,Jacob J. M. Francis,Hugo Malamut*

Main category: math.NA

TL;DR: This paper develops a discretization method for semigeostrophic rotating shallow water equations using optimal transport theory, employing Moreau-Yoshida regularization of the Wasserstein metric and entropy regularization to create a computationally tractable numerical scheme.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop a numerical discretization scheme for the semigeostrophic rotating shallow water equations that leverages their optimal transport formulation, addressing the computational challenges of solving these important geophysical fluid dynamics equations.

Method: The method involves: (1) formulating the problem using optimal transport theory with Moreau-Yoshida regularization of the Wasserstein metric, (2) introducing entropy regularization to the rotating shallow water equations, (3) discretizing by replacing measures with weighted sums of Dirac measures, (4) approximating the L2 norm of layer depth for potential energy, and (5) developing an iterative optimization method to solve the discrete problem.

Result: The authors successfully propose an iterative method for solving the discrete optimization problem, analyze its convergence properties, demonstrate the method numerically, and apply it to solve time-dependent shallow water problems in numerical examples.

Conclusion: The paper presents a novel computational approach that combines optimal transport theory with entropy regularization to effectively discretize and solve semigeostrophic rotating shallow water equations, providing a mathematically rigorous and numerically viable method for these important geophysical flow problems.

Abstract: We develop a discretisation of the semigeostrophic rotating shallow water
equations, based upon their optimal transport formulation. This takes the form
of a Moreau-Yoshida regularisation of the Wasserstein metric. Solutions of the
optimal transport formulation provide the shallow water layer depth represented
as a measure, which is itself the push forward of an evolving measure under the
semigeostrophic coordinate transformation. First, we propose and study an
entropy regularised version of the rotating shallow water equations. Second, we
discretise the regularised problem by replacing both measures with weighted
sums of Dirac measures, and approximate the (squared) L2 norm of the layer
depth, which defines the potential energy. We propose an iterative method to
solve the discrete optimisation problem relating the two measures, and analyse
its convergence. The iterative method is demonstrated numerically and applied
to the solution of the time-dependent shallow water problem in numerical
examples.

</details>


### [5] [On finite precision block Lanczos computations](https://arxiv.org/abs/2507.16484)
*Dorota Šimonová,Petr Tichý*

Main category: math.NA

TL;DR: This paper extends Greenbaum's 1989 finite precision analysis from the single-vector Lanczos algorithm to the block Lanczos algorithm, showing that finite precision block Lanczos results can be interpreted as exact results from a perturbed larger matrix.


<details>
  <summary>Details</summary>
Motivation: The motivation is to extend the mathematical understanding of finite precision effects in numerical algorithms from the well-established single-vector Lanczos case to the more complex block Lanczos algorithm, providing theoretical foundations for analyzing computational errors in block methods.

Method: The authors generalize Greenbaum's continuation process to the block setting, constructing careful perturbations that allow the finite precision block Lanczos algorithm to be completed in finite iterations. They derive sufficient conditions for keeping perturbations small and develop a mathematical model where block tridiagonal matrices from finite precision computations correspond to exact block Lanczos applied to a larger model matrix.

Result: The paper successfully extends the theoretical framework to block Lanczos, derives sufficient conditions for small perturbations (though whether these can always be satisfied remains an open question), and provides numerical experiments that demonstrate practical implementation and empirically validate the theoretical conditions.

Conclusion: The work establishes a mathematical model for finite precision block Lanczos computations analogous to Greenbaum's single-vector case, though the question of universal satisfiability of the sufficient conditions in the block case remains unresolved, representing an important open problem in numerical linear algebra.

Abstract: In her seminal 1989 work, Greenbaum demonstrated that the results produced by
the finite precision Lanczos algorithm after $k$ iterations can be interpreted
as exact Lanczos results applied to a larger matrix, whose eigenvalues lie in
small intervals around those of the original matrix. This establishes a
mathematical model for finite precision Lanczos computations. In this paper, we
extend these ideas to the block Lanczos algorithm. We generalize the
continuation process and show that it can be completed in a finite number of
iterations using carefully constructed perturbations. The block tridiagonal
matrices produced after $k$ iterations can then be interpreted as arising from
the exact block Lanczos algorithm applied to a larger model matrix. We derive
sufficient conditions under which the required perturbations remain small,
ensuring that the eigenvalues of the model matrix stay close to those of the
original matrix. While in the single-vector case these conditions are always
satisfiable, as shown by Greenbaum based on results by Paige, the question of
whether they can always be satisfied in the block case remains open. Finally,
we present numerical experiments demonstrating a practical implementation of
the continuation process and empirically assess the validity of the sufficient
conditions and the size of the perturbations.

</details>


### [6] [Neumann series of Bessel functions in direct and inverse spherically symmetric transmission eigenvalue problems](https://arxiv.org/abs/2507.16554)
*Vladislav V. Kravchenko,L. Estefania Murcia-Lozano,Nikolaos Pallikarakis*

Main category: math.NA

TL;DR: This paper introduces a novel Neumann Series of Bessel Functions (NSBF) methodology to solve both direct and inverse transmission eigenvalue problems in spherically symmetric domains with variable refractive index, achieving high accuracy for real and complex eigenvalues without prior assumptions on the refractive index contrast.


<details>
  <summary>Details</summary>
Motivation: The transmission eigenvalue problem (TEP) is central to inverse scattering theory, but numerical solutions for direct and inverse TEP in spherically symmetric domains with variable refractive index covering both real and complex eigenvalues remain challenging despite theoretical advances.

Method: The authors reformulate the TEP as a Sturm-Liouville equation via Liouville transformation, then expand its characteristic function in a Neumann Series of Bessel Functions (NSBF) with coefficients computed by recursive integration. For direct problems, eigenvalues are found by root finding on truncated NSBF partial sums. For inverse problems, they use a two-step approach: recovering the transformed interval length δ from spectral data using NSBF-based algorithm, then reconstructing the refractive index n(r) by solving a linear system for NSBF coefficients. A spectrum completion technique is also implemented for limited eigenvalue data.

Result: The method demonstrates high accuracy with only a few coefficients for various examples. Numerical examples confirm the method's robustness and accuracy across a wide range of refractive indices, with no a priori assumptions required on δ or the sign of the contrast 1-n(r).

Conclusion: The NSBF methodology provides an effective and robust solution for both direct and inverse transmission eigenvalue problems in spherically symmetric domains, offering high accuracy without restrictive assumptions and successfully handling both real and complex eigenvalues across diverse refractive index profiles.

Abstract: The transmission eigenvalue problem (TEP) plays a central role in inverse
scattering theory. Despite substantial theoretical progress, the numerical
solution of direct and inverse TEP in spherically symmetric domains with
variable refractive index covering real and complex eigenvalues remains
challenging. This study introduces a novel Neumann Series of Bessel Functions
(NSBF) methodology to address this challenge. After reformulating the TEP as a
Sturm-Liouville equation via a Liouville transformation, we expand its
characteristic function in an NSBF whose coefficients are computed by simple
recursive integration. In the direct problem, eigenvalues real or complex are
found by root finding on a truncated NSBF partial sum, yielding high accuracy
with a few coefficients, as demonstrated with various examples. For the inverse
problem, we develop a two-step approach: first, recovering the transformed
interval length $\delta$ from spectral data via a new NSBF-based algorithm, and
second, reconstructing the refractive index $n(r)$ by solving a linear system
for the first NSBF coefficients. A spectrum completion technique is also
implemented to complete the spectrum and solve the corresponding inverse
problem when eigenvalue data is limited. Numerical examples confirm the
method's robustness and accuracy across a wide range of refractive indices,
with no a priori assumptions on $\delta$ or the sign of the contrast $1-n(r)$.

</details>


### [7] [Data-Driven Adaptive Gradient Recovery for Unstructured Finite Volume Computations](https://arxiv.org/abs/2507.16571)
*G. de Romémont,F. Renac,F. Chinesta,J. Nunez,D. Gueyffier*

Main category: math.NA

TL;DR: A novel data-driven approach using modified DeepONet architecture enhances gradient reconstruction in unstructured finite volume methods for 2D Euler equations, achieving 20-60% accuracy improvements over traditional second-order schemes while maintaining physical constraints and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional second-order finite volume schemes for hyperbolic conservation laws on unstructured meshes have limited accuracy, particularly in shock-dominated regions. There is a need to extend structured-grid machine learning methodologies to unstructured meshes while maintaining physical consistency and conservation properties.

Method: Modified DeepONet architecture that incorporates local mesh geometry and topology for rotation invariance, enforces first-order constraints, and uses physics-informed regularization including entropy penalization, total variation diminishing penalization, and parameter regularization. Training on high-fidelity datasets from sine waves and randomized piecewise constant initial conditions with periodic boundaries.

Result: 20-60% improvement in solution accuracy compared to traditional second-order finite volume schemes, enhanced computational efficiency enabling high-fidelity simulations on coarser grids, improved mesh convergence rates, and successful validation on challenging geometry configurations from literature test cases.

Conclusion: The proposed ML-enhanced finite volume method successfully combines machine learning tools with traditional numerical schemes, delivering superior accuracy and efficiency while preserving essential stability and conservation properties for hyperbolic conservation laws, representing a new generation of physics-constrained hybrid solvers.

Abstract: We present a novel data-driven approach for enhancing gradient reconstruction
in unstructured finite volume methods for hyperbolic conservation laws,
specifically for the 2D Euler equations. Our approach extends previous
structured-grid methodologies to unstructured meshes through a modified
DeepONet architecture that incorporates local geometry in the neural network.
The architecture employs local mesh topology to ensure rotation invariance,
while also ensuring first-order constraint on the learned operator. The
training methodology incorporates physics-informed regularization through
entropy penalization, total variation diminishing penalization, and parameter
regularization to ensure physically consistent solutions, particularly in
shock-dominated regions. The model is trained on high-fidelity datasets
solutions derived from sine waves and randomized piecewise constant initial
conditions with periodic boundary conditions, enabling robust generalization to
complex flow configurations or geometries. Validation test cases from the
literature, including challenging geometry configuration, demonstrates
substantial improvements in accuracy compared to traditional second-order
finite volume schemes. The method achieves gains of 20-60% in solution accuracy
while enhancing computational efficiency. A convergence study has been conveyed
and reveal improved mesh convergence rates compared to the conventional solver.
The proposed algorithm is faster and more accurate than the traditional
second-order finite volume solver, enabling high-fidelity simulations on
coarser grids while preserving the stability and conservation properties
essential for hyperbolic conservation laws. This work is a part of a new
generation of solvers that are built by combining Machine-Learning (ML) tools
with traditional numerical schemes, all while ensuring physical constraint on
the results.

</details>


### [8] [A Conservative and Positivity-Preserving Discontinuous Galerkin Method for the Population Balance Equation](https://arxiv.org/abs/2507.16631)
*Ziyao Xu,Guanyang Liu,Yong-Tao Zhang*

Main category: math.NA

TL;DR: A conservative, positivity-preserving discontinuous Galerkin method for population balance equations that maintains number and mass conservation while ensuring positive particle densities through a novel moment-conserving limiter.


<details>
  <summary>Details</summary>
Motivation: Existing methods for population balance equations struggle to simultaneously maintain conservation properties (number and mass) and preserve positivity of particle number densities, especially for aggregation-breakage processes where standard limiters are not directly applicable due to local mass corresponding to first moments rather than zeroth moments.

Method: A discontinuous Galerkin scheme with standard treatment for growth and nucleation, novel symmetric double-integral discretization for aggregation and breakage using common refinement and quadrature rules, and a custom moment-conserving limiter that preserves first moments while enforcing nonnegativity across the domain.

Result: The method successfully achieves number conservation in growth, mass conservation in aggregation and breakage, and maintains positivity of number density. Numerical results demonstrate accuracy, conservation properties, and robustness of the proposed approach.

Conclusion: This is the first positivity-preserving algorithm that conserves a prescribed moment for population balance equations, providing a robust solution for modeling particle distribution dynamics while maintaining essential physical properties.

Abstract: We develop a conservative, positivity-preserving discontinuous Galerkin (DG)
method for the population balance equation (PBE), which models the distribution
of particle numbers across particle sizes due to growth, nucleation,
aggregation, and breakage. To ensure number conservation in growth and mass
conservation in aggregation and breakage, we design a DG scheme that applies
standard treatment for growth and nucleation, and introduces a novel
discretization for aggregation and breakage. The birth and death terms are
discretized in a symmetric double-integral form, evaluated using a common
refinement of the integration domain and carefully selected quadrature rules.
Beyond conservation, we focus on preserving the positivity of the number
density in aggregation-breakage. Since local mass corresponds to the first
moment, the classical Zhang-Shu limiter, which preserves the zeroth moment
(cell average), is not directly applicable. We address this by proving the
positivity of the first moment on each cell and constructing a
moment-conserving limiter that enforces nonnegativity across the domain. To our
knowledge, this is the first work to develop a positivity-preserving algorithm
that conserves a prescribed moment. Numerical results verify the accuracy,
conservation, and robustness of the proposed method.

</details>


### [9] [A quasi-Monte Carlo multiscale method for the wave propagation in random media](https://arxiv.org/abs/2507.16647)
*Panchi Li,Zhiwen Zhang*

Main category: math.NA

TL;DR: This paper develops a boundary-corrected multiscale method combined with quasi-Monte Carlo sampling to accurately solve the Helmholtz equation with random refractive index, achieving superconvergence in physical space and first-order convergence in random space.


<details>
  <summary>Details</summary>
Motivation: The need to accurately simulate wave propagation problems (Helmholtz equation) in bounded regions with uncertain/random material properties (refractive index), particularly when dealing with Robin boundary conditions that also contain randomness.

Method: A boundary-corrected multiscale method for spatial discretization combined with quasi-Monte Carlo (qMC) sampling for handling stochastic variables. The random refractive index is represented using a truncated infinite series parameterized by stochastic variables.

Result: The method achieves superconvergence rates in physical space (O(H^4) for L^2-error and O(H^2) for V-error) and almost first-order convergence in random space due to the qMC method. Wavenumber explicit convergence analysis is provided.

Conclusion: The proposed boundary-corrected multiscale method with qMC sampling provides an accurate and efficient numerical approach for solving Helmholtz problems with random refractive index, with theoretical convergence guarantees validated by numerical experiments.

Abstract: In this paper, we propose and analyze an accurate numerical approach to
simulate the Helmholtz problem in a bounded region with a random refractive
index, where the random refractive index is denoted using an infinite series
parameterized by stochastic variables. To calculate the statistics of the
solution numerically, we first truncate the parameterized model and adopt the
quasi-Monte Carlo (qMC) method to generate stochastic variables. We develop a
boundary-corrected multiscale method to discretize the truncated problem, which
allows us to accurately resolve the Robin boundary condition with randomness.
The proposed method exhibits superconvergence rates in the physical space
(theoretical analysis suggests $\mathcal{O}(H^4)$ for $L^2$-error and
$\mathcal{O}(H^2)$ for a defined $V$-error). Owing to the employment of the qMC
method, it also exhibits almost the first-order convergence rate in the random
space. We provide the wavenumber explicit convergence analysis and conduct
numerical experiments to validate key features of the proposed method.

</details>


### [10] [A $\star$-Product Approach for Analytical and Numerical Solutions of Nonautonomous Linear Fractional Differential Equations](https://arxiv.org/abs/2507.16652)
*Fabio Durastante,Pierre-Louis Giscard,Stefano Pozza*

Main category: math.NA

TL;DR: This paper introduces a novel method for solving nonautonomous linear ordinary fractional differential equations using the $\star$-product (a generalization of Volterra convolution) combined with discretization techniques, which can also yield closed-form solutions in certain cases.


<details>
  <summary>Details</summary>
Motivation: The need for effective solution methods for nonautonomous linear ordinary fractional differential equations, which are challenging to solve due to their time-varying coefficients and fractional nature.

Method: The approach reformulates the analytical solution using the $\star$-product (a generalization of the Volterra convolution) followed by appropriate discretization of the resulting expression. The $\star$-formalism framework is employed to enable systematic solution derivation.

Result: The method successfully provides a novel solution approach for nonautonomous linear ordinary fractional differential equations. In certain cases, the $\star$-formalism enables the derivation of closed-form solutions, demonstrating the practical utility of the framework.

Conclusion: The $\star$-product based approach offers a promising and versatile framework for solving nonautonomous linear ordinary fractional differential equations, with the added benefit of potentially yielding closed-form solutions in specific scenarios, highlighting its utility for this class of problems.

Abstract: This article presents a novel solution method for nonautonomous linear
ordinary fractional differential equations. The approach is based on
reformulating the analytical solution using the $\star$-product, a
generalization of the Volterra convolution, followed by an appropriate
discretization of the resulting expression. Additionally, we demonstrate that,
in certain cases, the $\star$-formalism enables the derivation of closed-form
solutions, further highlighting the utility of this framework.

</details>


### [11] [Time integration of dissipative stochastic PDEs](https://arxiv.org/abs/2507.16658)
*Helena Biščević,Raffaele D'Ambrosio*

Main category: math.NA

TL;DR: This paper studies numerical methods for stochastic reaction-diffusion problems, focusing on preserving mean-square dissipativity properties when using finite difference spatial discretization and stochastic θ-methods/θ-IMEX methods for time integration.


<details>
  <summary>Details</summary>
Motivation: The need to develop numerical methods that can preserve important mathematical properties (specifically mean-square dissipativity) when solving stochastic reaction-diffusion equations, which are important in modeling various physical and biological phenomena with random effects.

Method: The authors employ finite difference methods for spatial discretization and analyze stochastic θ-methods and stochastic θ-IMEX (implicit-explicit) methods for time integration, with particular focus on how spatial and temporal step sizes affect the conservation of mean-square dissipativity.

Result: The analysis demonstrates that stochastic θ-methods and stochastic θ-IMEX methods can effectively conserve mean-square dissipativity, with the conservation properties depending on the choice of spatial and temporal step sizes. Numerical experiments validate the theoretical findings.

Conclusion: Stochastic θ-methods and θ-IMEX methods are effective approaches for solving stochastic reaction-diffusion problems while preserving mean-square dissipativity, with proper selection of spatial and temporal discretization parameters being crucial for maintaining these conservative properties.

Abstract: The paper is focused on the numerical solution of stochastic
reaction-diffusion problems. A special attention is addressed to the
conservation of mean-square dissipativity in the time integration of the
spatially discretized problem, obtained by means of finite differences. The
analysis highlights the conservative ability of stochastic $\theta$-methods and
stochastic $\theta$-IMEX methods, emphasizing the roles of spatial and temporal
stepsizes. A selection of numerical experiments is provided, confirming the
theoretical expectations.

</details>


### [12] [Deep Unfolding Network for Nonlinear Multi-Frequency Electrical Impedance Tomography](https://arxiv.org/abs/2507.16678)
*Giovanni S. Alberti,Damiana Lazzaro,Serena Morigi,Luca Ratti,Matteo Santacesaria*

Main category: math.NA

TL;DR: This paper presents a novel variational network that combines graph neural networks (GNNs) with the Proximal Regularized Gauss Newton (PRGN) framework for multi-frequency Electrical Impedance Tomography (mfEIT), enabling better tissue conductivity estimation by preserving mesh structure and capturing inter-frequency correlations.


<details>
  <summary>Details</summary>
Motivation: Multi-frequency Electrical Impedance Tomography (mfEIT) is a promising biomedical imaging modality for estimating tissue conductivities across different frequencies, but existing methods face challenges in accurately reconstructing tissue properties while maintaining computational efficiency and interpretability.

Method: The authors develop a variational network that integrates graph neural networks (GNNs) within the iterative Proximal Regularized Gauss Newton (PRGN) framework. The approach unrolls the PRGN algorithm where each iteration becomes a network layer, combining classical iterative reconstruction with deep learning while preserving the irregular triangular mesh structure used in the nonlinear forward model.

Result: The proposed method successfully leverages physical insights from nonlinear model fitting alongside GNN's ability to capture inter-frequency correlations, enabling accurate reconstruction of overlapping tissue fraction concentrations while maintaining the mesh structure integrity.

Conclusion: The novel variational network effectively merges the interpretability of classical iterative reconstruction methods with the power of deep learning through GNNs, providing a model-based learning paradigm that addresses the challenges in multi-frequency EIT imaging for biomedical applications.

Abstract: Multi-frequency Electrical Impedance Tomography (mfEIT) represents a
promising biomedical imaging modality that enables the estimation of tissue
conductivities across a range of frequencies. Addressing this challenge, we
present a novel variational network, a model-based learning paradigm that
strategically merges the advantages and interpretability of classical iterative
reconstruction with the power of deep learning. This approach integrates graph
neural networks (GNNs) within the iterative Proximal Regularized Gauss Newton
(PRGN) framework. By unrolling the PRGN algorithm, where each iteration
corresponds to a network layer, we leverage the physical insights of nonlinear
model fitting alongside the GNN's capacity to capture inter-frequency
correlations. Notably, the GNN architecture preserves the irregular triangular
mesh structure used in the solution of the nonlinear forward model, enabling
accurate reconstruction of overlapping tissue fraction concentrations.

</details>


### [13] [The inverse initial data problem for anisotropic Navier-Stokes equations via Legendre time reduction method](https://arxiv.org/abs/2507.16810)
*Cong B. Van,Thuy T. Le,Loc H. Nguyen*

Main category: math.NA

TL;DR: A computational method using Legendre time reduction to reconstruct initial velocity fields in compressible anisotropic Navier-Stokes equations from boundary observations, converting the time-dependent inverse problem into a solvable elliptic system.


<details>
  <summary>Details</summary>
Motivation: The need to reconstruct initial velocity fields in fluid dynamics when direct measurements of internal fluid states are unavailable, particularly for applications involving anisotropic media where traditional inverse modeling approaches may be insufficient.

Method: Novel computational framework based on Legendre time reduction that projects velocity fields onto exponentially weighted Legendre basis in time, transforming the time-dependent inverse problem into a coupled time-independent elliptic system solved iteratively using Picard iteration and stabilized least-squares formulation.

Result: Numerical experiments in 2D demonstrate accurate and robust reconstruction of initial velocity fields even with significant measurement noise and complex anisotropic structures, showing the method's effectiveness under challenging conditions.

Conclusion: The Legendre time reduction approach provides a flexible and computationally tractable alternative for inverse modeling in fluid dynamics with anisotropic media, successfully handling noisy boundary data and complex flow structures.

Abstract: We consider the inverse initial data problem for the compressible anisotropic
Navier-Stokes equations, where the goal is to reconstruct the initial velocity
field from lateral boundary observations. This problem arises in applications
where direct measurements of internal fluid states are unavailable. We
introduce a novel computational framework based on Legendre time reduction,
which projects the velocity field onto an exponentially weighted Legendre basis
in time. This transformation reduces the original time-dependent inverse
problem to a coupled, time-independent elliptic system. The resulting reduced
model is solved iteratively using a Picard iteration and a stabilized
least-squares formulation under noisy boundary data. Numerical experiments in
two dimensions confirm that the method accurately and robustly reconstructs
initial velocity fields, even in the presence of significant measurement noise
and complex anisotropic structures. This approach offers a flexible and
computationally tractable alternative for inverse modeling in fluid dynamics
with anisotropic media.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [14] [Asymptotic behavior at infinity and existence of solutions to the Lagrangian mean curvature flow equation in $\mathbb R^{n+1}_-$](https://arxiv.org/abs/2507.16129)
*Jiguang Bao,Zixiao Liu*

Main category: math.AP

TL;DR: This paper studies ancient solutions to Lagrangian mean curvature flow, proving exponential convergence to quadratic polynomials at infinity and solving Dirichlet problems with prescribed asymptotic behavior in all dimensions n≥2 without restrictive conditions on matrix A.


<details>
  <summary>Details</summary>
Motivation: The paper aims to understand the asymptotic behavior of ancient solutions to Lagrangian mean curvature flow equation at infinity, particularly under conditions that allow for Liouville type rigidity theorems, and to establish connections between rigidity results, asymptotic analysis, and boundary value problems.

Method: The authors use asymptotic analysis techniques to study classical solutions under Liouville type rigidity conditions, derive explicit convergence rates for exponential convergence to quadratic polynomials, and develop methods to solve Dirichlet type problems with prescribed asymptotic behavior that work in all dimensions without requiring positive definiteness of matrix A.

Result: Every classical solution converges exponentially to a quadratic polynomial of the form τt + (1/2)x'Ax + bx + c at infinity with an explicitly derived convergence rate. The approach successfully handles Dirichlet type problems in all dimensions n≥2 without requiring matrix A to be positive definite or close to a scalar multiple of the identity matrix.

Conclusion: The paper establishes fundamental relationships among Liouville type rigidity, asymptotic analysis at infinity, and boundary value problems (both Dirichlet and initial value types) for Lagrangian mean curvature flow, providing a comprehensive framework that works in general dimensions with relaxed conditions on the governing matrix.

Abstract: This paper investigates the asymptotic behavior at infinity of ancient
solutions to the Lagrangian mean curvature flow equation. Under conditions that
admit Liouville type rigidity theorems, we prove that every classical solution
converges exponentially to a quadratic polynomial of form $\tau
t+\frac{1}{2}x'Ax+bx+c$ at infinity, with an explicitly derived convergence
rate. Furthermore, we investigate Dirichlet type problems with prescribed
asymptotic behavior, featuring two key innovations: applicability to all
dimensions $n\geq 2$, and no requirement for the matrix $A$ to be positive
definite or close to a scalar multiple of the identity matrix. These results
establish the relationship among Liouville type rigidity, asymptotic analysis
at infinity, and boundary value problems (including Dirichlet type and initial
value type).

</details>


### [15] [Analysis of a three-dimensional rapidly rotating convection model without thermal diffusion](https://arxiv.org/abs/2507.16143)
*Chongsheng Cao,Yanqiu Guo,Edriss S. Titi*

Main category: math.AP

TL;DR: This paper studies a 3D rapidly rotating convection model without thermal diffusion, proving global existence and uniqueness of weak solutions and well-posedness of strong solutions by first analyzing a regularized model with thermal diffusion and then taking the vanishing diffusivity limit.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand three-dimensional rapidly rotating convection with tall columnar structures in the challenging case where thermal diffusion is absent, which introduces significant analytical difficulties compared to models with thermal diffusion.

Method: The authors use a regularization approach: they first study a regularized model that includes thermal diffusion, establish delicate estimates that are independent of the thermal diffusion coefficient, and then rigorously justify the vanishing diffusivity limit to obtain results for the original model without thermal diffusion.

Result: The paper establishes global existence and uniqueness of weak solutions, as well as Hadamard well-posedness of global strong solutions for the three-dimensional rapidly rotating convection model without thermal diffusion.

Conclusion: The authors successfully overcome the analytical challenges posed by the absence of thermal diffusion through their regularization technique, providing a rigorous mathematical foundation for understanding rapidly rotating convection systems with tall columnar structures.

Abstract: We study a three-dimensional rapidly rotating convection model featuring tall
columnar structures, in the absence of thermal diffusion. We establish the
global existence and uniqueness of weak solutions, as well as the Hadamard
well-posedness of global strong solutions to this model. The lack of thermal
diffusion introduces significant challenges in the analysis. To overcome these
challenges, we first investigate the regularized model with thermal diffusion
and establish delicate estimates that are independent of the thermal diffusion
coefficient, and consequently justify the vanishing diffusivity limit. This
work serves as a continuation of our previous paper [6].

</details>


### [16] [Suppression of blow-up in 3-D Keller-Segel system with fractional diffusion via Couette flow in whole space](https://arxiv.org/abs/2507.16160)
*Shijin Deng,Binbin Shi,Weike Wang,Yucheng Wang*

Main category: math.AP

TL;DR: This paper studies a Keller-Segel model with fractional diffusion in 3D space under Couette flow, proving that sufficiently strong background flow prevents solution blow-up and ensures global existence with decay rates.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand how background Couette flow affects the behavior of solutions in a fractional Keller-Segel model, particularly whether strong flow can prevent the typical blow-up phenomenon and ensure solution stability in three-dimensional space.

Method: The main method involves analyzing Green's functions and obtaining crucial L¹ estimates without singularities at t=0. The authors overcome technical difficulties from the fractional heat kernel and Couette flow interaction by introducing a novel space-frequency mixed decomposition technique.

Result: The paper establishes that when the background Couette flow is sufficiently large, the enhanced dissipation prevents solution blow-up, guaranteeing global existence of solutions. Additionally, they derive time decay rates for solutions in Lᵖ norm.

Conclusion: Large background Couette flow induces sufficient dissipation enhancement to stabilize the fractional Keller-Segel system in 3D, preventing blow-up and ensuring global well-posedness with quantified decay behavior.

Abstract: In this paper, we consider a Keller-Segel model with a fractional diffusion
term in $\mathbb{R}^3$ in the background of a Couette flow. We show that when
the background Couette flow is large enough, the dissipation enhancement
induced could prevent the blow-up of solutions and thus prove the global
existence and also obtain time decay rates of the solution in $L^p$ norm. The
main tool of the proof is a corresponding Green's function and the key estimate
is its $L^1$ estimate without singularities at $t=0$. To fulfill such an
estimate, we meet great troubles caused by the fractional heat kernel together
with the Couette flow in the model considered here and overcome the troubles by
introducing a space-frequency mixed decomposition.

</details>


### [17] [Sovability of curvature equations with multiple singular sources on torus via Painleve VI equations](https://arxiv.org/abs/2507.16230)
*Zhijie Chen,Ting-Jung Kuo,Chang-Shou Lin*

Main category: math.AP

TL;DR: This paper studies a critical curvature equation with multiple singular sources on a torus and establishes a connection with Painlevé VI equations to determine when even solutions exist based on the location of singular points.


<details>
  <summary>Details</summary>
Motivation: The existence of solutions for the curvature equation with multiple singular sources on a torus is a long-standing problem in the critical case where apriori estimates do not hold, making it challenging to prove solution existence.

Method: The authors establish a deep connection between the curvature equation and Painlevé VI equations, using this relationship to analyze the existence of even solutions (satisfying u(z)=u(-z)) based on the position of singular points.

Result: The existence of even solutions depends on the location of the singular point p, and the authors provide a sharp criterion for determining this location in terms of Painlevé VI equations.

Conclusion: By connecting the curvature equation to Painlevé VI equations, the paper resolves the existence question for even solutions and provides precise conditions based on singular point locations, advancing understanding of this critical case problem.

Abstract: We study the curvature equation with multiple singular sources on a torus
\[\Delta u+e^{u}=8\pi \sum_{k=0}^{3}n_{k}\delta_{\frac{\omega_{k}}{2}}% +4\pi
\left( \delta_{p}+\delta_{-p}\right) \quad \text{ on
}\;E_{\tau}:=\mathbb{C}/(\mathbb Z+\mathbb{Z}\tau),\] where $n_k\in\mathbb N$
and $\delta_a$ denotes the Dirac measure at $a$. This is known as a critical
case for which the apriori estimate does not hold, and the existence of
solutions has been a long-standing problem. In this paper, by establishing a
deep connection with Painlev\'{e} VI equations, we show that the existence of
even solutions (i.e. $u(z)=u(-z)$) depends on the location of the singular
point $p$, and we give a sharp criterion of $p$ in terms of Painlev\'{e} VI
equations.

</details>


### [18] [Local well-posedness and asymptotic analysis of a nonlocal incompressible Navier--Stokes--Korteweg system](https://arxiv.org/abs/2507.16295)
*Jeongho Kim,Jaeyong Shin*

Main category: math.AP

TL;DR: This paper studies a relaxed version of the Navier-Stokes-Korteweg system where the classical capillarity term is replaced with a nonlocal approximation, proving well-posedness and convergence properties that justify using nonlocal models for capillarity-driven fluid flows.


<details>
  <summary>Details</summary>
Motivation: The classical third-order capillarity term in the Navier-Stokes-Korteweg system is challenging to handle mathematically and computationally. The authors are motivated to develop and rigorously analyze a nonlocal relaxation approach that can serve as a more tractable approximation for studying capillarity-driven incompressible fluid flows.

Method: The authors replace the classical third-order capillarity term with a nonlocal approximation in the inhomogeneous incompressible Navier-Stokes-Korteweg system. They analyze the well-posedness of this relaxed system under standard regularity and positivity assumptions, then study two key asymptotic limits: the nonlocal-to-local limit and the vanishing capillarity limit.

Result: The paper establishes local-in-time well-posedness of the relaxed system with existence time uniform with respect to both the capillarity coefficient and relaxation parameter. They prove convergence of solutions in both asymptotic limits: as the relaxation parameter tends to infinity (recovering the local system) and in the vanishing capillarity limit.

Conclusion: The analysis provides rigorous mathematical justification for using nonlocal relaxation models as approximations for capillarity-driven incompressible fluid flows. The uniform existence time and proven convergence properties validate the nonlocal approach as a reliable computational and theoretical tool.

Abstract: We consider a relaxed formulation of the inhomogeneous incompressible
Navier--Stokes--Korteweg system, where the classical third-order capillarity
term is replaced by a nonlocal approximation. We first establish the
local-in-time well-posedness of the relaxed system, under standard regularity
and positivity assumptions on the initial data. The existence time is uniform
with respect to both the capillarity coefficient and the relaxation parameter.
We then study two asymptotic limits of the system: the nonlocal-to-local limit
as the relaxation parameter tends to infinity, and the vanishing capillarity
limit. In each case, we prove convergence of the solution to that of the
corresponding target system. Our analysis provides a rigorous justification for
the use of nonlocal relaxation models in approximating capillarity-driven
incompressible fluid flows.

</details>


### [19] [Sharp Boundary Growth Rate Estimate of the Singular Equation $-Δ u=u^{-γ}$ in a Critical Cone](https://arxiv.org/abs/2507.16319)
*Leyun Wu,Chilin Zhang*

Main category: math.AP

TL;DR: This paper studies the sharp boundary growth rate estimates for solutions to the singular Lane-Emden-Fowler equation -Δu = u^(-γ) in critical C^(1,1) epigraphical cones, revealing fundamentally different behaviors for three parameter ranges and resolving an open question about solvability conditions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to obtain sharp boundary growth rate estimates for solutions to the singular Lane-Emden-Fowler equation in critical geometric domains (C^(1,1) epigraphical cones) and to resolve an open question about the necessity and sufficiency of solvability conditions from previous work.

Method: The authors introduce a sequence of reference points p_k = (16^(1-k))/2 * e_n to control solution values in the domain Ω = Cone_Σ ∩ B_1. They use Green function representation to derive a discrete integral equation for the sequence a_k = 16^(kφ)U(p_k), converting the PDE problem into a recursion that can be analyzed using basic ODE methods.

Result: The paper establishes that growth rate estimates exhibit fundamentally different behaviors in three cases: 1 < γ < 2, γ = 2, and γ > 2. They obtain sharp growth rate estimates near the origin for γ > 1 and prove that the additional solvability condition from previous work is both necessary and sufficient. They also derive optimal modulus of continuity for solutions.

Conclusion: The study successfully characterizes the sharp boundary behavior of solutions to the singular Lane-Emden-Fowler equation in critical domains, completely resolving the main open question from prior research and providing a comprehensive understanding of how the parameter γ affects solution behavior in different regimes.

Abstract: For $\gamma>0$, we study the sharp boundary growth rate estimate of solutions
to the Dirichlet problem of the singular Lane-Emden-Fowler equation
\begin{equation*}
  -\Delta u=u^{-\gamma} \end{equation*} in a critical $C^{1,1}$ epigraphical
cone $Cone_{\Sigma}$.
  We show that the growth rate estimate exhibits fundamentally different
behaviors in the following three cases: $1<\gamma<2$, $\gamma=2$, and
$\gamma>2$. Moreover, we obtain the sharp growth rate estimate near the origin
for $\gamma>1$. As a consequence, we show that when $Cone_{\Sigma}$ is a
$C^{1,1}$ epigraphical cone, the additional solvability condition in
\cite[Theorem 1.3]{GuLiZh25} is both sufficient and necessary to achieve the
growth rate therein, thereby resolving the main open question left in that
paper. With the growth rate estimate, we also derive the optimal modulus of
continuity for solutions via the interior Schauder estimate.
  Our approach is to control the values of a solution $U(x)$ in the region
$\Omega=Cone_{\Sigma}\cap B_{1}$ by introducing a sequence of reference points
$p_{k}=\frac{16^{1-k}}{2}\vec{e_{n}}$. From the Green function representation
of $U(x)$, we derive a discrete integral equation for the sequence
$a_{k}=16^{k\phi}U(p_{k})$. Such a computation converts the original PDE
problem into a recursion for a discrete integral equation, which can be
effectively analyzed using basic ODE methods.

</details>


### [20] [Bounded $H^\infty$-calculus for vectorial-valued operators with Gaussian kernel estimates](https://arxiv.org/abs/2507.16368)
*Davide Addona,Vincenzo Leone,Luca Lorenzi,Abdelaziz Rhandi*

Main category: math.AP

TL;DR: This paper proves that vector-valued generators of bounded holomorphic semigroups with Gaussian kernel estimates and bounded H∞-calculus in L² extend to bounded H∞-calculus for all Lᵖ spaces (1<p<∞), with applications to elliptic operators with matrix-valued potentials.


<details>
  <summary>Details</summary>
Motivation: The motivation is to extend the bounded H∞-calculus property from L² spaces to general Lᵖ spaces for vector-valued generators of holomorphic semigroups, which is important for understanding the functional calculus of elliptic operators in various function spaces.

Method: The method involves proving that vector-valued generators of bounded holomorphic semigroups represented by kernels satisfying Gaussian estimates with bounded H∞-calculus in L²(ℝᵈ;ℂᵐ) can be extended to all Lᵖ spaces with 1<p<∞.

Result: The main result establishes that the bounded H∞-calculus property extends from L² to all Lᵖ spaces (1<p<∞) for the considered class of operators. This is successfully applied to elliptic operators of the form -div(Q∇)+V with matrix-valued potential V.

Conclusion: The paper successfully extends bounded H∞-calculus from L² to general Lᵖ spaces for vector-valued generators with Gaussian kernel estimates, providing a powerful tool for analyzing elliptic operators with matrix-valued potentials in various function spaces.

Abstract: We prove that the vector-valued generator of a bounded holomorphic semigroup
represented by a kernel satisfying Gaussian estimates with bounded
$H^\infty$-calculus in $L^2(\mathbb R^d;\mathbb C^m)$ admits bounded
$H^\infty$-calculus for every $p\in (1,\infty)$. We apply this result to the
elliptic operator $-{\rm div}(Q\nabla)+V$, where the potential term V is a
matrix-valued function whose entries belong to $L^1_{\rm loc}(\mathbb R^d)$
and, for almost every $x\in \mathbb R^d$, $V(x)$ is a symmetric and nonnegative
definite matrix.

</details>


### [21] [The fully nonlinear Loewner-Nirenberg problem: Liouville theorems and counterexamples to local boundary estimates](https://arxiv.org/abs/2507.16383)
*Jonah A. J. Duncan,Luc Nguyen*

Main category: math.AP

TL;DR: This paper provides a complete classification of positive viscosity solutions to conformally invariant equations in the upper half-space, showing that solutions are either unique (when μ_Γ^+ > 1) or form a one-parameter family (when μ_Γ^+ ≤ 1), with applications to counterexamples in the Loewner-Nirenberg problem.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need to completely classify positive viscosity solutions to conformally invariant equations involving the Schouten tensor, particularly in the context of the Loewner-Nirenberg problem in the upper half-space, where solutions yield metrics of negative curvature-type that are locally complete near the boundary.

Method: The authors employ a novel application of the method of moving spheres, establishing new estimates and regularity results near the boundary, followed by delicate ordinary differential equation (ODE) analysis to classify the solution structure based on the parameter μ_Γ^+.

Result: The main result shows a dichotomy: when μ_Γ^+ > 1, the hyperbolic solution w^(0)(x) = x_n is the unique solution; when μ_Γ^+ ≤ 1, there exists a monotonically increasing one-parameter family of solutions {w^(a)(x_n)}_{a≥0} with the hyperbolic solution as the minimal element. All solutions depend only on x_n.

Conclusion: The paper establishes a complete classification of solutions to conformally invariant equations based on the critical parameter μ_Γ^+, revealing a phase transition in solution behavior. As an application, this classification provides counterexamples to local boundary C^0 estimates for the fully nonlinear Loewner-Nirenberg problem when μ_Γ^+ ≤ 1.

Abstract: In this paper we give a complete classification of positive viscosity
solutions $w$ to conformally invariant equations of the form
  \begin{align}\label{ab}\tag{$*$}
  \begin{cases}
  f(\lambda(-A_w)) = \frac{1}{2}, \quad \lambda(-A_w)\in\Gamma & \text{in
}\mathbb{R}_+^n \newline
  w = 0 & \text{on }\partial\mathbb{R}_+^n,
  \end{cases}
  \end{align}
  where $A_w$ is the Schouten tensor of the metric $g_w = w^{-2}|dx|^2$,
$\Gamma\subset\mathbb{R}^n$ is a symmetric convex cone and $f$ is an associated
defining function satisfying standard assumptions. Solutions to \eqref{ab}
yield metrics $g_w$ of negative curvature-type which are locally complete near
$\partial\mathbb{R}_+^n$. In particular, when $(f,\Gamma) =
(\sigma_1,\Gamma_1^+)$, \eqref{ab} is the Loewner-Nirenberg problem in the
upper half-space.
  More precisely, let $\mu_\Gamma^+$ denote the unique constant satisfying
$(-\mu_\Gamma^+, 1,\dots,1)\in\partial\Gamma$. We show that when $\mu_\Gamma^+
>1$ (e.g. when $\Gamma = \Gamma_k^+$ for $k<\frac{n}{2}$), the hyperbolic
solution $w^{(0)}(x) := x_n$ is the unique solution to \eqref{ab}. More
surprisingly, we show that when $\mu_\Gamma^+ \leq 1$ (e.g. when $\Gamma =
\Gamma_k^+$ for $k\geq \frac{n}{2}$), the solution set consists of a
monotonically increasing one-parameter family $\{w^{(a)}(x_n)\}_{a\geq 0}$, of
which the hyperbolic solution $w^{(0)}$ is the minimal solution. In either
case, solutions of \eqref{ab} are functions of $x_n$. Our proof involves a
novel application of the method of moving spheres for which we must establish
new estimates and regularity near $\partial\mathbb{R}_+^n$, followed by a
delicate ODE analysis. As an application, we give counterexamples to local
boundary $C^0$ estimates on solutions to the fully nonlinear Loewner-Nirenberg
problem when $\mu_\Gamma^+ \leq 1$.

</details>


### [22] [Homogenization and 3D-2D dimension reduction of a functional on manifold valued Sobolev spaces](https://arxiv.org/abs/2507.16386)
*Michela Eleuteri,Luca Lussardi,Andrea Torricelli,Elvira Zappale*

Main category: math.AP

TL;DR: This paper studies the simultaneous homogenization and dimensional reduction of integral functionals for manifold-valued Sobolev space maps, proving that the Γ-limit density is a tangential quasiconvex integrand with a cell formula representation.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the mathematical challenge of understanding how integral functionals behave when undergoing both homogenization (averaging over microscopic structures) and dimensional reduction simultaneously, specifically for maps taking values in manifolds rather than Euclidean spaces.

Method: The authors use Γ-convergence theory to analyze integral functionals in the superlinear growth regime for maps in manifold-valued Sobolev spaces, employing techniques from calculus of variations and homogenization theory.

Result: The main result establishes that the density of the Γ-limit is characterized as a tangential quasiconvex integrand that can be represented through a specific cell formula, providing an explicit characterization of the limiting behavior.

Conclusion: The paper successfully characterizes the limiting behavior of integral functionals under simultaneous homogenization and dimensional reduction in the manifold-valued setting, extending classical results to this more general geometric context through the tangential quasiconvex structure.

Abstract: We study simultaneous homogenization and dimensional reduction of integral
functionals for maps in manifold-valued Sobolev spaces. Due to the superlinear
growth regime, we prove that the density of the $\Gamma$-limit is a tangential
quasiconvex integrand represented by a cell formula.

</details>


### [23] [The $σ_k$-Loewner-Nirenberg problem on Riemannian manifolds for $k=\frac{n}{2}$ and beyond](https://arxiv.org/abs/2507.16394)
*Jonah A. J. Duncan,Luc Nguyen*

Main category: math.AP

TL;DR: This paper proves existence results for the fully nonlinear Loewner-Nirenberg problem on compact Riemannian manifolds with boundary, establishing solutions when μ_Γ^+ > 1-δ and extending previous work to include the critical case k=n/2 for σ_k-Loewner-Nirenberg problems.


<details>
  <summary>Details</summary>
Motivation: The motivation is to solve the challenging fully nonlinear Loewner-Nirenberg problem, which involves finding conformal metrics with prescribed eigenvalue conditions for the Schouten tensor. This extends fundamental work in conformal geometry and addresses the important threshold case k=n/2 that was previously unresolved.

Method: The method involves analyzing conformal metrics g_u = u^{-2}g_0 and their associated Schouten tensors A_{g_u}, working with symmetric convex cones Γ⊂ℝ^n and symmetric defining functions f. The approach establishes conditions based on the parameter μ_Γ^+ defined by (-μ_Γ^+,1,...,1)∈∂Γ and geometric constants δ.

Result: The main result shows that the fully nonlinear Loewner-Nirenberg problem admits solutions when μ_Γ^+ > 1-δ, where δ depends on geometric data. Specifically, this solves the σ_k-Loewner-Nirenberg problem for all k≤n/2, including the critical threshold case k=n/2. Additionally, solutions exist for both the Loewner-Nirenberg problem and Dirichlet boundary value problems when there exists a conformal metric with appropriate eigenvalue conditions.

Conclusion: The paper successfully extends previous results to include the important threshold case k=n/2 for σ_k-Loewner-Nirenberg problems and establishes new existence results for fully nonlinear conformal geometric problems. The work provides both conditional results (requiring μ_Γ^+ > 1-δ) and unconditional results when appropriate conformal metrics exist, advancing the understanding of nonlinear conformal geometry on manifolds with boundary.

Abstract: Let $(M^n,g_0)$ be a smooth compact Riemannian manifold of dimension $n\geq
3$ with smooth non-empty boundary $\partial M$. Let $\Gamma\subset\mathbb{R}^n$
be a symmetric convex cone and $f$ a symmetric defining function for $\Gamma$
satisfying standard assumptions. Denoting by $A_{g_u}$ the Schouten tensor of a
conformal metric $g_u = u^{-2}g_0$, we show that the associated fully nonlinear
Loewner-Nirenberg problem
  \begin{align*}
  \begin{cases}
  f(\lambda(-g_u^{-1}A_{g_u})) = \frac{1}{2}, \quad
\lambda(-g_u^{-1}A_{g_u})\in\Gamma & \text{on }M\backslash \partial M \newline
  u = 0 & \text{on }\partial M
  \end{cases}
  \end{align*}
  admits a solution if $\mu_\Gamma^+ > 1-\delta$, where $\mu_\Gamma^+$ is
defined by $(-\mu_\Gamma^+,1,\dots,1)\in\partial\Gamma$ and $\delta>0$ is a
constant depending on certain geometric data. In particular, we solve the
$\sigma_k$-Loewner-Nirenberg problem for all $k\leq \frac{n}{2}$, which extends
recent work of the authors to include the important threshold case
$k=\frac{n}{2}$. In the process, we establish that the fully nonlinear
Loewner-Nirenberg problem and corresponding Dirichlet boundary value problem
with positive boundary data admit solutions if there exists a conformal metric
$g\in[g_0]$ such that $\lambda(-g^{-1}A_g)\in\Gamma$ on $M$; these latter
results require no assumption on $\mu_\Gamma^+$ and are new when
$(1,0,\dots,0)\in\partial\Gamma$.

</details>


### [24] [Second-order boundary estimates for solutions to a class of quasilinear elliptic equations](https://arxiv.org/abs/2507.16402)
*Giuseppe Spadaro,Domenico Vuono*

Main category: math.AP

TL;DR: This paper proves global second-order regularity for quasilinear elliptic equations with Dirichlet and Neumann boundary conditions, requiring boundary curvature conditions, and derives integrability properties of the inverse gradient under sign assumptions on the source term.


<details>
  <summary>Details</summary>
Motivation: The motivation is to establish global second-order regularity results for quasilinear elliptic equations, which is fundamental for understanding solution behavior and properties. The study aims to extend regularity theory beyond local results to global ones under specific boundary and geometric conditions.

Method: The method involves proving global second-order regularity by imposing conditions on the integrability of the second fundamental form on the boundary of the domain. The approach considers both homogeneous Dirichlet and Neumann boundary conditions, and utilizes domain convexity to relax boundary regularity requirements.

Result: The main results include: (1) global second-order regularity for quasilinear elliptic equations under boundary curvature conditions, (2) integrability properties of the inverse gradient when the source term has a definite sign, and (3) elimination of boundary regularity requirements when the domain is convex.

Conclusion: The paper successfully establishes global second-order regularity for quasilinear elliptic equations under geometric boundary conditions. The results provide important theoretical foundations for understanding solution regularity, with practical implications showing that convex domains naturally provide the necessary geometric structure without additional boundary regularity assumptions.

Abstract: We prove global second-order regularity for a class of quasilinear elliptic
equations, both with homogeneous Dirichlet and Neumann boundary conditions. A
condition on the integrability of the second fundamental form on the boundary
of the domain is required. As a consequence, with the additional assumption
that the source term has a sign, we obtain integrability properties of the
inverse of the gradient of the solution. Assuming convexity of the domain, no
boundary regularity is required.

</details>


### [25] [The Fujita exponent for a heat equation with mixed local and nonlocal nonlinearities on the Heisenberg group](https://arxiv.org/abs/2507.16411)
*Ahmad Z. Fino,Mokhtar Kirane*

Main category: math.AP

TL;DR: This paper studies a semilinear heat equation on the Heisenberg group with mixed local and nonlocal nonlinearity, establishing conditions for local/global existence of solutions and identifying the critical Fujita exponent that separates global existence from finite-time blow-up.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand the solvability behavior of semilinear heat equations on the Heisenberg group, particularly how the geometric structure of this space interacts with mixed local and nonlocal nonlinearities to affect solution existence and blow-up phenomena.

Method: The authors use rigorous mathematical analysis to distinguish subcritical and supercritical regimes, employ capacity methods to prove non-existence results, and analyze mild solutions for regular nonnegative initial data to establish existence and uniqueness conditions.

Result: The paper establishes precise conditions for local-in-time existence and uniqueness of mild solutions, proves global existence under appropriate growth restrictions, identifies the Fujita exponent as the critical threshold, and obtains lifespan estimates in the supercritical regime showing how initial data size affects blow-up time.

Conclusion: The study successfully characterizes the complete solvability picture for semilinear heat equations on the Heisenberg group with mixed nonlinearity, providing a clear dichotomy between global existence and finite-time blow-up through the identification of the critical Fujita exponent.

Abstract: This article deals with the problems of local and global solvability for a
semilinear heat equation on the Heisenberg group involving a mixed local and
nonlocal nonlinearity. The characteristic features of such equations, arising
from the interplay between the geometric structure of the Heisenberg group and
the combined nonlinearity, are analyzed in detail. The need to distinguish
between subcritical and supercritical regimes is identified and justified
through rigorous analysis. On the basis of the study, the author suggests
precise conditions under which local-in-time mild solutions exist uniquely for
regular, nonnegative initial data. It is proved that global existence holds
under appropriate growth restrictions on the nonlinear terms. To complement
these results, it is shown, by employing the capacity method, that solutions
cannot exist globally in time when the nonlinearity exceeds a critical
threshold. As a result, the Fujita exponent is formulated and identified as the
dividing line between global existence and finite-time blow-up. In addition,
lifespan estimates were obtained in the supercritical regime, providing insight
into how the size of the initial data influences the time of blow-up.

</details>


### [26] [Global existence and optimal time-decay rates of the compressible Navier-Stokes equations with density-dependent viscosities](https://arxiv.org/abs/2507.16436)
*Jie Fan,Xiangdi Huang,Anchun Ni*

Main category: math.AP

TL;DR: This paper proves global existence and optimal decay rates for classical solutions to 3D isentropic compressible Navier-Stokes equations with density-dependent viscosities, allowing arbitrarily large Sobolev norms of initial data derivatives under small L¹∩L² initial data assumptions.


<details>
  <summary>Details</summary>
Motivation: To study the Cauchy problem for three-dimensional isentropic compressible Navier-Stokes equations with density-dependent viscosities μ=ρᵅ, λ=ρᵅ (α>0), particularly addressing the challenge of handling solutions when spatial derivatives of initial data may have arbitrarily large Sobolev norms.

Method: The proof combines three key techniques: Green's function method, energy method, and a time-decay regularity criterion. The analysis requires small initial data in L¹(ℝ³)∩L²(ℝ³) and a viscosity constraint |α-1|≪1.

Result: The authors establish global existence and optimal decay rates of classical solutions for the considered system. Importantly, their results allow the Sobolev norms of spatial derivatives of initial data to be arbitrarily large, which is a significant improvement over previous works.

Conclusion: The paper successfully extends the theory for compressible Navier-Stokes equations with density-dependent viscosities by proving global well-posedness under relaxed conditions on the initial data regularity, while maintaining optimal decay properties of the solutions.

Abstract: This paper is devoted to studying the Cauchy problem for the
three-dimensional isentropic compressible Navier-Stokes equations with
density-dependent viscosities given by
$\mu=\rho^\alpha,\lambda=\rho^\alpha(\alpha>0)$. We establish the global
existence and optimal decay rates of classical solutions under the assumptions
of small initial data in $L^1(\mathbb{R}^3)\cap L^2(\mathbb{R}^3)$ and the
viscosity constraint $|\alpha-1|\ll 1$. The key idea of our proof lies in the
combination of Green's function method, energy method and a time-decay
regularity criterion. In contrast to previous works, the Sobolev norms of the
spatial derivatives of the initial data may be arbitrarily large in our
analysis

</details>


### [27] [On the existence of a singular limit equation for a model of a self-propelled object motion](https://arxiv.org/abs/2507.16447)
*Masaharu Nagayama,Koya Sakakibara,Keisuke Takasao*

Main category: math.AP

TL;DR: A phase-field model is developed to describe deformable, self-propelled objects driven by surface tension, coupling an Allen-Cahn equation with reaction-diffusion for surfactant concentration, and proving convergence to a sharp-interface limit.


<details>
  <summary>Details</summary>
Motivation: To mathematically model the evolution of deformable, self-propelled objects (such as biological cells or active droplets) that move due to surface-tension effects, requiring a framework that can handle complex interface dynamics and surfactant interactions.

Method: Development of a phase-field model that couples an Allen-Cahn-type equation (to distinguish the object from surrounding fluid) with a reaction-diffusion equation for surfactant concentration, followed by mathematical analysis of the limit as interface-thickness parameter ε approaches zero.

Result: Successfully demonstrated that the phase-field model converges to a sharp-interface limit coupled with a reaction-diffusion equation, where the normal velocity is determined by mean curvature, surface tension, and volume-preserving effects.

Conclusion: The proposed phase-field model provides a mathematically rigorous framework for describing self-propelled deformable objects driven by surface tension, with proven convergence to physically meaningful sharp-interface dynamics.

Abstract: In this paper, a phase-field model is introduced to describe the evolution of
a deformable, self-propelled object driven by surface-tension effects. The
model couples an Allen-Cahn-type equation, which distinguishes the body from
the surrounding fluid, with a reaction-diffusion equation for the surfactant
concentration. As the interface-thickness parameter $\varepsilon$ tends to
zero, it is shown that the phase-field model converges to a sharp-interface
limit coupled with a reaction-diffusion equation. In particular, the normal
velocity is given by the mean curvature, surface tension, and volume-preserving
effect.

</details>


### [28] [Stability for multiple Lamb dipoles](https://arxiv.org/abs/2507.16474)
*Ken Abe,In-Jee Jeong,Yao Yao*

Main category: math.AP

TL;DR: This paper proves the Lyapunov stability of finite sums of Lamb dipoles in fluid dynamics under specific geometric arrangements, using energy estimates and Lagrangian methods to track conservation quantities.


<details>
  <summary>Details</summary>
Motivation: To establish the stability properties of multiple interacting Lamb dipoles (fundamental vortex structures in fluid mechanics) on the half-plane, which is important for understanding vortex dynamics and interactions in bounded fluid domains.

Method: The approach combines sharp energy estimates near individual Lamb dipoles with a Lagrangian bootstrapping scheme to quantitatively analyze the exchanges of circulation, enstrophy, impulse, and energy between different parts of the solution.

Result: Under the conditions that dipoles are sufficiently separated and faster dipoles are positioned to the right of slower ones, the authors successfully establish Lyapunov stability for finite sums of Lamb dipoles in the nonnegative vorticity class on the half-plane.

Conclusion: The study demonstrates that properly arranged Lamb dipole configurations remain stable over time, and the robust proof strategy suggests potential for extending these stability results to other vortex configurations and geometric settings.

Abstract: In the class of nonnegative vorticities on the half-plane, we establish the
Lyapunov stability of finite sums of Lamb dipoles under the initial assumptions
that the dipoles are sufficiently separated and that the faster dipoles are
positioned to the right of the slower ones. Our approach combines sharp energy
estimates near the Lamb dipoles with a Lagrangian bootstrapping scheme,
enabling us to quantify the exchanges of circulation, enstrophy, impulse, and
energy between various parts of the solution. The strategy of the proof is
robust, and we present several potential extensions of the result.

</details>


### [29] [Well-posedness and long-time behavior of a bulk-surface Cahn--Hilliard model with non-degenerate mobility](https://arxiv.org/abs/2507.16508)
*Jonas Stange*

Main category: math.AP

TL;DR: This paper studies a bulk-surface Cahn-Hilliard model in 2D with non-degenerate mobility and singular potentials, proving uniqueness and existence of weak solutions, regularity properties, and long-time convergence to stationary solutions.


<details>
  <summary>Details</summary>
Motivation: The paper extends recent theoretical advances for the Cahn-Hilliard equation with homogeneous Neumann boundary conditions to the more complex bulk-surface setting, addressing the need for rigorous mathematical analysis of phase separation models that involve both bulk and surface dynamics.

Method: The approach develops a new well-posedness and regularity theory for bulk-surface elliptic systems with non-constant coefficients, uses techniques for proving uniqueness of weak solutions with continuous dependence estimates, and analyzes propagation of uniform-in-time regularity and instantaneous separation properties.

Result: The paper establishes: (1) uniqueness of weak solutions with continuous dependence for sufficiently regular mobility functions, (2) existence of weak solutions with propagation of uniform-in-time regularity and instantaneous separation property under weaker mobility assumptions, and (3) long-time convergence of solutions to stationary bulk-surface Cahn-Hilliard solutions.

Conclusion: The work provides a comprehensive theoretical foundation for bulk-surface Cahn-Hilliard models, establishing both well-posedness and long-time behavior results, with the developed elliptic system theory having potential independent applications in related mathematical problems.

Abstract: We study a bulk-surface Cahn--Hilliard model with non-degenerate mobility and
singular potentials in two dimensions. Following the ideas of the recent work
by Conti, Galimberti, Gatti, and Giorgini [Calc. Var. Partial Differential
Equations, 64(3):Paper No. 87, 32, 2025] for the Cahn--Hilliard equation with
homogeneous Neumann boundary conditions, we show the uniqueness of weak
solutions together with a continuous dependence estimate for sufficiently
regular mobility functions. Next, under weaker assumptions on the mobility
functions, we show the existence of a weak solution that exhibits the
propagation of uniform-in-time regularity and satisfies the instantaneous
separation property. Lastly, we consider the long-time behavior and prove that
the unique weak solution converges to a solution of the stationary bulk-surface
Cahn--Hilliard equation. Our approach for the uniqueness proof relies on a new
well-posedness and regularity theory for a bulk-surface elliptic system with
non-constant coefficients, which may be of independent interest.

</details>


### [30] [Stability of an elastodynamic system with localized internal damping and acoustic boundary conditions](https://arxiv.org/abs/2507.16546)
*Abdelkhalek Balehouane,Hicham Kasri,Rokia Kechkar*

Main category: math.AP

TL;DR: This paper proves stability results for an elastodynamic system with acoustic boundary conditions and localized internal damping in a 3D bounded domain, using semigroup techniques and multiplier methods to handle the challenges posed by higher-order operators and boundary terms.


<details>
  <summary>Details</summary>
Motivation: The motivation is to establish stability results for elastodynamic systems with acoustic boundary conditions and localized internal damping, which is challenging due to the presence of higher-order operators, normal derivatives, and complex boundary terms that make standard stability analysis difficult.

Method: The method combines semigroup techniques for well-posedness analysis, multiplier approach for stability analysis, trace theorems for handling boundary conditions, ideas from Frota and Vicente's work, and new technical arguments to overcome the difficulties posed by higher-order operators and boundary terms.

Result: The paper successfully establishes well-posedness results and proves stability (asymptotic behavior of solutions) for the elastodynamic system with mixed boundary conditions (homogeneous Dirichlet on Γ₀ and acoustic boundary conditions on Γ₁) and localized internal damping.

Conclusion: The stability of elastodynamic systems with acoustic boundary conditions and localized internal damping can be proven despite the mathematical challenges from higher-order operators and boundary terms, using a combination of semigroup theory, multiplier methods, and specialized technical arguments.

Abstract: In this paper, we prove a stability result for an elastodynamic system with
acoustic boundary conditions and localized internal damping, defined in a
bounded domain $\Omega$ of $\mathbb{R}^3$. Here, the internal damping is only
assumed to be locally distributed and satisfies suitable assumptions. The
smooth boundary of $\Omega$ is $\Gamma=\Gamma_0\cup\Gamma_1$ such that
$\overline{\Gamma_0}\cap\overline{\Gamma_1}=\emptyset$. On $\Gamma_0$, we
consider the homogeneous Dirichlet boundary condition, and on $\Gamma_1$ , we
consider the acoustic boundary condition without a damping term. More
precisely, by making use of semigroup techniques, well-posedness results are
discussed, as well as the asymptotic behavior of solutions. The difficulty in
establishing the stability of the system arises from the presence of
higher-order operators, normal derivatives, and some boundary terms. The key
tools combine the multiplier approach, trace theorems, ideas from Frota and
Vicent\'e \cite{FrotaVicente2018}, and new technical arguments.

</details>


### [31] [Trace and Observability Inequalities for Laplace Eigenfunctions on the Torus](https://arxiv.org/abs/2507.16599)
*Nicolas Burq,Pierre Germain,Massimo Sorella,Hui Zhu*

Main category: math.AP

TL;DR: This paper characterizes Borel measures on d-dimensional torus for which trace and observability inequalities hold uniformly for all Laplace eigenfunctions, generalizing classical results to higher dimensions with applications to quantum limits and Schrödinger equation control.


<details>
  <summary>Details</summary>
Motivation: The authors aim to understand which measures μ allow uniform control of eigenfunctions through trace and observability inequalities, extending classical one-dimensional results by Zygmund and Bourgain-Rudnick to arbitrary dimensions on the torus.

Method: The approach combines three main tools: analyzing cluster structure of lattice points on spheres, using decoupling estimates, and constructing specific eigenfunctions that exhibit either strong concentration or vanishing behavior to test the trace and observability inequalities respectively.

Result: The paper provides complete characterization of measures satisfying the inequalities: sufficient conditions based on integrability and regularity of μ, and necessary conditions formulated through the dimension of the measure's support, successfully generalizing classical theorems to higher dimensions.

Conclusion: The results have broad applications including Cantor-Lebesgue type theorems, constraints on quantum limits, and control theory for Schrödinger equations, providing a comprehensive framework for understanding eigenfunction behavior with respect to arbitrary measures on tori.

Abstract: We investigate trace and observability inequalities for Laplace
eigenfunctions on the d-dimensional torus, with respect to arbitrary Borel
measures $\mu$. Specifically, we characterize the measures $\mu$ for which the
inequalities
  $$ \int |u|^2 d \mu \lesssim \int |u|^2 d x \quad \text{(trace)}, \qquad \int
|u|^2 d \mu \gtrsim \int |u|^2 d x \quad \text{(observability)}$$
  hold uniformly for all eigenfunctions $u$ of the Laplacian. Sufficient
conditions are derived based on the integrability and regularity of $\mu$,
while necessary conditions are formulated in terms of the dimension of the
support of the measure. These results generalize classical theorems of Zygmund
and Bourgain--Rudnick to higher dimensions. Applications include results in the
spirit of Cantor--Lebesgue theorems, constraints on quantum limits, and control
theory for the Schr\"odinger equation. Our approach combines several tools: the
cluster structure of lattice points on spheres; decoupling estimates; and the
construction of eigenfunctions exhibiting strong concentration or vanishing
behavior, tailored respectively to the trace and observability inequalities.

</details>


### [32] [On the interplay between inverse scattering for asymptotically hyperbolic manifolds and the Calderón problem for the Conformal Laplacian](https://arxiv.org/abs/2507.16646)
*Sebastián Muñoz-Thon*

Main category: math.AP

TL;DR: This paper proves that the scattering matrix at a specific energy level on asymptotically hyperbolic manifolds uniquely determines the boundary metric structure up to natural geometric transformations.


<details>
  <summary>Details</summary>
Motivation: To establish a new inverse scattering result for asymptotically hyperbolic manifolds by leveraging the known relationship between generalized eigenvalue problems and the Conformal Laplacian.

Method: The authors utilize the relation established by Guillarmou-Guillopé and Chang-González connecting the generalized eigenvalue problem for asymptotically hyperbolic manifolds with the Conformal Laplacian operator.

Result: On an asymptotically hyperbolic manifold of dimension n+1, the scattering matrix at energy (n+1)/2 uniquely determines the jet of the boundary metric up to diffeomorphism and conformal transformations.

Conclusion: The scattering matrix at the critical energy level (n+1)/2 contains sufficient geometric information to recover the boundary metric structure of asymptotically hyperbolic manifolds, providing a new tool for inverse problems in geometric analysis.

Abstract: In this short note, we use the relation obtained by Guillarmou--Guillop\'e
and Chang--Gonz\'alez between the generalized eigenvalue problem for
asymptotically hyperbolic (AH) manifolds and the Conformal Laplacian, to obtain
a new inverse scattering result: on an AH manifold of dimension $n+1$, we show
that the scattering matrix at energy $\frac{n+1}{2}$ determines the jet of the
metric on the boundary, up to a diffeomorphism and conformal factor.

</details>


### [33] [Existence and Uniqueness of Solutions to Nonlinear Diffusion with Memory in Random Media](https://arxiv.org/abs/2507.16659)
*Yixian Chen*

Main category: math.AP

TL;DR: This paper proves existence and uniqueness of weak solutions for a nonlinear diffusion equation with memory effects and spatial randomness using monotone operator theory and variational methods.


<details>
  <summary>Details</summary>
Motivation: To provide a rigorous mathematical foundation for studying memory-type diffusion processes in heterogeneous media, which are important for understanding transport phenomena in complex materials with memory effects and spatial variability.

Method: The analysis employs orthogonal approximation techniques, energy estimates, and monotone operator theory. The convolution structure arising from the memory kernel is handled within variational frameworks, with monotonicity and growth conditions imposed on the nonlinear function Φ.

Result: Existence and uniqueness of weak solutions is established for the nonlinear diffusion equation with memory kernel K and bounded random coefficient D(x,w), under appropriate monotonicity and growth conditions on the nonlinear term Φ.

Conclusion: The work provides a rigorous mathematical basis for studying memory-type diffusion in heterogeneous media, establishing well-posedness of the model through advanced variational and monotone operator techniques.

Abstract: This paper studies a nonlinear diffusion equation with memory and spatial
randomness: $$u_t=\nabla\cdot \big( D(x,w)\cdot\int_0^t K(t-s)
\nabla\cdot\Phi(u(x,s))ds \big)+f(x,t)$$ where $K$ is memory Kernel and
$D(x,w)$ is bounded random coefficient. Under monotonicity and growth
conditions on $\Phi$, the existence and uniqueness of weak solution is
established. The analysis employs Orthogonal approximation, energy estimates,
and monotone operator theory. The convolution structure is handled within
variational frameworks. The result provides a rigorous basis for studying
memory-type diffusion in heterogeneous media.

</details>


### [34] [Orthonormal Strichartz estimates on torus and waveguide manifold and applications](https://arxiv.org/abs/2507.16712)
*Divyang G. Bhimani,Subhash. R. Choudhary*

Main category: math.AP

TL;DR: This paper establishes orthonormal Strichartz estimates for fractional Schrödinger equations on torus and waveguide manifolds, improves decoupling inequalities, and applies these results to prove local well-posedness of Hartree equations with infinitely many particles.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop mathematical tools for analyzing fractional Schrödinger equations on specific geometric settings (torus and waveguide manifolds) and to handle challenging cases of Hartree equations with infinitely many particles that have non-trace class initial data.

Method: The authors establish orthonormal Strichartz estimates for fractional Schrödinger equations, improve ℓ² decoupling inequalities, and develop classical fractional Strichartz estimates specifically for waveguide manifolds.

Result: The paper successfully establishes the desired orthonormal Strichartz estimates on both torus and waveguide manifolds, improves existing decoupling inequalities, and proves local well-posedness for Hartree equations with infinitely many particles having non-trace class initial data.

Conclusion: The established estimates provide powerful analytical tools for fractional Schrödinger equations on specific manifolds and enable the treatment of previously challenging cases in many-body quantum mechanics, particularly Hartree equations with infinitely many particles.

Abstract: We establish orthonormal Strichartz estimates for the fractional
Schr\"odinger equations on torus and waveguide manifold. In the process, we
also improve $\ell^2$ decoupling inequality and establish classical fractional
Strichartz estimates on waveguide manifold. This maybe of independent interest.
As an application, we establish local well-posednes for the Hartree equations
with infinitely many particles with non-trace class initial data.

</details>


### [35] [$H^2$-regularity on convex domains for Robin eigenfunctions with parameter of arbitrary sign](https://arxiv.org/abs/2507.16726)
*Pier Domenico Lamberti,Luigi Provenzano*

Main category: math.AP

TL;DR: This paper proves that Robin eigenfunctions on convex domains have H² regularity for any parameter sign in the boundary conditions, extending classical results beyond positive parameters.


<details>
  <summary>Details</summary>
Motivation: Classical results for Robin eigenfunction regularity were limited to positive parameters in boundary conditions. The authors aim to extend this regularity theory to cover all parameter signs, providing a more complete understanding of Robin eigenfunctions on convex domains.

Method: The authors adapt classical arguments used for positive parameters and combine them with a Rellich-Pohožaev identity to handle the case of arbitrary parameter signs in Robin boundary conditions.

Result: The main result establishes H² regularity for Robin eigenfunctions on convex domains in ℝⁿ regardless of whether the boundary condition parameter is positive, negative, or zero.

Conclusion: Robin eigenfunctions on convex domains possess H² regularity universally across all parameter signs, completing the regularity theory for this important class of boundary value problems through the successful extension of classical techniques.

Abstract: We prove that the Robin eigenfunctions on convex domains of $\mathbb R^n$ are
$H^2$ regular regardless of the sign of the parameter involved in the boundary
conditions. The proof is an adaptation of a classical argument used in the case
of positive parameters combined with a Rellich-Pohozaev identity.

</details>


### [36] [Global finite energy solutions of the Maxwell-scalar field system on the Einstein cylinder](https://arxiv.org/abs/2507.16750)
*Jean-Philippe Nicolas,Grigalius Taujanskas*

Main category: math.AP

TL;DR: This paper proves the existence and uniqueness of global finite energy solutions for the Maxwell-scalar field system in Lorenz gauge on the Einstein cylinder, combining conformal patching, finite energy theorems, and null form estimates, though with some regularity losses in the scalar field and potential.


<details>
  <summary>Details</summary>
Motivation: To establish the mathematical foundations for understanding the long-term behavior of electromagnetic fields coupled with scalar fields in curved spacetime by proving global existence and uniqueness of solutions on the Einstein cylinder, which is important for general relativity and field theory applications.

Method: The authors employ a combination of four key techniques: (1) conformal patching arguments to handle the geometry of the Einstein cylinder, (2) the finite energy existence theorem in Lorenz gauge on Minkowski space developed by Selberg and Tesfahun, (3) careful localization of finite energy data to manage boundary conditions, and (4) null form estimates of Foschi-Klainerman type to control nonlinear interactions.

Result: The paper successfully establishes existence and uniqueness of global finite energy solutions for the Maxwell-scalar field system in Lorenz gauge on the Einstein cylinder. The energy-carrying components of the solution maintain their regularity throughout the evolution. However, there are small losses of regularity observed in both the scalar field and the electromagnetic potential due to technical limitations.

Conclusion: The study provides a rigorous mathematical proof of global well-posedness for the Maxwell-scalar field system on the Einstein cylinder, though the regularity losses in the scalar field and potential indicate that the null structure in Lorenz gauge is incomplete and that foliation-change arguments introduce technical challenges that prevent optimal regularity preservation.

Abstract: We prove the existence and uniqueness of global finite energy solutions of
the Maxwell-scalar field system in Lorenz gauge on the Einstein cylinder. Our
method is a combination of a conformal patching argument, the finite energy
existence theorem in Lorenz gauge on Minkowski space of Selberg and Tesfahun, a
careful localization of finite energy data, and null form estimates of
Foschi-Klainerman type. Although we prove that the energy-carrying components
of the solution maintain regularity, due to the incompleteness of the null
structure in Lorenz gauge and the nature of our foliation-change arguments we
find small losses of regularity in both the scalar field and the potential.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [37] [Radiological and Biological Dictionary of Radiomics Features: Addressing Understandable AI Issues in Personalized Breast Cancer; Dictionary Version BM1.0](https://arxiv.org/abs/2507.16041)
*Arman Gorji,Nima Sanati,Amir Hossein Pouria,Somayeh Sadat Mehrnia,Ilker Hacihaliloglu,Arman Rahmim,Mohammad R. Salmanpour*

Main category: physics.comp-ph

TL;DR: This study developed a dual-dictionary framework (BM1.0) that maps radiomic features to clinical BI-RADS descriptors to improve interpretability of AI models for breast cancer diagnosis, achieving 83% accuracy in classifying triple-negative breast cancer using MRI data from 1,549 patients.


<details>
  <summary>Details</summary>
Motivation: Radiomics-based AI models for breast cancer diagnosis lack interpretability, which limits their clinical adoption. There is a gap between radiomic features and the standardized BI-RADS lexicon that clinicians use, making it difficult to integrate AI insights into routine clinical practice.

Method: The researchers created a dual-dictionary framework consisting of: 1) A Clinically-Informed Feature Interpretation Dictionary (CIFID) mapping 56 radiomic features to BI-RADS descriptors through literature and expert review, and 2) A Data-Driven Feature Interpretation Dictionary (DDFID) for 52 additional features using SHAP explanations. They trained 27 machine learning classifiers with 27 feature selection methods on dynamic contrast-enhanced MRI data from 1,549 patients to classify triple-negative breast cancer versus non-TNBC.

Result: The best performing model (Variance Inflation Factor selection with Extra Trees Classifier) achieved 83% average cross-validation accuracy. Key predictive radiomic features aligned with clinical knowledge: higher Sphericity (round/oval shape) and lower Busyness (more homogeneous enhancement) were associated with triple-negative breast cancer. The framework successfully confirmed known imaging biomarkers and revealed novel interpretable associations.

Conclusion: The dual-dictionary approach (BM1.0) enhances AI model transparency by bridging radiomic features with clinical terminology, supporting the integration of radiomic features into routine breast cancer diagnosis and personalized care. This framework addresses the interpretability challenge that has limited clinical adoption of radiomics-based AI models.

Abstract: Radiomics-based AI models show promise for breast cancer diagnosis but often
lack interpretability, limiting clinical adoption. This study addresses the gap
between radiomic features (RF) and the standardized BI-RADS lexicon by
proposing a dual-dictionary framework. First, a Clinically-Informed Feature
Interpretation Dictionary (CIFID) was created by mapping 56 RFs to BI-RADS
descriptors (shape, margin, internal enhancement) through literature and expert
review. The framework was applied to classify triple-negative breast cancer
(TNBC) versus non-TNBC using dynamic contrast-enhanced MRI from a
multi-institutional cohort of 1,549 patients. We trained 27 machine learning
classifiers with 27 feature selection methods. SHapley Additive exPlanations
(SHAP) were used to interpret predictions and generate a complementary
Data-Driven Feature Interpretation Dictionary (DDFID) for 52 additional RFs.
The best model, combining Variance Inflation Factor (VIF) selection with Extra
Trees Classifier, achieved an average cross-validation accuracy of 0.83. Key
predictive RFs aligned with clinical knowledge: higher Sphericity (round/oval
shape) and lower Busyness (more homogeneous enhancement) were associated with
TNBC. The framework confirmed known imaging biomarkers and uncovered novel,
interpretable associations. This dual-dictionary approach (BM1.0) enhances AI
model transparency and supports the integration of RFs into routine breast
cancer diagnosis and personalized care.

</details>


### [38] [Iterative Born Solver for the Acoustic Helmholtz Equation with Heterogeneous Sound Speed and Density](https://arxiv.org/abs/2507.16087)
*Antonio Stanziola,Simon R. Arridge,Bradley E. Treeby,Benjamin T. Cox*

Main category: physics.comp-ph

TL;DR: A fast iterative solver for the acoustic Helmholtz equation that handles heterogeneous media with varying sound speed, density, and absorption using an extended Convergent Born Series method with FFT-based efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing numerical solutions for the acoustic Helmholtz equation in heterogeneous media are computationally challenging, especially for large-scale problems with spatially-varying density, which limits applications in biomedical acoustics and seismic imaging.

Method: The approach reformulates the Helmholtz equation as a first-order system and applies Vettenburg and Vellekoop's universal split-preconditioner, creating a matrix-free algorithm that uses Fast Fourier Transforms. The method extends the Convergent Born Series to handle arbitrary variations in sound speed, density, and absorption simultaneously without requiring expensive matrix decompositions.

Result: The solver achieves convergence for strong scattering scenarios, provides both forward and adjoint solutions, and demonstrates accuracy through analytical solution comparisons and transcranial ultrasound simulations. It operates with minimal memory overhead and is suitable for large-scale 3D problems.

Conclusion: The method offers a computationally efficient alternative to time-domain methods and matrix-based Helmholtz solvers, making it practical for applications ranging from medical ultrasound treatment planning to seismic exploration, with particular advantages for inverse problems requiring both forward and adjoint solutions.

Abstract: Efficient numerical solution of the acoustic Helmholtz equation in
heterogeneous media remains challenging, particularly for large-scale problems
with spatially-varying density - a limitation that restricts applications in
biomedical acoustics and seismic imaging. We present a fast iterative solver
that extends the Convergent Born Series method to handle arbitrary variations
in sound speed, density, and absorption simultaneously. Our approach
reformulates the Helmholtz equation as a first-order system and applies
Vettenburg and Vellekoop's universal split-preconditioner, yielding a
matrix-free algorithm that leverages Fast Fourier Transforms for computational
efficiency. Unlike existing Born series methods, our solver accommodates
heterogeneous density without requiring expensive matrix decompositions or
pre-processing steps, making it suitable for large-scale 3D problems with
minimal memory overhead. The method provides both forward and adjoint
solutions, enabling its application for inverse problems. We validate accuracy
through comparison against an analytical solution and demonstrate the solver's
practical utility through transcranial ultrasound simulations. The solver
achieves convergence for strong scattering scenarios, offering a
computationally efficient alternative to time-domain methods and matrix-based
Helmholtz solvers for applications ranging from medical ultrasound treatment
planning to seismic exploration.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [39] [Efficient dataset construction using active learning and uncertainty-aware neural networks for plasma turbulent transport surrogate models](https://arxiv.org/abs/2507.15976)
*Aaron Ho,Lorenzo Zanisi,Bram de Leeuw,Vincent Galvan,Pablo Rodriguez-Fernandez,Nathaniel T. Howard*

Main category: physics.plasm-ph

TL;DR: This study demonstrates using uncertainty-aware neural networks with active learning to efficiently build datasets for surrogate models in plasma turbulent transport, achieving good performance with significantly reduced training data requirements.


<details>
  <summary>Details</summary>
Motivation: To develop efficient surrogate models for expensive plasma turbulent transport simulations by reducing the required training dataset size through uncertainty-aware architectures and active learning, targeting applications where computational resources are limited.

Method: Applied ADEPT framework using SNGP architecture for classification and BNN-NCP architecture for regression, combined with active learning over 45 iterations. Used QuaLiKiz code as data labeller, starting with 10² samples and growing to 10⁴ samples to model all turbulent modes and transport fluxes.

Result: Achieved F₁ classification performance of ~0.8 and R² regression performance of ~0.75 on independent test sets across all outputs. Successfully demonstrated training set reduction while maintaining performance comparable to previous ADEPT pipeline, despite handling one additional input dimension.

Conclusion: The uncertainty-aware active learning approach successfully reduces dataset requirements for surrogate model generation in plasma transport problems. While improvement rates diminished faster than expected, the technique is generalizable to other surrogate modeling applications beyond plasma physics.

Abstract: This work demonstrates a proof-of-principle for using uncertainty-aware
architectures, in combination with active learning techniques and an
in-the-loop physics simulation code as a data labeller, to construct efficient
datasets for data-driven surrogate model generation. Building off of a previous
proof-of-principle successfully demonstrating training set reduction on static
pre-labelled datasets, using the ADEPT framework, this strategy was applied
again to the plasma turbulent transport problem within tokamak fusion plasmas,
specifically the QuaLiKiz quasilinear electrostatic gyrokinetic turbulent
transport code. While QuaLiKiz provides relatively fast evaluations, this study
specifically targeted small datasets to serve as a proxy for more expensive
codes, such as CGYRO or GENE. The newly implemented algorithm uses the SNGP
architecture for the classification component of the problem and the BNN-NCP
architecture for the regression component, training models for all turbulent
modes (ITG, TEM, ETG) and all transport fluxes ($Q_e$, $Q_i$, $\Gamma_e$,
$\Gamma_i$, and $\Pi_i$) described by the general QuaLiKiz output. With 45
active learning iterations, moving from a small initial training set of
$10^{2}$ to a final set of $10^{4}$, the resulting models reached a $F_1$
classification performance of ~0.8 and a $R^2$ regression performance of ~0.75
on an independent test set across all outputs. This extrapolates to reaching
the same performance and efficiency as the previous ADEPT pipeline, although on
a problem with 1 extra input dimension. While the improvement rate achieved in
this implementation diminishes faster than expected, the overall technique is
formulated with components that can be upgraded and generalized to many
surrogate modeling applications beyond plasma turbulent transport predictions.

</details>


### [40] [Predictive Hydrodynamic Simulations for Laser Direct-drive Implosion Experiments via Artificial Intelligence](https://arxiv.org/abs/2507.16227)
*Zixu Wang,Yuhan Wang,Junfei Ma,Fuyuan Wu,Junchi Yan,Xiaohui Yuan,Zhe Zhang,Jie Zhang*

Main category: physics.plasm-ph

TL;DR: This paper presents MULTI-Net, a Transformer-based AI model that predicts laser-driven implosion dynamics in fusion experiments, specifically for double-cone ignition (DCI) schemes, achieving accurate predictions of implosion velocity and plasma density.


<details>
  <summary>Details</summary>
Motivation: Traditional hydrodynamic simulations for laser fusion experiments are computationally expensive and may lack accuracy in predicting complex implosion dynamics. There is a need for faster, more accurate predictive models to enhance understanding and optimization of laser-driven fusion experiments.

Method: The authors developed MULTI-Net, a Transformer-based deep learning model that predicts implosion features based on laser waveforms and target radius. They introduced a Physics-Informed Decoder (PID) for high-dimensional sampling to reduce prediction errors compared to Latin hypercube sampling. The model was validated using DCI experiments from the SG-II Upgrade facility.

Result: MULTI-Net successfully predicted implosion dynamics measured by x-ray streak camera in DCI experiments. The model identified an optimal laser absorption factor of ~65% for one-dimensional DCI-R10 simulations. For shot 33, the predicted mean implosion velocity was 195 km/s and collided plasma density reached 117 g/cc, matching experimental observations.

Conclusion: The study demonstrates that AI-enhanced simulations can significantly improve prediction capabilities for complex laser fusion experiments. The MULTI-Net framework provides a data-driven approach that combines deep learning with physics-informed methods to achieve accurate and efficient predictions of implosion dynamics in laser-driven fusion experiments.

Abstract: This work presents predictive hydrodynamic simulations empowered by
artificial intelligence (AI) for laser driven implosion experiments, taking the
double-cone ignition (DCI) scheme as an example. A Transformer-based deep
learning model MULTI-Net is established to predict implosion features according
to laser waveforms and target radius. A Physics-Informed Decoder (PID) is
proposed for high-dimensional sampling, significantly reducing the prediction
errors compared to Latin hypercube sampling. Applied to DCI experiments
conducted on the SG-II Upgrade facility, the MULTI-Net model is able to predict
the implosion dynamics measured by the x-ray streak camera. It is found that an
effective laser absorption factor about 65\% is suitable for the
one-dimensional simulations of the DCI-R10 experiments. For shot 33, the mean
implosion velocity and collided plasma density reached 195 km/s and 117 g/cc,
respectively. This study demonstrates a data-driven AI framework that enhances
the prediction ability of simulations for complicated laser fusion experiments.

</details>


### [41] [Physics-Informed Neural Networks for High-Precision Grad-Shafranov Equilibrium Reconstruction](https://arxiv.org/abs/2507.16636)
*Cuizhi Zhou,Kaien Zhu*

Main category: physics.plasm-ph

TL;DR: This paper presents a multi-stage Physics-Informed Neural Networks (PINNs) approach for solving the Grad-Shafranov equation in plasma equilibrium reconstruction, achieving high precision with O(10^-8) error magnitude compared to analytical solutions.


<details>
  <summary>Details</summary>
Motivation: Plasma equilibrium reconstruction is a critical step in real-time diagnostic tasks for fusion research, requiring accurate and efficient solutions to the Grad-Shafranov equation for plasma physics applications.

Method: Multi-stage Physics-Informed Neural Networks (PINNs) approach to solve the Grad-Shafranov equation, utilizing neural networks that incorporate physics constraints through the governing equations.

Result: Achieved high-precision solutions with error magnitude of O(10^-8) between the second-stage neural network output and analytical solutions, demonstrating excellent accuracy in plasma equilibrium reconstruction.

Conclusion: The multi-stage PINNs approach provides a reliable and accurate tool for plasma equilibrium reconstruction in fusion research, offering a promising method for real-time diagnostic applications.

Abstract: The equilibrium reconstruction of plasma is a core step in real-time
diagnostic tasks in fusion research. This paper explores a multi-stage
Physics-Informed Neural Networks(PINNs) approach to solve the Grad-Shafranov
equation, achieving high-precision solutions with an error magnitude of
$O(10^{-8})$ between the output of the second-stage neural network and the
analytical solution. Our results demonstrate that the multi-stage PINNs
provides a reliable tool for plasma equilibrium reconstruction.

</details>


<div id='cond-mat.supr-con'></div>

# cond-mat.supr-con [[Back]](#toc)

### [42] [High-$T_{\rm c}$ Ag$_x$BC and Cu$_x$BC superconductors accessible via topochemical reactions](https://arxiv.org/abs/2507.14281)
*Daviti Gochitashvili,Charlsey R. Tomassetti,Elena R. Margine,Aleksey N. Kolmogorov*

Main category: cond-mat.supr-con

TL;DR: Researchers predict that metastable AgBC and CuBC compounds, derived from LiBC precursors through ion exchange, could exhibit high-temperature superconductivity above 50 K due to their unique metallic properties and two-gap superconducting behavior.


<details>
  <summary>Details</summary>
Motivation: Conventional high-Tc superconductor design through hole-doping of covalent materials is severely limited by thermodynamic constraints, creating a need to explore new synthetic pathways and materials that can overcome these limitations.

Method: Ab initio computational calculations combined with anisotropic Migdal-Eliashberg analysis to predict the properties of metastable AgBC and CuBC phases that can be synthesized via topochemical ion exchange reactions from LiBC precursors.

Result: The predicted AgBC and CuBC derivatives exhibit metallic behavior (unlike known stoichiometric layered metal borocarbides which are semiconducting) and feature honeycomb layers bridged by dumbbells. AgBC shows intrinsic hole-doping and unique electronic/vibrational features enabling two-gap superconductivity above 50 K.

Conclusion: Metastable metal borocarbide phases accessible through ion exchange reactions represent a promising new route for high-temperature superconductor design, with AgBC predicted to achieve superconducting temperatures above 50 K through a novel two-gap mechanism.

Abstract: Hole-doping of covalent materials has long served as a blueprint for
designing conventional high-$T_{\rm c}$ superconductors, but thermodynamic
constraints severely limit the space of realizable compounds. Our {\it ab
initio} results indicate that metastable Ag$_x$BC and Cu$_x$BC phases can be
accessed via standard topochemical ion exchange reactions starting from
Li$_x$BC precursors. Unlike all known stoichiometric layered metal
borocarbides, the predicted AgBC and CuBC derivatives, comprising honeycomb
layers bridged by dumbbells, are metallic rather than semiconducting.
Anisotropic Migdal-Eliashberg analysis reveals that the intrinsically
hole-doped AgBC possesses a unique combination of electronic and vibrational
features to exhibit two-gap superconductivity above 50 K.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [43] [Benchmarking CO$_2$ Storage Simulations: Results from the 11th Society of Petroleum Engineers Comparative Solution Project](https://arxiv.org/abs/2507.15861)
*Jan M. Nordbotten,Martin A. Fernø,Bernd Flemisch,Anthony R. Kovscek,Knut-Andreas Lie,Jakub W. Both,Olav Møyner,Tor Harald Sandve,Etienne Ahusborde,Sebastian Bauer,Zhangxing Chen,Holger Class,Chaojie Di,Didier Ding,David Element,Abbas Firoozabadi,Eric Flauraud,Jacques Franc,Firdovsi Gasanzade,Yousef Ghomian,Marie Ann Giddins,Christopher Green,Bruno R. B. Fernandes,George Hadjisotiriou,Glenn Hammond,Hai Huang,Dickson Kachuma,Michel Kern,Timo Koch,Prasanna Krishnamurthy,Kjetil Olsen Lye,David Landa-Marbán,Michael Nole,Paolo Orsini,Nicolas Ruby,Pablo Salinas,Mohammad Sayyafzadeh,Jakub Solovský,Jakob Torben,Adam Turner,Denis V. Voskov,Kai Wendel,AbdAllah A. Youssef*

Main category: physics.geo-ph

TL;DR: This paper summarizes the SPE11 benchmarking project for geological CO2 storage simulation tools, analyzing results from 18 participating groups and finding that both documented computational choices and undocumented human decisions significantly impact simulation variability.


<details>
  <summary>Details</summary>
Motivation: To benchmark and compare simulation tools for geological carbon dioxide storage through a comprehensive comparative study involving leading research institutions and industry participants worldwide.

Method: Conducted a comparative solution project with 45 registered groups (18 valid submissions), developed a global metric for analyzing relative distances between submissions, and performed quantitative statistical analysis to identify key factors influencing variability between results.

Result: Major qualitative variations were related to thermal effects, dissolution-driven convective mixing, and facies discontinuity resolution. Strong grid resolution dependence was observed across all SPE11 versions. Quantitative analysis revealed that undocumented human choices in simulation setup and reporting were as impactful as documented computational choices.

Conclusion: The study demonstrates that simulation variability in geological CO2 storage modeling is significantly influenced by both reported computational parameters and unreported human decisions in the simulation workflow, highlighting the importance of standardizing and documenting all aspects of the modeling process.

Abstract: The 11th Society of Petroleum Engineers Comparative Solution Project
(shortened SPE11 herein) benchmarked simulation tools for geological carbon
dioxide (CO$_2$) storage. A total of 45 groups from leading research
institutions and industry across the globe signed up to participate, with 18
ultimately contributing valid results that were included in the comparative
study reported here.
  This paper summarizes the SPE11. A comprehensive introduction and qualitative
discussion of the submitted data are provided, together with an overview of
online resources for accessing the full depth of data. A global metric for
analyzing the relative distance between submissions is proposed and used to
conduct a quantitative analysis of the submissions. This analysis attempts to
statistically resolve the key aspects influencing the variability between
submissions.
  The study shows that the major qualitative variation between the submitted
results is related to thermal effects, dissolution-driven convective mixing,
and resolution of facies discontinuities. Moreover, a strong dependence on grid
resolution is observed across all three versions of the SPE11. However, our
quantitative analysis suggests that the observed variations are predominantly
influenced by factors not documented in the technical responses provided by the
participants. We therefore identify that unreported variations due to human
choices within the process of setting up, conducting, and reporting on the
simulations underlying each SPE11 submission are at least as impactful as the
computational choices reported.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [44] [Prediction of Alpha-Particle-Immune Gate-All-Around Field-Effect Transistors (GAA-FET) Based SRAM Design](https://arxiv.org/abs/2507.15860)
*Albert Lu,Reza Arghavani,Hiu Yung Wong*

Main category: cs.ET

TL;DR: This paper demonstrates through 3D TCAD simulations that a sub-7nm GAA-FET SRAM with bottom dielectric isolation can be designed to be completely immune to single-event upsets caused by alpha particle radiation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the critical issue of single-event upsets (SEU) in SRAM caused by alpha particle radiation, which becomes increasingly problematic as memory devices scale down and become more susceptible to radiation-induced errors.

Method: The authors use 3D Technology Computer-Aided-Design (TCAD) simulations combined with ab initio calculations in PHITS to determine the maximum linear energy transfer (LETmax) for alpha particles in Si and SixGe1-x materials, then design a sub-7nm GAA-FET-based SRAM with bottom dielectric isolation (BDI).

Result: The designed SRAM shows complete immunity to single-event upsets from alpha particles, with no memory bit flips occurring even under worst-case radiation scenarios where the linear energy transfer exceeds the maximum theoretical value (LET > LETmax).

Conclusion: It is feasible to design radiation-hardened SRAM using GAA-FET technology with bottom dielectric isolation that provides complete protection against alpha particle-induced single-event upsets, offering a promising solution for reliable memory systems in radiation-sensitive applications.

Abstract: In this paper, using 3D Technology Computer-Aided-Design (TCAD) simulations,
we show that it is possible to design a static random-access memory (SRAM)
using gate-all-around field-effect-transistor (GAA-FET) technology so that it
is immune to single alpha particle radiation error. In other words, with the
design, there will be no single-event upset (SEU) due to alpha particles. We
first use ab initio calculations in PHITS to show that there is a maximum
linear energy transfer (LET), LETmax, for the alpha particle in Si and
Si$_x$Ge$_{1-x}$. Based on that, by designing a sub-7nm GAA-FET-based SRAM with
bottom dielectric isolation (BDI), we show that the SRAM does not flip even if
the particle strike is in the worst-case scenario (LET > LETmax).

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [45] [Modelling gyrosynchrotron emission from coronal energetic electrons in a CME flux rope](https://arxiv.org/abs/2507.16449)
*Edin Husidic,Nicolas Wijsen,Immanuel Christopher Jebaraj,Angelos Vourlidas,Luis Linan,Rami Vainio,Stefaan Poedts*

Main category: astro-ph.SR

TL;DR: This study uses sophisticated 3D numerical modeling combining MHD simulations, particle transport, and radio emission codes to investigate the origin and mechanisms of solar type IV radio bursts, finding that gyrosynchrotron emission from electrons trapped in CME magnetic flux ropes is the primary mechanism.


<details>
  <summary>Details</summary>
Motivation: Type IV radio bursts from solar flares and CMEs remain poorly understood despite being important diagnostic tools for the corona. In situ measurements are limited, and idealized assumptions in previous models don't capture the complex physics involved, necessitating more realistic numerical approaches.

Method: The researchers employed a multi-step modeling approach: (1) 3D COCONUT MHD model to generate coronal background with CME as modified Titov-Démoulin magnetic flux rope, (2) PARADISE particle transport code to inject and track energetic electrons in the flux rope, and (3) Ultimate Fast Gyrosynchrotron Codes to compute radio emission using realistic anisotropic electron distributions.

Result: Electrons injected near the magnetic flux rope's central axis remain confined and produce gyrosynchrotron emission spectra matching observed type IV characteristics. The strongest emission originates from CME flanks. Varying observer positions, CME properties, and electron energy spectral indices affects burst intensities and durations.

Conclusion: Gyrosynchrotron emission is identified as the major component in type IV radio burst spectra, though additional mechanisms cannot be excluded. The realistic modeling approach successfully reproduces observed type IV burst characteristics and provides new insights into their physical origins.

Abstract: Solar flares and coronal mass ejections (CMEs) can accelerate electrons,
causing bursts such as type IV emissions in the solar radio continuum. Although
radio spectroscopy is a powerful diagnostic tool for the corona, the origin and
mechanisms of type IV bursts remain uncertain. In situ measurements can
occasionally shed some light on these mechanisms, but they are limited in space
and time. Sophisticated numerical modelling offers the best approach to improve
our understanding of the physical processes involved. This research examines
type IV radio bursts, exploring the effects of various electron distribution
properties and CMEs on their generation and characteristics. To transcend
idealised assumptions, we employ realistic, anisotropic electron distributions
- obtained from particle transport simulations within complex
magnetohydrodynamic (MHD) environments - as input for radio emission models. We
use the 3D MHD model COCONUT to generate coronal background configurations,
including a CME modelled as a modified Titov-D\'{e}moulin magnetic flux rope
(MFR). These MHD simulations are used by the PARADISE particle transport code,
which injects energetic electrons into the MFR and tracks their evolution.
Finally, we feed the electron distributions and solar wind parameters into the
Ultimate Fast Gyrosynchrotron (GS) Codes to compute radio emission along lines
of sight. Electrons injected close to the MFR's central axis remain largely
confined, producing a GS emission spectrum resembling observed type IV
characteristics. Varying observer positions, CME properties, and spectral
indices of the electron energy distributions modify the intensities and
durations of the observed bursts. The strongest GS emission is observed to
originate from the CME flanks. Our results indicate that GS emission is the
major component in type IV spectra, although additional contributors cannot be
ruled out.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [46] [Physics-Driven Neural Network for Solving Electromagnetic Inverse Scattering Problems](https://arxiv.org/abs/2507.16321)
*Yutong Du,Zicheng Liu,Bazargul Matkerim,Changyou Li,Yali Zong,Bo Qi,Jingwei Kou*

Main category: eess.IV

TL;DR: A physics-driven neural network (PDNN) approach for inverse scattering problems that avoids generalization issues by training only on scattered field data rather than requiring large datasets, achieving high accuracy and stability for composite lossy scatterers.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning methods for inverse scattering problems heavily rely on large datasets and suffer from limited generalization capabilities, creating a need for a more robust approach that doesn't depend on extensive training data.

Method: A physics-driven neural network (PDNN) that iteratively updates solutions by optimizing hyperparameters through minimizing a loss function incorporating scattered field constraints and prior scatterer information. The method only requires input scattered fields and computation of corresponding predicted scattered fields, with subregion identification to accelerate imaging efficiency.

Result: Numerical and experimental validation showed high reconstruction accuracy and strong stability, even when dealing with challenging composite lossy scatterers. The approach successfully avoids generalization problems inherent in data-driven methods.

Conclusion: The proposed PDNN scheme offers a superior alternative to data-driven neural networks for inverse scattering problems by eliminating the need for extensive training datasets while maintaining high accuracy and stability across different scatterer types.

Abstract: In recent years, deep learning-based methods have been proposed for solving
inverse scattering problems (ISPs), but most of them heavily rely on data and
suffer from limited generalization capabilities. In this paper, a new solving
scheme is proposed where the solution is iteratively updated following the
updating of the physics-driven neural network (PDNN), the hyperparameters of
which are optimized by minimizing the loss function which incorporates the
constraints from the collected scattered fields and the prior information about
scatterers. Unlike data-driven neural network solvers, PDNN is trained only
requiring the input of collected scattered fields and the computation of
scattered fields corresponding to predicted solutions, thus avoids the
generalization problem. Moreover, to accelerate the imaging efficiency, the
subregion enclosing the scatterers is identified. Numerical and experimental
results demonstrate that the proposed scheme has high reconstruction accuracy
and strong stability, even when dealing with composite lossy scatterers.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [47] [From heteroclinic loops to homoclinic snaking in reversible systems: rigorous forcing through computer-assisted proofs](https://arxiv.org/abs/2507.16798)
*Jan Bouwe van den Berg,Gabriel William Duchesne,Jean-Philippe Lessard*

Main category: math.DS

TL;DR: This paper uses computer-assisted proofs to rigorously demonstrate homoclinic snaking in nonlinear pattern-forming systems, specifically proving its existence in Swift-Hohenberg and Gray-Scott problems through the analysis of heteroclinic connections.


<details>
  <summary>Details</summary>
Motivation: Homoclinic snaking is a widespread phenomenon in pattern-forming systems, but demonstrating its occurrence in non-perturbative regimes has been challenging. Traditional forcing theory based on patterned front solutions is difficult to analyze due to the nonlinear nature of these heteroclinic solutions.

Method: The authors employ computer-assisted proofs to find parameterized loops of heteroclinic connections between equilibria and periodic orbits in time reversible systems. This computational approach provides rigorous mathematical verification of the existence of these complex dynamical structures.

Result: The researchers successfully proved homoclinic snaking in both the Swift-Hohenberg and Gray-Scott problems using their computer-assisted proof methodology. They demonstrated the existence of continuous families of connecting orbits in these nonlinear dynamical systems.

Conclusion: Computer-assisted proofs of continuous families of connecting orbits in nonlinear dynamical systems are a powerful tool for understanding global dynamics and their parameter dependence. This approach provides rigorous verification of complex phenomena like homoclinic snaking that are difficult to analyze through traditional analytical methods.

Abstract: Homoclinic snaking is a widespread phenomenon observed in many
pattern-forming systems. Demonstrating its occurrence in non-perturbative
regimes has proven difficult, although a forcing theory has been developed
based on the identification of patterned front solutions. These heteroclinic
solutions are themselves challenging to analyze due to the nonlinear nature of
the problem. In this paper, we use computer-assisted proofs to find
parameterized loops of heteroclinic connections between equilibria and periodic
orbits in time reversible systems. This leads to a proof of homoclinic snaking
in both the Swift-Hohenberg and Gray-Scott problems. Our results demonstrate
that computer-assisted proofs of continuous families of connecting orbits in
nonlinear dynamical systems are a powerful tool for understanding global
dynamics and their dependence on parameters.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [48] [Dynamic correlations in a polar fluid: confronting stochastic density functional theory to simulations](https://arxiv.org/abs/2507.16239)
*Sleeba Varghese,Pierre Illien,Benjamin Rotenberg*

Main category: cond-mat.soft

TL;DR: This paper develops a Stochastic Density Functional Theory (SDFT) framework to model polarization dynamics in dipolar liquids, specifically the Stockmayer fluid, and validates it against Brownian Dynamics simulations.


<details>
  <summary>Details</summary>
Motivation: Understanding the dynamic behavior of polar fluids is essential for modeling complex systems such as electrolytes and biological media, requiring better theoretical frameworks to describe polarization dynamics in dipolar liquids.

Method: The authors develop a Stochastic Density Functional Theory (SDFT) framework starting from overdamped Langevin dynamics of dipolar particles, derive analytical expressions for intermediate scattering functions and dynamic structure factors of polarization field components, and incorporate the Kirkwood factor into a modified SDFT to improve accuracy.

Result: SDFT accurately captures longitudinal polarization fluctuations but underestimates transverse fluctuations due to neglect of dipolar correlations. The modified SDFT with Kirkwood factor achieves quantitative agreement for both longitudinal and transverse components across various dipole strengths when compared to Brownian Dynamics simulations.

Conclusion: SDFT serves as an effective coarse-grained description of polar fluid dynamics, and the study provides valuable insights into the role of collective effects in polarization relaxation, with the modified approach successfully accounting for dipolar correlations.

Abstract: Understanding the dynamic behavior of polar fluids is essential for modeling
complex systems such as electrolytes and biological media. In this work, we
develop and apply a Stochastic Density Functional Theory (SDFT) framework to
describe the polarization dynamics in the Stockmayer fluid, a prototypical
model of dipolar liquids consisting of Lennard-Jones particles with embedded
point dipoles. Starting from the overdamped Langevin dynamics of dipolar
particles, we derive analytical expressions for the intermediate scattering
functions and dynamic structure factors of the longitudinal and transverse
components of the polarization field, within linearized SDFT. To assess the
theory's validity, we compare its predictions with results from Brownian
Dynamics simulations of the Stockmayer fluid. We find that SDFT captures the
longitudinal polarization fluctuations accurately, while transverse
fluctuations are underestimated due to the neglect of dipolar correlations. By
incorporating the Kirkwood factor into a modified SDFT, we recover quantitative
agreement for both components across a range of dipole strengths. This study
highlights the utility of SDFT as a coarse-grained description of polar fluid
dynamics and provides insights into the role of collective effects in
polarization relaxation.

</details>


### [49] [A general model for frictional contacts in colloidal systems](https://arxiv.org/abs/2507.16388)
*Kay Hofmann,Kay-Robert Dormann,Benno Liebchen,Friederike Schmid*

Main category: cond-mat.soft

TL;DR: This paper develops thermodynamically consistent models for frictional contacts in colloidal simulations by deriving proper fluctuation-dissipation relations, creating new DPD thermostats that couple rotation and translation, and demonstrates applications in flow and active matter systems.


<details>
  <summary>Details</summary>
Motivation: Frictional contacts between particles are often neglected in colloidal simulations, but they are important because they couple translational and rotational degrees of freedom, affecting collective behavior in systems like colloids under shear and chiral active matter. Existing deterministic models don't properly account for thermal fluctuations at the colloidal scale.

Method: The authors derive correct fluctuation-dissipation relations for both linear and nonlinear instantaneous frictional contact interactions, ensuring thermodynamic consistency. This leads to the development of a new generalized class of dissipative particle dynamics (DPD) thermostats that incorporate rotation-translation coupling.

Result: The work produces thermodynamically consistent frictional contact models and demonstrates their effects through simulations of Poiseuille flow and motility induced phase separation in active Langevin particles, showing how frictional contacts influence collective behavior.

Conclusion: The paper successfully establishes proper theoretical foundation for including frictional contacts in colloidal simulations with thermal fluctuations, providing new computational tools (generalized DPD thermostats) that can better capture the physics of real colloidal systems where rotation-translation coupling matters.

Abstract: In simulations of colloidal matter, frictional contacts between particles are
often neglected. For spherical colloids, such an approximation can be
problematic, since frictional contacts couple translational and rotational
degrees of freedom, which may affect the collective behavior of, e.g., colloids
under shear and chiral active matter. Deterministic models for frictional
contacts have been proposed in the granular matter community. On the colloidal
scale, however, thermal fluctuations are important and should be included in a
thermodynamically consistent manner. Here, we derive the correct
fluctuation-dissipation relation for linear and nonlinear instantaneous
frictional contact interactions. Among other, this generates a new generalized
class of dissipative particle dynamics (DPD) thermostats with
rotation-translation coupling. We demonstrate effects of frictional contact
interactions using the examples of Poiseuille flow and motility induced phase
separation in active Langevin particles.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [50] [Backwards uniqueness for Mean curvature flow with asymptotically conical singularities](https://arxiv.org/abs/2507.16805)
*J. M. Daniels-Holgate,Or Hershkovits*

Main category: math.DG

TL;DR: This paper proves a backwards uniqueness result for mean curvature flow with singularities, showing that if two flows have the same singular limit and only isolated conical singularities, they must be identical throughout their evolution.


<details>
  <summary>Details</summary>
Motivation: The motivation is to establish the first backwards uniqueness result for geometric flows with singularities that doesn't require self-shrinking or global asymptotically conical behavior assumptions, addressing a fundamental question in the theory of mean curvature flow.

Method: The authors develop new global analytical tools that can simultaneously handle three challenging aspects: the core of the singularity, its asymptotic structure, and the smooth parts of the flows, enabling analysis of mean curvature flows with isolated, multiplicity one, asymptotically conical singularities.

Result: The main result proves that if two mean curvature flows of compact hypersurfaces encounter only isolated, multiplicity one, asymptotically conical singularities at the first singular time T, and have identical limits at the singular time, then the flows are identical for all times up to the singularity.

Conclusion: This work establishes the first backwards uniqueness theorem for geometric flows with singularities under minimal assumptions, representing a significant advance in understanding the uniqueness properties of mean curvature flow and requiring novel mathematical techniques to handle the complex structure of singularities.

Abstract: In this paper we demonstrate that if two mean curvature flows of compact
hypersurfaces $M^1_t$ and $M^2_t$ encounter only isolated, multiplicity one,
asymptotically conical singularities at the first singular time $T$, and if
$M^1_T=M^2_T$ then $M^1_t=M^2_t$ for every $t\in [0,T]$. This is seemingly the
first backwards uniqueness result for any geometric flow with singularities,
that assumes neither {self-shrinking} nor global asymptotically conical
{behaviour}. This necessitates the development of new global tools to deal with
both the core of the singularity, its asymptotic structure, and the smooth part
of the flows simultaneously.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [51] [The Intrinsic Riemannian Proximal Gradient Method for Convex Optimization](https://arxiv.org/abs/2507.16055)
*Ronny Bergmann,Hajg Jasa,Paula John,Max Pfeffer*

Main category: math.OC

TL;DR: This paper introduces a convex Riemannian proximal gradient (CRPG) method for geodesically convex optimization on Hadamard manifolds, achieving sublinear convergence for convex problems and linear convergence for strongly convex problems while demonstrating computational advantages over existing methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop an efficient optimization method for geodesically convex problems on Hadamard manifolds where the objective function consists of both smooth and nonsmooth components, addressing the need for intrinsic methods that work directly on the manifold without requiring embedding or tangent space operations.

Method: The paper introduces an intrinsic convex Riemannian proximal gradient (CRPG) method that employs the manifold proximal map for handling the nonsmooth component of the objective function, operating directly on the manifold structure rather than in embedding or tangent spaces.

Result: The method achieves sublinear convergence rates for convex problems and linear convergence rates for strongly convex problems. The authors derive fundamental proximal gradient inequalities that generalize Euclidean results to the Riemannian setting. Numerical experiments on hyperbolic spaces and manifolds of symmetric positive definite matrices show substantial computational advantages over existing methods.

Conclusion: The CRPG method provides an effective intrinsic approach for solving geodesically convex optimization problems on Hadamard manifolds with composite objective functions, offering theoretical convergence guarantees and practical computational benefits compared to existing approaches.

Abstract: We consider a class of (possibly strongly) geodesically convex optimization
problems on Hadamard manifolds, where the objective function splits into the
sum of a smooth and a possibly nonsmooth function. We introduce an intrinsic
convex Riemannian proximal gradient (CRPG) method that employs the manifold
proximal map for the nonsmooth step, without operating in the embedding or
tangent space. A sublinear convergence rate for convex problems and a linear
convergence rate for strongly convex problems is established, and we derive
fundamental proximal gradient inequalities that generalize the Euclidean case.
Our numerical experiments on hyperbolic spaces and manifolds of symmetric
positive definite matrices demonstrate substantial computational advantages
over existing methods.

</details>


### [52] [A robust and stable phase field method for structural topology optimization](https://arxiv.org/abs/2507.16519)
*Huangxin Chen,Piaopiao Dong,Dong Wang,Xiao-Ping Wang*

Main category: math.OC

TL;DR: A novel phase-field-based topology optimization method that uses an order parameter function to solve minimum compliance problems while guaranteeing bound preservation, volume conservation, and monotonic objective decay through an operator-splitting algorithm with Lagrange multipliers and limiter mechanisms.


<details>
  <summary>Details</summary>
Motivation: Traditional topology optimization methods face challenges in handling domain-dependent body forces and ensuring simultaneous satisfaction of critical optimization properties (bound preservation, volume conservation, and monotonic objective decay). A more robust framework is needed that can intrinsically represent design domains and boundaries while maintaining mathematical rigor throughout the optimization process.

Method: The paper develops a phase-field-based methodology using an order parameter function to characterize optimal structures, similar to phase-field models in materials science. The topology optimization problem is reformulated as a constrained minimization problem. An operator-splitting algorithm incorporating Lagrange multipliers is developed, enhanced with a novel limiter mechanism to handle domain-dependent body forces and ensure all critical optimization properties are satisfied.

Result: The proposed hybrid approach successfully guarantees strict bound preservation, exact volume conservation, and correct objective functional decaying rate. Numerical implementation shows the scheme's robustness through comprehensive 2D and 3D benchmark tests, demonstrating the effectiveness of the methodology in solving minimum compliance topology optimization problems.

Conclusion: The phase-field-based topology optimization framework with operator-splitting algorithm and limiter mechanism provides a mathematically rigorous and computationally robust solution for minimum compliance problems. The method successfully addresses the challenges of domain-dependent body forces while maintaining all critical optimization properties, as validated through extensive numerical benchmarks in both 2D and 3D cases.

Abstract: This paper presents a novel phase-field-based methodology for solving minimum
compliance problems in topology optimization under fixed external loads and
body forces. The proposed framework characterizes the optimal structure through
an order parameter function, analogous to phase-field models in materials
science, where the design domain and its boundary are intrinsically represented
by the order parameter function. The topology optimization problem is
reformulated as a constrained minimization problem with respect to this order
parameter, requiring simultaneous satisfaction of three critical properties:
bound preservation, volume conservation, and monotonic objective functional
decay throughout the optimization process. The principal mathematical challenge
arises from handling domain-dependent body forces, which necessitates the
development of a constrained optimization framework. To address this, we
develop an operator-splitting algorithm incorporating Lagrange multipliers,
enhanced by a novel limiter mechanism. This hybrid approach guarantees strict
bound preservation, exact volume conservation, and correct objective functional
decaying rate. Numerical implementation demonstrates the scheme's robustness
through comprehensive 2D and 3D benchmarks.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [53] [Adaptive Transition State Refinement with Learned Equilibrium Flows](https://arxiv.org/abs/2507.16521)
*Samir Darouich,Vinh Tong,Tanja Bien,Johannes Kästner,Mathias Niepert*

Main category: physics.chem-ph

TL;DR: A new generative AI approach improves initial guesses for transition state structures in chemical reactions, significantly reducing structural errors and speeding up quantum optimization by 3x while increasing success rates by 41%.


<details>
  <summary>Details</summary>
Motivation: Identifying transition states (TSs) - the high-energy configurations molecules pass through during chemical reactions - is essential for understanding and designing chemical processes, but accurately and efficiently identifying these states remains one of the most challenging problems in computational chemistry.

Method: A generative AI approach that improves the quality of initial guesses for TS structures, designed to be combined with existing techniques including machine learning models and fast approximate quantum methods to refine their predictions toward chemically accurate results.

Result: When applied to ML model TS guesses, the method reduces median structural error to 0.088 Å and median absolute error in reaction barrier heights to 0.79 kcal/mol. Starting from tight-binding approximation, it increases TS location success rate by 41% and speeds up high-level quantum optimization by 3x.

Conclusion: By making TS searches more accurate, robust, and efficient, this method could accelerate reaction mechanism discovery and support the development of new materials, catalysts, and pharmaceuticals.

Abstract: Identifying transition states (TSs), the high-energy configurations that
molecules pass through during chemical reactions, is essential for
understanding and designing chemical processes. However, accurately and
efficiently identifying these states remains one of the most challenging
problems in computational chemistry. In this work, we introduce a new
generative AI approach that improves the quality of initial guesses for TS
structures. Our method can be combined with a variety of existing techniques,
including both machine learning models and fast, approximate quantum methods,
to refine their predictions and bring them closer to chemically accurate
results. Applied to TS guesses from a state-of-the-art machine learning model,
our approach reduces the median structural error to just 0.088
$\unicode{x212B}$ and lowers the median absolute error in reaction barrier
heights to 0.79 kcal mol$^{-1}$. When starting from a widely used tight-binding
approximation, it increases the success rate of locating valid TSs by 41\% and
speeds up high-level quantum optimization by a factor of three. By making TS
searches more accurate, robust, and efficient, this method could accelerate
reaction mechanism discovery and support the development of new materials,
catalysts, and pharmaceuticals.

</details>


<div id='astro-ph.CO'></div>

# astro-ph.CO [[Back]](#toc)

### [54] [An AI super-resolution field emulator for cosmological hydrodynamics: the Lyman-α forest](https://arxiv.org/abs/2507.16189)
*Fatemeh Hafezianzadeh,Xiaowen Zhang,Yueying Ni,Rupert A. C. Croft,Tiziana DiMatteo,Mahdi Qezlou,Simeon Bird*

Main category: astro-ph.CO

TL;DR: A two-stage deep learning framework that can efficiently generate high-resolution cosmological hydrodynamic simulations from low-resolution inputs, achieving ~450x speedup while maintaining subpercent accuracy for key observables.


<details>
  <summary>Details</summary>
Motivation: Traditional high-resolution cosmological hydrodynamic simulations are computationally expensive, limiting the ability to generate large-volume mock datasets needed for next-generation cosmological surveys. There is a need for faster, accurate alternatives to full hydrodynamic simulations.

Method: A two-stage deep learning model: (1) stochastically generates high-resolution baryonic fields from low-resolution hydrodynamic simulations, (2) deterministically refines these fields using high-resolution initial conditions to reconstruct small-scale structures including displacement, velocity, internal energy, and gas/star classification. Trained on paired low- and high-resolution simulations from MP-Gadget.

Result: Achieves subpercent error for overdensity, temperature, velocity, and optical depth fields; 1.07% mean relative error in large-scale flux power spectrum; less than 10% error in flux probability distribution function; captures small-scale structures down to 100 kpc pressure smoothing scale relevant to Lyman-α forest; reduces compute time by factor of ~450 compared to full smoothed particle hydrodynamics.

Conclusion: The framework demonstrates significant potential as a powerful and efficient tool for generating high-resolution cosmological fields, offering fast and accurate alternatives to traditional hydrodynamic simulations and enabling creation of large-volume mock datasets for future cosmological surveys.

Abstract: We extend our super-resolution and emulation framework for cosmological dark
matter simulations to include hydrodynamics. We present a two-stage deep
learning model to emulate high-resolution (HR-HydroSim) baryonic fields from
low-resolution (LR-HydroSim) simulations at redshift $z = 3$. The method takes
as inputs an LR-HydroSim and the high-resolution initial conditions
(HR-HydroICs). First, the model stochastically generates high-resolution
baryonic fields from the LR-HydroSim. Second, a deterministic emulator refines
these fields using HR-HydroICs to reconstruct small-scale structures including
displacement, velocity, internal energy, and gas/star classification. Trained
on paired low- and high-resolution simulations produced with
\texttt{MP-Gadget}, the model captures small-scale structures of the
intergalactic medium and %Lyman-$\alpha$ forest observables down to the 100 kpc
pressure smoothing scale relevant to the Lyman-$\alpha$ forest. The model
achieves subpercent error for overdensity, temperature, velocity, and optical
depth fields, a mean relative error of 1.07\% in the large-scale flux power
spectrum (\(k < 3 \times 10^{-2}\ \mathrm{s/km}\)), and less than 10\% error in
the flux probability distribution function. Notably, the two-stage model
reduces the compute time by a factor of $\sim$450 compared to full smoothed
particle hydrodynamics at the same resolution. This work demonstrates the
potential of this framework as a powerful and efficient tool for generating
high-resolution fields offering fast and accurate alternatives to traditional
cosmological hydrodynamic simulations and enabling large-volume mock datasets
for next-generation cosmological surveys.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [55] [Singularities of Dirac-Coulomb propagators](https://arxiv.org/abs/2507.16085)
*Dean Baskin,Michał Wrochna,Jared Wunsch*

Main category: math-ph

TL;DR: This paper studies singularities in Dirac field propagators and N-point functions in Coulomb potentials, proving that in/out Dirac-Coulomb vacua are Hadamard states and that relative charge density between Hadamard states is well-defined, using diffractive propagation techniques.


<details>
  <summary>Details</summary>
Motivation: To rigorously analyze the mathematical structure of quantum field theory in the presence of Coulomb potentials, particularly understanding the singularity behavior of propagators and establishing well-defined charge density differences between quantum states near the Coulomb singularity at r=0.

Method: The authors employ a diffractive propagation of singularities theorem for the Dirac-Coulomb system, which they generalize to handle time-dependent potentials. This mathematical framework allows them to track how singularities propagate through the quantum field system.

Result: The paper establishes two main results: (1) The in and out Dirac-Coulomb vacua satisfy the Hadamard condition for r≠0, ensuring they are physically reasonable quantum states, and (2) The relative charge density between any two Hadamard states is mathematically well-defined as a locally integrable function, even in the problematic region near r=0.

Conclusion: The work provides a rigorous mathematical foundation for quantum field theory in Coulomb potentials by proving that the vacuum states have the correct regularity properties and that charge density differences remain well-behaved near the Coulomb singularity, extending previous results to time-dependent scenarios.

Abstract: In this paper we study singularities of propagators and $N$-point functions
for Dirac fields in a Coulomb potential, possibly with a $t$-dependent smooth
part for $|t|<T<\infty$. We show that the in and out Dirac-Coulomb vacua are
Hadamard states for $r\neq 0$. Furthermore, we prove that the relative charge
density of any two Hadamard states is well-defined as a locally integrable
function including near $r=0$. The results are based on a diffractive
propagation of singularities theorem for the Dirac-Coulomb system previously
obtained by the first and third authors, generalized here to the case of
$t$-dependent potentials.

</details>
