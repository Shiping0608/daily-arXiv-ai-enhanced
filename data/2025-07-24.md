<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 12]
- [math.AP](#math.AP) [Total: 12]
- [physics.comp-ph](#physics.comp-ph) [Total: 3]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 5]
- [cs.LG](#cs.LG) [Total: 1]
- [astro-ph.EP](#astro-ph.EP) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 2]
- [quant-ph](#quant-ph) [Total: 4]
- [cond-mat.mes-hall](#cond-mat.mes-hall) [Total: 1]
- [math.DS](#math.DS) [Total: 2]
- [cs.IT](#cs.IT) [Total: 1]
- [math.PR](#math.PR) [Total: 4]
- [cs.GR](#cs.GR) [Total: 1]
- [math.DG](#math.DG) [Total: 1]
- [astro-ph.SR](#astro-ph.SR) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [A parameterized block-splitting preconditioner for indefinite least squares problem](https://arxiv.org/abs/2507.16938)
*Davod Khojasteh Salkuyeh*

Main category: math.NA

TL;DR: A block splitting-based stationary iteration method is proposed for indefinite least squares problems, with optimal parameter selection and preconditioning for GMRES acceleration, demonstrating effectiveness through numerical experiments.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient iterative method for solving indefinite least squares problems, which are challenging due to their indefinite nature and require specialized approaches for effective solution.

Method: A stationary iteration method based on block splitting with optimal parameter selection, combined with a preconditioner applied to accelerate GMRES convergence. The approach includes theoretical analysis of convergence properties and eigenpair distribution of the preconditioned matrix.

Result: The proposed preconditioner effectively accelerates GMRES convergence for indefinite least squares problems. Numerical experiments demonstrate the method's effectiveness and show favorable comparisons with other well-known methods.

Conclusion: The block splitting-based stationary iteration with optimal parameter selection provides an effective preconditioning strategy for indefinite least squares problems, successfully accelerating GMRES convergence and outperforming existing methods in numerical comparisons.

Abstract: We present a stationary iteration based upon a block splitting for a class of
indefinite least squares problem. Convergence of the proposed method is
investigated and optimal value of the involving parameter is used. The induced
preconditioner is applied to accelerate the convergence of the GMRES method for
solving the problem. We also analysed the eigenpair distribution of the
preconditioned matrix. Some numerical are presented to show the effectiveness
of the preconditioner. Numerical comparison with other well-known methods are
also presented.

</details>


### [2] [DDFEM: A Python Package for Diffuse Domain Methods](https://arxiv.org/abs/2507.16964)
*Luke Benfield,Andreas Dedner*

Main category: math.NA

TL;DR: This paper introduces ddfem, a Python package that implements the Diffuse Domain Method (DDM) to solve PDEs on complex domains by reformulating them on simpler domains using phase-field functions, making DDM more accessible to researchers.


<details>
  <summary>Details</summary>
Motivation: Solving PDEs on complex geometries presents significant computational challenges. Traditional methods struggle with complex domain handling, so there's a need for more accessible tools that can transform complex domain problems into simpler formulations while maintaining accuracy.

Method: The paper develops ddfem, a Python package that uses the Diffuse Domain Method to embed complex geometries into simpler domains via phase-field functions. The package combines signed distance functions for domain definition, provides DDM transformers for second-order evolution equations, and introduces a normalized weighting approach for handling multiple boundary conditions using multiple phase fields.

Result: The ddfem package successfully provides an extensible framework for DDM transformations, offers intuitive complex domain definition through signed distance functions, handles multiple boundary condition types on distinct segments, and maintains compatibility with existing finite element solvers through UFL integration. Experiments were validated using the Dune-Fem framework.

Conclusion: The ddfem package makes Diffuse Domain Methods more accessible and practical for solving PDEs on complex domains. By providing a user-friendly Python interface and compatibility with existing finite element frameworks, it enables broader adoption of DDM approaches in computational science applications.

Abstract: Solving partial differential equations (PDEs) on complex domains can present
significant computational challenges. The Diffuse Domain Method (DDM) is an
alternative that reformulates the partial differential equations on a larger,
simpler domain. The original geometry is embedded into the problem by
representing it with a phase-field function. This paper introduces ddfem, an
extensible Python package to provide a framework for transforming PDEs into a
Diffuse Domain formulation. We aim to make the application of a variety of
different Diffuse Domain approaches more accessible and straightforward to use.
The ddfem package includes features to intuitively define complex domains by
combining signed distance functions and provides a number of DDM transformers
for general second evolution equations. In addition, we present a new approach
for combining multiple boundary conditions of different types on distinct
boundary segments. This is achieved by applying a normalised weighting, derived
from multiple phase fields, to combine the additional boundary terms in the
Diffuse Domain formulations. The domain definition and Diffuse Domain
transformation provided by our package are designed to be compatible with a
wide range of existing finite element solvers without requiring code
alterations. Both the original (non-linear) PDEs provided by the user and the
resulting transformed PDEs on the extended domain are defined using the Unified
Form Language UFL which is a domain specific language used by a number of
software packages. Our experiments were carried out using the Dune-Fem
framework.

</details>


### [3] [Exact closure for discrete large-eddy simulation](https://arxiv.org/abs/2507.17051)
*Syver DÃ¸ving Agdestein,Roel Verstappen,Benjamin Sanderse*

Main category: math.NA

TL;DR: This paper proposes new discretization-consistent expressions for the sub-filter stress (SFS) tensor in discrete Large Eddy Simulation (LES) using a novel two-grid filter approach that can exactly compute SFS when DNS data is available, showing that including non-symmetric components significantly improves accuracy compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional sub-filter stress expressions in discrete LES are discretization-inconsistent, leading to accumulated errors over time. The commonly used expression $\overline{u u} - \bar{u} \bar{u}$ does not properly account for the discrete nature of numerical schemes, creating a need for discretization-consistent approaches that better represent the physics in computational fluid dynamics simulations.

Method: The authors introduce a new two-grid filter with a "filter-swap" property that allows filtering and finite differencing to be interchanged. They develop three different two-grid filters based on volume- or surface-averaging for 3D incompressible Navier-Stokes equations. The method enables exact computation of SFS tensors when DNS data is available and produces commutator expressions in structural form (discrete divergence of SFS tensor).

Result: The new discretization-consistent SFS expressions are markedly different from traditional approaches, particularly for 1D Burgers' equation. Volume-averaging produces non-symmetric SFS tensors, but DNS results show the non-symmetric part plays an important role. When including the non-symmetric component, the new SFS expressions achieve zero a-posteriori error in LES, while existing methods show increasing errors over time.

Conclusion: The paper demonstrates that discretization-consistent SFS expressions with non-symmetric components significantly outperform traditional symmetric approaches in LES accuracy. The authors propose using non-symmetric tensor-basis closure models to approximate the new exact SFS expressions, providing a foundation for more accurate turbulence modeling in computational fluid dynamics.

Abstract: In this article we propose new discretization-consistent expressions for the
sub-filter stress (SFS) tensor in discrete LES, where the filter is induced by
the discretization. We introduce a new two-grid filter that allows us to
exactly compute the SFS tensor when DNS data is available. This new filter
satisfies a "filter-swap" property, such that filtering and finite differencing
can be interchanged and the resulting commutator expressions are of structural
form (they can be written as the discrete divergence of an SFS tensor). For 1D
conservation laws such as Burgers' equation, the resulting
discretization-consistent SFS expression is markedly different from the
commonly used (discretization-inconsistent) expression $\overline{u u} -
\bar{u} \bar{u}$. For the 3D incompressible Navier-Stokes equations, we propose
three new two-grid filters, based on either volume- or surface-averaging, each
inducing new discretization-consistent commutator expressions. We show that
volume-averaging is required to obtain a commutator expression of structural
form. However, the resulting SFS tensor is shown to be non-symmetric. Based on
DNS results, we show that the non-symmetric part of the SFS tensor plays an
important role in the discrete LES equation. When the non-symmetric part is
included, our SFS expressions give zero a-posteriori error in LES, while
existing SFS expressions give errors that increase over time. We propose to use
a class of non-symmetric tensor-basis closure models to approximate the new
exact SFS expressions.

</details>


### [4] [Matrix-Free Evaluation of High-Order Shifted Boundary Finite Element Operators](https://arxiv.org/abs/2507.17053)
*MichaÅ Wichrowski*

Main category: math.NA

TL;DR: This paper presents a matrix-free implementation of the shifted boundary method (SBM) for finite element analysis, achieving O(p^{2d-1}) computational complexity per face and demonstrating superior performance compared to matrix-free CutFEM.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop an efficient computational approach for solving partial differential equations on complex geometries using the shifted boundary method, while avoiding the computational overhead of explicit global matrix assembly in finite element analysis.

Method: The method employs a matrix-free approach that uses precomputed data and tensor-product structures to efficiently evaluate shifted boundary operators, avoiding explicit assembly of global matrices while maintaining the versatility of SBM for handling complex geometries.

Result: The proposed method achieves computational complexity of O(p^{2d-1}) per face for evaluating shifted boundary contributions on elements of polynomial degree p in d dimensions, and numerical experiments validate its accuracy, efficiency, and scalability for both continuous and discontinuous Galerkin formulations.

Conclusion: The matrix-free shifted boundary method implementation successfully provides an efficient and scalable solution for high-order finite element methods on complex geometries, outperforming matrix-free CutFEM implementations while maintaining computational accuracy.

Abstract: This paper presents a matrix-free approach for implementing the shifted
boundary method (SBM) in finite element analysis. The SBM is a versatile
technique for solving partial differential equations on complex geometries by
shifting boundary conditions to nearby surrogate boundaries. We focus on the
efficient evaluation of shifted boundary operators using precomputed data and
tensor-product structures. The proposed method avoids the explicit assembly of
global matrices, achieving a computational complexity of $O(p^{2d-1})$ per face
for the evaluation of shifted boundary contributions on elements of polynomial
degree $p$ in $d$ dimensions. Numerical experiments validate the accuracy and
efficiency of the approach, demonstrating its scalability and applicability to
high-order finite element methods for both continuous and discontinuous
Galerkin formulations. We compare the performance of the proposed method with a
matrix-free CutFEM implementation.

</details>


### [5] [Explicit Monotone Stable Super-Time-Stepping Methods for Finite Time Singularities](https://arxiv.org/abs/2507.17062)
*Zheng Tan,Tariq D. Aslam,Andrea L. Bertozzi*

Main category: math.NA

TL;DR: This paper introduces RKL and RKG super-time-stepping methods for numerically resolving finite-time singularities in nonlinear parabolic PDEs, achieving faster computation than implicit methods while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Finite-time singularities in nonlinear PDEs are challenging to resolve numerically due to large scale ranges and stability constraints that typically require implicit methods. There's a need for efficient explicit methods that can handle these singularities without compromising stability or accuracy.

Method: The paper employs Runge-Kutta-Legendre (RKL) and Runge-Kutta-Gegenbauer (RKG) super-time-stepping methods - multi-stage single step second-order explicit methods where timestep size scales as O(sÂ²) with stage number s. These methods avoid implicit solves while maintaining stability for diffusion problems.

Result: The RKL and RKG methods successfully resolve self-similar singularity structures in two nonlinear PDE examples, achieving significantly reduced runtime compared to implicit methods while maintaining comparable accuracy. The methods prove superior to traditional super-time-stepping approaches like RKC and ROCK.

Conclusion: RKL and RKG methods provide an effective alternative to implicit methods for solving nonlinear parabolic PDEs with finite-time singularities, offering computational efficiency gains without sacrificing numerical accuracy or stability. Theoretical monotonicity guarantees further establish their superiority over existing super-time-stepping methods.

Abstract: We explore a novel way to numerically resolve the scaling behavior of
finite-time singularities in solutions of nonlinear parabolic PDEs. The
Runge--Kutta--Legendre (RKL) and Runge--Kutta--Gegenbauer (RKG)
super-time-stepping methods were originally developed for nonlinear complex
physics problems with diffusion. These are multi-stage single step
second-order, forward-in-time methods with no implicit solves. The advantage is
that the timestep size for stability scales with stage number $s$ as
$\mathcal{O}(s^2)$. Many interesting nonlinear PDEs have finite-time
singularities, and the presence of diffusion often limits one to using implicit
or semi-implicit timestep methods for stability constraints. Finite-time
singularities are particularly challenging due to the large range of scales
that one desires to resolve, often with adaptive spatial grids and adaptive
timesteps. Here we show two examples of nonlinear PDEs for which the
self-similar singularity structure has time and space scales that are
resolvable using the RKL and RKG methods, without forcing even smaller
timesteps. Compared to commonly-used implicit numerical methods, we achieve
significantly smaller run time while maintaining comparable accuracy. We also
prove numerical monotonicity for both the RKL and RKG methods under their
linear stability conditions for the constant coefficient heat equation, in the
case of infinite domain and periodic boundary condition, leading to a
theoretical guarantee of the superiority of the RKL and RKG methods over
traditional super-time-stepping methods, such as the Runge-Kutta-Chebyshev
(RKC) and the orthogonal Runge-Kutta-Chebyshev (ROCK) methods.

</details>


### [6] [Design and analysis of twisted and BGG Stokes-de Rham polytopal complexes](https://arxiv.org/abs/2507.17333)
*Daniele A. Di Pietro,JÃ©rÃ´me Droniou,Kaibo Hu,Arax Leroy*

Main category: math.NA

TL;DR: This paper develops a discrete Bernstein-Gelfand-Gelfand (BGG) diagram for polygonal meshes using the DDR framework, creating a novel elasticity complex with proven mathematical properties including consistency estimates and PoincarÃ© inequalities.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need to develop robust computational frameworks for elasticity problems on generic polygonal meshes, extending the classical BGG construction from differential geometry to discrete settings while maintaining essential mathematical properties.

Method: The authors use the Discrete De Rham (DDR) framework to construct a discrete BGG diagram consisting of a discrete Stokes polygonal complex and a tensorised Discrete De Rham complex, which together form the basis for building an elasticity complex on polygonal meshes.

Result: The paper establishes complete homological and analytical properties of the discrete Stokes complex, including primal and adjoint consistency estimates and PoincarÃ© inequalities. Additionally, homological properties of the complexes derived from the BGG diagram are proven.

Conclusion: The discrete BGG construction successfully provides a mathematically rigorous elasticity complex for polygonal meshes with established theoretical guarantees, offering a new computational tool for elasticity problems on general polygonal discretizations.

Abstract: We design a discrete Bernstein--Gelfand--Gelfand (BGG) diagram on polygonal
meshes based on the DDR framework; the diagram is made of a discrete Stokes
polygonal complex and a tensorised Discrete De Rham complex, and the BGG
construction leads to a novel elasticity complex applicable on generic
polygonal meshes. Complete homological and analytical properties of the
discrete Stokes complex are established, including primal and adjoint
consistency estimates as well as Poincar\'e inequalities. Homological
properties of the complexes built from the BGG diagram are also established.

</details>


### [7] [An FDM-sFEM scheme on time-space manifolds and its superconvergence analysis](https://arxiv.org/abs/2507.17378)
*Chengrun Jiang,Guozhi Dong,Hailong Guo,Zuoqiang Shi*

Main category: math.NA

TL;DR: This paper develops a superconvergent numerical method for solving the Laplace-Beltrami operator on time-space product manifolds with Neumann boundary conditions, combining finite differences in time with surface finite elements in space, and achieves enhanced accuracy through post-processing techniques.


<details>
  <summary>Details</summary>
Motivation: The research is motivated by the need to solve dynamic optimal transport problems on general surfaces, which requires efficient and accurate discretization of the Laplace-Beltrami operator on time-space product manifolds with Neumann temporal boundary conditions.

Method: The authors propose a coupled numerical scheme that combines finite difference methods for temporal discretization with surface finite element methods for spatial discretization. They establish a new summation by parts formula and implement geometric error analysis within a framework based on Riemannian metric approximation.

Result: The method achieves superconvergence for the recovered gradient through post-processing techniques. The authors prove supercloseness of the semi-discrete solution and derive theoretical superconvergence results, which are validated through numerical examples.

Conclusion: The proposed coupled scheme successfully provides superconvergent discretization of the Laplace-Beltrami operator on time-space product manifolds, offering enhanced accuracy for dynamic optimal transport problems on surfaces through the combination of appropriate temporal and spatial discretization methods with post-processing techniques.

Abstract: We study superconvergent discretization of the Laplace-Beltrami operator on
time-space product manifolds with Neumann temporal boundary values, which arise
in the context of dynamic optimal transport on general surfaces. We propose a
coupled scheme that combines finite difference methods in time with surface
finite element methods in space. By establishing a new summation by parts
formula and proving the supercloseness of the semi-discrete solution, we derive
superconvergence results for the recovered gradient via post-processing
techniques. In addition, our geometric error analysis is implemented within a
novel framework based on the approximation of the Riemannian metric. Several
numerical examples are provided to validate and illustrate the theoretical
results.

</details>


### [8] [Univariate amenable functions](https://arxiv.org/abs/2507.17404)
*Carlos BeltrÃ¡n*

Main category: math.NA

TL;DR: This paper develops theory and practical tests for amenable and compatible functions in the context of univariate real analytic functions, building on previous work that established mathematical conditions for algorithm stability composition.


<details>
  <summary>Details</summary>
Motivation: To provide practical tools and characterizations for determining when backward stable algorithms remain forward stable and when compositions of stable algorithms preserve stability, specifically focusing on univariate real analytic functions.

Method: The authors elaborate on existing theory of amenable and compatible functions by developing simple tests for these concepts and systematically analyzing elementary functions to determine which satisfy the amenability condition.

Result: The paper produces practical tests for identifying amenable and compatible functions and creates comprehensive tables categorizing various elementary functions based on whether they are amenable or not.

Conclusion: The work successfully extends the theoretical framework of amenable and compatible functions to practical applications, providing concrete tools for analyzing the stability properties of algorithms involving univariate real analytic functions.

Abstract: The concepts of amenable and compatible functions have been introduced in a
recent work, in order to state precise mathematical theorems that guarantee
that a backward stable algorithm is also forward stable, and that the
composition of two stable algorithms results in an stable algorithm. In this
work, we elaborate in this theory for univariate real analytic functions,
providing simple tests for both concepts and producing tables for a number of
elementary functions which are or fail to be amenable.

</details>


### [9] [A new data-driven energy-stable Evolve-Filter-Relax model for turbulent flow simulation](https://arxiv.org/abs/2507.17423)
*Anna Ivagnes,Toby van Gastelen,Syver DÃ¸ving Agdestein,Benjamin Sanderse,Giovanni Stabile,Gianluigi Rozza*

Main category: math.NA

TL;DR: This paper proposes a data-driven approach to optimize the evolve-filter-relax (EFR) framework for turbulent flow simulation by learning optimal filters from DNS data in the frequency domain, achieving better accuracy and computational efficiency than traditional methods.


<details>
  <summary>Details</summary>
Motivation: The traditional EFR framework for turbulent flow simulation has limited flexibility with only two parameters and uses suboptimal standard differential filters. There is a need for more accurate and computationally efficient filtering approaches that can better capture turbulent flow dynamics.

Method: The authors develop a data-driven optimization approach that learns optimal filters from DNS data in the frequency domain using one-dimensional least-squares problems for each wavenumber. The relax parameter is determined by enforcing energy and/or enstrophy conservation to ensure method stability.

Result: The learned filter significantly outperforms standard differential filters and the Smagorinsky model in both decaying turbulence and Kolmogorov flow simulations, showing improved accuracy in energy spectra and temporal evolution of energy and enstrophy. The method is also more computationally efficient as it avoids solving linear systems.

Conclusion: The data-driven filter optimization approach successfully enhances the EFR framework's performance, providing better accuracy and computational efficiency for turbulent flow simulations while maintaining stability through energy/enstrophy conservation constraints.

Abstract: We present a novel approach to define the filter and relax steps in the
evolve-filter-relax (EFR) framework for simulating turbulent flows. The EFR
main advantages are its ease of implementation and computational efficiency.
However, as it only contains two parameters (one for the filter step and one
for the relax step) its flexibility is rather limited. In this work, we propose
a data-driven approach in which the optimal filter is found based on DNS data
in the frequency domain. The optimization step is computationally efficient and
only involves one-dimensional least-squares problems for each wavenumber.
Across both decaying turbulence and Kolmogorov flow, our learned filter
decisively outperforms the standard differential filter and the Smagorinsky
model, yielding significantly improved accuracy in energy spectra and in the
temporal evolution of both energy and enstrophy. In addition, the relax
parameter is determined by requiring energy and/or enstrophy conservation,
which enforces stability of the method and reduces the appearance of numerical
wiggles, especially when the filter is built in scarce data regimes. Applying
the learned filter is also more computationally efficient compared to
traditional differential filters, as it circumvents solving a linear system.

</details>


### [10] [Error estimates and adaptivity for a least-squares method applied to the Monge-AmpÃ¨re equation](https://arxiv.org/abs/2507.17569)
*Alexandre Caboussat,Anna Peruso,Marco Picasso*

Main category: math.NA

TL;DR: This paper presents a novel adaptive finite element method for solving the nonlinear Monge-AmpÃ¨re equation using a posteriori error indicators and mesh refinement strategies.


<details>
  <summary>Details</summary>
Motivation: The Monge-AmpÃ¨re equation is a challenging nonlinear PDE that requires efficient numerical methods. Current approaches lack robust adaptive strategies with reliable error control, making it difficult to achieve optimal computational efficiency while maintaining accuracy.

Method: The authors develop an iterative scheme that decouples the problem into (i) a pointwise nonlinear minimization and (ii) a linear biharmonic variational problem with Navier boundary conditions. They use mixed piecewise-linear finite elements and reformulate it as a coupled second-order system to derive both a priori and a posteriori error estimators for adaptive mesh refinement.

Result: Numerical tests demonstrate that errors in different norms scale appropriately with the theoretical predictions. The a posteriori error indicators effectively guide mesh refinement, leading to optimal convergence rates and computational efficiency.

Conclusion: The proposed method successfully combines nonlinear least-squares solving with adaptive finite element techniques, providing a robust and efficient approach for solving Monge-AmpÃ¨re equations on convex polygonal domains with reliable error control.

Abstract: We introduce novel a posteriori error indicators for a nonlinear
least-squares solver for smooth solutions of the Monge--Amp\`ere equation on
convex polygonal domains in $\mathbb{R}^2$. At each iteration, our iterative
scheme decouples the problem into (i) a pointwise nonlinear minimization
problem and (ii) a linear biharmonic variational problem. For the latter, we
derive an equivalence to a biharmonic problem with Navier boundary conditions
and solve it via mixed piecewise-linear finite elements. Reformulating this as
a coupled second-order system, we derive a priori and a posteriori
$\mathbb{P}^1$ finite element error estimators and we design a robust adaptive
mesh refinement strategy. Numerical tests confirm that errors in different
norms scale appropriately. Finally, we demonstrate the effectiveness of our a
posteriori indicators in guiding mesh refinement.

</details>


### [11] [Stable Iterative Solvers for Ill-conditioned Linear Systems](https://arxiv.org/abs/2507.17673)
*Vasileios Kalantzis,Mark S. Squillante,Chai Wah Wu*

Main category: math.NA

TL;DR: This paper proposes general algorithmic frameworks to stabilize Krylov subspace iterative methods for solving ill-conditioned large-scale linear systems, preventing divergence and improving reliability in high-performance computing applications.


<details>
  <summary>Details</summary>
Motivation: Krylov subspace methods for solving large-scale linear systems can diverge when dealing with ill-conditioned systems, which significantly limits their practical applicability in high-performance computing scenarios where such systems are common.

Method: The authors develop general algorithmic frameworks that modify existing Krylov subspace iterative solution methods to ensure algorithmic stability and prevent divergence. These frameworks are then implemented and integrated with current SciPy implementations of iterative methods.

Result: The proposed stable iterative approach demonstrates efficacy through numerical experiments conducted on a wide range of both synthetic and real-world ill-conditioned linear systems, showing improved performance compared to standard methods.

Conclusion: The general frameworks successfully stabilize Krylov subspace methods, making them more reliable for solving ill-conditioned large-scale linear systems and expanding their practical applicability in high-performance computing applications.

Abstract: Iterative solvers for large-scale linear systems such as Krylov subspace
methods can diverge when the linear system is ill-conditioned, thus
significantly reducing the applicability of these iterative methods in practice
for high-performance computing solutions of such large-scale linear systems. To
address this fundamental problem, we propose general algorithmic frameworks to
modify Krylov subspace iterative solution methods which ensure that the
algorithms are stable and do not diverge. We then apply our general frameworks
to current implementations of the corresponding iterative methods in SciPy and
demonstrate the efficacy of our stable iterative approach with respect to
numerical experiments across a wide range of synthetic and real-world
ill-conditioned linear systems.

</details>


### [12] [Data assimilation using a global Girsanov nudged particle filter](https://arxiv.org/abs/2507.17685)
*Maneesh Kumar Singh,Joshua Hope-Collins,Colin J. Cotter,Dan Crisan*

Main category: math.NA

TL;DR: A particle filtering algorithm for infinite dimensional stochastic models using Girsanov perturbations to nudge particles toward high-likelihood regions, with optimized control coupling and three-stage computation framework.


<details>
  <summary>Details</summary>
Motivation: Traditional particle filters struggle with stochastic models on infinite dimensional state spaces due to particle degeneracy and poor effective sample size, requiring better methods to guide particles toward regions of higher likelihood.

Method: Developed a particle filtering algorithm using Girsanov perturbations with coupled control variables across all particles, implemented through a three-stage optimization formulation that separates nonlinearities in ESS terms from forward problem nonlinearities, enabling parallel computation.

Result: Applied to stochastic Kuramoto-Sivashinsky equation, the nudging filter showed faster and more robust response to extreme events in assimilated data compared to temper-jitter particle filter, though with greater spread.

Conclusion: The proposed nudging particle filter with coupled control variables effectively handles infinite dimensional stochastic models and provides superior responsiveness to extreme data events, despite being more spread than alternative approaches.

Abstract: We present a particle filtering algorithm for stochastic models on infinite
dimensional state space, making use of Girsanov perturbations to nudge the
ensemble of particles into regions of higher likelihood. We argue that the
optimal control problem needs to couple control variables for all of the
particles to maintain an ensemble with good effective sample size (ESS). We
provide an optimisation formulation that separates the problem into three
stages, separating the nonlinearity in the ESS term in the functional with the
nonlinearity due to the forward problem, and allowing independent parallel
computation for each particle when calculations are performed over control
variable space. The particle filter is applied to the stochastic
Kuramoto-Sivashinsky equation, and compared with the temper-jitter particle
filter approach. We observe that whilst the nudging filter is over spread
compared to the temper-jitter filter, it responds to extreme events in the
assimilated data more quickly and robustly.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [13] [The $\overline\partial$-Robin Laplacian](https://arxiv.org/abs/2507.16895)
*Joaquim Duran*

Main category: math.AP

TL;DR: This paper studies Robin-type operators with parameter-dependent boundary conditions in 2D bounded domains, analyzing their resolvent convergence and eigenvalue properties as the boundary parameter varies from 0 to infinity, with applications to quantum dot Dirac operators.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand how Robin-type operators behave as their boundary parameter changes continuously, particularly to characterize eigenvalues that appear in quantum dot Dirac operators through this family of operators.

Method: The authors study the family of operators {R_a} associated with Robin boundary value problems featuring the boundary condition 2Î½Ìâ_zÌu + au = 0, analyzing their resolvent convergence properties and eigenvalue behavior as the parameter a varies over [0,+â).

Result: The paper establishes convergence results for the operators in resolvent sense and describes properties of their eigenvalues both for fixed parameter values and as functions of the varying parameter a.

Conclusion: The eigenvalue analysis of these Robin-type operators provides a characterization of positive eigenvalues in quantum dot Dirac operators, establishing a connection between classical boundary value problems and quantum mechanical systems.

Abstract: We study the family of operators $\{\mathcal{R}_a\}_{a\in [0,+\infty)}$
associated to the Robin-type problems in a bounded domain
$\Omega\subset\mathbb{R}^2$ $$
  \begin{cases}
  -\Delta u = f & \text{in } \Omega, \\
  2 \bar \nu \partial_{\bar z} u + au = 0 & \text{on } \partial\Omega,
  \end{cases} $$ and their dependency on the boundary parameter $a$ as it moves
along $[0,+\infty)$. In this regard, we study the convergence of such operators
in a resolvent sense. We also describe the eigenvalues of such operators and
show some of their properties, both for all fixed $a$ and as functions of the
parameter $a$. As shall be seen in more detail in a forthcoming paper, the
eigenvalues of these operators characterize the positive eigenvalues of quantum
dot Dirac operators.

</details>


### [14] [Asymptotic stability of the $2D$ temperature-dependent tropical climate model with the sharp decay rates](https://arxiv.org/abs/2507.17197)
*Hyunjin In,Dong-ha Kim,Junha Kim*

Main category: math.AP

TL;DR: This paper studies the asymptotic stability of a tropical climate model on R^2 with temperature-dependent diffusion and proves global existence of smooth solutions with sharp decay estimates in Sobolev norms.


<details>
  <summary>Details</summary>
Motivation: To analyze the long-term behavior and stability properties of tropical climate systems, particularly investigating how temperature-dependent diffusion affects the dynamics of barotropic and baroclinic modes in atmospheric modeling.

Method: Mathematical analysis using PDE theory to study two cases of the tropical climate model: (1) barotropic mode with linear damping and (2) barotropic mode without damping. The approach involves proving small data global existence and establishing temporal decay estimates in Sobolev spaces H^m(R^2).

Result: Successfully proved small data global existence of smooth solutions for both cases of the model. Established sharp temporal decay estimates for solutions in arbitrary Sobolev norms H^m(R^2) for m â¥ 0, demonstrating the asymptotic stability of the tropical climate system.

Conclusion: The tropical climate model with temperature-dependent diffusion exhibits asymptotic stability under both damping scenarios. The mathematical framework provides rigorous foundation for understanding the long-term behavior of tropical atmospheric dynamics with sharp decay characterization.

Abstract: We investigate the asymptotic stability of a tropical climate model posed on
$\bR^2$, with temperature-dependent diffusion in the barotropic mode $u$ and
linear damping in the first baroclinic mode $v$. We consider two distinct cases
for the barotropic component: one with linear damping and one without. For both
cases, we prove the small data global existence of smooth solutions.
Furthermore, we establish sharp temporal decay estimates for solutions in
arbitrary Sobolev norms $H^m (\bR^2)$, $m \ge 0$.

</details>


### [15] [Formation of vacuum state and delta-shock in the solution of two-dimensional Riemann problem for zero pressure gas dynamics](https://arxiv.org/abs/2507.17213)
*Anamika Pandey,T. Raja Sekhar*

Main category: math.AP

TL;DR: This paper studies two-dimensional pressureless Euler equations with three constant Riemann initial data, focusing on wave interactions and delta shock formation, deriving nine distinct solution patterns including Mach-reflection-like configurations.


<details>
  <summary>Details</summary>
Motivation: To investigate complex wave interactions in two-dimensional pressureless Euler equations, particularly the behavior of contact discontinuities and delta shocks that emerge from three constant Riemann initial data configurations.

Method: Generalized characteristic analysis is employed to derive solution patterns, complemented by numerical simulations using a semidiscrete central upwind scheme to validate theoretical results.

Result: Nine topologically distinct solution patterns are identified, featuring delta shock waves characterized by Dirac delta functions in density and internal energy. Some configurations show Mach-reflection-like features and vacuum region development. Numerical simulations show excellent agreement with analytical results.

Conclusion: The study successfully characterizes the complex wave interaction phenomena in two-dimensional pressureless Euler equations, providing a comprehensive classification of solution patterns and demonstrating the reliability of the theoretical framework through numerical validation.

Abstract: In this article, we investigate the two-dimensional pressureless Euler
equations with three constant Riemann initial data. Our primary focus is on the
wave interactions involving contact discontinuities and delta shocks. A
distinguishing feature of the solution is the emergence of a delta shock wave
which is characterized by a Dirac delta function appearing in both the density
and internal energy variables. By exploiting generalized characteristic
analysis, nine topologically distinct solution patterns are derived. Some of
these configurations exhibit features similar to Mach-reflection and in certain
cases, vacuum regions may also develop. To validate the theoretical results,
numerical simulations are carried out using a semidiscrete central upwind
scheme. The comparison between analytical and numerical results demonstrates
excellent agreement, providing

</details>


### [16] [Long time existence for a Boussinesq-like system with strong topography variation](https://arxiv.org/abs/2507.17282)
*Qi Lu,Jean-Claude Saut,Li Xu*

Main category: math.AP

TL;DR: This paper proves that solutions to a Boussinesq system with strong topography variations exist for long periods of time.


<details>
  <summary>Details</summary>
Motivation: The motivation is to establish mathematical rigor for the long-term behavior of fluid flow systems described by Boussinesq equations when the underlying topography has significant variations, which is important for understanding real-world fluid dynamics in complex terrains.

Method: The paper uses mathematical proof techniques to demonstrate the long time existence of solutions for the Boussinesq system under conditions of strong topographical variations.

Result: The main result is a mathematical proof establishing that solutions to the Boussinesq system with strong topography variations exist over long time periods.

Conclusion: The authors successfully prove long time existence for solutions of Boussinesq systems with strong topography variations, providing theoretical foundation for the stability and persistence of such fluid flow models.

Abstract: We prove the long time existence of solutions of a Boussinesq system with
strong topography variations.

</details>


### [17] [Blow-up problem for porous medium equation with absorption under nonlinear nonlocal boundary condition](https://arxiv.org/abs/2507.17310)
*Alexander Gladkov*

Main category: math.AP

TL;DR: This paper analyzes the porous medium equation with absorption under nonlinear nonlocal boundary conditions, establishing existence, comparison principles, and blow-up behavior of solutions.


<details>
  <summary>Details</summary>
Motivation: To understand the mathematical behavior of porous medium equations with absorption terms under complex nonlinear nonlocal boundary conditions, which are important in modeling fluid flow in porous media with boundary effects.

Method: Mathematical analysis techniques including proving local existence theorems, establishing comparison principles for solution ordering, and analyzing conditions for global existence versus finite-time blow-up of solutions.

Result: Successfully established local existence of solutions, proved comparison principle for solution behavior, determined conditions for global existence, and characterized blow-up phenomena for the porous medium equation with absorption under nonlinear nonlocal boundary conditions.

Conclusion: The paper provides a complete theoretical framework for understanding solution behavior of porous medium equations with absorption under nonlinear nonlocal boundary conditions, covering existence, uniqueness, and long-time dynamics including blow-up scenarios.

Abstract: In this paper we consider initial boundary value problem for porous medium
equation with absorption under nonlinear nonlocal boundary condition and
nonnegative initial datum. We prove local existence, comparison principle,
global existence and blow-up of solutions.

</details>


### [18] [Compensation effects for anisotropic energies of two-dimensional unit vector fields](https://arxiv.org/abs/2507.17345)
*Lia Bronsard,Dmitry Golovaty,Xavier Lamy,Peter Sternberg*

Main category: math.AP

TL;DR: This paper studies the anisotropic energy of 2D unit vector fields in the limit Îµâ0, showing that despite losing gradient control, the energy still controls 1/2-order derivatives, with optimal compactness results in W^{s,3} spaces for s<1/2.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand the behavior of highly anisotropic energy functionals for unit vector fields when one term (curl) becomes negligible compared to another (divergence), particularly investigating what regularity can still be controlled when full gradient control is lost.

Method: The authors adapt tools from hyperbolic conservation laws to analyze the energy functional E_Îµ(u) = â«(div u)Â² + Îµ(curl u)Â² dx for unit vector fields u: Î©ââÂ²âSÂ¹ in the limit Îµâ0, studying compactness properties and characterizing Î-limits.

Result: The main results show that bounded energy sequences are compact in W^{s,3}_loc(Î©) for s<1/2, this 1/2 order of differentiability is optimal (any map in W^{1/2,4}(Î©;SÂ¹) is a limit of bounded energy sequences), boundary traces are compact in LÂ¹(âÎ©), and explicit Î-limit characterizations are provided for single variable maps and thin-film models.

Conclusion: The paper establishes that even when anisotropic energies lose full gradient control, they can still provide meaningful regularity control at the optimal fractional order 1/2, with complete characterization of the limiting behavior through Î-convergence analysis.

Abstract: We study the highly anisotropic energy of two-dimensional unit vector fields
given by \begin{align*} E_\epsilon(u)= \int_{\Omega} (\mathrm{div}\,u)^2 +
\epsilon(\mathrm{curl}\,u)^2\, dx\,, \quad u\colon\Omega\subset\mathbb
R^2\to\mathbb S^1\, \end{align*}
  in the limit $\epsilon\to 0$. This energy clearly loses control on the full
gradient of $u$ as $\epsilon\to 0$, but, adapting tools from hyperbolic
conservations laws, we show that it still controls derivatives of order 1/2. In
particular, any bounded energy sequence $E_\epsilon(u_\epsilon)\leq C$ is
compact in $W^{s,3}_{\mathrm{loc}}(\Omega)$ for $s<1/2$. Moreover, this order
1/2 of differentiability is optimal, in the sense that any map $u\in
W^{1/2,4}(\Omega;\mathbb S^1)$ is a limit of a bounded energy sequence. We also
establish compactness of boundary traces in $L^1(\partial\Omega)$, and
characterize the $\Gamma$-limit in the simpler case of maps of a single
variable and in the case of a thin-film model.

</details>


### [19] [Stable self-similar singularity formation for infinite energy solutions of the incompressible porous medium equations](https://arxiv.org/abs/2507.17381)
*Charles Collot,Christophe Prange,Jin Tan*

Main category: math.AP

TL;DR: This paper analyzes the stability of explicit blow-up solutions to inviscid incompressible porous medium equations by transforming the finite-time blow-up problem into a global-time stability problem for the Proudman-Johnson equation, establishing sharp regularity thresholds and asymptotic stability results.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand the stability properties of special infinite energy solutions to the inviscid incompressible porous medium equations, particularly focusing on explicit self-similar blow-up solutions that were previously discovered, and to determine the conditions under which these solutions remain stable or become unstable.

Method: The key method involves a change of variables that transforms the study of finite-time blow-up solutions into the analysis of global-in-time solutions to the Proudman-Johnson equation. This transformation allows the authors to leverage existing theory for studying stability of steady states in reduced equations that appear in special classes of Euler equations and inviscid primitive equations.

Result: The main results include: (1) proof of stability of the explicit blow-up solution under smooth enough perturbations, (2) identification of a sharp regularity threshold below which the solution becomes unstable, and (3) asymptotic stability with decay estimates for a family of steady states of the Proudman-Johnson equation, which extends to corresponding stability results for Euler equations and inviscid primitive equations.

Conclusion: The paper successfully establishes the stability theory for blow-up solutions in inviscid incompressible porous medium equations by connecting them to the well-studied Proudman-Johnson equation. The approach not only resolves the stability question for the specific blow-up solutions but also provides broader implications for understanding steady state stability in related fluid dynamics equations including Euler and inviscid primitive equations.

Abstract: We consider a special class of infinite energy solutions to the inviscid
incompressible porous medium equations (IPM), introduced in
Castro-C\'ordoba-Gancedo-Orive [9]. The (IPM) equations then reduce to a
one-dimensional nonlocal nonlinear equation, for which an explicit self-similar
blow-up solution is found in [9]. We show the stability of this explicit
blow-up solution by smooth enough perturbations, and identity a sharp
regularity threshold below which it is unstable. The heart of our proof is a
change of variables that transforms the study of finite time blow-up solutions,
to the study of global-in-time solutions to the Proudman-Johnson equation,
which is a reduced equation that appears for special classes of solutions of
the two-dimensional Euler equations, and of the inviscid primitive equations
(or hydrostatic Euler equations). Our main result is in fact the asymptotic
stability with decay estimates for a family of steady states of this reduced
equation. It thus also implies the corresponding stability of steady states for
the associated classes of solutions to the Euler equations and inviscid
primitive equations.

</details>


### [20] [Asymptotic behavior of mass-critical SchrÃ¶dinger equation in $ \mathbb{R}$](https://arxiv.org/abs/2507.17463)
*Fanfei Meng,Yilin Song,Ruixiao Zhang*

Main category: math.AP

TL;DR: This paper studies the long-time behavior of the mass-critical nonlinear SchrÃ¶dinger equation, proving that solutions can be approximated by finite-dimensional Hamiltonian systems and establishing non-squeezing properties. Additionally, it analyzes homogenization of inhomogeneous NLS equations and shows convergence to homogeneous models.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand the long-time behavior of the mass-critical nonlinear SchrÃ¶dinger equation on the real line, particularly building upon previous work on global well-posedness and scattering. The authors are inspired by Killip-Visan-Zhang's pioneering work to show finite-dimensional approximations and extend results to inhomogeneous cases.

Method: The method involves: (1) introducing Fourier truncated mass-critical NLS on â and establishing uniform global space-time bounds, (2) showing that truncated NLS on rescaled torus approximates the truncated equation on â, (3) using Gromov's theorem to prove non-squeezing property for truncated NLS on torus, (4) connecting solutions via nonlinear profile decomposition, and (5) applying Ntekoume's method for homogenization analysis of inhomogeneous NLS.

Result: The main results are: (1) solutions to the mass-critical NLS can be approximated by finite-dimensional Hamiltonian systems (NLS on rescaled torus with Fourier truncated nonlinear terms), (2) establishment of non-squeezing property for both truncated and original NLS, (3) sufficient conditions for scattering in inhomogeneous NLS models, and (4) convergence of inhomogeneous solutions to homogeneous models as nââ.

Conclusion: The paper successfully demonstrates that mass-critical NLS solutions admit finite-dimensional approximations and satisfy non-squeezing properties. For inhomogeneous cases, the authors establish conditions ensuring scattering behavior and prove convergence to homogeneous models, allowing transfer of non-squeezing properties from homogeneous to inhomogeneous systems.

Abstract: In this paper, we study the long-time behavior for the mass-critical
nonlinear Schr\"odinger equation on the line
  \[
  i\partial_t u + \partial_x^2 u = |u|^4 u,
  u(0, x) = u_0 \in L_x^2(\Bbb R).
  \] The global well-posedness and scattering for this equation was solved in
Dodson [Amer. J. Math. (2016)]. Inspired by the pioneering work of
Killip-Visan-Zhang [Amer. J. Math. (2021)], we show that solution can be
approximated by a finite-dimensional Hamiltonian system. This system is the
nonlinear Schr\"odinger equation on the rescaled torus $\Bbb R/(L_n\Bbb Z)$
with Fourier truncated nonlinear term. To prove this, we introduce the Fourier
truncated mass-critical NLS on $\mathbb{R}$. First, we establish the uniformly
global space-time bound for this truncated model on $\mathbb{R}$. Second, we
show that the truncated NLS on rescaled torus can be approximated by the
truncated equation on $\Bbb R$. Then, using the Gromov theorem, we can show the
non-squeezing property for the truncated NLS on torus. The last step to show
the non-squeezing property for original NLS is to connect the solution with
truncated nonlinearity and a single equation in $\mathbb{R}$, which can be done
by performing the nonlinear profile decomposition.
  Our second result is to study the homogenization of the mass-critical
inhomogeneous NLS, where we add a $L^\infty$ function $h(nx)$ in front of the
nonlinear term. Based on the method of Ntekoume [Comm. PDE, (2020)], we give
the sufficient condition on $h$ such that the scattering holds for this
inhomogeneous model and show that the solution to inhomogeneous converges to
the homogeneous model when $n\to\infty$. As a corollary, we can transfer the
non-squeezing property from homogeneous model to inhomogeneous.

</details>


### [21] [On a global estimate and a Stampacchia-type maximum principle for Lane-Emden systems](https://arxiv.org/abs/2507.17465)
*Leandro G. Fernandes Jr.,Edir J. F. Leite*

Main category: math.AP

TL;DR: This paper proves global boundedness for Lane-Emden systems with general second-order elliptic operators in divergence form when exponents multiply to one, and shows that the classical Stampacchia maximum principle cannot be recovered as a special case for certain operators including the Laplacian.


<details>
  <summary>Details</summary>
Motivation: To establish boundedness properties for Lane-Emden systems with general elliptic operators and understand the limitations of extending classical maximum principles from single equations to systems of equations.

Method: Mathematical analysis using second-order elliptic operators in divergence form, focusing on systems where the product of positive exponents equals one, and examining the applicability of maximum principle techniques.

Result: Achieved global boundedness for the specified class of Lane-Emden systems and demonstrated that the Stampacchia maximum principle cannot be recovered as a particular case for certain divergence form operators, including when both operators are Laplacians.

Conclusion: The work successfully extends boundedness theory to Lane-Emden systems with general elliptic operators while revealing fundamental limitations in applying classical maximum principles to multi-equation systems, even in cases involving standard Laplacian operators.

Abstract: We establish a global boundedness result for Lane-Emden systems involving
general second-order elliptic operators in divergence form and arbitrary
positive exponents whose product equals one. Furthermore, we observe that, for
this class of systems -- and for certain operators in divergence form,
including the case when both operators are the Laplacian -- it is not possible
to recover the classical Stampacchia maximum principle as a particular case
corresponding to single equations.

</details>


### [22] [Global well-posedness of the Majda-Biello system in the resonant case on the real line](https://arxiv.org/abs/2507.17565)
*Xin Yang*

Main category: math.AP

TL;DR: This paper establishes global well-posedness for the Majda-Biello system with Î±=4 in the critical Sobolev regularity range sâ[3/4, 1) by developing a refined I-method with dual operators, bridging the gap between local and global theory.


<details>
  <summary>Details</summary>
Motivation: The Majda-Biello system with Î±=4 exhibits the most significant resonance effects, and while local well-posedness was known for sâ¥3/4 and global well-posedness for sâ¥1, there was a gap in the critical regularity range sâ[3/4, 1) that needed to be addressed to match the local theory threshold.

Method: The authors refine the I-method by introducing a pair of distinct I-operators specifically tailored to the resonant structure of the Majda-Biello system with Î±=4. They use a dual-operator framework combined with multilinear correction techniques to construct modified energies that provide pointwise control of multipliers.

Result: Global well-posedness is established for the Majda-Biello system with Î±=4 in the Sobolev regularity range sâ[3/4, 1), successfully bridging the gap between local and global theory. The modified energies constructed are almost conserved and provide effective control over the Sobolev norm globally in time.

Conclusion: The dual I-operator approach successfully extends global well-posedness to the critical regularity range, matching the local theory threshold. This new framework has potential applications to other coupled dispersive systems with strong resonant interactions, representing a significant advancement in the analysis of such systems.

Abstract: We study the Cauchy problem for the following Majda-Biello system in the case
$\alpha=4$, which has the most significant resonance effect, on the real line.
\[ \left\{ \begin{array}{l}
  u_{t} + u_{xxx} = - v v_x,
  v_{t} + \alpha v_{xxx} = - (uv)_{x},
  (u,v)|_{t=0} = (u_0,v_0) \in H^{s}(\mathbb{R}) \times H^{s}(\mathbb{R}),
\end{array} \right. \quad x \in \mathbb{R}, \, t \in \mathbb{R}. \] For Sobolev
regularity $s\in[\frac34, 1)$, we establish the global well-posedness by
refining the I-method. Previously, the critical index for the local
well-posedness was known to be $\frac34$, while the global well-posedness was
only obtained for $s\geq 1$. Our global well-posedness result bridges the gap
and matches the threshold in the local theory. The main novelty of our approach
is to introduce a pair of distinct $I$-operators, tailored to the resonant
structure of the Majda-Biello system with $\alpha=4$. This dual-operator
framework allows for pointwise control of the multipliers in the modified
energies constructed via the multilinear correction technique. These modified
energies are almost conserved and provide effective control over the Sobolev
norm of the solution globally in time. This new approach has potential
applications to other coupled dispersive systems exhibiting strong resonant
interactions.

</details>


### [23] [Segregated solutions for a class of systems with Lotka-Volterra interaction](https://arxiv.org/abs/2507.17644)
*Qing Guo,Angela Pistoia,Shixin Wen*

Main category: math.AP

TL;DR: This paper constructs segregated solutions for a coupled elliptic system with critical nonlinearity where components blow up at different critical points of the Robin function as a parameter approaches zero, despite the system lacking a variational structure.


<details>
  <summary>Details</summary>
Motivation: The system studied lacks a variational formulation due to its specific coupling form, which creates essentially different behaviors in subcritical, critical, and supercritical regimes. This non-variational nature makes it challenging to analyze and requires new approaches beyond standard variational methods.

Method: The authors develop appropriate functional settings to construct solutions despite the lack of variational structure. They work with a coupled elliptic system where the interaction coefficient Î²(Îµ) â 0 as Îµ â 0, and use techniques that can handle the critical Sobolev exponent p = (N+2)/(N-2) in dimensions N â¥ 4.

Result: The paper successfully constructs a family of segregated solutions where each component of the solution blows up at a different critical point of the Robin function as the parameter Îµ approaches zero. These solutions demonstrate the existence of positive solutions to the non-variational system.

Conclusion: The authors prove the existence of segregated positive solutions for a coupled elliptic system with critical nonlinearity that lacks variational structure. The construction works for the critical Sobolev exponent and shows that each solution component concentrates at distinct critical points of the Robin function in the limit.

Abstract: This paper deals with the existence of positive solutions to the system
  $$ -\Delta w_1 - \varepsilon w_1 = \mu_{1} w_1^{p} + \beta w_1 w_2\ \text{in
} \Omega,\
  -\Delta w_2 - \varepsilon w_2 = \mu_{2} w_2^{p} + \beta w_1 w_2 \ \text{in }
\Omega,\
  w_1 = w_2 = 0 \ \text{on } \partial \Omega,
  $$
  where $\Omega \subseteq \mathbb{R}^{N}$, $N \ge 4$, $ p ={N+2\over N-2}$ and
$ \varepsilon $ is positive and sufficiently small. The interaction coefficient
$ \beta = \beta(\varepsilon) \to 0 $ as $ \varepsilon \to 0 $.
  We construct a family of segregated solutions to this system, where each
component blows-up at a different critical point of the Robin function as
$\varepsilon \to 0. The system lacks a variational formulation due to its
specific coupling form,
  which leads to essentially different behaviors in the subcritical, critical,
and supercritical regimes and requires an
  appropriate functional settings to carry out the construction.

</details>


### [24] [Carleman estimate with piecewise weight and applications to inverse problems for first-order transport equations](https://arxiv.org/abs/2507.17675)
*P. Cannarsa,G. Floridia,M. Yamamoto*

Main category: math.AP

TL;DR: This paper proves improved Carleman estimates for first-order transport equations with more general coefficient conditions and applies them to establish Lipschitz stability for inverse problems involving initial value and source term determination.


<details>
  <summary>Details</summary>
Motivation: Existing Carleman estimates for first-order transport equations require restrictive conditions on the principal coefficients H(x). The authors aim to generalize these conditions and develop more flexible estimates that can handle broader classes of transport problems while maintaining their applicability to inverse problems.

Method: The key methodological innovation is constructing a piecewise smooth weight function in x based on a suitable decomposition of the domain Î©. The approach requires that the directed graph created by the corresponding stream field has no closed loops, which generalizes previous restrictive conditions on H(x).

Result: The authors successfully prove Carleman estimates under more generous conditions on the principal coefficients H(x) than previously available. They then apply these estimates to two specific inverse problems: determining an initial value and determining a spatial factor of a source term, establishing Lipschitz stability estimates for both cases.

Conclusion: The paper extends the theoretical framework for Carleman estimates in transport equations by relaxing coefficient restrictions through innovative weight function construction. The practical impact is demonstrated through applications to inverse problems, where Lipschitz stability is established under the new generalized conditions.

Abstract: We consider a first-order transport equation $\ppp_tu(x,t) + (H(x)\cdot\nabla
u(x,t)) + p(x)u(x,t) = F(x,t)$ for $x \in \OOO \subset \R^d$, where $\OOO$ is a
bounded domain and $0<t<T$. We prove a Carleman estimate for more generous
condition on the principal coefficients $H(x)$ than in the existing works. The
key is the construction of a piecewise smooth weight function in $x$ according
to a suitable decomposition of $\OOO$. Our assumptions on $H$ generalize the
conditions in the existing articles, and require that a directed graph created
by the corresponding stream field has no closed loops. Then, we apply our
Carleman estimate to two inverse problems of determinination of an initial
value and one of a spatial factor of a source term, so that we establish
Lipschitz stability estimates for the inverse problems.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [25] [Hybrid Boundary Physics-Informed Neural Networks for Solving Navier-Stokes Equations with Complex Boundary](https://arxiv.org/abs/2507.17535)
*Chuyu Zhou,ianyu Li,Chenxi Lan,Rongyu Du,Guoguo Xin,Pengyu Nan,Hangzhou Yang,Guoqing Wang,Xun Liu,Wei Li*

Main category: physics.comp-ph

TL;DR: The paper introduces HB-PINN, a hybrid physics-informed neural network that combines pretrained networks with boundary-constrained mechanisms to solve Navier-Stokes equations with complex boundary conditions, achieving state-of-the-art performance on benchmark fluid dynamics problems.


<details>
  <summary>Details</summary>
Motivation: Solving Navier-Stokes equations with complex boundary conditions using physics-informed neural networks (PINN) remains challenging, requiring better methods to handle both interior domain solutions and accurate boundary condition enforcement simultaneously.

Method: HB-PINN combines a pretrained network for efficient initialization with a boundary-constrained mechanism, featuring a primary network for inner domain points and a distance metric network that enhances boundary predictions to ensure accurate solutions across both boundary and interior regions.

Result: The method achieves state-of-the-art performance on benchmark scenarios including 2D cylinder wake flow and 2D blocked cavity flow with segmented inlet, demonstrating significantly improved accuracy over existing PINN-based approaches.

Conclusion: HB-PINN successfully addresses the challenge of solving Navier-Stokes equations with complex boundary conditions by effectively combining pretrained networks with boundary-enhanced mechanisms, establishing a new state-of-the-art for PINN-based fluid dynamics simulations.

Abstract: Physics-informed neural networks (PINN) have achieved notable success in
solving partial differential equations (PDE), yet solving the Navier-Stokes
equations (NSE) with complex boundary conditions remains a challenging task. In
this paper, we introduce a novel Hybrid Boundary PINN (HB-PINN) method that
combines a pretrained network for efficient initialization with a
boundary-constrained mechanism. The HB-PINN method features a primary network
focused on inner domain points and a distance metric network that enhances
predictions at the boundaries, ensuring accurate solutions for both boundary
and interior regions. Comprehensive experiments have been conducted on the NSE
under complex boundary conditions, including the 2D cylinder wake flow and the
2D blocked cavity flow with a segmented inlet. The proposed method achieves
state-of-the-art (SOTA) performance on these benchmark scenarios, demonstrating
significantly improved accuracy over existing PINN-based approaches.

</details>


### [26] [Coupling all-electron full-potential density functional theory with grid-based continuum embeddings](https://arxiv.org/abs/2507.17672)
*Jakob Filser,Edan Bainglass,Karsten Reuter,Oliviero Andreussi*

Main category: physics.comp-ph

TL;DR: This paper presents a novel smoothing scheme for continuum embedding models that enables coupling of the Environ library with all-electron DFT programs by transforming sharp electron density peaks into smooth grid representations while maintaining electrostatic accuracy.


<details>
  <summary>Details</summary>
Motivation: Previous continuum embedding models in Environ library could not couple with all-electron simulation packages due to sharp electron density peaks near atomic nuclei that are difficult to represent on regular grids, limiting the simulation of diverse systems and properties in electrochemistry and catalysis applications.

Method: Development of a novel smoothing scheme that transforms atom-centered electron densities into regular grid representations while preserving electrostatic calculation accuracy, creating a minimal and generic interface for seamless interoperability between Environ and all-electron DFT programs.

Result: Successfully demonstrated coupling of Environ with the FHI-aims package and presented benchmark simulations that validate the proposed smoothing method, enabling extension of continuum embedding models to all-electron DFT simulations.

Conclusion: The novel smoothing scheme successfully overcomes the technical barrier of sharp electron density peaks, enabling seamless integration of continuum embedding models with all-electron DFT programs and expanding the capability to simulate diverse electrochemical and catalytic systems.

Abstract: Recent advances in continuum embedding models have enabled the incorporation
of solvent and electrolyte effects into density functional theory (DFT)
simulations of material surfaces, significantly benefiting electrochemistry,
catalysis, and other applications. To extend the simulation of diverse systems
and properties, the implementation of continuum embedding models into the
Environ library adopts a modular programming paradigm, offering a flexible
interface for communication with various DFT programs. The speed and
scalability of the current implementation rely on a smooth definition of the
key physical properties of the atomistic system, in particular of its
electronic density. This has hindered the coupling of Environ with all-electron
simulation packages, as the sharp electron density peaks near atomic nuclei are
difficult to represent on regular grids. In this work, we introduce a novel
smoothing scheme that transforms atom-centered electron densities into a
regular grid representation while preserving the accuracy of electrostatic
calculations. This approach enables a minimal and generic interface,
facilitating seamless interoperability between Environ and all-electron DFT
programs. We demonstrate this development through the coupling of Environ with
the FHI-aims package and present benchmark simulations that validate the
proposed method.

</details>


### [27] [From Atoms to Dynamics: Learning the Committor Without Collective Variables](https://arxiv.org/abs/2507.17700)
*Sergio Contreras Arredondo,Chenyu Tang,Radu A. Talmazan,Alberto MegÃ­as,Cheng Giuseppe Chen,Christophe Chipot*

Main category: physics.comp-ph

TL;DR: A graph neural network architecture using geometric vector perceptrons is developed to predict committor functions directly from atomic coordinates, eliminating the need for hand-crafted collective variables and providing atom-level interpretability for molecular transitions.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for studying molecular transitions require hand-crafted collective variables (CVs) and prior assumptions about reaction mechanisms, which limits their applicability and interpretability. There is a need for automated, assumption-free approaches that can identify physically meaningful reaction coordinates and provide atom-level insights into complex molecular processes.

Method: The paper introduces a graph neural network architecture based on geometric vector perceptrons that takes atomic coordinates as direct input to predict committor functions. This approach bypasses the need for predefined collective variables and provides atom-level interpretability by identifying key atomic players in transitions.

Result: The method successfully predicts committor functions across diverse molecular systems with high accuracy. It provides atom-level interpretability by highlighting the importance of each heavy atom in transition mechanisms and yields precise estimates of rate constants for the underlying processes.

Conclusion: The proposed graph neural network approach enables CV-free learning and automated identification of physically meaningful reaction coordinates, opening new avenues for understanding and modeling complex molecular dynamics without requiring prior assumptions about transition mechanisms.

Abstract: This Brief Communication introduces a graph-neural-network architecture built
on geometric vector perceptrons to predict the committor function directly from
atomic coordinates, bypassing the need for hand-crafted collective variables
(CVs). The method offers atom-level interpretability, pinpointing the key
atomic players in complex transitions without relying on prior assumptions.
Applied across diverse molecular systems, the method accurately infers the
committor function and highlights the importance of each heavy atom in the
transition mechanism. It also yields precise estimates of the rate constants
for the underlying processes. The proposed approach opens new avenues for
understanding and modeling complex dynamics, by enabling CV-free learning and
automated identification of physically meaningful reaction coordinates of
complex molecular processes.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [28] [Single attosecond XUV pulse source via light-wave controlled relativistic laser-plasma interaction: Thomson Back Scattering Scheme](https://arxiv.org/abs/2507.16949)
*Mojtaba Shirozhan,Fabien QuÃ©rÃ©,Subhendu Kahaly*

Main category: physics.plasm-ph

TL;DR: Researchers demonstrate how to generate isolated attosecond pulses in the extreme ultraviolet range by optimizing laser waveforms to create relativistic electron sheets that coherently reflect and upshift light, offering a compact solution for ultrafast phenomena research.


<details>
  <summary>Details</summary>
Motivation: To develop a powerful method for generating bright, ultrashort pulses in the extreme ultraviolet range using Einstein's concept of reflecting light off mirrors moving near light speed, addressing key experimental challenges in creating compact, tunable sources for probing ultrafast phenomena.

Method: The method involves striking nanometre-scale foils with high-intensity, sharp-front laser pulses to create dense relativistic electron mirrors (single relativistic electron sheet - RES), then optimizing the drive laser waveform to reliably produce isolated attosecond pulses through Thomson backscattering, with parameter tuning for controlling pulse characteristics.

Result: Successfully demonstrated reliable production of single RES leading to isolated attosecond pulse generation with enhanced intensity and temporal compression. Simulations showed that tuning parameters like timing delay enables control over amplitude, duration, and bandwidth of the resulting attosecond Thomson backscattering pulses, with efficiency exceeding incoherent scattering by several orders of magnitude.

Conclusion: The optimization of drive laser waveforms enables reliable generation of isolated attosecond pulses through relativistic electron sheets, meeting key experimental challenges and paving the way for compact, tunable sources of isolated attosecond pulses for ultrafast phenomena research.

Abstract: Reflecting light off a mirror moving near light speed, as first envisioned by
Einstein, offers a powerful method for generating bright, ultrashort pulses in
the extreme ultraviolet range. Recent breakthroughs show that dense
relativistic electron mirrors can be created by striking a nanometre-scale foil
with a high-intensity, sharp-front laser pulse, forming a single relativistic
electron sheet (RES). This RES coherently reflects and upshifts a
counter-propagating laser beam from the infrared to the extreme ultraviolet
with efficiency exceeding incoherent scattering by over several orders of
magnitude. Here we demonstrate that optimizing the drive laser waveform can
reliably produce a single RES, leading to generation of isolated
\emph{attosecond} pulses enhancing both intensity and temporal compression of
the back reflected light in a controlled manner. Simulations reveal that tuning
parameters like timing delay enables control over the amplitude, duration, and
bandwidth of the resulting attosecond Thomson backscattering (TBS) pulse.
Together, these advances meet key experimental challenges and pave the way for
compact, tunable sources of isolated attosecond pulses for probing ultrafast
phenomena.

</details>


### [29] [Surrogate Modeling of Landau Damping with Deep Operator Networks](https://arxiv.org/abs/2507.16960)
*Simin Shekarpaz,Chuanfei Dong,Ziyu Huang*

Main category: physics.plasm-ph

TL;DR: Researchers developed a Deep Operator Networks (DeepONets) surrogate model to efficiently simulate plasma dynamics, particularly Landau damping, achieving high accuracy while reducing computational demands for large-scale space and astrophysical plasma modeling.


<details>
  <summary>Details</summary>
Motivation: Kinetic simulations provide high accuracy for microscale plasma physics but are computationally prohibitive for large-scale space and astrophysical systems, necessitating the development of efficient surrogate models that maintain accuracy while reducing computational costs.

Method: Built a surrogate model using Deep Operator Networks (DeepONets) trained on Vlasov-Poisson simulation data to model plasma dynamical evolution, specifically focusing on the Landau damping process in both linear and nonlinear regimes.

Result: The trained DeepONets successfully captured the evolution of electric field energy across various conditions in both linear and nonlinear regimes, demonstrating robust performance in reproducing complex plasma behaviors with high accuracy through extensive validation.

Conclusion: DeepONets provide a promising pathway for large-scale modeling of space and astrophysical plasmas by maintaining high accuracy while significantly reducing computational demands compared to traditional kinetic simulations.

Abstract: Kinetic simulations excel at capturing microscale plasma physics phenomena
with high accuracy, but their computational demands make them impractical for
modeling large-scale space and astrophysical systems. In this context, we build
a surrogate model, using Deep Operator Networks (DeepONets), based upon the
Vlasov-Poisson simulation data to model the dynamical evolution of plasmas,
focusing on the Landau damping process - a fundamental kinetic phenomenon in
space and astrophysical plasmas. The trained DeepONets are able to capture the
evolution of electric field energy in both linear and nonlinear regimes under
various conditions. Extensive validation highlights DeepONets' robust
performance in reproducing complex plasma behaviors with high accuracy, paving
the way for large-scale modeling of space and astrophysical plasmas.

</details>


### [30] [Radiation reaction effects on particle dynamics in intense counterpropagating laser pulses](https://arxiv.org/abs/2507.17046)
*Caleb Redshaw,Matthew R. Edwards*

Main category: physics.plasm-ph

TL;DR: This study uses particle-in-cell simulations to investigate how radiation reaction affects plasma dynamics when struck by counterpropagating circularly polarized laser pulses, finding that radiation reaction can reverse electron trapping and ion expulsion direction under specific parameter conditions.


<details>
  <summary>Details</summary>
Motivation: Understanding the impact of radiation reaction on high-intensity laser-plasma interactions is crucial as particles can lose substantial energy through radiation emission, which affects the fundamental dynamics of the interaction and needs to be characterized for applications in laser-plasma physics.

Method: The researchers used particle-in-cell simulations to study underdense plasma targets struck by counterpropagating circularly polarized laser pulses, systematically varying the relative wavelengths and intensities of the pulses to map out parameter regimes where radiation reaction effects become significant.

Result: The study identified a parameter range where radiation reaction detraps electrons from interference beat waves, reversing charge separation fields and ion expulsion direction. They derived three simple inequalities that predict this reversal based on wavelength, normalized vector potential, pulse duration ratios, and radiation reaction parameters, with good agreement between theoretical estimates and simulation results.

Conclusion: Radiation reaction can significantly alter plasma dynamics by reversing fundamental processes like electron trapping and ion acceleration direction. The derived parameter bounds provide a practical framework for predicting when these effects occur, and the researchers propose an experimental method to observe the transition to radiation-dominated dynamics.

Abstract: In high-intensity laser-plasma interactions, particles can lose a substantial
fraction of their energy by emitting radiation. Using particle-in-cell
simulations, we study the impact of radiation reaction on the dynamics of an
underdense plasma target struck by counterpropagating circularly polarized
laser pulses. By varying the relative wavelengths and intensities of the
pulses, we find a range of parameters where radiation reaction can detrap
electrons from the interference beat wave. The resulting charge separation
field and the dominant direction of ion expulsion are thus reversed by
radiative effects. Based on the electron dynamics during the interaction, we
estimate the bounds on the parameter regime where the reversal occurs. The
bounds take the form of three simple inequalities which depend only on the
wavelength, normalized vector potential, and pulse duration ratios of the two
lasers as well as the product of the pulse duration with a dimensionless
radiation reaction parameter. Our estimates, which predict whether radiation
reaction will change the final ion direction for a given set of laser
parameters, broadly agree with the simulated results. Finally, we outline an
experimental procedure by which the reversal could be used to observe the
transition to radiation-dominated dynamics.

</details>


### [31] [Identification and Characterization of a New Disruption Regime in ADITYA-U Tokamak](https://arxiv.org/abs/2507.17299)
*Soumitra Banerjee,Harshita Raj,Sk Injamul Hoque,Komal Yadav,Sharvil Patel,Ankit Kumar,Kaushlender Singh,Ashok Kumawat,Bharat Hegde,Subhojit Bose,Priyanka Verma,Kumudini Tahiliani,Asha Adhiya,Manoj Kumar,Rohit Kumar,Malay Bikash Chowdhuri,Nilam Ramaiya,Ananya Kundu,Suman Aich,Suman Dolui,K. A. Jadeja,K. M. Patel,Ankit Patel,Rakesh L. Tanna,Joydeep Ghosh*

Main category: physics.plasm-ph

TL;DR: Researchers discovered a new type of tokamak disruption in ADITYA-U that differs from conventional locked mode disruptions, featuring a two-phase evolution with frequency rise followed by collapse, governed solely by the (2/1) drift-tearing mode and triggered by core temperature hollowing.


<details>
  <summary>Details</summary>
Motivation: Disruptions pose significant challenges to tokamak reactor stability and future design, necessitating comprehensive understanding of different disruption mechanisms to improve plasma control and reactor safety.

Method: Statistical investigation conducted on ADITYA-U tokamak to characterize disruption events, analyzing mode frequency evolution, amplitude behavior, and identifying empirical thresholds including edge safety factor, current decay coefficient, current quench time, and rate to distinguish disruption types.

Result: Identified a novel disruption regime with distinctive two-phase evolution: initial steady frequency rise with nonlinearly saturated amplitude, followed by sudden frequency collapse with amplitude increase. This regime is dominated solely by (m/n = 2/1) drift-tearing mode, unlike typical ADITYA-U disruptions involving both 2/1 and 3/1 modes. Clear empirical thresholds were established to distinguish this from conventional locked mode disruptions.

Conclusion: The newly identified disruption regime represents a distinct category governed by (2/1) drift-tearing mode, initiated by core temperature hollowing and enhanced by synergistic effects between localized pressure gradients and current density profile steepening near the mode rational surface, providing new insights for tokamak disruption prediction and mitigation strategies.

Abstract: Disruptions continue to pose a significant challenge to the stable operation
and future design of tokamak reactors. A comprehensive statistical
investigation carried out on the ADITYA-U tokamak has led to the observation
and characterization of a novel disruption regime. In contrast to the
conventional Locked Mode Disruption (LMD), the newly identified disruption
exhibits a distinctive two-phase evolution: an initial phase characterized by a
steady rise in mode frequency with a nonlinearly saturated amplitude, followed
by a sudden frequency collapse accompanied by a pronounced increase in
amplitude. This behaviour signifies the onset of the precursor phase on a
significantly shorter timescale. Clear empirical thresholds have been
identified to distinguish this disruption type from conventional LMD events,
including edge safety factor, current decay coefficient, current quench (CQ)
time, and CQ rate. The newly identified disruption regime is predominantly
governed by the (m/n = 2/1) drift-tearing mode (DTM), which, in contrast to
typical disruptions in the ADITYA-U tokamak that involve both m/n = 2/1 and 3/1
modes, consistently manifests as the sole dominant instability. Initiated by
core temperature hollowing, the growth of this mode is significantly enhanced
by a synergistic interplay between a strongly localized pressure gradient and
the pronounced steepening of the current density profile in the vicinity of the
mode rational surface.

</details>


### [32] [Evaluation of the Transfer Matrix of a Plasma Ramp with Squared Cosine Shape via an Approximate Solution of the Mathieu Differential Equation](https://arxiv.org/abs/2507.17483)
*Stefano Romeo,Angelo Biagioni,Lucio Crincoli,Alessio Del Dotto,Massimo Ferrario,Anna Giribono,Gianmarco Parise,Andrea Renato Rossi,Gilles Jacopo Silvi,Cristina Vaccarezza*

Main category: physics.plasm-ph

TL;DR: This paper develops a transfer matrix approach for analyzing beam transport through cosine squared plasma ramps in plasma wakefield accelerators, showing that such ramps minimize emittance growth and can be applied to real experimental cases.


<details>
  <summary>Details</summary>
Motivation: Plasma wakefield accelerators generate high longitudinal electric fields for acceleration but also strong transverse fields that can degrade beam quality. Plasma ramps with smoothly varying density at injection/extraction points are needed as matching devices to preserve beam transverse quality, requiring proper analytical tools to evaluate beam evolution.

Method: The authors derive an approximate solution to the transverse equation of motion for a cosine squared plasma ramp profile and use this to construct a simple transfer matrix. They then apply this transfer matrix to analyze beam evolution and compare the cosine squared profile with experimentally measured plasma ramp profiles.

Result: The transfer matrix successfully describes beam transport through cosine squared plasma ramps and demonstrates that this ramp profile minimizes emittance growth due to betatron dephasing. The approach shows good agreement when validated against experimentally measured plasma ramp profiles.

Conclusion: Cosine squared plasma ramps provide an effective solution for beam matching in plasma wakefield accelerators by minimizing emittance growth, and the developed transfer matrix approach offers a practical tool for designing and analyzing such systems in real experimental conditions.

Abstract: The high longitudinal electric fields generated in plasma wakefields are very
attractive for a new generation of high gradient plasma based accelerators. On
the other hand, the strong transverse fields increase the demand for a proper
matching device in order to avoid the spoiling of beam transverse quality. A
solution can be provided by the use of a plasma ramp, a region at the plasma
injection/extraction with smoothly increasing/decreasing plasma density. The
transport of a beam inside a plasma ramp, beside its parameters, depends on the
profile of the ramp itself. Establishing the transfer matrix for a plasma ramp
represents a very useful tool in order to evaluate the beam evolution in the
plasma. In this paper a study of a cosine squared ramp is presented. An
approximate solution of the transverse equation of motion is evaluated and
exploited to provide a simple transfer matrix for the plasma ramp. The transfer
matrix is then employed to demonstrate that this kind of ramp has the effect to
minimize the emittance growth due to betatron dephasing. The behavior of a
squared cosine plasma ramp will be compared with an experimentally measured
plasma ramp profile in order to validate the applicability of the transfer
matrix to real cases.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [33] [ZORMS-LfD: Learning from Demonstrations with Zeroth-Order Random Matrix Search](https://arxiv.org/abs/2507.17096)
*Olivia Dry,Timothy L. Molloy,Wanxin Jin,Iman Shames*

Main category: cs.LG

TL;DR: The paper proposes ZORMS-LfD, a zeroth-order optimization method for learning optimal control problems from expert demonstrations that doesn't require gradient computation and works for both continuous and discrete time systems with constraints.


<details>
  <summary>Details</summary>
Motivation: Existing first-order methods for learning from demonstrations require gradient computation of costs, constraints, dynamics, and learning loss, which demands smoothness assumptions. Most methods are also limited to discrete-time problems, with constrained continuous-time problems receiving insufficient attention.

Method: ZORMS-LfD uses zeroth-order random matrix search optimization that can learn costs, constraints, and dynamics of constrained optimal control problems from expert demonstrations without requiring smoothness of the learning-loss landscape or gradient computation.

Result: ZORMS-LfD matches or exceeds state-of-the-art methods in learning loss and compute time across various benchmarks. For unconstrained continuous-time problems, it achieves similar performance with over 80% reduction in compute time. For constrained continuous-time problems, it outperforms gradient-free Nelder-Mead optimization.

Conclusion: ZORMS-LfD provides an effective gradient-free alternative for learning from demonstrations in optimal control, particularly excelling in continuous-time constrained problems where specialized methods are lacking, while offering significant computational efficiency improvements.

Abstract: We propose Zeroth-Order Random Matrix Search for Learning from Demonstrations
(ZORMS-LfD). ZORMS-LfD enables the costs, constraints, and dynamics of
constrained optimal control problems, in both continuous and discrete time, to
be learned from expert demonstrations without requiring smoothness of the
learning-loss landscape. In contrast, existing state-of-the-art first-order
methods require the existence and computation of gradients of the costs,
constraints, dynamics, and learning loss with respect to states, controls
and/or parameters. Most existing methods are also tailored to discrete time,
with constrained problems in continuous time receiving only cursory attention.
We demonstrate that ZORMS-LfD matches or surpasses the performance of
state-of-the-art methods in terms of both learning loss and compute time across
a variety of benchmark problems. On unconstrained continuous-time benchmark
problems, ZORMS-LfD achieves similar loss performance to state-of-the-art
first-order methods with an over $80$\% reduction in compute time. On
constrained continuous-time benchmark problems where there is no specialized
state-of-the-art method, ZORMS-LfD is shown to outperform the commonly used
gradient-free Nelder-Mead optimization method.

</details>


<div id='astro-ph.EP'></div>

# astro-ph.EP [[Back]](#toc)

### [34] [The $n$-body problem -- an alternative scheme for determining solutions for planetary systems](https://arxiv.org/abs/2507.17414)
*Pawel Wojda*

Main category: astro-ph.EP

TL;DR: This paper presents an alternative approach to solving the n-body problem by treating pair-wise momentum contributions as independent variables rather than using classical initial conditions, resulting in more stable gravitational system solutions.


<details>
  <summary>Details</summary>
Motivation: The classical n-body problem with known initial positions and velocities often produces unstable solutions. The authors aim to develop a more stable alternative approach that better accounts for gravitational interactions between all pairs of bodies in the system.

Method: The method treats momentum contributions from each pair-wise gravitational interaction as independent variables, with arbitrary initial values as long as they reproduce the correct total velocities. The total momentum and velocity of each mass is then reconstructed from these individual pair-wise contributions, incorporating all gravitational interactions between body pairs.

Result: The method was successfully tested on multiple systems: Sun-Earth-Moon three-body system, the complete solar system (8 planets + Sun + Moon), and the Pythagorean system. All tests demonstrated improved stability compared to classical n-body problem solutions, with the method able to produce stable solutions for these gravitational systems.

Conclusion: The alternative scheme provides a viable approach to solving n-body problems with enhanced stability by decomposing momentum into pair-wise interaction contributions. This method offers advantages over classical approaches for calculating long-term dynamics of gravitational systems like planetary systems.

Abstract: This study presents a general alternative scheme of the procedure and
necessary conditions for solving the $n$-body problem. The presented solution
is not a solution of the classical problem, where the initial conditions of
positions and initial velocities/momenta of the bodies are known. Starting from
the standard initial condition the procedure treats contributions to momentum
from particular pair-wise interactions as independent variables from which the
total momentum and velocity of a given mass is then reproduced. Initial values
of those contributions are arbitrary as long as the resulting velocities match
the initial condition. The obtained solutions take into account the
gravitational interactions between each pair of bodies, as a result of which
they are characterized by higher stability than the solutions of the classical
problem. The presented procedure was used to calculate the positions and mutual
velocities of three bodies: the Sun, the Earth and the Moon. It has also been
tested for our entire planetary system (8 planets) with the Sun and the Moon.
For these types of systems, the method allows obtaining solutions that are
stable. The procedure was also tested on the example of the Pythagorean system
of bodies.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [35] [The Generalized Matrix Separation Problem: Algorithms](https://arxiv.org/abs/2507.17069)
*Xuemei Chen,Owen Deen*

Main category: math.OC

TL;DR: This paper presents efficient iterative algorithms and implementation strategies for solving a convex optimization problem that recovers low-rank and sparse matrices from their corrupted observations, with a focus on practical matrix structures and a novel preconditioning technique.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop practical and efficient algorithms for the generalized matrix separation problem, which aims to recover a low-rank matrix Lâ and a sparse matrix Sâ from their corrupted observation Mâ=Lâ+HSâ, building upon the convex optimization framework proposed in prior work.

Method: The authors develop iterative algorithms for solving the convex optimization problem with ââ-norm and nuclear norm regularization. They focus on efficient implementation strategies for practical cases where H has special structures (circulant, separable, or block structured) and propose a novel preconditioning technique to improve algorithm performance.

Result: The proposed algorithms with preconditioning demonstrate significant improvements in efficiency, accuracy, and robustness compared to standard approaches. Numerical experiments validate the effectiveness of the implementation strategies, particularly for matrices with special structures, and show the practical benefits of the preconditioning technique.

Conclusion: The paper successfully provides a comprehensive algorithmic framework for the generalized matrix separation problem, with practical implementation strategies and theoretical guarantees for the preconditioning approach. The work serves as both an implementation manual and a contribution to improving the computational efficiency of matrix separation algorithms.

Abstract: When given a generalized matrix separation problem, which aims to recover a
low rank matrix $L_0$ and a sparse matrix $S_0$ from $M_0=L_0+HS_0$, the work
\cite{CW25} proposes a novel convex optimization problem whose objective
function is the sum of the $\ell_1$-norm and nuclear norm. In this paper we
detail the iterative algorithms and its associated computations for solving
this convex optimization problem. We present various efficient implementation
strategies, with attention to practical cases where $H$ is circulant,
separable, or block structured. Notably, we propose a preconditioning technique
that drastically improved the performance of our algorithms in terms of
efficiency, accuracy, and robustness. While this paper serves as an
illustrative algorithm implementation manual, we also provide theoretical
guarantee for our preconditioning strategy. Numerical results illustrate the
effectiveness of the proposed approach.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [36] [Application of new conformal cooling layouts to the green injection molding of complex slender polymeric parts with high dimensional specifications](https://arxiv.org/abs/2507.17723)
*Abelardo Torres Alba,Jorge Manuel Mercado Colmenero,Juan de Dios Caballero Garcia,Cristina Martin Donate*

Main category: cs.CE

TL;DR: This paper presents an innovative green conformal cooling system for injection molding that significantly reduces warpage in long, slender plastic parts by using additively manufactured cooling channels and highly conductive steel inserts, achieving 90.5% warpage reduction and meeting industrial requirements.


<details>
  <summary>Details</summary>
Motivation: Eliminating warpage in injection molded polymeric parts is critical, especially for long and slender geometries with high thickness variations that are impossible to manufacture with traditional technologies while meeting dimensional and sustainability requirements of the industry.

Method: The authors developed an innovative green conformal cooling system specifically designed for slender geometric shapes, combining highly conductive steel alloy inserts with additively manufactured conformal cooling channels to address warpage, cooling time, and residual stresses in complex manufacturing processes.

Result: The conformal cooling system achieved remarkable improvements: 66% reduction in cycle time (175.1s decrease), 78.5% reduction in temperature gradient (18.16Â°C), 81.88% reduction in residual stress (39.78 MPa), and 90.5% reduction in warpage (6.9mm), resulting in final warping of 0.72mm which meets the industrial requirement of <1mm.

Conclusion: The research demonstrates a turning point that makes manufacturing and sustainability possible for complex plastic geometries in injection molding, with particular relevance for the auto parts manufacturing industry where such geometric features will be in high demand in coming years.

Abstract: Eliminating warpage in injection molded polymeric parts is one of the most
important problems in the injection molding industry today. This situation is
critical in geometries that are particularly susceptible to warping due to
their geometric features, and this occurs with topologies of great length and
slenderness with high changes in thickness. These features are, in these
special geometries, impossible to manufacture with traditional technologies to
meet the dimensional and sustainable requirements of the industry. This paper
presents an innovative green conformal cooling system that is specifically
designed for parts with slender geometric shapes that are highly susceptible to
warping. Additionally, the work presented by the authors investigates the
importance of using highly conductive inserts made of steel alloys in
combination with the use of additively manufactured conformal channels for
reducing influential parameters, such as warpage, cooling time, and residual
stresses in the complex manufacturing of long and slender parts. The results of
this real industrial case study indicated that the use of conformal cooling
layouts decreased the cycle time by 175.1 s 66% below the current cooling time;
the temperature gradient by 78.5% specifically, 18.16 C; the residual stress by
39.78 MPa or 81.88%; and the warpage by 6.9 mm or 90.5%. In this way, it was
possible to achieve a final warping in the complex geometry studied of 0.72 mm,
which was under the maximum value required at the industrial level of 1 mm. The
resulting values obtained by the researchers present a turning point from which
the manufacturing and sustainability in the injection molding of said plastic
geometries is possible, and they take into account that the geometric
manufacturing features analyzed will present a great demand in the coming years
in the auto parts manufacturing industry.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [37] [Thermophysical and Mechanical Properties Prediction of Rear-earth High-entropy Pyrochlore Based on Deep-learning Potential](https://arxiv.org/abs/2507.17032)
*Yuxuan Wang,Guoqiang Lan,Huicong Chen,Jun Song*

Main category: cond-mat.mtrl-sci

TL;DR: Researchers developed a Deep Potential Molecular Dynamics (DPMD) model to predict properties of high-entropy pyrochlore oxides for thermal barrier coatings, overcoming limitations of traditional ab initio and classical MD methods by achieving both accuracy and scalability.


<details>
  <summary>Details</summary>
Motivation: High-entropy pyrochlore oxides show promise as next-generation thermal barrier coating materials due to their ultra-low thermal conductivity and excellent high-temperature stability. However, existing computational methods face significant limitations: ab initio methods struggle with phonon-disorder scattering and are limited by small simulation cell sizes, while classical MD lacks accurate interatomic potentials for multi-component high-entropy systems.

Method: The authors employed Deep Potential Molecular Dynamics (DPMD) approach, where a deep-potential (DP) model is trained on a limited dataset from ab initio molecular dynamics (AIMD) calculations. This enables large-scale molecular dynamics simulations with on-the-fly potential evaluations, combining the accuracy of ab initio methods with the scalability of classical MD.

Result: The deep potential model successfully achieved high accuracy in reproducing ab initio results while demonstrating strong generalizability. The model proved applicable not only to high-entropy systems but also to medium-entropy ceramics containing the same constituent elements, enabling prediction of thermophysical and mechanical properties of rare-earth high-entropy pyrochlore oxide systems.

Conclusion: The study successfully develops a deep potential model for rare-earth pyrochlore systems and demonstrates that deep-learning-based potential methods offer a powerful computational approach for designing high-entropy thermal barrier coating materials, bridging the gap between accuracy and computational efficiency in complex multi-component ceramic systems.

Abstract: High-entropy pyrochlore oxides possess ultra-low thermal conductivity and
excellent high-temperature phase stability, making them promising candidate for
next-generation thermal barrier coating (TBC) materials. However, reliable
predictive models for such complex and disordered systems remain challenging.
Ab initio methods, although accurate in describing anharmonic phonon-phonon
interactions, struggle to capture the strong inherent phonon-disorder
scattering in high-entropy systems. Moreover, the limited simulation cell size,
hundreds of atoms, cannot fully represent the configurational complexity of
high-entropy phases. On the other hand, classical molecular dynamics (MD)
simulations lack accurate and transferable interatomic potentials, particularly
in multi-component systems like high-entropy ceramics. In this work, we
employed Deep Potential Molecular Dynamics (DPMD) to predict the thermophysical
and mechanical properties of rare-earth high-entropy pyrochlore oxide system.
The deep-potential (DP) model is trained on a limited dataset from ab initio
molecular dynamics (AIMD) calculations, enabling large-scale molecular dynamics
simulations with on-the-fly potential evaluations. This model not only achieves
high accuracy in reproducing ab initio results but also demonstrates strong
generalizability, making it applicable to medium-entropy ceramics containing
the same constituent elements. Our study successfully develops a deep potential
model for rare-earth pyrochlore systems and demonstrates that the
deep-learning-based potential method offers a powerful computational approach
for designing high-entropy TBC materials.

</details>


### [38] [Mechanically and electrically switchable triferroic altermagnet in a pentagonal FeO2 monolayer](https://arxiv.org/abs/2507.17247)
*Deping Guo,Jiaqi Dai,Renhong Wang,Cong Wang,Wei Ji*

Main category: cond-mat.mtrl-sci

TL;DR: Researchers discovered pentagonal monolayer FeOâ as an intrinsic triferroic altermagnet with coexisting ferroelectric, ferroelastic, and altermagnetic orders that can be controlled by electric fields and mechanical strain, enabling configurable spintronic devices.


<details>
  <summary>Details</summary>
Motivation: Two-dimensional multiferroics are promising for low-power, multifunctional devices, but achieving intrinsic coexistence and mutual control of three coupled ferroic orders in a single layer has remained challenging.

Method: First-principles calculations were used to study pentagonal monolayer FeOâ, analyzing its crystal symmetry (glide mirror Mx symmetry with broken four-fold rotation C4z symmetry) and the coupling between ferroelectric, ferroelastic, and altermagnetic orders.

Result: The study identified six distinct polarization states that can be controlled by electric fields and mechanical strain. Electric-field-induced rotation of ferroelectric polarization reverses spin splitting sign, while uniaxial strain triggers ferroelastic switching that rotates polarization by 90Â° and reverses the altermagnetic state. NÃ©el temperatures exceed 200K.

Conclusion: This work extends intrinsic triferroicity to pentagonal monolayers and provides a symmetry-based route toward mechanically and electrically configurable altermagnetic spintronics, offering new possibilities for multifunctional device applications.

Abstract: Two-dimensional multiferroics promise low-power, multifunctional devices, yet
the intrinsic coexistence and mutual control of three coupled ferroic orders in
a single layer remains elusive. Here, we identify pentagonal monolayer FeO$_2$
as an intrinsic triferroic altermagnet where ferroelectric (FE), ferroelastic
(FA), and altermagnetic (AM) orders coexist and are tightly coupled,
accompanied by a competing antiferroelectric (AFE) phase using first-principles
calculations. The sole presence of glide mirror $M_x$ symmetry in a FeO$_2$
sublayer, with the breaking of four-fold rotation $C_{4z}$ symmetry, induces
in-plane vector ferroelectricity and twin-related ferroelastic strains. Both FE
and AFE phases break combined parity - time symmetry and display sizable
altermagnetic spin splitting with N\'eel temperatures over 200~K.
Electric-field-induced rotation of the FE polarization reverses the sign of the
spin splitting, while in-plane uniaxial strain triggers ferroelastic switching
that simultaneously rotates the FE polarization vector by $90^\circ$ and
reverses the AM state. These electric-field- and strain-mediated pathways
interlink six distinct polarization states that can be selected purely by
electric fields and/or mechanical strain. This work extends intrinsic
triferroicity to pentagonal monolayers and outlines a symmetry-based route
toward mechanically and electrically configurable altermagnetic spintronics.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [39] [Shot-Efficient ADAPT-VQE via Reused Pauli Measurements and Variance-Based Shot Allocation](https://arxiv.org/abs/2507.16879)
*Azhar Ikhtiarudin,Gagus Ketut Sunnardianto,Fadjar Fathurrahman,Mohammad Kemal Agusta,Hermawan Kresno Dipojono*

Main category: quant-ph

TL;DR: This paper proposes two strategies to reduce quantum measurement overhead in ADAPT-VQE: reusing measurement outcomes between optimization steps and applying variance-based shot allocation, achieving significant shot reduction while maintaining chemical accuracy.


<details>
  <summary>Details</summary>
Motivation: ADAPT-VQE shows promise for NISQ quantum algorithms but suffers from high quantum measurement (shot) overhead required for both circuit parameter optimization and operator selection, creating a major computational bottleneck.

Method: Two integrated strategies: (1) reusing Pauli measurement outcomes from VQE parameter optimization for operator gradient measurements in the next ADAPT-VQE iteration, and (2) applying variance-based shot allocation to both Hamiltonian and operator gradient measurements.

Result: Numerical results show that both methods, individually and combined, significantly reduce the number of shots needed to achieve chemical accuracy while maintaining result fidelity across studied molecular systems.

Conclusion: The proposed shot reduction strategies effectively address the measurement overhead challenge in ADAPT-VQE, making the algorithm more practical for NISQ devices by reducing quantum resource requirements without compromising accuracy.

Abstract: The Adaptive Variational Quantum Eigensolver (ADAPT-VQE) is a promising
approach for quantum algorithms in the Noisy Intermediate-Scale Quantum (NISQ)
era, offering advantages over traditional VQE methods by reducing circuit depth
and mitigating challenges in classical optimization. However, a major challenge
in ADAPT-VQE is the high quantum measurement (shot) overhead required for
circuit parameter optimization and operator selection. In this work, we propose
two integrated strategies to reduce the shot requirements in ADAPT-VQE. First,
we reuse Pauli measurement outcomes obtained during VQE parameter optimization
in the subsequent operator selection step of the next ADAPT-VQE iteration,
which involves operator gradient measurements. Second, we apply variance-based
shot allocation to both Hamiltonian and operator gradient measurements. Our
numerical results demonstrate that each method, individually and in
combination, significantly reduces the number of shots needed to achieve
chemical accuracy while maintaining result fidelity across the studied
molecular systems.

</details>


### [40] [Quantum walks reveal topological flat bands, robust edge states and topological phase transitions in cyclic graphs](https://arxiv.org/abs/2507.17250)
*Dinesh Kumar Panda,Colin Benjamin*

Main category: quant-ph

TL;DR: This paper introduces cyclic quantum walks (CQWs) as a novel scheme to simulate topological phenomena in quantum systems, enabling the generation of various topological phases, flat bands, and protected edge states without complex protocols.


<details>
  <summary>Details</summary>
Motivation: To develop a resource-efficient platform for simulating exotic topological phenomena in synthetic quantum systems, which are crucial for topological quantum computing and noise-resilient information processing, without requiring complex split-step or split-coin protocols.

Method: The authors propose step-dependent quantum walks on cyclic graphs (CQWs) using discrete Fourier transforms and an effective Hamiltonian approach. They analyze odd and even-site cyclic graphs to study their spectral characteristics and topological properties.

Result: The scheme successfully generates both gapped and gapless topological phases, including Dirac cone-like energy dispersions and topologically nontrivial flat bands. Rotationally symmetric flat bands emerge exclusively in 4n-site graphs. Protected edge states are engineered at interfaces between distinct topological phases, and these states remain robust against moderate static/dynamic gate disorder and phase-preserving perturbations.

Conclusion: Cyclic quantum walks provide a versatile and resource-efficient platform for engineering topological phases, transitions, edge states, and flat bands in quantum systems, opening new possibilities for fault-tolerant quantum technologies and topological quantum computing applications.

Abstract: Topological phases, edge states, and flat bands in synthetic quantum systems
are a key resource for topological quantum computing and noise-resilient
information processing. We introduce a scheme based on step-dependent quantum
walks on cyclic graphs, termed cyclic quantum walks (CQWs), to simulate exotic
topological phenomena using discrete Fourier transforms and an effective
Hamiltonian. Our approach enables the generation of both gapped and gapless
topological phases, including Dirac cone-like energy dispersions, topologically
nontrivial flat bands, and protected edge states, all without resorting to
split-step or split-coin protocols. Odd and even-site cyclic graphs exhibit
markedly different spectral characteristics, with rotationally symmetric flat
bands emerging exclusively $4n$-site graphs ($n\in \mathbf{N}$). We
analytically establish the conditions for the emergence of topological, gapped
flat bands and show that gap closings in rotation space imply the formation of
Dirac cones in momentum space. Further, we engineer protected edge states at
the interface between distinct topological phases in both odd and even cycle
graphs. We numerically demonstrate that the edge states are robust against
moderate static and dynamic gate disorder and remain stable against
phase-preserving perturbations. This scheme serves as a resource-efficient and
versatile platform for engineering topological phases, transitions, edge
states, and flat bands in quantum systems, opening new avenues for
fault-tolerant quantum technologies.

</details>


### [41] [Qubit-Efficient Quantum Algorithm for Linear Differential Equations](https://arxiv.org/abs/2507.16995)
*Di Fang,David Lloyd George,Yu Tong*

Main category: quant-ph

TL;DR: A quantum algorithm for solving linear ODEs that uses only one ancilla qubit and preserves locality, making it suitable for near-term quantum devices with provable runtime guarantees.


<details>
  <summary>Details</summary>
Motivation: As quantum hardware advances toward fault-tolerant computing, there's a critical need to develop quantum algorithms that are both theoretically rigorous and practical for near-term quantum devices with limited resources and connectivity constraints.

Method: The authors propose a quantum algorithm that solves linear ordinary differential equations using only a single ancilla qubit while maintaining locality preservation - when the ODE coefficient matrix is k-local, the algorithm only requires implementing (k+1)-local Hamiltonian time evolution.

Result: The algorithm achieves provable runtime guarantees while being hardware-friendly for near-term devices. The authors establish connections to Lindbladian simulation and demonstrate applicability to the interacting Hatano-Nelson model, a well-studied non-Hermitian system.

Conclusion: The proposed quantum algorithm successfully bridges theoretical quantum computing with practical hardware constraints, offering an efficient solution for linear ODE problems that can be implemented on current and near-future quantum devices while maintaining locality and using minimal ancilla resources.

Abstract: As quantum hardware rapidly advances toward the early fault-tolerant era, a
key challenge is to develop quantum algorithms that are not only theoretically
sound but also hardware-friendly on near-term devices. In this work, we propose
a quantum algorithm for solving linear ordinary differential equations (ODEs)
with a provable runtime guarantee. Our algorithm uses only a single ancilla
qubit, and is locality preserving, i.e., when the coefficient matrix of the ODE
is $k$-local, the algorithm only needs to implement the time evolution of
$(k+1)$-local Hamiltonians. We also discuss the connection between our proposed
algorithm and Lindbladian simulation as well as its application to the
interacting Hatano-Nelson model, a widely studied non-Hermitian model with rich
phenomenology.

</details>


### [42] [Instability of explicit time integration for strongly quenched dynamics with neural quantum states](https://arxiv.org/abs/2507.17421)
*Hrvoje Vrcan,Johan H. Mentink*

Main category: quant-ph

TL;DR: This paper investigates numerical instabilities in neural quantum state simulations of strongly driven quantum dynamics using time-dependent variational principle (TDVP) with explicit time integration, revealing a critical quenching strength that causes numerical breakdown even when physical observables remain regular.


<details>
  <summary>Details</summary>
Motivation: Neural quantum states show promise for simulating quantum dynamics beyond traditional variational methods, but studying strongly driven quantum dynamics with neural networks remains challenging due to numerical instabilities that need to be understood and addressed.

Method: The authors use restricted Boltzmann machine architecture to implement TDVP with explicit time integration schemes, comparing solutions with analytical results and implicit methods across different quench strengths. They test multiple TDVP formulations including those that eliminate small Fisher matrix eigenvalues and use geometric properties.

Result: A specific quenching strength was identified that consistently causes numerical breakdown across different TDVP formulations, even when physical observables show no irregularities. This breakdown occurs even in the absence of Monte Carlo noise and persists across various mathematical reformulations of the equations of motion.

Conclusion: The study concludes that current explicit time integration schemes for TDVP equations are insufficient for simulating strongly nonequilibrium quantum dynamics with neural-network quantum states, and alternative computational methods must be developed to maintain efficiency while ensuring numerical stability.

Abstract: Neural quantum states have recently demonstrated significant potential for
simulating quantum dynamics beyond the capabilities of existing variational
ans\"atze. However, studying strongly driven quantum dynamics with neural
networks has proven challenging so far. Here, we focus on assessing several
sources of numerical instabilities that can appear in the simulation of quantum
dynamics based on the time-dependent variational principle (TDVP) with the
computationally efficient explicit time integration scheme. Using the
restricted Boltzmann machine architecture, we compare solutions obtained by
TDVP with analytical solutions and implicit methods as a function of the quench
strength. Interestingly, we uncover a quenching strength that leads to a
numerical breakdown in the absence of Monte Carlo noise, despite the fact that
physical observables don't exhibit irregularities. This breakdown phenomenon
appears consistently across several different TDVP formulations, even those
that eliminate small eigenvalues of the Fisher matrix or use geometric
properties to recast the equation of motion. We conclude that alternative
methods need to be developed to leverage the computational efficiency of
explicit time integration of the TDVP equations for simulating strongly
nonequilibrium quantum dynamics with neural-network quantum states.

</details>


<div id='cond-mat.mes-hall'></div>

# cond-mat.mes-hall [[Back]](#toc)

### [43] [Beyond symmetry protection: Robust feedback-enforced edge states in non-Hermitian stacked quantum spin Hall systems](https://arxiv.org/abs/2507.17295)
*Mengjie Yang,Ching Hua Lee*

Main category: cond-mat.mes-hall

TL;DR: This paper demonstrates that coupling two quantum spin Hall (QSH) layers with intermediate strength and non-Hermitian directed amplification can create robust helical edge transport modes without symmetry protection, challenging the conventional view that strongly coupled QSH layers become trivial.


<details>
  <summary>Details</summary>
Motivation: Conventional wisdom suggests that strongly coupling two QSH layers results in a trivial topological phase with no protected edge states. The authors aim to challenge this traditional view and explore alternative mechanisms for creating robust helical transport modes in stacked QSH systems.

Method: The authors employ intermediate inter-layer coupling (between strong and weak coupling regimes) combined with competitive non-Hermitian directed amplification. This feedback-enforced mechanism suppresses bulk modes while accumulating bulk excitations into helical edge transport modes without relying on symmetry protection.

Result: The mechanism successfully creates robust helical edge transport modes that persist over broad parameter ranges and remain stable even on fractal or irregular boundaries. Bulk excitations inevitably accumulate into these edge modes while bulk transport is suppressed.

Conclusion: The findings challenge the traditional understanding of stacked QSH insulators as inevitably trivial and open new possibilities for designing helical topological devices using feedback-enforced non-Hermitian engineering rather than conventional symmetry-enforced robustness.

Abstract: Conventional wisdom holds that strongly coupling two QSH layers yields a
trivial $\mathbb{Z}_2$ phase and no protected topological edge states. We
demonstrate that, in a regime with intermediate inter-layer coupling (neither
in the strong or weak coupling regimes) and competitive non-Hermitian directed
amplification, bulk modes are suppressed while arbitrary bulk excitations
inevitably accumulate into robust helical edge transport modes - without
relying on any symmetry protection. Our feedback-enforced mechanism persists
over broad parameter ranges and remains robust even on fractal or irregular
boundaries. These findings challenge the traditional view of stacked QSH
insulators as inevitably trivial, and open up new avenues for designing helical
topological devices that exploit feedback-enforced non-Hermitian engineering,
instead of symmetry-enforced robustness.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [44] [Avoiding spectral pollution for transfer operators using residuals](https://arxiv.org/abs/2507.16915)
*April Herwig,Matthew J. Colbrook,Oliver Junge,PÃ©ter Koltai,Julia Slipantschuk*

Main category: math.DS

TL;DR: This paper presents new algorithms for computing spectral properties of transfer operators without spectral pollution, extending beyond Koopman operators to provide robust spectral estimation tools for nonlinear dynamical systems.


<details>
  <summary>Details</summary>
Motivation: Finite-dimensional approximations of Koopman and transfer operators suffer from spectral pollution, which introduces spurious eigenvalues that compromise spectral computations. While provably convergent methods exist for Koopman operators, analogous tools for general transfer operators are limited.

Method: The authors develop algorithms for computing spectral properties of transfer operators without spectral pollution, including extensions to the Hardy-Hilbert space. The approach addresses the functional-analytic subtleties in defining the "true" Koopman spectrum.

Result: Case studies demonstrate the accuracy and flexibility of the approach, ranging from families of Blaschke maps with known spectrum to molecular dynamics models of protein folding. The methods successfully identify spectral features even when corresponding eigenfunctions lie outside the chosen space.

Conclusion: The proposed methods offer robust tools for spectral estimation across a broad range of applications, providing pollution-free spectral analysis for transfer operators and advancing the computational capabilities for nonlinear dynamical systems analysis.

Abstract: Koopman operator theory enables linear analysis of nonlinear dynamical
systems by lifting their evolution to infinite-dimensional function spaces.
However, finite-dimensional approximations of Koopman and transfer
(Frobenius--Perron) operators are prone to spectral pollution, introducing
spurious eigenvalues that can compromise spectral computations. While recent
advances have yielded provably convergent methods for Koopman operators,
analogous tools for general transfer operators remain limited. In this paper,
we present algorithms for computing spectral properties of transfer operators
without spectral pollution, including extensions to the Hardy-Hilbert space.
Case studies--ranging from families of Blaschke maps with known spectrum to a
molecular dynamics model of protein folding--demonstrate the accuracy and
flexibility of our approach. Notably, we demonstrate that spectral features can
arise even when the corresponding eigenfunctions lie outside the chosen space,
highlighting the functional-analytic subtleties in defining the "true" Koopman
spectrum. Our methods offer robust tools for spectral estimation across a broad
range of applications.

</details>


### [45] [Exactness, Cohomology, and Uniqueness in First-Order Differential Equations](https://arxiv.org/abs/2507.16457)
*Hemanta Mandal*

Main category: math.DS

TL;DR: This paper explores how the topology of domains affects the solvability of first-order differential equations using de Rham cohomology, showing that vanishing first cohomology groups (not just simple connectivity) is the key requirement for global solutions.


<details>
  <summary>Details</summary>
Motivation: While local existence theorems for first-order ODEs are well-established, understanding their global behavior requires examining the relationship between differential equation solvability and the topological properties of the underlying domain, particularly moving beyond the traditional requirement of simple connectivity.

Method: The authors use de Rham cohomology theory to analyze closed 1-forms associated with first-order ODEs, focusing on conditions for global potential existence. They construct explicit examples on non-simply connected manifolds like the real projective plane âPÂ² and compare with domains like âÂ²\{0} to demonstrate topological effects.

Result: The study demonstrates that triviality of the first de Rham cohomology group HÂ¹_dR is the fundamental requirement for global integrability, not simple connectivity. Non-simply connected manifolds like âPÂ² can still support global solutions due to vanishing HÂ¹_dR, while topological obstructions manifest as non-uniqueness or multi-valued behavior in other cases.

Conclusion: The paper provides a unifying geometric-topological-analytic framework for understanding global behavior of first-order ODEs, establishing the correspondence between integrating factors and cohomology class trivialization, and connecting classical symmetry methods with modern topological tools beyond local existence theorems.

Abstract: This paper investigates the relationship between the solvability of
first-order differential equations and the topology of the underlying domain
through the lens of de\,Rham cohomology. We analyze the conditions under which
a closed 1-form associated with a first-order ODE admits a global potential,
thereby reducing the problem to the exactness of differential forms. While it
is well known that exactness is guaranteed on simply connected domains, we show
that triviality of the first de\,Rham cohomology group is the more fundamental
requirement for global integrability and uniqueness of solutions. In
particular, we demonstrate that certain non-simply connected manifolds, such as
the real projective plane $\mathbb{RP}^2$, still support global solutions due
to the vanishing of $H^1_{\text{dR}}$. By explicitly constructing examples on
$\mathbb{RP}^2$ and comparing them with domains like $\mathbb{R}^2 \setminus
\{0\}$, we illustrate how topological obstructions manifest analytically as
non-uniqueness or multi-valued behavior. We further discuss the correspondence
between integrating factors and the trivialization of cohomology classes,
drawing connections to classical symmetry methods and potential theory. This
synthesis of geometric, topological, and analytic perspectives provides a
unifying framework for understanding the global behavior of first-order ODEs
beyond local existence theorems.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [46] [Fast One-Pass Sparse Approximation of the Top Eigenvectors of Huge Low-Rank Matrices? Yes, $MAM^*$!](https://arxiv.org/abs/2507.17036)
*Edem Boahen,Simone Brugiapaglia,Hung-Hsu Chou,Mark Iwen,Felix Krahmer*

Main category: cs.IT

TL;DR: This paper presents one-pass algorithms for sparse approximation of top eigenvectors of massive matrices using compressive sensing, requiring only memory proportional to the sparse output size rather than the full matrix size.


<details>
  <summary>Details</summary>
Motivation: The need to compute sparse principal component analysis (sparse PCA) and eigenvector approximations for extremely massive matrices that are too large to store in memory, requiring memory-efficient single-pass algorithms.

Method: Compressive-sensing-based one-pass algorithms that use a single compact linear sketch to approximate leading eigenvectors of huge approximately low-rank matrices, with recovery algorithms that run in time sublinear to the original matrix size.

Result: The algorithms can approximate top eigenvectors using memory footprint proportional to the desired sparse approximations rather than the full matrix, with runtime principally dependent on the sparse approximation size. Experiments on matrices with ~10^16 entries demonstrate practical feasibility.

Conclusion: The proposed compressive sensing approach enables provably-accurate sparse eigenvector approximation for massive matrices in a single pass with dramatically reduced memory requirements and sublinear runtime complexity.

Abstract: Motivated by applications such as sparse PCA, in this paper we present
provably-accurate one-pass algorithms for the sparse approximation of the top
eigenvectors of extremely massive matrices based on a single compact linear
sketch. The resulting compressive-sensing-based approaches can approximate the
leading eigenvectors of huge approximately low-rank matrices that are too large
to store in memory based on a single pass over its entries while utilizing a
total memory footprint on the order of the much smaller desired sparse
eigenvector approximations. Finally, the compressive sensing recovery algorithm
itself (which takes the gathered compressive matrix measurements as input, and
then outputs sparse approximations of its top eigenvectors) can also be
formulated to run in a time which principally depends on the size of the sought
sparse approximations, making its runtime sublinear in the size of the large
matrix whose eigenvectors one aims to approximate. Preliminary experiments on
huge matrices having $\sim 10^{16}$ entries illustrate the developed theory and
demonstrate the practical potential of the proposed approach.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [47] [Quantitative convergence for displacement monotone Mean Field Games of control](https://arxiv.org/abs/2507.17014)
*Joe Jackson,AlpÃ¡r R. MÃ©szÃ¡ros*

Main category: math.PR

TL;DR: This paper proves quantitative convergence results for N-player stochastic differential games to Mean Field Games of Controls (MFGC), where agents interact through both states and controls, by solving a nontrivial fixed-point problem on measure spaces and establishing regularity for both open-loop and closed-loop Nash equilibria.


<details>
  <summary>Details</summary>
Motivation: Existing works on Mean Field Games of Controls either restrict to separable Hamiltonians (making the fixed-point problem trivial) or assume convergence properties without proof. There was a need to rigorously analyze the nontrivial fixed-point problem that arises naturally in MFGC formulations with non-separable Hamiltonians, and to establish quantitative convergence rates for both open-loop and closed-loop equilibria.

Method: The authors develop a detailed structural analysis of the fixed-point equation on measure spaces and its N-player analogue. They work with general non-separable Hamiltonians satisfying displacement monotonicity conditions along with mild regularity and growth conditions. The approach involves establishing new regularity results for fixed-point maps, deriving sharp a priori estimates for the N-player Nash system, and controlling the discrepancy between open and closed-loop strategies.

Result: The paper establishes quantitative convergence results for both open-loop and closed-loop Nash equilibria in N-player stochastic differential games to their mean field limits. New regularity results are obtained for the fixed-point maps, and sharp a priori estimates are derived for the N-player Nash system. The framework successfully accommodates common noise and handles the challenging case of non-separable Hamiltonians.

Conclusion: The authors successfully provide a rigorous mathematical foundation for Mean Field Games of Controls with non-separable Hamiltonians by solving the nontrivial fixed-point problem and establishing quantitative convergence rates. This advances the theoretical understanding of mean field games where agents interact through both states and controls, extending beyond previous limitations of separable Hamiltonians.

Abstract: In this paper we establish quantitative convergence results for both open and
closed-loop Nash equilibria of N-player stochastic differential games in the
setting of Mean Field Games of Controls (MFGC), a class of models where
interactions among agents occur through both states and controls. Our analysis
covers a general class of non-separable Hamiltonians satisfying a displacement
monotonicity condition, along with mild regularity and growth conditions at
infinity. A major novelty of our work is the rigorous treatment of a nontrivial
fixed-point problem on a space of measures, which arises naturally in the MFGC
formulation. Unlike prior works that either restrict to separable Hamiltonians
- rendering the fixed-point map trivial - or assume convergence or regularity
properties of the fixed point map, we develop a detailed structural analysis of
this equation and its N-player analogue. This leads to new regularity results
for the fixed-point maps and, in turn, to quantitative convergence of open-loop
equilibria. We further derive sharp a priori estimates for the N-player Nash
system, enabling us to control the discrepancy between open and closed-loop
strategies, and thus to conclude the convergence of closed-loop equilibria. Our
framework also accommodates common noise in a natural way.

</details>


### [48] [The Dirichlet problem for stochastic partial differential equations with nonlocal operators in $C^{1,Ï}$ open sets](https://arxiv.org/abs/2507.17166)
*Kyeong-Hun Kim,Junhee Ryu*

Main category: math.PR

TL;DR: This paper develops a comprehensive Sobolev regularity theory for stochastic partial differential equations (SPDEs) with Dirichlet boundary conditions in smooth domains, establishing existence, uniqueness, and regularity estimates for solutions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to establish a rigorous mathematical framework for analyzing the regularity properties of solutions to stochastic partial differential equations with Dirichlet boundary conditions, particularly for nonlocal operators and generalized Gaussian noise, which are important in many applications but lack comprehensive theoretical foundations.

Method: The authors employ Sobolev space theory and weighted function spaces to analyze SPDEs in C^{1,Ï} open sets. They work with substantially large classes of nonlocal operators and generalized Gaussian noise to develop their regularity framework.

Result: The main results include: (1) existence and uniqueness of strong solutions in weighted Sobolev spaces, and (2) maximal L_p-regularity estimates for the solutions of the Dirichlet problem for stochastic partial differential equations.

Conclusion: The paper successfully establishes a comprehensive Sobolev regularity theory for SPDEs with Dirichlet boundary conditions, providing both existence/uniqueness results and sharp regularity estimates that advance the theoretical understanding of stochastic partial differential equations in smooth domains.

Abstract: This paper provides a comprehensive Sobolev regularity theory for the
Dirichlet problem of stochastic partial differential equations in
$C^{1,\sigma}$ open sets. We consider substantially large classes of nonlocal
operators and generalized Gaussian noise. Our main results include the
existence and uniqueness of strong solutions in weighted Sobolev spaces, along
with maximal $L_p$-regularity estimates for the solutions.

</details>


### [49] [Existence results for the Cox-Ingersoll-Ross model with variable exponent diffusion](https://arxiv.org/abs/2507.17173)
*Mustafa Avci*

Main category: math.PR

TL;DR: The paper proposes a generalized Cox-Ingersoll-Ross model with state-dependent variable exponent function in the diffusion coefficient, analyzing its mathematical properties and demonstrating its validity through numerical experiments.


<details>
  <summary>Details</summary>
Motivation: To develop a more theoretically flexible framework that generalizes the classical Cox-Ingersoll-Ross model by incorporating a state-dependent variable exponent function p(Â·) in the diffusion coefficient, addressing limitations of existing models.

Method: Development of a new stochastic model with variable exponent diffusion coefficient, followed by mathematical analysis of existence, uniqueness, higher moment properties, and martingale properties of solutions, complemented by numerical experiments.

Result: The theoretical analysis establishes existence and uniqueness of solutions, characterizes higher moment and martingale properties. Numerical experiments validate the model's effectiveness and demonstrate its computational efficiency.

Conclusion: The proposed stochastic model with state-dependent variable exponent successfully generalizes the Cox-Ingersoll-Ross model, providing a more flexible theoretical framework with proven mathematical properties and demonstrated practical validity through numerical validation.

Abstract: We propose a new stochastic model where the diffusion coefficient involves a
state-dependent variable exponent function $p(\cdot)$. This new theoretically
flexible framework generalizes the classical Cox-Ingersoll-Ross model. The
existence, uniqueness, higher moment and martingale properties of solutions are
analyzed. The validity and efficiency of the model is illustrated with
numerical experiments.

</details>


### [50] [Weak stability by noise for approximations of doubly nonlinear evolution equations](https://arxiv.org/abs/2507.17331)
*Carlo Orrieri,Luca Scarpa,Ulisse Stefanelli*

Main category: math.PR

TL;DR: This paper proves existence and uniqueness in law of probabilistically weak solutions for doubly nonlinear stochastic evolution equations with rough additive noise, showing that stochastic noise can restore uniqueness that is typically lost in deterministic doubly nonlinear equations.


<details>
  <summary>Details</summary>
Motivation: Doubly nonlinear equations typically lack uniqueness in the deterministic case, making their analysis challenging. The authors investigate whether adding stochastic noise can restore well-posedness properties that are absent in the deterministic setting.

Method: The authors assume additive noise that is "rough enough" and construct probabilistically weak solutions of Friedrichs type for doubly nonlinear stochastic evolution equations. They then analyze the uniqueness in law of these solutions.

Result: The paper establishes existence of probabilistically weak solutions of Friedrichs type and proves their uniqueness in law. Additionally, they demonstrate stability for approximations of stochastic doubly nonlinear equations in a weak probabilistic sense.

Conclusion: The stochastic noise has a regularizing effect that is genuinely stochastic in nature - it restores uniqueness properties in doubly nonlinear equations that do not possess uniqueness in the deterministic case. This represents a fundamental difference between stochastic and deterministic doubly nonlinear evolution equations.

Abstract: Doubly nonlinear stochastic evolution equations are considered. Upon assuming
the additive noise to be rough enough, we prove the existence of
probabilistically weak solutions of Friedrichs type and study their uniqueness
in law. This entails stability for approximations of stochastic doubly
nonlinear equations in a weak probabilistic sense. Such effect is a genuinely
stochastic, as doubly nonlinear equations are not even expected to exhibit
uniqueness in the deterministic case.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [51] [A Scientist Question: Research on the Impact of Super Structured Quadrilateral Meshes on Convergence and Accuracy of Finite Element Analysis](https://arxiv.org/abs/2507.17184)
*Hui Zhao*

Main category: cs.GR

TL;DR: This paper proposes studying how global arrangement patterns of super structured quadrilateral meshes affect finite element calculation convergence and accuracy, moving beyond traditional local mesh quality metrics to address the current reliance on experience-based mesh generation.


<details>
  <summary>Details</summary>
Motivation: Current mesh generation practices in industry and academia rely heavily on experience rather than rigorous scientific principles, focusing only on local quality metrics (quadrilaterals/hexahedrons approximating squares/cubes) while ignoring the impact of global mesh arrangement patterns on finite element calculation convergence and accuracy.

Method: The paper proposes using modern two-dimensional and three-dimensional geometric topology theories including moduli space, TeichmÃ¼ller space, harmonic foliations, dynamical systems, surface mappings, and meromorphic quadratic differentials to generate and design super-structured quadrilateral meshes with controllable overall arrangement structures.

Result: The paper establishes a new research direction that explores the relationship between global mesh arrangement structures and finite element calculation performance, providing a theoretical framework to move away from experience-based mesh generation toward scientifically rigorous approaches.

Conclusion: This work opens a new research field that can help solve the non-rigorous reliance on experience in mesh generation by enabling clear judgments about which global mesh arrangements ensure finite element calculation convergence, potentially revolutionizing mesh generation practices in computational simulations.

Abstract: In the current practices of both industry and academia, the convergence and
accuracy of finite element calculations are closely related to the methods and
quality of mesh generation. For years, the research on high-quality mesh
generation in the domestic academic field has mainly referred to the local
quality of quadrilaterals and hexahedrons approximating that of squares and
cubes. The main contribution of this paper is to propose a brand-new research
direction and content: it is necessary to explore and study the influence of
the overall global arrangement structure and pattern of super structured
quadrilateral meshes on the convergence and calculation accuracy of finite
element calculations. Through the research in this new field, it can help solve
the non-rigorous state of serious reliance on "experience" in the mesh
generation stage during simulation in the current industry and academia, and
make clear judgments on which global arrangements of mesh generation can ensure
the convergence of finite element calculations. In order to generate and design
super-structured quadrilateral meshes with controllable overall arrangement
structures, a large number of modern two-dimensional and three-dimensional
geometric topology theories are required, such as moduli space, Teichm\"uller
space, harmonic foliations, dynamical systems, surface mappings, meromorphic
quadratic differentials, surface mappings, etc.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [52] [Convexity and the degenerate special Lagrangian equation](https://arxiv.org/abs/2507.17018)
*Vasanth Pidaparthy,Yanir A. Rubinstein*

Main category: math.DG

TL;DR: This paper resolves open questions about convexity properties of subsolutions to the degenerate special Lagrangian equation (DSL) by introducing a novel space-time coordinate transformation that preserves the space-time Lagrangian angle.


<details>
  <summary>Details</summary>
Motivation: The paper addresses two unresolved questions from previous work: (1) whether subsolutions in the top branch of DSL are convex in space-time, and (2) whether subsolutions in the second branch possess any convexity properties. These questions were raised by Rubinstein-Solomon (2015) and partially addressed by Darvas-Rubinstein (2019).

Method: The key methodological innovation is a space-time coordinate transformation that preserves the space-time Lagrangian angle. This transformation enables the derivation of a partial CÂ² estimate, which is crucial for analyzing the convexity properties of DSL subsolutions.

Result: The paper successfully settles both open questions about convexity properties of DSL subsolutions. Additionally, it demonstrates that the top two branches of the DSL subequation have a â-product structure in the sense of Ross-Witt-NystrÃ¶m.

Conclusion: Through the novel coordinate transformation technique, the authors provide complete answers to the outstanding convexity questions for DSL subsolutions and reveal new structural properties of the equation that connect it to the â-product framework.

Abstract: In 2015 Rubinstein--Solomon introduced the degenerate special Lagrangian
equation (DSL) that governs geodesics in the space of positive Lagrangians,
showed that subsolutions in the top branch of DSL are convex in space, and
raised the question of whether they should be convex in space-time and whether
subsolutions in the second branch possess any convexity properties. In 2019,
Darvas--Rubinstein gave a partial answer to the first problem by showing
subsolutions in the top branch must be bi-convex. We settle both questions. The
key new ingredient is a space-time coordinate transformation that preserves the
space-time Lagrangian angle and allows for a partial $C^2$ estimate. This also
shows that the top two branches of the DSL subequation have a $\star$-product
structure in the sense of Ross--Witt-Nystr\"om.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [53] [Characterizing proton polytropic indices inside near-Earth magnetic clouds and ICME sheaths](https://arxiv.org/abs/2507.17447)
*Debesh Bhattacharjee,Saikat Majumder,Prasad Subramanian*

Main category: astro-ph.SR

TL;DR: This study analyzes the polytropic index of protons in interplanetary coronal mass ejections (ICMEs) using Wind spacecraft data, finding distinct thermodynamic behaviors in different plasma regions and evidence for external heating from the corona.


<details>
  <summary>Details</summary>
Motivation: To understand the thermodynamics of ICME plasma by quantifying the polytropic index (Î³) which describes expansion/compression processes, and to investigate the energy budget and heating mechanisms in magnetic clouds and their associated regions.

Method: Statistical analysis of a large sample of well-observed ICME events from Wind spacecraft at 1 AU, calculating polytropic indices for protons in three distinct regions: ambient solar wind, sheath regions, and magnetic clouds (MCs).

Result: Found characteristic polytropic index pattern: high (~1.6) in ambient solar wind, low (~1.05) in sheath, and intermediate (~1.2) in magnetic clouds. The polytropic index is independent of small-scale density fluctuations. Stored energy in MC plasma is not used for cross-sectional expansion at 1 AU. Calculated heating gradient of ~0.21 erg cmâ»Â¹ gâ»Â¹ from corona to MC protons.

Conclusion: The sub-adiabatic nature of magnetic cloud plasma indicates external heating, likely from thermal conduction from the corona. The calculated heating gradient matches the required proton heating budget, supporting the hypothesis of coronal thermal conduction as a heating mechanism for ICME plasma.

Abstract: The thermodynamics of interplanetary coronal mass ejections (ICMEs) is often
described using a polytropic process. Estimating the polytopic index ($\gamma$)
allows us to quantify the expansion or compression of the ICME plasma arising
from changes in the plasma temperature. In this study, we estimate $\gamma$ for
protons inside the magnetic clouds (MCs), their associated sheaths, and ambient
solar wind for a large sample of well-observed events observed by the Wind
spacecraft at 1 AU. We find that $\gamma$ shows a high ($\approx 1.6$) - low
($\approx 1.05$) - high ($\approx 1.2$) behavior inside the ambient solar wind,
sheath, and MCs, respectively. We also find that the proton polytropic index is
independent of small-scale density fluctuations. Furthermore, our results show
that the stored energy inside MC plasma is not expended in expanding its
cross-section at 1 AU. The sub-adiabatic nature of MC plasma implies external
heating - possibly due to thermal conduction from the corona. We find that the
heating gradient per unit mass from the corona to the protons of MC at 1 AU is
$\approx 0.21$ erg cm$^{-1}$ g$^{-1}$ which is in agreement with the required
proton heating budget.

</details>
