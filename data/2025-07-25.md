<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 8]
- [math.AP](#math.AP) [Total: 20]
- [physics.comp-ph](#physics.comp-ph) [Total: 6]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 4]
- [math.CV](#math.CV) [Total: 2]
- [math.ST](#math.ST) [Total: 1]
- [quant-ph](#quant-ph) [Total: 3]
- [gr-qc](#gr-qc) [Total: 1]
- [astro-ph.GA](#astro-ph.GA) [Total: 1]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 1]
- [physics.acc-ph](#physics.acc-ph) [Total: 2]
- [nlin.CD](#nlin.CD) [Total: 1]
- [math.CO](#math.CO) [Total: 1]
- [hep-th](#hep-th) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [math.DG](#math.DG) [Total: 2]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Recovery Thresholding Hyperinterpolations in Signal Processing](https://arxiv.org/abs/2507.17916)
*Congpei An,Jiashu Ran*

Main category: math.NA

TL;DR: The paper introduces recovery thresholding hyperinterpolations for sparse signal reconstruction, integrating thresholding operators to maintain sparsity and using Newton's method for nonconvex optimization.


<details>
  <summary>Details</summary>
Motivation: To address sparse signal reconstruction in noisy environments by preserving sparsity during recovery.

Method: Integrates thresholding operators (hard, springback, Newton) into hyperinterpolation, extends Newton's method for multivariable nonconvex problems.

Result: Demonstrates robust performance in reconstructing signals corrupted by Gaussian and impulse noise, outperforming traditional methods.

Conclusion: The proposed methods effectively preserve sparsity and achieve accurate signal recovery, validated through numerical experiments.

Abstract: This paper introduces recovery thresholding hyperinterpolations, a novel
class of methods for sparse signal reconstruction in the presence of noise. We
develop a framework that integrates thresholding operators--including hard
thresholding, springback, and Newton thresholding--directly into the
hyperinterpolation structure to maintain sparsity during signal recovery. Our
approach leverages Newton's method to minimize one-dimensional nonconvex
functions, which we then extend to solve multivariable nonconvex regularization
problems. The proposed methods demonstrate robust performance in reconstructing
signals corrupted by both Gaussian and impulse noise. Through numerical
experiments, we validate the effectiveness of these recovery thresholding
hyperinterpolations for signal reconstruction and function denoising
applications, showing their advantages over traditional approaches in
preserving signal sparsity while achieving accurate recovery.

</details>


### [2] [A novel finite element method for simulating surface plasmon polaritons on complex graphene sheets](https://arxiv.org/abs/2507.17928)
*Jichun Li,Michael Neunteufel,Li Zhu*

Main category: math.NA

TL;DR: A simplified graphene model and new finite element method are developed to accurately simulate surface plasmon polaritons (SPPs) on graphene surfaces.


<details>
  <summary>Details</summary>
Motivation: SPPs provide insights into nano-optical and electrodynamic responses, but accurate simulation is challenging due to complex interfaces and boundary conditions.

Method: A simplified graphene model and a new finite element method are proposed, with stability analysis for the continuous model.

Result: Numerical results show the new model effectively captures SPPs for various complex graphene sheets.

Conclusion: The proposed model and method successfully address the challenges of simulating SPPs on graphene surfaces.

Abstract: Surface plasmon polaritons (SPPs) are generated on the graphene surface, and
provide a window into the nano-optical and electrodynamic response of their
host material and its dielectric environment. An accurate simulation of SPPs
presents several unique challenges, since SPPs often occur at complex
interfaces between materials of different dielectric constants and appropriate
boundary conditions at the graphene interfaces are crucial. Here we develop a
simplified graphene model and propose a new finite element method accordingly.
Stability for the continuous model is established, and extensive numerical
results are presented to demonstrate that the new model can capture the SPPs
very well for various complex graphene sheets.

</details>


### [3] [A stabilized Two-Step Formulation of Maxwell's Equations in the time-domain](https://arxiv.org/abs/2507.18235)
*Leon Herles,Mario Mally,Jörg Ostrowski,Sebastian Schöps,Melina Merkel*

Main category: math.NA

TL;DR: The paper presents a stabilized two-step formulation of Maxwell's equations for broad frequency ranges, addressing low-frequency instabilities with a generalized tree-cotree gauge.


<details>
  <summary>Details</summary>
Motivation: Numerical instabilities at low frequencies in electromagnetic field simulations motivate the need for a robust method.

Method: A Galerkin discretization in space with tailored time-discretization schemes for first- and second-order PDEs, combined with a generalized tree-cotree gauge.

Result: The method demonstrates stability, accuracy, and applicability to nonlinear, temperature-dependent materials in 3D problems.

Conclusion: The proposed approach effectively addresses low-frequency instabilities and is suitable for practical applications.

Abstract: Simulating electromagnetic fields across broad frequency ranges is
challenging due to numerical instabilities at low frequencies. This work
extends a stabilized two-step formulation of Maxwell's equations to the
time-domain. Using a Galerkin discretization in space, we apply two different
time-discretization schemes that are tailored to the first- and second-order in
time partial differential equations of the two-step solution procedure used
here. To address the low-frequency instability, we incorporate a generalized
tree-cotree gauge that removes the singularity of the curl-curl operator,
ensuring robustness even in the static limit. Numerical results on academic and
application-oriented 3D problems confirm stability, accuracy, and the method's
applicability to nonlinear, temperature-dependent materials.

</details>


### [4] [EigenWave: An Optimal O(N) Method for Computing Eigenvalues and Eigenvectors by Time-Filtering the Wave Equation](https://arxiv.org/abs/2507.18282)
*Daniel Appelo,Jeffrey W. Banks,William D. Henshaw,Ngan Le,Donald W. Schwendeman*

Main category: math.NA

TL;DR: EigenWave is an algorithm for computing eigenvalues and eigenvectors of elliptic boundary value problems using a WaveHoltz-based iteration with time filtering, targeting specific frequencies efficiently.


<details>
  <summary>Details</summary>
Motivation: The need for computing eigenvalues anywhere in the spectrum without inverting indefinite matrices, and efficiently solving for multiple eigenpairs near a target frequency.

Method: The algorithm solves a time-dependent wave equation iteratively, filtering solutions to emphasize eigenmodes near a target frequency. It can be embedded in a matrix-free Arnoldi algorithm and uses implicit time-stepping with multigrid for efficiency.

Result: Demonstrated efficiency with linear scaling (O(N)) and accuracy in computing eigenpairs of the Laplacian in complex geometries, using second-order and fourth-order approximations in 2D and 3D.

Conclusion: EigenWave provides an optimal, efficient, and flexible method for eigenvalue computation, especially useful for complex geometries and high-order approximations.

Abstract: An algorithm named EigenWave is described to compute eigenvalues and
eigenvectors of elliptic boundary value problems. The algorithm, based on the
recently developed WaveHoltz scheme, solves a related time-dependent wave
equation as part of an iteration. At each iteration, the solution to the wave
equation is filtered in time. As the iteration progresses, the filtered
solution generally contains relatively larger and larger proportions of
eigenmodes whose eigenvalues are near a chosen target frequency (target
eigenvalue). The ability to choose an arbitrary target frequency enables the
computation of eigenvalues anywhere in the spectrum, without the need to invert
an indefinite matrix, as is common with other approaches. Furthermore, the
iteration can be embedded within a matrix-free Arnoldi algorithm, which enables
the efficient computation of multiple eigenpairs near the target frequency. For
efficiency, the time-dependent wave equation can be solved with implicit
time-stepping and only about $10$ time-steps per-period are needed, independent
of the mesh spacing. When the (definite) implicit time-stepping equations are
solved with a multigrid algorithm, the cost of the resulting EigenWave scheme
scales linearly with the number of grid points $N$ as the mesh is refined,
giving an optimal $O(N)$ algorithm. The approach is demonstrated by finding
eigenpairs of the Laplacian in complex geometry using overset grids. Results in
two and three space dimensions are presented using second-order and
fourth-order accurate approximations.

</details>


### [5] [Parametric design and adaptive sizing of lattice structures for 3d additive manufacturing](https://arxiv.org/abs/2507.18318)
*Jorge Manuel Mercado-Colmenero,Daniel Diaz - Perete,Miguel Angel Rubio- Paramio,Cristina Martin-Donate*

Main category: math.NA

TL;DR: A parametric design model and adaptive mechanical analysis for a novel lattice structure in 3D additive manufacturing, optimizing volume, mass, and rigidity.


<details>
  <summary>Details</summary>
Motivation: To advance additive manufacturing by enabling dynamic adaptability of lattice structures for complex parts, improving mechanical performance.

Method: Geometric parameterization, mechanical adaptive sizing, and numerical validation using a 2D beam element model and rigidity analysis.

Result: The lattice structure's pyramidal geometry and parameterization enhance adaptability and mechanical performance, validated numerically.

Conclusion: The research introduces a systematic, adaptive approach to lattice design, improving efficiency and performance in industrial applications.

Abstract: The present research is developed into the realm of industrial design
engineering and additive manufacturing by introducing a parametric design model
and adaptive mechanical analysis for a new lattice structure, with a focus on
3D additive manufacturing of complex parts. Focusing on the land-scape of
complex parts additive manufacturing, this research proposes geometric
parameterization, mechanical adaptive sizing, and numerical validation of a
novel lattice structure to optimize the final printed part volume and mass, as
well as its structural rigidity. The topology of the lattice structures
exhibited pyramidal geometry. Complete parameterization of the lattice
structure ensures that the known geometric parameters adjust to defined
restrictions, enabling dynamic adaptability based on its load states and
boundary conditions, thereby enhancing its mechanical performance. The core
methodology integrates analytical automation with mechanical analysis by
employing a model based in two-dimensional beam elements. The dimensioning of
the lattice structure is analyzed using rigidity models of its sub-elements,
providing an evaluation of its global structural behavior after applying the
superposition principle. Numerical validation was performed to validate the
proposed analytical model. This step ensures that the analytical model defined
for dimensioning the lattice structure adjusts to its real mechanical behavior
and allows its validation. The present manuscript aims to advance additive
manufacturing methodologies by offering a systematic and adaptive approach to
lattice structure design. Parametric and adaptive techniques foster new
industrial design engineering methods, enabling the dynamic tailoring of
lattice structures to meet their mechanical demands and enhance their overall
efficiency and performance.

</details>


### [6] [On MAP estimates and source conditions for drift identification in SDEs](https://arxiv.org/abs/2507.18443)
*Daniel Tenbrinck,Nikolas Uesseler,Philipp Wacker,Benedikt Wirth*

Main category: math.NA

TL;DR: The paper addresses identifying the drift in an SDE from observations, deriving a MAP estimate, proving differentiability, and validating convergence rates numerically.


<details>
  <summary>Details</summary>
Motivation: To solve the inverse problem of drift identification in SDEs using observations, ensuring theoretical and practical validity.

Method: Derives a MAP estimate, proves differentiability and tangential cone conditions, and reviews existing theory for convergence rates.

Result: Numerical simulations in 1D support the theoretical convergence rates for the MAP estimate.

Conclusion: The study confirms the validity of convergence rates for drift identification in SDEs, backed by theory and simulations.

Abstract: We consider the inverse problem of identifying the drift in an SDE from $n$
observations of its solution at $M+1$ distinct time points. We derive a
corresponding MAP estimate, we prove differentiability properties as well as a
so-called tangential cone condition for the forward operator, and we review the
existing theory for related problems, which under a slightly stronger
tangential cone condition would additionally yield convergence rates for the
MAP estimate as $n\to\infty$. Numerical simulations in 1D indicate that such
convergence rates indeed hold true.

</details>


### [7] [Solution of Least Squares Problems with Randomized Preconditioned Normal Equations](https://arxiv.org/abs/2507.18466)
*Ilse C. F. Ipsen*

Main category: math.NA

TL;DR: Randomized preconditioners improve accuracy in solving full column-rank least squares problems, matching QR-based methods even for ill-conditioned matrices.


<details>
  <summary>Details</summary>
Motivation: To enhance the accuracy and efficiency of solving least squares problems using preconditioned normal equations, especially for ill-conditioned matrices.

Method: Uses randomized preconditioners (symmetrically or non-symmetrically) on normal equations, comparing results to QR-based methods.

Result: Preconditioned solutions achieve accuracy comparable to QR-based methods, with perturbation bounds matching the original problem.

Conclusion: Randomized preconditioners are effective, requiring minimal sampling, and provide reliable solutions for ill-conditioned least squares problems.

Abstract: We consider the solution of full column-rank least squares problems by means
of normal equations that are preconditioned, symmetrically or
non-symmetrically, with a randomized preconditioner. With an effective
preconditioner, the solutions from the preconditioned normal equations are
almost as accurate as those from the QR-based Matlab backslash (mldivide)
command -- even for highly illconditioned matrices. This means the accuracy of
the preconditioned normal equations depends on the residual of the original
least squares problem. We present non-intuitive but realistic perturbation
bounds for the relative error in the computed solutions and show that, with an
effective preconditioner, these bounds are essentially equal to the
perturbation bound for the original least squares problem. Probabilitistic
condition number bounds corroborate the effectiveness of the randomized
preconditioner computed with small amounts of sampling.

</details>


### [8] [Fast Multipole Method for Maxwell's Equations in Layered Media](https://arxiv.org/abs/2507.18491)
*Heng Yuan,Bo Wang,Wenzhong Zhang,Wei Cai*

Main category: math.NA

TL;DR: A fast multipole method (FMM) for solving Maxwell's equations in 3-D layered media is presented, using the magnetic vector potential and dyadic Green's function, achieving O(N log N) complexity.


<details>
  <summary>Details</summary>
Motivation: To efficiently solve Maxwell's equations in layered media, addressing computational challenges and improving numerical stability.

Method: Uses the magnetic vector potential under the Lorenz gauge, derives dyadic Green's function via Helmholtz layered Green's functions, and employs Chebyshev polynomial expansion for M2L translations.

Result: Demonstrates O(N log N) complexity and rapid convergence for low-frequency electromagnetic wave interactions.

Conclusion: The FMM method is effective for solving Maxwell's equations in layered media with high computational efficiency.

Abstract: We present a fast multipole method (FMM) for solving Maxwell's equations in
three-dimensional (3-D) layered media, based on the magnetic vector potential
$\boldsymbol A$ under the Lorenz gauge, to derive the layered dyadic Green's
function. The dyadic Green's function is represented using three scalar
Helmholtz layered Green's functions, with all interface-induced reaction field
components expressed through a unified integral representation. By introducing
equivalent polarization images for sources and effective locations for targets
to reflect the actual transmission distance of different reaction field
components, multiple expansions (MEs) and local expansions (LEs) are derived
for the far-field governed by actual transmission distance. To further enhance
computational efficiency and numerical stability, we employ a Chebyshev
polynomial expansion of the associated Legendre functions to speed up the
calculation of multipole-to-local (M2L) expansion translations. Finally,
leveraging the FMM framework of the Helmholtz equation in 3-D layered media, we
develop a FMM for the dyadic Green's function of Maxwell's equations in layered
media. Numerical experiments demonstrate the $\mathcal O(N\log N)$-complexity
of the resulting FMM method, and rapid convergence for interactions of
low-frequency electromagnetic wave sources in 3-D layered media.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [9] [Existence and nonexistence of sign-changing solutions for linearly perturbed superlinear equations on exterior domains](https://arxiv.org/abs/2507.17863)
*Md Suzan Ahamed,Joseph Iaia*

Main category: math.AP

TL;DR: Existence and nonexistence of radial sign-changing solutions for a nonlinear PDE in exterior domains, depending on the parameter α.


<details>
  <summary>Details</summary>
Motivation: To explore the behavior of solutions to a specific nonlinear PDE with singular and superlinear terms, particularly focusing on sign-changing solutions in exterior domains.

Method: Analysis of radial solutions for the given PDE, using assumptions on the growth and singularity of the nonlinear term f(u) and the behavior of K(|x|).

Result: Existence of infinitely many sign-changing solutions for N+q(N-2) < α < 2(N-1), and nonexistence for 0 < α ≤ 2.

Conclusion: The parameter α critically determines the existence of solutions, with a specific range allowing for infinitely many sign-changing solutions.

Abstract: In this paper, we study radial solutions of $\Delta u + K(|x|) f(u)+\frac{
(N-2)^2 u}{|x|^{2+(N-2)\delta}} =0, \ 0<\delta<2$ in the exterior of the ball
of radius $R>0$ in ${\mathbb R}^{N}$ where $f$ grows superlinearly at infinity
and is singular at $0$ with $f(u) \sim -\frac{1}{|u|^{q-1}u}$ and $0<q<1$ for
small $u$. We assume $K(|x|) \sim |x|^{-\alpha}$ for large $|x|$ and establish
the existence of an infinite number of sign-changing solutions when $N+q(N-2)
<\alpha <2(N-1).$ We also prove nonexistence for $0<\alpha \leq2$.

</details>


### [10] [Diffusion over ramified domains: solvability and fine regularity](https://arxiv.org/abs/2507.17909)
*Kevin Silva-Pérez,Alejandro Vélez-Santiago*

Main category: math.AP

TL;DR: The paper studies a domain with a fractal boundary modeling bronchial trees, analyzing diffusion equations with mixed boundary conditions. It proves unique solvability and global Hölder continuity of solutions, even for the critical case where the domain is roughest.


<details>
  <summary>Details</summary>
Motivation: The work aims to model oxygen diffusion in bronchial trees, addressing challenges posed by non-Lipschitz domains and fractal boundaries.

Method: The authors analyze a generalized diffusion equation with inhomogeneous mixed-type boundary conditions, proving solvability and continuity for stationary and time-dependent cases.

Result: Unique solvability and global Hölder continuity are established for weak solutions, including the critical parameter case where the domain is non-extension.

Conclusion: This is the first proof of global uniform continuity for Robin-type boundary problems on non-extension domains, with implications for modeling diffusion in complex structures.

Abstract: We consider a domain $\Omega\subseteq\mathbb{R\!}^{\,2}$ with branched
fractal boundary $\Gamma^{\infty}$ and parameter $\tau\in[1/2,\tau^{\ast}]$
introduced by Achdou and Tchou \cite{ACH08}, for $\tau^{\ast}\simeq 0.593465$,
which acts as an idealization of the bronchial trees in the lungs systems. For
each $\tau\in[1/2,\tau^{\ast}]$, the corresponding region $\Omega$ is a
non-Lipschitz domain, which attains its roughest structure at the critical
value $\tau=\tau^{\ast}$ in such way that in this endpoint parameter the region
$\Omega$ fails to be an extension domain, and its ramified boundary
$\Gamma^{\infty}$ is not post-critically finite. Then, we investigate a model
equation related to the diffusion of oxygen through the bronchial trees by
considering the realization of a generalized diffusion equation with
inhomogeneous mixed-type boundary conditions. Under minimal assumptions, we
first show that the stationary version of the above diffusion equation in
uniquely solvable, and that the corresponding weak solution in globally
H\"older continuous on $\overline{\Omega}$. Since we are including the critical
case $\tau=\tau^{\ast}$, this is the first time in which global uniform
continuity of weak solutions of a Robin-type boundary value problem is attained
over a non-extension domain. Furthermore, after two transitioning procedures,
we prove the unique solvability of the inhomogeneous time-dependent diffusion
equation, and we show that the corresponding weak solution is globally
uniformly continuous over $[0,T]\times\overline{\Omega}$ for each fixed
parameter $T>0$.

</details>


### [11] [Nonspherically symmetric equilibrium bubbles in a steadily rotating incompressible fluid](https://arxiv.org/abs/2507.17915)
*Chen-Chih Lai,Michael I. Weinstein*

Main category: math.AP

TL;DR: The paper presents rotational equilibrium solutions for an isobaric gas model, extending Gavrilov's work. It includes nonspherical bubble shapes and numerical simulations using PINN.


<details>
  <summary>Details</summary>
Motivation: To explore nontrivial rotational equilibria in inviscid isobaric gas models and extend existing results to nonspherical and axisymmetric cases.

Method: Theoretical analysis of equilibrium solutions, construction of horn-torus-shaped bubbles, and numerical simulation using Physics-Informed Neural Networks (PINN).

Result: Existence of rotational equilibrium solutions and nonspherical bubble shapes confirmed; numerical simulations validate theoretical findings.

Conclusion: The study advances understanding of rotational equilibria in inviscid isobaric models and demonstrates practical numerical validation.

Abstract: This note presents two nontrivial, rotational equilibrium solutions to the
spatial uniform gas pressure (isobaric) approximate model of Prosperetti in the
inviscid case. Building on Gavrilov's work [GAFA 2019], we first establish the
existence of equilibrium solutions with nontrivial (rotational) liquid flow.
Second, we construct a nonspherically symmetric, horn-torus-shaped equilibrium
bubble under mild spatial decay conditions of the liquid flow. In addition, we
extend earlier results on the characterization of spherical equilibrium bubbles
to the axisymmetric, purely azimuthal setting. Finally, we implement a
numerical simulation of the equilibrium bubble shape using the Physics-Informed
Neural Network (PINN) approximation.

</details>


### [12] [Existence of smooth solutions of the Navier-Stokes equations in three-dimensional Euclidean space](https://arxiv.org/abs/2507.18063)
*Genqian Liu*

Main category: math.AP

TL;DR: The paper proves the existence of smooth solutions for 3D incompressible Navier-Stokes equations by linking them to parabolic inertia Lamé equations and analyzing the limit of a Lamé constant.


<details>
  <summary>Details</summary>
Motivation: To establish the existence of smooth solutions for the incompressible Navier-Stokes equations in 3D space by leveraging their connection to parabolic inertia Lamé equations.

Method: Prove the existence and uniqueness of smooth solutions for parabolic inertia Lamé equations, then take the limit of one Lamé constant (λ→∞) while fixing the other (μ>0).

Result: Demonstrates the existence of smooth solutions for the incompressible Navier-Stokes equations in ℝ³.

Conclusion: The approach successfully links the two equations and provides a pathway to proving smooth solutions for Navier-Stokes via Lamé equations.

Abstract: Based on the essential connection of the parabolic inertia Lam\'{e} equations
and Navier-Stokes equations, we prove the existence of smooth solutions of the
incompressible Navier-Stokes equations in three-dimensional Euclidean space
$\mathbb{R}^3$ by showing the existence and uniqueness of smooth solutions of
the parabolic inertia Lam\'{e} equations and by letting a Lam\'{e} constant
$\lambda$ tends to infinity (the other Lam\'{e} constant $\mu>0$ is fixed).

</details>


### [13] [The magnetohydrodynamical system in bounded $\mathscr{C}^1$ domains of dimension $n\ge3$](https://arxiv.org/abs/2507.18195)
*Sylvie Monniaux*

Main category: math.AP

TL;DR: Existence of mild solutions for the magnetohydrodynamical system in critical spaces is proven for $\mathscr{C}^1$ domains in dimensions $n \ge 3$.


<details>
  <summary>Details</summary>
Motivation: To establish the existence of solutions for the magnetohydrodynamical system in less smooth ($\mathscr{C}^1$) domains, leveraging recent advances in Stokes operator regularity.

Method: Uses recent regularity results for the Stokes operator in $\mathscr{C}^1$ domains and introduces a Leibniz-like formula for differential forms.

Result: Existence of mild solutions in critical spaces for $n \ge 3$ is proven.

Conclusion: The study successfully extends solvability to $\mathscr{C}^1$ domains, combining Stokes operator regularity with a new differential forms formula.

Abstract: Existence of mild solutions for the magnetohydrodynamical system in
$\mathscr{C}^1$ domains is established in critical spaces in dimension $n\ge
3$. The proof relies on recent regularity results on the Stokes operator in
$\mathscr{C}^1$ domains and a Leibniz-like formula for differential forms
proved here in Section 2.

</details>


### [14] [$H^s_x$ regularity of solutions to the stationary Boltzmann equation with the incoming boundary condition](https://arxiv.org/abs/2507.18211)
*Daisuke Kawagoe*

Main category: math.AP

TL;DR: Existence of solutions for the stationary Boltzmann equation in a weighted $L^\infty$ space with fractional Sobolev regularity, without requiring positive Gaussian curvature on the boundary.


<details>
  <summary>Details</summary>
Motivation: To address the stationary Boltzmann equation with specific cross-section forms and boundary conditions, extending results to cases without strict boundary curvature assumptions.

Method: Proves well-posedness of the linearized problem in a weighted $L^2$ space, develops $L^2-L^\infty$ estimates, and uses velocity averaging lemma for regularity. Extends to weakly nonlinear problems via bilinear estimates.

Result: Solutions exist in $H^{1-}_x$ regularity for $-2 \leq \gamma \leq 1$, with worse regularity for $-3 < \gamma < -2$.

Conclusion: The approach successfully handles the Boltzmann equation under relaxed boundary conditions, providing regularity results for a range of cross-section parameters.

Abstract: We consider the stationary Boltzmann equation with the cross section of the
form $B(|v - \tilde{v}, \theta|) = B_0 |v - \tilde{v}|^\gamma \cos \theta \sin
\theta$ for $-3 < \gamma \leq 1$ in a bounded convex domain under the incoming
boundary condition. In this article, we shall show the existence of a solution
in a weighted $L^\infty$ space with fractional Sobolev regularity without
assuming the positivity of the Gaussian curvature on the boundary. For boundary
data sufficiently smooth and close to the standard Maxwellian, the solution has
$H^{1-}_x$ regularity for $-2 \leq \gamma \leq 1$, while only worse regularity
is obtained for $-3 < \gamma < -2$. We first show the well-posedness of the
linearized problem on a weighted $L^2$ space and develop the $L^2-L^\infty$
estimate without the stochastic cycle. We next investigate $H^s_x$ regularity
of the solution to the linearized problem. The velocity averaging lemma plays a
key role in our analysis. We finally derive a bilinear estimate to extend
results on the linearized problem to the weakly nonlinear problem.

</details>


### [15] [Quantum ergodicity for contact metric structures](https://arxiv.org/abs/2507.18216)
*Lino Benedetto*

Main category: math.AP

TL;DR: Proof of Quantum Ergodicity (QE) for subLaplacians on contact metric manifolds with ergodic Reeb flow, using a semiclassical pseudodifferential calculus and microlocal projectors.


<details>
  <summary>Details</summary>
Motivation: To extend QE theorems to subLaplacians on contact manifolds, leveraging ergodicity of the Reeb flow.

Method: Uses a specialized semiclassical pseudodifferential calculus and microlocal projectors (Landau projectors) to approximate the subLaplacian's action.

Result: Demonstrates QE for eigenfunctions of subLaplacians under ergodic Reeb flow.

Conclusion: The proof aligns with classical QE approaches once microlocal Weyl laws are established.

Abstract: This paper is dedicated to the proof of a Quantum Ergodicity (QE) theorem for
the eigenfunctions of subLaplacians on contact metric manifolds, under the
assumption that the Reeb flow is ergodic. To do so, we rely on a semiclassical
pseudodifferential calculus developed for general filtered manifolds that we
specialize to the setting of contact manifolds. Our strategy is then
reminiscent of an implementation of the Born-Oppenheimer approximation as we
rely on the construction of microlocal projectors in our calculus which commute
with the subLaplacian, called Landau projectors. The subLaplacian is then shown
to act effectively on the range of each Landau projector as the Reeb vector
field does. The remainder of the proof follows the classical path towards QE,
once microlocal Weyl laws have been established.

</details>


### [16] [Long-time existence for the 2D ideal Boussinesq and the 2D density-dependent Euler equations](https://arxiv.org/abs/2507.18244)
*Hantaek Bae,Milton Lopes Filho,Anna Mazzucato,Helena Nussenzveig Lopes*

Main category: math.AP

TL;DR: The paper proves long-time existence of smooth solutions for 2D ideal Boussinesq and non-homogeneous Euler equations with small perturbations.


<details>
  <summary>Details</summary>
Motivation: To address the long-time behavior of smooth solutions for these equations under small perturbations.

Method: Develops an elementary technique with broad applicability.

Result: Confirms known results (Danchin & Fanelli 2013, Danchin 2011) with a new approach.

Conclusion: The technique is simple and widely applicable, reinforcing prior findings.

Abstract: We establish long-time existence of smooth solutions to the 2D ideal
Boussinesq equations and to the 2D non-homogeneous incompressible Euler
equations for initial data consisting of small temperature perturbations, or
small density perturbations, of smooth initial flows which are not necessarily
small. Both results are known (see Danchin and Fanelli 2013, Danchin 2011 in
the references) but the technique we develop to prove them is at the same time
elementary and has broad potential applicability.

</details>


### [17] [Well-posedness of the compressible boundary layer equations with analytic initial data](https://arxiv.org/abs/2507.18247)
*Ya-Guang Wang,Yi-Lei Zhao*

Main category: math.AP

TL;DR: Study of compressible boundary layer equations' well-posedness with analytic tangential data, using Littlewood-Paley theory for local existence and uniqueness.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of thermal and viscous layers in compressible viscous flow under small viscosity and heat conductivity limits.

Method: Employ Littlewood-Paley theory for a priori estimates, focusing on solutions analytic in tangential and Sobolev in normal variables.

Result: Local existence and uniqueness of solutions in the specified function space.

Conclusion: The compressible boundary layer equations are well-posed under the given conditions.

Abstract: We study the well-posedness of the compressible boundary layer equations with
data being analytic in the tangential variable of the boundary. The
compressible boundary layer equations, a nonlinear coupled system of degenerate
parabolic equations and an elliptic equation, describe the behavior of thermal
layer and viscous layer in the small viscosity and heat conductivity limit, for
the two-dimensional compressible viscous flow with heat conduction with nonslip
and zero heat flux boundary conditions. We use the Littlewood-Paley theory to
establish the a priori estimates for solutions of this compressible boundary
layer problem, and obtain the local existence and uniqueness of the solution in
the space of analytic in the tangential variable and Sobolev in the normal
variable.

</details>


### [18] [Nonlinear Hardy-Stein type identities for harmonic functions relative to symmetric integro-differential operators](https://arxiv.org/abs/2507.18308)
*Tomasz Klimsiak,Andrzej Rozkosz*

Main category: math.AP

TL;DR: The paper presents Hardy-Stein type identities for harmonic functions related to integro-differential operators with mixed local and nonlocal components, extending results to convex compositions and conditional identities. Applications include norm characterizations and Littlewood-Paley estimates.


<details>
  <summary>Details</summary>
Motivation: The study aims to generalize Hardy-Stein identities to operators with mixed local and nonlocal components, addressing gaps in existing literature for such mixed-type operators.

Method: Probabilistic methods are employed to derive identities for harmonic functions and their convex compositions, with conditional identities for ratios. Examples include divergence form and purely nonlocal operators.

Result: Hardy-Stein identities are established for mixed-type operators, with applications in characterizing harmonic Hardy space norms and proving Littlewood-Paley estimates.

Conclusion: The paper successfully extends Hardy-Stein identities to mixed-type operators, providing new insights and tools for harmonic analysis in this broader context.

Abstract: We show identities of Hardy-Stein type for harmonic functions relative to
integro-differential operators corresponding to general symmetric regular
Dirichlet forms satisfying the absolute continuity condition. The novelty is
that we consider operators of mixed type containing both local and nonlocal
component. Moreover, the identities are proved for compositions of harmonic
functions and general convex functions. We also provide some conditional
identities, i.e. identities for ratios of harmonic functions. As an application
we give a characterization of norms in harmonic Hardy spaces and prove
Littlewood--Paley type estimates for square functions. To illustrate general
results, we discuss in some details the case of divergence form operator and
purely nonlocal operator defined by some jump kernel. Our proofs are rather
short and use mainly probabilistic methods.

</details>


### [19] [Quadratic flatness and Regularity for Codimension-One Varifolds with Bounded Anisotropic Mean Curvature](https://arxiv.org/abs/2507.18357)
*Mario Santilli,Sławomir Kolasiński*

Main category: math.AP

TL;DR: The paper proves the existence of a dense C¹,α-regular part in the support of a varifold with bounded mean ϕ-curvature, under certain conditions, and links it to a quadratic flatness theorem.


<details>
  <summary>Details</summary>
Motivation: The study aims to understand the regularity properties of varifolds with bounded mean curvature, extending known results to more general settings.

Method: The authors use a quadratic flatness theorem and apply the Allard anisotropic regularity theorem to analyze the regularity of varifolds.

Result: They show the existence of a dense C¹,α-regular part in the varifold's support and relate it to points where the support admits two mutually tangent balls.

Conclusion: The results provide insights into the structure of varifolds and their regularity, with implications for geometric measure theory.

Abstract: Given a uniformly convex norm $ \phi $ on $ \mathbf{R}^{n+1} $ and a
unit-density $ n $-dimensional varifold $ V $ in an open subset of $
\mathbf{R}^{n+1} $ with bounded mean $ \phi $-curvature, under the hypothesis
  $$ \textrm{$\mathcal{H}^n \llcorner {\rm spt}\| V \| $ is absolutely
continuous with respect to $ \| V \| $},$$
  we prove that there exists an open dense $ \mathscr{C}^{1, \alpha}$-regular
part in $ {\rm spt} \| V \| $. Moreover we prove that the $ \mathscr{C}^{1,
\alpha}$-regular part of $ V $ is $ \mathcal{H}^n $-almost equal to $({\rm
spt}\| V \|)^\ast $, the subset of $ {\rm spt}\| V \| $ where at least one blow
up (in Hausdorff distance) of $ {\rm spt}\| V \| $ is not equal to $
\mathbf{R}^{n+1} $.
  These results are consequences of a quadratic flatness theorem asserting that
if $ V $ is a general -- not necessarily rectifiable -- varifold $ V $ with
bounded mean $ \phi $-curvature and locally $ \mathcal{H}^n $-finite support,
then the set of points $ R $ where $ {\rm spt}\| V \|$ admits (exactly) two
mutually tangent balls satisfies $ \mathcal{H}^n(({\rm spt}\| V \|)^\ast
\setminus R) =0 $ and meets each relatively open set of $ {\rm spt}\| V \| $ on
a set of positive $ \mathcal{H}^n $-measure. Indeed, quadratic flatness at a
point guarantees that the Allard anisotropic regularity theorem can be applied,
hence proving that $ {\rm spt}\| V \|$ is regular around that point.

</details>


### [20] [Consistency of tug-of-war type operators on random data clouds](https://arxiv.org/abs/2507.18383)
*Jeongmin Han,Huajie Liu*

Main category: math.AP

TL;DR: The paper analyzes a tug-of-war type operator on geometric graphs and its Dirichlet problem on random data clouds, focusing on convergence as data points increase and step size shrinks.


<details>
  <summary>Details</summary>
Motivation: To explore the connection between the tug-of-war operator and its model problem through convergence analysis.

Method: Study the convergence of value functions with increasing data points and shrinking step size, emphasizing operator consistency.

Result: The analysis reveals the relationship between the tug-of-war operator and the corresponding model problem.

Conclusion: Consistency of the operator is crucial for establishing the connection between the tug-of-war type operator and its model problem.

Abstract: In this paper, we study a tug-of-war type operator on geometric graphs and
its associated Dirichlet problem on a random data cloud.
  Specifically, we analyze the convergence of the value functions as the number
of data points increases and the step size of the game shrinks. This analysis
reveals the connection between our tug-of-war type operator and the
corresponding model problem.
  A key ingredient in establishing this result is the consistency of the
operator.

</details>


### [21] [Homogenization and 3D-2D dimension reduction of a functional on manifold valued BV space](https://arxiv.org/abs/2507.18390)
*Luca Lussardi,Andrea Torricelli,Elvira Zappale*

Main category: math.AP

TL;DR: The paper analyzes the homogenization and dimension reduction of an energy functional with linear growth for manifold-valued Sobolev functions using Γ-convergence, leading to an integral representation in the space of manifold-constrained BV functions.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of energy functionals with linear growth in the context of manifold-valued Sobolev functions under simultaneous homogenization and dimension reduction.

Method: The study employs Γ-convergence techniques to analyze the energy functional and derive an integral representation.

Result: An integral representation result is obtained in the space of manifold-constrained functions with bounded variation.

Conclusion: The work provides a rigorous framework for analyzing such energy functionals, contributing to the understanding of homogenization and dimension reduction in manifold-valued settings.

Abstract: We study the simultaneous homogenization and dimension reduction of an energy
functional with linear growth defined on the space of manifold valued Sobolev
functions. The study is carried out by $\Gamma$-convergence, providing an
integral representation result in the space of manifold constrained functions
with bounded variation

</details>


### [22] [Asymmetric Kedem-Katchalsky boundary conditions for systems with spatial heterogeneities](https://arxiv.org/abs/2507.18400)
*Pablo Álvarez-Caudevilla,Cristina Brändle,Fermin González-Pereiro*

Main category: math.AP

TL;DR: The paper studies a model of two species interacting across adjacent areas with spatial growth variations and crowding effects, focusing on refuge zones and boundary conditions. It identifies conditions for unique positive solutions and explores population behavior near critical growth parameters.


<details>
  <summary>Details</summary>
Motivation: To understand how two species interact across separate but adjacent habitats, especially in refuge zones with unlimited resources, and how boundary conditions influence their population dynamics.

Method: Uses a system of equations with spatial growth rates and crowding effects, incorporating asymmetric Kedem-Katchalsky boundary conditions to model species interaction. Analyzes existence, non-existence, and behavior of positive solutions.

Result: A unique positive population distribution exists within a specific growth rate range. Bifurcation points and non-simultaneous blow-up phenomena are identified, where one population grows infinitely while the other remains bounded.

Conclusion: The study provides insights into species interaction dynamics in refuge zones and boundary conditions, highlighting critical growth parameters and unique population behaviors.

Abstract: This work investigates a model describing the interaction of two species in
habiting separate but adjacent areas. These populations are governed by a
system of
  equations that account for spatial variations in growth rates and the effects
of crowding.
  A key feature is the presence of areas within each domain where resources are
unlimited
  and crowding effects are absent. The species interact solely through a common
bound ary interface, which is modeled by asymmetric Kedem-Katchalsky boundary
conditions.
  The paper provides existence, non-existence, and behavior of positive
solutions for the
  system. It is shown that a unique positive population distribution exists
when one of the
  growth rate parameters falls within a specific range defined by two critical
values. One of
  these critical values represents a bifurcation point where the population can
emerge from
  extinction, while the other is determined by the characteristics of the
refuge areas. The
  study also examines how the populations behave as the growth parameter
approaches the
  upper critical value. This analysis reveals the phenomenon of
non-simultaneous blow-up,
  where one population component can grow infinitely large within its refuge
zone while
  the other remains bounded.

</details>


### [23] [Strong time regularity and decay of $L^\infty$ solutions to $2\times 2$ systems of conservation laws](https://arxiv.org/abs/2507.18427)
*Luca Talamini*

Main category: math.AP

TL;DR: The paper analyzes $\mathbf L^\infty$ solutions for $2\times2$ conservation law systems, proving regularity for finite entropy solutions and a decay estimate for vanishing viscosity solutions, using a kinetic formulation.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of solutions to $2\times2$ conservation law systems, particularly focusing on regularity and decay properties.

Method: Uses a kinetic formulation to unify the analysis of finite entropy solutions and vanishing viscosity solutions.

Result: Finite entropy solutions belong to $C^0(\mathbb R^+; \mathbf L^1_{loc}(\mathbb R))$, and a dispersive decay estimate is established for vanishing viscosity solutions.

Conclusion: The kinetic formulation provides a unified framework for analyzing both regularity and decay in conservation law systems.

Abstract: We consider $\mathbf L^\infty$ solutions to $2\times 2$ systems of
conservation laws. For genuinely nonlinear systems we prove that finite entropy
solutions (in particular entropy solutions, if a uniformly convex entropy
exists) belong to $C^0(\mathbb R^+; \mathbf L^1_{loc}(\mathbb R))$. Our second
result establishes a dispersive-type decay estimate for vanishing viscosity
solutions. Both results are unified by the use of a kinetic formulation.

</details>


### [24] [Gradient regularity for double-phase orthotropic functionals](https://arxiv.org/abs/2507.18474)
*Stefano Almi,Chiara Leone,Gianluigi Manzo*

Main category: math.AP

TL;DR: Higher integrability and Lipschitz regularity are proven for local minimizers of a double-phase orthotropic functional under specific conditions on the exponents and weight function.


<details>
  <summary>Details</summary>
Motivation: To extend regularity results for minimizers of double-phase functionals, particularly in the orthotropic case, under weaker assumptions on the weight function and exponents.

Method: Analyze the functional with a Hölder continuous weight function and specific exponent conditions, using Sobolev regularity assumptions for further estimates.

Result: Higher integrability is proven for local minimizers, and explicit Lipschitz regularity estimates are derived under Sobolev regularity of the weight function.

Conclusion: The results generalize and refine existing regularity theory for double-phase functionals, providing explicit estimates under broader conditions.

Abstract: We prove higher integrability for local minimizers of the double-phase
orthotropic functional \[
  \sum_{i=1}^{n}\int_\Omega\left(\left|u_{x_i}\right|^p+a(x)\left|
u_{x_i}\right|^q\right)dx \] when the weight function $a \geq0$ is assumed to
be $\alpha$-H\"older continuous, while the exponents $p, q$ are such that $2
\leq p \leq q$ and $\frac{q}{p} < 1 + \frac{\alpha}{n}$. Under natural Sobolev
regularity of~$a$, we further obtain explicit Lipschitz regularity estimates
for local minimizers.

</details>


### [25] [A dichotomy result for a modified Schrödinger equations on unbounded domains](https://arxiv.org/abs/2507.18528)
*Anna Maria Candela,Giuliana Palmieri,Addolorata Salvatore*

Main category: math.AP

TL;DR: The paper investigates bounded positive solutions for a generalized PDE problem (P) with variational structure, extending the modified Schrödinger equation. It proves existence under certain conditions, even without radial symmetry.


<details>
  <summary>Details</summary>
Motivation: To explore the existence of bounded positive solutions for a nonlinear PDE problem (P) in unbounded domains, generalizing the modified Schrödinger equation.

Method: Uses variational methods and limits of solutions on bounded domains to find solutions for (P). Stronger hypotheses ensure nontrivial solutions or specific behavior.

Result: Existence of at least one bounded positive solution for (P) is proven. Under stronger conditions, solutions are nontrivial or exhibit specific asymptotic behavior.

Conclusion: The study successfully establishes the existence of bounded positive solutions for (P) and provides insights into their behavior under additional hypotheses.

Abstract: This article aims to investigate the existence of bounded positive solutions
of problem \[ (P)\qquad \left\{ \begin{array}{ll} - {\rm div} (a(x,u,\nabla u))
+ A_t(x,u,\nabla u) = g(x,u) &\hbox{in $\Omega$,}\\ u\ = \ 0 & \hbox{on
$\partial\Omega$,} \end{array}\right.\] with $A_t(x,t,\xi) = \frac{\partial
A}{\partial t}(x,t,\xi)$, $a(x,t,\xi) = \nabla_\xi A(x,t,\xi)$ for a given
$A(x,t,\xi)$ which grows as $|\xi|^p + |t|^p$ , $p > 1$, where $\Omega
\subseteq \mathbb{R}^N$, $N \ge 2$, is an open connected domain with Lipschitz
boundary and infinite Lebesgue measure, eventually $\Omega = \mathbb{R}^N$,
which generalizes the modified Schr\"odinger equation \[ - {\rm div} ((A^*_1(x)
+ A^*_2(x)|u|^{s}) \nabla u) + \frac{s}2 A^*_2(x)\ |u|^{s - 2} u\ |\nabla u|^2
+ u\ =\ |u|^{\mu-2}u \quad\hbox{in $\mathbb{R}^3$.} \] Under suitable
assumptions on $A(x,t,\xi)$ and $g(x,t)$, problem $(P)$ has a variational
structure. Then, even in lack of radial symmetry hypotheses, one bounded
positive solution of $(P)$ can be found by passing to the limit on a sequence
$(u_k)_k$ of bounded solutions on bounded domains. Furthermore, if stronger
hypotheses are satisfied, either such a solution is nontrivial or a constant
$\bar{\lambda} > 0$ and a sequence of points $(y_k)_k \subset \mathbb{R}^N$
exist such that \[ |y_k| \to +\infty\qquad \hbox{and}\qquad \int_{B_1(y_k)}
|u_k|^p dx \ge \bar{\lambda}\quad \hbox{for all $k \ge 1$.} \]

</details>


### [26] [Schrodinger-Poisson-Slater equations with nonlinearity subscaled near zero](https://arxiv.org/abs/2507.18568)
*Shibo Liu,Kanishka Perera*

Main category: math.AP

TL;DR: The paper studies a zero-mass Schrödinger-Poisson-Slater equation with subscaled nonlinearity near zero and asymptotically scaled at infinity, obtaining solutions via Morse theory and Clark's theorem.


<details>
  <summary>Details</summary>
Motivation: To address the existence of solutions for the given equation under specific nonlinearity conditions, leveraging advanced mathematical tools.

Method: Uses Morse theory for a nonzero solution and a version of Clark's theorem for multiple solutions, with abstract results on critical groups at infinity.

Result: A nonzero solution is obtained via Morse theory, and a sequence of solutions is derived for odd nonlinearities using Clark's theorem.

Conclusion: The study successfully demonstrates the existence of solutions under the specified conditions, contributing to the understanding of such equations.

Abstract: We study the following zero-mass Schr{\"o}dinger-Poisson-Slater equation \[ -
\Delta u + \left( \frac{1}{4 \pi | x |} \ast u^2 \right) u = f (| x |, u)
\text{,} \qquad u \in \mathcal{D}^{1, 2} (\mathbb{R}^3) \text{} \] with
nonlinearity subscaled near zero in the sense that $f (| x |, t) \approx a | t
|^{p - 2} t$ as $| t | \rightarrow 0$ for some $p\in\big(\frac{18}{7},3\big)$.
A nonzero solution is obtained via Morse theory when the nonlinearity is
asymptotically scaled at infinity. For this purpose we prove an abstract result
on the critical groups at infinity for functionals satisfying the geometric
assumptions of the scaled saddle point theorem of Mercuri \& Perera
[arXiv:2411.15887]. For the case that $f (| x |, \cdot)$ is odd, a sequence of
solutions are obtained via a version of Clark's theorem due to Kajikiya [J.\
Funct.\ Anal.\ 225 (2005) 352--370].

</details>


### [27] [Implementation of the inverse scattering transform method for the nonlinear Schrödinger equation](https://arxiv.org/abs/2507.18586)
*Vladislav V. Kravchenko*

Main category: math.AP

TL;DR: The paper presents a novel method for solving the nonlinear Schrödinger equation using series representations for Jost solutions, simplifying direct and inverse scattering problems without traditional complex techniques.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in solving the nonlinear Schrödinger equation by simplifying the inverse scattering transform method, avoiding complex procedures like the Gelfand-Levitan-Marchenko equation.

Method: Uses new series representations for Jost solutions of the Zakharov-Shabat system, reducing the problem to computing series coefficients and solving linear algebraic equations.

Result: Develops a simple, efficient algorithm for numerical solution of the initial-value problem, demonstrated with numerical examples.

Conclusion: The method provides a straightforward and effective alternative to existing techniques for solving the nonlinear Schrödinger equation.

Abstract: We study the initial-value problem for the nonlinear Schr\"odinger equation.
Application of the inverse scattering transform method involves solving direct
and inverse scattering problems for the Zakharov-Shabat system with complex
potentials. We solve these problems by using new series representations for the
Jost solutions of the Zakharov-Shabat system. The representations have the form
of power series with respect to a transformed spectral parameter. In terms of
the representations, solution of the direct scattering problem reduces to
computing the series coefficients following a simple recurrent integration
procedure, computation of the scattering coefficients by multiplying
corresponding pairs of polynomials (partial sums of the series representations)
and locating zeros of a polynomial inside the unit disk. Solution of the
inverse scattering problem reduces to the solution of a system of linear
algebraic equations for the power series coefficients, while the potential is
recovered from the first coefficients. The system is obtained directly from the
scattering relations. Thus, unlike other existing techniques, the method does
not involve solving the Gelfand-Levitan-Marchenko equation or the matrix
Riemann-Hilbert problem. The overall approach leads to a simple and efficient
algorithm for the numerical solution of the initial-value problem for the
nonlinear Schr\"odinger equation, which is illustrated by numerical examples.

</details>


### [28] [Vortex dynamics for the Gross-Pitaevskii equation](https://arxiv.org/abs/2507.18590)
*Manuel del Pino,Rowan Juneman,Monica Musso*

Main category: math.AP

TL;DR: The paper rigorously analyzes the asymptotics of vortex dynamics in the Gross-Pitaevskii equation, showing Helmholtz-Kirchhoff system dominance and validating a formal expansion.


<details>
  <summary>Details</summary>
Motivation: To understand the formal asymptotics of vortex dynamics in the Gross-Pitaevskii equation, particularly for multi-vortex solutions.

Method: Constructs n-vortex solutions, computes asymptotic expansions of vortex positions, and analyzes dynamics governed by the Helmholtz-Kirchhoff system.

Result: The leading-order dynamics is confirmed as the Helmholtz-Kirchhoff system, with the first correction linked to a linear wave equation.

Conclusion: The study validates a formal expansion by Ovchinnikov and Sigal and provides precise descriptions of vortex dynamics for large time intervals.

Abstract: We rigorously establish the formal asymptotics of Neu for Gross-Pitaevskii
vortex dynamics in the plane. Given any integer $n\geq2$, we construct a family
of $n$-vortex solutions with vortices of degree $\pm1$, and describe precisely
the solution profile and associated vortex dynamics on an arbitrarily large,
finite time interval. We compute an asymptotic expansion of the vortex
positions in terms of the vortex core size $\epsilon>0$, and show that the
dynamics is governed at leading order as $\epsilon\to0$ by the classical
Helmholtz-Kirchhoff system. Moreover, we show that the first correction to the
leading order dynamics is determined by the solution of a linear wave equation,
justifying a formal expansion found by Ovchinnikov and Sigal.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [29] [Multi-Head Neural Operator for Modelling Interfacial Dynamics](https://arxiv.org/abs/2507.17763)
*Mohammad Sadegh Eshaghi,Navid Valizadeh,Cosmin Anitescu,Yizheng Wang,Xiaoying Zhuang,Timon Rabczuk*

Main category: physics.comp-ph

TL;DR: The paper introduces the Multi-Head Neural Operator (MHNO), a neural operator framework for solving time-dependent PDEs, addressing challenges like error accumulation and computational cost. MHNO outperforms existing methods in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional numerical methods for solving stiff, time-dependent PDEs are computationally expensive, especially for high-dimensional problems. Neural operators offer a promising alternative, but existing ones struggle with error accumulation or resource demands.

Method: MHNO uses a novel architecture with time-step-specific projection operators and explicit temporal connections inspired by message-passing. It predicts all time steps in one forward pass, capturing long-term dependencies without parameter overgrowth.

Result: MHNO is applied to various phase field equations, showing superior accuracy, scalability, and efficiency compared to existing neural operator methods.

Conclusion: MHNO is a promising next-generation tool for phase field modeling, offering improved performance and computational efficiency.

Abstract: Interfacial dynamics underlie a wide range of phenomena, including phase
transitions, microstructure coarsening, pattern formation, and thin-film
growth, and are typically described by stiff, time-dependent nonlinear partial
differential equations (PDEs). Traditional numerical methods, including finite
difference, finite element, and spectral techniques, often become
computationally prohibitive when dealing with high-dimensional problems or
systems with multiple scales. Neural operators (NOs), a class of deep learning
models, have emerged as a promising alternative by learning mappings between
function spaces and efficiently approximating solution operators. In this work,
we introduce the Multi-Head Neural Operator (MHNO), an extended neural operator
framework specifically designed to address the temporal challenges associated
with solving time-dependent PDEs. Unlike existing neural operators, which
either struggle with error accumulation or require substantial computational
resources for high-dimensional tensor representations, MHNO employs a novel
architecture with time-step-specific projection operators and explicit temporal
connections inspired by message-passing mechanisms. This design allows MHNO to
predict all time steps after a single forward pass, while effectively capturing
long-term dependencies and avoiding parameter overgrowth. We apply MHNO to
solve various phase field equations, including antiphase boundary motion,
spinodal decomposition, pattern formation, atomic scale modeling, and molecular
beam epitaxy growth model, and compare its performance with existing NO-based
methods. Our results show that MHNO achieves superior accuracy, scalability,
and efficiency, demonstrating its potential as a next-generation computational
tool for phase field modeling. The code and data supporting this work is
publicly available at https://github.com/eshaghi-ms/MHNO.

</details>


### [30] [Machine Learning Workflow for Analysis of High-Dimensional Order Parameter Space: A Case Study of Polymer Crystallization from Molecular Dynamics Simulations](https://arxiv.org/abs/2507.17980)
*Elyar Tourani,Brian J. Edwards,Bamin Khomami*

Main category: physics.comp-ph

TL;DR: A machine learning workflow is introduced to accurately quantify crystallinity in polymers using atomistic data, reducing biases by combining multiple descriptors and identifying minimal order parameters.


<details>
  <summary>Details</summary>
Motivation: Current methods for identifying crystallization pathways in polymers rely on single order parameters with systematic biases and sensitivity to cut-off points.

Method: An integrated ML workflow uses high-dimensional feature vectors, low-dimensional embeddings, and unsupervised clustering to label crystalline/amorphous atoms, followed by supervised learning to identify minimal order parameters.

Result: Three order parameters suffice to recreate labels, achieving over 0.98 AUC. The crystallinity index (C-index) remains bimodal and enables efficient on-the-fly computation.

Conclusion: The workflow offers a data-driven OP selection strategy and a metric for monitoring structural transformations, revealing entropy's early dominance and symmetry's later relevance in crystallization.

Abstract: Currently, identification of crystallization pathways in polymers is being
carried out using molecular simulation-based data on a preset cut-off point on
a single order parameter (OP) to define nucleated or crystallized regions.
Aside from sensitivity to cut-off, each of these OPs introduces its own
systematic biases. In this study, an integrated machine learning workflow is
presented to accurately quantify crystallinity in polymeric systems using
atomistic molecular dynamics data. Each atom is represented by a
high-dimensional feature vector that combines geometric, thermodynamic-like,
and symmetry-based descriptors. Low dimensional embeddings are employed to
expose latent structural fingerprints within atomic environments. Subsequently,
unsupervised clustering on the embeddings identified crystalline and amorphous
atoms with high fidelity. After generating high quality labels with
multidimensional data, we use supervised learning techniques to identify a
minimal set of order parameters that can fully capture this label. Various
tests were conducted to reduce the feature set, demonstrating that using only
three order parameters is sufficient to recreate the crystallization labels.
Based on these observed OPs, the crystallinity index (C-index) is defined as
the logistic regression model's probability of crystallinity, remaining bimodal
throughout the process and achieving over 0.98 classification performance
(AUC). Notably, a model trained on one or a few snapshots enables efficient
on-the-fly computation of crystallinity. Lastly, we demonstrate how the optimal
C-index fit evolves during various stages of crystallization, supporting the
hypothesis that entropy dominates early nucleation, while symmetry gains
relevance later. This workflow provides a data-driven strategy for OP selection
and a metric to monitor structural transformations in large-scale polymer
simulations.

</details>


### [31] [Hierarchical Finite-Element Analysis of Multiscale Electromagnetic Problems via Sparse Operator-Adapted Wavelet Decomposition](https://arxiv.org/abs/2507.17989)
*F. Şık,F. L. Teixeira,B. Shanker*

Main category: physics.comp-ph

TL;DR: A FEM framework with wavelet decomposition decouples resolution levels for efficient multiscale electromagnetic analysis, achieving high precision and near-linear complexity.


<details>
  <summary>Details</summary>
Motivation: Existing adaptive FEM methods couple resolution levels, causing computational overhead when finer details are added.

Method: Uses a hierarchical algorithm with sparse matrix-vector multiplications and Krylov subspace solvers with ILU preconditioners.

Result: Numerical experiments confirm high precision and nearly linear computational complexity.

Conclusion: The proposed method efficiently decouples resolution levels, reducing computational overhead while maintaining accuracy.

Abstract: In this paper, we present a finite element method (FEM) framework enhanced by
an operator-adapted wavelet decomposition algorithm designed for the efficient
analysis of multiscale electromagnetic problems. Usual adaptive FEM approaches,
while capable of achieving the desired accuracy without requiring a complete
re-meshing of the computational domain, inherently couple different resolution
levels. This coupling requires recomputation of coarser-level solutions
whenever finer details are added to improve accuracy, resulting in substantial
computational overhead. Our proposed method addresses this issue by decoupling
resolution levels. This feature enables independent computations at each scale
that can be incorporated into the solutions to improve accuracy whenever
needed, without requiring re-computation of coarser-level solutions. The main
algorithm is hierarchical, constructing solutions from finest to coarser levels
through a series of sparse matrix-vector multiplications. Due to its sparse
nature, the overall computational complexity of the algorithm is nearly linear.
Moreover, Krylov subspace iterative solvers are employed to solve the final
linear equations, with ILU preconditioners that enhance solver convergence and
maintain overall computational efficiency. The numerical experiments presented
in this article verify the high precision and nearly linear computational
complexity of the proposed algorithm.

</details>


### [32] [A causality inspired acceleration method for the fast temporal superposition of the finite line source solutions](https://arxiv.org/abs/2507.18200)
*Marc Basquens,Alberto Lazzarotto*

Main category: physics.comp-ph

TL;DR: A fast method for computing thermal interactions in solids, improving performance over existing non-history temporal superposition algorithms by leveraging heat wave properties and reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: Addressing the need for efficient computation in time-dependent problems with multiple sources and scales, such as borehole heat exchangers, where existing methods are computationally expensive.

Method: Uses non-history temporal superposition with heat wave propagation properties to define influence regions, reducing integration areas and favoring numerical integration. Replaces Bakhalov-Vasil'eva with an asymptotic method for oscillatory integrals.

Result: Significantly reduces precomputation costs (by orders of magnitude), making the method viable for large-scale simulations with hundreds of sources and time steps.

Conclusion: The new method outperforms existing approaches, enabling efficient simulations for complex thermal interaction problems.

Abstract: We present a novel, fast method to compute thermal interactions in solids,
useful for time-dependent problems involving several sources and several time
and space scales such as the ones encountered in the physics of fields of
closed loop borehole heat exchangers. The new method is based on the
non-history temporal superposition acceleration algorithm, but presents better
performance compared to the originally proposed scheme. The main idea behind it
is to leverage the propagation properties of the heat wave. Despite the basic
physical solutions of heat transfer being non-causal, it is possible to
establish an influence region by fixing an acceptable error tolerance. This
allows to reduce the necessary integration regions in such a way that numerical
integration is favored. The better behaviour of the integrand arising from this
approach allows us to replace the use of Bakhalov-Vasil'eva method in favor of
the asymptotic method for the computation of highly oscillatory integrals that
has better properties from a computational perspective in the present
application. Extensive testing is presented to evaluate the robustness of the
new method and to compare its performance against the originally proposed
non-history method and the convolution using the FFT algorithm for a range of
error tolerances. The results show that the computational cost is highly
reduced for the precomputation, which includes all the computations done before
starting the time-stepping scheme. The reduction is of several orders of
magnitude, depending on the specific case. This cost was the bottleneck of the
original non-history implementation, and reducing it in this way makes the
method suitable for simulations involving hundreds of sources and hundreds of
thousands of time steps that can arise in simulations of borehole fields.

</details>


### [33] [Atomistic Generative Diffusion for Materials Modeling](https://arxiv.org/abs/2507.18314)
*Nikolaj Rønne,Bjørk Hammer*

Main category: physics.comp-ph

TL;DR: A generative modeling framework for atomistic systems using diffusion processes for atomic positions and types, achieving high fidelity and diversity in generating structures.


<details>
  <summary>Details</summary>
Motivation: To enable flexible and physically grounded generation of atomic structures across diverse chemical and structural domains.

Method: Combines score-based diffusion for atomic positions with continuous-time discrete diffusion for atomic types, applied to metallic clusters and 2D materials.

Result: Strong performance in fidelity and diversity, demonstrated through precision-recall metrics and capabilities like atomic type interpolation and symmetry-guided sampling.

Conclusion: The framework, implemented in AGeDi, offers an extensible solution for atomistic generative diffusion modeling.

Abstract: We present a generative modeling framework for atomistic systems that
combines score-based diffusion for atomic positions with a novel
continuous-time discrete diffusion process for atomic types. This approach
enables flexible and physically grounded generation of atomic structures across
chemical and structural domains. Applied to metallic clusters and
two-dimensional materials using the QCD and C2DB datasets, our models achieve
strong performance in fidelity and diversity, evaluated using precision-recall
metrics against synthetic baselines. We demonstrate atomic type interpolation
for generating bimetallic clusters beyond the training distribution, and use
classifier-free guidance to steer sampling toward specific crystallographic
symmetries in two-dimensional materials. These capabilities are implemented in
Atomistic Generative Diffusion (AGeDi), an open-source, extensible software
package for atomistic generative diffusion modeling.

</details>


### [34] [Topology-Preserving Coupling of Compressible Fluids and Thin Deformables](https://arxiv.org/abs/2507.18460)
*Jonathan Panuelos,Eitan Grinspun,David Levin*

Main category: physics.comp-ph

TL;DR: A novel method for discretizing coupled compressible fluid and thin deformable structures ensures leakproofness by preserving fluid domain connectedness, using Voronoi-based partitioning and Godunov-style integration.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of leakproofness in fluid-structure interactions, especially with thin structures, while enabling accurate force exchange.

Method: Combines constrained Voronoi-based spatial partitioning with Godunov-style finite-volume time integration to discretize the fluid domain conforming to the fluid-solid interface.

Result: Validated on scenarios like a balloon, champagne cork, and supersonic asteroid, showing bidirectional energy transfer without fluid leakage.

Conclusion: The method effectively ensures leakproofness and accurate fluid-structure interaction, even with thin structures.

Abstract: We present a novel discretization of coupled compressible fluid and thin
deformable structures that provides sufficient and necessary leakproofness by
preserving the path connectedness of the fluid domain. Our method employs a
constrained Voronoi-based spatial partitioning combined with Godunov-style
finite-volume time integration. The fluid domain is discretized into cells that
conform exactly to the fluid-solid interface, allowing boundary conditions to
be sharply resolved exactly at the interface. This enables direct force
exchange between the fluid and solid while ensuring that no fluid leaks through
the solid, even when arbitrarily thin. We validate our approach on a series of
challenging scenarios -- including a balloon propelled by internal compressed
air, a champagne cork ejecting after overcoming friction, and a supersonic
asteroid -- demonstrating bidirectional energy transfer between fluid and
solid.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [35] [Bright polarised x-ray flashes from dense plasmas](https://arxiv.org/abs/2507.18078)
*Q. Qian,C. P. Ridgers,S. V. Bulanov,T. Grismayer,P. Hadjisolomou,D. Seipt,M. Vranic,A. G. R. Thomas*

Main category: physics.plasm-ph

TL;DR: The paper explores using polarization of X-rays as an indicator of strong-field QED (SFQED) plasma production, distinguishing it from other hard X-ray sources like bremsstrahlung.


<details>
  <summary>Details</summary>
Motivation: Understanding SFQED plasmas is crucial for astrophysical environments like pulsar magnetospheres, but their dynamics are poorly understood.

Method: A laser of intensity $10^{21}$ Wcm$^{-2}$ is used on a solid Al target to produce X-rays, with polarization measured as a key observable.

Result: Photons from the X-ray flash with energy >10 keV are >65% polarized, confirming SFQED plasma production.

Conclusion: Polarization is a reliable indicator of SFQED plasma, aiding future studies and applications in extreme astrophysical conditions.

Abstract: Creating a plasma dominated by strong-field QED (SFQED) effects is a major
goal of new multi-PW laser facilities. This is motivated by the fact that the
fundamental dynamics of such plasmas is poorly understood and plays an
important role in the electrodynamics of extreme astrophysical environments
such as pulsar magnetospheres. The most obvious observable for which such a
regime has been reached is the production of a bright flash of x-rays, but
distinguishing this from other sources of hard x-rays (e.g., bremsstrahlung) is
a major challenge. Here we show that the photons from the X-ray flash are
highly polarised, as compared to the unpolarised background, i.e., polarisation
is an indicator that the SFQED plasma has really produced. For a laser of
intensity $10^{21}$ Wcm$^{-2}$ impinging on a solid Al target, the photons of
the flash with energy $>10$\thinspace keV are $>65\%$ polarised.

</details>


### [36] [Electron acoustic shock and solitary waves in spin polarized dense rotating quantum plasmas](https://arxiv.org/abs/2507.18228)
*Atherv Saxena,Punit Kumar*

Main category: physics.plasm-ph

TL;DR: Study of electrostatic waves in a quantum plasma with electron, positron, and ion components, considering spin, Fermi pressure, and quantum effects. Shock waves are analyzed, showing quantum effects enhance dispersion and stabilize shocks.


<details>
  <summary>Details</summary>
Motivation: To understand wave propagation in astrophysical quantum plasmas, incorporating spin, rotation, and quantum effects, which are crucial for modeling such environments.

Method: Derived coupled dispersion relations for electron, positron, and ion modes. Analyzed electron acoustic shock waves using the Korteweg de Vries Burgers method.

Result: Quantum effects enhance wave dispersion and modify shock profiles by broadening and stabilizing the shock structure.

Conclusion: Quantum effects significantly influence wave propagation and shock dynamics in astrophysical quantum plasmas, providing insights for modeling such systems.

Abstract: The propagation of electrostatic waves in a three-component electron positron
ion astrophysical quantum plasma under the influence of uniform rotation is
analysed, incorporating the effects of particle spin, Fermi pressure, and the
quantum Bohm potential. Spin polarisation arising due to the alignment of
particle spins under the influence of a strong external magnetic field leads to
an imbalance in the population of spin-up and spin-down states. Additionally,
key astrophysical factors such as rotation and gravitational influence have
been considered. The coupled dispersion relations for electron, positron, and
ion modes have been derived. Further, the electron acoustic shock wave is
studied using the Korteweg de Vries Burgers method, and the shock wave solution
has been obtained. Quantum effects are found to contribute to enhanced wave
dispersion and modify the shock profile by broadening and stabilising the shock
structure.

</details>


### [37] [Plasma Position Constrained Free-Boundary MHD Equilibrium in Tokamaks using pyIPREQ](https://arxiv.org/abs/2507.18324)
*Udaya Maurya,Amit K. Singh,Suman Aich,Jagabandhu Kumar,Rohit Kumar,Daniel Raju*

Main category: physics.plasm-ph

TL;DR: pyIPREQ is a new MHD equilibrium code for Tokamak plasmas, enhancing PEST and IPREQ with limiter boundaries, magnetic axis constraints, and vertical instability handling, validated against benchmarks and applied to ADITYA-U and SST-1 Tokamaks.


<details>
  <summary>Details</summary>
Motivation: To improve Tokamak plasma modeling by developing a versatile MHD equilibrium code with enhanced capabilities like limiter boundaries and magnetic axis constraints.

Method: Uses finite difference and Green's function approach, building on PEST and IPREQ, with added features for boundary specification and instability handling.

Result: Validated against benchmarks and applied to ADITYA-U and SST-1 Tokamaks, demonstrating accurate equilibrium predictions.

Conclusion: pyIPREQ is a robust tool for Tokamak plasma equilibrium studies, offering advanced features and reliable performance.

Abstract: A free-boundary, axisymmetric magnetohydrodynamic (MHD) equilibrium code,
pyIPREQ, has been developed for Tokamak plasmas using finite difference and
Green's function approach. The code builds upon the foundational frameworks of
the PEST and IPREQ codes, introducing several enhancements and new
capabilities. Notably, pyIPREQ supports the specification of limiter boundaries
and enables the computation of key physical quantities. The code has also been
extended to compute equilibria constrained by a prescribed magnetic axis
position, which is particularly useful when such information is available from
diagnostics like Sine-Cosine coils. In addition, pyIPREQ includes functionality
to address vertical instabilities, a requirement for accurately modeling
elongated plasma configurations. Benchmarking has been carried out against
published results and the original IPREQ code. Applications are demonstrated
for ADITYA-U Tokamak experiments, where magnetic axis measurements are
available, and predictions are also made for SST-1 and ADITYA-U Tokamaks under
various operational scenarios.

</details>


### [38] [Nonlocal current-driven heat flow in ideal plasmas](https://arxiv.org/abs/2507.18430)
*Nicholas Mitchell,David Chapman,Grigory Kagan*

Main category: physics.plasm-ph

TL;DR: The paper explores nonlocal effects on current-driven heat flux in plasmas, revealing significant enhancements due to large currents and high ionizations.


<details>
  <summary>Details</summary>
Motivation: To address the understudied nonlocal regimes of current-driven heat flow and friction in collisional plasmas, which are crucial for fusion and astrophysical contexts.

Method: A first-principles reduced kinetic method (RKM) is applied to study nonlocal effects on current-driven transport.

Result: Large currents enhance current-driven heat flux via a novel nonlocal mechanism, especially for higher effective ionizations (Z*). Effects are notable even for weak flows (Nu ≳ 1/100).

Conclusion: Nonlocal mechanisms significantly impact current-driven heat transport, with implications for fusion and astrophysical plasmas.

Abstract: Electron heat flux is an important and often dominant mechanism of energy
transport in a variety of collisional plasmas in a confined fusion or
astrophysical context. While nonlocal conductive heat transport, driven by
strong temperature gradients, has been investigated extensively in previous
literature, nonlocal regimes of the current-driven heat flow and friction have
not received the same attention. In this work, a first-principles reduced
kinetic method (RKM) is applied to study nonlocal effects on current-driven
transport. In addition to nonlocality due to sharp gradients, sufficiently
large currents are found to significantly enhance current-driven heat flux due
to a novel nonlocal mechanism, with this enhancement being increasingly
prevalent for higher effective ionizations $Z^*$. Introducing the dimensionless
number $N_u \equiv \vert \boldsymbol{u}_e - \boldsymbol{u}_i \vert /
v_{\text{th},e}$, these enhancements occur for even relatively weak flows $N_u
\gtrsim 1/100$, analogously to standard nonlocal effects becoming significant
for Knudsen numbers $N_K \gtrsim 1/100$.

</details>


<div id='math.CV'></div>

# math.CV [[Back]](#toc)

### [39] [Formally Integrable Structures III. Levi Flat Structures](https://arxiv.org/abs/2507.18341)
*Qingchun Ji,Jun Yao*

Main category: math.CV

TL;DR: The paper constructs differential complexes using formal integrability, resolves basic sections of vector bundles, and introduces convexity for Hermitian metrics to prove vanishing theorems and global solvability in Levi flat and elliptic structures.


<details>
  <summary>Details</summary>
Motivation: To provide resolutions for basic sections of vector bundles and realize the Morse-Novikov-Treves complex globally, while addressing convexity and solvability in specific geometric contexts.

Method: Utilizes formal integrability to construct differential complexes and introduces convexity for Hermitian metrics on basic line bundles.

Result: Establishes vanishing theorems, global solvability of the Morse-Novikov-Treves complex, and results for singular cohomology and canonical forms in elliptic structures.

Conclusion: The framework resolves key geometric problems and extends results to Levi flat and elliptic structures, demonstrating broad applicability.

Abstract: In this paper, we utilize formal integrability to construct a class of
differential complexes, thereby providing a resolution for the sheaf of basic
sections of a basic vector bundle, as well as a global realization of the
Morse-Novikov-Treves complex. In the context of Levi flat structures, we
introduce a notion of convexity for each Hermitian metric on a basic line
bundle, which enables us to establish vanishing theorems and the global
solvability of the Morse-Novikov-Treves complex. In the special case of
elliptic structures, we further obtain results concerning singular cohomology
and the extension problem for canonical forms.

</details>


### [40] [A new approach to the Monge-Ampère eigenvalue problem](https://arxiv.org/abs/2507.18409)
*Chinh H. Lu,Ahmed Zeriahi*

Main category: math.CV

TL;DR: The paper studies the eigenvalue problem for the complex Monge-Ampère operator in hyperconvex domains, proving uniqueness of eigenfunctions and providing a Rayleigh quotient formula. It introduces an iterative method for eigenvalues and eigenfunctions under continuity assumptions, using plurisubharmonic envelopes. The approach simplifies existing arguments and extends to complex Hessian operators and real Monge-Ampère via logarithmic transformation.


<details>
  <summary>Details</summary>
Motivation: To address the eigenvalue problem for the complex Monge-Ampère operator in bounded hyperconvex domains, focusing on non-pluripolar measures and finite energy classes.

Method: Relies on plurisubharmonic envelopes for partial sublinearization, introduces an iterative procedure for eigenvalues and eigenfunctions, and extends results to complex Hessian and real Monge-Ampère operators.

Result: Uniqueness of eigenfunctions (up to constants), Rayleigh quotient formula, and iterative solution under continuity assumptions.

Conclusion: The method is novel, simplifies existing arguments, and extends to related operators, providing new insights and results.

Abstract: We study the eigenvalue problem for the complex Monge-Amp\`ere operator in
bounded hyperconvex domains in $\C^n$, where the right-hand side is a
non-pluripolar positive Borel measure. We establish the uniqueness of
eigenfunctions in the finite energy class introduced by Cegrell, up to positive
multiplicative constants, and provide a Rayleigh quotient type formula for
computing the eigenvalue.
  Under a natural continuity assumption on the measure, we further show that
both the eigenvalue and eigenfunctions can be obtained via an iterative
procedure starting from any negative finite energy function.
  Our approach relies on the fine properties of plurisubharmonic envelopes,
which allow a partial sublinearization of the nonlinear problem. As far as we
know, this method is new, even in the linear case, and not only yields new
results but also significantly simplifies existing arguments in the literature.
Moreover, it extends naturally to the setting of complex Hessian operators.
  Finally, by translating our results from the complex Monge-Amp\`ere setting
via a logarithmic transformation, we also obtain several interesting analogues
for the real Monge-Amp\`ere operator.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [41] [Data assimilation with the 2D Navier-Stokes equations: Optimal Gaussian asymptotics for the posterior measure](https://arxiv.org/abs/2507.18279)
*Dimiri Konen,Richard Nickl*

Main category: math.ST

TL;DR: The paper proves a functional Bernstein-von Mises theorem for posterior measures in a data assimilation problem with the 2D Navier-Stokes equation, showing Gaussian approximation and strong consistency.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of posterior measures in data assimilation for the Navier-Stokes equation and their Gaussian approximation.

Method: Assigning a Gaussian process prior to the initial condition and analyzing the posterior measure via a linear parabolic PDE with Gaussian initial condition.

Result: The posterior measure is approximated by a Gaussian random vector field, with strong consistency in the supremum norm, enabling $1/\sqrt{N}$-consistent estimators.

Conclusion: The Bayesian data assimilation algorithm attains the local asymptotic minimax bound, validating its use for uncertainty quantification and credible bands.

Abstract: A functional Bernstein - von Mises theorem is proved for posterior measures
arising in a data assimilation problem with the two-dimensional Navier-Stokes
equation where a Gaussian process prior is assigned to the initial condition of
the system. The posterior measure, which provides the update in the space of
all trajectories arising from a discrete sample of the (deterministic)
dynamics, is shown to be approximated by a Gaussian random vector field arising
from the solution to a linear parabolic PDE with Gaussian initial condition.
The approximation holds in the strong sense of the supremum norm on the
regression functions, showing that predicting future states of Navier-Stokes
systems admits $1/\sqrt N$-consistent estimators even for commonly used
nonparametric models. Consequences for coverage of credible bands and
uncertainty quantification are discussed. A local asymptotic minimax theorem is
derived that describes the lower bound for estimating the state of the
nonlinear system, which is shown to be attained by the Bayesian data
assimilation algorithm.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [42] [Shallow quantum circuit for generating O(1)-entanged approximate state designs](https://arxiv.org/abs/2507.17871)
*Wonjun Lee,Minki Hhan,Gil Young Cho,Hyukjoon Kwon*

Main category: quant-ph

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Random quantum states have various applications in quantum information
science, including quantum cryptography, quantum simulation, and benchmarking
quantum devices. In this work, we discover a new ensemble of quantum states
that serve as an $\epsilon$-approximate state $t$-design while possessing
extremely low entanglement, magic, and coherence. We show that those resources
such quantum states can reach their theoretical lower bounds, $\Omega\left(\log
(t/\epsilon)\right)$, which are also proven in this work. This implies that for
fixed $t$ and $\epsilon$, those resources do not scale with the system size,
i.e., $O(1)$ with respect to the total number of qubits $n$ in the system.
Moreover, we explicitly construct an ancilla-free shallow quantum circuit for
generating such states. To this end, we develop an algorithm that transforms
$k$-qubit approximate state designs into $n$-qubit ones through a sequence of
multi-controlled gates, without increasing the support size. The depth of such
a quantum circuit is $O\left(t [\log t]^3 \log n \log(1/\epsilon)\right)$,
which is the most efficient among existing algorithms without ancilla qubits. A
class of shallow quantum circuits proposed in our work offers reduced cost for
classical simulation of random quantum states, leading to potential
applications in various quantum information processing tasks. As a concrete
example for demonstrating utility of our algorithm, we propose classical shadow
tomography using an $O(1)$-entangled estimator, which can achieve shorter
runtime compared to conventional schemes.

</details>


### [43] [Stability of Continuous Time Quantum Walks in Complex Networks](https://arxiv.org/abs/2507.17880)
*Adithya L J,Johannes Nokkala,Jyrki Piilo,Chandrakala Meena*

Main category: quant-ph

TL;DR: The paper examines how network topology and decoherence models affect the stability of continuous time quantum walks (CTQWs), finding that heterogeneous networks and intrinsic decoherence preserve quantum properties best.


<details>
  <summary>Details</summary>
Motivation: To understand how different network topologies and decoherence mechanisms influence the stability of quantum properties in CTQWs.

Method: Study CTQWs in various network topologies (cycle, complete, Erdős-Rényi, small-world, scale-free, star) under decoherence models (intrinsic, Haken-Strobl, QSWs). Metrics include node occupation probabilities, coherence norms, fidelity, quantum-classical distance, and von Neumann entropy.

Result: Heterogeneous networks (star, scale-free) are most stable; homogeneous networks (cycle, Erdős-Rényi) are vulnerable. Intrinsic decoherence preserves coherence best, QSWs the worst. Node centrality in heterogeneous networks impacts stability.

Conclusion: Network topology and decoherence model jointly determine quantum stability, with heterogeneous networks and intrinsic decoherence offering the best preservation of quantum properties.

Abstract: We investigate the stability of continuous time quantum walks (CTQWs) in a
range of network topologies under different decoherence mechanisms, defining
stability as the system's ability to preserve quantum properties over time. The
networks studied range from homogeneous to heterogeneous structures, including
cycle, complete, Erd\H{o}s-R\'enyi, small-world, scale-free, and star
topologies. The decoherence models considered are intrinsic decoherence,
Haken-Strobl noise, and quantum stochastic walks (QSWs). To assess quantum
stability, we employ several metrics: node occupation probabilities, the
$\ell_1$-norm of coherence, fidelity with the initial state, quantum-classical
distance, and von Neumann entropy. Our results reveal that the interplay of
both network topology and decoherence model influences coherence preservation.
Intrinsic decoherence results in the slowest decay of coherence, followed by
Haken-Strobl noise, while QSW causes the most rapid loss of coherence. The
stability ranking among network topologies varies depending on the decoherence
model and quantifier used. For example, under Haken-Strobl and intrinsic
decoherence, the quantum-classical distance ranks the cycle network more stable
than scale-free networks, although other metrics consistently favour scale-free
topologies. In general, heterogeneous networks, such as star and scale-free
networks, exhibit the highest stability, whereas homogeneous topologies, such
as cycle and Erd\H{o}s-R\'enyi networks, are more vulnerable to decoherence.
The complete graph, despite its homogeneity, remains highly stable due to its
dense connectivity. Furthermore, in heterogeneous networks, the centrality of
the initialised node, measured by degree or closeness, has a pronounced impact
on stability, underscoring the role of local topological features in quantum
dynamics.

</details>


### [44] [Advancing the hBN Defects Database through Photophysical Characterization of Bulk hBN](https://arxiv.org/abs/2507.18093)
*Chanaprom Cholsuk,Sujin Suwanna,Tobias Vogl*

Main category: quant-ph

TL;DR: A database of bulk hBN defects with photophysical properties is created to bridge theory-experiment gaps, aiding quantum emitter identification and machine learning in quantum materials.


<details>
  <summary>Details</summary>
Motivation: Address discrepancies between theoretical (monolayer) and experimental (bulk) studies of hBN defects by providing a comprehensive bulk defect database.

Method: Systematically evaluate over 120 neutral defects across charge states (-2 to 2), computing properties like zero-phonon line, photoluminescence, and electron-phonon coupling.

Result: Vacancies influence electron-phonon coupling strength, and the HR factor correlates with configuration coordinates. Data is publicly accessible via an API.

Conclusion: The database bridges theory-experiment gaps, supports quantum emitter identification, and enables machine learning applications in quantum materials.

Abstract: Quantum emitters in hexagonal boron nitride (hBN) have gained significant
attention due to a wide range of defects that offer high quantum efficiency and
single-photon purity at room temperature. Most theoretical studies on hBN
defects simulate monolayers, as this is computationally cheaper than
calculating bulk structures. However, most experimental studies are carried out
on multilayer to bulk hBN, which creates additional possibilities for
discrepancies between theory and experiment. In this work, we present an
extended database of hBN defects that includes a comprehensive set of bulk hBN
defects along with their excited-state photophysical properties. The database
features over 120 neutral defects, systematically evaluated across charge
states ranging from -2 to 2 (600 defects in total). For each defect, the most
stable charge and spin configurations are identified and used to compute the
zero-phonon line, photoluminescence spectrum, absorption spectrum, Huang-Rhys
(HR) factor, interactive radiative lifetimes, transition dipole moments, and
polarization characteristics. Our analysis reveals that the electron-phonon
coupling strength is primarily influenced by the presence of vacancies, which
tend to induce stronger lattice distortions and broaden phonon sidebands.
Additionally, correlation analysis shows that while most properties are
independent, the HR factor strongly correlates with the configuration
coordinates. All data are publicly available at https://h-bn.info, along with a
new application programming interface (API) to facilitate integration with
machine learning workflows. This database is therefore designed to bridge the
gap between theory and experiment, aid in the reliable identification of
quantum emitters, and support the development of machine-learning-driven
approaches in quantum materials research.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [45] [Stability of Big Bang singularity for the Einstein-Maxwell-scalar field-Vlasov system in the full strong sub-critical regime](https://arxiv.org/abs/2507.18585)
*Xinliang An,Taoran He,Dawei Shen*

Main category: gr-qc

TL;DR: Study of Kasner solutions' stability in the Einstein-Maxwell-scalar field-Vlasov system, identifying a new sub-critical regime and proving nonlinear stability.


<details>
  <summary>Details</summary>
Motivation: To extend stability results from simpler systems to the more complex Einstein-Maxwell-scalar field-Vlasov system, addressing challenges posed by the Vlasov field.

Method: Detailed mathematical analysis and new arguments to identify a strong sub-critical regime.

Result: Nonlinear stability proven for Kasner exponents in the identified regime, extending prior work.

Conclusion: The study successfully generalizes stability results to a more physically complex system.

Abstract: In $3+1$ dimensions, we study the stability of Kasner solutions for the
Einstein-Maxwell-scalar field-Vlasov system. This system incorporates gravity,
electromagnetic, weak and strong interactions for the initial stage of our
universe. Due to the presence of the Vlasov field, various new challenges
arise. By observing detailed mathematical structures and designing new delicate
arguments, we identify a new strong sub-critical regime and prove the nonlinear
stability with Kasner exponents lying in this full regime. This extends the
result of Fournodavlos-Rodnianski-Speck [8] from the Einstein-scalar field
system to the physically more complex system with the Vlasov field.

</details>


<div id='astro-ph.GA'></div>

# astro-ph.GA [[Back]](#toc)

### [46] [Numerical Study of Bar Suppression in Galaxy Models Due to Disc Heating](https://arxiv.org/abs/2507.18083)
*Alejandro López Gómez,Ruslan Gabbasov,Isaura Luisa Fuentes-Carrera*

Main category: astro-ph.GA

TL;DR: The study explores how the softening parameter (ε) and disc mass fraction (m_d) influence bar formation in galaxies via N-body simulations, revealing complex interactions and numerical heating effects.


<details>
  <summary>Details</summary>
Motivation: Understanding the controversial dynamics of bar formation, evolution, and destruction in galaxies, particularly how numerical and physical parameters like ε and m_d affect these processes.

Method: Conducted N-body simulations with varying particle resolutions, analyzing the impact of ε and m_d on bar strength, formation delay, and numerical heating.

Result: Found that ε and m_d interact to either enhance or weaken bars, with small ε values causing numerical heating and bar suppression. High m_d reduces drift in acceleration profiles but doesn't always delay bar formation.

Conclusion: The vertical acceleration profile of the disc reliably indicates numerical heating effects, and careful parameter selection is crucial for accurate simulations.

Abstract: The process of bar formation, evolution and destruction is still a
controversial topic regarding galaxy dynamics. Numerical simulations show that
these phenomena strongly depend on physical and numerical parameters. In this
work, we study the combined influence of the softening parameter, $\epsilon$
and disc mass fraction, $m_{\mathrm{d}}$ on the formation and evolution of bars
in isolated disc-halo models via $N$-body simulations with different particle
resolutions. Previous studies indicate that the bar strength depends on
$m_{\mathrm{d}}$ as $\propto m_{\mathrm{d}}^{-1}$, which is seen as a delay in
bar formation. However, the distorsion parameter, $\eta$, which measures the
bar's momentum through time, shows that an increase in $m_{\mathrm{d}}$ does
not always induce a delay in bar formation. This suggests that $\epsilon$
interact to either enhance or weaken the bar. Moreover, numerical heating
dominates in models with small softening values, creating highly accelerated
particles at the centre of discs, regardless of $m_{\mathrm{d}}$ or resolution.
These enhanced particle accelerations produce chaotic orbits for $\epsilon \leq
5\,$pc, resulting in bar suppression due to collisional dynamics in the centre.
In our high resolution models ($N \approx 10^{7}$), small softening values are
incapable of reproducing the bar instability. The role of disc mass is as
follows: increasing $m_{\mathrm{d}}$ for moderate $\epsilon$ ($\geq 10\,$pc)
reduces the amount of drift in the acceleration profile, without affecting the
bar's behaviour. Models with lower $m_{\mathrm{d}}$ values coupled with small
softening values, have an excess of highly accelerated particles, introducing
unwanted effects into otherwise reliable simulations. Finally, we show that the
evolution of the disc's vertical acceleration profile is a reliable indicator
of numerical heating introduced by $\epsilon$ and the bar.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [47] [Deep Variational Free Energy Calculation of Hydrogen Hugoniot](https://arxiv.org/abs/2507.18540)
*Zihang Li,Hao Xie,Xinyang Dong,Lei Wang*

Main category: cond-mat.str-el

TL;DR: A deep variational free energy framework using three neural networks computes hydrogen's equation of state in warm dense matter, resolving discrepancies in deuterium Hugoniot curve data.


<details>
  <summary>Details</summary>
Motivation: To address discrepancies in theoretical and experimental results for hydrogen's equation of state in the warm dense matter region, particularly focusing on the deuterium Hugoniot curve.

Method: Uses three deep generative models: a normalizing flow for nuclei, an autoregressive transformer for excited electrons, and a permutational equivariant flow for Hartree-Fock electrons. Joint optimization minimizes variational free energy.

Result: Provides accurate thermodynamic properties and equation of state for dense hydrogen, serving as a benchmark for deuterium in warm dense matter.

Conclusion: The framework successfully resolves discrepancies and offers reliable data for hydrogen in warm dense matter, validated against existing theoretical and experimental results.

Abstract: We develop a deep variational free energy framework to compute the equation
of state of hydrogen in the warm dense matter region. This method parameterizes
the variational density matrix of hydrogen nuclei and electrons at finite
temperature using three deep generative models: a normalizing flow model that
represents the Boltzmann distribution of the classical nuclei, an
autoregressive transformer that models the distribution of electrons in excited
states, and a permutational equivariant flow model that constructs backflow
coordinates for electrons in Hartree-Fock orbitals. By jointly optimizing the
three neural networks to minimize the variational free energy, we obtain the
equation of state and related thermodynamic properties of dense hydrogen. We
compare our results with other theoretical and experimental results on the
deuterium Hugoniot curve, aiming to resolve existing discrepancies. The
calculated results provide a valuable benchmark for deuterium in the warm dense
matter region.

</details>


<div id='physics.acc-ph'></div>

# physics.acc-ph [[Back]](#toc)

### [48] [A Supervised Machine Learning Framework for Multipactor Breakdown Prediction in High-Power Radio Frequency Devices and Accelerator Components: A Case Study in Planar Geometry](https://arxiv.org/abs/2507.17881)
*Asif Iqbal,John Verboncoeur,Peng Zhang*

Main category: physics.acc-ph

TL;DR: The study applies supervised machine learning to predict multipactor susceptibility in RF devices, using tree-based models and MLPs, showing promise but highlighting dataset limitations.


<details>
  <summary>Details</summary>
Motivation: Accurate prediction of multipactor susceptibility is critical for RF and accelerator systems but computationally intensive, motivating the use of ML for faster, data-driven solutions.

Method: Simulation-derived data trained regression models (RF, ET, XGBoost, MLPs) to predict electron growth rate, evaluated with IoU, SSIM, and Pearson correlation.

Result: Tree-based models outperformed MLPs; MLPs with combined objective functions during hyperparameter optimization showed better performance. PCA revealed dataset limitations.

Conclusion: ML shows promise for multipactor prediction but requires broader dataset coverage for generalization, paving the way for accelerated RF system design.

Abstract: Multipactor is a nonlinear electron avalanche phenomenon that can severely
impair the performance of high-power radio frequency (RF) devices and
accelerator systems. Accurate prediction of multipactor susceptibility across
different materials and operational regimes remains a critical yet
computationally intensive challenge in accelerator component design and RF
engineering. This study presents the first application of supervised machine
learning (ML) for predicting multipactor susceptibility in two-surface planar
geometries. A simulation-derived dataset spanning six distinct secondary
electron yield (SEY) material profiles is used to train regression models -
including Random Forest (RF), Extra Trees (ET), Extreme Gradient Boosting
(XGBoost), and funnel-structured Multilayer Perceptrons (MLPs) - to predict the
time-averaged electron growth rate, ${\delta}_{avg}$. Performance is evaluated
using Intersection over Union (IoU), Structural Similarity Index (SSIM), and
Pearson correlation coefficient. Tree-based models consistently outperform MLPs
in generalizing across disjoint material domains. MLPs trained using a
scalarized objective function that combines IoU and SSIM during Bayesian
hyperparameter optimization with 5-fold cross-validation outperform those
trained with single-objective loss functions. Principal Component Analysis
reveals that performance degradation for certain materials stems from disjoint
feature-space distributions, underscoring the need for broader dataset
coverage. This study demonstrates both the promise and limitations of ML-based
multipactor prediction and lays the groundwork for accelerated, data-driven
modeling in advanced RF and accelerator system design.

</details>


### [49] [Advanced Ceramic Plasma Discharge Capillaries for high repetition rate operation](https://arxiv.org/abs/2507.18226)
*Lucio Crincoli,Romain Demitra,Valerio Lollo,Donato Pellegrini,Marco Pitti,Lucilla Pronti,Martina Romani,Massimo Ferrario,Angelo Biagioni*

Main category: physics.acc-ph

TL;DR: The paper presents an innovative ceramic-based capillary design for high-repetition-rate plasma discharges, validated experimentally and numerically for longevity and performance.


<details>
  <summary>Details</summary>
Motivation: To address the need for durable plasma sources in high-repetition-rate applications like particle accelerators and light sources, focusing on material longevity under high-voltage discharges.

Method: Experimental testing at 10-150 Hz and numerical simulations to analyze heat transfer and longevity of ceramic capillaries.

Result: Ceramic capillaries maintain plasma properties and structural integrity at high repetition rates, suitable for 100-400 Hz operations.

Conclusion: The proposed ceramic capillary design is viable for high-repetition-rate plasma applications, aligning with projects like EuPRAXIA@SPARC_LAB.

Abstract: In view of future applications of plasma-based particle accelerators, within
the fields of high-energy physics and new light sources, the capability of
plasma sources to operate at high repetition rates is crucial. In particular
for gas-filled plasma discharge capillaries, which allow direct control over
plasma properties, a key aspect is the longevity of the material, subject to
erosion due to the heat flux delivered by high voltage plasma discharges. In
this regard, we present an innovative design of discharge capillaries based on
the use of different ceramic materials, which can sustain high voltage plasma
discharges at high repetition rate and, moreover, be easily machined for the
complex geometries required for plasma-based accelerators. Experimental
campaigns are carried out at 10-150 Hz, assessing the longevity of ceramic
capillaries by means of different diagnostic techniques. In addition, numerical
simulations are performed to analyze the heat transfer within the whole plasma
source. Results from experimental and numerical analysis highlight the
capability of ceramic capillaries to preserve plasma properties and the
integrity of the source during long-term plasma discharge operation at high
repetition rate. In particular, we demonstrated the suitability of the proposed
solution for the operative range of 100-400 Hz, foreseen for EuPRAXIA@SPARC_LAB
project.

</details>


<div id='nlin.CD'></div>

# nlin.CD [[Back]](#toc)

### [50] [Toda lattice formed in nonequilibrium steady states of SWCNT](https://arxiv.org/abs/2507.18412)
*Heeyuen Koh,Shigeo Maruyama*

Main category: nlin.CD

TL;DR: The paper connects thermal conductivity in nanoscale low-dimensional systems to Toda lattice dynamics using a coarse-grained molecular dynamics model derived from nonequilibrium simulations.


<details>
  <summary>Details</summary>
Motivation: To explain the length dependency of high thermal conductivity in low-dimensional systems by linking nonequilibrium steady states (NESS) to Toda lattice equilibrium dynamics.

Method: Uses a coarse-grained molecular dynamics (CGMD) model from nonequilibrium molecular dynamics (NEMD) data, incorporating longitudinal and flexural modulation as a Hamiltonian with an overdamping perturbation term.

Result: The derived Smoluchowski equation shows the potential energy function in NESS matches the Toda lattice under specific conditions, validated by numerical data.

Conclusion: The model successfully links NESS thermal conductivity to Toda lattice dynamics, with numerical confirmation of the derived restrictions.

Abstract: Toda lattice or FPUT chain-like dynamics have been regarded as the
prerequisite condition to explain the length dependency of high thermal
conductivity of low-dimensional systems at the nanoscale. In this paper, a
hypothetical condition is introduced that establishes a theoretical connection
between the thermal conductivity of a nanoscale low-dimensional system in
nonequilibrium steady states(NESS) and the canonical motion of the equation in
the Toda lattice in equilibrium. The hypothesis relies on a numerically driven
coarse grained molecular dynamics system acquired from the trajectory data of
nonequilibrium molecular dynamics(NEMD) simulation. It models the macroscopic
motion from longitudinal and flexural modulation observed in NEMD as a separate
Hamiltonian in CGMD with a perturbation term governed by an overdamping
process, which is assumed to be dominant during heat transfer. The Smoluchowski
equation for the perturbation, which is derived from the cross correlated
states between two degrees of freedom, suggests that the potential energy
function induced from NESS is identical to that of Toda Lattice under the
specific condition in the partition function for CG particles. The restrictions
derived from the model are well confirmed by the data from the numerically
driven CG model.

</details>


<div id='math.CO'></div>

# math.CO [[Back]](#toc)

### [51] [Guessing sequences of eigenvectors for LMPs defining spectrahedral relaxations of Eulerian rigidly convex sets](https://arxiv.org/abs/2507.18434)
*Alejandro González Nevado*

Main category: math.CO

TL;DR: The paper improves bounds for extreme roots of univariate Eulerian polynomials, enhancing the accuracy of spectrahedral relaxations for Eulerian rigidly convex sets. Numerical experiments yield a growing exponential difference from previous bounds.


<details>
  <summary>Details</summary>
Motivation: To enhance the accuracy of spectrahedral relaxations for Eulerian rigidly convex sets by improving bounds for extreme roots of univariate Eulerian polynomials.

Method: Numerical experiments are used to construct a sequence of vectors, linearizing bounds and comparing them to previous results.

Result: A new bound is established, with a growing exponential difference from prior bounds, improving the diagonal measure of accuracy for spectrahedral relaxations.

Conclusion: The proposed method provides a better measure of accuracy for spectrahedral relaxations, outperforming previous bounds as the dimension grows.

Abstract: Stable multivariate Eulerian polynomials were introduced by Br\"and\'en.
Particularizing some variables, it is possible to extract real zero
multivariate Eulerian polynomials from them. These real zero multivariate
Eulerian polynomials can be fed into constructions of spectrahedral relaxations
providing therefore approximations to the (Eulerian) rigidly convex sets
defined by these polynomials. The accuracy of these approximations is measured
through the behaviour in the diagonal, where the usual univariate Eulerian
polynomials sit. In particular, in this sense, the accuracy of the global
spectrahedral approximation produced by the spectrahedral relaxation can be
measured in terms of bounds for the extreme roots of univariate Eulerian
polynomials. The bounds thus obtained beat the previous bounds found in the
literature. However, the bound explicitly studied and obtained before beat the
previously known bounds by a quantity going to $0$ when $n$ goes to infinity.
Here we use numerical experiments to construct a sequence of vectors providing
a (linearized) bound whose difference with the previous known bounds is a
growing exponential function (going therefore fast to infinity when $n$ grows).
This allows us to establish a better (diagonal) measure of accuracy for the
spectrahedral relaxation of the Eulerian rigidly convex sets. In particular, we
will achieve this by linearizing through the sequence of vectors
$\{(y,(-2^{m-i})_{i=3}^{m},(0,\frac{1}{2}),(1)_{i=1}^{m})\in\mathbb{R}^{n+1}\}_{n=1}^{\infty}$
for even $n=2m$.

</details>


<div id='hep-th'></div>

# hep-th [[Back]](#toc)

### [52] [Analytic Regression of Feynman Integrals from High-Precision Numerical Sampling](https://arxiv.org/abs/2507.17815)
*Oscar Barrera,Aurélien Dersy,Rabia Husain,Matthew D. Schwartz,Xiaoyuan Zhang*

Main category: hep-th

TL;DR: The paper presents a method for deriving exact analytic solutions from high-precision numerical data, focusing on Feynman integrals, by leveraging known function spaces and lattice reduction.


<details>
  <summary>Details</summary>
Motivation: Exact analytic solutions are often desired in mathematics and physics, but numerical methods typically yield approximations. This work addresses the gap by combining high-precision data with analytic insights.

Method: The approach combines high-precision numerical integration with analytic knowledge of the function space, using lattice reduction to deduce exact answers. Trade-offs between data points, precision, and compute are explored.

Result: The method successfully derives exact solutions for Feynman integrals and demonstrates broader applicability to problems with well-understood function spaces.

Conclusion: This bottom-up approach complements top-down methods, offering a practical solution for exact analytic descriptions when function spaces are known.

Abstract: In mathematics or theoretical physics one is often interested in obtaining an
exact analytic description of some data which can be produced, in principle, to
arbitrary accuracy. For example, one might like to know the exact analytical
form of a definite integral. Such problems are not well-suited to numerical
symbolic regression, since typical numerical methods lead only to
approximations. However, if one has some sense of the function space in which
the analytic result should lie, it is possible to deduce the exact answer by
judiciously sampling the data at a sufficient number of points with sufficient
precision. We demonstrate how this can be done for the computation of Feynman
integrals. We show that by combining high-precision numerical integration with
analytic knowledge of the function space one can often deduce the exact answer
using lattice reduction. A number of examples are given as well as an
exploration of the trade-offs between number of datapoints, number of
functional predicates, precision of the data, and compute. This method provides
a bottom-up approach that neatly complements the top-down Landau-bootstrap
approach of trying to constrain the exact answer using the analytic structure
alone. Although we focus on the application to Feynman integrals, the
techniques presented here are more general and could apply to a wide range of
problems where an exact answer is needed and the function space is sufficiently
well understood.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [53] [Fourier Neural Operators for Non-Markovian Processes:Approximation Theorems and Experiments](https://arxiv.org/abs/2507.17887)
*Wonjae Lee,Taeyoung Kim,Hyungbin Park*

Main category: cs.LG

TL;DR: The paper introduces MFNO, a neural operator for stochastic systems, extending FNO with mirror padding to handle non-periodic inputs. It proves MFNO's approximation capabilities and shows superior performance over baselines.


<details>
  <summary>Details</summary>
Motivation: To address limitations of standard architectures in learning stochastic dynamics, particularly for non-periodic inputs.

Method: Extends FNO with mirror padding, enabling non-periodic input handling. Uses Wong-Zakai theorems and approximation techniques.

Result: MFNO approximates stochastic solutions accurately, generalizes well, and outperforms baselines like LSTMs and DeepONet.

Conclusion: MFNO is effective for stochastic systems, offering theoretical guarantees and practical advantages over existing methods.

Abstract: This paper introduces an operator-based neural network, the mirror-padded
Fourier neural operator (MFNO), designed to learn the dynamics of stochastic
systems. MFNO extends the standard Fourier neural operator (FNO) by
incorporating mirror padding, enabling it to handle non-periodic inputs. We
rigorously prove that MFNOs can approximate solutions of path-dependent
stochastic differential equations and Lipschitz transformations of fractional
Brownian motions to an arbitrary degree of accuracy. Our theoretical analysis
builds on Wong--Zakai type theorems and various approximation techniques.
Empirically, the MFNO exhibits strong resolution generalization--a property
rarely seen in standard architectures such as LSTMs, TCNs, and DeepONet.
Furthermore, our model achieves performance that is comparable or superior to
these baselines while offering significantly faster sample path generation than
classical numerical schemes.

</details>


### [54] [Low-rank adaptive physics-informed HyperDeepONets for solving differential equations](https://arxiv.org/abs/2507.18346)
*Etienne Zeudong,Elsa Cardoso-Bihlo,Alex Bihlo*

Main category: cs.LG

TL;DR: PI-LoRA-HyperDeepONets reduce complexity and improve performance in operator learning by using low-rank adaptation (LoRA) to decompose hypernetwork output weights.


<details>
  <summary>Details</summary>
Motivation: Address the high memory and computational costs of HyperDeepONets while maintaining or improving expressivity.

Method: Introduce PI-LoRA-HyperDeepONets, leveraging LoRA to decompose hypernetwork output weights into low-rank matrices, reducing parameters and adding regularization.

Result: Achieves up to 70% parameter reduction and consistently outperforms regular HyperDeepONets in predictive accuracy and generalization.

Conclusion: PI-LoRA-HyperDeepONets offer a more efficient and effective alternative for operator learning in physics-informed settings.

Abstract: HyperDeepONets were introduced in Lee, Cho and Hwang [ICLR, 2023] as an
alternative architecture for operator learning, in which a hypernetwork
generates the weights for the trunk net of a DeepONet. While this improves
expressivity, it incurs high memory and computational costs due to the large
number of output parameters required. In this work we introduce, in the
physics-informed machine learning setting, a variation, PI-LoRA-HyperDeepONets,
which leverage low-rank adaptation (LoRA) to reduce complexity by decomposing
the hypernetwork's output layer weight matrix into two smaller low-rank
matrices. This reduces the number of trainable parameters while introducing an
extra regularization of the trunk networks' weights. Through extensive
experiments on both ordinary and partial differential equations we show that
PI-LoRA-HyperDeepONets achieve up to 70\% reduction in parameters and
consistently outperform regular HyperDeepONets in terms of predictive accuracy
and generalization.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [55] [Jacobi Hamiltonian Integrators](https://arxiv.org/abs/2507.18573)
*Adérito Araújo,Gonçalo Inocêncio Oliveira,João Nuno Mestre*

Main category: math.DG

TL;DR: A method for constructing structure-preserving integrators for Hamiltonian systems in Jacobi manifolds is developed, extending techniques from Poisson systems to include time-dependent and dissipative phenomena.


<details>
  <summary>Details</summary>
Motivation: To extend Hamiltonian mechanics to Jacobi manifolds, which generalize contact and Poisson manifolds, for modeling time-dependent, dissipative, and thermodynamic systems.

Method: Leverages the correspondence between Jacobi and homogeneous Poisson manifolds, building on Poisson Hamiltonian Integrators (PHI) to create Jacobi Hamiltonian Integrators while preserving homogeneity structure.

Result: Theoretical tools for generalization are developed, and a numerical integration technique compatible with Jacobi dynamics is outlined.

Conclusion: The approach provides a clear pathway for structure-preserving integration of time-dependent and dissipative systems within the Jacobi framework.

Abstract: We develop a method of constructing structure-preserving integrators for
Hamiltonian systems in Jacobi manifolds. Hamiltonian mechanics, rooted in
symplectic and Poisson geometry, has long provided a foundation for modelling
conservative systems in classical physics. Jacobi manifolds, generalizing both
contact and Poisson manifolds, extend this theory and are suitable for
incorporating time-dependent, dissipative and thermodynamic phenomena.
  Building on recent advances in geometric integrators - specifically Poisson
Hamiltonian Integrators (PHI), which preserve key features of Poisson systems -
we propose a construction of Jacobi Hamiltonian Integrators. Our approach
explores the correspondence between Jacobi and homogeneous Poisson manifolds,
with the aim of extending the PHI techniques while ensuring preservation of the
homogeneity structure.
  This work develops the theoretical tools required for this generalization and
outlines a numerical integration technique compatible with Jacobi dynamics. By
focusing on the homogeneous Poisson perspective rather than on direct contact
realizations, we provide a clear pathway for structure-preserving integration
of time-dependent and dissipative systems within the Jacobi framework.

</details>


### [56] [Constant mean curvature Radial graphs over domains of $\mathbb{S}^n$](https://arxiv.org/abs/2507.18496)
*Flávio Cruz,José T. Cruz,Jocel Oliveira*

Main category: math.DG

TL;DR: Existence of hypersurfaces with constant mean curvature and prescribed boundaries in Euclidean space, extending Serrin's result to positive mean curvature.


<details>
  <summary>Details</summary>
Motivation: To generalize Serrin's classical result for constant mean curvature hypersurfaces by including cases with positive mean curvature and prescribed boundaries.

Method: Representing hypersurfaces as radial graphs over domains of the unit sphere, assuming positive mean curvature of the boundary and existence of a subsolution for the Dirichlet problem.

Result: Demonstrated existence of such hypersurfaces under the given conditions.

Conclusion: Successfully extended Serrin's result to include positive constant mean curvature cases, providing a broader framework for hypersurface analysis.

Abstract: We establish the existence of hypersurfaces with constant mean curvature and
a prescribed boundary in Euclidean space, represented as radial graphs over
domains of the unit sphere. Under the assumptions that the mean curvature of
the domain's boundary is positive and that a subsolution exists for the
associated Dirichlet problem, we extend Serrin's classical result to include
the case of positive constant mean curvature.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [57] [On zero-order consistency residue and background pressure for the conservative SPH fluid dynamics](https://arxiv.org/abs/2507.18210)
*Feng Wang,Xiangyu Hu*

Main category: physics.flu-dyn

TL;DR: The paper addresses the zero-order consistency issue in SPH, linking it to non-physical damping in pressure-driven and gravity-driven flows. It identifies the root cause as zero-order gradient consistency residue and explores mitigation strategies, including reverse kernel gradient correction.


<details>
  <summary>Details</summary>
Motivation: To understand and mitigate the non-physical numerical damping in SPH simulations caused by zero-order gradient consistency residue, which affects both pressure-driven and gravity-driven flows.

Method: Theoretical analysis and numerical experiments are conducted to study the residue's behavior. Key factors like water depth, dynamic pressure, channel length, resolution, and outlet pressure are tested. The reverse kernel gradient correction technique is introduced and evaluated.

Result: The residue is identified as the root cause of damping. The reverse kernel gradient correction helps but has limitations. The FDA nozzle test confirms the residue's impact in complex geometries.

Conclusion: Correction schemes are necessary for scenarios with high background pressure, as the residue's adverse effects are unavoidable in certain conditions.

Abstract: As one of the major challenges for the conservative smoothed particle
hydrodynamics (SPH) method, the zero-order consistency issue, although thought
to be mitigated by the particle regularization scheme, such as the transport
velocity formulation, significantly damps the flow in a long channel for both
laminar and turbulent simulations. Building on this finding, this paper not
only thoroughly analyzes the damping reason in this pressure-driven channel
flow, but also relates this problem with the excessive numerical dissipation in
the gravity-driven free-surface flow. The common root cause of the non-physical
numerical damping in the two typical flow scenarios, the zero-order gradient
consistency residue, is exposed. The adverse influence of the background
pressure on the residue for the two scenarios is revealed and discussed. To
comprehensively understand the behavior of the residue and mitigate its
potential adverse effects, we conduct both theoretical analysis and numerical
experiments focusing on the key sensitive factors. For studying the
residue-induced non-physical energy dissipation in the gravity-driven
free-surface flow, the water depth and input dynamic pressure in the inviscid
standing wave case are tested. To investigate the velocity loss in the
pressure-driven channel flow, we examine the effects of the channel length,
resolution, and outlet pressure. The state-of-the-art reverse kernel gradient
correction technique is introduced for the two typical flows, and proved to be
effective in reducing the residue effect, but we find its correction capability
is fundamentally limited. Finally, the FDA nozzle, an engineering benchmark, is
tested to demonstrate the residue influence in a complex geometry, highlighting
the necessity of correction schemes in scenarios with unavoidable high
background pressure.

</details>
