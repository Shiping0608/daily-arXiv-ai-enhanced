<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 8]
- [math.AP](#math.AP) [Total: 20]
- [physics.comp-ph](#physics.comp-ph) [Total: 6]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 4]
- [math.CV](#math.CV) [Total: 2]
- [physics.acc-ph](#physics.acc-ph) [Total: 2]
- [cs.LG](#cs.LG) [Total: 2]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [astro-ph.GA](#astro-ph.GA) [Total: 1]
- [nlin.CD](#nlin.CD) [Total: 1]
- [gr-qc](#gr-qc) [Total: 1]
- [hep-th](#hep-th) [Total: 1]
- [math.CO](#math.CO) [Total: 1]
- [math.DG](#math.DG) [Total: 2]
- [quant-ph](#quant-ph) [Total: 3]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 1]
- [math.ST](#math.ST) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Recovery Thresholding Hyperinterpolations in Signal Processing](https://arxiv.org/abs/2507.17916)
*Congpei An,Jiashu Ran*

Main category: math.NA

TL;DR: A novel method, recovery thresholding hyperinterpolations, is introduced for sparse signal reconstruction under noise, integrating thresholding operators into hyperinterpolation for sparsity preservation.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of sparse signal reconstruction in noisy environments by maintaining sparsity during recovery.

Method: Integrates thresholding operators (hard, springback, Newton) into hyperinterpolation, using Newton's method for nonconvex minimization, extended to multivariable problems.

Result: Demonstrates robust performance in reconstructing signals corrupted by Gaussian and impulse noise, outperforming traditional methods in sparsity preservation and accuracy.

Conclusion: The proposed methods are effective for signal reconstruction and denoising, offering improved sparsity and recovery accuracy.

Abstract: This paper introduces recovery thresholding hyperinterpolations, a novel
class of methods for sparse signal reconstruction in the presence of noise. We
develop a framework that integrates thresholding operators--including hard
thresholding, springback, and Newton thresholding--directly into the
hyperinterpolation structure to maintain sparsity during signal recovery. Our
approach leverages Newton's method to minimize one-dimensional nonconvex
functions, which we then extend to solve multivariable nonconvex regularization
problems. The proposed methods demonstrate robust performance in reconstructing
signals corrupted by both Gaussian and impulse noise. Through numerical
experiments, we validate the effectiveness of these recovery thresholding
hyperinterpolations for signal reconstruction and function denoising
applications, showing their advantages over traditional approaches in
preserving signal sparsity while achieving accurate recovery.

</details>


### [2] [A novel finite element method for simulating surface plasmon polaritons on complex graphene sheets](https://arxiv.org/abs/2507.17928)
*Jichun Li,Michael Neunteufel,Li Zhu*

Main category: math.NA

TL;DR: A simplified graphene model and new finite element method are developed to accurately simulate surface plasmon polaritons (SPPs) on graphene surfaces, addressing challenges like complex interfaces and boundary conditions.


<details>
  <summary>Details</summary>
Motivation: SPPs offer insights into nano-optical and electrodynamic responses, but accurate simulation is challenging due to complex material interfaces and boundary conditions.

Method: A simplified graphene model and a new finite element method are proposed, with stability analysis for the continuous model.

Result: Numerical results show the model effectively captures SPPs for various complex graphene sheets.

Conclusion: The proposed model and method provide an accurate and stable solution for simulating SPPs on graphene.

Abstract: Surface plasmon polaritons (SPPs) are generated on the graphene surface, and
provide a window into the nano-optical and electrodynamic response of their
host material and its dielectric environment. An accurate simulation of SPPs
presents several unique challenges, since SPPs often occur at complex
interfaces between materials of different dielectric constants and appropriate
boundary conditions at the graphene interfaces are crucial. Here we develop a
simplified graphene model and propose a new finite element method accordingly.
Stability for the continuous model is established, and extensive numerical
results are presented to demonstrate that the new model can capture the SPPs
very well for various complex graphene sheets.

</details>


### [3] [A stabilized Two-Step Formulation of Maxwell's Equations in the time-domain](https://arxiv.org/abs/2507.18235)
*Leon Herles,Mario Mally,Jörg Ostrowski,Sebastian Schöps,Melina Merkel*

Main category: math.NA

TL;DR: The paper presents a stabilized two-step time-domain method for simulating electromagnetic fields, addressing low-frequency instabilities with a generalized tree-cotree gauge.


<details>
  <summary>Details</summary>
Motivation: Numerical instabilities at low frequencies make simulating broad-frequency electromagnetic fields challenging.

Method: Extends a two-step Maxwell's equations formulation to time-domain, using Galerkin spatial discretization and tailored time-discretization schemes. Incorporates a generalized tree-cotree gauge for stability.

Result: Demonstrates stability and accuracy in 3D problems, including nonlinear, temperature-dependent materials.

Conclusion: The method effectively addresses low-frequency instabilities and is applicable to complex material behaviors.

Abstract: Simulating electromagnetic fields across broad frequency ranges is
challenging due to numerical instabilities at low frequencies. This work
extends a stabilized two-step formulation of Maxwell's equations to the
time-domain. Using a Galerkin discretization in space, we apply two different
time-discretization schemes that are tailored to the first- and second-order in
time partial differential equations of the two-step solution procedure used
here. To address the low-frequency instability, we incorporate a generalized
tree-cotree gauge that removes the singularity of the curl-curl operator,
ensuring robustness even in the static limit. Numerical results on academic and
application-oriented 3D problems confirm stability, accuracy, and the method's
applicability to nonlinear, temperature-dependent materials.

</details>


### [4] [EigenWave: An Optimal O(N) Method for Computing Eigenvalues and Eigenvectors by Time-Filtering the Wave Equation](https://arxiv.org/abs/2507.18282)
*Daniel Appelo,Jeffrey W. Banks,William D. Henshaw,Ngan Le,Donald W. Schwendeman*

Main category: math.NA

TL;DR: EigenWave is an algorithm for computing eigenvalues and eigenvectors of elliptic boundary value problems using a WaveHoltz-based iteration, targeting specific frequencies efficiently.


<details>
  <summary>Details</summary>
Motivation: To compute eigenvalues anywhere in the spectrum without inverting indefinite matrices, enabling efficient and scalable solutions.

Method: Uses WaveHoltz scheme to solve a time-dependent wave equation, filtering solutions iteratively to target specific eigenmodes. Can be embedded in a matrix-free Arnoldi algorithm for multiple eigenpairs.

Result: Achieves linear scaling (O(N)) with grid points, demonstrated on Laplacian eigenpairs in complex geometries using overset grids.

Conclusion: EigenWave provides an efficient, scalable, and flexible method for eigenvalue computation, validated in 2D and 3D with high-order accuracy.

Abstract: An algorithm named EigenWave is described to compute eigenvalues and
eigenvectors of elliptic boundary value problems. The algorithm, based on the
recently developed WaveHoltz scheme, solves a related time-dependent wave
equation as part of an iteration. At each iteration, the solution to the wave
equation is filtered in time. As the iteration progresses, the filtered
solution generally contains relatively larger and larger proportions of
eigenmodes whose eigenvalues are near a chosen target frequency (target
eigenvalue). The ability to choose an arbitrary target frequency enables the
computation of eigenvalues anywhere in the spectrum, without the need to invert
an indefinite matrix, as is common with other approaches. Furthermore, the
iteration can be embedded within a matrix-free Arnoldi algorithm, which enables
the efficient computation of multiple eigenpairs near the target frequency. For
efficiency, the time-dependent wave equation can be solved with implicit
time-stepping and only about $10$ time-steps per-period are needed, independent
of the mesh spacing. When the (definite) implicit time-stepping equations are
solved with a multigrid algorithm, the cost of the resulting EigenWave scheme
scales linearly with the number of grid points $N$ as the mesh is refined,
giving an optimal $O(N)$ algorithm. The approach is demonstrated by finding
eigenpairs of the Laplacian in complex geometry using overset grids. Results in
two and three space dimensions are presented using second-order and
fourth-order accurate approximations.

</details>


### [5] [Parametric design and adaptive sizing of lattice structures for 3d additive manufacturing](https://arxiv.org/abs/2507.18318)
*Jorge Manuel Mercado-Colmenero,Daniel Diaz - Perete,Miguel Angel Rubio- Paramio,Cristina Martin-Donate*

Main category: math.NA

TL;DR: A parametric design model and adaptive mechanical analysis for a novel lattice structure in 3D additive manufacturing, optimizing part volume, mass, and rigidity.


<details>
  <summary>Details</summary>
Motivation: To advance additive manufacturing by enabling dynamic adaptability of lattice structures for complex parts, improving mechanical performance.

Method: Geometric parameterization, mechanical adaptive sizing, and numerical validation using a 2D beam element model and rigidity analysis.

Result: The lattice structure's geometric parameters adapt to load states, enhancing mechanical performance, validated by numerical analysis.

Conclusion: The approach systematically improves lattice structure design, fostering dynamic tailoring for industrial applications.

Abstract: The present research is developed into the realm of industrial design
engineering and additive manufacturing by introducing a parametric design model
and adaptive mechanical analysis for a new lattice structure, with a focus on
3D additive manufacturing of complex parts. Focusing on the land-scape of
complex parts additive manufacturing, this research proposes geometric
parameterization, mechanical adaptive sizing, and numerical validation of a
novel lattice structure to optimize the final printed part volume and mass, as
well as its structural rigidity. The topology of the lattice structures
exhibited pyramidal geometry. Complete parameterization of the lattice
structure ensures that the known geometric parameters adjust to defined
restrictions, enabling dynamic adaptability based on its load states and
boundary conditions, thereby enhancing its mechanical performance. The core
methodology integrates analytical automation with mechanical analysis by
employing a model based in two-dimensional beam elements. The dimensioning of
the lattice structure is analyzed using rigidity models of its sub-elements,
providing an evaluation of its global structural behavior after applying the
superposition principle. Numerical validation was performed to validate the
proposed analytical model. This step ensures that the analytical model defined
for dimensioning the lattice structure adjusts to its real mechanical behavior
and allows its validation. The present manuscript aims to advance additive
manufacturing methodologies by offering a systematic and adaptive approach to
lattice structure design. Parametric and adaptive techniques foster new
industrial design engineering methods, enabling the dynamic tailoring of
lattice structures to meet their mechanical demands and enhance their overall
efficiency and performance.

</details>


### [6] [On MAP estimates and source conditions for drift identification in SDEs](https://arxiv.org/abs/2507.18443)
*Daniel Tenbrinck,Nikolas Uesseler,Philipp Wacker,Benedikt Wirth*

Main category: math.NA

TL;DR: The paper addresses the inverse problem of estimating the drift in an SDE from observations, proving theoretical properties and suggesting convergence rates supported by numerical simulations.


<details>
  <summary>Details</summary>
Motivation: To solve the inverse problem of drift identification in SDEs using observations, ensuring theoretical validity and practical applicability.

Method: Derives a MAP estimate, proves differentiability and tangential cone conditions for the forward operator, and reviews existing theory for convergence rates.

Result: Theoretical properties are proven, and numerical simulations in 1D support the convergence rates of the MAP estimate.

Conclusion: The study confirms the validity of the approach and suggests convergence rates, supported by empirical evidence.

Abstract: We consider the inverse problem of identifying the drift in an SDE from $n$
observations of its solution at $M+1$ distinct time points. We derive a
corresponding MAP estimate, we prove differentiability properties as well as a
so-called tangential cone condition for the forward operator, and we review the
existing theory for related problems, which under a slightly stronger
tangential cone condition would additionally yield convergence rates for the
MAP estimate as $n\to\infty$. Numerical simulations in 1D indicate that such
convergence rates indeed hold true.

</details>


### [7] [Solution of Least Squares Problems with Randomized Preconditioned Normal Equations](https://arxiv.org/abs/2507.18466)
*Ilse C. F. Ipsen*

Main category: math.NA

TL;DR: The paper explores solving full column-rank least squares problems using randomized preconditioned normal equations, achieving accuracy comparable to QR-based methods even for ill-conditioned matrices.


<details>
  <summary>Details</summary>
Motivation: To improve the accuracy and efficiency of solving least squares problems, especially for ill-conditioned matrices, using randomized preconditioning.

Method: Utilizes randomized preconditioners (symmetrically or non-symmetrically) on normal equations and analyzes their accuracy and perturbation bounds.

Result: Preconditioned normal equations yield solutions nearly as accurate as QR-based methods, with perturbation bounds matching those of the original problem.

Conclusion: Randomized preconditioning is effective for solving least squares problems, even with minimal sampling, as supported by probabilistic condition number bounds.

Abstract: We consider the solution of full column-rank least squares problems by means
of normal equations that are preconditioned, symmetrically or
non-symmetrically, with a randomized preconditioner. With an effective
preconditioner, the solutions from the preconditioned normal equations are
almost as accurate as those from the QR-based Matlab backslash (mldivide)
command -- even for highly illconditioned matrices. This means the accuracy of
the preconditioned normal equations depends on the residual of the original
least squares problem. We present non-intuitive but realistic perturbation
bounds for the relative error in the computed solutions and show that, with an
effective preconditioner, these bounds are essentially equal to the
perturbation bound for the original least squares problem. Probabilitistic
condition number bounds corroborate the effectiveness of the randomized
preconditioner computed with small amounts of sampling.

</details>


### [8] [Fast Multipole Method for Maxwell's Equations in Layered Media](https://arxiv.org/abs/2507.18491)
*Heng Yuan,Bo Wang,Wenzhong Zhang,Wei Cai*

Main category: math.NA

TL;DR: A fast multipole method (FMM) for solving Maxwell's equations in 3-D layered media using magnetic vector potential and dyadic Green's function, achieving O(N log N) complexity.


<details>
  <summary>Details</summary>
Motivation: To efficiently solve Maxwell's equations in layered media, addressing computational challenges and numerical stability.

Method: Uses dyadic Green's function derived from scalar Helmholtz layered Green's functions, introduces equivalent polarization images, and employs Chebyshev polynomial expansion for M2L translations.

Result: Demonstrates O(N log N) complexity and rapid convergence for low-frequency electromagnetic wave interactions.

Conclusion: The developed FMM is efficient and stable for solving Maxwell's equations in 3-D layered media.

Abstract: We present a fast multipole method (FMM) for solving Maxwell's equations in
three-dimensional (3-D) layered media, based on the magnetic vector potential
$\boldsymbol A$ under the Lorenz gauge, to derive the layered dyadic Green's
function. The dyadic Green's function is represented using three scalar
Helmholtz layered Green's functions, with all interface-induced reaction field
components expressed through a unified integral representation. By introducing
equivalent polarization images for sources and effective locations for targets
to reflect the actual transmission distance of different reaction field
components, multiple expansions (MEs) and local expansions (LEs) are derived
for the far-field governed by actual transmission distance. To further enhance
computational efficiency and numerical stability, we employ a Chebyshev
polynomial expansion of the associated Legendre functions to speed up the
calculation of multipole-to-local (M2L) expansion translations. Finally,
leveraging the FMM framework of the Helmholtz equation in 3-D layered media, we
develop a FMM for the dyadic Green's function of Maxwell's equations in layered
media. Numerical experiments demonstrate the $\mathcal O(N\log N)$-complexity
of the resulting FMM method, and rapid convergence for interactions of
low-frequency electromagnetic wave sources in 3-D layered media.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [9] [Existence and nonexistence of sign-changing solutions for linearly perturbed superlinear equations on exterior domains](https://arxiv.org/abs/2507.17863)
*Md Suzan Ahamed,Joseph Iaia*

Main category: math.AP

TL;DR: The paper studies radial solutions of a PDE in exterior domains, proving existence of infinitely many sign-changing solutions under certain conditions and nonexistence for others.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of solutions to a specific PDE with singular and superlinear terms, particularly in exterior domains.

Method: Analyzes radial solutions of the given PDE, focusing on the interplay between the singular term, superlinear growth, and the coefficient $K(|x|)$.

Result: Existence of infinitely many sign-changing solutions when $N+q(N-2) < \alpha < 2(N-1)$, and nonexistence for $0 < \alpha \leq 2$.

Conclusion: The study provides conditions for existence and nonexistence of solutions, contributing to the understanding of nonlinear PDEs in exterior domains.

Abstract: In this paper, we study radial solutions of $\Delta u + K(|x|) f(u)+\frac{
(N-2)^2 u}{|x|^{2+(N-2)\delta}} =0, \ 0<\delta<2$ in the exterior of the ball
of radius $R>0$ in ${\mathbb R}^{N}$ where $f$ grows superlinearly at infinity
and is singular at $0$ with $f(u) \sim -\frac{1}{|u|^{q-1}u}$ and $0<q<1$ for
small $u$. We assume $K(|x|) \sim |x|^{-\alpha}$ for large $|x|$ and establish
the existence of an infinite number of sign-changing solutions when $N+q(N-2)
<\alpha <2(N-1).$ We also prove nonexistence for $0<\alpha \leq2$.

</details>


### [10] [Diffusion over ramified domains: solvability and fine regularity](https://arxiv.org/abs/2507.17909)
*Kevin Silva-Pérez,Alejandro Vélez-Santiago*

Main category: math.AP

TL;DR: The paper studies a domain with a fractal boundary modeling bronchial trees, analyzing diffusion equations with mixed boundary conditions. It proves unique solvability and global Hölder continuity of solutions, including the critical case where the domain is roughest.


<details>
  <summary>Details</summary>
Motivation: The research aims to model oxygen diffusion in bronchial trees, addressing challenges posed by non-Lipschitz domains and fractal boundaries.

Method: The authors analyze a generalized diffusion equation with inhomogeneous mixed-type boundary conditions, proving solvability and continuity under minimal assumptions.

Result: Unique solvability and global Hölder continuity of weak solutions are established, even for the critical case where the domain is most irregular.

Conclusion: The work provides foundational results for diffusion in non-extension domains, extending understanding of boundary value problems in fractal-like structures.

Abstract: We consider a domain $\Omega\subseteq\mathbb{R\!}^{\,2}$ with branched
fractal boundary $\Gamma^{\infty}$ and parameter $\tau\in[1/2,\tau^{\ast}]$
introduced by Achdou and Tchou \cite{ACH08}, for $\tau^{\ast}\simeq 0.593465$,
which acts as an idealization of the bronchial trees in the lungs systems. For
each $\tau\in[1/2,\tau^{\ast}]$, the corresponding region $\Omega$ is a
non-Lipschitz domain, which attains its roughest structure at the critical
value $\tau=\tau^{\ast}$ in such way that in this endpoint parameter the region
$\Omega$ fails to be an extension domain, and its ramified boundary
$\Gamma^{\infty}$ is not post-critically finite. Then, we investigate a model
equation related to the diffusion of oxygen through the bronchial trees by
considering the realization of a generalized diffusion equation with
inhomogeneous mixed-type boundary conditions. Under minimal assumptions, we
first show that the stationary version of the above diffusion equation in
uniquely solvable, and that the corresponding weak solution in globally
H\"older continuous on $\overline{\Omega}$. Since we are including the critical
case $\tau=\tau^{\ast}$, this is the first time in which global uniform
continuity of weak solutions of a Robin-type boundary value problem is attained
over a non-extension domain. Furthermore, after two transitioning procedures,
we prove the unique solvability of the inhomogeneous time-dependent diffusion
equation, and we show that the corresponding weak solution is globally
uniformly continuous over $[0,T]\times\overline{\Omega}$ for each fixed
parameter $T>0$.

</details>


### [11] [Nonspherically symmetric equilibrium bubbles in a steadily rotating incompressible fluid](https://arxiv.org/abs/2507.17915)
*Chen-Chih Lai,Michael I. Weinstein*

Main category: math.AP

TL;DR: The paper presents rotational equilibrium solutions for an isobaric model, extends Gavrilov's work, and numerically simulates bubble shapes using PINN.


<details>
  <summary>Details</summary>
Motivation: To explore nontrivial rotational equilibrium solutions in an inviscid, isobaric model and extend existing results on bubble characterization.

Method: Theoretical analysis of equilibrium solutions, extension of Gavrilov's work, and numerical simulation using Physics-Informed Neural Networks (PINN).

Result: Existence of rotational equilibrium solutions and construction of a nonspherical, horn-torus-shaped bubble under mild conditions.

Conclusion: The study advances understanding of equilibrium solutions in inviscid models and demonstrates the utility of PINN for simulating complex bubble shapes.

Abstract: This note presents two nontrivial, rotational equilibrium solutions to the
spatial uniform gas pressure (isobaric) approximate model of Prosperetti in the
inviscid case. Building on Gavrilov's work [GAFA 2019], we first establish the
existence of equilibrium solutions with nontrivial (rotational) liquid flow.
Second, we construct a nonspherically symmetric, horn-torus-shaped equilibrium
bubble under mild spatial decay conditions of the liquid flow. In addition, we
extend earlier results on the characterization of spherical equilibrium bubbles
to the axisymmetric, purely azimuthal setting. Finally, we implement a
numerical simulation of the equilibrium bubble shape using the Physics-Informed
Neural Network (PINN) approximation.

</details>


### [12] [Existence of smooth solutions of the Navier-Stokes equations in three-dimensional Euclidean space](https://arxiv.org/abs/2507.18063)
*Genqian Liu*

Main category: math.AP

TL;DR: Existence of smooth solutions for 3D Navier-Stokes equations is proven via parabolic inertia Lamé equations, letting λ→∞.


<details>
  <summary>Details</summary>
Motivation: To establish smooth solutions for incompressible Navier-Stokes equations by leveraging their connection with parabolic inertia Lamé equations.

Method: Prove existence/uniqueness of smooth solutions for parabolic inertia Lamé equations, then let Lamé constant λ→∞ while fixing μ>0.

Result: Existence of smooth solutions for 3D Navier-Stokes equations is demonstrated.

Conclusion: The approach successfully links Lamé and Navier-Stokes equations, providing a new proof for smooth solutions.

Abstract: Based on the essential connection of the parabolic inertia Lam\'{e} equations
and Navier-Stokes equations, we prove the existence of smooth solutions of the
incompressible Navier-Stokes equations in three-dimensional Euclidean space
$\mathbb{R}^3$ by showing the existence and uniqueness of smooth solutions of
the parabolic inertia Lam\'{e} equations and by letting a Lam\'{e} constant
$\lambda$ tends to infinity (the other Lam\'{e} constant $\mu>0$ is fixed).

</details>


### [13] [The magnetohydrodynamical system in bounded $\mathscr{C}^1$ domains of dimension $n\ge3$](https://arxiv.org/abs/2507.18195)
*Sylvie Monniaux*

Main category: math.AP

TL;DR: Existence of mild solutions for the magnetohydrodynamical system in critical spaces is proven in $\mathscr{C}^1$ domains for dimensions $n\ge3$.


<details>
  <summary>Details</summary>
Motivation: To establish the existence of solutions for the magnetohydrodynamical system in less smooth domains ($\mathscr{C}^1$) and higher dimensions.

Method: Relies on recent regularity results for the Stokes operator in $\mathscr{C}^1$ domains and a newly proved Leibniz-like formula for differential forms.

Result: Existence of mild solutions is confirmed in critical spaces for the given conditions.

Conclusion: The study successfully extends the understanding of solutions for the magnetohydrodynamical system to $\mathscr{C}^1$ domains and higher dimensions.

Abstract: Existence of mild solutions for the magnetohydrodynamical system in
$\mathscr{C}^1$ domains is established in critical spaces in dimension $n\ge
3$. The proof relies on recent regularity results on the Stokes operator in
$\mathscr{C}^1$ domains and a Leibniz-like formula for differential forms
proved here in Section 2.

</details>


### [14] [$H^s_x$ regularity of solutions to the stationary Boltzmann equation with the incoming boundary condition](https://arxiv.org/abs/2507.18211)
*Daisuke Kawagoe*

Main category: math.AP

TL;DR: Existence of solutions for the stationary Boltzmann equation in a bounded convex domain, with weighted $L^\infty$ and fractional Sobolev regularity, without requiring positive Gaussian curvature on the boundary.


<details>
  <summary>Details</summary>
Motivation: To address the existence and regularity of solutions for the Boltzmann equation under specific cross-section conditions and boundary data, extending previous results.

Method: 1. Prove well-posedness of the linearized problem in a weighted $L^2$ space. 2. Develop $L^2-L^\infty$ estimates. 3. Investigate $H^s_x$ regularity using the velocity averaging lemma. 4. Derive a bilinear estimate for the weakly nonlinear problem.

Result: Solutions exist with $H^{1-}_x$ regularity for $-2 \leq \gamma \leq 1$ and worse regularity for $-3 < \gamma < -2$.

Conclusion: The study successfully extends the understanding of the Boltzmann equation's solutions under relaxed boundary curvature conditions, leveraging linearized analysis and bilinear estimates.

Abstract: We consider the stationary Boltzmann equation with the cross section of the
form $B(|v - \tilde{v}, \theta|) = B_0 |v - \tilde{v}|^\gamma \cos \theta \sin
\theta$ for $-3 < \gamma \leq 1$ in a bounded convex domain under the incoming
boundary condition. In this article, we shall show the existence of a solution
in a weighted $L^\infty$ space with fractional Sobolev regularity without
assuming the positivity of the Gaussian curvature on the boundary. For boundary
data sufficiently smooth and close to the standard Maxwellian, the solution has
$H^{1-}_x$ regularity for $-2 \leq \gamma \leq 1$, while only worse regularity
is obtained for $-3 < \gamma < -2$. We first show the well-posedness of the
linearized problem on a weighted $L^2$ space and develop the $L^2-L^\infty$
estimate without the stochastic cycle. We next investigate $H^s_x$ regularity
of the solution to the linearized problem. The velocity averaging lemma plays a
key role in our analysis. We finally derive a bilinear estimate to extend
results on the linearized problem to the weakly nonlinear problem.

</details>


### [15] [Quantum ergodicity for contact metric structures](https://arxiv.org/abs/2507.18216)
*Lino Benedetto*

Main category: math.AP

TL;DR: Proof of Quantum Ergodicity for subLaplacians on contact metric manifolds with ergodic Reeb flow, using semiclassical pseudodifferential calculus and microlocal projectors.


<details>
  <summary>Details</summary>
Motivation: To extend Quantum Ergodicity theorems to subLaplacians on contact manifolds, leveraging the ergodicity of the Reeb flow.

Method: Uses a semiclassical pseudodifferential calculus tailored for filtered manifolds, constructing microlocal projectors (Landau projectors) that commute with the subLaplacian.

Result: Demonstrates that the subLaplacian acts effectively on the range of each Landau projector, akin to the Reeb vector field.

Conclusion: The proof aligns with classical Quantum Ergodicity approaches once microlocal Weyl laws are established.

Abstract: This paper is dedicated to the proof of a Quantum Ergodicity (QE) theorem for
the eigenfunctions of subLaplacians on contact metric manifolds, under the
assumption that the Reeb flow is ergodic. To do so, we rely on a semiclassical
pseudodifferential calculus developed for general filtered manifolds that we
specialize to the setting of contact manifolds. Our strategy is then
reminiscent of an implementation of the Born-Oppenheimer approximation as we
rely on the construction of microlocal projectors in our calculus which commute
with the subLaplacian, called Landau projectors. The subLaplacian is then shown
to act effectively on the range of each Landau projector as the Reeb vector
field does. The remainder of the proof follows the classical path towards QE,
once microlocal Weyl laws have been established.

</details>


### [16] [Long-time existence for the 2D ideal Boussinesq and the 2D density-dependent Euler equations](https://arxiv.org/abs/2507.18244)
*Hantaek Bae,Milton Lopes Filho,Anna Mazzucato,Helena Nussenzveig Lopes*

Main category: math.AP

TL;DR: The paper proves long-time existence of smooth solutions for 2D ideal Boussinesq and non-homogeneous Euler equations for small perturbations of initial flows, using an elementary and broadly applicable technique.


<details>
  <summary>Details</summary>
Motivation: To address the long-time existence of smooth solutions for specific fluid dynamics equations, even with non-small initial flows, by leveraging small perturbations.

Method: Develops an elementary technique to prove the existence of solutions for small temperature or density perturbations of smooth initial flows.

Result: Establishes long-time existence of smooth solutions for the 2D ideal Boussinesq and non-homogeneous Euler equations under the specified conditions.

Conclusion: The technique is not only effective for the given equations but also has broad potential applicability in similar problems.

Abstract: We establish long-time existence of smooth solutions to the 2D ideal
Boussinesq equations and to the 2D non-homogeneous incompressible Euler
equations for initial data consisting of small temperature perturbations, or
small density perturbations, of smooth initial flows which are not necessarily
small. Both results are known (see Danchin and Fanelli 2013, Danchin 2011 in
the references) but the technique we develop to prove them is at the same time
elementary and has broad potential applicability.

</details>


### [17] [Well-posedness of the compressible boundary layer equations with analytic initial data](https://arxiv.org/abs/2507.18247)
*Ya-Guang Wang,Yi-Lei Zhao*

Main category: math.AP

TL;DR: The paper analyzes the well-posedness of compressible boundary layer equations with analytic tangential data, proving local existence and uniqueness of solutions.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of thermal and viscous layers in compressible viscous flow with heat conduction under small viscosity and heat conductivity limits.

Method: Uses Littlewood-Paley theory for a priori estimates, focusing on solutions analytic in the tangential variable and Sobolev in the normal variable.

Result: Establishes local existence and uniqueness of solutions in the specified function space.

Conclusion: The compressible boundary layer equations are well-posed under the given conditions, with solutions existing uniquely in the described space.

Abstract: We study the well-posedness of the compressible boundary layer equations with
data being analytic in the tangential variable of the boundary. The
compressible boundary layer equations, a nonlinear coupled system of degenerate
parabolic equations and an elliptic equation, describe the behavior of thermal
layer and viscous layer in the small viscosity and heat conductivity limit, for
the two-dimensional compressible viscous flow with heat conduction with nonslip
and zero heat flux boundary conditions. We use the Littlewood-Paley theory to
establish the a priori estimates for solutions of this compressible boundary
layer problem, and obtain the local existence and uniqueness of the solution in
the space of analytic in the tangential variable and Sobolev in the normal
variable.

</details>


### [18] [Nonlinear Hardy-Stein type identities for harmonic functions relative to symmetric integro-differential operators](https://arxiv.org/abs/2507.18308)
*Tomasz Klimsiak,Andrzej Rozkosz*

Main category: math.AP

TL;DR: The paper establishes Hardy-Stein identities for harmonic functions under general symmetric Dirichlet forms, including mixed local-nonlocal operators, and extends results to convex compositions and conditional identities. Applications include norm characterizations and Littlewood-Paley estimates.


<details>
  <summary>Details</summary>
Motivation: To generalize Hardy-Stein identities for harmonic functions to mixed-type operators and explore applications in harmonic Hardy spaces and square functions.

Method: Uses probabilistic methods to derive identities for harmonic functions, including compositions with convex functions and conditional identities for ratios.

Result: Proves Hardy-Stein identities for mixed-type operators, provides norm characterizations in harmonic Hardy spaces, and establishes Littlewood-Paley estimates.

Conclusion: The results extend classical theory to mixed operators and offer new tools for analyzing harmonic functions and their applications.

Abstract: We show identities of Hardy-Stein type for harmonic functions relative to
integro-differential operators corresponding to general symmetric regular
Dirichlet forms satisfying the absolute continuity condition. The novelty is
that we consider operators of mixed type containing both local and nonlocal
component. Moreover, the identities are proved for compositions of harmonic
functions and general convex functions. We also provide some conditional
identities, i.e. identities for ratios of harmonic functions. As an application
we give a characterization of norms in harmonic Hardy spaces and prove
Littlewood--Paley type estimates for square functions. To illustrate general
results, we discuss in some details the case of divergence form operator and
purely nonlocal operator defined by some jump kernel. Our proofs are rather
short and use mainly probabilistic methods.

</details>


### [19] [Quadratic flatness and Regularity for Codimension-One Varifolds with Bounded Anisotropic Mean Curvature](https://arxiv.org/abs/2507.18357)
*Mario Santilli,Sławomir Kolasiński*

Main category: math.AP

TL;DR: The paper proves the existence of a dense C^{1,α}-regular part in the support of a varifold with bounded mean ϕ-curvature, under certain conditions, and links it to quadratic flatness.


<details>
  <summary>Details</summary>
Motivation: To understand the regularity and structure of varifolds with bounded mean curvature, extending known results to more general settings.

Method: Uses a quadratic flatness theorem to analyze points where the varifold's support admits tangent balls, applying Allard's anisotropic regularity theorem.

Result: Shows a dense C^{1,α}-regular part exists in the varifold's support, almost equal to a specific subset where blow-ups are non-trivial.

Conclusion: The results generalize regularity theorems for varifolds, providing insights into their geometric structure under weaker assumptions.

Abstract: Given a uniformly convex norm $ \phi $ on $ \mathbf{R}^{n+1} $ and a
unit-density $ n $-dimensional varifold $ V $ in an open subset of $
\mathbf{R}^{n+1} $ with bounded mean $ \phi $-curvature, under the hypothesis
  $$ \textrm{$\mathcal{H}^n \llcorner {\rm spt}\| V \| $ is absolutely
continuous with respect to $ \| V \| $},$$
  we prove that there exists an open dense $ \mathscr{C}^{1, \alpha}$-regular
part in $ {\rm spt} \| V \| $. Moreover we prove that the $ \mathscr{C}^{1,
\alpha}$-regular part of $ V $ is $ \mathcal{H}^n $-almost equal to $({\rm
spt}\| V \|)^\ast $, the subset of $ {\rm spt}\| V \| $ where at least one blow
up (in Hausdorff distance) of $ {\rm spt}\| V \| $ is not equal to $
\mathbf{R}^{n+1} $.
  These results are consequences of a quadratic flatness theorem asserting that
if $ V $ is a general -- not necessarily rectifiable -- varifold $ V $ with
bounded mean $ \phi $-curvature and locally $ \mathcal{H}^n $-finite support,
then the set of points $ R $ where $ {\rm spt}\| V \|$ admits (exactly) two
mutually tangent balls satisfies $ \mathcal{H}^n(({\rm spt}\| V \|)^\ast
\setminus R) =0 $ and meets each relatively open set of $ {\rm spt}\| V \| $ on
a set of positive $ \mathcal{H}^n $-measure. Indeed, quadratic flatness at a
point guarantees that the Allard anisotropic regularity theorem can be applied,
hence proving that $ {\rm spt}\| V \|$ is regular around that point.

</details>


### [20] [Consistency of tug-of-war type operators on random data clouds](https://arxiv.org/abs/2507.18383)
*Jeongmin Han,Huajie Liu*

Main category: math.AP

TL;DR: Study of a tug-of-war operator on geometric graphs and its Dirichlet problem on random data clouds, analyzing convergence and consistency.


<details>
  <summary>Details</summary>
Motivation: To understand the connection between the tug-of-war operator and its model problem through convergence analysis.

Method: Analyze the convergence of value functions as data points increase and step size shrinks, focusing on operator consistency.

Result: Reveals the relationship between the tug-of-war operator and its model problem.

Conclusion: Consistency of the operator is crucial for establishing the connection between the operator and the model problem.

Abstract: In this paper, we study a tug-of-war type operator on geometric graphs and
its associated Dirichlet problem on a random data cloud.
  Specifically, we analyze the convergence of the value functions as the number
of data points increases and the step size of the game shrinks. This analysis
reveals the connection between our tug-of-war type operator and the
corresponding model problem.
  A key ingredient in establishing this result is the consistency of the
operator.

</details>


### [21] [Homogenization and 3D-2D dimension reduction of a functional on manifold valued BV space](https://arxiv.org/abs/2507.18390)
*Luca Lussardi,Andrea Torricelli,Elvira Zappale*

Main category: math.AP

TL;DR: The paper analyzes homogenization and dimension reduction of an energy functional with linear growth for manifold-valued Sobolev functions using Γ-convergence, leading to an integral representation in the space of manifold-constrained BV functions.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of energy functionals with linear growth in the context of manifold-valued Sobolev functions under simultaneous homogenization and dimension reduction.

Method: The study employs Γ-convergence techniques to analyze the problem and derive an integral representation.

Result: An integral representation result is obtained in the space of manifold-constrained functions with bounded variation.

Conclusion: The work provides a theoretical framework for homogenization and dimension reduction in manifold-valued Sobolev spaces, contributing to the understanding of such functionals.

Abstract: We study the simultaneous homogenization and dimension reduction of an energy
functional with linear growth defined on the space of manifold valued Sobolev
functions. The study is carried out by $\Gamma$-convergence, providing an
integral representation result in the space of manifold constrained functions
with bounded variation

</details>


### [22] [Asymmetric Kedem-Katchalsky boundary conditions for systems with spatial heterogeneities](https://arxiv.org/abs/2507.18400)
*Pablo Álvarez-Caudevilla,Cristina Brändle,Fermin González-Pereiro*

Main category: math.AP

TL;DR: The paper studies a model of two species interacting at a shared boundary, with unique population distributions emerging under specific growth rate conditions, including non-simultaneous blow-up behavior.


<details>
  <summary>Details</summary>
Motivation: To understand how two species interact spatially, especially in refuge areas with unlimited resources, and how their populations behave under varying growth rates.

Method: Uses a system of equations with asymmetric Kedem-Katchalsky boundary conditions to model spatial variations and crowding effects. Analyzes existence, non-existence, and behavior of positive solutions.

Result: A unique positive solution exists for a specific growth rate range, with bifurcation points and non-simultaneous blow-up observed.

Conclusion: The model reveals critical growth rate thresholds and asymmetric population behaviors, highlighting the role of refuge areas in species dynamics.

Abstract: This work investigates a model describing the interaction of two species in
habiting separate but adjacent areas. These populations are governed by a
system of
  equations that account for spatial variations in growth rates and the effects
of crowding.
  A key feature is the presence of areas within each domain where resources are
unlimited
  and crowding effects are absent. The species interact solely through a common
bound ary interface, which is modeled by asymmetric Kedem-Katchalsky boundary
conditions.
  The paper provides existence, non-existence, and behavior of positive
solutions for the
  system. It is shown that a unique positive population distribution exists
when one of the
  growth rate parameters falls within a specific range defined by two critical
values. One of
  these critical values represents a bifurcation point where the population can
emerge from
  extinction, while the other is determined by the characteristics of the
refuge areas. The
  study also examines how the populations behave as the growth parameter
approaches the
  upper critical value. This analysis reveals the phenomenon of
non-simultaneous blow-up,
  where one population component can grow infinitely large within its refuge
zone while
  the other remains bounded.

</details>


### [23] [Strong time regularity and decay of $L^\infty$ solutions to $2\times 2$ systems of conservation laws](https://arxiv.org/abs/2507.18427)
*Luca Talamini*

Main category: math.AP

TL;DR: The paper analyzes $\mathbf L^\infty$ solutions for $2\times 2$ systems of conservation laws, proving regularity for finite entropy solutions and a decay estimate for vanishing viscosity solutions using a kinetic formulation.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of solutions to $2\times 2$ systems of conservation laws, particularly focusing on regularity and decay properties.

Method: Uses a kinetic formulation to unify the analysis of finite entropy solutions and vanishing viscosity solutions.

Result: Finite entropy solutions belong to $C^0(\mathbb R^+; \mathbf L^1_{loc}(\mathbb R))$, and a dispersive decay estimate is established for vanishing viscosity solutions.

Conclusion: The kinetic formulation provides a unified framework for analyzing regularity and decay in conservation law systems.

Abstract: We consider $\mathbf L^\infty$ solutions to $2\times 2$ systems of
conservation laws. For genuinely nonlinear systems we prove that finite entropy
solutions (in particular entropy solutions, if a uniformly convex entropy
exists) belong to $C^0(\mathbb R^+; \mathbf L^1_{loc}(\mathbb R))$. Our second
result establishes a dispersive-type decay estimate for vanishing viscosity
solutions. Both results are unified by the use of a kinetic formulation.

</details>


### [24] [Gradient regularity for double-phase orthotropic functionals](https://arxiv.org/abs/2507.18474)
*Stefano Almi,Chiara Leone,Gianluigi Manzo*

Main category: math.AP

TL;DR: Higher integrability and Lipschitz regularity for minimizers of a double-phase orthotropic functional with Hölder continuous weights.


<details>
  <summary>Details</summary>
Motivation: To extend regularity results for minimizers of double-phase functionals, particularly under weaker assumptions on the weight function and exponents.

Method: Analyze local minimizers of the functional with Hölder continuous weights and specific exponent conditions.

Result: Proved higher integrability and derived explicit Lipschitz regularity estimates for local minimizers.

Conclusion: The results generalize and refine existing regularity theory for double-phase functionals.

Abstract: We prove higher integrability for local minimizers of the double-phase
orthotropic functional \[
  \sum_{i=1}^{n}\int_\Omega\left(\left|u_{x_i}\right|^p+a(x)\left|
u_{x_i}\right|^q\right)dx \] when the weight function $a \geq0$ is assumed to
be $\alpha$-H\"older continuous, while the exponents $p, q$ are such that $2
\leq p \leq q$ and $\frac{q}{p} < 1 + \frac{\alpha}{n}$. Under natural Sobolev
regularity of~$a$, we further obtain explicit Lipschitz regularity estimates
for local minimizers.

</details>


### [25] [A dichotomy result for a modified Schrödinger equations on unbounded domains](https://arxiv.org/abs/2507.18528)
*Anna Maria Candela,Giuliana Palmieri,Addolorata Salvatore*

Main category: math.AP

TL;DR: The paper investigates bounded positive solutions for a generalized PDE problem (P) with variational structure, extending the modified Schrödinger equation. Solutions are found via limit analysis on bounded domains, with conditions for nontriviality or specific behavior at infinity.


<details>
  <summary>Details</summary>
Motivation: To explore the existence of bounded positive solutions for a nonlinear PDE problem (P) in unbounded domains, generalizing the modified Schrödinger equation and addressing lack of radial symmetry.

Method: Uses variational methods and limit analysis on sequences of solutions in bounded domains. Assumes specific growth conditions on the function A(x,t,ξ) and g(x,t).

Result: Proves existence of at least one bounded positive solution for (P). Under stronger conditions, shows either nontriviality or specific behavior of solutions at infinity.

Conclusion: The study successfully generalizes and solves the problem (P), providing insights into bounded solutions in unbounded domains without radial symmetry.

Abstract: This article aims to investigate the existence of bounded positive solutions
of problem \[ (P)\qquad \left\{ \begin{array}{ll} - {\rm div} (a(x,u,\nabla u))
+ A_t(x,u,\nabla u) = g(x,u) &\hbox{in $\Omega$,}\\ u\ = \ 0 & \hbox{on
$\partial\Omega$,} \end{array}\right.\] with $A_t(x,t,\xi) = \frac{\partial
A}{\partial t}(x,t,\xi)$, $a(x,t,\xi) = \nabla_\xi A(x,t,\xi)$ for a given
$A(x,t,\xi)$ which grows as $|\xi|^p + |t|^p$ , $p > 1$, where $\Omega
\subseteq \mathbb{R}^N$, $N \ge 2$, is an open connected domain with Lipschitz
boundary and infinite Lebesgue measure, eventually $\Omega = \mathbb{R}^N$,
which generalizes the modified Schr\"odinger equation \[ - {\rm div} ((A^*_1(x)
+ A^*_2(x)|u|^{s}) \nabla u) + \frac{s}2 A^*_2(x)\ |u|^{s - 2} u\ |\nabla u|^2
+ u\ =\ |u|^{\mu-2}u \quad\hbox{in $\mathbb{R}^3$.} \] Under suitable
assumptions on $A(x,t,\xi)$ and $g(x,t)$, problem $(P)$ has a variational
structure. Then, even in lack of radial symmetry hypotheses, one bounded
positive solution of $(P)$ can be found by passing to the limit on a sequence
$(u_k)_k$ of bounded solutions on bounded domains. Furthermore, if stronger
hypotheses are satisfied, either such a solution is nontrivial or a constant
$\bar{\lambda} > 0$ and a sequence of points $(y_k)_k \subset \mathbb{R}^N$
exist such that \[ |y_k| \to +\infty\qquad \hbox{and}\qquad \int_{B_1(y_k)}
|u_k|^p dx \ge \bar{\lambda}\quad \hbox{for all $k \ge 1$.} \]

</details>


### [26] [Schrodinger-Poisson-Slater equations with nonlinearity subscaled near zero](https://arxiv.org/abs/2507.18568)
*Shibo Liu,Kanishka Perera*

Main category: math.AP

TL;DR: The paper studies a zero-mass Schrödinger-Poisson-Slater equation with subscaled nonlinearity near zero and asymptotically scaled at infinity. It obtains nonzero solutions using Morse theory and a sequence of solutions for odd nonlinearities via Clark's theorem.


<details>
  <summary>Details</summary>
Motivation: The motivation is to analyze the zero-mass Schrödinger-Poisson-Slater equation with specific nonlinearity conditions, aiming to find solutions under varying scaling behaviors.

Method: The methods include Morse theory for nonzero solutions and a version of Clark's theorem for sequences of solutions when the nonlinearity is odd. An abstract result on critical groups at infinity is also proven.

Result: A nonzero solution is obtained for asymptotically scaled nonlinearities, and a sequence of solutions is derived for odd nonlinearities.

Conclusion: The paper successfully applies advanced mathematical tools to derive solutions for the given equation under specific nonlinearity conditions, contributing to the understanding of such systems.

Abstract: We study the following zero-mass Schr{\"o}dinger-Poisson-Slater equation \[ -
\Delta u + \left( \frac{1}{4 \pi | x |} \ast u^2 \right) u = f (| x |, u)
\text{,} \qquad u \in \mathcal{D}^{1, 2} (\mathbb{R}^3) \text{} \] with
nonlinearity subscaled near zero in the sense that $f (| x |, t) \approx a | t
|^{p - 2} t$ as $| t | \rightarrow 0$ for some $p\in\big(\frac{18}{7},3\big)$.
A nonzero solution is obtained via Morse theory when the nonlinearity is
asymptotically scaled at infinity. For this purpose we prove an abstract result
on the critical groups at infinity for functionals satisfying the geometric
assumptions of the scaled saddle point theorem of Mercuri \& Perera
[arXiv:2411.15887]. For the case that $f (| x |, \cdot)$ is odd, a sequence of
solutions are obtained via a version of Clark's theorem due to Kajikiya [J.\
Funct.\ Anal.\ 225 (2005) 352--370].

</details>


### [27] [Implementation of the inverse scattering transform method for the nonlinear Schrödinger equation](https://arxiv.org/abs/2507.18586)
*Vladislav V. Kravchenko*

Main category: math.AP

TL;DR: The paper presents a new method for solving the initial-value problem for the nonlinear Schrödinger equation using series representations for Jost solutions, simplifying direct and inverse scattering problems.


<details>
  <summary>Details</summary>
Motivation: To develop a simpler and more efficient numerical method for solving the nonlinear Schrödinger equation by avoiding complex techniques like the Gelfand-Levitan-Marchenko equation.

Method: Uses power series representations for Jost solutions of the Zakharov-Shabat system, reducing scattering problems to recurrent integration and linear algebra.

Result: The method simplifies solving direct and inverse scattering problems, leading to an efficient numerical algorithm.

Conclusion: The approach provides a straightforward and effective numerical solution for the nonlinear Schrödinger equation, demonstrated by examples.

Abstract: We study the initial-value problem for the nonlinear Schr\"odinger equation.
Application of the inverse scattering transform method involves solving direct
and inverse scattering problems for the Zakharov-Shabat system with complex
potentials. We solve these problems by using new series representations for the
Jost solutions of the Zakharov-Shabat system. The representations have the form
of power series with respect to a transformed spectral parameter. In terms of
the representations, solution of the direct scattering problem reduces to
computing the series coefficients following a simple recurrent integration
procedure, computation of the scattering coefficients by multiplying
corresponding pairs of polynomials (partial sums of the series representations)
and locating zeros of a polynomial inside the unit disk. Solution of the
inverse scattering problem reduces to the solution of a system of linear
algebraic equations for the power series coefficients, while the potential is
recovered from the first coefficients. The system is obtained directly from the
scattering relations. Thus, unlike other existing techniques, the method does
not involve solving the Gelfand-Levitan-Marchenko equation or the matrix
Riemann-Hilbert problem. The overall approach leads to a simple and efficient
algorithm for the numerical solution of the initial-value problem for the
nonlinear Schr\"odinger equation, which is illustrated by numerical examples.

</details>


### [28] [Vortex dynamics for the Gross-Pitaevskii equation](https://arxiv.org/abs/2507.18590)
*Manuel del Pino,Rowan Juneman,Monica Musso*

Main category: math.AP

TL;DR: The paper rigorously analyzes the asymptotics of Gross-Pitaevskii vortex dynamics, constructing n-vortex solutions and linking their motion to the Helmholtz-Kirchhoff system.


<details>
  <summary>Details</summary>
Motivation: To understand the formal asymptotics of vortex dynamics in the Gross-Pitaevskii equation, particularly for n-vortex solutions.

Method: Constructs n-vortex solutions with degree ±1, computes asymptotic expansions of vortex positions, and analyzes dynamics as the core size approaches zero.

Result: Shows leading-order dynamics follow the Helmholtz-Kirchhoff system, with corrections from a linear wave equation.

Conclusion: Validates a formal expansion by Ovchinnikov and Sigal, providing precise vortex dynamics on finite time intervals.

Abstract: We rigorously establish the formal asymptotics of Neu for Gross-Pitaevskii
vortex dynamics in the plane. Given any integer $n\geq2$, we construct a family
of $n$-vortex solutions with vortices of degree $\pm1$, and describe precisely
the solution profile and associated vortex dynamics on an arbitrarily large,
finite time interval. We compute an asymptotic expansion of the vortex
positions in terms of the vortex core size $\epsilon>0$, and show that the
dynamics is governed at leading order as $\epsilon\to0$ by the classical
Helmholtz-Kirchhoff system. Moreover, we show that the first correction to the
leading order dynamics is determined by the solution of a linear wave equation,
justifying a formal expansion found by Ovchinnikov and Sigal.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [29] [Multi-Head Neural Operator for Modelling Interfacial Dynamics](https://arxiv.org/abs/2507.17763)
*Mohammad Sadegh Eshaghi,Navid Valizadeh,Cosmin Anitescu,Yizheng Wang,Xiaoying Zhuang,Timon Rabczuk*

Main category: physics.comp-ph

TL;DR: The paper introduces the Multi-Head Neural Operator (MHNO) to efficiently solve time-dependent nonlinear PDEs, outperforming existing neural operators in accuracy and scalability.


<details>
  <summary>Details</summary>
Motivation: Traditional numerical methods are computationally prohibitive for high-dimensional or multi-scale problems, prompting the need for efficient alternatives like neural operators.

Method: MHNO uses a novel architecture with time-step-specific projection operators and explicit temporal connections to predict all time steps in one forward pass.

Result: MHNO achieves superior accuracy and efficiency in solving phase field equations compared to existing neural operator methods.

Conclusion: MHNO is a promising next-generation tool for phase field modeling, with publicly available code and data.

Abstract: Interfacial dynamics underlie a wide range of phenomena, including phase
transitions, microstructure coarsening, pattern formation, and thin-film
growth, and are typically described by stiff, time-dependent nonlinear partial
differential equations (PDEs). Traditional numerical methods, including finite
difference, finite element, and spectral techniques, often become
computationally prohibitive when dealing with high-dimensional problems or
systems with multiple scales. Neural operators (NOs), a class of deep learning
models, have emerged as a promising alternative by learning mappings between
function spaces and efficiently approximating solution operators. In this work,
we introduce the Multi-Head Neural Operator (MHNO), an extended neural operator
framework specifically designed to address the temporal challenges associated
with solving time-dependent PDEs. Unlike existing neural operators, which
either struggle with error accumulation or require substantial computational
resources for high-dimensional tensor representations, MHNO employs a novel
architecture with time-step-specific projection operators and explicit temporal
connections inspired by message-passing mechanisms. This design allows MHNO to
predict all time steps after a single forward pass, while effectively capturing
long-term dependencies and avoiding parameter overgrowth. We apply MHNO to
solve various phase field equations, including antiphase boundary motion,
spinodal decomposition, pattern formation, atomic scale modeling, and molecular
beam epitaxy growth model, and compare its performance with existing NO-based
methods. Our results show that MHNO achieves superior accuracy, scalability,
and efficiency, demonstrating its potential as a next-generation computational
tool for phase field modeling. The code and data supporting this work is
publicly available at https://github.com/eshaghi-ms/MHNO.

</details>


### [30] [Machine Learning Workflow for Analysis of High-Dimensional Order Parameter Space: A Case Study of Polymer Crystallization from Molecular Dynamics Simulations](https://arxiv.org/abs/2507.17980)
*Elyar Tourani,Brian J. Edwards,Bamin Khomami*

Main category: physics.comp-ph

TL;DR: A machine learning workflow is introduced to accurately quantify crystallinity in polymers using atomistic data, reducing reliance on biased single-order parameters and identifying a minimal set of three OPs for high-fidelity classification.


<details>
  <summary>Details</summary>
Motivation: Current methods for identifying crystallization pathways in polymers rely on preset cut-off points for single-order parameters, which introduce biases and sensitivity issues. This study aims to overcome these limitations with a data-driven approach.

Method: An integrated workflow combines high-dimensional feature vectors (geometric, thermodynamic-like, symmetry-based) with low-dimensional embeddings and unsupervised clustering. Supervised learning then identifies a minimal set of three order parameters for crystallinity classification.

Result: The method achieves over 0.98 AUC in classification, with a bimodal crystallinity index (C-index) derived from logistic regression. It efficiently computes crystallinity on-the-fly and reveals entropy-driven early nucleation and symmetry-driven later stages.

Conclusion: The workflow offers a robust, data-driven strategy for order parameter selection and monitoring structural transformations in polymer simulations, addressing biases and improving accuracy.

Abstract: Currently, identification of crystallization pathways in polymers is being
carried out using molecular simulation-based data on a preset cut-off point on
a single order parameter (OP) to define nucleated or crystallized regions.
Aside from sensitivity to cut-off, each of these OPs introduces its own
systematic biases. In this study, an integrated machine learning workflow is
presented to accurately quantify crystallinity in polymeric systems using
atomistic molecular dynamics data. Each atom is represented by a
high-dimensional feature vector that combines geometric, thermodynamic-like,
and symmetry-based descriptors. Low dimensional embeddings are employed to
expose latent structural fingerprints within atomic environments. Subsequently,
unsupervised clustering on the embeddings identified crystalline and amorphous
atoms with high fidelity. After generating high quality labels with
multidimensional data, we use supervised learning techniques to identify a
minimal set of order parameters that can fully capture this label. Various
tests were conducted to reduce the feature set, demonstrating that using only
three order parameters is sufficient to recreate the crystallization labels.
Based on these observed OPs, the crystallinity index (C-index) is defined as
the logistic regression model's probability of crystallinity, remaining bimodal
throughout the process and achieving over 0.98 classification performance
(AUC). Notably, a model trained on one or a few snapshots enables efficient
on-the-fly computation of crystallinity. Lastly, we demonstrate how the optimal
C-index fit evolves during various stages of crystallization, supporting the
hypothesis that entropy dominates early nucleation, while symmetry gains
relevance later. This workflow provides a data-driven strategy for OP selection
and a metric to monitor structural transformations in large-scale polymer
simulations.

</details>


### [31] [Hierarchical Finite-Element Analysis of Multiscale Electromagnetic Problems via Sparse Operator-Adapted Wavelet Decomposition](https://arxiv.org/abs/2507.17989)
*F. Şık,F. L. Teixeira,B. Shanker*

Main category: physics.comp-ph

TL;DR: A FEM framework with operator-adapted wavelet decomposition decouples resolution levels for efficient multiscale electromagnetic analysis, achieving near-linear computational complexity.


<details>
  <summary>Details</summary>
Motivation: Existing adaptive FEM methods couple resolution levels, requiring costly recomputation when refining accuracy. This work aims to decouple levels for independent computation.

Method: Hierarchical algorithm constructs solutions from finest to coarsest levels using sparse matrix-vector multiplications, with Krylov subspace solvers and ILU preconditioners.

Result: Numerical experiments confirm high precision and near-linear computational complexity.

Conclusion: The proposed method efficiently decouples resolution levels, reducing computational overhead while maintaining accuracy.

Abstract: In this paper, we present a finite element method (FEM) framework enhanced by
an operator-adapted wavelet decomposition algorithm designed for the efficient
analysis of multiscale electromagnetic problems. Usual adaptive FEM approaches,
while capable of achieving the desired accuracy without requiring a complete
re-meshing of the computational domain, inherently couple different resolution
levels. This coupling requires recomputation of coarser-level solutions
whenever finer details are added to improve accuracy, resulting in substantial
computational overhead. Our proposed method addresses this issue by decoupling
resolution levels. This feature enables independent computations at each scale
that can be incorporated into the solutions to improve accuracy whenever
needed, without requiring re-computation of coarser-level solutions. The main
algorithm is hierarchical, constructing solutions from finest to coarser levels
through a series of sparse matrix-vector multiplications. Due to its sparse
nature, the overall computational complexity of the algorithm is nearly linear.
Moreover, Krylov subspace iterative solvers are employed to solve the final
linear equations, with ILU preconditioners that enhance solver convergence and
maintain overall computational efficiency. The numerical experiments presented
in this article verify the high precision and nearly linear computational
complexity of the proposed algorithm.

</details>


### [32] [A causality inspired acceleration method for the fast temporal superposition of the finite line source solutions](https://arxiv.org/abs/2507.18200)
*Marc Basquens,Alberto Lazzarotto*

Main category: physics.comp-ph

TL;DR: A fast method for computing thermal interactions in solids, improving performance over existing non-history temporal superposition algorithms by leveraging heat wave properties and reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: Addressing the need for efficient computation in time-dependent problems with multiple sources and scales, particularly in borehole heat exchanger physics.

Method: Uses non-history temporal superposition acceleration, optimizes integration regions by fixing error tolerance, and replaces Bakhalov-Vasil'eva with an asymptotic method for oscillatory integrals.

Result: Significantly reduces precomputation costs by orders of magnitude, making it feasible for large-scale simulations.

Conclusion: The method is robust and highly efficient, enabling simulations with hundreds of sources and time steps in borehole fields.

Abstract: We present a novel, fast method to compute thermal interactions in solids,
useful for time-dependent problems involving several sources and several time
and space scales such as the ones encountered in the physics of fields of
closed loop borehole heat exchangers. The new method is based on the
non-history temporal superposition acceleration algorithm, but presents better
performance compared to the originally proposed scheme. The main idea behind it
is to leverage the propagation properties of the heat wave. Despite the basic
physical solutions of heat transfer being non-causal, it is possible to
establish an influence region by fixing an acceptable error tolerance. This
allows to reduce the necessary integration regions in such a way that numerical
integration is favored. The better behaviour of the integrand arising from this
approach allows us to replace the use of Bakhalov-Vasil'eva method in favor of
the asymptotic method for the computation of highly oscillatory integrals that
has better properties from a computational perspective in the present
application. Extensive testing is presented to evaluate the robustness of the
new method and to compare its performance against the originally proposed
non-history method and the convolution using the FFT algorithm for a range of
error tolerances. The results show that the computational cost is highly
reduced for the precomputation, which includes all the computations done before
starting the time-stepping scheme. The reduction is of several orders of
magnitude, depending on the specific case. This cost was the bottleneck of the
original non-history implementation, and reducing it in this way makes the
method suitable for simulations involving hundreds of sources and hundreds of
thousands of time steps that can arise in simulations of borehole fields.

</details>


### [33] [Atomistic Generative Diffusion for Materials Modeling](https://arxiv.org/abs/2507.18314)
*Nikolaj Rønne,Bjørk Hammer*

Main category: physics.comp-ph

TL;DR: A generative modeling framework for atomistic systems combining score-based diffusion for atomic positions and discrete diffusion for atomic types, achieving high fidelity and diversity in generating structures.


<details>
  <summary>Details</summary>
Motivation: To enable flexible and physically grounded generation of atomic structures across diverse chemical and structural domains.

Method: Combines score-based diffusion for atomic positions with continuous-time discrete diffusion for atomic types, implemented in the AGeDi software.

Result: Strong performance in fidelity and diversity, demonstrated through atomic type interpolation and guided sampling for crystallographic symmetries.

Conclusion: The framework and AGeDi software provide a powerful tool for atomistic generative modeling, with applications in metallic clusters and 2D materials.

Abstract: We present a generative modeling framework for atomistic systems that
combines score-based diffusion for atomic positions with a novel
continuous-time discrete diffusion process for atomic types. This approach
enables flexible and physically grounded generation of atomic structures across
chemical and structural domains. Applied to metallic clusters and
two-dimensional materials using the QCD and C2DB datasets, our models achieve
strong performance in fidelity and diversity, evaluated using precision-recall
metrics against synthetic baselines. We demonstrate atomic type interpolation
for generating bimetallic clusters beyond the training distribution, and use
classifier-free guidance to steer sampling toward specific crystallographic
symmetries in two-dimensional materials. These capabilities are implemented in
Atomistic Generative Diffusion (AGeDi), an open-source, extensible software
package for atomistic generative diffusion modeling.

</details>


### [34] [Topology-Preserving Coupling of Compressible Fluids and Thin Deformables](https://arxiv.org/abs/2507.18460)
*Jonathan Panuelos,Eitan Grinspun,David Levin*

Main category: physics.comp-ph

TL;DR: A novel discretization method ensures leakproofness in coupled fluid-structure systems by preserving fluid domain connectedness, using Voronoi-based partitioning and Godunov-style integration.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of leakproofness in simulations of coupled compressible fluids and thin deformable structures, ensuring accurate boundary conditions and force exchange.

Method: Combines constrained Voronoi-based spatial partitioning with Godunov-style finite-volume time integration to discretize the fluid domain conforming exactly to the fluid-solid interface.

Result: Validated on scenarios like a balloon, champagne cork, and supersonic asteroid, demonstrating bidirectional energy transfer without fluid leakage.

Conclusion: The method effectively resolves fluid-solid interactions sharply, ensuring leakproofness even for thin structures.

Abstract: We present a novel discretization of coupled compressible fluid and thin
deformable structures that provides sufficient and necessary leakproofness by
preserving the path connectedness of the fluid domain. Our method employs a
constrained Voronoi-based spatial partitioning combined with Godunov-style
finite-volume time integration. The fluid domain is discretized into cells that
conform exactly to the fluid-solid interface, allowing boundary conditions to
be sharply resolved exactly at the interface. This enables direct force
exchange between the fluid and solid while ensuring that no fluid leaks through
the solid, even when arbitrarily thin. We validate our approach on a series of
challenging scenarios -- including a balloon propelled by internal compressed
air, a champagne cork ejecting after overcoming friction, and a supersonic
asteroid -- demonstrating bidirectional energy transfer between fluid and
solid.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [35] [Bright polarised x-ray flashes from dense plasmas](https://arxiv.org/abs/2507.18078)
*Q. Qian,C. P. Ridgers,S. V. Bulanov,T. Grismayer,P. Hadjisolomou,D. Seipt,M. Vranic,A. G. R. Thomas*

Main category: physics.plasm-ph

TL;DR: The paper explores using polarization of X-rays as an indicator of strong-field QED (SFQED) plasma production in high-intensity laser experiments, distinguishing it from other X-ray sources.


<details>
  <summary>Details</summary>
Motivation: Understanding SFQED plasmas is crucial for extreme astrophysical environments like pulsar magnetospheres, but their dynamics are poorly understood.

Method: A laser of intensity 10^21 Wcm^-2 is used on a solid Al target to produce X-rays, with polarization analyzed to identify SFQED effects.

Result: X-rays >10 keV from the SFQED plasma are >65% polarized, unlike unpolarized background sources like bremsstrahlung.

Conclusion: Polarization is a reliable indicator of SFQED plasma production, aiding in distinguishing it from other X-ray sources.

Abstract: Creating a plasma dominated by strong-field QED (SFQED) effects is a major
goal of new multi-PW laser facilities. This is motivated by the fact that the
fundamental dynamics of such plasmas is poorly understood and plays an
important role in the electrodynamics of extreme astrophysical environments
such as pulsar magnetospheres. The most obvious observable for which such a
regime has been reached is the production of a bright flash of x-rays, but
distinguishing this from other sources of hard x-rays (e.g., bremsstrahlung) is
a major challenge. Here we show that the photons from the X-ray flash are
highly polarised, as compared to the unpolarised background, i.e., polarisation
is an indicator that the SFQED plasma has really produced. For a laser of
intensity $10^{21}$ Wcm$^{-2}$ impinging on a solid Al target, the photons of
the flash with energy $>10$\thinspace keV are $>65\%$ polarised.

</details>


### [36] [Electron acoustic shock and solitary waves in spin polarized dense rotating quantum plasmas](https://arxiv.org/abs/2507.18228)
*Atherv Saxena,Punit Kumar*

Main category: physics.plasm-ph

TL;DR: The paper studies electrostatic waves in a quantum plasma with electrons, positrons, and ions, considering spin, Fermi pressure, and quantum effects. It derives dispersion relations and analyzes shock waves, finding quantum effects enhance dispersion and stabilize shocks.


<details>
  <summary>Details</summary>
Motivation: To understand wave propagation in astrophysical quantum plasmas, incorporating spin, rotation, and quantum effects, which are crucial for astrophysical environments.

Method: Derived coupled dispersion relations for electron, positron, and ion modes. Used the Korteweg de Vries Burgers method to study electron acoustic shock waves.

Result: Quantum effects enhance wave dispersion and modify shock profiles by broadening and stabilizing the shock structure.

Conclusion: Quantum effects significantly influence wave propagation and shock stability in astrophysical quantum plasmas.

Abstract: The propagation of electrostatic waves in a three-component electron positron
ion astrophysical quantum plasma under the influence of uniform rotation is
analysed, incorporating the effects of particle spin, Fermi pressure, and the
quantum Bohm potential. Spin polarisation arising due to the alignment of
particle spins under the influence of a strong external magnetic field leads to
an imbalance in the population of spin-up and spin-down states. Additionally,
key astrophysical factors such as rotation and gravitational influence have
been considered. The coupled dispersion relations for electron, positron, and
ion modes have been derived. Further, the electron acoustic shock wave is
studied using the Korteweg de Vries Burgers method, and the shock wave solution
has been obtained. Quantum effects are found to contribute to enhanced wave
dispersion and modify the shock profile by broadening and stabilising the shock
structure.

</details>


### [37] [Plasma Position Constrained Free-Boundary MHD Equilibrium in Tokamaks using pyIPREQ](https://arxiv.org/abs/2507.18324)
*Udaya Maurya,Amit K. Singh,Suman Aich,Jagabandhu Kumar,Rohit Kumar,Daniel Raju*

Main category: physics.plasm-ph

TL;DR: pyIPREQ is a new MHD equilibrium code for Tokamak plasmas, enhancing PEST and IPREQ with features like limiter boundaries, magnetic axis constraints, and vertical instability handling.


<details>
  <summary>Details</summary>
Motivation: To improve Tokamak plasma equilibrium modeling by incorporating diagnostic data and addressing stability issues.

Method: Uses finite difference and Green's function approach, building on PEST and IPREQ, with added capabilities like limiter boundaries and magnetic axis constraints.

Result: Benchmarked against existing codes and applied to ADITYA-U and SST-1 Tokamaks, showing accurate predictions.

Conclusion: pyIPREQ is a robust tool for Tokamak equilibrium analysis, validated and applicable to real-world experiments.

Abstract: A free-boundary, axisymmetric magnetohydrodynamic (MHD) equilibrium code,
pyIPREQ, has been developed for Tokamak plasmas using finite difference and
Green's function approach. The code builds upon the foundational frameworks of
the PEST and IPREQ codes, introducing several enhancements and new
capabilities. Notably, pyIPREQ supports the specification of limiter boundaries
and enables the computation of key physical quantities. The code has also been
extended to compute equilibria constrained by a prescribed magnetic axis
position, which is particularly useful when such information is available from
diagnostics like Sine-Cosine coils. In addition, pyIPREQ includes functionality
to address vertical instabilities, a requirement for accurately modeling
elongated plasma configurations. Benchmarking has been carried out against
published results and the original IPREQ code. Applications are demonstrated
for ADITYA-U Tokamak experiments, where magnetic axis measurements are
available, and predictions are also made for SST-1 and ADITYA-U Tokamaks under
various operational scenarios.

</details>


### [38] [Nonlocal current-driven heat flow in ideal plasmas](https://arxiv.org/abs/2507.18430)
*Nicholas Mitchell,David Chapman,Grigory Kagan*

Main category: physics.plasm-ph

TL;DR: The paper explores nonlocal effects on current-driven heat flux in plasmas, identifying a novel mechanism that enhances heat flux for even weak flows, especially in high-ionization scenarios.


<details>
  <summary>Details</summary>
Motivation: To address the understudied nonlocal regimes of current-driven heat flow and friction in plasmas, particularly in fusion and astrophysical contexts.

Method: A first-principles reduced kinetic method (RKM) is applied to study nonlocal effects on current-driven transport.

Result: Large currents enhance current-driven heat flux due to a novel nonlocal mechanism, especially for higher effective ionizations. Weak flows (Nu ≳ 1/100) also show significant enhancements.

Conclusion: Nonlocal effects in current-driven transport are significant and can be enhanced by even weak flows, with implications for high-ionization plasmas.

Abstract: Electron heat flux is an important and often dominant mechanism of energy
transport in a variety of collisional plasmas in a confined fusion or
astrophysical context. While nonlocal conductive heat transport, driven by
strong temperature gradients, has been investigated extensively in previous
literature, nonlocal regimes of the current-driven heat flow and friction have
not received the same attention. In this work, a first-principles reduced
kinetic method (RKM) is applied to study nonlocal effects on current-driven
transport. In addition to nonlocality due to sharp gradients, sufficiently
large currents are found to significantly enhance current-driven heat flux due
to a novel nonlocal mechanism, with this enhancement being increasingly
prevalent for higher effective ionizations $Z^*$. Introducing the dimensionless
number $N_u \equiv \vert \boldsymbol{u}_e - \boldsymbol{u}_i \vert /
v_{\text{th},e}$, these enhancements occur for even relatively weak flows $N_u
\gtrsim 1/100$, analogously to standard nonlocal effects becoming significant
for Knudsen numbers $N_K \gtrsim 1/100$.

</details>


<div id='math.CV'></div>

# math.CV [[Back]](#toc)

### [39] [Formally Integrable Structures III. Levi Flat Structures](https://arxiv.org/abs/2507.18341)
*Qingchun Ji,Jun Yao*

Main category: math.CV

TL;DR: The paper constructs differential complexes using formal integrability, resolving basic sections of vector bundles and realizing the Morse-Novikov-Treves complex. It introduces convexity for Hermitian metrics in Levi flat structures, proving vanishing theorems and global solvability. For elliptic structures, it addresses singular cohomology and canonical form extensions.


<details>
  <summary>Details</summary>
Motivation: To resolve basic sections of vector bundles and globally realize the Morse-Novikov-Treves complex, leveraging formal integrability and convexity in Hermitian metrics.

Method: Utilizes formal integrability to construct differential complexes, introduces convexity for Hermitian metrics in Levi flat structures, and applies these to elliptic structures.

Result: Establishes vanishing theorems, global solvability of the Morse-Novikov-Treves complex, and results for singular cohomology and canonical form extensions in elliptic structures.

Conclusion: The approach successfully resolves basic sections and realizes the Morse-Novikov-Treves complex, with applications to Levi flat and elliptic structures.

Abstract: In this paper, we utilize formal integrability to construct a class of
differential complexes, thereby providing a resolution for the sheaf of basic
sections of a basic vector bundle, as well as a global realization of the
Morse-Novikov-Treves complex. In the context of Levi flat structures, we
introduce a notion of convexity for each Hermitian metric on a basic line
bundle, which enables us to establish vanishing theorems and the global
solvability of the Morse-Novikov-Treves complex. In the special case of
elliptic structures, we further obtain results concerning singular cohomology
and the extension problem for canonical forms.

</details>


### [40] [A new approach to the Monge-Ampère eigenvalue problem](https://arxiv.org/abs/2507.18409)
*Chinh H. Lu,Ahmed Zeriahi*

Main category: math.CV

TL;DR: The paper addresses the eigenvalue problem for the complex Monge-Ampère operator in hyperconvex domains, focusing on uniqueness of eigenfunctions and a Rayleigh quotient formula. It introduces an iterative method for eigenvalues and eigenfunctions, leveraging plurisubharmonic envelopes, and extends results to real Monge-Ampère via logarithmic transformation.


<details>
  <summary>Details</summary>
Motivation: The study aims to solve the eigenvalue problem for the complex Monge-Ampère operator, addressing gaps in uniqueness and computational methods, while extending insights to real Monge-Ampère.

Method: The approach uses plurisubharmonic envelopes for partial sublinearization, an iterative procedure for eigenvalues and eigenfunctions, and logarithmic transformation for real analogues.

Result: Uniqueness of eigenfunctions is proven, a Rayleigh quotient formula is provided, and iterative solutions are derived. The method simplifies existing arguments and extends to complex Hessian operators.

Conclusion: The paper presents a novel, simplified method for solving the eigenvalue problem, with broad applicability to both complex and real Monge-Ampère operators.

Abstract: We study the eigenvalue problem for the complex Monge-Amp\`ere operator in
bounded hyperconvex domains in $\C^n$, where the right-hand side is a
non-pluripolar positive Borel measure. We establish the uniqueness of
eigenfunctions in the finite energy class introduced by Cegrell, up to positive
multiplicative constants, and provide a Rayleigh quotient type formula for
computing the eigenvalue.
  Under a natural continuity assumption on the measure, we further show that
both the eigenvalue and eigenfunctions can be obtained via an iterative
procedure starting from any negative finite energy function.
  Our approach relies on the fine properties of plurisubharmonic envelopes,
which allow a partial sublinearization of the nonlinear problem. As far as we
know, this method is new, even in the linear case, and not only yields new
results but also significantly simplifies existing arguments in the literature.
Moreover, it extends naturally to the setting of complex Hessian operators.
  Finally, by translating our results from the complex Monge-Amp\`ere setting
via a logarithmic transformation, we also obtain several interesting analogues
for the real Monge-Amp\`ere operator.

</details>


<div id='physics.acc-ph'></div>

# physics.acc-ph [[Back]](#toc)

### [41] [A Supervised Machine Learning Framework for Multipactor Breakdown Prediction in High-Power Radio Frequency Devices and Accelerator Components: A Case Study in Planar Geometry](https://arxiv.org/abs/2507.17881)
*Asif Iqbal,John Verboncoeur,Peng Zhang*

Main category: physics.acc-ph

TL;DR: Supervised ML models predict multipactor susceptibility in RF devices, with tree-based methods outperforming MLPs. Dataset limitations highlight the need for broader coverage.


<details>
  <summary>Details</summary>
Motivation: Accurate prediction of multipactor susceptibility is critical but computationally intensive, prompting the use of ML for faster, data-driven solutions.

Method: Simulation-derived data trained regression models (RF, ET, XGBoost, MLPs) to predict electron growth rate, evaluated using IoU, SSIM, and Pearson correlation.

Result: Tree-based models generalize better than MLPs. MLPs with combined IoU and SSIM objectives perform best after hyperparameter optimization.

Conclusion: ML shows promise for multipactor prediction but requires broader datasets to address feature-space limitations, aiding RF and accelerator design.

Abstract: Multipactor is a nonlinear electron avalanche phenomenon that can severely
impair the performance of high-power radio frequency (RF) devices and
accelerator systems. Accurate prediction of multipactor susceptibility across
different materials and operational regimes remains a critical yet
computationally intensive challenge in accelerator component design and RF
engineering. This study presents the first application of supervised machine
learning (ML) for predicting multipactor susceptibility in two-surface planar
geometries. A simulation-derived dataset spanning six distinct secondary
electron yield (SEY) material profiles is used to train regression models -
including Random Forest (RF), Extra Trees (ET), Extreme Gradient Boosting
(XGBoost), and funnel-structured Multilayer Perceptrons (MLPs) - to predict the
time-averaged electron growth rate, ${\delta}_{avg}$. Performance is evaluated
using Intersection over Union (IoU), Structural Similarity Index (SSIM), and
Pearson correlation coefficient. Tree-based models consistently outperform MLPs
in generalizing across disjoint material domains. MLPs trained using a
scalarized objective function that combines IoU and SSIM during Bayesian
hyperparameter optimization with 5-fold cross-validation outperform those
trained with single-objective loss functions. Principal Component Analysis
reveals that performance degradation for certain materials stems from disjoint
feature-space distributions, underscoring the need for broader dataset
coverage. This study demonstrates both the promise and limitations of ML-based
multipactor prediction and lays the groundwork for accelerated, data-driven
modeling in advanced RF and accelerator system design.

</details>


### [42] [Advanced Ceramic Plasma Discharge Capillaries for high repetition rate operation](https://arxiv.org/abs/2507.18226)
*Lucio Crincoli,Romain Demitra,Valerio Lollo,Donato Pellegrini,Marco Pitti,Lucilla Pronti,Martina Romani,Massimo Ferrario,Angelo Biagioni*

Main category: physics.acc-ph

TL;DR: The paper presents an innovative ceramic-based design for plasma discharge capillaries, enabling high repetition rates and longevity, validated through experiments and simulations.


<details>
  <summary>Details</summary>
Motivation: Future applications of plasma-based particle accelerators require high repetition rates and material durability, especially for gas-filled plasma discharge capillaries.

Method: Experimental campaigns at 10-150 Hz and numerical simulations were conducted to assess the longevity and heat transfer of ceramic capillaries.

Result: Ceramic capillaries maintained plasma properties and source integrity at high repetition rates (100-400 Hz), suitable for the EuPRAXIA@SPARC_LAB project.

Conclusion: The ceramic capillary design is viable for high-repetition-rate plasma-based accelerators, meeting project requirements.

Abstract: In view of future applications of plasma-based particle accelerators, within
the fields of high-energy physics and new light sources, the capability of
plasma sources to operate at high repetition rates is crucial. In particular
for gas-filled plasma discharge capillaries, which allow direct control over
plasma properties, a key aspect is the longevity of the material, subject to
erosion due to the heat flux delivered by high voltage plasma discharges. In
this regard, we present an innovative design of discharge capillaries based on
the use of different ceramic materials, which can sustain high voltage plasma
discharges at high repetition rate and, moreover, be easily machined for the
complex geometries required for plasma-based accelerators. Experimental
campaigns are carried out at 10-150 Hz, assessing the longevity of ceramic
capillaries by means of different diagnostic techniques. In addition, numerical
simulations are performed to analyze the heat transfer within the whole plasma
source. Results from experimental and numerical analysis highlight the
capability of ceramic capillaries to preserve plasma properties and the
integrity of the source during long-term plasma discharge operation at high
repetition rate. In particular, we demonstrated the suitability of the proposed
solution for the operative range of 100-400 Hz, foreseen for EuPRAXIA@SPARC_LAB
project.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [43] [Fourier Neural Operators for Non-Markovian Processes:Approximation Theorems and Experiments](https://arxiv.org/abs/2507.17887)
*Wonjae Lee,Taeyoung Kim,Hyungbin Park*

Main category: cs.LG

TL;DR: The paper introduces MFNO, a neural operator for stochastic systems, extending FNO with mirror padding for non-periodic inputs. It theoretically and empirically outperforms baselines like LSTMs and DeepONet.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing architectures in handling non-periodic inputs and approximating stochastic dynamics accurately.

Method: Extends FNO with mirror padding, leverages Wong--Zakai theorems and approximation techniques for theoretical guarantees.

Result: MFNO achieves strong resolution generalization and faster sample path generation, outperforming LSTMs, TCNs, and DeepONet.

Conclusion: MFNO is a robust and efficient solution for learning stochastic dynamics, with theoretical and empirical advantages over existing methods.

Abstract: This paper introduces an operator-based neural network, the mirror-padded
Fourier neural operator (MFNO), designed to learn the dynamics of stochastic
systems. MFNO extends the standard Fourier neural operator (FNO) by
incorporating mirror padding, enabling it to handle non-periodic inputs. We
rigorously prove that MFNOs can approximate solutions of path-dependent
stochastic differential equations and Lipschitz transformations of fractional
Brownian motions to an arbitrary degree of accuracy. Our theoretical analysis
builds on Wong--Zakai type theorems and various approximation techniques.
Empirically, the MFNO exhibits strong resolution generalization--a property
rarely seen in standard architectures such as LSTMs, TCNs, and DeepONet.
Furthermore, our model achieves performance that is comparable or superior to
these baselines while offering significantly faster sample path generation than
classical numerical schemes.

</details>


### [44] [Low-rank adaptive physics-informed HyperDeepONets for solving differential equations](https://arxiv.org/abs/2507.18346)
*Etienne Zeudong,Elsa Cardoso-Bihlo,Alex Bihlo*

Main category: cs.LG

TL;DR: PI-LoRA-HyperDeepONets use low-rank adaptation to reduce complexity and improve performance over HyperDeepONets.


<details>
  <summary>Details</summary>
Motivation: To address the high memory and computational costs of HyperDeepONets while maintaining expressivity.

Method: Leverages low-rank adaptation (LoRA) to decompose the hypernetwork's output layer weight matrix into smaller low-rank matrices.

Result: Achieves up to 70% parameter reduction and better predictive accuracy and generalization.

Conclusion: PI-LoRA-HyperDeepONets offer a more efficient and effective alternative to HyperDeepONets in physics-informed machine learning.

Abstract: HyperDeepONets were introduced in Lee, Cho and Hwang [ICLR, 2023] as an
alternative architecture for operator learning, in which a hypernetwork
generates the weights for the trunk net of a DeepONet. While this improves
expressivity, it incurs high memory and computational costs due to the large
number of output parameters required. In this work we introduce, in the
physics-informed machine learning setting, a variation, PI-LoRA-HyperDeepONets,
which leverage low-rank adaptation (LoRA) to reduce complexity by decomposing
the hypernetwork's output layer weight matrix into two smaller low-rank
matrices. This reduces the number of trainable parameters while introducing an
extra regularization of the trunk networks' weights. Through extensive
experiments on both ordinary and partial differential equations we show that
PI-LoRA-HyperDeepONets achieve up to 70\% reduction in parameters and
consistently outperform regular HyperDeepONets in terms of predictive accuracy
and generalization.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [45] [On zero-order consistency residue and background pressure for the conservative SPH fluid dynamics](https://arxiv.org/abs/2507.18210)
*Feng Wang,Xiangyu Hu*

Main category: physics.flu-dyn

TL;DR: The paper addresses the zero-order consistency issue in SPH, linking it to numerical damping in pressure-driven and gravity-driven flows, and proposes the reverse kernel gradient correction to mitigate it.


<details>
  <summary>Details</summary>
Motivation: To understand and mitigate the non-physical numerical damping in SPH caused by zero-order gradient consistency residue, particularly in pressure-driven and gravity-driven flows.

Method: Theoretical analysis and numerical experiments are conducted to study the residue's behavior. The reverse kernel gradient correction technique is introduced and tested.

Result: The residue causes damping in flows; the correction technique helps but has limitations. The FDA nozzle test shows the need for corrections in high-pressure scenarios.

Conclusion: The zero-order consistency residue is a key issue in SPH, and correction schemes are necessary, especially in complex or high-pressure flows.

Abstract: As one of the major challenges for the conservative smoothed particle
hydrodynamics (SPH) method, the zero-order consistency issue, although thought
to be mitigated by the particle regularization scheme, such as the transport
velocity formulation, significantly damps the flow in a long channel for both
laminar and turbulent simulations. Building on this finding, this paper not
only thoroughly analyzes the damping reason in this pressure-driven channel
flow, but also relates this problem with the excessive numerical dissipation in
the gravity-driven free-surface flow. The common root cause of the non-physical
numerical damping in the two typical flow scenarios, the zero-order gradient
consistency residue, is exposed. The adverse influence of the background
pressure on the residue for the two scenarios is revealed and discussed. To
comprehensively understand the behavior of the residue and mitigate its
potential adverse effects, we conduct both theoretical analysis and numerical
experiments focusing on the key sensitive factors. For studying the
residue-induced non-physical energy dissipation in the gravity-driven
free-surface flow, the water depth and input dynamic pressure in the inviscid
standing wave case are tested. To investigate the velocity loss in the
pressure-driven channel flow, we examine the effects of the channel length,
resolution, and outlet pressure. The state-of-the-art reverse kernel gradient
correction technique is introduced for the two typical flows, and proved to be
effective in reducing the residue effect, but we find its correction capability
is fundamentally limited. Finally, the FDA nozzle, an engineering benchmark, is
tested to demonstrate the residue influence in a complex geometry, highlighting
the necessity of correction schemes in scenarios with unavoidable high
background pressure.

</details>


<div id='astro-ph.GA'></div>

# astro-ph.GA [[Back]](#toc)

### [46] [Numerical Study of Bar Suppression in Galaxy Models Due to Disc Heating](https://arxiv.org/abs/2507.18083)
*Alejandro López Gómez,Ruslan Gabbasov,Isaura Luisa Fuentes-Carrera*

Main category: astro-ph.GA

TL;DR: The paper examines how the softening parameter (ε) and disc mass fraction (m_d) influence bar formation in galaxies via N-body simulations, revealing complex interactions and numerical heating effects.


<details>
  <summary>Details</summary>
Motivation: Understanding the controversial dynamics of bar formation, evolution, and destruction in galaxies, particularly how physical and numerical parameters like ε and m_d affect these processes.

Method: Conducted N-body simulations with varying particle resolutions to study the combined effects of ε and m_d on bar formation in isolated disc-halo models.

Result: Bar strength depends on m_d, but ε interacts to modify bar formation. Numerical heating dominates for small ε, suppressing bars. High-resolution models show ε's critical role, while m_d affects acceleration profiles.

Conclusion: The study highlights the interplay of ε and m_d in bar dynamics, with numerical heating as a key factor, and suggests vertical acceleration profiles as indicators of numerical effects.

Abstract: The process of bar formation, evolution and destruction is still a
controversial topic regarding galaxy dynamics. Numerical simulations show that
these phenomena strongly depend on physical and numerical parameters. In this
work, we study the combined influence of the softening parameter, $\epsilon$
and disc mass fraction, $m_{\mathrm{d}}$ on the formation and evolution of bars
in isolated disc-halo models via $N$-body simulations with different particle
resolutions. Previous studies indicate that the bar strength depends on
$m_{\mathrm{d}}$ as $\propto m_{\mathrm{d}}^{-1}$, which is seen as a delay in
bar formation. However, the distorsion parameter, $\eta$, which measures the
bar's momentum through time, shows that an increase in $m_{\mathrm{d}}$ does
not always induce a delay in bar formation. This suggests that $\epsilon$
interact to either enhance or weaken the bar. Moreover, numerical heating
dominates in models with small softening values, creating highly accelerated
particles at the centre of discs, regardless of $m_{\mathrm{d}}$ or resolution.
These enhanced particle accelerations produce chaotic orbits for $\epsilon \leq
5\,$pc, resulting in bar suppression due to collisional dynamics in the centre.
In our high resolution models ($N \approx 10^{7}$), small softening values are
incapable of reproducing the bar instability. The role of disc mass is as
follows: increasing $m_{\mathrm{d}}$ for moderate $\epsilon$ ($\geq 10\,$pc)
reduces the amount of drift in the acceleration profile, without affecting the
bar's behaviour. Models with lower $m_{\mathrm{d}}$ values coupled with small
softening values, have an excess of highly accelerated particles, introducing
unwanted effects into otherwise reliable simulations. Finally, we show that the
evolution of the disc's vertical acceleration profile is a reliable indicator
of numerical heating introduced by $\epsilon$ and the bar.

</details>


<div id='nlin.CD'></div>

# nlin.CD [[Back]](#toc)

### [47] [Toda lattice formed in nonequilibrium steady states of SWCNT](https://arxiv.org/abs/2507.18412)
*Heeyuen Koh,Shigeo Maruyama*

Main category: nlin.CD

TL;DR: The paper connects thermal conductivity in nanoscale low-dimensional systems to Toda lattice dynamics, using a coarse-grained molecular dynamics model derived from nonequilibrium simulations.


<details>
  <summary>Details</summary>
Motivation: To explain the length dependency of high thermal conductivity in low-dimensional systems by linking nonequilibrium steady states to Toda lattice equilibrium dynamics.

Method: A coarse-grained molecular dynamics (CGMD) model is developed from nonequilibrium molecular dynamics (NEMD) data, incorporating longitudinal and flexural modulation as a Hamiltonian with a perturbation term.

Result: The model shows that the potential energy function in nonequilibrium states matches the Toda lattice under specific conditions, confirmed by numerical data.

Conclusion: The hypothesis successfully bridges nonequilibrium thermal conductivity with equilibrium Toda lattice dynamics, validated by numerical simulations.

Abstract: Toda lattice or FPUT chain-like dynamics have been regarded as the
prerequisite condition to explain the length dependency of high thermal
conductivity of low-dimensional systems at the nanoscale. In this paper, a
hypothetical condition is introduced that establishes a theoretical connection
between the thermal conductivity of a nanoscale low-dimensional system in
nonequilibrium steady states(NESS) and the canonical motion of the equation in
the Toda lattice in equilibrium. The hypothesis relies on a numerically driven
coarse grained molecular dynamics system acquired from the trajectory data of
nonequilibrium molecular dynamics(NEMD) simulation. It models the macroscopic
motion from longitudinal and flexural modulation observed in NEMD as a separate
Hamiltonian in CGMD with a perturbation term governed by an overdamping
process, which is assumed to be dominant during heat transfer. The Smoluchowski
equation for the perturbation, which is derived from the cross correlated
states between two degrees of freedom, suggests that the potential energy
function induced from NESS is identical to that of Toda Lattice under the
specific condition in the partition function for CG particles. The restrictions
derived from the model are well confirmed by the data from the numerically
driven CG model.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [48] [Stability of Big Bang singularity for the Einstein-Maxwell-scalar field-Vlasov system in the full strong sub-critical regime](https://arxiv.org/abs/2507.18585)
*Xinliang An,Taoran He,Dawei Shen*

Main category: gr-qc

TL;DR: Study of Kasner solutions' stability in $3+1$ dimensions for the Einstein-Maxwell-scalar field-Vlasov system, identifying a new sub-critical regime and proving nonlinear stability.


<details>
  <summary>Details</summary>
Motivation: To extend stability results from simpler systems to the more complex Einstein-Maxwell-scalar field-Vlasov system, addressing challenges posed by the Vlasov field.

Method: Detailed mathematical analysis and new arguments to identify a strong sub-critical regime.

Result: Nonlinear stability proven for Kasner exponents in the identified regime, extending prior work.

Conclusion: The study successfully generalizes stability results to a more complex physical system, overcoming challenges introduced by the Vlasov field.

Abstract: In $3+1$ dimensions, we study the stability of Kasner solutions for the
Einstein-Maxwell-scalar field-Vlasov system. This system incorporates gravity,
electromagnetic, weak and strong interactions for the initial stage of our
universe. Due to the presence of the Vlasov field, various new challenges
arise. By observing detailed mathematical structures and designing new delicate
arguments, we identify a new strong sub-critical regime and prove the nonlinear
stability with Kasner exponents lying in this full regime. This extends the
result of Fournodavlos-Rodnianski-Speck [8] from the Einstein-scalar field
system to the physically more complex system with the Vlasov field.

</details>


<div id='hep-th'></div>

# hep-th [[Back]](#toc)

### [49] [Analytic Regression of Feynman Integrals from High-Precision Numerical Sampling](https://arxiv.org/abs/2507.17815)
*Oscar Barrera,Aurélien Dersy,Rabia Husain,Matthew D. Schwartz,Xiaoyuan Zhang*

Main category: hep-th

TL;DR: A method combining high-precision numerical integration and analytic knowledge of function spaces is proposed to deduce exact analytical results, demonstrated with Feynman integrals.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of obtaining exact analytical descriptions from numerical data, especially in cases like Feynman integrals where approximations are insufficient.

Method: Uses high-precision numerical integration and lattice reduction, leveraging known function spaces to deduce exact answers.

Result: Demonstrates successful exact computation of Feynman integrals, balancing data points, precision, and computational resources.

Conclusion: The method offers a bottom-up approach complementary to top-down analytic methods, applicable broadly where exact answers are needed and function spaces are understood.

Abstract: In mathematics or theoretical physics one is often interested in obtaining an
exact analytic description of some data which can be produced, in principle, to
arbitrary accuracy. For example, one might like to know the exact analytical
form of a definite integral. Such problems are not well-suited to numerical
symbolic regression, since typical numerical methods lead only to
approximations. However, if one has some sense of the function space in which
the analytic result should lie, it is possible to deduce the exact answer by
judiciously sampling the data at a sufficient number of points with sufficient
precision. We demonstrate how this can be done for the computation of Feynman
integrals. We show that by combining high-precision numerical integration with
analytic knowledge of the function space one can often deduce the exact answer
using lattice reduction. A number of examples are given as well as an
exploration of the trade-offs between number of datapoints, number of
functional predicates, precision of the data, and compute. This method provides
a bottom-up approach that neatly complements the top-down Landau-bootstrap
approach of trying to constrain the exact answer using the analytic structure
alone. Although we focus on the application to Feynman integrals, the
techniques presented here are more general and could apply to a wide range of
problems where an exact answer is needed and the function space is sufficiently
well understood.

</details>


<div id='math.CO'></div>

# math.CO [[Back]](#toc)

### [50] [Guessing sequences of eigenvectors for LMPs defining spectrahedral relaxations of Eulerian rigidly convex sets](https://arxiv.org/abs/2507.18434)
*Alejandro González Nevado*

Main category: math.CO

TL;DR: The paper improves bounds for extreme roots of univariate Eulerian polynomials, enhancing accuracy in spectrahedral relaxations for Eulerian rigidly convex sets.


<details>
  <summary>Details</summary>
Motivation: To refine the accuracy of spectrahedral relaxations for Eulerian rigidly convex sets by improving bounds for extreme roots of Eulerian polynomials.

Method: Uses numerical experiments to construct a sequence of vectors, linearizing bounds to outperform previous results.

Result: Achieves a growing exponential difference with prior bounds, improving diagonal accuracy measures.

Conclusion: The new linearized bound significantly enhances the accuracy of spectrahedral relaxations for Eulerian rigidly convex sets.

Abstract: Stable multivariate Eulerian polynomials were introduced by Br\"and\'en.
Particularizing some variables, it is possible to extract real zero
multivariate Eulerian polynomials from them. These real zero multivariate
Eulerian polynomials can be fed into constructions of spectrahedral relaxations
providing therefore approximations to the (Eulerian) rigidly convex sets
defined by these polynomials. The accuracy of these approximations is measured
through the behaviour in the diagonal, where the usual univariate Eulerian
polynomials sit. In particular, in this sense, the accuracy of the global
spectrahedral approximation produced by the spectrahedral relaxation can be
measured in terms of bounds for the extreme roots of univariate Eulerian
polynomials. The bounds thus obtained beat the previous bounds found in the
literature. However, the bound explicitly studied and obtained before beat the
previously known bounds by a quantity going to $0$ when $n$ goes to infinity.
Here we use numerical experiments to construct a sequence of vectors providing
a (linearized) bound whose difference with the previous known bounds is a
growing exponential function (going therefore fast to infinity when $n$ grows).
This allows us to establish a better (diagonal) measure of accuracy for the
spectrahedral relaxation of the Eulerian rigidly convex sets. In particular, we
will achieve this by linearizing through the sequence of vectors
$\{(y,(-2^{m-i})_{i=3}^{m},(0,\frac{1}{2}),(1)_{i=1}^{m})\in\mathbb{R}^{n+1}\}_{n=1}^{\infty}$
for even $n=2m$.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [51] [Jacobi Hamiltonian Integrators](https://arxiv.org/abs/2507.18573)
*Adérito Araújo,Gonçalo Inocêncio Oliveira,João Nuno Mestre*

Main category: math.DG

TL;DR: A method for constructing structure-preserving integrators for Hamiltonian systems in Jacobi manifolds is proposed, extending techniques from Poisson systems to handle time-dependent and dissipative phenomena.


<details>
  <summary>Details</summary>
Motivation: Jacobi manifolds generalize symplectic and Poisson structures, making them suitable for modeling dissipative and thermodynamic systems. Existing methods like Poisson Hamiltonian Integrators (PHI) need extension to Jacobi frameworks.

Method: The approach leverages the correspondence between Jacobi and homogeneous Poisson manifolds, extending PHI techniques while preserving homogeneity. Theoretical tools and numerical integration methods are developed.

Result: The paper provides a theoretical foundation and numerical technique for structure-preserving integration in Jacobi dynamics, focusing on homogeneous Poisson perspectives.

Conclusion: The method offers a clear pathway for integrating time-dependent and dissipative systems within the Jacobi framework, preserving key structural properties.

Abstract: We develop a method of constructing structure-preserving integrators for
Hamiltonian systems in Jacobi manifolds. Hamiltonian mechanics, rooted in
symplectic and Poisson geometry, has long provided a foundation for modelling
conservative systems in classical physics. Jacobi manifolds, generalizing both
contact and Poisson manifolds, extend this theory and are suitable for
incorporating time-dependent, dissipative and thermodynamic phenomena.
  Building on recent advances in geometric integrators - specifically Poisson
Hamiltonian Integrators (PHI), which preserve key features of Poisson systems -
we propose a construction of Jacobi Hamiltonian Integrators. Our approach
explores the correspondence between Jacobi and homogeneous Poisson manifolds,
with the aim of extending the PHI techniques while ensuring preservation of the
homogeneity structure.
  This work develops the theoretical tools required for this generalization and
outlines a numerical integration technique compatible with Jacobi dynamics. By
focusing on the homogeneous Poisson perspective rather than on direct contact
realizations, we provide a clear pathway for structure-preserving integration
of time-dependent and dissipative systems within the Jacobi framework.

</details>


### [52] [Constant mean curvature Radial graphs over domains of $\mathbb{S}^n$](https://arxiv.org/abs/2507.18496)
*Flávio Cruz,José T. Cruz,Jocel Oliveira*

Main category: math.DG

TL;DR: Existence of hypersurfaces with constant mean curvature and prescribed boundaries in Euclidean space, extending Serrin's result to positive mean curvature.


<details>
  <summary>Details</summary>
Motivation: To generalize Serrin's classical result to include hypersurfaces with positive constant mean curvature, addressing cases where the boundary's mean curvature is positive and a subsolution exists.

Method: Representing hypersurfaces as radial graphs over domains of the unit sphere and solving the associated Dirichlet problem under given assumptions.

Result: Demonstrates the existence of such hypersurfaces under the specified conditions.

Conclusion: The study successfully extends Serrin's result to positive constant mean curvature, providing a broader framework for analyzing hypersurfaces with prescribed boundaries.

Abstract: We establish the existence of hypersurfaces with constant mean curvature and
a prescribed boundary in Euclidean space, represented as radial graphs over
domains of the unit sphere. Under the assumptions that the mean curvature of
the domain's boundary is positive and that a subsolution exists for the
associated Dirichlet problem, we extend Serrin's classical result to include
the case of positive constant mean curvature.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [53] [Shallow quantum circuit for generating O(1)-entanged approximate state designs](https://arxiv.org/abs/2507.17871)
*Wonjun Lee,Minki Hhan,Gil Young Cho,Hyukjoon Kwon*

Main category: quant-ph

TL;DR: A new ensemble of quantum states with low entanglement, magic, and coherence is introduced, serving as an ε-approximate state t-design. These states achieve theoretical lower bounds for resources and are efficiently generated via shallow quantum circuits.


<details>
  <summary>Details</summary>
Motivation: To address the need for efficient generation of random quantum states with minimal resources for applications in quantum cryptography, simulation, and device benchmarking.

Method: Develops an algorithm to transform k-qubit approximate state designs into n-qubit ones using multi-controlled gates, ensuring shallow circuit depth without ancilla qubits.

Result: The constructed quantum circuits have depth O(t [log t]³ log n log(1/ε)), the most efficient among ancilla-free methods, and enable classical simulation of random states.

Conclusion: The proposed states and circuits reduce resource requirements and improve efficiency, with demonstrated utility in classical shadow tomography for faster runtime.

Abstract: Random quantum states have various applications in quantum information
science, including quantum cryptography, quantum simulation, and benchmarking
quantum devices. In this work, we discover a new ensemble of quantum states
that serve as an $\epsilon$-approximate state $t$-design while possessing
extremely low entanglement, magic, and coherence. We show that those resources
such quantum states can reach their theoretical lower bounds, $\Omega\left(\log
(t/\epsilon)\right)$, which are also proven in this work. This implies that for
fixed $t$ and $\epsilon$, those resources do not scale with the system size,
i.e., $O(1)$ with respect to the total number of qubits $n$ in the system.
Moreover, we explicitly construct an ancilla-free shallow quantum circuit for
generating such states. To this end, we develop an algorithm that transforms
$k$-qubit approximate state designs into $n$-qubit ones through a sequence of
multi-controlled gates, without increasing the support size. The depth of such
a quantum circuit is $O\left(t [\log t]^3 \log n \log(1/\epsilon)\right)$,
which is the most efficient among existing algorithms without ancilla qubits. A
class of shallow quantum circuits proposed in our work offers reduced cost for
classical simulation of random quantum states, leading to potential
applications in various quantum information processing tasks. As a concrete
example for demonstrating utility of our algorithm, we propose classical shadow
tomography using an $O(1)$-entangled estimator, which can achieve shorter
runtime compared to conventional schemes.

</details>


### [54] [Stability of Continuous Time Quantum Walks in Complex Networks](https://arxiv.org/abs/2507.17880)
*Adithya L J,Johannes Nokkala,Jyrki Piilo,Chandrakala Meena*

Main category: quant-ph

TL;DR: The paper examines how network topology and decoherence models affect the stability of continuous time quantum walks, finding heterogeneous networks like star and scale-free topologies most stable.


<details>
  <summary>Details</summary>
Motivation: To understand how different network structures and decoherence mechanisms influence the preservation of quantum properties over time.

Method: Study CTQWs in various network topologies (cycle, complete, Erdős-Rényi, small-world, scale-free, star) under intrinsic decoherence, Haken-Strobl noise, and QSWs, using metrics like coherence norms and entropy.

Result: Heterogeneous networks (star, scale-free) are most stable; intrinsic decoherence preserves coherence longest, while QSWs cause rapid loss. Local topology features like node centrality also impact stability.

Conclusion: Network topology and decoherence model interplay determines quantum stability, with heterogeneous networks and intrinsic decoherence offering the best coherence preservation.

Abstract: We investigate the stability of continuous time quantum walks (CTQWs) in a
range of network topologies under different decoherence mechanisms, defining
stability as the system's ability to preserve quantum properties over time. The
networks studied range from homogeneous to heterogeneous structures, including
cycle, complete, Erd\H{o}s-R\'enyi, small-world, scale-free, and star
topologies. The decoherence models considered are intrinsic decoherence,
Haken-Strobl noise, and quantum stochastic walks (QSWs). To assess quantum
stability, we employ several metrics: node occupation probabilities, the
$\ell_1$-norm of coherence, fidelity with the initial state, quantum-classical
distance, and von Neumann entropy. Our results reveal that the interplay of
both network topology and decoherence model influences coherence preservation.
Intrinsic decoherence results in the slowest decay of coherence, followed by
Haken-Strobl noise, while QSW causes the most rapid loss of coherence. The
stability ranking among network topologies varies depending on the decoherence
model and quantifier used. For example, under Haken-Strobl and intrinsic
decoherence, the quantum-classical distance ranks the cycle network more stable
than scale-free networks, although other metrics consistently favour scale-free
topologies. In general, heterogeneous networks, such as star and scale-free
networks, exhibit the highest stability, whereas homogeneous topologies, such
as cycle and Erd\H{o}s-R\'enyi networks, are more vulnerable to decoherence.
The complete graph, despite its homogeneity, remains highly stable due to its
dense connectivity. Furthermore, in heterogeneous networks, the centrality of
the initialised node, measured by degree or closeness, has a pronounced impact
on stability, underscoring the role of local topological features in quantum
dynamics.

</details>


### [55] [Advancing the hBN Defects Database through Photophysical Characterization of Bulk hBN](https://arxiv.org/abs/2507.18093)
*Chanaprom Cholsuk,Sujin Suwanna,Tobias Vogl*

Main category: quant-ph

TL;DR: The paper presents a database of bulk hBN defects with detailed photophysical properties, aiming to bridge the gap between theory and experiment in quantum emitter research.


<details>
  <summary>Details</summary>
Motivation: Address discrepancies between theoretical (monolayer) and experimental (bulk) studies of hBN defects by providing a comprehensive bulk defect database.

Method: Systematically evaluated over 120 neutral defects across charge states (-2 to 2), computing properties like zero-phonon line, photoluminescence, and electron-phonon coupling.

Result: Vacancies strongly influence electron-phonon coupling, and the HR factor correlates with configuration coordinates. Data is publicly available with an API for machine learning.

Conclusion: The database aids in reliable quantum emitter identification and supports machine-learning-driven quantum materials research.

Abstract: Quantum emitters in hexagonal boron nitride (hBN) have gained significant
attention due to a wide range of defects that offer high quantum efficiency and
single-photon purity at room temperature. Most theoretical studies on hBN
defects simulate monolayers, as this is computationally cheaper than
calculating bulk structures. However, most experimental studies are carried out
on multilayer to bulk hBN, which creates additional possibilities for
discrepancies between theory and experiment. In this work, we present an
extended database of hBN defects that includes a comprehensive set of bulk hBN
defects along with their excited-state photophysical properties. The database
features over 120 neutral defects, systematically evaluated across charge
states ranging from -2 to 2 (600 defects in total). For each defect, the most
stable charge and spin configurations are identified and used to compute the
zero-phonon line, photoluminescence spectrum, absorption spectrum, Huang-Rhys
(HR) factor, interactive radiative lifetimes, transition dipole moments, and
polarization characteristics. Our analysis reveals that the electron-phonon
coupling strength is primarily influenced by the presence of vacancies, which
tend to induce stronger lattice distortions and broaden phonon sidebands.
Additionally, correlation analysis shows that while most properties are
independent, the HR factor strongly correlates with the configuration
coordinates. All data are publicly available at https://h-bn.info, along with a
new application programming interface (API) to facilitate integration with
machine learning workflows. This database is therefore designed to bridge the
gap between theory and experiment, aid in the reliable identification of
quantum emitters, and support the development of machine-learning-driven
approaches in quantum materials research.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [56] [Deep Variational Free Energy Calculation of Hydrogen Hugoniot](https://arxiv.org/abs/2507.18540)
*Zihang Li,Hao Xie,Xinyang Dong,Lei Wang*

Main category: cond-mat.str-el

TL;DR: A deep variational free energy framework using three generative models computes hydrogen's equation of state in warm dense matter, resolving discrepancies in deuterium Hugoniot data.


<details>
  <summary>Details</summary>
Motivation: To address discrepancies in theoretical and experimental results for hydrogen's equation of state in the warm dense matter region.

Method: Uses three deep generative models: normalizing flow for nuclei, autoregressive transformer for excited electrons, and permutational equivariant flow for Hartree-Fock electrons, optimized to minimize variational free energy.

Result: Provides the equation of state and thermodynamic properties of dense hydrogen, benchmarking deuterium in warm dense matter.

Conclusion: The framework successfully resolves discrepancies and offers a reliable benchmark for deuterium in warm dense matter.

Abstract: We develop a deep variational free energy framework to compute the equation
of state of hydrogen in the warm dense matter region. This method parameterizes
the variational density matrix of hydrogen nuclei and electrons at finite
temperature using three deep generative models: a normalizing flow model that
represents the Boltzmann distribution of the classical nuclei, an
autoregressive transformer that models the distribution of electrons in excited
states, and a permutational equivariant flow model that constructs backflow
coordinates for electrons in Hartree-Fock orbitals. By jointly optimizing the
three neural networks to minimize the variational free energy, we obtain the
equation of state and related thermodynamic properties of dense hydrogen. We
compare our results with other theoretical and experimental results on the
deuterium Hugoniot curve, aiming to resolve existing discrepancies. The
calculated results provide a valuable benchmark for deuterium in the warm dense
matter region.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [57] [Data assimilation with the 2D Navier-Stokes equations: Optimal Gaussian asymptotics for the posterior measure](https://arxiv.org/abs/2507.18279)
*Dimiri Konen,Richard Nickl*

Main category: math.ST

TL;DR: The paper proves a functional Bernstein-von Mises theorem for posterior measures in a data assimilation problem with the 2D Navier-Stokes equation, showing Gaussian approximation of the posterior and its implications for uncertainty quantification.


<details>
  <summary>Details</summary>
Motivation: To understand the approximation of posterior measures in data assimilation for the Navier-Stokes equation and its implications for prediction and uncertainty quantification.

Method: Assigns a Gaussian process prior to the initial condition, analyzes the posterior measure, and approximates it with a Gaussian random vector field from a linear parabolic PDE.

Result: The posterior is strongly approximated by a Gaussian field, enabling $1/\sqrt{N}$-consistent estimators for future state prediction. Credible bands and uncertainty quantification are discussed.

Conclusion: The Bayesian data assimilation algorithm attains the local asymptotic minimax bound, validating its efficiency for estimating the state of nonlinear systems.

Abstract: A functional Bernstein - von Mises theorem is proved for posterior measures
arising in a data assimilation problem with the two-dimensional Navier-Stokes
equation where a Gaussian process prior is assigned to the initial condition of
the system. The posterior measure, which provides the update in the space of
all trajectories arising from a discrete sample of the (deterministic)
dynamics, is shown to be approximated by a Gaussian random vector field arising
from the solution to a linear parabolic PDE with Gaussian initial condition.
The approximation holds in the strong sense of the supremum norm on the
regression functions, showing that predicting future states of Navier-Stokes
systems admits $1/\sqrt N$-consistent estimators even for commonly used
nonparametric models. Consequences for coverage of credible bands and
uncertainty quantification are discussed. A local asymptotic minimax theorem is
derived that describes the lower bound for estimating the state of the
nonlinear system, which is shown to be attained by the Bayesian data
assimilation algorithm.

</details>
