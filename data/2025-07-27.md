<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 8]
- [math.AP](#math.AP) [Total: 20]
- [physics.comp-ph](#physics.comp-ph) [Total: 6]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 4]
- [cs.LG](#cs.LG) [Total: 2]
- [quant-ph](#quant-ph) [Total: 3]
- [astro-ph.GA](#astro-ph.GA) [Total: 1]
- [nlin.CD](#nlin.CD) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 1]
- [math.DG](#math.DG) [Total: 2]
- [math.CO](#math.CO) [Total: 1]
- [math.CV](#math.CV) [Total: 2]
- [gr-qc](#gr-qc) [Total: 1]
- [math.ST](#math.ST) [Total: 1]
- [hep-th](#hep-th) [Total: 1]
- [physics.acc-ph](#physics.acc-ph) [Total: 2]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Recovery Thresholding Hyperinterpolations in Signal Processing](https://arxiv.org/abs/2507.17916)
*Congpei An,Jiashu Ran*

Main category: math.NA

TL;DR: The paper introduces recovery thresholding hyperinterpolations for sparse signal reconstruction, integrating thresholding operators into hyperinterpolation to maintain sparsity. It uses Newton's method for nonconvex optimization and shows robust performance against noise.


<details>
  <summary>Details</summary>
Motivation: To address sparse signal reconstruction in noisy environments by integrating thresholding operators into hyperinterpolation, preserving sparsity and improving recovery accuracy.

Method: Develops a framework combining thresholding operators (hard, springback, Newton) with hyperinterpolation, extending Newton's method to solve multivariable nonconvex regularization problems.

Result: Demonstrates robust performance in reconstructing signals corrupted by Gaussian and impulse noise, outperforming traditional methods in sparsity preservation and recovery accuracy.

Conclusion: The proposed methods are effective for signal reconstruction and denoising, offering advantages in sparsity preservation and noise resilience.

Abstract: This paper introduces recovery thresholding hyperinterpolations, a novel
class of methods for sparse signal reconstruction in the presence of noise. We
develop a framework that integrates thresholding operators--including hard
thresholding, springback, and Newton thresholding--directly into the
hyperinterpolation structure to maintain sparsity during signal recovery. Our
approach leverages Newton's method to minimize one-dimensional nonconvex
functions, which we then extend to solve multivariable nonconvex regularization
problems. The proposed methods demonstrate robust performance in reconstructing
signals corrupted by both Gaussian and impulse noise. Through numerical
experiments, we validate the effectiveness of these recovery thresholding
hyperinterpolations for signal reconstruction and function denoising
applications, showing their advantages over traditional approaches in
preserving signal sparsity while achieving accurate recovery.

</details>


### [2] [A novel finite element method for simulating surface plasmon polaritons on complex graphene sheets](https://arxiv.org/abs/2507.17928)
*Jichun Li,Michael Neunteufel,Li Zhu*

Main category: math.NA

TL;DR: A simplified graphene model and a new finite element method are developed to accurately simulate surface plasmon polaritons (SPPs) on graphene surfaces, addressing challenges like complex interfaces and boundary conditions.


<details>
  <summary>Details</summary>
Motivation: SPPs offer insights into nano-optical and electrodynamic responses, but their simulation is challenging due to complex material interfaces and boundary conditions.

Method: A simplified graphene model and a new finite element method are proposed, with stability analysis for the continuous model.

Result: Numerical results show the new model effectively captures SPPs for various complex graphene sheets.

Conclusion: The proposed model and method provide an accurate and stable approach for simulating SPPs on graphene surfaces.

Abstract: Surface plasmon polaritons (SPPs) are generated on the graphene surface, and
provide a window into the nano-optical and electrodynamic response of their
host material and its dielectric environment. An accurate simulation of SPPs
presents several unique challenges, since SPPs often occur at complex
interfaces between materials of different dielectric constants and appropriate
boundary conditions at the graphene interfaces are crucial. Here we develop a
simplified graphene model and propose a new finite element method accordingly.
Stability for the continuous model is established, and extensive numerical
results are presented to demonstrate that the new model can capture the SPPs
very well for various complex graphene sheets.

</details>


### [3] [A stabilized Two-Step Formulation of Maxwell's Equations in the time-domain](https://arxiv.org/abs/2507.18235)
*Leon Herles,Mario Mally,Jörg Ostrowski,Sebastian Schöps,Melina Merkel*

Main category: math.NA

TL;DR: The paper extends a stabilized two-step Maxwell's equations formulation to the time-domain, addressing low-frequency instabilities with a generalized tree-cotree gauge, ensuring robustness and accuracy.


<details>
  <summary>Details</summary>
Motivation: Simulating electromagnetic fields across broad frequency ranges is challenging due to numerical instabilities at low frequencies.

Method: A Galerkin discretization in space is applied with two tailored time-discretization schemes for the two-step solution procedure. A generalized tree-cotree gauge is used to address low-frequency instability.

Result: Numerical results confirm stability, accuracy, and applicability to nonlinear, temperature-dependent materials in 3D problems.

Conclusion: The method effectively stabilizes and extends Maxwell's equations for broad frequency ranges, including the static limit.

Abstract: Simulating electromagnetic fields across broad frequency ranges is
challenging due to numerical instabilities at low frequencies. This work
extends a stabilized two-step formulation of Maxwell's equations to the
time-domain. Using a Galerkin discretization in space, we apply two different
time-discretization schemes that are tailored to the first- and second-order in
time partial differential equations of the two-step solution procedure used
here. To address the low-frequency instability, we incorporate a generalized
tree-cotree gauge that removes the singularity of the curl-curl operator,
ensuring robustness even in the static limit. Numerical results on academic and
application-oriented 3D problems confirm stability, accuracy, and the method's
applicability to nonlinear, temperature-dependent materials.

</details>


### [4] [EigenWave: An Optimal O(N) Method for Computing Eigenvalues and Eigenvectors by Time-Filtering the Wave Equation](https://arxiv.org/abs/2507.18282)
*Daniel Appelo,Jeffrey W. Banks,William D. Henshaw,Ngan Le,Donald W. Schwendeman*

Main category: math.NA

TL;DR: EigenWave is an efficient algorithm for computing eigenvalues and eigenvectors of elliptic boundary value problems using a WaveHoltz-based iteration and matrix-free Arnoldi embedding.


<details>
  <summary>Details</summary>
Motivation: The need for computing eigenvalues anywhere in the spectrum without inverting indefinite matrices motivates the development of EigenWave.

Method: The algorithm iteratively solves a time-dependent wave equation, filters solutions, and embeds the iteration in a matrix-free Arnoldi algorithm for efficiency.

Result: EigenWave achieves linear scaling (O(N)) with grid points, demonstrated in 2D/3D Laplacian problems using second- and fourth-order approximations.

Conclusion: EigenWave provides an optimal, matrix-free approach for eigenvalue computation, validated in complex geometries.

Abstract: An algorithm named EigenWave is described to compute eigenvalues and
eigenvectors of elliptic boundary value problems. The algorithm, based on the
recently developed WaveHoltz scheme, solves a related time-dependent wave
equation as part of an iteration. At each iteration, the solution to the wave
equation is filtered in time. As the iteration progresses, the filtered
solution generally contains relatively larger and larger proportions of
eigenmodes whose eigenvalues are near a chosen target frequency (target
eigenvalue). The ability to choose an arbitrary target frequency enables the
computation of eigenvalues anywhere in the spectrum, without the need to invert
an indefinite matrix, as is common with other approaches. Furthermore, the
iteration can be embedded within a matrix-free Arnoldi algorithm, which enables
the efficient computation of multiple eigenpairs near the target frequency. For
efficiency, the time-dependent wave equation can be solved with implicit
time-stepping and only about $10$ time-steps per-period are needed, independent
of the mesh spacing. When the (definite) implicit time-stepping equations are
solved with a multigrid algorithm, the cost of the resulting EigenWave scheme
scales linearly with the number of grid points $N$ as the mesh is refined,
giving an optimal $O(N)$ algorithm. The approach is demonstrated by finding
eigenpairs of the Laplacian in complex geometry using overset grids. Results in
two and three space dimensions are presented using second-order and
fourth-order accurate approximations.

</details>


### [5] [Parametric design and adaptive sizing of lattice structures for 3d additive manufacturing](https://arxiv.org/abs/2507.18318)
*Jorge Manuel Mercado-Colmenero,Daniel Diaz - Perete,Miguel Angel Rubio- Paramio,Cristina Martin-Donate*

Main category: math.NA

TL;DR: A parametric design model and adaptive mechanical analysis for a new lattice structure in 3D additive manufacturing, optimizing volume, mass, and rigidity.


<details>
  <summary>Details</summary>
Motivation: To enhance additive manufacturing of complex parts by introducing adaptive lattice structures for improved mechanical performance.

Method: Geometric parameterization, mechanical adaptive sizing, and numerical validation using a 2D beam element model.

Result: The lattice structure's topology adapts dynamically to load states, validated by numerical analysis for real mechanical behavior.

Conclusion: The approach advances additive manufacturing by enabling dynamic, efficient lattice structure design tailored to mechanical demands.

Abstract: The present research is developed into the realm of industrial design
engineering and additive manufacturing by introducing a parametric design model
and adaptive mechanical analysis for a new lattice structure, with a focus on
3D additive manufacturing of complex parts. Focusing on the land-scape of
complex parts additive manufacturing, this research proposes geometric
parameterization, mechanical adaptive sizing, and numerical validation of a
novel lattice structure to optimize the final printed part volume and mass, as
well as its structural rigidity. The topology of the lattice structures
exhibited pyramidal geometry. Complete parameterization of the lattice
structure ensures that the known geometric parameters adjust to defined
restrictions, enabling dynamic adaptability based on its load states and
boundary conditions, thereby enhancing its mechanical performance. The core
methodology integrates analytical automation with mechanical analysis by
employing a model based in two-dimensional beam elements. The dimensioning of
the lattice structure is analyzed using rigidity models of its sub-elements,
providing an evaluation of its global structural behavior after applying the
superposition principle. Numerical validation was performed to validate the
proposed analytical model. This step ensures that the analytical model defined
for dimensioning the lattice structure adjusts to its real mechanical behavior
and allows its validation. The present manuscript aims to advance additive
manufacturing methodologies by offering a systematic and adaptive approach to
lattice structure design. Parametric and adaptive techniques foster new
industrial design engineering methods, enabling the dynamic tailoring of
lattice structures to meet their mechanical demands and enhance their overall
efficiency and performance.

</details>


### [6] [On MAP estimates and source conditions for drift identification in SDEs](https://arxiv.org/abs/2507.18443)
*Daniel Tenbrinck,Nikolas Uesseler,Philipp Wacker,Benedikt Wirth*

Main category: math.NA

TL;DR: The paper addresses the inverse problem of identifying drift in an SDE using observations, derives a MAP estimate, and analyzes its properties and convergence rates.


<details>
  <summary>Details</summary>
Motivation: To solve the inverse problem of drift identification in SDEs from observed data, ensuring theoretical and practical validity.

Method: Derives a MAP estimate, proves differentiability and tangential cone conditions for the forward operator, and reviews existing theory.

Result: Numerical simulations in 1D support the theoretical convergence rates for the MAP estimate as observations increase.

Conclusion: The study confirms the validity of the MAP estimate for drift identification in SDEs, with numerical evidence backing theoretical claims.

Abstract: We consider the inverse problem of identifying the drift in an SDE from $n$
observations of its solution at $M+1$ distinct time points. We derive a
corresponding MAP estimate, we prove differentiability properties as well as a
so-called tangential cone condition for the forward operator, and we review the
existing theory for related problems, which under a slightly stronger
tangential cone condition would additionally yield convergence rates for the
MAP estimate as $n\to\infty$. Numerical simulations in 1D indicate that such
convergence rates indeed hold true.

</details>


### [7] [Solution of Least Squares Problems with Randomized Preconditioned Normal Equations](https://arxiv.org/abs/2507.18466)
*Ilse C. F. Ipsen*

Main category: math.NA

TL;DR: The paper explores solving full column-rank least squares problems using randomized preconditioned normal equations, achieving accuracy comparable to QR-based methods even for ill-conditioned matrices.


<details>
  <summary>Details</summary>
Motivation: To improve the accuracy and efficiency of solving least squares problems, especially for ill-conditioned matrices, using randomized preconditioners.

Method: Preconditioning normal equations symmetrically or non-symmetrically with a randomized preconditioner and analyzing perturbation bounds.

Result: Preconditioned normal equations yield solutions as accurate as QR-based methods, with perturbation bounds matching those of the original problem.

Conclusion: Randomized preconditioners with minimal sampling are effective, providing accurate solutions and robust perturbation bounds.

Abstract: We consider the solution of full column-rank least squares problems by means
of normal equations that are preconditioned, symmetrically or
non-symmetrically, with a randomized preconditioner. With an effective
preconditioner, the solutions from the preconditioned normal equations are
almost as accurate as those from the QR-based Matlab backslash (mldivide)
command -- even for highly illconditioned matrices. This means the accuracy of
the preconditioned normal equations depends on the residual of the original
least squares problem. We present non-intuitive but realistic perturbation
bounds for the relative error in the computed solutions and show that, with an
effective preconditioner, these bounds are essentially equal to the
perturbation bound for the original least squares problem. Probabilitistic
condition number bounds corroborate the effectiveness of the randomized
preconditioner computed with small amounts of sampling.

</details>


### [8] [Fast Multipole Method for Maxwell's Equations in Layered Media](https://arxiv.org/abs/2507.18491)
*Heng Yuan,Bo Wang,Wenzhong Zhang,Wei Cai*

Main category: math.NA

TL;DR: A fast multipole method (FMM) for solving Maxwell's equations in 3-D layered media is presented, using the magnetic vector potential and dyadic Green's function. The method achieves O(N log N) complexity and rapid convergence for low-frequency wave sources.


<details>
  <summary>Details</summary>
Motivation: To efficiently solve Maxwell's equations in 3-D layered media, addressing computational challenges and numerical stability.

Method: Uses the magnetic vector potential under the Lorenz gauge, dyadic Green's function representation, equivalent polarization images, and Chebyshev polynomial expansion for M2L translations.

Result: Demonstrates O(N log N) complexity and rapid convergence for low-frequency electromagnetic wave interactions.

Conclusion: The developed FMM method is efficient and stable for solving Maxwell's equations in layered media.

Abstract: We present a fast multipole method (FMM) for solving Maxwell's equations in
three-dimensional (3-D) layered media, based on the magnetic vector potential
$\boldsymbol A$ under the Lorenz gauge, to derive the layered dyadic Green's
function. The dyadic Green's function is represented using three scalar
Helmholtz layered Green's functions, with all interface-induced reaction field
components expressed through a unified integral representation. By introducing
equivalent polarization images for sources and effective locations for targets
to reflect the actual transmission distance of different reaction field
components, multiple expansions (MEs) and local expansions (LEs) are derived
for the far-field governed by actual transmission distance. To further enhance
computational efficiency and numerical stability, we employ a Chebyshev
polynomial expansion of the associated Legendre functions to speed up the
calculation of multipole-to-local (M2L) expansion translations. Finally,
leveraging the FMM framework of the Helmholtz equation in 3-D layered media, we
develop a FMM for the dyadic Green's function of Maxwell's equations in layered
media. Numerical experiments demonstrate the $\mathcal O(N\log N)$-complexity
of the resulting FMM method, and rapid convergence for interactions of
low-frequency electromagnetic wave sources in 3-D layered media.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [9] [Existence and nonexistence of sign-changing solutions for linearly perturbed superlinear equations on exterior domains](https://arxiv.org/abs/2507.17863)
*Md Suzan Ahamed,Joseph Iaia*

Main category: math.AP

TL;DR: The paper studies radial solutions of a nonlinear PDE in exterior domains, proving existence of infinitely many sign-changing solutions under certain conditions and nonexistence for others.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of solutions to a specific nonlinear PDE with singular and superlinear terms, particularly focusing on radial solutions in exterior domains.

Method: Analyzes the PDE using assumptions on the behavior of the nonlinear term $f(u)$ and the coefficient $K(|x|)$, employing techniques to establish existence and nonexistence results.

Result: Existence of infinitely many sign-changing solutions is proven for $N+q(N-2) < \alpha < 2(N-1)$, while nonexistence is shown for $0 < \alpha \leq 2$.

Conclusion: The study provides a detailed analysis of the PDE's solutions, highlighting conditions under which solutions exist or do not exist, contributing to the understanding of nonlinear PDEs in exterior domains.

Abstract: In this paper, we study radial solutions of $\Delta u + K(|x|) f(u)+\frac{
(N-2)^2 u}{|x|^{2+(N-2)\delta}} =0, \ 0<\delta<2$ in the exterior of the ball
of radius $R>0$ in ${\mathbb R}^{N}$ where $f$ grows superlinearly at infinity
and is singular at $0$ with $f(u) \sim -\frac{1}{|u|^{q-1}u}$ and $0<q<1$ for
small $u$. We assume $K(|x|) \sim |x|^{-\alpha}$ for large $|x|$ and establish
the existence of an infinite number of sign-changing solutions when $N+q(N-2)
<\alpha <2(N-1).$ We also prove nonexistence for $0<\alpha \leq2$.

</details>


### [10] [Diffusion over ramified domains: solvability and fine regularity](https://arxiv.org/abs/2507.17909)
*Kevin Silva-Pérez,Alejandro Vélez-Santiago*

Main category: math.AP

TL;DR: The paper studies a domain with a fractal boundary, modeling bronchial trees, and analyzes a diffusion equation for oxygen transport, proving unique solvability and global continuity of solutions.


<details>
  <summary>Details</summary>
Motivation: The work aims to understand diffusion processes in non-Lipschitz domains, particularly in idealized lung systems, addressing gaps in prior research.

Method: The authors analyze a generalized diffusion equation with mixed boundary conditions, focusing on stationary and time-dependent cases, including critical parameter values.

Result: They prove unique solvability and global Hölder continuity for stationary solutions, and uniform continuity for time-dependent solutions, even in non-extension domains.

Conclusion: This is the first demonstration of global continuity for such problems in non-extension domains, advancing understanding of diffusion in complex geometries.

Abstract: We consider a domain $\Omega\subseteq\mathbb{R\!}^{\,2}$ with branched
fractal boundary $\Gamma^{\infty}$ and parameter $\tau\in[1/2,\tau^{\ast}]$
introduced by Achdou and Tchou \cite{ACH08}, for $\tau^{\ast}\simeq 0.593465$,
which acts as an idealization of the bronchial trees in the lungs systems. For
each $\tau\in[1/2,\tau^{\ast}]$, the corresponding region $\Omega$ is a
non-Lipschitz domain, which attains its roughest structure at the critical
value $\tau=\tau^{\ast}$ in such way that in this endpoint parameter the region
$\Omega$ fails to be an extension domain, and its ramified boundary
$\Gamma^{\infty}$ is not post-critically finite. Then, we investigate a model
equation related to the diffusion of oxygen through the bronchial trees by
considering the realization of a generalized diffusion equation with
inhomogeneous mixed-type boundary conditions. Under minimal assumptions, we
first show that the stationary version of the above diffusion equation in
uniquely solvable, and that the corresponding weak solution in globally
H\"older continuous on $\overline{\Omega}$. Since we are including the critical
case $\tau=\tau^{\ast}$, this is the first time in which global uniform
continuity of weak solutions of a Robin-type boundary value problem is attained
over a non-extension domain. Furthermore, after two transitioning procedures,
we prove the unique solvability of the inhomogeneous time-dependent diffusion
equation, and we show that the corresponding weak solution is globally
uniformly continuous over $[0,T]\times\overline{\Omega}$ for each fixed
parameter $T>0$.

</details>


### [11] [Nonspherically symmetric equilibrium bubbles in a steadily rotating incompressible fluid](https://arxiv.org/abs/2507.17915)
*Chen-Chih Lai,Michael I. Weinstein*

Main category: math.AP

TL;DR: The paper presents rotational equilibrium solutions for an isobaric gas pressure model, extending Gavrilov's work. It includes nonspherical bubble shapes and numerical simulations using PINN.


<details>
  <summary>Details</summary>
Motivation: To explore nontrivial rotational equilibrium solutions in inviscid isobaric models and extend existing results on bubble shapes.

Method: Theoretical analysis of equilibrium solutions, construction of nonspherical bubble shapes, and numerical simulation using Physics-Informed Neural Networks (PINN).

Result: Existence of rotational equilibrium solutions and nonspherical bubble shapes confirmed; numerical simulations validate theoretical findings.

Conclusion: The study advances understanding of equilibrium solutions in inviscid isobaric models and demonstrates practical application of PINN for bubble shape simulation.

Abstract: This note presents two nontrivial, rotational equilibrium solutions to the
spatial uniform gas pressure (isobaric) approximate model of Prosperetti in the
inviscid case. Building on Gavrilov's work [GAFA 2019], we first establish the
existence of equilibrium solutions with nontrivial (rotational) liquid flow.
Second, we construct a nonspherically symmetric, horn-torus-shaped equilibrium
bubble under mild spatial decay conditions of the liquid flow. In addition, we
extend earlier results on the characterization of spherical equilibrium bubbles
to the axisymmetric, purely azimuthal setting. Finally, we implement a
numerical simulation of the equilibrium bubble shape using the Physics-Informed
Neural Network (PINN) approximation.

</details>


### [12] [Existence of smooth solutions of the Navier-Stokes equations in three-dimensional Euclidean space](https://arxiv.org/abs/2507.18063)
*Genqian Liu*

Main category: math.AP

TL;DR: Existence of smooth solutions for 3D incompressible Navier-Stokes equations is proven via parabolic inertia Lamé equations, letting λ→∞.


<details>
  <summary>Details</summary>
Motivation: To establish smooth solutions for Navier-Stokes equations by leveraging their connection with parabolic inertia Lamé equations.

Method: Prove existence/uniqueness of smooth solutions for parabolic inertia Lamé equations, then take λ→∞ while fixing μ>0.

Result: Existence of smooth solutions for incompressible Navier-Stokes equations in ℝ³ is demonstrated.

Conclusion: The approach successfully links Lamé and Navier-Stokes equations, providing a new proof for smooth solutions.

Abstract: Based on the essential connection of the parabolic inertia Lam\'{e} equations
and Navier-Stokes equations, we prove the existence of smooth solutions of the
incompressible Navier-Stokes equations in three-dimensional Euclidean space
$\mathbb{R}^3$ by showing the existence and uniqueness of smooth solutions of
the parabolic inertia Lam\'{e} equations and by letting a Lam\'{e} constant
$\lambda$ tends to infinity (the other Lam\'{e} constant $\mu>0$ is fixed).

</details>


### [13] [The magnetohydrodynamical system in bounded $\mathscr{C}^1$ domains of dimension $n\ge3$](https://arxiv.org/abs/2507.18195)
*Sylvie Monniaux*

Main category: math.AP

TL;DR: Existence of mild solutions for the magnetohydrodynamical system in critical spaces is proven for $\mathscr{C}^1$ domains in dimensions $n \ge 3$.


<details>
  <summary>Details</summary>
Motivation: To establish the existence of solutions for the magnetohydrodynamical system in less smooth domains ($\mathscr{C}^1$), leveraging recent advancements in regularity theory.

Method: Relies on recent regularity results for the Stokes operator in $\mathscr{C}^1$ domains and a newly proved Leibniz-like formula for differential forms.

Result: Existence of mild solutions is confirmed in critical spaces for dimensions $n \ge 3$.

Conclusion: The study extends solvability results for the magnetohydrodynamical system to $\mathscr{C}^1$ domains, using innovative analytical tools.

Abstract: Existence of mild solutions for the magnetohydrodynamical system in
$\mathscr{C}^1$ domains is established in critical spaces in dimension $n\ge
3$. The proof relies on recent regularity results on the Stokes operator in
$\mathscr{C}^1$ domains and a Leibniz-like formula for differential forms
proved here in Section 2.

</details>


### [14] [$H^s_x$ regularity of solutions to the stationary Boltzmann equation with the incoming boundary condition](https://arxiv.org/abs/2507.18211)
*Daisuke Kawagoe*

Main category: math.AP

TL;DR: Existence of a solution for the stationary Boltzmann equation in a bounded convex domain, with specific cross-section form, under incoming boundary conditions, without requiring positive Gaussian curvature.


<details>
  <summary>Details</summary>
Motivation: To address the existence and regularity of solutions for the Boltzmann equation with a given cross-section form, especially without relying on boundary curvature assumptions.

Method: 1. Prove well-posedness of the linearized problem in a weighted $L^2$ space. 2. Develop $L^2-L^\infty$ estimates. 3. Investigate $H^s_x$ regularity using the velocity averaging lemma. 4. Derive a bilinear estimate for the weakly nonlinear problem.

Result: Solutions exist in a weighted $L^\infty$ space with fractional Sobolev regularity. For $-2 \leq \gamma \leq 1$, solutions have $H^{1-}_x$ regularity; worse regularity for $-3 < \gamma < -2$.

Conclusion: The study successfully establishes solution existence and regularity for the Boltzmann equation under relaxed boundary conditions, leveraging linearized analysis and bilinear estimates.

Abstract: We consider the stationary Boltzmann equation with the cross section of the
form $B(|v - \tilde{v}, \theta|) = B_0 |v - \tilde{v}|^\gamma \cos \theta \sin
\theta$ for $-3 < \gamma \leq 1$ in a bounded convex domain under the incoming
boundary condition. In this article, we shall show the existence of a solution
in a weighted $L^\infty$ space with fractional Sobolev regularity without
assuming the positivity of the Gaussian curvature on the boundary. For boundary
data sufficiently smooth and close to the standard Maxwellian, the solution has
$H^{1-}_x$ regularity for $-2 \leq \gamma \leq 1$, while only worse regularity
is obtained for $-3 < \gamma < -2$. We first show the well-posedness of the
linearized problem on a weighted $L^2$ space and develop the $L^2-L^\infty$
estimate without the stochastic cycle. We next investigate $H^s_x$ regularity
of the solution to the linearized problem. The velocity averaging lemma plays a
key role in our analysis. We finally derive a bilinear estimate to extend
results on the linearized problem to the weakly nonlinear problem.

</details>


### [15] [Quantum ergodicity for contact metric structures](https://arxiv.org/abs/2507.18216)
*Lino Benedetto*

Main category: math.AP

TL;DR: The paper proves a Quantum Ergodicity theorem for subLaplacians on contact metric manifolds with ergodic Reeb flow, using a semiclassical pseudodifferential calculus and microlocal projectors.


<details>
  <summary>Details</summary>
Motivation: To extend Quantum Ergodicity results to subLaplacians on contact manifolds, leveraging the ergodicity of the Reeb flow.

Method: Uses a semiclassical pseudodifferential calculus tailored for contact manifolds, constructing microlocal projectors (Landau projectors) that commute with the subLaplacian. The proof mimics the Born-Oppenheimer approximation.

Result: Demonstrates that the subLaplacian acts effectively as the Reeb vector field on the range of each Landau projector, enabling the proof of Quantum Ergodicity.

Conclusion: The approach successfully extends Quantum Ergodicity to subLaplacians on contact manifolds, relying on microlocal techniques and ergodic Reeb flow.

Abstract: This paper is dedicated to the proof of a Quantum Ergodicity (QE) theorem for
the eigenfunctions of subLaplacians on contact metric manifolds, under the
assumption that the Reeb flow is ergodic. To do so, we rely on a semiclassical
pseudodifferential calculus developed for general filtered manifolds that we
specialize to the setting of contact manifolds. Our strategy is then
reminiscent of an implementation of the Born-Oppenheimer approximation as we
rely on the construction of microlocal projectors in our calculus which commute
with the subLaplacian, called Landau projectors. The subLaplacian is then shown
to act effectively on the range of each Landau projector as the Reeb vector
field does. The remainder of the proof follows the classical path towards QE,
once microlocal Weyl laws have been established.

</details>


### [16] [Long-time existence for the 2D ideal Boussinesq and the 2D density-dependent Euler equations](https://arxiv.org/abs/2507.18244)
*Hantaek Bae,Milton Lopes Filho,Anna Mazzucato,Helena Nussenzveig Lopes*

Main category: math.AP

TL;DR: The paper proves long-time smooth solutions for 2D ideal Boussinesq and non-homogeneous Euler equations with small perturbations, using an elementary and broadly applicable technique.


<details>
  <summary>Details</summary>
Motivation: To address the long-time existence of smooth solutions for perturbed initial data in 2D fluid dynamics.

Method: Develops an elementary technique for proving long-time existence, applicable to small temperature or density perturbations.

Result: Establishes smooth solutions for 2D Boussinesq and Euler equations under small perturbations.

Conclusion: The technique is simple yet broadly useful for similar problems in fluid dynamics.

Abstract: We establish long-time existence of smooth solutions to the 2D ideal
Boussinesq equations and to the 2D non-homogeneous incompressible Euler
equations for initial data consisting of small temperature perturbations, or
small density perturbations, of smooth initial flows which are not necessarily
small. Both results are known (see Danchin and Fanelli 2013, Danchin 2011 in
the references) but the technique we develop to prove them is at the same time
elementary and has broad potential applicability.

</details>


### [17] [Well-posedness of the compressible boundary layer equations with analytic initial data](https://arxiv.org/abs/2507.18247)
*Ya-Guang Wang,Yi-Lei Zhao*

Main category: math.AP

TL;DR: Study of well-posedness for compressible boundary layer equations with analytic tangential data, using Littlewood-Paley theory for local existence and uniqueness.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of thermal and viscous layers in compressible viscous flow with heat conduction under nonslip and zero heat flux conditions.

Method: Employ Littlewood-Paley theory to derive a priori estimates and establish local existence and uniqueness in a mixed analytic-Sobolev space.

Result: Local existence and uniqueness of solutions in the specified function space.

Conclusion: The compressible boundary layer equations are well-posed for analytic tangential data, with solutions existing uniquely in the given space.

Abstract: We study the well-posedness of the compressible boundary layer equations with
data being analytic in the tangential variable of the boundary. The
compressible boundary layer equations, a nonlinear coupled system of degenerate
parabolic equations and an elliptic equation, describe the behavior of thermal
layer and viscous layer in the small viscosity and heat conductivity limit, for
the two-dimensional compressible viscous flow with heat conduction with nonslip
and zero heat flux boundary conditions. We use the Littlewood-Paley theory to
establish the a priori estimates for solutions of this compressible boundary
layer problem, and obtain the local existence and uniqueness of the solution in
the space of analytic in the tangential variable and Sobolev in the normal
variable.

</details>


### [18] [Nonlinear Hardy-Stein type identities for harmonic functions relative to symmetric integro-differential operators](https://arxiv.org/abs/2507.18308)
*Tomasz Klimsiak,Andrzej Rozkosz*

Main category: math.AP

TL;DR: The paper establishes Hardy-Stein identities for harmonic functions under integro-differential operators with mixed local and nonlocal components, extends results to convex compositions, and provides applications like norm characterizations and Littlewood-Paley estimates.


<details>
  <summary>Details</summary>
Motivation: To generalize Hardy-Stein identities for harmonic functions to operators with mixed local and nonlocal components and explore their applications.

Method: Uses probabilistic methods to derive identities for harmonic functions and their convex compositions, with conditional identities for ratios.

Result: Proves Hardy-Stein identities, characterizes norms in harmonic Hardy spaces, and derives Littlewood-Paley estimates.

Conclusion: The results extend classical identities to mixed-type operators and demonstrate their utility in harmonic analysis.

Abstract: We show identities of Hardy-Stein type for harmonic functions relative to
integro-differential operators corresponding to general symmetric regular
Dirichlet forms satisfying the absolute continuity condition. The novelty is
that we consider operators of mixed type containing both local and nonlocal
component. Moreover, the identities are proved for compositions of harmonic
functions and general convex functions. We also provide some conditional
identities, i.e. identities for ratios of harmonic functions. As an application
we give a characterization of norms in harmonic Hardy spaces and prove
Littlewood--Paley type estimates for square functions. To illustrate general
results, we discuss in some details the case of divergence form operator and
purely nonlocal operator defined by some jump kernel. Our proofs are rather
short and use mainly probabilistic methods.

</details>


### [19] [Quadratic flatness and Regularity for Codimension-One Varifolds with Bounded Anisotropic Mean Curvature](https://arxiv.org/abs/2507.18357)
*Mario Santilli,Sławomir Kolasiński*

Main category: math.AP

TL;DR: The paper proves the existence of a dense C1,α-regular part in the support of a varifold with bounded mean ϕ-curvature, under certain conditions, and relates it to points where blow-ups are not flat.


<details>
  <summary>Details</summary>
Motivation: To understand the regularity properties of varifolds with bounded mean curvature in a uniformly convex normed space.

Method: Uses a quadratic flatness theorem and applies the Allard anisotropic regularity theorem to analyze the varifold's support.

Result: Shows the existence of a dense C1,α-regular part and relates it to non-flat blow-up points.

Conclusion: The results provide insights into the regularity and structure of varifolds under given curvature constraints.

Abstract: Given a uniformly convex norm $ \phi $ on $ \mathbf{R}^{n+1} $ and a
unit-density $ n $-dimensional varifold $ V $ in an open subset of $
\mathbf{R}^{n+1} $ with bounded mean $ \phi $-curvature, under the hypothesis
  $$ \textrm{$\mathcal{H}^n \llcorner {\rm spt}\| V \| $ is absolutely
continuous with respect to $ \| V \| $},$$
  we prove that there exists an open dense $ \mathscr{C}^{1, \alpha}$-regular
part in $ {\rm spt} \| V \| $. Moreover we prove that the $ \mathscr{C}^{1,
\alpha}$-regular part of $ V $ is $ \mathcal{H}^n $-almost equal to $({\rm
spt}\| V \|)^\ast $, the subset of $ {\rm spt}\| V \| $ where at least one blow
up (in Hausdorff distance) of $ {\rm spt}\| V \| $ is not equal to $
\mathbf{R}^{n+1} $.
  These results are consequences of a quadratic flatness theorem asserting that
if $ V $ is a general -- not necessarily rectifiable -- varifold $ V $ with
bounded mean $ \phi $-curvature and locally $ \mathcal{H}^n $-finite support,
then the set of points $ R $ where $ {\rm spt}\| V \|$ admits (exactly) two
mutually tangent balls satisfies $ \mathcal{H}^n(({\rm spt}\| V \|)^\ast
\setminus R) =0 $ and meets each relatively open set of $ {\rm spt}\| V \| $ on
a set of positive $ \mathcal{H}^n $-measure. Indeed, quadratic flatness at a
point guarantees that the Allard anisotropic regularity theorem can be applied,
hence proving that $ {\rm spt}\| V \|$ is regular around that point.

</details>


### [20] [Consistency of tug-of-war type operators on random data clouds](https://arxiv.org/abs/2507.18383)
*Jeongmin Han,Huajie Liu*

Main category: math.AP

TL;DR: Analysis of a tug-of-war operator on geometric graphs and its Dirichlet problem on random data clouds, focusing on convergence and consistency.


<details>
  <summary>Details</summary>
Motivation: To understand the connection between the tug-of-war operator and its model problem by analyzing convergence as data points increase and step size shrinks.

Method: Study the tug-of-war operator on geometric graphs and its Dirichlet problem, analyzing convergence of value functions.

Result: Reveals the connection between the operator and the model problem, with consistency being a key factor.

Conclusion: The study successfully links the tug-of-war operator to its model problem through convergence analysis and operator consistency.

Abstract: In this paper, we study a tug-of-war type operator on geometric graphs and
its associated Dirichlet problem on a random data cloud.
  Specifically, we analyze the convergence of the value functions as the number
of data points increases and the step size of the game shrinks. This analysis
reveals the connection between our tug-of-war type operator and the
corresponding model problem.
  A key ingredient in establishing this result is the consistency of the
operator.

</details>


### [21] [Homogenization and 3D-2D dimension reduction of a functional on manifold valued BV space](https://arxiv.org/abs/2507.18390)
*Luca Lussardi,Andrea Torricelli,Elvira Zappale*

Main category: math.AP

TL;DR: The paper analyzes the homogenization and dimension reduction of an energy functional with linear growth for manifold-valued Sobolev functions using Γ-convergence, leading to an integral representation in the space of manifold-constrained BV functions.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of energy functionals with linear growth in the context of manifold-valued Sobolev functions under simultaneous homogenization and dimension reduction.

Method: The study employs Γ-convergence techniques to analyze the problem and derive an integral representation.

Result: An integral representation result is obtained in the space of manifold-constrained functions with bounded variation.

Conclusion: The work provides a theoretical framework for homogenization and dimension reduction in manifold-valued Sobolev spaces, with implications for variational problems in constrained settings.

Abstract: We study the simultaneous homogenization and dimension reduction of an energy
functional with linear growth defined on the space of manifold valued Sobolev
functions. The study is carried out by $\Gamma$-convergence, providing an
integral representation result in the space of manifold constrained functions
with bounded variation

</details>


### [22] [Asymmetric Kedem-Katchalsky boundary conditions for systems with spatial heterogeneities](https://arxiv.org/abs/2507.18400)
*Pablo Álvarez-Caudevilla,Cristina Brändle,Fermin González-Pereiro*

Main category: math.AP

TL;DR: The paper explores a model of two species interacting at a shared boundary, with unique population distributions emerging under specific growth rate conditions, including non-simultaneous blow-up behavior.


<details>
  <summary>Details</summary>
Motivation: To understand the dynamics of two species inhabiting adjacent areas with resource-rich refuges and asymmetric boundary interactions.

Method: Uses a system of equations with spatial variations in growth rates and crowding effects, incorporating Kedem-Katchalsky boundary conditions.

Result: Existence of unique positive solutions within a specific growth rate range, with bifurcation and non-simultaneous blow-up phenomena.

Conclusion: The model highlights critical growth rate thresholds and asymmetric population behaviors, providing insights into species coexistence and extinction scenarios.

Abstract: This work investigates a model describing the interaction of two species in
habiting separate but adjacent areas. These populations are governed by a
system of
  equations that account for spatial variations in growth rates and the effects
of crowding.
  A key feature is the presence of areas within each domain where resources are
unlimited
  and crowding effects are absent. The species interact solely through a common
bound ary interface, which is modeled by asymmetric Kedem-Katchalsky boundary
conditions.
  The paper provides existence, non-existence, and behavior of positive
solutions for the
  system. It is shown that a unique positive population distribution exists
when one of the
  growth rate parameters falls within a specific range defined by two critical
values. One of
  these critical values represents a bifurcation point where the population can
emerge from
  extinction, while the other is determined by the characteristics of the
refuge areas. The
  study also examines how the populations behave as the growth parameter
approaches the
  upper critical value. This analysis reveals the phenomenon of
non-simultaneous blow-up,
  where one population component can grow infinitely large within its refuge
zone while
  the other remains bounded.

</details>


### [23] [Strong time regularity and decay of $L^\infty$ solutions to $2\times 2$ systems of conservation laws](https://arxiv.org/abs/2507.18427)
*Luca Talamini*

Main category: math.AP

TL;DR: The paper analyzes $\mathbf L^\infty$ solutions for $2\times 2$ conservation law systems, proving regularity for finite entropy solutions and a decay estimate for vanishing viscosity solutions using a kinetic formulation.


<details>
  <summary>Details</summary>
Motivation: To understand the regularity and decay properties of solutions to $2\times 2$ conservation law systems, particularly for genuinely nonlinear cases.

Method: Uses a kinetic formulation to unify the analysis of finite entropy solutions and vanishing viscosity solutions.

Result: Finite entropy solutions are shown to belong to $C^0(\mathbb R^+; \mathbf L^1_{loc}(\mathbb R))$, and a dispersive decay estimate is established for vanishing viscosity solutions.

Conclusion: The kinetic formulation provides a unified framework for analyzing both regularity and decay in conservation law systems.

Abstract: We consider $\mathbf L^\infty$ solutions to $2\times 2$ systems of
conservation laws. For genuinely nonlinear systems we prove that finite entropy
solutions (in particular entropy solutions, if a uniformly convex entropy
exists) belong to $C^0(\mathbb R^+; \mathbf L^1_{loc}(\mathbb R))$. Our second
result establishes a dispersive-type decay estimate for vanishing viscosity
solutions. Both results are unified by the use of a kinetic formulation.

</details>


### [24] [Gradient regularity for double-phase orthotropic functionals](https://arxiv.org/abs/2507.18474)
*Stefano Almi,Chiara Leone,Gianluigi Manzo*

Main category: math.AP

TL;DR: Higher integrability and Lipschitz regularity are proven for local minimizers of a double-phase orthotropic functional under specific conditions on the weight function and exponents.


<details>
  <summary>Details</summary>
Motivation: To extend regularity results for minimizers of double-phase functionals, particularly in cases where the weight function and exponents satisfy certain smoothness and growth conditions.

Method: Analyzes the double-phase orthotropic functional with an α-Hölder continuous weight function and exponents p, q satisfying 2 ≤ p ≤ q and q/p < 1 + α/n. Uses Sobolev regularity assumptions for explicit Lipschitz estimates.

Result: Higher integrability and explicit Lipschitz regularity estimates are established for local minimizers under the given conditions.

Conclusion: The results generalize and refine regularity theory for double-phase functionals, providing explicit bounds under weaker assumptions.

Abstract: We prove higher integrability for local minimizers of the double-phase
orthotropic functional \[
  \sum_{i=1}^{n}\int_\Omega\left(\left|u_{x_i}\right|^p+a(x)\left|
u_{x_i}\right|^q\right)dx \] when the weight function $a \geq0$ is assumed to
be $\alpha$-H\"older continuous, while the exponents $p, q$ are such that $2
\leq p \leq q$ and $\frac{q}{p} < 1 + \frac{\alpha}{n}$. Under natural Sobolev
regularity of~$a$, we further obtain explicit Lipschitz regularity estimates
for local minimizers.

</details>


### [25] [A dichotomy result for a modified Schrödinger equations on unbounded domains](https://arxiv.org/abs/2507.18528)
*Anna Maria Candela,Giuliana Palmieri,Addolorata Salvatore*

Main category: math.AP

TL;DR: The paper investigates bounded positive solutions for a generalized PDE problem (P) with variational structure, extending the modified Schrödinger equation. Solutions are found via limiting sequences on bounded domains, with conditions ensuring nontriviality or specific behavior at infinity.


<details>
  <summary>Details</summary>
Motivation: To explore the existence of bounded positive solutions for a nonlinear PDE problem (P) in unbounded domains, generalizing the modified Schrödinger equation and addressing challenges like lack of radial symmetry.

Method: Utilizes variational methods and limiting sequences of solutions on bounded domains to derive bounded positive solutions for (P). Stronger hypotheses ensure nontrivial solutions or specific behavior at infinity.

Result: Under suitable assumptions, problem (P) admits a bounded positive solution. Stronger conditions yield either nontrivial solutions or sequences with specific integral bounds at infinity.

Conclusion: The study successfully establishes conditions for bounded positive solutions in unbounded domains, generalizing prior results and addressing symmetry constraints.

Abstract: This article aims to investigate the existence of bounded positive solutions
of problem \[ (P)\qquad \left\{ \begin{array}{ll} - {\rm div} (a(x,u,\nabla u))
+ A_t(x,u,\nabla u) = g(x,u) &\hbox{in $\Omega$,}\\ u\ = \ 0 & \hbox{on
$\partial\Omega$,} \end{array}\right.\] with $A_t(x,t,\xi) = \frac{\partial
A}{\partial t}(x,t,\xi)$, $a(x,t,\xi) = \nabla_\xi A(x,t,\xi)$ for a given
$A(x,t,\xi)$ which grows as $|\xi|^p + |t|^p$ , $p > 1$, where $\Omega
\subseteq \mathbb{R}^N$, $N \ge 2$, is an open connected domain with Lipschitz
boundary and infinite Lebesgue measure, eventually $\Omega = \mathbb{R}^N$,
which generalizes the modified Schr\"odinger equation \[ - {\rm div} ((A^*_1(x)
+ A^*_2(x)|u|^{s}) \nabla u) + \frac{s}2 A^*_2(x)\ |u|^{s - 2} u\ |\nabla u|^2
+ u\ =\ |u|^{\mu-2}u \quad\hbox{in $\mathbb{R}^3$.} \] Under suitable
assumptions on $A(x,t,\xi)$ and $g(x,t)$, problem $(P)$ has a variational
structure. Then, even in lack of radial symmetry hypotheses, one bounded
positive solution of $(P)$ can be found by passing to the limit on a sequence
$(u_k)_k$ of bounded solutions on bounded domains. Furthermore, if stronger
hypotheses are satisfied, either such a solution is nontrivial or a constant
$\bar{\lambda} > 0$ and a sequence of points $(y_k)_k \subset \mathbb{R}^N$
exist such that \[ |y_k| \to +\infty\qquad \hbox{and}\qquad \int_{B_1(y_k)}
|u_k|^p dx \ge \bar{\lambda}\quad \hbox{for all $k \ge 1$.} \]

</details>


### [26] [Schrodinger-Poisson-Slater equations with nonlinearity subscaled near zero](https://arxiv.org/abs/2507.18568)
*Shibo Liu,Kanishka Perera*

Main category: math.AP

TL;DR: The paper studies a zero-mass Schrödinger-Poisson-Slater equation with subscaled nonlinearity near zero and asymptotically scaled at infinity. Solutions are obtained via Morse theory and a version of Clark's theorem.


<details>
  <summary>Details</summary>
Motivation: To explore solutions for the given equation under specific nonlinearity conditions, leveraging advanced mathematical tools.

Method: Uses Morse theory for a nonzero solution and a version of Clark's theorem for a sequence of solutions, with abstract results on critical groups at infinity.

Result: A nonzero solution is derived via Morse theory, and a sequence of solutions is obtained for odd nonlinearities using Clark's theorem.

Conclusion: The paper successfully applies abstract critical point theory to derive solutions for the studied equation under specified conditions.

Abstract: We study the following zero-mass Schr{\"o}dinger-Poisson-Slater equation \[ -
\Delta u + \left( \frac{1}{4 \pi | x |} \ast u^2 \right) u = f (| x |, u)
\text{,} \qquad u \in \mathcal{D}^{1, 2} (\mathbb{R}^3) \text{} \] with
nonlinearity subscaled near zero in the sense that $f (| x |, t) \approx a | t
|^{p - 2} t$ as $| t | \rightarrow 0$ for some $p\in\big(\frac{18}{7},3\big)$.
A nonzero solution is obtained via Morse theory when the nonlinearity is
asymptotically scaled at infinity. For this purpose we prove an abstract result
on the critical groups at infinity for functionals satisfying the geometric
assumptions of the scaled saddle point theorem of Mercuri \& Perera
[arXiv:2411.15887]. For the case that $f (| x |, \cdot)$ is odd, a sequence of
solutions are obtained via a version of Clark's theorem due to Kajikiya [J.\
Funct.\ Anal.\ 225 (2005) 352--370].

</details>


### [27] [Implementation of the inverse scattering transform method for the nonlinear Schrödinger equation](https://arxiv.org/abs/2507.18586)
*Vladislav V. Kravchenko*

Main category: math.AP

TL;DR: The paper presents a novel method for solving the nonlinear Schrödinger equation using series representations for Jost solutions, simplifying direct and inverse scattering problems.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the initial-value problem for the nonlinear Schrödinger equation by improving the efficiency of the inverse scattering transform method.

Method: The method involves solving direct and inverse scattering problems for the Zakharov-Shabat system using power series representations for Jost solutions, avoiding complex equations like Gelfand-Levitan-Marchenko or Riemann-Hilbert problems.

Result: The approach yields a simple and efficient numerical algorithm, demonstrated by successful numerical examples.

Conclusion: The proposed method simplifies the solution process for the nonlinear Schrödinger equation, offering a practical alternative to existing techniques.

Abstract: We study the initial-value problem for the nonlinear Schr\"odinger equation.
Application of the inverse scattering transform method involves solving direct
and inverse scattering problems for the Zakharov-Shabat system with complex
potentials. We solve these problems by using new series representations for the
Jost solutions of the Zakharov-Shabat system. The representations have the form
of power series with respect to a transformed spectral parameter. In terms of
the representations, solution of the direct scattering problem reduces to
computing the series coefficients following a simple recurrent integration
procedure, computation of the scattering coefficients by multiplying
corresponding pairs of polynomials (partial sums of the series representations)
and locating zeros of a polynomial inside the unit disk. Solution of the
inverse scattering problem reduces to the solution of a system of linear
algebraic equations for the power series coefficients, while the potential is
recovered from the first coefficients. The system is obtained directly from the
scattering relations. Thus, unlike other existing techniques, the method does
not involve solving the Gelfand-Levitan-Marchenko equation or the matrix
Riemann-Hilbert problem. The overall approach leads to a simple and efficient
algorithm for the numerical solution of the initial-value problem for the
nonlinear Schr\"odinger equation, which is illustrated by numerical examples.

</details>


### [28] [Vortex dynamics for the Gross-Pitaevskii equation](https://arxiv.org/abs/2507.18590)
*Manuel del Pino,Rowan Juneman,Monica Musso*

Main category: math.AP

TL;DR: The paper rigorously analyzes the asymptotics of vortex dynamics in the Gross-Pitaevskii equation, showing Helmholtz-Kirchhoff system dominance and validating a formal expansion.


<details>
  <summary>Details</summary>
Motivation: To understand the precise behavior of vortex dynamics in the Gross-Pitaevskii equation, especially for multi-vortex solutions, and to validate prior formal expansions.

Method: Constructs families of n-vortex solutions, computes asymptotic expansions of vortex positions, and analyzes dynamics in terms of vortex core size.

Result: The leading-order dynamics is governed by the Helmholtz-Kirchhoff system, with the first correction determined by a linear wave equation, validating Ovchinnikov and Sigal's expansion.

Conclusion: The study provides rigorous validation of formal expansions and detailed insights into vortex dynamics in the Gross-Pitaevskii equation.

Abstract: We rigorously establish the formal asymptotics of Neu for Gross-Pitaevskii
vortex dynamics in the plane. Given any integer $n\geq2$, we construct a family
of $n$-vortex solutions with vortices of degree $\pm1$, and describe precisely
the solution profile and associated vortex dynamics on an arbitrarily large,
finite time interval. We compute an asymptotic expansion of the vortex
positions in terms of the vortex core size $\epsilon>0$, and show that the
dynamics is governed at leading order as $\epsilon\to0$ by the classical
Helmholtz-Kirchhoff system. Moreover, we show that the first correction to the
leading order dynamics is determined by the solution of a linear wave equation,
justifying a formal expansion found by Ovchinnikov and Sigal.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [29] [Multi-Head Neural Operator for Modelling Interfacial Dynamics](https://arxiv.org/abs/2507.17763)
*Mohammad Sadegh Eshaghi,Navid Valizadeh,Cosmin Anitescu,Yizheng Wang,Xiaoying Zhuang,Timon Rabczuk*

Main category: physics.comp-ph

TL;DR: The paper introduces the Multi-Head Neural Operator (MHNO), a neural operator framework for solving time-dependent PDEs, outperforming existing methods in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional numerical methods for interfacial dynamics are computationally expensive for high-dimensional problems, prompting the need for efficient alternatives like neural operators.

Method: MHNO uses time-step-specific projection operators and temporal connections inspired by message-passing to predict all time steps in one forward pass.

Result: MHNO achieves superior accuracy and scalability in solving phase field equations compared to existing neural operator methods.

Conclusion: MHNO is a promising next-generation tool for phase field modeling, with publicly available code and data.

Abstract: Interfacial dynamics underlie a wide range of phenomena, including phase
transitions, microstructure coarsening, pattern formation, and thin-film
growth, and are typically described by stiff, time-dependent nonlinear partial
differential equations (PDEs). Traditional numerical methods, including finite
difference, finite element, and spectral techniques, often become
computationally prohibitive when dealing with high-dimensional problems or
systems with multiple scales. Neural operators (NOs), a class of deep learning
models, have emerged as a promising alternative by learning mappings between
function spaces and efficiently approximating solution operators. In this work,
we introduce the Multi-Head Neural Operator (MHNO), an extended neural operator
framework specifically designed to address the temporal challenges associated
with solving time-dependent PDEs. Unlike existing neural operators, which
either struggle with error accumulation or require substantial computational
resources for high-dimensional tensor representations, MHNO employs a novel
architecture with time-step-specific projection operators and explicit temporal
connections inspired by message-passing mechanisms. This design allows MHNO to
predict all time steps after a single forward pass, while effectively capturing
long-term dependencies and avoiding parameter overgrowth. We apply MHNO to
solve various phase field equations, including antiphase boundary motion,
spinodal decomposition, pattern formation, atomic scale modeling, and molecular
beam epitaxy growth model, and compare its performance with existing NO-based
methods. Our results show that MHNO achieves superior accuracy, scalability,
and efficiency, demonstrating its potential as a next-generation computational
tool for phase field modeling. The code and data supporting this work is
publicly available at https://github.com/eshaghi-ms/MHNO.

</details>


### [30] [Machine Learning Workflow for Analysis of High-Dimensional Order Parameter Space: A Case Study of Polymer Crystallization from Molecular Dynamics Simulations](https://arxiv.org/abs/2507.17980)
*Elyar Tourani,Brian J. Edwards,Bamin Khomami*

Main category: physics.comp-ph

TL;DR: The paper introduces a machine learning workflow to accurately quantify crystallinity in polymers using atomistic data, reducing reliance on biased single-order parameters and defining a robust crystallinity index (C-index).


<details>
  <summary>Details</summary>
Motivation: Current methods for identifying crystallization pathways in polymers rely on preset cut-off points for single-order parameters, which introduce biases and sensitivity issues. A more accurate, data-driven approach is needed.

Method: The study uses high-dimensional feature vectors for atoms, low-dimensional embeddings, and unsupervised clustering to label crystalline/amorphous atoms. Supervised learning identifies minimal order parameters, and a logistic regression model defines the C-index.

Result: The workflow achieves over 0.98 classification performance (AUC) with just three order parameters. The C-index remains bimodal and efficiently computes crystallinity on-the-fly.

Conclusion: The method provides a data-driven strategy for order parameter selection and monitoring structural transformations, showing entropy dominates early nucleation while symmetry becomes relevant later.

Abstract: Currently, identification of crystallization pathways in polymers is being
carried out using molecular simulation-based data on a preset cut-off point on
a single order parameter (OP) to define nucleated or crystallized regions.
Aside from sensitivity to cut-off, each of these OPs introduces its own
systematic biases. In this study, an integrated machine learning workflow is
presented to accurately quantify crystallinity in polymeric systems using
atomistic molecular dynamics data. Each atom is represented by a
high-dimensional feature vector that combines geometric, thermodynamic-like,
and symmetry-based descriptors. Low dimensional embeddings are employed to
expose latent structural fingerprints within atomic environments. Subsequently,
unsupervised clustering on the embeddings identified crystalline and amorphous
atoms with high fidelity. After generating high quality labels with
multidimensional data, we use supervised learning techniques to identify a
minimal set of order parameters that can fully capture this label. Various
tests were conducted to reduce the feature set, demonstrating that using only
three order parameters is sufficient to recreate the crystallization labels.
Based on these observed OPs, the crystallinity index (C-index) is defined as
the logistic regression model's probability of crystallinity, remaining bimodal
throughout the process and achieving over 0.98 classification performance
(AUC). Notably, a model trained on one or a few snapshots enables efficient
on-the-fly computation of crystallinity. Lastly, we demonstrate how the optimal
C-index fit evolves during various stages of crystallization, supporting the
hypothesis that entropy dominates early nucleation, while symmetry gains
relevance later. This workflow provides a data-driven strategy for OP selection
and a metric to monitor structural transformations in large-scale polymer
simulations.

</details>


### [31] [Hierarchical Finite-Element Analysis of Multiscale Electromagnetic Problems via Sparse Operator-Adapted Wavelet Decomposition](https://arxiv.org/abs/2507.17989)
*F. Şık,F. L. Teixeira,B. Shanker*

Main category: physics.comp-ph

TL;DR: A FEM framework with wavelet decomposition decouples resolution levels for efficient multiscale electromagnetic analysis, achieving high precision and near-linear complexity.


<details>
  <summary>Details</summary>
Motivation: Existing adaptive FEM methods couple resolution levels, causing computational overhead when finer details are added.

Method: Uses operator-adapted wavelet decomposition to decouple resolution levels, enabling independent computations. Hierarchical algorithm with sparse matrix-vector multiplications and Krylov subspace solvers with ILU preconditioners.

Result: Numerical experiments confirm high precision and near-linear computational complexity.

Conclusion: The method efficiently addresses computational overhead in multiscale electromagnetic problems.

Abstract: In this paper, we present a finite element method (FEM) framework enhanced by
an operator-adapted wavelet decomposition algorithm designed for the efficient
analysis of multiscale electromagnetic problems. Usual adaptive FEM approaches,
while capable of achieving the desired accuracy without requiring a complete
re-meshing of the computational domain, inherently couple different resolution
levels. This coupling requires recomputation of coarser-level solutions
whenever finer details are added to improve accuracy, resulting in substantial
computational overhead. Our proposed method addresses this issue by decoupling
resolution levels. This feature enables independent computations at each scale
that can be incorporated into the solutions to improve accuracy whenever
needed, without requiring re-computation of coarser-level solutions. The main
algorithm is hierarchical, constructing solutions from finest to coarser levels
through a series of sparse matrix-vector multiplications. Due to its sparse
nature, the overall computational complexity of the algorithm is nearly linear.
Moreover, Krylov subspace iterative solvers are employed to solve the final
linear equations, with ILU preconditioners that enhance solver convergence and
maintain overall computational efficiency. The numerical experiments presented
in this article verify the high precision and nearly linear computational
complexity of the proposed algorithm.

</details>


### [32] [A causality inspired acceleration method for the fast temporal superposition of the finite line source solutions](https://arxiv.org/abs/2507.18200)
*Marc Basquens,Alberto Lazzarotto*

Main category: physics.comp-ph

TL;DR: A fast method for computing thermal interactions in solids, leveraging heat wave propagation properties to reduce computational costs significantly.


<details>
  <summary>Details</summary>
Motivation: Addressing time-dependent thermal problems in complex systems like borehole heat exchangers, where existing methods are computationally expensive.

Method: Uses non-history temporal superposition with an influence region based on error tolerance, replacing Bakhalov-Vasil'eva with an asymptotic method for oscillatory integrals.

Result: Computational cost for precomputation is reduced by orders of magnitude, making it feasible for large-scale simulations.

Conclusion: The method is robust and efficient, enabling simulations with hundreds of sources and time steps, overcoming bottlenecks of prior approaches.

Abstract: We present a novel, fast method to compute thermal interactions in solids,
useful for time-dependent problems involving several sources and several time
and space scales such as the ones encountered in the physics of fields of
closed loop borehole heat exchangers. The new method is based on the
non-history temporal superposition acceleration algorithm, but presents better
performance compared to the originally proposed scheme. The main idea behind it
is to leverage the propagation properties of the heat wave. Despite the basic
physical solutions of heat transfer being non-causal, it is possible to
establish an influence region by fixing an acceptable error tolerance. This
allows to reduce the necessary integration regions in such a way that numerical
integration is favored. The better behaviour of the integrand arising from this
approach allows us to replace the use of Bakhalov-Vasil'eva method in favor of
the asymptotic method for the computation of highly oscillatory integrals that
has better properties from a computational perspective in the present
application. Extensive testing is presented to evaluate the robustness of the
new method and to compare its performance against the originally proposed
non-history method and the convolution using the FFT algorithm for a range of
error tolerances. The results show that the computational cost is highly
reduced for the precomputation, which includes all the computations done before
starting the time-stepping scheme. The reduction is of several orders of
magnitude, depending on the specific case. This cost was the bottleneck of the
original non-history implementation, and reducing it in this way makes the
method suitable for simulations involving hundreds of sources and hundreds of
thousands of time steps that can arise in simulations of borehole fields.

</details>


### [33] [Atomistic Generative Diffusion for Materials Modeling](https://arxiv.org/abs/2507.18314)
*Nikolaj Rønne,Bjørk Hammer*

Main category: physics.comp-ph

TL;DR: A generative modeling framework for atomistic systems using diffusion processes for atomic positions and types, achieving high fidelity and diversity in generating structures.


<details>
  <summary>Details</summary>
Motivation: To enable flexible and physically grounded generation of atomic structures across chemical and structural domains.

Method: Combines score-based diffusion for atomic positions with continuous-time discrete diffusion for atomic types.

Result: Strong performance in fidelity and diversity, demonstrated through atomic type interpolation and guided sampling for crystallographic symmetries.

Conclusion: Implemented in AGeDi, an open-source tool for atomistic generative diffusion modeling.

Abstract: We present a generative modeling framework for atomistic systems that
combines score-based diffusion for atomic positions with a novel
continuous-time discrete diffusion process for atomic types. This approach
enables flexible and physically grounded generation of atomic structures across
chemical and structural domains. Applied to metallic clusters and
two-dimensional materials using the QCD and C2DB datasets, our models achieve
strong performance in fidelity and diversity, evaluated using precision-recall
metrics against synthetic baselines. We demonstrate atomic type interpolation
for generating bimetallic clusters beyond the training distribution, and use
classifier-free guidance to steer sampling toward specific crystallographic
symmetries in two-dimensional materials. These capabilities are implemented in
Atomistic Generative Diffusion (AGeDi), an open-source, extensible software
package for atomistic generative diffusion modeling.

</details>


### [34] [Topology-Preserving Coupling of Compressible Fluids and Thin Deformables](https://arxiv.org/abs/2507.18460)
*Jonathan Panuelos,Eitan Grinspun,David Levin*

Main category: physics.comp-ph

TL;DR: A novel discretization method ensures leakproof coupling of compressible fluids and thin deformable structures by preserving fluid domain connectedness.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of fluid leaking through thin structures in coupled simulations, ensuring accurate boundary conditions and bidirectional energy transfer.

Method: Combines constrained Voronoi-based spatial partitioning with Godunov-style finite-volume time integration, conforming cells exactly to fluid-solid interfaces.

Result: Validated on scenarios like a balloon, champagne cork, and supersonic asteroid, demonstrating leakproofness and bidirectional energy transfer.

Conclusion: The method effectively resolves fluid-solid interactions sharply, preventing leaks and enabling accurate force exchange.

Abstract: We present a novel discretization of coupled compressible fluid and thin
deformable structures that provides sufficient and necessary leakproofness by
preserving the path connectedness of the fluid domain. Our method employs a
constrained Voronoi-based spatial partitioning combined with Godunov-style
finite-volume time integration. The fluid domain is discretized into cells that
conform exactly to the fluid-solid interface, allowing boundary conditions to
be sharply resolved exactly at the interface. This enables direct force
exchange between the fluid and solid while ensuring that no fluid leaks through
the solid, even when arbitrarily thin. We validate our approach on a series of
challenging scenarios -- including a balloon propelled by internal compressed
air, a champagne cork ejecting after overcoming friction, and a supersonic
asteroid -- demonstrating bidirectional energy transfer between fluid and
solid.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [35] [Bright polarised x-ray flashes from dense plasmas](https://arxiv.org/abs/2507.18078)
*Q. Qian,C. P. Ridgers,S. V. Bulanov,T. Grismayer,P. Hadjisolomou,D. Seipt,M. Vranic,A. G. R. Thomas*

Main category: physics.plasm-ph

TL;DR: The paper explores using polarization of X-ray flashes as an indicator of strong-field QED (SFQED) plasma production, distinguishing it from other hard X-ray sources like bremsstrahlung.


<details>
  <summary>Details</summary>
Motivation: Understanding SFQED plasmas is crucial for extreme astrophysical environments like pulsar magnetospheres, but their dynamics are poorly understood.

Method: A laser of intensity $10^{21}$ Wcm$^{-2}$ is used on a solid Al target to produce X-ray flashes, with polarization measured for photons >10 keV.

Result: Photons from the X-ray flash with energy >10 keV are >65% polarized, unlike the unpolarized background.

Conclusion: Polarization serves as a clear indicator of SFQED plasma production, aiding in distinguishing it from other X-ray sources.

Abstract: Creating a plasma dominated by strong-field QED (SFQED) effects is a major
goal of new multi-PW laser facilities. This is motivated by the fact that the
fundamental dynamics of such plasmas is poorly understood and plays an
important role in the electrodynamics of extreme astrophysical environments
such as pulsar magnetospheres. The most obvious observable for which such a
regime has been reached is the production of a bright flash of x-rays, but
distinguishing this from other sources of hard x-rays (e.g., bremsstrahlung) is
a major challenge. Here we show that the photons from the X-ray flash are
highly polarised, as compared to the unpolarised background, i.e., polarisation
is an indicator that the SFQED plasma has really produced. For a laser of
intensity $10^{21}$ Wcm$^{-2}$ impinging on a solid Al target, the photons of
the flash with energy $>10$\thinspace keV are $>65\%$ polarised.

</details>


### [36] [Electron acoustic shock and solitary waves in spin polarized dense rotating quantum plasmas](https://arxiv.org/abs/2507.18228)
*Atherv Saxena,Punit Kumar*

Main category: physics.plasm-ph

TL;DR: Analysis of electrostatic waves in a quantum plasma with electron-positron-ion components, including spin, Fermi pressure, and quantum effects, under rotation and magnetic fields. Shock waves are studied, showing quantum effects modify dispersion and shock profiles.


<details>
  <summary>Details</summary>
Motivation: To understand wave propagation in quantum plasmas with spin effects, Fermi pressure, and quantum potentials, especially under astrophysical conditions like rotation and magnetic fields.

Method: Derived coupled dispersion relations for electron, positron, and ion modes. Used the Korteweg de Vries Burgers method to study electron acoustic shock waves.

Result: Quantum effects enhance wave dispersion and modify shock profiles by broadening and stabilizing the shock structure. Shock wave solutions were obtained.

Conclusion: Quantum effects significantly influence wave propagation and shock dynamics in astrophysical quantum plasmas, providing insights into such environments.

Abstract: The propagation of electrostatic waves in a three-component electron positron
ion astrophysical quantum plasma under the influence of uniform rotation is
analysed, incorporating the effects of particle spin, Fermi pressure, and the
quantum Bohm potential. Spin polarisation arising due to the alignment of
particle spins under the influence of a strong external magnetic field leads to
an imbalance in the population of spin-up and spin-down states. Additionally,
key astrophysical factors such as rotation and gravitational influence have
been considered. The coupled dispersion relations for electron, positron, and
ion modes have been derived. Further, the electron acoustic shock wave is
studied using the Korteweg de Vries Burgers method, and the shock wave solution
has been obtained. Quantum effects are found to contribute to enhanced wave
dispersion and modify the shock profile by broadening and stabilising the shock
structure.

</details>


### [37] [Plasma Position Constrained Free-Boundary MHD Equilibrium in Tokamaks using pyIPREQ](https://arxiv.org/abs/2507.18324)
*Udaya Maurya,Amit K. Singh,Suman Aich,Jagabandhu Kumar,Rohit Kumar,Daniel Raju*

Main category: physics.plasm-ph

TL;DR: pyIPREQ is a new MHD equilibrium code for Tokamak plasmas, enhancing PEST and IPREQ with features like limiter boundaries, magnetic axis constraints, and vertical instability handling.


<details>
  <summary>Details</summary>
Motivation: To improve upon existing MHD equilibrium codes (PEST, IPREQ) for Tokamak plasmas by adding new capabilities and addressing limitations like boundary specification and instability modeling.

Method: Uses finite difference and Green's function approach, extends functionalities for limiter boundaries, magnetic axis constraints, and vertical instabilities.

Result: Benchmarked against published results and IPREQ, applied to ADITYA-U and SST-1 Tokamaks with accurate predictions.

Conclusion: pyIPREQ is a robust tool for Tokamak plasma equilibrium studies, validated and applicable to real experiments.

Abstract: A free-boundary, axisymmetric magnetohydrodynamic (MHD) equilibrium code,
pyIPREQ, has been developed for Tokamak plasmas using finite difference and
Green's function approach. The code builds upon the foundational frameworks of
the PEST and IPREQ codes, introducing several enhancements and new
capabilities. Notably, pyIPREQ supports the specification of limiter boundaries
and enables the computation of key physical quantities. The code has also been
extended to compute equilibria constrained by a prescribed magnetic axis
position, which is particularly useful when such information is available from
diagnostics like Sine-Cosine coils. In addition, pyIPREQ includes functionality
to address vertical instabilities, a requirement for accurately modeling
elongated plasma configurations. Benchmarking has been carried out against
published results and the original IPREQ code. Applications are demonstrated
for ADITYA-U Tokamak experiments, where magnetic axis measurements are
available, and predictions are also made for SST-1 and ADITYA-U Tokamaks under
various operational scenarios.

</details>


### [38] [Nonlocal current-driven heat flow in ideal plasmas](https://arxiv.org/abs/2507.18430)
*Nicholas Mitchell,David Chapman,Grigory Kagan*

Main category: physics.plasm-ph

TL;DR: The paper explores nonlocal effects on current-driven heat transport in plasmas, revealing significant enhancements due to large currents and high ionizations.


<details>
  <summary>Details</summary>
Motivation: To address the understudied nonlocal regimes of current-driven heat flow and friction in collisional plasmas.

Method: Uses a first-principles reduced kinetic method (RKM) to analyze nonlocal effects.

Result: Large currents enhance heat flux via a novel nonlocal mechanism, especially for higher ionizations.

Conclusion: Nonlocal effects in current-driven transport are significant even for weak flows, analogous to standard nonlocal effects.

Abstract: Electron heat flux is an important and often dominant mechanism of energy
transport in a variety of collisional plasmas in a confined fusion or
astrophysical context. While nonlocal conductive heat transport, driven by
strong temperature gradients, has been investigated extensively in previous
literature, nonlocal regimes of the current-driven heat flow and friction have
not received the same attention. In this work, a first-principles reduced
kinetic method (RKM) is applied to study nonlocal effects on current-driven
transport. In addition to nonlocality due to sharp gradients, sufficiently
large currents are found to significantly enhance current-driven heat flux due
to a novel nonlocal mechanism, with this enhancement being increasingly
prevalent for higher effective ionizations $Z^*$. Introducing the dimensionless
number $N_u \equiv \vert \boldsymbol{u}_e - \boldsymbol{u}_i \vert /
v_{\text{th},e}$, these enhancements occur for even relatively weak flows $N_u
\gtrsim 1/100$, analogously to standard nonlocal effects becoming significant
for Knudsen numbers $N_K \gtrsim 1/100$.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [39] [Fourier Neural Operators for Non-Markovian Processes:Approximation Theorems and Experiments](https://arxiv.org/abs/2507.17887)
*Wonjae Lee,Taeyoung Kim,Hyungbin Park*

Main category: cs.LG

TL;DR: The paper introduces MFNO, a neural operator for stochastic systems, extending FNO with mirror padding to handle non-periodic inputs. It theoretically and empirically outperforms standard architectures.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of learning dynamics in stochastic systems, especially for non-periodic inputs, where standard architectures like FNO fall short.

Method: MFNO extends FNO by incorporating mirror padding, enabling non-periodic input handling. Theoretical proofs use Wong-Zakai theorems and approximation techniques.

Result: MFNO approximates stochastic differential equations accurately, shows strong resolution generalization, and outperforms baselines like LSTMs and DeepONet.

Conclusion: MFNO is a robust, efficient alternative for stochastic system dynamics, offering theoretical guarantees and empirical superiority over existing methods.

Abstract: This paper introduces an operator-based neural network, the mirror-padded
Fourier neural operator (MFNO), designed to learn the dynamics of stochastic
systems. MFNO extends the standard Fourier neural operator (FNO) by
incorporating mirror padding, enabling it to handle non-periodic inputs. We
rigorously prove that MFNOs can approximate solutions of path-dependent
stochastic differential equations and Lipschitz transformations of fractional
Brownian motions to an arbitrary degree of accuracy. Our theoretical analysis
builds on Wong--Zakai type theorems and various approximation techniques.
Empirically, the MFNO exhibits strong resolution generalization--a property
rarely seen in standard architectures such as LSTMs, TCNs, and DeepONet.
Furthermore, our model achieves performance that is comparable or superior to
these baselines while offering significantly faster sample path generation than
classical numerical schemes.

</details>


### [40] [Low-rank adaptive physics-informed HyperDeepONets for solving differential equations](https://arxiv.org/abs/2507.18346)
*Etienne Zeudong,Elsa Cardoso-Bihlo,Alex Bihlo*

Main category: cs.LG

TL;DR: PI-LoRA-HyperDeepONets use low-rank adaptation to reduce complexity and improve performance over HyperDeepONets.


<details>
  <summary>Details</summary>
Motivation: To address the high memory and computational costs of HyperDeepONets while maintaining or improving expressivity.

Method: Leverage low-rank adaptation (LoRA) to decompose the hypernetwork's output layer weight matrix into smaller low-rank matrices.

Result: Achieves up to 70% parameter reduction and better predictive accuracy and generalization.

Conclusion: PI-LoRA-HyperDeepONets offer a more efficient and effective alternative to HyperDeepONets in physics-informed machine learning.

Abstract: HyperDeepONets were introduced in Lee, Cho and Hwang [ICLR, 2023] as an
alternative architecture for operator learning, in which a hypernetwork
generates the weights for the trunk net of a DeepONet. While this improves
expressivity, it incurs high memory and computational costs due to the large
number of output parameters required. In this work we introduce, in the
physics-informed machine learning setting, a variation, PI-LoRA-HyperDeepONets,
which leverage low-rank adaptation (LoRA) to reduce complexity by decomposing
the hypernetwork's output layer weight matrix into two smaller low-rank
matrices. This reduces the number of trainable parameters while introducing an
extra regularization of the trunk networks' weights. Through extensive
experiments on both ordinary and partial differential equations we show that
PI-LoRA-HyperDeepONets achieve up to 70\% reduction in parameters and
consistently outperform regular HyperDeepONets in terms of predictive accuracy
and generalization.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [41] [Shallow quantum circuit for generating O(1)-entanged approximate state designs](https://arxiv.org/abs/2507.17871)
*Wonjun Lee,Minki Hhan,Gil Young Cho,Hyukjoon Kwon*

Main category: quant-ph

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Random quantum states have various applications in quantum information
science, including quantum cryptography, quantum simulation, and benchmarking
quantum devices. In this work, we discover a new ensemble of quantum states
that serve as an $\epsilon$-approximate state $t$-design while possessing
extremely low entanglement, magic, and coherence. We show that those resources
such quantum states can reach their theoretical lower bounds, $\Omega\left(\log
(t/\epsilon)\right)$, which are also proven in this work. This implies that for
fixed $t$ and $\epsilon$, those resources do not scale with the system size,
i.e., $O(1)$ with respect to the total number of qubits $n$ in the system.
Moreover, we explicitly construct an ancilla-free shallow quantum circuit for
generating such states. To this end, we develop an algorithm that transforms
$k$-qubit approximate state designs into $n$-qubit ones through a sequence of
multi-controlled gates, without increasing the support size. The depth of such
a quantum circuit is $O\left(t [\log t]^3 \log n \log(1/\epsilon)\right)$,
which is the most efficient among existing algorithms without ancilla qubits. A
class of shallow quantum circuits proposed in our work offers reduced cost for
classical simulation of random quantum states, leading to potential
applications in various quantum information processing tasks. As a concrete
example for demonstrating utility of our algorithm, we propose classical shadow
tomography using an $O(1)$-entangled estimator, which can achieve shorter
runtime compared to conventional schemes.

</details>


### [42] [Stability of Continuous Time Quantum Walks in Complex Networks](https://arxiv.org/abs/2507.17880)
*Adithya L J,Johannes Nokkala,Jyrki Piilo,Chandrakala Meena*

Main category: quant-ph

TL;DR: The study examines how network topology and decoherence models affect the stability of continuous time quantum walks (CTQWs), finding heterogeneous networks like star and scale-free topologies most stable.


<details>
  <summary>Details</summary>
Motivation: To understand how different network topologies and decoherence mechanisms influence the preservation of quantum properties in CTQWs.

Method: Analyzed CTQWs in various network topologies (cycle, complete, Erdős-Rényi, small-world, scale-free, star) under intrinsic decoherence, Haken-Strobl noise, and QSWs, using metrics like node occupation probabilities and von Neumann entropy.

Result: Heterogeneous networks (star, scale-free) are most stable; intrinsic decoherence preserves coherence longest, QSWs degrade it fastest. Complete graphs remain stable due to dense connectivity.

Conclusion: Network topology and decoherence model significantly impact quantum stability, with heterogeneous networks and dense connectivity enhancing coherence preservation.

Abstract: We investigate the stability of continuous time quantum walks (CTQWs) in a
range of network topologies under different decoherence mechanisms, defining
stability as the system's ability to preserve quantum properties over time. The
networks studied range from homogeneous to heterogeneous structures, including
cycle, complete, Erd\H{o}s-R\'enyi, small-world, scale-free, and star
topologies. The decoherence models considered are intrinsic decoherence,
Haken-Strobl noise, and quantum stochastic walks (QSWs). To assess quantum
stability, we employ several metrics: node occupation probabilities, the
$\ell_1$-norm of coherence, fidelity with the initial state, quantum-classical
distance, and von Neumann entropy. Our results reveal that the interplay of
both network topology and decoherence model influences coherence preservation.
Intrinsic decoherence results in the slowest decay of coherence, followed by
Haken-Strobl noise, while QSW causes the most rapid loss of coherence. The
stability ranking among network topologies varies depending on the decoherence
model and quantifier used. For example, under Haken-Strobl and intrinsic
decoherence, the quantum-classical distance ranks the cycle network more stable
than scale-free networks, although other metrics consistently favour scale-free
topologies. In general, heterogeneous networks, such as star and scale-free
networks, exhibit the highest stability, whereas homogeneous topologies, such
as cycle and Erd\H{o}s-R\'enyi networks, are more vulnerable to decoherence.
The complete graph, despite its homogeneity, remains highly stable due to its
dense connectivity. Furthermore, in heterogeneous networks, the centrality of
the initialised node, measured by degree or closeness, has a pronounced impact
on stability, underscoring the role of local topological features in quantum
dynamics.

</details>


### [43] [Advancing the hBN Defects Database through Photophysical Characterization of Bulk hBN](https://arxiv.org/abs/2507.18093)
*Chanaprom Cholsuk,Sujin Suwanna,Tobias Vogl*

Main category: quant-ph

TL;DR: A database of bulk hBN defects with photophysical properties is created to bridge theory-experiment gaps and support quantum emitter identification and machine learning in quantum materials.


<details>
  <summary>Details</summary>
Motivation: Address discrepancies between theoretical (monolayer) and experimental (bulk) studies of hBN defects by providing a comprehensive bulk defect database.

Method: Systematically evaluate over 120 neutral defects (600 total) across charge states, computing properties like zero-phonon line, photoluminescence, and electron-phonon coupling.

Result: Vacancies strongly influence electron-phonon coupling, and the Huang-Rhys factor correlates with configuration coordinates.

Conclusion: The database and API (https://h-bn.info) aim to enhance quantum emitter identification and integrate with machine learning workflows.

Abstract: Quantum emitters in hexagonal boron nitride (hBN) have gained significant
attention due to a wide range of defects that offer high quantum efficiency and
single-photon purity at room temperature. Most theoretical studies on hBN
defects simulate monolayers, as this is computationally cheaper than
calculating bulk structures. However, most experimental studies are carried out
on multilayer to bulk hBN, which creates additional possibilities for
discrepancies between theory and experiment. In this work, we present an
extended database of hBN defects that includes a comprehensive set of bulk hBN
defects along with their excited-state photophysical properties. The database
features over 120 neutral defects, systematically evaluated across charge
states ranging from -2 to 2 (600 defects in total). For each defect, the most
stable charge and spin configurations are identified and used to compute the
zero-phonon line, photoluminescence spectrum, absorption spectrum, Huang-Rhys
(HR) factor, interactive radiative lifetimes, transition dipole moments, and
polarization characteristics. Our analysis reveals that the electron-phonon
coupling strength is primarily influenced by the presence of vacancies, which
tend to induce stronger lattice distortions and broaden phonon sidebands.
Additionally, correlation analysis shows that while most properties are
independent, the HR factor strongly correlates with the configuration
coordinates. All data are publicly available at https://h-bn.info, along with a
new application programming interface (API) to facilitate integration with
machine learning workflows. This database is therefore designed to bridge the
gap between theory and experiment, aid in the reliable identification of
quantum emitters, and support the development of machine-learning-driven
approaches in quantum materials research.

</details>


<div id='astro-ph.GA'></div>

# astro-ph.GA [[Back]](#toc)

### [44] [Numerical Study of Bar Suppression in Galaxy Models Due to Disc Heating](https://arxiv.org/abs/2507.18083)
*Alejandro López Gómez,Ruslan Gabbasov,Isaura Luisa Fuentes-Carrera*

Main category: astro-ph.GA

TL;DR: The study explores how softening parameter (ε) and disc mass fraction (m_d) influence bar formation in galaxies via N-body simulations, revealing complex interactions and numerical heating effects.


<details>
  <summary>Details</summary>
Motivation: Understanding the controversial dynamics of bar formation, evolution, and destruction in galaxies, particularly how physical and numerical parameters like ε and m_d affect these processes.

Method: Conducted N-body simulations with varying particle resolutions, analyzing the impact of ε and m_d on bar strength, distortion parameter (η), and numerical heating.

Result: Bar formation is delayed by higher m_d but not consistently; small ε values cause numerical heating and bar suppression. High-resolution models with small ε fail to reproduce bar instability.

Conclusion: The interplay of ε and m_d significantly affects bar dynamics, with numerical heating from small ε introducing chaos. The disc's vertical acceleration profile reliably indicates numerical heating.

Abstract: The process of bar formation, evolution and destruction is still a
controversial topic regarding galaxy dynamics. Numerical simulations show that
these phenomena strongly depend on physical and numerical parameters. In this
work, we study the combined influence of the softening parameter, $\epsilon$
and disc mass fraction, $m_{\mathrm{d}}$ on the formation and evolution of bars
in isolated disc-halo models via $N$-body simulations with different particle
resolutions. Previous studies indicate that the bar strength depends on
$m_{\mathrm{d}}$ as $\propto m_{\mathrm{d}}^{-1}$, which is seen as a delay in
bar formation. However, the distorsion parameter, $\eta$, which measures the
bar's momentum through time, shows that an increase in $m_{\mathrm{d}}$ does
not always induce a delay in bar formation. This suggests that $\epsilon$
interact to either enhance or weaken the bar. Moreover, numerical heating
dominates in models with small softening values, creating highly accelerated
particles at the centre of discs, regardless of $m_{\mathrm{d}}$ or resolution.
These enhanced particle accelerations produce chaotic orbits for $\epsilon \leq
5\,$pc, resulting in bar suppression due to collisional dynamics in the centre.
In our high resolution models ($N \approx 10^{7}$), small softening values are
incapable of reproducing the bar instability. The role of disc mass is as
follows: increasing $m_{\mathrm{d}}$ for moderate $\epsilon$ ($\geq 10\,$pc)
reduces the amount of drift in the acceleration profile, without affecting the
bar's behaviour. Models with lower $m_{\mathrm{d}}$ values coupled with small
softening values, have an excess of highly accelerated particles, introducing
unwanted effects into otherwise reliable simulations. Finally, we show that the
evolution of the disc's vertical acceleration profile is a reliable indicator
of numerical heating introduced by $\epsilon$ and the bar.

</details>


<div id='nlin.CD'></div>

# nlin.CD [[Back]](#toc)

### [45] [Toda lattice formed in nonequilibrium steady states of SWCNT](https://arxiv.org/abs/2507.18412)
*Heeyuen Koh,Shigeo Maruyama*

Main category: nlin.CD

TL;DR: The paper connects thermal conductivity in nanoscale low-dimensional systems to Toda lattice dynamics, using a coarse-grained molecular dynamics model derived from nonequilibrium simulations.


<details>
  <summary>Details</summary>
Motivation: To explain the length dependency of high thermal conductivity in low-dimensional systems by linking nonequilibrium steady states to Toda lattice equilibrium dynamics.

Method: A numerically driven coarse-grained molecular dynamics (CGMD) model is developed from nonequilibrium molecular dynamics (NEMD) data, incorporating longitudinal and flexural modulation as a separate Hamiltonian with a perturbation term.

Result: The model shows that the potential energy function from nonequilibrium states matches the Toda lattice under specific conditions, supported by numerical data.

Conclusion: The hypothesis successfully bridges nonequilibrium thermal conductivity with Toda lattice dynamics, validated by the CGMD model.

Abstract: Toda lattice or FPUT chain-like dynamics have been regarded as the
prerequisite condition to explain the length dependency of high thermal
conductivity of low-dimensional systems at the nanoscale. In this paper, a
hypothetical condition is introduced that establishes a theoretical connection
between the thermal conductivity of a nanoscale low-dimensional system in
nonequilibrium steady states(NESS) and the canonical motion of the equation in
the Toda lattice in equilibrium. The hypothesis relies on a numerically driven
coarse grained molecular dynamics system acquired from the trajectory data of
nonequilibrium molecular dynamics(NEMD) simulation. It models the macroscopic
motion from longitudinal and flexural modulation observed in NEMD as a separate
Hamiltonian in CGMD with a perturbation term governed by an overdamping
process, which is assumed to be dominant during heat transfer. The Smoluchowski
equation for the perturbation, which is derived from the cross correlated
states between two degrees of freedom, suggests that the potential energy
function induced from NESS is identical to that of Toda Lattice under the
specific condition in the partition function for CG particles. The restrictions
derived from the model are well confirmed by the data from the numerically
driven CG model.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [46] [On zero-order consistency residue and background pressure for the conservative SPH fluid dynamics](https://arxiv.org/abs/2507.18210)
*Feng Wang,Xiangyu Hu*

Main category: physics.flu-dyn

TL;DR: The paper addresses the zero-order consistency issue in SPH, linking it to numerical damping in pressure-driven and gravity-driven flows, and proposes the reverse kernel gradient correction technique to mitigate the issue.


<details>
  <summary>Details</summary>
Motivation: To understand and mitigate the non-physical numerical damping in SPH simulations caused by zero-order gradient consistency residue, particularly in pressure-driven channel flows and gravity-driven free-surface flows.

Method: Theoretical analysis and numerical experiments are conducted to study the residue's behavior, focusing on factors like water depth, dynamic pressure, channel length, resolution, and outlet pressure. The reverse kernel gradient correction technique is introduced and tested.

Result: The reverse kernel gradient correction reduces the residue effect but has fundamental limitations. The FDA nozzle test demonstrates the residue's impact in complex geometries.

Conclusion: Correction schemes are necessary in scenarios with high background pressure, as the residue significantly affects flow simulations, and the proposed technique offers partial mitigation.

Abstract: As one of the major challenges for the conservative smoothed particle
hydrodynamics (SPH) method, the zero-order consistency issue, although thought
to be mitigated by the particle regularization scheme, such as the transport
velocity formulation, significantly damps the flow in a long channel for both
laminar and turbulent simulations. Building on this finding, this paper not
only thoroughly analyzes the damping reason in this pressure-driven channel
flow, but also relates this problem with the excessive numerical dissipation in
the gravity-driven free-surface flow. The common root cause of the non-physical
numerical damping in the two typical flow scenarios, the zero-order gradient
consistency residue, is exposed. The adverse influence of the background
pressure on the residue for the two scenarios is revealed and discussed. To
comprehensively understand the behavior of the residue and mitigate its
potential adverse effects, we conduct both theoretical analysis and numerical
experiments focusing on the key sensitive factors. For studying the
residue-induced non-physical energy dissipation in the gravity-driven
free-surface flow, the water depth and input dynamic pressure in the inviscid
standing wave case are tested. To investigate the velocity loss in the
pressure-driven channel flow, we examine the effects of the channel length,
resolution, and outlet pressure. The state-of-the-art reverse kernel gradient
correction technique is introduced for the two typical flows, and proved to be
effective in reducing the residue effect, but we find its correction capability
is fundamentally limited. Finally, the FDA nozzle, an engineering benchmark, is
tested to demonstrate the residue influence in a complex geometry, highlighting
the necessity of correction schemes in scenarios with unavoidable high
background pressure.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [47] [Deep Variational Free Energy Calculation of Hydrogen Hugoniot](https://arxiv.org/abs/2507.18540)
*Zihang Li,Hao Xie,Xinyang Dong,Lei Wang*

Main category: cond-mat.str-el

TL;DR: A deep variational free energy framework using three generative models computes hydrogen's equation of state in warm dense matter, validated against deuterium Hugoniot data.


<details>
  <summary>Details</summary>
Motivation: To resolve discrepancies in theoretical and experimental results for hydrogen's equation of state in the warm dense matter region.

Method: Uses three deep generative models: a normalizing flow for nuclei, an autoregressive transformer for excited electrons, and a permutational equivariant flow for Hartree-Fock electrons, optimized to minimize variational free energy.

Result: Provides the equation of state and thermodynamic properties of dense hydrogen, benchmarked against deuterium Hugoniot data.

Conclusion: The framework offers a reliable benchmark for deuterium in warm dense matter, addressing prior inconsistencies.

Abstract: We develop a deep variational free energy framework to compute the equation
of state of hydrogen in the warm dense matter region. This method parameterizes
the variational density matrix of hydrogen nuclei and electrons at finite
temperature using three deep generative models: a normalizing flow model that
represents the Boltzmann distribution of the classical nuclei, an
autoregressive transformer that models the distribution of electrons in excited
states, and a permutational equivariant flow model that constructs backflow
coordinates for electrons in Hartree-Fock orbitals. By jointly optimizing the
three neural networks to minimize the variational free energy, we obtain the
equation of state and related thermodynamic properties of dense hydrogen. We
compare our results with other theoretical and experimental results on the
deuterium Hugoniot curve, aiming to resolve existing discrepancies. The
calculated results provide a valuable benchmark for deuterium in the warm dense
matter region.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [48] [Jacobi Hamiltonian Integrators](https://arxiv.org/abs/2507.18573)
*Adérito Araújo,Gonçalo Inocêncio Oliveira,João Nuno Mestre*

Main category: math.DG

TL;DR: A method for constructing structure-preserving integrators for Hamiltonian systems in Jacobi manifolds is developed, extending techniques from Poisson Hamiltonian Integrators (PHI) to handle time-dependent and dissipative phenomena.


<details>
  <summary>Details</summary>
Motivation: Jacobi manifolds generalize symplectic and Poisson structures, making them suitable for modeling dissipative and thermodynamic systems. Existing methods like PHI need extension to preserve structure in Jacobi systems.

Method: The approach leverages the correspondence between Jacobi and homogeneous Poisson manifolds, adapting PHI techniques while preserving homogeneity. Theoretical tools and numerical integration methods are developed.

Result: The work provides a theoretical foundation and numerical technique for structure-preserving integration in Jacobi dynamics, focusing on homogeneous Poisson perspectives.

Conclusion: The method extends PHI to Jacobi manifolds, enabling structure-preserving integration for time-dependent and dissipative systems.

Abstract: We develop a method of constructing structure-preserving integrators for
Hamiltonian systems in Jacobi manifolds. Hamiltonian mechanics, rooted in
symplectic and Poisson geometry, has long provided a foundation for modelling
conservative systems in classical physics. Jacobi manifolds, generalizing both
contact and Poisson manifolds, extend this theory and are suitable for
incorporating time-dependent, dissipative and thermodynamic phenomena.
  Building on recent advances in geometric integrators - specifically Poisson
Hamiltonian Integrators (PHI), which preserve key features of Poisson systems -
we propose a construction of Jacobi Hamiltonian Integrators. Our approach
explores the correspondence between Jacobi and homogeneous Poisson manifolds,
with the aim of extending the PHI techniques while ensuring preservation of the
homogeneity structure.
  This work develops the theoretical tools required for this generalization and
outlines a numerical integration technique compatible with Jacobi dynamics. By
focusing on the homogeneous Poisson perspective rather than on direct contact
realizations, we provide a clear pathway for structure-preserving integration
of time-dependent and dissipative systems within the Jacobi framework.

</details>


### [49] [Constant mean curvature Radial graphs over domains of $\mathbb{S}^n$](https://arxiv.org/abs/2507.18496)
*Flávio Cruz,José T. Cruz,Jocel Oliveira*

Main category: math.DG

TL;DR: Existence of hypersurfaces with constant mean curvature and prescribed boundary in Euclidean space, extending Serrin's result to positive mean curvature.


<details>
  <summary>Details</summary>
Motivation: To generalize Serrin's classical result by including cases of positive constant mean curvature for hypersurfaces with prescribed boundaries.

Method: Represent hypersurfaces as radial graphs over domains of the unit sphere, assuming positive mean curvature of the boundary and existence of a subsolution for the Dirichlet problem.

Result: Demonstrates the existence of such hypersurfaces under the given assumptions.

Conclusion: Extends Serrin's result to positive constant mean curvature, providing a broader framework for studying hypersurfaces.

Abstract: We establish the existence of hypersurfaces with constant mean curvature and
a prescribed boundary in Euclidean space, represented as radial graphs over
domains of the unit sphere. Under the assumptions that the mean curvature of
the domain's boundary is positive and that a subsolution exists for the
associated Dirichlet problem, we extend Serrin's classical result to include
the case of positive constant mean curvature.

</details>


<div id='math.CO'></div>

# math.CO [[Back]](#toc)

### [50] [Guessing sequences of eigenvectors for LMPs defining spectrahedral relaxations of Eulerian rigidly convex sets](https://arxiv.org/abs/2507.18434)
*Alejandro González Nevado*

Main category: math.CO

TL;DR: The paper improves bounds for extreme roots of univariate Eulerian polynomials, enhancing accuracy in spectrahedral relaxations for Eulerian rigidly convex sets. Numerical experiments yield a growing exponential difference from prior bounds.


<details>
  <summary>Details</summary>
Motivation: To enhance the accuracy of spectrahedral relaxations for Eulerian rigidly convex sets by improving bounds for extreme roots of Eulerian polynomials.

Method: Uses numerical experiments to construct a sequence of vectors, linearizing bounds and comparing them to previous results.

Result: A new bound is established, growing exponentially with n, outperforming prior bounds.

Conclusion: The improved bound provides a better measure of accuracy for spectrahedral relaxations, advancing the understanding of Eulerian rigidly convex sets.

Abstract: Stable multivariate Eulerian polynomials were introduced by Br\"and\'en.
Particularizing some variables, it is possible to extract real zero
multivariate Eulerian polynomials from them. These real zero multivariate
Eulerian polynomials can be fed into constructions of spectrahedral relaxations
providing therefore approximations to the (Eulerian) rigidly convex sets
defined by these polynomials. The accuracy of these approximations is measured
through the behaviour in the diagonal, where the usual univariate Eulerian
polynomials sit. In particular, in this sense, the accuracy of the global
spectrahedral approximation produced by the spectrahedral relaxation can be
measured in terms of bounds for the extreme roots of univariate Eulerian
polynomials. The bounds thus obtained beat the previous bounds found in the
literature. However, the bound explicitly studied and obtained before beat the
previously known bounds by a quantity going to $0$ when $n$ goes to infinity.
Here we use numerical experiments to construct a sequence of vectors providing
a (linearized) bound whose difference with the previous known bounds is a
growing exponential function (going therefore fast to infinity when $n$ grows).
This allows us to establish a better (diagonal) measure of accuracy for the
spectrahedral relaxation of the Eulerian rigidly convex sets. In particular, we
will achieve this by linearizing through the sequence of vectors
$\{(y,(-2^{m-i})_{i=3}^{m},(0,\frac{1}{2}),(1)_{i=1}^{m})\in\mathbb{R}^{n+1}\}_{n=1}^{\infty}$
for even $n=2m$.

</details>


<div id='math.CV'></div>

# math.CV [[Back]](#toc)

### [51] [Formally Integrable Structures III. Levi Flat Structures](https://arxiv.org/abs/2507.18341)
*Qingchun Ji,Jun Yao*

Main category: math.CV

TL;DR: The paper constructs differential complexes using formal integrability, resolving basic vector bundle sections and realizing the Morse-Novikov-Treves complex. It introduces convexity for Hermitian metrics on basic line bundles, proving vanishing theorems and global solvability. For elliptic structures, it addresses singular cohomology and canonical forms.


<details>
  <summary>Details</summary>
Motivation: To resolve basic sections of vector bundles and globally realize the Morse-Novikov-Treves complex, leveraging formal integrability and convexity in Hermitian metrics.

Method: Uses formal integrability to build differential complexes, introduces convexity for Hermitian metrics, and applies these to Levi flat and elliptic structures.

Result: Vanishing theorems, global solvability of the Morse-Novikov-Treves complex, and results on singular cohomology and canonical forms in elliptic structures.

Conclusion: The framework resolves basic sections and realizes the Morse-Novikov-Treves complex, with applications in vanishing theorems and global solvability, extending to elliptic structures.

Abstract: In this paper, we utilize formal integrability to construct a class of
differential complexes, thereby providing a resolution for the sheaf of basic
sections of a basic vector bundle, as well as a global realization of the
Morse-Novikov-Treves complex. In the context of Levi flat structures, we
introduce a notion of convexity for each Hermitian metric on a basic line
bundle, which enables us to establish vanishing theorems and the global
solvability of the Morse-Novikov-Treves complex. In the special case of
elliptic structures, we further obtain results concerning singular cohomology
and the extension problem for canonical forms.

</details>


### [52] [A new approach to the Monge-Ampère eigenvalue problem](https://arxiv.org/abs/2507.18409)
*Chinh H. Lu,Ahmed Zeriahi*

Main category: math.CV

TL;DR: The paper studies the eigenvalue problem for the complex Monge-Ampère operator in hyperconvex domains, proving uniqueness of eigenfunctions and providing a Rayleigh quotient formula. It introduces an iterative method for eigenvalues and eigenfunctions under continuity assumptions, using plurisubharmonic envelopes. The approach simplifies existing arguments and extends to complex Hessian operators and the real Monge-Ampère operator.


<details>
  <summary>Details</summary>
Motivation: To address the eigenvalue problem for the complex Monge-Ampère operator in bounded hyperconvex domains, particularly with non-pluripolar measures, and to unify and simplify existing methods.

Method: Relies on plurisubharmonic envelopes to partially sublinearize the nonlinear problem, introducing an iterative procedure for eigenvalues and eigenfunctions under continuity assumptions.

Result: Uniqueness of eigenfunctions up to constants, a Rayleigh quotient formula, and an iterative method for solutions. The approach simplifies existing arguments and extends to other operators.

Conclusion: The method is novel and effective, providing new results and simplifying prior work, with applications to complex Hessian and real Monge-Ampère operators.

Abstract: We study the eigenvalue problem for the complex Monge-Amp\`ere operator in
bounded hyperconvex domains in $\C^n$, where the right-hand side is a
non-pluripolar positive Borel measure. We establish the uniqueness of
eigenfunctions in the finite energy class introduced by Cegrell, up to positive
multiplicative constants, and provide a Rayleigh quotient type formula for
computing the eigenvalue.
  Under a natural continuity assumption on the measure, we further show that
both the eigenvalue and eigenfunctions can be obtained via an iterative
procedure starting from any negative finite energy function.
  Our approach relies on the fine properties of plurisubharmonic envelopes,
which allow a partial sublinearization of the nonlinear problem. As far as we
know, this method is new, even in the linear case, and not only yields new
results but also significantly simplifies existing arguments in the literature.
Moreover, it extends naturally to the setting of complex Hessian operators.
  Finally, by translating our results from the complex Monge-Amp\`ere setting
via a logarithmic transformation, we also obtain several interesting analogues
for the real Monge-Amp\`ere operator.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [53] [Stability of Big Bang singularity for the Einstein-Maxwell-scalar field-Vlasov system in the full strong sub-critical regime](https://arxiv.org/abs/2507.18585)
*Xinliang An,Taoran He,Dawei Shen*

Main category: gr-qc

TL;DR: Study of Kasner solutions' stability in $3+1$ dimensions for the Einstein-Maxwell-scalar field-Vlasov system, extending prior work to include Vlasov field complexities.


<details>
  <summary>Details</summary>
Motivation: To analyze stability in a system combining gravity, electromagnetic, and particle interactions, addressing challenges posed by the Vlasov field.

Method: Detailed mathematical analysis and new arguments to identify a strong sub-critical regime for stability.

Result: Proven nonlinear stability of Kasner solutions within the identified regime, extending prior results.

Conclusion: The study successfully extends stability analysis to a more complex system, highlighting the role of the Vlasov field.

Abstract: In $3+1$ dimensions, we study the stability of Kasner solutions for the
Einstein-Maxwell-scalar field-Vlasov system. This system incorporates gravity,
electromagnetic, weak and strong interactions for the initial stage of our
universe. Due to the presence of the Vlasov field, various new challenges
arise. By observing detailed mathematical structures and designing new delicate
arguments, we identify a new strong sub-critical regime and prove the nonlinear
stability with Kasner exponents lying in this full regime. This extends the
result of Fournodavlos-Rodnianski-Speck [8] from the Einstein-scalar field
system to the physically more complex system with the Vlasov field.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [54] [Data assimilation with the 2D Navier-Stokes equations: Optimal Gaussian asymptotics for the posterior measure](https://arxiv.org/abs/2507.18279)
*Dimiri Konen,Richard Nickl*

Main category: math.ST

TL;DR: The paper proves a functional Bernstein-von Mises theorem for posterior measures in a data assimilation problem with the 2D Navier-Stokes equation, showing Gaussian approximation of the posterior and its implications for uncertainty quantification.


<details>
  <summary>Details</summary>
Motivation: To establish theoretical guarantees for Bayesian data assimilation in nonlinear systems like the Navier-Stokes equation, focusing on posterior approximation and uncertainty quantification.

Method: Assigns a Gaussian process prior to the initial condition, analyzes the posterior measure via a linear parabolic PDE with Gaussian initial condition, and evaluates approximation in the supremum norm.

Result: The posterior is approximated by a Gaussian random vector field, yielding $1/\sqrt{N}$-consistent estimators for future state prediction. Bayesian methods attain the local asymptotic minimax lower bound.

Conclusion: The study provides strong theoretical support for Bayesian data assimilation in nonlinear systems, with practical implications for credible bands and uncertainty quantification.

Abstract: A functional Bernstein - von Mises theorem is proved for posterior measures
arising in a data assimilation problem with the two-dimensional Navier-Stokes
equation where a Gaussian process prior is assigned to the initial condition of
the system. The posterior measure, which provides the update in the space of
all trajectories arising from a discrete sample of the (deterministic)
dynamics, is shown to be approximated by a Gaussian random vector field arising
from the solution to a linear parabolic PDE with Gaussian initial condition.
The approximation holds in the strong sense of the supremum norm on the
regression functions, showing that predicting future states of Navier-Stokes
systems admits $1/\sqrt N$-consistent estimators even for commonly used
nonparametric models. Consequences for coverage of credible bands and
uncertainty quantification are discussed. A local asymptotic minimax theorem is
derived that describes the lower bound for estimating the state of the
nonlinear system, which is shown to be attained by the Bayesian data
assimilation algorithm.

</details>


<div id='hep-th'></div>

# hep-th [[Back]](#toc)

### [55] [Analytic Regression of Feynman Integrals from High-Precision Numerical Sampling](https://arxiv.org/abs/2507.17815)
*Oscar Barrera,Aurélien Dersy,Rabia Husain,Matthew D. Schwartz,Xiaoyuan Zhang*

Main category: hep-th

TL;DR: A method combining high-precision numerical integration and analytic knowledge of function spaces to deduce exact answers for problems like Feynman integrals, using lattice reduction.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of obtaining exact analytic descriptions from numerical data, especially in cases like Feynman integrals where approximations are insufficient.

Method: Combine high-precision numerical integration with analytic insights about the function space, using lattice reduction to deduce exact answers.

Result: Demonstrated effectiveness for Feynman integrals, with examples showing trade-offs between data points, precision, and computational resources.

Conclusion: The method offers a bottom-up approach complementing top-down analytic methods, applicable broadly where exact answers are needed and function spaces are understood.

Abstract: In mathematics or theoretical physics one is often interested in obtaining an
exact analytic description of some data which can be produced, in principle, to
arbitrary accuracy. For example, one might like to know the exact analytical
form of a definite integral. Such problems are not well-suited to numerical
symbolic regression, since typical numerical methods lead only to
approximations. However, if one has some sense of the function space in which
the analytic result should lie, it is possible to deduce the exact answer by
judiciously sampling the data at a sufficient number of points with sufficient
precision. We demonstrate how this can be done for the computation of Feynman
integrals. We show that by combining high-precision numerical integration with
analytic knowledge of the function space one can often deduce the exact answer
using lattice reduction. A number of examples are given as well as an
exploration of the trade-offs between number of datapoints, number of
functional predicates, precision of the data, and compute. This method provides
a bottom-up approach that neatly complements the top-down Landau-bootstrap
approach of trying to constrain the exact answer using the analytic structure
alone. Although we focus on the application to Feynman integrals, the
techniques presented here are more general and could apply to a wide range of
problems where an exact answer is needed and the function space is sufficiently
well understood.

</details>


<div id='physics.acc-ph'></div>

# physics.acc-ph [[Back]](#toc)

### [56] [A Supervised Machine Learning Framework for Multipactor Breakdown Prediction in High-Power Radio Frequency Devices and Accelerator Components: A Case Study in Planar Geometry](https://arxiv.org/abs/2507.17881)
*Asif Iqbal,John Verboncoeur,Peng Zhang*

Main category: physics.acc-ph

TL;DR: The paper applies supervised machine learning to predict multipactor susceptibility in RF devices, using simulation-derived data and evaluating models like Random Forest and MLPs. Tree-based models outperform MLPs, but performance varies by material due to feature-space issues.


<details>
  <summary>Details</summary>
Motivation: Accurate prediction of multipactor susceptibility is critical for RF and accelerator systems but computationally intensive. Machine learning offers a potential solution.

Method: Simulation-derived data from six SEY material profiles trains regression models (RF, ET, XGBoost, MLPs). Performance is evaluated using IoU, SSIM, and Pearson correlation.

Result: Tree-based models generalize better than MLPs. MLPs with combined objective functions perform well. PCA shows performance degradation due to disjoint feature spaces.

Conclusion: ML shows promise for multipactor prediction but requires broader dataset coverage for robustness. The study advances data-driven RF and accelerator design.

Abstract: Multipactor is a nonlinear electron avalanche phenomenon that can severely
impair the performance of high-power radio frequency (RF) devices and
accelerator systems. Accurate prediction of multipactor susceptibility across
different materials and operational regimes remains a critical yet
computationally intensive challenge in accelerator component design and RF
engineering. This study presents the first application of supervised machine
learning (ML) for predicting multipactor susceptibility in two-surface planar
geometries. A simulation-derived dataset spanning six distinct secondary
electron yield (SEY) material profiles is used to train regression models -
including Random Forest (RF), Extra Trees (ET), Extreme Gradient Boosting
(XGBoost), and funnel-structured Multilayer Perceptrons (MLPs) - to predict the
time-averaged electron growth rate, ${\delta}_{avg}$. Performance is evaluated
using Intersection over Union (IoU), Structural Similarity Index (SSIM), and
Pearson correlation coefficient. Tree-based models consistently outperform MLPs
in generalizing across disjoint material domains. MLPs trained using a
scalarized objective function that combines IoU and SSIM during Bayesian
hyperparameter optimization with 5-fold cross-validation outperform those
trained with single-objective loss functions. Principal Component Analysis
reveals that performance degradation for certain materials stems from disjoint
feature-space distributions, underscoring the need for broader dataset
coverage. This study demonstrates both the promise and limitations of ML-based
multipactor prediction and lays the groundwork for accelerated, data-driven
modeling in advanced RF and accelerator system design.

</details>


### [57] [Advanced Ceramic Plasma Discharge Capillaries for high repetition rate operation](https://arxiv.org/abs/2507.18226)
*Lucio Crincoli,Romain Demitra,Valerio Lollo,Donato Pellegrini,Marco Pitti,Lucilla Pronti,Martina Romani,Massimo Ferrario,Angelo Biagioni*

Main category: physics.acc-ph

TL;DR: The paper presents a ceramic-based discharge capillary design for high-repetition-rate plasma accelerators, validated experimentally and numerically.


<details>
  <summary>Details</summary>
Motivation: To address the need for durable plasma sources in high-repetition-rate applications like particle accelerators and light sources.

Method: Experimental testing at 10-150 Hz and numerical simulations to analyze heat transfer and longevity.

Result: Ceramic capillaries maintain plasma properties and source integrity at high repetition rates (100-400 Hz).

Conclusion: The design is suitable for high-repetition-rate applications, such as the EuPRAXIA@SPARC_LAB project.

Abstract: In view of future applications of plasma-based particle accelerators, within
the fields of high-energy physics and new light sources, the capability of
plasma sources to operate at high repetition rates is crucial. In particular
for gas-filled plasma discharge capillaries, which allow direct control over
plasma properties, a key aspect is the longevity of the material, subject to
erosion due to the heat flux delivered by high voltage plasma discharges. In
this regard, we present an innovative design of discharge capillaries based on
the use of different ceramic materials, which can sustain high voltage plasma
discharges at high repetition rate and, moreover, be easily machined for the
complex geometries required for plasma-based accelerators. Experimental
campaigns are carried out at 10-150 Hz, assessing the longevity of ceramic
capillaries by means of different diagnostic techniques. In addition, numerical
simulations are performed to analyze the heat transfer within the whole plasma
source. Results from experimental and numerical analysis highlight the
capability of ceramic capillaries to preserve plasma properties and the
integrity of the source during long-term plasma discharge operation at high
repetition rate. In particular, we demonstrated the suitability of the proposed
solution for the operative range of 100-400 Hz, foreseen for EuPRAXIA@SPARC_LAB
project.

</details>
