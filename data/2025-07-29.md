<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 19]
- [math.AP](#math.AP) [Total: 29]
- [physics.comp-ph](#physics.comp-ph) [Total: 5]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 11]
- [nucl-th](#nucl-th) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 3]
- [math.OC](#math.OC) [Total: 4]
- [cs.LG](#cs.LG) [Total: 3]
- [hep-ph](#hep-ph) [Total: 1]
- [cs.GR](#cs.GR) [Total: 1]
- [physics.optics](#physics.optics) [Total: 2]
- [math.SP](#math.SP) [Total: 1]
- [math.FA](#math.FA) [Total: 3]
- [quant-ph](#quant-ph) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [math.DS](#math.DS) [Total: 1]
- [stat.ML](#stat.ML) [Total: 1]
- [math.ST](#math.ST) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [physics.app-ph](#physics.app-ph) [Total: 1]
- [math.CA](#math.CA) [Total: 1]
- [gr-qc](#gr-qc) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Interpolation-Based Gradient-Error Bounds for Use in Derivative-Free Optimization of Noisy Functions](https://arxiv.org/abs/2507.19661)
*Alejandro G. Marchetti,Dominique Bonvin*

Main category: math.NA

TL;DR: The paper analyzes gradient estimation errors from linear interpolation under noise, decomposing errors into deterministic and stochastic parts, and proposes less conservative bounds for DFO.


<details>
  <summary>Details</summary>
Motivation: To understand and improve gradient estimation accuracy in noisy environments, addressing the conservatism of existing bounds.

Method: Decompose gradient error, derive upper bounds, compare them, and propose approximate bounds. Implement a DFO scheme with these bounds as constraints.

Result: Classical bounds are overly conservative; proposed approximate bounds offer more realistic error estimates.

Conclusion: The study suggests using approximate gradient error bounds in DFO for better performance, demonstrated through a sequential programming approach.

Abstract: In this paper, we analyze the accuracy of gradient estimates obtained by
linear interpolation when the underlying function is subject to bounded
measurement noise. The total gradient error is decomposed into a deterministic
component arising from the interpolation (finite-difference) approximation, and
a stochastic component due to noise. Various upper bounds for both error
components are derived and compared through several illustrative examples. Our
comparative study reveals that strict deterministic bounds, including those
commonly used in derivative-free optimization (DFO), tend to be overly
conservative. To address this, we propose approximate gradient error bounds
that aim to upper bound the gradient error norm more realistically, without the
excessive conservatism of classical bounds. Finally, drawing inspiration from
dual real-time optimization strategies, we present a DFO scheme based on
sequential programming, where the approximate gradient error bounds are
enforced as constraints within the optimization problem.

</details>


### [2] [Implementation and Basis Construction for Smooth Finite Element Spaces](https://arxiv.org/abs/2507.19732)
*Chunyu Chen,Long Chen,Tingyi Gao,Xuehai Huang,Huayi Wei*

Main category: math.NA

TL;DR: The paper presents an explicit, computable framework for $C^m$ conforming finite elements, addressing the gap between theory and practical computation by modifying degrees of freedom to align with Bernstein polynomials.


<details>
  <summary>Details</summary>
Motivation: Prior work lacked explicit basis functions, limiting practical use, while spline theory faced challenges in stable, locally supported bases. This work bridges the gap for computational applications.

Method: Modifies degrees of freedom (moments of normal derivatives) to align with Bernstein polynomial dual basis, enabling structured local bases on simplices.

Result: Develops explicit, computable framework for smooth finite elements, facilitating efficient matrix assembly and global continuity.

Conclusion: The work closes the theory-practice gap, making smooth finite element methods viable for high-order PDEs and computational applications.

Abstract: The construction of $C^m$ conforming finite elements on simplicial meshes has
recently advanced through the groundbreaking work of Hu, Lin, and Wu (Found.
Comput. Math. 24, 2024). Their framework characterizes smoothness via moments
of normal derivatives over subsimplices, leading to explicit degrees of freedom
and unisolvence, unifying earlier constructions. However, the absence of
explicit basis functions has left these spaces largely inaccessible for
practical computation. In parallel, multivariate spline theory (Chui and Lai,
J. Approx. Theory 60, 1990) enforces $C^m$ smoothness through linear
constraints on Bernstein--B\'{e}zier coefficients, but stable, locally
supported bases remain elusive beyond low dimensions. Building on the geometric
decomposition of the simplicial lattice proposed by Chen and Huang (Math. Comp.
93, 2024), this work develops an explicit, computable framework for smooth
finite elements. The degrees of freedom defined by moments of normal
derivatives are modified to align with the dual basis of the Bernstein
polynomials, yielding structured local bases on each simplex. Explicit basis
construction is essential not merely for completeness, but for enabling
efficient matrix assembly, global continuity, and scalable solution of
high-order elliptic partial differential equations. This development closes the
gap between theoretical existence and practical realization, making smooth
finite element methods accessible to broad computational applications.

</details>


### [3] [A Modified Dielectric Contrast based Integral Equation for 2D TE Scattering by Inhomogeneous Domains](https://arxiv.org/abs/2507.19777)
*Akshay Pratap Singh,Kuldeep Singh,Rajendra Mitharwal*

Main category: math.NA

TL;DR: A modified domain integral equation method for TE scattering improves numerical accuracy and convergence using RWG basis functions and a redefined dielectric contrast.


<details>
  <summary>Details</summary>
Motivation: Address numerical challenges from gradient-divergence operators in traditional electric field-based formulations.

Method: Uses RWG basis functions on triangular meshes for geometric conformity, tangential continuity, and singularity extraction.

Result: Validated on a two-layered dielectric cylinder, showing agreement with analytical results and robust convergence with mesh refinement.

Conclusion: The method effectively mitigates numerical issues and enhances accuracy without increasing computational iterations.

Abstract: This work presents a modified domain integral equation approach for the
forward problem of TE scattering, employing a modified definition of dielectric
contrast and discretizing the electric field density using Rao-Wilton-Glisson
(RWG) basis functions. The proposed formulation mitigates the numerical
challenges introduced by the gradient-divergence operator in traditional
electric field-based vector formulations. The use of RWG basis functions over
triangular meshes enhances geometric conformity, ensures tangential continuity
across dielectric interfaces, and facilitates the application of well known
singularity extraction techniques for numerical accuracy. Validation through
numerical experiments on a two-layered dielectric cylinder demonstrates
excellent agreement between computed and analytical scattered fields.
Convergence studies confirm improving solution accuracy with mesh refinement
indicating robustness with respect to discretization without increasing the
iterations.

</details>


### [4] [An inverse random diffraction grating problem for the Helmholtz equation](https://arxiv.org/abs/2507.19744)
*Zhiqi Sun,Yiwen Lin*

Main category: math.NA

TL;DR: The paper addresses the inverse scattering problem for random periodic structures using a stochastic surface model based on the Wiener process, introducing a novel reconstruction method (RPSS) with Monte Carlo sampling.


<details>
  <summary>Details</summary>
Motivation: To simulate real-world grating profiles affected by manufacturing defects and surface wear, requiring a stochastic approach for accurate modeling.

Method: Proposes a stochastic surface model using Wiener process discretization, linear interpolation, and RPSS for inversion, combined with Monte Carlo and wavenumber continuation.

Result: Demonstrates effective reconstruction of key statistics of random surfaces in benchmark scenarios.

Conclusion: The framework and RPSS are successful, with potential for future extensions in reconstruction mechanisms.

Abstract: This paper investigates the inverse scattering problem of time-harmonic plane
waves incident on a perfectly reflecting random periodic structure. To simulate
random perturbations arising from manufacturing defects and surface wear in
real-world grating profiles, we propose a stochastic surface modeling framework
motivated by the discretization of the Wiener process. Our approach introduces
randomness at discrete nodes and then applies linear interpolation to construct
the surface, marking a novel attempt to incorporate the concepts of the Wiener
process into random surface representation. Under this framework, each
realization of the random surface generates a Lipschitz-continuous diffraction
grating, mathematically represented as a sum of a baseline profile and a
weighted linear combination of local `tent' basis functions, meanwhile
preserving key statistics of the random surface. Building on this
representation, we introduce the Recursive Parametric Smoothing Strategy (RPSS)
to invert the key statistics of our random surfaces. Combined with Monte Carlo
sampling and a wavenumber continuation strategy, our reconstruction scheme
demonstrates effectiveness across multiple benchmark scenarios. Several
numerical results are presented along with some discussions in the end on
reconstruction mechanisms and future extensions.

</details>


### [5] [Deep Uzawa for Kinetic Transport with Lagrange-Enforced Boundaries](https://arxiv.org/abs/2507.19907)
*Charalambos Makridakis,Aaron Pim,Tristan Pryer,Nikolaos Rekatsinas*

Main category: math.NA

TL;DR: A neural network framework for solving stationary linear transport equations with inflow boundary conditions, using a mesh-free, automatic differentiation-compatible approach.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of solving stationary linear transport equations with boundary conditions, leveraging neural networks for flexibility and accuracy.

Method: Represents the solution via a neural network, imposes boundary conditions using a Lagrange multiplier (Uzawa-inspired saddle-point formulation), and handles scattering/heterogeneous media.

Result: Convergence is proven; numerical experiments confirm accurate handling of anisotropic transport, boundary conditions, and scattering dynamics.

Conclusion: The framework is effective for solving transport equations, with potential extensions to more complex problems.

Abstract: We propose a neural network framework for solving stationary linear transport
equations with inflow boundary conditions. The method represents the solution
using a neural network and imposes the boundary condition via a Lagrange
multiplier, based on a saddle-point formulation inspired by the classical Uzawa
algorithm. The scheme is mesh-free, compatible with automatic differentiation
and extends naturally to problems with scattering and heterogeneous media. We
establish convergence of the continuum formulation and analyse the effects of
quadrature error, neural approximation and inexact optimisation in the discrete
implementation. Numerical experiments show that the method captures anisotropic
transport, enforces boundary conditions and resolves scattering dynamics
accurately.

</details>


### [6] [A Bi-fidelity numerical method for velocity discretization of Boltzmann equations](https://arxiv.org/abs/2507.19945)
*Nicolas Crouseilles,Zhen Hao,Liu Liu*

Main category: math.NA

TL;DR: A bi-fidelity algorithm for velocity discretization of Boltzmann-type kinetic equations is introduced, combining low- and high-fidelity models for efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the computational cost of high-fidelity models in kinetic equations while maintaining accuracy under multiple scales.

Method: Uses a low-fidelity model to identify key velocity points via a greedy approach, then applies a high-fidelity model only at these points to construct a surrogate. Integrates a simpler collision term in the low-fidelity model and an asymptotic-preserving scheme in the high-fidelity step.

Result: Demonstrates weak asymptotic-preserving properties and empirical error bounds. Numerical experiments confirm effectiveness and robustness across various regimes.

Conclusion: The proposed bi-fidelity method is efficient and robust for solving Boltzmann-type kinetic equations under multiple scales.

Abstract: In this paper, we introduce a bi-fidelity algorithm for velocity
discretization of Boltzmann-type kinetic equations under multiple scales. The
proposed method employs a simpler and computationally cheaper low-fidelity
model to capture a small set of significant velocity points through the greedy
approach, then evaluates the high-fidelity model only at these few velocity
points and to reconstruct a bi-fidelity surrogate. This novel method integrates
a simpler collision term of relaxation type in the low-fidelity model and an
asymptotic-preserving scheme in the high-fidelity update step. Both linear
Boltzmann under diffusive scaling and the nonlinear full Boltzmann in
hyperbolic scaling are discussed. We show the weak asymptotic-preserving
property and empirical error bound estimates. Extensive numerical experiments
on linear semiconductor and nonlinear Boltzmann problems with smooth or
discontinuous initial conditions and under various regimes have been carefully
studied, which demonstrates the effectiveness and robustness of our proposed
scheme.

</details>


### [7] [Time-continuous strongly conservative space-time finite element methods for the dynamic Biot model](https://arxiv.org/abs/2507.19955)
*Johannes Kraus,Maria Lymbery,Kevin Osthues*

Main category: math.NA

TL;DR: The paper proposes a variational space-time finite element method for the dynamic Biot model, coupling fluid flow and solid deformation in porous media, with proven error estimates and numerical validation.


<details>
  <summary>Details</summary>
Motivation: To address the interaction between fluid flow and solid deformation in saturated porous media, including wave propagation, using a three-field formulation.

Method: A family of variational space-time finite element methods combining continuous-in-time Galerkin ansatz with H(div)-conforming approximations for displacement and flux fields, and piecewise polynomial pressure approximation.

Result: Error estimates in a combined energy norm for the maximum norm in time are proven, supported by numerical experiments for various polynomial orders.

Conclusion: The proposed method is effective and validated for modeling the dynamic Biot problem, with strong conservation properties and stability.

Abstract: We consider the dynamic Biot model (see [Biot, M. A. J. Appl. Phys. 33,
1482--1498 (1962)]) describing the interaction between fluid flow and solid
deformation including wave propagation phenomena in both the liquid and solid
phases of a saturated porous medium. This model couples a hyperbolic equation
for momentum balance to a second-order in time dynamic Darcy law and a
parabolic equation for the balance of mass and is here considered in
three-field formulation with the displacement of the elastic matrix, the fluid
velocity, and the fluid pressure being the physical fields of interest.
  A family of variational space-time finite element methods is proposed, which
combines a continuous-in-time Galerkin ansatz of arbitrary polynomial degree
with $H(\mathrm{div})$-conforming approximations of the displacement field, its
time derivative, and the flux field--of discontinuous Galerkin (DG) type for
displacements--with a piecewise polynomial pressure approximation, providing an
inf-sup stable strongly conservative mixed method in each case. We prove error
estimates in a combined energy norm in space for the maximum norm in time. The
theoretical results are confirmed by numerical experiments for different
polynomial orders in space and time.

</details>


### [8] [Geometric-Perturbation-Robust Cut-Cell Scheme for Two-Material Flows: Exact Pressure-Equilibrium Preservation and Rigorous Analysis](https://arxiv.org/abs/2507.19966)
*Chaoyi Cai,Di Wu,Jianxian Qiu*

Main category: math.NA

TL;DR: A cut-cell method for compressible two-material flows is introduced, ensuring pressure equilibrium across interfaces despite small geometric perturbations. It uses evolved geometric moments and equilibrium-compatible reconstructions for stability and high-order accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing sharp-interface schemes are sensitive to interface geometry, causing spurious pressure oscillations. A robust method is needed to maintain pressure equilibrium under geometric perturbations.

Method: The method employs a cut-cell approach with strict consistency between conserved variables and geometric moments. It introduces equilibrium-compatible reconstructions and a third-order EC-MRWENO variant.

Result: The scheme achieves second-order accuracy at interfaces and third-order in smooth regions, proving robustness under geometric perturbations and topology changes.

Conclusion: The proposed framework is the first cut-cell solver that is both provably robust against geometric perturbations and genuinely high-order, validated by extensive 2D tests.

Abstract: Preserving pressure equilibrium across material interfaces is critical for
the stability of compressible multi-material flow simulations, yet most
interface-fitted sharp-interface schemes are notoriously sensitive to interface
geometry: even slight perturbations of the captured (or tracked) interface can
trigger large spurious pressure oscillations. We present a cut-cell method that
is geometric-perturbation-robust (GPR) for the compressible two-material flows.
By construction, the scheme provably preserves exact interfacial pressure
equilibrium in the presence of small interface-position errors. The key is a
strict consistency between the conserved variables and the geometric moments
(i.e., the integrals of monomials) of every cut cell. We formulate auxiliary
transport equations, whose discrete solutions furnish evolved geometric moment,
these geometric moments remain perfectly synchronized with the conserved
variables -- even on a deforming mesh. Surpassing the classical geometric
conservation law, our approach keeps all higher-order geometric moments
consistent, thereby eliminating accuracy loss due to geometric mismatches. To
prevent the reconstruction step from destroying pressure equilibrium, we
introduce the notion of equilibrium-compatible (EC) reconstructions. A
carefully designed modification equips any conventional weighted essentially
non-oscillatory (WENO) reconstruction with the EC property; we detail a
third-order EC multi-resolution WENO (EC-MRWENO) variant. The tight coupling of
EC-MRWENO with the evolved moments yields the first cut-cell solver that is
simultaneously provably GPR and genuinely high-order: it attains second-order
accuracy precisely at material interfaces, while preserving third-order
accuracy in smooth regions. Extensive two-dimensional tests confirm the
framework's robustness, accuracy and stability under geometric perturbations
and topology changes.

</details>


### [9] [A generalized ENO reconstruction in compact GKS for compressible flow simulations](https://arxiv.org/abs/2507.20461)
*Fengxiang Zhao,Kun Xu*

Main category: math.NA

TL;DR: A generalized ENO-type nonlinear reconstruction (GENO) for compressible flow simulations is introduced, combining accuracy and non-oscillatory behavior. It simplifies high-order nonlinear scheme construction and outperforms WENO methods.


<details>
  <summary>Details</summary>
Motivation: To develop a nonlinear reconstruction method that preserves accuracy while avoiding oscillations at discontinuities, simplifying high-order scheme construction, especially for unstructured meshes.

Method: GENO uses a smooth path function to connect high-order linear reconstruction with a lower-order alternative, adapting based on stencil smoothness. It is tested with compact gas-kinetic schemes and Riemann-solver-based methods.

Result: GENO shows superior accuracy, robustness, and shock-capturing capabilities compared to WENO methods, especially with compact schemes.

Conclusion: GENO advances nonlinear scheme construction, proving ENO-type reconstruction as practical for engineering applications.

Abstract: This paper presents a generalized ENO (GENO)-type nonlinear reconstruction
scheme for compressible flow simulations. The proposed reconstruction preserves
the accuracy of the linear scheme while maintaining essentially non-oscillatory
behavior at discontinuities. By generalizing the adaptive philosophy of ENO
schemes, the method employs a smooth path function that directly connects
high-order linear reconstruction with a reliable lower-order alternative. This
direct adaptive approach significantly simplifies the construction of nonlinear
schemes, particularly for very high-order methods on unstructured meshes. A
comparative analysis with various WENO methods demonstrates the reliability and
accuracy of the proposed reconstruction, which provides an optimal transition
between linear and nonlinear reconstructions across all limiting cases based on
stencil smoothness. The consistency and performance of the GENO reconstruction
are validated through implementation in both high-order compact gas-kinetic
schemes (GKS) and non-compact Riemann-solver-based methods. Benchmark tests
confirm the robustness and shock-capturing capabilities of GENO, with
particularly superior performance when integrated with compact schemes. This
work advances the construction methodology of nonlinear schemes and establishes
ENO-type reconstruction as a mature and practical approach for engineering
applications.

</details>


### [10] [Imaging a moving point source in R^3 from the time of arrival at sparse observation points](https://arxiv.org/abs/2507.20204)
*Guanqiu Ma,Haonan Zhang,Hongxia Guo*

Main category: math.NA

TL;DR: A novel numerical method for 3D trajectory reconstruction using arrival times at 5-7 observation points, proving uniqueness and stability.


<details>
  <summary>Details</summary>
Motivation: To reconstruct trajectories in 3D space without prior knowledge of emission time or location, using minimal observation points.

Method: Measures arrival times at 5-7 strategically placed observation points, leveraging their geometric configuration for uniqueness proofs.

Result: Mathematical proofs confirm trajectory and emission moment uniqueness; numerical experiments validate method effectiveness.

Conclusion: The method is effective, stable, and uniquely determines trajectories with minimal data points.

Abstract: In this paper, we introduce a novel numerical method for reconstructing the
trajectory within three-dimensional space, where both the emission moment and
spatial location of the point source are unknown. Our approach relies solely on
measuring the time of arrival at five or seven properly chosen observation
points. By utilizing the distinctive geometric configuration of these five or
seven observation points, we establish the uniqueness of the trajectory and
emission moment of the point source through rigorous mathematical proofs.
Moreover, we analyze the stability of our proposed method. The effectiveness of
the method is also verified by numerical experiments.

</details>


### [11] [Spectral element methods for boundary-value problems of functional differential equations](https://arxiv.org/abs/2507.20266)
*Alessia andò,Jan Sieber*

Main category: math.NA

TL;DR: The paper proves geometric convergence of the spectral element method for solving periodic BVPs with state-dependent delays, assuming analyticity of the solution.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of solving functional differential equations with state-dependent delays, where classical differentiability and analyticity assumptions may not hold.

Method: Uses piecewise polynomial collocation via the spectral element method for periodic boundary value problems.

Result: Geometric convergence (accuracy ~e^(-ηm)) is achieved for analytic solutions; finite-order convergence for solutions with limited differentiability.

Conclusion: The method is effective for analytic solutions, even when the right-hand side lacks classical differentiability.

Abstract: We prove convergence of the spectral element method for piecewise polynomial
collocation applied to periodic boundary value problems (BVP) for functional
  differential equations with possibly state-dependent delays. If the exact
solution of the BVP has an analytic extension then the collocation solution
converges geometrically. This means that the accuracy of the approximation is
of order $\mathrm{e}^{-\eta m}$ for some $\eta>0$ depending on the size of the
mesh, when using polynomials of degree $m$. If the exact solution has a finite
order of continuous differentiability then the collocation solution converges
with this order.
  For functional differential equations with state-dependent delays the
right-hand side cannot be expected to be differentiable with respect to its
arguments in the classical sense, and analyticity of the solution does not
necessarily follow from analyticity of the coefficients in the right-hand side.
Thus, our geometric convergence statement assumes analyticity of the solution,
rather than of the right-hand side.

</details>


### [12] [A Hybrid Particle-Continuum Method for Simulating Fast Ice via Subgrid Iceberg Interaction](https://arxiv.org/abs/2507.20306)
*Carolin Mehlmann,Saskia Kahl*

Main category: math.NA

TL;DR: A novel subgrid-scale coupling mechanism between icebergs and sea-ice models is proposed to simulate fast-ice formation, addressing a gap in current climate models.


<details>
  <summary>Details</summary>
Motivation: Current global climate models lack representation of fast-ice formation due to iceberg grounding, which occurs at subgrid scales.

Method: A hybrid particle-continuum approach integrates iceberg feedback into sea-ice dynamics using a Green's function (Stokeslet) and finite-element discretization.

Result: The method provides a stable numerical framework for simulating subgrid iceberg effects on sea-ice dynamics, demonstrated through test cases.

Conclusion: This work offers a practical solution for modeling fast-ice formation in Earth system models, compatible with existing frameworks.

Abstract: A significant fraction (4%-13%) of Antarctic sea ice remains stationary as
landfast sea-ice ("fast ice"), typically anchored by grounded icebergs. Current
global climate models do not represent fast-ice formation due to iceberg
grounding, as iceberg-sea-ice interaction mostly occurs at subgrid scales. We
propose a novel subgrid-scale coupling mechanism between Lagrangian iceberg
particles and an Eulerian sea-ice continuum model. This hybrid
particle-continuum approach integrates feedback from icebergs into the sea-ice
momentum equation via a Green's function, a Stokeslet, representing the drag
exerted by a point force on the viscous-plastic medium. The coupled system,
including the Stokeslet induced drag, is discretized using a finite-element
method with piecewise linear basis functions. The approach assumes that
individual icebergs have diameters smaller than the grid spacing. The presented
finite-element discretization is compatible with existing unstructured-mesh
ocean model frameworks such as FESOM and ICON, ensuring practical applicability
in Earth system modeling. This work provides and analyzes, for the first time,
a stable numerical framework to capture the effects of individual subgrid-scale
icebergs on sea-ice dynamics. We derive an a-priori stability estimate bounding
a functional of the sea-ice system and show that the momentum equation
including the subgrid iceberg-sea-ice drag remains stable. Numerical test cases
demonstrate the capability of the approach to capture fast-ice formation due to
subgrid iceberg grounding on coarse horizontal grids.

</details>


### [13] [Efficient numerical methods for the uncertain Boltzmann equation based on a hybrid solver](https://arxiv.org/abs/2507.20316)
*Yiwen Lin,Liu Liu*

Main category: math.NA

TL;DR: The paper compares multi-level Monte Carlo (MLMC) and multi-fidelity methods for solving the Boltzmann equation with uncertain parameters, using an asymptotic-preserving-hybrid (APH) scheme. Both methods are efficient and preserve physical properties, with guidelines provided for choosing between them.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of solving the Boltzmann equation with uncertain parameters efficiently while maintaining accuracy and preserving physical properties.

Method: Proposes MLMC and multi-fidelity methods using an APH scheme for the Boltzmann equation. MLMC uses a hierarchy of models with variance reduction, while multi-fidelity combines APH (high-fidelity) with a finite volume scheme (low-fidelity).

Result: APH-based MLMC and multi-fidelity methods are significantly faster than standard approaches while maintaining accuracy.

Conclusion: The methods are effective, with guidelines for selection based on solution smoothness and computational resources.

Abstract: In this work, we propose and compare several approaches to solve the
Boltzmann equation with uncertain parameters, including multi-level Monte Carlo
and multi-fidelity methods that employ an asymptotic-preserving-hybrid (APH)
scheme (Filbet and Rey, 2015) for the deterministic Boltzmann model. By
constructing a hierarchy of models from finer to coarser meshes in phase space
for the APH scheme and adopting variance reduction techniques, the MLMC method
is able to allocate computational resources across different hierarchies
quasi-optimally. On the other hand, in the bi-fidelity method we choose the APH
scheme for the Boltzmann equation as the high-fidelity solver, and a finite
volume scheme for the compressible Euler system as the low-fidelity model.
Since both methods are non-intrusive, they can preserve the physical properties
of the deterministic solver. Extensive numerical experiments demonstrate that
our APH-based MLMC and multi-fidelity methods are significantly faster than
standard approaches, while maintaining accuracy. We also provide practical
guidelines for selection between APH-based MLMC and multi-fidelity approaches,
based on solution smoothness and computational resource availability.

</details>


### [14] [Subset selection for matrices in spectral norm](https://arxiv.org/abs/2507.20435)
*Ivan Kozyrev,Alexander Osinsky*

Main category: math.NA

TL;DR: The paper introduces a deterministic approximation algorithm for selecting a subset of columns from a matrix to minimize the pseudoinverse's spectral or Frobenius norm, improving upon existing methods.


<details>
  <summary>Details</summary>
Motivation: The subset selection problem for matrices is NP-hard, and existing methods involve intermediate weighting steps. The goal is to simplify and improve the approximation quality.

Method: A potential-based framework is refined using a single barrier function, enabling direct column selection and adaptive updates, bypassing weighting steps.

Result: The method provides better approximation guarantees in key parameter regimes and outperforms competitors in numerical experiments.

Conclusion: The proposed algorithm is efficient, improves existing bounds, and is supported by a complete C++ implementation for reproducibility.

Abstract: We address the subset selection problem for matrices, where the goal is to
select a subset of $k$ columns from a "short-and-fat" matrix $X \in
\mathbb{R}^{m \times n}$, such that the pseudoinverse of the sampled submatrix
has as small spectral or Frobenius norm as possible. For the NP-hard spectral
norm variant, we propose a new deterministic approximation algorithm. Our
method refines the potential-based framework of spectral sparsification by
specializing it to a single barrier function. This key modification enables
direct, unweighted column selection, bypassing the intermediate weighting step
required by previous approaches. It also allows for a novel adaptive update
strategy for the barrier. This approach yields a new, explicit bound on the
approximation quality that improves upon existing guarantees in key parameter
regimes, without increasing the asymptotic computational complexity.
Furthermore, numerical experiments demonstrate that the proposed method
consistently outperforms its direct competitors. A complete C++ implementation
is provided to support our findings and facilitate future research.

</details>


### [15] [On rank-2 Nonnegative Matrix Factorizations and their variants](https://arxiv.org/abs/2507.20612)
*Etna Lindy,Vanni Noferini,Paul Van Dooren*

Main category: math.NA

TL;DR: The paper proposes a method for finding the best nonnegative rank-2 approximation of any nonnegative matrix, using a suboptimal initial approximation to improve the Alternating Nonnegative Least Squares method.


<details>
  <summary>Details</summary>
Motivation: The problem is motivated by the need for efficient and high-quality nonnegative rank-2 approximations, particularly for applications in graph theory and symmetric matrices.

Method: The method involves revisiting the theory of nonnegative factorizations, constructing a cheaply computable initial approximation, and refining it using the Alternating Nonnegative Least Squares method.

Result: Numerical experiments show improved computational efficiency and output quality with the proposed initial value.

Conclusion: The approach provides a practical solution for nonnegative rank-2 approximation, with potential benefits for graph-theoretical applications and symmetric matrices.

Abstract: We consider the problem of finding the best nonnegative rank-2 approximation
of an arbitrary nonnegative matrix. We first revisit the theory, including an
explicit parametrization of all possible nonnegative factorizations of a
nonnegative matrix of rank 2. Based on this result, we construct a cheaply
computable (albeit suboptimal) nonnegative rank-2 approximation for an
arbitrary nonnegative matrix input. This can then be used as a starting point
for the Alternating Nonnegative Least Squares method to find a nearest
approximate nonnegative rank-2 factorization of the input; heuristically, our
newly proposed initial value results in both improved computational complexity
and enhanced output quality. We provide extensive numerical experiments to
support these claims. Motivated by graph-theoretical applications, we also
study some variants of the problem, including matrices with symmetry
constraints.

</details>


### [16] [A fixed-time stable dynamical model for solving EVLCPs](https://arxiv.org/abs/2507.20652)
*Yufei Wei,Shiping Lin,Cairong Chen,Dongmei Yu,Deren Han*

Main category: math.NA

TL;DR: A fixed-time stable dynamical system is developed for solving the extended vertical linear complementarity problem (EVLCP), reformulated as generalized absolute value equations. The system avoids smoothing techniques and ensures fixed-time stability.


<details>
  <summary>Details</summary>
Motivation: To address the EVLCP efficiently by leveraging generalized absolute value equations and ensuring fixed-time convergence without smoothing.

Method: Reformulate EVLCP as generalized absolute value equations, develop a dynamical system for solving them, and prove fixed-time stability.

Result: The dynamical system solves EVLCP with fixed-time stability, provides a unique solvability condition, and a new error bound. Numerical results validate the claims.

Conclusion: The proposed dynamical system effectively solves EVLCP with fixed-time stability, offering theoretical and practical advancements.

Abstract: A fixed-time stable dynamical system for solving the extended vertical linear
complementarity problem (EVLCP) is developed. The system is based on the
reformulation of EVLCP as a special case of a new kind of generalized absolute
value equations. Some properties of the new kind of generalized absolute value
equations are explored which are useful for developing a fixed-time stable
dynamical system for solving it. Without using any smoothing technique, we
develop a dynamical system for solving the new kind of generalized absolute
value equations and prove its fixed-time stability. The model is applicable for
solving EVLCP. As two by-products, a new condition which guarantees the unique
solvability of EVLCP and a new error bound of EVLCP are given. Numerical
results are given to demonstrate our claims.

</details>


### [17] [A general framework for the OSRC-preconditioned EFIE in computational electromagnetics](https://arxiv.org/abs/2507.20707)
*Marion Darbas,Ignacia Fierro-Piccardo*

Main category: math.NA

TL;DR: Study of EFIE preconditioning using OSRC operators, comparing MtE and EtM maps for spectral properties and performance, with MtE emerging as cost-effective.


<details>
  <summary>Details</summary>
Motivation: To bridge theory and practice in preconditioning EFIE, evaluating strengths and limitations of MtE and EtM formulations.

Method: Analyzed spectral properties, discretization behavior, and numerical performance of MtE and EtM preconditioners on smooth, closed geometries.

Result: MtE formulation proved cost-effective for Boundary Element Methods; implementation guidelines and improvements proposed.

Conclusion: Findings serve as a reference for preconditioned boundary integral formulations in computational electromagnetics.

Abstract: This work presents a comprehensive study of preconditioning strategies for
the Electric Field Integral Equation (EFIE) using On-Surface Radiation
Condition (OSRC) operators. We examine two distinct formulations -- the
Magnetic-to-Electric (MtE) and Electric-to-Magnetic (EtM) maps -- used to
precondition the EFIE, and we analyze their spectral properties, discretization
behavior, and numerical performance. A central objective is to bridge the gap
between theoretical development and practical implementation, identifying the
strengths and limitations of each approach. Through numerical experiments on
smooth, closed geometries, we show that the MtE formulation stands out as a
cost-effective preconditioner in the context of Boundary Element Methods. We
also offer implementation guidelines and propose improvements to address
existing challenges. These findings provide a valuable reference for
researchers and practitioners working with preconditioned boundary integral
formulations in computational electromagnetics.

</details>


### [18] [Enhancing Complex Injection Mold Design Validation Using Multicombined RV Environments](https://arxiv.org/abs/2507.20732)
*J. M. Mercado-Colmenero,D. F. Garcia-Molina,B. Gutierrez-Jimenez,C. Martin-Donate*

Main category: math.NA

TL;DR: VR-based multimodal environment improves injection mold design validation, outperforming traditional CAD methods in efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Challenges in real complex injection mold design validation due to limited CAD proficiency and high costs of prototyping.

Method: Developed a VR-based multimodal environment with interactive 3D models and dynamic visualizations for mold validation.

Result: Significant improvements in interference detection, topology tracking, and assembly validation compared to CAD-only methods.

Conclusion: VR technology reduces manufacturing errors and is poised to become essential in the injection molding industry.

Abstract: The intricate design of real complex injection molds poses significant
challenges. Mold design vali-dation often falls to operators with tool-handling
experience but limited CAD proficiency. Unlike other industries, the scale and
costs of injection mold fabrication hinder prototyping before pro-duction.
Virtual reality (VR) has emerged as a revolutionary solution offering a safe,
immersive, and realistic experience and accessible using QR codes. This paper
presents a new multimodal virtual environment tailored to validate mold design
complexities. Integrating knowledge-enriched visual tools like interactive 3D
models and dynamic visualizations enables users to explore complex mold
designs. Statistical analyses, including the Wilcoxon test, unveil significant
differences in interference detection, internal topology tracking, and
validation of assembly and disassembly accessibility for both small and large
mold components when comparing validation conducted through traditional means
using solely CAD systems versus the utilization of multidimensional validation
methods. Efficiency gains in using VR devices for mold design validation in a
hybrid environment in the analysis of relative frequencies. The present study
surpasses the state of the art illustrating how VR technology can substantially
reduce manufacturing errors in injection molding processes, thereby offering
important advantages for manufacturers emerging as an essential tool for this
impact in-dustry in the next years.

</details>


### [19] [Explicit and Effectively Symmetric Runge-Kutta Methods](https://arxiv.org/abs/2507.21006)
*Daniil Shmelev,Kurusch Ebrahimi-Fard,Nikolas Tapia,Cristopher Salvi*

Main category: math.NA

TL;DR: The paper introduces Explicit and Effectively Symmetric (EES) schemes, a new class of explicit Runge-Kutta methods with near-symmetric properties, outperforming higher-order explicit schemes and matching implicit symmetric schemes at lower cost.


<details>
  <summary>Details</summary>
Motivation: Symmetry in numerical methods is valuable for Hamiltonian systems and Neural ODEs, but symmetric Runge-Kutta schemes are typically implicit and computationally expensive.

Method: A Hopf algebraic approach decomposes B-series methods into symmetric and antisymmetric components. New order conditions minimize antisymmetry, enabling explicit, near-symmetric EES schemes.

Result: Second-order EES schemes outperform higher-order explicit methods (e.g., RK4, RK5) and match implicit symmetric schemes with lower computational cost.

Conclusion: EES schemes offer a practical alternative to implicit symmetric methods, combining efficiency with near-symmetric performance.

Abstract: Symmetry is a key property of numerical methods. The geometric properties of
symmetric schemes make them an attractive option for integrating Hamiltonian
systems, whilst their ability to exactly recover the initial condition without
the need to store the entire solution trajectory makes them ideal for the
efficient implementation of Neural ODEs. In this work, we present a Hopf
algebraic approach to the study of symmetric B-series methods. We show that
every B-series method can be written as the composition of a symmetric and
"antisymmetric" component, and explore the structure of this decomposition for
Runge-Kutta schemes. A major bottleneck of symmetric Runge-Kutta schemes is
their implicit nature, which requires solving a nonlinear system at each step.
By introducing a new set of order conditions which minimise the antisymmetric
component of a scheme, we derive what we call Explicit and Effectively
Symmetric (EES) schemes -- a new class of explicit Runge-Kutta schemes with
near-symmetric properties. We present examples of second-order EES schemes and
demonstrate that, despite their low order, these schemes readily outperform
higher-order explicit schemes such as RK4 and RK5, and achieve results
comparable to implicit symmetric schemes at a significantly lower computational
cost.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [20] [Global strong solutions for the triangular Shigesada-Kawasaki-Teramoto cross-diffusion system in three dimensions and parabolic regularisation for increasing functions](https://arxiv.org/abs/2507.19512)
*Hector Bouton,Laurent Desvillettes,Helge Dietert*

Main category: math.AP

TL;DR: Existence of global strong solutions for the 3D SKT cross-diffusion system with Lotka-Volterra terms, using a method involving rough coefficient parabolic equations.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of proving global strong solutions for the SKT cross-diffusion system in three dimensions, especially with Lotka-Volterra reaction terms.

Method: Study of a parabolic equation with rough coefficients and homogeneous Neumann boundary conditions, leveraging the assumption ∂ₜw ≥ 0.

Result: Global strong solutions for the SKT system are proven, with the method also applicable to reversible chemistry reaction-diffusion systems.

Conclusion: The approach successfully establishes global solutions for complex cross-diffusion systems, extending to other reaction-diffusion models.

Abstract: We prove the existence of global strong solutions to the triangular
Shigesada-Kawasaki-Teramoto (SKT) cross-diffusion system with Lokta-Volterra
reaction terms in three dimensions. A key part is the independent careful study
of the parabolic equation $a\partial_t w - \Delta w = f$ with a rough
coefficient $a$, homogeneous Neumann boundary conditions, and the special
assumption $\partial_t w \ge 0$. By the same method, we obtain estimates for
solutions to reaction-diffusion systems modelling reversible chemistry.

</details>


### [21] [Nodal set for the Schrödinger equation under a local growth condition](https://arxiv.org/abs/2507.19600)
*Igor Kukavica,Linfeng Li*

Main category: math.AP

TL;DR: Upper bound on nodal set size for Schrödinger equation solutions under local doubling condition.


<details>
  <summary>Details</summary>
Motivation: To quantify the size of nodal sets for solutions of the Schrödinger equation with coefficients in Sobolev spaces.

Method: Assumes local doubling condition for solutions and derives bounds using Sobolev norms of coefficients.

Result: Algebraic upper bound on the (n-1)-dimensional Hausdorff measure of the nodal set.

Conclusion: Provides a theoretical limit on nodal set size, useful for understanding solution behavior.

Abstract: We address the upper bound on the size of the nodal set for a solution $w$ of
the Schr\"odinger equation $\Delta w= W\cdot \nabla w+V w$ in an open set in
$\mathbb{R}^n$, where the coefficients belong to certain Sobolev spaces.
Assuming a local doubling condition for the solution $w$, we establish an upper
bound on the $(n-1)$-dimensional Hausdorff measure of the nodal set, with the
bound depending algebraically on the Sobolev norms of $W$ and $V$.

</details>


### [22] [Pointwise boundary estimates for fully nonlinear elliptic equations with nonzero Dirichlet boundary conditions](https://arxiv.org/abs/2507.19645)
*Mengni Li,Chaofan Shi*

Main category: math.AP

TL;DR: The paper develops boundary estimates for fully nonlinear elliptic equations with general boundary conditions, deriving global Hölder regularity and extending previous Monge-Ampère results.


<details>
  <summary>Details</summary>
Motivation: To address boundary estimates for a class of fully nonlinear elliptic equations, including nonzero boundary conditions, and generalize prior work.

Method: Uses subsolution and supersolution methods to derive pointwise boundary upper and lower bounds for convex solutions, leveraging domain and function convexity.

Result: Achieves global Hölder regularity and generalizes previous Monge-Ampère equation results.

Conclusion: The work provides foundational boundary estimates and extends existing theory, with convexity playing a key role.

Abstract: In this paper, we investigate boundary estimates for the Dirichlet problem
for a class of fully nonlinear elliptic equations with general boundary
conditions, including nonzero boundary conditions. Given specific structural
conditions on the problem, we develop pointwise boundary upper and lower bound
estimates for convex solutions based on the subsolution and supersolution
method. The global H\"older regularity can be derived as a direct consequence
of these pointwise boundary estimates. These results fundamentally hinge on
careful descriptions of the convexity properties of both the domains and the
functions involved. Moreover, previous results on Monge-Amp\`ere equations with
nonzero Dirichlet boundary conditions can be regarded as a special case of our
results.

</details>


### [23] [A further remark on the density estimate for degenerate Allen-Cahn equations: $Δ_{p}$-type equations for $1<p<\frac{n}{n-1}$ with rough coefficients](https://arxiv.org/abs/2507.19820)
*Chilin Zhang*

Main category: math.AP

TL;DR: Study of Allen-Cahn equations with Ginzburg-Landau energies, removing regularity assumptions and establishing density estimates for level sets of minimizers.


<details>
  <summary>Details</summary>
Motivation: To extend previous work by removing regularity constraints on Ginzburg-Landau energies and analyze minimizers under new conditions.

Method: Analyze Allen-Cahn equations with degenerate double-well potentials and Dirichlet energy, focusing on nontrivial minimizers.

Result: Density estimates for level sets of minimizers are derived under specific conditions on the potential and energy.

Conclusion: The study successfully generalizes prior results by relaxing assumptions and provides insights into the behavior of minimizers.

Abstract: In this short remark on a previous paper \cite{SZ25}, we continue the study
of Allen-Cahn equations associated with Ginzburg-Landau energies
\begin{equation*}
  J(v,\Omega)=\int_{\Omega}\Big\{F(\nabla v,v,x)+W(v,x)\Big\}dx,
\end{equation*} involving a Dirichlet energy
$F(\vec{\xi},\tau,x)\sim|\vec{\xi}|^{p}$ and a degenerate double-well potential
$W(\tau,x)\sim(1-\tau^{2})^{m}$. In contrast to \cite{SZ25}, we remove all
regularity assumptions on the Ginzburg-Landau energy. Then, with further
assumptions that $1<p<\frac{n}{n-1}$ and that $W(\tau,x)$ is monotone in $\tau$
on both sides of $0$, we establish a density estimate for the level sets of
nontrivial minimizers $|u|\leq1$.

</details>


### [24] [Mass threshold for global existence in chemotaxis systems with critical flux limitation](https://arxiv.org/abs/2507.19866)
*Xuan Mao,Hengling Wang,Jianlu Yan*

Main category: math.AP

TL;DR: The paper studies a flux-limited chemotaxis system in a unit ball, identifying a critical exponent for blow-up and a mass threshold for global bounded solutions under radial symmetry.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of the flux-limited chemotaxis system, particularly the conditions for blow-up and global bounded solutions.

Method: Analyzes the system under no-flux and Dirichlet boundary conditions, focusing on radially symmetric solutions and critical exponent α.

Result: Identifies α = N/(N-1) as the threshold for blow-up and derives a mass condition for globally bounded weak solutions.

Conclusion: The system's behavior depends on initial mass and symmetry, with critical flux limitation preventing blow-up under specific conditions.

Abstract: This paper investigates the flux-limited chemotaxis system, proposed by
Kohatsu and Senba~(2025), \begin{equation*}
  \begin{cases} u_t = \Delta u -\nabla\cdot(u|\nabla v|^{\alpha-2}\nabla v),\\
\:\:0=\Delta v + u,
  \end{cases} \end{equation*} posed in the unit ball of $\mathbb{R}^N$ for some
$N\geq2$, subject to no-flux and homogeneous Dirichlet boundary conditions. Due
to precedents, e.g., Tello (2022) and Winkler (2022), the exponent $\alpha =
\frac{N}{N-1}$ is the threshold for finite-time blow-up under symmetry
assumptions. We further find that under the framework of radially symmetric
solutions, the system with critical flux limitation admits a globally bounded
weak solution if and only if initial mass is strictly less than $\omega_N
\big(\frac{N^2}{N-1}\big)^{N-1}$, where $\omega_N$ denotes the measure of the
unit sphere $\mathbb{S}^{N-1}$. Asymptotic behaviors are also considered.

</details>


### [25] [Pohozaev identities for weak solutions of Grushin type p-sub-Laplacian equation via domain variations](https://arxiv.org/abs/2507.19913)
*Yawei Wei,Xiaodong Zhou*

Main category: math.AP

TL;DR: Study of Pohozaev identities for weak solutions of degenerate elliptic equations with Grushin-type p-sub-Laplacian under C^1-regularity.


<details>
  <summary>Details</summary>
Motivation: To derive Pohozaev identities for weak solutions in degenerate elliptic equations, expanding understanding under minimal regularity assumptions.

Method: Uses domain variations to derive local Pohozaev identities (translating and scaling types).

Result: Obtains local identities and a global Pohozaev identity of scaling type in ℝ^(N+l).

Conclusion: Demonstrates the applicability of domain variations for deriving Pohozaev identities under C^1-regularity.

Abstract: In this paper, we study Pohozaev identities for weak solutions of degenerate
elliptic equations involving Grushin type p-sub-Laplacian under only
$C^1$-regularity assumption. By using domain variations, we obtain the local
Pohozaev identities of translating type and scaling type. As an application, a
global Pohozaev identity of scaling type in $\mathbb{R}^{N+l}$ is also derived.

</details>


### [26] [Stability of oppositely-propagating pair of Hill's spherical vortices](https://arxiv.org/abs/2507.19935)
*Young-Jin Sim*

Main category: math.AP

TL;DR: The paper proves the stability of two Hill's spherical vortices moving apart in 3D axisymmetric Euler equations without swirl, showing their profiles remain close to Hill's vortex over time.


<details>
  <summary>Details</summary>
Motivation: To understand the stability and interaction dynamics of vortex pairs in incompressible fluids, particularly Hill's spherical vortices.

Method: Analyzing interaction energy evolution and energy-maximizing sequences in a variational problem, ensuring small initial interaction energy keeps vortex profiles close to Hill's vortex.

Result: Each vortex remains close to Hill's vortex profile, and an optimal speed estimate for their separation is derived.

Conclusion: The study confirms stability and provides optimal speed estimates for separating Hill's vortices, with implications for fluid dynamics.

Abstract: We establish the stability of a pair of Hill's spherical vortices moving away
from each other in 3D incompressible axisymmetric Euler equations without
swirl. Each vortex in the pair propagates away from its odd-symmetric
counterpart, while keeping its vortex profile close to Hill's vortex. This is
achieved by analyzing the evolution of the interaction energy of the pair and
combining it with the compactness of energy-maximizing sequences in the
variational problem concerning Hill's vortex. The key strategy is to confirm
that, if the interaction energy is initially small enough, the kinetic energy
of each vortex in the pair remains so close to that of a single Hill's vortex
for all time that each vortex profile stays close to the energy maximizer:
Hill's vortex. An estimate of the propagating speed of each vortex in the pair
is also obtained by tracking the center of mass of each vortex. The estimate
can be understood as optimal in the sense of the power exponent of the
$\varepsilon$--the small perturbation measured in the ($L^1\cap L^2$+impulse)
norm--appearing in the error bound cannot be improved.

</details>


### [27] [Strichartz estimates for a Schrödinger equation on the half-line with a Neumann boundary condition](https://arxiv.org/abs/2507.19998)
*Nicola Garofalo,Gigliola Staffilani*

Main category: math.AP

TL;DR: New Strichartz estimates for the Bessel operator and a fractal Tomas-Stein theorem for the Hankel transform are proven. These are applied to show well-posedness for nonlinear L²ₐ-critical problems.


<details>
  <summary>Details</summary>
Motivation: To extend Strichartz estimates and restriction theorems for the Bessel operator and Hankel transform, and apply them to nonlinear problems.

Method: Prove new Strichartz estimates and a fractal Tomas-Stein theorem, then use them to analyze well-posedness of nonlinear problems.

Result: Global well-posedness for L²ₐ-critical problems and local well-posedness for sub-critical cases.

Conclusion: The new estimates and theorems provide tools for analyzing well-posedness in nonlinear problems involving the Bessel operator and Hankel transform.

Abstract: In this paper we prove some new Strichartz estimates related to the Cauchy
problem for the Bessel operator on the half-line and we establish a fractal
version of the Tomas-Stein restriction theorem for the Hankel transform. Then
we use the proved Strichartz estimates to show global in time well-posedness
for a class of nonlinear $L^2_a$-critical problems, and local in time
well-posedness in the sub-critical case.

</details>


### [28] [Long-term behaviour of the primitive equations with wind-driven boundary conditions: Convergence to the Ekman spiral](https://arxiv.org/abs/2507.20031)
*Tim Binz*

Main category: math.AP

TL;DR: The paper proves that solutions to 3D incompressible primitive equations with wind-driven boundary conditions and Coriolis force converge exponentially to the Ekman spiral, which is the unique equilibrium.


<details>
  <summary>Details</summary>
Motivation: To understand the long-term behavior of the 3D incompressible primitive equations under specific boundary conditions and forces.

Method: Analysis of the 3D incompressible primitive equations with wind-driven boundary conditions and Coriolis force.

Result: Every solution converges exponentially to the Ekman spiral as time approaches infinity, confirming it as the unique equilibrium.

Conclusion: The Ekman spiral is the sole equilibrium state for the system under the given conditions.

Abstract: In this article we investigate the long-term behaviour of the 3D
incompressible, primitive equations with wind-driven boundary conditions and
Coriolis force. We show that every solution converges exponentially fast to the
Ekman spiral as $t \to + \infty$. In particular, this implies that the Ekman
spiral is the unique equilibrium of the system.

</details>


### [29] [Refraction laws in temporal media](https://arxiv.org/abs/2507.20032)
*Cristian E. Gutiérrez,Eric Stachura*

Main category: math.AP

TL;DR: Rigorous derivation of boundary conditions for Maxwell's system at temporal interfaces, leading to a general Snell's Law and explicit reflection/transmission coefficients without simplifying assumptions.


<details>
  <summary>Details</summary>
Motivation: To address scattering of electromagnetic waves at temporal interfaces, extending spatial interface concepts to time-dependent scenarios with non-simplified assumptions.

Method: Derive boundary conditions for electric and magnetic fields at temporal interfaces, assuming precise material parameters, and generalize Snell's Law.

Result: Explicit formulas for reflection and transmission coefficients, valid for non-smooth fields and non-constant material parameters.

Conclusion: The work provides a foundational framework for analyzing temporal interfaces in Maxwell's system without restrictive assumptions.

Abstract: We consider the time dependent Maxwell system in the sense of distributions
in the context of temporal interfaces. Just as with spatial interfaces,
electromagnetic waves at temporal interfaces scatter and create a transmitted
and reflected wave. We provide a rigorous derivation of boundary conditions for
the electric and magnetic fields at temporal interfaces with precise
assumptions on the material parameters. In turn, we use this to obtain a
general Snell's Law at such interfaces. From this, we obtain explicit formulas
for the reflection and transmission coefficients. Unlike previous works, we do
not make any simplifying ansatz on the solution to the Maxwell system, nor do
we assume that the fields are smooth. We also consider material parameters
which are not necessarily constant on either side of the temporal interface.

</details>


### [30] [Fractional Trudinger-Moser type inequalities with logarithmic convolution potentials](https://arxiv.org/abs/2507.20069)
*Huxiao Luo,Shiying Wang*

Main category: math.AP

TL;DR: The paper establishes a fractional Trudinger-Moser inequality with a logarithmic convolution potential and explores extremal functions. It also analyzes radial symmetry of solutions to the Euler-Lagrange equation.


<details>
  <summary>Details</summary>
Motivation: To extend Trudinger-Moser type inequalities to fractional Sobolev spaces with logarithmic potentials and investigate properties of extremal functions and solutions.

Method: Uses fractional Sobolev spaces and logarithmic convolution potentials. The moving plane method is applied for symmetry analysis.

Result: Proves the inequality and confirms existence of extremal functions. Shows radial symmetry and decreasing property of positive solutions.

Conclusion: The work generalizes Trudinger-Moser inequalities and provides insights into the structure of solutions in fractional settings.

Abstract: We establish the following fractional Trudinger-Moser type inequality with
logarithmic convolution potential $$ \sup_{u\in
W^{\frac{1}{2},2}_0(I),\|u\|_{W_0^{\frac{1}{2},2}}\leq1}\int_{I} \int_{I} \log
\frac{1}{|x-y|} G(u(x))G(u(y)) \, dx \, dy<+\infty,$$ where $G(s)\leq
C\frac{e^{\pi s^{2}}}{(1 + |s|)^{\gamma}}~ \forall s\in\mathbb{R}$ with some
constant $C>0,\gamma\geq1$, the domain $I\subset\mathbb{R}$ is a bounded
interval. This type of inequality in the entire space $\mathbb{R}$ is also
considered. Moreover, we study the existence of corresponding extremal
functions.
  In addition, by the moving plane method, we obtain the radial symmetry and
radial decreasing property of positive solutions to the corresponding
Euler-Lagrange equation.

</details>


### [31] [Strichartz estimate for discrete Schrödinger equation on layered King's grid](https://arxiv.org/abs/2507.20142)
*ZhiQiang Wan,Heng Zhang*

Main category: math.AP

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We establish the sharp \( l^1 \to l^{\infty} \) decay estimate for the
discrete Schr\"odinger equation (DS) on the Layered King's Grid (LKG), with a
dispersive decay rate of \( \langle t \rangle^{-13/12} \), which is faster than
that for $3$-dimensional lattice (\( \langle t \rangle^{-1} \), see
\cite{SK05}). This decay estimate enables us to derive the corresponding
Strichartz estimate via the standard Keel--Tao argument. Our approach relies on
using techniques from Newton polyhedra to analyze singularities.

</details>


### [32] [Liouville theorems for anisotropic $p$-Laplace equations with a semilinear term](https://arxiv.org/abs/2507.20182)
*Weizhao Liang,Tian Wu,Jin Yan*

Main category: math.AP

TL;DR: The paper explores Liouville theorems for solutions to the anisotropic $p$-Laplace equation, analyzing cases where the semilinear term $f$ is positive, negative, or sign-changing. It uses Serrin's technique for positive/negative $f$ and the invariant tensor method for subcritical cases, providing a simplified proof for critical cases and triviality results for sign-changing solutions.


<details>
  <summary>Details</summary>
Motivation: The study aims to extend understanding of Liouville theorems for the anisotropic $p$-Laplace equation, addressing cases with varying signs of the semilinear term $f$ and simplifying proofs for critical cases.

Method: Serrin's technique is used for positive/negative $f$, while the invariant tensor method is applied to subcritical cases. A differential identity from the subcritical case is extended to the critical case for simplified proofs.

Result: Positive supersolutions (subsolutions) are constant for positive (negative) $f$. Nonexistence results are proven for subcritical cases, and triviality is shown for stable sign-changing solutions under certain conditions.

Conclusion: The paper advances the understanding of Liouville theorems for the anisotropic $p$-Laplace equation, offering new techniques and simplified proofs for critical cases and stability conditions.

Abstract: In this paper, we investigate Liouville theorems for solutions to the
anisotropic $p$-Laplace equation $$-\Delta_p^H u=-\operatorname{div}(a(\nabla
u))=f(u),\quad\text{in }\mathbb{R}^n,$$ where the semilinear term $f$ may be
positive, negative, or sign-changing. When $f$ is positive (negative) and
satisfies certain conditions, Serrin's technique is applied to show that every
positive supersolution (subsolution) must be constant. For the subcritical
case, we use the invariant tensor method to prove nonexistence results for
positive solutions. In particular, by applying the differential identity
established in the subcritical case to the critical case, we provide a
simplified new proof of the classification of positive solutions to the
critical case $f(u)=u^{p^*-1}$. For sign-changing solutions, every stable
solution or solution that is stable outside a compact set is trivial under
certain conditions on $f$.

</details>


### [33] [Non-autonomous problem for a $2m$-th order semilinear nonlocal parabolic equation](https://arxiv.org/abs/2507.20257)
*Flank D. M. Bezerra,Silvia Sastre-Gómez*

Main category: math.AP

TL;DR: Existence and characterization of pullback attractors for a 2m-th order non-autonomous quasilinear parabolic equation under growth and regularity conditions. Also examines an autonomous version.


<details>
  <summary>Details</summary>
Motivation: To study the behavior and attractors of high-order quasilinear parabolic equations, both non-autonomous and autonomous, under specific conditions.

Method: Analyzes the equation under growth and regularity conditions for nonlinear functions, proving existence and characterization of pullback attractors.

Result: Demonstrates the existence and characterization of pullback attractors for the non-autonomous case and extends analysis to the autonomous version.

Conclusion: The paper successfully establishes results for pullback attractors in both non-autonomous and autonomous high-order quasilinear parabolic equations.

Abstract: In this paper we consider a $2m$-th order non autonomous quasilinear
parabolic equation. Under suitable conditions of growth and regularity for the
nonlinear functions present in the model, we prove a result of existence and
characterization of pullback attractors. Moreover, we consider an autonomous
version from the $2m$-th order non autonomous quasilinear parabolic equation in
question.

</details>


### [34] [Local and global well-posedness for the kinetic derivative NLS on $\mathbb{R}$](https://arxiv.org/abs/2507.20271)
*Nobu Kishimoto,Kiyeon Lee*

Main category: math.AP

TL;DR: The paper studies the well-posedness of the kinetic derivative nonlinear Schrödinger equation (KDNLS) on the real line, focusing on local and global solutions in Sobolev spaces.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand the behavior of solutions to KDNLS under varying parameters, particularly the role of the Hilbert transform and dissipation.

Method: The energy method and gauge transformations are used to address resonant interactions, with additional a priori bounds for global well-posedness in the dissipative case.

Result: Local well-posedness is proven in Sobolev spaces for both dissipative and non-dissipative cases, with global well-posedness shown for the dissipative case.

Conclusion: The study provides a comprehensive analysis of KDNLS, highlighting the impact of parameter β on solution behavior and well-posedness.

Abstract: We investigate the local and global well-posedness of the kinetic derivative
nonlinear Schr\"{o}dinger equation(KDNLS) on $\mathbb{R}$, described by \[
i\partial_t u + \partial_x^2 u = i\alpha \partial_x (|u|^2 u) + i\beta
\partial_x (\mathcal{H} (|u|^2) u), \] where $\alpha, \beta \in \mathbb{R}$,
and $\mathcal{H}$ represents the Hilbert transformation. For KDNLS, the $L^2$
norm of a solution is decreasing (resp. increasing, conserved) when $\beta$ is
negative (resp. positive, zero). Focusing on the Sobolev spaces $H^2$ and $ H^2
\cap H^{1,1} $, we establish local well-posedness via the energy method
combined with gauge transformations to address resonant interactions in both
cases of negative and positive $\beta$. For the dissipative case $\beta < 0$,
we further demonstrate global well-posedness by deriving an a priori bound in
$H^2$.

</details>


### [35] [Chemotaxis models with mixed mechanisms: boundedness in growth-dominated regimes](https://arxiv.org/abs/2507.20310)
*Tongxing Li,Silvia Frassu,Giuseppe Viglialoro*

Main category: math.AP

TL;DR: The paper analyzes a chemotaxis-growth system with nonlinear reactions and gradient damping, proving global existence and uniform boundedness of solutions under certain conditions.


<details>
  <summary>Details</summary>
Motivation: To understand how chemotaxis, damping, and nonlocal reactions interact to prevent blow-up in structured models, especially when local growth complicates mass control.

Method: A two-step approach: first ensuring bounded total mass, then establishing full uniform boundedness under suitable parameter and dimension conditions.

Result: Solutions exist globally and remain uniformly bounded, demonstrating the interplay of chemotaxis, damping, and nonlocal effects.

Conclusion: The study reveals how structured interactions in the system prevent blow-up, even when local growth dominates.

Abstract: We study a chemotaxis-growth system with nonlinear local and nonlocal
reactions and gradient-dependent damping. Under suitable conditions on the
system parameters and spatial dimension, we prove that solutions exist globally
in time and remain uniformly bounded. Unlike classical cases, when local growth
dominates, mass control is not automatic. To address this, we use a two-step
approach: first ensuring bounded total mass, then establishing full uniform
boundedness. The results highlight how chemotaxis, damping, and nonlocal
effects interact to prevent blow-up in structured models.

</details>


### [36] [Multiplicity of singular solutions for semilinear elliptic equations with superlinear source terms](https://arxiv.org/abs/2507.20450)
*Yohei Fujishima,Norisuke Ioku*

Main category: math.AP

TL;DR: The paper generalizes multiplicity results for singular solutions of the nonlinear elliptic equation near the origin, extending known results for specific nonlinearities to broader classes.


<details>
  <summary>Details</summary>
Motivation: To extend the understanding of singular solutions for the nonlinear elliptic equation beyond the concrete model nonlinearity, addressing more complex and varied forms of nonlinear functions.

Method: The authors apply their developed classification of nonlinear functions and transformations to analyze the multiplicity of singular solutions.

Result: The study successfully generalizes multiplicity results to various nonlinearities, including combinations of power, logarithmic, and exponential terms.

Conclusion: The findings broaden the scope of known results, providing a framework for analyzing singular solutions in a wider range of nonlinear elliptic equations.

Abstract: This paper investigates the multiplicity of singular solutions for the
nonlinear elliptic equation $-\Delta u =f(u)$ near the origin. Applying the
classification of nonlinear functions and the transformation, which were
developed by the authors, we generalize the multiplicity results known for the
concrete model nonlinearity $f(u)=u^p$ with $\frac{N}{N-2}<p<\frac{N+2}{N-2}$.
Our result applies to various nonlinearities, such as $f(s)=s^p+s^r$ with
$0<r<p$, $f(s)=s^p(\log s)^r$ with $r\in \mathbb{R}$, $f(s)=s^p\exp((\log
s)^r)$ with $0<r<1$ and $f(s)=s^p+s^r(\log s)^{\beta}$ with $0<r<p$ and $\beta
\in \mathbb{R}$, for $\frac{N}{N-2}<p<\frac{N+2}{N-2}$.

</details>


### [37] [Existence and convergence of ground state solutions for Choquard-type systems on lattice graphs](https://arxiv.org/abs/2507.20464)
*Lidan Wang*

Main category: math.AP

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this paper, we study the $p$-Laplacian system with Choquard-type
nonlinearity $$ \begin{cases}-\Delta_{p} u+(\lambda a+1)|u|^{p-2}
u=\frac{1}{\gamma} \left(R_\alpha\ast F(u,v)\right)F_{u}(u, v), \\ -\Delta_{p}
v+(\lambda b+1)|v|^{p-2} v=\frac{1}{\gamma} \left(R_\alpha\ast
F(u,v)\right)F_{v}(u, v),\end{cases} $$ on lattice graphs $\mathbb{Z}^N$, where
$\alpha \in(0,N),\,p\geq 2,\,\gamma> \frac{(N+\alpha)p}{2N},\,\lambda>0$ is a
parameter and $R_{\alpha}$ is the Green's function of the discrete fractional
Laplacian that behaves as the Riesz potential. Under some assumptions on the
functions $a,\,b$ and $F$, we prove the existence and asymptotic behavior of
ground state solutions by the method of Nehari manifold.

</details>


### [38] [Vanishing discount limits for first-order fully nonlinear Hamilton-Jacobi equations on noncompact domains](https://arxiv.org/abs/2507.20472)
*Son N. T. Tu,Jianlu Zhang*

Main category: math.AP

TL;DR: The paper analyzes the asymptotic behavior of solutions to a nonlinear Hamilton-Jacobi equation as a parameter approaches zero, using variational methods and Mather-type measures.


<details>
  <summary>Details</summary>
Motivation: To understand the limiting behavior of solutions to the Hamilton-Jacobi equation in a nonlinear context, especially when the Aubry set is localized.

Method: A variational approach is employed, including a modified variational formula to connect global and local solutions.

Result: Derives limiting Mather-type measures and establishes a selection principle for solutions.

Conclusion: The method extends localization techniques to nonlinear frameworks, providing insights into the asymptotic behavior of solutions.

Abstract: We study the asymptotic behavior of solutions to the fully nonlinear
Hamilton-Jacobi equation $H(x, Du, \lambda u) = 0$ in $\mathbb{R}^n$ as
$\lambda \to 0^+$. Under the assumption that the Aubry set is localized, we
employ a variational approach to derive limiting Mather-type measures and
formulate a selection principle. Central to our analysis is a modified
variational formula that bridges global and local state-constraint solutions,
thereby extending localization techniques to the nonlinear framework.

</details>


### [39] [Viscous-inertial waves on the surface of the Sun: modeling, forward and inverse problems](https://arxiv.org/abs/2507.20488)
*Tram Thi Ngoc Nguyen,Damien Fournier,Thorsten Hohage*

Main category: math.AP

TL;DR: The paper presents a mathematical framework for studying solar inertial oscillations, focusing on toroidal motions and solving a fourth-order scalar equation. It proves well-posedness, explores inverse problems for parameter reconstruction, and validates robustness through numerical experiments.


<details>
  <summary>Details</summary>
Motivation: To advance understanding of the Sun's dynamics by analyzing solar inertial oscillations, particularly under toroidal motion assumptions, and to develop methods for reconstructing key parameters from observations.

Method: The study uses a fourth-order scalar equation for inertial waves, proves well-posedness under specific conditions, and employs iterative regularization methods (Nesterov-Landweber) for inverse problem solving. Numerical experiments validate the approach.

Result: Well-posedness of wave solutions is proven, and the inverse problem of reconstructing viscosity and differential rotation parameters is successfully addressed, with robustness confirmed across various observation schemes and noise levels.

Conclusion: The framework provides a robust tool for studying solar inertial oscillations and reconstructing key parameters, offering new insights into the Sun's dynamics.

Abstract: This paper develops a mathematical framework for studying the newly
discovered solar inertial oscillations, offering promising new avenues for
exploring the Sun's dynamics. Under the assumption of purely toroidal motions,
the stream function of the flow satisfies a fourth-order scalar equation
governing inertial waves on the rotating Sun. We prove well-posedness of wave
solutions under explicit conditions on differential rotation. Moreover, we
study the inverse problem of simultaneously reconstructing viscosity and
differential rotation parameters from either complete or partial surface
observations. To this end, we verify the tangential cone condition, ensuring
the convergence of iterative regularization methods. Numerical experiments
employing the Nesterov-Landweber iteration confirm robustness of the
reconstruction across different observation schemes and noise levels.

</details>


### [40] [Higher regularity in nonlocal free boundary problems](https://arxiv.org/abs/2507.20581)
*Begoña Barrios,Xavier Ros-Oton,Marvin Weidner*

Main category: math.AP

TL;DR: The paper investigates higher regularity in nonlocal free boundary problems for integro-differential operators of order 2s, proving C^∞ smoothness for C^{2,α} free boundaries in the nonlocal one-phase problem. It also addresses overdetermined problems and the nonlocal obstacle problem, introducing new tools like integration by parts formulas and boundary Hölder estimates.


<details>
  <summary>Details</summary>
Motivation: To extend the understanding of regularity in nonlocal free boundary problems, particularly for integro-differential operators, and to generalize results beyond the fractional Laplacian case (s=1/2).

Method: Develops new integration by parts formulas and boundary Hölder estimates for nonlocal equations with Neumann boundary conditions, applying these tools to analyze the nonlocal one-phase problem and overdetermined problems.

Result: Proves that C^{2,α} free boundaries in the nonlocal one-phase problem are C^∞, even for general integro-differential operators. Also shows smoothness of ∂Ω in overdetermined problems with smooth boundary conditions.

Conclusion: The approach is robust and applicable to various nonlocal problems, providing new insights and tools for studying regularity in free boundary problems.

Abstract: We study the higher regularity in nonlocal free boundary problems posed for
general integro-differential operators of order $2s$. Our main result is for
the nonlocal one-phase (Bernoulli) problem, for which we establish that
$C^{2,\alpha}$ free boundaries are $C^\infty$. This is new even for the
fractional Laplacian, as it was only known in case $s=\frac12$. We also
establish a general result for overdetermined problems, showing that if the
boundary condition is smooth, then so is $\partial\Omega$. Our approach is very
robust and works as well for the nonlocal obstacle problem, where it yields a
new proof of the higher regularity of free boundaries, completely different
from the one in [AbRo20]. In order to prove our results, we need to develop,
among other tools, new integration by parts formulas and delicate boundary
H\"older estimates for nonlocal equations with (local) Neumann boundary
conditions that had not been studied before and are of independent interest.

</details>


### [41] [Deriving sub-diffusion equations](https://arxiv.org/abs/2507.20602)
*Benoît Perthame,Min Tang*

Main category: math.AP

TL;DR: The paper derives sub-diffusion equations from microscopic mechanisms involving age-structured waiting times and jumps at renewals, addressing integrability challenges with Laplace transforms.


<details>
  <summary>Details</summary>
Motivation: Sub-diffusions are less studied compared to super-diffusions, despite their wide applications in fields like fluids, plasma physics, and biology. This paper aims to rigorously derive sub-diffusions from microscopic principles.

Method: The authors use age-structured equations for waiting time distributions and jumps at renewals, employing Laplace transforms to handle integrability issues in the age equilibrium.

Result: The paper successfully derives sub-diffusion equations, overcoming the challenge of non-integrable age equilibrium by leveraging Laplace transforms.

Conclusion: The work provides a rigorous microscopic foundation for sub-diffusions, highlighting the utility of Laplace transforms in addressing integrability problems.

Abstract: Sub-diffusion equations are used in a large range of applications including
fluids, plasma physics and biology. Their mathematical analysis is advanced
even if a much larger literature addresses super-diffusions. The goal of this
paper is to provide the microscopic mechanism and rigorous derivation of
sub-diffusions when the waiting time distribution of particles follows an
age-structured equation and jumps occur at each renewal. The major difficulty
to recover sub-diffusions, unlike normal diffusions, is that the assumption of
long waiting time implies lack of integrability for the age equilibrium. This
prevents to establish strong a priori estimates. Here, the Laplace transform
plays the role that Fourier transform plays for the more traditional case of
fast diffusions.

</details>


### [42] [Minimization of Degenerate Nonlinear Functionals under Radial Symmetry](https://arxiv.org/abs/2507.20603)
*Valeria Chiadò Piat,Virginia De Cicco,Anderson Melchor Hernandez*

Main category: math.AP

TL;DR: The paper studies minimization of nonlinear functionals with degenerate radial weights, proving existence of minimizers and their radial symmetry under $p$-growth conditions.


<details>
  <summary>Details</summary>
Motivation: To address minimization problems for functionals with degenerate weights lacking classical assumptions like doubling or Muckenhoupt conditions.

Method: Uses a weighted Poincaré inequality with an auxiliary weight, building on prior work by Chiadò Piat et al.

Result: Existence of minimizers in a new functional class and proof of their radial symmetry.

Conclusion: The approach successfully handles degenerate weights and establishes radial symmetry for minimizers.

Abstract: In this work, we study the minimization of nonlinear functionals in dimension
$d\geq 1$ that depend on a degenerate radial weight $w$. Our goal is to prove
the existence of minimizers in a suitable functional class here introduced and
to establish that the minimizers of such functionals, which exhibit $p$-growth
with $1 < p < +\infty$, are radially symmetric. In our analysis, we adopt the
approach developed in [Chiad\`o Piat, De Cicco and Melchor Hernandez, NoDEA
$2025$, De Cicco and Serra Cassano, ESAIM:COCV $2024$], where $w$ does not
satisfy classical assumptions such as doubling or Muckenhoupt conditions. The
core of our method relies on proving the validity of a weighted Poincar\'e
inequality involving a suitably constructed auxiliary weight.

</details>


### [43] [Classification of singular limits for free boundary and singularly perturbed elliptic problems: the Dancer-Yan spikes revisited](https://arxiv.org/abs/2507.20725)
*Daniele Bartolucci,Aleks Jevnikar,Juncheng Wei,Ruijun Wu*

Main category: math.AP

TL;DR: The paper classifies singular limits in a 2D free boundary problem from plasma physics, revealing a richer spiking structure than just Dancer-Yan spikes.


<details>
  <summary>Details</summary>
Motivation: To understand the asymptotic behaviors in a plasma physics free boundary problem, particularly focusing on the spiking structures in 2D.

Method: Classification under natural integral bounds, with detailed local-to-global analysis of spike differences.

Result: Identifies a richer spiking structure beyond Dancer-Yan spikes, with precise descriptions of singular behaviors.

Conclusion: The 2D case exhibits more complex spiking behavior than higher dimensions, requiring nuanced analysis.

Abstract: We classify the singular limits relative to a free boundary problem arising
in plasma physics in dimension $d=2$, under suitable natural integral bounds.
It turns out that one of the asymptotic behaviors allowed corresponds to the
Dancer-Yan spikes (J. London Math. Soc. ({\bf 78}) 2008, 639--662).
Interestingly enough, roughly speaking and unlike the higher dimensional case,
it is not true that any solution in the limit is a Dancer-Yan spike. Indeed,
the spiking structure is more rich and we succeed in a detailed description of
the singular behavior by a careful analysis, from local to global, of the tiny
difference between the maximum value of the spikes and their ``vanishing
level'' defining the free boundary.

</details>


### [44] [Existence, uniqueness, and long-time asymptotic behavior of regular solutions in multidimensional thermoelasticity](https://arxiv.org/abs/2507.20794)
*Piotr Michał Bies,Tomasz Cieślak,Mario Fuest,Johannes Lankeit,Boris Muha,Srdan Trifunović*

Main category: math.AP

TL;DR: The paper extends a 1D thermoelasticity model to 2D/3D tori, introducing a Fisher information-based functional to prove global/local existence of solutions and analyze asymptotic behavior.


<details>
  <summary>Details</summary>
Motivation: To generalize previous 1D results to higher dimensions and study the behavior of solutions over time.

Method: Introduces a novel functional involving Fisher information for temperature, applied to a nonlinear thermoelasticity model.

Result: Proves existence of unique solutions for small/large data; shows temperature stabilizes to a constant and displacement decomposes into oscillating and decaying parts.

Conclusion: The model successfully extends to higher dimensions, with clear asymptotic behavior and analogous results for the Lamé operator.

Abstract: We study a simplified nonlinear thermoelasticity model on two- and
three-dimensional tori. A novel functional involving the Fisher information
associated with temperature is introduced, extending the previous
one-dimensional approach from the first two authors (SIAM J.\ Math.\ Anal.\
\textbf{55} (2023), 7024--7038)) to higher dimensions. Using this functional,
we prove global/local existence of unique regular solutions for small/large
initial data. Furthermore, we analyze the asymptotic behavior as time
approaches infinity and show that the temperature stabilizes to a constant
state, while the displacement naturally decomposes into two distinct
components: a divergence-free part oscillating indefinitely according to a
homogeneous wave equation and a curl-free part converging to zero. Analogous
results for the Lam\'e operator are also stated.

</details>


### [45] [Convergence of two-scale expansions for elastic heterogeneous plates](https://arxiv.org/abs/2507.20874)
*Virginie Ehrlacher,Arthur Lebée,Frédéric Legoll,Adrien Lesage*

Main category: math.AP

TL;DR: The paper proves strong convergence results for highly oscillatory problems in thin domains, comparing solutions to their two-scale expansions. It covers linear diffusion and elasticity, with novel proof strategies for bending cases.


<details>
  <summary>Details</summary>
Motivation: To address the gap in understanding the convergence of solutions to highly oscillatory problems in thin domains, particularly for linear elasticity, which presents unique challenges.

Method: Classical homogenization arguments are adapted for linear diffusion and membrane elasticity. A new proof strategy is introduced for bending elasticity in two dimensions.

Result: Strong convergence results are established for linear diffusion and membrane elasticity. A novel approach successfully tackles the bending case.

Conclusion: The study provides rigorous convergence proofs for thin-domain oscillatory problems, with innovative methods for bending elasticity, advancing the field.

Abstract: The aim of this article is to prove strong convergence results on the
difference between the solution to highly oscillatory problems posed in thin
domains and its two-scale expansion. We first consider the case of the linear
diffusion equation and establish such results in arbitrary dimensions, by using
a straightforward adaptation of the classical arguments used for the
homogenization of highly oscillatory problems posed on fixed (non-thin)
domains. We next consider the linear elasticity problem, which raises
challenging difficulties in its full generality. Under some classical
assumptions on the symmetries of the elasticity tensor, the problem can be
split into two independent problems, the membrane problem and the bending
problem. Focusing on two-dimensional problems, we show that the membrane case
can actually be addressed using a careful adaptation of classical arguments. In
the bending case, the scheme of the proof used in the membrane and diffusion
cases can however not be straightforwardly adapted. In that bending case, we
establish the desired strong convergence results by using a different strategy
of proof, which seems, up to our knowledge, to be new.

</details>


### [46] [Regularity of stable solutions to the MEMS problem up to the optimal dimension 6](https://arxiv.org/abs/2507.20916)
*Renzo Bruera,Xavier Cabre*

Main category: math.AP

TL;DR: The paper establishes interior regularity estimates for stable solutions to semilinear elliptic equations with MEMS-type nonlinearities, independent of boundary conditions, valid up to dimension 6.


<details>
  <summary>Details</summary>
Motivation: To address the regularity of stable solutions to semilinear elliptic equations with nonlinearities that blow up at a boundary point, extending understanding beyond known results.

Method: The study uses interior regularity estimates, focusing on the $L^\infty$ norm of a primitive of the nonlinearity, controlled by the $L^1$ norm of the solution. Assumptions include a Crandall-Rabinowitz type condition for dimensions $n \geq 3$.

Result: Interior regularity estimates are proven for dimensions up to 6, with counterexamples for higher dimensions. Global estimates for the Dirichlet problem are improved for $n \leq 6$, and broader results are given for $n \leq 2$.

Conclusion: The paper provides new regularity estimates for stable solutions, extending known results and highlighting dimensional constraints.

Abstract: In this article we address the regularity of stable solutions to semilinear
elliptic equations $-\Delta u = f(u)$ with MEMS type nonlinearities. More
precisely, we will have $0\leq u \leq 1$ in a domain $\Omega \subset
\mathbb{R}^n$ and $f:[0,1)\to (0,+\infty)$ blowing up at $u=1$ and
nonintegrable near 1. In this context, a solution $u$ is regular if $u<1$ in
all $\Omega$ or, equivalently, if $-\Delta u = f(u)<+\infty$ in $\Omega$.
  This paper establishes for the first time interior regularity estimates that
are independent of the boundary condition that $u$ may satisfy. Our results
hold up to the optimal dimension $n=6$ (there are counterexamples for $n\geq
7$) but require a Crandall-Rabinowitz type assumption on the nonlinearity $f$
when $n\geq 3$. Our main estimate controls the $L^\infty$ norm of $F(u)$ in a
ball, where $F$ is a primitive of $f$, by only the $L^1$ norm of $u$ in a
larger ball.
  Under the same assumptions, we also give global estimates in dimensions
$n\leq 6$ for the Dirichlet problem with vanishing boundary condition,
improving previously known results. In dimensions $n\leq 2$, our estimate holds
for all nonnegative, nondecreasing, convex nonlinearities which blow up at 1
and are nonintegrable near 1.

</details>


### [47] [Coherent Structures in Flame Fronts](https://arxiv.org/abs/2507.20918)
*Sultan Aitzhan,Benjamin F. Akers,David M. Ambrose*

Main category: math.AP

TL;DR: Study of traveling waves in flame fronts, proving existence and computing such waves in a periodic, unbounded setting.


<details>
  <summary>Details</summary>
Motivation: Understand the dynamics of flame fronts as interfaces between burnt and unburnt gas phases during combustion.

Method: Uses arclength parameterization framework for traveling waves, building on prior work by the authors and Wright.

Result: Proves existence of vertically traveling waves of permanent form in periodic, unbounded settings.

Conclusion: Demonstrates successful analysis and computation of traveling waves in flame fronts, advancing understanding of combustion dynamics.

Abstract: We study traveling waves in a coordinate-free model of flame fronts. The
flame front is the interface between the burnt and unburnt phases of a gas
undergoing combustion. The front therefore moves in a preferred direction, as
the unburnt gas is consumed. In the horizontally periodic, vertically unbounded
setting, we prove the existence of waves of permanent form which are traveling
in the vertical direction. We also compute these waves. The analysis and
computation use the framework of traveling waves in the arclength
parameterization as previously developed by two of the authors and Wright.

</details>


### [48] [Normalized solutions for the nonlinear Schrödinger equation with potentials](https://arxiv.org/abs/2507.20961)
*Matteo Rizzi,Xueqin Peng*

Main category: math.AP

TL;DR: The paper addresses finding normalized solutions to a Schrödinger equation with a fixed L²-norm, using energy minimization and tackling nonradial terms with the splitting lemma. Radial solutions are also explored.


<details>
  <summary>Details</summary>
Motivation: The study aims to solve a Schrödinger equation with constraints, focusing on challenges posed by nonradial terms and fixed L²-norm conditions.

Method: Solutions are constructed by minimizing the energy functional on a constraint, using the splitting lemma to handle nonradial terms and ensure compactness. Radial symmetry is leveraged for specific cases.

Result: Normalized solutions are found for the given equation, with radial solutions obtained when the term h is radial. Solutions converge to a ground state as μ approaches zero.

Conclusion: The paper successfully addresses the problem by combining energy minimization and symmetry considerations, providing solutions for both nonradial and radial cases.

Abstract: In this paper, we find normalized solutions to the following Schr\"{o}dinger
equation \begin{equation}\notag \begin{aligned} &-\Delta
u-\frac{\mu}{|x|^2}h(x)u+\lambda u =f(u)\quad\text{in}\quad\mathbb{R}^{N},\\ &
u>0,\quad \int_{\mathbb{R}^{N}}u^2dx=a^2, \end{aligned} \end{equation} where
$N\geq3$, $a>0$ is fixed, $f$ satisfies mass-subcritical growth conditions and
$h$ is a given bounded function with $||h||_\infty\le 1$. The
$L^2(\mathbb{R}^N)$-norm of $u$ is fixed and $\lambda$ appears as a Lagrange
multiplier. Our solutions are constructed by minimizing the corresponding
energy functional on a suitable constraint. Due to the presence of a possibly
nonradial term $h$, establishing compactness becomes challenging. To address
this difficulty, we employ the splitting lemma to exclude both the vanishing
and the dichotomy of a given any minimizing sequence for appropriate $a > 0$.
  Furthermore, we show that if $h$ is radial, then radial solutions can be
obtained for any $a>0$. In this case, the radial symmetry allows us to prove
that such solutions converge to a ground state solution of the limit problem as
$\mu \to 0^+$.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [49] [Proper Orthogonal Decomposition-based Model-Order Reduction for Smoothed Particle Hydrodynamics Simulation](https://arxiv.org/abs/2507.19825)
*Lidong Fang,Zilong Song,Kirk Fraser,Faisal Habib,Christopher Drummond,Huaxiong Huang*

Main category: physics.comp-ph

TL;DR: A projection-based model-order reduction (MOR) technique using proper orthogonal decomposition (POD) is applied to smoothed particle hydrodynamics (SPH) simulations, showing reduced computational error and effective acceleration.


<details>
  <summary>Details</summary>
Motivation: To explore the feasibility of POD-based MOR in SPH simulations and quantify the associated POD error, using friction stir spot welding as a test case.

Method: Utilizes POD to generate a subspace basis for reduction, with linearization and freezing coefficients for acceleration.

Result: POD-MOR significantly reduces computational error compared to uniform particle reduction and works effectively in predictive settings with varying parameters.

Conclusion: POD-MOR is a promising technique for SPH simulations, offering accuracy and computational efficiency.

Abstract: In this paper, we present a projection-based model-order reduction (MOR)
technique for smoothed particle hydrodynamics (SPH) simulations, which is a
mesh-free approach within the Lagrangian framework. Our approach utilizes the
proper orthogonal decomposition (POD) technique to generate a subspace basis
for the reduction process.
  The main objective of this study is to conduct an initial exploration of the
feasibility of employing POD-based MOR (POD-MOR) in SPH simulations and to
quantify the associated POD error. To illustrate the effectiveness of this
approach, we consider the friction stir spot welding problem, which involves
the coupling of flow equations and heat equation.
  Our findings reveal that, with the same degrees of freedom, POD-MOR
significantly reduces computational error compared to the uniform reduction of
particle numbers in SPH simulations.
  Additionally, the acceleration technique of POD-MOR for SPH simulation via
linearization and freezing coefficients has been shown to be effective while
keeping the error small. We have also showed the effectiveness of POD-MOR in
predictive settings in SPH simulations with different parameter values.

</details>


### [50] [Multi-Resolution Training-Enhanced Kolmogorov-Arnold Networks for Multi-Scale PDE Problems](https://arxiv.org/abs/2507.19888)
*Yu-Sen Yang,Ling Guo,Xiaodan Ren*

Main category: physics.comp-ph

TL;DR: MR-PIKAN, a multi-resolution training-enhanced PIKAN framework, reduces computational costs while maintaining accuracy for solving multi-scale PDE problems.


<details>
  <summary>Details</summary>
Motivation: Multi-scale PDEs are challenging for conventional MLP-based methods due to spectral bias. PIKAN addresses this but requires high-resolution training points, increasing computational costs.

Method: Proposes MR-PIKAN, which trains the hybrid model sequentially or alternately across different resolutions.

Result: Validated on multi-scale PDE problems, MR-PIKAN reduces training time without losing accuracy.

Conclusion: MR-PIKAN efficiently solves complex multi-scale PDEs in forward and inverse settings.

Abstract: Multi-scale PDE problems present significant challenges in scientific
computing. While conventional MLP-based deep learning methods exhibit spectral
bias in resolving multi-scale features, the physics-informed Kolmogorov-Arnold
network (PIKAN) mitigates this issue through its novel architecture,
demonstrating certain advantages. On the other hand, insights from the
information bottleneck theory suggest that high-resolution training points are
essential for these hybrid methods to accurately capture multi-scale behavior,
although this requirement often leads to longer training times. To address this
challenge, we propose a simple yet effective multi-resolution training-enhanced
PIKAN framework, termed MR-PIKAN, which trains the data-physics hybrid model
either sequentially or alternately across different resolutions. The proposed
MR-PIKAN is validated on various multi-scale forward and inverse PDE problems.
Numerical results indicate that this new training strategy effectively reduces
computational costs without sacrificing accuracy, thereby enabling efficient
solutions of complex multi-scale PDEs in both forward and inverse settings.

</details>


### [51] [Iterative Pretraining Framework for Interatomic Potentials](https://arxiv.org/abs/2507.20118)
*Taoyong Cui,Zhongyao Wang,Dongzhan Zhou,Yuqiang Li,Lei Bai,Wanli Ouyang,Mao Su,Shufei Zhang*

Main category: physics.comp-ph

TL;DR: IPIP is a framework for improving MLIPs by iterative pretraining with a forgetting mechanism, achieving higher accuracy and efficiency than general-purpose models.


<details>
  <summary>Details</summary>
Motivation: Existing pretraining strategies for MLIPs suffer from mismatched objectives or require extensive labeled data, limiting their performance and generalization.

Method: IPIP uses iterative pretraining with a forgetting mechanism to avoid suboptimal local minima, employing lightweight architectures.

Result: IPIP reduces prediction error by over 80% and achieves up to 4x speedup in the Mo-S-O system.

Conclusion: IPIP offers a more accurate and efficient alternative to general-purpose MLIPs for specialized tasks.

Abstract: Machine learning interatomic potentials (MLIPs) enable efficient molecular
dynamics (MD) simulations with ab initio accuracy and have been applied across
various domains in physical science. However, their performance often relies on
large-scale labeled training data. While existing pretraining strategies can
improve model performance, they often suffer from a mismatch between the
objectives of pretraining and downstream tasks or rely on extensive labeled
datasets and increasingly complex architectures to achieve broad
generalization. To address these challenges, we propose Iterative Pretraining
for Interatomic Potentials (IPIP), a framework designed to iteratively improve
the predictive performance of MLIP models. IPIP incorporates a forgetting
mechanism to prevent iterative training from converging to suboptimal local
minima. Unlike general-purpose foundation models, which frequently underperform
on specialized tasks due to a trade-off between generality and system-specific
accuracy, IPIP achieves higher accuracy and efficiency using lightweight
architectures. Compared to general-purpose force fields, this approach achieves
over 80% reduction in prediction error and up to 4x speedup in the challenging
Mo-S-O system, enabling fast and accurate simulations.

</details>


### [52] [Neuromorphic Photonic Processing and Memory with Spiking Resonant Tunnelling Diode Neurons and Neural Networks](https://arxiv.org/abs/2507.20866)
*Dafydd Owen-Newns,Joshua Robertson,Giovanni Donati,Jose Figueiredo,Edward Wasige,Kathy Ludge,Bruno Romeira,Antonio Hurtado*

Main category: physics.comp-ph

TL;DR: The paper explores neuromorphic computing using photonic techniques, focusing on resonant tunnelling diodes (RTDs) for their neuronal behavior replication and efficiency. It demonstrates RTD applications in edge-detection, neural networks, and memory systems.


<details>
  <summary>Details</summary>
Motivation: To advance artificial intelligence by leveraging photonic neuromorphic computing, utilizing RTDs for their high bandwidth, low-crosstalk, and parallelism.

Method: Uses optically-triggered spiking RTD neurons for applications like edge-detection, constructing a two-layer feedforward photonic spiking neural network (pSNN), and a tunable neuromorphic optical spiking memory system.

Result: RTD-based systems show excellent performance in complex dataset classification and adjustable memory storage of spiking patterns.

Conclusion: RTDs are promising for next-generation neuromorphic hardware due to their efficiency, speed, and versatility in photonic applications.

Abstract: Neuromorphic computing-modelled after the functionality and efficiency of
biological neural systems-offers promising new directions for advancing
artificial intelligence and computational models. Photonic techniques for
neuromorphic computing hardware are attracting increasing research interest,
thanks to their potentials for ultra high bandwidths, low-crosstalk and high
parallelism. Among these, approaches based upon resonant tunnelling diodes
(RTDs) have recently gained attention as potential building blocks for
next-generation light-enabled neuromorphic hardware, due to their capacity to
replicate key neuronal behaviours such as excitable spiking and refractoriness,
added to their potentials for high operational speeds, energy efficiency and
compact footprints. In particular, their ability to function as opto-electronic
spiking neurons makes them strong candidates for integration into novel event
based neuromorphic computing systems. This work demonstrates the application of
optically-triggered spiking RTD neurons to a multiplicity of applications and
architectures, these include systems based upon single elements for multi-modal
(photonic-electronic) fast rising edge-detection in time-series data, the
construction of a two-layer feedforward artificial photonic spiking neural
network (pSNN) using RTD neurons as the nonlinear nodes delivering excellent
performance in complex dataset classification tasks, and a pSNN comprised of
multiple coupled light-sensitive RTD spiking neurons that supports performance
as an adjustable neuromorphic optical spiking memory system with a tunable
storage time of spiking patterns.

</details>


### [53] [Parallel athermal quasistatic deformation stepping of molecular systems](https://arxiv.org/abs/2507.20802)
*Maximilian Reihn,Franz Bamer,Benjamin Stamm*

Main category: physics.comp-ph

TL;DR: The paper introduces a parallel athermal stepping scheme to speed up the computationally expensive athermal quasistatic deformation method in molecular simulations.


<details>
  <summary>Details</summary>
Motivation: The traditional athermal quasistatic deformation method is computationally intensive due to sequential affine deformation and energy minimization steps.

Method: The proposed method uses a two-level parallel approach: Level I for large-increment initial guesses and Level II for fine-resolution parallel steps between these guesses.

Result: Testing shows speed-ups of 1.49 to 27.09 times with 4 to 32 threads, averaging 3.0 to 6.62, while maintaining accuracy.

Conclusion: The method offers a significant computational advantage for athermal molecular simulations without sacrificing accuracy.

Abstract: The athermal quasistatic deformation method provides an elegant solution to
overcome the limitation of short time spans in molecular simulations. It
provides overdamped conditions, allowing for the extraction of purely
structural responses in the absence of thermal vibration. However, it requires
computationally expensive sequences of affine deformation followed by
minimization of the potential energy to incrementally find the path in the
potential energy landscape that corresponds to the correct solution trajectory.
Therefore, we propose an athermal parallel stepping scheme that significantly
improves the computational time necessary to find the correct solution
trajectory using a multi-thread approach. Our approach proposes athermal
stepping at two levels. Level I stepping provides a sequence of initial guesses
at large increments by affine deformation of the system and land-marking anchor
points on the potential energy landscape. Level II stepping performs a set of
individual finely resolved athermal quasistatic deformation steps between the
inherent structures of the initial level I guesses executed in parallel. The
evaluated candidate trajectory is then verified by consecutively comparing the
configuration of every last level II result with the corresponding inherent
structure of the level I guesses at the same strain states. If the two
configurations are not equivalent, the solution must be rejected and
recalculated from this point. Rigorous numerical testing with $4,8,16$ and $32$
parallel threads demonstrates that our method achieves computational speed-ups
of factors ranging from $1.49$ to $27.09$, with averages of $3.0$ to $6.62$
depending on the number of threads, while maintaining simulation accuracy,
offering a powerful new tool for athermal molecular simulations.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [54] [Exploring the fusion power plant design space: comparative analysis of positive and negative triangularity tokamaks through optimization](https://arxiv.org/abs/2507.19668)
*T. Slendebroek,A. O. Nelson,O. M. Meneghini,G. Dose,A. G. Ghiozzi,J. Harvey,B. C. Lyons,J. McClenaghan,T. F. Neiser,D. B. Weisberg,M. G. Yoo,E. Bursch,C. Holland*

Main category: physics.plasm-ph

TL;DR: The study compares positive (PT) and negative triangularity (NT) tokamaks for fusion plants, finding similar cost-performance trade-offs but differing design constraints. PT requires larger machines due to operational limits, while NT enables compact, high-field designs. Configuration choice should consider risk tolerance.


<details>
  <summary>Details</summary>
Motivation: To determine the optimal tokamak configuration (PT or NT) for fusion power plants by evaluating trade-offs between cost, reliability, and engineering constraints.

Method: Used the FUSE framework for multi-objective optimization, performing over 200,000 design evaluations to compare PT and NT configurations under constraints like net electric power, tritium breeding, and power exhaust limits.

Result: PT and NT achieve similar cost-performance but differ in design: PT is resilient but requires larger machines, while NT allows compact, high-field designs. PT's viability depends on uncertain physics, whereas NT is robust to variations.

Conclusion: Configuration selection should be risk-informed: PT is optimal if constraints are predictable, while NT is better for handling uncertainties.

Abstract: The optimal configuration choice between positive triangularity (PT) and
negative triangularity (NT) tokamaks for fusion power plants hinges on
navigating different operational constraints rather than achieving specific
plasma performance metrics. This study presents a systematic comparison using
constrained multi-objective optimization with the integrated FUsion Synthesis
Engine (FUSE) framework. Over 200,000 integrated design evaluations were
performed exploring the trade-offs between capital cost minimization and
operational reliability (maximizing $q_{95}$) while satisfying engineering
constraints including 250 $\pm$ 50 MW net electric power, tritium breeding
ratio $>$1.1, power exhaust limits and an hour flattop time. Both
configurations achieve similar cost-performance Pareto fronts through
contrasting design philosophies. PT, while demonstrating resilience to pedestal
degradation (compensating for up to 40% reduction), are constrained to larger
machines ($R_0$ $>$ 6.5 m) by the narrow operational window between L-H
threshold requirements and the research-established power exhaust limit
($P_{sol}/R$ $<$ 15 MW/m). This forces optimization through comparatively
reduced magnetic field ($\sim$8T). NT configurations exploit their freedom from
these constraints to access compact, high-field designs ($R_0 \sim 5.5$ m,
$B_0$ $>$ 12 T), creating natural synergy with advancing HTS technology.
Sensitivity analyses reveal that PT's economic viability depends critically on
uncertainties in L-H threshold scaling and power handling limits. Notably, a
50% variation in either could eliminate viable designs or enable access to the
compact design space. These results suggest configuration selection should be
risk-informed: PT offers the lowest-cost path when operational constraints can
be confidently predicted, while NT is robust to large variations in constraints
and physics uncertainties.

</details>


### [55] [Semi-Implicit Continuum Kinetic Modeling of Weakly Collisional Parallel Transport in a Magnetic Mirror](https://arxiv.org/abs/2507.19729)
*M. Dorf,M. Dorr,V. Geyko,D. Ghosh,M. Umansky,J. Angus*

Main category: physics.plasm-ph

TL;DR: IMEX kinetic simulations in magnetic mirror configurations using COGENT enable larger time steps and faster computations compared to explicit methods, with validation via a reduced bounce-averaged model.


<details>
  <summary>Details</summary>
Motivation: To overcome severe time-step limitations in explicit schemes for weakly collisional parallel plasma transport in magnetic mirrors.

Method: Uses a Jacobian-free Newton-Krylov method with algebraic multigrid preconditioning and a reduced bounce-averaged model for validation.

Result: IMEX allows time steps 2.5x10^4 times larger than explicit methods, yielding a 2500x speedup in simulations.

Conclusion: The IMEX approach is efficient for kinetic simulations, validated by the bounce-averaged model and collision operator comparisons.

Abstract: We present implicit-explicit (IMEX) kinetic simulations of weakly collisional
parallel plasma transport in magnetic mirror configurations using the continuum
code \textsc{COGENT}. The numerical scheme employs a Jacobian-free
Newton--Krylov method with algebraic multigrid preconditioning to overcome the
severe time-step limitations imposed by strong mirror forces in fully explicit
schemes. Applied to parameters relevant to the WHAM mirror experiment, the IMEX
approach enables time steps up to $2.5 \times 10^4$ times larger than those
permitted by explicit methods, resulting in a 2500x speedup in 1D--2V
simulations of parallel transport with kinetic ions and Boltzmann electrons.
Additionally, a reduced bounce-averaged model for a square mirror is
implemented to support the computationally intensive fully kinetic simulations.
The bounce-averaged formulation is used to evaluate the numerical convergence
of the velocity-space discretization algorithms and to assess the role of the
collision model by comparing simulations employing the nonlinear Fokker--Planck
and the simplified Lenard--Bernstein--Dougherty collision operators.

</details>


### [56] [On the interplay between plasma triangularity and micro-tearing turbulence](https://arxiv.org/abs/2507.20244)
*Alessandro Balestri,Justin Ball,Stefano Coda*

Main category: physics.plasm-ph

TL;DR: Negative triangularity in tokamaks increases susceptibility to micro-tearing modes (MTMs), worsening transport compared to positive triangularity. Spherical tokamaks are particularly affected, but lowering magnetic shear can mitigate this.


<details>
  <summary>Details</summary>
Motivation: To understand how triangularity shapes plasma turbulence and transport in tokamaks, especially comparing negative and positive triangularity scenarios.

Method: Used linear and nonlinear flux tube GENE simulations for various tokamaks (TCV, DIII-D, MAST-U, SMART, EU-DEMO) to study MTM turbulence.

Result: Negative triangularity is more prone to MTMs, worsening transport, while positive triangularity remains dominated by electrostatic turbulence. Spherical tokamaks are especially vulnerable unless magnetic shear is reduced.

Conclusion: Negative triangularity's benefits are preserved in conventional tokamaks but compromised in spherical ones unless magnetic shear is lowered to prevent strong MTM transport.

Abstract: In this work, we study the interplay between triangularity and micro-tearing
turbulence using linear and nonlinear flux tube GENE simulations. We consider
scenarios with negative and positive triangularity plasma shaping taken from
existing tokamaks (TCV, DIII-D, MAST-U and SMART) and EU-DEMO. The study of all
these tokamaks reveals a coherent picture. Negative triangularity geometry is
more susceptible to micro-tearing modes (MTM), which, when present, make
transport much worse than in positive triangularity. At sufficiently large
$\beta$ (the ratio of plasma pressure over magnetic pressure), magnetic shear
and ratio of electron to ion temperature gradient, all the scenarios with
negative triangularity are dominated by MTM turbulence. In contrast, the
corresponding scenarios with positive triangularity remain dominated by
electrostatic turbulence and MTMs are subdominant or stable. We observe that
conventional tokamaks usually operate in a parameter space far away from the
onset of this MTM-dominated regime in negative triangularity, thus preserving
the beneficial effect of negative triangularity on turbulent transport. In
contrast, spherical tokamaks operate close to this regime and may ultimately
exhibit worse transport at negative triangularity than positive triangularity.
We find that lowering the magnetic shear in spherical tokamaks can preserve the
beneficial effect of negative triangularity on electrostatic turbulence and
prevent strong MTM transport. Finally, linear and nonlinear simulations reveal
the reason for stronger MTMs: the magnetic drifts are faster in the negative
triangularity geometry.

</details>


### [57] [Hybrid fluid-kinetic cylindrical equilibria with axial background magnetic field](https://arxiv.org/abs/2507.20248)
*D. A. Kaltsas,A. I. Kuiroukidis,G. N. Throumoulopoulos*

Main category: physics.plasm-ph

TL;DR: The paper constructs self-consistent quasineutral screw-pinch equilibria using a hybrid model with fluid electrons and kinetic ions, solving quasilinear ODEs numerically to study static and dynamic equilibria.


<details>
  <summary>Details</summary>
Motivation: To explore equilibria in a hybrid model coupling fluid electrons and kinetic ions, focusing on the impact of free parameters on equilibrium characteristics.

Method: A hybrid model with fluid electrons and Vlasov-governed kinetic ions is used. The problem is reduced to four quasilinear ODEs, solved numerically for equilibria with and without ion sheared velocities.

Result: Static and dynamic equilibria are obtained, with isotropic electron pressure and non-gyrotropic ion pressure. The influence of free parameters on equilibrium properties is analyzed.

Conclusion: The study successfully models screw-pinch equilibria, highlighting the role of kinetic ions and providing insights into equilibrium characteristics under varying parameters.

Abstract: Self-consistent, one-dimensional quasineutral screw-pinch equilibria are
constructed within a hybrid model that couples fluid electrons with kinetic
ions governed by the Vlasov equation. The equilibria depend on the radial
coordinate perpendicular to the cylindrical axis and include an axial
background magnetic field. Adopting a three-parameter ion distribution function
depending on the energy and the canonical momenta conjugate to the two
ignorable coordinates, the problem is reduced to a set of four quasilinear ODEs
which are solved numerically. Both static equilibria and equilibria with
macroscopic ion sheared velocities are obtained. The pressure of the electron
fluid is isotropic and the electron contribution to the current density is
parallel to the magnetic field, while the kinetic ions are associated with a
non-gyrotropic pressure tensor. By means of the solutions the various
equilibrium quantities are calculated and the impact of the free parameters on
the equilibrium characteristics is examined.

</details>


### [58] [Multi-Machine Scaling Laws for Fuel and Impurity Puffing Rates Sufficient for Detachment Access: a Systematic Review of Magnetic Confinement Fusion Devices](https://arxiv.org/abs/2507.20523)
*M. Moscheni,A. Herrmann,R. Kembleton,M. Kryjak,S. Lazerson,F. Levi,M. Siccinio,P. Staniec,T. Giegerich,C. Tantos,the Gauss Fusion GmbH Team*

Main category: physics.plasm-ph

TL;DR: Multi-machine scaling laws predict fuel and impurity puffing rates for plasma detachment, validated across devices with high accuracy. Key correlations include divertor volume and plasma opaqueness, with simplified formulas provided for practical use.


<details>
  <summary>Details</summary>
Motivation: Address plasma-wall interaction in fusion reactors by deriving predictive scaling laws for detachment, a reactor-relevant solution.

Method: Analyzed 457 experimental and numerical data points from 32 machines, deriving physics-informed scaling laws and validating them in 40 plasmas.

Result: Laws predict detachment conditions within a factor of 1.5-2 accuracy. Key findings include correlations with divertor volume and plasma opaqueness, with simplified formulas for practical use.

Conclusion: The study provides actionable scaling laws for reactor design and edge plasma modeling, demonstrating the feasibility of physics-based detachment predictions.

Abstract: From a database of 457 experimental and numerical data from 32 machines among
solid-walled tokamaks, stellarators and linear plasma devices, we derive
physics-informed multi-machine scaling laws predictive of fuel and impurity
puffing rates sufficient to access plasma detachment -- leading candidate for a
reactor-relevant solution to the open issue of plasma-wall interaction.
Validation of our laws in up to 40 plasmas in low- and high-confinement mode
also featuring advanced configurations demonstrates accuracy within a factor
1.5 in up to 50% of the instances, and within a factor 2 on average. Divertor
volume alone is found to correlate to fuelling. The addition of plasma
opaqueness leads to the empirically-calibrated law $\Gamma_{\text{D}}\propto
[n_{\text{sep}}\times a\times(S_{\text{div}}/V_{\text{div}})^{-1.5}]^{1.05}$
valid across all toroidal devices. Its simplification to
$\Gamma_{\text{D}}^{\text{HDL}}\propto 0.43\times
a^{1.58}\times\lambda_q^{-0.89}$ avoids a dependence on $n_{\text{sep}}$ and
bears intrinsic significance provided by the H/L density limit and the power
fall-off length. Non-linearities of the impurity dynamics manifest in the
seeding rate -- which the present work identifies both a generalised formula
for, and the Greenwald-Eich-Scarabosio simplification
$\Gamma_{\text{Z}}^{\text{GES}}\propto a^{1.51}\times\lambda_q^{-0.27}$.
Similar relationships are defined for stellarators, closely following the
mainstream but without providing data for validation -- which should stimulate
further investigations. In the era of nuclear fusion reactor design, these
results find immediate applicability in informing reactor fuel cycle design and
edge plasma modelling. More generally, this study demonstrates that
physics-based laws able to relate detachment to engineering actuators exist,
and pave the way for analytical models bridging to the true inputs of
experimental operation.

</details>


### [59] [Characterizing ion-acoustic shock wave collisions in Martian multicomponent plasma environments](https://arxiv.org/abs/2507.20626)
*Jayshree Mondal,Prasanta Chatterjee,Laxmikanta Mandi,Biswajit Sahu*

Main category: physics.plasm-ph

TL;DR: The paper investigates colliding ion-acoustic shock waves in Martian plasmas, analyzing their dynamics and effects using theoretical and numerical methods.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of ion-acoustic shock waves in Martian multicomponent plasmas, including their collisions and interactions, using observed atmospheric parameters.

Method: A modified Poincaré-Lighthill-Kuo perturbation method is used to derive Burgers' equations, and numerical analysis is performed with MAVEN spacecraft data.

Result: Collisions broaden shock structures, with kinematic viscosity enhancing this effect, as shown in electrostatic potential profiles.

Conclusion: The study provides insights into shock wave dynamics in Martian plasmas, highlighting the role of collisions and viscosity.

Abstract: We present theoretical investigation of colliding ion-acoustic (IA) shock
waves in Martian multicomponent plasmas consisting of hydrogen ($H^+$), oxygen
($O^+$) and oxygen molecule ($O_2^+$) ions, including background superthermal
electrons (modeled by a $\kappa$-(kappa) distribution function). A set of
Burgers' equations is obtained by adopting a modified Poincar\'e-Lighthill-Kuo
(PLK) perturbation method to describe the head-on-collision dynamics of
dissipative nonlinear IA wave structures. We have estimated the spatio-temporal
scales using parameters typically observed in the Martian atmosphere by the
MAVEN spacecraft, for which shock waves are theoretically expected to undergo
mutual collisions in the multicomponent plasma. The effects of head-on
collisions on the electrostatic potential profiles arising from one-fold and
two-fold IA shock interactions are explored. Our numerical analysis reveals
that the collision leads to a noticeable broadening of the shock structures
with the enhancement of kinematic viscosity.

</details>


### [60] [Numerical Studies for EuPRAXIA@SPARC\_LAB Plasma Beam Driven Working Point](https://arxiv.org/abs/2507.20691)
*Stefano Romeo,Alessio Del Dotto,Massimo Ferrario,Anna Giribono,Andrea Renato Rossi,Gilles Jacopo Silvi,Cristina Vaccarezza*

Main category: physics.plasm-ph

TL;DR: The paper explores optimizing plasma-based acceleration for EuPRAXIA@SPARC_LAB, balancing high gradient, low emittance, and energy spread for soft x-ray FELs.


<details>
  <summary>Details</summary>
Motivation: Design a stable plasma-based accelerator for high-gradient, low-energy-spread beams to enable soft x-ray FELs.

Method: Simulation scans varying plasma density and driver-witness separation to identify an optimal working point.

Result: Achieved a witness beam with >1kA peak current, <0.1% core energy spread, and ~1 GV/m gradient.

Conclusion: Parametric analysis confirms stability requirements to maintain energy jitter at energy spread levels.

Abstract: The realization of a plasma based user facility on the model of
EuPRAXIA@SPARC\_LAB requires to design a working point for the operation that
allows to get an high accelerating gradient preserving a low emittance and low
energy spread of the accelerated beam. Such beam is supposed to pilot a soft
x-ray free electron laser with a wavelength of 2-\SI{4}{\nano\meter}. In this
work several simulation scans are presented, varying at the same time the
plasma density and driver-witness separation in order to show that, in a
realistic working point for EuPRAXIA@SPARC\_LAB, it is possible to find an
ideal compromise for a witness with a peak current >1kA that allows to preserve
the energy spread of the core (80\% of the charge) below 0.1\%, while
maintaining an accelerating gradient inside the plasma module around of 1 GV/m.
The study is completed with a parametric analysis with the aim of establishing
the stability requirements of the RF working point and the plasma channel in
order to preserve the energy jitter at the same level of the energy spread.

</details>


### [61] [Two-dimensional spatially resolved measurements of helium metastable densities by tunable diode laser absorption spectroscopy in atmospheric pressure RF plasma jets](https://arxiv.org/abs/2507.20748)
*David A Schulenberg,Xiao-Kun Wang,Mate Vass,Ihor Korolov,Thomas Mussenbrock,Julian Schulze*

Main category: physics.plasm-ph

TL;DR: A new 2D TDLAS method replaces mechanical scanning for faster, higher-resolution mapping of helium metastable densities in plasma jets.


<details>
  <summary>Details</summary>
Motivation: Traditional TDLAS methods are slow and limit spatial resolution due to mechanical scanning.

Method: Uses a rotating optical diffuser for uniform illumination and a short-wavelength infrared camera for high-resolution absorption profiling.

Result: Achieves ~10 μm spatial resolution and faster data acquisition, with results matching simulations and prior data.

Conclusion: The advanced 2D TDLAS method improves efficiency and accuracy in measuring helium metastable densities.

Abstract: Helium metastable species play a critical role in sustaining radio-frequency
(RF) driven micro atmospheric pressure plasma jets through Penning ionization
and for the generation of reactive oxygen and nitrogen species (RONS). Their
densities are typically measured using tunable diode laser absorption
spectroscopy (TDLAS). Most spatially resolved TDLAS approaches rely on
mechanical scanning of a narrow laser beam across the plasma, which is
time-consuming and limits spatial resolution. In this work, we present an
advanced two-dimensional (2D) TDLAS method that enables direct spatial mapping
of helium metastable densities without the need for mechanical scanning. A
rotating optical diffuser is employed to suppress speckle interference and
generate uniform illumination across the plasma region. The absorption profile
is captured using a short-wavelength infrared camera equipped with a
telecentric lens, achieving high spatial resolution (approximately 10 {\mu}m)
across the entire field of view. This approach significantly enhances both data
quality and acquisition speed. The improved 2D TDLAS system is applied to
measure helium metastable densities in plasma jets with structured electrodes
driven by different tailored voltage waveforms. The results show very good
qualitative agreement with fluid simulations and previously reported
experimental data.

</details>


### [62] [Plasma pressure response to non-inductive current drive in axisymmetric visco-resistive MHD steady-states](https://arxiv.org/abs/2507.20771)
*Anna Krupka,Marie-Christine Firpo*

Main category: physics.plasm-ph

TL;DR: The paper investigates axisymmetric tokamak plasma solutions, improves pressure modeling with non-inductive current drives, and validates the model numerically.


<details>
  <summary>Details</summary>
Motivation: To address the failure of current modeling in producing realistic pressure levels in tokamak plasmas.

Method: Introduces non-inductive current drives (e.g., neutral beam injection) and formulates Poisson's equation for pressure. Validates with numerical simulations.

Result: Enhanced model improves pressure profiles. Non-inductive drives affect velocity moderately but can cause current reversals and non-nested flux surfaces.

Conclusion: The proposed model successfully addresses pressure limitations and reveals complex effects of non-inductive drives.

Abstract: We investigate self-consistent, steady-state axisymmetric solutions of
incompressible tokamak plasma using a visco-resistive magnetohydrodynamic
model. A key contribution of this work is the formulation of Poisson's equation
that governs the pressure profile. Our analysis reveals that the current
modeling fails to produce realistic pressure levels. To overcome this
limitation, we introduce additional non-inductive current drives, akin to those
generated by neutral beam injection or radio frequency heating, modeled as
modifications to the toroidal current. Numerical simulations validate our
enhanced model, showing significant improvements in pressure profile
characteristics. In the cases examined, the effect of these current drives on
the velocity profiles is moderate, except when the non-inductive current drives
induce reversals in the total toroidal current density, leading to non-nested
flux surfaces with internal separatrices.

</details>


### [63] [Enhanced beam transport via space charge mitigation in a multistage accelerator for fusion plasma diagnostics](https://arxiv.org/abs/2507.20948)
*M. Nishiura,K. Nakamura,K. Ueda,A. Shimizu,H. Takubo,M. Kanda,T. Ido,M. Okamura*

Main category: physics.plasm-ph

TL;DR: Optimized voltage allocation in a multistage acceleration system improves high-current negative ion beam transport, enhancing plasma diagnostics and beam efficiency.


<details>
  <summary>Details</summary>
Motivation: Strong space-charge effects degrade transport efficiency of high-current negative ion beams, especially heavy ions like Au-, limiting plasma potential diagnostics.

Method: Introduced an electrostatic lens effect via optimized voltage allocation in a multistage acceleration system, validated by IGUN simulations and LHD-HIBP experiments.

Result: Achieved a 2-3 fold increase in Au-beam current, enabling plasma potential measurements at higher densities (up to $1.75\times 10^{19}$ m$^{-3}$) with better signal-to-noise.

Conclusion: The method provides a compact, effective solution for high-current beam transport, applicable beyond plasma diagnostics to other accelerator systems.

Abstract: Efficient transport of high-current negative ion beams is critical for
accurate plasma potential diagnostics using heavy-ion beam probe (HIBP) systems
in magnetically confined fusion plasmas. However, strong space-charge effects
often degrade transport efficiency, particularly for heavy ions such as Au-. In
this study, we demonstrate a substantial improvement in beam transport by
introducing an electrostatic lens effect through optimized voltage allocation
in a multistage acceleration system. Numerical simulations using IGUN,
supported by experiments with the LHD-HIBP system, show that this approach
effectively suppresses space-charge-induced beam divergence and loss. Without
requiring mechanical modifications to the beamline, the optimized configuration
enables a 2-3 fold increase in Au-beam current injected into the tandem
accelerator. Consequently, plasma potential measurements were extended to
higher-density plasmas, reaching line-averaged electron densities up to
$1.75\times 10^{19}$ m$^{-3}$ with improved signal-to-noise ratio. This
technique offers a compact, practical, and highly effective solution for
transporting high-current heavy-ion beams under space-charge-dominated
conditions. Beyond its impact on plasma diagnostics, the method is broadly
applicable to a wide range of accelerator systems, including those used in
scientific and industrial applications where high-intensity beam transport is
required.

</details>


### [64] [Optimizing Particle Transport for Enhanced Confinement in Quasi-Isodynamic Stellarators](https://arxiv.org/abs/2507.21003)
*A. Bañón Navarro,A. Di Siena,F. Jenko,A. Merlo,E. Laude*

Main category: physics.plasm-ph

TL;DR: A new stellarator design with reduced mirror ratio improves particle confinement and energy confinement by enhancing inward particle flux and suppressing turbulence.


<details>
  <summary>Details</summary>
Motivation: Modern QI stellarators like Stellaris suffer from poor particle confinement, limiting performance.

Method: Gyrokinetic simulations (GENE--Tango) identified suppressed inward thermodiffusion due to unfavorable magnetic geometry. A new configuration with reduced mirror ratio was designed.

Result: The optimized configuration nearly doubles energy confinement compared to Stellaris, with strongly peaked density profiles and suppressed turbulence.

Conclusion: Optimizing particle transport is crucial for next-generation stellarator designs.

Abstract: Despite significant advances in reducing turbulent heat losses, modern
quasi-isodynamic (QI) stellarators -- such as Stellaris -- continue to suffer
from poor particle confinement, which fundamentally limits their overall
performance. Using gyrokinetic simulations within the GENE--Tango framework, we
identify suppressed inward thermodiffusion, caused by unfavorable magnetic
geometry, as the primary cause. To overcome this limitation, we design a new
configuration with a reduced mirror ratio, which enhances the contribution of
passing electrons to the inward particle flux. This facilitates the formation
of strongly peaked density profiles, suppresses turbulence, and leads to a
substantial improvement in confinement. Our optimized configuration achieves
nearly a twofold increase in energy confinement compared to Stellaris,
highlighting the crucial role of optimizing particle transport in
next-generation stellarator designs.

</details>


<div id='nucl-th'></div>

# nucl-th [[Back]](#toc)

### [65] [Nuclear Schiff moment of fluorine isotope $^{19}$F](https://arxiv.org/abs/2507.19811)
*Kia Boon Ng,Stephan Foster,Lan Cheng,Petr Navratil,Stephan Malbrunot-Ettenauer*

Main category: nucl-th

TL;DR: First ab initio calculation of the nuclear Schiff moment (NSM) in $^{19}$F, combined with quantum-chemistry and experimental data to set bounds on NSM.


<details>
  <summary>Details</summary>
Motivation: NSMs probe beyond-Standard-Model physics, specifically time-reversal and parity-inversion symmetry violations in nuclei.

Method: Used the no-core shell model for ab initio NSM calculation in $^{19}$F and quantum-chemistry for HfF$^+$ sensitivity.

Result: First experimental bound on the NSM of $^{19}$F, leveraging high-precision molecular electric dipole moment measurements.

Conclusion: The study provides a novel framework for probing fundamental symmetries using NSMs and experimental data.

Abstract: Nuclear Schiff moments (NSMs) are sensitive probes for physics beyond the
Standard Model of particle physics, signaling violations of time-reversal and
parity-inversion symmetries in atomic nuclei. In this Letter, we report the
first-ever calculation of a NSM in a nuclear ab initio framework, employing the
no-core shell model to study the fluorine isotope $^{19}$F. We further perform
quantum-chemistry calculations to evaluate the sensitivity of the hafnium
monofluoride cation, HfF$^+$, to the NSM of $^{19}$F. Combined with recent
high-precision measurements of the molecular electric dipole moment of HfF$^+$,
our results enable the first experimental bound on the NSM of $^{19}$F.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [66] [A Data-Driven Approach for Predicting Hydrodynamic Forces on Spherical Particles Using Volume Fraction Representations](https://arxiv.org/abs/2507.20767)
*Alexander Metelkin,Sam Jacob Jacob,Bernhard Vowinckel*

Main category: physics.flu-dyn

TL;DR: A data-driven method using volume fractions and FCNN improves hydrodynamic force predictions in particle-laden flows, outperforming models with direct particle position inputs.


<details>
  <summary>Details</summary>
Motivation: Enhance accuracy of Lagrangian point-particle methods by leveraging pr-DNS data for better hydrodynamic force predictions.

Method: Uses volume fractions on an auxiliary grid as input features for FCNN, trained with pr-DNS data of flow through particle arrays.

Result: Improved prediction accuracy and flexibility for systems with varying particle sizes and shapes.

Conclusion: Volume-fraction-based FCNN models offer superior performance and adaptability in simulating particle-laden flows.

Abstract: Particle-laden flows are simulated at various scales using numerical
techniques that range from particle-resolved Direct Numerical Simulations
(pr-DNS) for small-scale systems to Lagrange point-particle methods for
laboratory-scale problems, and Euler-Euler approaches for larger-scale
applications. Recent research has been particularly focused on the development
of both physics-based and data-driven closures to enhance the accuracy of the
Lagrangian point-particle approach by leveraging highly resolved data from
pr-DNS. In this study, a data-driven methodology is presented for the
prediction of hydrodynamic forces acting on spherical particles immersed in an
ambient flow field, where neighboring particle information is represented by
volume fractions. The volume fractions are computed on an auxiliary grid with
cell sizes on the order of the particle diameter. The volume fraction values in
the vicinity of each particle are used as input features for the data-driven
model to predict the corresponding hydrodynamic forces and moments. The
training data was generated by a series of pr-DNS of flow through arrays of
randomly distributed, fixed-position particles at various Reynolds numbers and
particle volume fractions. The data-driven model is built using Fully Connected
Neural Networks (FCNN). Improved prediction accuracy of hydrodynamic forces and
torques is demonstrated in comparison to FCNN models that rely on direct
particle position inputs. In addition, the proposed volume-fraction-based
approach exhibits greater flexibility than previously introduced models by
accommodating systems with particles of different sizes and shapes.

</details>


### [67] [Noise-expansion cascade: an origin of randomness of turbulence](https://arxiv.org/abs/2410.14941)
*Shijun Liao,Shijie Qin*

Main category: physics.flu-dyn

TL;DR: The paper introduces a 'noise-expansion cascade' phenomenon in turbulence, showing how micro-level noises in initial conditions amplify to macro-level, influencing turbulence characteristics and revealing randomness origins.


<details>
  <summary>Details</summary>
Motivation: To explore the origin of randomness in turbulence and understand how micro-level disturbances impact macro-level turbulence characteristics.

Method: Conducted numerical experiments using the Navier-Stokes equations for two-dimensional turbulent Kolmogorov flow, analyzing noise amplification.

Result: Micro-level noises consistently expand to macro-level, significantly altering turbulence statistics and revealing a connection between micro-level randomness and macro-level disorder.

Conclusion: The 'noise-expansion cascade' is a fundamental property of turbulence, emphasizing the need to consider thermal fluctuations in simulations and offering insights into turbulence randomness.

Abstract: Randomness is one of the most important characteristics of turbulence, but
its origin remains an open question. By means of a ``thought experiment'' via
several clean numerical experiments based on the Navier-Stokes equations for
two-dimensional turbulent Kolmogorov flow, we reveal a new phenomenon, which we
call the ``noise-expansion cascade'' whereby all micro-level
noises/disturbances at different orders of magnitudes in the initial condition
of Navier-Stokes equations enlarge consistently, say, one by one like an
inverse cascade, to macro-level. More importantly, each noise/disturbance input
may greatly change the macro-level characteristics and statistics of the
resulting turbulence, clearly indicating that micro-level noise/disturbance
might have great influence on macro-level characteristics and statistics of
turbulence. Besides, the noise-expansion cascade closely connects randomness of
micro-level noise/disturbance and macro-level disorder of turbulence, thus
revealing an origin of randomness of turbulence. This also highly suggests that
unavoidable thermal fluctuations must be considered when simulating turbulence,
even if such fluctuations are several orders of magnitudes smaller than other
external environmental disturbances. Hopefully, the ``noise-expansion cascade''
as a fundamental property of the NS equations could greatly deepen our
understandings about turbulence, and besides is helpful for attacking the
fourth millennium problem posed by Clay Mathematics Institute in 2000.

</details>


### [68] [Clean numerical simulation (CNS) of three-dimensional turbulent Kolmogorov flow](https://arxiv.org/abs/2507.11805)
*Shijie Qin,Shijun Liao*

Main category: physics.flu-dyn

TL;DR: The paper highlights the impact of numerical noise on turbulence simulations, comparing DNS and CNS methods for 3D Kolmogorov flow, revealing significant deviations in DNS results.


<details>
  <summary>Details</summary>
Motivation: To investigate the influence of numerical noise on 3D turbulent flows, given its importance in physics, and to compare DNS with the more accurate CNS method.

Method: The study uses the clean numerical simulation (CNS) method to solve a 3D Kolmogorov turbulent flow, comparing results with traditional DNS.

Result: DNS results are heavily polluted by numerical noise, showing large deviations from CNS benchmarks in flow fields, energy cascades, and statistics.

Conclusion: CNS provides more accurate simulations of 3D turbulence, exposing the limitations of DNS due to numerical noise.

Abstract: Turbulence holds immense importance across various scientific and engineering
disciplines. The direct numerical simulation (DNS) of turbulence proposed by
Orszag in 1970 is a milestone in fluid mechanics, which began an era of
numerical experiment for turbulence. Many researchers have reported that
turbulence should be chaotic, since spatiotemporal trajectories are very
sensitive to small disturbance. In 2006 E.D. Lorenz discovered that numerical
noise might have a great influence on statistic characteristics of chaotic
systems. The above-mentioned facts logically lead to the conclusion that
numerical noises of turbulence (as a chaotic system) might have great influence
on turbulent flows. This is indeed true for a two-dimensional (2D) Kolmogorov
turbulent flow, as currently revealed by a much more accurate algorithm than
DNS, namely the ``clean numerical simulation'' (CNS). Different from DNS, CNS
can greatly reduce both of truncation error and round-off error to any required
small level so that artificial numerical noise can be rigorously negligible
throughout a time interval long enough for calculating statistics. However, In
physics, 3D turbulent flow is more important than 2D turbulence, as pointed out
by Nobel Prize winner T.D. Lee. So, for the first time, we solve a 3D turbulent
Kolmogorov flow by means of CNS in this paper, and compare our CNS result with
that given by DNS in details. It is found that the spatial-temporal
trajectories of the 3D Kolmogorov turbulent flow given by DNS are badly
polluted by artificial numerical noise rather quickly, and the DNS result has
huge deviations from the CNS benchmark solution not only in the flow field and
the energy cascade but also even in statistics.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [69] [Hierarchical clustering and dimensional reduction for optimal control of large-scale agent-based models](https://arxiv.org/abs/2507.19644)
*Angela Monti,Fasma Diele,Dante Kalise*

Main category: math.OC

TL;DR: A scalable control framework for large-scale ABMs using agent clustering and POD-based reduction to improve efficiency in optimal control.


<details>
  <summary>Details</summary>
Motivation: Address computational challenges in controlling high-dimensional ABMs due to increasing agent numbers and state space size.

Method: Integrates agent clustering and Proper Orthogonal Decomposition (POD) into a feedback loop for reduced-order system control.

Result: Significantly improves control efficiency, even in complex scenarios where direct control fails.

Conclusion: Highlights the method's effectiveness and relevance, especially in opinion dynamics models for environmental applications.

Abstract: Agent-based models (ABMs) provide a powerful framework to describe complex
systems composed of interacting entities, capable of producing emergent
collective behaviours such as consensus formation or clustering. However, the
increasing dimensionality of these models -- in terms of both the number of
agents and the size of their state space -- poses significant computational
challenges, particularly in the context of optimal control. In this work, we
propose a scalable control frame work for large-scale ABMs based on a twofold
model order reduction strategy: agent clustering and projection-based reduction
via Proper Orthogonal Decomposition (POD). These techniques are integrated into
a feedback loop that enables the design and application of optimal control laws
over a reduced-order representation of the system. To illustrate the
effectiveness of the approach, we consider the opinion dynamics model, a
prototyp ical first-order ABM where agents interact through state-dependent
influence functions. We show that our method significantly improves control
efficiency, even in scenarios where direct control fails due to model
complexity. Beyond its methodological contributions, this work also highlights
the rel evance of opinion dynamics models in environmental contexts -- for
example, modeling the diffusion of pro-environmental attitudes or
decision-making processes in sustainable policy adoption -- where controlling
consensus formation plays a crucial role.

</details>


### [70] [A general perspective on CBO methods with stochastic rate of information](https://arxiv.org/abs/2507.20029)
*Stefano Almi,Alessandro Baldi,Marco Morandotti,Francesco Solombrino*

Main category: math.OC

TL;DR: The paper analyzes Consensus-Based Optimization (CBO) models with stochastic information rates, proving well-posedness, finite-particle approximation, and mean-field convergence. It shows particles converge to consensus under mild assumptions, requiring only minimal initial knowledge.


<details>
  <summary>Details</summary>
Motivation: To extend CBO models by incorporating stochastic information rates, enabling a more realistic representation of agents' knowledge and energy landscapes.

Method: Theoretical analysis of stochastic CBO models, including well-posedness proofs, finite-particle approximation, and mean-field convergence to a kinetic PDE.

Result: Particles concentrate around consensus under mild conditions, with even minimal initial knowledge ensuring convergence. The framework generalizes existing CBO models.

Conclusion: The study generalizes CBO models, demonstrating robust convergence to consensus with minimal initial knowledge, broadening applicability.

Abstract: This paper studies a class of Consensus-Based Optimization (CBO) models
featuring an additional stochastic rate of information, modeling the agents'
knowledge of the environment and energy landscape. The well-posedness of the
stochastic system is proved, together with its finite-particle approximation
and the mean-field convergence to a kinetic PDE. Particles are shown to
concentrate around the consensus point under mild assumptions on the initial
spatial distribution and initial level of knowledge. In particular, the
analysis unveils that a positive, however small, initial level of knowledge is
enough for convergence to consensus to happen. The framework presented is
general enough to include the first instances of CBO proposed in the
literature.

</details>


### [71] [A direct approach of the existence of the solution to a Riccati equation](https://arxiv.org/abs/2507.20171)
*Gabriela Marinoschi*

Main category: math.OC

TL;DR: The paper explores solving the algebraic Riccati equation for $H^{\infty}$-optimal control using a Hilbert-Schmidt operator approach, proving existence under specific conditions and applying it to a parabolic equation with a Hardy-type potential.


<details>
  <summary>Details</summary>
Motivation: The challenge of solving the algebraic Riccati equation in $H^{\infty}$-optimal control problems motivates the study of its existence and solution methods.

Method: A direct operatorial approach in the Hilbert-Schmidt operator space is used to prove the existence of solutions under assumptions for coercive and non-negative operators.

Result: Existence proofs for the Riccati equation are provided, and a related $H^{\infty}$-optimal control result is derived. An example with a parabolic equation and Hardy potential is given.

Conclusion: The paper successfully demonstrates the existence of solutions to the Riccati equation under certain conditions and applies the findings to a practical control problem.

Abstract: Finding the state feedback control in an $% H^{\infty }$-optimal control
problem involves a challenging approach of the associated algebraic Riccati
equation of the generic form $A^{\ast }P+PA+P\Gamma P=F$. In view of this
objective, we explore first in this paper the existence of the solution to this
algebraic Riccati equation by a direct operatorial approach in the space of
Hilbert-Schmidt operators. The proofs are provided, under certain assumptions
on the operators $\Gamma $ and $F,$ for the cases with $A$ coercive and $A\geq
0,$ respectively. Next, relying on the existence of the solution to the Riccati
equation, we provide a result concerning the associated $H^{\infty }$-optimal
control problem. An example regarding the application of the existence proof
for the solution to the Riccati equation is given for a parabolic equation with
a singular potential of Hardy type.

</details>


### [72] [Stochastic gradient with least-squares control variates](https://arxiv.org/abs/2507.20981)
*Fabio Nobile,Matteo Raviola,Nathan Schaeffer*

Main category: math.OC

TL;DR: A novel variance reduction method for SGD is proposed, suitable for objectives given by expectations over continuous distributions, with theoretical guarantees and empirical validation.


<details>
  <summary>Details</summary>
Motivation: Existing variance reduction techniques like SAGA are limited to finite-sum objectives and degrade with large sums. The paper addresses the need for efficient methods for continuous distributions.

Method: The method constructs a control variate using weighted discrete least-squares to fit a linear model to past gradients, reducing variance while maintaining efficiency.

Result: Theoretical sublinear convergence is proven for strongly convex objectives, and numerical experiments on PDE-constrained optimization validate the method's effectiveness.

Conclusion: The proposed approach outperforms existing methods for continuous distributions, offering both theoretical and practical advantages.

Abstract: The stochastic gradient descent (SGD) method is a widely used approach for
solving stochastic optimization problems, but its convergence is typically
slow. Existing variance reduction techniques, such as SAGA, improve convergence
by leveraging stored gradient information; however, they are restricted to
settings where the objective functional is a finite sum, and their performance
degrades when the number of terms in the sum is large. In this work, we propose
a novel approach which is well suited when the objective is given by an
expectation over random variables with a continuous probability distribution.
Our method constructs a control variate by fitting a linear model to past
gradient evaluations using weighted discrete least-squares, effectively
reducing variance while preserving computational efficiency. We establish
theoretical sublinear convergence guarantees for strongly convex objectives and
demonstrate the method's effectiveness through numerical experiments on random
PDE-constrained optimization problems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [73] [Breaking the Precision Ceiling in Physics-Informed Neural Networks: A Hybrid Fourier-Neural Architecture for Ultra-High Accuracy](https://arxiv.org/abs/2507.20929)
*Wei Shan Lee,Chi Kiu Althina Chau,Kei Chon Sio,Kam Ian Leong*

Main category: cs.LG

TL;DR: A hybrid Fourier-neural architecture breaks the precision ceiling of PINNs for fourth-order PDEs, achieving ultra-low L2 errors and outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Overcome the precision limitations of PINNs in engineering applications by addressing architectural and optimization gaps.

Method: Combines a truncated Fourier series with a deep neural network for residual corrections, using a two-phase optimization strategy (Adam + L-BFGS) and adaptive weight balancing.

Result: Achieves unprecedented L2 error of 1.94×10−7, 17× better than standard PINNs and 15-500× better than traditional methods.

Conclusion: Demonstrates that ultra-precision in scientific computing is achievable through proper design, bridging the gap between machine learning and traditional numerical methods.

Abstract: Physics-informed neural networks (PINNs) have plateaued at errors of
$10^{-3}$-$10^{-4}$ for fourth-order partial differential equations, creating a
perceived precision ceiling that limits their adoption in engineering
applications. We break through this barrier with a hybrid Fourier-neural
architecture for the Euler-Bernoulli beam equation, achieving unprecedented L2
error of $1.94 \times 10^{-7}$-a 17-fold improvement over standard PINNs and
\(15-500\times\) better than traditional numerical methods. Our approach
synergistically combines a truncated Fourier series capturing dominant modal
behavior with a deep neural network providing adaptive residual corrections. A
systematic harmonic optimization study revealed a counter-intuitive discovery:
exactly 10 harmonics yield optimal performance, with accuracy catastrophically
degrading from $10^{-7}$ to $10^{-1}$ beyond this threshold. The two-phase
optimization strategy (Adam followed by L-BFGS) and adaptive weight balancing
enable stable ultra-precision convergence. GPU-accelerated implementation
achieves sub-30-minute training despite fourth-order derivative complexity. By
addressing 12 critical gaps in existing approaches-from architectural rigidity
to optimization landscapes-this work demonstrates that ultra-precision is
achievable through proper design, opening new paradigms for scientific
computing where machine learning can match or exceed traditional numerical
methods.

</details>


### [74] [Computational Advantages of Multi-Grade Deep Learning: Convergence Analysis and Performance Insights](https://arxiv.org/abs/2507.20351)
*Ronglong Fang,Yuesheng Xu*

Main category: cs.LG

TL;DR: MGDL outperforms SGDL in image tasks, showing robustness to learning rate and better training stability due to eigenvalue analysis.


<details>
  <summary>Details</summary>
Motivation: To investigate the computational advantages of MGDL over SGDL in image regression, denoising, and deblurring tasks.

Method: Analyzed gradient descent (GD) convergence and Jacobian matrix eigenvalue distributions for MGDL and SGDL.

Result: MGDL is more robust to learning rate choices and exhibits enhanced training stability compared to SGDL.

Conclusion: MGDL's superior performance is mathematically justified, making it a promising alternative to SGDL.

Abstract: Multi-grade deep learning (MGDL) has been shown to significantly outperform
the standard single-grade deep learning (SGDL) across various applications.
This work aims to investigate the computational advantages of MGDL focusing on
its performance in image regression, denoising, and deblurring tasks, and
comparing it to SGDL. We establish convergence results for the gradient descent
(GD) method applied to these models and provide mathematical insights into
MGDL's improved performance. In particular, we demonstrate that MGDL is more
robust to the choice of learning rate under GD than SGDL. Furthermore, we
analyze the eigenvalue distributions of the Jacobian matrices associated with
the iterative schemes arising from the GD iterations, offering an explanation
for MGDL's enhanced training stability.

</details>


### [75] [Diagonally-Weighted Generalized Method of Moments Estimation for Gaussian Mixture Modeling](https://arxiv.org/abs/2507.20459)
*Liu Zhang,Oscar Mickelin,Sheng Xu,Amit Singer*

Main category: cs.LG

TL;DR: The paper introduces Diagonally-Weighted GMM (DGMM) to address the computational and storage inefficiencies of MM and GMM in high-dimensional data, offering better statistical efficiency and stability.


<details>
  <summary>Details</summary>
Motivation: Traditional moment-based methods (MM and GMM) face computational and storage challenges in high dimensions, especially with higher-order moments. DGMM aims to overcome these bottlenecks.

Method: Proposes DGMM, a diagonally-weighted variant of GMM, and designs an efficient algorithm for parameter estimation in weakly separated heteroscedastic low-rank Gaussian mixtures without explicit tensor computation.

Result: DGMM achieves smaller estimation errors and significantly shorter runtime compared to MM and GMM in numerical studies.

Conclusion: DGMM provides a practical and efficient alternative to MM and GMM for high-dimensional data, balancing statistical efficiency, computational complexity, and stability.

Abstract: Since Pearson [Philosophical Transactions of the Royal Society of London. A,
185 (1894), pp. 71-110] first applied the method of moments (MM) for modeling
data as a mixture of one-dimensional Gaussians, moment-based estimation methods
have proliferated. Among these methods, the generalized method of moments (GMM)
improves the statistical efficiency of MM by weighting the moments
appropriately. However, the computational complexity and storage complexity of
MM and GMM grow exponentially with the dimension, making these methods
impractical for high-dimensional data or when higher-order moments are
required. Such computational bottlenecks are more severe in GMM since it
additionally requires estimating a large weighting matrix. To overcome these
bottlenecks, we propose the diagonally-weighted GMM (DGMM), which achieves a
balance among statistical efficiency, computational complexity, and numerical
stability. We apply DGMM to study the parameter estimation problem for weakly
separated heteroscedastic low-rank Gaussian mixtures and design a
computationally efficient and numerically stable algorithm that obtains the
DGMM estimator without explicitly computing or storing the moment tensors. We
implement the proposed algorithm and empirically validate the advantages of
DGMM: in numerical studies, DGMM attains smaller estimation errors while
requiring substantially shorter runtime than MM and GMM. The code and data will
be available upon publication at https://github.com/liu-lzhang/dgmm.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [76] [Data-parallel leading-order event generation in MadGraph5_aMC@NLO](https://arxiv.org/abs/2507.21039)
*Stephan Hageböck,Daniele Massaro,Olivier Mattelaer,Stefan Roiser,Andrea Valassi,Zenny Wettersten*

Main category: hep-ph

TL;DR: The CUDACPP plugin accelerates tree-level event generation in MadGraph5_aMC@NLO using SIMD and GPU offloading, achieving significant speed-ups.


<details>
  <summary>Details</summary>
Motivation: To enhance the efficiency of leading-order tree-level event generation by leveraging data-parallel helicity amplitudes for CPUs and GPUs.

Method: Uses templated C++ and CUDA to compile for CPUs (SSE4, AVX2, AVX-512) and GPUs (CUDA/HIP), exploiting SIMD and GPU parallelism.

Result: Linear speed-up with SIMD register size, GPU offloading outperforms SIMD, and high-multiplicity QCD processes see 10x speed-up.

Conclusion: CUDACPP effectively accelerates event generation, aligning with predictions and optimizing performance for both CPUs and GPUs.

Abstract: The CUDACPP plugin for MadGraph5_aMC@NLO aims to accelerate leading order
tree-level event generation by providing the MadEvent event generator with
data-parallel helicity amplitudes. These amplitudes are written in templated
C++ and CUDA, allowing them to be compiled for CPUs supporting SSE4, AVX2, and
AVX-512 instruction sets as well as CUDA- and HIP-enabled GPUs. Using SIMD
instruction sets, CUDACPP-generated amplitudes are shown to speed up linearly
with SIMD register size, and GPU offloading is shown to provide acceleration
beyond that of SIMD instructions. Additionally, the resulting speed-up in event
generation perfectly aligns with predictions from measured runtime fractions
spent in amplitude routines, and proper GPU utilisation can speed up
high-multiplicity QCD processes by an order of magnitude when compared to
optimal CPU usage in server-grade CPUs.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [77] [Methodology for intelligent injection point location based on geometric algorithms and discrete topologies for virtual digital twin environments](https://arxiv.org/abs/2507.20922)
*J. Mercado Colmenero,A. Torres Alba,C. Martin Donate*

Main category: cs.GR

TL;DR: A novel method for locating injection points in molded parts using intelligent models and geometric algorithms, validated by simulations, reduces expert dependency and costs.


<details>
  <summary>Details</summary>
Motivation: To automate and optimize the injection point location process, ensuring uniform plastic distribution and reducing manual design efforts.

Method: Uses geometric algorithms: one calculates the center of mass, and two sub-algorithms evaluate geometry and optimal injection point location via nodal quadrature and orthogonal projection.

Result: Validated in six case studies, showing uniform plastic distribution, minimal pressure loss, and no need for expert intervention.

Conclusion: The method surpasses current standards, reduces costs, and is adaptable to various design environments, enhancing digital twin applications.

Abstract: This article presents an innovative methodology for locating injection points
in injection-molded parts using intelligent models with geometric algorithms
for discrete topologies. The first algorithm calculates the center of mass of
the discrete model based on the center of mass of each triangular facet in the
system, ensuring uniform molten plastic distribution during mold cavity
filling. Two sub-algorithms intelligently evaluate the geometry and optimal
injection point location. The first sub-algorithm generates a geometric matrix
based on a two-dimensional nodal quadrature adapted to the part's bounding box.
The second sub-algorithm projects the nodal matrix and associated circular
areas orthogonally on the part's surface along the demolding direction. The
optimal injection point location is determined by minimizing the distance to
the center of mass from the first algorithm's result. This novel methodology
has been validated through rheological simulations in six case studies with
complex geometries. The results demonstrate uniform and homogeneous molten
plastic distribution with minimal pressure loss during the filling phase.
Importantly, this methodology does not require expert intervention, reducing
time and costs associated with manual injection mold feed system design. It is
also adaptable to various design environments and virtual twin systems, not
tied to specific CAD software. The validated results surpass the state of the
art, offering an agile alternative for digital twin applications in new product
design environments, reducing dependence on experts, facilitating designer
training, and ultimately cutting costs

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [78] [Terahertz frequency conversion at plasma-induced time boundary](https://arxiv.org/abs/2507.20663)
*Yindong Huang,Bin Zhou,Aijun Xuan,Mingxin Gao,Jing Lou,Xiaomin Qu,Zengxiu Zhao,Ce Shang,Xuchen Wang,Chao Chang,Viktar Asadchy*

Main category: physics.optics

TL;DR: Study explores THz wave frequency shifts via ultrafast laser-induced air-to-plasma transitions, revealing spectral engineering advantages over spatial boundaries.


<details>
  <summary>Details</summary>
Motivation: To understand light-matter interactions in time-varying media and explore applications in THz technology.

Method: Combined experimental and theoretical analysis of THz wave frequency shifts at ultrafast time boundaries.

Result: Abrupt refractive index changes cause broadband red/blue shifts, with distinctive amplitude variations, showing temporal manipulations' superiority.

Conclusion: Findings advance THz technology and enable studies of temporal reflection, photonic time crystals, and modulated matter.

Abstract: We report on the frequency conversions of terahertz (THz) waves at ultrafast
time boundaries created via femtosecond laser-induced air-to-plasma phase
transitions. Our combined experimental and theoretical approach reveals that
the abrupt change in refractive index at the ultrafast time boundaries drives
both the red and blue shifts over the broadband THz spectrum due to the
dispersive plasma, with distinctive amplitude variations. The present study
contrasts these effects with those from spatial boundaries, highlighting the
superior efficacy of temporal manipulations for spectral engineering. These
findings not only deepen the understanding of light-matter interactions in
time-varying media but also pave the way for innovative applications in THz
technology and lay the groundwork for the observation of temporal reflection
effects, photonic time crystals, and spatio-temporally modulated matter.

</details>


### [79] [Dissipative Coupling in Photonic and Plasmonic Resonators](https://arxiv.org/abs/2507.20132)
*Tong Wu,Philippe Lalanne*

Main category: physics.optics

TL;DR: A coupled-quasinormal-mode (cQNM) framework is introduced for analyzing dissipative coupling in photonic and plasmonic resonators, offering precise predictions and novel features beyond classical CMT.


<details>
  <summary>Details</summary>
Motivation: The need for theoretical frameworks to predict resonant behavior in complex nanophotonic systems beyond the limitations of classical temporal coupled-mode theory (CMT).

Method: Development of a cQNM framework with closed-form expressions for dissipative coupling coefficients and a new coupling scheme via time derivatives of excitation coefficients.

Result: Accurate predictions of exotic phenomena (e.g., zero-coupling between close cavities, level-attraction effects) and facilitation of rapid parameter space exploration.

Conclusion: The cQNM framework is efficient, user-friendly, and extendable to nonlinear and quantum systems, enhancing device design capabilities.

Abstract: The rapid progress of nanophotonics demands theoretical frameworks capable of
predicting the resonant behavior of complex systems comprising constituents of
varying nature, operating beyond the weak-coupling, high-Q regime where
classical temporal coupled-mode theory (CMT) is applicable. This work presents
a coupled-quasinormal-mode (cQNM) framework for analyzing dissipative coupling
with photonic and plasmonic resonators. The framework provides rigorous
closed-form expressions for dissipative coupling coefficients and introduces
novel features, such as a new coupling scheme via time derivatives of
excitation coefficients. It delivers transparent and accurate predictions of
exotic phenomena-such as zero-coupling between very close cavities and
level-attraction effects that are only vaguely captured by traditional CMT
models. Efficient and user-friendly, this framework facilitates rapid parameter
space exploration for device design and offers potential for extension to
nonlinear and quantum systems in future applications.

</details>


<div id='math.SP'></div>

# math.SP [[Back]](#toc)

### [80] [Improved Berezin-Li-Yau inequality and Kröger inequality and consequences](https://arxiv.org/abs/2507.20330)
*Zaihui Gan,Renjin Jiang,Fanghua Lin*

Main category: math.SP

TL;DR: The paper improves the Berezin-Li-Yau and Kröger inequalities in ℝⁿ (n≥2), resolving an open question from 2006. It shows infinite Dirichlet eigenvalues satisfy Pólya's conjecture for n≥3 and infinite Neumann eigenvalues for n≥5 with discrete spectrum.


<details>
  <summary>Details</summary>
Motivation: To advance understanding of spectral inequalities and resolve Weidl's 2006 open question about Kröger's inequality.

Method: Quantitative improvements to the Berezin-Li-Yau and Kröger inequalities in ℝⁿ.

Result: Proves infinite Dirichlet eigenvalues meet Pólya's conjecture for n≥3 and infinite Neumann eigenvalues for n≥5 with discrete spectrum.

Conclusion: The work resolves a long-standing question and extends Pólya's conjecture to more cases, contributing to spectral theory.

Abstract: We provide quantitative improvements to the Berezin-Li-Yau inequality and the
Kr\"oger inequality, in $\mathbb{R}^n$, $n\ge 2$. The improvement on Kr\"oger's
inequality resolves an open question raised by Weidl from 2006. The
improvements allow us to show that, for any open bounded domains, there are
infinite many Dirichlet eigenvalues satisfying P\'olya's conjecture if $n\ge
3$, and infinite many Neumann eigenvalues satisfying P\'olya's conjecture if
$n\ge 5$ and the Neumann spectrum is discrete.

</details>


<div id='math.FA'></div>

# math.FA [[Back]](#toc)

### [81] [Matrices with invariant by rotation numerical ranges](https://arxiv.org/abs/2507.20631)
*Michel Crouzeix*

Main category: math.FA

TL;DR: The paper characterizes d x d matrices whose numerical ranges remain unchanged under rotations of angle 2π/d.


<details>
  <summary>Details</summary>
Motivation: To understand the properties of matrices that exhibit rotational symmetry in their numerical ranges.

Method: Analyzes the invariance of numerical ranges under specific rotational transformations.

Result: Identifies the specific class of d x d matrices with this rotational invariance property.

Conclusion: Provides a clear characterization of matrices with rotationally invariant numerical ranges, contributing to matrix theory.

Abstract: We characterize the d x d matrices whose numerical ranges are invariant by
rotations of angle 2$\pi$/d.

</details>


### [82] [Optimal decay of semi-uniformly stable operator semigroups with empty spectrum](https://arxiv.org/abs/2507.20376)
*Morgan Callewaert,Lenny Neyt,Jasson Vindas*

Main category: math.FA

TL;DR: The paper demonstrates that the decay rate of a semi-uniformly stable operator semigroup cannot be determined solely from its spectrum, using a constructed example where the generator has an empty spectrum but exhibits non-uniform decay.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of spectral analysis in predicting the decay behavior of operator semigroups, highlighting cases where spectral information is insufficient.

Method: Constructs a Banach space and a bounded semigroup with an infinitesimal generator having an empty spectrum, then analyzes the decay behavior of the semigroup.

Result: Shows that even with an empty spectrum, the semigroup can exhibit non-uniform decay, disproving the sufficiency of spectral data for decay rate quantification.

Conclusion: Spectral information alone is inadequate for determining the decay rate of semi-uniformly stable operator semigroups, necessitating additional analysis.

Abstract: We show that it is impossible to quantify the decay rate of a semi-uniformly
stable operator semigroup based on sole knowledge of its spectrum. More
precisely, given an arbitrary positive function $r$ vanishing at $\infty$, we
construct a Banach space $X$ and a bounded semigroup $ (T(t))_{t \geq 0}$ of
operators on it whose infinitesimal generator $A$ has empty spectrum
$\sigma(A)=\varnothing$, but for which, for some $x \in X$,
$$\limsup_{t\to\infty} \frac{\|T(t)A^{-1}x\|_{X}}{r(t)}=\infty.$$

</details>


### [83] [Sampling and entropy numbers in the uniform norm](https://arxiv.org/abs/2507.20770)
*Mario Ullrich*

Main category: math.FA

TL;DR: A sharp bound between sampling and entropy numbers for convex sets of bounded functions is proven.


<details>
  <summary>Details</summary>
Motivation: To establish a precise relationship between sampling and entropy numbers in the uniform norm for general convex sets of bounded functions.

Method: Theoretical proof involving convex analysis and functional analysis techniques.

Result: A sharp bound is derived, clarifying the relationship between sampling and entropy numbers.

Conclusion: The study provides a rigorous theoretical foundation for understanding sampling and entropy bounds in functional analysis.

Abstract: We prove a sharp bound between sampling numbers and entropy numbers in the
uniform norm for general convex sets of bounded functions.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [84] [Quantum-Efficient Convolution through Sparse Matrix Encoding and Low-Depth Inner Product Circuits](https://arxiv.org/abs/2507.19658)
*Mohammad Rasoul Roshanshah,Payman Kazemikhah,Hossein Aghababa*

Main category: quant-ph

TL;DR: A resource-efficient quantum algorithm for convolution operations using sparse reshaping and optimized encoding, reducing costs and enabling scalable quantum-enhanced feature extraction.


<details>
  <summary>Details</summary>
Motivation: Extending convolution operations into the quantum domain is challenging due to inefficient data encoding and high circuit complexity. This work aims to address these issues.

Method: Reformulates convolution as structured matrix multiplication using sparse reshaping, optimized QRAM state encoding, and low-depth SWAP test circuits for inner product estimation.

Result: The method reduces redundant preparation costs, scales logarithmically with input size under sparsity, and integrates into hybrid quantum-classical pipelines.

Conclusion: Provides a scalable pathway for quantum-enhanced feature extraction, advancing quantum convolutional neural networks and data-driven quantum inference.

Abstract: Convolution operations are foundational to classical image processing and
modern deep learning architectures, yet their extension into the quantum domain
has remained algorithmically and physically costly due to inefficient data
encoding and prohibitive circuit complexity. In this work, we present a
resource-efficient quantum algorithm that reformulates the convolution product
as a structured matrix multiplication via a novel sparse reshaping formalism.
Leveraging the observation that localized convolutions can be encoded as doubly
block-Toeplitz matrix multiplications, we construct a quantum framework wherein
sparse input patches are prepared using optimized key-value QRAM state
encoding, while convolutional filters are represented as quantum states in
superposition. The convolution outputs are computed through inner product
estimation using a low-depth SWAP test circuit, which yields probabilistic
amplitude information with reduced sampling overhead. Our architecture supports
batched convolution across multiple filters using a generalized SWAP circuit.
Compared to prior quantum convolutional approaches, our method eliminates
redundant preparation costs, scales logarithmically with input size under
sparsity, and enables direct integration into hybrid quantum-classical machine
learning pipelines. This work provides a scalable and physically realizable
pathway toward quantum-enhanced feature extraction, opening up new
possibilities for quantum convolutional neural networks and data-driven quantum
inference.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [85] [Active Learning for Predicting the Enthalpy of Mixing inBinary Liquids Based on Ab Initio Molecular Dynamics](https://arxiv.org/abs/2507.20885)
*Quentin Bizot,Ryo Tamura,Guillaume Deffrennes*

Main category: cond-mat.mtrl-sci

TL;DR: An active learning approach identifies gaps in data for predicting the enthalpy of mixing in liquid alloys, focusing on refractory elements, and uses ab initio simulations to improve predictions.


<details>
  <summary>Details</summary>
Motivation: Improving predictions of the enthalpy of mixing in liquid alloys by addressing data gaps, especially for refractory elements, to enhance machine learning models.

Method: Active learning identifies data needs; ab initio molecular dynamics simulations for 29 equimolar alloys of Ir, Os, Re, and W are performed.

Result: More accurate predictions of enthalpy of mixing, with trends for refractory elements of period 6 analyzed using clustering.

Conclusion: The study highlights the importance of targeted data acquisition for refractory elements and links findings to Miedema's semi-empirical theory.

Abstract: The enthalpy of mixing in the liquid phase is an important property for
predicting phase formation in alloys. It can be estimated in a large
compositional space from pair wise interactions between elements, for which
machine learning has recently provided the most accurate predictions. Further
improvements requires acquiring high quality data in liquids where models are
poorly constrained. In this study, we propose an active learning approach to
identify in which liquids additional data are most needed to improve an initial
dataset that covers over 400 binary liquids. We identify a critical need for
new data on liquids containing refractory elements, which we address by
performing ab initio molecular dynamics simulations for 29 equimolar alloys of
Ir, Os, Re and W. This enables more accurate predictions of the enthalpy of
mixing, and we discuss the trends obtained for refractory elements of period 6.
We use clustering analysis to interpret the results of active learning and to
explore how our features can be linked to Miedema's semi empirical theory.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [86] [Operator Inference Aware Quadratic Manifolds with Isotropic Reduced Coordinates for Nonintrusive Model Reduction](https://arxiv.org/abs/2507.20463)
*Paul Schwerdtner,Prakash Mohan,Julie Bessac,Marc T. Henry de Frahan,Benjamin Peherstorfer*

Main category: math.DS

TL;DR: A greedy training procedure for quadratic manifolds improves reduced model accuracy by considering both reconstruction and prediction errors, outperforming methods focusing only on reconstruction.


<details>
  <summary>Details</summary>
Motivation: Existing methods minimize reconstruction error but ignore downstream model accuracy, leading to suboptimal embeddings.

Method: Proposes a greedy training procedure that balances reconstruction error and reduced model prediction error.

Result: Achieves up to two orders of magnitude higher accuracy in reduced models compared to traditional methods.

Conclusion: The greedy approach yields smoother embeddings and significantly improves downstream model performance.

Abstract: Quadratic manifolds for nonintrusive reduced modeling are typically trained
to minimize the reconstruction error on snapshot data, which means that the
error of models fitted to the embedded data in downstream learning steps is
ignored. In contrast, we propose a greedy training procedure that takes into
account both the reconstruction error on the snapshot data and the prediction
error of reduced models fitted to the data. Because our procedure learns
quadratic manifolds with the objective of achieving accurate reduced models, it
avoids oscillatory and other non-smooth embeddings that can hinder learning
accurate reduced models. Numerical experiments on transport and turbulent flow
problems show that quadratic manifolds trained with the proposed greedy
approach lead to reduced models with up to two orders of magnitude higher
accuracy than quadratic manifolds trained with respect to the reconstruction
error alone.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [87] [Adaptive Bayesian Data-Driven Design of Reliable Solder Joints for Micro-electronic Devices](https://arxiv.org/abs/2507.19663)
*Leo Guo,Adwait Inamdar,Willem D. van Driel,GuoQi Zhang*

Main category: stat.ML

TL;DR: The paper proposes an adaptive Bayesian optimization (BO) framework for solder joint reliability, outperforming regular BO by 3% and saving computational costs.


<details>
  <summary>Details</summary>
Motivation: Addressing the computational expense of simulating solder joint reliability under thermomechanical loading by leveraging efficient data-driven methods.

Method: Uses adaptive BO with Gaussian process regression, employing multiple acquisition functions and surrogate modeling for computational savings.

Result: Adaptive BO outperforms regular BO by 3% on average and reduces computational expenses by half in minimizing creep strain under thermal load.

Conclusion: The adaptive BO framework is efficient and cost-effective, with open-source implementations provided for reproducibility.

Abstract: Solder joint reliability related to failures due to thermomechanical loading
is a critically important yet physically complex engineering problem. As a
result, simulated behavior is oftentimes computationally expensive. In an
increasingly data-driven world, the usage of efficient data-driven design
schemes is a popular choice. Among them, Bayesian optimization (BO) with
Gaussian process regression is one of the most important representatives. The
authors argue that computational savings can be obtained from exploiting
thorough surrogate modeling and selecting a design candidate based on multiple
acquisition functions. This is feasible due to the relatively low computational
cost, compared to the expensive simulation objective. This paper addresses the
shortcomings in the adjacent literature by providing and implementing a novel
heuristic framework to perform BO with adaptive hyperparameters across the
various optimization iterations. Adaptive BO is subsequently compared to
regular BO when faced with synthetic objective minimization problems. The
results show the efficiency of adaptive BO when compared any worst-performing
regular Bayesian schemes. As an engineering use case, the solder joint
reliability problem is tackled by minimizing the accumulated non-linear creep
strain under a cyclic thermal load. Results show that adaptive BO outperforms
regular BO by 3% on average at any given computational budget threshold,
critically saving half of the computational expense budget. This practical
result underlines the methodological potential of the adaptive Bayesian
data-driven methodology to achieve better results and cut optimization-related
expenses. Lastly, in order to promote the reproducibility of the results, the
data-driven implementations are made available on an open-source basis.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [88] [A global Lipschitz stability perspective for understanding approximate approaches in Bayesian sequential learning](https://arxiv.org/abs/2507.20379)
*Liliang Wang,Alex A. Gorodetsky*

Main category: math.ST

TL;DR: A non-asymptotic error analysis framework for Bayesian sequential learning (BSL) is introduced, bounding learning errors using total variation, Hellinger, and Wasserstein distances. It establishes global Lipschitz stability and provides two sets of error bounds.


<details>
  <summary>Details</summary>
Motivation: To understand the impact of incremental approximations in BSL on long-term inference performance, covering inverse problems, state estimation, and parameter-state estimation.

Method: The framework bounds the learning error between true and approximate posteriors using three distribution metrics and establishes global Lipschitz stability.

Result: Two sets of upper bounds on learning error are provided, showing stability and practical estimability. Conditions for learning error decay are identified.

Conclusion: This is the first work to establish global Lipschitz stability and a general error analysis framework for approximate BSL methods, with potential for practical applications.

Abstract: We establish a general, non-asymptotic error analysis framework for
understanding the effects of incremental approximations made by practical
approaches for Bayesian sequential learning (BSL) on their long-term inference
performance. Our setting covers inverse problems, state estimation, and
parameter-state estimation. In these settings, we bound the difference-termed
the learning error-between the unknown true posterior and the approximate
posterior computed by these approaches, using three widely used distribution
metrics: total variation, Hellinger, and Wasserstein distances. This framework
builds on our establishment of the global Lipschitz stability of the posterior
with respect to the prior across these settings. To the best of our knowledge,
this is the first work to establish such global Lipschitz stability under the
Hellinger and Wasserstein distances and the first general error analysis
framework for approximate BSL methods.
  Our framework offers two sets of upper bounds on the learning error. The
first set demonstrates the stability of general approximate BSL methods with
respect to the incremental approximation process, while the second set is
estimable in many practical scenarios.
  Furthermore, as an initial step toward understanding the phenomenon of
learning error decay, which is sometimes observed, we identify sufficient
conditions under which data assimilation leads to learning error reduction.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [89] [An Optimal Transport-Based Method for Computing LM Rate and Its Convergence Analysis](https://arxiv.org/abs/2507.20129)
*Shitong Wu,Wenhao Ye,Xinwei Li,Lingyi Chen,Wenyi Zhang,Huihui Wu,Hao Wu*

Main category: cs.IT

TL;DR: The paper introduces a novel algorithm for efficiently computing the LM rate, a tighter lower bound on mismatch capacity, by reformulating it as an optimal transport problem and using the Sinkhorn algorithm.


<details>
  <summary>Details</summary>
Motivation: The LM rate is a critical performance indicator but computationally intensive to calculate, especially with large input alphabets, making standard methods impractical.

Method: Reformulate the LM rate computation as an optimal transport problem with constraints and develop a Sinkhorn-based algorithm.

Result: The proposed algorithm shows sub-linear convergence and is feasible and efficient in numerical experiments.

Conclusion: The new method provides a practical solution for computing the LM rate, overcoming computational challenges of traditional approaches.

Abstract: The mismatch capacity characterizes the highest information rate of the
channel under a prescribed decoding metric and serves as a critical performance
indicator in numerous practical communication scenarios. Compared to the
commonly used Generalized Mutual Information (GMI), the Lower bound on the
Mismatch capacity (LM rate) generally provides a tighter lower bound on the
mismatch capacity. However, the efficient computation of the LM rate is
significantly more challenging than that of the GMI, particularly as the size
of the channel input alphabet increases. This growth in complexity renders
standard numerical methods (e.g., interior point methods) computationally
intensive and, in some cases, impractical. In this work, we reformulate the
computation of the LM rate as a special instance of the optimal transport (OT)
problem with an additional constraint. Building on this formulation, we develop
a novel numerical algorithm based on the Sinkhorn algorithm, which is well
known for its efficiency in solving entropy regularized optimization problems.
We further provide the convergence analysis of the proposed algorithm,
revealing that the algorithm has a sub-linear convergence rate. Numerical
experiments demonstrate the feasibility and efficiency of the proposed
algorithm for the computation of the LM rate.

</details>


<div id='physics.app-ph'></div>

# physics.app-ph [[Back]](#toc)

### [90] [Visualizing the Link Between Nanomorphology and Energetic Disorder in 3D Organic Solar Cells](https://arxiv.org/abs/2507.20040)
*Pelin Çiloğlu,Carmen Tretmans,Carsten Deibel,Jan-F. Pietschmann,Martin Stoll,Roderick C. I. MacKenzie*

Main category: physics.app-ph

TL;DR: A 3D hybrid model is developed to study BHJ solar cells, revealing that energetic disorder limits performance by suppressing charge extraction, emphasizing the need for nanoscale optimization alongside mesoscale control.


<details>
  <summary>Details</summary>
Motivation: Current models for BHJ solar cells either oversimplify or are computationally intensive, lacking a balance between accuracy and efficiency.

Method: A 3D hybrid model combines phase-field morphologies with energetic disorder effects, analyzing carrier densities and disorder interplay.

Result: Energetic disorder suppresses charge extraction even with intact percolation pathways, highlighting nanoscale packing's importance.

Conclusion: Multiscale design strategies are essential for optimizing next-generation BHJ devices, addressing both nanoscale and mesoscale factors.

Abstract: The performance of organic bulk heterojunction (BHJ) solar cells is highly
sensitive to both nanomorphology and energetic disorder arising from
microscopic molecular packing and structural defects. However, most models used
to understand these devices are either one-dimensional effective medium
approximations that neglect spatial and energetic disorder or three-dimensional
Monte Carlo simulations that are computationally intensive. In this work, we
present the results from a three-dimensional hybrid model capable of operating
at both high carrier densities and incorporating the effects of energetic
disorder. We first generate realistic morphologies using a phase-field approach
that accounts for solvent evaporation during film formation. Using these
example morphologies, we systematically study the interplay between energetic
disorder and configurational disorder at carrier densities representative of
real device operation. This enables us to separate and visualize the impact of
the nanomorphology and energetic disorder on device performance. Our results
reveal that, even when macroscopic percolation pathways remain intact,
energetic disorder limits performance primarily through suppressed charge
extraction in interconnected domains. This suggest that optimizing molecular
packing at the nanoscale is as critical as controlling phase separation at the
mesoscale, highlighting the need for multiscale design strategies in
next-generation BHJ devices.

</details>


<div id='math.CA'></div>

# math.CA [[Back]](#toc)

### [91] [Potential Theory and the Boundary of Combinatorial Graphs](https://arxiv.org/abs/2507.20833)
*Stefan Steinerberger*

Main category: math.CA

TL;DR: The paper explores boundary concepts in finite, connected graphs, proving discrete analogues of classical potential theory results like Pólya's theorem, Faber-Krahn inequality, and Hardy inequality.


<details>
  <summary>Details</summary>
Motivation: To extend classical potential theory results from compact domains in Euclidean space to finite, connected graphs by defining and analyzing a well-behaved boundary.

Method: Defines a boundary for graphs and proves discrete versions of classical results, including random walk behavior, inequalities, and measure-theoretic properties.

Result: Establishes analogues of Pólya's theorem, Faber-Krahn, Hardy, and other inequalities, along with stability and measure support results for graph boundaries.

Conclusion: The graph boundary behaves analogously to classical boundaries, enabling the transfer of potential theory insights to discrete settings.

Abstract: Let $G=(V,E)$ be a finite, connected graph. We investigate a notion of
boundary $\partial G \subseteq V$ and argue that it is well behaved from the
point of view of potential theory. This is done by proving a number of discrete
analogous of classical results for compact domains $\Omega \subset
\mathbb{R}^d$. These include (1) an analogue of P\'olya's result that a random
walk in $\Omega$ typically hits the boundary $\partial \Omega$ within $\lesssim
\mbox{diam}(\Omega)^2$ units of time, (2) an analogue of the Faber-Krahn
inequality, (3) an analogue of the Hardy inequality, (4) an analogue of the
Alexandrov-Bakelman-Pucci estimate, (5) a stability estimate for hot spots and
(6) a Theorem of Bj\"orck stating that probability measures $\mu$ that maximize
$\int_{\Omega \times \Omega} \|x-y\|^{\alpha} d\mu(x) d\mu(y)$ are fully
supported in the boundary.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [92] [Geometric reflective boundary conditions for asymptotically Anti-de Sitter spaces](https://arxiv.org/abs/2507.20661)
*Ludovic Souêtre*

Main category: gr-qc

TL;DR: The paper solves the initial boundary value problem for the 4D vacuum Einstein equations with a negative cosmological constant, introducing new geometric reflective boundary conditions. It extends Friedrich's work and provides local existence, uniqueness, and smoothness conditions for the solutions.


<details>
  <summary>Details</summary>
Motivation: To address the initial boundary value problem for asymptotically Anti-de Sitter spaces by introducing a new family of boundary conditions that generalize existing ones (Neumann and Dirichlet) and provide deeper geometric insights.

Method: Uses Friedrich's framework and extended conformal Einstein equations, rewritten in tensorial formalism. Introduces geometric boundary conditions derived from gauge-dependent ones via an auxiliary system.

Result: Proves local existence and uniqueness for the new boundary conditions, along with smoothness conditions for unphysical fields. Provides examples of asymptotically Anti-de Sitter spaces and their boundary data.

Conclusion: The new boundary conditions unify and extend previous work, offering a broader geometric perspective and practical insights into boundary stress-energy tensors.

Abstract: This article solves the initial boundary value problem for the vacuum
Einstein equations with a negative cosmological constant in dimension 4, giving
rise to asymptotically Anti-de Sitter spaces. We introduce a new family of
geometric reflective boundary conditions, which can be regarded as the
homogeneous Robin boundary conditions, involving both the conformal class and
the stress-energy tensor of the timelike conformal boundary. This family
includes as a special case the homogeneous Neumann boundary condition,
consisting of setting the boundary stress-energy tensor to zero. It also
agrees, in a limit case, with the homogeneous Dirichlet boundary condition,
where one fixes a locally conformally flat conformal class on the boundary,
already covered in Friedrich's pioneering work of 1995.
  The proof of local existence and uniqueness for this family of boundary
conditions relies notably on Friedrich's framework and his extended conformal
Einstein equations. These are rewritten in a tensorial formalism, rather than a
spinorial one, as it facilitates the comparison with the Fefferman-Graham
expansion of asymptotically Anti-de Sitter metrics. Similarly to Friedrich's
proof, our geometric boundary conditions are eventually inferred from
gauge-dependent boundary conditions by means of an auxiliary system on the
conformal boundary.
  In addition, our analysis also comprises new necessary and sufficient
conditions for the unphysical fields associated to the initial data to be
smooth up to the conformal boundary. Finally, the paper contains examples of
asymptotically Anti-de Sitter spaces, with a focus on their conformal boundary
data. This provides valuable insight into the possible shapes of boundary
stress-energy tensors.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [93] [Solving the MIT Inverse Problem by Considering Skin and Proximity Effects in Coils](https://arxiv.org/abs/2507.20004)
*Hassan Yazdanian,Reza Jafari,Hamid Abrishami Moghaddam*

Main category: physics.med-ph

TL;DR: The paper introduces an improved MIT inverse problem technique by accounting for skin and proximity effects in coils, enhancing accuracy for low-conductivity reconstructions.


<details>
  <summary>Details</summary>
Motivation: Existing MIT methods ignore skin and proximity effects, leading to inaccuracies in low-conductivity reconstructions.

Method: Incorporates skin and proximity effects in the forward method, uses a regularized Gauss-Newton algorithm, and computes a new Jacobian matrix.

Result: The improved method outperforms earlier techniques, proving essential for accurate absolute conductivity reconstruction.

Conclusion: Considering skin and proximity effects significantly improves MIT reconstruction accuracy, especially for low-conductivity applications.

Abstract: This paper presents an improved technique for solving the inverse problem in
magnetic induction tomography (MIT) by considering skin and proximity effects
in coils. MIT is a non-contact, noninvasive, and low-cost imaging modality for
obtaining the distribution of conductivity inside an object. Reconstruction of
low conductivity distribution by MIT requires more accurate techniques since
measured signals are inherently weak and the reconstruction problem is highly
nonlinear and ill-posed. Previous MIT inverse problem studies have ignored skin
and proximity effects inside coils in the forward method. In this article, the
improved technique incorporates these effects in the forward method.
Furthermore, it employs the regularized Gauss-Newton algorithm to reconstruct
the conductivity distribution. The regularization parameter is obtained by an
adaptive method using the two input parameters: a coefficient and an initial
conductivity distribution. The new Jacobian matrix is computed based on a
standard technique. To compare the early and improved forward methods in
possible medical and industrial applications with low conductivity regions, a
2D 8-coil MIT system is modeled, and image reconstruction is performed for
synthetic phantoms. Results show that it is crucial to use the improved forward
method for the reconstruction of the absolute conductivity values.

</details>
