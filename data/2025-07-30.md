<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 10]
- [math.AP](#math.AP) [Total: 11]
- [physics.comp-ph](#physics.comp-ph) [Total: 4]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 1]
- [gr-qc](#gr-qc) [Total: 1]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [math.CV](#math.CV) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.DC](#cs.DC) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 2]
- [physics.data-an](#physics.data-an) [Total: 1]
- [math.PR](#math.PR) [Total: 2]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [math.CA](#math.CA) [Total: 1]
- [math.OC](#math.OC) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Numerical PDE solvers outperform neural PDE solvers](https://arxiv.org/abs/2507.21269)
*Patrick Chatain,Michael Rizvi-Martel,Guillaume Rabusseau,Adam Oberman*

Main category: math.NA

TL;DR: DeepFDM is a differentiable finite-difference framework for learning spatially varying coefficients in PDEs, outperforming other methods in accuracy, efficiency, and interpretability.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of learning spatially varying coefficients in time-dependent PDEs with a robust, interpretable, and efficient method.

Method: Embeds a forward-Euler discretization into a convolutional architecture, enforcing stability and convergence via CFL-compliant coefficient parameterizations.

Result: Achieves lower errors, faster training, and fewer parameters than Fourier Neural Operators, U-Nets, and ResNets, with accurate coefficient recovery.

Conclusion: DeepFDM is a strong baseline for data-driven PDE solution and identification, offering transparency and efficiency.

Abstract: We present DeepFDM, a differentiable finite-difference framework for learning
spatially varying coefficients in time-dependent partial differential equations
(PDEs). By embedding a classical forward-Euler discretization into a
convolutional architecture, DeepFDM enforces stability and first-order
convergence via CFL-compliant coefficient parameterizations. Model weights
correspond directly to PDE coefficients, yielding an interpretable
inverse-problem formulation. We evaluate DeepFDM on a benchmark suite of scalar
PDEs: advection, diffusion, advection-diffusion, reaction-diffusion and
inhomogeneous Burgers' equations-in one, two and three spatial dimensions. In
both in-distribution and out-of-distribution tests (quantified by the Hellinger
distance between coefficient priors), DeepFDM attains normalized mean-squared
errors one to two orders of magnitude smaller than Fourier Neural Operators,
U-Nets and ResNets; requires 10-20X fewer training epochs; and uses 5-50X fewer
parameters. Moreover, recovered coefficient fields accurately match
ground-truth parameters. These results establish DeepFDM as a robust,
efficient, and transparent baseline for data-driven solution and identification
of parametric PDEs.

</details>


### [2] [Structure Preserving Finite Volume Schemes on Voronoi Grids: Curl Involution, Asymptotic Limit and Thermodynamics](https://arxiv.org/abs/2507.21351)
*Walter Boscheri,Firas Dhaouadi*

Main category: math.NA

TL;DR: A new curl-free, thermodynamically compatible finite volume scheme for compressible heat conducting flows on Voronoi grids is proposed, ensuring energy conservation and entropy production.


<details>
  <summary>Details</summary>
Motivation: To address the need for accurate and compatible numerical methods for compressible heat conducting flows, particularly in hyperbolic form.

Method: Uses compatible discrete curl-grad operators on Voronoi grids, a cell solver for thermal impulse, and transfers residuals as subfluxes for entropy-compatible semi-discrete methods.

Result: The scheme conserves energy, ensures non-negative entropy production, and is asymptotically consistent with the Fourier law.

Conclusion: The proposed scheme is validated numerically, demonstrating its effectiveness for compressible heat conducting flows.

Abstract: We propose a new curl-free and thermodynamically compatible finite volume
scheme on Voronoi grids to solve compressible heat conducting flows written in
first-order hyperbolic form. The approach is based on the definition of
compatible discrete curl-grad operators, exploiting the triangular nature of
the dual mesh. We design a cell solver reminiscent of the nodal solvers used in
Lagrangian schemes to discretize the evolution equation for the thermal impulse
vector, and we demonstrate that the resulting numerical scheme ensures energy
conservation, local non-negative entropy production, as well as asymptotic
consistency with the classical Fourier law in the stiff relaxation limit. A
novel technique is proposed to transfer residuals from the dual to the primal
mesh as subfluxes, which eventually yields the construction of entropy
compatible semi-discrete methods. The scheme and its properties are validated
on a set of numerical test cases.

</details>


### [3] [Divergence-free Preserving Mix Finite Element Methods for Fourth-order Active Fluid Model](https://arxiv.org/abs/2507.21392)
*Nan Zheng,Xu Guo,Wenlong Pei,Wenju Zhao*

Main category: math.NA

TL;DR: The paper presents a mixed FEM for solving nonlinear fourth-order active fluid equations by introducing auxiliary variables and coupling with the DLN time integrator, ensuring robustness and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of solving nonlinear fourth-order active fluid equations by relaxing regularity requirements and improving computational efficiency.

Method: Introduces auxiliary variables to transform the problem into second-order equations, uses a divergence-free preserving mixed FEM, and couples it with the DLN time integrator for a fully discrete scheme.

Result: The scheme is proven to be bounded and accurate, with numerical experiments validating its efficiency for complex simulations.

Conclusion: The proposed method effectively solves the active fluid equations with enhanced robustness and computational efficiency.

Abstract: This paper is concerned with mixed finite element method (FEM) for solving
the two-dimensional, nonlinear fourth-order active fluid equations. By
introducing an auxiliary variable $w=-\Delta u$, the original fourth problem is
transformed into a system of second-order equations, which relaxes the
regularity requirements of standard $H^2$-conforming finite spaces. To further
enhance the robustness and efficiency of the algorithm, an additional auxiliary
variable $\phi$, treated analogously to the pressure, is introduced, leading to
a divergence-free preserving mixed finite element scheme. A fully discrete
scheme is then constructed by coupling the spatial mixed FEM with the
variable-step Dahlquist-Liniger-Nevanlinna (DLN) time integrator. The
boundedness of the scheme and corresponding error estimates can be rigorously
proven under appropriate assumptions due to unconditional non-linear stability
and second-order accuracy of the DLN method. To enhance computational
efficiency in practice, we develop an adaptive time-stepping strategy based on
a minimum-dissipation criterion. Several numerical experiments are displayed to
fully validate the theoretical results and demonstrate the accuracy and
efficiency of the scheme for complex active fluid simulations.

</details>


### [4] [Variational inference and density estimation with non-negative tensor train](https://arxiv.org/abs/2507.21519)
*Xun Tang,Rajat Dwaraknath,Lexing Ying*

Main category: math.NA

TL;DR: Proposes a two-stage method to compress high-dimensional discrete distributions into non-negative tensor train (NTT) format, showing faster convergence than existing methods.


<details>
  <summary>Details</summary>
Motivation: Efficiently compress high-dimensional discrete distributions for variational inference and density estimation tasks.

Method: Two-stage approach: encode distribution into tensor train format, then fit using NTT with log barrier and second-order alternating minimization.

Result: Faster convergence than multiplicative update methods; accurate compression in numerical experiments.

Conclusion: The NTT fitting procedure is efficient and effective for compressing high-dimensional distributions.

Abstract: This work proposes an efficient numerical approach for compressing a
high-dimensional discrete distribution function into a non-negative tensor
train (NTT) format. The two settings we consider are variational inference and
density estimation, whereby one has access to either the unnormalized analytic
formula of the distribution or the samples generated from the distribution. In
particular, the compression is done through a two-stage approach. In the first
stage, we use existing subroutines to encode the distribution function in a
tensor train format. In the second stage, we use an NTT ansatz to fit the
obtained tensor train. For the NTT fitting procedure, we use a log barrier term
to ensure the positivity of each tensor component, and then utilize a
second-order alternating minimization scheme to accelerate convergence. In
practice, we observe that the proposed NTT fitting procedure exhibits
drastically faster convergence than an alternative multiplicative update method
that has been previously proposed. Through challenging numerical experiments,
we show that our approach can accurately compress target distribution
functions.

</details>


### [5] [Structure-Preserving Discretization and Model Reduction for Energy-Based Models](https://arxiv.org/abs/2507.21552)
*Robert Altmann,Attila Karsai,Philipp Schulze*

Main category: math.NA

TL;DR: The paper explores discretization strategies for energy-based models, ensuring dissipation-preserving schemes through a Petrov-Galerkin approach, validated with numerical examples.


<details>
  <summary>Details</summary>
Motivation: To address temporal, spatial, and model order reduction discretization for energy-based models while preserving dissipation properties.

Method: Uses a Petrov-Galerkin ansatz with appropriate projections for discretization.

Result: Demonstrates effectiveness via numerical results for a nonlinear circuit model and the Cahn-Hilliard equation.

Conclusion: The framework successfully combines existing ideas to ensure dissipation-preserving discretization across various systems.

Abstract: We investigate discretization strategies for a recently introduced class of
energy-based models. The model class encompasses classical port-Hamiltonian
systems, generalized gradient flows, and certain systems with algebraic
constraints. Our framework combines existing ideas from the literature and
systematically addresses temporal discretization, spatial discretization, and
model order reduction, ensuring that all resulting schemes are
dissipation-preserving in the sense of a discrete dissipation inequality. For
this, we use a Petrov-Galerkin ansatz together with appropriate projections.
Numerical results for a nonlinear circuit model and the Cahn-Hilliard equation
illustrate the effectiveness of the approach.

</details>


### [6] [Efficient and stable diffusion generated methods for ground state computation in Bose--Einstein condensates](https://arxiv.org/abs/2507.21564)
*Jing Guo,Yongyong Cai,Dong Wang*

Main category: math.NA

TL;DR: The paper introduces relaxed formulations of the Gross-Pitaevskii energy functional for BECs, achieving first- and second-order accuracy. The methods ensure concavity, enabling energy-dissipative algorithms and adaptive strategies for efficiency.


<details>
  <summary>Details</summary>
Motivation: To develop stable and efficient numerical methods for approximating the ground state of Bose-Einstein condensates (BECs) by relaxing the Gross-Pitaevskii energy functional.

Method: Two relaxed formulations of the energy functional are introduced, with sequential linear programming-based numerical methods and adaptive refinement strategies.

Result: The methods demonstrate stability, convergence, and energy dissipation, with the adaptive strategy improving computational performance.

Conclusion: The proposed relaxed formulations and adaptive algorithms provide effective and efficient numerical solutions for BEC ground state approximation.

Abstract: This paper investigates numerical methods for approximating the ground state
of Bose--Einstein condensates (BECs) by introducing two relaxed formulations of
the Gross--Pitaevskii energy functional. These formulations achieve first- and
second-order accuracy with respect to the relaxation parameter \( \tau \), and
are shown to converge to the original energy functional as \( \tau \to 0 \). A
key feature of the relaxed functionals is their concavity, which ensures that
local minima lie on the boundary of the concave hull. This property prevents
energy increases during constraint normalization and enables the development of
energy-dissipative algorithms. Numerical methods based on sequential linear
programming are proposed, accompanied by rigorous analysis of their stability
with respect to the relaxed energy. To enhance computational efficiency, an
adaptive strategy is introduced, dynamically refining solutions obtained with
larger relaxation parameters to achieve higher accuracy with smaller ones.
Numerical experiments demonstrate the stability, convergence, and energy
dissipation of the proposed methods, while showcasing the adaptive strategy's
effectiveness in improving computational performance.

</details>


### [7] [Solving Boundary Handling Analytically in Two Dimensions for Smoothed Particle Hydrodynamics](https://arxiv.org/abs/2507.21686)
*Rene Winchenbach,Andreas Kolb*

Main category: math.NA

TL;DR: A fully analytic method for evaluating boundary integrals in 2D SPH, replacing conventional boundary particles with direct integration over triangles, enabling higher-order boundary conditions and mesh-based solver coupling.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of conventional SPH boundary methods (e.g., boundary particles or wall re-normalization) by providing a direct, analytic integration approach for flexible and accurate coupling with mesh-based solvers.

Method: Direct evaluation of area integrals for SPH kernels and gradients over triangular boundaries, using closed-form solutions for compact polynomials decomposed into elementary integrals. Angular components relate to Chebyshev polynomials, and radial integrals are solved via Gaussian hypergeometric functions.

Result: The method outperforms numerical quadrature by up to five orders of magnitude in accuracy for integrals and gradients, validated against high-precision quadrature and exact solutions.

Conclusion: The analytic solution provides a robust, adaptable framework for SPH and other contexts requiring polygonal integration, addressing grand challenges in SPH with open-source implementation.

Abstract: We present a fully analytic approach for evaluating boundary integrals in two
dimensions for Smoothed Particle Hydrodynamics (SPH). Conventional methods
often rely on boundary particles or wall re-normalization approaches derived
from applying the divergence theorem, whereas our method directly evaluates the
area integrals for SPH kernels and gradients over triangular boundaries. This
direct integration strategy inherently accommodates higher-order boundary
conditions, such as piecewise cubic fields defined via Finite Element stencils,
enabling analytic and flexible coupling with mesh-based solvers. At the core of
our approach is a general solution for compact polynomials of arbitrary degree
over triangles by decomposing the boundary elements into elementary integrals
that can be solved with closed-form solutions. We provide a complete,
closed-form solution for these generalized integrals, derived by relating the
angular components to Chebyshev polynomials and solving the resulting radial
integral via a numerically stable evaluation of the Gaussian hypergeometric
function $_2F_1$. Our solution is robust and adaptable and works regardless of
triangle geometries and kernel functions. We validate the accuracy against
high-precision numerical quadrature rules, as well as in problems with known
exact solutions. We provide an open-source implementation of our general
solution using differentiable programming to facilitate the adoption of our
approach to SPH and other contexts that require analytic integration over
polygonal domains. Our analytic solution outperforms existing numerical
quadrature rules for this problem by up to five orders of magnitude, for
integrals and their gradients, while providing a flexible framework to couple
arbitrary triangular meshes analytically to Lagrangian schemes, building a
strong foundation for addressing several grand challenges in SPH and beyond.

</details>


### [8] [Non-periodic Fourier propagation algorithms for partial differential equations](https://arxiv.org/abs/2507.21757)
*Channa Hatharasinghe,Run Yan Teh,Jesse van Rhijn,Peter D. Drummond,Margaret D. Reid*

Main category: math.NA

TL;DR: A Fourier method using fast trigonometric expansions on uniform grids with non-periodic boundaries (DST/DCT) is implemented for solving parabolic PDEs, outperforming other methods in accuracy and speed for rapidly varying solutions.


<details>
  <summary>Details</summary>
Motivation: Non-uniform grids can introduce larger spatial errors for rapidly varying solutions, prompting the need for a more efficient and accurate method.

Method: The method employs fast discrete sine/cosine transforms (DST/DCT) on uniform grids, using either Fourier spectral derivatives or an interaction picture approach, adaptable to Dirichlet/Neumann boundary conditions.

Result: For the 1D heat equation, the method achieves machine precision accuracy. It also outperforms polynomial spectral and finite element methods in accuracy and speed for rapidly varying solutions.

Conclusion: The proposed Fourier method is highly accurate and efficient, especially for problems with rapidly varying spatial solutions, and is adaptable to various boundary conditions.

Abstract: Spectral methods for solving partial differential equations (PDEs) and
stochastic partial differential equations (SPDEs) often use Fourier or
polynomial spectral expansions on either uniform and non-uniform grids.
However, while very widely used, especially for slowly-varying solutions,
non-uniform spatial grids can give larger spatial discretization errors if the
solutions change rapidly in space. Here, we implement a Fourier method that
employs fast trigonometric expansions on a uniform grid with non-periodic
boundaries using fast discrete sine transforms (DST) or/and discrete cosine
transforms (DCT) to solve parabolic PDEs. We implement this method in two ways:
either using a Fourier spectral derivative or a Fourier interaction picture
approach. These methods can treat vector fields with a combination of Dirichlet
and/or Neumann boundary conditions in one or more space dimensions. We use them
to solve a variety of PDEs with analytical solutions, including the Peregrine
solitary wave solution. For the 1D heat equation problem, our method with an
interaction picture is accurate up to the machine precision. A soluble example
of an SPDE with non-periodic boundaries is also treated. We compare the results
obtained from these algorithms with those from publicly available solvers that
use either polynomial spectral or finite element methods. For problems with
solutions that vary rapidly in space, our method outperforms the other methods
by recording lower spatial discretization errors, as well being faster in many
cases, due to the efficiency improvements given by fast transforms.

</details>


### [9] [Fast multipole method for the Laplace equation in half plane with Robin boundary condition](https://arxiv.org/abs/2507.21913)
*Chunzhi Xiang,Bo Wang,Wenzhong Zhang,Wei Cai*

Main category: math.NA

TL;DR: A fast multipole method (FMM) for solving the 2D Laplace equation in a half-plane with Robin boundary conditions, using a novel expansion theory for the Green's function's reaction component.


<details>
  <summary>Details</summary>
Motivation: To efficiently solve the Laplace equation in a half-plane with Robin boundary conditions, leveraging the FMM framework for computational speed.

Method: Uses Fourier transform to express the reaction field as a Sommerfeld-type integral, derives far-field approximations, and integrates them into the classic FMM with a tree structure of original and image charges.

Result: The method achieves exponential convergence and $O(N)$ computational complexity, validated by numerical examples.

Conclusion: The developed FMM is efficient and accurate for solving the half-plane Laplace equation with Robin boundary conditions.

Abstract: In this paper, we present a fast multipole method (FMM) for solving the
two-dimensional Laplace equation in a half-plane with Robin boundary
conditions. The method is based on a novel expansion theory for the reaction
component of the Green's function. By applying the Fourier transform, the
reaction field component is obtained in a Sommerfeld-type integral form. We
derive far-field approximations and corresponding shifting and translation
operators from the Fourier integral representation. The FMM for the reaction
component is then developed by using the new far-field approximations
incorporated into the classic FMM framework in which the tree structure is
constructed from the original and image charges. Combining this with the
standard FMM for the free-space components, we develop a fast algorithm to
compute the interaction of the half plane Laplace Green's function. We prove
that the method exhibits exponential convergence, similar to the free-space
FMM. Finally, numerical examples are presented to validate the theoretical
results and demonstrate that the FMM achieves $O(N)$ computational complexity.

</details>


### [10] [Structure-preserving nodal DG method for Euler equations with gravity II: general equilibrium states](https://arxiv.org/abs/2507.21948)
*Yuchang Liu,Wei Guo,Yan Jiang,Mengping Zhang*

Main category: math.NA

TL;DR: An entropy-stable nodal DG scheme for Euler equations with gravity, ensuring well-balanced properties and compatibility with positivity-preserving limiters.


<details>
  <summary>Details</summary>
Motivation: To address the need for accurate and robust numerical methods for Euler equations with gravity, including general equilibrium solutions.

Method: Novel treatment of gravitational source term using entropy-conservative fluxes and linear entropy correction, designed for positivity preservation.

Result: Theoretical analysis confirms accuracy and structure-preserving properties; numerical experiments demonstrate robustness and efficiency.

Conclusion: The scheme is effective for Euler equations with gravity, balancing accuracy, stability, and computational efficiency.

Abstract: We develop an entropy-stable nodal discontinuous Galerkin (DG) scheme for the
Euler equations with gravity, which is also well-balanced with respect to
general equilibrium solutions, including both hydrostatic and moving
equilibria. The core of our approach lies in a novel treatment of the
gravitational source term, combining entropy-conservative numerical fluxes with
a linear entropy correction. In addition, the proposed formulation is carefully
designed to ensure compatibility with a positivity-preserving limiter. We
provide a rigorous theoretical analysis to establish the accuracy and
structure-preserving properties of the proposed scheme. Extensive numerical
experiments confirm the robustness and efficiency of the scheme.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [11] [Comparison principle for Singular Fractional $ g- $Laplacian Problems](https://arxiv.org/abs/2507.21185)
*Abdelhamid Gouasmia,Kaushik Bal*

Main category: math.AP

TL;DR: The paper establishes a novel comparison principle and proves uniqueness of weak solutions for a class of fractional elliptic problems in local Orlicz-Sobolev spaces.


<details>
  <summary>Details</summary>
Motivation: To address the uniqueness of weak solutions for fractional elliptic problems with nonlinear terms, leveraging the fractional g-Laplacian framework.

Method: A refined variational approach incorporating a G-fractional version of the Díaz--Saa inequality and a G-fractional analogue of Picone's identity.

Result: Uniqueness of weak solutions is proven for the given fractional elliptic problem under suitable conditions.

Conclusion: The tools developed are also applicable to other problems like eigenvalue simplicity and Hardy-type inequalities, demonstrating broader utility.

Abstract: In this paper, we establish a novel comparison principle of independent
interest and prove the uniqueness of weak solutions within the local
Orlicz--Sobolev space framework, for the following class of fractional elliptic
problems:
  \begin{equation*}
  (-\Delta)^{s}_{g} u = f(x) u^{-\alpha} + k(x) u^{\beta}, \quad u > 0 \quad
\text{in } \Omega; \quad u = 0 \quad \text{in } \mathbb{R}^{N} \setminus
\Omega,
  \end{equation*}
  where \( \Omega \subset \mathbb{R}^{N} \) is a smooth bounded domain, \(
\alpha > 0 \), and \( \beta > 0 \) satisfies a suitable upper bound. Here, \(
(-\Delta)^{s}_{g} \) denotes the fractional \( g \)-Laplacian, with \( g \)
being the derivative of a Young function \( G \). The function \( f \) is
assumed to be nontrivial, while \( k \) is a positive function, and both \( f
\) and \( k \) are assumed to lie in suitable Orlicz spaces. Our analysis
relies on a refined variational approach that incorporates a \( G \)-fractional
version of the D\'iaz--Saa inequality together with a \( G \)-fractional
analogue of Picone's identity. These tools, which are of independent interest,
also play a key role in the study of simplicity of eigenvalues, Sturmian-type
comparison results, Hardy-type inequalities, and related topics.

</details>


### [12] [Reiterated $Σ$-Convergence In Orlicz Setting and Applications](https://arxiv.org/abs/2507.21201)
*J. Dongho,Joel Fotso Tachago,H. Nnang,T. F. A. Tchinda*

Main category: math.AP

TL;DR: Extension of reiterated Σ-convergence to Orlicz-Sobolev spaces for homogenizing multiscale problems in deterministic settings.


<details>
  <summary>Details</summary>
Motivation: To address homogenization of multiscale problems with solutions in Orlicz-Sobolev spaces, leveraging ergodic reiterated homogenization supralgebra.

Method: Extends reiterated Σ-convergence to Orlicz-Sobolev spaces, applies to nonlinear degenerate elliptic operators with nonstandard growth.

Result: Demonstrates applicability to deterministic reiterated homogenization and derives concrete problems under varied structural hypotheses.

Conclusion: The framework successfully generalizes multiscale homogenization to Orlicz-Sobolev spaces, with practical applications in nonlinear problems.

Abstract: The concept of reiterated $\Sigma$-convergence (and more generally of
multiscale $\Sigma$-convergence) is extended to framework of Orlicz-Sobolev
spaces, in order to deals with homogenization of multiscales problems in
general deterministic setting and whose solutions leads in this type of spaces.
This concept relies on the notion of reiterated homogenization supralgebra that
we will assumed being ergodic. An application to the deterministic reiterated
homogenization of nonlinear degenerate elliptic operators with nonstandard
growth is given and some concrete homogenization problems following varied
structure hypothesis are deduce from this latter.

</details>


### [13] [Mean field limit for interacting systems on co-evolving networks](https://arxiv.org/abs/2507.21312)
*Sebastian Throm*

Main category: math.AP

TL;DR: A mean-field description is developed for particle systems with memory on co-evolving networks, addressing the infinite particle limit.


<details>
  <summary>Details</summary>
Motivation: To model collective behavior in adaptive networks, where particle interactions and network dynamics are coupled, and to understand the system's behavior in the infinite particle limit.

Method: A general mean-field approach is applied to particle systems with non-local time interactions (memory), including non-linear weight dynamics in co-evolving networks.

Result: The work provides a mean-field framework suitable for a broad class of systems on co-evolving networks, accommodating memory effects.

Conclusion: The proposed mean-field description advances the understanding of particle systems with memory on adaptive networks, particularly in the infinite particle limit.

Abstract: Interacting particle systems are in frequent use to model collective
behaviour in various situations and applications. For many systems, the
interaction between the agents is restricted to an underlying network structure
and often, the latter also evolves in time with its dynamics coupled to the
evolution of the particles. Due to their relevance for applications such
systems on adaptive or co-evolutionary networks have received increasing
interest in recent years. In particular, a fundamental question concerns the
behaviour of the system in the infinite particle limit. In this work we provide
a mean-field description for a general particle system which exhibits
non-locality in time (memory). The result applies particularly to a large class
of systems on co-evolving networks including non-linear weight dynamics.

</details>


### [14] [Stability of Large-Amplitude Viscous Shock Under Periodic Perturbation for 1-d Viscoelasticity with Non-Convex Constitutive Relations](https://arxiv.org/abs/2507.21470)
*Yu Mei,Peng Yuan*

Main category: math.AP

TL;DR: The paper studies the long-term behavior of viscous shock profiles in a 1D viscoelastic system with non-convex stress relations and space-periodic perturbations, proving convergence to a shifted shock profile without amplitude restrictions.


<details>
  <summary>Details</summary>
Motivation: To extend prior results by handling large-amplitude shocks and space-periodic perturbations simultaneously, while also allowing non-convex constitutive relations.

Method: Decompose large-amplitude shocks into smaller ones, use transforms and weight functions to manage non-convexity in energy estimates.

Result: The solution converges to a shifted viscous shock profile, partially determined by the perturbation, with no amplitude restrictions.

Conclusion: The work generalizes previous findings by addressing non-convexity and large shocks, demonstrating robustness in handling complex perturbations.

Abstract: This paper investigates the large-time behavior of the viscous shock profile
for the one-dimensional system of viscoelasticity, subject to initial
perturbations that approach space-periodic functions at far fields. We
specifically address the case with non-convex constitutive stress relations and
non-degenerate Lax's shock. Under the assumptions of suitably small initial
perturbations satisfying a zero-mass type condition, we prove that the solution
of the system converges to a viscous shock profile with a shift, which is
partially determined by the space-periodic perturbation. Notably, our result
imposes no amplitude restrictions on the viscous shock waves. This work extends
the result of Kawashima-Matsumura (\textit{Commun. Pure Appl. Math.}
\textbf{47}, 1994) by simultaneously handling both large-amplitude shocks and
space-periodic perturbations, while also generalizing the result of Huang-Yuan
(\textit{Commun. Math. Phys.} \textbf{387}, 2021) by allowing for a non-convex
constitutive relation. The key ingredient of proof is decomposing the
large-amplitude shock wave into small-amplitude shocks and, for each,
introducing suitable transform and weight functions to counteract the adverse
effects of non-convex constitutive relations encountered during weighted energy
estimates on the system in effective velocity and deformation gradient
variables.

</details>


### [15] [The parabolic Harnack inequality on non-local Dirichlet spaces in the view of pure analysis](https://arxiv.org/abs/2507.21604)
*Guanhua Liu*

Main category: math.AP

TL;DR: The paper presents a general theory for parabolic Harnack inequalities (PHI) in regular Dirichlet forms, using Nash and Moser methods, and expands equivalent characterizations of PHI.


<details>
  <summary>Details</summary>
Motivation: To establish a comprehensive theory for PHI in regular Dirichlet forms without killing, leveraging analytic methods.

Method: Uses Nash and Moser approaches to prove PHI and derives key properties. Combines recent results on weak Harnack inequalities.

Result: Enlarges the list of equivalent characterizations of PHI.

Conclusion: The study advances the understanding of PHI and its properties in regular Dirichlet forms.

Abstract: This paper provides the general theory on parabolic Harnack inequalities
(PHI, for short) for regular Dirichlet forms without killing part. We prove PHI
by pure analytic methods, using both Nash and Moser approaches, and yield some
important properties contained in PHI. Combining our recent result on weak
Harnack inequalities, we greatly enlarge the list of equivalent
characterizations of PHI.

</details>


### [16] [Existence analysis of a three-species memristor drift-diffusion system coupled to electric networks](https://arxiv.org/abs/2507.21725)
*Ansgar Jüngel,Tuan Tung Nguyen*

Main category: math.AP

TL;DR: Existence of global weak solutions for a coupled PDE-algebraic system in memristor devices is proven using fixed-point theorems and energy estimates.


<details>
  <summary>Details</summary>
Motivation: To model and analyze the complex interactions in memristor devices, coupling drift-diffusion equations with electric network dynamics.

Method: Combines drift-diffusion equations, Poisson equation, and differential-algebraic network equations. Uses Leray-Schauder fixed-point theorem, energy inequalities, and Gagliardo-Nirenberg inequality.

Result: Solutions are bounded and strictly positive under suitable assumptions.

Conclusion: The proof establishes the existence of physically meaningful solutions for the coupled system.

Abstract: The existence of global weak solutions to a partial-differential-algebraic
system is proved. The system consists of the drift-diffusion equations for the
electron, hole, and oxide vacancy densities in a memristor device, the Poisson
equation for the electric potential, and the differential-algebraic equations
for an electric network. The memristor device is modeled by a two-dimensional
bounded domain, and mixed Dirichlet-Neumann boundary conditions for the
electron and hole densities as well as the potential are imposed. The coupling
is realized via the total current through the memristor terminal and the
network node potentials at the terminals. The network equations are decomposed
in a differential and an algebraic part. The existence proof is based on the
Leray-Schauder fixed-point theorem, a priori estimates coming from the free
energy inequality, and a logarithmic-type Gagliardo-Nirenberg inequality. It is
shown, under suitable assumptions, that the solutions are bounded and strictly
positive.

</details>


### [17] [The conformal logarithmic Laplacian on the sphere: Yamabe-type problems and Sobolev spaces](https://arxiv.org/abs/2507.21779)
*Juan Carlos Fernández,Alberto Saldaña*

Main category: math.AP

TL;DR: The paper analyzes the conformal logarithmic Laplacian on the sphere, its spectral properties, conformal invariance, and connection to the logarithmic Laplacian in Euclidean space via stereographic projection. It also links Yamabe-type problems and introduces a Hilbert space for variational study.


<details>
  <summary>Details</summary>
Motivation: To understand the spectral and conformal properties of the logarithmic Laplacian on the sphere and its connection to Euclidean space, bridging gaps in Yamabe-type problems.

Method: Detailed investigation of the operator's properties, conformal invariance, and stereographic projection to connect it with the Euclidean case. Introduction of a Hilbert space for variational analysis.

Result: Established spectral properties, conformal invariance, and a precise connection between the sphere and Euclidean space. Extended classification results for Yamabe-type problems.

Conclusion: The study provides a comprehensive framework for analyzing logarithmic-type operators and their applications, unifying results for the sphere and Euclidean space.

Abstract: We study the conformal logarithmic Laplacian on the sphere, an explicit
singular integral operator that arises as the derivative (with respect to the
order parameter) of the conformal fractional Laplacian at zero. Our analysis
provides a detailed investigation of its spectral properties, its conformal
invariance, and the associated \(Q\)-curvature problem. Furthermore, we
establish a precise connection between this operator on the sphere and the
logarithmic Laplacian in \(\mathbb{R}^N\) via stereographic projection. This
correspondence bridges classification results for two Yamabe-type problems
previously studied in the literature, extending one of them to the weak
setting. To this end, we introduce a Hilbert space that serves as the
logarithmic counterpart of the homogeneous fractional Sobolev space, offering a
natural functional framework for the variational study of logarithmic-type
equations in unbounded domains.

</details>


### [18] [Travelling front solutions in a spatially heterogeneous reaction-diffusion system](https://arxiv.org/abs/2507.21797)
*M. Chirilus-Bruckner,L. van Vianen,F. Veerman*

Main category: math.AP

TL;DR: The paper studies a slow-fast reaction-diffusion system with spatially varying coefficients, revealing bi-stability and the existence of stationary and travelling fronts with non-uniform speeds. It extends Fenichel theory and avoids spectral analysis for front dynamics.


<details>
  <summary>Details</summary>
Motivation: To understand the dynamics of reaction-diffusion systems with spatially varying coefficients, particularly focusing on bi-stability and front solutions connecting stable states.

Method: Extends Fenichel theory to non-compact cases, uses non-autonomous spatial dynamics for front solutions, and derives a delay-differential equation for interface motion.

Result: Demonstrates bi-stability, constructs stationary and travelling fronts, and validates a leading-order expression for front dynamics numerically.

Conclusion: The work generalizes treatment of spatially varying coefficients and offers a novel approach to front dynamics, applicable beyond traditional spectral analysis.

Abstract: We investigate a two-component reaction-diffusion system with a slow-fast
structure and spatially varying coefficients $ f_1 $ and $ f_2 $ appearing in
the slow equation. Under mild boundedness and regularity conditions on $ f_1 $
and $ f_2 $ the system is shown to exhibit bi-stability in the form of two
stable stationary heterogeneous background states. These background states can
be connected by stationary and travelling front solutions. Travelling fronts
feature an interface that moves with a non-uniform speed through the motionless
spatially varying background states it connects. As a result, unlike classical
travelling waves, these fronts are not stationary in any co-moving frame. We
construct both the background states and stationary fronts using an extension
of Fenichel theory to the non-compact case. Additionally, we establish the
existence of travelling front solutions and derive a leading-order expression
for the dynamic position of the moving interface through a non-autonomous
spatial dynamics approach. This expression takes the form of a
delay-differential equation, and its accuracy is validated through numerical
simulations. A key contribution of our work lies in the general treatment of $
f_1 $ and $ f_2 $, which are neither (necessarily) asymptotically small nor
restricted to specific forms such as periodic or localised structures.
Furthermore, our derivation of the front position formula circumvents the
traditional reliance on spectral analysis, enabling us to describe front
dynamics beyond bifurcations from stationary fronts. This approach has the
potential to be extended to other settings in which spectral properties at
onset preclude conventional reduction techniques.

</details>


### [19] [Weak solutions of a viscous model for fluid-bubbles interaction](https://arxiv.org/abs/2507.21850)
*Cosmin Burtea,David Gérard-Varet*

Main category: math.AP

TL;DR: A system of Navier-Stokes type for spherical gas bubbles in liquid is derived, relaxing stress continuity to maintain sphericity. Weak solutions are constructed until bubble collisions, addressing new mathematical challenges.


<details>
  <summary>Details</summary>
Motivation: To model the dynamics of spherical gas bubbles in a liquid while preserving their shape over time, relaxing traditional stress continuity conditions.

Method: Derived from a complete model treating bubbles as gas inclusions with homogeneous barotropic pressure and free surfaces. Weak solutions are constructed à la Leray, accounting for bubble compression/dilation.

Result: Weak solutions are successfully constructed up to the point of bubble collisions, despite new mathematical complexities introduced by bubble compression/dilation.

Conclusion: The relaxed system provides a viable framework for modeling spherical bubble dynamics, though challenges remain in handling compression/dilation effects.

Abstract: We present a system of Navier-Stokes type that describes the dynamics of
several spherical bubbles of gas in a liquid. It is derived from a more
complete model, where the bubbles are seen as inclusions of gas of homogeneous
barotropic pressure with free surfaces. The usual condition of continuity of
the stress is relaxed in order to preserve the sphericity of the bubbles
through time. We construct weak solutions \`a la Leray for this relaxed system,
up to collision between the bubbles. Although these solutions are reminiscent
of weak solutions for fluid-solid interaction systems, accounting for the
compression/dilation of the bubbles creates new and significant mathematical
difficulties.

</details>


### [20] [Spreading speeds in rapidly and slowly varying almost periodic media](https://arxiv.org/abs/2507.21870)
*Xing Liang,Linfeng Xu,Tao Zhou*

Main category: math.AP

TL;DR: The paper analyzes the spreading speeds of Fisher-KPP equations in varying almost periodic settings, focusing on homogenization of Hamilton-Jacobi equations, limits, convergence rates, and the impact of adding a reaction term.


<details>
  <summary>Details</summary>
Motivation: To understand the asymptotic behavior of spreading speeds in Fisher-KPP equations under rapidly and slowly varying almost periodic conditions.

Method: Uses a variational formula involving generalized principal eigenvalues to study homogenization of Hamilton-Jacobi equations.

Result: Presents limits and convergence rates of spreading speeds and examines the effect of adding a fixed reaction term.

Conclusion: The study provides insights into the behavior of spreading speeds in Fisher-KPP equations under varying conditions and the influence of additional reaction terms.

Abstract: This paper is concerned with the asymptotic behavior of the spreading speeds
of Fisher-KPP equations in rapidly and slowly varying almost periodic settings
respectively. Thanks to the variational formula for the spreading speeds
involving generalized principal eigenvalue, we are able to concentrate on the
homogenization problem of certain Hamilton-Jacobi equations. We will present
also the limits and the convergence rate of the spreading speeds. At last, we
will consider the effect on the spreading speeds after adding a fixed reaction
term.

</details>


### [21] [Pohozaev-like identity for the regional fractional laplacian](https://arxiv.org/abs/2507.21962)
*Sidy M. Djitte*

Main category: math.AP

TL;DR: A new integration by parts formula for the regional fractional Laplacian is derived, leading to a Pohozaev-like identity for weak solutions and applications in eigenvalue problems and unique continuation properties.


<details>
  <summary>Details</summary>
Motivation: To extend integration by parts techniques to the regional fractional Laplacian in bounded domains and explore its implications for PDEs.

Method: Derive an integration by parts formula for the regional fractional Laplacian in C^2 bounded sets, then apply it to weak solutions of the Dirichlet problem.

Result: A Pohozaev-like identity is proven for weak solutions, with applications in eigenvalue problems and boundary-type unique continuation.

Conclusion: The new formula and identity provide tools for analyzing fractional Laplacian problems and potential extensions to unique continuation properties.

Abstract: We establish a new integration by parts formula for the regional fractional
laplacian $(-\Delta)^s_\Omega$ in bounded open sets of class $C^2$. As a direct
application, we prove that weak solutions to the corresponding Dirichlet
problem satisfy a Pohozaev-like identity with an explicit remainder term. We
apply the later to eigenvalue problems in the unit ball and discuss its
potential use in establishing boundary-type unique continuation properties.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [22] [Linear and Nonlinear Optical Properties of Molecules from Real-Time Propagation Based on the Bethe-Salpeter Equation](https://arxiv.org/abs/2507.21279)
*Štěpán Marek,Jan Wilhelm*

Main category: physics.comp-ph

TL;DR: A real-time propagation method for computing linear and nonlinear optical properties of molecules using the Bethe-Salpeter equation, benchmarked against standard linear-response methods with good agreement.


<details>
  <summary>Details</summary>
Motivation: To develop a computationally efficient method for studying both linear and nonlinear optical properties of molecules, addressing limitations of the standard linear-response Bethe-Salpeter equation.

Method: Time evolution of the one-particle density matrix under an external electric field, incorporating electron-electron interactions via a screened exchange approximation and quasiparticle energies from GW calculations.

Result: Very good agreement with linear-response methods (30 meV deviation in peak positions) and successful simulation of nonlinear phenomena like second harmonic generation.

Conclusion: The method is accurate and computationally feasible, offering broad applicability for studying optical properties of molecules.

Abstract: We present a real-time propagation method for computing linear and nonlinear
optical properties of molecules based on the Bethe-Salpeter equation. The
method follows the time evolution of the one-particle density matrix under an
external electric field. We include electron-electron interaction effects
through a self-energy based on the screened exchange approximation.
Quasiparticle energies are taken from a prior $GW$ calculation to construct the
effective single-particle Hamiltonian and we represent all operators and
wavefunctions in an atom-centered Gaussian basis. We benchmark the accuracy of
the real-time propagation against the standard linear-response Bethe-Salpeter
equation using a set of organic molecules. We find very good agreement when
computing linear-response isotropic polarizability spectra from both
approaches, with a mean absolute deviation of 30~meV in peak positions. Beyond
linear response, we simulate second harmonic generation and optical
rectification in a non-centrosymmetric molecule. These phenomena are not
captured by the commonly used linear-response Bethe-Salpeter equation. We
foresee broad applicability of real-time propagation based on the
Bethe-Salpeter equation for the study of linear and nonlinear optical
properties of molecules as the method has a similar computational cost as
time-dependent density functional theory with hybrid functionals.

</details>


### [23] [Physics-Informed Neural Networks with Dynamical Boundary Constraints](https://arxiv.org/abs/2507.21800)
*Andrés Martínez-Esteban,Pablo Calvo-Barlés,Luis Martín-Moreno,Sergio G Rodrigo*

Main category: physics.comp-ph

TL;DR: PINNs embed physical information into neural network loss functions but struggle with convergence on misleading solutions, especially in multi-scale problems. The DBC algorithm addresses this by imposing loss function restrictions based on prior PINN training.


<details>
  <summary>Details</summary>
Motivation: PINNs often converge on trivial or incorrect solutions despite low loss values, particularly in multi-scale differential equations.

Method: The Dynamical Boundary Constraint (DBC) algorithm restricts the loss function using prior PINN training.

Result: The DBC algorithm was tested on various physics examples, showing improved handling of multi-scale behavior.

Conclusion: DBC effectively mitigates PINN convergence issues in multi-scale problems, enhancing solution accuracy.

Abstract: Physics-informed neural networks (PINNs) are numerical solvers that embed all
the physical information of a system into the loss function of a neural
network. In this way the learned solution accounts for data (if available), the
governing differential equations, or any other constraint known of the physical
problem. However, they face serious issues, notably their tendency to converge
on trivial or misleading solutions. The latter occurs when, although the loss
function reaches low values the model makes incorrect predictions. These
difficulties become especially significant in differential equations involving
multi-scale behavior, such as rapidly varying terms and solutions exhibiting
strong oscillatory behavior. To address these challenges, we introduce the
Dynamical Boundary Constraint (DBC) algorithm, which imposes restrictions on
the loss function based on prior training of the PINN. To demonstrate its
applicability, we tested this approach on examples of different areas of
physics.

</details>


### [24] [Chromo: A High-Performance Python Interface to Hadronic Event Generators for Collider and Cosmic-Ray Simulations](https://arxiv.org/abs/2507.21856)
*Anatoli Fedynitch,Hans Dembinski,Anton Prosekin*

Main category: physics.comp-ph

TL;DR: Chromo is a Python package providing a unified interface to multiple hadronic event generators, offering high performance and integration with the Scientific Python ecosystem.


<details>
  <summary>Details</summary>
Motivation: To simplify and unify the use of various hadronic event generators (like EPOS, DPMJet, etc.) in collider and astroparticle physics simulations.

Method: Built as an abstraction layer on top of Fortran and C++ implementations, Chromo offers a Python interface with zero overhead, supporting event export in HepMC, ROOT, and SVG formats.

Result: Chromo preserves the performance of direct generator calls while being easy to install and integrate with Python tools like Jupyter notebooks.

Conclusion: Chromo is a versatile tool for hadronic and nuclear simulations, enhancing workflows in astroparticle physics, as demonstrated in the MCEq cascade solver.

Abstract: Simulations of hadronic and nuclear interactions are essential in both
collider and astroparticle physics. The Chromo package provides a unified
Python interface to multiple widely used hadronic event generators, including
EPOS, DPMJet, Sibyll, QGSJet, and Pythia. Built on top of their original
Fortran and C++ implementations, Chromo offers a zero-overhead abstraction
layer suitable for use in Python scripts, Jupyter notebooks, or from the
command line, while preserving the performance of direct calls to the
generators. It is easy to install via precompiled binary wheels distributed
through PyPI, and it integrates well with the Scientific Python ecosystem.
Chromo supports event export in HepMC, ROOT, and SVG formats and provides a
consistent interface for inspecting, filtering, and modifying particle
collision events. This paper describes the architecture, typical use cases, and
performance characteristics of Chromo and its role in contemporary
astroparticle simulations, such as in the MCEq cascade solver.

</details>


### [25] [Following the Committor Flow: A Data-Driven Discovery of Transition Pathways](https://arxiv.org/abs/2507.21961)
*Cheng Giuseppe Chen,Chenyu Tang,Alberto Megías,Radu A. Talmazan,Sergio Contreras Arredondo,Benoît Roux,Christophe Chipot*

Main category: physics.comp-ph

TL;DR: An iterative framework is proposed to infer the committor probability and identify key transition pathways in molecular systems, enhancing sampling and refining results until convergence.


<details>
  <summary>Details</summary>
Motivation: The challenge of discovering transition pathways and rare events in molecular systems, with the committor probability being a critical but hard-to-compute reaction coordinate.

Method: An iterative approach using biased sampling, neural networks to approximate the committor, and discretized strings on isocommittor surfaces to refine pathways.

Result: The framework accurately estimates reaction rate constants and is validated on benchmark systems like peptide transitions and a Diels-Alder reaction.

Conclusion: The method effectively refines committor and transition paths, providing accurate mechanistic insights and reaction rate estimates.

Abstract: The discovery of transition pathways to unravel distinct reaction mechanisms
and, in general, rare events that occur in molecular systems is still a
challenge. Recent advances have focused on analyzing the transition path
ensemble using the committor probability, widely regarded as the most
informative one-dimensional reaction coordinate. Consistency between transition
pathways and the committor function is essential for accurate mechanistic
insight. In this work, we propose an iterative framework to infer the committor
and, subsequently, to identify the most relevant transition pathways. Starting
from an initial guess for the transition path, we generate biased sampling from
which we train a neural network to approximate the committor probability. From
this learned committor, we extract dominant transition channels as discretized
strings lying on isocommittor surfaces. These pathways are then used to enhance
sampling and iteratively refine both the committor and the transition paths
until convergence. The resulting committor enables accurate estimation of the
reaction rate constant. We demonstrate the effectiveness of our approach on
benchmark systems, including a two-dimensional model potential, peptide
conformational transitions, and a Diels--Alder reaction.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [26] [Weak decaying collective-excitation approximation for Yukawa one-component plasmas](https://arxiv.org/abs/2507.21525)
*Ilnaz I. Fairushin,Anatolii V. Mokshin*

Main category: physics.plasm-ph

TL;DR: The paper develops a theoretical model for weak decaying collective excitations in strongly coupled Yukawa plasmas, aligning with molecular dynamics simulations without fitting parameters.


<details>
  <summary>Details</summary>
Motivation: To model collective excitations in systems with long-range interactions, specifically Yukawa plasmas, bridging hydrodynamics and interparticle scales.

Method: Uses self-consistent relaxation theory to derive dynamic structure factors and dispersion characteristics, comparing with molecular dynamics simulations.

Result: The model matches simulation data and connects to the damped harmonic oscillator model at small wave numbers, providing a sound attenuation coefficient.

Conclusion: The theoretical framework successfully describes collective excitations in Yukawa plasmas, validated by simulations.

Abstract: In this paper, the theoretical model of weak decaying collective excitations
characteristic of many-particle systems with long-range interaction potentials
is developed using the example of one-component strongly coupled Yukawa
plasmas. The proposed model is based on the self-consistent relaxation theory
of collective dynamics and covers spatial scales from extended hydrodynamics to
scales related to the mean interparticle distance. The theoretical model
reproduces the dynamic structure factor spectra and the corresponding
dispersion characteristics in agreement with molecular dynamics simulation data
without using any fitting parameters. In the limit of small wave numbers, the
correspondence of the proposed theoretical model with the damped harmonic
oscillator model is established. The simple analytical expression for the sound
attenuation coefficient of strongly coupled Yukawa plasmas is obtained.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [27] [Elastic rigid rod in an expanding universe](https://arxiv.org/abs/2507.21227)
*Simão Correia,José Natário,Jorge Drumond Silva*

Main category: gr-qc

TL;DR: The paper examines the motion of a rigid elastic rod in an expanding universe, showing its behavior depends on a parameter κ. Small κ leads to bounded oscillations, while large κ causes infinite stretching, aligning with the idea that small bound systems resist Hubble flow.


<details>
  <summary>Details</summary>
Motivation: To understand how bound systems like rigid rods behave in an expanding universe and whether they follow the Hubble flow.

Method: Analyzes an initial boundary value problem (IBVP) for a nonlinear wave equation with parameter κ, representing the rod's length relative to the cosmological scale.

Result: For small κ, the rod's length oscillates around its initial value; for large κ, it stretches infinitely, supporting the idea that small systems resist expansion.

Conclusion: Small bound systems resist cosmological expansion, while larger ones may follow the Hubble flow, with implications for tethered galaxy scenarios.

Abstract: We study the motion of a rigid elastic rod, initially set in its relaxed
state along a spacelike geodesic, in an expanding
Friedmann-Lema\^itre-Robertson-Walker universe. This leads to an initial
boundary value problem (IBVP) for a nonlinear wave equation whose nonlinearity
depends on a parameter $\kappa \geq 0$, related to the ratio between the rod's
length and the cosmological scale. We show that if $\kappa$ is small enough
then the solution to the IBVP is global in time and bounded, meaning that the
rod's length oscillates around its initial value. For greater values of
$\kappa$, however, the solution to the IBVP blows up in finite coordinate time,
indicating that the rod is infinitely stretched by the cosmological expansion.
This supports the widely held belief that sufficiently small bound systems do
not follow the Hubble flow, whereas larger systems may do so. Similar
conclusions apply to the tethered galaxy version of this problem, where the rod
is used to connect two point masses (which results in nonlinear boundary
conditions for the IBVP).

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [28] [Learning Kinetic Monte Carlo stochastic dynamics with Deep Generative Adversarial Networks](https://arxiv.org/abs/2507.21763)
*Daniele Lanzoni,Olivier Pierre-Louis,Roberto Bergamaschini,Francesco Montalenti*

Main category: cond-mat.stat-mech

TL;DR: GANs are used to learn stochastic dynamics, replacing traditional models for simulating particle systems with thermal fluctuations, achieving accurate results with reduced computational cost.


<details>
  <summary>Details</summary>
Motivation: To leverage GANs for learning stochastic dynamics in particle systems, capturing thermal fluctuations more efficiently than traditional methods like Kinetic Monte Carlo.

Method: A conditional GAN is trained on a dataset from Kinetic Monte Carlo simulations to propagate system states stochastically, with modifications for better convergence and accuracy.

Result: The GAN quantitatively reproduces equilibrium and kinetic properties, including scaling laws, with deviations of a few percent from exact values.

Conclusion: GANs are effective for simulating stochastic dynamics, though extrapolation limits exist, suggesting future work to address these constraints.

Abstract: We show that Generative Adversarial Networks (GANs) may be fruitfully
exploited to learn stochastic dynamics, surrogating traditional models while
capturing thermal fluctuations. Specifically, we showcase the application to a
two-dimensional, many-particle system, focusing on surface-step fluctuations
and on the related time-dependent roughness. After the construction of a
dataset based on Kinetic Monte Carlo simulations, a conditional GAN is trained
to propagate stochastically the state of the system in time, allowing the
generation of new sequences with a reduced computational cost. Modifications
with respect to standard GANs, which facilitate convergence and increase
accuracy, are discussed. The trained network is demonstrated to quantitatively
reproduce equilibrium and kinetic properties, including scaling laws, with
deviations of a few percent from the exact value. Extrapolation limits and
future perspectives are critically discussed.

</details>


<div id='math.CV'></div>

# math.CV [[Back]](#toc)

### [29] [A priori estimates for the complex Monge-Ampère equation after Krylov](https://arxiv.org/abs/2507.21729)
*Sławomir Dinew,Marcin Sroka*

Main category: math.CV

TL;DR: Analytic proofs for Krylov $C^{1,1}$ estimates and Bedford-Taylor interior $C^{1,1}$ estimates in degenerate complex Monge-Ampère equations.


<details>
  <summary>Details</summary>
Motivation: To provide rigorous analytic proofs for key estimates in the study of degenerate complex Monge-Ampère equations, addressing gaps in existing literature.

Method: Uses analytic techniques to derive $C^{1,1}$ estimates for solutions of the degenerate complex Monge-Ampère equation, focusing on Krylov and Bedford-Taylor cases.

Result: Successful establishment of analytic proofs for the Krylov $C^{1,1}$ estimates and the Bedford-Taylor interior $C^{1,1}$ estimate.

Conclusion: The proofs enhance understanding of regularity in degenerate complex Monge-Ampère equations, contributing to broader mathematical analysis.

Abstract: We establish an analytic proof for the Krylov $C^{1,1}$ estimates for
solutions of degenerate complex Monge-Amp\`ere equation. We also provide an
analytic proof of the Bedford-Taylor interior $C^{1,1}$ estimate.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [30] [Evaluating Isoreticular Series of CALF-20 for Biogas Upgrading using a Pressure/Vacuum Swing Adsorption (PVSA) Process](https://arxiv.org/abs/2507.21126)
*Changdon Shin,Sunghyun Yoon,Yongchul G. Chung*

Main category: physics.chem-ph

TL;DR: The study evaluates CALF-20 and its derivatives for biogas upgrading using PVSA, identifying FumCALF-20 as the top performer for high CH4 purity and recovery.


<details>
  <summary>Details</summary>
Motivation: To design high-performance adsorbent materials for efficient CO2/CH4 separation in biogas upgrading.

Method: Combined molecular simulations (GCMC) and PVSA cycle optimization (TSEMO algorithm) to assess six CALF-20 derivatives.

Result: FumCALF-20 achieved CH4 purity > 0.90 with high recovery, outperforming other materials.

Conclusion: Process-level optimization is crucial for evaluating MOFs in energy-efficient biogas upgrading.

Abstract: Cyclic swing adsorption processes, such as pressure vacuum swing adsorption
(PVSA), have emerged as a promising technology for upgrading biogas by
separating carbon dioxide (CO2) from methane (CH4). The rational design of
adsorbent materials with tailored properties is important for deployment of
high-performance PVSA technology. Metal-organic frameworks (MOFs), particularly
the CALF-20 isoreticular series, have attracted interest due to their high CO2
selectivity, thermal and water stability. In this study, we report a multiscale
assessment of CALF-20 and its isoreticular five derivatives by integrating
molecular simulations and PVSA cycle optimization. Structural parameters such
as pore volume, pore size, and isosteric adsorption enthalpy were first
calculated, followed by atomistic grand canonical Monte Carlo (GCMC)
simulations. Process-level performances of the six materials were evaluated and
optimized using the Thompson Sampling Efficient Multi-objective Optimization
(TSEMO) algorithm. From the process-level optimization, we found that
FumCALF-20 is the only material that can reach CH4 purity > 0.90 while
maintaining high recovery. Other materials either lacked sufficient CO2
capacity or showed inefficient CH4 desorption at low pressures. This study
underscores the value of process-level optimization in MOF evaluation and
screening for energy-efficient biogas upgrading.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [31] [evoxels: A differentiable physics framework for voxel-based microstructure simulations](https://arxiv.org/abs/2507.21748)
*Simon Daubner,Alexander E. Cohen,Benjamin Dörich,Samuel J. Cooper*

Main category: cs.LG

TL;DR: The paper introduces evoxels, a differentiable physics framework for integrating microscopy data, simulations, and machine learning to optimize material design.


<details>
  <summary>Details</summary>
Motivation: Bridging experimental and computational domains is crucial for inverse material design, linking desired performance to optimal microstructures and manufacturing.

Method: The evoxels framework uses a Pythonic, voxel-based approach to combine segmented 3D microscopy data, physical simulations, inverse modeling, and machine learning.

Result: This integration accelerates material discovery and enhances understanding of process-structure-property relationships.

Conclusion: Evoxels provides a unified solution for advancing inverse material design through interdisciplinary integration.

Abstract: Materials science inherently spans disciplines: experimentalists use advanced
microscopy to uncover micro- and nanoscale structure, while theorists and
computational scientists develop models that link processing, structure, and
properties. Bridging these domains is essential for inverse material design
where you start from desired performance and work backwards to optimal
microstructures and manufacturing routes. Integrating high-resolution imaging
with predictive simulations and data-driven optimization accelerates discovery
and deepens understanding of process-structure-property relationships. The
differentiable physics framework evoxels is based on a fully Pythonic, unified
voxel-based approach that integrates segmented 3D microscopy data, physical
simulations, inverse modeling, and machine learning.

</details>


### [32] [Discovering Interpretable Ordinary Differential Equations from Noisy Data](https://arxiv.org/abs/2507.21841)
*Rahul Golder,M. M. Faruque Hasan*

Main category: cs.LG

TL;DR: Proposes an unsupervised method for discovering interpretable ODE models from noisy data using spline transformations and linear estimation.


<details>
  <summary>Details</summary>
Motivation: Current methods lack physical interpretability and accuracy in modeling system dynamics.

Method: Uses an approximate general solution and spline transformation to linearly estimate ODE coefficients, leveraging gradient information.

Result: Achieves high accuracy and sparsity in ODE discovery without regularization, robust to noise.

Conclusion: Enables practical data-driven learning of physical phenomena with interpretable models.

Abstract: The data-driven discovery of interpretable models approximating the
underlying dynamics of a physical system has gained attraction in the past
decade. Current approaches employ pre-specified functional forms or basis
functions and often result in models that lack physical meaning and
interpretability, let alone represent the true physics of the system. We
propose an unsupervised parameter estimation methodology that first finds an
approximate general solution, followed by a spline transformation to linearly
estimate the coefficients of the governing ordinary differential equation
(ODE). The approximate general solution is postulated using the same functional
form as the analytical solution of a general homogeneous, linear,
constant-coefficient ODE. An added advantage is its ability to produce a
high-fidelity, smooth functional form even in the presence of noisy data. The
spline approximation obtains gradient information from the functional form
which are linearly independent and creates the basis of the gradient matrix.
This gradient matrix is used in a linear system to find the coefficients of the
ODEs. From the case studies, we observed that our modeling approach discovers
ODEs with high accuracy and also promotes sparsity in the solution without
using any regularization techniques. The methodology is also robust to noisy
data and thus allows the integration of data-driven techniques into real
experimental setting for data-driven learning of physical phenomena.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [33] [Data-driven quantum Koopman method for simulating nonlinear dynamics](https://arxiv.org/abs/2507.21890)
*Baoyang Zhang,Zhen Lu,Yaomin Zhao,Yue Yang*

Main category: quant-ph

TL;DR: The paper introduces the Quantum Koopman Method (QKM), a data-driven framework for simulating nonlinear dynamics using quantum computation by transforming them into linear unitary evolution in higher-dimensional spaces.


<details>
  <summary>Details</summary>
Motivation: Quantum computation's potential for simulating physical systems is limited by the requirement of unitary evolution, which inherently constrains its application to nonlinear dynamics. The QKM aims to bridge this gap.

Method: The QKM leverages Koopman operator theory for global linearization, mapping system states into Hilbert spaces via a deep autoencoder. It decomposes states into modulus and phase components, with evolution governed by unitary Koopman operators acting on the phase, constructed from data-learned diagonal Hamiltonians.

Result: Validated across diverse nonlinear systems, QKM maintains relative errors below 6% for reaction-diffusion systems and shear flows, and captures key statistics in 2D turbulence.

Conclusion: The QKM provides a practical pathway for quantum-accelerated simulation of nonlinear phenomena, combining deep learning for linearization and quantum algorithms for unitary evolution.

Abstract: Quantum computation offers potential exponential speedups for simulating
certain physical systems, but its application to nonlinear dynamics is
inherently constrained by the requirement of unitary evolution. We propose the
quantum Koopman method (QKM), a data-driven framework that bridges this gap
through transforming nonlinear dynamics into linear unitary evolution in
higher-dimensional observable spaces. Leveraging the Koopman operator theory to
achieve a global linearization, our approach maps system states into a
hierarchy of Hilbert spaces using a deep autoencoder. Within the linearized
embedding spaces, the state representation is decomposed into modulus and phase
components, and the evolution is governed by a set of unitary Koopman operators
that act exclusively on the phase. These operators are constructed from
diagonal Hamiltonians with coefficients learned from data, a structure designed
for efficient implementation on quantum hardware. This architecture enables
direct multi-step prediction, and the operator's computational complexity
scales logarithmically with the observable space dimension. The QKM is
validated across diverse nonlinear systems. Its predictions maintain relative
errors below 6% for reaction-diffusion systems and shear flows, and capture key
statistics in 2D turbulence. This work establishes a practical pathway for
quantum-accelerated simulation of nonlinear phenomena, exploring a framework
built on the synergy between deep learning for global linearization and quantum
algorithms for unitary dynamics evolution.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [34] [The Performance of Low-Synchronization Variants of Reorthogonalized Block Classical Gram--Schmidt](https://arxiv.org/abs/2507.21791)
*Erin Carson,Yuxin Ma*

Main category: cs.DC

TL;DR: The paper evaluates the performance of low-synchronization BCGS variants (BCGSI+P-1S and BCGSI+P-2S) on distributed memory systems, showing significant speedups and stability advantages.


<details>
  <summary>Details</summary>
Motivation: Communication costs, especially global synchronization, are a bottleneck in distributed memory systems for orthogonalizing vectors. Reducing synchronization points while maintaining stability is crucial.

Method: Performance comparison of BCGSI+P-1S and BCGSI+P-2S against other low-synchronization BCGS variants on distributed memory systems.

Result: BCGSI+P-1S achieves up to 4x speedup, BCGSI+P-2S up to 2x, with similar performance to less stable variants. Both are stable and efficient.

Conclusion: BCGSI+P-1S and BCGSI+P-2S are recommended for economic QR factorization due to their stability and performance advantages.

Abstract: Numerous applications, such as Krylov subspace solvers, make extensive use of
the block classical Gram-Schmidt (BCGS) algorithm and its reorthogonalized
variants for orthogonalizing a set of vectors. For large-scale problems in
distributed memory settings, the communication cost, particularly the global
synchronization cost, is a major performance bottleneck. In recent years, many
low-synchronization BCGS variants have been proposed in an effort to reduce the
number of synchronization points. The work [E. Carson, Y. Ma, arXiv preprint
2411.07077] recently proposed stable one-synchronization and
two-synchronization variants of BCGS, i.e., BCGSI+P-1S and BCGSI+P-2S. In this
work, we evaluate the performance of BCGSI+P-1S and BCGSI+P-2S on a distributed
memory system compared to other well-known low-synchronization BCGS variants.
In comparison to the classical reorthogonalized BCGS algorithm (BCGSI+),
numerical experiments demonstrate that BCGSI+P-1S and BCGSI+P-2S can achieve up
to 4 times and 2 times speedups, respectively, and perform similarly to other
(less stable) one-synchronization and two-synchronization variants. BCGSI+P-1S
and BCGSI+P-2S are therefore recommended as the best choice in practice for
computing an economic QR factorization on distributed memory systems due to
their superior stability when compared to other variants with the same
synchronization cost.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [35] [Unified machine-learning framework for property prediction and time-evolution simulation of strained alloy microstructure](https://arxiv.org/abs/2507.21760)
*Andrea Fantasia,Daniele Lanzoni,Niccolò Di Eugenio,Angelo Monteleone,Roberto Bergamaschini,Francesco Montalenti*

Main category: cond-mat.mtrl-sci

TL;DR: A unified ML framework predicts alloy microstructure evolution under elastic fields, extracting parameters and forecasting future states accurately.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of predicting temporal microstructure evolution in alloys influenced by elastic fields, using a scalable and generalizable approach.

Method: Combines convolutional recurrent neural networks with phase field simulations to extract elastic parameters and predict microstructure evolution.

Result: Accurate predictions of misfit conditions and microstructure evolution, even near critical conditions, with scalability and minimal extrapolation errors.

Conclusion: The framework is generalizable and applicable beyond the studied system, potentially using experimental data to infer unknown parameters.

Abstract: We introduce a unified machine-learning framework designed to conveniently
tackle the temporal evolution of alloy microstructures under the influence of
an elastic field. This approach allows for the simultaneous extraction of
elastic parameters from a short trajectory and for the prediction of further
microstructure evolution under their influence. This is demonstrated by
focusing on spinodal decomposition in the presence of a lattice mismatch eta,
and by carrying out an extensive comparison between the ground-truth evolution
supplied by phase field simulations and the predictions of suitable
convolutional recurrent neural network architectures. The two tasks may then be
performed subsequently into a cascade framework. Under a wide spectrum of
misfit conditions, the here-presented cascade model accurately predicts eta and
the full corresponding microstructure evolution, also when approaching critical
conditions for spinodal decomposition. Scalability to larger computational
domain sizes and mild extrapolation errors in time (for time sequences five
times longer than the sampled ones during training) are demonstrated. The
proposed framework is general and can be applied beyond the specific,
prototypical system considered here as an example. Intriguingly, experimental
videos could be used to infer unknown external parameters, prior to simulating
further temporal evolution.

</details>


### [36] [Investigating CO Adsorption on Cu(111) and Rh(111) Surfaces Using Machine Learning Exchange-Correlation Functionals](https://arxiv.org/abs/2507.21789)
*Xinyuan Liang,Renxi Liu,Mohan Chen*

Main category: cond-mat.mtrl-sci

TL;DR: The paper addresses the CO adsorption puzzle in surface chemistry by using the Deep Kohn-Sham (DeePKS) method to train machine-learned exchange-correlation functionals, achieving experimental accuracy at lower computational cost.


<details>
  <summary>Details</summary>
Motivation: The persistent failure of GGA in replicating CO's experimental adsorption preferences on transition-metal surfaces motivates the need for more accurate and computationally feasible solutions.

Method: The DeePKS method is employed to train machine-learned functionals, with principal component analysis used to generalize models to multi-element systems. System-specific models for Cu(111) and Rh(111) are developed.

Result: The trained models successfully replicate experimental site preferences, with adsorption energy differences of ~10 meV compared to HSE06. A single model for both surfaces also performs comparably well.

Conclusion: The DeePKS approach offers a promising path to universal models for catalyst exploration, combining hybrid functional accuracy with reduced computational cost.

Abstract: The "CO adsorption puzzle", a persistent failure of utilizing generalized
gradient approximations (GGA) in density functional theory to replicate CO's
experimental preference for top-site adsorption on transition-metal surfaces,
remains a critical barrier in surface chemistry. While hybrid functionals such
as HSE06 partially resolve this discrepancy, their prohibitive computational
cost limits broader applications. We tackle this issue by adopting the Deep
Kohn-Sham (DeePKS) method to train machine-learned exchange-correlation
functionals. Principal component analysis reveals that the input descriptors
for electronic structures separate distinctly across different chemical
environments, enabling the DeePKS models to generalize to multi-element
systems. We train system-specific DeePKS models for transition-metal surfaces
Cu(111) and Rh(111). These models successfully recover experimental site
preferences, yielding adsorption energy differences of about 10 meV compared to
HSE06. Furthermore, a single model for the two surfaces is trained, and the
model achieves comparable accuracy in predicting not only adsorption energies
and site preference but also potential energy surfaces and relaxed surface
adsorption structures. The above work demonstrates a promising path towards
universal models, enabling catalyst exploration with hybrid functional accuracy
at substantially reduced cost.

</details>


<div id='physics.data-an'></div>

# physics.data-an [[Back]](#toc)

### [37] [Towards a Large Physics Benchmark](https://arxiv.org/abs/2507.21695)
*Kristian G. Barman,Sascha Caron,Faegheh Hasibi,Eugene Shalugin,Yoris Marcet,Johannes Otte,Henk W. de Regt,Merijn Moody*

Main category: physics.data-an

TL;DR: A benchmark framework for evaluating large language models in physics, scoring questions for correctness, difficulty, and surprise, with contributions from the community.


<details>
  <summary>Details</summary>
Motivation: To steer AI development in fundamental physics by providing a structured evaluation system based on scientific understanding and creativity.

Method: Developed a scoring system with three question types: multiple-choice, analytical problems, and open-ended tasks, evaluated by experts. Includes a living benchmark for ongoing relevance.

Result: A diverse dataset, including a machine learning challenge for high-energy physics events, with community-driven contributions.

Conclusion: The benchmark aims to guide meaningful AI contributions to physics research, inviting ongoing participation.

Abstract: We introduce a benchmark framework developed by and for the scientific
community to evaluate, monitor and steer large language model development in
fundamental physics. Building on philosophical concepts of scientific
understanding and creativity, we develop a scoring system in which each
question is scored by an expert for its correctness, difficulty, and surprise.
The questions are of three forms: (i) multiple-choice questions for conceptual
understanding, (ii) analytical problems requiring mathematical derivation, and
(iii) openended tasks requiring complex problem solving. Our current dataset
contains diverse set of examples, including a machine learning challenge to
classify high-energy physics events, such as the four top quark signal. To
ensure continued relevance, we propose a living benchmark, where physicists
contribute questions, for instance alongside new publications. We invite
contributions via: http://www.physicsbenchmarks.org/. We hope that this
benchmark will enable a targeted AI development that can make a meaningful
contribution to fundamental physics research.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [38] [Backpropagation in unstable diffusions](https://arxiv.org/abs/2507.21497)
*Angxiu Ni*

Main category: math.PR

TL;DR: The paper introduces the adjoint path-kernel method for computing parameter-gradients in SDEs, applicable even with gradient explosion, and demonstrates it on the Lorenz 96 system.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of computing parameter-gradients in SDEs, especially in cases with gradient explosion or non-hyperbolic systems with multiplicative noise.

Method: Derives the adjoint path-kernel method, extending conventional backpropagation, and develops a Monte-Carlo-type algorithm.

Result: The method is shown to work effectively, demonstrated on the 40-dimensional Lorenz 96 system.

Conclusion: The adjoint path-kernel method provides a scalable and robust solution for gradient computation in SDEs, even in challenging scenarios.

Abstract: We derive the adjoint path-kernel method for the parameter-gradient of SDEs,
where the observable is averaged at a particular time or over the stationary
measure. Its cost is almost independent of the number of parameters; it extends
the conventional backpropagation method to cases with gradient explosion. It
works for non-hyperbolic systems with multiplicative noise controlled by
parameters. We derive a Monte-Carlo-type algorithm and demonstrate it on the
40-dimensional Lorenz 96 system.

</details>


### [39] [Exponentially mixing flows with slow enhanced dissipation rates](https://arxiv.org/abs/2507.21305)
*William Cooperman,Gautam Iyer,Keefer Rowan,Seungjae Son*

Main category: math.PR

TL;DR: The paper explores the dissipation time of a passive scalar in incompressible flows, contrasting previous results with new findings for $C^0$ flows.


<details>
  <summary>Details</summary>
Motivation: To understand how the regularity of the flow affects the dissipation time of a passive scalar, especially in cases where enhanced dissipation is absent.

Method: The authors construct a family of incompressible $C^0$ flows that are exponentially mixing and analyze their dissipation time. They also estimate dissipation times for mixing flows with improved bounds.

Result: For $C^0$ flows, the dissipation time scales as $1/\kappa$, showing no enhanced dissipation, unlike $C^1$ flows. Improved bounds for mixing flows are derived.

Conclusion: The regularity of the flow significantly impacts dissipation time, with $C^0$ flows lacking enhanced dissipation despite being mixing.

Abstract: Consider a passive scalar which is advected by an incompressible flow $u$ and
has small molecular diffusivity $\kappa$. Previous results show that if $u$ is
exponentially mixing and $C^1$, then the dissipation time is $O(|\log
\kappa|^2)$. We produce a family of incompressible flows which are $C^0$ and
exponentially mixing, uniformly in $\kappa$; however have a dissipation time of
order $1/\kappa$ (i.e. exhibits no enhanced dissipation). We also estimate the
dissipation time of mixing flows, and obtain improved bounds in terms of the
mixing rate with explicit constants, and allow for a time inhomogeneous mixing
rate which is typical for random constructions of mixing flows.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [40] [Efficient Reduced Order Modeling Based on HODMD to Predict Intraventricular Flow Dynamics](https://arxiv.org/abs/2507.21716)
*Eneko Lazpita,Jesus Garicano-Mena,Soledad Le Clainche*

Main category: physics.flu-dyn

TL;DR: A reduced order model (ROM) using higher order dynamic mode decomposition (HODMD) is proposed for accurate and efficient cardiac blood flow simulation, achieving low error rates and significant computational savings.


<details>
  <summary>Details</summary>
Motivation: Accurate and efficient modeling of cardiac blood flow is essential for cardiovascular research and clinical applications, but current CFD methods are complex and costly.

Method: The study introduces a HODMD-based ROM to reconstruct and predict left ventricle flow fields, tested on two idealized ventricular geometries with distinct flow regimes.

Result: The model achieves reconstruction and prediction errors below 5-10%, with a computational speed-up of 10^5 compared to full-order simulations.

Conclusion: The HODMD-based ROM is robust and interpretable, suitable for real-time analysis and patient-specific models in cardiac hemodynamics.

Abstract: Accurate and efficient modeling of cardiac blood flow is crucial for
advancing data-driven tools in cardiovascular research and clinical
applications. Recently, the accuracy and availability of computational fluid
dynamics (CFD) methodologies for simulating intraventricular flow have
increased. However, these methods remain complex and computationally costly.
This study presents a reduced order model (ROM) based on higher order dynamic
mode decomposition (HODMD). The proposed approach enables accurate
reconstruction and long term prediction of left ventricle flow fields. The
method is tested on two idealized ventricular geometries exhibiting distinct
flow regimes to assess its robustness under different hemodynamic conditions.
By leveraging a small number of training snapshots and focusing on the dominant
periodic components representing the physics of the system, the HODMD-based
model accurately reconstructs the flow field over entire cardiac cycles and
provides reliable long-term predictions beyond the training window. The
reconstruction and prediction errors remain below 5\% for the first geometry
and below 10\% for the second, even when using as few as the first 3 cycles of
simulated data, representing the transitory regime. Additionally, the approach
reduces computational costs with a speed-up factor of at least $10^{5}$
compared to full-order simulations, enabling fast surrogate modeling of complex
cardiac flows. These results highlight the potential of spectrally-constrained
HODMD as a robust and interpretable ROM for simulating intraventricular
hemodynamics. This approach shows promise for integration in real-time analysis
and patient specific models.

</details>


<div id='math.CA'></div>

# math.CA [[Back]](#toc)

### [41] [Excess-continuous prox-regular sweeping processes](https://arxiv.org/abs/2507.21646)
*Vincenzo Recupero,Federico Stra*

Main category: math.CA

TL;DR: The paper improves uniqueness results for Moreau's sweeping processes by using the asymmetric excess distance and dropping boundary continuity assumptions.


<details>
  <summary>Details</summary>
Motivation: To generalize and improve existing results on sweeping processes by relaxing continuity requirements and using the excess distance.

Method: Analyzes Moreau's sweeping processes with prox-regular sets continuous in time under the excess distance, assuming a uniform interior cone condition.

Result: Proves unique solution existence for sweeping processes under weaker conditions than previous literature.

Conclusion: The work extends applicability of sweeping processes by allowing more general moving constraints.

Abstract: In this paper we consider the Moreau's sweeping processes driven by a time
dependent prox-regular set $C(t)$ which is continuous in time with respect to
the asymmetric distance $e$ called the excess, defined by $e(A,B) := \sup_{x
\in A} d(x,B)$ for every pair of sets $A$, $B$ in a Hilbert space. As observed
by J.J. Moreau in his pioneering works, the excess provides the natural
topological framework for sweeping process. Assuming a uniform interior cone
condition for $C(t)$, we prove that the associated sweeping process has a
unique solution, thereby improving the existing result on continuous
prox-regular sweeping processes in two directions: indeed, in the previous
literature $C(t)$ was supposed to be continuous in time with respect to the
symmetric Hausdorff distance instead of the excess and also its boundary
$\partial C(t)$ was required to be continuous in time, an assumption which we
completely drop. Therefore our result allows to consider a much wider class of
continuously moving constraints.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [42] [Distributionally Robust Shape and Topology Optimization](https://arxiv.org/abs/2507.21574)
*Charles Dapogny,Julien Prando,Boris Thibert*

Main category: math.OC

TL;DR: The paper introduces distributional robustness from convex optimization to address optimal design under uncertainty, focusing on imperfectly known parameter distributions. It tackles three problem classes using Wasserstein distance, moment-based ambiguity, and conditional value at risk, deriving tractable reformulations via convex duality.


<details>
  <summary>Details</summary>
Motivation: To handle optimal design problems where cost functions depend on uncertain parameters with imperfectly known distributions, ensuring robustness against distributional uncertainty.

Method: Uses convex duality to reformulate bilevel programs into single-level problems, addressing ambiguity sets based on Wasserstein distance, moments, and conditional value at risk.

Result: Tractable reformulations for robust optimal design problems, demonstrated through numerical examples in topology and shape optimization.

Conclusion: The proposed methods effectively address distributional uncertainty in optimal design, with broad applicability beyond the discussed examples.

Abstract: This article aims to introduce the paradigm of distributional robustness from
the field of convex optimization to tackle optimal design problems under
uncertainty. We consider realistic situations where the physical model, and
thereby the cost function of the design to be minimized depend on uncertain
parameters. The probability distribution of the latter is itself known
imperfectly, through a nominal law, reconstructed from a few observed samples.
The distributionally robust optimal design problem is an intricate bilevel
program which consists in minimizing the worst value of a statistical quantity
of the cost function (typically, its expectation) when the law of the uncertain
parameters belongs to a certain ``ambiguity set''. We address three classes of
such problems: firstly, this ambiguity set is made of the probability laws
whose Wasserstein distance to the nominal law is less than a given threshold;
secondly, the ambiguity set is based on the first- and second-order moments of
the actual and nominal probability laws. Eventually, a statistical quantity of
the cost other than its expectation is made robust with respect to the law of
the parameters, namely its conditional value at risk. Using techniques from
convex duality, we derive tractable, single-level reformulations of these
problems, framed over augmented sets of variables. Our methods are essentially
agnostic of the optimal design framework; they are described in a unifying
abstract framework, before being applied to multiple situations in
density-based topology optimization and in geometric shape optimization.
Several numerical examples are discussed in two and three space dimensions to
appraise the features of the proposed techniques.

</details>
