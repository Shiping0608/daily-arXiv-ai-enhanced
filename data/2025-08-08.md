<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 11]
- [math.AP](#math.AP) [Total: 13]
- [physics.comp-ph](#physics.comp-ph) [Total: 2]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 3]
- [cs.AI](#cs.AI) [Total: 1]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 3]
- [cs.PF](#cs.PF) [Total: 1]
- [physics.class-ph](#physics.class-ph) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [On the optimization of discrepancy measures](https://arxiv.org/abs/2508.04926)
*François Clément,Nathan Kirk,Art B. Owen,T. Konstantin Rusch*

Main category: math.NA

TL;DR: The paper introduces the average squared discrepancy as a new criterion for constructing low-discrepancy point sets in the unit cube, addressing issues with traditional $L_\infty$ and $L_2$ star discrepancies.


<details>
  <summary>Details</summary>
Motivation: The $L_\infty$ star discrepancy is computationally expensive and non-differentiable, while the $L_2$ star discrepancy, though smooth, has pathologies. The goal is to find a better criterion.

Method: Proposes the average squared discrepancy, which averages over $2^d$ versions of the $L_2$ star discrepancy anchored in the vertices of $[0,1]^d$. It is computationally efficient and equivalent to Hickernell's weighted symmetric $L_2$ criterion.

Result: The average squared discrepancy avoids the problems of traditional measures and performs well in numerical studies, optimizing it also improves $L_2$ star discrepancy performance.

Conclusion: The average squared discrepancy is a superior criterion for constructing low-discrepancy point sets, addressing the limitations of existing methods.

Abstract: Points in the unit cube with low discrepancy can be constructed using algebra
or, more recently, by direct computational optimization of a criterion. The
usual $L_\infty$ star discrepancy is a poor criterion for this because it is
computationally expensive and lacks differentiability. Its usual replacement,
the $L_2$ star discrepancy, is smooth but exhibits other pathologies shown by
J. Matou\v{s}ek. In an attempt to address these problems, we introduce the
\textit{average squared discrepancy} which averages over $2^d$ versions of the
$L_2$ star discrepancy anchored in the different vertices of $[0,1]^d$. Not
only can this criterion be computed in $O(dn^2)$ time, like the $L_2$ star
discrepancy, but also we show that it is equivalent to a weighted symmetric
$L_2$ criterion of Hickernell's by a constant factor. We compare this criterion
with a wide range of traditional discrepancy measures, and show that only the
average squared discrepancy avoids the problems raised by Matou\v{s}ek.
Furthermore, we present a comprehensive numerical study showing in particular
that optimizing for the average squared discrepancy leads to strong performance
for the $L_2$ star discrepancy, whereas the converse does not hold.

</details>


### [2] [Toroidal area-preserving parameterizations of genus-one closed surfaces](https://arxiv.org/abs/2508.05111)
*Marco Sutti,Mei-Heng Yueh*

Main category: math.NA

TL;DR: Four Riemannian geometry-based algorithms for toroidal area-preserving parameterizations of genus-one surfaces, validated numerically and applied to surface registration and texture mapping.


<details>
  <summary>Details</summary>
Motivation: To compute toroidal area-preserving parameterizations for genus-one closed surfaces, addressing challenges in surface parameterization.

Method: Proposes four algorithms: projected gradient descent, projected conjugate gradient, Riemannian gradient, and Riemannian conjugate gradient, minimizing stretch energy on a power manifold of ring tori.

Result: Numerical experiments confirm the framework's effectiveness.

Conclusion: The algorithms are successfully applied to surface registration and texture mapping, demonstrating practical utility.

Abstract: We consider the problem of computing toroidal area-preserving
parameterizations of genus-one closed surfaces. We propose four algorithms
based on Riemannian geometry: the projected gradient descent method, the
projected conjugate gradient method, the Riemannian gradient method, and the
Riemannian conjugate gradient method. Our objective function is based on the
stretch energy functional, and the minimization is constrained on a power
manifold of ring tori embedded in three-dimensional Euclidean space. Numerical
experiments on several mesh models demonstrate the effectiveness of the
proposed framework. Finally, we show how to use the proposed algorithms in the
context of surface registration and texture mapping applications.

</details>


### [3] [An asymptotic-preserving active flux scheme for the hyperbolic heat equation in the diffusive scaling](https://arxiv.org/abs/2508.05166)
*Junming Duan,Wasilij Barsukow,Christian Klingenberg*

Main category: math.NA

TL;DR: The Active Flux (AF) method, a high-order finite volume scheme, is shown to be asymptotically-preserving (AP) for the hyperbolic heat equation without modification, with its limit scheme discretizing the heat equation.


<details>
  <summary>Details</summary>
Motivation: The AF method's structure-preserving property motivates exploring its AP behavior in diffusive scaling, aiming to validate its effectiveness for hyperbolic heat equations.

Method: The method employs Jacobian Splitting (JS) for updating point values and uses formal asymptotic analysis, discrete Fourier analysis, and numerical experiments.

Result: The JS-based AF method is proven AP for the hyperbolic heat equation, with its limit scheme correctly discretizing the heat equation.

Conclusion: The AF method, unmodified, is effective and AP for hyperbolic heat equations, validated through analysis and experiments.

Abstract: The Active Flux (AF) method is a compact, high-order finite volume scheme
that enhances flexibility by introducing point values at cell interfaces as
additional degrees of freedom alongside cell averages. The method of lines is
employed here for temporal discretization. A common approach for updating point
values relies on the Jacobian Splitting (JS) method, which incorporates
upwinding. A key advantage of the AF method over standard finite volume schemes
is its structure-preserving property, motivating the investigation of its
asymptotic-preserving (AP) behavior in the diffusive scaling. We show that the
JS-based AF method without any modification is AP for solving the hyperbolic
heat equation, in the sense that the limit scheme is a discretization of the
limit heat equation. We use formal asymptotic analysis, discrete Fourier
analysis, and numerical experiments to illustrate our findings.

</details>


### [4] [An Investigation into the Distribution of Ratios of Particle Solver-based Likelihoods](https://arxiv.org/abs/2508.05303)
*Emil Løvbak,Sebastian Krumscheid*

Main category: math.NA

TL;DR: The paper explores the Metropolis-Hastings algorithm for sampling posterior distributions in Bayesian inverse problems with random likelihoods, focusing on PDE solutions and Gaussian noise.


<details>
  <summary>Details</summary>
Motivation: To understand the impact of Gaussian noise in likelihood approximations on the Metropolis-Hastings algorithm's performance.

Method: Theoretical analysis and numerical experiments on a one-dimensional diffusion PDE with Gaussian observation error and particle-based Monte Carlo likelihood approximation.

Result: Examines how Gaussian noise in likelihood evaluations affects acceptance probability ratios in the algorithm.

Conclusion: Provides insights into the behavior of the Metropolis-Hastings algorithm under noisy likelihood approximations.

Abstract: We investigate the use of the Metropolis-Hastings algorithm to sample
posterior distribution in a Bayesian inverse problem, where the likelihood
function is random. Concretely, we consider the case where one has full field
observations of a PDE solution, in case a one-dimensional diffusion equation,
subject to a Gaussian observation error. Assuming one uses a particle-based
Monte Carlo simulation when approximating the likelihood function, one gets an
approximate likelihood with additive Gaussian noise in the log-likelihood. We
study how these two Gaussian distributions affect the distribution of ratios of
approximate likelihood evaluations, as required when evaluating acceptance
probabilities in the Metropolis-Hastings algorithm. We do so through both
theoretical analysis and numerical experiments.

</details>


### [5] [A low-rank solver for the Stokes-Darcy model with random hydraulic conductivity and Beavers-Joseph condition](https://arxiv.org/abs/2508.05328)
*Yujun Zhu,Yulan Ning,Zhipeng Yang,Xiaoming He,Ju Ming*

Main category: math.NA

TL;DR: The paper introduces an efficient low-rank solver for the stochastic Stokes-Darcy model with random hydraulic conductivity, reducing computational costs while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: To address high computational costs and memory requirements in solving the stochastic Stokes-Darcy interface model with random hydraulic conductivity.

Method: Uses a generalized low-rank approximation for stiffness matrices, proposes a strategy for optimal data compression ratios, and conducts error analysis.

Result: The solver maintains high numerical precision with reduced computational and space complexities, validated by numerical experiments.

Conclusion: The proposed low-rank solver is efficient and accurate, with theoretical and experimental validation.

Abstract: This paper proposes, analyzes, and demonstrates an efficient low-rank solver
for the stochastic Stokes-Darcy interface model with a random hydraulic
conductivity both in the porous media domain and on the interface. We consider
three interface conditions with randomness, including the Beavers-Joseph
interface condition with the random hydraulic conductivity, on the interface
between the free flow and the porous media flow. Our solver employs a novel
generalized low-rank approximation of the large-scale stiffness matrices, which
can significantly cut down the computational costs and memory requirements
associated with matrix inversion without losing accuracy. Therefore, by
adopting a suitable data compression ratio, the low-rank solver can maintain a
high numerical precision with relatively low computational and space
complexities. We also propose a strategy to determine the best choice of data
compression ratios. Furthermore, we carry out the error analysis of the
generalized low-rank matrix approximation algorithm and the low-rank solver.
Finally, numerical experiments are conducted to validate the proposed
algorithms and the theoretical conclusions.

</details>


### [6] [The domain-of-dependence stabilization for cut-cell meshes is fully discretely stable](https://arxiv.org/abs/2508.05372)
*Louis Petri,Gunnar Birke,Christian Engwer,Hendrik Ranocha*

Main category: math.NA

TL;DR: The paper analyzes the stability of domain-of-dependence stabilization for hyperbolic problems, focusing on small cut cells. It demonstrates fully discrete stability under a time step restriction independent of cell size, with numerical verification.


<details>
  <summary>Details</summary>
Motivation: Address instability issues caused by small cut cells in hyperbolic problems by redistributing mass at a semi-discrete level.

Method: Conducts analysis for the linear advection model in one dimension, using operator norm estimates to derive stability conditions.

Result: Achieves fully discrete stability without dependence on arbitrarily small cells, with insights into stability mechanisms and challenges for higher-order polynomials.

Conclusion: Proposes a solution for stability challenges, verified numerically in 1D and 2D simulations, offering a feasible CFL-like condition.

Abstract: We present a fully discrete stability analysis of the domain-of-dependence
stabilization for hyperbolic problems. The method aims to address issues caused
by small cut cells by redistributing mass around the neighborhood of a small
cut cell at a semi-discrete level. Our analysis is conducted for the linear
advection model problem in one spatial dimension. We demonstrate that fully
discrete stability can be achieved under a time step restriction that does not
depend on the arbitrarily small cells, using an operator norm estimate.
Additionally, this analysis offers a detailed understanding of the stability
mechanism and highlights some challenges associated with higher-order
polynomials. We also propose a way to mitigate these issues to derive a
feasible CFL-like condition. The analytical findings, as well as the proposed
solution are verified numerically in one- and two-dimensional simulations.

</details>


### [7] [Inverse inequalities for kernel-based approximation on bounded domains and Riemannian manifolds](https://arxiv.org/abs/2508.05376)
*Zhengjie Sun,Leevan Ling*

Main category: math.NA

TL;DR: The paper extends inverse inequalities to kernel-based approximation spaces on bounded Lipschitz domains and compact Riemannian manifolds, addressing challenges not present in polynomial spaces.


<details>
  <summary>Details</summary>
Motivation: To generalize inverse inequalities, previously limited to polynomial spaces, to kernel-based trial spaces, which are less studied and more complex.

Method: For Lipschitz domains, the authors extend Bernstein inequalities to all Sobolev orders and derive Nikolskii inequalities. For Riemannian manifolds, they analyze restricted kernels from Euclidean spaces.

Result: The theory achieves the desired form of inequalities but may require slightly more kernel smoothness than the standard assumption.

Conclusion: The work successfully extends inverse inequalities to kernel-based spaces, though with potential additional smoothness requirements for kernels.

Abstract: This paper establishes inverse inequalities for kernel-based approximation
spaces defined on bounded Lipschitz domains in $\mathbb{R}^d$ and compact
Riemannian manifolds. While inverse inequalities are well-studied for
polynomial spaces, their extension to kernel-based trial spaces poses
significant challenges. For bounded Lipschitz domains, we extend prior
Bernstein inequalities, which only apply to a limited range of Sobolev orders,
to all orders on the lower bound and $L_2$ on the upper, and derive Nikolskii
inequalities that bound $L_\infty$ norms by $L_2$ norms. Our theory achieves
the desired form but may require slightly more smoothness on the kernel than
the regular $>d/2$ assumption. For compact Riemannian manifolds, we focus on
restricted kernels, which are defined as the restriction of positive definite
kernels from the ambient Euclidean space to the manifold, and prove their
counterparts.

</details>


### [8] [Randomized Krylov-Schur eigensolver with deflation](https://arxiv.org/abs/2508.05400)
*Jean-Guillaume de Damas,Laura Grigori*

Main category: math.NA

TL;DR: A novel algorithm, randomized Krylov-Schur (rKS), is introduced for solving large-scale eigenvalue problems efficiently.


<details>
  <summary>Details</summary>
Motivation: To address the need for scalable and accurate methods for computing eigenpairs in large-scale problems.

Method: Uses randomized Krylov-Schur (rKS) with sketch-orthogonalization and stable Schur reordering, plus a deflation technique for converged eigenpairs.

Result: Demonstrates scalability and accuracy in numerical experiments.

Conclusion: rKS is a practical and efficient method for solving large-scale eigenvalue problems.

Abstract: This work introduces a novel algorithm to solve large-scale eigenvalue
problems and seek a small set of eigenpairs. The method, called randomized
Krylov-Schur (rKS), has a simple implementation and benefits from fast and
efficient operations in low-dimensional spaces, such as
sketch-orthogonalization processes and stable reordering of Schur
factorizations. It also includes a practical deflation technique for converged
eigenpairs, enabling the computation of the eigenspace associated with a given
part of the spectrum. Numerical experiments are provided to demonstrate the
scalability and accuracy of the method.

</details>


### [9] [A unified framework for the analysis, numerical approximation and model reduction of linear operator equations, Part I: Well-posedness in space and time](https://arxiv.org/abs/2508.05407)
*Moritz Feuerle,Richard Löscher,Olaf Steinbach,Karsten Urban*

Main category: math.NA

TL;DR: A unified framework for constructing well-posed formulations for linear operator equations, including PDEs, using operator completion and extension.


<details>
  <summary>Details</summary>
Motivation: To provide a general approach for formulating well-posed problems for various linear operator equations, including elliptic, parabolic, and hyperbolic PDEs.

Method: Completion and extension of operators from the strong form of the problem, incorporating known weak variational forms and novel space-time variational forms.

Result: Theoretical foundation for unified numerical approximation and model reduction of parameterized linear operator equations.

Conclusion: The framework establishes a basis for future work on numerical methods and model reduction for these equations.

Abstract: We present a unified framework to construct well-posed formulations for large
classes of linear operator equations including elliptic, parabolic and
hyperbolic partial differential equations. This general approach incorporates
known weak variational formulations as well as novel space-time variational
forms of the hyperbolic wave equation. The main concept is completion and
extension of operators starting from the strong form of the problem.
  This paper lays the theoretical foundation for a unified approach towards
numerical approximation methods and also model reduction of parameterized
linear operator equations which will be the subject of the following parts.

</details>


### [10] [Learning Geometric-Aware Quadrature Rules for Functional Minimization](https://arxiv.org/abs/2508.05445)
*Costas Smaragdakis*

Main category: math.NA

TL;DR: QuadrANN, a GNN architecture, learns optimal quadrature weights for non-uniform point clouds, improving integration accuracy for PDE solvers.


<details>
  <summary>Details</summary>
Motivation: Accurate numerical integration over non-uniform point clouds is challenging for mesh-free PDE solvers, requiring adaptive and permutation-invariant methods.

Method: QuadrANN uses a deep message-passing GNN to encode local geometric features and global context, generating adaptive quadrature rules.

Result: QuadrANN reduces variance in integral estimation compared to Quasi-Monte Carlo, especially in critical areas with singularities.

Conclusion: QuadrANN enhances stability in integration, benefiting deep learning-based variational solvers for PDEs.

Abstract: Accurate numerical integration over non-uniform point clouds is a challenge
for modern mesh-free machine learning solvers for partial differential
equations (PDEs) using variational principles. While standard Monte Carlo (MC)
methods are not capable of handling a non-uniform point cloud, modern neural
network architectures can deal with permutation-invariant inputs, creating
quadrature rules for any point cloud. In this work, we introduce QuadrANN, a
Graph Neural Network (GNN) architecture designed to learn optimal quadrature
weights directly from the underlying geometry of point clouds. The design of
the model exploits a deep message-passing scheme where the initial layer
encodes rich local geometric features from absolute and relative positions as
well as an explicit local density measure. In contrast, the following layers
incorporate a global context vector. These architectural choices allow the
QuadrANN to generate a data-driven quadrature rule that is
permutation-invariant and adaptive to both local point density and the overall
domain shape. We test our methodology on a series of challenging test cases,
including integration on convex and non-convex domains and estimating the
solution of the Heat and Fokker-Planck equations. Across all the tests,
QuadrANN reduces the variance of the integral estimation compared to standard
Quasi-Monte Carlo methods by warping the point clouds to be more dense in
critical areas where the integrands present certain singularities. This
enhanced stability in critical areas of the domain at hand is critical for the
optimization of energy functionals, leading to improved deep learning-based
variational solvers.

</details>


### [11] [Numerical analysis of the stochastic Navier-Stokes equations](https://arxiv.org/abs/2508.05564)
*Dominic Breit,Andreas Prohl,Jörn Wichman*

Main category: math.NA

TL;DR: The paper surveys optimally convergent numerical methods for stochastic Stokes and Navier-Stokes equations, highlights failures of deterministic methods in stochastic settings, and proposes benchmarks for comparing new algorithms.


<details>
  <summary>Details</summary>
Motivation: To address the need for reliable numerical methods for stochastic fluid models, which differ fundamentally from deterministic ones, and to improve comparison of new schemes in terms of accuracy and complexity.

Method: Survey of existing methods, computational illustration of deterministic method failures, and proposal of a benchmark for comparing algorithms.

Result: Deterministic methods perform sub-optimally in stochastic settings; modifications addressing the probabilistic nature restore optimal performance.

Conclusion: A benchmark is proposed to better compare new algorithms, complementing theoretical studies with realistic simulations.

Abstract: The developments over the last five decades concerning numerical
discretisations of the incompressible Navier--Stokes equations have lead to
reliable tools for their approximation: those include stable methods to
properly address the incompressibility constraint, stable discretisations to
account for convection dominated problems, efficient time (splitting) methods,
and methods to tackle their nonlinear character. While these tools may
successfully be applied to reliably simulate even more complex fluid flow PDE
models, their understanding requires a fundamental revision in the case of
stochastic fluid models, which are gaining increased importance nowadays.
  This work motivates and surveys optimally convergent numerical methods for
the stochastic Stokes and Navier--Stokes equations that were obtained in the
last decades. Furtheremore, we computationally illustrate the failure of some
of those methods from the deterministic setting, if they are straight-forwardly
applied to the stochastic case. In fact, we explain why some of these
deterministic methods perform sub-optimally by highlighting crucial analytical
differences between the deterministic and stochastic equations -- and how
modifications of the deterministic methods restore their optimal performance if
they properly address the probabilistic nature of the stochastic problem.
  Next to the numerical analysis of schemes, we propose a general benchmark of
prototypic fluid flow problems driven by different types of noise to also
compare new algorithms by simulations in terms of complexities, efficiencies,
and possible limitations. The driving motivation is to reach a better
comparison of simulations for new schemes in terms of accuracy and
complexities, and to also complement theoretical performance studies for
restricted settings of data by more realistic ones.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [12] [Two-dimensional Carreau law for a quasi-newtonian fluid flow through a thin domain with a slightly rough boundary](https://arxiv.org/abs/2508.04785)
*María Anguiano,Francisco J. Suárez-Grau*

Main category: math.AP

TL;DR: The paper studies the steady-state quasi-Newtonian Stokes flow in a thin domain with a rough boundary, deriving an effective 2D Reynolds model using asymptotic techniques.


<details>
  <summary>Details</summary>
Motivation: To understand the impact of a slightly rough boundary on the flow behavior in thin domains, which is relevant for lubrication applications.

Method: Asymptotic techniques, sharp a priori estimates, compactness results, and monotonicity arguments are used to derive the limit model.

Result: An effective nonlinear two-dimensional Reynolds model is derived, incorporating the effects of the oscillating boundary.

Conclusion: The derived model is useful for applications involving lubrication regimes with rough boundaries.

Abstract: This study investigates the asymptotic behavior of the steady-state
quasi-Newtonian Stokesflow with viscosity given by the Carreau law within a
thin domain, focusing on the effects of a slightly rough boundary of the
domain. Employing asymptotic techniques with respect to the domain's thickness,
we rigorously derive the effective nonlinear two-dimensional Reynolds model
describing the fluid flow. The mathematical analysis is based on deriving the
sharp a priori estimates and proving the compactness results of the rescaled
functions together with monotonicity arguments. The resulting limit model
incorporates contributions of the oscillating boundary and thus, it could prove
useful in the applications involving this lubrication regime.

</details>


### [13] [Regularity of solutions to degenerate and singular free boundary problems with volume constraint](https://arxiv.org/abs/2508.04856)
*T. M. Nascimento,X. H. Nguyen,P. R. Stinga*

Main category: math.AP

TL;DR: Existence and regularity of solutions for degenerate/singular elliptic free boundary problems with prescribed positivity set volume.


<details>
  <summary>Details</summary>
Motivation: To address challenges in degenerate and singular elliptic free boundary problems where the volume of the solution's positivity set is fixed.

Method: Mathematical analysis and proofs to establish solution existence and regularity under given constraints.

Result: Demonstrated existence and regularity of solutions for the specified problems.

Conclusion: The study successfully resolves the posed problems, contributing to the understanding of elliptic free boundary problems with volume constraints.

Abstract: We prove existence and regularity of solutions to degenerate and singular
elliptic free boundary problems, where the volume of the positivity set of the
solution is prescribed.

</details>


### [14] [Transition from Continuous to Jumping Solutions in 2D Quasi-static Elastic Contact Problems with Coulomb Friction: the Mathematics Underlying the Onset of Brake Squeal](https://arxiv.org/abs/2508.04863)
*Patrick Ballard,Flaviana Iurlano*

Main category: math.AP

TL;DR: The paper analyzes the quasi-static elastic contact problem with Coulomb friction, proving existence of solutions under optimal conditions and highlighting spontaneous jumps as indicators of dynamic transitions.


<details>
  <summary>Details</summary>
Motivation: To understand the conditions for existence of solutions in quasi-static elastic contact problems with Coulomb friction and explore the transition to dynamic behavior.

Method: Formulates the problem in a general setting, uses ideas from prior work to derive optimal friction conditions, and provides examples of solution jumps.

Result: Proves existence of absolutely continuous solutions under optimal friction conditions; shows jumps occur when conditions are violated, indicating dynamic transitions.

Conclusion: Spontaneous jumps in solutions reveal a shift from quasi-static to dynamic behavior, modeling the onset of friction-induced vibrations.

Abstract: We formulate the quasi-static elastic contact problem with Coulomb friction
in a very general setting, with possible jumps in time for both the load and
the solution. Exploiting ideas originating in our recent paper [4], we exhibit
an optimal condition on the magnitude of the friction coefficient under which
we prove the existence of an absolutely continuous solution for arbitrary
absolutely continuous loads in the case of the most general 2D problem. We
provide examples showing that, when the condition is violated, spontaneous
jumps in time of the solution may occur, even when the load varies absolutely
continuously in time. We argue that these spontaneous jumps in time of the
solution in the quasi-static problem reveal a transition of the process from a
quasi-static nature to a dynamic nature, interpreted as the mathematical
signature of the onset of friction-induced vibrations in the elastodynamic
contact problem with dry friction.

</details>


### [15] [Parabolic abstract evolution equations in cylindrical domains and uniformly local Sobolev spaces](https://arxiv.org/abs/2508.05220)
*Joly Romain*

Main category: math.AP

TL;DR: The paper studies parabolic equations in transverse Hilbert spaces, focusing on solutions with infinite energy and the Cauchy problem in uniformly local spaces. It addresses ill-posedness in weak versions and well-posedness in stronger versions, using abstract evolution theory.


<details>
  <summary>Details</summary>
Motivation: To explore the behavior of parabolic equations in transverse Hilbert spaces, particularly for infinite-energy solutions, and to compare weak and strong versions of uniformly local spaces.

Method: The authors analyze the linear operator ∂²ₓₓ - B in uniformly local Lebesgue spaces, noting its non-sectorial nature due to non-density of its domain. They employ abstract evolution equation theory to establish well-posedness.

Result: The Cauchy problem is shown to be well-posed in the weak version of uniformly local spaces using abstract evolution theory, despite the operator's non-sectorial nature.

Conclusion: The paper provides insights into the comparison of uniformly local space versions and introduces a new example of differential operators with non-dense domains.

Abstract: In this article, we consider parabolic equations of the type $$\partial_t
u(x,t)=\Delta u(x,t) - Bu(x,t) + F(u(x,t))$$ where $u$ is valued in a
transverse Hilbert space $Y$ and $B$ is a positive self-adjoint operator on
$Y$, allowing a different diffusion mechanism in the transverse direction. We
aim at considering solutions with infinite energy and we study the Cauchy
problem in the uniformly local spaces associated with the norm
$$\|u\|_{L^2_{\text{ul}}(\mathbb{R},Y)}= \sup_{a\in\mathbb{R}^d}
\|u(x)\|_{L^2(B(a,1),Y)}.$$ For the classical parabolic equation, i.e. if
$Y=\mathbb{R}$, it is known that the Cauchy problem is ill-posed in the weak
version of the uniformly local spaces but well-posed in a stronger version,
where additional uniform continuity is required. In this paper, we show that
the linear operator $\partial^2_{xx} - B$ is not necessarily a sectorial
operator in any version of the uniformly local Lebesgue space, due to the
possible non-density of its domain. Then, we use the theory of parabolic
abstract evolution equations to set a well-posed Cauchy problem, even in the
weak version of the uniformly local space. In particular, we believe that this
paper offers a new perspective on the comparison between both versions of the
uniformly local spaces and also provides a new natural example of differential
operators with non-dense domain.

</details>


### [16] [Isometric Immersions and Weak Solutions to the Darboux Equation](https://arxiv.org/abs/2508.05230)
*Wentao Cao,Jonas Hirsch,Dominik Inauen*

Main category: math.AP

TL;DR: The paper extends weak solutions for the Darboux equation to $C^{1,	heta}$ with $\theta>1/2$, maintaining its link to isometric immersions in $\mathbb{R}^3$.


<details>
  <summary>Details</summary>
Motivation: To address the Darboux equation in low-regularity settings, bridging gaps in understanding isometric immersions of Riemannian manifolds.

Method: Introduces weak solutions for $C^{1,	heta}$ metrics, extends flatness criteria via weak Gaussian curvature analysis.

Result: Classical correspondence between Darboux solutions and isometric immersions holds in low-regularity.

Conclusion: The work generalizes the Darboux equation's applicability, validating its role in low-regularity scenarios.

Abstract: We study the Darboux equation, a fundamental PDE arising in the theory of
isometric immersions of two-dimensional Riemannian manifolds into
$\mathbb{R}^3$, in the low-regularity regime. We introduce a notion of weak
solution for $u\in C^{1,\theta}$ with $\theta>1/2$, and show that the classical
correspondence between solutions of the Darboux equation and isometric
immersions remains valid in this regime. The key ingredient is an extension of
the classical flatness criterion to H\"older continuous metrics, achieved via
an analysis of a weak notion of Gaussian curvature.

</details>


### [17] [A viscosity solution as a piecewise classical solution to a free boundary problem for the optimal switching problem with simultaneous multiple switches](https://arxiv.org/abs/2508.05252)
*Kiyoshi Suzuki*

Main category: math.AP

TL;DR: The paper proves the uniqueness of the viscosity solution for an optimal switching problem, identifies connected regions, and converts the problem into free boundary problems. Under certain assumptions, it constructs the solution as piecewise classical solutions and verifies it as the viscosity solution.


<details>
  <summary>Details</summary>
Motivation: To address the lack of exact identification of switching regions in the infinite horizon optimal switching problem and provide a method to compute free boundaries and solutions.

Method: Converts the problem into a system of free boundary problems, proves smooth pasting conditions, and constructs the viscosity solution as piecewise classical solutions. Numerical and analytical methods are used.

Result: The series of piecewise classical solutions is verified as the viscosity solution, and explicit solutions with identified regions are computed using Python.

Conclusion: The method successfully identifies switching regions and provides explicit solutions, demonstrating practical applicability through computational implementation.

Abstract: \citeN{suzuki2020optimal} proves the uniqueness of the viscosity solution to
a variational inequality which is solved by the value function of the infinite
horizon optimal switching problem with simultaneous multiple switchings.
Although it also identifies each connected region possibly including at most
one connected switching region, the exact switching regions of the solution are
not identified. The problem is finally converted into a system of free boundary
problems and generally solved by the numerical calculation. However, if the PDE
part of the variational inequality has a classical solution, the viscosity
solution may be constructed as a series of piecewise classical solutions,
possibly analytical.
  Under a certain assumption we prove that the series of piecewise classical
solutions is indeed the viscosity solution on $\real{}$, after we prove the
smooth pasting condition is its necessary condition, and establish the
algorithm to compute all the free boundaries. Applying the results to the
concrete problem studied in \citeN{suzuki2020optimal} we find the explicit
solution and identify the continuation and switching regions in a computer with
Python programs.

</details>


### [18] [Existence of spiral strategies for blocking fire spreading](https://arxiv.org/abs/2508.05324)
*Stefano Bianchini,Martina Zizza*

Main category: math.AP

TL;DR: The paper proves a sharp version of Bressan's Fire Conjecture, showing that spiral-like barriers can confine a fire if the construction speed exceeds a critical value, otherwise they cannot.


<details>
  <summary>Details</summary>
Motivation: To determine the effectiveness of spiral-like barriers in blocking fire spread when constructed at finite speeds, addressing a gap in existing research.

Method: The proof involves defining spiral barriers, analyzing them as Retarded Differential Equations (RDEs), reformulating the problem as a functional minimization, constructing optimal spirals, and using a homotopy argument. Numerical evaluation is used due to complexity.

Result: A spiral barrier confines fire if the construction speed σ > 2.614; otherwise, it fails. This confirms the conjecture for spiral-like barriers.

Conclusion: The critical speed σ = 2.614 is the threshold for effective fire confinement using spiral barriers, validated through theoretical and numerical analysis.

Abstract: In this paper we address the problem for blocking fire by constructing a wall
$\zeta$ whose shape is spiral-like. This is supposed to be the best strategy
when a single firefighter is constructing the wall with a finite construction
speed $\sigma$: the barriers which satisfy this bound on the construction speed
are called admissible.
  We prove a sharp version of Bressan's Fire Conjecture in this case, i.e. when
admissible barriers are spiral-like curves: namely, there exists a spiral-like
barrier confining the fire in a bounded region of $\mathbb R^2$ if and only if
the speed of construction of the barrier $\sigma$ is strictly larger than a
critical speed $\bar \sigma = 2.614...$.
  The existence of confining spiral barriers for $\sigma > \bar \sigma$ is
already known [Bressan A. et al., 2008, Klein R. et al., 2019], while we
concentrate on the negative side, i.e. if $\sigma \leq \bar \sigma$ no
admissible spiral blocks the fire.
  The proof of these results relies on: 1) the precise definition of spiral
barrier and its representation; 2) the analysis of saturated spiral barriers as
a Retarded Differential Equation (RDE) in the spirit of [Klein R. et al.,
2019]; 3) the equivalent reformulation of the conjecture as a minimum problem
for a prescribed functional; 4) the construction of the optimal closing spiral;
5) the analysis of a differentiable path of admissible spirals along which the
functional is differentiable, and in particular increasing when moving from the
optimal spiral to any other one (homotopy argument).
  Due to the complexity of the solution, the evaluation of the quantities
needed to prove that the functional is increasing is performed numerically.

</details>


### [19] [Geometrical characterizations of radiating and non-radiating elastic sources and mediums with applications](https://arxiv.org/abs/2508.05401)
*Huaian Diao,Xiaoxu Fei,Hongyu Liu*

Main category: math.AP

TL;DR: The paper studies two types of time-harmonic elastic wave scattering problems, deriving quantitative results about scatterer properties and establishing uniqueness results for source or medium identification from far-field measurements.


<details>
  <summary>Details</summary>
Motivation: To understand the geometrical properties of scatterers and their impact on elastic wave scattering, aiming to provide qualitative and quantitative insights for practical applications.

Method: Uses Helmholtz decomposition, Lamé operator estimates, global energy estimates, and combines CGO solutions with local regularity estimates for microlocal analysis.

Result: Shows scatterers with small support or high-curvature boundaries radiate at any frequency, and establishes local/global uniqueness for scatterer identification.

Conclusion: The findings provide new geometric insights and tools for analyzing elastic wave scattering, with implications for inverse problems and material characterization.

Abstract: In this paper, we investigate two types of time-harmonic elastic wave
scattering problems. The first one involves the scattered wave generated by an
active elastic source with compact support. The second one concerns elastic
wave scattering caused by an inhomogeneous medium, also with compact support.
We derive several novel quantitative results concerning the geometrical
properties of the underlying scatterer, the associated source or incident wave
field, and the physical parameters. In particular, we show that a scatterer
with either a small support or high-curvature boundary points must radiate at
any frequency. These qualitative characterizations allow us to establish
several local and global uniqueness results for determining the support of the
source or medium scatterer from a single far-field measurement. Furthermore, we
reveal new geometric properties of elastic transmission eigenfunctions. To
derive a quantitative relationship between the intensity of a radiating or
non-radiating source and the diameter of its support, we utilize the Helmholtz
decomposition, the translation-invariant $L^2$-norm estimate for the Lam\'e
operator, and global energy estimates. Another pivotal technical approach
combines complex geometric optics (CGO) solutions with local regularity
estimates, facilitating microlocal analysis near admissible $K$-curvature
boundary points.

</details>


### [20] [Modulation of the Monokinetic Limit for Models of Collective Dynamics](https://arxiv.org/abs/2508.05478)
*Alina Chertock,Roman Shvydkoy,Trevor Teolis*

Main category: math.AP

TL;DR: Analysis of monokinetic limits from the kinetic Cucker-Smale model to the pressureless Euler alignment system, showing convergence to Gaussian and explicit transport profiles.


<details>
  <summary>Details</summary>
Motivation: To understand the transition from kinetic models to macroscopic systems under different regimes.

Method: Modulation analysis in two regimes: strong Fokker-Planck force with vanishing noise and Knudsen number, and a pure noiseless Vlasov scheme.

Result: Convergence to Gaussian distribution in the first regime and to a profile satisfying an explicit transport equation in the second.

Conclusion: The study provides insights into the behavior of monokinetic limits under varying conditions.

Abstract: In this work, we perform modulation analysis of monokinetic limits from the
kinetic Cucker- Smale model to the pressureless Euler alignment system. Two
regimes are considered -- a strong Fokker- Planck force with vanishing noise
and Knudsen number, and a pure noiseless Vlasov scheme. In the former case, we
demonstrate convergence of the modulated profile to the standard Gaussian
distribution, while in the latter case, the distribution converges to a profile
satisfying an explicit transport equation along limiting characteristics.

</details>


### [21] [Velocity optimization of self-equilibrated obstacles in a two-dimensional viscous flow](https://arxiv.org/abs/2508.05481)
*Gilles A. Francfort,Alessandro Giacomini,Scott Weady*

Main category: math.AP

TL;DR: The paper studies self-equilibration of an obstacle in a 2D fluid under steady-state flow and optimizes its velocity by varying shape, using measure theory for general cases.


<details>
  <summary>Details</summary>
Motivation: To understand and optimize the motion of obstacles in fluid dynamics under steady-state conditions, especially for irregular shapes.

Method: Analyzes self-equilibration conditions for obstacles in Stokes and Navier-Stokes fluids, using measure theory to handle general shape variations.

Result: Develops a framework for optimizing obstacle velocity by shape variation, applicable to a broad class of irregular obstacles.

Conclusion: The approach extends fluid dynamics analysis to irregular obstacles, enabling optimization of motion through shape design.

Abstract: An obstacle is immersed in an externally driven 2D Stokes or Navier-Stokes
fluid. We study the self-equilibration conditions for that obstacle under
steady state assumptions on the flow. We then seek to optimize the
translational and/or angular velocity of the obstacle by varying its shape. To
allow general variations, we must consider a very large class of obstacles for
which the notion of trace is meaningless. This forces us to revisit the notion
of self-equilibration for both Stokes and Navier-Stokes in a measure theoretic
environment.

</details>


### [22] [The $L^p$ boundedness of wave operators for the Laplace operator with finite rank perturbations](https://arxiv.org/abs/2508.05533)
*Han Cheng,Shanlin Huang,Avy Soffer,Zhao Wu*

Main category: math.AP

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper investigates the $L^p$ boundedness of wave operators for the
Laplace operator with finite rank perturbations \begin{equation*}
  H=-\Delta+\sum\limits_{i=1}^N\langle\cdot\,, \varphi_i\rangle \varphi_i
\qquad \mbox{on}\,\,\, \R^d. \end{equation*} For dimensions $d\ge 3$, we prove
that the wave operators $W_\pm(H,H_0)$ are bounded on $L^p$ for the full range
$1\le p\le \infty$. This extends the work of Nier and the third author
\cite{NS} by resolving the previously unexplored question of boundedness at the
endpoint cases $p=1$ and $p=\infty$. In lower dimensions $d = 1, 2$, we
establish the $L^p$-boundedness of the wave operators for the first time.
Furthermore, we reveal an intriguing dichotomy in the endpoint case $p = 1$:
\begin{itemize}
  \item If $\int_{\mathbb{R}^d} \varphi_i(x) \, \d x = 0$ holds for every $1\le
i\le N$, then the wave operators are bounded on $L^p(\mathbb{R}^d)$ for all $1
\leq p \leq \infty$.
  \item If there exists at least one $i$ ($1\le i\le N$) such that
$\int_{\mathbb{R}^d}\varphi_i(x)\d x\ne0$, then the wave operators remain
bounded for $1 < p < \infty$ and satisfy weak type $(1,1)$ estimates, but fail
to be bounded on $L^1(\mathbb{R}^d)$. \end{itemize}

</details>


### [23] [On a general class of free boundary Monge-Ampère equations](https://arxiv.org/abs/2508.05551)
*Tristan C. Collins,Benjy Firester*

Main category: math.AP

TL;DR: The paper addresses a class of free boundary Monge-Ampère equations, with applications in optimal transport, eigenvalue problems, and geometric challenges.


<details>
  <summary>Details</summary>
Motivation: To solve complex Monge-Ampère equations with free boundaries, extending their utility to diverse mathematical and geometric problems.

Method: Generalizes the Monge-Ampère equation with specific conditions and applies it to problems like optimal transport and eigenvalue analysis.

Result: Provides solutions to the equations and demonstrates their applicability in various mathematical and geometric contexts.

Conclusion: The work advances the understanding and application of Monge-Ampère equations in theoretical and applied mathematics.

Abstract: We solve a general class of free boundary Monge-Amp\`ere equations given by
\[
  \det D^2u = \lambda \dfrac{f(-u)}{g(u^\star)h(\nabla u)}\chi_{\{u<0\}} \;
\text{ in } \mathbb{R}^n, \quad \nabla u (\mathbb{R}^n) = P \] where $P$ is a
bounded convex set containing the origin, and $h>0$ on $P$. We consider
applications to optimal transport with degenerate densities, Monge-Amp\`ere
eigenvalue problems, and geometric problems including a hemispherical Minkowski
problem and free boundary K\"ahler-Ricci solitons on toric Fano manifolds.

</details>


### [24] [Bounds for spectral projectors on the three-dimensional torus](https://arxiv.org/abs/2508.05573)
*Pierre Germain,Simon L. Rydin Myerson,Daniel Pezzi*

Main category: math.AP

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study $L^2$ to $L^p$ operator norms of spectral projectors for the
Euclidean Laplacian on the torus in the case where the spectral window is
narrow. With a window of constant size this is a classical result of Sogge; in
the small-window limit we are left with $L^p$ norms of eigenfunctions of the
Laplacian, as considered for instance by Bourgain. For the three-dimensional
torus we prove new cases of a previous conjecture of the first two authors
concerning the size of these norms; we also refine certain prior results to
remove $\epsilon$-losses in all dimensions. We use methods from number theory:
the geometry of numbers, the circle method and exponential sum bounds due to
Guo. We complement these techniques with height splitting and a bilinear
argument to prove sharp results.
  We exposit on the various techniques used and their limitations.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [25] [Hyperbolic tiling neighborhoods in O(1) time](https://arxiv.org/abs/2508.04765)
*Yanick Thurn,Manuel Schrauth,Johanna Erdmenger*

Main category: physics.comp-ph

TL;DR: A novel method for constructing hyperbolic tilings and their adjacency graphs simultaneously, without explicit coordinates, enabling large-scale simulations.


<details>
  <summary>Details</summary>
Motivation: Current recursive methods for hyperbolic tilings lack efficient adjacency graph generation, limiting applications.

Method: Combinatoric rules are used to build tilings and graphs simultaneously, avoiding explicit coordinates.

Result: The approach generates exact, scalable hyperbolic graphs with size-independent complexity, outperforming existing methods.

Conclusion: This method enables large-scale simulations and is implemented for practical use.

Abstract: Tilings of the hyperbolic plane are of significant interest among many
branches of mathematics, physics and computer science. Yet, their construction
remains a non-trivial task. Current approaches primarily use tree-based
recursive algorithms, which are fundamentally limited: they do not readily
yield the neighborhood graph representing cell adjacencies, which is however
required for many applications. We introduce a novel approach that allows to
build hyperbolic tilings and their associated graph structure simultaneously,
using only combinatoric rules without requiring an explicit coordinate
representation. This allows to generate arbitrarily large, exact hyperbolic
graphs, with an algorithmic complexity that does not depend on the lattice
size. We provide an easy-to-use implementation which substantially outperforms
existing methods, hence rendering ultra large-scale numerical simulations on
these geometric structures accessible for the scientific community.

</details>


### [26] [Identifying Optimal Regression Models For DEM Simulation Datasets](https://arxiv.org/abs/2508.05308)
*B. D. Jenkins,A. L. Nicusan,A. Neveu,G. Lumay,F. Francqui,J. P. K. Seville,C. R. K. Windows-Yule*

Main category: physics.comp-ph

TL;DR: The paper proposes a framework using k-fold cross-validation to benchmark regression models for DEM data, identifying a histogram-based gradient boosting model as optimal among 16 tested.


<details>
  <summary>Details</summary>
Motivation: To address the lack of benchmarking in particle technology for regression tasks, which leads to suboptimal models and hinders real-time decision-making.

Method: Applied k-fold cross-validation to evaluate regression models on a DEM dataset of packing fractions with varying inter-particle properties.

Result: A histogram-based gradient boosting model was optimal, offering good predictive accuracy and acceptable training/inference times.

Conclusion: The proposed framework helps select the best regression model for DEM data, improving real-time evaluations and process optimization.

Abstract: Developing fast regression models (surrogate/metamodels) from DEM data is key
for practical industrial application to allow real-time evaluations. However,
benchmarking different models is often overlooked in particle technology for
regression tasks, as model selection is frequently not the primary research
focus. This can lead to the use of suboptimal models, resulting in subpar
predictive accuracy, slow evaluations, or poor generalisation, hindering
effective real-time decision-making and process optimisation. In this work, we
discuss applying k-fold cross-validation to assess regression models for
tabular DEM datasets and propose a simple framework for readers to follow to
find the optimal model for their data. An example demonstrates its application
to a DEM dataset of packing fractions measured in a simple measuring beaker
with varying inter-particle properties, namely, average particle diameter,
coefficient of restitution, coefficient of sliding friction, coefficient of
rolling resistance, and cohesive energy density. Out of 16 different models
tested, a histogram-based gradient boosting model was found to be optimal,
providing a good fit with acceptable training and inference times.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [27] [Electrodeless Magnetohydrodynamic Local Force Generator for Aerocapture](https://arxiv.org/abs/2508.04806)
*Bernard Parent,Felipe Martin Rodriguez Fuentes,Spencer LaFoley*

Main category: physics.plasm-ph

TL;DR: A novel electrodeless MHD system for planetary entry aerocapture is proposed, outperforming previous methods by generating stronger forces without electrodes.


<details>
  <summary>Details</summary>
Motivation: To improve planetary entry aerocapture by addressing limitations of existing MHD systems, which are either global or require electrodes.

Method: The system uses two magnets to create a Faraday EMF, eliminating electrodes. Simulations were conducted using CFDWARP, a CFD code coupling aerodynamics, MHD, and plasma effects.

Result: The electrodeless system generates forces several times greater than two-electrode systems at the same magnetic field strength.

Conclusion: The proposed MHD system is a promising advancement for planetary entry aerocapture, offering superior performance without electrodes.

Abstract: This paper presents a novel magnetohydrodynamics (MHD) system for planetary
entry aerocapture. The system is advantaged over previous approaches by having
the following two characteristics: (i) it can be deployed locally to one or
various flow regions, and (ii) it does not make use of electrodes. Previous MHD
systems for planetary entry were either electrodeless global systems or
two-electrode local systems. The proposed novel MHD system employs two magnets
to establish a current loop resulting in a Faraday electromotive force (EMF).
The first magnet is positioned to ensure the magnetic field faces outward from
the shell, while the second magnet is oriented to ensure the magnetic field
faces inward toward the shell. Preliminary findings demonstrate that when
located on the surface of an Earth entry capsule at a flight Mach number of 35,
the novel electrodeless MHD system can generate forces several times greater
than a two-electrode system while utilizing the same magnetic field strength.
The study is conducted entirely through numerical simulation using CFDWARP, a
computational fluid dynamics (CFD) code that employs advanced numerical methods
allowing for the full coupling between aerodynamics, magnetohydrodynamics, and
non-neutral plasma sheaths. The physical model includes an 11-species
finite-rate chemical solver including real gas effects, the drift-diffusion
model for all charged species, along with an electric field potential equation
that satisfies Gauss's law.

</details>


### [28] [Magnetic shear effects on ballooning turbulence in the boundary of fusion devices](https://arxiv.org/abs/2508.04881)
*Z. Tecchiolli,A. J. Coelho,J. Loizu,B. De Lucca,P. Ricci*

Main category: physics.plasm-ph

TL;DR: The paper investigates how magnetic shear affects ballooning-driven plasma edge turbulence, showing differences in turbulent eddy scales between high and low shear conditions.


<details>
  <summary>Details</summary>
Motivation: To understand the impact of magnetic shear on turbulence in plasma edges, which is crucial for predicting pressure gradients and transport properties.

Method: Nonlinear 3D simulations using the GBS code, complemented by linear numerical and analytical studies of ballooning modes.

Result: High magnetic shear leads to scale separation (kx << ky), while low shear results in kx ~ ky, aligning with recent stellarator simulations. Linear studies confirm the transition in mode structure with shear.

Conclusion: Magnetic shear significantly influences turbulence properties, with high shear enhancing poloidal mode coupling and scale separation, validated by analytical and numerical results.

Abstract: The effect of magnetic shear on ballooning-driven plasma edge turbulence is
studied through nonlinear simulations complemented by linear numerical and
analytical investigations. Nonlinear, 3D, global, flux-driven simulations using
the GBS code show that the scale separation between radial, x, and poloidal, y,
size of turbulent eddies, kx << ky , considered by Ricci et al. (2008) and
extensively used to predict pressure gradient lengths, SOL width, particle and
heat fluxes, is observed with high magnetic shear. In contrast, for low
magnetic shear, kx ~ ky is observed, with fluctuation properties resembling
those shown by recent low-shear stellarator simulations reported in Coelho et
al. (2024a). Global linear investigations of the ballooning mode qualitatively
captures the transition in mode structure with varying magnetic shear, showing
that kx << ky is achieved with sufficiently strong poloidal mode coupling
enhanced by increasing magnetic shear, resistivity, toroidal mode number, and
equilibrium gradient scale length. This is confirmed by an analytical study
considering a dominant poloidal mode and its sidebands, which highlights that
the poloidal mode structure is determined by curvature and k parallel effects

</details>


### [29] [Information Propagation in Predator-Prey Dynamics of Turbulent Plasma](https://arxiv.org/abs/2508.05127)
*Tomohiro Tanogami,Makoto Sasaki,Tatsuya Kobayashi*

Main category: physics.plasm-ph

TL;DR: The paper explores cyclic oscillations in fusion plasmas using a stochastic predator-prey model, suggesting observed oscillations may be quasi-cycles, not limit cycles, and demonstrates information flow from zonal flow to turbulence.


<details>
  <summary>Details</summary>
Motivation: To understand the self-regulating interaction between drift-wave turbulence and zonal flow in magnetically confined fusion plasmas and its causality.

Method: Construct a simple stochastic predator-prey model incorporating intrinsic fluctuations and analyze its statistical properties using information theory.

Result: The model shows quasi-cycles due to noise amplification, suggesting observed plasma oscillations may be quasi-cycles. Information flows from zonal flow to turbulence.

Conclusion: The analysis provides a theoretical basis for turbulence regulation by controlling zonal flow, with quasi-cycles potentially common under various conditions.

Abstract: Magnetically confined fusion plasmas exhibit predator-prey-like cyclic
oscillations through the self-regulating interaction between drift-wave
turbulence and zonal flow. To elucidate the detailed mechanism and causality
underlying this phenomenon, we construct a simple stochastic predator-prey
model that incorporates intrinsic fluctuations and analyze its statistical
properties from an information-theoretic perspective. We first show that the
model exhibits persistent fluctuating cyclic oscillations called quasi-cycles
due to amplification of intrinsic noise. This result suggests the possibility
that the previously observed periodic oscillations in a toroidal plasma are not
limit cycles but quasi-cycles, and that such quasi-cycles may be widely
observed under various conditions. For this model, we further prove that
information of zonal flow is propagated to turbulence. This
information-theoretic analysis may provide a theoretical basis for regulating
turbulence by controlling zonal flow.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [30] [The Missing Reward: Active Inference in the Era of Experience](https://arxiv.org/abs/2508.05619)
*Bo Wen*

Main category: cs.AI

TL;DR: Active Inference (AIF) can enable autonomous AI agents to learn from experience without human reward engineering, addressing scalability challenges in current AI paradigms.


<details>
  <summary>Details</summary>
Motivation: Current AI systems rely heavily on human-engineered rewards, creating scalability issues and hindering autonomous intelligence.

Method: Proposes using AIF to replace external rewards with intrinsic free energy minimization, integrating Large Language Models as generative world models.

Result: AIF offers a framework for agents to autonomously balance exploration and exploitation, aligning with human values.

Conclusion: AIF provides a promising path for developing autonomous, scalable AI systems that learn efficiently from experience.

Abstract: This paper argues that Active Inference (AIF) provides a crucial foundation
for developing autonomous AI agents capable of learning from experience without
continuous human reward engineering. As AI systems begin to exhaust
high-quality training data and rely on increasingly large human workforces for
reward design, the current paradigm faces significant scalability challenges
that could impede progress toward genuinely autonomous intelligence. The
proposal for an ``Era of Experience,'' where agents learn from self-generated
data, is a promising step forward. However, this vision still depends on
extensive human engineering of reward functions, effectively shifting the
bottleneck from data curation to reward curation. This highlights what we
identify as the \textbf{grounded-agency gap}: the inability of contemporary AI
systems to autonomously formulate, adapt, and pursue objectives in response to
changing circumstances. We propose that AIF can bridge this gap by replacing
external reward signals with an intrinsic drive to minimize free energy,
allowing agents to naturally balance exploration and exploitation through a
unified Bayesian objective. By integrating Large Language Models as generative
world models with AIF's principled decision-making framework, we can create
agents that learn efficiently from experience while remaining aligned with
human values. This synthesis offers a compelling path toward AI systems that
can develop autonomously while adhering to both computational and physical
constraints.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [31] [Constitutive modeling of viscoelastic solids at large strains based on the theory of evolving natural configurations](https://arxiv.org/abs/2508.05043)
*Tarun Singh,Sandipan Paul*

Main category: cond-mat.soft

TL;DR: The paper revisits nonlinear viscoelastic models using evolving natural configurations theory, focusing on Maxwell and Kelvin-Voigt types, and introduces Lagrangian-based models for Zener and Poynting-Thompson solids. It highlights the need for stress-space formulation for Kelvin-Voigt materials and validates models with experimental data.


<details>
  <summary>Details</summary>
Motivation: To improve modeling of dissipative processes in viscoelastic solids by leveraging the theory of evolving natural configurations, addressing gaps in existing nonlinear constitutive models.

Method: Uses Lagrangian framework to model Maxwell and Kelvin-Voigt materials, with strain-space and stress-space formulations respectively. Develops standard solid models (Zener, Poynting-Thompson) and numerical algorithms for boundary value problems.

Result: Demonstrates that derived models match experimental data well, especially the Poynting-Thompson model in polymer stretching. Studies relaxation and rate effects.

Conclusion: The proposed models effectively capture viscoelastic behavior, with the Poynting-Thompson model showing strong experimental validation, advancing the field of dissipative process modeling.

Abstract: The theory of evolving natural configurations is an effective technique to
model dissipative processes. In this paper, we use this theory to revisit
nonlinear constitutive models of viscoelastic solids. Particularly, a Maxwell
and a Kelvin-Voigt model and their associated standard solids, viz., a Zener
and a Poynting-Thompson solids respectively, have been modeled within a
Lagrangian framework. We show that while a strain-space formulation of the
evolving natural configurations is useful in modeling Maxwell-type materials, a
stress-space formulation that incorporates a rate of dissipation function in
terms of the relevant configurational forces is required for modeling the
Kelvin-Voigt type materials. Furthermore, we also show that the basic Maxwell
and Kelvin-Voigt models can be obtained as limiting cases from the derived
standard solid models. Integration algorithms for the proposed models have been
developed and numerical solutions for a relevant boundary value problem are
obtained. The response of the developed models have been compared and
benchmarked with experimental data. Specifically, the response of the novel
Poynting-Thompson model is studied in details. This model shows a very good
match with the existing experimental data obtained from a uniaxial stretching
of polymers over a large extent of strain. The relaxation behavior and rate
effects for the developed models have been studied.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [32] [The use of open boundaries in stochastic hydrodynamic models of nucleation](https://arxiv.org/abs/2508.05528)
*James F. Lutsko*

Main category: cond-mat.stat-mech

TL;DR: The paper explores using open boundary conditions in stochastic hydrodynamics to better align computational models with experimental conditions, addressing challenges like preserving fluctuation-dissipation relations and dynamic stationary points.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between computational limitations and experimental conditions in modeling first-order phase transitions, particularly crystallization, using stochastic hydrodynamics.

Method: Proposes the use of open boundary conditions in stochastic hydrodynamics, analyzing their impact on the model's properties like fluctuation-dissipation relations and dynamic stability.

Result: Open boundary conditions bring computational models closer to experimental conditions but introduce new challenges regarding model consistency and dynamics.

Conclusion: Open boundary conditions are a promising approach for realistic modeling, though further investigation is needed to address their theoretical and practical implications.

Abstract: Stochastic hydrodynamics is a central tool in the study of first order phase
transitions at a fundamental level. Combined with sophisticated free energy
models, e.g. as developed in classical Density Functional Theory, complex
processes such as crystallization can be modeled and information such as free
energy barriers, nucleation pathways and the unstable eigenvector and
eigenvalues determined. The latter are particularly interesting as they play
key roles in defining the natural (unbiased) order parameter and the nucleation
rate respectively. As is often the case, computational realities restrict the
size of system that can be modeled and this makes it difficult to achieve
experimental conditions for which the volume is effectively infinite. In this
paper, the use of open boundary conditions is discussed. By using an open
system, the calculations become much closer to experimental conditions however,
the introduction of open boundary conditions raises a number of questions
concerning the stochastic model such as whether the fluctuation-dissipation
relation is preserved and whether stationary points on the free energy surface
remain stationary points of the dynamics.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [33] [Model-based framework for automated quantification of error sources in quantum state tomography](https://arxiv.org/abs/2508.05538)
*Junpei Oba,Hsin-Pin Lo,Yasuhiro Yamada,Takayuki Matsui,Takuya Ikuta,Yuya Yonezu,Toshimori Honjo,Seiji Kajita,Hiroki Takesue*

Main category: quant-ph

TL;DR: Proposes an automated method to quantify error sources in quantum state generation by combining simulation and parameter optimization, improving state quality.


<details>
  <summary>Details</summary>
Motivation: High-quality quantum states are crucial for quantum applications, but error sources degrade quality. Quantum state tomography (QST) fails to identify individual errors.

Method: Combines simulation and parameter optimization to model error sources and reproduce experimental density matrices, focusing on time-bin entangled photon pairs.

Result: Optimization reduced trace distance from 0.177 to 0.024, explaining 86% of errors. Improved state quality validated the method.

Conclusion: The modular framework is effective for identifying and reducing errors, applicable to various quantum platforms.

Abstract: High-quality quantum state generation is essential for advanced quantum
information processing, including quantum communication, quantum sensing, and
quantum computing. In practice, various error sources degrade the quality of
quantum states, and quantum state tomography (QST) is a standard diagnostic
tool. However, in QST, multiple error sources gather in a single density
matrix, making it difficult to identify individual error sources. To address
this problem, we propose an automated method for quantifying error sources by
combining simulation and parameter optimization to reproduce the experimental
density matrix. We focus on the experimental generation of time-bin entangled
photon pairs, for which we model the relevant error sources and simulate the
density matrix with adjustable model parameters, thereby optimizing the
parameters and minimizing the trace distance to the experimental data.
Optimization of the parameters reduced the trace distance from 0.177 to 0.024,
indicating that our modeled error sources explain 86% of the errors. Reducing
the predicted error sources improves the state quality, consistent with our
predictions and thus validating the proposed method. In addition, the modular
structure of this framework makes it applicable to other quantum platforms,
such as superconducting qubits, atoms, and solid-state spins.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [34] [High Purcell enhancement in all-TMDC nanobeam resonator designs with optically active monolayers for nanolasers](https://arxiv.org/abs/2508.05333)
*Felix Binkowski,Aris Koulas-Simos,Fridtjof Betz,Matthias Plock,Ivan Sekulic,Phillip Manley,Martin Hammerschmidt,Philipp-Immanuel Schneider,Lin Zschiedrich,Battulga Munkhbat,Stephan Reitzenstein,Sven Burger*

Main category: physics.optics

TL;DR: A nanobeam resonator with an optically active monolayer achieves high Purcell enhancement, optimized for high-beta-factor nanolaser operation using TMDC materials.


<details>
  <summary>Details</summary>
Motivation: To design a resonator for high Purcell enhancement and efficient nanolaser operation using atomically thin layers.

Method: Developed a theoretical framework using resonance expansion to model and optimize Purcell enhancement, focusing on high-Q resonances and electric field confinement.

Result: Numerical optimization revealed a high-Q resonance enabling strong electric field confinement in the monolayer, maximizing modal gain.

Conclusion: The proposed resonator and framework effectively enhance Purcell factor and modal gain for nanolaser applications.

Abstract: We propose a nanobeam resonator incorporating an optically active monolayer,
designed to achieve a high Purcell enhancement. The resonator is fully composed
of transition metal dichalcogenide (TMDC) materials and intended to operate as
a high-beta-factor nanolaser. A theoretical framework that models and optimizes
the Purcell enhancement associated with the emission from atomically thin
layers is developed. This framework is based on a resonance expansion, enabling
spectral resolution of physical quantities governed by high-Q resonances. The
numerical optimization of the resonator leads to the presence of a high-Q
resonance supporting a strong electric field confinement in the monolayer to
maximize the modal gain.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [35] [Provable Post-Training Quantization: Theoretical Analysis of OPTQ and Qronos](https://arxiv.org/abs/2508.04853)
*Haoyu Zhang,Shihao Zhang,Ian Colbert,Rayan Saab*

Main category: cs.LG

TL;DR: This paper provides the first quantitative error bounds for OPTQ (GPTQ) and Qronos, two leading post-training quantization methods, analyzing their theoretical guarantees and practical implications.


<details>
  <summary>Details</summary>
Motivation: Despite OPTQ's widespread use in reducing neural network costs, it lacks rigorous theoretical guarantees. This work aims to fill that gap by deriving error bounds for OPTQ and Qronos.

Method: The paper analyzes OPTQ's iterative procedure, deriving non-asymptotic 2-norm error bounds and infinity-norm bounds for its stochastic variant. It also extends this analysis to Qronos.

Result: The analysis justifies practical design choices (e.g., feature ordering by norm) and provides guidance for parameter selection. Stronger infinity-norm bounds for stochastic OPTQ aid in controlling quantization alphabets.

Conclusion: The paper offers theoretical foundations for OPTQ and Qronos, explaining their empirical success and guiding future applications and improvements.

Abstract: Post-training quantization (PTQ) has become a crucial tool for reducing the
memory and compute costs of modern deep neural networks, including large
language models (LLMs). Among PTQ algorithms, the OPTQ framework-also known as
GPTQ-has emerged as a leading method due to its computational efficiency and
strong empirical performance. Despite its widespread adoption, however, OPTQ
lacks rigorous quantitative theoretical guarantees. This paper presents the
first quantitative error bounds for both deterministic and stochastic variants
of OPTQ, as well as for Qronos, a recent related state-of-the-art PTQ
algorithm. We analyze how OPTQ's iterative procedure induces quantization error
and derive non-asymptotic 2-norm error bounds that depend explicitly on the
calibration data and a regularization parameter that OPTQ uses. Our analysis
provides theoretical justification for several practical design choices,
including the widely used heuristic of ordering features by decreasing norm, as
well as guidance for selecting the regularization parameter. For the stochastic
variant, we establish stronger infinity-norm error bounds, which enable control
over the required quantization alphabet and are particularly useful for
downstream layers and nonlinearities. Finally, we extend our analysis to
Qronos, providing new theoretical bounds, for both its deterministic and
stochastic variants, that help explain its empirical advantages.

</details>


### [36] [Deep Neural Networks with General Activations: Super-Convergence in Sobolev Norms](https://arxiv.org/abs/2508.05141)
*Yahong Yang,Juncai He*

Main category: cs.LG

TL;DR: Deep neural networks with general activations achieve superior approximation rates in Sobolev spaces, outperforming classical methods like finite elements and spectral methods, termed 'super-convergence.'


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in error-estimation theory for neural-network-based PDE solutions and demonstrate their superior accuracy compared to traditional numerical methods.

Method: Analysis of deep fully-connected neural networks with general activation functions in Sobolev spaces, measuring errors in $W^{m,p}$-norm for $m < n$.

Result: Deep networks exhibit super-convergence, surpassing classical methods in approximating weak PDE solutions.

Conclusion: The work provides a unified theoretical foundation for neural networks in scientific computing, closing a significant gap in PDE approximation theory.

Abstract: This paper establishes a comprehensive approximation result for deep
fully-connected neural networks with commonly-used and general activation
functions in Sobolev spaces $W^{n,\infty}$, with errors measured in the
$W^{m,p}$-norm for $m < n$ and $1\le p \le \infty$. The derived rates surpass
those of classical numerical approximation techniques, such as finite element
and spectral methods, exhibiting a phenomenon we refer to as
\emph{super-convergence}. Our analysis shows that deep networks with general
activations can approximate weak solutions of partial differential equations
(PDEs) with superior accuracy compared to traditional numerical methods at the
approximation level. Furthermore, this work closes a significant gap in the
error-estimation theory for neural-network-based approaches to PDEs, offering a
unified theoretical foundation for their use in scientific computing.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [37] [Optimization of Ab-Initio Based Tight-Binding Models](https://arxiv.org/abs/2508.04861)
*Henrik Dick,Thomas Dahm*

Main category: cond-mat.mtrl-sci

TL;DR: A machine-learning-inspired method for optimizing tight-binding models improves accuracy and reduces parameters compared to Wannier functions.


<details>
  <summary>Details</summary>
Motivation: Standard methods like density functional theory struggle with complex systems (e.g., interfaces, grain boundaries), requiring simplified models. Tight-binding models, though useful, face accuracy limitations.

Method: Develops a procedure to optimize tight-binding model parameters using machine-learning techniques, minimizing parameters while matching ab-initio band structure data.

Result: Produces models with fewer orbitals and shorter ranges than Wannier functions, achieving equal or better accuracy.

Conclusion: The method is more efficient for automated tight-binding model construction, especially in large-scale materials calculations.

Abstract: The electronic structure of solids can routinely be calculated by standard
methods like density functional theory. However, in complicated situations like
interfaces, grain boundaries or contact geometries one needs to resort to more
simplified models of the electronic structure. Tight-binding models are using a
reduced set of orbitals and aim to approximate the electronic structure by
short range hopping processes. For example, maximally localized Wannier
functions are often used for that purpose. However, their accuracy is limited
by the need to disentangle the electronic bands. Here, we develop and
investigate a different procedure to obtain tight-binding models inspired by
machine-learning techniques. The model parameters are optimized in such a way
as to reproduce ab-initio band structure data as accurately as possible using
an as small as possible number of model parameters. The procedure is shown to
result in models with smaller ranges and fewer orbitals than maximally
localized Wannier functions but same or even better accuracy. We argue that
such a procedure is more useful for automated construction of tight-binding
models particularly for large-scale materials calculations.

</details>


### [38] [Many-body perturbation theory vs. density functional theory: A systematic benchmark for band gaps of solids](https://arxiv.org/abs/2508.05247)
*Max Großmann,Marc Thieme,Malte Grunert,Erich Runge*

Main category: cond-mat.mtrl-sci

TL;DR: The paper benchmarks $GW$ methods against DFT for band gaps, showing full-frequency $GW$ and QS$G\hat{W}$ outperform DFT and PPA-based methods.


<details>
  <summary>Details</summary>
Motivation: To evaluate the accuracy and efficiency of $GW$ methods compared to DFT for predicting band gaps in solids.

Method: Systematic comparison of four $GW$ variants ($G_{0}W_{0}$-PPA, QP$G_{0}W_{0}$, QS$GW$, QS$G\hat{W}$) against DFT functionals (mBJ, HSE06).

Result: Full-frequency $GW$ and QS$G\hat{W}$ significantly outperform DFT and PPA-based methods, with QS$G\hat{W}$ achieving near-perfect accuracy.

Conclusion: QS$G\hat{W}$ is the most accurate method for band gaps, even identifying questionable experimental data, while PPA-based $GW$ offers limited improvement over DFT.

Abstract: We benchmark many-body perturbation theory against density functional theory
(DFT) for the band gaps of solids. We systematically compare four $GW$ variants
$-$ $G_{0}W_{0}$ using the Godby-Needs plasmon-pole approximation
($G_{0}W_{0}$-PPA), full-frequency quasiparticle $G_{0}W_{0}$ (QP$G_{0}W_{0}$),
full-frequency quasiparticle self-consistent $GW$ (QS$GW$), and QS$GW$
augmented with vertex corrections in $W$ (QS$G\hat{W}$) $-$ against the
currently best performing and popular density functionals mBJ and HSE06. Our
results show that $G_{0}W_{0}$-PPA calculations offer only a marginal accuracy
gain over the best DFT methods, however at a higher cost. Replacing the PPA
with a full-frequency integration of the dielectric screening improves the
predictions dramatically, almost matching the accuracy of the QS$G\hat{W}$. The
QS$GW$ removes starting-point bias, but systematically overestimates
experimental gaps by about $15\%$. Adding vertex corrections to the screened
Coulomb interaction, i.e., performing a QS$G\hat{W}$ calculation, eliminates
the overestimation, producing band gaps that are so accurate that they even
reliably flag questionable experimental measurements.

</details>


### [39] [Hole-doping reduces the coercive field in ferroelectric hafnia](https://arxiv.org/abs/2508.05345)
*Pravan Omprakash,Gwan Yeong Jung,Guodong Ren,Rohan Mishra*

Main category: cond-mat.mtrl-sci

TL;DR: Hole doping in ferroelectric hafnia reduces the coercive field, making it more efficient for memory and logic applications.


<details>
  <summary>Details</summary>
Motivation: The high coercive field in undoped hafnia hinders efficient device operations, prompting exploration of doping to mitigate this issue.

Method: First-principles calculations and phenomenological modeling were used to study the effects of hole doping on hafnia's polarization switching.

Result: Hole doping lowers the coercive field from 8 MV/cm to 6 MV/cm and alters the preferred switching pathway, reducing energy barriers.

Conclusion: Hole doping transforms hafnia into a proper ferroelectric with a lower coercive field, enhancing its potential for applications.

Abstract: Ferroelectric hafnia holds promise for next-generation memory and logic
applications because of its CMOS compatibility. However, the high coercive
field required for polarization switching in hafnia remains a critical
challenge for efficient device operations. Using first-principles calculations
and phenomenological modeling, we predict that hole doping can reduce the
coercive field from 8 MV/cm in undoped hafnia to 6 MV/cm in hafnia doped with
0.2 holes per formula unit (f.u.). In the absence of doping, the reversal of
polarization of the Pca21 phase is preferred through the non-polar, tetragonal
P42/nmc phase. This switching pathway involves the coupling of three hard
distortion modes that render undoped hafnia as an improper ferroelectric. The
overall energy barrier through this pathway remains unchanged (80 meV/f.u.)
upon hole doping. However, the introduction of holes hardens the polar
distortion mode that connects the polar Pca21 phase to the non-polar,
orthorhombic Pbcm phase, and reduces the energy barrier from 180 meV/f.u. in
undoped hafnia to 80 meV/f.u. at 0.2 holes/f.u.. Overall, hole doping makes the
latter switching pathway through the Pbcm phase competitive, and renders hafnia
as a proper ferroelectric with a lower coercive field.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [40] [Mapping Sparse Triangular Solves to GPUs via Fine-grained Domain Decomposition](https://arxiv.org/abs/2508.04917)
*Atharva Gondhalekar,Kjetil Haugen,Thomas Gibson,Wu-chun Feng*

Main category: cs.PF

TL;DR: The paper proposes a fine-grained domain decomposition method for GPU-accelerated sparse linear system solves, improving performance by reducing memory bottlenecks and increasing parallelism.


<details>
  <summary>Details</summary>
Motivation: Sparse triangular solves in preconditioned iterative methods cause bottlenecks due to irregular memory access and data dependencies, limiting GPU performance.

Method: A fine-grained domain decomposition strategy creates non-overlapping subdomains, each handled by a GPU thread block, fitting in shared memory to reduce global memory access and synchronization.

Result: Achieves a 10.7× speedup for triangular solves and a 3.2× speedup for ILU0-preconditioned BiCGSTAB on AMD MI210 GPU.

Conclusion: The method effectively adapts triangular solves to GPU architecture, balancing parallelism and convergence efficiency.

Abstract: Sparse linear systems are typically solved using preconditioned iterative
methods, but applying preconditioners via sparse triangular solves introduces
bottlenecks due to irregular memory accesses and data dependencies. This work
leverages fine-grained domain decomposition to adapt triangular solves to the
GPU architecture. We develop a fine-grained domain decomposition strategy that
generates non-overlapping subdomains, increasing parallelism in the application
of preconditioner at the expense of a modest increase in the iteration count
for convergence. Each subdomain is assigned to a thread block and is sized such
that the subdomain vector fits in the GPU shared memory, eliminating the need
for inter-block synchronization and reducing irregular global memory accesses.
Compared to other state-of-the-art implementations using the ROCm$^{\text{TM}}$
software stack, we achieve a 10.7$\times$ speedup for triangular solves and a
3.2$\times$ speedup for the ILU0-preconditioned biconjugate gradient stabilized
(BiCGSTAB) solver on the AMD Instinct$^{\text{TM}}$ MI210 GPU.

</details>


<div id='physics.class-ph'></div>

# physics.class-ph [[Back]](#toc)

### [41] [A Time-Domain Method of Auxiliary Sources for Efficient Analysis of Transient Electromagnetic Scattering by Moderately Conductive Cylinders](https://arxiv.org/abs/2508.05217)
*Minas Kouroublakis,Nikolaos L. Tsitsas,Yehuda Leviatan*

Main category: physics.class-ph

TL;DR: A time-domain MAS-SIBC method is proposed for modeling electromagnetic scattering from moderately conductive cylindrical scatterers, validated with numerical tests.


<details>
  <summary>Details</summary>
Motivation: To address the need for efficient modeling of scattering from moderately conductive materials (e.g., carbon-based composites, conductive polymers) used in RF applications.

Method: Combines the Method of Auxiliary Sources (MAS) with Standard Impedance Boundary Condition (SIBC) in the time domain, focusing on surface effects for efficiency.

Result: Accurate and computationally efficient modeling of scattering, validated against analytical solutions and commercial solvers.

Conclusion: Time-domain MAS-SIBC is a promising approach for scattering problems involving moderately conductive materials.

Abstract: This paper presents a time-domain implementation of the Method of Auxiliary
Sources (MAS) combined with the Standard Impedance Boundary Condition (SIBC)
for electromagnetic scattering problems involving cylindrical scatterers with
finite but moderate conductivity. The proposed approach focuses on solving the
two-dimensional problem using a first-order SIBC, which is valid when the
conductivity is sufficiently higher than the maximum spectral frequency times
the dielectric permittivity of the scatterer. This regime includes moderately
conductive materials--such as carbon-based composites, conductive polymers, and
doped dielectrics--that are increasingly used in real-world radio-frequency
applications, including wearable electronics, electromagnetic interference
shielding, and biomedical sensors. Under the above validity conditions, the
interaction between the incident wave and the scatterer is dominated by surface
effects, allowing for an efficient and accurate modeling strategy without the
need to compute internal fields. The theoretical formulation of the time-domain
MAS-SIBC method is developed, followed by extensive numerical testing on
various geometries whose cross section is a closed curve. Such geometries
include circular, elliptical, super-circular, rounded-triangular, and
inverted-elliptical scatterers. A planar geometry is also tested. All results
are validated against analytical solutions and commercial frequency-domain
solvers, demonstrating the accuracy and practical potential of the proposed
method. The findings suggest that time-domain MAS-SIBC offers a promising and
computationally efficient approach for modeling scattering from materials even
with moderate conductivity.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [42] [Simulation of Non-Premixed, Supersonic Combustion using the Discontinuous Galerkin Method on Fully Unstructured Grids](https://arxiv.org/abs/2508.04930)
*Cal J. Rising,Eric J. Ching,Ryan F. Johnson*

Main category: physics.flu-dyn

TL;DR: 3D simulations of a reacting hydrogen jet in supersonic crossflow using a DG method show grid and polynomial order sensitivity, with DG(p=2) yielding accurate results. High resolution is needed for shock and flame stabilization. The work pioneers unstructured tetrahedral mesh use for such flows.


<details>
  <summary>Details</summary>
Motivation: To develop an accurate and efficient method for simulating high-speed airbreathing propulsion vehicles, focusing on grid and polynomial order sensitivity.

Method: Discontinuous Galerkin (DG) formulation for 3D simulations of a hydrogen jet in supersonic crossflow, examining grid size and polynomial order effects.

Result: DG(p=2) agrees with experiments; high resolution captures shock and flame stabilization. Mixing/combustion is non-premixed diffusion mode. Pioneers unstructured tetrahedral mesh use.

Conclusion: The DG method is effective for complex, high-speed reacting flows, with potential for complex geometries and physics.

Abstract: In this study, three-dimensional simulations of a reacting hydrogen jet in
supersonic crossflow using a structure-preserving discontinuous Galerkin (DG)
formulation are examined. The hydrogen jet, with a momentum flux ratio of five,
is injected into a high enthalpy crossflow. The sensitivities of the solution
to the grid element size and polynomial order are investigated to determine an
accurate and computationally efficient approach to simulating high-speed
airbreathing propulsion vehicles. The results demonstrate that DG(p = 2)
solutions, which are nominally third-order accurate in smooth regions of the
flow, show reasonable agreement with existing experimental results. The
separation shock formation behind the jet is found to be heavily grid dependent
and necessary for accurate simulations of the reacting jet in supersonic
crossflow. It is determined that the highest resolution cell and polynomial
order is required to capture the upstream separation shock and consequently the
flame stabilization point. The mixing and combustion mode is also determined
using the flame index and demonstrates the flow is heavily skewed towards a
non-premixed diffusion mode which is consistent with previously run simulations
of this case using traditional finite volume schemes and sub grid scale
modeling approaches. Beyond this analysis, the novelty of this work lies in
demonstrating a high-speed, multicomponent, chemically reacting flow on a fully
unstructured tetrahedral mesh: a first-of-its-kind calculation. This highlights
the potential of these methods for simulating fluids in complex geometries with
complex physics

</details>
