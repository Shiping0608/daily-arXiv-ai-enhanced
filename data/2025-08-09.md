<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 11]
- [math.AP](#math.AP) [Total: 13]
- [physics.comp-ph](#physics.comp-ph) [Total: 2]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 3]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [physics.class-ph](#physics.class-ph) [Total: 1]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [quant-ph](#quant-ph) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 3]
- [cs.PF](#cs.PF) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [On the optimization of discrepancy measures](https://arxiv.org/abs/2508.04926)
*François Clément,Nathan Kirk,Art B. Owen,T. Konstantin Rusch*

Main category: math.NA

TL;DR: The paper introduces the average squared discrepancy to address issues with traditional discrepancy measures like $L_\infty$ and $L_2$ star discrepancy, showing computational efficiency and avoiding pathologies.


<details>
  <summary>Details</summary>
Motivation: Traditional discrepancy measures like $L_\infty$ and $L_2$ star discrepancy have computational and theoretical limitations, prompting the search for a better criterion.

Method: The authors propose the average squared discrepancy, which averages over $2^d$ versions of the $L_2$ star discrepancy, and compare it with traditional measures.

Result: The average squared discrepancy is computationally efficient, avoids pathologies, and performs well compared to other measures, especially for $L_2$ star discrepancy.

Conclusion: The average squared discrepancy is a robust and efficient alternative to traditional discrepancy measures, addressing their shortcomings effectively.

Abstract: Points in the unit cube with low discrepancy can be constructed using algebra
or, more recently, by direct computational optimization of a criterion. The
usual $L_\infty$ star discrepancy is a poor criterion for this because it is
computationally expensive and lacks differentiability. Its usual replacement,
the $L_2$ star discrepancy, is smooth but exhibits other pathologies shown by
J. Matou\v{s}ek. In an attempt to address these problems, we introduce the
\textit{average squared discrepancy} which averages over $2^d$ versions of the
$L_2$ star discrepancy anchored in the different vertices of $[0,1]^d$. Not
only can this criterion be computed in $O(dn^2)$ time, like the $L_2$ star
discrepancy, but also we show that it is equivalent to a weighted symmetric
$L_2$ criterion of Hickernell's by a constant factor. We compare this criterion
with a wide range of traditional discrepancy measures, and show that only the
average squared discrepancy avoids the problems raised by Matou\v{s}ek.
Furthermore, we present a comprehensive numerical study showing in particular
that optimizing for the average squared discrepancy leads to strong performance
for the $L_2$ star discrepancy, whereas the converse does not hold.

</details>


### [2] [Toroidal area-preserving parameterizations of genus-one closed surfaces](https://arxiv.org/abs/2508.05111)
*Marco Sutti,Mei-Heng Yueh*

Main category: math.NA

TL;DR: The paper presents four algorithms for computing toroidal area-preserving parameterizations of genus-one surfaces, using Riemannian geometry and stretch energy minimization.


<details>
  <summary>Details</summary>
Motivation: The problem involves parameterizing genus-one closed surfaces while preserving area, which is useful for applications like surface registration and texture mapping.

Method: Four algorithms are proposed: projected gradient descent, projected conjugate gradient, Riemannian gradient, and Riemannian conjugate gradient, all minimizing stretch energy on a power manifold of ring tori.

Result: Numerical experiments confirm the effectiveness of the framework.

Conclusion: The algorithms are successfully applied to surface registration and texture mapping, demonstrating practical utility.

Abstract: We consider the problem of computing toroidal area-preserving
parameterizations of genus-one closed surfaces. We propose four algorithms
based on Riemannian geometry: the projected gradient descent method, the
projected conjugate gradient method, the Riemannian gradient method, and the
Riemannian conjugate gradient method. Our objective function is based on the
stretch energy functional, and the minimization is constrained on a power
manifold of ring tori embedded in three-dimensional Euclidean space. Numerical
experiments on several mesh models demonstrate the effectiveness of the
proposed framework. Finally, we show how to use the proposed algorithms in the
context of surface registration and texture mapping applications.

</details>


### [3] [An asymptotic-preserving active flux scheme for the hyperbolic heat equation in the diffusive scaling](https://arxiv.org/abs/2508.05166)
*Junming Duan,Wasilij Barsukow,Christian Klingenberg*

Main category: math.NA

TL;DR: The paper analyzes the Active Flux (AF) method, a high-order finite volume scheme, demonstrating its asymptotic-preserving (AP) behavior for the hyperbolic heat equation without modifications.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the AF method's structure-preserving property, exploring its AP behavior in diffusive scaling.

Method: The method employs Jacobian Splitting (JS) for updating point values and uses formal asymptotic analysis, discrete Fourier analysis, and numerical experiments.

Result: The JS-based AF method is shown to be AP for the hyperbolic heat equation, with the limit scheme discretizing the heat equation.

Conclusion: The AF method's AP behavior is validated, confirming its suitability for solving the hyperbolic heat equation.

Abstract: The Active Flux (AF) method is a compact, high-order finite volume scheme
that enhances flexibility by introducing point values at cell interfaces as
additional degrees of freedom alongside cell averages. The method of lines is
employed here for temporal discretization. A common approach for updating point
values relies on the Jacobian Splitting (JS) method, which incorporates
upwinding. A key advantage of the AF method over standard finite volume schemes
is its structure-preserving property, motivating the investigation of its
asymptotic-preserving (AP) behavior in the diffusive scaling. We show that the
JS-based AF method without any modification is AP for solving the hyperbolic
heat equation, in the sense that the limit scheme is a discretization of the
limit heat equation. We use formal asymptotic analysis, discrete Fourier
analysis, and numerical experiments to illustrate our findings.

</details>


### [4] [An Investigation into the Distribution of Ratios of Particle Solver-based Likelihoods](https://arxiv.org/abs/2508.05303)
*Emil Løvbak,Sebastian Krumscheid*

Main category: math.NA

TL;DR: The paper explores the Metropolis-Hastings algorithm for sampling posterior distributions in Bayesian inverse problems with random likelihood functions, focusing on PDE solutions and Gaussian noise.


<details>
  <summary>Details</summary>
Motivation: To understand the impact of Gaussian noise in likelihood approximations on the Metropolis-Hastings algorithm's performance.

Method: Theoretical analysis and numerical experiments are used to study the effects of additive Gaussian noise in log-likelihood evaluations.

Result: The study examines how Gaussian distributions influence likelihood ratio distributions, crucial for acceptance probabilities in Metropolis-Hastings.

Conclusion: The findings provide insights into the behavior of the algorithm under noisy likelihood approximations, aiding practical implementations.

Abstract: We investigate the use of the Metropolis-Hastings algorithm to sample
posterior distribution in a Bayesian inverse problem, where the likelihood
function is random. Concretely, we consider the case where one has full field
observations of a PDE solution, in case a one-dimensional diffusion equation,
subject to a Gaussian observation error. Assuming one uses a particle-based
Monte Carlo simulation when approximating the likelihood function, one gets an
approximate likelihood with additive Gaussian noise in the log-likelihood. We
study how these two Gaussian distributions affect the distribution of ratios of
approximate likelihood evaluations, as required when evaluating acceptance
probabilities in the Metropolis-Hastings algorithm. We do so through both
theoretical analysis and numerical experiments.

</details>


### [5] [A low-rank solver for the Stokes-Darcy model with random hydraulic conductivity and Beavers-Joseph condition](https://arxiv.org/abs/2508.05328)
*Yujun Zhu,Yulan Ning,Zhipeng Yang,Xiaoming He,Ju Ming*

Main category: math.NA

TL;DR: The paper introduces an efficient low-rank solver for the stochastic Stokes-Darcy interface model with random hydraulic conductivity, reducing computational costs while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: To address high computational and memory demands in solving stochastic Stokes-Darcy interface problems with randomness in hydraulic conductivity.

Method: Uses a generalized low-rank approximation for stiffness matrices, proposes a strategy for optimal data compression ratios, and conducts error analysis.

Result: The solver reduces computational and memory costs without sacrificing accuracy, validated by numerical experiments.

Conclusion: The low-rank solver is efficient and accurate, with theoretical and experimental validation.

Abstract: This paper proposes, analyzes, and demonstrates an efficient low-rank solver
for the stochastic Stokes-Darcy interface model with a random hydraulic
conductivity both in the porous media domain and on the interface. We consider
three interface conditions with randomness, including the Beavers-Joseph
interface condition with the random hydraulic conductivity, on the interface
between the free flow and the porous media flow. Our solver employs a novel
generalized low-rank approximation of the large-scale stiffness matrices, which
can significantly cut down the computational costs and memory requirements
associated with matrix inversion without losing accuracy. Therefore, by
adopting a suitable data compression ratio, the low-rank solver can maintain a
high numerical precision with relatively low computational and space
complexities. We also propose a strategy to determine the best choice of data
compression ratios. Furthermore, we carry out the error analysis of the
generalized low-rank matrix approximation algorithm and the low-rank solver.
Finally, numerical experiments are conducted to validate the proposed
algorithms and the theoretical conclusions.

</details>


### [6] [The domain-of-dependence stabilization for cut-cell meshes is fully discretely stable](https://arxiv.org/abs/2508.05372)
*Louis Petri,Gunnar Birke,Christian Engwer,Hendrik Ranocha*

Main category: math.NA

TL;DR: The paper analyzes the stability of domain-of-dependence stabilization for hyperbolic problems, focusing on small cut cells. It achieves fully discrete stability with a time step restriction independent of cell size and addresses challenges with higher-order polynomials.


<details>
  <summary>Details</summary>
Motivation: To address stability issues caused by small cut cells in hyperbolic problems, which can disrupt numerical simulations.

Method: The study uses a linear advection model in one dimension, applying operator norm estimates to analyze stability and proposing a CFL-like condition for higher-order polynomials.

Result: Fully discrete stability is achieved under a time step restriction independent of small cells. The analysis reveals stability mechanisms and challenges with higher-order polynomials.

Conclusion: The proposed method and analysis are validated numerically in 1D and 2D simulations, offering a feasible solution for stability in hyperbolic problems with small cut cells.

Abstract: We present a fully discrete stability analysis of the domain-of-dependence
stabilization for hyperbolic problems. The method aims to address issues caused
by small cut cells by redistributing mass around the neighborhood of a small
cut cell at a semi-discrete level. Our analysis is conducted for the linear
advection model problem in one spatial dimension. We demonstrate that fully
discrete stability can be achieved under a time step restriction that does not
depend on the arbitrarily small cells, using an operator norm estimate.
Additionally, this analysis offers a detailed understanding of the stability
mechanism and highlights some challenges associated with higher-order
polynomials. We also propose a way to mitigate these issues to derive a
feasible CFL-like condition. The analytical findings, as well as the proposed
solution are verified numerically in one- and two-dimensional simulations.

</details>


### [7] [Inverse inequalities for kernel-based approximation on bounded domains and Riemannian manifolds](https://arxiv.org/abs/2508.05376)
*Zhengjie Sun,Leevan Ling*

Main category: math.NA

TL;DR: The paper extends inverse inequalities to kernel-based approximation spaces on bounded Lipschitz domains and compact Riemannian manifolds, addressing challenges not present in polynomial spaces.


<details>
  <summary>Details</summary>
Motivation: To generalize inverse inequalities, traditionally studied for polynomial spaces, to kernel-based trial spaces, which are less explored.

Method: Extends Bernstein inequalities for bounded Lipschitz domains to all Sobolev orders and derives Nikolskii inequalities. For Riemannian manifolds, focuses on restricted kernels from Euclidean space.

Result: Achieves desired inverse inequalities but may require slightly more kernel smoothness. Proves analogous results for restricted kernels on manifolds.

Conclusion: The work successfully extends inverse inequalities to kernel-based spaces, though with potential additional smoothness requirements, and provides foundational results for manifolds.

Abstract: This paper establishes inverse inequalities for kernel-based approximation
spaces defined on bounded Lipschitz domains in $\mathbb{R}^d$ and compact
Riemannian manifolds. While inverse inequalities are well-studied for
polynomial spaces, their extension to kernel-based trial spaces poses
significant challenges. For bounded Lipschitz domains, we extend prior
Bernstein inequalities, which only apply to a limited range of Sobolev orders,
to all orders on the lower bound and $L_2$ on the upper, and derive Nikolskii
inequalities that bound $L_\infty$ norms by $L_2$ norms. Our theory achieves
the desired form but may require slightly more smoothness on the kernel than
the regular $>d/2$ assumption. For compact Riemannian manifolds, we focus on
restricted kernels, which are defined as the restriction of positive definite
kernels from the ambient Euclidean space to the manifold, and prove their
counterparts.

</details>


### [8] [Randomized Krylov-Schur eigensolver with deflation](https://arxiv.org/abs/2508.05400)
*Jean-Guillaume de Damas,Laura Grigori*

Main category: math.NA

TL;DR: A novel algorithm, randomized Krylov-Schur (rKS), is introduced for solving large-scale eigenvalue problems efficiently, focusing on computing a small set of eigenpairs with scalability and accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of solving large-scale eigenvalue problems efficiently, especially for extracting a small set of eigenpairs, with a simple and practical approach.

Method: The rKS algorithm leverages low-dimensional operations like sketch-orthogonalization and stable Schur reordering, along with a deflation technique for converged eigenpairs.

Result: Numerical experiments confirm the method's scalability and accuracy in computing eigenspaces for targeted spectral regions.

Conclusion: The rKS algorithm is an effective and practical solution for large-scale eigenvalue problems, combining simplicity with robust performance.

Abstract: This work introduces a novel algorithm to solve large-scale eigenvalue
problems and seek a small set of eigenpairs. The method, called randomized
Krylov-Schur (rKS), has a simple implementation and benefits from fast and
efficient operations in low-dimensional spaces, such as
sketch-orthogonalization processes and stable reordering of Schur
factorizations. It also includes a practical deflation technique for converged
eigenpairs, enabling the computation of the eigenspace associated with a given
part of the spectrum. Numerical experiments are provided to demonstrate the
scalability and accuracy of the method.

</details>


### [9] [A unified framework for the analysis, numerical approximation and model reduction of linear operator equations, Part I: Well-posedness in space and time](https://arxiv.org/abs/2508.05407)
*Moritz Feuerle,Richard Löscher,Olaf Steinbach,Karsten Urban*

Main category: math.NA

TL;DR: A unified framework for well-posed formulations of linear operator equations, including PDEs, using operator completion and extension.


<details>
  <summary>Details</summary>
Motivation: To provide a general approach for constructing well-posed formulations for various linear operator equations, including elliptic, parabolic, and hyperbolic PDEs.

Method: Completion and extension of operators from the strong form of the problem, incorporating weak variational and novel space-time variational forms.

Result: Theoretical foundation for unified numerical approximation and model reduction of parameterized linear operator equations.

Conclusion: The framework supports future work on numerical methods and model reduction for linear operator equations.

Abstract: We present a unified framework to construct well-posed formulations for large
classes of linear operator equations including elliptic, parabolic and
hyperbolic partial differential equations. This general approach incorporates
known weak variational formulations as well as novel space-time variational
forms of the hyperbolic wave equation. The main concept is completion and
extension of operators starting from the strong form of the problem.
  This paper lays the theoretical foundation for a unified approach towards
numerical approximation methods and also model reduction of parameterized
linear operator equations which will be the subject of the following parts.

</details>


### [10] [Learning Geometric-Aware Quadrature Rules for Functional Minimization](https://arxiv.org/abs/2508.05445)
*Costas Smaragdakis*

Main category: math.NA

TL;DR: QuadrANN, a GNN architecture, learns optimal quadrature weights for non-uniform point clouds, improving integration accuracy for PDE solvers.


<details>
  <summary>Details</summary>
Motivation: Accurate numerical integration over non-uniform point clouds is challenging for mesh-free PDE solvers, requiring adaptive and permutation-invariant methods.

Method: QuadrANN uses a deep message-passing GNN to encode local geometric features and global context, generating data-driven quadrature rules.

Result: QuadrANN reduces variance in integral estimation compared to Quasi-Monte Carlo, especially in critical areas with singularities.

Conclusion: QuadrANN enhances stability in integration, benefiting deep learning-based variational solvers for PDEs.

Abstract: Accurate numerical integration over non-uniform point clouds is a challenge
for modern mesh-free machine learning solvers for partial differential
equations (PDEs) using variational principles. While standard Monte Carlo (MC)
methods are not capable of handling a non-uniform point cloud, modern neural
network architectures can deal with permutation-invariant inputs, creating
quadrature rules for any point cloud. In this work, we introduce QuadrANN, a
Graph Neural Network (GNN) architecture designed to learn optimal quadrature
weights directly from the underlying geometry of point clouds. The design of
the model exploits a deep message-passing scheme where the initial layer
encodes rich local geometric features from absolute and relative positions as
well as an explicit local density measure. In contrast, the following layers
incorporate a global context vector. These architectural choices allow the
QuadrANN to generate a data-driven quadrature rule that is
permutation-invariant and adaptive to both local point density and the overall
domain shape. We test our methodology on a series of challenging test cases,
including integration on convex and non-convex domains and estimating the
solution of the Heat and Fokker-Planck equations. Across all the tests,
QuadrANN reduces the variance of the integral estimation compared to standard
Quasi-Monte Carlo methods by warping the point clouds to be more dense in
critical areas where the integrands present certain singularities. This
enhanced stability in critical areas of the domain at hand is critical for the
optimization of energy functionals, leading to improved deep learning-based
variational solvers.

</details>


### [11] [Numerical analysis of the stochastic Navier-Stokes equations](https://arxiv.org/abs/2508.05564)
*Dominic Breit,Andreas Prohl,Jörn Wichman*

Main category: math.NA

TL;DR: The paper surveys optimally convergent numerical methods for stochastic Stokes and Navier--Stokes equations, highlighting differences from deterministic methods and proposing benchmarks for new algorithms.


<details>
  <summary>Details</summary>
Motivation: To address the need for reliable numerical methods for stochastic fluid models, which differ fundamentally from deterministic ones, and to provide benchmarks for comparing new algorithms.

Method: Surveys and analyzes numerical methods for stochastic fluid equations, compares deterministic and stochastic approaches, and proposes computational benchmarks.

Result: Identifies sub-optimal performance of deterministic methods in stochastic settings and suggests modifications to restore optimality. Introduces benchmarks for evaluating new algorithms.

Conclusion: The study emphasizes the importance of adapting deterministic methods for stochastic problems and provides tools for comparing and improving numerical schemes in realistic settings.

Abstract: The developments over the last five decades concerning numerical
discretisations of the incompressible Navier--Stokes equations have lead to
reliable tools for their approximation: those include stable methods to
properly address the incompressibility constraint, stable discretisations to
account for convection dominated problems, efficient time (splitting) methods,
and methods to tackle their nonlinear character. While these tools may
successfully be applied to reliably simulate even more complex fluid flow PDE
models, their understanding requires a fundamental revision in the case of
stochastic fluid models, which are gaining increased importance nowadays.
  This work motivates and surveys optimally convergent numerical methods for
the stochastic Stokes and Navier--Stokes equations that were obtained in the
last decades. Furtheremore, we computationally illustrate the failure of some
of those methods from the deterministic setting, if they are straight-forwardly
applied to the stochastic case. In fact, we explain why some of these
deterministic methods perform sub-optimally by highlighting crucial analytical
differences between the deterministic and stochastic equations -- and how
modifications of the deterministic methods restore their optimal performance if
they properly address the probabilistic nature of the stochastic problem.
  Next to the numerical analysis of schemes, we propose a general benchmark of
prototypic fluid flow problems driven by different types of noise to also
compare new algorithms by simulations in terms of complexities, efficiencies,
and possible limitations. The driving motivation is to reach a better
comparison of simulations for new schemes in terms of accuracy and
complexities, and to also complement theoretical performance studies for
restricted settings of data by more realistic ones.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [12] [Two-dimensional Carreau law for a quasi-newtonian fluid flow through a thin domain with a slightly rough boundary](https://arxiv.org/abs/2508.04785)
*María Anguiano,Francisco J. Suárez-Grau*

Main category: math.AP

TL;DR: The paper studies the steady-state quasi-Newtonian Stokes flow in a thin domain with a rough boundary, deriving an effective 2D Reynolds model using asymptotic techniques.


<details>
  <summary>Details</summary>
Motivation: To understand the impact of slightly rough boundaries on fluid flow in thin domains and derive a practical model for lubrication applications.

Method: Asymptotic techniques, sharp a priori estimates, compactness results, and monotonicity arguments are used to derive the limit model.

Result: An effective nonlinear two-dimensional Reynolds model is derived, incorporating the effects of the oscillating boundary.

Conclusion: The derived model is useful for applications in lubrication regimes, capturing boundary roughness effects.

Abstract: This study investigates the asymptotic behavior of the steady-state
quasi-Newtonian Stokesflow with viscosity given by the Carreau law within a
thin domain, focusing on the effects of a slightly rough boundary of the
domain. Employing asymptotic techniques with respect to the domain's thickness,
we rigorously derive the effective nonlinear two-dimensional Reynolds model
describing the fluid flow. The mathematical analysis is based on deriving the
sharp a priori estimates and proving the compactness results of the rescaled
functions together with monotonicity arguments. The resulting limit model
incorporates contributions of the oscillating boundary and thus, it could prove
useful in the applications involving this lubrication regime.

</details>


### [13] [Regularity of solutions to degenerate and singular free boundary problems with volume constraint](https://arxiv.org/abs/2508.04856)
*T. M. Nascimento,X. H. Nguyen,P. R. Stinga*

Main category: math.AP

TL;DR: Existence and regularity of solutions for degenerate/singular elliptic free boundary problems with prescribed positivity set volume.


<details>
  <summary>Details</summary>
Motivation: To address challenges in degenerate and singular elliptic free boundary problems where the volume of the solution's positivity set is fixed.

Method: Proving existence and regularity of solutions under the given constraints.

Result: Demonstrated existence and regularity of solutions for the specified problems.

Conclusion: The study successfully establishes solutions for these complex free boundary problems with volume constraints.

Abstract: We prove existence and regularity of solutions to degenerate and singular
elliptic free boundary problems, where the volume of the positivity set of the
solution is prescribed.

</details>


### [14] [Transition from Continuous to Jumping Solutions in 2D Quasi-static Elastic Contact Problems with Coulomb Friction: the Mathematics Underlying the Onset of Brake Squeal](https://arxiv.org/abs/2508.04863)
*Patrick Ballard,Flaviana Iurlano*

Main category: math.AP

TL;DR: The paper analyzes the quasi-static elastic contact problem with Coulomb friction, proving existence of solutions under optimal friction conditions and showing spontaneous jumps as evidence of dynamic transitions.


<details>
  <summary>Details</summary>
Motivation: To understand the conditions for existence of solutions in quasi-static elastic contact problems with friction and explore the transition to dynamic behavior.

Method: Formulates the problem in a general setting, uses optimal friction conditions, and provides examples of solution jumps.

Result: Proves existence of absolutely continuous solutions under certain friction conditions; shows spontaneous jumps indicate dynamic transitions.

Conclusion: Spontaneous jumps in solutions reveal a shift from quasi-static to dynamic behavior, indicating onset of friction-induced vibrations.

Abstract: We formulate the quasi-static elastic contact problem with Coulomb friction
in a very general setting, with possible jumps in time for both the load and
the solution. Exploiting ideas originating in our recent paper [4], we exhibit
an optimal condition on the magnitude of the friction coefficient under which
we prove the existence of an absolutely continuous solution for arbitrary
absolutely continuous loads in the case of the most general 2D problem. We
provide examples showing that, when the condition is violated, spontaneous
jumps in time of the solution may occur, even when the load varies absolutely
continuously in time. We argue that these spontaneous jumps in time of the
solution in the quasi-static problem reveal a transition of the process from a
quasi-static nature to a dynamic nature, interpreted as the mathematical
signature of the onset of friction-induced vibrations in the elastodynamic
contact problem with dry friction.

</details>


### [15] [Parabolic abstract evolution equations in cylindrical domains and uniformly local Sobolev spaces](https://arxiv.org/abs/2508.05220)
*Joly Romain*

Main category: math.AP

TL;DR: The paper analyzes parabolic equations with transverse Hilbert space-valued solutions, focusing on the Cauchy problem in uniformly local spaces. It highlights issues with sectoriality and domain density, proposing a well-posed solution using abstract evolution equations.


<details>
  <summary>Details</summary>
Motivation: To study solutions with infinite energy for parabolic equations in transverse Hilbert spaces and address the ill-posedness of the Cauchy problem in uniformly local spaces.

Method: Uses the theory of parabolic abstract evolution equations to analyze the linear operator and establish well-posedness in weak uniformly local spaces.

Result: Shows the linear operator may not be sectorial due to non-dense domains but successfully sets a well-posed Cauchy problem.

Conclusion: Provides insights into uniformly local spaces and introduces a new example of differential operators with non-dense domains.

Abstract: In this article, we consider parabolic equations of the type $$\partial_t
u(x,t)=\Delta u(x,t) - Bu(x,t) + F(u(x,t))$$ where $u$ is valued in a
transverse Hilbert space $Y$ and $B$ is a positive self-adjoint operator on
$Y$, allowing a different diffusion mechanism in the transverse direction. We
aim at considering solutions with infinite energy and we study the Cauchy
problem in the uniformly local spaces associated with the norm
$$\|u\|_{L^2_{\text{ul}}(\mathbb{R},Y)}= \sup_{a\in\mathbb{R}^d}
\|u(x)\|_{L^2(B(a,1),Y)}.$$ For the classical parabolic equation, i.e. if
$Y=\mathbb{R}$, it is known that the Cauchy problem is ill-posed in the weak
version of the uniformly local spaces but well-posed in a stronger version,
where additional uniform continuity is required. In this paper, we show that
the linear operator $\partial^2_{xx} - B$ is not necessarily a sectorial
operator in any version of the uniformly local Lebesgue space, due to the
possible non-density of its domain. Then, we use the theory of parabolic
abstract evolution equations to set a well-posed Cauchy problem, even in the
weak version of the uniformly local space. In particular, we believe that this
paper offers a new perspective on the comparison between both versions of the
uniformly local spaces and also provides a new natural example of differential
operators with non-dense domain.

</details>


### [16] [Isometric Immersions and Weak Solutions to the Darboux Equation](https://arxiv.org/abs/2508.05230)
*Wentao Cao,Jonas Hirsch,Dominik Inauen*

Main category: math.AP

TL;DR: The paper extends the Darboux equation's weak solutions to low-regularity regimes (C^{1,θ}, θ>1/2) and validates their correspondence with isometric immersions.


<details>
  <summary>Details</summary>
Motivation: To address the Darboux equation in low-regularity settings, bridging gaps in the theory of isometric immersions for Riemannian manifolds.

Method: Introduces weak solutions for C^{1,θ} with θ>1/2 and extends the flatness criterion to Hölder continuous metrics via weak Gaussian curvature analysis.

Result: The classical link between Darboux equation solutions and isometric immersions holds even in low-regularity regimes.

Conclusion: The work successfully generalizes the Darboux equation's applicability, maintaining its foundational role in isometric immersion theory.

Abstract: We study the Darboux equation, a fundamental PDE arising in the theory of
isometric immersions of two-dimensional Riemannian manifolds into
$\mathbb{R}^3$, in the low-regularity regime. We introduce a notion of weak
solution for $u\in C^{1,\theta}$ with $\theta>1/2$, and show that the classical
correspondence between solutions of the Darboux equation and isometric
immersions remains valid in this regime. The key ingredient is an extension of
the classical flatness criterion to H\"older continuous metrics, achieved via
an analysis of a weak notion of Gaussian curvature.

</details>


### [17] [A viscosity solution as a piecewise classical solution to a free boundary problem for the optimal switching problem with simultaneous multiple switches](https://arxiv.org/abs/2508.05252)
*Kiyoshi Suzuki*

Main category: math.AP

TL;DR: The paper extends the work of Suzuki (2020) by proving the uniqueness of viscosity solutions for a variational inequality in optimal switching problems, and provides a method to compute free boundaries and identify switching regions analytically or numerically.


<details>
  <summary>Details</summary>
Motivation: To address the gap in Suzuki (2020) regarding the exact identification of switching regions and to provide a method for constructing viscosity solutions as piecewise classical solutions.

Method: Proves smooth pasting as a necessary condition, establishes an algorithm for free boundaries, and applies it to a concrete problem with Python.

Result: Explicit solutions and identification of continuation and switching regions are achieved.

Conclusion: The series of piecewise classical solutions is confirmed as the viscosity solution, and the method is validated through practical application.

Abstract: \citeN{suzuki2020optimal} proves the uniqueness of the viscosity solution to
a variational inequality which is solved by the value function of the infinite
horizon optimal switching problem with simultaneous multiple switchings.
Although it also identifies each connected region possibly including at most
one connected switching region, the exact switching regions of the solution are
not identified. The problem is finally converted into a system of free boundary
problems and generally solved by the numerical calculation. However, if the PDE
part of the variational inequality has a classical solution, the viscosity
solution may be constructed as a series of piecewise classical solutions,
possibly analytical.
  Under a certain assumption we prove that the series of piecewise classical
solutions is indeed the viscosity solution on $\real{}$, after we prove the
smooth pasting condition is its necessary condition, and establish the
algorithm to compute all the free boundaries. Applying the results to the
concrete problem studied in \citeN{suzuki2020optimal} we find the explicit
solution and identify the continuation and switching regions in a computer with
Python programs.

</details>


### [18] [Existence of spiral strategies for blocking fire spreading](https://arxiv.org/abs/2508.05324)
*Stefano Bianchini,Martina Zizza*

Main category: math.AP

TL;DR: The paper proves a sharp version of Bressan's Fire Conjecture for spiral-like barriers, showing confinement is possible only if construction speed exceeds a critical value.


<details>
  <summary>Details</summary>
Motivation: To determine the minimal construction speed for a spiral-like barrier to block a fire, addressing Bressan's Fire Conjecture.

Method: Uses spiral barrier definitions, Retarded Differential Equations, functional minimization, and numerical analysis.

Result: Confinement is possible if construction speed σ > critical speed σ̄ = 2.614; otherwise, it fails.

Conclusion: The critical speed σ̄ is the threshold for effective fire confinement using spiral barriers.

Abstract: In this paper we address the problem for blocking fire by constructing a wall
$\zeta$ whose shape is spiral-like. This is supposed to be the best strategy
when a single firefighter is constructing the wall with a finite construction
speed $\sigma$: the barriers which satisfy this bound on the construction speed
are called admissible.
  We prove a sharp version of Bressan's Fire Conjecture in this case, i.e. when
admissible barriers are spiral-like curves: namely, there exists a spiral-like
barrier confining the fire in a bounded region of $\mathbb R^2$ if and only if
the speed of construction of the barrier $\sigma$ is strictly larger than a
critical speed $\bar \sigma = 2.614...$.
  The existence of confining spiral barriers for $\sigma > \bar \sigma$ is
already known [Bressan A. et al., 2008, Klein R. et al., 2019], while we
concentrate on the negative side, i.e. if $\sigma \leq \bar \sigma$ no
admissible spiral blocks the fire.
  The proof of these results relies on: 1) the precise definition of spiral
barrier and its representation; 2) the analysis of saturated spiral barriers as
a Retarded Differential Equation (RDE) in the spirit of [Klein R. et al.,
2019]; 3) the equivalent reformulation of the conjecture as a minimum problem
for a prescribed functional; 4) the construction of the optimal closing spiral;
5) the analysis of a differentiable path of admissible spirals along which the
functional is differentiable, and in particular increasing when moving from the
optimal spiral to any other one (homotopy argument).
  Due to the complexity of the solution, the evaluation of the quantities
needed to prove that the functional is increasing is performed numerically.

</details>


### [19] [Geometrical characterizations of radiating and non-radiating elastic sources and mediums with applications](https://arxiv.org/abs/2508.05401)
*Huaian Diao,Xiaoxu Fei,Hongyu Liu*

Main category: math.AP

TL;DR: The paper investigates two types of time-harmonic elastic wave scattering problems, deriving quantitative results about scatterer geometry and physical parameters. It shows that certain scatterers must radiate at any frequency and establishes uniqueness results for determining scatterer support from far-field measurements.


<details>
  <summary>Details</summary>
Motivation: To understand the geometric and physical properties of scatterers in elastic wave problems and derive uniqueness results for their identification.

Method: Uses Helmholtz decomposition, Lamé operator estimates, global energy estimates, and combines complex geometric optics (CGO) solutions with local regularity estimates.

Result: Demonstrates that scatterers with small support or high-curvature boundaries radiate at any frequency, and establishes local/global uniqueness results for scatterer identification.

Conclusion: The study provides new insights into elastic wave scattering, enabling unique identification of scatterers and revealing geometric properties of transmission eigenfunctions.

Abstract: In this paper, we investigate two types of time-harmonic elastic wave
scattering problems. The first one involves the scattered wave generated by an
active elastic source with compact support. The second one concerns elastic
wave scattering caused by an inhomogeneous medium, also with compact support.
We derive several novel quantitative results concerning the geometrical
properties of the underlying scatterer, the associated source or incident wave
field, and the physical parameters. In particular, we show that a scatterer
with either a small support or high-curvature boundary points must radiate at
any frequency. These qualitative characterizations allow us to establish
several local and global uniqueness results for determining the support of the
source or medium scatterer from a single far-field measurement. Furthermore, we
reveal new geometric properties of elastic transmission eigenfunctions. To
derive a quantitative relationship between the intensity of a radiating or
non-radiating source and the diameter of its support, we utilize the Helmholtz
decomposition, the translation-invariant $L^2$-norm estimate for the Lam\'e
operator, and global energy estimates. Another pivotal technical approach
combines complex geometric optics (CGO) solutions with local regularity
estimates, facilitating microlocal analysis near admissible $K$-curvature
boundary points.

</details>


### [20] [Modulation of the Monokinetic Limit for Models of Collective Dynamics](https://arxiv.org/abs/2508.05478)
*Alina Chertock,Roman Shvydkoy,Trevor Teolis*

Main category: math.AP

TL;DR: The paper analyzes monokinetic limits in the kinetic Cucker-Smale model, comparing two regimes: strong Fokker-Planck force with vanishing noise and a noiseless Vlasov scheme. Results show convergence to Gaussian and transport equation profiles, respectively.


<details>
  <summary>Details</summary>
Motivation: To understand the transition from kinetic models to macroscopic systems, specifically focusing on the Cucker-Smale model's monokinetic limits.

Method: Modulation analysis is applied to two regimes: (1) strong Fokker-Planck force with vanishing noise and Knudsen number, and (2) a pure noiseless Vlasov scheme.

Result: In the first regime, the modulated profile converges to a Gaussian distribution. In the second, it converges to a profile satisfying an explicit transport equation.

Conclusion: The study successfully links kinetic models to macroscopic systems, providing insights into convergence behaviors under different conditions.

Abstract: In this work, we perform modulation analysis of monokinetic limits from the
kinetic Cucker- Smale model to the pressureless Euler alignment system. Two
regimes are considered -- a strong Fokker- Planck force with vanishing noise
and Knudsen number, and a pure noiseless Vlasov scheme. In the former case, we
demonstrate convergence of the modulated profile to the standard Gaussian
distribution, while in the latter case, the distribution converges to a profile
satisfying an explicit transport equation along limiting characteristics.

</details>


### [21] [Velocity optimization of self-equilibrated obstacles in a two-dimensional viscous flow](https://arxiv.org/abs/2508.05481)
*Gilles A. Francfort,Alessandro Giacomini,Scott Weady*

Main category: math.AP

TL;DR: Study of self-equilibration and shape optimization for obstacles in 2D Stokes/Navier-Stokes fluids under steady-state conditions.


<details>
  <summary>Details</summary>
Motivation: To understand and optimize obstacle behavior in fluid dynamics, addressing challenges in defining equilibration for a broad class of obstacles.

Method: Revisits self-equilibration in measure-theoretic terms for Stokes and Navier-Stokes fluids, allowing general shape variations.

Result: Framework for analyzing and optimizing obstacle velocities (translational/angular) by shape variation.

Conclusion: Provides a theoretical foundation for optimizing obstacle dynamics in complex fluid environments.

Abstract: An obstacle is immersed in an externally driven 2D Stokes or Navier-Stokes
fluid. We study the self-equilibration conditions for that obstacle under
steady state assumptions on the flow. We then seek to optimize the
translational and/or angular velocity of the obstacle by varying its shape. To
allow general variations, we must consider a very large class of obstacles for
which the notion of trace is meaningless. This forces us to revisit the notion
of self-equilibration for both Stokes and Navier-Stokes in a measure theoretic
environment.

</details>


### [22] [The $L^p$ boundedness of wave operators for the Laplace operator with finite rank perturbations](https://arxiv.org/abs/2508.05533)
*Han Cheng,Shanlin Huang,Avy Soffer,Zhao Wu*

Main category: math.AP

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper investigates the $L^p$ boundedness of wave operators for the
Laplace operator with finite rank perturbations \begin{equation*}
  H=-\Delta+\sum\limits_{i=1}^N\langle\cdot\,, \varphi_i\rangle \varphi_i
\qquad \mbox{on}\,\,\, \R^d. \end{equation*} For dimensions $d\ge 3$, we prove
that the wave operators $W_\pm(H,H_0)$ are bounded on $L^p$ for the full range
$1\le p\le \infty$. This extends the work of Nier and the third author
\cite{NS} by resolving the previously unexplored question of boundedness at the
endpoint cases $p=1$ and $p=\infty$. In lower dimensions $d = 1, 2$, we
establish the $L^p$-boundedness of the wave operators for the first time.
Furthermore, we reveal an intriguing dichotomy in the endpoint case $p = 1$:
\begin{itemize}
  \item If $\int_{\mathbb{R}^d} \varphi_i(x) \, \d x = 0$ holds for every $1\le
i\le N$, then the wave operators are bounded on $L^p(\mathbb{R}^d)$ for all $1
\leq p \leq \infty$.
  \item If there exists at least one $i$ ($1\le i\le N$) such that
$\int_{\mathbb{R}^d}\varphi_i(x)\d x\ne0$, then the wave operators remain
bounded for $1 < p < \infty$ and satisfy weak type $(1,1)$ estimates, but fail
to be bounded on $L^1(\mathbb{R}^d)$. \end{itemize}

</details>


### [23] [On a general class of free boundary Monge-Ampère equations](https://arxiv.org/abs/2508.05551)
*Tristan C. Collins,Benjy Firester*

Main category: math.AP

TL;DR: The paper addresses a general class of free boundary Monge-Ampère equations, with applications in optimal transport, eigenvalue problems, and geometric problems.


<details>
  <summary>Details</summary>
Motivation: To solve complex Monge-Ampère equations with free boundaries, extending applications to diverse mathematical problems.

Method: Analyzes the equation involving determinants and gradients, with constraints on the domain and function properties.

Result: Provides solutions to the equations, applicable to optimal transport, eigenvalue problems, and geometric scenarios.

Conclusion: The framework extends the understanding and solvability of Monge-Ampère equations in various mathematical contexts.

Abstract: We solve a general class of free boundary Monge-Amp\`ere equations given by
\[
  \det D^2u = \lambda \dfrac{f(-u)}{g(u^\star)h(\nabla u)}\chi_{\{u<0\}} \;
\text{ in } \mathbb{R}^n, \quad \nabla u (\mathbb{R}^n) = P \] where $P$ is a
bounded convex set containing the origin, and $h>0$ on $P$. We consider
applications to optimal transport with degenerate densities, Monge-Amp\`ere
eigenvalue problems, and geometric problems including a hemispherical Minkowski
problem and free boundary K\"ahler-Ricci solitons on toric Fano manifolds.

</details>


### [24] [Bounds for spectral projectors on the three-dimensional torus](https://arxiv.org/abs/2508.05573)
*Pierre Germain,Simon L. Rydin Myerson,Daniel Pezzi*

Main category: math.AP

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study $L^2$ to $L^p$ operator norms of spectral projectors for the
Euclidean Laplacian on the torus in the case where the spectral window is
narrow. With a window of constant size this is a classical result of Sogge; in
the small-window limit we are left with $L^p$ norms of eigenfunctions of the
Laplacian, as considered for instance by Bourgain. For the three-dimensional
torus we prove new cases of a previous conjecture of the first two authors
concerning the size of these norms; we also refine certain prior results to
remove $\epsilon$-losses in all dimensions. We use methods from number theory:
the geometry of numbers, the circle method and exponential sum bounds due to
Guo. We complement these techniques with height splitting and a bilinear
argument to prove sharp results.
  We exposit on the various techniques used and their limitations.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [25] [Hyperbolic tiling neighborhoods in O(1) time](https://arxiv.org/abs/2508.04765)
*Yanick Thurn,Manuel Schrauth,Johanna Erdmenger*

Main category: physics.comp-ph

TL;DR: A novel method for constructing hyperbolic tilings and their adjacency graphs simultaneously using combinatoric rules, outperforming existing recursive algorithms.


<details>
  <summary>Details</summary>
Motivation: Current recursive methods for hyperbolic tilings lack efficient adjacency graph generation, limiting practical applications.

Method: Uses combinatoric rules to build tilings and graphs without explicit coordinates, enabling scalable and exact constructions.

Result: Generates arbitrarily large hyperbolic graphs with size-independent complexity, outperforming existing methods.

Conclusion: The approach enables large-scale simulations and broadens accessibility for scientific applications.

Abstract: Tilings of the hyperbolic plane are of significant interest among many
branches of mathematics, physics and computer science. Yet, their construction
remains a non-trivial task. Current approaches primarily use tree-based
recursive algorithms, which are fundamentally limited: they do not readily
yield the neighborhood graph representing cell adjacencies, which is however
required for many applications. We introduce a novel approach that allows to
build hyperbolic tilings and their associated graph structure simultaneously,
using only combinatoric rules without requiring an explicit coordinate
representation. This allows to generate arbitrarily large, exact hyperbolic
graphs, with an algorithmic complexity that does not depend on the lattice
size. We provide an easy-to-use implementation which substantially outperforms
existing methods, hence rendering ultra large-scale numerical simulations on
these geometric structures accessible for the scientific community.

</details>


### [26] [Identifying Optimal Regression Models For DEM Simulation Datasets](https://arxiv.org/abs/2508.05308)
*B. D. Jenkins,A. L. Nicusan,A. Neveu,G. Lumay,F. Francqui,J. P. K. Seville,C. R. K. Windows-Yule*

Main category: physics.comp-ph

TL;DR: The paper proposes a framework using k-fold cross-validation to benchmark regression models for DEM data, identifying a histogram-based gradient boosting model as optimal among 16 tested.


<details>
  <summary>Details</summary>
Motivation: Fast and accurate regression models are needed for real-time industrial applications, but model benchmarking is often overlooked, leading to suboptimal choices.

Method: The study applies k-fold cross-validation to evaluate regression models on DEM datasets, focusing on packing fractions with varying particle properties.

Result: A histogram-based gradient boosting model outperformed 15 others, offering good accuracy and efficiency.

Conclusion: The proposed framework aids in selecting optimal regression models for DEM data, improving real-time decision-making and process optimization.

Abstract: Developing fast regression models (surrogate/metamodels) from DEM data is key
for practical industrial application to allow real-time evaluations. However,
benchmarking different models is often overlooked in particle technology for
regression tasks, as model selection is frequently not the primary research
focus. This can lead to the use of suboptimal models, resulting in subpar
predictive accuracy, slow evaluations, or poor generalisation, hindering
effective real-time decision-making and process optimisation. In this work, we
discuss applying k-fold cross-validation to assess regression models for
tabular DEM datasets and propose a simple framework for readers to follow to
find the optimal model for their data. An example demonstrates its application
to a DEM dataset of packing fractions measured in a simple measuring beaker
with varying inter-particle properties, namely, average particle diameter,
coefficient of restitution, coefficient of sliding friction, coefficient of
rolling resistance, and cohesive energy density. Out of 16 different models
tested, a histogram-based gradient boosting model was found to be optimal,
providing a good fit with acceptable training and inference times.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [27] [Electrodeless Magnetohydrodynamic Local Force Generator for Aerocapture](https://arxiv.org/abs/2508.04806)
*Bernard Parent,Felipe Martin Rodriguez Fuentes,Spencer LaFoley*

Main category: physics.plasm-ph

TL;DR: A novel electrodeless MHD system for planetary entry aerocapture is proposed, outperforming previous methods by generating stronger forces without electrodes.


<details>
  <summary>Details</summary>
Motivation: To improve planetary entry aerocapture by addressing limitations of existing MHD systems, which were either global and electrodeless or local with electrodes.

Method: The system uses two magnets to create a Faraday EMF, avoiding electrodes. Simulations with CFDWARP, a CFD code, model aerodynamics, MHD, and plasma sheaths.

Result: The electrodeless system generates forces several times greater than electrode-based systems at the same magnetic field strength.

Conclusion: The novel MHD system offers a superior alternative for planetary entry aerocapture, combining local deployment and electrode-free operation.

Abstract: This paper presents a novel magnetohydrodynamics (MHD) system for planetary
entry aerocapture. The system is advantaged over previous approaches by having
the following two characteristics: (i) it can be deployed locally to one or
various flow regions, and (ii) it does not make use of electrodes. Previous MHD
systems for planetary entry were either electrodeless global systems or
two-electrode local systems. The proposed novel MHD system employs two magnets
to establish a current loop resulting in a Faraday electromotive force (EMF).
The first magnet is positioned to ensure the magnetic field faces outward from
the shell, while the second magnet is oriented to ensure the magnetic field
faces inward toward the shell. Preliminary findings demonstrate that when
located on the surface of an Earth entry capsule at a flight Mach number of 35,
the novel electrodeless MHD system can generate forces several times greater
than a two-electrode system while utilizing the same magnetic field strength.
The study is conducted entirely through numerical simulation using CFDWARP, a
computational fluid dynamics (CFD) code that employs advanced numerical methods
allowing for the full coupling between aerodynamics, magnetohydrodynamics, and
non-neutral plasma sheaths. The physical model includes an 11-species
finite-rate chemical solver including real gas effects, the drift-diffusion
model for all charged species, along with an electric field potential equation
that satisfies Gauss's law.

</details>


### [28] [Magnetic shear effects on ballooning turbulence in the boundary of fusion devices](https://arxiv.org/abs/2508.04881)
*Z. Tecchiolli,A. J. Coelho,J. Loizu,B. De Lucca,P. Ricci*

Main category: physics.plasm-ph

TL;DR: The study examines how magnetic shear affects ballooning-driven plasma edge turbulence, showing that high shear leads to scale separation (kx << ky), while low shear results in kx ~ ky, aligning with stellarator simulations.


<details>
  <summary>Details</summary>
Motivation: To understand the impact of magnetic shear on turbulence properties and validate previous predictions about pressure gradients and transport in plasma edges.

Method: Nonlinear 3D simulations (GBS code), linear numerical investigations, and analytical studies of ballooning modes and poloidal mode coupling.

Result: High magnetic shear causes kx << ky, while low shear leads to kx ~ ky. Linear and analytical studies confirm the transition in mode structure with shear variation.

Conclusion: Magnetic shear significantly influences turbulence scale separation, with implications for predicting plasma edge behavior in fusion devices.

Abstract: The effect of magnetic shear on ballooning-driven plasma edge turbulence is
studied through nonlinear simulations complemented by linear numerical and
analytical investigations. Nonlinear, 3D, global, flux-driven simulations using
the GBS code show that the scale separation between radial, x, and poloidal, y,
size of turbulent eddies, kx << ky , considered by Ricci et al. (2008) and
extensively used to predict pressure gradient lengths, SOL width, particle and
heat fluxes, is observed with high magnetic shear. In contrast, for low
magnetic shear, kx ~ ky is observed, with fluctuation properties resembling
those shown by recent low-shear stellarator simulations reported in Coelho et
al. (2024a). Global linear investigations of the ballooning mode qualitatively
captures the transition in mode structure with varying magnetic shear, showing
that kx << ky is achieved with sufficiently strong poloidal mode coupling
enhanced by increasing magnetic shear, resistivity, toroidal mode number, and
equilibrium gradient scale length. This is confirmed by an analytical study
considering a dominant poloidal mode and its sidebands, which highlights that
the poloidal mode structure is determined by curvature and k parallel effects

</details>


### [29] [Information Propagation in Predator-Prey Dynamics of Turbulent Plasma](https://arxiv.org/abs/2508.05127)
*Tomohiro Tanogami,Makoto Sasaki,Tatsuya Kobayashi*

Main category: physics.plasm-ph

TL;DR: The paper explores cyclic oscillations in fusion plasmas using a stochastic predator-prey model, suggesting observed oscillations are quasi-cycles, not limit cycles, and demonstrates information flow from zonal flow to turbulence.


<details>
  <summary>Details</summary>
Motivation: To understand the mechanism behind cyclic oscillations in magnetically confined fusion plasmas and the interaction between drift-wave turbulence and zonal flow.

Method: Construct a stochastic predator-prey model incorporating intrinsic fluctuations and analyze its statistical properties using information theory.

Result: The model shows quasi-cycles due to noise amplification, suggesting observed plasma oscillations are quasi-cycles. Information flows from zonal flow to turbulence.

Conclusion: The study provides a theoretical basis for turbulence regulation by controlling zonal flow, with quasi-cycles potentially common under various conditions.

Abstract: Magnetically confined fusion plasmas exhibit predator-prey-like cyclic
oscillations through the self-regulating interaction between drift-wave
turbulence and zonal flow. To elucidate the detailed mechanism and causality
underlying this phenomenon, we construct a simple stochastic predator-prey
model that incorporates intrinsic fluctuations and analyze its statistical
properties from an information-theoretic perspective. We first show that the
model exhibits persistent fluctuating cyclic oscillations called quasi-cycles
due to amplification of intrinsic noise. This result suggests the possibility
that the previously observed periodic oscillations in a toroidal plasma are not
limit cycles but quasi-cycles, and that such quasi-cycles may be widely
observed under various conditions. For this model, we further prove that
information of zonal flow is propagated to turbulence. This
information-theoretic analysis may provide a theoretical basis for regulating
turbulence by controlling zonal flow.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [30] [Constitutive modeling of viscoelastic solids at large strains based on the theory of evolving natural configurations](https://arxiv.org/abs/2508.05043)
*Tarun Singh,Sandipan Paul*

Main category: cond-mat.soft

TL;DR: The paper revisits nonlinear viscoelastic solid models using the theory of evolving natural configurations, comparing Maxwell and Kelvin-Voigt models and their standard solids, with numerical validation against experimental data.


<details>
  <summary>Details</summary>
Motivation: To improve modeling of dissipative processes in viscoelastic solids by leveraging the theory of evolving natural configurations.

Method: Develops Lagrangian-based models for Maxwell and Kelvin-Voigt materials, introduces strain-space and stress-space formulations, and derives standard solid models (Zener and Poynting-Thompson). Numerical algorithms are implemented and validated.

Result: The Poynting-Thompson model aligns well with experimental data, especially in polymer stretching. Relaxation and rate effects are analyzed.

Conclusion: The proposed models effectively capture viscoelastic behavior, with the Poynting-Thompson model showing strong experimental agreement.

Abstract: The theory of evolving natural configurations is an effective technique to
model dissipative processes. In this paper, we use this theory to revisit
nonlinear constitutive models of viscoelastic solids. Particularly, a Maxwell
and a Kelvin-Voigt model and their associated standard solids, viz., a Zener
and a Poynting-Thompson solids respectively, have been modeled within a
Lagrangian framework. We show that while a strain-space formulation of the
evolving natural configurations is useful in modeling Maxwell-type materials, a
stress-space formulation that incorporates a rate of dissipation function in
terms of the relevant configurational forces is required for modeling the
Kelvin-Voigt type materials. Furthermore, we also show that the basic Maxwell
and Kelvin-Voigt models can be obtained as limiting cases from the derived
standard solid models. Integration algorithms for the proposed models have been
developed and numerical solutions for a relevant boundary value problem are
obtained. The response of the developed models have been compared and
benchmarked with experimental data. Specifically, the response of the novel
Poynting-Thompson model is studied in details. This model shows a very good
match with the existing experimental data obtained from a uniaxial stretching
of polymers over a large extent of strain. The relaxation behavior and rate
effects for the developed models have been studied.

</details>


<div id='physics.class-ph'></div>

# physics.class-ph [[Back]](#toc)

### [31] [A Time-Domain Method of Auxiliary Sources for Efficient Analysis of Transient Electromagnetic Scattering by Moderately Conductive Cylinders](https://arxiv.org/abs/2508.05217)
*Minas Kouroublakis,Nikolaos L. Tsitsas,Yehuda Leviatan*

Main category: physics.class-ph

TL;DR: The paper introduces a time-domain MAS-SIBC method for electromagnetic scattering in moderately conductive cylindrical scatterers, validated through numerical tests.


<details>
  <summary>Details</summary>
Motivation: To address the need for efficient modeling of scattering in moderately conductive materials (e.g., carbon-based composites, conductive polymers) used in RF applications.

Method: Combines Method of Auxiliary Sources (MAS) with Standard Impedance Boundary Condition (SIBC) in 2D, focusing on surface effects without internal field computation.

Result: Validated against analytical solutions and commercial solvers, showing accuracy for various scatterer geometries.

Conclusion: Time-domain MAS-SIBC is efficient and promising for modeling scattering in moderately conductive materials.

Abstract: This paper presents a time-domain implementation of the Method of Auxiliary
Sources (MAS) combined with the Standard Impedance Boundary Condition (SIBC)
for electromagnetic scattering problems involving cylindrical scatterers with
finite but moderate conductivity. The proposed approach focuses on solving the
two-dimensional problem using a first-order SIBC, which is valid when the
conductivity is sufficiently higher than the maximum spectral frequency times
the dielectric permittivity of the scatterer. This regime includes moderately
conductive materials--such as carbon-based composites, conductive polymers, and
doped dielectrics--that are increasingly used in real-world radio-frequency
applications, including wearable electronics, electromagnetic interference
shielding, and biomedical sensors. Under the above validity conditions, the
interaction between the incident wave and the scatterer is dominated by surface
effects, allowing for an efficient and accurate modeling strategy without the
need to compute internal fields. The theoretical formulation of the time-domain
MAS-SIBC method is developed, followed by extensive numerical testing on
various geometries whose cross section is a closed curve. Such geometries
include circular, elliptical, super-circular, rounded-triangular, and
inverted-elliptical scatterers. A planar geometry is also tested. All results
are validated against analytical solutions and commercial frequency-domain
solvers, demonstrating the accuracy and practical potential of the proposed
method. The findings suggest that time-domain MAS-SIBC offers a promising and
computationally efficient approach for modeling scattering from materials even
with moderate conductivity.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [32] [The use of open boundaries in stochastic hydrodynamic models of nucleation](https://arxiv.org/abs/2508.05528)
*James F. Lutsko*

Main category: cond-mat.stat-mech

TL;DR: The paper explores stochastic hydrodynamics for modeling first-order phase transitions, focusing on open boundary conditions to better match experimental conditions, while addressing challenges like preserving fluctuation-dissipation relations and dynamic stability.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between computational models and experimental conditions by using open boundary conditions in stochastic hydrodynamics, despite potential issues with the fluctuation-dissipation relation and dynamic stability.

Method: Utilizes stochastic hydrodynamics combined with free energy models (e.g., Density Functional Theory) and introduces open boundary conditions to simulate infinite-volume systems.

Result: Open boundary conditions bring computational models closer to experimental conditions but raise questions about the stochastic model's integrity, such as the preservation of fluctuation-dissipation relations and dynamic stability.

Conclusion: Open boundary conditions are promising for aligning simulations with experiments but require careful consideration of their impact on stochastic models and dynamic properties.

Abstract: Stochastic hydrodynamics is a central tool in the study of first order phase
transitions at a fundamental level. Combined with sophisticated free energy
models, e.g. as developed in classical Density Functional Theory, complex
processes such as crystallization can be modeled and information such as free
energy barriers, nucleation pathways and the unstable eigenvector and
eigenvalues determined. The latter are particularly interesting as they play
key roles in defining the natural (unbiased) order parameter and the nucleation
rate respectively. As is often the case, computational realities restrict the
size of system that can be modeled and this makes it difficult to achieve
experimental conditions for which the volume is effectively infinite. In this
paper, the use of open boundary conditions is discussed. By using an open
system, the calculations become much closer to experimental conditions however,
the introduction of open boundary conditions raises a number of questions
concerning the stochastic model such as whether the fluctuation-dissipation
relation is preserved and whether stationary points on the free energy surface
remain stationary points of the dynamics.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [33] [The Missing Reward: Active Inference in the Era of Experience](https://arxiv.org/abs/2508.05619)
*Bo Wen*

Main category: cs.AI

TL;DR: Active Inference (AIF) offers a foundation for autonomous AI agents to learn from experience without human reward engineering, addressing scalability challenges in current paradigms.


<details>
  <summary>Details</summary>
Motivation: Current AI systems face scalability issues due to reliance on human reward engineering and high-quality training data, hindering autonomous intelligence.

Method: Proposes using AIF to replace external rewards with an intrinsic drive to minimize free energy, integrating Large Language Models as generative world models.

Result: AIF enables agents to autonomously balance exploration and exploitation, learning efficiently while aligning with human values.

Conclusion: AIF bridges the grounded-agency gap, offering a path to autonomous AI development within computational and physical constraints.

Abstract: This paper argues that Active Inference (AIF) provides a crucial foundation
for developing autonomous AI agents capable of learning from experience without
continuous human reward engineering. As AI systems begin to exhaust
high-quality training data and rely on increasingly large human workforces for
reward design, the current paradigm faces significant scalability challenges
that could impede progress toward genuinely autonomous intelligence. The
proposal for an ``Era of Experience,'' where agents learn from self-generated
data, is a promising step forward. However, this vision still depends on
extensive human engineering of reward functions, effectively shifting the
bottleneck from data curation to reward curation. This highlights what we
identify as the \textbf{grounded-agency gap}: the inability of contemporary AI
systems to autonomously formulate, adapt, and pursue objectives in response to
changing circumstances. We propose that AIF can bridge this gap by replacing
external reward signals with an intrinsic drive to minimize free energy,
allowing agents to naturally balance exploration and exploitation through a
unified Bayesian objective. By integrating Large Language Models as generative
world models with AIF's principled decision-making framework, we can create
agents that learn efficiently from experience while remaining aligned with
human values. This synthesis offers a compelling path toward AI systems that
can develop autonomously while adhering to both computational and physical
constraints.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [34] [Provable Post-Training Quantization: Theoretical Analysis of OPTQ and Qronos](https://arxiv.org/abs/2508.04853)
*Haoyu Zhang,Shihao Zhang,Ian Colbert,Rayan Saab*

Main category: cs.LG

TL;DR: The paper provides the first quantitative error bounds for OPTQ (GPTQ) and Qronos, analyzing their iterative quantization errors and offering theoretical justification for practical design choices.


<details>
  <summary>Details</summary>
Motivation: Despite OPTQ's widespread use in PTQ for deep neural networks, it lacks rigorous theoretical guarantees. This paper aims to fill that gap.

Method: The authors derive non-asymptotic 2-norm error bounds for deterministic and stochastic OPTQ variants, and extend the analysis to Qronos.

Result: The analysis justifies practical heuristics (e.g., feature ordering by norm) and provides guidance for parameter selection. Stronger infinity-norm bounds are established for stochastic OPTQ.

Conclusion: The paper enhances understanding of OPTQ and Qronos, offering theoretical insights that support their empirical success and practical use.

Abstract: Post-training quantization (PTQ) has become a crucial tool for reducing the
memory and compute costs of modern deep neural networks, including large
language models (LLMs). Among PTQ algorithms, the OPTQ framework-also known as
GPTQ-has emerged as a leading method due to its computational efficiency and
strong empirical performance. Despite its widespread adoption, however, OPTQ
lacks rigorous quantitative theoretical guarantees. This paper presents the
first quantitative error bounds for both deterministic and stochastic variants
of OPTQ, as well as for Qronos, a recent related state-of-the-art PTQ
algorithm. We analyze how OPTQ's iterative procedure induces quantization error
and derive non-asymptotic 2-norm error bounds that depend explicitly on the
calibration data and a regularization parameter that OPTQ uses. Our analysis
provides theoretical justification for several practical design choices,
including the widely used heuristic of ordering features by decreasing norm, as
well as guidance for selecting the regularization parameter. For the stochastic
variant, we establish stronger infinity-norm error bounds, which enable control
over the required quantization alphabet and are particularly useful for
downstream layers and nonlinearities. Finally, we extend our analysis to
Qronos, providing new theoretical bounds, for both its deterministic and
stochastic variants, that help explain its empirical advantages.

</details>


### [35] [Deep Neural Networks with General Activations: Super-Convergence in Sobolev Norms](https://arxiv.org/abs/2508.05141)
*Yahong Yang,Juncai He*

Main category: cs.LG

TL;DR: Deep neural networks with general activations achieve superior approximation rates in Sobolev spaces, outperforming classical methods like finite elements and spectral methods, termed as 'super-convergence.'


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in error-estimation theory for neural-network-based PDE solutions and provide a unified theoretical foundation for their use in scientific computing.

Method: Analysis of deep fully-connected neural networks with general activation functions in Sobolev spaces, measuring errors in the $W^{m,p}$-norm for $m < n$.

Result: Deep networks surpass classical numerical methods in approximating weak PDE solutions, demonstrating 'super-convergence.'

Conclusion: This work establishes a comprehensive theoretical foundation for neural networks in PDE approximation, highlighting their superior accuracy over traditional methods.

Abstract: This paper establishes a comprehensive approximation result for deep
fully-connected neural networks with commonly-used and general activation
functions in Sobolev spaces $W^{n,\infty}$, with errors measured in the
$W^{m,p}$-norm for $m < n$ and $1\le p \le \infty$. The derived rates surpass
those of classical numerical approximation techniques, such as finite element
and spectral methods, exhibiting a phenomenon we refer to as
\emph{super-convergence}. Our analysis shows that deep networks with general
activations can approximate weak solutions of partial differential equations
(PDEs) with superior accuracy compared to traditional numerical methods at the
approximation level. Furthermore, this work closes a significant gap in the
error-estimation theory for neural-network-based approaches to PDEs, offering a
unified theoretical foundation for their use in scientific computing.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [36] [Model-based framework for automated quantification of error sources in quantum state tomography](https://arxiv.org/abs/2508.05538)
*Junpei Oba,Hsin-Pin Lo,Yasuhiro Yamada,Takayuki Matsui,Takuya Ikuta,Yuya Yonezu,Toshimori Honjo,Seiji Kajita,Hiroki Takesue*

Main category: quant-ph

TL;DR: Proposes an automated method to quantify error sources in quantum state generation by combining simulation and parameter optimization, improving state quality and validating the approach.


<details>
  <summary>Details</summary>
Motivation: High-quality quantum states are crucial for quantum information processing, but error sources degrade quality. Current QST methods struggle to identify individual errors.

Method: Combines simulation and parameter optimization to model error sources and reproduce experimental density matrices, focusing on time-bin entangled photon pairs.

Result: Reduced trace distance from 0.177 to 0.024, explaining 86% of errors, and improved state quality validated the method.

Conclusion: The framework is effective for identifying and mitigating errors in quantum state generation and is adaptable to other quantum platforms.

Abstract: High-quality quantum state generation is essential for advanced quantum
information processing, including quantum communication, quantum sensing, and
quantum computing. In practice, various error sources degrade the quality of
quantum states, and quantum state tomography (QST) is a standard diagnostic
tool. However, in QST, multiple error sources gather in a single density
matrix, making it difficult to identify individual error sources. To address
this problem, we propose an automated method for quantifying error sources by
combining simulation and parameter optimization to reproduce the experimental
density matrix. We focus on the experimental generation of time-bin entangled
photon pairs, for which we model the relevant error sources and simulate the
density matrix with adjustable model parameters, thereby optimizing the
parameters and minimizing the trace distance to the experimental data.
Optimization of the parameters reduced the trace distance from 0.177 to 0.024,
indicating that our modeled error sources explain 86% of the errors. Reducing
the predicted error sources improves the state quality, consistent with our
predictions and thus validating the proposed method. In addition, the modular
structure of this framework makes it applicable to other quantum platforms,
such as superconducting qubits, atoms, and solid-state spins.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [37] [Simulation of Non-Premixed, Supersonic Combustion using the Discontinuous Galerkin Method on Fully Unstructured Grids](https://arxiv.org/abs/2508.04930)
*Cal J. Rising,Eric J. Ching,Ryan F. Johnson*

Main category: physics.flu-dyn

TL;DR: 3D simulations of a hydrogen jet in supersonic crossflow using DG methods show grid and polynomial order sensitivity, with DG(p=2) matching experiments. High resolution is needed for accurate shock and flame stabilization. The work pioneers unstructured tetrahedral mesh use for such flows.


<details>
  <summary>Details</summary>
Motivation: To develop an accurate and efficient approach for simulating high-speed airbreathing propulsion vehicles using DG methods.

Method: Structure-preserving discontinuous Galerkin (DG) formulation for 3D simulations of a reacting hydrogen jet in supersonic crossflow, examining grid and polynomial order sensitivities.

Result: DG(p=2) agrees with experiments; high resolution captures separation shock and flame stabilization. Mixing/combustion is non-premixed diffusion mode, consistent with prior simulations.

Conclusion: The study demonstrates DG methods' potential for complex geometries and physics, pioneering unstructured tetrahedral mesh use for high-speed reacting flows.

Abstract: In this study, three-dimensional simulations of a reacting hydrogen jet in
supersonic crossflow using a structure-preserving discontinuous Galerkin (DG)
formulation are examined. The hydrogen jet, with a momentum flux ratio of five,
is injected into a high enthalpy crossflow. The sensitivities of the solution
to the grid element size and polynomial order are investigated to determine an
accurate and computationally efficient approach to simulating high-speed
airbreathing propulsion vehicles. The results demonstrate that DG(p = 2)
solutions, which are nominally third-order accurate in smooth regions of the
flow, show reasonable agreement with existing experimental results. The
separation shock formation behind the jet is found to be heavily grid dependent
and necessary for accurate simulations of the reacting jet in supersonic
crossflow. It is determined that the highest resolution cell and polynomial
order is required to capture the upstream separation shock and consequently the
flame stabilization point. The mixing and combustion mode is also determined
using the flame index and demonstrates the flow is heavily skewed towards a
non-premixed diffusion mode which is consistent with previously run simulations
of this case using traditional finite volume schemes and sub grid scale
modeling approaches. Beyond this analysis, the novelty of this work lies in
demonstrating a high-speed, multicomponent, chemically reacting flow on a fully
unstructured tetrahedral mesh: a first-of-its-kind calculation. This highlights
the potential of these methods for simulating fluids in complex geometries with
complex physics

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [38] [Optimization of Ab-Initio Based Tight-Binding Models](https://arxiv.org/abs/2508.04861)
*Henrik Dick,Thomas Dahm*

Main category: cond-mat.mtrl-sci

TL;DR: The paper introduces a machine-learning-inspired method to create tight-binding models for electronic structure calculations, achieving better accuracy with fewer parameters than traditional methods like Wannier functions.


<details>
  <summary>Details</summary>
Motivation: Traditional methods like density functional theory and Wannier functions are limited in accuracy and complexity for interfaces, grain boundaries, or contact geometries. A simpler, more accurate approach is needed.

Method: A machine-learning-inspired procedure optimizes tight-binding model parameters to closely match ab-initio band structure data while minimizing the number of parameters.

Result: The new method produces models with smaller ranges and fewer orbitals than Wannier functions, yet maintains or improves accuracy.

Conclusion: This approach is more efficient for automated, large-scale materials calculations, offering a practical alternative to traditional methods.

Abstract: The electronic structure of solids can routinely be calculated by standard
methods like density functional theory. However, in complicated situations like
interfaces, grain boundaries or contact geometries one needs to resort to more
simplified models of the electronic structure. Tight-binding models are using a
reduced set of orbitals and aim to approximate the electronic structure by
short range hopping processes. For example, maximally localized Wannier
functions are often used for that purpose. However, their accuracy is limited
by the need to disentangle the electronic bands. Here, we develop and
investigate a different procedure to obtain tight-binding models inspired by
machine-learning techniques. The model parameters are optimized in such a way
as to reproduce ab-initio band structure data as accurately as possible using
an as small as possible number of model parameters. The procedure is shown to
result in models with smaller ranges and fewer orbitals than maximally
localized Wannier functions but same or even better accuracy. We argue that
such a procedure is more useful for automated construction of tight-binding
models particularly for large-scale materials calculations.

</details>


### [39] [Many-body perturbation theory vs. density functional theory: A systematic benchmark for band gaps of solids](https://arxiv.org/abs/2508.05247)
*Max Großmann,Marc Thieme,Malte Grunert,Erich Runge*

Main category: cond-mat.mtrl-sci

TL;DR: The paper benchmarks many-body perturbation theory (GW methods) against DFT for band gaps of solids, showing QSGŴ as the most accurate method.


<details>
  <summary>Details</summary>
Motivation: To compare the accuracy and cost of GW variants and DFT methods for predicting band gaps in solids.

Method: Systematic comparison of four GW variants (G0W0-PPA, QPG0W0, QSGW, QSGŴ) against DFT methods (mBJ, HSE06).

Result: QSGŴ is highly accurate, correcting overestimations in QSGW and flagging questionable experimental data. G0W0-PPA offers marginal improvement over DFT at higher cost.

Conclusion: Full-frequency GW methods, especially QSGŴ, outperform DFT and simpler GW variants for band gap predictions.

Abstract: We benchmark many-body perturbation theory against density functional theory
(DFT) for the band gaps of solids. We systematically compare four $GW$ variants
$-$ $G_{0}W_{0}$ using the Godby-Needs plasmon-pole approximation
($G_{0}W_{0}$-PPA), full-frequency quasiparticle $G_{0}W_{0}$ (QP$G_{0}W_{0}$),
full-frequency quasiparticle self-consistent $GW$ (QS$GW$), and QS$GW$
augmented with vertex corrections in $W$ (QS$G\hat{W}$) $-$ against the
currently best performing and popular density functionals mBJ and HSE06. Our
results show that $G_{0}W_{0}$-PPA calculations offer only a marginal accuracy
gain over the best DFT methods, however at a higher cost. Replacing the PPA
with a full-frequency integration of the dielectric screening improves the
predictions dramatically, almost matching the accuracy of the QS$G\hat{W}$. The
QS$GW$ removes starting-point bias, but systematically overestimates
experimental gaps by about $15\%$. Adding vertex corrections to the screened
Coulomb interaction, i.e., performing a QS$G\hat{W}$ calculation, eliminates
the overestimation, producing band gaps that are so accurate that they even
reliably flag questionable experimental measurements.

</details>


### [40] [Hole-doping reduces the coercive field in ferroelectric hafnia](https://arxiv.org/abs/2508.05345)
*Pravan Omprakash,Gwan Yeong Jung,Guodong Ren,Rohan Mishra*

Main category: cond-mat.mtrl-sci

TL;DR: Hole doping in ferroelectric hafnia reduces the coercive field, making it more efficient for memory and logic applications.


<details>
  <summary>Details</summary>
Motivation: The high coercive field in hafnia hinders efficient device operations, prompting exploration of doping to lower it.

Method: First-principles calculations and phenomenological modeling were used to study the effects of hole doping on hafnia.

Result: Hole doping reduces the coercive field from 8 MV/cm to 6 MV/cm and alters the polarization switching pathway.

Conclusion: Hole doping transforms hafnia into a proper ferroelectric with a lower coercive field, enhancing its device potential.

Abstract: Ferroelectric hafnia holds promise for next-generation memory and logic
applications because of its CMOS compatibility. However, the high coercive
field required for polarization switching in hafnia remains a critical
challenge for efficient device operations. Using first-principles calculations
and phenomenological modeling, we predict that hole doping can reduce the
coercive field from 8 MV/cm in undoped hafnia to 6 MV/cm in hafnia doped with
0.2 holes per formula unit (f.u.). In the absence of doping, the reversal of
polarization of the Pca21 phase is preferred through the non-polar, tetragonal
P42/nmc phase. This switching pathway involves the coupling of three hard
distortion modes that render undoped hafnia as an improper ferroelectric. The
overall energy barrier through this pathway remains unchanged (80 meV/f.u.)
upon hole doping. However, the introduction of holes hardens the polar
distortion mode that connects the polar Pca21 phase to the non-polar,
orthorhombic Pbcm phase, and reduces the energy barrier from 180 meV/f.u. in
undoped hafnia to 80 meV/f.u. at 0.2 holes/f.u.. Overall, hole doping makes the
latter switching pathway through the Pbcm phase competitive, and renders hafnia
as a proper ferroelectric with a lower coercive field.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [41] [Mapping Sparse Triangular Solves to GPUs via Fine-grained Domain Decomposition](https://arxiv.org/abs/2508.04917)
*Atharva Gondhalekar,Kjetil Haugen,Thomas Gibson,Wu-chun Feng*

Main category: cs.PF

TL;DR: The paper proposes a fine-grained domain decomposition method to optimize sparse triangular solves on GPUs, improving parallelism and reducing memory bottlenecks.


<details>
  <summary>Details</summary>
Motivation: Sparse triangular solves in preconditioned iterative methods cause performance bottlenecks due to irregular memory access and dependencies on GPUs.

Method: A fine-grained domain decomposition strategy creates non-overlapping subdomains, each handled by a thread block, fitting in GPU shared memory to reduce global memory access and synchronization.

Result: Achieves a 10.7× speedup for triangular solves and 3.2× speedup for ILU0-preconditioned BiCGSTAB on AMD MI210 GPU.

Conclusion: The method effectively adapts triangular solves to GPU architecture, balancing parallelism and convergence efficiency.

Abstract: Sparse linear systems are typically solved using preconditioned iterative
methods, but applying preconditioners via sparse triangular solves introduces
bottlenecks due to irregular memory accesses and data dependencies. This work
leverages fine-grained domain decomposition to adapt triangular solves to the
GPU architecture. We develop a fine-grained domain decomposition strategy that
generates non-overlapping subdomains, increasing parallelism in the application
of preconditioner at the expense of a modest increase in the iteration count
for convergence. Each subdomain is assigned to a thread block and is sized such
that the subdomain vector fits in the GPU shared memory, eliminating the need
for inter-block synchronization and reducing irregular global memory accesses.
Compared to other state-of-the-art implementations using the ROCm$^{\text{TM}}$
software stack, we achieve a 10.7$\times$ speedup for triangular solves and a
3.2$\times$ speedup for the ILU0-preconditioned biconjugate gradient stabilized
(BiCGSTAB) solver on the AMD Instinct$^{\text{TM}}$ MI210 GPU.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [42] [High Purcell enhancement in all-TMDC nanobeam resonator designs with optically active monolayers for nanolasers](https://arxiv.org/abs/2508.05333)
*Felix Binkowski,Aris Koulas-Simos,Fridtjof Betz,Matthias Plock,Ivan Sekulic,Phillip Manley,Martin Hammerschmidt,Philipp-Immanuel Schneider,Lin Zschiedrich,Battulga Munkhbat,Stephan Reitzenstein,Sven Burger*

Main category: physics.optics

TL;DR: A nanobeam resonator with an optically active monolayer achieves high Purcell enhancement, optimized for high-beta-factor nanolaser operation using TMDC materials.


<details>
  <summary>Details</summary>
Motivation: To enhance light-matter interaction in atomically thin layers for efficient nanolaser applications.

Method: Developed a theoretical framework using resonance expansion to model and optimize Purcell enhancement, focusing on high-Q resonances for electric field confinement.

Result: Numerical optimization revealed a high-Q resonance enabling strong electric field confinement in the monolayer, maximizing modal gain.

Conclusion: The proposed resonator design and framework effectively enhance light emission in TMDC monolayers, suitable for high-performance nanolasers.

Abstract: We propose a nanobeam resonator incorporating an optically active monolayer,
designed to achieve a high Purcell enhancement. The resonator is fully composed
of transition metal dichalcogenide (TMDC) materials and intended to operate as
a high-beta-factor nanolaser. A theoretical framework that models and optimizes
the Purcell enhancement associated with the emission from atomically thin
layers is developed. This framework is based on a resonance expansion, enabling
spectral resolution of physical quantities governed by high-Q resonances. The
numerical optimization of the resonator leads to the presence of a high-Q
resonance supporting a strong electric field confinement in the monolayer to
maximize the modal gain.

</details>
