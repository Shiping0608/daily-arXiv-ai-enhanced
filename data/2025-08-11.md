<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 14]
- [math.AP](#math.AP) [Total: 18]
- [physics.comp-ph](#physics.comp-ph) [Total: 4]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 6]
- [cs.DS](#cs.DS) [Total: 3]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [math.DG](#math.DG) [Total: 2]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [math.FA](#math.FA) [Total: 1]
- [math.DS](#math.DS) [Total: 1]
- [cs.LG](#cs.LG) [Total: 3]
- [math.OC](#math.OC) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [astro-ph.HE](#astro-ph.HE) [Total: 1]
- [stat.ML](#stat.ML) [Total: 1]
- [math.SP](#math.SP) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [On the Choice of Subspace for the Quasi-minimal Residual Method for Linear Inverse Problems](https://arxiv.org/abs/2508.05793)
*Moshen Hu,Lucas Onisk*

Main category: math.NA

TL;DR: The paper evaluates Krylov subspace methods (GMRES, QMR, and their range-restricted variants) for solving linear discrete ill-posed problems, finding range-restricted QMR superior in certain cases.


<details>
  <summary>Details</summary>
Motivation: To assess the effectiveness of Krylov subspace methods, particularly range-restricted variants, in solving ill-posed linear problems and their sensitivity to noise.

Method: Analysis of GMRES, QMR, and their range-restricted variants, focusing on subspace selection and its impact on solution quality.

Result: Range-restricted QMR outperforms standard QMR, and range-restricted GMRES is superior to conventional GMRES. Range-restricted QMR is less sensitive to errors, especially with uncertain noise levels.

Conclusion: Range-restricted Krylov subspace methods, particularly QMR, offer improved performance for ill-posed problems, especially under uncertain noise conditions.

Abstract: Inverse problems arise in various scientific and engineering applications,
necessitating robust numerical methods for their solution. In this work, we
consider the effectiveness of Krylov subspace iterative methods, including
GMRES, QMR, and their range restricted variants for solving linear discrete
ill-posed problems. We analyze the impact of subspace selection on solution
quality. Our findings indicate that range restricted QMR can outperform
standard QMR, and confirm the previously observed behavior that range
restricted GMRES can be superior to conventional GMRES in terms of
approximation efficacy. Notably, range restricted QMR demonstrates a key
advantage over GMRES with respect to range restricted QMR's singular spectrum
which can make the method less sensitive to errors that are naturally present
making it particularly effective when the noise level in the problem is
uncertain.

</details>


### [2] [A Minimal Perturbation Approach For The Rectangular Multiparameter Eigenvalue Problem](https://arxiv.org/abs/2508.05948)
*Shanheng Han,Lei-Hong Zhang,Ren-Cang Li*

Main category: math.NA

TL;DR: A minimal perturbation framework is proposed to solve the rectangular multiparameter eigenvalue problem (RMEP) by defining approximate solutions, with methods for computing single or complete sets of eigen-tuples.


<details>
  <summary>Details</summary>
Motivation: The RMEP involves rectangular matrices and may lack solutions in its original form, necessitating a framework for approximate solutions.

Method: An alternating iterative scheme for single eigen-tuples and transforming RMEP into a standard MEP for complete sets.

Result: Validated on RMEPs from discretizing multiparameter Sturm-Liouville and Helmholtz equations using least-squares spectral method.

Conclusion: The proposed framework effectively addresses RMEPs, providing practical solutions for both single and complete eigen-tuples.

Abstract: The rectangular multiparameter eigenvalue problem (RMEP) involves rectangular
coefficient matrices (usually with more rows than columns) and may potentially
have no solution in its original form. A minimal perturbation framework is
proposed to defines approximate solutions. Computationally, two particular
scenarios are considered: computing one approximate eigen-tuple or a complete
set of approximate eigen-tuples. For computing one approximate eigen-tuple, an
alternating iterative scheme with proven convergence is devised, while for a
complete set of approximate eigen-tuples, the framework leads to a standard MEP
(RMEP with square coefficient matrices) for numerical solutions. The proposed
approach is validated on RMEPs from discretizing the multiparameter
Sturm-Liouville equation and the Helmholtz equations by the least-squares
spectral method.

</details>


### [3] [Hierarchical Tucker Low-Rank Matrices: Construction and Matrix-Vector Multiplication](https://arxiv.org/abs/2508.05958)
*Yingzhou Li,Jingyu Liu*

Main category: math.NA

TL;DR: Proposes a hierarchical Tucker low-rank (HTLR) matrix for efficient approximation of non-oscillatory kernel functions with linear complexity.


<details>
  <summary>Details</summary>
Motivation: To improve efficiency in approximating non-oscillatory kernel functions by reducing memory and runtime costs.

Method: Uses hierarchical matrix structure with Tucker low-rank blocks, employing high-dimensional interpolation and tensor contractions for construction and matrix-vector multiplication.

Result: Numerical experiments show HTLR matrices perform well in memory and runtime, and work on both uniform and quasi-uniform grids.

Conclusion: HTLR matrices offer efficient, versatile solutions for kernel function approximation with linear complexity.

Abstract: In this paper, a hierarchical Tucker low-rank (HTLR) matrix is proposed to
approximate non-oscillatory kernel functions in linear complexity. The HTLR
matrix is based on the hierarchical matrix, with the low-rank blocks replaced
by Tucker low-rank blocks. Using high-dimensional interpolation as well as
tensor contractions, algorithms for the construction and matrix-vector
multiplication of HTLR matrices are proposed admitting linear and quasi-linear
complexities respectively. Numerical experiments demonstrate that the HTLR
matrix performs well in both memory and runtime. Furthermore, the HTLR matrix
can also be applied on quasi-uniform grids in addition to uniform grids,
enhancing its versatility.

</details>


### [4] [$k\ell$-refinement: An adaptive mesh refinement scheme for hiearchical hybrid grids](https://arxiv.org/abs/2508.06049)
*Benjamin Mann,Ulrich Rüde*

Main category: math.NA

TL;DR: An adaptive mesh refinement technique for hierarchical hybrid grids is introduced, balancing scalability and performance on parallel systems by limiting unstructured refinement to the coarsest level and using a multigrid-based error estimator.


<details>
  <summary>Details</summary>
Motivation: To achieve scalability and maintain high performance on massively parallel systems while addressing the computational cost of traditional mesh adaptivity methods.

Method: Combines classical unstructured refinement on the coarsest level with structured refinement, leveraging grid hierarchy and multigrid schemes for efficient error estimation.

Result: The method maintains performance and scalability, validated through numerical experiments in the HyTeG framework.

Conclusion: The proposed technique effectively balances flexibility and performance, offering a scalable solution for large-scale parallel computing.

Abstract: This work introduces an adaptive mesh refinement technique for hierarchical
hybrid grids with the goal to reach scalability and maintain excellent
performance on massively parallel computer systems. On the block structured
hierarchical hybrid grids, this is accomplished by using classical,
unstructured refinement only on the coarsest level of the hierarchy, while
keeping the number of structured refinement levels constant on the whole
domain. This leads to a compromise where the excellent performance
characteristics of hierarchical hybrid grids can be maintained at the price
that the flexibility of generating locally refined meshes is constrained.
Furthermore, mesh adaptivity often relies on a posteriori error estimators or
error indicators that tend to become computationally expensive. Again with the
goal of preserving scalability and performance, a method is proposed that
leverages the grid hierarchy and the full multigrid scheme that generates a
natural sequence of approximations on the nested hierarchy of grids. This
permits to compute a cheap error estimator that is well-suited for large-scale
parallel computing. We present the theoretical foundations for both global and
local error estimates and present a rigorous analysis of their effectivity. The
proposed method, including error estimator and the adaptive coarse grid
refinement, is implemented in the finite element framework HyTeG. Extensive
numerical experiments are conducted to validate the effectiveness, as well as
performance and scalability.

</details>


### [5] [A Preliminary Study on the Dimensional Stability Classification of Polynomial Spline Spaces over T-meshes](https://arxiv.org/abs/2508.06217)
*Bingru Huang,Falai Chen*

Main category: math.NA

TL;DR: The paper defines dimensional stability for spline spaces over T-meshes, introduces a classification framework, and establishes a link between conformality vector spaces and rank stability. It also provides methods for basis function construction.


<details>
  <summary>Details</summary>
Motivation: To mathematically define and classify dimensional stability for spline spaces over T-meshes, addressing topological and order structures.

Method: Uses $k$-partition decomposition of T-connected components, analysis of CNDC, and structurally similar maps to propose absolute stability. Diagonalizable T-meshes are decomposed into one-dimensional T $l$-edges for basis construction.

Result: Establishes a correspondence between conformality vector spaces and rank stability, enabling basis function construction for spline spaces.

Conclusion: The findings provide a foundation for understanding dimensional stability and developing spline space basis functions.

Abstract: This paper introduces the concept of dimensional stability for spline spaces
over T-meshes, providing the first mathematical definition and a preliminary
classification framework. We define dimensional stability as an invariant
within the structurally isomorphic class, contingent on the rank stability of
the conformality matrix. Absolute stability is proposed via structurally
similar maps to address topological and order structures. Through the
$k$-partition decomposition of T-connected components and analysis of the CNDC,
we establish a correspondence between conformality vector spaces and rank
stability. For diagonalizable T-meshes, decomposition into independent
one-dimensional T $l$-edges facilitates basis function construction, while
arbitrary T-meshes are partitioned into one- and two-dimensional components.
These findings lay the groundwork for understanding dimensional stability and
developing spline space basis functions.

</details>


### [6] [Fully discrete error analysis of finite element discretizations of time-dependent Stokes equations in a stream-function formulation](https://arxiv.org/abs/2508.06235)
*Dmitriy Leykekhman,Boris Vexler,Jakob Wagner*

Main category: math.NA

TL;DR: Best approximation error estimates for fully discrete Galerkin solutions of the time-dependent Stokes problem using stream-function formulation, with arbitrary-degree discontinuous Galerkin time discretization and general space discretization.


<details>
  <summary>Details</summary>
Motivation: To provide error estimates for Galerkin solutions of the Stokes problem without requiring additional regularity assumptions, applicable to various space discretization methods.

Method: Uses discontinuous Galerkin method for time discretization and a general framework for space discretization, ensuring Galerkin orthogonality.

Result: Error estimates are derived without extra regularity assumptions, applicable to conformal C1 and C0 interior penalty methods, and useful for optimal control problems.

Conclusion: The framework is versatile, covering multiple discretization methods and requiring only natural regularity, making it broadly applicable.

Abstract: In this paper we establish best approximation type error estimates for the
fully discrete Galerkin solutions of the time-dependent Stokes problem using
the stream-function formulation. For the time discretization we use the
discontinuous Galerkin method of arbitrary degree, whereas we present the space
discretization in a general framework. This makes our result applicable for a
wide variety of space discretization methods, provided some Galerkin
orthogonality conditions are satisfied. As an example, conformal $C^1$ and
$C^0$ interior penalty methods are covered by our analysis. The results do not
require any additional regularity assumptions beyond the natural regularity
given by the domain and data and can be used for optimal control problems.

</details>


### [7] [Numerical Considerations in Weighted Model Counting](https://arxiv.org/abs/2508.06264)
*Randal E. Bryant*

Main category: math.NA

TL;DR: The paper proposes a method to compute weighted model counts efficiently with guaranteed precision by combining multiple numeric representations, addressing issues of floating-point inaccuracy and rational arithmetic inefficiency.


<details>
  <summary>Details</summary>
Motivation: Weighted model counting is crucial in probabilistic reasoning and risk assessment, but existing methods using floating-point arithmetic are inaccurate, while rational arithmetic is computationally expensive.

Method: The paper combines numeric representations: extended-range double (ERD) for nonnegative weights to avoid underflow/overflow, and interval floating-point with rational arithmetic for mixed weights, ensuring efficiency and precision.

Result: The approach provides tight bounds on precision loss for nonnegative weights and handles mixed weights robustly, as demonstrated by challenging evaluations.

Conclusion: The proposed method efficiently computes weighted model counts with guaranteed precision, overcoming limitations of traditional approaches.

Abstract: Weighted model counting computes the sum of the rational-valued weights
associated with the satisfying assignments for a Boolean formula, where the
weight of an assignment is given by the product of the weights assigned to the
positive and negated variables comprising the assignment. Weighted model
counting finds applications across a variety of domains including probabilistic
reasoning and quantitative risk assessment.
  Most weighted model counting programs operate by (explicitly or implicitly)
converting the input formula into a form that enables arithmetic evaluation,
using multiplication for conjunctions and addition for disjunctions. Performing
this evaluation using floating-point arithmetic can yield inaccurate results,
and it cannot quantify the level of precision achieved. Computing with rational
arithmetic gives exact results, but it is costly in both time and space.
  This paper describes how to combine multiple numeric representations to
efficiently compute weighted model counts that are guaranteed to achieve a
user-specified precision. When all weights are nonnegative, we prove that the
precision loss of arithmetic evaluation using floating-point arithmetic can be
tightly bounded. We show that supplementing a standard IEEE double-precision
representation with a separate 64-bit exponent, a format we call extended-range
double (ERD), avoids the underflow and overflow issues commonly encountered in
weighted model counting. For problems with mixed negative and positive weights,
we show that a combination of interval floating-point arithmetic and rational
arithmetic can achieve the twin goals of efficiency and guaranteed precision.
For our evaluations, we have devised especially challenging formulas and weight
assignments, demonstrating the robustness of our approach.

</details>


### [8] [A Fully Discrete Truly Multidimensional Active Flux Method For The Two-Dimensional Euler Equations](https://arxiv.org/abs/2508.06273)
*Erik Chudzik,Christiane Helzel,Amelie Porfetye*

Main category: math.NA

TL;DR: The paper introduces new limiting strategies for third-order Active Flux methods to ensure positivity of pressure and density, along with reflecting boundary conditions, demonstrating accurate results on coarse grids.


<details>
  <summary>Details</summary>
Motivation: To enhance the Active Flux method by addressing positivity preservation and boundary conditions for hyperbolic conservation laws.

Method: Uses third-order accurate, fully discrete Active Flux methods with compact stencils, employing bicharacteristics for point value updates and new limiting strategies.

Result: Numerical results confirm the method's accuracy on coarse grids while maintaining positivity.

Conclusion: The proposed strategies improve the robustness and applicability of Active Flux methods for hyperbolic conservation laws.

Abstract: The Active Flux method is a finite volume method for hyperbolic conservation
laws that uses both cell averages and point values as degrees of freedom.
Several versions of such methods are currently under development. We focus on
third order accurate, fully discrete Active Flux methods with compact stencil
in space and time. These methods require exact or approximate evolution
operators for the update of the point value degrees of freedom which are
provided by the method of bicharacteristics. Here we propose new limiting
strategies that guarantee positivity of pressure and density and furthermore
discuss the implementation of reflecting boundary conditions. Numerical results
show that the method leads to accurate approximates on coarse grids.

</details>


### [9] [Parallelized computation of quasi-periodic solutions for finite element problems: A Fourier series expansion-based shooting method](https://arxiv.org/abs/2508.06302)
*Junqing Wu,Ling Hong,Mingwu Li,Jun Jiang*

Main category: math.NA

TL;DR: The paper introduces FSE-Shooting, a Fourier series expansion-based shooting method for computing quasi-periodic solutions in high-dimensional nonlinear mechanical systems. It efficiently handles systems with multiple base frequencies and demonstrates versatility in complex dynamical systems.


<details>
  <summary>Details</summary>
Motivation: Quasi-periodic solutions are crucial for understanding nonlinear dynamical systems, but their computation is challenging, especially in high-dimensional cases. The paper aims to address this by proposing a parallelizable and efficient method.

Method: The method involves representing quasi-periodic solutions as trajectories on invariant tori, using Fourier series expansion to parameterize initial conditions, and employing shooting methods to satisfy coupling conditions. Parallel computation and Newmark integration enhance efficiency.

Result: The method successfully computes quasi-periodic solutions in high-dimensional systems, including a 3D shell structure with 1872 DOFs, demonstrating efficiency and versatility.

Conclusion: FSE-Shooting is an effective tool for analyzing quasi-periodic solutions in complex nonlinear dynamical systems, offering computational efficiency and stability assessment capabilities.

Abstract: High-dimensional nonlinear mechanical systems admit quasi-periodic solutions
that are essential for the understanding of the dynamical systems. These
quasi-periodic solutions stay on some invariant tori governed by complex PDEs
in hyper-time. Here, we propose a Fourier series expansion-based shooting
method (FSE-Shooting) for the parallelized computation of quasi-periodic
solution with $d$ base frequencies ($d \ge 2$). We represent the associated
$d$-torus as a collection of trajectories initialized at a ($d-1$)-torus. We
drive a set of ODEs that hold for any of these trajectories. We also derive a
set of boundary conditions that couple the initial and terminal states of these
trajectories and then formulate a set of nonlinear algebraic equations via the
coupling conditions. We use Fourier series expansion to parameterize the
($d-1$)-torus and shooting method to iterate the Fourier coefficients
associated with initial torus such that the coupling conditions are satisfied.
In particular, the terminal points of these trajectories are parallelized
computed via Newmark integration, where the time points and Fourier
coefficients are transformed to each other by alternating Frequency-Time
method. A straightforward phase condition is devised to track the
quasi-periodic solutions with priori unknown base frequencies. Additionally,
the by-product of the FSE-Shooting can be also directly used to compute the
Lyapunov exponents to assess the stabilities of quasi-periodic solutions. The
results of three finite element systems show the efficiency and versatility of
FSE-Shooting in high-dimensional nonlinear dynamical systems, including a
three-dimensional shell structure with $1872$ DOFs.

</details>


### [10] [A Tensor Train Approach for Deterministic Arithmetic Operations on Discrete Representations of Probability Distributions](https://arxiv.org/abs/2508.06303)
*Gerhard Kirsten,Bilgesu Bilgin,Janith Petangoda,Phillip Stanley-Marbell*

Main category: math.NA

TL;DR: The paper introduces a tensor train method for efficient arithmetic operations on high-dimensional probability distributions, avoiding exponential growth in memory and computation.


<details>
  <summary>Details</summary>
Motivation: High-dimensional probability distributions are crucial for uncertainty quantification and Bayesian inference, but current methods face scalability issues due to the curse of dimensionality and stochasticity in Monte Carlo approaches.

Method: The proposed method uses tensor train decomposition to compactly represent distributions with Dirac deltas, enabling deterministic arithmetic operations in a compressed format. Efficient implementation leverages sparse matrices and specialized data structures.

Result: Theoretical and numerical results show polynomial scaling in memory and computation, with significant improvements in performance compared to conventional methods.

Conclusion: The tensor train approach enables tractable deterministic computations on high-dimensional distributions, overcoming previous scalability limitations.

Abstract: Computing with discrete representations of high-dimensional probability
distributions is fundamental to uncertainty quantification, Bayesian inference,
and stochastic modeling. However, storing and manipulating such distributions
suffers from the curse of dimensionality, as memory and computational costs
grow exponentially with dimension. Monte Carlo methods require thousands to
billions of samples, incurring high computational costs and producing
inconsistent results due to stochasticity. We present an efficient tensor train
method for performing exact arithmetic operations on discretizations of
continuous probability distributions while avoiding exponential growth. Our
approach leverages low-rank tensor train decomposition to represent latent
random variables compactly using Dirac deltas, enabling deterministic addition,
subtraction and multiplication operations directly in the compressed format. We
develop an efficient implementation using sparse matrices and specialized data
structures that further enhances performance. Theoretical analysis demonstrates
polynomial scaling of memory and computational complexity under rank
assumptions, and shows how statistics of latent variables can be computed with
polynomial complexity. Numerical experiments spanning randomized linear algebra
to stochastic differential equations demonstrate orders-of-magnitude
improvements in memory usage and computational time compared to conventional
approaches, enabling tractable deterministic computations on discretized random
variables in previously intractable dimensions.

</details>


### [11] [Higher Order Regularization using Harmonic Eigenfunctions for Model-Based Reconstruction in Magnetic Particle Imaging](https://arxiv.org/abs/2508.06306)
*Thomas März,Vladyslav Gapyak,Andreas Weinmann*

Main category: math.NA

TL;DR: A model-based variational reconstruction technique for Magnetic Particle Imaging (MPI) is proposed, using a higher-order regularizer diagonalized by harmonic eigenfunctions, improving image quality through a two-stage process.


<details>
  <summary>Details</summary>
Motivation: To enhance the reconstruction of spatial particle distribution in MPI by addressing the limitations of existing methods through a robust variational approach.

Method: A two-stage algorithm: first, reconstructing the MPI core response with a higher-order regularizer; second, solving an ill-posed deconvolution problem to produce the final image.

Result: The proposed method improves the quality of the final image by enhancing the core stage reconstruction, supported by theoretical foundation and numerical examples.

Conclusion: Higher-order regularization in the core stage significantly improves MPI image reconstruction, demonstrating the method's effectiveness.

Abstract: Magnetic Particle Imaging (MPI) is a recent imaging modality where
superparamagnetic nanoparticles are employed as tracers. The reconstruction
task is to obtain the spatial particle distribution from a voltage signal
induced by the particles. Generally, in computational imaging variational
reconstruction techniques are common and rely on a mathematical model to
describe the underlying physics. For the MPI reconstruction task we propose a
model-based variational reconstruction technique which incorporates a higher
order regularizer, where the regularizer is diagonalized by harmonic
eigenfunctions. The proposed image reconstruction algorithm features two major
stages: in the first stage, the core stage, the components of the MPI core
response are reconstructed. This is the MPI-specific data approximation task
which we formulate as a variational problem incorporating the higher order
regularizer. The relationship between the particle distribution, the MPI core
response and the measured data is given by a mathematical model which was
introduced in our earlier research. According to this model the MPI core
response is tied to the particle distribution by convolution. Therefore the
outcome of the core stage yields the data for the second stage, the
deconvolution stage, in which the final reconstructed image is produced by
solving an ill-posed deconvolution problem in a robust way relying on earlier
research. Interestingly, the quality of the final image depends significantly
on the quality of the result of the core stage. A contribution is thus the
enhancement of the core stage via higher order regularization. We provide a
theoretical foundation for our approach and demonstrate its benefit with
numerical examples.

</details>


### [12] [Rational minimax approximation of matrix-valued functions](https://arxiv.org/abs/2508.06378)
*Lei-Hong Zhang,Ya-Nan Zhang,Chenkun Zhang,Shanheng Han*

Main category: math.NA

TL;DR: A framework for matrix-valued rational minimax approximation, generalizing scalar theory, with a dual problem approach and efficient numerical method.


<details>
  <summary>Details</summary>
Motivation: To extend classical scalar approximation theory to matrix-valued functions, addressing the challenge of minimizing worst-case Frobenius norm errors.

Method: Reformulates the min-max problem via Lagrangian duality, derives a dual problem over the probability simplex, and proposes an efficient numerical method (m-d-Lawson).

Result: The dual problem solution yields the minimax approximant under certain conditions, with numerical experiments showing efficiency.

Conclusion: The framework provides a novel and effective computational approach for matrix-valued rational approximation.

Abstract: In this paper, we present a rigorous framework for rational minimax
approximation of matrix-valued functions that generalizes classical scalar
approximation theory. Given sampled data $\{(x_\ell, {F}(x_\ell))\}_{\ell=1}^m$
where ${F}:\mathbb{C} \to \mathbb{C}^{s \times t}$ is a matrix-valued function,
we study the problem of finding a matrix-valued rational approximant ${R}(x) =
{P}(x)/q(x)$ (with ${P}:\mathbb{C} \to \mathbb{C}^{s \times t}$ a matrix-valued
polynomial and $q(x)$ a nonzero scalar polynomial of prescribed degrees) that
minimizes the worst-case Frobenius norm error over the given nodes: $$
\inf_{{R}(x) = {P}(x)/q(x)} \max_{1 \leq \ell \leq m} \|{F}(x_\ell) -
{R}(x_\ell)\|_{\rm F}. $$ By reformulating this min-max optimization problem
through Lagrangian duality, we derive a maximization dual problem over the
probability simplex. We analyze weak and strong duality properties and
establish a sufficient condition ensuring that the solution of the dual problem
yields the minimax approximant $R(x)$. For numerical implementation, we propose
an efficient method (\textsf{m-d-Lawson}) to solve the dual problem,
generalizing Lawson's iteration to matrix-valued functions. Numerical
experiments are conducted and compared to state-of-the-art approaches,
demonstrating its efficiency as a novel computational framework for
matrix-valued rational approximation.

</details>


### [13] [Heterogeneous optimized Schwarz Methods for heat conduction in composites with thermal contact resistance](https://arxiv.org/abs/2508.06408)
*Huan Zhang,Hui Zhang,Yan Wang,Yingxiang Xu*

Main category: math.NA

TL;DR: The paper proposes using the optimized Schwarz method (OSM) to solve heat transfer in composites with thermal contact resistance (TCR). It introduces a scaled Robin transmission condition for faster convergence and analyzes its benefits, such as mesh independence and improved convergence with larger TCR and heterogeneity contrast.


<details>
  <summary>Details</summary>
Motivation: Imperfect layer contact in composites causes TCR, leading to interfacial temperature discontinuity, which complicates heat transfer analysis. Traditional monolithic solving methods face ill-conditioned systems due to high contrast and interface jumps.

Method: The OSM decouples the heterogeneous problem into homogeneous subproblems. A scaled Robin transmission condition replaces the standard Robin to optimize convergence, with rigorous parameter optimization. Energy estimate and Fourier analysis validate convergence.

Result: Larger TCR and heterogeneity contrast accelerate OSM convergence. Mesh-independent convergence is achieved asymptotically, unlike mesh-dependent results without TCR. Thermal conductivity also aids convergence, similar to heterogeneity.

Conclusion: The OSM with scaled Robin transmission effectively addresses TCR in composites, offering faster convergence and mesh independence. Numerical experiments validate its potential for nonlinear problems on irregular domains.

Abstract: Heat transfer in composites is critical in engineering, where imperfect layer
contact causes thermal contact resistance (TCR), leading to interfacial
temperature discontinuity. We propose solving this numerically using the
optimized Schwarz method (OSM), which decouples the heterogeneous problem into
homogeneous subproblems. This avoids ill-conditioned systems from monolithic
solving due to high contrast and interface jumps. Both energy estimate and
Fourier analysis are used to prove the convergence of this algorithm when the
standard Robin condition is applied to transmit information between subdomains.
To achieve fast convergence, instead of the standard Robin, the scaled Robin
transmission condition is proposed, and the involved free parameter is
rigorously optimized. The results reveal several new findings due to the
presence of TCR: first, the larger the TCR, the faster the OSM converges;
second, mesh-independent convergence is achieved in the asymptotic sense, in
contrast to the mesh-dependent results without TCR; and last, the heterogeneity
contrast benefits the convergence, with a larger contrast leading to faster
convergence. Interestingly, different from the case without TCR, the thermal
conductivity also benefits the convergence, similar to the effect of
heterogeneity. Numerical experiments confirm the theoretical findings and
demonstrate the method's potential for nonlinear problems on irregular domains.

</details>


### [14] [Weak approximation of stochastic differential equations with sticky boundary conditions](https://arxiv.org/abs/2508.06487)
*Akash Sharma*

Main category: math.NA

TL;DR: The paper introduces numerical schemes for approximating sticky SDEs, addressing the lack of existing methods for such boundary conditions.


<details>
  <summary>Details</summary>
Motivation: Existing numerical schemes focus on stopped or reflected SDEs, leaving a gap for sticky SDEs, which involve boundary adhesion.

Method: Constructs half-order and first-order numerical schemes for weak approximation of sticky SDEs, applicable to linear parabolic PDEs with sticky boundary conditions.

Result: Theoretical analysis and numerical experiments validate the schemes' convergence and effectiveness.

Conclusion: The proposed schemes fill a gap in numerical methods for sticky SDEs and demonstrate practical utility for solving related PDEs.

Abstract: Sticky diffusion models a Markovian particle experiencing reflection and
temporary adhesion phenomena at the boundary. Numerous numerical schemes exist
for approximating stopped or reflected stochastic differential equations
(SDEs), but this is not the case for sticky SDEs. In this paper, we construct
and analyze half-order and first-order numerical schemes for the weak
approximation of stochastic differential equations with sticky boundary
conditions. We present the algorithms in general setting such that they can be
used to solve general linear parabolic partial differential equations with
second-order sticky boundary condition via the probabilistic representations of
their solutions. Since the sticky diffusion spends non-zero amount of time on
boundary, it poses extra challenge in designing the schemes and obtaining their
order of convergence. We support the theoretical results with numerical
experiments.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [15] [Energy Estimates for Fractional Evolution Equations](https://arxiv.org/abs/2508.05780)
*Paulo M. Carvalho-Neto,Cicero L. Frota,Juan C. Oyola Ballesteros,Pedro G. P. Torelli*

Main category: math.AP

TL;DR: A refined energy inequality for weak solutions of fractional evolution equations is presented, removing the need for difficult-to-verify regularity conditions and enabling broader applicability.


<details>
  <summary>Details</summary>
Motivation: Address the impracticality of verifying regularity conditions in classical fractional energy inequalities.

Method: Revisits the inequality for square integrable functions, using a new proof based on essential boundedness in time.

Result: The refined inequality naturally incorporates regularity, expanding its applicability and supporting the Faedo Galerkin method.

Conclusion: The work provides a rigorous foundation for fractional evolution problems, demonstrated by proving existence, uniqueness, and regularity for the fractional heat equation.

Abstract: This work presents a more broadly applicable version of an energy inequality
for weak solutions of evolution equations involving fractional time
derivatives. Unlike the classical identity that relates the time derivative of
the squared norm to the inner product of the derivative and the function
itself, its fractional analogue typically requires a regularity condition that
is difficult to verify in practice. We revisit this inequality in the setting
of square integrable functions over an open domain and provide a new proof
based solely on the assumption that the solution is essentially bounded in
time. In our approach, the required regularity emerges naturally within the
argument, rather than being imposed beforehand. This refinement expands the
scope of applicability of the inequality and offers a rigorous foundation for
using the Faedo Galerkin method in problems governed by fractional evolution
equations. As an application, we prove existence, uniqueness, and regularity of
weak solutions to the fractional heat equation.

</details>


### [16] [There and Back Again: Revisiting the Failure of Concatenation in Mittag-Leffler Functions](https://arxiv.org/abs/2508.05788)
*Paulo M. Carvalho-Neto,Cesar E. T. Ledesma*

Main category: math.AP

TL;DR: The paper proves that the semigroup property for the Mittag-Leffler function holds only if α=1 or λ=0, highlighting the nonlocality of fractional-order dynamics.


<details>
  <summary>Details</summary>
Motivation: To address the lack of rigorous proof for the failure of the semigroup property in the Mittag-Leffler function, a key fractional analogue of the exponential function.

Method: Analytical proof demonstrating the conditions under which the semigroup property holds.

Result: The semigroup property is valid only when α=1 or λ=0.

Conclusion: The study clarifies the nonlocal nature of fractional-order differential equations by establishing precise criteria for semigroup compatibility.

Abstract: The function $t \mapsto E_{\alpha}(\lambda t^\alpha)$ is widely regarded as
the fractional analogue of the exponential function, yet its algebraic
properties remain poorly understood. In particular, standard references lack a
rigorous proof of the failure of the semigroup property. In this work, we fill
this gap by providing an analytical proof that the semigroup property holds if
and only if either $\alpha = 1$ or $\lambda = 0$. Our result establishes a
precise criterion for when Mittag-Leffler dynamics are compatible with
semigroup evolution, thereby emphasizing the intrinsic nonlocality of
fractional-order differential equations.

</details>


### [17] [Hunter-type Implosion Profiles for Energy-supercritical Polytropic Equations of State](https://arxiv.org/abs/2508.05817)
*Ely Sandine*

Main category: math.AP

TL;DR: The paper constructs smooth self-similar solutions for the isentropic gravitational Euler-Poisson system with a polytropic equation of state, focusing on the energy-supercritical range (1<γ<6/5). It extends previous work and introduces a framework for proving local analyticity near singular points.


<details>
  <summary>Details</summary>
Motivation: To extend prior constructions of self-similar solutions (Hunter and Larson-Penston-type) and address the energy-supercritical range, while providing a general framework for analyzing singular points in such systems.

Method: The authors rigorously construct solutions for the Euler-Poisson system with a polytropic equation of state, using a framework to prove local analyticity near singular points.

Result: A family of smooth self-similar solutions is constructed for the specified range of polytropic indices, complementing existing results.

Conclusion: The work successfully extends prior constructions and provides a reusable framework for analyzing singularities in similar quasilinear systems.

Abstract: We rigorously construct a family of smooth self-similar solutions to the
isentropic gravitational Euler-Poisson system with a polytropic equation of
state for polytropic indices lying in the full energy-supercritical range,
$1<\gamma<\frac{6}{5}$. The result is an extension of the author's previous
construction of Hunter solutions in the isothermal case, $\gamma=1$, and
complements a construction of Larson-Penston-type solutions by
Guo-Had\v{z}i\'c-Jang-Schrecker for the same system in the full
mass-supercritical range, $1<\gamma<\frac{4}{3}$. As an ingredient in the
proof, a general framework is introduced for proving local analyticity of
solutions to this system in the vicinity of singular points. This framework
could be used for other quasilinear self-similar blow-up constructions.

</details>


### [18] [An Analysis of the Riemann Problem for a $2 \times 2$ System of Keyfitz-Kranzer Type Conservation Laws Using Shadow Waves and Dafermos Regularization](https://arxiv.org/abs/2508.05927)
*Josh Culver,Aubrey Ayres,Evan Halloran,Ryan Lin,Emily Peng,Charis Tsikkou*

Main category: math.AP

TL;DR: The paper analyzes self-similar Riemann solutions in a system of two conservation laws, highlighting overcompressive delta shocks and their emergence from Dafermos regularization. It uses GSPT and numerical simulations to study these solutions.


<details>
  <summary>Details</summary>
Motivation: To understand transport dynamics under density constraints, focusing on minimal yet representative systems that model crowding-limited transport in various applications.

Method: Employed blow-up techniques within Geometric Singular Perturbation Theory (GSPT) and numerical simulations using the Local Lax-Friedrichs scheme.

Result: Demonstrated the existence of overcompressive delta shocks as singular limits of Dafermos regularization, resolving their internal structure.

Conclusion: The system serves as a versatile prototype for crowding-limited transport, applicable to fields like biology, ecology, granular physics, and traffic modeling.

Abstract: We consider a system of two conservation laws and provide a detailed
description of both classical and non-classical self-similar Riemann solutions.
In particular, we demonstrate the existence of overcompressive delta shocks as
singular limits of the Dafermos regularization of the system. The system is
chosen for its minimal yet representative structure, which captures the
essential features of transport dynamics under density constraints. Our
analysis is carried out using blow-up techniques within the framework of
Geometric Singular Perturbation Theory (GSPT), allowing us to resolve the
internal structure of these singular solutions. Despite its simplicity, the
system serves as a versatile prototype for crowding-limited transport across a
range of applications, including biological aggregation, ecological dispersal,
granular compaction, and traffic congestion. Our findings are supported by
numerical simulations using the Local Lax-Friedrichs scheme.

</details>


### [19] [Nonexistence of positive radial solutions for semipositone $φ$-Laplacian problems with superlinear reaction term](https://arxiv.org/abs/2508.05930)
*Sigifredo Herrón,Emer Lopera,Diana Sánchez*

Main category: math.AP

TL;DR: The paper proves the nonexistence of positive radial solutions for a specific PDE problem involving the ϕ-Laplacian operator for large λ.


<details>
  <summary>Details</summary>
Motivation: To address the lack of positive radial solutions for the given PDE under certain conditions, particularly for large λ.

Method: Combines indirect argument and energy analysis to study the problem.

Result: Demonstrates that no positive radial solutions exist for sufficiently large λ.

Conclusion: The findings confirm the nonexistence of solutions under the specified conditions, contributing to the understanding of such PDEs.

Abstract: The aim of this paper is to prove the nonexistence of positive radial
solutions to the problem $-\Delta_\phi u=\lambda f(u)$, $x\in B_1(0)$, $u(x)=0$
on $|x|=1$, for $\lambda>0$ sufficiently large. Here, $\phi$ is a continuous
function, $\Delta_\phi$ denotes the $\phi$-Laplacian operator which is defined
by $\Delta_\phi (u):=div (\phi (|\nabla u|) \nabla u)$, and $B_1(0)$ is the
unit ball in $\mathbb{R}^N$, with $N>1$. Furthermore, $f$ is a continuous,
nondecreasing function such that $f(0)<0$, and its behavior at infinity is
intimately related to $\phi$. Our findings are presented in a combined format,
employing both an indirect argument and an energy analysis.

</details>


### [20] [$L^{2}$-estimates for the linear elastic waves](https://arxiv.org/abs/2508.05955)
*Hiroshi Takeda*

Main category: math.AP

TL;DR: The paper analyzes the long-term behavior of solutions to the Cauchy problem for elastic wave equations, providing optimal L² estimates for large times.


<details>
  <summary>Details</summary>
Motivation: To understand the large time behavior of elastic wave solutions and establish precise L² norm bounds under minimal regularity assumptions.

Method: Approximation of the solution using a smooth auxiliary function with carefully chosen parameters.

Result: Optimal upper and lower bounds for the L² norms of each solution component are derived for large times.

Conclusion: The study successfully characterizes the long-time behavior of elastic wave solutions with minimal regularity requirements.

Abstract: This paper is concerned with the large time behavior of the solution to the
Cauchy problem for the elastic wave equations. In particular, optimal $L^{2}$
estimates of the elastic waves are obtained in the sense that the upper and
lower bounds of the $L^{2}$ norms of each component of the solution are proved
for large $t$, under the minimum assumptions necessary regarding regularity
with respect to initial data. The proof is based on the approximation of the
solution by a smooth auxiliary function with suitable parameters.

</details>


### [21] [Global solutions in $L^{p}_{v}L^{\infty}_{x}$ for the Boltzmann equation in bounded domains](https://arxiv.org/abs/2508.05985)
*Dingqun Deng,Jong-in Kim,Donghyun Lee*

Main category: math.AP

TL;DR: The paper explores solutions to the Boltzmann equation in bounded domains using relaxed function spaces, focusing on hard potentials and diffuse reflection boundary conditions. It constructs unique global solutions for large initial data with small relative entropy, showing exponential convergence to equilibrium.


<details>
  <summary>Details</summary>
Motivation: Prior work on the Boltzmann equation in bounded domains has been limited to uniformly bounded function classes. This paper aims to extend the theory to relaxed function spaces, addressing the initial-boundary value problem under specific conditions.

Method: The study uses the $L^{p}_{v}L^\infty_{x}$ space for the Boltzmann equation, considering hard potentials and diffuse reflection boundary conditions. A key tool is a pointwise estimate for the gain term, bounded by $L^p_v$ and $L^2_v$ norms.

Result: Unique global-in-time mild solutions are constructed for large initial data with small relative entropy, demonstrating exponential convergence to the global Maxwellian.

Conclusion: The work offers a new perspective on convergence to equilibrium in bounded domains, complementing existing literature and expanding the theoretical framework for the Boltzmann equation.

Abstract: The existence theory for solutions to the Boltzmann equation in bounded
domains has primarily been developed within uniformly bounded function classes,
such as $L^{\infty}_{x,v}$, as in [Duan-Huang-Wang-Yang,2017],
[Duan-Wang,2019], [Guo,2010]. In this paper, we investigate solutions in
relaxed function spaces $L^{p}_{v}L^\infty_{x}$ for the initial-boundary value
problem of the Boltzmann equation in bounded domains. We consider the case of
hard potential under diffuse reflection boundary conditions and assume cutoff
model. For large initial data in a weighted $L^{p}_{v}L^\infty_{x}$ space with
small relative entropy, we construct unique global-in-time mild solution that
converge exponentially to the global Maxwellian. A pointwise estimate for the
gain term, bounded in terms of $L^p_v$ and $L^2_v$ norms, is essential to prove
our main results. Relative to [Gualdani-Mischler-Mouhot,2017], our work
provides an alternative perspective on convergence to equilibrium in the
presence of boundary conditions.

</details>


### [22] [Weighted estimates for the Stokes semigroup in the half-space](https://arxiv.org/abs/2508.06119)
*Angelica Pia Di Feola,Vittorio Pane*

Main category: math.AP

TL;DR: The paper studies the Stokes system in the half-space using weighted Lebesgue spaces, proving existence, uniqueness, and regularity of solutions. It generalizes prior work and lays groundwork for Navier-Stokes analysis.


<details>
  <summary>Details</summary>
Motivation: To extend the theory of the Stokes system to weighted Lebesgue spaces, generalizing previous results and paving the way for Navier-Stokes applications.

Method: Introduces a new weight function based on distances from fixed points and analyzes the Stokes system in weighted Lebesgue spaces.

Result: Establishes existence, uniqueness, and regularity of strong solutions in the half-space.

Conclusion: The findings generalize earlier work and serve as a foundation for future Navier-Stokes research in weighted spaces.

Abstract: We investigate the initial-boundary value problem for the Stokes system in
the half-space, within the framework of weighted Lebesgue spaces. Introducing a
new weight function defined via a product of powers of distances from fixed
points, we establish existence, uniqueness, and regularity results for strong
solutions to the Stokes problem in the half space. Our analysis generalizes
previous results for the Stokes system in radial-weighted spaces (Galdi and
Maremonti, J. Math. Fluid Mech. 25:7,2023; Maremonti and Pane, J. Math. Fluid
Mech. 27:2,2025) and extends the theory to our setting. These results represent
a first step toward the analysis of the Navier-Stokes system in weighted
spaces, with applications in both half-space and exterior domain
configurations.

</details>


### [23] [Event triggered control and exponential stability for infinite dimensional linear systems $\star$](https://arxiv.org/abs/2508.06144)
*Lucie Baudouin,Sylvain Ervedoza*

Main category: math.AP

TL;DR: The paper analyzes exponential stabilization of infinite-dimensional systems with event-triggered control, ensuring stability and avoiding Zeno behavior.


<details>
  <summary>Details</summary>
Motivation: To unify the analysis of exponential stabilization for abstract infinite-dimensional systems using event-triggered control, maintaining stability properties.

Method: Uses a Lyapunov functional to design an event-triggering mechanism for sampled control, ensuring stability and solution regularity.

Result: Proves exponential stability, solution existence/regularity, and avoidance of Zeno behavior for the closed-loop system.

Conclusion: A well-designed event-triggering mechanism preserves stability in infinite-dimensional systems, validated by Lyapunov analysis.

Abstract: This article aims at providing a unified analysis of the exponential
stabilization of some abstract infinite dimensional systems undergoing an
event-triggering mechanism that samples the control input. The partial
differential equation is supposed to be defined by a skew-adjoint operator and
controlled and observed through bounded operators. The continuously controlled
closed loop system is assumed to be exponentially stable and the goal is to
prove that a well-designed event-triggering mechanism to rule the time updates
of the sampled control will allow to keep such a stability property. The key of
the proof relies on the existence of an adequate Lyapunov functional. Existence
and regularity of the solution to the closed-loop event-triggered system are
also proven, along with the avoidance of Zeno behavior.

</details>


### [24] [Sharp non-existence threshold for a parabolic Hardy-H{é}non equation with quasilinear diffusion](https://arxiv.org/abs/2508.06164)
*Razvan Gabriel Iagar,Philippe Laurençot*

Main category: math.AP

TL;DR: The paper identifies optimal conditions for initial data leading to non-existence of non-negative solutions to a parabolic Hardy-Hénon equation. It shows a threshold for non-existence based on initial conditions and parameters.


<details>
  <summary>Details</summary>
Motivation: To determine the precise conditions under which non-negative solutions to the Cauchy problem for the parabolic Hardy-Hénon equation fail to exist, given specific initial data.

Method: Analyzes the equation with parameters m, σ, and p, and derives a threshold for γ in the initial condition that leads to non-existence of solutions.

Result: Non-existence occurs for γ below a derived threshold, which is shown to be optimal by comparing with self-similar solutions.

Conclusion: The threshold for γ is optimal, as confirmed by the existence of self-similar solutions at the limiting value.

Abstract: Optimal conditions for initial data leading to non-existence of non-negative
solutions to the Cauchy problem for the parabolic Hardy-H{\'e}non equation $$
\partial\_tu=\Delta u^m+|x|^{\sigma}u^p, \quad
(t,x)\in(0,\infty)\times\mathbb{R}^N, $$ with $m>0$, $\sigma>0$ and
$p>\max\{1,m\}$, are identified. Assuming that the initial condition satisfies
$$ u\_0\in L^{\infty}(\mathbb{R}^N), \quad
\lim\limits\_{|x|\to\infty}|x|^{\gamma}u\_0(x)=L\in(0,\infty), \quad u\_0\geq0,
$$ it is shown that non-existence of solution occurs for $$
\gamma<\frac{\sigma+2}{p-m} - \frac{2\max{\{p-p\_G,0\}}}{(p-1)(p-m)} $$ with $$
p\_G:=1+\frac{\sigma(1-m)}{2}. $$ The above threshold for non-existence is
optimal, in view of the existence of self-similar solutions for the limiting
value of $\gamma$.

</details>


### [25] [Inverse Source Problems for the Time-Fractional Evolution Equation](https://arxiv.org/abs/2508.06209)
*Rahmonov Askar Ahmadovich*

Main category: math.AP

TL;DR: The paper studies inverse problems for identifying source terms in a time-fractional diffusion-wave equation using interior point data.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of identifying time-dependent and time-independent source terms in fractional diffusion-wave equations, which are crucial for modeling real-world phenomena.

Method: The spectral method is used to analyze the direct problem for existence, uniqueness, and regularity. Inverse problems are then tackled using properties derived from the direct problem.

Result: The study establishes the feasibility of recovering both time-independent and time-dependent source terms from measured data.

Conclusion: The proposed approach effectively solves the inverse problems, providing a foundation for further applications in modeling and analysis.

Abstract: In this paper, we investigate the direct and linear inverse problems of
identifying time-dependent and time-independent source terms in a
time-fractional diffusion-wave equation, using measured data at an interior
point of the time interval. We first establish the existence, uniqueness, and
regularity of the solution to the direct problem by employing the spectral
method. Then, based on the properties of the direct problem, we study two
inverse problems: one involving the recovery of a time-independent source term,
and the other concerning the identification of a time-dependent coefficient
function in the source term.

</details>


### [26] [Lipschitz Stability for Polyhedral Elastic Inclusions from Partial Data](https://arxiv.org/abs/2508.06241)
*Andrea Aspri,Elena Beretta,Elisa Francini,Antonino Morassi,Edi Rosset,Eva Sincich,Sergio Vessella*

Main category: math.AP

TL;DR: The paper addresses the inverse problem of identifying a polyhedral inclusion in an elastic body using boundary measurements, proving a constructive Lipschitz stability estimate.


<details>
  <summary>Details</summary>
Motivation: To solve the inverse problem of determining hidden inclusions in elastic materials, which is crucial for applications like non-destructive testing and medical imaging.

Method: Uses boundary measurements of traction and displacement on part of the boundary, assuming homogeneous isotropic materials and specific geometric conditions.

Result: Proves a constructive Lipschitz stability estimate from the local Dirichlet-to-Neumann map.

Conclusion: The method provides a stable and constructive way to identify polyhedral inclusions in elastic bodies.

Abstract: The paper deals with the inverse problem of determining a polyhedral
inclusion compactly contained in an elastic body from boundary measurements of
traction and displacement taken on an open portion of the boundary. Both the
inclusion and the body are made of homogeneous isotropic material. Under
suitable assumptions on the geometry of the unknown inclusion, we prove a
constructive Lipschitz stability estimate from the local Dirichlet-to-Neumann
map.

</details>


### [27] [Instantaneous continuous loss of Sobolev regularity for the 3D incompressible Euler equation](https://arxiv.org/abs/2508.06333)
*In-Jee Jeong,Luis Martínez-Zoroa,Wojciech S. Ożański*

Main category: math.AP

TL;DR: The paper demonstrates the instantaneous and continuous loss of supercritical Sobolev regularity for the 3D incompressible Euler equations in ℝ³, constructing specific initial conditions and solutions to show this behavior.


<details>
  <summary>Details</summary>
Motivation: To understand the regularity loss in solutions of the 3D Euler equations, which is crucial for analyzing fluid dynamics and turbulence.

Method: Construct divergence-free initial vorticity with specific Sobolev regularity and analyze the corresponding local-in-time solution's behavior.

Result: For any given Sobolev exponent s ∈ (0, 3/2), the solution loses regularity continuously in time, with explicit bounds on the regularity loss.

Conclusion: The study confirms the loss of supercritical Sobolev regularity in the 3D Euler equations, providing insights into the behavior of fluid flows and potential turbulence mechanisms.

Abstract: We prove instantaneous and continuous-in-time loss of supercritical Sobolev
regularity for the 3D incompressible Euler equations in $\mathbb{R}^{3}$.
Namely, for any $s\in (0,3/2)$ and $\varepsilon >0$, we construct a
divergence-free initial vorticity $\omega_0$ defined in $\mathbb{R}^{3}$
satisfying $\| \omega_0 \|_{H^s}\leq \varepsilon$, as well as $T>0$, $c>0$ and
a corresponding local-in-time solution $\omega$ such that, for each $t\in
[0,T]$, $\omega (\cdot ,t ) \in {H^{\frac{s-ct}{1+ct}}}$ and $ \omega (\cdot ,t
) \not \in {H^\beta }$ for any $\beta > \frac{s-ct}{1+ct} $. Moreover, $\omega$
is unique among all solutions with initial condition $\omega_0$ which are
locally $C^2$ and belong to $C([0,T];L^p )$ for any $p>3 $.

</details>


### [28] [Sub-supersolutions method for singular quasilinear systems involving gradient terms](https://arxiv.org/abs/2508.06359)
*Abdelkrim Moussaoui*

Main category: math.AP

TL;DR: Existence, regularity, and location of solutions for quasilinear singular elliptic systems with gradient dependence are proven using sub-supersolution methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of solving quasilinear singular elliptic systems with general gradient dependence.

Method: Develops and applies a sub-supersolution method to analyze the systems.

Result: Existence of positive solutions for convective and absorption singular systems is demonstrated.

Conclusion: The sub-supersolution method effectively solves the problem, providing theoretical and practical insights.

Abstract: Existence, regularity and location of solutions to quasilinear singular
elliptic systems with general gradient dependence are established developing a
method of sub-supersolution. The abstract theorems involving sub-supersolutions
are applied to prove the existence of positive solutions for convective and
absorption singular systems.

</details>


### [29] [Existence and nonexistence of solutions for singular quadratic quasilinear equations](https://arxiv.org/abs/2508.06375)
*David Arcoya,José Carmona,Tommaso Leonori,Pedro J. Martínez-Aparicio,Luigi Orsina,Francesco Petitta*

Main category: math.AP

TL;DR: The paper investigates the existence and nonexistence of nonnegative solutions for nonlinear elliptic problems with singular gradient terms, showing that γ<2 is necessary and sufficient for solutions in H¹₀(Ω).


<details>
  <summary>Details</summary>
Motivation: To understand the conditions under which solutions exist for nonlinear elliptic problems with singular lower order terms and natural gradient growth.

Method: Analyzes the model problem involving a PDE with singular gradient terms and Dirichlet boundary conditions, focusing on the parameter γ.

Result: Proves that γ<2 is necessary and sufficient for the existence of solutions in H¹₀(Ω) for sufficiently regular f.

Conclusion: The condition γ<2 is critical for the existence of solutions in the given context.

Abstract: We study both existence and nonexistence of nonnegative solutions for
nonlinear elliptic problems with singular lower order terms that have natural
growth with respect to the gradient, whose model is $$ \begin{cases}
  -\Delta u + \frac{|\nabla u|^2}{u^{\gamma}} = f & \mbox{in } \Omega,\newline
  \hfill u=0 \hfill & \mbox{on } \partial \Omega, \end{cases} $$ where $\Omega$
is an open bounded subset of $\mathbb{R}^N $, $\gamma> 0$ and $f$ is a function
which is strictly positive on every compactly contained subset of $\Omega$. As
a consequence of our main results, we prove that the condition $\gamma<2$ is
necessary and sufficient for the existence of solutions in $H^{1}_{0}(\Omega)$
for every sufficiently regular $f$ as above.

</details>


### [30] [Global strong solutions to the frame hydrodynamics for biaxial nematic phases](https://arxiv.org/abs/2508.06376)
*Minjiang Feng,Sirui Li,Qi Zeng*

Main category: math.AP

TL;DR: Global well-posedness of strong solutions for frame hydrodynamics of biaxial nematic phases is proven for small initial data in 2D and 3D.


<details>
  <summary>Details</summary>
Motivation: To study the coupled system of orthonormal frame evolution and Navier-Stokes equations derived from a molecular-theory-based dynamical tensor model.

Method: Analysis of nonlinear terms with rotational derivatives on SO(3) and leveraging the dissipative structure of the system.

Result: Global well-posedness of strong solutions for small initial data in two and three dimensions.

Conclusion: The dissipative structure and careful estimates of nonlinear terms ensure the global existence of solutions for small data.

Abstract: In this article, we consider the frame hydrodynamics of biaxial nematic
phases, a coupled system between the evolution of the orthonormal frame and the
Navier--Stokes equation, which is derived from a molecular-theory-based
dynamical tensor model about two second-order tensors. In two and three
dimensions, we establish global well-posedness of strong solutions to the
Cauchy problem of frame hydrodynamics for small initial data. The key
ingredient of the proof relies on estimates of nonlinear terms with rotational
derivatives on $SO(3)$, together with the dissipative structure of the frame
hydrodynamics.

</details>


### [31] [Diffuse measures and nonlinear parabolic equations](https://arxiv.org/abs/2508.06384)
*Francesco Petitta,Augusto C. Ponce,Alessio Porretta*

Main category: math.AP

TL;DR: The paper studies properties of solutions to a parabolic PDE with Dirichlet boundary conditions, focusing on a priori estimates, diffuse measures, and renormalized solutions. It also extends results to nonlinear operators.


<details>
  <summary>Details</summary>
Motivation: To analyze solutions of parabolic PDEs involving p-Laplacian and diffuse measures, aiming to generalize existing results and introduce new solution concepts.

Method: Proves a priori estimates for p-parabolic capacity, approximates diffuse measures, and introduces renormalized solutions. Applies these to solve PDEs with nonlinear terms.

Result: Existence of solutions for PDEs with nonlinear terms and diffuse measures, with uniqueness under nondecreasing conditions. Extends to general nonlinear operators.

Conclusion: The paper advances the understanding of parabolic PDEs with diffuse measures, introducing renormalized solutions and proving existence/uniqueness results.

Abstract: Given a parabolic cylinder $Q =(0,T)\times\Omega$, where $\Omega\subset
\mathbb{R}^{N}$ is a bounded domain, we prove new properties of solutions of \[
u_t-\Delta_p u = \mu \quad \text{in $Q$} \] with Dirichlet boundary conditions,
where $\mu$ is a finite Radon measure in $Q$. We first prove a priori estimates
on the $p$-parabolic capacity of level sets of $u$. We then show that diffuse
measures (i.e.\@ measures which do not charge sets of zero parabolic
$p$-capacity) can be strongly approximated by the measures $\mu_k =
(T_k(u))_t-\Delta_p(T_k(u))$, and we introduce a new notion of renormalized
solution based on this property. We finally apply our new approach to prove the
existence of solutions of $$ u_t-\Delta_{p} u + h(u)=\mu \quad \text{in $Q$,}
$$ for any function $h$ such that $h(s)s\geq 0$ and for any diffuse measure
$\mu$; when $h$ is nondecreasing we also prove uniqueness in the renormalized
formulation. Extensions are given to the case of more general nonlinear
operators in divergence form.

</details>


### [32] [A duality approach to the fractional Laplacian with measure data](https://arxiv.org/abs/2508.06390)
*Kenneth H. Karlsen,Francesco Petitta,Suleyman Ulusoy*

Main category: math.AP

TL;DR: A duality method is introduced to prove existence and uniqueness of solutions to nonlocal problems involving the fractional Laplacian with compactly supported measures.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of solving nonlocal problems with vanishing conditions at infinity, particularly for the fractional Laplacian.

Method: A duality method is employed to analyze the problem.

Result: Existence and uniqueness of solutions are proven for the given nonlocal problem.

Conclusion: The duality method effectively resolves the nonlocal problem with compactly supported measures.

Abstract: We describe a duality method to prove both existence and uniqueness of
solutions to nonlocal problems like $$ (-\Delta)^s v = \mu \quad \text{in}\
\mathbb{R}^N, $$ with vanishing conditions at infinity.
  Here $\mu$ is a bounded Radon measure whose support is compactly contained in
$\mathbb{R}^N$, $N\geq2$, and $-(\Delta)^s$ is the fractional Laplace operator
of order $s\in (1/2,1)$.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [33] [A second-order particle Fokker-Planck-Master method for diatomic gas flows](https://arxiv.org/abs/2508.05642)
*Joonbeom Kim,Eunji Jun*

Main category: physics.comp-ph

TL;DR: The study introduces the USP-FPM method for diatomic gases, achieving second-order accuracy in time and space, outperforming DSMC and FPM in computational efficiency while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing FP models lack second-order accuracy for diatomic gases, which are crucial for engineering applications like atmospheric reentry.

Method: The USP-FPM method enhances temporal accuracy with second-order relaxations and spatial accuracy via polynomial reconstruction. Tested on homogeneous relaxation, Poiseuille flow, and hypersonic cylinder flow.

Result: USP-FPM provides accurate solutions with coarser cells and larger time steps, achieving a 28x speed-up over DSMC for hypersonic flow.

Conclusion: The USP-FPM method is a computationally efficient and accurate alternative for simulating diatomic gas flows, especially in high-speed applications.

Abstract: The direct simulation Monte Carlo (DSMC) method is widely used to describe
rarefied gas flows. The DSMC method accounts for the transport and collisions
of computational particles, resulting in higher computational costs in the
continuum regime. The Fokker-Planck (FP) model approximates particle collisions
as Brownian motion to reduce computational cost. Advanced FP models have been
developed to enhance physical fidelity, ensuring the correct Prandtl number and
the H-theorem. The FP model has further been extended to handle diatomic gases,
such as the Fokker-Planck-Master (FPM) model. Alongside these developments in
modeling, computational efficiency has also been improved by achieving
second-order spatial and temporal accuracy, as demonstrated in the unified
stochastic particle FP (USP-FP) method. However, these accuracy improvements
have not yet been extended to diatomic gases, which are essential for
engineering applications such as atmospheric reentry. This study proposes a
unified stochastic particle Fokker-Planck-Master (USP-FPM) method for diatomic
gases that achieves second-order accuracy in both time and space. Temporal
accuracy is enhanced by reproducing second-order energy, viscous stress, and
heat flux relaxations. Spatial accuracy is improved by employing a first-order
polynomial reconstruction method. Three test cases are investigated:
homogeneous relaxation, Poiseuille flow, and hypersonic flow around a cylinder.
The results show that the USP-FPM method provides accurate solutions even with
coarser cell sizes and larger time steps compared to the DSMC and FPM methods.
In particular, for the hypersonic flow around a cylinder, the USP-FPM method
achieves a speed-up factor of 28 compared to the DSMC method, while maintaining
accuracy.

</details>


### [34] [Hybrid Physics-Machine Learning Models for Quantitative Electron Diffraction Refinements](https://arxiv.org/abs/2508.05908)
*Shreshth A. Malik,Tiarnan A. S. Doherty,Benjamin Colmey,Stephen J. Roberts,Yarin Gal,Paul A. Midgley*

Main category: physics.comp-ph

TL;DR: A hybrid physics-machine learning framework combines differentiable simulations with neural networks for high-fidelity electron microscopy, improving refinement of crystal structures.


<details>
  <summary>Details</summary>
Motivation: Traditional methods struggle to model real-world experimental effects in electron microscopy, limiting quantitative analysis.

Method: Integrates differentiable physical simulations with neural networks, enabling joint optimization of parameters and experimental variables via automatic differentiation.

Result: Achieves state-of-the-art refinement in 3D electron diffraction, accurately recovering atomic positions, thermal displacements, and thickness profiles.

Conclusion: Differentiable hybrid modeling is a powerful paradigm for overcoming experimental complexities in quantitative electron microscopy.

Abstract: High-fidelity electron microscopy simulations required for quantitative
crystal structure refinements face a fundamental challenge: while physical
interactions are well-described theoretically, real-world experimental effects
are challenging to model analytically. To address this gap, we present a novel
hybrid physics-machine learning framework that integrates differentiable
physical simulations with neural networks. By leveraging automatic
differentiation throughout the simulation pipeline, our method enables
gradient-based joint optimization of physical parameters and neural network
components representing experimental variables, offering superior scalability
compared to traditional second-order methods. We demonstrate this framework
through application to three-dimensional electron diffraction (3D-ED) structure
refinement, where our approach learns complex thickness distributions directly
from diffraction data rather than relying on simplified geometric models. This
method achieves state-of-the-art refinement performance across synthetic and
experimental datasets, recovering atomic positions, thermal displacements, and
thickness profiles with high fidelity. The modular architecture proposed can
naturally be extended to accommodate additional physical phenomena and extended
to other electron microscopy techniques. This establishes differentiable hybrid
modeling as a powerful new paradigm for quantitative electron microscopy, where
experimental complexities have historically limited analysis.

</details>


### [35] [Advancing Material Modeling in Hydrocodes Beyond Equations of State](https://arxiv.org/abs/2508.06012)
*Tim A. Linke,Dane M. Sterbentz,Jean-Pierre R. Delplanque,Sebastien Hamel,Kevin A. Korner,Philip C. Myint,Lorin X. Benedict,Jonathan L. Belof*

Main category: physics.comp-ph

TL;DR: A multiscale simulation framework combines Finite Element Method and molecular dynamics, bypassing traditional EOS for detailed microscale physics. Validated with experiments and EOS models, it shows high efficiency and potential for large-scale modeling.


<details>
  <summary>Details</summary>
Motivation: To incorporate detailed microscale physics not easily represented by coarse-grained models and bypass traditional EOS limitations.

Method: Couples Finite Element Method with molecular dynamics using lifting and restriction operators for consistency. Validated with experiments and conventional EOS models.

Result: Demonstrated in shock-driven hydrodynamic flow, outperforming state-of-the-art EOS models with 99% weak scaling efficiency.

Conclusion: The framework is efficient and viable for large-scale multiscale modeling across diverse materials and conditions.

Abstract: We present a multiscale simulation framework that couples the Finite Element
Method with molecular dynamics. Bypassing traditional equations of state (EOS)
by using in-line atomistic simulations, the method offers the advantage of
incorporating detailed microscale physics not easily represented with
coarse-grained models. Coupling consistency with the continuum code is ensured
through the use of lifting and restriction operators, in line with
heterogeneous multiscale methods. The concurrent continuum-atomistic framework
is validated through comparison with experimental results and conventional EOS
models, and demonstrated in a shock-driven hydrodynamic flow simulation under
extreme conditions. We further evaluate the framework's usability by comparing
it to state-of-the-art EOS models of deuterium. A computational performance
study reveals that the atomistic EOS evaluation is a feasible alternative to
conventional approaches, and demonstrates a weak scaling of 99% efficiency.
These results highlight the framework's potential for large-scale multiscale
modeling across a broad range of materials and conditions.

</details>


### [36] [Real-time physics-informed reconstruction of transient fields using sensor guidance and higher-order time differentiation](https://arxiv.org/abs/2508.06070)
*Hong-Kyun Noh,Jeong-Hoon Park,Minseok Choi,Jae Hyuk Lim*

Main category: physics.comp-ph

TL;DR: FTI-PBSM is a physics-informed surrogate model for real-time PDE solutions, using higher-order numerical differentiation instead of AD for temporal causality, improving stability, efficiency, and generalization.


<details>
  <summary>Details</summary>
Motivation: To address limitations of conventional PINN-based models, such as reliance on AD and complex architectures, by simplifying temporal causality imposition and enhancing performance.

Method: Uses higher-order numerical differentiation (e.g., Central Difference) instead of AD for temporal causality, trained on sparse sensor data from varied PDE cases.

Result: Outperforms baseline in accuracy, generalization, robustness to noise, and reduces training time across four PDE problems.

Conclusion: FTI-PBSM offers a simpler, more efficient, and robust framework for real-time PDE solutions with sparse data.

Abstract: This study proposes FTI-PBSM (Fixed-Time-Increment Physics-informed neural
network-Based Surrogate Model), a novel physics-informed surrogate modeling
framework designed for real-time reconstruction of transient responses in
time-dependent Partial Differential Equations (PDEs) using only sparse,
time-dependent sensor measurements. Unlike conventional Physics-Informed Neural
Network (PINN)-based models that rely on Automatic Differentiation (AD) over
both spatial and temporal domains and require dedicated causal network
architectures to impose temporal causality, the proposed approach entirely
removes AD in the time direction. Instead, it leverages higher-order numerical
differentiation methods, such as the Central Difference, Adams-Bashforth, and
Backward Differentiation Formula, to explicitly impose temporal causality. This
leads to a simplified model architecture with improved training stability,
computational efficiency, and extrapolation capability. Furthermore, FTI-PBSM
is trained on sparse sensor measurements from multiple PDE cases generated by
varying PDE coefficients, with the sensor data serving as model input. This
enables the model to learn a parametric PDE family and generalize to unseen
physical cases, accurately reconstructing full-field transient solutions in
real time. The proposed model is validated on four representative PDE
problems-the convection equation, diffusion-reaction dynamics, Korteweg-de
Vries (KdV) equation, and Allen-Cahn equation-and demonstrates superior
prediction accuracy and generalization performance compared to a causal PBSM,
which is used as the baseline model, in both interpolation and extrapolation
tasks. It also shows strong robustness to sensor noise and variations in
training data size, while significantly reducing training time.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [37] [Transport of Particles in Strongly Turbulent 3D Magnetized Plasmas](https://arxiv.org/abs/2508.05727)
*Heinz Isliker,Loukas Vlahos*

Main category: physics.plasm-ph

TL;DR: The paper reviews particle transport in turbulent 3D magnetized plasmas, highlighting two energization mechanisms: stochastic and systematic acceleration, leading to distinct transport properties and energy distributions.


<details>
  <summary>Details</summary>
Motivation: To understand particle dynamics in turbulent plasmas with intense magnetic fluctuations and coherent structures, which influence energization and transport.

Method: Analysis of particle energization mechanisms (stochastic and systematic acceleration) and their interplay, using theoretical models like Fokker-Planck and fractional transport equations.

Result: Stochastic acceleration follows Gaussian statistics, while systematic acceleration exhibits Levy flight statistics. The interplay forms Kappa distributions, with systematic acceleration dominating early turbulence and stochastic heating later.

Conclusion: The study provides insights into particle transport in turbulent plasmas, emphasizing the roles of coherent structures and distinct energization mechanisms, with implications for modeling and understanding plasma behavior.

Abstract: In this review, we examine particle transport in strongly turbulent
three-dimensional (3D) magnetized plasmas, characterized by intense
(large-amplitude) magnetic field fluctuations. Such environments naturally give
rise to a network of coherent structures (CoSs), including current sheets,
filaments, shocks, switchbacks, and significant magnetic perturbations, which
critically influence particle dynamics at the kinetic level. Within this
turbulent regime, two fundamental particle energization mechanisms emerge,
stochastic acceleration and systematic acceleration. Systematic acceleration
within open turbulent volumes promotes the development of power-law tails in
energy distributions. Our analysis distinguishes the roles of two electric
fields: the perpendicular (or convective) fields, which drive stochastic
heating via interactions with randomly moving scatterers, and the parallel
electric fields, which enable systematic particle acceleration in regions of
strong currents. Combined with accurate estimates of particle escape times in
finite volumes, the interplay of these mechanisms leads to the formation of
Kappa distributions. The transport properties differ significantly between the
two energization modes. Stochastic energization follows Gaussian statistics and
can be effectively described by the Fokker-Planck equation. In contrast,
systematic acceleration exhibits Levy flight statistics, necessitating a
fractional transport equation for an accurate description. Furthermore, the
fractal spatial distribution of CoSs introduces deviations from traditional
transport models, influencing e.g. particle escape times. Systematic
acceleration is most efficient during the early, high-energy phases of
turbulence, while stochastic heating becomes dominant during the later stages,
contributing to gradual particle energization.

</details>


### [38] [Reconstruction of 2D line-integrated electron density using angular filter refractometry and a fast marching Eikonal solver](https://arxiv.org/abs/2508.05837)
*Brendan McCluskey,Jesse Griff-McMahon,Daniel Haberberger,Vicente Valenzuela-Villaseca,Huws Landsberger,William Fox*

Main category: physics.plasm-ph

TL;DR: A fast marching Eikonal solver is used to invert AFR data for 2D line-integrated electron density reconstruction, validated with synthetic and experimental data.


<details>
  <summary>Details</summary>
Motivation: Prior methods for analyzing AFR data were limited to 1D or forward-fitting techniques, lacking direct 2D inversion.

Method: A fast marching Eikonal solver is applied to directly invert AFR data, verified with synthetic data and tested in laser-driven experiments.

Result: The method successfully reconstructs 2D electron density, agreeing with 1D results and validating via forward modeling.

Conclusion: The technique is effective but could benefit from additional measurements for improved precision.

Abstract: Refraction of an optical probe beam by a plasma can be measured with angular
filter refractometry (AFR), which produces an image containing intensity
contours that correspond to curves of constant refraction angle. Further
analysis is required to reconstruct the underlying line-integrated electron
density. Most prior efforts to calculate density from AFR data have been
limited to 1D analysis or forward-fitting techniques. In this paper, we detail
the use of a fast marching Eikonal solver to directly invert AFR data and
obtain the 2D line-integrated electron density. The analysis method is first
verified with synthetic data and then applied to laser-driven experiments of
single and double plume expansion collected at the OMEGA EP Laser Facility. The
calculated densities agree with 1D results and are shown to be consistent with
the original AFR measurements via forward modeling. We also discuss how
additional measurements could improve the precision of this technique.

</details>


### [39] [Energetics and limitations of passive electron transpiration cooling for hypersonic leading edges](https://arxiv.org/abs/2508.05900)
*Bryce Boyer,Timothy S. Fisher*

Main category: physics.plasm-ph

TL;DR: ETC for hypersonic vehicles cools via thermionic emission but faces heating from collected flowfield electrons. A 1D plasma sheath model shows passive ETC can reverse cooling at high plasma densities, with mitigation strategies like dielectric coatings or blunt geometries having trade-offs.


<details>
  <summary>Details</summary>
Motivation: To address the overlooked heating effect of flowfield electron collection in ETC studies and evaluate its impact on cooling performance.

Method: Integration of a 1D collisionless plasma sheath model into a discretized leading-edge framework to analyze surface potentials and charged-particle fluxes, with a parametric study on plasma and vehicle properties.

Result: Passive ETC can reverse cooling due to thermionic space-charge overcompensation at high plasma densities. Mitigation strategies like dielectric coatings or blunt geometries have limitations.

Conclusion: ETC effectiveness is limited by flowfield electron heating, and mitigation strategies come with trade-offs, restricting its application in certain vehicle geometries.

Abstract: Electron transpiration cooling (ETC) offers a promising approach for thermal
management of hypersonic vehicles by leveraging thermionic emission from the
leading edge. While emitted electrons cool the surface, subsequent collection
of flowfield electrons induces heating, limiting ETC effectiveness unless
collection occurs in cooler aftbody regions. Most existing ETC studies neglect
this heating contribution, assuming ideal downstream collection. This work
integrates a one-dimensional collisionless plasma sheath model into a
discretized leading-edge framework to predict surface potentials and
charged-particle fluxes. A parametric study examines how plasma and vehicle
properties affect ETC performance. Results reveal that passive ETC is
susceptible to thermionic space-charge overcompensation, which can reverse the
intended cooling effect and cause surface heating at high plasma densities.
Flowfield electron heating can be mitigated but not eliminated by using
dielectric coatings or blunt geometries. The same mechanisms that protect blunt
bodies from adverse electrical conduction and flowfield heating also preclude
incorporation of ETC-based cooling in those vehicle geometries.

</details>


### [40] [High-Energy Photon Generation from Self-Organized Plasma Cavities in Field-Enhanced Laser-Preplasma Interactions](https://arxiv.org/abs/2508.06045)
*Prokopis Hadjisolomou,Rashid Shaisultanon,Tae Moon Jeong,Christopher Paul Ridgers,Sergei Vladimirovich Bulanov*

Main category: physics.plasm-ph

TL;DR: An ultraintense Nd:glass laser interacting with near-critical plasma forms a highly efficient γ-ray source, with simulations showing enhanced laser intensity and extreme photon yield.


<details>
  <summary>Details</summary>
Motivation: To explore efficient γ-ray generation using ultraintense lasers for applications like photonuclear physics.

Method: Three-dimensional particle-in-cell simulations to study laser-plasma interaction, focusing on relativistic self-focusing and electron cavity effects.

Result: Peak γ-photon yield exceeds 20% of laser energy, with Nd:glass lasers outperforming Ti:Sa lasers in photon number and efficiency.

Conclusion: Nd:glass lasers offer a robust, scalable method for ultra-bright γ-ray generation, ideal for high-power applications.

Abstract: The interaction of an ultraintense Nd:glass laser pulse with a near-critical
plasma self-organizes into a highly efficient $\gamma$-ray source.
Three-dimensional particle-in-cell simulations demonstrate that relativistic
self-focusing, aided by a self-generated electron cavity, enhances the laser
intensity by more than an order of magnitude, driving the system into the
radiation-reaction-dominated regime, i.e. one where the electrons lose a
substantial amount of their energy as hard radiation. Peak photon emission
occurs near $0.5$ times the relativistic critical density, with a
$\gamma$-photon yield exceeding $20\%$ of the laser energy. Compared to Ti:Sa
lasers of the same power, the longer duration of Nd:glass laser pulses leads to
an order of magnitude increase in $\gamma$-photon number in the extreme
conversion efficiency regime, making them particularly well-suited for
photonuclear physics applications. These findings point to a robust and
scalable mechanism for compact, ultra-bright $\gamma$-ray generation in the
multi-petawatt regime.

</details>


### [41] [Multiscale turbulence in stellarators](https://arxiv.org/abs/2508.06116)
*G. Merlo,A. Bañón Navarro,T. Görler,F. Jenko,F. Wilms*

Main category: physics.plasm-ph

TL;DR: First gyrokinetic simulations of multiscale turbulence in W7-X show cross-scale interactions between electron- and ion-scale turbulence, impacting transport predictions.


<details>
  <summary>Details</summary>
Motivation: To understand multiscale turbulence in stellarators, particularly W7-X, and its impact on transport under various conditions.

Method: Gyrokinetic simulations using W7-X magnetic geometry and experimentally relevant parameters, exploring ETG, ITG, and MTM regimes.

Result: ETG turbulence drives transport without streamers; ITG regimes show zonal flow erosion, while MTM conditions allow persistence due to isotropic ETG turbulence.

Conclusion: Cross-scale effects are crucial for accurate transport predictions in W7-X and future stellarators.

Abstract: We present the first gyrokinetic simulations of multiscale turbulence in a
stellarator, using the magnetic geometry of Wendelstein 7-X (W7-X) and
experimentally relevant parameters. A broad range of scenarios is explored,
including regimes where electron-temperature-gradient (ETG) turbulence coexists
with varying levels of ion-temperature-gradient (ITG) turbulence, as well as
cases involving microtearing modes (MTMs) relevant to high-$\beta$ and
reactor-like conditions. Notably, while ETG turbulence does not form radial
streamers as in tokamaks, it can still drive significant transport and interact
with ion-scale turbulence. In electrostatic ITG-dominated regimes,
electron-scale fluctuations erode zonal flows, enhancing ion-scale transport,
while ion-scale turbulence suppresses ETG activity. In contrast, under
electromagnetic MTM conditions, the isotropic nature of ETG turbulence limits
its suppressive effect, allowing MTMs to persist. These findings underscore the
critical role of cross-scale effects for accurate transport predictions in W7-X
and future stellarators.

</details>


### [42] [Characterization and automated optimization of laser-driven proton beams from converging liquid sheet jet targets](https://arxiv.org/abs/2508.06462)
*G. D. Glenn,F. Treffert,H. Ahmed,S. Astbury,M. Borghesi,N. Bourgeois,C. B. Curry,S. J. D. Dann,S. DiIorio,N. P. Dover,T. Dzelzainis,O. Ettlinger,M. Gauthier,L. Giuffrida,R. J. Gray,J. S. Green,G. S. Hicks,C. Hyland,V. Istokskaia,M. King,B. Loughran,D. Margarone,O. McCusker,P. McKenna,Z. Najmudin,C. Parisuaña,P. Parsons,C. Spindloe,M. J. V. Streeter,D. R. Symes,A. G. R. Thomas,N. Xu,S. H. Glenzer,C. A. J. Palmer*

Main category: physics.plasm-ph

TL;DR: A multi-Hz ion acceleration platform using a liquid sheet jet target is presented, demonstrating real-time optimization of proton beam energy via Bayesian methods, achieving an 11% increase in energy.


<details>
  <summary>Details</summary>
Motivation: To develop high repetition rate laser-driven ion sources for applications in medicine, materials science, and physics, requiring robust and autonomous optimization.

Method: Employed a liquid sheet jet target and used Bayesian optimization to tune the laser wavefront for real-time, closed-loop optimization of proton beam energy.

Result: Achieved an 11% increase in maximum proton energy compared to manual optimization, enhancing energy concentration in the laser focal spot.

Conclusion: Closed-loop optimization schemes show promise for tuning future ion accelerators for high repetition rate operation.

Abstract: Compact, stable, and versatile laser-driven ion sources hold great promise
for applications ranging from medicine to materials science and fundamental
physics. While single-shot sources have demonstrated favorable beam properties,
including the peak fluxes necessary for several applications, high repetition
rate operation will be necessary to generate and sustain the high average flux
needed for many of the most exciting applications of laser-driven ion sources.
Further, to navigate through the high-dimensional space of laser and target
parameters towards experimental optima, it is essential to develop ion
acceleration platforms compatible with machine learning learning techniques and
capable of autonomous real-time optimization. Here we present a multi-Hz ion
acceleration platform employing a liquid sheet jet target. We characterize the
laser-plasma interaction and the laser-driven proton beam across a variety of
key parameters governing the interaction using an extensive suite of online
diagnostics. We also demonstrate real-time, closed-loop optimization of the ion
beam maximum energy by tuning the laser wavefront using a Bayesian optimization
scheme. This approach increased the maximum proton energy by 11% compared to a
manually-optimized wavefront by enhancing the energy concentration within the
laser focal spot, demonstrating the potential for closed-loop optimization
schemes to tune future ion accelerators for robust high repetition rate
operation.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [43] [Debiasing Polynomial and Fourier Regression](https://arxiv.org/abs/2508.05920)
*Chris Camaño,Raphael A. Meyer,Kevin Shu*

Main category: cs.DS

TL;DR: The paper introduces a debiasing method for approximating an unknown function using polynomial regression, leveraging random matrix theory to achieve unbiased, near-optimal results.


<details>
  <summary>Details</summary>
Motivation: Existing randomized algorithms for polynomial approximation produce biased estimates, which is undesirable. The goal is to develop an unbiased method with near-optimal sample complexity.

Method: The proposed method evaluates the function at eigenvalues of a specially designed random complex matrix, tailored to the distribution of interest, to debias polynomial regression.

Result: The estimator is unbiased, achieves near-optimal sample complexity, and outperforms iid leverage score sampling in experiments. It also debiases Fourier series approximation methods.

Conclusion: The debiasing method effectively addresses bias in polynomial and Fourier series approximations, offering practical improvements over existing techniques.

Abstract: We study the problem of approximating an unknown function
$f:\mathbb{R}\to\mathbb{R}$ by a degree-$d$ polynomial using as few function
evaluations as possible, where error is measured with respect to a probability
distribution $\mu$. Existing randomized algorithms achieve near-optimal sample
complexities to recover a $ (1+\varepsilon) $-optimal polynomial but produce
biased estimates of the best polynomial approximation, which is undesirable.
  We propose a simple debiasing method based on a connection between polynomial
regression and random matrix theory. Our method involves evaluating
$f(\lambda_1),\ldots,f(\lambda_{d+1})$ where $\lambda_1,\ldots,\lambda_{d+1}$
are the eigenvalues of a suitably designed random complex matrix tailored to
the distribution $\mu$. Our estimator is unbiased, has near-optimal sample
complexity, and experimentally outperforms iid leverage score sampling.
  Additionally, our techniques enable us to debias existing methods for
approximating a periodic function with a truncated Fourier series with
near-optimal sample complexity.

</details>


### [44] [The Beauty of Anisotropic Mesh Refinement: Omnitrees for Efficient Dyadic Discretizations](https://arxiv.org/abs/2508.06316)
*Theresa Pollinger,Masado Ishii,Jens Domke*

Main category: cs.DS

TL;DR: Omnitrees generalize octrees for anisotropic problems, improving convergence and storage efficiency, validated on 3D and 4D tasks.


<details>
  <summary>Details</summary>
Motivation: Octrees enforce isotropic refinement, which is inefficient for anisotropic problems. Omnitrees address this by allowing refinement only in important dimensions.

Method: Introduces omnitrees as an anisotropic generalization of octrees, refining only locally important dimensions to optimize tree structure depth and width.

Result: Omnitrees improve convergence rates by 1.5x, reduce storage for equivalent error bounds, and maximize information density faster than octrees, especially in higher dimensions.

Conclusion: Omnitrees enhance AMR efficiency for anisotropic problems and enable new high-dimensional applications.

Abstract: Structured adaptive mesh refinement (AMR), commonly implemented via quadtrees
and octrees, underpins a wide range of applications including databases,
computer graphics, physics simulations, and machine learning. However, octrees
enforce isotropic refinement in regions of interest, which can be especially
inefficient for problems that are intrinsically anisotropic--much resolution is
spent where little information is gained. This paper presents omnitrees as an
anisotropic generalization of octrees and related data structures. Omnitrees
allow to refine only the locally most important dimensions, providing tree
structures that are less deep than bintrees and less wide than octrees. As a
result, the convergence of the AMR schemes can be increased by up to a factor
of the dimensionality d for very anisotropic problems, quickly offsetting their
modest increase in storage overhead. We validate this finding on the problem of
binary shape representation across 4,166 three-dimensional objects: Omnitrees
increase the mean convergence rate by 1.5x, require less storage to achieve
equivalent error bounds, and maximize the information density of the stored
function faster than octrees. These advantages are projected to be even
stronger for higher-dimensional problems. We provide a first validation by
introducing a time-dependent rotation to create four-dimensional
representations, and discuss the properties of their 4-d octree and omnitree
approximations. Overall, omnitree discretizations can make existing AMR
approaches more efficient, and open up new possibilities for high-dimensional
applications.

</details>


### [45] [Does block size matter in randomized block Krylov low-rank approximation?](https://arxiv.org/abs/2508.06486)
*Tyler Chen,Ethan N. Epperly,Raphael A. Meyer,Christopher Musco,Akash Rao*

Main category: cs.DS

TL;DR: The paper resolves the theory-practice gap in randomized block Krylov iteration for rank-$k$ matrix approximation, proving a $(1 + \varepsilon)$-factor approximation is achievable with $\tilde O(k/\sqrt{\varepsilon})$ matrix-vector products for any block size $1 \le b \le k$.


<details>
  <summary>Details</summary>
Motivation: Prior work showed inefficiencies or gaps in performance for block sizes between $1$ and $k$, despite practical optimizations. This paper aims to bridge this gap.

Method: The study uses randomized block Krylov iteration, analyzing the minimum singular value of random block Krylov matrices.

Result: The paper proves that the method achieves a $(1 + \varepsilon)$-factor approximation efficiently for any block size $1 \le b \le k$.

Conclusion: The findings close the theory-practice gap and provide insights applicable to faster algorithms for sparse linear systems.

Abstract: We study the problem of computing a rank-$k$ approximation of a matrix using
randomized block Krylov iteration. Prior work has shown that, for block size $b
= 1$ or $b = k$, a $(1 + \varepsilon)$-factor approximation to the best
rank-$k$ approximation can be obtained after $\tilde O(k/\sqrt{\varepsilon})$
matrix-vector products with the target matrix. On the other hand, when $b$ is
between $1$ and $k$, the best known bound on the number of matrix-vector
products scales with $b(k-b)$, which could be as large as $O(k^2)$.
Nevertheless, in practice, the performance of block Krylov methods is often
optimized by choosing a block size $1 \ll b \ll k$. We resolve this
theory-practice gap by proving that randomized block Krylov iteration produces
a $(1 + \varepsilon)$-factor approximate rank-$k$ approximation using $\tilde
O(k/\sqrt{\varepsilon})$ matrix-vector products for any block size $1\le b\le
k$. Our analysis relies on new bounds for the minimum singular value of a
random block Krylov matrix, which may be of independent interest. Similar
bounds are central to recent breakthroughs on faster algorithms for sparse
linear systems [Peng & Vempala, SODA 2021; Nie, STOC 2022].

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [46] [Comment on "Mineral-water reactions in Earth's mantle: Predictions from Born theory and ab initio molecular dynamics" by Fowler et al. 2024 (Geochim. Cosmochim. Acta 372, 111-123)](https://arxiv.org/abs/2508.05656)
*Jiajia Huang,Ding Pan*

Main category: physics.geo-ph

TL;DR: The paper resolves discrepancies in dielectric constant calculations of water under extreme conditions, validating Pan et al.'s results over Fowler et al.'s due to methodological differences in dipole moment calculations.


<details>
  <summary>Details</summary>
Motivation: Address inconsistencies between two studies (Fowler et al. and Pan et al.) on dielectric constant calculations of water under extreme conditions.

Method: Reproduced AIMD simulations using CP2K code with extended duration and identical system size.

Result: Validated Pan et al.'s dielectric constant (39.4) as accurate, contrasting Fowler et al.'s 51, attributing the discrepancy to dipole moment calculation methods.

Conclusion: The findings clarify critical issues in dipole moment treatment for periodic systems, impacting mineral-water interaction modeling in Earth's mantle.

Abstract: This comment addresses discrepancies in dielectric constant calculations of
water under extreme conditions (~10 GPa and 1000 K) between Fowler et al.'s
recent study [Geochim. Cosmochim. Acta 372, 111-123 (2024)] and the earlier
work by Pan et al. [Proc. Natl. Acad. Sci. 110, 6646-6650 (2013)]. Through
reproduced ab initio molecular dynamics (AIMD) simulations using the CP2K code
with extended duration and identical system size, we rigorously validate that
Pan et al.'s original results (39.4) are well-converged, contrasting with
Fowler et al.'s reported value of 51. The observed discrepancy cannot be
attributed to simulation duration limitations, but rather to methodological
differences in dipole moment calculation. Our analysis highlights critical
issues in the treatment of dipole moment fluctuations in periodic systems
within the framework of modern theory of polarization. This clarification has
significant implications for modeling mineral-water interactions in Earth's
mantle using Born theory.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [47] [Minimal surfaces with arbitrary genus in 3-spheres of positive Ricci curvature](https://arxiv.org/abs/2508.06019)
*Adrian Chun-Pong Chu*

Main category: math.DG

TL;DR: The paper explores topological structures in surfaces with singularities in the 3-sphere and applies this to prove a result about minimal surfaces in Riemannian 3-spheres.


<details>
  <summary>Details</summary>
Motivation: To understand the topological properties of surfaces with singularities in the 3-sphere and apply this knowledge to Riemannian geometry.

Method: Analyzes the topological structure of surfaces with finitely many singularities in the 3-sphere and uses this to study minimal surfaces.

Result: Proves that every Riemannian 3-sphere of positive Ricci curvature contains a genus g embedded minimal surface with area bounded by twice the first Simon-Smith width.

Conclusion: The study provides insights into the interplay between topology and geometry, particularly in the context of minimal surfaces in positively curved spaces.

Abstract: We describe some topological structure in the set of all surfaces with
finitely many singularities in the 3-sphere.
  As an application, we prove that every Riemannian 3-sphere of positive Ricci
curvature contains, for every g, a genus g embedded minimal surface with area
at most twice the first Simon-Smith width of the ambient 3-sphere.

</details>


### [48] [An index formula for families of end-periodic Dirac operators](https://arxiv.org/abs/2508.06029)
*Alex R. Taylor*

Main category: math.DG

TL;DR: A transgression formula for the renormalized Chern character of the Bismut superconnection is derived, leading to an index formula for families of Dirac operators on end-periodic manifolds, involving a new 'end-periodic eta form'.


<details>
  <summary>Details</summary>
Motivation: To generalize the Bismut-Cheeger eta form and the end-periodic eta invariant by deriving a transgression formula in the context of end-periodic fiber bundles and Clifford modules.

Method: Uses the Fourier-Laplace transform of the Bismut superconnection and the renormalized supertrace of Mrowka-Ruberman-Saveliev.

Result: An index formula for families of Dirac operators on end-periodic manifolds, incorporating a new end-periodic eta form.

Conclusion: The work extends existing eta forms and invariants, providing a unified framework for index theory in end-periodic settings.

Abstract: We derive a transgression formula for the renormalized Chern character of the
Bismut superconnection in the context of end-periodic fiber bundles and
families of end-periodic Clifford modules. The transgression is expressed in
terms of the Fourier-Laplace transform of the Bismut superconnection using the
renormalized supertrace of Mrowka-Ruberman-Saveliev. Consequently, we establish
an index formula for families of Dirac operators on end-periodic manifolds. The
index formula involves a new ``end-periodic eta form'' which generalizes both
the Bismut-Cheeger eta form and the end-periodic eta invariant of
Mrowka-Ruberman-Saveliev.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [49] [On-the-Fly Machine Learning of Interatomic Potentials for Elastic Property Modeling in Al-Mg-Zr Solid Solutions](https://arxiv.org/abs/2508.06311)
*Lukas Volkmer,Leonardo Medrano Sandonas,Philip Grimm,Julia Kristin Hufenbach,Gianaurelio Cuniberti*

Main category: cond-mat.mtrl-sci

TL;DR: The paper explores elastic properties of Al-Mg-Zr alloys using ML and QM simulations, developing accurate MLIPs to predict composition-dependent properties efficiently.


<details>
  <summary>Details</summary>
Motivation: To advance lightweight, resilient Al alloys for energy-efficient engineering by understanding their elastic properties.

Method: Combines ML techniques (on-the-fly learning with Bayesian regression and MACE neural networks) with QM simulations to develop MLIPs.

Result: MLIPs predict elastic properties with minimal deviation from experiments, enabling systematic exploration of Al-Mg-Zr phase space.

Conclusion: The MLIPs are reliable and transferable, useful for designing tailored Al alloys, and lay groundwork for future multiscale studies.

Abstract: The development of resilient and lightweight Aluminum alloys is central to
advancing structural materials for energy-efficient engineering applications.
To address this challenge, in this study, we explore the elastic properties of
Al-Mg-Zr solid solutions by integrating advanced machine learning (ML)
techniques with quantum-mechanical (QM) atomistic simulations. For this
purpose, we develop accurate and transferable machine-learned interatomic
potentials (MLIPs) using two complementary approaches: (i) an on-the-fly
learning scheme combined with Bayesian linear regression during ab initio
molecular dynamics simulations, and (ii) the equivariant neural network
architecture MACE. Both MLIPs facilitate the prediction of
composition-dependent elastic properties while drastically reducing the
computational cost compared to conventional QM methods. Comparison with
ultrasonic measurements shows that the deviation between simulation and
experiment remains within a few GPa across all Al-Mg-Zr systems investigated.
These potentials also enable the systematic exploration of the Al-Mg-Zr solid
solution phase space and provide insights into the elastic behavior as a
function of alloying element concentration. Hence, our findings demonstrate the
reliability and transferability of the parameterized on-the-fly MLIPs, making
them valuable for accelerating the design of Al alloys with tailored
physicomechanical properties in complex compositional spaces. While the present
study focuses on homogeneous phases, it establishes a foundation for future
multiscale simulations that include microstructural features such as
precipitates and grain boundaries.

</details>


<div id='math.FA'></div>

# math.FA [[Back]](#toc)

### [50] [Transfinite Iteration of Operator Transforms and Spectral Projections in Hilbert and Banach Spaces](https://arxiv.org/abs/2508.06025)
*Faruk Alpay,Taylan Alpay,Hamdi Alakkad*

Main category: math.FA

TL;DR: The paper studies multi-layer iterations of bounded operator transforms, proving convergence to spectral/ergodic projections under specific conditions. It covers normal operators on Hilbert space and reflexive Banach spaces, providing explicit examples and counterexamples.


<details>
  <summary>Details</summary>
Motivation: To understand the convergence behavior of ordinal-indexed, multi-layer iterations of bounded operator transforms under functional-calculus hypotheses, and to identify spectral properties and ergodic projections.

Method: The study involves analyzing normal operators on Hilbert space and Ritt or sectorial operators in reflexive Banach spaces. It uses functional-calculus hypotheses, spectral mapping, and ergodic theory to prove convergence.

Result: For normal operators, iterates converge strongly to spectral projections. In Banach spaces, composite layers yield power-bounded projections. Explicit examples and counterexamples are provided.

Conclusion: The paper establishes convergence results for operator iterations under specific conditions, with applications in spectral theory and ergodic projections, supported by examples and counterexamples.

Abstract: We study ordinal-indexed, multi-layer iterations of bounded operator
transforms and prove convergence to spectral/ergodic projections under
functional-calculus hypotheses. For normal operators on Hilbert space and
polynomial or holomorphic layers that are contractive on the spectrum and fix
the peripheral spectrum only at fixed points, the iterates converge in the
strong operator topology by a countable stage to the spectral projection onto
the joint peripheral fixed set. We describe spectral mapping at finite stages
and identify the spectrum of the limit via the essential range. In reflexive
Banach spaces, for Ritt or sectorial operators with a bounded H-infinity
functional calculus, the composite layer is power-bounded and its mean-ergodic
projection yields an idempotent commuting with the original operator; under a
peripheral-separation condition the powers converge strongly to this
projection. We provide explicit two-layer Schur filters, a concise
Schur/Nevanlinna-Pick lemma, a Fejer-type monotonicity bound implying
stabilization by the first countable limit (omega), examples that attain
exactly the omega stage, and counterexamples outside the hypotheses.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [51] [High multiplicity and global structure of coexistence states in a predator-prey model with saturation](https://arxiv.org/abs/2508.05714)
*Kousuke Kuto,Julián López-Gómez,Eduardo Muñoz-Hernández*

Main category: math.DS

TL;DR: A diffusive predator-prey system with saturation can exhibit numerous coexistence states under certain parameter ranges, especially with high saturation rates.


<details>
  <summary>Details</summary>
Motivation: To explore the conditions under which a predator-prey system with saturation can sustain multiple coexistence states.

Method: Analysis of the model under varying parameter values, focusing on the saturation rate's impact.

Result: The system can have arbitrarily many coexistence states for large saturation rates, with a detailed global structure in the limiting case.

Conclusion: Saturation rates significantly influence the diversity of coexistence states in predator-prey systems.

Abstract: This paper establishes that, under the appropriate range of values of the
parameters involved in the formulation of the model, a diffusive predator-prey
system with saturation can have an arbitrarily large number of coexistence
states for sufficiently large saturation rates. Moreover, it ascertains the
global structure of the set of coexistence states in the limiting system as the
saturation rate blows-up.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [52] [Fast, Convex and Conditioned Network for Multi-Fidelity Vectors and Stiff Univariate Differential Equations](https://arxiv.org/abs/2508.05921)
*Siddharth Rout*

Main category: cs.LG

TL;DR: The paper addresses poor optimization in neural PDE solvers due to ill-conditioning, proposing Shifted Gaussian Encoding to improve matrix rank and convergence, achieving significant accuracy gains.


<details>
  <summary>Details</summary>
Motivation: Neural PDE solvers often fail due to ill-conditioning, not expressivity, especially in multi-fidelity and stiff problems.

Method: Introduces Shifted Gaussian Encoding, an activation filtering step to enhance matrix rank and expressivity while maintaining convexity in PIELMs.

Result: Extends solvable Peclet numbers by two orders, reduces error by six orders in multi-frequency learning, and outperforms deep networks in speed and accuracy.

Conclusion: Conditioning, not depth, is the bottleneck in neural solvers; simple architectural changes can yield substantial improvements.

Abstract: Accuracy in neural PDE solvers often breaks down not because of limited
expressivity, but due to poor optimisation caused by ill-conditioning,
especially in multi-fidelity and stiff problems. We study this issue in
Physics-Informed Extreme Learning Machines (PIELMs), a convex variant of neural
PDE solvers, and show that asymptotic components in governing equations can
produce highly ill-conditioned activation matrices, severely limiting
convergence. We introduce Shifted Gaussian Encoding, a simple yet effective
activation filtering step that increases matrix rank and expressivity while
preserving convexity. Our method extends the solvable range of Peclet numbers
in steady advection-diffusion equations by over two orders of magnitude,
achieves up to six orders lower error on multi-frequency function learning, and
fits high-fidelity image vectors more accurately and faster than deep networks
with over a million parameters. This work highlights that conditioning, not
depth, is often the bottleneck in scientific neural solvers and that simple
architectural changes can unlock substantial gains.

</details>


### [53] [Machine Learning-Based Nonlinear Nudging for Chaotic Dynamical Systems](https://arxiv.org/abs/2508.05778)
*Jaemin Oh,Jinsil Lee,Youngjoon Hong*

Main category: cs.LG

TL;DR: Neural network nudging is proposed to learn nudging terms for nonlinear state space models, validated on chaotic systems.


<details>
  <summary>Details</summary>
Motivation: Designing effective nudging terms for nonlinear systems is challenging, motivating a data-driven approach.

Method: Proposes neural network nudging, leveraging Kazantzis-Kravaris-Luenberger observer theory for theoretical grounding.

Result: Evaluated on chaotic systems (Lorenz 96, Kuramoto-Sivashinsky, Kolmogorov flow), showing effectiveness.

Conclusion: Neural network nudging is a viable method for nonlinear state space models, supported by theory and experiments.

Abstract: Nudging is an empirical data assimilation technique that incorporates an
observation-driven control term into the model dynamics. The trajectory of the
nudged system approaches the true system trajectory over time, even when the
initial conditions differ. For linear state space models, such control terms
can be derived under mild assumptions. However, designing effective nudging
terms becomes significantly more challenging in the nonlinear setting. In this
work, we propose neural network nudging, a data-driven method for learning
nudging terms in nonlinear state space models. We establish a theoretical
existence result based on the Kazantzis--Kravaris--Luenberger observer theory.
The proposed approach is evaluated on three benchmark problems that exhibit
chaotic behavior: the Lorenz 96 model, the Kuramoto--Sivashinsky equation, and
the Kolmogorov flow.

</details>


### [54] [Optimal Linear Baseline Models for Scientific Machine Learning](https://arxiv.org/abs/2508.05831)
*Alexander DeLise,Kyle Loh,Krish Patel,Meredith Teague,Andrea Arnold,Matthias Chung*

Main category: cs.LG

TL;DR: The paper presents a theoretical framework for analyzing linear encoder-decoder architectures in scientific machine learning, focusing on interpretability and Bayes risk minimization.


<details>
  <summary>Details</summary>
Motivation: To address the opacity of nonlinear neural networks and provide interpretable solutions for scientific machine learning problems.

Method: Develops a unified framework using Bayes risk minimization, deriving closed-form, rank-constrained linear and affine linear optimal mappings for forward and inverse tasks.

Result: Generalizes existing formulations to handle rank deficiencies and validates findings with numerical experiments across biomedical imaging, finance, and fluid dynamics.

Conclusion: Offers a robust baseline for benchmarking neural network models in scientific machine learning, emphasizing interpretability.

Abstract: Across scientific domains, a fundamental challenge is to characterize and
compute the mappings from underlying physical processes to observed signals and
measurements. While nonlinear neural networks have achieved considerable
success, they remain theoretically opaque, which hinders adoption in contexts
where interpretability is paramount. In contrast, linear neural networks serve
as a simple yet effective foundation for gaining insight into these complex
relationships. In this work, we develop a unified theoretical framework for
analyzing linear encoder-decoder architectures through the lens of Bayes risk
minimization for solving data-driven scientific machine learning problems. We
derive closed-form, rank-constrained linear and affine linear optimal mappings
for forward modeling and inverse recovery tasks. Our results generalize
existing formulations by accommodating rank-deficiencies in data, forward
operators, and measurement processes. We validate our theoretical results by
conducting numerical experiments on datasets from simple biomedical imaging,
financial factor analysis, and simulations involving nonlinear fluid dynamics
via the shallow water equations. This work provides a robust baseline for
understanding and benchmarking learned neural network models for scientific
machine learning problems.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [55] [Kahan's Automatic Step-Size Control for Unconstrained Optimization](https://arxiv.org/abs/2508.06002)
*Yifeng Meng,Chungen Shen,Linuo Xue,Lei-Hong Zhang*

Main category: math.OC

TL;DR: The paper analyzes the Kahan Gradient Descent (KGD) step-size strategies, showing equivalence to the Barzilai-Borwein (BB) step, and proves convergence rates for quadratic and general unconstrained minimization.


<details>
  <summary>Details</summary>
Motivation: To rigorously analyze the efficiency and effectiveness of KGD step-size strategies, which iterate step-sizes using previous iteration information, and extend their application to general unconstrained minimization.

Method: Derives a short KGD step-size, proves R-linear convergence for quadratic models, and proposes an adaptive framework for general minimization with global and local convergence proofs.

Result: For quadratic models, KGD step-sizes converge R-linearly. The adaptive framework ensures global convergence and local R-linear rates for general problems. Numerical experiments validate efficiency and robustness.

Conclusion: KGD step-sizes are effective and efficient, with proven convergence properties, and outperform other adaptive gradient methods in numerical tests.

Abstract: The Barzilai and Borwein (BB) gradient method is one of the most widely-used
line-search gradient methods. It computes the step-size for the current iterate
by using the information carried in the previous iteration. Recently, William
Kahan [Kahan, Automatic Step-Size Control for Minimization Iterations,
Technical report, University of California, Berkeley CA, USA, 2019] proposed
new Gradient Descent (KGD) step-size strategies which iterate the step-size
itself by effectively utilizing the information in the previous iteration. In
the quadratic model, such a new step-size is shown to be mathematically
equivalent to the long BB step, but no rigorous mathematical proof of its
efficiency and effectiveness for the general unconstrained minimization is
available. In this paper, by this equivalence with the long BB step, we first
derive a short version of KGD step-size and show that, for the strongly convex
quadratic model with a Hessian matrix $H$, both the long and short KGD
step-size (and hence BB step-sizes) gradient methods converge at least
R-linearly with a rate $1-\frac{1}{{\rm cond}(H)}$. For the general
unconstrained minimization, we further propose an adaptive framework to
effectively use the KGD step-sizes; global convergence and local R-linear
convergence rate are proved. Numerical experiments are conducted on the CUTEst
collection as well as the practical logistic regression problems, and we
compare the performance of the proposed methods with various BB step-size
approaches and other recently proposed adaptive gradient methods to demonstrate
the efficiency and robustness.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [56] [Spatiotemporal shaping of broadband helical light pulses at relativistic intensities](https://arxiv.org/abs/2508.05943)
*Andrew Longman,Danny Attiyah,Elizabeth Grace,Christopher Gardner,Tayyab Suratwala,Gary Tham,Colin Harthcock,Robert Fedosejevs,Franklin Dollar*

Main category: physics.optics

TL;DR: First experimental realization of relativistic-intensity light springs, achieving peak intensities above 1.4×10¹⁸ W/cm², enabling direct coupling of laser OAM to plasma OAM.


<details>
  <summary>Details</summary>
Motivation: Spatiotemporal control of laser pulses at relativistic intensities has broad implications for laser-plasma acceleration, radiation sources, and extreme-field science. Light springs, with helical spatiotemporal profiles, could couple directly to helical plasma waves but were previously limited to low-power systems.

Method: Spectrally split a high-power laser pulse, imprint distinct helical phases on each component, and coherently recombine them. Techniques like hyperspectral imaging and off-axis holography validated the results.

Result: Achieved peak intensities above 1.4×10¹⁸ W/cm², confirmed excellent agreement with theory, and demonstrated superluminal rotational velocities. Spectral chirp allowed control of transverse mode structure evolution.

Conclusion: This platform opens new regimes for ultra-intense laser-plasma interactions, enabling direct coupling of laser orbital angular momentum to plasma.

Abstract: Spatiotemporal control of laser pulses at relativistic intensities is a
longstanding goal with broad implications in laser-plasma acceleration,
high-brightness radiation sources, and extreme-field science. Laser pulses with
helical spatiotemporal intensity profiles, often referred to as light springs,
carry multiple spectral and orbital angular momentum (OAM) modes, producing a
rotating intensity profile capable of coupling directly to helical plasma
waves. Until now, light springs have only been realized on low-power systems,
limited by optical damage thresholds and large-aperture beamline constraints.
Here, we report the first experimental realization of light springs at
relativistic intensities, achieving peak intensities above $1.4\times10^{18}$
W/cm$^{2}$. Our approach spectrally splits a high-power laser pulse, imprints
distinct helical phases on each component, and coherently recombines them.
Hyperspectral imaging, off-axis holography, and spectral phase reconstruction
confirm excellent agreement with theory and reveal the potential to drive
superluminal rotational velocities. Introducing spectral chirp demonstrates
further control of the temporal evolution of the transverse mode structure.
This platform opens new regimes of ultra-intense laser-plasma interaction where
laser OAM can directly couple to plasma OAM.

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [57] [Constraining Active Galactic Nucleus Jets with Spectrum and Core Shift: The Case of M87](https://arxiv.org/abs/2508.06158)
*Kouichi Hirotani,Hsien Shang,Ruben Krasnopolsky,Satoki Matsushita,Britton Jeter,Keiichi Asada*

Main category: astro-ph.HE

TL;DR: The paper models AGN jets analytically, using VLBI and simulations to infer jet properties, applies it to M87, and suggests ALMA and VLBI observations could determine jet composition and collimation.


<details>
  <summary>Details</summary>
Motivation: To understand the properties and evolution of AGN jets, particularly the M87 jet, by combining analytical modeling with observational data.

Method: Analytical modeling of jets with energy conservation, using VLBI observations and simulations to constrain parameters, and integrating radiative transfer for SED and core shift calculations.

Result: Found plasma injection at 7 Schwarzschild radii, M87 jet likely pair plasma, quasi-parabolic geometry, and radio emission within 1000 Schwarzschild radii. Hadronic jets show potential for inverted SED.

Conclusion: Combined ALMA and VLBI observations could discriminate jet composition and collimation near the black hole.

Abstract: We analytically model stationary and axisymmetric active galactic nucleus
jets, assuming energy conservation along each magnetic flux tube. Using
very-long-baseline interferometry (VLBI) observations and published general
relativistic magnetohydrodynamic simulations, we constrain the evolution of the
bulk Lorentz factor, the magnetization parameter, and the magnetic field
strength along the jet. We then infer the electron density, emission
coefficient, and absorption coefficient at each point, and integrate the
radiative transfer equation to compute the spectral energy distribution (SED)
and the core shift of the synchrotron emission from the relativistic jet.
Applying the method to the M87 jet, we find that the hot plasmas are injected
at the altitude of seven Schwarzschild radii from the black hole (BH), that the
M87 jet is likely composed of a pair plasma, and that the jet flowline geometry
is quasi-parabolic as reported at much greater distances. Fitting the
nonthermal fraction of the leptonic jet as a function of position, we also find
that most of the radio photons are emitted within 1000 Schwarzschild radii from
the BH. Although hadronic jets do not reproduce all the VLBI observations
consistently in our model, we also discuss that their heavy mass allows a
stronger magnetic field within the observational constraints, leading to an
inverted SED in sub-millimeter wavelengths by the thermal emission from the jet
base. It is therefore implied that contemporaneous observations of the M87 jet
with Atacama Large Millimeter/submillimeter Array (ALMA) and VLBI could
discriminate the jet composition and its collimation within the central 100
Schwarzschild radii.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [58] [Stochastic Trace Optimization of Parameter Dependent Matrices Based on Statistical Learning Theory](https://arxiv.org/abs/2508.05764)
*Arvind K. Saibaba,Ilse C. F. Ipsen*

Main category: stat.ML

TL;DR: A Monte Carlo estimator is proposed for minimizing the trace of parameter-dependent matrices, with bounds on sampling amount for backward error control. Two bound types (epsilon nets and generic chaining) are derived, favoring small offdiagonal mass and compact parameter spaces.


<details>
  <summary>Details</summary>
Motivation: To efficiently minimize the trace of matrices dependent on parameters, ensuring bounded backward error with high probability.

Method: Monte Carlo estimator for trace minimization, with sampling bounds derived via epsilon nets and generic chaining.

Result: Bounds predict small sampling for matrices with small offdiagonal mass and compact parameter spaces, with weak or implicit dependence on matrix dimension.

Conclusion: Epsilon net bounds are easier to evaluate, while chaining bounds may be superior but are harder to compute. Comparisons between them remain challenging.

Abstract: We consider matrices $\boldsymbol{A}(\boldsymbol\theta)\in\mathbb{R}^{m\times
m}$ that depend, possibly nonlinearly, on a parameter $\boldsymbol\theta$ from
a compact parameter space $\Theta$. We present a Monte Carlo estimator for
minimizing $\text{trace}(\boldsymbol{A}(\boldsymbol\theta))$ over all
$\boldsymbol\theta\in\Theta$, and determine the sampling amount so that the
backward error of the estimator is bounded with high probability. We derive two
types of bounds, based on epsilon nets and on generic chaining. Both types
predict a small sampling amount for matrices
$\boldsymbol{A}(\boldsymbol\theta)$ with small offdiagonal mass, and parameter
spaces $\Theta$ of small ``size.'' Dependence on the matrix dimension~$m$ is
only weak or not explicit. The bounds based on epsilon nets are easier to
evaluate and come with fully specified constants. In contrast, the bounds based
on chaining depend on the Talagrand functionals which are difficult to
evaluate, except in very special cases. Comparisons between the two types of
bounds are difficult, although the literature suggests that chaining bounds can
be superior.

</details>


<div id='math.SP'></div>

# math.SP [[Back]](#toc)

### [59] [Selection of the ground state on a compact metric graph](https://arxiv.org/abs/2508.05986)
*Robert Marangell,Dmitry E. Pelinovsky*

Main category: math.SP

TL;DR: The paper analyzes the Fisher-KPP model on compact metric graphs, showing ground states are either trivial or nontrivial. Stability depends on edge sizes, with a sharp criterion for intermediate cases. A novel method using the period function is introduced for flower graphs.


<details>
  <summary>Details</summary>
Motivation: To understand the ground states of the Fisher-KPP model on metric graphs and their stability under varying edge conditions.

Method: Uses comparison theory, energy minimizers, graph Laplacian eigenvalues, and introduces a novel period function method for flower graphs.

Result: Trivial ground state is stable for small edges, nontrivial for large edges, with a sharp criterion for intermediate cases.

Conclusion: The study provides a comprehensive stability analysis of ground states in the Fisher-KPP model on metric graphs, with a new method for flower graphs.

Abstract: We show that the ground state in the Fisher--KPP model on a compact metric
graph with Dirichlet conditions on boundary vertices is either trivial (zero)
or nontrivial and strictly positive. For positive initial data, we prove that
the trivial ground state is globally asymptotically stable if the edges of the
metric graph are uniformly small and the nontrivial ground state is globally
asymptotically stable if the edges are uniformly large. For the intermediate
case, we find a sharp criterion for the existence, uniqueness and global
asymptotic stability of the trivial versus nontrivial ground state. Besides
standard methods based on the comparison theory, energy minimizers, and the
lowest eigenvalue of the graph Laplacian, we develop a novel method based on
the period function for differential equations to characterize the nontrivial
ground state in the particular case of flower graphs.

</details>
