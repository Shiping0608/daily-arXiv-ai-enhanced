<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 10]
- [math.AP](#math.AP) [Total: 6]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 6]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]
- [stat.ML](#stat.ML) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [math-ph](#math-ph) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [math.MG](#math.MG) [Total: 2]
- [astro-ph.EP](#astro-ph.EP) [Total: 1]
- [math.FA](#math.FA) [Total: 1]
- [physics.atom-ph](#physics.atom-ph) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Unsteady Navier-Stokes Equations On A Stationary Surface](https://arxiv.org/abs/2508.09266)
*Charles M. Elliott,Achilleas Mavrakis*

Main category: math.NA

TL;DR: A fully discrete numerical method for unsteady Navier-Stokes equations on surfaces in ℝ³ is analyzed, using SFEM with a Taylor-Hood pair and a Lagrange multiplier for weak tangential enforcement. Stability and error analysis are provided, with optimal and suboptimal results based on the Lagrange multiplier's space richness. Numerical simulations validate the theory.


<details>
  <summary>Details</summary>
Motivation: To develop and analyze a numerical method for solving unsteady Navier-Stokes equations on surfaces, addressing challenges like tangential condition enforcement and geometric approximation errors.

Method: Surface finite element method (SFEM) with a generalized Taylor-Hood pair and a Lagrange multiplier for weak tangential enforcement. Stability and error analysis are performed for different finite element space configurations.

Result: Optimal L²-norm bounds for velocity when the Lagrange multiplier space matches velocity space, and suboptimal otherwise. Optimal L²-norm bounds for pressure under certain conditions. Numerical simulations confirm theoretical results.

Conclusion: The method is effective, with performance dependent on the Lagrange multiplier's finite element space. Super-parametric elements improve results, and numerical tests support the analysis.

Abstract: In this paper we consider a fully discrete numerical method for the unsteady
Navier-Stokes equations on a smooth closed stationary surface in
$\mathbb{R}^3$. We use the surface finite element method (SFEM) with a
generalized Taylor-Hood finite element pair
$\mathrm{\mathbf{P}}_{k_u}$/$\mathrm{P}_{k_{pr}}$/$\mathrm{P}_{k_{\lambda}}$,
where we enforce the tangential condition of the velocity field weakly, by
introducing an extra Lagrange multiplier $\lambda$. Depending on the richness
of the finite element space involving this extra Lagrange multiplier we present
a full discrete stability and error analysis. For the velocity, we establish
optimal $L^{2}(a_h)$-norm bounds ($a_h$ - an energy norm) when $k_\lambda=k_u$
and suboptimal with respect to the geometric approximation error when
$k_{\lambda} = k_u-1$ (optimal when \emph{super-parametric finite elements} are
used). For the pressure, optimal $L^2(L^2)$-norm error bounds are established
when $k_\lambda=k_u$. Assuming further regularity assumptions for our
continuous problem, we are also able to show optimal convergence (using
\emph{super-parametric finite elements} again) when $k_\lambda=k_u-1$.
Numerical simulations that confirm the established theory are provided, along
with a comparative analysis against a penalty approach.

</details>


### [2] [A Nitsche method for Navier--Stokes/generalized poroelasticity interface problems](https://arxiv.org/abs/2508.09388)
*Aparna Bansal,Nicolas A. Barnafi,Dwijendra Narain Pandey,Ricardo Ruiz-Baier*

Main category: math.NA

TL;DR: A monolithic finite element method for coupled Navier-Stokes/poroelastic flow using Nitsche-type formulation, proven stable with error estimates and validated numerically.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of fluid-structure interaction in time-dependent coupled Navier-Stokes and poroelastic flow problems.

Method: Monolithic finite element discretization with implicit time stepping and Nitsche-type interface handling.

Result: Proven stability, a priori error estimates, and numerical validation of convergence rates and coupled dynamics.

Conclusion: The method is robust, stable, and accurate for capturing coupled fluid-poroelastic dynamics.

Abstract: We consider a time-dependent coupled Navier--Stokes/generalized poroelastic
flow problem and propose a unified and monolithic finite element discretization
based on implicit time stepping. To handle the fluid-structure interface we
employ a Nitsche-type formulation. The resulting discrete problem is shown to
be well-posed using the theory of differential-algebraic equations (DAEs) and
the Banach fixed-point theorem. We prove stability and derive a priori error
estimates for the fully discrete scheme. The stability and convergence of the
method are ensured by a properly chosen penalty parameter independent of the
mesh size. Numerical tests are presented to confirm the theoretical convergence
rates and to illustrate the ability of the method to capture the coupled
dynamics accurately.

</details>


### [3] [Trigonometric Interpolation Based Approach for Second Order Fredholm Integro-Differential Equations](https://arxiv.org/abs/2508.09413)
*Xiaorong Zou*

Main category: math.NA

TL;DR: The paper enhances a trigonometric interpolation algorithm for non-periodic functions to 2D space and applies it to solve second-order Fredholm integro-differential equations (FIDE), demonstrating high accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: To extend a 1D trigonometric interpolation algorithm to 2D space and apply it to solve FIDEs, addressing challenges like singularities and general boundary conditions.

Method: The enhanced 2D trigonometric interpolation algorithm is developed and tested on FIDEs with various boundary conditions and kernel types, including singular ones.

Result: The algorithm achieves high accuracy with moderate grid sizes, handles singularities effectively, and performs well across diverse scenarios.

Conclusion: The enhanced 2D algorithm is versatile, efficient, and accurate, with potential extensions to other types of integro-differential equations.

Abstract: A trigonometric interpolation algorithm for non-periodic functions has been
recently proposed and applied to study general ordinary differential equation
(ODE). This paper enhances the algorithm to approximate functions in $2$-dim
space. Performance of the enhanced algorithm is expected to be similar as in
$1$-dim case and achieve accuracy aligned with the smoothness of the target
function, which is confirmed by numerical examples.
  As an application, the $2$-dim trigonometric interpolation method is used to
develop an algorithm for the solution of a second order Fredholm
integro-differential equation (FIDE). There are several advantages of the
algorithm. First of all, it converges quickly and high accuracy can be achieved
with a moderate size of grid points; Secondly, it can effectively address
singularities of kernel functions and work well with general boundary
conditions. Finally, it can be enhanced to copy with other IDE such as Volterra
IDE or IDE with high order ODE component. The tests conducted in this paper
include various boundary conditions with both continuous kernels and integrable
ones with singularity. Decent performance is observed across all covered
scenarios with a moderate size of grid points.

</details>


### [4] [Generalized Plane Wave quasi-Trefftz spaces for wave propagation in inhomogeneous media](https://arxiv.org/abs/2508.09435)
*Ilaria Fontana,Lise-Marie Imbert-Gerard*

Main category: math.NA

TL;DR: The paper discusses numerical methods for wave propagation in inhomogeneous media, focusing on Trefftz and quasi-Trefftz methods, and introduces Generalized Plane Waves as a solution.


<details>
  <summary>Details</summary>
Motivation: Wave propagation in inhomogeneous media is modeled by PDEs with variable coefficients, but exact solutions are often unavailable, necessitating specialized numerical methods.

Method: The paper explores Trefftz methods (using exact PDE solutions) and quasi-Trefftz methods (using high-order approximate solutions), particularly Generalized Plane Waves.

Result: Quasi-Trefftz methods, like Generalized Plane Waves, provide viable solutions for variable-coefficient PDEs where exact solutions are unavailable.

Conclusion: The study highlights the potential of quasi-Trefftz methods, especially Generalized Plane Waves, for solving wave propagation problems in inhomogeneous media.

Abstract: Partial Differential Equations (PDEs) models for wave propagation in
inhomogeneous media are relevant for many applications. We will discuss
numerical methods tailored for tackling problems governed by these
variable-coefficient PDEs. Trefftz methods rely, in broad terms, on the idea of
approximating solutions to PDEs via Galerkin methods using basis functions that
are exact solutions of the PDE, making explicit use of information about the
ambient medium. However, wave propagation in inhomogeneous media is modeled by
PDEs with variable coefficients, and in general no exact solutions are
available. Quasi-Trefftz methods have been introduced, in the case of the
Helmholtz equation, to address this problem: they rely instead on high-order
approximate solutions constructed locally. We will discuss basis of Generalized
Plane Waves, a particular kind of quasi-Trefftz functions, and how their
construction can be related to the construction of polynomial quasi-Trefftz
bases.

</details>


### [5] [A hyperbolic finite difference scheme for anisotropic diffusion equations: preserving the discrete maximum principle](https://arxiv.org/abs/2508.09509)
*Tokuhiro Eto,Rei Kawashima*

Main category: math.NA

TL;DR: The paper analyzes the monotonicity of a hyperbolic system approach for anisotropic diffusion in quasineutral plasmas, ensuring the discrete maximum principle (DMP) with optimal parameter selection.


<details>
  <summary>Details</summary>
Motivation: To address the lack of understanding of the monotonicity of the hyperbolic system approach for anisotropic diffusion cases.

Method: Mathematical analysis and numerical experiments to determine optimal parameter conditions for DMP.

Result: The approach preserves DMP with appropriate preconditioning and parameter choice, even with linear discretization.

Conclusion: The hyperbolic system approach is robust for anisotropic diffusion when parameters are optimally selected.

Abstract: A hyperbolic system approach is proposed for robust computation of
anisotropic diffusion equations that appear in quasineutral plasmas. Though the
approach exhibits merits of high extensibility and accurate flux computation,
the monotonicity of the scheme for anisotropic diffusion cases has not been
understood. In this study, the discrete maximum principle (DMP) of the
hyperbolic system approach is analyzed and tested in various anisotropic
diffusion cases. A mathematical analysis is conducted to obtain an optimal
condition of an arbitrary parameter to guarantee the DMP, and numerical
experiments reveal an adoptive selection of the parameter for DMP-preserving
results. It is confirmed that, with an appropriate preconditioning matrix and
parameter choice, the hyperbolic system approach preserves the DMP even with a
linear discretization.

</details>


### [6] [Random Greedy Fast Block Kaczmarz Method for Solving Large-Scale Nonlinear Systems](https://arxiv.org/abs/2508.09596)
*Renjie Ding,Dongling Wang*

Main category: math.NA

TL;DR: A novel Random Greedy Fast Block Kaczmarz method is proposed for solving large-scale nonlinear systems efficiently, combining random and greedy strategies while avoiding costly pseudoinversion.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency of solving large-scale nonlinear systems due to computationally expensive pseudoinversion of Jacobian submatrices.

Method: Integrates random and greedy strategies, avoiding pseudoinversion, and uses a stochastic greedy condition number and relaxation parameter for convergence.

Result: Achieves linear convergence in expectation, with faster convergence under favorable conditions, outperforming other algorithms in efficiency and robustness.

Conclusion: The proposed method is efficient and robust for large-scale nonlinear systems, offering superior performance over comparable algorithms.

Abstract: To efficiently solve large scale nonlinear systems, we propose a novel Random
Greedy Fast Block Kaczmarz method. This approach integrates the strengths of
random and greedy strategies while avoiding the computationally expensive
pseudoinversion of Jacobian submatrices, thus enabling efficient solutions for
large scale problems. Our theoretical analysis establishes that the proposed
method achieves linear convergence in expectation, with its convergence rates
upper bound determined by the stochastic greedy condition number and the
relaxation parameter. Numerical experiments confirm that when the Jacobian
matrix exhibits a favorable stochastic greedy condition number and an
appropriate relaxation parameter is selected, the algorithm convergence is
significantly accelerated. As a result, the proposed method outperforms other
comparable algorithms in both efficiency and robustness.

</details>


### [7] [Gap-SBM: A New Conceptualization of the Shifted Boundary Method with Optimal Convergence for the Neumann and Dirichlet Problems](https://arxiv.org/abs/2508.09613)
*J. Haydel Collins,Kangan Li,Alexei Lozinski,Guglielmo Scovazzi*

Main category: math.NA

TL;DR: A new Shifted Boundary Method (SBM) is proposed for Dirichlet and Neumann boundary conditions, ensuring optimal accuracy in L²- and H¹-norms. It involves three stages: geometry approximation, solution extension, and variational integration.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of accurately handling boundary conditions in numerical methods, ensuring optimal error norms.

Method: Three-stage approach: 1) Approximate gap geometry using distance maps, 2) Extend numerical solutions and test functions, 3) Apply quadrature and shift operators for variational integration.

Result: Demonstrated optimal accuracy in L²- and H¹-norms through extensive 2D tests.

Conclusion: The SBM is effective for boundary condition treatment, achieving theoretical and practical optimal performance.

Abstract: We propose and mathematically analyze a new Shifted Boundary Method for the
treatment of Dirichlet and Neumann boundary conditions, with provable optimal
accuracy in the $L^2$- and $H^1$-norms of the error. The proposed method is
built on three stages. First, the distance map between the SBM surrogate
boundary and the true boundary is used to construct an approximation to the
geometry of the gap between the two. Then, the representations of the numerical
solution and test functions are extended from the surrogate domain to such gap.
Finally, approximate quadrature formulas and specific shift operators are
applied to integrate a variational formulation that also involves the fields
extended in the gap. An extensive set of two-dimensional tests demonstrates the
theoretical findings and the overall optimal performance of the proposed
method.

</details>


### [8] [Per-antenna power constraints: constructing Pareto-optimal precoders with cubic complexity under non-negligible noise conditions](https://arxiv.org/abs/2508.09646)
*Sergey Petrov,Samson Lasaulce,Merouane Debbah*

Main category: math.NA

TL;DR: A new algorithm for precoding matrix construction under per-antenna power constraints achieves SINR multiobjective Pareto-optimality with near-Zero-Forcing complexity.


<details>
  <summary>Details</summary>
Motivation: Current methods like Zero-Forcing are suboptimal for per-antenna power constraints and perform poorly with significant background noise.

Method: Proposes a computational algorithm that constructs a Pareto-optimal precoder with complexity close to Zero-Forcing.

Result: The algorithm efficiently parameterizes the Pareto boundary, allowing adjustable user throughput importance.

Conclusion: The presented algorithm offers a practical, near-optimal solution for real-time precoding under per-antenna constraints.

Abstract: Precoding matrix construction is a key element of the wireless signal
processing using the multiple-input and multiple-output model. It is
established that the problem of global throughput optimization under
per-antenna power constraints belongs, in general, to the class of monotonic
optimization problems, and is unsolvable in real-time. The most widely used
real-time baseline is the suboptimal solution of Zero-Forcing, which achieves a
cubic complexity by discarding the background noise coefficients. This
baseline, however, is not readily adapted to per-antenna power constraints, and
performs poorly if background noise coefficients are not negligible. In this
paper, we are going to present a computational algorithm which constructs a
precoder that is SINR multiobjective Pareto-optimal under per-antenna power
constraints - with a complexity that differs from that of Zero-Forcing only by
a constant factor. The algorithm has a set of input parameters, changing which
skews the importance of particular user throughputs: these parameters make up
an efficient parameterization of the entire Pareto boundary.

</details>


### [9] [Fast and Simple Multiclass Data Segmentation: An Eigendecomposition and Projection-Free Approach](https://arxiv.org/abs/2508.09738)
*Chiara Faccio,Margherita Porcelli,Francesco Rinaldi,Martin Stoll*

Main category: math.NA

TL;DR: A new framework for graph-based machine learning overcomes computational bottlenecks in the Allen-Cahn equation by using a penalty-based reformulation and an eigendecomposition-free optimization scheme, achieving faster performance with comparable accuracy.


<details>
  <summary>Details</summary>
Motivation: The computational inefficiency of existing methods (e.g., convexity splitting and MBO scheme) due to graph Laplacian eigendecomposition and repeated projections motivates the development of a more efficient approach.

Method: The proposed framework combines a penalty-based reformulation of the segmentation problem with an eigendecomposition-free optimization scheme, relying on sparse matrix-vector products for efficiency.

Result: Experiments show the framework matches or surpasses the accuracy of CS and MBO methods while being significantly faster, especially for large-scale problems.

Conclusion: The new method offers a scalable and efficient alternative for graph-based semi-supervised learning, addressing key computational challenges.

Abstract: Graph-based machine learning has seen an increased interest over the last
decade with many connections to other fields of applied mathematics. Learning
based on partial differential equations, such as the phase-field Allen-Cahn
equation, allows efficient handling of semi-supervised learning approaches on
graphs. The numerical solution of the graph Allen-Cahn equation via a convexity
splitting or the Merriman-Bence-Osher (MBO) scheme, albeit being a widely used
approach, requires the calculation of a graph Laplacian eigendecomposition and
repeated projections over the unit simplex to maintain valid partitions. The
computational efficiency of those methods is hence limited by those two
bottlenecks in practice, especially when dealing with large-scale instances. In
order to overcome these limitations, we propose a new framework combining a
novel penalty-based reformulation of the segmentation problem, which ensures
valid partitions (i.e., binary solutions) for appropriate parameter choices,
with an eigendecomposition and projection-free optimization scheme, which has a
small per-iteration complexity (by relying primarily on sparse matrix-vector
products) and guarantees good convergence properties. Experiments on synthetic
and real-world datasets related to data segmentation in networks and images
demonstrate that the proposed framework achieves comparable or better accuracy
than the CS and MBO methods while being significantly faster, particularly for
large-scale problems.

</details>


### [10] [Condition number for finite element discretisation of nonlocal PDE systems with applications to biology](https://arxiv.org/abs/2508.09781)
*Olusegun E. Adebayo,Raluca Eftimie,Dumitru Trucu*

Main category: math.NA

TL;DR: The paper analyzes the condition number of a coupled non-local reaction-diffusion-advection system for wound healing, providing bounds and discussing parameter and discretization effects.


<details>
  <summary>Details</summary>
Motivation: To understand and control the numerical stability of a wound healing model by analyzing its condition number.

Method: Finite element discretization of the coupled system, followed by establishing bounds for the condition number and examining parameter and discretization impacts.

Result: Bounds for the condition number are derived, and the influence of model parameters, time-stepping step size, and spatial grid size on conditioning is analyzed.

Conclusion: The study identifies parameter ranges and discretization choices that maintain a well-conditioned system, aiding in stable numerical simulations.

Abstract: In this work, we investigate the condition number for a system of coupled
non-local reaction-diffusion-advection equations developed in the context of
modelling normal and abnormal wound healing.
  Following a finite element discretisation of the coupled non-local system, we
establish bounds for this condition number.
  We further discuss how model parameter choices affect the conditioning of the
system. Finally, we discuss how the step size of the chosen time-stepping
scheme and the spatial grid size of the finite element methods affect the bound
for the condition number, while also suggesting possible parameter ranges that
could keep the model well conditioned.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [11] [Global uniform regularity for the 3D incompressible MHD equations with slip boundary condition near a background magnetic field](https://arxiv.org/abs/2508.09609)
*Jincheng Gao,Lianyun Peng,Jiahong Wu,Zheng-an Yao*

Main category: math.AP

TL;DR: The paper resolves the global regularity problem for 3D incompressible MHD equations with slip boundary conditions and a background magnetic field, using an anisotropic system with weak dissipation.


<details>
  <summary>Details</summary>
Motivation: Motivated by geophysical applications, the study explores how a background magnetic field stabilizes fluid dynamics in anisotropic MHD systems.

Method: A two-tier energy method is developed, coupling conormal and tangential derivatives, and four energy functionals are constructed to establish global bounds.

Result: Global-in-time uniform bounds are proven, independent of viscosity and resistivity, enabling rigorous justification of the vanishing dissipation limit.

Conclusion: The work demonstrates how magnetic fields enhance dissipation and stabilize fluid dynamics, solving a challenging problem in anisotropic MHD systems.

Abstract: This paper resolves the global regularity problem for the three-dimensional
incompressible magnetohydrodynamics (MHD) equations in the upper half-space
with slip boundary conditions, in the presence of a background magnetic field.
Motivated by geophysical applications, we consider an anisotropic MHD system
with weak dissipation in the $x_2$ and $x_3$ directions and small vertical
magnetic diffusion. By exploiting the stabilizing effect induced by the
background magnetic field and constructing a hierarchy of four energy
functionals, we establish global-in-time uniform bounds that are independent of
the viscosity in the $x_2$ and $x_3$ directions and the vertical resistivity. A
key innovation in our analysis is the development of a two-tier energy method,
which couples the boundedness of conormal derivatives with the decay of
tangential derivatives. These global conormal regularity estimates, together
with sharp decay rates, enable us to rigorously justify the vanishing
dissipation limit and derive explicit long-time convergence rates to the MHD
system with vanishing dissipation in the $x_2$ and $x_3$ directions and no
vertical magnetic diffusion. In the absence of a magnetic field, the
global-in-time vanishing viscosity limit for the 3D incompressible
Navier-Stokes equations with anisotropic dissipation remains a challenging open
problem. This work reveals the mechanism by which the magnetic field enhances
dissipation and stabilizes the fluid dynamics in the vanishing viscosity limit.

</details>


### [12] [Unconditional uniqueness for the derivative nonlinear Schrödinger equation by normal form approach](https://arxiv.org/abs/2508.09740)
*Nobu Kishimoto*

Main category: math.AP

TL;DR: Uniqueness of solutions for the derivative nonlinear Schrödinger equation in $L^\infty_tH^{1/2}_x$ is proven using a refined normal form reduction (NFR) method and Strichartz estimates.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of proving uniqueness at the $H^{1/2}$ regularity, where logarithmic divergences occur, by leveraging advanced techniques.

Method: A two-stage NFR approach: finite NFR applications to derive an intermediate equation, followed by infinite NFR. Uses refined Strichartz estimates for $B^{0+}_{\infty,1}$ control.

Result: Successful proof of uniqueness in $L^\infty_tH^{1/2}_x$, overcoming previous limitations.

Conclusion: The refined NFR method and Strichartz estimates enable direct uniqueness proof in $L^\infty_tH^{1/2}_x$, advancing the understanding of the equation.

Abstract: We prove uniqueness of solutions to the Cauchy problem for the derivative
nonlinear Schr\"{o}dinger equation in $L^\infty_tH^{1/2}_x$. Our proof is based
on the method of normal form reduction (NFR), which has been employed to obtain
the uniqueness in $C_tH^s_x$, $s>1/2$. To overcome logarithmic divergences at
the $H^{1/2}$ regularity, we exploit the $B^{0+}_{\infty,1}$ control of
solutions provided by a refined Strichartz estimate. Our NFR argument consists
of two stages: we first use NFR finitely many times to derive an intermediate
equation in which the main cubic nonlinearity is restricted to a certain type
of frequency interaction; we then apply the infinite NFR scheme to the
intermediate equation. Moreover, we modify the usual NFR argument relying on
continuity in time of solutions so that the uniqueness in the class
$L^\infty_tH^{1/2}_x$ can be obtained directly.

</details>


### [13] [On the uniqueness of strong solution to the nonhomogeneous incompressible Navier-Stokes-Cahn-Hilliard system](https://arxiv.org/abs/2508.09761)
*Lingxin Jiang,Jiahong Wu,Fuyi Xu*

Main category: math.AP

TL;DR: The paper resolves the open question of uniqueness for strong solutions in the nonhomogeneous incompressible Navier-Stokes-Cahn-Hilliard system without additional smoothness assumptions on initial density.


<details>
  <summary>Details</summary>
Motivation: To address the unresolved uniqueness of strong solutions in the system, previously requiring extra smoothness assumptions.

Method: Uses time-weighted estimates and the Lagrangian approach to prove uniqueness.

Result: Uniqueness of strong solutions is established without additional smoothness assumptions.

Conclusion: The work successfully resolves the uniqueness question, advancing the understanding of the system.

Abstract: This paper is mainly concerned with an initial-boundary value problem of the
nonhomogeneous incompressible Navier-Stokes-Cahn-Hilliard system with the
Landau potential in a two and three dimensions. Global existence of strong
solutions with bounded and strictly positive density for this system was proven
by Giorgini and Temam \cite{GT}; however, an additional smoothness assumption
on the initial density was needed to prove uniqueness in \cite{GT1}. Whether
uniqueness holds without this additional assumption has remained an open
question. The present work solves this question and we finally establish
uniqueness of the strong solution in the framework in \cite{GT}. Our method
relies on some extra time weighted estimates and the Lagrangian approach.

</details>


### [14] [Fujita-Kato solution for the 3D compressible pressureless Navier-Stokes equations with discontinuous and large-variation density](https://arxiv.org/abs/2508.09764)
*Xiaojie Wang. Jiahong Wu. Fuyi Xu*

Main category: math.AP

TL;DR: Global existence and uniqueness of Fujita-Kato solutions for 3D compressible pressureless Navier-Stokes equations with discontinuous, large-variation initial density and critical initial velocity.


<details>
  <summary>Details</summary>
Motivation: Addressing the Cauchy problem for 3D compressible pressureless Navier-Stokes equations, derived from collective behavior models, to extend understanding beyond classical systems.

Method: Uses time-weighted estimates and the Lagrangian approach to construct solutions.

Result: Proves global-in-time existence and uniqueness of Fujita-Kato solutions under specified initial conditions.

Conclusion: The study successfully establishes rigorous solutions for the system, contributing to the analysis of collective behavior models.

Abstract: This paper mainly focuses on the Cauchy problem to the 3D compressible
pressureless Navier-Stokes equations arising from models of collective
behavior, which can be derived by taking the high Mach number limit of the
classical compressible Navier-Stokes system. We construct the global-in-time
existence and uniqueness of the so-called Fujita-Kato solution to the system,
provided that the initial density $\rho_0$ is discontinuous, large-variation
and the initial velocity $u_0$ is in a critical functional framework. Our
method relies on some time weighted estimates and the Lagrangian approach.

</details>


### [15] [An alternative condition for the solvability of the Dirichlet problem for the minimal surface equation on non-mean convex domains](https://arxiv.org/abs/2508.09806)
*Ari J. Aiolfi,Giovanni da Silva Nunes,Jaime Ripoll,Lisandra Sauer,Rodrigo Soares*

Main category: math.AP

TL;DR: A new solvability condition for the Dirichlet problem in non-mean convex domains, derived from a second-order ODE, offering practical advantages over the Jenkins-Serrin criterion.


<details>
  <summary>Details</summary>
Motivation: To address limitations of existing criteria (e.g., Jenkins-Serrin) for solving the Dirichlet problem in non-mean convex domains by introducing a more flexible and geometrically intuitive condition.

Method: Derives a solvability condition from a second-order ODE, producing a novel barrier. Applies to Hadamard manifolds and unbounded domains, separating domain geometry from boundary data.

Result: The new condition is more practical, less restrictive, and works where Jenkins-Serrin fails, especially in Euclidean cases.

Conclusion: The proposed framework provides a clearer, more manageable approach to solvability analysis, decoupling domain geometry from boundary data.

Abstract: We propose an alternative condition for the solvability of the Dirichlet
problem for the minimal surface equation that applies to non-mean convex
domains. This condition is derived from a second-order ordinary differential
equation whose solution produces a barrier that appears to be novel in the
context of barrier constructions. It admits an explicit formulation and, in the
setting of Hadamard manifolds, reveals a direct and transparent relationship
between the geometry of the domain and the behavior of the boundary data
required for solvability. The condition also extends naturally to unbounded
domains. In the Euclidean case, it is not only more practical to verify but
also less restrictive than the classical Jenkins - Serrin criterion, ensuring
the existence of solutions in situations where that approach fail. Furthermore,
unlike the Jenkins-Serrin condition, our appproach separates the geometric
properties of the domain from its boundary data, providing a clearer and more
manageable framework for solvability analysis.

</details>


### [16] [Sharp quantitative integral inequalities for harmonic extensions](https://arxiv.org/abs/2508.09940)
*Rupert L. Frank,Jonas W. Peteranderl,Larry Read*

Main category: math.AP

TL;DR: Quantitative proof of a sharp integral inequality for the Poisson operator and its adjoint, with optimal stability exponent.


<details>
  <summary>Details</summary>
Motivation: To establish a quantitative version of a known sharp integral inequality, extending insights from prior work on stability exponents.

Method: Proving the inequality for both the Poisson operator and its adjoint, focusing on norm strength and stability exponent.

Result: Achieved the strongest possible norm and optimal stability exponent, which differs from 2, aligning with prior observations.

Conclusion: The study confirms the variability of stability exponents, similar to findings in $p$-Sobolev inequalities, enhancing understanding of such inequalities.

Abstract: We prove a quantitative version of a sharp integral inequality by Hang, Wang,
and Yan for both the Poisson operator and its adjoint. Our result has the
strongest possible norm and the optimal stability exponent. This stability
exponent is not necessarily equal to 2, displaying the same phenomenon that
Figalli and Zhang observed for the $p$-Sobolev inequality.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [17] [A Pseudo-Fermion Propagator Approach to the Fermion Sign Problem](https://arxiv.org/abs/2508.09557)
*Yunuo Xiong,Hongwei Xiong*

Main category: physics.comp-ph

TL;DR: A pseudo-fermion propagator is introduced to avoid the fermion sign problem, enabling efficient simulations of fermionic systems with accurate energy predictions.


<details>
  <summary>Details</summary>
Motivation: To address the fermion sign problem in simulations of fermionic systems, which hinders efficient and reliable calculations.

Method: Construct a pseudo-fermion propagator by replacing the fermionic determinant with its absolute value and adjust energy based on a non-interacting system.

Result: Accurate energy predictions for fermionic systems, validated by first-principles simulations of quantum dots in a 2D harmonic potential.

Conclusion: The pseudo-fermion propagator offers a promising approach for first-principles simulations of fermionic systems.

Abstract: In this work, within the framework of path integral Monte Carlo, we construct
a pseudo-fermion propagator by replacing the original fermionic determinant
with its absolute value. This modified propagator defines an auxiliary system
free from the fermion sign problem, enabling efficient simulations of fermionic
systems. We found that by shifting the pseudo-fermion energy based on the
energy of a non-interacting fermion system, we can efficiently and reliably
infer the energy of fermionic systems in various situations, from strong
quantum degeneracy to weak quantum degeneracy. We have performed
first-principles simulations of quantum dots confined in a two-dimensional
harmonic potential and found excellent agreement with benchmark results
provided by other established methods. We believe that this pseudo-fermion
propagator framework opens up new possibilities for first-principles
simulations of fermionic systems.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [18] [Generation of High Order Harmonics in Vacuum for Various Configurations of Interacting Electromagnetic Field](https://arxiv.org/abs/2508.09214)
*Pavel Sasorov,Sergei Bulanov*

Main category: physics.plasm-ph

TL;DR: The paper investigates high-order harmonic (HOH) generation in the quantum vacuum using intense electromagnetic waves, focusing on perturbation theory and specific beam geometries.


<details>
  <summary>Details</summary>
Motivation: To explore HOH generation in the quantum vacuum under intense electromagnetic waves, extending previous work on plane wave collisions.

Method: Uses the Heisenberg-Euler formalism and perturbation theory, analyzing general and specific beam geometries (4π-dipole and Gaussian beams).

Result: Derives explicit expressions for HOH generation, identifying optimal and experimentally realistic beam geometries.

Conclusion: Generalizes earlier results on HOH generation, providing insights for both theoretical and experimental applications.

Abstract: High order harmonic (HOH) generation by interacting extremely intense
electromagnetic waves in the quantum vacuum is investigated within the
framework of the Heisenberg-Euler formalism. We consider here the process in
the lowest order of a perturbation theory relative to the electromagnetic (EM)
beam intensity, giving contribution to the HOH generation. The main expressions
are obtained for a general geometry, whyle polarizations of different sub-beams
forming the EM beam focus are almost the same. Nevertheless, explicit
expressions for the HOH generation are derived for the $4\pi$-dipole in-coming
waves and for the two crossing Gaussian beams. The former geometry of the EM
beam is optimal at a given EM wave power, whereas the latter one is more
realistic from the experimental point of view. We consider also a relationship
of our present general results with the results, obtained earlier for the HOH
generation during of collision of two plane electromagnetic waves.

</details>


### [19] [Surface Current Optimization and Coil-Cutting Algorithms for Stage-Two Stellarator Optimization](https://arxiv.org/abs/2508.09321)
*Dario Panici,Rory Conlin,Rahul Gaur,Daniel W. Dudt,Yigit Gunsur Elmacioglu,Matt Landreman,Todd Elder,Nadav Snir,Itay Gissis,Yasha Nikulshin,Egemen Kolemen*

Main category: physics.plasm-ph

TL;DR: The paper details the mathematical and physical properties of the surface current potential in stellarator coil optimization, including derivations and an algorithm for discretizing surface currents into coils.


<details>
  <summary>Details</summary>
Motivation: To address gaps in literature regarding the coil cutting procedure and the properties of the surface current potential in stellarator optimization.

Method: Presents mathematical derivations of surface current properties, details an algorithm for discretizing surface currents into coils, and implements it in the DESC code.

Result: Provides explicit derivations and a practical algorithm for coil optimization, demonstrated with modular and helical coil examples.

Conclusion: The work enhances stellarator coil design by clarifying surface current properties and offering a usable tool in the DESC code.

Abstract: Stellarator optimization often takes a two-stage approach, where in the first
stage the boundary is varied in order to optimize for some physics metrics,
while in the second stage the boundary is kept fixed and coils are sought to
generate a magnetic field that can recreate the desired stellarator. Past
literature dealing with this stage lacks details on the coil cutting procedure
and the mathematical and physical properties of the surface current potential
which dictates it. In this work, some basic physical quantities of the surface
current and how they relate to the parameters in the current potential are
presented, and supported for the first time by explicit mathematical
derivations. Additionally, the details of how to account for the presence of an
external field in the surface current algorithm are explicitly presented. These
relations underpin the procedure of discretizing the surface current into
coils. Finally, the conventionally-used algorithm for discretizing the surface
current into coils is detailed, along with an example coil optimization for
both a modular and a helical coilset. The algorithm is implemented in the
\texttt{DESC} code, with both modular and helical coil capabilities, where it
is available for use in stellarator coil design.

</details>


### [20] [The Enhancement of Ion Heating in Kinetic, Anti-Parallel Reconnection in the Presence of a Flow Shear](https://arxiv.org/abs/2508.09424)
*Colby C. Haggerty,Derek Sikorski,Michael A. Shay,Tai D. Phan,Paul A. Cassak,Giulia Murtas,Prayash S. Pyakurel*

Main category: physics.plasm-ph

TL;DR: Flow shear in magnetic reconnection enhances ion heating by 300%, reduces outflow speeds, and modifies reconnection geometry, explaining suppressed reconnection in turbulent plasmas.


<details>
  <summary>Details</summary>
Motivation: To understand how upstream, magnetic field-aligned flow shear affects anti-parallel magnetic reconnection, contrasting with previous Hall MHD studies.

Method: 2.5D Particle-In-Cell simulations were used to study the kinetic effects of flow shear on reconnection.

Result: Flow shear increases ion heating by 300%, reduces outflow speeds, and modifies reconnection geometry by converting flow shear energy into thermal energy.

Conclusion: The findings explain suppressed reconnection in turbulent plasmas and highlight flow shear's role in energy dissipation.

Abstract: We investigate the kinetic effects of upstream, magnetic field-aligned, flow
shear on anti-parallel magnetic reconnection using 2.5D Particle-In-Cell
simulations. Our results demonstrate that flow shear significantly alters the
reconnection process, leading to enhanced ion heating, reduced outflow speeds,
and a modified reconnection geometry. In contrast to previous Hall
Magnetohydrodynaic (MHD) studies, we find that reconnection becomes a more
efficient plasma heating mechanism in the presence of sub-Alfv\'enic flow
shear, with ion heating increasing by as much as 300\%. This enhanced heating
is achieved by efficiently converting the incoming flow shear energy into
thermal energy through istropization in the exhaust. The enhanced heating leads
to a pressure gradient away form the x-line exerting a force that reduces the
outflow jet speed and slows down the reconnection process. This conversion is
due to beam selection effects, mixing and scattering in the exhaust. A
theoretical model is developed which predicts well the exhaust heating and
outflow speed reduction. These results offer a potential explanation for recent
Parker Solar Probe observations of suppressed reconnection in the presence of
flow shear and carry significant implications for energy dissipation in
turbulent plasmas.

</details>


### [21] [Extracting temperature of plasma through fusion reactions within a transport approach](https://arxiv.org/abs/2508.09431)
*Zhe Zhu,Jun Xu*

Main category: physics.plasm-ph

TL;DR: The paper investigates temperature extraction from proton-to-neutron yield ratios in fusion reactions using laser-driven deuterium clusters. Simulations show deviations from thermal distributions, affecting temperature estimates.


<details>
  <summary>Details</summary>
Motivation: To understand how temperature extraction from fusion reactions is influenced by non-thermal deuteron distributions and local density fluctuations.

Method: Particle-in-cell simulations of Coulomb explosion in deuterium clusters with stochastic fusion reactions, assuming log-normal deuteron distributions.

Result: The weighted yield ratio gives higher effective temperatures than spectrum fitting due to excess deuterons in intermediate/high-energy regions. Density fluctuations further skew results.

Conclusion: Non-thermal deuteron distributions and density fluctuations complicate temperature extraction, leading to overestimated values from yield ratios.

Abstract: We have investigated the method of extracting the temperature from weighted
proton-to-neutron yield ratio from fusion reactions as in the previous
experiment~[W. Bang, {\it et al.}, Phys. Rev. Lett. \textbf{111}, 055002
(2013).] using the Texas Petawatt laser beam. The Coulomb explosion of
deuterium clusters is simulated based on the particle-in-cell model in a box
system with periodic boundary conditions, and fusion reactions are incorporated
through the stochastic method. As long as the deuteron numbers in deuterium
clusters follow a log-normal distribution, the low-energy part of the final
deuteron spectrum can be fitted by a Maxwell-Boltzmann distribution, while
there are more deuterons in the intermediate- and high-energy region compared
to a thermal distribution, and dominate the weighted yield ratio. Therefore,
the effective temperature extracted from the weighted yield ratio is generally
higher than that from fitting the final deuteron spectrum. The local density
fluctuation, which intrinsically exists due to the log-normal distribution of
deuteron numbers, further enhances hot deuteron-deuteron collisions, and
significantly affects the weighted yield ratio.

</details>


### [22] [Exact expressions for nonperturbative guiding center theory in symmetric fields](https://arxiv.org/abs/2508.09484)
*I. Hollas,R. Agarwal,J. W. Burby,A. J. Brizard*

Main category: physics.plasm-ph

TL;DR: The paper applies a nonperturbative guiding center formalism to charged particle dynamics in fields with two-parameter symmetry, deriving exact constants of motion that align with Kruskal's adiabatic invariant series.


<details>
  <summary>Details</summary>
Motivation: To extend the guiding center formalism to nonperturbative regimes and validate its predictions against established perturbative results.

Method: Uses a nonperturbative guiding center formalism to analyze charged particle dynamics in symmetric fields, comparing results with Kruskal's adiabatic invariant series.

Result: The nonperturbative model makes exact predictions, even without resolving the cyclotron timescale, validating its theoretical robustness.

Conclusion: The nonperturbative guiding center formalism is theoretically sound and performs well in symmetric field scenarios, bridging perturbative and nonperturbative regimes.

Abstract: We apply a recently-developed nonperturbative guiding center formalism to
charged particle dynamics in fields with two-parameter continuous symmetry
groups. This entails finding exact constants of motion, valid in the
nonperturbative regime, that agree with Kruskal's adiabatic invariant series to
all orders in the perturbative regime, when the field scale length is large
compared with a typical gyroradius. We demonstrate that the nonperturbative
guiding center model makes exact predictions in these cases, even though it
eliminates the cyclotron timescale, thereby establishing a theoretical baseline
for performance of the nonperturbative formalism.

</details>


### [23] [Exploring the Physics of the Plasma Liner Experiment: A Multi-dimensional Study with FLASH, OSIRIS, and HELIOS](https://arxiv.org/abs/2508.09895)
*E. C. Hansen,P. Farmakis,D. Michta,C. Ren,H. Wen,S. Langendorf,F. Chu,P. Tzeferacos*

Main category: physics.plasm-ph

TL;DR: The PLX experiment at LANL explores fusion via PJMIF, simulating three phases (target formation, liner formation, compression) with codes like FLASH, OSIRIS, and HELIOS. Results show potential for achieving fusion-relevant conditions.


<details>
  <summary>Details</summary>
Motivation: To achieve fusion using Plasma-Jet-Driven Magneto-Inertial Fusion (PJMIF) by studying plasma dynamics in three distinct phases.

Method: Simulated three phases (target formation, liner formation, compression) using 1D, 2D, and 3D codes (FLASH, OSIRIS, HELIOS) to analyze plasma regimes and physics.

Result: PLX can form a preheated, magnetized target plasma and compress it to fusion-relevant conditions (temperatures >1 keV).

Conclusion: The PLX experiment demonstrates feasibility of PJMIF for achieving fusion, supported by multi-code simulations.

Abstract: The Plasma Liner Experiment (PLX) at Los Alamos National Laboratory (LANL) is
a platform that seeks to achieve fusion via a concept known as
Plasma-Jet-Driven Magneto-Inertial Fusion (PJMIF). The experiment consists of
three main phases: (1) target formation in which up to four plasma guns shoot
magnetized hydrogen or deuterium-tritium jets to form a quasi-spherical target,
(2) liner formation in which a constellation of 36 guns fire high-atomic number
(e.g., xenon) jets to form a liner shell, and (3) target compression in which
the formed liner implodes the pre-formed target. Each phase of the PLX probes
different plasma regimes with different physics at play, thus we simulated each
phase separately and with multiple codes. Here we highlight some of the 1D, 2D,
and 3D simulation results of all three phases from the FLASH, OSIRIS, and
HELIOS codes. Some of the key physical processes involved include shock
dynamics, kinetic effects, anisotropic thermal conduction, resistive magnetic
diffusion, radiation transport, and magnetized jet dynamics. Our simulations
show that the PLX can form a preheated (~40 eV), magnetized (electron Hall
parameter >1) target plasma, and a quasi-collisional liner shell that can
subsequently compress the target to fusion-relevant conditions (e.g.,
temperatures >1 keV).

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [24] [Thermal gradient effect on hydrogen transport in tungsten](https://arxiv.org/abs/2508.09169)
*Sanad Alturk,Jacob Jeffries,Muhammed Kose,Enrique Martinez*

Main category: cond-mat.mtrl-sci

TL;DR: The paper studies hydrogen transport in tungsten under thermal gradients, critical for fusion energy safety and efficiency.


<details>
  <summary>Details</summary>
Motivation: Understanding tritium retention in tungsten is vital for fusion energy deployment due to radioactive and fuel efficiency concerns.

Method: Developed an analytical approach to compute the heat of transport (Q*) using molecular dynamics simulations.

Result: Found Q* as a function of temperature and other parameters, showing its independence of thermal gradient to first order.

Conclusion: The study provides insights into hydrogen transport in tungsten, aiding fusion device design and safety.

Abstract: One key challenge for efficiency and safety in fusion devices is the
retention of tritium (T) in plasma-facing components. Tritium retention
generates radioactive concerns and decreases the amount of fuel available to
generate power. Hence, understanding the behavior of T in tungsten (W), as the
main candidate as armor material, is critical to the deployment of fusion as a
reliable energy source. In this work, we have studied the effect of a thermal
gradient in the transport properties of hydrogen (as a T surrogate) in pure W.
Strong thermal gradients develop in the divertor as a result of the intense
energy fluxes arriving at the material. We have developed an analytical
approach to compute the heat of transport ($Q^*$) that is parameterized from
molecular dynamics (MD) simulations. $Q^*$ is a parameter needed in
irreversible thermodynamics frameworks to understand mass transport in the
presence of thermal gradients. We show that $Q^*$ can be written as a function
of temperature, temperature gradient, a characteristic length and the ratio of
the rates towards hot and cold regions. Furthermore, we describe how, to first
order, the dependence of $Q^*$ on the thermal gradient vanishes, in agreement
with MD results. On average, we find $Q^*=-5.41\times 10^{-3}kT^2~\text{eV}$
for H in pure W, with $k$ the Boltzmann constant and $T$ the temperature.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [25] [The First Differentiable Transfer-Based Algorithm for Discrete MicroLED Repair](https://arxiv.org/abs/2508.09206)
*Ning-Yuan Lue*

Main category: cs.LG

TL;DR: A novel repair algorithm for laser-enabled microLED transfer reduces steps by 50% and speeds up planning, outperforming RL and proximity methods.


<details>
  <summary>Details</summary>
Motivation: Efficient microLED fabrication requires minimizing XY stage motion and adapting to diverse optimization goals.

Method: A differentiable transfer module models discrete shifts, enabling gradient-based optimization for flexible objective designs.

Result: Achieves 50% fewer transfer steps and sub-2-minute planning for 2000x2000 arrays.

Conclusion: The method offers a scalable, adaptable solution for faster microLED repair in AR/VR and display fabrication.

Abstract: Laser-enabled selective transfer, a key process in high-throughput microLED
fabrication, requires computational models that can plan shift sequences to
minimize motion of XY stages and adapt to varying optimization objectives
across the substrate. We propose the first repair algorithm based on a
differentiable transfer module designed to model discrete shifts of transfer
platforms, while remaining trainable via gradient-based optimization. Compared
to local proximity searching algorithms, our approach achieves superior repair
performance and enables more flexible objective designs, such as minimizing the
number of steps. Unlike reinforcement learning (RL)-based approaches, our
method eliminates the need for handcrafted feature extractors and trains
significantly faster, allowing scalability to large arrays. Experiments show a
50% reduction in transfer steps and sub-2-minute planning time on 2000x2000
arrays. This method provides a practical and adaptable solution for
accelerating microLED repair in AR/VR and next-generation display fabrication.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [26] [Distributional Sensitivity Analysis: Enabling Differentiability in Sample-Based Inference](https://arxiv.org/abs/2508.09347)
*Pi-Yueh Chuang,Ahmed Attia,Emil Constantinescu*

Main category: stat.ML

TL;DR: Two analytical formulae for estimating sensitivity (gradient/Jacobian) of a random vector's distributional parameters, with numerical algorithms and validation.


<details>
  <summary>Details</summary>
Motivation: To enable uncertainty quantification and parameter inference without model fitting or high-dimensional integrals, useful for black-box samplers or expensive simulations.

Method: Two formulae: one using partial derivatives of inverse mapping, the other a diagonal approximation for efficiency. Four numerical algorithms for approximation.

Result: Verified correctness and effectiveness, demonstrated in a nuclear physics application.

Conclusion: The method avoids model fitting and high-dimensional integrals, making it suitable for black-box samplers and deep learning integration.

Abstract: We present two analytical formulae for estimating the sensitivity -- namely,
the gradient or Jacobian -- at given realizations of an arbitrary-dimensional
random vector with respect to its distributional parameters. The first formula
interprets this sensitivity as partial derivatives of the inverse mapping
associated with the vector of 1-D conditional distributions. The second
formula, intended for optimization methods that tolerate inexact gradients,
introduces a diagonal approximation that reduces computational cost at the cost
of some accuracy. We additionally provide four second-order numerical
algorithms to approximate both formulae when closed forms are unavailable. We
performed verification and validation studies to demonstrate the correctness of
these numerical algorithms and the effectiveness of the proposed formulae. A
nuclear physics application showcases how our work enables uncertainty
quantification and parameter inference for quantum correlation functions. Our
approach differs from existing methods by avoiding the need for model fitting,
knowledge of sampling algorithms, and evaluation of high-dimensional integrals.
It is therefore particularly useful for sample-based inverse problems when the
sampler operates as a black box or requires expensive physics simulations.
Moreover, our method renders arbitrary sampling subroutines differentiable,
facilitating their integration into programming frameworks for deep learning
and automatic differentiation. Algorithmic details and code implementations are
provided in this paper and in our open-source software DistroSA to enable
reproducibility and further development.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [27] [Boundary layer flow dynamics of propulsive flapping foils with increasing Reynolds numbers](https://arxiv.org/abs/2508.09590)
*Andhini N. Zurman-Nasution,Gabriel D. Wymouth,Bharathram Ganapathisubramani*

Main category: physics.flu-dyn

TL;DR: The paper examines flapping foils at high Reynolds numbers, revealing relaminarization in boundary layers due to leading-edge vortices, with implications for noise and drag reduction.


<details>
  <summary>Details</summary>
Motivation: To understand how unsteady boundary layers in flapping foils change with increasing Reynolds numbers and the role of leading-edge vortices.

Method: Studied flapping foils at St=0.3 and Re=10^4, 10^5, 10^6, analyzing boundary layer characteristics, LEV generation, and relaminarization effects.

Result: Higher Re leads to smaller, more numerous LEVs with stable breakdown, extended laminar regions, and reduced wall friction fluctuations.

Conclusion: Relaminarization may persist beyond Re=10^6, suggesting potential for high-Re flapping foils in noise and drag reduction applications.

Abstract: We study flapping foils at optimally propulsive Strouhal number $St=0.3$ with
increasing chord-based Reynolds number at $Re = 10^4$, $10^5$, and $10^6$ to
examine changes in their unsteady boundary layers. Despite being prescribed the
same freestream, the inner boundary layer characteristics exhibit different
trends due to the generation of leading-edge vortices (LEVs) and their
advection into the downstream flow. Propulsive flapping foils show an extended
laminar region known as the \textit{relaminarization}, during which the
velocity profiles deviate from the standard log law. This
\textit{relaminarization} is accompanied by a significant decrease in the
cyclic fluctuation of the wall friction coefficient and an increase in the
shape factor while the freestream velocity increases under favourable pressure
gradient conditions. This phenomenon intensifies with increasing $Re$. We found
that higher $Re$ produces smaller LEVs in greater quantities, with a more rapid
but stable breakdown, without resulting in a more chaotic turbulent downstream.
This study strongly indicates that the \textit{relaminarization} could extend
beyond $Re=10^6$ as predicted by \citet{Fukagata2023}. The results support the
potential for further exploration of flapping foils at high $Re$ for noise and
drag reduction.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [28] [Semi-discrete multi-to -one dimensional variational problems](https://arxiv.org/abs/2508.09490)
*Omar Abdul Halim,Daniyar Omarov,Brendan Pass*

Main category: math.OC

TL;DR: The paper studies semi-discrete variational problems in economic matching and game theory, focusing on optimal transport between unequal dimensions. It simplifies solution structures under certain conditions, enabling efficient algorithms with rigorous convergence guarantees.


<details>
  <summary>Details</summary>
Motivation: The research addresses challenges in economic matching and game theory, particularly in scenarios with continuous agent attributes and finite outcomes, such as Cournot-Nash equilibria and hedonic pricing.

Method: The authors formulate the problem as optimal transport between spaces of unequal dimensions, leveraging discrete strategy spaces and simplifying solution structures under specific conditions.

Result: The proposed algorithms outperform standard optimal transport solvers, especially for large discrete outcomes, with proven convergence guarantees.

Conclusion: The approach offers significant computational advantages, validated by numerical examples, making it a practical solution for such problems.

Abstract: We study a class of semi-discrete variational problems that arise in economic
matching and game theory, where agents with continuous attributes are matched
to a finite set of outcomes with a one dimensional structure. Such problems
appear in applications including Cournot-Nash equilibria, and hedonic pricing,
and can be formulated as problems involving optimal transport between spaces of
unequal dimensions. In our discrete strategy space setting, we establish
analogues of results developed for a continuum of strategies in
\cite{nenna2020variational}, ensuring solutions have a particularly simple
structure under certain conditions. This has important numerical consequences,
as it is natural to discretize when numerically computing solutions. We
leverage our results to develop efficient algorithms for these problems which
scale significantly better than standard optimal transport solvers,
particularly when the number of discrete outcomes is large, provided our
conditions are satisfied. We also establish rigorous convergence guarantees for
these algorithms. We illustrate the advantages of our approach by solving a
range of numerical examples; in many of these our new solvers outperform
alternatives by a considerable margin.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [29] [Efficient predecision scheme for Metropolis Monte Carlo simulation of long-range interacting lattice systems](https://arxiv.org/abs/2508.09775)
*Fabio Müller,Wolfhard Janke*

Main category: cond-mat.stat-mech

TL;DR: A fast predecision scheme for Metropolis Monte Carlo simulations reduces computational complexity for long-range interacting lattice models.


<details>
  <summary>Details</summary>
Motivation: To address the high computational cost of simulating long-range interactions in lattice models, which traditionally scales poorly with system size.

Method: Proposes a predecision scheme that reduces complexity from O(N²) to O(N^(2−σ/d)) or O(N), depending on the potential's decay rate (σ). Tested on various spin models.

Result: The algorithm matches exact simulations' Markov chains while significantly lowering computational cost, making it efficient for large systems.

Conclusion: The method's simplicity, generality, and efficiency make it broadly applicable for studying long-range interactions in lattice models, particularly in nonequilibrium physics.

Abstract: We propose a fast and general predecision scheme for Metropolis Monte Carlo
simulation of $d$-dimensional long-range interacting lattice models. For
potentials of the form $V(r)=r^{-d-\sigma}$, this reduces the computational
complexity from $O\left(N^2\right)$ to $O\left(N^{2-\sigma/d}\right)$ for
$\sigma < d$ and to $O\left(N \right)$ for $\sigma > d$, respectively. The
algorithm is implemented and tested for several $\mathrm{O}(n)$ spin models
ranging from the Ising over the XY to the Edwards-Anderson spin-glass model.
With the same random number sequence it produces exactly the same Markov chain
as a simulation with explicit summation of all terms in the Hamiltonian. Due to
its generality, its simplicity, and its reduced computational complexity it has
the potential to find broad application and thus lead to a deeper understanding
of the role of long-range interactions in the physics of lattice models,
especially in nonequilibrium settings.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [30] [Edge states in square lattice media and their deformations](https://arxiv.org/abs/2508.09352)
*Jonah Chaban,Jeremy L. Marzuola,Michael I. Weinstein*

Main category: math-ph

TL;DR: The paper studies edge states in 2D systems with periodic bulk media and linear deformations, focusing on their behavior near band gaps opened by magnetic perturbations. Effective edge Hamiltonians describe bifurcations, with analytical and numerical results provided.


<details>
  <summary>Details</summary>
Motivation: To understand edge states in systems with slowly interpolating periodic bulk media and their topological properties, particularly near band gaps induced by magnetic perturbations.

Method: Analysis of 2D edge Hamiltonians for square lattice symmetries and linear deformations, using multiple-scale analysis to derive effective edge Hamiltonians. Numerical simulations complement analytical results.

Result: Two distinct edge state curves traverse the band gap, consistent with bulk-edge correspondence. Bifurcations near degeneracies are governed by matrix Schrödinger (case 1) or Dirac operators (case 2).

Conclusion: The study confirms the bulk-edge correspondence and provides insights into edge state behavior near band gaps, with effective Hamiltonians accurately describing bifurcations.

Abstract: Edge states are time-harmonic solutions of conservative wave systems which
are plane wave-like parallel to and localized transverse to an interface
between two bulk media. We study a class of 2D edge Hamiltonians modeling a
medium which slowly interpolates between periodic bulk media via a domain wall
across a "rational" line defect. We consider the cases of (1) periodic bulk
media having the symmetries of a square lattice, and (2) linear deformations of
such media. Our bulk Hamiltonians break time-reversal symmetry due to
perturbation by a magnetic term, which opens a band gap about the band
structure degeneracies of the unperturbed bulk Hamiltonian. In case (1), these
are quadratic band degeneracies; in case (2), they are pairs of conical
degeneracies. We demonstrate that this band gap is traversed by two distinct
edge state curves, consistent with the bulk-edge correspondence principle of
topological physics. Blow-ups of these curves near the bulk band degeneracies
are described by effective (homogenized) edge Hamiltonians derived via
multiple-scale analysis which control the bifurcation of edge states. In case
(1), the bifurcation is governed by a matrix Schr\"{o}dinger operator; in case
(2), it is governed by a pair of Dirac operators. We present analytical results
and numerical simulations for both the full 2D edge Hamiltonian spectral
problem and the spectra of effective edge Hamiltonians.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [31] [GPU accelerated MHD in the DISPATCH framework using directive-based programming](https://arxiv.org/abs/2508.09568)
*Michael Haahr,Troels Haugbølle,Åke Nordlund,Sven Karlsson,Eloi Martaillé Richard*

Main category: astro-ph.IM

TL;DR: A GPU-accelerated MHD solver using OpenMP offloading achieved significant speedups (7.3x-9.8x) compared to CPU implementations, maintaining accuracy and portability.


<details>
  <summary>Details</summary>
Motivation: To improve performance of astrophysical codes by leveraging GPU acceleration while ensuring portability and accuracy.

Method: Implemented a GPU-accelerated MHD solver with OpenMP target offloading, grouping patches into "bunches" to reduce kernel launch overhead.

Result: Achieved 7.3x speedup on a mini-app and 9.8x in large-scale 3D tests, with core MHD updates being 100x faster on GPU.

Conclusion: OpenMP offloading provides substantial performance gains for astrophysical codes, and new codes should be structured for efficient GPU offloading.

Abstract: We present a GPU-accelerated implementation of a magnetohydrodynamic (MHD)
solver using directive-based programming with OpenMP target offloading. The
solver is integrated into the DISPATCH framework, which organises the
computational domain into a collection of asynchronously updated patches. To
reduce GPU kernel launch overhead, patches are grouped into "bunches" that are
updated collectively. While porting the particular solver required a complete
code refactoring, it yielded performance gains on both GPU and CPU. A
stand-alone mini-app achieved a 7.3x speed-up compared to a single NVIDIA A100
GPU to seven AMD 7F72 Rome CPU cores. Within the full DISPATCH framework, the
GPU-accelerated MHD Bunch solver showed excellent agreement with the CPU-based
reference implementation on standard test problems such as the Sod shock tube
and Orszag-Tang vortex. In large-scale 3D tests, the GPU implementation
achieved a 9.8x overall speedup, comparing one GPU to 12 CPU cores, with the
core MHD update routine being two orders of magnitude faster on the GPU than on
a single CPU core. These results demonstrate that OpenMP offloading can provide
substantial performance improvements for astrophysical codes while maintaining
portability and accuracy. The work also demonstrates how new codes should be
structured to allow simple and efficient directive-based GPU offloading.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [32] [Similarities in the Initiation of Upward Positive and Downward Negative Lightning Flashes](https://arxiv.org/abs/2508.09440)
*Toma Oregel-Chaumont,Mohammad Azadifar,Antonio Šunjerga,Marcos Rubinstein,Farhad Rachidi*

Main category: physics.ao-ph

TL;DR: The study compares upward positive lightning pulses and downward negative lightning PBPs, finding similarities and correlations, suggesting common mechanisms in lightning initiation.


<details>
  <summary>Details</summary>
Motivation: To understand the relationship between upward positive lightning pulses and downward negative lightning PBPs, and explore common physical mechanisms in lightning initiation.

Method: Analysis of simultaneous channel-base current and electric field data from the S\"{a}ntis tower, along with statistical comparisons and high-speed camera footage.

Result: Notable similarities between Category A/B upward pulses and Classical/Narrow PBPs, correlations in field-current parameters, and evidence of Category B pulses produced by recoil leaders.

Conclusion: Detailed observations of upward lightning provide insights into lightning initiation and propagation, revealing common mechanisms between upward and downward processes.

Abstract: This study examines the relationship between upward negative stepped leader
pulses in upward positive lightning and preliminary breakdown pulses (PBPs) in
downward negative lightning discharges. Through analysis of simultaneous
channel-base current and electric field data from the S\"{a}ntis tower in
Switzerland, we found notable similarities between Category A and B upward
positive lightning pulses and the "Classical" and "Narrow" PBPs observed in
downward negative flashes. Statistical comparisons reveal correlations between
electric field and current parameters for Category A pulses, supporting the
field-current relationship for preliminary breakdown proposed in previous
studies. These results suggest common physical mechanisms underlying both
upward and downward lightning initiation processes, providing valuable insights
into lightning initiation that could not be obtained from conventional field
measurements alone. Furthermore, high-speed camera footage revealed that
Category B pulses can be produced by a downward-propagating recoil leader. As a
whole, these findings demonstrate that detailed observations of upward
lightning can offer valuable insight into the complex processes underlying
lightning initiation and propagation.

</details>


<div id='math.MG'></div>

# math.MG [[Back]](#toc)

### [33] [Mixed Christoffel-Minkowski problems for bodies of revolution](https://arxiv.org/abs/2508.09794)
*Leo Brauner,Georg C. Hofstätter,Oscar Ortega-Moreno*

Main category: math.MG

TL;DR: The paper solves the mixed Christoffel-Minkowski problem for axially symmetric convex bodies, refining Firey's work and introducing new methods for transforming mixed area measures and volumes.


<details>
  <summary>Details</summary>
Motivation: To address the mixed Christoffel-Minkowski problem for Borel measures on the Euclidean unit sphere, especially for axially symmetric convex bodies, and refine existing classifications.

Method: Introduces a method to transform mixed area measures and volumes for axially symmetric bodies and improves Firey's estimates on local behavior of area measures.

Result: Provides a complete solution for the problem in the axially symmetric case, refining Firey's classification and yielding new Hadwiger-type theorems.

Conclusion: The work advances understanding of mixed area measures and volumes for symmetric convex bodies, with broader implications for convex valuations.

Abstract: The mixed Christoffel-Minkowski problem asks for necessary and sufficient
conditions for a Borel measure on the Euclidean unit sphere to be the mixed
area measure of some convex bodies, one of which, appearing multiple times, is
free and the rest are fixed. In the case where all bodies involved are
symmetric around a common axis, we provide a complete solution to this problem,
without assuming any regularity. In particular, we refine Firey's
classification of area measures of figures of revolution.
  In our argument, we introduce an easy way to transform mixed area measures
and mixed volumes involving axially symmetric bodies, and we significantly
improve Firey's estimate on the local behavior of area measures. As a secondary
result, we obtain a family of Hadwiger type theorems for convex valuations that
are invariant under rotations around an axis.

</details>


### [34] [The Christoffel problem for the disk area measure](https://arxiv.org/abs/2508.09800)
*Leo Brauner,Georg C. Hofstätter,Oscar Ortega-Moreno*

Main category: math.MG

TL;DR: The paper solves the mixed Christoffel problem for specific cases where fixed reference bodies are (n-1)-dimensional disks with parallel axes and the measure has no mass at the poles.


<details>
  <summary>Details</summary>
Motivation: To find necessary and sufficient conditions for a Borel measure on the Euclidean unit sphere to be the mixed area measure of certain convex bodies.

Method: Focuses on cases where fixed reference bodies are (n-1)-dimensional disks with parallel axes and the measure excludes mass at the poles.

Result: Provides a solution for the mixed Christoffel problem under the specified conditions.

Conclusion: The study successfully addresses the problem for the given constrained scenario.

Abstract: The mixed Christoffel problem asks for necessary and sufficient conditions
for a Borel measure on the Euclidean unit sphere to be the mixed area measure
of some convex bodies, all but one of them are fixed. We provide a solution in
the case where the fixed reference bodies are (n-1)-dimensional disks with
parallel axes and the measure has no mass at the poles of the sphere determined
by this axis.

</details>


<div id='astro-ph.EP'></div>

# astro-ph.EP [[Back]](#toc)

### [35] [Machine Learning for Exoplanet Detection: A Comparative Analysis Using Kepler Data](https://arxiv.org/abs/2508.09689)
*Reihaneh Karimi,Mahdiyar Mousavi-Sadr,Mohammad H. Zhoolideh Haghighi,Fatemeh S. Tabatabaei*

Main category: astro-ph.EP

TL;DR: A machine learning framework for exoplanet identification using Kepler data, with Random Forest achieving the highest accuracy (99.8%). SMOTE improves performance by addressing class imbalance.


<details>
  <summary>Details</summary>
Motivation: To enhance exoplanet discovery by leveraging machine learning for automated detection in large photometric datasets.

Method: Evaluated four supervised algorithms (Random Forest, KNN, Decision Tree, Logistic Regression) on Kepler data using metrics like accuracy, F1-score, and AUC-ROC. SMOTE was applied to address class imbalance.

Result: Random Forest outperformed others with 99.8% accuracy. KNN, Decision Tree, and Logistic Regression followed with 99.3%, 97.1%, and 95.8% accuracy, respectively. SMOTE improved all models.

Conclusion: Ensemble-based ML, especially Random Forest, is effective for exoplanet detection, with potential for ground-based observatories like INO.

Abstract: The discovery of exoplanets has expanded our understanding of planetary
systems and opened new avenues for astronomical research. In this study, we
present a machine learning (ML) framework for exoplanet identification using a
time-series photometric dataset from the Kepler Space Telescope, comprising
3,198 flux measurements across 5,074 stars. We investigate the performance of
four supervised classification algorithms, namely Random Forest, k-Nearest
Neighbors (KNN), Decision Tree, and Logistic Regression, using a comprehensive
set of evaluation metrics such as accuracy, precision, recall, F1-score, Area
Under the Receiver Operating Characteristic Curve (AUC-ROC), confusion
matrices, and learning curves. Among the models, Random Forest achieves the
highest accuracy (99.8\%) and near-perfect F1-scores, demonstrating superior
generalization and robustness. KNN also performs strongly, achieving 99.3\%
accuracy, while Decision Tree demonstrates moderate performance with 97.1\%
accuracy, and Logistic Regression trails behind with the lowest accuracy and
generalization at 95.8\%. Notably, the application of the Synthetic Minority
Over-sampling Technique (SMOTE) significantly improves performance across all
models by addressing class imbalance. These findings underscore the
effectiveness of ensemble-based machine learning techniques, particularly
Random Forest, in handling large volumes of photometric data for automated
exoplanet detection. This approach holds significant potential for
implementation at ground-based facilities, such as the Iranian National
Observatory (INO), where such extensive and precise datasets can further
advance exoplanet discovery and characterization efforts.

</details>


<div id='math.FA'></div>

# math.FA [[Back]](#toc)

### [36] [Doubling measures and Poincaré inequalities for sphericalizations of metric spaces](https://arxiv.org/abs/2508.09795)
*Anders Björn,Jana Björn,Xining Li*

Main category: math.FA

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The identification between the complex plane and the Riemann sphere preserves
holomorphic and harmonic functions and is a classical tool. In this paper we
consider a similar mapping from an unbounded metric space $X$ to a bounded
space and show how it preserves $p$-harmonic functions and Poincar\'e
inequalities. When $X$ is Ahlfors regular, this was shown in our earlier paper
(J. Math. Anal. Appl. 474 (2019), 852-875). Here we only require the much
weaker (and more natural) doubling property of the measure. Furthermore, we
consider a broader class of transformed measures. The sphericalization is then
applied to obtain new results for the Dirichlet boundary value problem in
unbounded sets and for boundary regularity at infinity for $p$-harmonic
functions. Some of these results are new also for unweighted $\mathbf{R}^n$, $n
\ge 2$ and $p\ne2$.

</details>


<div id='physics.atom-ph'></div>

# physics.atom-ph [[Back]](#toc)

### [37] [Laboratory Measurements of Ca XIX Dielectronic Recombination Satellites](https://arxiv.org/abs/2508.09975)
*Filipe Grilo,Marc Botz,Chintan Shah,Thomas Pfeifer,José R. Crespo López-Urrutia,Pedro Amaro*

Main category: physics.atom-ph

TL;DR: Measurements of Ca XIX Kα emission and satellite lines via dielectronic recombination (DR) with high precision, validating theoretical models and atomic data for astrophysical plasmas.


<details>
  <summary>Details</summary>
Motivation: To experimentally benchmark atomic data for Ca XIX, a key ion in high-temperature astrophysical plasmas, by comparing measurements with theoretical predictions and observations.

Method: Used a cryogenic electron beam ion trap to achieve 8 eV electron-energy resolution, measuring energies of exciting electrons and emitted photons with high precision (0.05% and 0.1% uncertainties). Compared results with Flexible Atomic Code (FAC) calculations and OPEN-ADAS database.

Result: Energies for KLL satellites agreed well with FAC predictions and prior calculations. DR coefficient rates matched OPEN-ADAS values. FAC synthetic spectra aligned with XRISM observations, validating DR satellite contributions.

Conclusion: The study successfully validated Ca XIX atomic data, demonstrating the importance of DR satellites in modeling astrophysical plasmas and confirming the reliability of FAC and OPEN-ADAS for such applications.

Abstract: We report measurements of the K$\alpha$ emission from the astrophysically
very abundant Ca XIX (He-like ion) and its satellite lines resonantly excited
by dielectronic recombination (DR). We achieve an electron-energy resolution of
8 eV in a cryogenic electron beam ion trap, and determine the energies of the
exciting electrons and the emitted photons up to the KLn ($n\le 8$) manifold
with $0.05\%$ and $0.1\%$ respective uncertainties. For the KLL satellites,
energies agree very well with our predictions using the Flexible Atomic Code
(FAC) and previous state-of-the-art calculations. Our calculations also agree
with our experimental direct excitation cross-sections for K$\alpha$ within
their $10\%$ uncertainty. We extract DR coefficient rates and find good
agreement with values tabulated in the OPEN-ADAS database. As an application,
we experimentally benchmark Ca XIX atomic data used to model high-temperature
astrophysical plasmas by comparing FAC synthetic spectra with recent XRISM
observations revealing the contributions of DR satellites to the Ca XIX lines.

</details>
