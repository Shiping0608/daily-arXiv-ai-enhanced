<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 12]
- [math.AP](#math.AP) [Total: 12]
- [physics.comp-ph](#physics.comp-ph) [Total: 4]
- [physics.bio-ph](#physics.bio-ph) [Total: 1]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]
- [cs.MS](#cs.MS) [Total: 1]
- [cs.DC](#cs.DC) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [math.DG](#math.DG) [Total: 2]
- [physics.chem-ph](#physics.chem-ph) [Total: 2]
- [math.PR](#math.PR) [Total: 1]
- [math.FA](#math.FA) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 2]
- [quant-ph](#quant-ph) [Total: 2]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 2]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 3]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [A tensor-based dynamic mode decomposition based on the $\star_{\boldsymbol{M}}$-product](https://arxiv.org/abs/2508.10126)
*Arvind K. Saibaba,Misha E. Kilmer,Khalil Hall-Hooper,Fan Tian,Alex Mize*

Main category: math.NA

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Dynamic mode decomposition (DMD) is a data-driven method for estimating the
dynamics of a discrete dynamical system. This paper proposes a tensor-based
approach to DMD for applications in which the states can be viewed as tensors.
Specifically, we use the $\star_{\boldsymbol{M}}$-product framework for tensor
decompositions which we demonstrate offers excellent compression compared to
matrix-based methods and can be implemented in a computationally efficient
manner. We show how the proposed approach is connected to the traditional DMD
and physics-informed DMD frameworks. We give a computational framework for
computing the tensor-based DMD and detail the computational costs. We also give
a randomized algorithm that enables efficient $\star_{\boldsymbol{M}}$-DMD
computations in the streaming setting. The numerical results show that the
proposed method achieves equal or better accuracy for the same storage compared
to the standard DMD on these examples and is more efficient to compute.

</details>


### [2] [A Generalized Alternating Anderson Acceleration Method](https://arxiv.org/abs/2508.10158)
*Yunhui He,Santolo Leveque*

Main category: math.NA

TL;DR: A generalized alternating Anderson acceleration method is proposed, combining fixed-point and Anderson iterations for solving linear and nonlinear problems, with proven convergence and improved efficiency over existing methods.


<details>
  <summary>Details</summary>
Motivation: To enhance flexibility and efficiency in solving linear and nonlinear problems by combining fixed-point and Anderson acceleration iterations.

Method: A periodic scheme alternating between fixed-point iteration steps and Anderson acceleration steps, with convergence analysis for specific parameter choices.

Result: Proven convergence under contractive conditions and demonstrated efficiency in accelerating various iterative methods like Jacobi and gradient descent.

Conclusion: The proposed method outperforms existing techniques in iteration count and CPU time for well-chosen parameters.

Abstract: In this work, we propose a generalized alternating Anderson acceleration
method, a periodic scheme composed of $t$ fixed-point iteration steps,
interleaved with $s$ steps of Anderson acceleration with window size $m$, to
solve linear and nonlinear problems. This allows flexibility to use different
combinations of fixed-point iteration and Anderson iteration. We present a
convergence analysis of the proposed scheme for accelerating the Richardson
iteration in the linear case, with a focus on specific parameter choices of
interest. Specifically, we prove convergence of the proposed method under
contractive fixed-point iteration and provide a sufficient condition for
convergence when the Richardson iteration matrix is diagonalizable and
noncontractive. To demonstrate the broader applicability of our proposed
method, we use it to accelerate Jacobi iteration, Picard iteration, gradient
descent, and the alternating direction method of multipliers in solving partial
differential equations and nonlinear, nonsmooth optimization problems. The
numerical results illustrate that the proposed scheme is more efficient than
the existing windowed Anderson acceleration and alternating Anderson ($s=1$) in
terms of iteration number and CPU time for careful choice of parameters $m, s,
t$.

</details>


### [3] [SSBE-PINN: A Sobolev Boundary Scheme Boosting Stability and Accuracy in Elliptic/Parabolic PDE Learning](https://arxiv.org/abs/2508.10322)
*Qixuan Zhou,Chuqi Chen,Tao Luo,Yang Xiang*

Main category: math.NA

TL;DR: The paper introduces Sobolev-Stable Boundary Enforcement (SSBE) to improve boundary loss in PINNs, ensuring better convergence in the H1 norm and robustness under finite-sample regimes.


<details>
  <summary>Details</summary>
Motivation: Standard PINNs often fail to achieve accurate convergence in the H1 norm due to boundary approximation errors, necessitating a more stable method.

Method: SSBE redefines boundary loss using Sobolev norms, incorporating boundary regularity directly into training, with theoretical guarantees for bounded H1 error.

Result: SSBE outperforms standard PINNs in relative L2 and H1 errors across various PDEs, including high-dimensional settings.

Conclusion: SSBE provides a principled and practical solution for enhancing gradient fidelity and solution accuracy in neural network-based PDE solvers.

Abstract: Physics-Informed Neural Networks (PINNs) have emerged as a powerful framework
for solving partial differential equations (PDEs), yet they often fail to
achieve accurate convergence in the H1 norm, especially in the presence of
boundary approximation errors. In this work, we propose a novel method called
Sobolev-Stable Boundary Enforcement (SSBE), which redefines the boundary loss
using Sobolev norms to incorporate boundary regularity directly into the
training process. We provide rigorous theoretical analysis demonstrating that
SSBE ensures bounded H1 error via a stability guarantee and derive
generalization bounds that characterize its robustness under finite-sample
regimes. Extensive numerical experiments on linear and nonlinear PDEs,
including Poisson, heat, and elliptic problems, show that SSBE consistently
outperforms standard PINNs in terms of both relative L2 and H1 errors, even in
high-dimensional settings. The proposed approach offers a principled and
practical solution for improving gradient fidelity and overall solution
accuracy in neural network based PDE solvers.

</details>


### [4] [A Semi-Lagrangian scheme on embedded manifolds using generalized local polynomial reproductions](https://arxiv.org/abs/2508.10344)
*Thomas Hangelbroek,Christian Rieger,Grady B. Wright*

Main category: math.NA

TL;DR: The paper analyzes uniform convergence rates for high-order semi-Lagrangian schemes on manifolds, introducing a novel mesh-free remapping operator for stability and accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of achieving high approximation orders and strong stability in solving PDEs on manifolds, particularly for advection equations on surfaces.

Method: Extends Falcone and Ferretti's error analysis, proposes a mesh-free remapping operator using ℓ1 minimizing generalized polynomial reproduction, and includes numerical experiments.

Result: Theoretical results are supported by numerical experiments, demonstrating the effectiveness of the proposed remapping operator.

Conclusion: The framework advances the numerical solution of PDEs and ODEs on manifolds, with potential for future research directions.

Abstract: We analyze rates of uniform convergence for a class of high-order
semi-Lagrangian schemes for first-order, time-dependent partial differential
equations on embedded submanifolds of $\mathbb{R}^d$ (including advection
equations on surfaces) by extending the error analysis of Falcone and Ferretti.
A central requirement in our analysis is a remapping operator that achieves
both high approximation orders and strong stability, a combination that is
challenging to obtain and of independent interest. For this task, we propose a
novel mesh-free remapping operator based on $\ell_1$ minimizing generalized
polynomial reproduction, which uses only point values and requires no
additional geometric information from the manifold (such as access to tangent
spaces or curvature). Our framework also rigorously addresses the numerical
solution of ordinary differential equations on manifolds via projection
methods. We include numerical experiments that support the theoretical results
and also suggest some new directions for future research.

</details>


### [5] [Product Of Exponentials (POE) Splines on Lie-Groups: Limitations, Extensions, and Application to SO(3) and SE(3)](https://arxiv.org/abs/2508.10513)
*Andreas Mueller*

Main category: math.NA

TL;DR: The paper introduces a new method for constructing splines on Lie groups using solutions of the Poisson equation, addressing limitations of existing methods that rely on local geodesics or polynomials starting at the identity. It provides algorithms for 3rd- and 4th-order splines and extends formulations for arbitrary initial conditions.


<details>
  <summary>Details</summary>
Motivation: Existing methods for spline construction on Lie groups rely on restrictive assumptions (e.g., starting at the identity) and may not accurately reflect the desired curve. This paper aims to overcome these limitations by leveraging the Poisson equation.

Method: Local curves are derived as solutions of the Poisson equation on the Lie group, ensuring boundary conditions and geometric fidelity. Closed-form expressions enable 3rd- and 4th-order spline algorithms, with extensions for arbitrary initial conditions.

Result: The proposed method produces higher-order splines that respect the group's geometry and boundary conditions. It outperforms existing methods in reconstructing motions, as demonstrated with examples like Cosserat rods.

Conclusion: The Poisson equation-based approach offers a flexible and accurate framework for spline construction on Lie groups, overcoming limitations of traditional methods and enabling applications like shape reconstruction.

Abstract: Existing methods for constructing splines and Bezier curves on a Lie group G
involve repeated products of exponentials deduced from local geodesics, w.r.t.
a Riemannian metric, or rely on general polynomials. Moreover, each of these
local curves is supposed to start at the identity of $G$. Both assumptions may
not reflect the actual curve to be interpolated. This paper pursues a different
approach to construct splines on $G$. Local curves are expressed as solutions
of the Poisson equation on G. Therewith, the local interpolations satisfies the
boundary conditions while respecting the geometry of $G$. A $k$th-order
approximation of the solutions gives rise to a $k$th-order product of
exponential (POE) spline. Algorithms for constructing 3rd- and 4th-order
splines are derived from closed form expressions for the approximate solutions.
Additionally, spline algorithms are introduced that allow prescribing a vector
field the curve must follow at the interpolation points. It is shown that the
established algorithms, where $k$th-order POE-splines are constructed by
concatenating local curves starting at the identity, cannot exactly reconstruct
a $k$th-order motion. To tackle this issue, the formulations are extended by
allowing for local curves between arbitrary points, rather than curves
emanating from the identity. This gives rise to a global $k$th-order spline
with arbitrary initial conditions. Several examples are presented, in
particular the shape reconstruction of slender rods modeled as geometrically
non-linear Cosserat rods.

</details>


### [6] [On The Eventual Periodicity of Fractional Order Dispersive Wave Equations Using RBFs and Transform](https://arxiv.org/abs/2508.10547)
*Hameed Ullah Jan,Marjan Uddin,Irshad Ali Shah,Salam Ullah Khan*

Main category: math.NA

TL;DR: The paper presents a numerical scheme combining radial basis functions finite difference (RBF-FD) with Laplace transform to solve fractional order dispersive wave equations, focusing on efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of solving high-order fractional and integer-type nonlinear PDEs efficiently, especially in complex domains, by leveraging local methods and avoiding time instability issues.

Method: Uses RBF-FD for spatial discretization and Laplace transform to handle time-fractional derivatives, avoiding time-stepping. Linearization and iterative schemes address nonlinear terms.

Result: Demonstrates high accuracy and efficiency in solving nonlinear fractional KdV and Burgers equations, with sparse matrices for computational reliability.

Conclusion: The proposed method is reliable, efficient, and scalable for large-scale problems, offering a robust alternative to traditional approaches.

Abstract: In this research work, let us focus on the construction of numerical scheme
based on radial basis functions finite difference (RBF-FD) method combined with
the Laplace transform for the solution of fractional order dispersive wave
equations. The numerical scheme is then applied to examine the eventual
periodicity of the proposed model subject to the periodic boundary conditions.
The implementation of proposed technique for high order fractional and integer
type nonlinear partial differential equations (PDEs) is beneficial because this
method is local in nature, therefore it yields and resulted in sparse
differentiation matrices instead of full and dense matrices. Only small
dimensions of linear systems of equations are to be solved for every center in
the domain and hence this procedure is more reliable and efficient to solve
large scale physical and engineering problems in complex domain. Laplace
transform is utilized for obtaining the equivalent time-independent equation in
Laplace space and also valuable to handle time-fractional derivatives in the
Caputo sense. Application of Laplace transform avoids the time steeping
procedure which commonly encounters the time instability issues. The solution
to the transformed model is then obtained by computing the inversion of Laplace
transform with an appropriate contour in a complex space, which is approximated
by trapezoidal rule with high accuracy. Also since the Laplace transform
operator is linear, it cannot be used to transform non-linear terms therefore
let us use a linearization approach and an appropriate iterative scheme. The
proposed approach is tasted for some nonlinear fractional order KdV and Burgers
equations. The capacity, high order accuracy and efficiency of our approach are
demonstrated using examples and results

</details>


### [7] [RBF-FD Method for Some Dispersive Wave Equations and Their Eventual Periodicity](https://arxiv.org/abs/2508.10558)
*Marjan Uddin,Hameed Ullah Jan,Muhammad Usman*

Main category: math.NA

TL;DR: The paper approximates solutions and analyzes eventual periodicity for dispersive wave equations with periodic forcing, using a local numerical scheme based on radial kernels and RK4 for time integration.


<details>
  <summary>Details</summary>
Motivation: To study the periodic behavior (eventual periodicity) of solutions for dispersive wave equations under periodic forcing on bounded domains.

Method: A numerical scheme using radial kernels (local like finite difference) and RK4 for temporal integration, leveraging sparse differentiation matrices.

Result: The scheme efficiently recovers solutions, validated against existing methods in literature.

Conclusion: The proposed method is effective for solving IBVPs of dispersive wave equations with periodic forcing.

Abstract: In this paper, we approximate the solution and also discuss the periodic
behavior termed as eventual periodicity of solutions of (IBVPs) for some
dispersive wave equations on a bounded domain corresponding to periodic
forcing. The constructed numerical scheme is based on radial kernels and local
in nature like finite difference method. The temporal variable is executed
through RK4 scheme. Due to the local nature and sparse differentiation matrices
our numerical scheme efficiently recovers the solution. The results achieved
are validated and examined with other methods accessible in the literature.

</details>


### [8] [CutVEM: Conforming virtual element method on embedded domains with shape-agnostic element agglomeration](https://arxiv.org/abs/2508.10570)
*Ramsharan Rangarajan,N. Sukumar*

Main category: math.NA

TL;DR: The paper introduces CutVEM, a novel element agglomeration algorithm for the virtual element method (VEM), improving robustness on polygonal meshes with cut cells while maintaining accuracy and optimal convergence.


<details>
  <summary>Details</summary>
Motivation: The VEM is robust on general polygonal meshes but suffers from poor conditioning in scenarios with cut cells due to embedded interfaces. This work aims to address this limitation.

Method: The proposed CutVEM uses an element agglomeration algorithm based on the element stability ratio, iteratively optimizing mesh stability without altering degrees of freedom.

Result: CutVEM significantly improves the condition numbers of stiffness matrices and retains the VEM's accuracy and convergence rates in numerical experiments.

Conclusion: CutVEM effectively addresses the conditioning issues of VEM on cut-cell meshes, making it a robust choice for simulations involving evolving geometries and embedded interfaces.

Abstract: The virtual element method (VEM) is a stabilized Galerkin method that is
robust and accurate on general polygonal meshes. This feature makes it an
appealing candidate for simulations involving meshes with embedded interfaces
and evolving geometries. However, the method can yield poorly conditioned
stiffness matrices in such scenarios due to meshes having cut cells. We propose
a novel element agglomeration algorithm for the virtual element method to
address this issue. The agglomeration algorithm renders the VEM robust over
planar polygonal meshes, particularly on finite element meshes cut by immersed
geometries. The algorithm relies on the element stability ratio, which we
define using the extreme eigenvalues of the element stiffness matrix. The
resulting element agglomeration criterion is free from nebulous polygon quality
metrics and is defined independently of polygon shapes. The algorithm proceeds
iteratively and element-wise to maximize the minimum element stability ratio,
even at the expense of degrading elements with better ratios. Crucially,
element agglomeration alters the number of elements, not the degree of freedom
count. The resulting method, which we label as CutVEM, retains node locations
of cut elements unchanged, and yields discretizations that conform to embedded
interfaces. This, in turn, facilitates straightforward imposition of boundary
conditions and interfacial constraints. Through detailed numerical experiments
that sample varied element-interface intersections, we demonstrate that CutVEM
enjoys dramatically improved condition numbers of global stiffness matrices
over the VEM. Furthermore, simulations of prototypical heat conduction problems
with Dirichlet and Neumann boundary conditions on domains with immersed
geometries show that element agglomeration does not noticeably degrade solution
accuracy and that CutVEM retains the VEM's optimal convergence rate.

</details>


### [9] [Efficient and Optimally Accurate Numerical Algorithms for Stochastic Turbulent Flow Problems](https://arxiv.org/abs/2508.10578)
*Brandiece N. Berry,Md Mahmudul Islam,Muhammad Mohebujjaman,Neethu Suma Raveendran*

Main category: math.NA

TL;DR: Proposed a filter-based EEV model for stochastic turbulent flows and a generic IMEX algorithm for efficient ensemble schemes, proving stability and accuracy for high Reynolds numbers.


<details>
  <summary>Details</summary>
Motivation: Address computational challenges in stochastic turbulent flow problems by reducing cost and memory requirements.

Method: Developed a linearized IMEX EEV algorithm with shared coefficient matrices and analyzed two stable, accurate time-stepping schemes.

Result: Proven stability and optimal convergence for 2D/3D problems; validated with high Reynolds number tests.

Conclusion: The schemes perform well for high Reynolds numbers, offering computational efficiency and accuracy.

Abstract: In this paper, we first propose a filter-based continuous Ensemble Eddy
Viscosity (EEV) model for stochastic turbulent flow problems. We then propose a
generic algorithm for a family of fully discrete, grad-div regularized,
efficient ensemble parameterized schemes for this model. The linearized
Implicit-Explicit (IMEX) EEV generic algorithm shares a common coefficient
matrix for each realization per time-step, but with different right-hand-side
vectors, which reduces the computational cost and memory requirements to the
order of solving deterministic flow problems. Two family members of the
proposed time-stepping algorithm are analyzed and proven to be stable. It is
found that one is first-order and the other is second-order accurate in time
for any stable finite element pairs. Avoiding the discrete inverse inequality,
the optimal convergence of both schemes is proven rigorously for both 2D and 3D
problems. For appropriately large grad-div parameters, both schemes are
unconditionally stable and allow weakly divergence-free elements. Several
numerical tests are given for high expected Reynolds number ($\textbf{E}[Re]$)
problems. The convergence rates are verified using manufactured solutions with
$\textbf{E}[Re]=10^{3},10^{4},\;\text{and}\; 10^{5}$. For various high
$\textbf{E}[Re]$, the schemes are implemented on benchmark problems which
includes: A 2D channel flow over a unit step problem, a non-intrusive
Stochastic Collocation Method (SCM) is used to examine the performance of the
schemes on a 2D Regularized Lid Driven Cavity (RLDC) problem, and a 3D RLDC
problem, and found them perform well.

</details>


### [10] [Nonlinear filtering based on density approximation and deep BSDE prediction](https://arxiv.org/abs/2508.10630)
*Kasper Bågmark,Adam Andersson,Stig Larsson*

Main category: math.NA

TL;DR: A new Bayesian filter using backward stochastic differential equations and neural networks is introduced, with offline training for online application. Theoretical and numerical results confirm its effectiveness.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient Bayesian filter leveraging deep learning and stochastic methods for real-time applications.

Method: Uses nonlinear Feynman-Kac representation and deep BSDE method with neural networks to approximate filtering density. Trained offline for online use.

Result: Proves a mixed error bound under elliptic conditions and confirms theoretical convergence in numerical tests.

Conclusion: The method is effective, with theoretical backing and practical validation.

Abstract: A novel approximate Bayesian filter based on backward stochastic differential
equations is introduced. It uses a nonlinear Feynman--Kac representation of the
filtering problem and the approximation of an unnormalized filtering density
using the well-known deep BSDE method and neural networks. The method is
trained offline, which means that it can be applied online with new
observations. A mixed a priori-a posteriori error bound is proved under an
elliptic condition. The theoretical convergence rate is confirmed in two
numerical examples.

</details>


### [11] [Isogeometric multi-patch shell analysis using the Geometry + Simulation Modules](https://arxiv.org/abs/2508.10648)
*Hugo M. Verhelst,Angelos Mantzaflaris,Matthias Möller*

Main category: math.NA

TL;DR: The paper details the implementation of Isogeometric Kirchhoff-Love shells in G+Smo, focusing on software design and compatibility for future extensions.


<details>
  <summary>Details</summary>
Motivation: To bridge CAD and FEA using splines, particularly for thin shell analysis, and to provide a versatile ecosystem for multi-patch shell problems.

Method: Implementation includes patch coupling via penalty methods, unstructured splines, goal-oriented error estimators, and advanced algorithms for wrinkling in hyperelastic membranes.

Result: Three new modules in G+Smo (Kirchhoff-Love shells, structural analysis, unstructured splines) enable fast, extendable solvers for shell modeling.

Conclusion: The modules enhance G+Smo's versatility for shell problems and are designed for future research extensions.

Abstract: Isogeometric Analysis (IGA) bridges Computer-Aided Design (CAD) and Finite
Element Analysis (FEA) by employing splines as a common basis for geometry and
analysis. One of the advantages of IGA is in the realm of thin shell analysis:
due to the arbitrary continuity of the spline basis, Kirchhoff-Love shells can
be modeled without the need to introduce unknowns for the mid-plane rotations,
leading to a reduction in the number of unknowns. In this paper, we provide the
background of an implementation of Isogeometric Kirchhoff--Love shells within
the Geometry + Simulation Modules (G+Smo). This paper accompanies multiple
previous publications and elaborates on the design of the software used in
these papers, rather than the novelty of the methods presented therein. The
presented implementation provides patch coupling via penalty methods and
unstructured splines, goal-oriented error estimators, several algorithms for
structural analysis and advanced algorithms for the modeling of wrinkling in
hyperelastic membranes. These methods are all contained in three new modules in
G+Smo: a module for Kirchhoff-Love shells, a module for structural analysis,
and a module for unstructured spline constructions. As motivated in this paper,
the modules are implemented to be compatible with future developments. For
example, by providing base implementations of material laws, by using black-box
functions for the structural analysis module, or by providing a standardized
approach for the implementation of unstructured spline constructions. Overall,
this paper demonstrates that the new modules contribute to a versatile
ecosystem for the modeling of multi-patch shell problems through fast
off-the-shelf solvers with a simple interface, designed to be extended in
future research.

</details>


### [12] [The Hu-Zhang element for linear elasticity on curved domains](https://arxiv.org/abs/2508.10674)
*Wei Chen,Xinyuan Du,Jun Hu*

Main category: math.NA

TL;DR: The paper extends the Hu-Zhang element to curved domains, ensuring symmetry and H(div)-conformity, and addresses stability and convergence issues with novel techniques.


<details>
  <summary>Details</summary>
Motivation: To adapt the Hu-Zhang element for curved domains while maintaining key properties like symmetry and H(div)-conformity, which is challenging due to non-polynomial structures.

Method: A novel inf-sup condition is introduced to analyze stability. Local p-enrichment is used to rectify suboptimal stress L^2-error convergence.

Result: Optimal convergence rates are achieved for most variables, though stress L^2-error remains suboptimal without enrichment. Numerical experiments support the theory.

Conclusion: The extended Hu-Zhang element effectively handles curved domains, with local enrichment resolving convergence issues, validated by numerical results.

Abstract: This paper extends the Hu-Zhang element for linear elasticity problems to
curved domains, preserving strong symmetry and H(div)-conformity. The
non-polynomial structure of the curved Hu-Zhang element makes it difficult to
analyze the stability result, which is overcome by establishing a novel inf-sup
condition. Optimal convergence rates are achieved for all variables except the
stress $L^2$-error. This suboptimality originates from the fact that the
divergence space of the curved Hu-Zhang element is not contained in the
discrete displacement space, which is rectified by local $p$-enrichment in the
Hu-Zhang space on curved boundary elements. Some numerical experiments validate
the theoretical results.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [13] [An inverse problem on a metric graph with cycle](https://arxiv.org/abs/2508.10121)
*Sergei Avdonin,Julian Edward*

Main category: math.AP

TL;DR: The paper analyzes a quantum graph with a ring and two edges, using given spectral data to determine edge lengths and potential functions.


<details>
  <summary>Details</summary>
Motivation: To solve the inverse problem of determining unknown geometric and potential properties of a quantum graph from partial spectral data.

Method: Uses eigenvalues and eigenfunction derivatives at specific vertices to reconstruct edge lengths and potential functions.

Result: Demonstrates that the given spectral data is sufficient to uniquely determine the edge lengths and potential functions.

Conclusion: The approach successfully solves the inverse problem for the described quantum graph configuration.

Abstract: Consider a quantum graph consisting of a ring with two attached edges, and
assume Kirchhoff-Neumann conditions hold at the internal vertices. Associated
to this graph is a Schr\"{o}dinger type operator $L=-\Delta +q(x)$ with
Dirichlet boundary conditions at the two boundary nodes. Let $\{ \omega_n^2, \
\varphi_n(x)\}$ be the eigenvalues and associated normalized eigenfunctions.
Let $v_1$ be a boundary vertex, and $v_2$ the adjacent internal vertex. Assume
we know the following data: $\{ \omega_n^2,\partial_x
\varphi_n(v_1),\partial_x\varphi_n(v_2)\}.$ Here $\partial_x\varphi_n(v_2)$
refers to an outward normal derivative at $v_2$ along one of the edges incident
to the other internal vertex. From this data we determine the following unknown
quantities: the lengths of edges and the potential functions on each edge.

</details>


### [14] [Non-Decaying Solutions to the 2D Dissipative Quasi-Geostrophic Equations](https://arxiv.org/abs/2508.10254)
*David M. Ambrose,Ryan Aschoff,Elaine Cozzi,James P. Kelliher*

Main category: math.AP

TL;DR: The paper studies the 2D surface quasi-geostrophic equation with subcritical diffusion, proving existence, uniqueness, and global solutions under specific conditions, including a maximum principle and extension to $L^{\infty}$ data.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of solving the surface quasi-geostrophic equation without decay or periodicity assumptions, especially for $L^{\infty}$ data where Riesz transforms are unbounded.

Method: Analyzes solutions under subcritical diffusion, uses smoothness assumptions for classical solutions, and employs a density argument for $L^{\infty}$ data. Introduces a relaxed model with generalized constitutive law.

Result: Existence of unique local solutions for $L^{\infty}$ data, global classical solutions for smooth data, and extension to $L^{\infty}$ data via density. Maximum principle holds.

Conclusion: The paper successfully extends solutions to broader data classes and introduces a relaxed model with similar properties, advancing understanding of the quasi-geostrophic equation.

Abstract: We consider the surface quasi-geostrophic equation in two spatial dimensions,
with subcritical diffusion (i.e. with fractional diffusion of order $2\alpha$
for $\alpha>\frac{1}{2}$.) We establish existence of solutions without assuming
either decay at spatial infinity or spatial periodicity. One obstacle is that
for $L^{\infty}$ data, the constitutive law may not be applicable, as Riesz
transforms are unbounded. However, for $L^{\infty}$ initial data for which the
constitutive law does converge, we demonstrate that there exists a unique
solution locally in time, and that the constitutive law continues to hold at
positive times. In the case that $\alpha\in(\frac{1}{2},1]$ and that the
initial data has some smoothness (specifically, if the data is in $C^{2}$), we
demonstrate a maximum principle and show that this unique solution is actually
classical and global in time. Then, a density argument allows us to show that
mild solutions with only $L^{\infty}$ data are also global in time, and also
possess this maximum principle. Finally, we introduce a related problem in
which we replace the usual constitutive law for the surface quasi-geostrophic
equation with a generalization of Sertfati type, and prove the same results for
this relaxed model.

</details>


### [15] [Stability of flat-core pinned p-elasticae](https://arxiv.org/abs/2508.10314)
*Tatsuya Miura,Kensuke Yoshizawa*

Main category: math.AP

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We classify the stability of flat-core $p$-elasticae in $\mathbf{R}^d$
subject to the pinned boundary condition. Together with previous work, this
completes the classification of stable pinned $p$-elasticae in $\mathbf{R}^d$
for all $p\in(1,\infty)$ and $d\geq2$.

</details>


### [16] [An Analysis of the Riemann Problem for a $2 \times 2$ System of Keyfitz-Kranzer Type Balance Laws With a Time-Dependent Source Term](https://arxiv.org/abs/2508.10347)
*Josh Culver,Aubrey Ayres,Evan Halloran,Ryan Lin,Emily Peng,Charis Tsikkou*

Main category: math.AP

TL;DR: The paper analyzes Riemann solutions for a system of conservation and balance laws with a time-dependent source term, focusing on non-classical delta shocks, vacuum states, and critical density thresholds.


<details>
  <summary>Details</summary>
Motivation: To understand transport processes under density constraints, applicable to biological aggregation, traffic congestion, and more.

Method: Comprehensive analysis of Riemann solutions, including non-self-similar structures, and numerical validation using the Local Lax-Friedrichs scheme.

Result: Discovery of diverse solution structures, including delta shocks and dynamic solutions due to the time-dependent source term.

Conclusion: The system serves as a versatile prototype for density-constrained transport, with theoretical insights confirmed numerically.

Abstract: We consider a system consisting of one conservation law and one balance law
with a time-dependent source term, and provide a comprehensive analysis of
Riemann solutions, including the non-classical overcompressive delta shocks.
The minimal yet representative structure of the system captures essential
features of transport under density constraints and, despite its simplicity,
serves as a versatile prototype for crowd-limited transport processes across
diverse contexts, including biological aggregation, ecological dispersal,
granular compaction, and traffic congestion. In addition to non-self-similar
solutions mentioned above, the associated Riemann problem admits solution
structures that traverse vacuum states ($\rho = 0$) and the critical density
threshold ($\rho = \bar{\rho}$), where mobility vanishes and characteristic
speed degenerates. Moreover, the explicit time dependence in the source term
leads to the breakdown of self-similarity, resulting in distinct Riemann
solutions over successive time intervals and highlighting the dynamic nature of
the solution landscape. The theoretical findings are numerically confirmed
using the Local Lax-Friedrichs scheme.

</details>


### [17] [Blow-up phenomena for a boundary Yamabe problem with umbilic boundary](https://arxiv.org/abs/2508.10387)
*Giusi Vaira*

Main category: math.AP

TL;DR: Existence of positive solutions for a linear perturbation of the scalar and boundary mean curvature problem in Riemannian manifolds with umbilic boundary, under non-zero Weyl tensor and negative scalar curvature for dimensions ≥8.


<details>
  <summary>Details</summary>
Motivation: To address the classical geometric problem of prescribing scalar and boundary mean curvature in a Riemannian manifold, extending it with a linear perturbation under specific conditions.

Method: Analyzes the problem in a Riemannian manifold with umbilic boundary, assuming non-zero Weyl tensor and negative scalar curvature, focusing on dimensions ≥8.

Result: Demonstrates the existence of positive solutions for the perturbed problem in dimensions ≥8.

Conclusion: The study confirms the feasibility of positive solutions under the given geometric constraints, particularly for higher dimensions.

Abstract: We consider a linear perturbation of the classical geometric problem of
prescribing the scalar and the boundary mean curvature problem in a Riemannian
manifold with umbilic boundary provided the Weyl tensor is non-zero everywhere.
We will deal with the case of negative scalar curvature showing the existence
of a positive solutions when $n\geq 8$.

</details>


### [18] [Regularity estimates on harmonic eigenmaps with arbitrary number of coordinates](https://arxiv.org/abs/2508.10448)
*Romain Petrides*

Main category: math.AP

TL;DR: The paper investigates the regularity of harmonic maps on surfaces, focusing on their dependence on the target manifold's dimension, particularly for ellipsoids.


<details>
  <summary>Details</summary>
Motivation: To understand the independence of regularity estimates of harmonic maps with respect to the target manifold's dimension and their connection to eigenvalue optimization.

Method: The study uses Laplace harmonic eigenmaps (harmonic maps into ellipsoids) and tools for handling convergence of almost critical metrics via Palais-Smale sequences.

Result: The gathered tools aid in analyzing convergence of almost critical metrics and could support a broader regularity theory for critical points of eigenvalue combinations.

Conclusion: The findings provide foundational insights for future research on regularity theory for critical points of infinite eigenvalue combinations.

Abstract: We revisit the well-established regularity estimates on harmonic maps on
surfaces to question their independence with respect to the dimension of the
target manifold. We are mainly interested in harmonic maps into target
ellipsoids, that we call Laplace harmonic eigenmaps. These maps are related to
critical metrics in the context of eigenvalue optimization. The tools that we
gather here are useful to handle convergence of almost critical metrics via
Palais-Smale sequences of (almost harmonic) eigenmaps. They could also be a
preliminary step for a general regularity theory for critical points of
infinite combinations of eigenvalues.

</details>


### [19] [On $\mathrm{BV}^{\mathbb{A}}$-Minimisers in two Dimensions](https://arxiv.org/abs/2508.10508)
*Ferdinand Eitler,Peter Lewintan*

Main category: math.AP

TL;DR: The paper examines the regularity of BV^𝔸-minimisers for ℂ-elliptic operators in 2D, leveraging their structure to establish gradient integrability.


<details>
  <summary>Details</summary>
Motivation: To understand the regularity properties of minimisers for ℂ-elliptic differential operators in two dimensions.

Method: Relies on the special structure of ℂ-elliptic operators and extends techniques from the symmetric gradient case.

Result: Gradient integrability is proven for the sharp ellipticity range.

Conclusion: The study confirms regularity for BV^𝔸-minimisers in the specified ellipticity range.

Abstract: We investigate into the regularity of $\mathrm{BV}^{\mathbb{A}}$-minimisers
for $\mathbb{C}$-elliptic differential operators $\mathbb{A}$ in $2$
dimensions. Our studies strongly rely on the special structure of such
differential operators. The gradient integrability is established for the sharp
ellipticity range known from the (symmetric) gradient case.

</details>


### [20] [Vectorial Double Phase Obstacle Problems](https://arxiv.org/abs/2508.10690)
*Filomena De Filippis,Antonella Nastasi,Cintia Pacchiano Camacho*

Main category: math.AP

TL;DR: Study of partial regularity for vector-valued minimizers of double phase functionals under vectorial obstacle constraints.


<details>
  <summary>Details</summary>
Motivation: To understand the regularity properties of minimizers in constrained optimization problems involving double phase functionals.

Method: Analysis of vector-valued local minimizers under topological constraints for double phase functionals.

Result: Partial regularity results are derived for the minimizers under the given constraints.

Conclusion: The study provides insights into the regularity of solutions in constrained double phase functional problems.

Abstract: We investigate partial regularity for vector valued local minimizers of
double phase functionals, under vectorial obstacle type constraints satisfying
appropriate topological properties.

</details>


### [21] [From Kac particles to the Landau equation with hard potentials: BBGKY hierarchy method](https://arxiv.org/abs/2508.10697)
*Shuchen Guo*

Main category: math.AP

TL;DR: The paper proves propagation of chaos for the Landau equation with hard potentials using a sharper Povzner-type inequality and coupling methods.


<details>
  <summary>Details</summary>
Motivation: To study the Kac particle model for the space-homogenous Landau equation and ensure uniform propagation of exponential moments.

Method: Uses a sharper Povzner-type inequality and coupling methods to analyze the many-particle Liouville equation.

Result: Uniform-in-time propagation of exponential moments and uniqueness of weak solutions for the Landau hierarchy.

Conclusion: Propagation of chaos is proven for the Landau equation with hard potentials.

Abstract: We study the Kac particle model for the space-homogenous Landau equation with
hard potentials. By showing a sharper Povzner-type inequality, we obtain the
uniform-in-time and uniform-in-N propagation of exponential moment for the
first marginal of the solution of the many-particle Liouville equation. This
key property enables us to show the uniqueness of weak solutions of the
corresponding infinite Landau hierarchy by coupling method. As a result, we
prove the propagation of chaos for the Landau equation with hard potentials.

</details>


### [22] [Well-posedness and relaxation in a simplified model for viscoelastic phase separation via Hilbertian gradients flows](https://arxiv.org/abs/2508.10722)
*Moritz Immanuel Gau,Katharina Hopf*

Main category: math.AP

TL;DR: The paper analyzes a gradient-flow approach to a Cahn-Hilliard model for viscoelastic phase separation, proving global well-posedness and stability, and studying asymptotic behavior under parameter scaling.


<details>
  <summary>Details</summary>
Motivation: To address the well-posedness and stability of a Cahn-Hilliard model for viscoelastic phase separation, particularly overcoming challenges like non-semiconvexity due to phase-dependent bulk modulus.

Method: Uses time-incremental minimization and generalized contractivity estimates to analyze the model.

Result: Establishes global well-posedness for moderately regular data, existence of gradient-flow solutions for finite-energy data, and derives stability estimates. Also recovers classical equations under specific parameter scalings.

Conclusion: The study successfully tackles the challenges of the model, providing insights into its behavior and connecting it to known equations under certain conditions.

Abstract: This article is concerned with a gradient-flow approach to a Cahn-Hilliard
model for viscoelastic phase separation introduced by Zhou et al. (Phys. Rev.
E, 2006) in its variant with constant mobility. By means of time-incremental
minimisation and generalised contractivity estimates, we establish the global
well-posedness of the Cauchy problem for moderately regular initial data. For
general finite-energy data we obtain the existence of gradient-flow solutions
and a stability estimate of weak-strong type. We further study the asymptotic
behaviour for relaxation time and bulk modulus depending on a small parameter.
Depending on the scaling, we recover the Cahn-Hilliard, the mass-conserving
Allen-Cahn or the viscous Cahn-Hilliard equation. A challenge in the
well-posedness analysis is the failure of semiconvexity of the appropriate
driving functional, which is caused by a phase-dependence of the bulk modulus.

</details>


### [23] [$\mathrm{C}^2$ estimates for general $p$-Hessian equations on closed Riemannian manifolds](https://arxiv.org/abs/2508.10773)
*Yuxiang Qiao*

Main category: math.AP

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study the $\mathrm{C}^2$ estimates for $p$-Hessian equations with general
left-hand and right-hand terms on closed Riemannian manifolds of dimension $n$.
To overcome the constraints of closed manifolds, we advance a new kind of
"subsolution", called pseudo-solution, which generalizes
"$\mathcal{C}$-subsolution" to some extent and is well-defined for fully
general $p$-Hessian equations. Based on pseudo-solutions, we prove the
$\mathrm{C}^0$ estimates, first-order estimates for general $p$-Hessian
equations, and the corresponding second-order estimates when $p\in\{2, n-1,
n\}$, under sharp conditions -- we don't impose curvature restrictions,
convexity conditions or "MTW condition" on our main results. Some other
conclusions related to a priori estimates and different kinds of "subsolutions"
are also given, including estimates for "semi-convex" solutions and when there
exists a pseudo-solution.

</details>


### [24] [Upper bound on heat kernels of finite particle systems of Keller-Segel type](https://arxiv.org/abs/2508.10892)
*S. E. Boutiah,D. Kinzebulatov*

Main category: math.AP

TL;DR: Upper bound on the heat kernel for Keller-Segel finite particle system, showing blow-up effects.


<details>
  <summary>Details</summary>
Motivation: Understand the critical behavior of the Keller-Segel system, particularly in two dimensions.

Method: Connects Keller-Segel finite particles to non-local operators to analyze the system.

Result: Derives an upper bound on the heat kernel, highlighting blow-up effects.

Conclusion: The approach provides insights into the critical behavior of the Keller-Segel system in two dimensions.

Abstract: We obtain an upper bound on the heat kernel of the Keller-Segel finite
particle system that exhibits blow up effects. The proof exploits a connection
between Keller-Segel finite particles and certain non-local operators. The
latter allows to address some aspects of the critical behaviour of the
Keller-Segel system resulting from its two-dimensionality.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [25] [MCP-Enabled LLM for Meta-optics Inverse Design: Leveraging Differentiable Solver without LLM Expertise](https://arxiv.org/abs/2508.10277)
*Yi Huang,Bowen Zheng,Yunxi Dong,Hong Tang,Huan Zhao,Rakibul Hasan Shawon,Sensong An,Hualiang Zhang*

Main category: physics.comp-ph

TL;DR: A framework using Model Context Protocol (MCP) and LLMs simplifies inverse design for metasurfaces by providing verified code templates and documentation, outperforming natural language prompting in efficiency and quality.


<details>
  <summary>Details</summary>
Motivation: To make automatic differentiation (AD) and inverse design accessible to researchers without extensive programming or theoretical expertise.

Method: Proposes an MCP-assisted framework where LLMs dynamically access verified code templates and documentation to generate inverse design codes, evaluated using the TorchRDIT solver.

Result: Structured prompting outperforms natural language in design quality, efficiency, computational cost, and error reduction.

Conclusion: MCP enables accessible and efficient inverse design, generalizable to other scientific tasks with minimal server requirements.

Abstract: Automatic differentiation (AD) enables powerful metasurface inverse design
but requires extensive theoretical and programming expertise. We present a
Model Context Protocol (MCP) assisted framework that allows researchers to
conduct inverse design with differentiable solvers through large language
models (LLMs). Since LLMs inherently lack knowledge of specialized solvers, our
proposed solution provides dynamic access to verified code templates and
comprehensive documentation through dedicated servers. The LLM autonomously
accesses these resources to generate complete inverse design codes without
prescribed coordination rules. Evaluation on the Huygens meta-atom design task
with the differentiable TorchRDIT solver shows that while both natural language
and structured prompting strategies achieve high success rates, structured
prompting significantly outperforms in design quality, workflow efficiency,
computational cost, and error reduction. The minimalist server design, using
only 5 APIs, demonstrates how MCP makes sophisticated computational tools
accessible to researchers without programming expertise, offering a
generalizable integration solution for other scientific tasks.

</details>


### [26] [Sum-of-Gaussians tensor neural networks for high-dimensional Schrödinger equation](https://arxiv.org/abs/2508.10454)
*Qi Zhou,Teng Wu,Jianghao Liu,Qingyuan Sun,Hehu Xie,Zhenli Xu*

Main category: physics.comp-ph

TL;DR: The paper introduces SOG-TNN, a sum-of-Gaussians tensor neural network for solving high-dimensional Schrödinger equations efficiently, overcoming dimensionality challenges with low-rank tensor representations and SOG decomposition.


<details>
  <summary>Details</summary>
Motivation: High-dimensional Schrödinger equations are computationally challenging due to the curse of dimensionality and singular Coulomb interactions. The goal is to develop an efficient, accurate, and low-memory method.

Method: SOG-TNN uses low-rank tensor product representations and SOG decomposition for Coulomb interactions. A range-splitting scheme divides Gaussian terms into short-, long-, and mid-range components, each treated with specialized techniques (asymptotic expansion, Chebyshev expansion, SVD).

Result: Numerical results show SOG-TNN performs outstandingly, efficiently handling high-dimensional problems and resolving Coulomb interaction challenges.

Conclusion: SOG-TNN is a promising method for large, complex quantum systems, offering accuracy, efficiency, and reduced computational costs.

Abstract: We propose an accurate, efficient, and low-memory sum-of-Gaussians tensor
neural network (SOG-TNN) algorithm for solving the high-dimensional
Schr\"odinger equation. The SOG-TNN utilizes a low-rank tensor product
representation of the solution to overcome the curse of dimensionality
associated with high-dimensional integration. To handle the Coulomb
interaction, we introduce an SOG decomposition to approximate the interaction
kernel such that it is dimensionally separable, leading to a tensor
representation with rapid convergence. We further develop a range-splitting
scheme that partitions the Gaussian terms into short-, long-, and mid-range
components. They are treated with the asymptotic expansion, the low-rank
Chebyshev expansion, and the model reduction with singular-value decomposition,
respectively, significantly reducing the number of two-dimensional integrals in
computing electron-electron interactions. The SOG decomposition well resolves
the computational challenge due to the singularity of the Coulomb interaction,
leading to an efficient algorithm for the high-dimensional problem under the
TNN framework. Numerical results demonstrate the outstanding performance of the
new method, revealing that the SOG-TNN is a promising way for tackling large
and complex quantum systems.

</details>


### [27] [Virtual Sensing for Solder Layer Degradation and Temperature Monitoring in IGBT Modules](https://arxiv.org/abs/2508.10515)
*Andrea Urgolo,Monika Stipsitz,Helios Sanchis-Alepuz*

Main category: physics.comp-ph

TL;DR: The paper proposes a machine learning-based virtual sensing method to estimate IGBT module degradation and temperature maps using limited physical sensors, achieving high accuracy.


<details>
  <summary>Details</summary>
Motivation: Direct measurement of IGBT degradation indicators is challenging due to physical inaccessibility and harsh environments, necessitating alternative methods.

Method: Uses synthetic data and machine learning to estimate solder layer degradation and temperature maps from limited sensor inputs.

Result: Achieves 1.17% mean absolute error in degraded solder area estimation and 4.56% maximum relative error in temperature reproduction.

Conclusion: Machine learning-based virtual sensing is feasible for accurate IGBT degradation monitoring, offering a practical solution for inaccessible indicators.

Abstract: Monitoring the degradation state of Insulated Gate Bipolar Transistor (IGBT)
modules is essential for ensuring the reliability and longevity of power
electronic systems, especially in safety-critical and high-performance
applications. However, direct measurement of key degradation indicators - such
as junction temperature, solder fatigue or delamination - remains challenging
due to the physical inaccessibility of internal components and the harsh
environment. In this context, machine learning-based virtual sensing offers a
promising alternative by bridging the gap from feasible sensor placement to the
relevant but inaccessible locations. This paper explores the feasibility of
estimating the degradation state of solder layers, and the corresponding full
temperature maps based on a limited number of physical sensors. Based on
synthetic data of a specific degradation mode, we obtain a high accuracy in the
estimation of the degraded solder area (1.17% mean absolute error), and are
able to reproduce the surface temperature of the IGBT with a maximum relative
error of 4.56% (corresponding to an average relative error of 0.37%).

</details>


### [28] [Physics-Informed Deep Contrast Source Inversion: A Unified Framework for Inverse Scattering Problems](https://arxiv.org/abs/2508.10555)
*Haoran Sun,Daoqi Liu,Hongyu Zhou,Maokun Li,Shenheng Xu,Fan Yang*

Main category: physics.comp-ph

TL;DR: DeepCSI is a physics-informed deep learning framework for fast and accurate inverse scattering problem solving, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of nonlinearity and diverse measurement scenarios in inverse scattering problems for electromagnetic imaging and medical diagnostics.

Method: Uses a residual multilayer perceptron (ResMLP) to model current distributions, linearizing the problem. Combines state equation loss, data equation loss, and total variation regularization in a hybrid loss function.

Result: Achieves high-precision, robust reconstruction under various conditions (full-data, phaseless, multi-frequency), outperforming traditional CSI methods.

Conclusion: DeepCSI provides an efficient, universal solution for complex inverse scattering problems, simplifying and improving accuracy.

Abstract: Inverse scattering problems are critical in electromagnetic imaging and
medical diagnostics but are challenged by their nonlinearity and diverse
measurement scenarios. This paper proposes a physics-informed deep contrast
source inversion framework (DeepCSI) for fast and accurate medium
reconstruction across various measurement conditions. Inspired by contrast
source inversion (CSI) and neural operator methods, a residual multilayer
perceptron (ResMLP) is employed to model current distributions in the region of
interest under different transmitter excitations, effectively linearizing the
nonlinear inverse scattering problem and significantly reducing the
computational cost of traditional full-waveform inversion. By modeling medium
parameters as learnable tensors and utilizing a hybrid loss function that
integrates state equation loss, data equation loss, and total variation
regularization, DeepCSI establishes a fully differentiable framework for joint
optimization of network parameters and medium properties. Compared with
conventional methods, DeepCSI offers advantages in terms of simplicity and
universal modeling capabilities for diverse measurement scenarios, including
phase-less and multi-frequency observation. Simulations and experiments
demonstrate that DeepCSI achieves high-precision, robust reconstruction under
full-data, phaseless data, and multifrequency conditions, outperforming
traditional CSI methods and providing an efficient and universal solution for
complex inverse scattering problems.

</details>


<div id='physics.bio-ph'></div>

# physics.bio-ph [[Back]](#toc)

### [29] [Run-and-Tumble Escape in Pursuit-Evasion Dynamics of Intelligent Active Particles](https://arxiv.org/abs/2508.10727)
*Segun Goh,Dennis Haustein,Gerhard Gompper*

Main category: physics.bio-ph

TL;DR: The paper studies a pursuit-evasion game between a deterministic pursuer and a stochastic evader, analyzing strategies and outcomes in 2D simulations.


<details>
  <summary>Details</summary>
Motivation: To understand and model adversarial interactions between agile agents, with applications in bioinspired robotics.

Method: Numerical simulations of a pursuer-evader pair, with the evader's tumbling frequency and direction varying based on threat level.

Result: Two key scenarios emerge: high-risk backward maneuvers for dominant pursuers and forward tumbling to prolong capture time.

Conclusion: The findings can inform the design of robotic systems with effective evasion strategies.

Abstract: The pursuit-evasion game is studied for two adversarial active agents,
modelled as a deterministic self-steering pursuer and a stochastic, cognitive
evader. The pursuer chases the evader by reorienting its propulsion direction
with limited maneuverability, while the evader escapes by executing sharp,
unpredictable turns, whose timing and direction the pursuer cannot anticipate.
To make the target responsive and agile when the threat level is high, the
tumbling frequency is set to increase with decreasing distance from the
pursuer; furthermore, the range of preferred tumbling directions is varied.
Numerical simulations of such a pursuit-target pair in two spatial dimensions
reveal two important scenarios. For dominant pursuers, the evader is compelled
to adopt a high-risk strategy that allows the pursuer to approach closely
before the evader executes a potentially game-changing backward maneuver to
pull away from the pursuer. Otherwise, a strategy where the evader tumbles
forward with continuous slight adjustments of the propulsion direction can
significantly increase the capture time by preventing the pursuer from aligning
with the target propulsion direction, while maintaining the persistence of the
target motion. Our results can guide the design of bioinspired robotic systems
with efficient evasion capabilities.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [30] [Effective permeability conditions for diffusive transport through impermeable membranes with gaps](https://arxiv.org/abs/2508.10694)
*Molly Brennan,Edwina F. Yeo,Philip Pearce,Mohit P. Dalwadi*

Main category: cond-mat.soft

TL;DR: The paper uses multiscale analysis to derive effective coupling conditions for transport across membranes with periodic gaps, validating results with simulations. It generalizes classic membrane conditions and predicts bacterial membrane permeability dominated by thickness.


<details>
  <summary>Details</summary>
Motivation: To understand how microscale geometry affects macroscale transport across membranes, particularly in bacterial membranes with periodic gaps.

Method: Formal multiscale analysis and numerical simulations to derive and validate effective macroscale coupling conditions.

Result: Analytic expressions for membrane permeability in terms of microscale geometry, revealing a memory property for time-varying concentrations. Bacterial membrane permeability is primarily influenced by thickness.

Conclusion: The derived conditions apply broadly, from bacterial membranes to industrial filtration, predicting how microstructure changes affect transport, especially under time-varying conditions.

Abstract: Membranes regulate transport in a wide variety of industrial and biological
applications. The microscale geometry of the membrane can significantly affect
overall transport through the membrane, but the precise nature of this
multiscale coupling is not well characterised in general. Motivated by the
application of transport across a bacterial membrane, in this paper we use
formal multiscale analysis to derive explicit effective coupling conditions for
macroscale transport across a two-dimensional impermeable membrane with
periodically spaced gaps, and validate these with numerical simulations. We
derive analytic expressions for effective macroscale quantities associated with
the membrane, such as the permeability, in terms of the microscale geometry.
Our results generalise the classic constitutive membrane coupling conditions to
a wider range of membrane geometries and time-varying scenarios. Specifically,
we demonstrate that if the exterior concentration varies in time, for membranes
with long channels, the transport gains a memory property where the coupling
conditions depend on the system history. By applying our effective conditions
in the context of small molecule transport through gaps in bacterial membranes
called porins, we predict that bacterial membrane permeability is primarily
dominated by the thickness of the membrane. Furthermore, we predict how
alterations to membrane microstructure, for example via changes to porin
expression, might affect overall transport, including when external
concentrations vary in time. These results will apply to a broad range of
physical applications with similar membrane structures, from medical and
industrial filtration to carbon capture.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [31] [Self-Supervised Temporal Super-Resolution of Energy Data using Generative Adversarial Transformer](https://arxiv.org/abs/2508.10587)
*Xuanhao Mu,Gökhan Demirel,Yuzhe Zhang,Jianlei Liu,Thorsten Schlachter,Veit Hagenmeyer*

Main category: cs.LG

TL;DR: The paper introduces a Generative Adversarial Transformers (GATs) method for upsampling energy network time series, reducing RMSE by 9% and improving MPC accuracy by 13%, without needing high-resolution training data.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of conventional upsampling methods and advanced models (like time series generation, Super-Resolution, and imputation models) in energy network design, which suffer from information loss, noise, or reliance on unavailable high-resolution data.

Method: Proposes a Generative Adversarial Transformers (GATs) approach that learns to generate high-resolution time series without requiring ground-truth high-resolution data.

Result: The method reduces RMSE by 9% and improves MPC accuracy by 13% compared to conventional interpolation.

Conclusion: GATs offer a promising solution for upsampling in energy networks, overcoming the limitations of existing methods by eliminating the need for high-resolution training data.

Abstract: To bridge the temporal granularity gap in energy network design and operation
based on Energy System Models, resampling of time series is required. While
conventional upsampling methods are computationally efficient, they often
result in significant information loss or increased noise. Advanced models such
as time series generation models, Super-Resolution models and imputation models
show potential, but also face fundamental challenges. The goal of time series
generative models is to learn the distribution of the original data to generate
high-resolution series with similar statistical characteristics. This is not
entirely consistent with the definition of upsampling. Time series
Super-Resolution models or imputation models can degrade the accuracy of
upsampling because the input low-resolution time series are sparse and may have
insufficient context. Moreover, such models usually rely on supervised learning
paradigms. This presents a fundamental application paradox: their training
requires the high-resolution time series that is intrinsically absent in
upsampling application scenarios. To address the mentioned upsampling issue,
this paper introduces a new method utilizing Generative Adversarial
Transformers (GATs), which can be trained without access to any ground-truth
high-resolution data. Compared with conventional interpolation methods, the
introduced method can reduce the root mean square error (RMSE) of upsampling
tasks by 9%, and the accuracy of a model predictive control (MPC) application
scenario is improved by 13%.

</details>


<div id='cs.MS'></div>

# cs.MS [[Back]](#toc)

### [32] [Concepts for Composing Finite Element Function Space Bases](https://arxiv.org/abs/2508.10125)
*Christian Engwer,Carsten Gräser,Steffen Müthing,Simon Praetorius,Oliver Sander*

Main category: cs.MS

TL;DR: The paper discusses software concepts for handling composed function spaces in finite element discretizations, using tree-based representations to adapt to various data layouts and linear algebra codes.


<details>
  <summary>Details</summary>
Motivation: To address the need for flexible handling of composed function spaces in multi-physics PDE models, enabling compatibility with diverse data structures and solvers.

Method: Representation of product spaces as trees of simpler bases, allowing derivation of multi-indexed degrees of freedom for adaptable data layouts.

Result: Demonstrates the approach with the stationary Stokes equation using Taylor-Hood elements and implements it in the DUNE module dune-functions.

Conclusion: The proposed abstractions enable flexible and efficient handling of function spaces, facilitating integration with various linear algebra tools and solvers.

Abstract: Finite Element discretizations of coupled multi-physics partial differential
equation models require the handling of composed function spaces. In this paper
we discuss software concepts and abstractions to handle the composition of
function spaces, based on a representation of product spaces as trees of
simpler bases. From this description, many different numberings of degrees of
freedom by multi-indices can be derived in a natural way, allowing to adapt the
function spaces to very different data layouts, so that it opens the
possibility to directly use the finite element code with very different linear
algebra codes, different data structures, and different algebraic solvers.
  A recurring example throughout the paper is the stationary Stokes equation
with Taylor--Hood elements as these are naturally formulated as product spaces
and highlight why different storage patterns are desirable.
  In the second half of the paper we discuss a particular realization of most
of these concepts in the \dunemodule{dune-functions} module, as part of the
DUNE ecosystem.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [33] [Mixed-Precision Performance Portability of FFT-Based GPU-Accelerated Algorithms for Block-Triangular Toeplitz Matrices](https://arxiv.org/abs/2508.10202)
*Sreeram Venkat,Kasia Swirydowicz,Noah Wolfe,Omar Ghattas*

Main category: cs.DC

TL;DR: A framework using Hipify enables performance portability for FFTMatvec, allowing it to run on AMD GPUs. A dynamic mixed-precision approach optimizes performance for desired error tolerance, scaling to 2,048 GPUs on Frontier.


<details>
  <summary>Details</summary>
Motivation: Hardware diversity and GPU performance gains in lower precision motivate adoption of mixed-precision algorithms and performance portability in HPC workflows.

Method: Uses Hipify for porting CUDA-based FFTMatvec to AMD GPUs, integrates optimizations into rocBLAS, and employs a dynamic mixed-precision framework with Pareto front analysis.

Result: Achieves seamless performance on AMD GPUs (MI250X, MI300X, MI355X) and scales to 2,048 GPUs on Frontier.

Conclusion: The framework successfully combines performance portability and mixed-precision optimization for HPC applications.

Abstract: The hardware diversity displayed in leadership-class computing facilities,
alongside the immense performance boosts exhibited by today's GPUs when
computing in lower precision, provide a strong incentive for scientific HPC
workflows to adopt mixed-precision algorithms and performance portability
models. We present an on-the-fly framework using Hipify for performance
portability and apply it to FFTMatvec-an HPC application that computes
matrix-vector products with block-triangular Toeplitz matrices. Our approach
enables FFTMatvec, initially a CUDA-only application, to run seamlessly on AMD
GPUs with excellent observed performance. Performance optimizations for AMD
GPUs are integrated directly into the open-source rocBLAS library, keeping the
application code unchanged. We then present a dynamic mixed-precision framework
for FFTMatvec; a Pareto front analysis determines the optimal mixed-precision
configuration for a desired error tolerance. Results are shown for AMD Instinct
MI250X, MI300X, and the newly launched MI355X GPUs. The performance-portable,
mixed-precision FFTMatvec is scaled to 2,048 GPUs on the OLCF Frontier
supercomputer.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [34] [TOBACO: Topology Optimization via Band-limited Coordinate Networks for Compositionally Graded Alloys](https://arxiv.org/abs/2508.10320)
*Aaditya Chandrasekhar,Stefan Knapik,Deepak Sharma,John Reidy,Ian McCue,Jian Cao,Wei Chen*

Main category: cs.CE

TL;DR: A topology optimization framework using neural networks to design Compositionally Graded Alloys (CGAs) with controlled gradation, leveraging additive manufacturing advancements.


<details>
  <summary>Details</summary>
Motivation: CGAs offer superior material properties and cost-effectiveness, but their fabrication via additive manufacturing requires managing compositional gradation limits.

Method: A band-limited coordinate neural network represents composition distribution, ensuring implicit compliance with gradation constraints while benefiting from mesh independence and high-resolution design.

Result: Demonstrated effectiveness in elastic and thermo-elastic topology optimization examples.

Conclusion: The framework successfully designs optimized CGA components with controlled gradation, addressing manufacturing constraints.

Abstract: Compositionally Graded Alloys (CGAs) offer unprecedented design flexibility
by enabling spatial variations in composition; tailoring material properties to
local loading conditions. This flexibility leads to components that are
stronger, lighter, and more cost-effective than traditional monolithic
counterparts. The fabrication of CGAs have become increasingly feasible owing
to recent advancements in additive manufacturing (AM), particularly in
multi-material printing and improved precision in material deposition. However,
AM of CGAs requires imposition of manufacturing constraints; in particular
limits on the maximum spatial gradation of composition.
  This paper introduces a topology optimization (TO) based framework for
designing optimized CGA components with controlled compositional gradation. In
particular, we represent the constrained composition distribution using a
band-limited coordinate neural network. By regulating the network's bandwidth,
we ensure implicit compliance with gradation limits, eliminating the need for
explicit constraints. The proposed approach also benefits from the inherent
advantages of TO using coordinate networks, including mesh independence,
high-resolution design extraction, and end-to-end differentiability. The
effectiveness of our framework is demonstrated through various elastic and
thermo-elastic TO examples.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [35] [Intrinsic and Normal Mean Ricci Curvatures: A Bochner--Weitzenboeck Identity for Simple d-Vectors](https://arxiv.org/abs/2508.10306)
*Pawel Gajer,Jacques Ravel*

Main category: math.DG

TL;DR: The paper introduces two pointwise subspace averages of sectional curvature and derives a Bochner--Weitzenboeck identity, leading to applications in harmonic simple d-vectors and eigenvalue bounds.


<details>
  <summary>Details</summary>
Motivation: To generalize curvature averages and derive new geometric identities with applications in differential geometry.

Method: Uses Jacobi-field expansions to define intrinsic and normal mean Ricci curvatures and applies them to derive a Bochner--Weitzenboeck identity.

Result: A curvature term equals d(n-d) times the normal mean Ricci, enabling vanishing criteria and eigenvalue bounds.

Conclusion: The introduced curvature averages provide new tools for geometric analysis, with applications in harmonic forms and spectral geometry.

Abstract: We introduce two pointwise subspace averages of sectional curvature on a
d-dimensional plane Pi in T_p M: (i) the intrinsic mean Ricci (the average of
sectional curvatures of 2-planes contained in Pi); and (ii) the normal (mixed)
mean Ricci (the average of sectional curvatures of 2-planes spanned by one
vector in Pi and one in Pi^perp). Using Jacobi-field expansions, these means
occur as the r^2/6 coefficients in the intrinsic (d-1)-sphere and normal
(n-d-1)-sphere volume elements. A direct consequence is a Bochner--Weitzenboeck
identity for simple d-vectors V (built from an orthonormal frame X_1,...,X_d
with Pi = span{X_i}): the curvature term equals d(n-d) times the normal mean
Ricci of Pi. This yields two immediate applications: (a) a Bochner vanishing
criterion for harmonic simple d-vectors under a positive lower bound on the
normal mean Ricci; and (b) a Lichnerowicz-type lower bound for the first
eigenvalue of the Hodge Laplacian on simple d-eigenfields.

</details>


### [36] [Isoperimetric inequalities involving Steklov eigenvalues on surfaces](https://arxiv.org/abs/2508.10721)
*Romain Petrides*

Main category: math.DG

TL;DR: The paper explores optimal constants for isoperimetric inequalities involving Steklov eigenvalues on surfaces with boundary, focusing on Riemannian surfaces with shared topology or conformal class. It provides new examples of optimal topological disks and proves inequalities linking conformal invariants of Steklov eigenvalues on surfaces and disks. The appendix addresses rigidity of the first conformal Steklov eigenvalue on annuli and Möbius bands.


<details>
  <summary>Details</summary>
Motivation: To understand and characterize optimal constants in isoperimetric inequalities related to Steklov eigenvalues, particularly for surfaces with boundary, and to explore their behavior under topological and conformal constraints.

Method: The study involves analyzing Riemannian surfaces with given topology or conformal class, deriving inequalities, and providing explicit examples (topological disks) to illustrate optimal constants. The appendix uses rigidity analysis for specific surfaces (annuli and Möbius bands).

Result: New examples of topological disks achieving optimal constants are presented. Inequalities connecting conformal invariants of Steklov eigenvalues on surfaces and disks are proven. Rigidity results for the first conformal Steklov eigenvalue on annuli and Möbius bands are established.

Conclusion: The work advances understanding of isoperimetric inequalities and Steklov eigenvalues on surfaces with boundary, offering new insights into optimal constants and their dependence on topology and conformal structure.

Abstract: We give results on optimal constants of isoperimetric inequalities involving
Steklov eigenvalues on surfaces with boundary. We both consider this question
on Riemannian surfaces with a same given topology or more specifically
belonging to the same conformal class. We provide new examples of topological
disks that realize optimal constants. We prove inequalities that relate
conformal invariants associated to combinations of Steklov eigenvalues on a
compact Riemannian surface with boundary and the ones on the disk. In the
appendix, we show rigidity of the first conformal Steklov eigenvalue on annuli
and M\"obius bands.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [37] [AEGISS -- Atomic orbital and Entropy-based Guided Inference for Space Selection -- A novel semi-automated active space selection workflow for quantum chemistry and quantum computing applications](https://arxiv.org/abs/2508.10671)
*Fabio Tarocco,Pi A. B. Haase,Fabijan Pavošević,Vijay Krishna,Leonardo Guidoni,Stefan Knecht,Martina Stella*

Main category: physics.chem-ph

TL;DR: A novel method for automated active space selection in quantum chemistry combines orbital entropy analysis and atomic orbital projections, validated on Ru(II)-complexes for photodynamic therapy.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of a unified, robust, and automated framework for active space selection in systems with strong electron correlation, crucial for quantum computing applications.

Method: Integrates AVAS and AutoCAS approaches, using orbital entropy and atomic orbital projections to guide active space construction.

Result: Reliably identifies compact, chemically intuitive active spaces for challenging systems like Ru(II)-complexes, suitable for classical and quantum computing.

Conclusion: The method offers a consistent, flexible, and automated solution for active space selection, accessible via a user-friendly package for broader research use.

Abstract: The selection of a balanced active space is a critical step in
multi-reference quantum chemistry calculations, particularly for systems with
strong electron correlation. Likewise, active space selection is a key to
unlock the potential of contemporary quantum computing in quantum chemistry.
Albeit recent progress, there remains a lack of a unified, robust, and fully
automated framework for active space selection that performs reliably across a
wide range of molecular systems.
  In this work, we present a novel approach inspired by both the AVAS (Atomic
Valence Active Space) and AutoCAS methods. Our method unifies orbital entropy
analysis with atomic orbital projections to guide the construction of
chemically and physically meaningful active spaces. This integrated scheme
enables a more consistent and flexible selection of active orbitals while
retaining automation and scalability. We validate our approach on a set of
molecular systems relevant to photodynamic therapy, in particular a set of
Ru(II)-complexes, selected to span increasing levels of electron correlation
and structural complexity. These molecules serve as challenging test cases due
to the presence of strong static correlation and the need for highly accurate
electronic structure descriptions. Our results demonstrate that the method can
reliably identify compact, chemically intuitive active spaces that capture the
essential physics, making it suitable for both classical and quantum
computational frameworks.
  Furthermore, we have developed this approach in a package that is intuitive
to use for users and can be interfaced with both standard quantum chemistry and
quantum computing applications, making it accessible to a broad research
community.

</details>


### [38] [Performance of universal machine-learned potentials with explicit long-range interactions in biomolecular simulations](https://arxiv.org/abs/2508.10841)
*Viktor Zaverkin,Matheus Ferraz,Francesco Alesiani,Mathias Niepert*

Main category: physics.chem-ph

TL;DR: The paper evaluates equivariant message-passing architectures for biomolecular simulations, finding that model size and training data composition affect accuracy, while long-range electrostatics show inconsistent impact.


<details>
  <summary>Details</summary>
Motivation: To assess the applicability of universal machine-learned potentials in biomolecular simulations, addressing gaps in transferability and accuracy.

Method: Systematic evaluation of equivariant message-passing architectures trained on SPICE-v2 dataset, with and without long-range dispersion/electrostatics, tested on various systems.

Result: Larger models improve benchmark accuracy but not simulation properties; training data composition matters; long-range electrostatics inconsistently impact systems.

Conclusion: Imbalanced datasets and immature evaluation practices hinder universal machine-learned potentials' applicability to biomolecular simulations.

Abstract: Universal machine-learned potentials promise transferable accuracy across
compositional and vibrational degrees of freedom, yet their application to
biomolecular simulations remains underexplored. This work systematically
evaluates equivariant message-passing architectures trained on the SPICE-v2
dataset with and without explicit long-range dispersion and electrostatics. We
assess the impact of model size, training data composition, and electrostatic
treatment across in- and out-of-distribution benchmark datasets, as well as
molecular simulations of bulk liquid water, aqueous NaCl solutions, and
biomolecules, including alanine tripeptide, the mini-protein Trp-cage, and
Crambin. While larger models improve accuracy on benchmark datasets, this trend
does not consistently extend to properties obtained from simulations. Predicted
properties also depend on the composition of the training dataset. Long-range
electrostatics show no systematic impact across systems. However, for Trp-cage,
their inclusion yields increased conformational variability. Our results
suggest that imbalanced datasets and immature evaluation practices currently
challenge the applicability of universal machine-learned potentials to
biomolecular simulations.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [39] [A Unified Framework from Boltzmann Transport to Proton Treatment Planning](https://arxiv.org/abs/2508.10596)
*Andreas E. Kyprianou,Aaron Pim,Tristan Pryer*

Main category: math.PR

TL;DR: The paper integrates deterministic and stochastic models for proton transport, proving their duality and applying it to dose computation and treatment planning.


<details>
  <summary>Details</summary>
Motivation: To rigorously unify deterministic and stochastic perspectives of proton transport for improved accuracy in proton therapy.

Method: Combines Boltzmann-Fokker-Planck (deterministic) and diffusion process (stochastic) models, linking them via adjoint relationships and resolvents.

Result: Demonstrates duality between models, derives dose computation consistency, and formulates a hybrid optimization framework for treatment planning.

Conclusion: The framework bridges stochastic simulation with deterministic control, enabling advanced optimization in proton therapy.

Abstract: This work develops a rigorous mathematical formulation of proton transport by
integrating both deterministic and stochastic perspectives. The deterministic
framework is based on the Boltzmann-Fokker-Planck equation, formulated as an
operator equation in a suitable functional setting. The stochastic approach
models proton evolution via a track-length parameterised diffusion process,
whose infinitesimal generator provides an alternative description of transport.
  A key result is the duality between the stochastic and deterministic
formulations, established through the adjoint relationship between the
transport operator and the stochastic generator. We prove that the resolvent of
the stochastic process corresponds to the Green's function of the deterministic
equation, providing a natural link between fluence-based and particle-based
transport descriptions. The theory is applied to dose computation, where we
show that the classical relation: dose = (fluence * mass stopping power) arises
consistently in both approaches.
  Building on this foundation, we formulate a hybrid optimisation framework for
treatment planning, in which dose is computed using a stochastic model while
optimisation proceeds via adjoint-based PDE methods. We prove existence and
differentiability of the objective functional and derive the first-order
optimality system. This framework bridges stochastic simulation with
deterministic control theory and provides a foundation for future work in
constrained, adaptive and uncertainty-aware optimisation in proton therapy.

</details>


<div id='math.FA'></div>

# math.FA [[Back]](#toc)

### [40] [New Lower Bounds for the Minimum Singular Value in Matrix Selection](https://arxiv.org/abs/2508.10452)
*Zhiqiang Xu*

Main category: math.FA

TL;DR: The paper uses interlacing polynomials to maximize the minimum singular value of a submatrix, improving bounds for specific cases.


<details>
  <summary>Details</summary>
Motivation: To address the matrix selection problem by maximizing the minimum singular value of a submatrix, improving upon existing methods.

Method: Employs the interlacing polynomial method, leveraging root-coefficient relationships for tighter bounds, especially when submatrix dimensions are close to the original.

Result: Achieves a tighter lower bound for the minimum singular value, improving the Hong-Pan result for specific cases.

Conclusion: The interlacing polynomial approach provides a more effective solution for maximizing the minimum singular value in matrix selection.

Abstract: The objective of the matrix selection problem is to select a submatrix
$A_{S}\in \mathbb{R}^{n\times k}$ from $A\in \mathbb{R}^{n\times m}$ such that
its minimum singular value is maximized. In this paper, we employ the
interlacing polynomial method to investigate this problem. This approach allows
us to identify a submatrix $A_{S_0}\in \mathbb{R}^{n\times k}$ and establish a
lower bound for its minimum singular value. Specifically, unlike common
interlacing polynomial approaches that estimate the smallest root of the
expected characteristic polynomial via barrier functions, we leverage the
direct relationship between roots and coefficients. This leads to a tighter
lower bound when $k$ is close to $n$. For the case where
$AA^{\top}=\mathbb{I}_n$ and $k=n$, our result improves the well-known result
by Hong-Pan, which involves extracting a basis from a tight frame and
establishing a lower bound for the minimum singular value of the basis matrix.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [41] [Compressible boundary layers over isotropic porous surfaces](https://arxiv.org/abs/2508.10422)
*Ludovico Fossà,Pierre Ricco*

Main category: physics.flu-dyn

TL;DR: The paper studies a compressible laminar boundary layer over an isotropic porous substrate using asymptotic and numerical methods, extending Tsiberkin's self-similar solution to include compressibility, heat conduction, and nonlinear drag.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of compressible laminar boundary layers over porous substrates, including effects like heat conduction and nonlinear drag.

Method: Asymptotic and numerical methods are used, with the porous substrate modeled as an array of cubes. Momentum and enthalpy balance equations are derived via volume averaging.

Result: The velocity profile shows an inflection point at the interface, with reduced adiabatic recovery temperature and velocity gradient for high porosity, large grains, and high Mach numbers. Bottom temperature has negligible impact on shear stresses.

Conclusion: The study extends existing solutions and highlights key influences of porosity, grain size, and Mach number on boundary layer behavior.

Abstract: A compressible laminar boundary layer developing over an isotropic porous
substrate is investigated by asymptotic and numerical methods. The substrate is
modeled as an array of cubes. The momentum and enthalpy balance equations are
derived by volume averaging. The self-similar solution proposed by Tsiberkin
(2018) [Transp. Porous Media 121(1):109-120] for streamwise-growing
permeability is extended to include compressibility, heat conduction and a
nonlinear drag. The velocity profile shows an inflection point at the free
fluid-porous interfacial layer, below which it decreases to zero. A marked
reduction of the adiabatic recovery temperature of the fluid and the velocity
gradient at the interface is observed for high porosity, large grains and
relatively high Mach numbers. The temperature imposed at the bottom of the
porous substrate has a negligible influence on the shear stresses.

</details>


### [42] [Relative accuracy of turbulence simulations using pseudo-spectral and finite difference solvers](https://arxiv.org/abs/2508.10808)
*Akash Rodhiya,Shashwat Bhattacharya,Mahendra K Verma*

Main category: physics.flu-dyn

TL;DR: Spectral and finite-difference methods yield similar accuracy in turbulence simulations, despite spectral methods being more accurate for single timesteps.


<details>
  <summary>Details</summary>
Motivation: To compare the accuracy of spectral and finite-difference methods in turbulence simulations, challenging the assumption that spectral methods are always superior.

Method: Simulated forced hydrodynamic turbulence on a uniform 256³ grid for varying Reynolds numbers, comparing energy evolution, flow profiles, and statistical measures.

Result: Both methods produced nearly identical results for energy, energy spectrum, flux, and velocity distributions, suggesting numerical errors cancel out in turbulence attractors.

Conclusion: Finite-difference methods are equally effective for turbulence simulations and more efficient for large grids, making them a practical alternative to spectral methods.

Abstract: For a single timestep, a spectral solver is known to be more accurate than
its finite-difference counterpart. However, as we show in this paper,
turbulence simulations using the two methods have nearly the same accuracy. In
this paper, we simulate forced hydrodynamic turbulence on a uniform 256$^3$
grid for Reynolds numbers 965, 1231, 1515, and 1994. We show that the two
methods yield nearly the same evolution for the total energy and the flow
profiles. In addition, the steady-state energy spectrum, energy flux, and
probability distribution functions of the velocity and its derivatives are very
similar. We argue that within a turbulence attractor, the numerical errors are
likely to get cancelled (rather than get added up), which leads to similar
results for the finite-difference and spectral methods. These findings are very
valuable, considering that a parallel finite-difference simulation is more
versatile and efficient (for large grids) than its spectral counterpart.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [43] [Simulating Mass-Dependent Decoherence in Quantum Computers: Baseline Signatures for Testing Gravity-Induced Collapse](https://arxiv.org/abs/2508.10590)
*Viswak R Balaji,Samuel Punch*

Main category: quant-ph

TL;DR: Simulation study of mass-dependent decoherence models inspired by Penrose's gravity-induced collapse hypothesis, using Qiskit AerSimulator to test collapse signatures in quantum computing experiments.


<details>
  <summary>Details</summary>
Motivation: To explore whether quantum superpositions collapse due to gravitational effects, as suggested by Penrose's objective reduction theory, and to provide a protocol for testing this in quantum computers.

Method: Implemented a mass-dependent dephasing noise channel in Qiskit AerSimulator, applied to GHZ state parity measurements, branch-mass entanglement tests, and Grover's search.

Result: Generated distinctive collapse signatures differing from constant-rate dephasing, serving as a baseline for future hardware experiments.

Conclusion: The study offers a reproducible protocol for probing fundamental quantum mechanics questions using quantum computers, with potential to confirm or refute gravity-induced collapse effects.

Abstract: We present a quantum computing simulation study of mass-dependent decoherence
models inspired by Penrose's gravity-induced collapse hypothesis. According to
objective reduction (OR) theory, quantum superpositions become unstable when
the gravitational self-energy difference between branches exceeds a certain
threshold, leading to a collapse time $\tau \approx \hbar / E_G$. In this work,
we implement a mass-dependent dephasing noise channel, $p(m) = 1 - e^{-k
m^{\alpha}}$, within the Qiskit AerSimulator, where $m$ is a proxy for the
effective mass of a superposition, mapped to circuit parameters such as the
number of entangled qubits or branch size. We apply this model to three
canonical quantum computing experiments: GHZ state parity measurements,
branch-mass entanglement tests, and Grover's search to generate distinctive
collapse signatures that differ qualitatively from constant-rate dephasing. The
resulting patterns serve as a baseline reference: if future hardware
experiments exhibit the same scaling trends under ideal isolation, this could
indicate a contribution from mass-dependent collapse processes. Conversely,
deviation toward constant-noise behaviour would suggest the absence of such
gravitationally induced effects. Our results provide a reproducible protocol
and reference for using quantum computers as potential testbeds for probing
fundamental questions in quantum mechanics.

</details>


### [44] [Deep Learning in Classical and Quantum Physics](https://arxiv.org/abs/2508.10666)
*Timothy Heightman,Marcin Płodzień*

Main category: quant-ph

TL;DR: The paper discusses the transformative role of deep learning (DL) in quantum science, highlighting its benefits and risks, and provides a graduate-level guide for its responsible application.


<details>
  <summary>Details</summary>
Motivation: To address the growing importance of DL in quantum science and the need for literacy in this area, while also acknowledging its potential pitfalls.

Method: The paper offers a structured, progressive sequence of lecture notes combining conceptual explanations with practical examples.

Result: It equips readers with the knowledge to apply DL effectively in quantum physics, chemistry, and engineering, while understanding its limitations.

Conclusion: DL is a powerful tool for quantum science, but its responsible use requires awareness of its constraints and risks.

Abstract: Scientific progress is tightly coupled to the emergence of new research
tools. Today, machine learning (ML)-especially deep learning (DL)-has become a
transformative instrument for quantum science and technology. Owing to the
intrinsic complexity of quantum systems, DL enables efficient exploration of
large parameter spaces, extraction of patterns from experimental data, and
data-driven guidance for research directions. These capabilities already
support tasks such as refining quantum control protocols and accelerating the
discovery of materials with targeted quantum properties, making ML/DL literacy
an essential skill for the next generation of quantum scientists. At the same
time, DL's power brings risks: models can overfit noisy data, obscure causal
structure, and yield results with limited physical interpretability.
Recognizing these limitations and deploying mitigation strategies is crucial
for scientific rigor. These lecture notes provide a comprehensive,
graduate-level introduction to DL for quantum applications, combining
conceptual exposition with hands-on examples. Organized as a progressive
sequence, they aim to equip readers to decide when and how to apply DL
effectively, to understand its practical constraints, and to adapt AI methods
responsibly to problems across quantum physics, chemistry, and engineering.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [45] [FreeBird.jl: An Extensible Toolbox for Simulating Interfacial Phase Equilibria](https://arxiv.org/abs/2508.10237)
*Ray Yang,Junchi Chen,Douglas Thibodeaux,Robert B. Wexler*

Main category: cond-mat.stat-mech

TL;DR: FreeBird.jl is a Julia-based platform for studying phase equilibria at interfaces, supporting various system configurations and sampling algorithms.


<details>
  <summary>Details</summary>
Motivation: To provide a flexible and extensible tool for computational studies of interfacial thermodynamics.

Method: Uses Julia's type hierarchies and multiple dispatch for modular integration of system definitions, energy evaluators, and sampling schemes (e.g., nested sampling, Wang-Landau, Metropolis Monte Carlo).

Result: A versatile framework capable of handling atomistic and lattice systems with customizable geometries and multi-component mixtures.

Conclusion: FreeBird.jl offers a powerful and adaptable solution for exploring interfacial phenomena.

Abstract: We present FreeBird.jl, an extensible Julia-based platform for computational
studies of phase equilibria at generic interfaces. The package supports a range
of system configurations, from atomistic solid surfaces to coarse-grained
lattice$-$gas models, with energies evaluated using classical interatomic
potentials or lattice Hamiltonians. Both atomistic and lattice systems
accommodate single- or multi-component mixtures with flexibly definable surface
and lattice geometries. Implemented sampling algorithms include nested
sampling, Wang$-$Landau sampling, Metropolis Monte Carlo, and, for tractable
lattice systems, exact enumeration. Leveraging Julia's type hierarchies and
multiple dispatch, FreeBird.jl provides a modular interface that allows
seamless integration of system definitions, energy evaluators, and sampling
schemes. Designed for flexibility, extensibility, and performance, FreeBird.jl
offers a versatile framework for exploring the thermodynamics of interfacial
phenomena.

</details>


### [46] [Variational boundary based tensor network renormalization group](https://arxiv.org/abs/2508.10418)
*Feng-Feng Song,Naoki Kawashima*

Main category: cond-mat.stat-mech

TL;DR: A real-space renormalization group algorithm for 2D tensor networks uses variational boundary tensors for global optimization, improving accuracy without increasing computational complexity.


<details>
  <summary>Details</summary>
Motivation: To enhance the accuracy of tensor renormalization group (TRG) methods in higher dimensions while maintaining manageable computational costs.

Method: Utilizes variational boundary tensors for global optimization and constructs renormalization projectors based on this environment, leveraging tensor canonical forms.

Result: Achieves higher accuracy than existing TRG methods without entanglement filtering, with the same computational complexity.

Conclusion: Provides a practical approach for extending TRG to higher dimensions efficiently.

Abstract: We propose a real-space renormalization group algorithm for accurately
coarse-graining two-dimensional tensor networks. The central innovation of our
method lies in utilizing variational boundary tensors as a globally optimized
environment for the entire system. Based on this optimized environment, we
construct renormalization projectors that significantly enhance accuracy. By
leveraging the canonical form of tensors, our algorithm maintains the same
computational complexity as the original tensor renormalization group (TRG)
method, yet achieves higher accuracy than existing approaches that do not
incorporate entanglement filtering. Our work offers a practical pathway for
extending TRG methods to higher dimensions while keeping computational costs
manageable.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [47] [Type-I Multiferroic VHfO$_4$ with Strain-Switchable Magnetic Orders and Magnetoelectric Coupling](https://arxiv.org/abs/2508.10380)
*Qisheng Yu,Boyu Liu,Hongjun Xiang,Shi Liu*

Main category: cond-mat.mtrl-sci

TL;DR: The paper proposes VHfO$_4$ as a novel multiferroic oxide, combining ferroelectricity and magnetism, with strain-tunable magnetoelectric coupling for spintronics applications.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the complementary properties of vanadium-based ferromagnets and HfO$_2$-based ferroelectrics, aiming to create a material with concurrent ferroic orders.

Method: First-principles DFT calculations and parallel tempering Monte Carlo simulations are used to analyze dynamic stability, ferroic orders, and strain effects.

Result: VHfO$_4$ exhibits robust ferroelectric polarization and V-driven magnetism, with strain engineering enabling tunable magnetoelectric coupling and four distinct magnetic states.

Conclusion: VHfO$_4$ is established as a Type-I multiferroic with atomic-scale ferroic origins and strain-tunable cross-coupling, promising for voltage-controlled spintronics devices.

Abstract: Motivated by the complementary properties of vanadium-based ferromagnets and
HfO$_2$-based ferroelectrics, we propose a novel multiferroic oxide, VHfO$_4$,
through 50\% Hf$^{4+}$ substitution with V$^{4+}$ in the ferroelectric $Pca2_1$
phase of HfO$_2$. First-principles DFT calculations reveal that the
$Pca2_1$-like VHfO$_4$ phase exhibits dynamic stability and concurrent ferroic
orders: robust ferroelectric polarization comparable to HfO$_2$ and V-driven
magnetism. Parallel tempering Monte Carlo simulations identify an
antiferromagnetic ground state, while strain engineering enables tunable
magnetoelectric coupling. Biaxial in-plane strain induces four magnetic states:
intralayer FM/interlayer AFM, intralayer AFM/interlayer FM, spiral-like
non-collinear order, and discrete alternating spin alignment. Critically,
$c$-axis strain modulates magnetic energy landscapes, demonstrating
electromechanical control of magnetism. This work establishes VHfO$_4$ as a
Type-I multiferroic with coexisting atomic-scale ferroic origins and
strain-tunable cross-coupling, offering a platform for voltage-controlled
spintronics devices.

</details>


### [48] [FastTrack: a fast method to evaluate mass transport in solid leveraging universal machine learning interatomic potential](https://arxiv.org/abs/2508.10505)
*Hanwen Kang,Tenglong Lu,Zhanbin Qi,Jiandong Guo,Sheng Meng,Miao Liu*

Main category: cond-mat.mtrl-sci

TL;DR: A fast, accurate framework for computing atomic migration barriers in crystals using machine learning force fields (MLFFs) and 3D potential energy surface sampling, achieving significant speedups over DFT-NEB.


<details>
  <summary>Details</summary>
Motivation: To address the computational cost and accuracy challenges in calculating atomic migration barriers in materials, leveraging MLFFs for efficiency and precision.

Method: Combines MLFFs with 3D potential energy surface sampling and interpolation, suppresses periodic self-interactions via supercell expansion, and extracts minimum energy pathways without predefined NEB images.

Result: MLFF-derived barriers are within tens of meV of DFT and experimental values, with ~100x speedups over DFT-NEB. Benchmarked GPTFF, CHGNet, and MACE, showing improved accuracy with fine-tuning.

Conclusion: The framework offers a rapid, accurate alternative for high-throughput materials screening, supported by an open-source package for PES visualization.

Abstract: We introduce a rapid, accurate framework for computing atomic migration
barriers in crystals by combining universal machine learning force fields
(MLFFs) with 3D potential energy surface sampling and interpolation. Our method
suppresses periodic self interactions via supercell expansion, builds a
continuous PES from MLFF energies on a spatial grid, and extracts minimum
energy pathways without predefined NEB images. Across twelve benchmark
electrode and electrolyte materials including LiCoO2, LiFePO4, and LGPS our
MLFF-derived barriers lie within tens of meV of DFT and experiment, while
achieving ~10^2 x speedups over DFT-NEB. We benchmark GPTFF, CHGNet, and MACE,
show that fine-tuning on PBE/PBE+U data further enhances accuracy, and provide
an open-source package for high-throughput materials screening and interactive
PES visualization.

</details>


### [49] [Symmetry-Constrained Multi-Scale Physics-Informed Neural Networks for Graphene Electronic Band Structure Prediction](https://arxiv.org/abs/2508.10718)
*Wei Shan Lee,I Hang Kwok,Kam Ian Leong,Chi Kiu Althina Chau,Kei Chon Sio*

Main category: cond-mat.mtrl-sci

TL;DR: SCMS-PINN v35 predicts graphene band structures with high accuracy by enforcing crystallographic symmetries and using multi-head ResNet-6 pathways.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle to balance computational efficiency and physical accuracy in predicting electronic band structures of 2D materials.

Method: Uses Symmetry-Constrained Multi-Scale Physics-Informed Neural Network (SCMS-PINN) with three ResNet-6 pathways (K-head, M-head, General head) and progressive Dirac constraint scheduling.

Result: Achieves 99.99% training loss reduction, predicts Dirac point gaps within 30.3 µeV, and maintains symmetry preservation.

Conclusion: The framework enables accurate and efficient prediction of band structures, paving the way for broader applications in 2D materials.

Abstract: Accurate prediction of electronic band structures in two-dimensional
materials remains a fundamental challenge, with existing methods struggling to
balance computational efficiency and physical accuracy. We present the
Symmetry-Constrained Multi-Scale Physics-Informed Neural Network (SCMS-PINN)
v35, which directly learns graphene band structures while rigorously enforcing
crystallographic symmetries through a multi-head architecture. Our approach
introduces three specialized ResNet-6 pathways -- K-head for Dirac physics,
M-head for saddle points, and General head for smooth interpolation --
operating on 31 physics-informed features extracted from k-points. Progressive
Dirac constraint scheduling systematically increases the weight parameter from
5.0 to 25.0, enabling hierarchical learning from global topology to local
critical physics. Training on 10,000 k-points over 300 epochs achieves 99.99\%
reduction in training loss (34.597 to 0.003) with validation loss of 0.0085.
The model predicts Dirac point gaps within 30.3 $\mu$eV of theoretical zero and
achieves average errors of 53.9 meV (valence) and 40.5 meV (conduction) across
the Brillouin zone. All twelve C$_{6v}$ operations are enforced through
systematic averaging, guaranteeing exact symmetry preservation. This framework
establishes a foundation for extending physics-informed learning to broader
two-dimensional materials for accelerated discovery.

</details>
