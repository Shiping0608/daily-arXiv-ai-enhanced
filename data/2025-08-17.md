<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 12]
- [math.AP](#math.AP) [Total: 12]
- [physics.comp-ph](#physics.comp-ph) [Total: 4]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 2]
- [cs.MS](#cs.MS) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 2]
- [physics.bio-ph](#physics.bio-ph) [Total: 1]
- [cs.DC](#cs.DC) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 2]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 3]
- [quant-ph](#quant-ph) [Total: 2]
- [math.FA](#math.FA) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [math.DG](#math.DG) [Total: 2]
- [math.PR](#math.PR) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [A tensor-based dynamic mode decomposition based on the $\star_{\boldsymbol{M}}$-product](https://arxiv.org/abs/2508.10126)
*Arvind K. Saibaba,Misha E. Kilmer,Khalil Hall-Hooper,Fan Tian,Alex Mize*

Main category: math.NA

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Dynamic mode decomposition (DMD) is a data-driven method for estimating the
dynamics of a discrete dynamical system. This paper proposes a tensor-based
approach to DMD for applications in which the states can be viewed as tensors.
Specifically, we use the $\star_{\boldsymbol{M}}$-product framework for tensor
decompositions which we demonstrate offers excellent compression compared to
matrix-based methods and can be implemented in a computationally efficient
manner. We show how the proposed approach is connected to the traditional DMD
and physics-informed DMD frameworks. We give a computational framework for
computing the tensor-based DMD and detail the computational costs. We also give
a randomized algorithm that enables efficient $\star_{\boldsymbol{M}}$-DMD
computations in the streaming setting. The numerical results show that the
proposed method achieves equal or better accuracy for the same storage compared
to the standard DMD on these examples and is more efficient to compute.

</details>


### [2] [A Generalized Alternating Anderson Acceleration Method](https://arxiv.org/abs/2508.10158)
*Yunhui He,Santolo Leveque*

Main category: math.NA

TL;DR: A generalized alternating Anderson acceleration method is proposed, combining fixed-point iteration and Anderson acceleration for solving linear and nonlinear problems, with proven convergence and improved efficiency over existing methods.


<details>
  <summary>Details</summary>
Motivation: To enhance flexibility and efficiency in solving linear and nonlinear problems by combining fixed-point iteration and Anderson acceleration in a periodic scheme.

Method: A periodic scheme with $t$ fixed-point iteration steps interleaved with $s$ Anderson acceleration steps (window size $m$), analyzed for convergence in linear cases and applied to various iterative methods.

Result: Convergence is proven for contractive fixed-point iteration, and a sufficient condition is provided for noncontractive cases. Numerical results show improved efficiency over existing methods.

Conclusion: The proposed method offers a flexible and efficient approach for accelerating iterative solvers, outperforming existing techniques with careful parameter choices.

Abstract: In this work, we propose a generalized alternating Anderson acceleration
method, a periodic scheme composed of $t$ fixed-point iteration steps,
interleaved with $s$ steps of Anderson acceleration with window size $m$, to
solve linear and nonlinear problems. This allows flexibility to use different
combinations of fixed-point iteration and Anderson iteration. We present a
convergence analysis of the proposed scheme for accelerating the Richardson
iteration in the linear case, with a focus on specific parameter choices of
interest. Specifically, we prove convergence of the proposed method under
contractive fixed-point iteration and provide a sufficient condition for
convergence when the Richardson iteration matrix is diagonalizable and
noncontractive. To demonstrate the broader applicability of our proposed
method, we use it to accelerate Jacobi iteration, Picard iteration, gradient
descent, and the alternating direction method of multipliers in solving partial
differential equations and nonlinear, nonsmooth optimization problems. The
numerical results illustrate that the proposed scheme is more efficient than
the existing windowed Anderson acceleration and alternating Anderson ($s=1$) in
terms of iteration number and CPU time for careful choice of parameters $m, s,
t$.

</details>


### [3] [SSBE-PINN: A Sobolev Boundary Scheme Boosting Stability and Accuracy in Elliptic/Parabolic PDE Learning](https://arxiv.org/abs/2508.10322)
*Qixuan Zhou,Chuqi Chen,Tao Luo,Yang Xiang*

Main category: math.NA

TL;DR: Proposes Sobolev-Stable Boundary Enforcement (SSBE) to improve PINNs' accuracy by redefining boundary loss using Sobolev norms, ensuring bounded H1 error and better performance in PDE solving.


<details>
  <summary>Details</summary>
Motivation: PINNs often fail to achieve accurate convergence in the H1 norm due to boundary approximation errors, motivating a need for improved boundary enforcement methods.

Method: Introduces SSBE, which redefines boundary loss using Sobolev norms to incorporate boundary regularity directly into training, backed by theoretical analysis.

Result: SSBE outperforms standard PINNs in relative L2 and H1 errors across linear/nonlinear PDEs, including Poisson, heat, and elliptic problems.

Conclusion: SSBE provides a principled, practical solution for enhancing gradient fidelity and accuracy in neural network-based PDE solvers.

Abstract: Physics-Informed Neural Networks (PINNs) have emerged as a powerful framework
for solving partial differential equations (PDEs), yet they often fail to
achieve accurate convergence in the H1 norm, especially in the presence of
boundary approximation errors. In this work, we propose a novel method called
Sobolev-Stable Boundary Enforcement (SSBE), which redefines the boundary loss
using Sobolev norms to incorporate boundary regularity directly into the
training process. We provide rigorous theoretical analysis demonstrating that
SSBE ensures bounded H1 error via a stability guarantee and derive
generalization bounds that characterize its robustness under finite-sample
regimes. Extensive numerical experiments on linear and nonlinear PDEs,
including Poisson, heat, and elliptic problems, show that SSBE consistently
outperforms standard PINNs in terms of both relative L2 and H1 errors, even in
high-dimensional settings. The proposed approach offers a principled and
practical solution for improving gradient fidelity and overall solution
accuracy in neural network based PDE solvers.

</details>


### [4] [A Semi-Lagrangian scheme on embedded manifolds using generalized local polynomial reproductions](https://arxiv.org/abs/2508.10344)
*Thomas Hangelbroek,Christian Rieger,Grady B. Wright*

Main category: math.NA

TL;DR: The paper analyzes high-order semi-Lagrangian schemes for PDEs on manifolds, introducing a stable remapping operator and numerical experiments.


<details>
  <summary>Details</summary>
Motivation: To extend error analysis for PDEs on manifolds and address challenges in achieving high approximation orders with stability.

Method: Proposes a mesh-free remapping operator using ‚Ñì1 minimizing generalized polynomial reproduction, requiring only point values.

Result: The framework supports high-order convergence and stability, validated by numerical experiments.

Conclusion: The approach is effective for PDEs on manifolds and opens new research directions.

Abstract: We analyze rates of uniform convergence for a class of high-order
semi-Lagrangian schemes for first-order, time-dependent partial differential
equations on embedded submanifolds of $\mathbb{R}^d$ (including advection
equations on surfaces) by extending the error analysis of Falcone and Ferretti.
A central requirement in our analysis is a remapping operator that achieves
both high approximation orders and strong stability, a combination that is
challenging to obtain and of independent interest. For this task, we propose a
novel mesh-free remapping operator based on $\ell_1$ minimizing generalized
polynomial reproduction, which uses only point values and requires no
additional geometric information from the manifold (such as access to tangent
spaces or curvature). Our framework also rigorously addresses the numerical
solution of ordinary differential equations on manifolds via projection
methods. We include numerical experiments that support the theoretical results
and also suggest some new directions for future research.

</details>


### [5] [Product Of Exponentials (POE) Splines on Lie-Groups: Limitations, Extensions, and Application to SO(3) and SE(3)](https://arxiv.org/abs/2508.10513)
*Andreas Mueller*

Main category: math.NA

TL;DR: The paper introduces a new method for constructing splines on Lie groups by solving the Poisson equation, addressing limitations of existing methods that rely on local geodesics or polynomials starting at the identity.


<details>
  <summary>Details</summary>
Motivation: Existing methods for spline construction on Lie groups often assume curves start at the identity and use local geodesics or polynomials, which may not accurately reflect the actual curve to be interpolated.

Method: Local curves are derived as solutions to the Poisson equation on the Lie group, ensuring boundary conditions and geometric integrity. Higher-order approximations yield product of exponential (POE) splines, with algorithms for 3rd- and 4th-order splines.

Result: The new approach allows for splines with arbitrary initial conditions, overcoming the limitation of existing methods that cannot exactly reconstruct higher-order motions. Examples include shape reconstruction of slender rods.

Conclusion: The proposed method provides a more flexible and accurate framework for spline construction on Lie groups, with practical applications in geometric modeling.

Abstract: Existing methods for constructing splines and Bezier curves on a Lie group G
involve repeated products of exponentials deduced from local geodesics, w.r.t.
a Riemannian metric, or rely on general polynomials. Moreover, each of these
local curves is supposed to start at the identity of $G$. Both assumptions may
not reflect the actual curve to be interpolated. This paper pursues a different
approach to construct splines on $G$. Local curves are expressed as solutions
of the Poisson equation on G. Therewith, the local interpolations satisfies the
boundary conditions while respecting the geometry of $G$. A $k$th-order
approximation of the solutions gives rise to a $k$th-order product of
exponential (POE) spline. Algorithms for constructing 3rd- and 4th-order
splines are derived from closed form expressions for the approximate solutions.
Additionally, spline algorithms are introduced that allow prescribing a vector
field the curve must follow at the interpolation points. It is shown that the
established algorithms, where $k$th-order POE-splines are constructed by
concatenating local curves starting at the identity, cannot exactly reconstruct
a $k$th-order motion. To tackle this issue, the formulations are extended by
allowing for local curves between arbitrary points, rather than curves
emanating from the identity. This gives rise to a global $k$th-order spline
with arbitrary initial conditions. Several examples are presented, in
particular the shape reconstruction of slender rods modeled as geometrically
non-linear Cosserat rods.

</details>


### [6] [On The Eventual Periodicity of Fractional Order Dispersive Wave Equations Using RBFs and Transform](https://arxiv.org/abs/2508.10547)
*Hameed Ullah Jan,Marjan Uddin,Irshad Ali Shah,Salam Ullah Khan*

Main category: math.NA

TL;DR: The paper proposes a numerical scheme combining radial basis functions finite difference (RBF-FD) and Laplace transform to solve fractional order dispersive wave equations, demonstrating efficiency and accuracy for nonlinear PDEs.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of solving high-order fractional and integer-type nonlinear PDEs efficiently, avoiding dense matrices and time instability issues.

Method: Uses RBF-FD for spatial discretization, Laplace transform for time-independent equations, and an iterative scheme for nonlinear terms.

Result: The method yields sparse matrices, avoids time-stepping instability, and achieves high accuracy in solving nonlinear fractional PDEs like KdV and Burgers equations.

Conclusion: The proposed approach is efficient, reliable, and accurate for large-scale problems, validated through examples.

Abstract: In this research work, let us focus on the construction of numerical scheme
based on radial basis functions finite difference (RBF-FD) method combined with
the Laplace transform for the solution of fractional order dispersive wave
equations. The numerical scheme is then applied to examine the eventual
periodicity of the proposed model subject to the periodic boundary conditions.
The implementation of proposed technique for high order fractional and integer
type nonlinear partial differential equations (PDEs) is beneficial because this
method is local in nature, therefore it yields and resulted in sparse
differentiation matrices instead of full and dense matrices. Only small
dimensions of linear systems of equations are to be solved for every center in
the domain and hence this procedure is more reliable and efficient to solve
large scale physical and engineering problems in complex domain. Laplace
transform is utilized for obtaining the equivalent time-independent equation in
Laplace space and also valuable to handle time-fractional derivatives in the
Caputo sense. Application of Laplace transform avoids the time steeping
procedure which commonly encounters the time instability issues. The solution
to the transformed model is then obtained by computing the inversion of Laplace
transform with an appropriate contour in a complex space, which is approximated
by trapezoidal rule with high accuracy. Also since the Laplace transform
operator is linear, it cannot be used to transform non-linear terms therefore
let us use a linearization approach and an appropriate iterative scheme. The
proposed approach is tasted for some nonlinear fractional order KdV and Burgers
equations. The capacity, high order accuracy and efficiency of our approach are
demonstrated using examples and results

</details>


### [7] [RBF-FD Method for Some Dispersive Wave Equations and Their Eventual Periodicity](https://arxiv.org/abs/2508.10558)
*Marjan Uddin,Hameed Ullah Jan,Muhammad Usman*

Main category: math.NA

TL;DR: The paper approximates solutions and analyzes eventual periodicity for dispersive wave equations with periodic forcing, using a radial kernel-based numerical scheme and RK4 for temporal execution.


<details>
  <summary>Details</summary>
Motivation: To efficiently solve initial-boundary value problems (IBVPs) for dispersive wave equations on bounded domains with periodic forcing, leveraging local numerical methods.

Method: A radial kernel-based numerical scheme, locally implemented like finite differences, with temporal execution via RK4. Uses sparse differentiation matrices for efficiency.

Result: The scheme efficiently recovers solutions, validated against existing methods in literature.

Conclusion: The proposed local numerical method effectively approximates solutions and captures eventual periodicity for the studied IBVPs.

Abstract: In this paper, we approximate the solution and also discuss the periodic
behavior termed as eventual periodicity of solutions of (IBVPs) for some
dispersive wave equations on a bounded domain corresponding to periodic
forcing. The constructed numerical scheme is based on radial kernels and local
in nature like finite difference method. The temporal variable is executed
through RK4 scheme. Due to the local nature and sparse differentiation matrices
our numerical scheme efficiently recovers the solution. The results achieved
are validated and examined with other methods accessible in the literature.

</details>


### [8] [CutVEM: Conforming virtual element method on embedded domains with shape-agnostic element agglomeration](https://arxiv.org/abs/2508.10570)
*Ramsharan Rangarajan,N. Sukumar*

Main category: math.NA

TL;DR: The paper introduces CutVEM, a novel element agglomeration algorithm for the virtual element method (VEM) to improve conditioning on cut-cell meshes, maintaining accuracy and optimal convergence.


<details>
  <summary>Details</summary>
Motivation: The VEM's robustness on polygonal meshes is compromised by poor conditioning in cut-cell scenarios, necessitating a solution to retain its advantages for evolving geometries.

Method: Proposes an element agglomeration algorithm based on element stability ratios, iteratively optimizing mesh conditioning without altering degrees of freedom.

Result: CutVEM significantly improves stiffness matrix conditioning while preserving solution accuracy and optimal convergence rates.

Conclusion: CutVEM effectively addresses VEM's conditioning issues on cut-cell meshes, enhancing its applicability for complex geometries without sacrificing performance.

Abstract: The virtual element method (VEM) is a stabilized Galerkin method that is
robust and accurate on general polygonal meshes. This feature makes it an
appealing candidate for simulations involving meshes with embedded interfaces
and evolving geometries. However, the method can yield poorly conditioned
stiffness matrices in such scenarios due to meshes having cut cells. We propose
a novel element agglomeration algorithm for the virtual element method to
address this issue. The agglomeration algorithm renders the VEM robust over
planar polygonal meshes, particularly on finite element meshes cut by immersed
geometries. The algorithm relies on the element stability ratio, which we
define using the extreme eigenvalues of the element stiffness matrix. The
resulting element agglomeration criterion is free from nebulous polygon quality
metrics and is defined independently of polygon shapes. The algorithm proceeds
iteratively and element-wise to maximize the minimum element stability ratio,
even at the expense of degrading elements with better ratios. Crucially,
element agglomeration alters the number of elements, not the degree of freedom
count. The resulting method, which we label as CutVEM, retains node locations
of cut elements unchanged, and yields discretizations that conform to embedded
interfaces. This, in turn, facilitates straightforward imposition of boundary
conditions and interfacial constraints. Through detailed numerical experiments
that sample varied element-interface intersections, we demonstrate that CutVEM
enjoys dramatically improved condition numbers of global stiffness matrices
over the VEM. Furthermore, simulations of prototypical heat conduction problems
with Dirichlet and Neumann boundary conditions on domains with immersed
geometries show that element agglomeration does not noticeably degrade solution
accuracy and that CutVEM retains the VEM's optimal convergence rate.

</details>


### [9] [Efficient and Optimally Accurate Numerical Algorithms for Stochastic Turbulent Flow Problems](https://arxiv.org/abs/2508.10578)
*Brandiece N. Berry,Md Mahmudul Islam,Muhammad Mohebujjaman,Neethu Suma Raveendran*

Main category: math.NA

TL;DR: Proposed a filter-based Ensemble Eddy Viscosity model and a grad-div regularized, efficient ensemble algorithm for stochastic turbulent flows, proving stability and accuracy for high Reynolds numbers.


<details>
  <summary>Details</summary>
Motivation: Address computational challenges in stochastic turbulent flow problems by reducing costs and memory requirements while maintaining accuracy.

Method: Developed a linearized IMEX EEV algorithm with shared coefficient matrices and different right-hand-side vectors, analyzed two stable time-stepping schemes (first and second-order).

Result: Proven stability and optimal convergence for 2D/3D problems; validated with high Reynolds number tests, showing good performance.

Conclusion: The proposed schemes are efficient, stable, and accurate for high Reynolds number stochastic turbulent flows.

Abstract: In this paper, we first propose a filter-based continuous Ensemble Eddy
Viscosity (EEV) model for stochastic turbulent flow problems. We then propose a
generic algorithm for a family of fully discrete, grad-div regularized,
efficient ensemble parameterized schemes for this model. The linearized
Implicit-Explicit (IMEX) EEV generic algorithm shares a common coefficient
matrix for each realization per time-step, but with different right-hand-side
vectors, which reduces the computational cost and memory requirements to the
order of solving deterministic flow problems. Two family members of the
proposed time-stepping algorithm are analyzed and proven to be stable. It is
found that one is first-order and the other is second-order accurate in time
for any stable finite element pairs. Avoiding the discrete inverse inequality,
the optimal convergence of both schemes is proven rigorously for both 2D and 3D
problems. For appropriately large grad-div parameters, both schemes are
unconditionally stable and allow weakly divergence-free elements. Several
numerical tests are given for high expected Reynolds number ($\textbf{E}[Re]$)
problems. The convergence rates are verified using manufactured solutions with
$\textbf{E}[Re]=10^{3},10^{4},\;\text{and}\; 10^{5}$. For various high
$\textbf{E}[Re]$, the schemes are implemented on benchmark problems which
includes: A 2D channel flow over a unit step problem, a non-intrusive
Stochastic Collocation Method (SCM) is used to examine the performance of the
schemes on a 2D Regularized Lid Driven Cavity (RLDC) problem, and a 3D RLDC
problem, and found them perform well.

</details>


### [10] [Nonlinear filtering based on density approximation and deep BSDE prediction](https://arxiv.org/abs/2508.10630)
*Kasper B√•gmark,Adam Andersson,Stig Larsson*

Main category: math.NA

TL;DR: A new Bayesian filter using backward stochastic differential equations (BSDEs) and neural networks is introduced, with offline training for online application.


<details>
  <summary>Details</summary>
Motivation: To address the filtering problem by leveraging nonlinear Feynman-Kac representation and deep BSDE methods for efficient approximation.

Method: Uses deep BSDE and neural networks to approximate unnormalized filtering density, trained offline for online use.

Result: Proves a mixed error bound under elliptic conditions and confirms theoretical convergence in numerical examples.

Conclusion: The method is effective, with proven error bounds and practical validation through numerical tests.

Abstract: A novel approximate Bayesian filter based on backward stochastic differential
equations is introduced. It uses a nonlinear Feynman--Kac representation of the
filtering problem and the approximation of an unnormalized filtering density
using the well-known deep BSDE method and neural networks. The method is
trained offline, which means that it can be applied online with new
observations. A mixed a priori-a posteriori error bound is proved under an
elliptic condition. The theoretical convergence rate is confirmed in two
numerical examples.

</details>


### [11] [Isogeometric multi-patch shell analysis using the Geometry + Simulation Modules](https://arxiv.org/abs/2508.10648)
*Hugo M. Verhelst,Angelos Mantzaflaris,Matthias M√∂ller*

Main category: math.NA

TL;DR: The paper details the implementation of Isogeometric Kirchhoff-Love shells in G+Smo, focusing on software design and compatibility with future developments, rather than novel methods.


<details>
  <summary>Details</summary>
Motivation: To bridge CAD and FEA using splines for thin shell analysis, reducing unknowns by leveraging spline continuity.

Method: Implementation includes patch coupling, error estimators, structural analysis algorithms, and wrinkling modeling, organized into three new G+Smo modules.

Result: The modules enable versatile modeling of multi-patch shell problems with fast solvers and extensible interfaces.

Conclusion: The implementation supports future research by providing a flexible and extensible framework for shell analysis.

Abstract: Isogeometric Analysis (IGA) bridges Computer-Aided Design (CAD) and Finite
Element Analysis (FEA) by employing splines as a common basis for geometry and
analysis. One of the advantages of IGA is in the realm of thin shell analysis:
due to the arbitrary continuity of the spline basis, Kirchhoff-Love shells can
be modeled without the need to introduce unknowns for the mid-plane rotations,
leading to a reduction in the number of unknowns. In this paper, we provide the
background of an implementation of Isogeometric Kirchhoff--Love shells within
the Geometry + Simulation Modules (G+Smo). This paper accompanies multiple
previous publications and elaborates on the design of the software used in
these papers, rather than the novelty of the methods presented therein. The
presented implementation provides patch coupling via penalty methods and
unstructured splines, goal-oriented error estimators, several algorithms for
structural analysis and advanced algorithms for the modeling of wrinkling in
hyperelastic membranes. These methods are all contained in three new modules in
G+Smo: a module for Kirchhoff-Love shells, a module for structural analysis,
and a module for unstructured spline constructions. As motivated in this paper,
the modules are implemented to be compatible with future developments. For
example, by providing base implementations of material laws, by using black-box
functions for the structural analysis module, or by providing a standardized
approach for the implementation of unstructured spline constructions. Overall,
this paper demonstrates that the new modules contribute to a versatile
ecosystem for the modeling of multi-patch shell problems through fast
off-the-shelf solvers with a simple interface, designed to be extended in
future research.

</details>


### [12] [The Hu-Zhang element for linear elasticity on curved domains](https://arxiv.org/abs/2508.10674)
*Wei Chen,Xinyuan Du,Jun Hu*

Main category: math.NA

TL;DR: The paper extends the Hu-Zhang element to curved domains, ensuring symmetry and H(div)-conformity, and addresses stability and convergence challenges.


<details>
  <summary>Details</summary>
Motivation: To adapt the Hu-Zhang element for curved domains while maintaining key properties like symmetry and H(div)-conformity.

Method: Introduces a non-polynomial curved Hu-Zhang element, establishes a novel inf-sup condition for stability, and uses local p-enrichment to improve convergence.

Result: Achieves optimal convergence rates for most variables, though stress L¬≤-error remains suboptimal. Numerical experiments support the theory.

Conclusion: The curved Hu-Zhang element is successfully extended, with local p-enrichment resolving divergence space issues, validated by numerical results.

Abstract: This paper extends the Hu-Zhang element for linear elasticity problems to
curved domains, preserving strong symmetry and H(div)-conformity. The
non-polynomial structure of the curved Hu-Zhang element makes it difficult to
analyze the stability result, which is overcome by establishing a novel inf-sup
condition. Optimal convergence rates are achieved for all variables except the
stress $L^2$-error. This suboptimality originates from the fact that the
divergence space of the curved Hu-Zhang element is not contained in the
discrete displacement space, which is rectified by local $p$-enrichment in the
Hu-Zhang space on curved boundary elements. Some numerical experiments validate
the theoretical results.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [13] [An inverse problem on a metric graph with cycle](https://arxiv.org/abs/2508.10121)
*Sergei Avdonin,Julian Edward*

Main category: math.AP

TL;DR: The paper studies a quantum graph with a ring and two edges, using given eigenvalue and eigenfunction derivative data to determine edge lengths and potential functions.


<details>
  <summary>Details</summary>
Motivation: To solve the inverse problem of determining unknown geometric and potential properties of a quantum graph from spectral data.

Method: Uses a Schr√∂dinger-type operator with Dirichlet boundary conditions and given eigenvalue and eigenfunction derivative data at specific vertices.

Result: The lengths of edges and potential functions on each edge are determined from the provided data.

Conclusion: The approach successfully reconstructs the quantum graph's geometric and potential properties from spectral information.

Abstract: Consider a quantum graph consisting of a ring with two attached edges, and
assume Kirchhoff-Neumann conditions hold at the internal vertices. Associated
to this graph is a Schr\"{o}dinger type operator $L=-\Delta +q(x)$ with
Dirichlet boundary conditions at the two boundary nodes. Let $\{ \omega_n^2, \
\varphi_n(x)\}$ be the eigenvalues and associated normalized eigenfunctions.
Let $v_1$ be a boundary vertex, and $v_2$ the adjacent internal vertex. Assume
we know the following data: $\{ \omega_n^2,\partial_x
\varphi_n(v_1),\partial_x\varphi_n(v_2)\}.$ Here $\partial_x\varphi_n(v_2)$
refers to an outward normal derivative at $v_2$ along one of the edges incident
to the other internal vertex. From this data we determine the following unknown
quantities: the lengths of edges and the potential functions on each edge.

</details>


### [14] [Non-Decaying Solutions to the 2D Dissipative Quasi-Geostrophic Equations](https://arxiv.org/abs/2508.10254)
*David M. Ambrose,Ryan Aschoff,Elaine Cozzi,James P. Kelliher*

Main category: math.AP

TL;DR: Existence and uniqueness of solutions for the 2D surface quasi-geostrophic equation with subcritical diffusion, including global solutions for smooth initial data and extension to $L^{\infty}$ data via density arguments.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of solving the surface quasi-geostrophic equation without decay or periodicity assumptions, especially for $L^{\infty}$ data where Riesz transforms are unbounded.

Method: Analyze solutions for $L^{\infty}$ data, prove local existence and uniqueness, extend to global solutions for smooth data, and generalize results to a relaxed model with a Sertfati-type constitutive law.

Result: Local existence and uniqueness for $L^{\infty}$ data, global classical solutions for smooth data, and extension to $L^{\infty}$ data via density arguments.

Conclusion: The study successfully establishes solutions under relaxed conditions and extends results to a generalized model, demonstrating robustness in handling non-decaying and non-periodic data.

Abstract: We consider the surface quasi-geostrophic equation in two spatial dimensions,
with subcritical diffusion (i.e. with fractional diffusion of order $2\alpha$
for $\alpha>\frac{1}{2}$.) We establish existence of solutions without assuming
either decay at spatial infinity or spatial periodicity. One obstacle is that
for $L^{\infty}$ data, the constitutive law may not be applicable, as Riesz
transforms are unbounded. However, for $L^{\infty}$ initial data for which the
constitutive law does converge, we demonstrate that there exists a unique
solution locally in time, and that the constitutive law continues to hold at
positive times. In the case that $\alpha\in(\frac{1}{2},1]$ and that the
initial data has some smoothness (specifically, if the data is in $C^{2}$), we
demonstrate a maximum principle and show that this unique solution is actually
classical and global in time. Then, a density argument allows us to show that
mild solutions with only $L^{\infty}$ data are also global in time, and also
possess this maximum principle. Finally, we introduce a related problem in
which we replace the usual constitutive law for the surface quasi-geostrophic
equation with a generalization of Sertfati type, and prove the same results for
this relaxed model.

</details>


### [15] [Stability of flat-core pinned p-elasticae](https://arxiv.org/abs/2508.10314)
*Tatsuya Miura,Kensuke Yoshizawa*

Main category: math.AP

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We classify the stability of flat-core $p$-elasticae in $\mathbf{R}^d$
subject to the pinned boundary condition. Together with previous work, this
completes the classification of stable pinned $p$-elasticae in $\mathbf{R}^d$
for all $p\in(1,\infty)$ and $d\geq2$.

</details>


### [16] [An Analysis of the Riemann Problem for a $2 \times 2$ System of Keyfitz-Kranzer Type Balance Laws With a Time-Dependent Source Term](https://arxiv.org/abs/2508.10347)
*Josh Culver,Aubrey Ayres,Evan Halloran,Ryan Lin,Emily Peng,Charis Tsikkou*

Main category: math.AP

TL;DR: Analysis of Riemann solutions for a system with a conservation law and a balance law, featuring non-classical delta shocks, vacuum states, and dynamic solutions due to a time-dependent source term.


<details>
  <summary>Details</summary>
Motivation: To understand transport processes under density constraints, applicable to diverse fields like biology, ecology, and traffic.

Method: Comprehensive analysis of Riemann solutions, including non-self-similar structures, and numerical validation using the Local Lax-Friedrichs scheme.

Result: Discovery of dynamic solution landscapes, including vacuum states and critical density thresholds, with non-classical delta shocks.

Conclusion: The system serves as a versatile prototype for density-constrained transport, with theoretical insights confirmed numerically.

Abstract: We consider a system consisting of one conservation law and one balance law
with a time-dependent source term, and provide a comprehensive analysis of
Riemann solutions, including the non-classical overcompressive delta shocks.
The minimal yet representative structure of the system captures essential
features of transport under density constraints and, despite its simplicity,
serves as a versatile prototype for crowd-limited transport processes across
diverse contexts, including biological aggregation, ecological dispersal,
granular compaction, and traffic congestion. In addition to non-self-similar
solutions mentioned above, the associated Riemann problem admits solution
structures that traverse vacuum states ($\rho = 0$) and the critical density
threshold ($\rho = \bar{\rho}$), where mobility vanishes and characteristic
speed degenerates. Moreover, the explicit time dependence in the source term
leads to the breakdown of self-similarity, resulting in distinct Riemann
solutions over successive time intervals and highlighting the dynamic nature of
the solution landscape. The theoretical findings are numerically confirmed
using the Local Lax-Friedrichs scheme.

</details>


### [17] [Blow-up phenomena for a boundary Yamabe problem with umbilic boundary](https://arxiv.org/abs/2508.10387)
*Giusi Vaira*

Main category: math.AP

TL;DR: The paper explores a linear perturbation of the geometric problem of prescribing scalar and boundary mean curvature in a Riemannian manifold with umbilic boundary, focusing on negative scalar curvature and proving existence of positive solutions for dimensions n‚â•8.


<details>
  <summary>Details</summary>
Motivation: The study aims to extend understanding of geometric problems involving scalar and boundary mean curvature in Riemannian manifolds, particularly when the Weyl tensor is non-zero and the scalar curvature is negative.

Method: The authors employ a linear perturbation approach to address the problem, specifically analyzing the case of negative scalar curvature in manifolds with umbilic boundaries.

Result: The key result is the existence of positive solutions to the problem for dimensions n‚â•8, under the given conditions.

Conclusion: The findings contribute to the broader field of geometric analysis by providing solutions to curvature prescription problems in higher dimensions under specific constraints.

Abstract: We consider a linear perturbation of the classical geometric problem of
prescribing the scalar and the boundary mean curvature problem in a Riemannian
manifold with umbilic boundary provided the Weyl tensor is non-zero everywhere.
We will deal with the case of negative scalar curvature showing the existence
of a positive solutions when $n\geq 8$.

</details>


### [18] [Regularity estimates on harmonic eigenmaps with arbitrary number of coordinates](https://arxiv.org/abs/2508.10448)
*Romain Petrides*

Main category: math.AP

TL;DR: The paper investigates the regularity of harmonic maps into ellipsoids (Laplace harmonic eigenmaps) and their independence from the target manifold's dimension, with implications for eigenvalue optimization.


<details>
  <summary>Details</summary>
Motivation: To explore the dimension independence of regularity estimates for harmonic maps and their relevance in eigenvalue optimization and critical metrics.

Method: Focuses on harmonic maps into ellipsoids, termed Laplace harmonic eigenmaps, and analyzes their properties using tools for handling convergence of almost critical metrics.

Result: Provides insights into the regularity of such maps and their behavior in Palais-Smale sequences, aiding in the study of eigenvalue optimization.

Conclusion: The findings contribute to understanding harmonic maps and lay groundwork for a broader regularity theory for critical points of eigenvalue combinations.

Abstract: We revisit the well-established regularity estimates on harmonic maps on
surfaces to question their independence with respect to the dimension of the
target manifold. We are mainly interested in harmonic maps into target
ellipsoids, that we call Laplace harmonic eigenmaps. These maps are related to
critical metrics in the context of eigenvalue optimization. The tools that we
gather here are useful to handle convergence of almost critical metrics via
Palais-Smale sequences of (almost harmonic) eigenmaps. They could also be a
preliminary step for a general regularity theory for critical points of
infinite combinations of eigenvalues.

</details>


### [19] [On $\mathrm{BV}^{\mathbb{A}}$-Minimisers in two Dimensions](https://arxiv.org/abs/2508.10508)
*Ferdinand Eitler,Peter Lewintan*

Main category: math.AP

TL;DR: Study of regularity and gradient integrability for BV^ùî∏-minimisers of ‚ÑÇ-elliptic operators in 2D.


<details>
  <summary>Details</summary>
Motivation: To understand the regularity properties of minimisers for ‚ÑÇ-elliptic differential operators in two dimensions.

Method: Relies on the special structure of ‚ÑÇ-elliptic operators, building on known ellipticity ranges from the symmetric gradient case.

Result: Gradient integrability is established for the sharp ellipticity range.

Conclusion: The findings confirm the regularity of BV^ùî∏-minimisers for ‚ÑÇ-elliptic operators in 2D.

Abstract: We investigate into the regularity of $\mathrm{BV}^{\mathbb{A}}$-minimisers
for $\mathbb{C}$-elliptic differential operators $\mathbb{A}$ in $2$
dimensions. Our studies strongly rely on the special structure of such
differential operators. The gradient integrability is established for the sharp
ellipticity range known from the (symmetric) gradient case.

</details>


### [20] [Vectorial Double Phase Obstacle Problems](https://arxiv.org/abs/2508.10690)
*Filomena De Filippis,Antonella Nastasi,Cintia Pacchiano Camacho*

Main category: math.AP

TL;DR: Study of partial regularity for vector-valued local minimizers of double phase functionals under vectorial obstacle constraints.


<details>
  <summary>Details</summary>
Motivation: To understand the regularity properties of minimizers in constrained settings, particularly for double phase functionals.

Method: Analysis of vector-valued local minimizers under topological constraints.

Result: Partial regularity results are derived for the minimizers under the given constraints.

Conclusion: The study provides insights into the regularity of solutions in constrained variational problems.

Abstract: We investigate partial regularity for vector valued local minimizers of
double phase functionals, under vectorial obstacle type constraints satisfying
appropriate topological properties.

</details>


### [21] [From Kac particles to the Landau equation with hard potentials: BBGKY hierarchy method](https://arxiv.org/abs/2508.10697)
*Shuchen Guo*

Main category: math.AP

TL;DR: The paper proves propagation of chaos for the Landau equation with hard potentials using a sharper Povzner-type inequality and coupling methods.


<details>
  <summary>Details</summary>
Motivation: To analyze the Kac particle model for the space-homogenous Landau equation with hard potentials and establish uniform-in-time properties.

Method: Uses a sharper Povzner-type inequality to show uniform propagation of exponential moments and coupling methods for uniqueness of weak solutions.

Result: Demonstrates uniform-in-time propagation of exponential moments and proves propagation of chaos for the Landau equation.

Conclusion: The study successfully establishes propagation of chaos for the Landau equation with hard potentials, leveraging key analytical tools.

Abstract: We study the Kac particle model for the space-homogenous Landau equation with
hard potentials. By showing a sharper Povzner-type inequality, we obtain the
uniform-in-time and uniform-in-N propagation of exponential moment for the
first marginal of the solution of the many-particle Liouville equation. This
key property enables us to show the uniqueness of weak solutions of the
corresponding infinite Landau hierarchy by coupling method. As a result, we
prove the propagation of chaos for the Landau equation with hard potentials.

</details>


### [22] [Well-posedness and relaxation in a simplified model for viscoelastic phase separation via Hilbertian gradients flows](https://arxiv.org/abs/2508.10722)
*Moritz Immanuel Gau,Katharina Hopf*

Main category: math.AP

TL;DR: The paper analyzes a gradient-flow approach to a Cahn-Hilliard model for viscoelastic phase separation, proving global well-posedness for moderately regular initial data and existence of gradient-flow solutions for finite-energy data. It also explores asymptotic behavior under parameter scaling.


<details>
  <summary>Details</summary>
Motivation: To address the well-posedness and stability of a Cahn-Hilliard model for viscoelastic phase separation, particularly overcoming challenges like the lack of semiconvexity in the driving functional.

Method: Uses time-incremental minimization and generalized contractivity estimates to establish well-posedness and stability. Studies asymptotic behavior under parameter scaling.

Result: Global well-posedness for moderately regular initial data, existence of gradient-flow solutions for finite-energy data, and recovery of known equations (Cahn-Hilliard, mass-conserving Allen-Cahn, viscous Cahn-Hilliard) under specific scalings.

Conclusion: The gradient-flow approach successfully addresses the model's challenges, providing well-posedness and stability results, and revealing connections to other equations through asymptotic analysis.

Abstract: This article is concerned with a gradient-flow approach to a Cahn-Hilliard
model for viscoelastic phase separation introduced by Zhou et al. (Phys. Rev.
E, 2006) in its variant with constant mobility. By means of time-incremental
minimisation and generalised contractivity estimates, we establish the global
well-posedness of the Cauchy problem for moderately regular initial data. For
general finite-energy data we obtain the existence of gradient-flow solutions
and a stability estimate of weak-strong type. We further study the asymptotic
behaviour for relaxation time and bulk modulus depending on a small parameter.
Depending on the scaling, we recover the Cahn-Hilliard, the mass-conserving
Allen-Cahn or the viscous Cahn-Hilliard equation. A challenge in the
well-posedness analysis is the failure of semiconvexity of the appropriate
driving functional, which is caused by a phase-dependence of the bulk modulus.

</details>


### [23] [$\mathrm{C}^2$ estimates for general $p$-Hessian equations on closed Riemannian manifolds](https://arxiv.org/abs/2508.10773)
*Yuxiang Qiao*

Main category: math.AP

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study the $\mathrm{C}^2$ estimates for $p$-Hessian equations with general
left-hand and right-hand terms on closed Riemannian manifolds of dimension $n$.
To overcome the constraints of closed manifolds, we advance a new kind of
"subsolution", called pseudo-solution, which generalizes
"$\mathcal{C}$-subsolution" to some extent and is well-defined for fully
general $p$-Hessian equations. Based on pseudo-solutions, we prove the
$\mathrm{C}^0$ estimates, first-order estimates for general $p$-Hessian
equations, and the corresponding second-order estimates when $p\in\{2, n-1,
n\}$, under sharp conditions -- we don't impose curvature restrictions,
convexity conditions or "MTW condition" on our main results. Some other
conclusions related to a priori estimates and different kinds of "subsolutions"
are also given, including estimates for "semi-convex" solutions and when there
exists a pseudo-solution.

</details>


### [24] [Upper bound on heat kernels of finite particle systems of Keller-Segel type](https://arxiv.org/abs/2508.10892)
*S. E. Boutiah,D. Kinzebulatov*

Main category: math.AP

TL;DR: Upper bound on the heat kernel of Keller-Segel finite particle system, showing blow-up effects.


<details>
  <summary>Details</summary>
Motivation: To understand the critical behavior of the Keller-Segel system in two dimensions.

Method: Exploits a connection between Keller-Segel finite particles and non-local operators.

Result: Established an upper bound on the heat kernel, revealing blow-up effects.

Conclusion: The approach provides insights into the critical behavior of the Keller-Segel system due to its two-dimensionality.

Abstract: We obtain an upper bound on the heat kernel of the Keller-Segel finite
particle system that exhibits blow up effects. The proof exploits a connection
between Keller-Segel finite particles and certain non-local operators. The
latter allows to address some aspects of the critical behaviour of the
Keller-Segel system resulting from its two-dimensionality.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [25] [MCP-Enabled LLM for Meta-optics Inverse Design: Leveraging Differentiable Solver without LLM Expertise](https://arxiv.org/abs/2508.10277)
*Yi Huang,Bowen Zheng,Yunxi Dong,Hong Tang,Huan Zhao,Rakibul Hasan Shawon,Sensong An,Hualiang Zhang*

Main category: physics.comp-ph

TL;DR: A framework using Model Context Protocol (MCP) and LLMs simplifies inverse design for metasurfaces by dynamically accessing verified code templates, eliminating the need for programming expertise.


<details>
  <summary>Details</summary>
Motivation: To make automatic differentiation (AD) and inverse design accessible to researchers lacking theoretical and programming expertise.

Method: Proposes MCP-assisted framework with LLMs, providing dynamic access to verified code templates and documentation for generating inverse design codes.

Result: Structured prompting outperforms natural language in design quality, efficiency, cost, and error reduction. Minimalist server design (5 APIs) proves effective.

Conclusion: MCP democratizes sophisticated computational tools, offering a generalizable solution for scientific tasks beyond metasurface design.

Abstract: Automatic differentiation (AD) enables powerful metasurface inverse design
but requires extensive theoretical and programming expertise. We present a
Model Context Protocol (MCP) assisted framework that allows researchers to
conduct inverse design with differentiable solvers through large language
models (LLMs). Since LLMs inherently lack knowledge of specialized solvers, our
proposed solution provides dynamic access to verified code templates and
comprehensive documentation through dedicated servers. The LLM autonomously
accesses these resources to generate complete inverse design codes without
prescribed coordination rules. Evaluation on the Huygens meta-atom design task
with the differentiable TorchRDIT solver shows that while both natural language
and structured prompting strategies achieve high success rates, structured
prompting significantly outperforms in design quality, workflow efficiency,
computational cost, and error reduction. The minimalist server design, using
only 5 APIs, demonstrates how MCP makes sophisticated computational tools
accessible to researchers without programming expertise, offering a
generalizable integration solution for other scientific tasks.

</details>


### [26] [Sum-of-Gaussians tensor neural networks for high-dimensional Schr√∂dinger equation](https://arxiv.org/abs/2508.10454)
*Qi Zhou,Teng Wu,Jianghao Liu,Qingyuan Sun,Hehu Xie,Zhenli Xu*

Main category: physics.comp-ph

TL;DR: The paper introduces SOG-TNN, a tensor neural network method for solving high-dimensional Schr√∂dinger equations efficiently by decomposing the Coulomb interaction into separable components and using range-splitting techniques.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the curse of dimensionality in solving high-dimensional Schr√∂dinger equations and handle the computational challenges of Coulomb interactions.

Method: The method involves a sum-of-Gaussians tensor neural network (SOG-TNN) with low-rank tensor representation, SOG decomposition for Coulomb interaction, and range-splitting for efficient computation.

Result: Numerical results show SOG-TNN performs outstandingly, efficiently solving high-dimensional problems.

Conclusion: SOG-TNN is a promising approach for large and complex quantum systems.

Abstract: We propose an accurate, efficient, and low-memory sum-of-Gaussians tensor
neural network (SOG-TNN) algorithm for solving the high-dimensional
Schr\"odinger equation. The SOG-TNN utilizes a low-rank tensor product
representation of the solution to overcome the curse of dimensionality
associated with high-dimensional integration. To handle the Coulomb
interaction, we introduce an SOG decomposition to approximate the interaction
kernel such that it is dimensionally separable, leading to a tensor
representation with rapid convergence. We further develop a range-splitting
scheme that partitions the Gaussian terms into short-, long-, and mid-range
components. They are treated with the asymptotic expansion, the low-rank
Chebyshev expansion, and the model reduction with singular-value decomposition,
respectively, significantly reducing the number of two-dimensional integrals in
computing electron-electron interactions. The SOG decomposition well resolves
the computational challenge due to the singularity of the Coulomb interaction,
leading to an efficient algorithm for the high-dimensional problem under the
TNN framework. Numerical results demonstrate the outstanding performance of the
new method, revealing that the SOG-TNN is a promising way for tackling large
and complex quantum systems.

</details>


### [27] [Virtual Sensing for Solder Layer Degradation and Temperature Monitoring in IGBT Modules](https://arxiv.org/abs/2508.10515)
*Andrea Urgolo,Monika Stipsitz,Helios Sanchis-Alepuz*

Main category: physics.comp-ph

TL;DR: The paper proposes a machine learning-based virtual sensing method to estimate IGBT module degradation, achieving high accuracy in predicting solder degradation and temperature maps.


<details>
  <summary>Details</summary>
Motivation: Direct measurement of IGBT degradation indicators is challenging due to physical inaccessibility and harsh environments, necessitating alternative methods.

Method: Uses synthetic data and machine learning to estimate solder layer degradation and full temperature maps from limited physical sensors.

Result: Achieves 1.17% mean absolute error in degraded solder area estimation and 4.56% maximum relative error in temperature reproduction.

Conclusion: Machine learning-based virtual sensing is feasible and accurate for monitoring IGBT degradation, offering a practical alternative to direct measurements.

Abstract: Monitoring the degradation state of Insulated Gate Bipolar Transistor (IGBT)
modules is essential for ensuring the reliability and longevity of power
electronic systems, especially in safety-critical and high-performance
applications. However, direct measurement of key degradation indicators - such
as junction temperature, solder fatigue or delamination - remains challenging
due to the physical inaccessibility of internal components and the harsh
environment. In this context, machine learning-based virtual sensing offers a
promising alternative by bridging the gap from feasible sensor placement to the
relevant but inaccessible locations. This paper explores the feasibility of
estimating the degradation state of solder layers, and the corresponding full
temperature maps based on a limited number of physical sensors. Based on
synthetic data of a specific degradation mode, we obtain a high accuracy in the
estimation of the degraded solder area (1.17% mean absolute error), and are
able to reproduce the surface temperature of the IGBT with a maximum relative
error of 4.56% (corresponding to an average relative error of 0.37%).

</details>


### [28] [Physics-Informed Deep Contrast Source Inversion: A Unified Framework for Inverse Scattering Problems](https://arxiv.org/abs/2508.10555)
*Haoran Sun,Daoqi Liu,Hongyu Zhou,Maokun Li,Shenheng Xu,Fan Yang*

Main category: physics.comp-ph

TL;DR: DeepCSI, a physics-informed deep contrast source inversion framework, linearizes nonlinear inverse scattering problems using ResMLP, reducing computational costs and achieving high-precision reconstructions across diverse measurement scenarios.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of nonlinearity and diverse measurement conditions in inverse scattering problems, crucial for electromagnetic imaging and medical diagnostics.

Method: Proposes DeepCSI, combining contrast source inversion and neural operator methods with ResMLP to model current distributions, using a hybrid loss function for joint optimization of network and medium parameters.

Result: Outperforms traditional CSI methods, achieving robust reconstructions under full-data, phaseless, and multi-frequency conditions.

Conclusion: DeepCSI provides an efficient, universal solution for complex inverse scattering problems, simplifying modeling and improving accuracy.

Abstract: Inverse scattering problems are critical in electromagnetic imaging and
medical diagnostics but are challenged by their nonlinearity and diverse
measurement scenarios. This paper proposes a physics-informed deep contrast
source inversion framework (DeepCSI) for fast and accurate medium
reconstruction across various measurement conditions. Inspired by contrast
source inversion (CSI) and neural operator methods, a residual multilayer
perceptron (ResMLP) is employed to model current distributions in the region of
interest under different transmitter excitations, effectively linearizing the
nonlinear inverse scattering problem and significantly reducing the
computational cost of traditional full-waveform inversion. By modeling medium
parameters as learnable tensors and utilizing a hybrid loss function that
integrates state equation loss, data equation loss, and total variation
regularization, DeepCSI establishes a fully differentiable framework for joint
optimization of network parameters and medium properties. Compared with
conventional methods, DeepCSI offers advantages in terms of simplicity and
universal modeling capabilities for diverse measurement scenarios, including
phase-less and multi-frequency observation. Simulations and experiments
demonstrate that DeepCSI achieves high-precision, robust reconstruction under
full-data, phaseless data, and multifrequency conditions, outperforming
traditional CSI methods and providing an efficient and universal solution for
complex inverse scattering problems.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [29] [FreeBird.jl: An Extensible Toolbox for Simulating Interfacial Phase Equilibria](https://arxiv.org/abs/2508.10237)
*Ray Yang,Junchi Chen,Douglas Thibodeaux,Robert B. Wexler*

Main category: cond-mat.stat-mech

TL;DR: FreeBird.jl is a Julia-based platform for computational studies of phase equilibria at interfaces, supporting various system configurations and sampling algorithms.


<details>
  <summary>Details</summary>
Motivation: To provide a flexible and extensible tool for studying interfacial thermodynamics across diverse systems and methods.

Method: Uses Julia's type hierarchies and multiple dispatch for modular integration of system definitions, energy evaluators, and sampling schemes (e.g., nested sampling, Wang-Landau, Metropolis Monte Carlo).

Result: A versatile framework capable of handling atomistic and lattice systems with customizable geometries and energy evaluations.

Conclusion: FreeBird.jl offers a powerful, adaptable solution for computational studies of interfacial phenomena.

Abstract: We present FreeBird.jl, an extensible Julia-based platform for computational
studies of phase equilibria at generic interfaces. The package supports a range
of system configurations, from atomistic solid surfaces to coarse-grained
lattice$-$gas models, with energies evaluated using classical interatomic
potentials or lattice Hamiltonians. Both atomistic and lattice systems
accommodate single- or multi-component mixtures with flexibly definable surface
and lattice geometries. Implemented sampling algorithms include nested
sampling, Wang$-$Landau sampling, Metropolis Monte Carlo, and, for tractable
lattice systems, exact enumeration. Leveraging Julia's type hierarchies and
multiple dispatch, FreeBird.jl provides a modular interface that allows
seamless integration of system definitions, energy evaluators, and sampling
schemes. Designed for flexibility, extensibility, and performance, FreeBird.jl
offers a versatile framework for exploring the thermodynamics of interfacial
phenomena.

</details>


### [30] [Variational boundary based tensor network renormalization group](https://arxiv.org/abs/2508.10418)
*Feng-Feng Song,Naoki Kawashima*

Main category: cond-mat.stat-mech

TL;DR: A new real-space renormalization group algorithm for 2D tensor networks uses variational boundary tensors for global optimization, improving accuracy without increasing computational complexity.


<details>
  <summary>Details</summary>
Motivation: To enhance the accuracy of tensor renormalization group (TRG) methods in higher dimensions while maintaining manageable computational costs.

Method: Utilizes variational boundary tensors for global optimization and constructs renormalization projectors based on the canonical form of tensors.

Result: Achieves higher accuracy than existing TRG methods without entanglement filtering, with the same computational complexity.

Conclusion: Provides a practical approach for extending TRG methods to higher dimensions efficiently.

Abstract: We propose a real-space renormalization group algorithm for accurately
coarse-graining two-dimensional tensor networks. The central innovation of our
method lies in utilizing variational boundary tensors as a globally optimized
environment for the entire system. Based on this optimized environment, we
construct renormalization projectors that significantly enhance accuracy. By
leveraging the canonical form of tensors, our algorithm maintains the same
computational complexity as the original tensor renormalization group (TRG)
method, yet achieves higher accuracy than existing approaches that do not
incorporate entanglement filtering. Our work offers a practical pathway for
extending TRG methods to higher dimensions while keeping computational costs
manageable.

</details>


<div id='cs.MS'></div>

# cs.MS [[Back]](#toc)

### [31] [Concepts for Composing Finite Element Function Space Bases](https://arxiv.org/abs/2508.10125)
*Christian Engwer,Carsten Gr√§ser,Steffen M√ºthing,Simon Praetorius,Oliver Sander*

Main category: cs.MS

TL;DR: The paper discusses software abstractions for handling composed function spaces in finite element discretizations, using tree structures for product spaces to enable flexible degree-of-freedom numbering and compatibility with diverse linear algebra tools.


<details>
  <summary>Details</summary>
Motivation: To address the need for adaptable function space representations in multi-physics PDE models, enabling compatibility with various data layouts and solvers.

Method: Representation of product spaces as trees of simpler bases, allowing derivation of multi-index degree-of-freedom numberings. The stationary Stokes equation with Taylor-Hood elements serves as an example.

Result: The approach facilitates flexible function space adaptation for diverse data structures and algebraic solvers, demonstrated in the DUNE module dune-functions.

Conclusion: The proposed abstractions and software concepts enhance the versatility of finite element codes for multi-physics applications, as showcased in the DUNE ecosystem.

Abstract: Finite Element discretizations of coupled multi-physics partial differential
equation models require the handling of composed function spaces. In this paper
we discuss software concepts and abstractions to handle the composition of
function spaces, based on a representation of product spaces as trees of
simpler bases. From this description, many different numberings of degrees of
freedom by multi-indices can be derived in a natural way, allowing to adapt the
function spaces to very different data layouts, so that it opens the
possibility to directly use the finite element code with very different linear
algebra codes, different data structures, and different algebraic solvers.
  A recurring example throughout the paper is the stationary Stokes equation
with Taylor--Hood elements as these are naturally formulated as product spaces
and highlight why different storage patterns are desirable.
  In the second half of the paper we discuss a particular realization of most
of these concepts in the \dunemodule{dune-functions} module, as part of the
DUNE ecosystem.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [32] [AEGISS -- Atomic orbital and Entropy-based Guided Inference for Space Selection -- A novel semi-automated active space selection workflow for quantum chemistry and quantum computing applications](https://arxiv.org/abs/2508.10671)
*Fabio Tarocco,Pi A. B. Haase,Fabijan Pavo≈°eviƒá,Vijay Krishna,Leonardo Guidoni,Stefan Knecht,Martina Stella*

Main category: physics.chem-ph

TL;DR: A novel automated method for selecting balanced active spaces in quantum chemistry, combining orbital entropy analysis and atomic orbital projections, validated on Ru(II)-complexes for photodynamic therapy.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of a unified, robust, and automated framework for active space selection in systems with strong electron correlation, crucial for both classical and quantum computing applications.

Method: Integrates orbital entropy analysis with atomic orbital projections, inspired by AVAS and AutoCAS, to guide the construction of chemically meaningful active spaces.

Result: Reliably identifies compact, chemically intuitive active spaces for challenging systems like Ru(II)-complexes, capturing essential physics.

Conclusion: The method is scalable, automated, and suitable for both classical and quantum computational frameworks, with a user-friendly package for broad accessibility.

Abstract: The selection of a balanced active space is a critical step in
multi-reference quantum chemistry calculations, particularly for systems with
strong electron correlation. Likewise, active space selection is a key to
unlock the potential of contemporary quantum computing in quantum chemistry.
Albeit recent progress, there remains a lack of a unified, robust, and fully
automated framework for active space selection that performs reliably across a
wide range of molecular systems.
  In this work, we present a novel approach inspired by both the AVAS (Atomic
Valence Active Space) and AutoCAS methods. Our method unifies orbital entropy
analysis with atomic orbital projections to guide the construction of
chemically and physically meaningful active spaces. This integrated scheme
enables a more consistent and flexible selection of active orbitals while
retaining automation and scalability. We validate our approach on a set of
molecular systems relevant to photodynamic therapy, in particular a set of
Ru(II)-complexes, selected to span increasing levels of electron correlation
and structural complexity. These molecules serve as challenging test cases due
to the presence of strong static correlation and the need for highly accurate
electronic structure descriptions. Our results demonstrate that the method can
reliably identify compact, chemically intuitive active spaces that capture the
essential physics, making it suitable for both classical and quantum
computational frameworks.
  Furthermore, we have developed this approach in a package that is intuitive
to use for users and can be interfaced with both standard quantum chemistry and
quantum computing applications, making it accessible to a broad research
community.

</details>


### [33] [Performance of universal machine-learned potentials with explicit long-range interactions in biomolecular simulations](https://arxiv.org/abs/2508.10841)
*Viktor Zaverkin,Matheus Ferraz,Francesco Alesiani,Mathias Niepert*

Main category: physics.chem-ph

TL;DR: The paper evaluates machine-learned potentials for biomolecular simulations, focusing on model size, training data, and electrostatic effects, finding challenges in transferability and evaluation practices.


<details>
  <summary>Details</summary>
Motivation: To assess the applicability of universal machine-learned potentials in biomolecular simulations, addressing gaps in transferability and evaluation.

Method: Systematic evaluation of equivariant message-passing architectures trained on SPICE-v2 dataset, testing model size, training data composition, and electrostatic treatments.

Result: Larger models improve benchmark accuracy but not simulation properties. Training data composition affects predictions. Long-range electrostatics show inconsistent impact, except for Trp-cage.

Conclusion: Imbalanced datasets and immature evaluation practices limit the current applicability of universal machine-learned potentials in biomolecular simulations.

Abstract: Universal machine-learned potentials promise transferable accuracy across
compositional and vibrational degrees of freedom, yet their application to
biomolecular simulations remains underexplored. This work systematically
evaluates equivariant message-passing architectures trained on the SPICE-v2
dataset with and without explicit long-range dispersion and electrostatics. We
assess the impact of model size, training data composition, and electrostatic
treatment across in- and out-of-distribution benchmark datasets, as well as
molecular simulations of bulk liquid water, aqueous NaCl solutions, and
biomolecules, including alanine tripeptide, the mini-protein Trp-cage, and
Crambin. While larger models improve accuracy on benchmark datasets, this trend
does not consistently extend to properties obtained from simulations. Predicted
properties also depend on the composition of the training dataset. Long-range
electrostatics show no systematic impact across systems. However, for Trp-cage,
their inclusion yields increased conformational variability. Our results
suggest that imbalanced datasets and immature evaluation practices currently
challenge the applicability of universal machine-learned potentials to
biomolecular simulations.

</details>


<div id='physics.bio-ph'></div>

# physics.bio-ph [[Back]](#toc)

### [34] [Run-and-Tumble Escape in Pursuit-Evasion Dynamics of Intelligent Active Particles](https://arxiv.org/abs/2508.10727)
*Segun Goh,Dennis Haustein,Gerhard Gompper*

Main category: physics.bio-ph

TL;DR: The paper studies a pursuit-evasion game between a deterministic pursuer and a stochastic evader, analyzing strategies and outcomes in 2D simulations.


<details>
  <summary>Details</summary>
Motivation: To understand adversarial dynamics between a pursuer with limited maneuverability and an evader with unpredictable, threat-responsive behavior, and to inform bioinspired robotic designs.

Method: Numerical simulations in two spatial dimensions model the pursuer's deterministic steering and the evader's stochastic, distance-dependent tumbling behavior.

Result: Two key scenarios emerge: high-risk backward maneuvers for dominant pursuers and forward tumbling with slight adjustments to prolong capture time.

Conclusion: The findings provide insights for designing agile robotic systems with effective evasion strategies.

Abstract: The pursuit-evasion game is studied for two adversarial active agents,
modelled as a deterministic self-steering pursuer and a stochastic, cognitive
evader. The pursuer chases the evader by reorienting its propulsion direction
with limited maneuverability, while the evader escapes by executing sharp,
unpredictable turns, whose timing and direction the pursuer cannot anticipate.
To make the target responsive and agile when the threat level is high, the
tumbling frequency is set to increase with decreasing distance from the
pursuer; furthermore, the range of preferred tumbling directions is varied.
Numerical simulations of such a pursuit-target pair in two spatial dimensions
reveal two important scenarios. For dominant pursuers, the evader is compelled
to adopt a high-risk strategy that allows the pursuer to approach closely
before the evader executes a potentially game-changing backward maneuver to
pull away from the pursuer. Otherwise, a strategy where the evader tumbles
forward with continuous slight adjustments of the propulsion direction can
significantly increase the capture time by preventing the pursuer from aligning
with the target propulsion direction, while maintaining the persistence of the
target motion. Our results can guide the design of bioinspired robotic systems
with efficient evasion capabilities.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [35] [Mixed-Precision Performance Portability of FFT-Based GPU-Accelerated Algorithms for Block-Triangular Toeplitz Matrices](https://arxiv.org/abs/2508.10202)
*Sreeram Venkat,Kasia Swirydowicz,Noah Wolfe,Omar Ghattas*

Main category: cs.DC

TL;DR: A framework using Hipify enables performance portability for FFTMatvec on AMD GPUs, with mixed-precision optimization for HPC workflows.


<details>
  <summary>Details</summary>
Motivation: Leveraging hardware diversity and GPU performance in lower precision for scientific HPC workflows.

Method: On-the-fly Hipify framework for porting CUDA-only FFTMatvec to AMD GPUs, integrating optimizations into rocBLAS, and dynamic mixed-precision analysis.

Result: Seamless performance on AMD GPUs (MI250X, MI300X, MI355X) and scaling to 2,048 GPUs on Frontier.

Conclusion: The framework successfully combines performance portability and mixed-precision optimization for HPC applications.

Abstract: The hardware diversity displayed in leadership-class computing facilities,
alongside the immense performance boosts exhibited by today's GPUs when
computing in lower precision, provide a strong incentive for scientific HPC
workflows to adopt mixed-precision algorithms and performance portability
models. We present an on-the-fly framework using Hipify for performance
portability and apply it to FFTMatvec-an HPC application that computes
matrix-vector products with block-triangular Toeplitz matrices. Our approach
enables FFTMatvec, initially a CUDA-only application, to run seamlessly on AMD
GPUs with excellent observed performance. Performance optimizations for AMD
GPUs are integrated directly into the open-source rocBLAS library, keeping the
application code unchanged. We then present a dynamic mixed-precision framework
for FFTMatvec; a Pareto front analysis determines the optimal mixed-precision
configuration for a desired error tolerance. Results are shown for AMD Instinct
MI250X, MI300X, and the newly launched MI355X GPUs. The performance-portable,
mixed-precision FFTMatvec is scaled to 2,048 GPUs on the OLCF Frontier
supercomputer.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [36] [TOBACO: Topology Optimization via Band-limited Coordinate Networks for Compositionally Graded Alloys](https://arxiv.org/abs/2508.10320)
*Aaditya Chandrasekhar,Stefan Knapik,Deepak Sharma,John Reidy,Ian McCue,Jian Cao,Wei Chen*

Main category: cs.CE

TL;DR: A topology optimization framework using neural networks for designing Compositionally Graded Alloys (CGAs) with controlled gradation, leveraging additive manufacturing advancements.


<details>
  <summary>Details</summary>
Motivation: CGAs offer superior material properties but require controlled gradation during additive manufacturing, necessitating a design framework to optimize composition distribution.

Method: Uses a band-limited coordinate neural network to represent composition distribution, ensuring compliance with gradation limits without explicit constraints. Benefits include mesh independence and high-resolution design.

Result: Demonstrated effectiveness through elastic and thermo-elastic topology optimization examples.

Conclusion: The framework successfully designs optimized CGA components with controlled gradation, leveraging neural networks and additive manufacturing capabilities.

Abstract: Compositionally Graded Alloys (CGAs) offer unprecedented design flexibility
by enabling spatial variations in composition; tailoring material properties to
local loading conditions. This flexibility leads to components that are
stronger, lighter, and more cost-effective than traditional monolithic
counterparts. The fabrication of CGAs have become increasingly feasible owing
to recent advancements in additive manufacturing (AM), particularly in
multi-material printing and improved precision in material deposition. However,
AM of CGAs requires imposition of manufacturing constraints; in particular
limits on the maximum spatial gradation of composition.
  This paper introduces a topology optimization (TO) based framework for
designing optimized CGA components with controlled compositional gradation. In
particular, we represent the constrained composition distribution using a
band-limited coordinate neural network. By regulating the network's bandwidth,
we ensure implicit compliance with gradation limits, eliminating the need for
explicit constraints. The proposed approach also benefits from the inherent
advantages of TO using coordinate networks, including mesh independence,
high-resolution design extraction, and end-to-end differentiability. The
effectiveness of our framework is demonstrated through various elastic and
thermo-elastic TO examples.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [37] [Compressible boundary layers over isotropic porous surfaces](https://arxiv.org/abs/2508.10422)
*Ludovico Foss√†,Pierre Ricco*

Main category: physics.flu-dyn

TL;DR: The paper investigates a compressible laminar boundary layer over an isotropic porous substrate using asymptotic and numerical methods, extending Tsiberkin's self-similar solution to include compressibility, heat conduction, and nonlinear drag.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of compressible laminar boundary layers over porous substrates, including effects of compressibility, heat conduction, and nonlinear drag.

Method: Asymptotic and numerical methods are used to analyze the boundary layer, extending Tsiberkin's self-similar solution with additional factors.

Result: The velocity profile shows an inflection point at the interface, with reduced adiabatic recovery temperature and velocity gradient for high porosity, large grains, and high Mach numbers. Bottom temperature has negligible impact on shear stresses.

Conclusion: The study highlights the influence of porosity, grain size, and Mach number on boundary layer behavior, with minimal effect from substrate temperature on shear stresses.

Abstract: A compressible laminar boundary layer developing over an isotropic porous
substrate is investigated by asymptotic and numerical methods. The substrate is
modeled as an array of cubes. The momentum and enthalpy balance equations are
derived by volume averaging. The self-similar solution proposed by Tsiberkin
(2018) [Transp. Porous Media 121(1):109-120] for streamwise-growing
permeability is extended to include compressibility, heat conduction and a
nonlinear drag. The velocity profile shows an inflection point at the free
fluid-porous interfacial layer, below which it decreases to zero. A marked
reduction of the adiabatic recovery temperature of the fluid and the velocity
gradient at the interface is observed for high porosity, large grains and
relatively high Mach numbers. The temperature imposed at the bottom of the
porous substrate has a negligible influence on the shear stresses.

</details>


### [38] [Relative accuracy of turbulence simulations using pseudo-spectral and finite difference solvers](https://arxiv.org/abs/2508.10808)
*Akash Rodhiya,Shashwat Bhattacharya,Mahendra K Verma*

Main category: physics.flu-dyn

TL;DR: Spectral and finite-difference methods yield similar accuracy in turbulence simulations despite spectral methods being more accurate per timestep.


<details>
  <summary>Details</summary>
Motivation: To compare the accuracy of spectral and finite-difference methods in turbulence simulations, challenging the assumption that spectral methods are always superior.

Method: Simulated forced hydrodynamic turbulence on a 256¬≥ grid for varying Reynolds numbers, comparing energy evolution, flow profiles, and statistical measures.

Result: Both methods produced nearly identical results in energy, spectra, flux, and velocity distributions, suggesting error cancellation in turbulence attractors.

Conclusion: Finite-difference methods are equally effective for turbulence simulations and more efficient for large grids, making them a practical alternative to spectral methods.

Abstract: For a single timestep, a spectral solver is known to be more accurate than
its finite-difference counterpart. However, as we show in this paper,
turbulence simulations using the two methods have nearly the same accuracy. In
this paper, we simulate forced hydrodynamic turbulence on a uniform 256$^3$
grid for Reynolds numbers 965, 1231, 1515, and 1994. We show that the two
methods yield nearly the same evolution for the total energy and the flow
profiles. In addition, the steady-state energy spectrum, energy flux, and
probability distribution functions of the velocity and its derivatives are very
similar. We argue that within a turbulence attractor, the numerical errors are
likely to get cancelled (rather than get added up), which leads to similar
results for the finite-difference and spectral methods. These findings are very
valuable, considering that a parallel finite-difference simulation is more
versatile and efficient (for large grids) than its spectral counterpart.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [39] [Type-I Multiferroic VHfO$_4$ with Strain-Switchable Magnetic Orders and Magnetoelectric Coupling](https://arxiv.org/abs/2508.10380)
*Qisheng Yu,Boyu Liu,Hongjun Xiang,Shi Liu*

Main category: cond-mat.mtrl-sci

TL;DR: The paper proposes VHfO$_4$ as a novel multiferroic oxide, combining ferroelectric and magnetic properties, with strain-tunable magnetoelectric coupling for spintronics applications.


<details>
  <summary>Details</summary>
Motivation: To leverage the complementary properties of vanadium-based ferromagnets and HfO$_2$-based ferroelectrics for creating a new multiferroic material.

Method: First-principles DFT calculations and parallel tempering Monte Carlo simulations were used to study the stability, ferroic orders, and strain effects on VHfO$_4$.

Result: VHfO$_4$ exhibits dynamic stability, robust ferroelectric polarization, and V-driven magnetism. Strain engineering enables tunable magnetoelectric coupling and multiple magnetic states.

Conclusion: VHfO$_4$ is a Type-I multiferroic with coexisting ferroic orders and strain-tunable cross-coupling, promising for voltage-controlled spintronics.

Abstract: Motivated by the complementary properties of vanadium-based ferromagnets and
HfO$_2$-based ferroelectrics, we propose a novel multiferroic oxide, VHfO$_4$,
through 50\% Hf$^{4+}$ substitution with V$^{4+}$ in the ferroelectric $Pca2_1$
phase of HfO$_2$. First-principles DFT calculations reveal that the
$Pca2_1$-like VHfO$_4$ phase exhibits dynamic stability and concurrent ferroic
orders: robust ferroelectric polarization comparable to HfO$_2$ and V-driven
magnetism. Parallel tempering Monte Carlo simulations identify an
antiferromagnetic ground state, while strain engineering enables tunable
magnetoelectric coupling. Biaxial in-plane strain induces four magnetic states:
intralayer FM/interlayer AFM, intralayer AFM/interlayer FM, spiral-like
non-collinear order, and discrete alternating spin alignment. Critically,
$c$-axis strain modulates magnetic energy landscapes, demonstrating
electromechanical control of magnetism. This work establishes VHfO$_4$ as a
Type-I multiferroic with coexisting atomic-scale ferroic origins and
strain-tunable cross-coupling, offering a platform for voltage-controlled
spintronics devices.

</details>


### [40] [FastTrack: a fast method to evaluate mass transport in solid leveraging universal machine learning interatomic potential](https://arxiv.org/abs/2508.10505)
*Hanwen Kang,Tenglong Lu,Zhanbin Qi,Jiandong Guo,Sheng Meng,Miao Liu*

Main category: cond-mat.mtrl-sci

TL;DR: A fast, accurate framework using MLFFs for atomic migration barrier computation in crystals, achieving high speed and accuracy compared to DFT and experiments.


<details>
  <summary>Details</summary>
Motivation: To address the computational inefficiency of DFT-NEB methods while maintaining accuracy in predicting atomic migration barriers in materials.

Method: Combines MLFFs with 3D potential energy surface sampling, supercell expansion to suppress periodic self-interactions, and extracts minimum energy pathways without predefined NEB images.

Result: MLFF-derived barriers match DFT and experimental results within tens of meV, with ~100x speedups over DFT-NEB. Fine-tuning on PBE/PBE+U data improves accuracy.

Conclusion: The framework offers a rapid, accurate alternative to DFT-NEB, with potential for high-throughput materials screening and PES visualization.

Abstract: We introduce a rapid, accurate framework for computing atomic migration
barriers in crystals by combining universal machine learning force fields
(MLFFs) with 3D potential energy surface sampling and interpolation. Our method
suppresses periodic self interactions via supercell expansion, builds a
continuous PES from MLFF energies on a spatial grid, and extracts minimum
energy pathways without predefined NEB images. Across twelve benchmark
electrode and electrolyte materials including LiCoO2, LiFePO4, and LGPS our
MLFF-derived barriers lie within tens of meV of DFT and experiment, while
achieving ~10^2 x speedups over DFT-NEB. We benchmark GPTFF, CHGNet, and MACE,
show that fine-tuning on PBE/PBE+U data further enhances accuracy, and provide
an open-source package for high-throughput materials screening and interactive
PES visualization.

</details>


### [41] [Symmetry-Constrained Multi-Scale Physics-Informed Neural Networks for Graphene Electronic Band Structure Prediction](https://arxiv.org/abs/2508.10718)
*Wei Shan Lee,I Hang Kwok,Kam Ian Leong,Chi Kiu Althina Chau,Kei Chon Sio*

Main category: cond-mat.mtrl-sci

TL;DR: SCMS-PINN v35 predicts graphene band structures with high accuracy by enforcing crystallographic symmetries and using multi-head ResNet-6 pathways.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle to balance computational efficiency and accuracy in predicting electronic band structures of 2D materials.

Method: Uses Symmetry-Constrained Multi-Scale Physics-Informed Neural Network (SCMS-PINN) with three ResNet-6 pathways (K-head, M-head, General head) and progressive Dirac constraint scheduling.

Result: Achieves 99.99% training loss reduction, predicts Dirac point gaps within 30.3 ¬µeV, and enforces all twelve C6v operations.

Conclusion: The framework enables accurate band structure prediction and can be extended to other 2D materials.

Abstract: Accurate prediction of electronic band structures in two-dimensional
materials remains a fundamental challenge, with existing methods struggling to
balance computational efficiency and physical accuracy. We present the
Symmetry-Constrained Multi-Scale Physics-Informed Neural Network (SCMS-PINN)
v35, which directly learns graphene band structures while rigorously enforcing
crystallographic symmetries through a multi-head architecture. Our approach
introduces three specialized ResNet-6 pathways -- K-head for Dirac physics,
M-head for saddle points, and General head for smooth interpolation --
operating on 31 physics-informed features extracted from k-points. Progressive
Dirac constraint scheduling systematically increases the weight parameter from
5.0 to 25.0, enabling hierarchical learning from global topology to local
critical physics. Training on 10,000 k-points over 300 epochs achieves 99.99\%
reduction in training loss (34.597 to 0.003) with validation loss of 0.0085.
The model predicts Dirac point gaps within 30.3 $\mu$eV of theoretical zero and
achieves average errors of 53.9 meV (valence) and 40.5 meV (conduction) across
the Brillouin zone. All twelve C$_{6v}$ operations are enforced through
systematic averaging, guaranteeing exact symmetry preservation. This framework
establishes a foundation for extending physics-informed learning to broader
two-dimensional materials for accelerated discovery.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [42] [Simulating Mass-Dependent Decoherence in Quantum Computers: Baseline Signatures for Testing Gravity-Induced Collapse](https://arxiv.org/abs/2508.10590)
*Viswak R Balaji,Samuel Punch*

Main category: quant-ph

TL;DR: Simulation study of mass-dependent decoherence models inspired by Penrose's gravity-induced collapse hypothesis, using Qiskit AerSimulator to test collapse signatures in quantum computing experiments.


<details>
  <summary>Details</summary>
Motivation: To explore whether quantum superpositions collapse due to gravitational effects, as suggested by objective reduction (OR) theory, and to provide a protocol for testing this in quantum computers.

Method: Implemented a mass-dependent dephasing noise channel in Qiskit AerSimulator, applied to GHZ state parity measurements, branch-mass entanglement tests, and Grover's search.

Result: Generated distinctive collapse signatures differing from constant-rate dephasing, serving as a reference for future hardware experiments.

Conclusion: The study offers a reproducible protocol for using quantum computers to probe fundamental questions in quantum mechanics, with potential implications for gravity-induced collapse theories.

Abstract: We present a quantum computing simulation study of mass-dependent decoherence
models inspired by Penrose's gravity-induced collapse hypothesis. According to
objective reduction (OR) theory, quantum superpositions become unstable when
the gravitational self-energy difference between branches exceeds a certain
threshold, leading to a collapse time $\tau \approx \hbar / E_G$. In this work,
we implement a mass-dependent dephasing noise channel, $p(m) = 1 - e^{-k
m^{\alpha}}$, within the Qiskit AerSimulator, where $m$ is a proxy for the
effective mass of a superposition, mapped to circuit parameters such as the
number of entangled qubits or branch size. We apply this model to three
canonical quantum computing experiments: GHZ state parity measurements,
branch-mass entanglement tests, and Grover's search to generate distinctive
collapse signatures that differ qualitatively from constant-rate dephasing. The
resulting patterns serve as a baseline reference: if future hardware
experiments exhibit the same scaling trends under ideal isolation, this could
indicate a contribution from mass-dependent collapse processes. Conversely,
deviation toward constant-noise behaviour would suggest the absence of such
gravitationally induced effects. Our results provide a reproducible protocol
and reference for using quantum computers as potential testbeds for probing
fundamental questions in quantum mechanics.

</details>


### [43] [Deep Learning in Classical and Quantum Physics](https://arxiv.org/abs/2508.10666)
*Timothy Heightman,Marcin P≈Çodzie≈Ñ*

Main category: quant-ph

TL;DR: The paper discusses the transformative role of deep learning (DL) in quantum science, highlighting its benefits and risks, and provides a graduate-level guide for its responsible application.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the growing importance of DL in quantum science and technology, emphasizing its potential to solve complex problems while acknowledging its limitations.

Method: The method involves a comprehensive, graduate-level introduction to DL for quantum applications, combining conceptual explanations with practical examples.

Result: The result is an educational resource that equips quantum scientists with the skills to apply DL effectively and responsibly.

Conclusion: The conclusion underscores the necessity of DL literacy in quantum science, balanced with awareness of its risks and limitations.

Abstract: Scientific progress is tightly coupled to the emergence of new research
tools. Today, machine learning (ML)-especially deep learning (DL)-has become a
transformative instrument for quantum science and technology. Owing to the
intrinsic complexity of quantum systems, DL enables efficient exploration of
large parameter spaces, extraction of patterns from experimental data, and
data-driven guidance for research directions. These capabilities already
support tasks such as refining quantum control protocols and accelerating the
discovery of materials with targeted quantum properties, making ML/DL literacy
an essential skill for the next generation of quantum scientists. At the same
time, DL's power brings risks: models can overfit noisy data, obscure causal
structure, and yield results with limited physical interpretability.
Recognizing these limitations and deploying mitigation strategies is crucial
for scientific rigor. These lecture notes provide a comprehensive,
graduate-level introduction to DL for quantum applications, combining
conceptual exposition with hands-on examples. Organized as a progressive
sequence, they aim to equip readers to decide when and how to apply DL
effectively, to understand its practical constraints, and to adapt AI methods
responsibly to problems across quantum physics, chemistry, and engineering.

</details>


<div id='math.FA'></div>

# math.FA [[Back]](#toc)

### [44] [New Lower Bounds for the Minimum Singular Value in Matrix Selection](https://arxiv.org/abs/2508.10452)
*Zhiqiang Xu*

Main category: math.FA

TL;DR: The paper uses interlacing polynomials to maximize the minimum singular value of a submatrix, improving bounds for specific cases.


<details>
  <summary>Details</summary>
Motivation: To address the matrix selection problem by maximizing the minimum singular value of a submatrix, enhancing existing results.

Method: Employs the interlacing polynomial method, focusing on direct root-coefficient relationships for tighter bounds.

Result: Achieves a tighter lower bound for the minimum singular value, especially when $k$ is close to $n$, improving prior work by Hong-Pan.

Conclusion: The interlacing polynomial method provides a stronger lower bound for the matrix selection problem, particularly in critical cases.

Abstract: The objective of the matrix selection problem is to select a submatrix
$A_{S}\in \mathbb{R}^{n\times k}$ from $A\in \mathbb{R}^{n\times m}$ such that
its minimum singular value is maximized. In this paper, we employ the
interlacing polynomial method to investigate this problem. This approach allows
us to identify a submatrix $A_{S_0}\in \mathbb{R}^{n\times k}$ and establish a
lower bound for its minimum singular value. Specifically, unlike common
interlacing polynomial approaches that estimate the smallest root of the
expected characteristic polynomial via barrier functions, we leverage the
direct relationship between roots and coefficients. This leads to a tighter
lower bound when $k$ is close to $n$. For the case where
$AA^{\top}=\mathbb{I}_n$ and $k=n$, our result improves the well-known result
by Hong-Pan, which involves extracting a basis from a tight frame and
establishing a lower bound for the minimum singular value of the basis matrix.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [45] [Self-Supervised Temporal Super-Resolution of Energy Data using Generative Adversarial Transformer](https://arxiv.org/abs/2508.10587)
*Xuanhao Mu,G√∂khan Demirel,Yuzhe Zhang,Jianlei Liu,Thorsten Schlachter,Veit Hagenmeyer*

Main category: cs.LG

TL;DR: The paper introduces a new method using Generative Adversarial Transformers (GATs) for upsampling energy network time series, reducing RMSE by 9% and improving MPC accuracy by 13%.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of conventional upsampling methods and advanced models (e.g., time series generation, Super-Resolution, imputation) in energy network design, which suffer from information loss, noise, or reliance on unavailable high-resolution data.

Method: Proposes Generative Adversarial Transformers (GATs), a method that learns the distribution of original data to generate high-resolution series without needing ground-truth high-resolution data for training.

Result: GATs reduce RMSE by 9% in upsampling tasks and improve MPC accuracy by 13% compared to conventional interpolation methods.

Conclusion: The GATs method effectively addresses the upsampling paradox and outperforms existing approaches, offering a viable solution for energy network applications.

Abstract: To bridge the temporal granularity gap in energy network design and operation
based on Energy System Models, resampling of time series is required. While
conventional upsampling methods are computationally efficient, they often
result in significant information loss or increased noise. Advanced models such
as time series generation models, Super-Resolution models and imputation models
show potential, but also face fundamental challenges. The goal of time series
generative models is to learn the distribution of the original data to generate
high-resolution series with similar statistical characteristics. This is not
entirely consistent with the definition of upsampling. Time series
Super-Resolution models or imputation models can degrade the accuracy of
upsampling because the input low-resolution time series are sparse and may have
insufficient context. Moreover, such models usually rely on supervised learning
paradigms. This presents a fundamental application paradox: their training
requires the high-resolution time series that is intrinsically absent in
upsampling application scenarios. To address the mentioned upsampling issue,
this paper introduces a new method utilizing Generative Adversarial
Transformers (GATs), which can be trained without access to any ground-truth
high-resolution data. Compared with conventional interpolation methods, the
introduced method can reduce the root mean square error (RMSE) of upsampling
tasks by 9%, and the accuracy of a model predictive control (MPC) application
scenario is improved by 13%.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [46] [Effective permeability conditions for diffusive transport through impermeable membranes with gaps](https://arxiv.org/abs/2508.10694)
*Molly Brennan,Edwina F. Yeo,Philip Pearce,Mohit P. Dalwadi*

Main category: cond-mat.soft

TL;DR: The paper uses multiscale analysis to derive effective coupling conditions for transport across membranes with periodic gaps, validated by simulations. It generalizes classic conditions and reveals a memory effect for time-varying scenarios, predicting bacterial membrane permeability is dominated by thickness.


<details>
  <summary>Details</summary>
Motivation: To understand the multiscale coupling between microscale membrane geometry and macroscale transport, particularly for bacterial membranes with periodic gaps.

Method: Formal multiscale analysis and numerical simulations to derive and validate effective macroscale coupling conditions.

Result: Derived analytic expressions for membrane permeability, showing a memory effect for time-varying scenarios. Predicted bacterial membrane permeability is dominated by thickness.

Conclusion: The findings generalize membrane coupling conditions and apply to various applications like filtration and carbon capture, with implications for understanding bacterial transport.

Abstract: Membranes regulate transport in a wide variety of industrial and biological
applications. The microscale geometry of the membrane can significantly affect
overall transport through the membrane, but the precise nature of this
multiscale coupling is not well characterised in general. Motivated by the
application of transport across a bacterial membrane, in this paper we use
formal multiscale analysis to derive explicit effective coupling conditions for
macroscale transport across a two-dimensional impermeable membrane with
periodically spaced gaps, and validate these with numerical simulations. We
derive analytic expressions for effective macroscale quantities associated with
the membrane, such as the permeability, in terms of the microscale geometry.
Our results generalise the classic constitutive membrane coupling conditions to
a wider range of membrane geometries and time-varying scenarios. Specifically,
we demonstrate that if the exterior concentration varies in time, for membranes
with long channels, the transport gains a memory property where the coupling
conditions depend on the system history. By applying our effective conditions
in the context of small molecule transport through gaps in bacterial membranes
called porins, we predict that bacterial membrane permeability is primarily
dominated by the thickness of the membrane. Furthermore, we predict how
alterations to membrane microstructure, for example via changes to porin
expression, might affect overall transport, including when external
concentrations vary in time. These results will apply to a broad range of
physical applications with similar membrane structures, from medical and
industrial filtration to carbon capture.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [47] [Intrinsic and Normal Mean Ricci Curvatures: A Bochner--Weitzenboeck Identity for Simple d-Vectors](https://arxiv.org/abs/2508.10306)
*Pawel Gajer,Jacques Ravel*

Main category: math.DG

TL;DR: The paper introduces two pointwise subspace averages of sectional curvature and uses Jacobi-field expansions to derive Bochner--Weitzenboeck identities and applications in harmonic analysis.


<details>
  <summary>Details</summary>
Motivation: To generalize curvature averages and derive new identities for geometric analysis, particularly for harmonic simple d-vectors and Hodge Laplacian eigenvalues.

Method: Defines intrinsic and normal mean Ricci averages, uses Jacobi-field expansions to relate these to volume elements, and derives Bochner--Weitzenboeck identities.

Result: Establishes a curvature term identity and provides applications like a Bochner vanishing criterion and a Lichnerowicz-type eigenvalue bound.

Conclusion: The introduced curvature averages and derived identities offer tools for analyzing harmonic forms and eigenvalues in geometric contexts.

Abstract: We introduce two pointwise subspace averages of sectional curvature on a
d-dimensional plane Pi in T_p M: (i) the intrinsic mean Ricci (the average of
sectional curvatures of 2-planes contained in Pi); and (ii) the normal (mixed)
mean Ricci (the average of sectional curvatures of 2-planes spanned by one
vector in Pi and one in Pi^perp). Using Jacobi-field expansions, these means
occur as the r^2/6 coefficients in the intrinsic (d-1)-sphere and normal
(n-d-1)-sphere volume elements. A direct consequence is a Bochner--Weitzenboeck
identity for simple d-vectors V (built from an orthonormal frame X_1,...,X_d
with Pi = span{X_i}): the curvature term equals d(n-d) times the normal mean
Ricci of Pi. This yields two immediate applications: (a) a Bochner vanishing
criterion for harmonic simple d-vectors under a positive lower bound on the
normal mean Ricci; and (b) a Lichnerowicz-type lower bound for the first
eigenvalue of the Hodge Laplacian on simple d-eigenfields.

</details>


### [48] [Isoperimetric inequalities involving Steklov eigenvalues on surfaces](https://arxiv.org/abs/2508.10721)
*Romain Petrides*

Main category: math.DG

TL;DR: The paper explores optimal constants for isoperimetric inequalities related to Steklov eigenvalues on surfaces with boundary, focusing on Riemannian surfaces with specific topologies or conformal classes. It provides new examples of optimal topological disks and proves inequalities linking conformal invariants of Steklov eigenvalues on surfaces and disks. The appendix addresses rigidity of the first conformal Steklov eigenvalue on annuli and M√∂bius bands.


<details>
  <summary>Details</summary>
Motivation: To advance understanding of isoperimetric inequalities and Steklov eigenvalues on surfaces with boundary, particularly in specific topological or conformal contexts.

Method: The study involves analyzing Riemannian surfaces with given topologies or conformal classes, proving inequalities, and examining rigidity in specific cases like annuli and M√∂bius bands.

Result: New examples of optimal topological disks are identified, and inequalities relating conformal invariants of Steklov eigenvalues are established. Rigidity results for the first conformal Steklov eigenvalue are also presented.

Conclusion: The findings contribute to the theoretical framework of isoperimetric inequalities and Steklov eigenvalues, with implications for understanding geometric and conformal properties of surfaces.

Abstract: We give results on optimal constants of isoperimetric inequalities involving
Steklov eigenvalues on surfaces with boundary. We both consider this question
on Riemannian surfaces with a same given topology or more specifically
belonging to the same conformal class. We provide new examples of topological
disks that realize optimal constants. We prove inequalities that relate
conformal invariants associated to combinations of Steklov eigenvalues on a
compact Riemannian surface with boundary and the ones on the disk. In the
appendix, we show rigidity of the first conformal Steklov eigenvalue on annuli
and M\"obius bands.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [49] [A Unified Framework from Boltzmann Transport to Proton Treatment Planning](https://arxiv.org/abs/2508.10596)
*Andreas E. Kyprianou,Aaron Pim,Tristan Pryer*

Main category: math.PR

TL;DR: The paper integrates deterministic and stochastic models for proton transport, establishes duality between them, and applies the theory to dose computation and treatment planning.


<details>
  <summary>Details</summary>
Motivation: To rigorously unify deterministic and stochastic perspectives of proton transport for improved accuracy in dose computation and treatment planning.

Method: Combines the Boltzmann-Fokker-Planck equation (deterministic) with a stochastic diffusion process, proving duality and resolvent-Green's function equivalence. Applies this to dose computation and hybrid optimization for treatment planning.

Result: Duality between stochastic and deterministic models is proven, and the classical dose-fluence relationship is validated. A hybrid optimization framework for treatment planning is developed.

Conclusion: The framework bridges stochastic simulation with deterministic control, enabling future advancements in proton therapy optimization.

Abstract: This work develops a rigorous mathematical formulation of proton transport by
integrating both deterministic and stochastic perspectives. The deterministic
framework is based on the Boltzmann-Fokker-Planck equation, formulated as an
operator equation in a suitable functional setting. The stochastic approach
models proton evolution via a track-length parameterised diffusion process,
whose infinitesimal generator provides an alternative description of transport.
  A key result is the duality between the stochastic and deterministic
formulations, established through the adjoint relationship between the
transport operator and the stochastic generator. We prove that the resolvent of
the stochastic process corresponds to the Green's function of the deterministic
equation, providing a natural link between fluence-based and particle-based
transport descriptions. The theory is applied to dose computation, where we
show that the classical relation: dose = (fluence * mass stopping power) arises
consistently in both approaches.
  Building on this foundation, we formulate a hybrid optimisation framework for
treatment planning, in which dose is computed using a stochastic model while
optimisation proceeds via adjoint-based PDE methods. We prove existence and
differentiability of the objective functional and derive the first-order
optimality system. This framework bridges stochastic simulation with
deterministic control theory and provides a foundation for future work in
constrained, adaptive and uncertainty-aware optimisation in proton therapy.

</details>
