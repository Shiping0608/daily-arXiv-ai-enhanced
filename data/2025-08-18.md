<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 12]
- [math.AP](#math.AP) [Total: 7]
- [physics.comp-ph](#physics.comp-ph) [Total: 3]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 5]
- [physics.optics](#physics.optics) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.NE](#cs.NE) [Total: 1]
- [math.DG](#math.DG) [Total: 1]
- [physics.ins-det](#physics.ins-det) [Total: 1]
- [math.MG](#math.MG) [Total: 1]
- [cond-mat.soft](#cond-mat.soft) [Total: 2]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [astro-ph.HE](#astro-ph.HE) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [hep-ex](#hep-ex) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [A finite element framework for simulating residential burglary in realistic urban geometries](https://arxiv.org/abs/2508.11055)
*Baoli Hao,Kamrun Mily,Annalisa Quaini,Ming Zhong*

Main category: math.NA

TL;DR: A PDE model for residential burglary prediction, derived from an agent-based model, is solved using a finite element method with natural boundary conditions and a decoupled numerical scheme.


<details>
  <summary>Details</summary>
Motivation: To improve burglary prediction by developing a robust, efficient PDE model with realistic boundary conditions and computational methods.

Method: Derived a PDE model from an agent-based model, applied natural boundary conditions, and used a decoupled finite element scheme for numerical solution.

Result: Demonstrated robustness and efficiency in numerical tests, including heterogeneous parameters and realistic geometry (Chicago).

Conclusion: The model and open-source code provide a foundation for future multiscale, multi-physics research in crime prediction.

Abstract: We consider a partial differential equation (PDE) model to predict
residential burglary derived from a probabilistic agent-based model through a
mean-field limit operation. The PDE model is a nonlinear, coupled system of two
equations in two variables (attractiveness of residential sites and density of
criminals), similar to the Keller-Segel model for aggregation based on
chemotaxis. Unlike previous works, which applied periodic boundary conditions,
we enforce boundary conditions that arise naturally from the variational
formulation of the PDE problem, i.e., the starting point for the application of
a finite element method. These conditions specify the value of the normal
derivatives of the system variables at the boundary. For the numerical solution
of the PDE problem discretized in time and space, we propose a scheme that
decouples the computation of the attractiveness from the computation of the
criminal density at each time step, resulting in the solution of two linear
algebraic systems per iteration. Through numerous numerical tests, we
demonstrate the robustness and computational efficiency of this approach.
Leveraging the flexibility allowed by the finite element method, we show
results for spatially heterogeneous model parameters and a realistic geometry
(city of Chicago). The paper includes a discussion of future perspectives to
build multiscale, 'multi-physics' models that can become a tool for the
community. The robust and efficient code developed for this paper, which is
shared open-source, is intended as the solid base for this broader research
program.

</details>


### [2] [Sharp Collocated Projection Method for Immiscible Two-Phase Flows](https://arxiv.org/abs/2508.11107)
*Adam L. Binswanger,Matthew Blomquist,Scott R. West,Shilpa Khatri,Maxime Theillard*

Main category: math.NA

TL;DR: A sharp collocated projection method for immiscible two-phase Navier-Stokes equations on adaptive quadtree/octree grids, using hybrid finite difference-volume discretizations for sharp boundary handling.


<details>
  <summary>Details</summary>
Motivation: To solve two-phase flow problems accurately with simplified data structures, leveraging adaptive grids and sharp discretizations.

Method: Uses nodal collocation on non-graded adaptive quadtree/octree grids, with hybrid finite difference-finite volume discretizations for sharp boundary and interfacial conditions.

Result: Demonstrated high accuracy in canonical 2D/3D examples, with potential for extension to more complex physics.

Conclusion: The method is accurate and efficient, suitable for scientific and engineering applications.

Abstract: We present a sharp collocated projection method for solving the immiscible,
two-phase Navier-Stokes equations in two- and three-dimensions. Our method is
built using non-graded adaptive quadtree and octree grids, where all of the
fluid variables are defined on the nodes, and we leverage this framework to
design novel spatial and temporal discretizations for the two-phase problem.
The benefits of the nodal collocation framework are best exemplified through
our novel discretizations, which employ a hybrid finite difference-finite
volume methodology to treat the boundary and interfacial jump conditions in an
entirely sharp manner. We demonstrate the capabilities of our novel approach
using a variety of canonical two- and three-dimensional examples and outline
how our framework can be extended to address more complicated physics. The
overall algorithm achieves high accuracy with simplified data structures,
making this solver ideal for scientific and engineering applications.

</details>


### [3] [Goal-Oriented Low-Rank Tensor Decompositions for Numerical Simulation Data](https://arxiv.org/abs/2508.11139)
*Daniel M. Dunlavy,Eric T. Phipps,Hemanth Kolla,John N. Shadid,Edward Phillips*

Main category: math.NA

TL;DR: A new low-dimensional model using low-rank tensor decompositions minimizes differences between model and simulation data, incorporating quantities of interest for accurate analysis.


<details>
  <summary>Details</summary>
Motivation: To improve dimensionality reduction by directly integrating quantities of interest and invariants from high-dimensional simulation data.

Method: Uses low-rank tensor decompositions to minimize discrepancies between model and simulation data, including derived functions.

Result: Applied successfully to combustion and plasma physics simulations, demonstrating effectiveness.

Conclusion: The approach enables more accurate analysis without full high-dimensional data, enhancing simulation insights.

Abstract: We introduce a new low-dimensional model of high-dimensional numerical
simulation data based on low-rank tensor decompositions. Our new model aims to
minimize differences between the model data and simulation data as well as
functions of the model data and functions of the simulation data. This novel
approach to dimensionality reduction of simulation data provides a means of
directly incorporating quantities of interests and invariants associated with
conservation principles associated with the simulation data into the
low-dimensional model, thus enabling more accurate analysis of the simulation
without requiring access to the full set of high-dimensional data.
Computational results of applying this approach to two standard low-rank tensor
decompositions of data arising from simulation of combustion and plasma physics
are presented.

</details>


### [4] [Two intriguing variants of the AAA algorithm for rational approximation](https://arxiv.org/abs/2508.11169)
*William Mitchell*

Main category: math.NA

TL;DR: Two variants of the AAA algorithm, AAAsmooth and AAAbudget, are introduced to improve approximation of functions or data sets in ℝ or ℂ. AAAsmooth avoids spurious poles and ensures smoother convergence, while AAAbudget reduces computational cost by using derivative information.


<details>
  <summary>Details</summary>
Motivation: The original AAA algorithm has limitations, such as spurious poles in real-valued problems and computational inefficiency. The variants aim to address these issues.

Method: AAAsmooth uses a complex linear combination of the last two right singular vectors to eliminate spurious poles. AAAbudget incorporates first derivative information, allowing the use of a smaller, square Loewner matrix.

Result: AAAsmooth provides smoother convergence and better results, while AAAbudget is significantly faster with similar approximation quality.

Conclusion: The variants enhance the AAA algorithm, offering improved performance and efficiency for rational function approximation.

Abstract: We consider the problem of finding a rational function in barycentric form to
approximate a given function or data set in $\mathbb{R}$ or $\mathbb{C}$. The
famous AAA algorithm, introduced in 2018, constructs such a rational function:
the barycentric weights are the entries of the final right singular vector of a
Loewner matrix with more rows than columns. We present two variants of the AAA
algorithm, inspired by two intriguing quotations from the original paper. In
the first, which we call AAAsmooth, we take the barycentric weights to be a
complex linear combination of the last two right singular vectors, which
eliminates the problem of spurious poles in real-valued problems and yields
smoother convergence curves. In the second, AAAbudget, we incorporate first
derivative information. This allows us to use a smaller, square alternative to
the Loewner matrix, so the SVDs are cheaper while the resulting approximant is
similar to the result of standard AAA. We present numerical tests showing that
while both variants behave fairly similarly to standard AAA, AAAsmooth can give
somewhat better results and AAAbudget can be much faster.

</details>


### [5] [Fluid Dynamics and Domain Reconstruction from Noisy Flow Images Using Physics-Informed Neural Networks and Quasi-Conformal Mapping](https://arxiv.org/abs/2508.11216)
*Han Zhang,Xue-Cheng Tai,Jean-Michel Morel,Raymond H. Chan*

Main category: math.NA

TL;DR: The paper proposes a method to denoise blood flow images by solving an optimization problem using Physics-Informed Neural Networks and quasi-conformal mapping, validated on synthetic and real-like data.


<details>
  <summary>Details</summary>
Motivation: High-quality blood flow imaging is crucial for medical diagnosis but challenging due to noise and artifacts from short acquisition times or device errors.

Method: The task is framed as an optimization problem, decomposed into fluid and geometry subproblems solved iteratively using Physics-Informed Neural Networks and quasi-conformal mapping.

Result: The method effectively reconstructs flow images, validated on synthetic and real-like data, showing robustness against noise.

Conclusion: The framework provides a high-quality solution for denoising flow images, with potential applications in medical diagnostics.

Abstract: Blood flow imaging provides important information for hemodynamic behavior
within the vascular system and plays an essential role in medical diagnosis and
treatment planning. However, obtaining high-quality flow images remains a
significant challenge. In this work, we address the problem of denoising flow
images that may suffer from artifacts due to short acquisition times or
device-induced errors. We formulate this task as an optimization problem, where
the objective is to minimize the discrepancy between the modeled velocity
field, constrained to satisfy the Navier-Stokes equations, and the observed
noisy velocity data. To solve this problem, we decompose it into two
subproblems: a fluid subproblem and a geometry subproblem. The fluid subproblem
leverages a Physics-Informed Neural Network to reconstruct the velocity field
from noisy observations, assuming a fixed domain. The geometry subproblem aims
to infer the underlying flow region by optimizing a quasi-conformal mapping
that deforms a reference domain. These two subproblems are solved in an
alternating Gauss-Seidel fashion, iteratively refining both the velocity field
and the domain. Upon convergence, the framework yields a high-quality
reconstruction of the flow image. We validate the proposed method through
experiments on synthetic flow data in a converging channel geometry under
varying levels of Gaussian noise, and on real-like flow data in an aortic
geometry with signal-dependent noise. The results demonstrate the effectiveness
and robustness of the approach. Additionally, ablation studies are conducted to
assess the influence of key hyperparameters.

</details>


### [6] [Polynomial preserving recovery for PHT-splines](https://arxiv.org/abs/2508.11233)
*Ying Cai,Falai Chen,Hailong Guo,Hongmei Kang,Zhimin Zhang*

Main category: math.NA

TL;DR: A polynomial-preserving recovery method for PHT-splines in isogeometric analysis improves gradient accuracy without needing superconvergent points, validated by numerical results.


<details>
  <summary>Details</summary>
Motivation: To enhance gradient approximations in isogeometric analysis using PHT-splines by leveraging local interpolation properties and avoiding reliance on superconvergent points.

Method: The method uses superconvergence of difference quotients and interior error estimates to recover gradients on translation-invariant meshes, also developing an error estimator for adaptive refinement.

Result: Theoretical superconvergence of recovered gradients is confirmed, and a recovery-based error estimator is introduced for adaptive refinement.

Conclusion: Numerical results validate the method's effectiveness, demonstrating improved gradient accuracy and practical utility in adaptive refinement.

Abstract: We propose a polynomial preserving recovery method for PHT-splines within
isogeometric analysis to obtain more accurate gradient approximations. The
method fully exploits the local interpolation properties of PHT-splines and
avoids the need for information on gradient superconvergent points. By
leveraging the superconvergence argument of difference quotients and the
interior error estimate, we establish the superconvergence property of the
recovered gradient on translation invariant meshes. As a byproduct, a
recovery-based a posteriori error estimator is developed for adaptive
refinement. Numerical results confirm the theoretical findings and demonstrate
the effectiveness of the proposed method.

</details>


### [7] [AirBreath Sensing: Protecting Over-the-Air Distributed Sensing Against Interference](https://arxiv.org/abs/2508.11267)
*Zhanwei Wang,Mingyao Cui,Huiling Yang,Qunsong Zeng,Min Sheng,Kaibin Huang*

Main category: math.NA

TL;DR: AirBreath sensing is proposed to mitigate interference in ISEA systems by balancing feature compression and spread spectrum under bandwidth constraints.


<details>
  <summary>Details</summary>
Motivation: To address the vulnerability of AirComp in ISEA systems to interference while maintaining spectrum efficiency.

Method: Proposes AirBreath sensing, combining feature compression and spread spectrum, and optimizes the tradeoff using breathing depth.

Result: Demonstrates effective interference mitigation and near-optimal performance in experiments.

Conclusion: AirBreath sensing offers a practical solution for interference-free ISEA systems.

Abstract: A distinctive function of sixth-generation (6G) networks is the integration
of distributed sensing and edge artificial intelligence (AI) to enable
intelligent perception of the physical world. This resultant platform, termed
integrated sensing and edge AI (ISEA), is envisioned to enable a broad spectrum
of Internet-of-Things (IoT) applications, including remote surgery, autonomous
driving, and holographic telepresence. Recently, the communication bottleneck
confronting the implementation of an ISEA system is overcome by the development
of over-the-air computing (AirComp) techniques, which facilitate simultaneous
access through over-the-air data feature fusion. Despite its advantages,
AirComp with uncoded transmission remains vulnerable to interference. To tackle
this challenge, we propose AirBreath sensing, a spectrum-efficient framework
that cascades feature compression and spread spectrum to mitigate interference
without bandwidth expansion. This work reveals a fundamental tradeoff between
these two operations under a fixed bandwidth constraint: increasing the
compression ratio may reduce sensing accuracy but allows for more aggressive
interference suppression via spread spectrum, and vice versa. This tradeoff is
regulated by a key variable called breathing depth, defined as the feature
subspace dimension that matches the processing gain in spread spectrum. To
optimally control the breathing depth, we mathematically characterize and
optimize this aforementioned tradeoff by designing a tractable surrogate for
sensing accuracy, measured by classification discriminant gain (DG).
Experimental results on real datasets demonstrate that AirBreath sensing
effectively mitigates interference in ISEA systems, and the proposed control
algorithm achieves near-optimal performance as benchmarked with a brute-force
search.

</details>


### [8] [Combining Nonlinear FETI-DP Methods and Quasi-Newton Methods using an SQP Approach](https://arxiv.org/abs/2508.11309)
*Stephan Köhler,Oliver Rheinbach*

Main category: math.NA

TL;DR: Combining nonlinear FETI-DP with Quasi-Newton methods via SQP for solving nonlinear finite element problems, using Hessian approximations and restarts for convergence acceleration.


<details>
  <summary>Details</summary>
Motivation: To enhance the efficiency of solving nonlinear finite element problems by integrating parallel iterative methods (FETI-DP) with Quasi-Newton techniques.

Method: Uses nonlinear FETI-DP with Lagrange multipliers, Quasi-Newton approximations of Hessian in SQP, and restarts for better convergence.

Result: Numerical experiments on nonlinear structural mechanics problems demonstrate the method's effectiveness.

Conclusion: The combined approach improves convergence and efficiency for nonlinear finite element problems.

Abstract: The combination of nonlinear FETI-DP (Dual Primal Finite Element Tearing and
Interconnecting) and Quasi-Newton methods using a sequential quadratic
programming (SQP) approach is considered. Nonlinear FETI-DP methods are
parallel iterative solution methods for nonlinear finite element problems,
based on divide and conquer, using Lagrange multipliers. In the method, we use
Quasi-Newton approximations of Hessian for the quadratic programs, where the
initial approximation uses the exact Hessian. To accelerate the convergence, we
recompute the initial Hessian and restart the Quasi-Newton approximation. We
provide numerical experiments using homogeneous model problems from nonlinear
structural mechanics.

</details>


### [9] [Virtual element method for thermomechanical analysis of electronic packaging structures with multi-scale features](https://arxiv.org/abs/2508.11410)
*Yanpeng Gong,Sishuai Li,Fei Qin,Bingbing Xu*

Main category: math.NA

TL;DR: The paper introduces VEM and SFVEM for thermomechanical analysis in electronic packaging, leveraging polygonal meshes for complex geometries and multi-scale features.


<details>
  <summary>Details</summary>
Motivation: To address challenges in analyzing electronic packaging reliability with complex geometries and multi-scale features using flexible mesh methods.

Method: Uses VEM and SFVEM with a non-matching mesh strategy, combining polygonal meshes for small-scale regions and quadrilateral meshes for larger domains.

Result: Accurately captures stress concentrations and predicts thermal/mechanical responses in structures like TSV, BGA, and PBGA.

Conclusion: VEM and SFVEM are effective for thermomechanical analysis in electronic packaging, validated by benchmarks and case studies.

Abstract: This paper presents two approaches: the virtual element method (VEM) and the
stabilization-free virtual element method (SFVEM) for analyzing
thermomechanical behavior in electronic packaging structures with geometric
multi-scale features. Since the virtual element method allows the use of
arbitrary polygonal elements, the inherent mesh flexibility of VEM allows
localized mesh modifications without affecting global mesh structure, making it
particularly effective for the analysis of electronic packaging reliability
involving complex geometries and multiple geometric scales. The approach
implements a novel non-matching mesh generation strategy that strategically
combines polygonal meshes for complex small-scale regions with regular
quadrilateral meshes for larger domains. The VEM formulation addresses both
heat conduction and thermomechanical coupling problems, with comprehensive
verification through analytical benchmarks and practical electronic packaging
case studies, including Through-Silicon Via (TSV), Ball Grid Array (BGA), and
Plastic Ball Grid Array (PBGA) structures. Results demonstrate that the method
accurately captures stress concentrations at material interfaces and provides
reliable thermal and mechanical response predictions. Some MATLAB codes for the
numerical examples are provided at
https://github.com/yanpeng-gong/VEM-electronic-packaging and on the VEMhub
website (www.vemhub.com).

</details>


### [10] [A Monotonicity-Based Regularization Approach to Shape Reconstruction for the Helmholtz Equation](https://arxiv.org/abs/2508.11439)
*Sarah Eberle-Blick,Bastian Harrach,Xianchao Wang*

Main category: math.NA

TL;DR: A convex data-fitting method for reconstructing unknown scatterers using the Helmholtz equation, leveraging monotonicity relations and eigenvalue minimization.


<details>
  <summary>Details</summary>
Motivation: To solve the inverse boundary value problem for unknown scatterers without requiring additional PDE solutions.

Method: Develops a convex formulation using monotonicity relations, pixel-wise constraints, and minimizes a data-fitting functional based on eigenvalues.

Result: The method ensures global convergence and stability, validated by numerical experiments for shape reconstruction.

Conclusion: The proposed approach is theoretically sound and effective for reconstructing scatterer shapes.

Abstract: We consider an inverse boundary value problem for determining unknown
scatterers, which is governed by the Helmholtz equation in a bounded domain. To
address this, we develop a novel convex data-fitting formulation that is
capable of reconstructing the shape of the unknown scatterers.Our formulation
is based on a monotonicity relation between the scattering index and boundary
measurements. We use this relation to obtain a pixel-wise constraint on the
unknown scattering index, and then minimize a data-fitting functional defined
as the sum of all positive eigenvalues of a linearized residual operator. The
main advantages of our new approach are that this is a convex data-fitting
problem that does not require additional PDE solutions. The global convergence
and stability of the method are rigorously established to demonstrate the
theoretical soundness. In addition, several numerical experiments are conducted
to verify the effectiveness of the proposed approach in shape reconstruction.

</details>


### [11] [Augmented Lagrangian Solvers for Poroelasticity with Fracture Contact Mechanics](https://arxiv.org/abs/2508.11508)
*Marius Nevland,Inga Berre,Jakub Wiktor Both,Eirik Keilegavlen*

Main category: math.NA

TL;DR: The paper explores solvers for coupled fluid flow, poroelasticity, and fracture contact mechanics, proposing a new augmented Lagrangian solver to address convergence issues in nonlinear systems.


<details>
  <summary>Details</summary>
Motivation: The complexity and nonlinearity of coupled subsurface processes, especially fracture contact mechanics, cause convergence issues in existing solvers, limiting practical implementation.

Method: The study investigates augmented Lagrangian solvers, including generalized Newton and return map methods, and introduces a new hybrid solver. Numerical experiments in 2D and 3D assess performance.

Result: Classical solvers are sensitive to augmentation parameters and behave unpredictably with complexity, while the new solver shows broader convergence but still exhibits unpredictability.

Conclusion: The new solver improves robustness but highlights the need for further research to address unpredictability in complex scenarios.

Abstract: In the subsurface, fractures and the surrounding porous rock can deform in
interaction with fluid flow. Advanced mathematical models governing these
coupled processes typically combine fluid flow, poroelasticity, and fracture
contact mechanics, representing fractures as co-dimension one objects within
the porous medium. The resulting system of equations is complex and highly
nonlinear. As a result, convergence issues with nonlinear solvers are common,
which hinders the practical implementation of such models.
  One particular source of difficulty for the nonlinear solvers comes from the
fracture contact mechanics, due to its inherently nonsmooth character. In this
paper, we investigate solvers based on the augmented Lagrangian formulation of
the frictional contact problem. This includes two classical solvers, namely the
generalized Newton method (using complementarity functions) and the return map
method. In addition, we propose a new augmented Lagrangian solver that combines
the two approaches.
  Numerical experiments in two and three dimensions are conducted to assess the
performance of the solvers on problems of mixed-dimensional poromechanics with
fracture contact mechanics. The experiments indicate that for simpler setups,
the solvers behave quite predictably and in accordance with established
heuristics from the contact mechanics literature. However, as the complexity of
the problem increases, the solvers become more unpredictable, and break with
these heuristics. In particular, the two classical solvers become highly
sensitive to the augmentation parameter. The new solver converges across a
larger range of this parameter in most cases, but nevertheless also displays
unpredictable behavior.

</details>


### [12] [A multigrid method for CutFEM and its implementation on GPU](https://arxiv.org/abs/2508.11608)
*Cu Cui,Guido Kanschat*

Main category: math.NA

TL;DR: A multigrid method for unfitted finite element discretization of Dirichlet boundary problems, using Nitsche's method and ghost penalties, shows fast convergence with moderate runtime losses compared to fitted methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of Dirichlet boundary value problems in unfitted finite element discretizations, ensuring stability and efficiency.

Method: Uses Nitsche's method for boundary conditions, ghost penalties for stabilization, standard intergrid operators, and a parallel multiplicative vertex-patch smoother.

Result: Fast convergence achieved; runtime losses are moderate compared to fitted methods, despite optimization limitations on cut patches.

Conclusion: The method is effective for unfitted discretizations, balancing convergence speed and computational efficiency.

Abstract: We present a multigrid method for an unfitted finite element discretization
of the Dirichlet boundary value problem. The discretization employs Nitsche's
method to implement the boundary condition and additional face based ghost
penalties for stabilization. We apply standard intergrid operators, relying on
the fact that the relevant domain of computation does not grow under mesh
refinement. The smoother is a parallel implementation of the multiplicative
vertex-patch smoother with inconsistent treatment of ghost penalties. Our
computational results show that we obtain a fast converging method.
Furthermore, runtime comparison to fitted methods show that the losses are
moderate although many optimizations for Cartesian vertex patches cannot be
applied on cut patches.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [13] [On the relaxation of polyconvex functionals with linear growth under strict convergence in $BV$](https://arxiv.org/abs/2508.11041)
*Riccardo Scala*

Main category: math.AP

TL;DR: The paper analyzes relaxation of polyconvex functionals with linear growth in BV space, showing Borel measure properties in 2D but not in higher dimensions, and compares relaxed area functionals.


<details>
  <summary>Details</summary>
Motivation: To understand the relaxation properties of polyconvex functionals in BV space and their behavior under strict convergence, particularly in 2D vs. higher dimensions.

Method: The study uses relaxation techniques and integral representation formulas for BV functions, focusing on strict convergence and Borel measure properties.

Result: In 2D, the relaxed functional is a Borel measure, but this fails in higher dimensions. The relaxed area functional can exceed the actual graph area for some maps.

Conclusion: The dimensionality of the domain critically affects the relaxation properties of polyconvex functionals in BV space, with distinct behaviors in 2D and higher dimensions.

Abstract: We consider the relaxation of polyconvex functionals with linear growth with
respect to the strict convergence in the space of functions of bounded
variation. These functionals appears as relaxation of $F(u,\Omega):=\int_\Omega
f(\nabla u)dx$, where $u:\Omega\rightarrow \mathbb R^m$, and $f$ is polyconvex.
In constrast with the case of relaxation with respect to the standard
$L^1$-convergence, in the case that $\Omega$ is $2$-dimensional, we prove that
the sets map $A\mapsto F(u,A)$ for $A$ open, is, for every $u\in
BV(\Omega;\mathbb R^m)$, $m\geq1$, the restriction of a Borel measure. This is
not true in the case $\Omega\subset\mathbb R^n$, with $n\geq3$. Using the
integral representation formula for a special class of functions, we also show
the presence of Cartesian maps whose relaxed area functional with respect to
the $L^1$-convergence is strictly larger than the area of its graph.

</details>


### [14] [A porous medium equation with spatially inhomogeneous absorption. Part II: Large time behavior](https://arxiv.org/abs/2508.11046)
*Razvan Gabriel Iagar,Diana-Rodica Munteanu*

Main category: math.AP

TL;DR: The paper analyzes the long-term behavior of solutions to a quasilinear absorption-diffusion equation, identifying various asymptotic profiles and their convergence based on critical exponents and initial conditions.


<details>
  <summary>Details</summary>
Motivation: To understand the asymptotic behavior of solutions to the given equation under specific conditions, particularly focusing on critical exponents and initial data.

Method: Study the Cauchy problem for the equation with exponents p>m>1 and σ>0, using radially symmetric self-similar solutions and analyzing convergence on time-expanding sets.

Result: Identified multiple asymptotic profiles, including self-similar solutions with specific decay rates, and established their uniqueness in some cases.

Conclusion: The study provides a comprehensive understanding of the asymptotic behavior of solutions, highlighting the role of critical exponents and initial conditions in determining the profiles.

Abstract: We study the large time behavior of solutions to the Cauchy problem for the
quasilinear absorption-diffusion equation $$ \partial_tu=\Delta
u^m-|x|^{\sigma}u^p, \quad (x,t)\in\real^N\times(0,\infty), $$ with exponents
$p>m>1$ and $\sigma>0$ and with initial conditions either satisfying $$ u_0\in
L^{\infty}(\real^N)\cap C(\real^N), \quad
\lim\limits_{|x|\to\infty}|x|^{\theta}u_0(x)=A\in(0,\infty) $$ for some
$\theta\geq0$. A number of different asymptotic profiles are identified, and
uniform convergence on time-expanding sets towards them is established,
according to the position of both $p$ and $\theta$ with respect to the
following critical exponents $$ p_F(\sigma)=m+\frac{\sigma+2}{N}, \quad
\theta_*=\frac{\sigma+2}{p-m}, \quad \theta^*=N. $$ More precisely, solutions
in radially symmetric self-similar form decaying as $|x|\to\infty$ with the
rates $$ u(x,t)\sim A|x|^{-\theta_*}, \quad {\rm or} \quad u(x,t)\sim
\left(\frac{1}{p-1}\right)^{1/(p-1)}|x|^{-\sigma/(p-1)}, $$ are obtained as
asymptotic profiles in some of these cases, while asymptotic simplifications or
logarithmic corrections in the time scales also appear in other cases. The
uniqueness of some of these self-similar solutions, left aside in the first
part of this work, is also established.

</details>


### [15] [Numerical study of a nonlocal nonlinear Schrödinger equation (MMT model)](https://arxiv.org/abs/2508.11064)
*Amin Esfahani,Gulcin M. Muslu*

Main category: math.AP

TL;DR: The paper studies a nonlocal nonlinear Schrödinger equation (MMT model), analyzing the long-term behavior of solutions, standing wave dynamics, and stability, including a two-parameter family of solutions and the semi-classical limit.


<details>
  <summary>Details</summary>
Motivation: To understand the impact of the nonlocal operator on solution behavior and explore the dynamics and stability of standing waves in the MMT model.

Method: Numerical generation of standing waves via Petviashvili's iteration, stability analysis using the split-step Fourier method, and investigation of the semi-classical limit.

Result: Identified conditions for global boundedness of solutions, constructed and analyzed a two-parameter family of standing waves, and explored the semi-classical limit in focusing and defocusing cases.

Conclusion: The study provides insights into the behavior and stability of solutions in the nonlocal nonlinear Schrödinger equation, with implications for understanding similar models.

Abstract: In this paper, we study a nonlocal nonlinear Schr\"odinger equation (MMT
model). We investigate the effect of the nonlocal operator appearing in the
nonlinearity on the long-term behavior of solutions, and we identify the
conditions under which the solutions of the Cauchy problem associated with this
equation is bounded globally in time in the energy space. We also explore the
dynamical behavior of standing wave solutions. Therefore, we first numerically
generate standing wave solutions of nonlocal nonlinear Schr\"odinger equation
by using the Petviashvili's iteration method and their stability is
investigated by the split-step Fourier method. This equation also has a
two-parameter family of standing wave solutions. In a second step, we
meticulously concern with the construction and stability of a two-parameter
family of standing wave solutions numerically. Finally, we investigate the
semi-classical limit of the nonlocal nonlinear Schr\"odinger equation in both
focusing and defocusing cases.

</details>


### [16] [$\mathrm{L}^p$-based Sobolev theory for PDEs on closed manifolds of class $C^m$](https://arxiv.org/abs/2508.11109)
*Gonzalo A. Benavides,Ricardo H. Nochetto,Mansur Shakipov*

Main category: math.AP

TL;DR: The paper investigates L^p-based well-posedness and Sobolev regularity for weak formulations of Laplace--Beltrami, Bochner Laplace, tangent Stokes, and tangent Navier--Stokes problems on compact manifolds. It uses variational methods and duality to derive regularity results and applies them to define a Stokes operator and analyze its properties.


<details>
  <summary>Details</summary>
Motivation: To establish a comprehensive L^p-based regularity theory for differential operators on manifolds without boundary, enabling applications to fluid dynamics and spectral analysis.

Method: Localization and translation of problems into flat domains, variational techniques, duality, and functional analysis tools like the Banach--Neč--Babuška theorem.

Result: Higher-regularity results for velocity and pressure in tangent Stokes problems, well-posedness for tangent Oseen and Navier--Stokes equations, and spectral analysis of the Stokes operator.

Conclusion: The derived regularity theory provides a robust framework for analyzing differential operators on manifolds, with applications in fluid dynamics and beyond.

Abstract: We study the $\mathrm{L}^p$-based ($1<p<\infty$) well-posedness and Sobolev
regularity for appropriate weak formulations of the (stationary)
Laplace--Beltrami, Bochner Laplace, tangent Stokes, and tangent Navier--Stokes
problems on compact, connected $d$-dimensional manifolds without boundary of
class $C^m$ ($m \geq 2$) embedded in $\mathrm{R}^{d+1}$, in terms of the
regularity of the source terms and the manifold. Firstly, we localize and
translate the Laplace--Beltrami problem into flat domains to establish
$\mathrm{L}^p$-based ($2 \leq p < \infty$) Sobolev regularity results as a
consequence of the powerful Calder\'{o}n-Zygmund theory. From then onwards, our
strategy is purely ``variational'', building upon the abstract functional
analytic idea of ``duality'' and classical results such as the
Banach--Ne\v{c}as--Babu\v{s}ka theorem and the generalized Babu\v{s}ka--Brezzi
theory in reflexive Banach spaces to analyze and relate to each other different
strong, weak and ultra-weak formulations for the aforementioned problems. In
particular, by exploiting the lack of manifold boundary, we are able to
decouple the velocity and the pressure unknowns in the tangent Stokes problem
to establish their higher-regularity $\mathbf{W}^{k,p} \times
\mathrm{W}^{k-1,p}$ ($k \geq 2$) as a consequence of the $\mathrm{L}^p$-based
well-posedness and regularity theory for the Laplace--Beltrami and
Bochner--Laplace operators. We finish this work by applying this newly derived
regularity theory to: i) define and appropriate Stokes operator and study its
spectral and regularity properties, ii) establish $\mathrm{L}^p$-based
($1<p<\infty$) existence and well-posedness for the (stationary) tangent Oseen
and Navier--Stokes equations via a Galerkin method and bootstrapping, and
higher $\mathrm{L}^p$-based Sobolev regularity results. iii) transfer our
regularity results to other choices of vector Laplace operators.

</details>


### [17] [Non-uniqueness of weak solutions to the 3D Hall-MHD equations on the plane](https://arxiv.org/abs/2508.11193)
*Yi Peng,Huaqiao Wang,Chenlu Zhang*

Main category: math.AP

TL;DR: The paper demonstrates non-uniqueness of weak solutions with non-trivial magnetic fields in 3D Hall-MHD equations using convex integration and intermittent flows. It also explores magnetic helicity non-conservation and strong limits of weak solutions.


<details>
  <summary>Details</summary>
Motivation: To address the non-uniqueness problem of weak solutions in Hall-MHD equations and understand their behavior, particularly focusing on magnetic fields and helicity.

Method: Uses convex integration and constructs new intermittent flows, including projecting 3D flows to 2D Mikado flows. Analyzes weak solutions in specific function spaces.

Result: Proves non-uniqueness of weak solutions, non-conservation of magnetic helicity, and identifies strong limits of weak solutions in ideal Hall-MHD.

Conclusion: The study provides insights into the behavior of weak solutions in Hall-MHD, highlighting non-uniqueness and non-conservation properties, with implications for understanding turbulence and magnetic fields.

Abstract: We prove the non-uniqueness of weak solutions with non-trivial magnetic
fields to the 3D Hall-MHD equations on the plane in the space $C^0_t L_x^2$
through the convex integration scheme and by constructing new errors and new
intermittent flows. In particular, based on the construction of 3D intermittent
flows, we obtain the $2\frac{1}{2}$D Mikado flows through a projection onto the
plane. Moreover, we prove that the constructed weak solution do not conserve
the magnetic helicity and find that weak solutions of the ideal Hall-MHD
equations in $C^{\bar{\beta}}_{t,x}$ ($\bar{\beta}>0$) are the strong vanishing
viscosity and resistive limit of weak solutions to the Hall-MHD equations.

</details>


### [18] [An inverse problem for the fractional Allen-Cahn equation](https://arxiv.org/abs/2508.11208)
*Li Li*

Main category: math.AP

TL;DR: Analysis of an inverse problem for the fractional Allen-Cahn equation using asymptotics and unique continuation.


<details>
  <summary>Details</summary>
Motivation: To understand and solve inverse problems related to the fractional Allen-Cahn equation, leveraging its mathematical properties.

Method: Utilizes asymptotics for the fractional equation and unique continuation properties.

Result: Provides insights into solving the inverse problem for the fractional Allen-Cahn equation.

Conclusion: The approach successfully addresses the inverse problem by combining asymptotics and unique continuation techniques.

Abstract: We study an inverse problem for the fractional Allen-Cahn equation. Our
formulation and arguments rely on the asymptotics for the fractional equation
and unique continuation properties.

</details>


### [19] [Perturbation of the nonlinear Schrödinger equation by a localized nonlinearity](https://arxiv.org/abs/2508.11463)
*Gong Chen,Jiaqi Liu,Yuanhong Tian*

Main category: math.AP

TL;DR: The paper simplifies proofs for key bounds and estimates in infinite-dimensional integrable systems, extending applicability to other models, and demonstrates consistent long-time behavior in perturbed equations.


<details>
  <summary>Details</summary>
Motivation: To provide simpler proofs for key estimates in integrable systems and extend understanding to focussing problems and other models.

Method: Revisits perturbative theory, introduces improved estimates, and examines perturbations of the cubic nonlinear Schrödinger equation.

Result: Shows the perturbed equation retains the long-time behavior of the integrable nonlinear Schrödinger equation.

Conclusion: The work advances understanding of perturbative integrable systems and offers simpler proofs for key results.

Abstract: We revisit the perturbative theory of infinite dimensional integrable systems
developed by P. Deift and X. Zhou \cite{DZ-2}, aiming to provide new and
simpler proofs of some key $L^\infty$ bounds and $L^p$ \emph{\textit{a priori}}
estimates. Our proofs emphasizes a further step towards understanding focussing
problems and extends the applicability to other integrable models. As a
concrete application, we examine the perturbation of the one-dimensional
defocussing cubic nonlinear Schr\"odinger equation by a localized higher-order
term. We introduce improved estimates to control the power of the perturbative
term and demonstrate that the perturbed equation exhibits the same long-time
behavior as the completely integrable nonlinear Schr\"odinger equation.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [20] [Novel discretization method to calculate g-functions of vertical geothermal boreholes with improved accuracy and efficiency](https://arxiv.org/abs/2508.11154)
*Yue Yang,Xiaodong Yang,Chenhui Lin,Luo Xu,Qi Wang,Shuwei Xu,Wenchuan Wu*

Main category: physics.comp-ph

TL;DR: A novel discretization method for g-function calculations in geothermal boreholes improves computational efficiency and accuracy compared to the SFLS model.


<details>
  <summary>Details</summary>
Motivation: Existing methods like the SFLS model are inefficient and inaccurate for fine-grained discretization in g-function calculations.

Method: Reformulates g-function calculation as spatio-temporal integral equations, uses Gauss-Legendre quadrature for spatial integrals, and applies regularization for stability.

Result: Achieves 20-200x faster computation with comparable or better accuracy than the SFLS model.

Conclusion: The proposed method efficiently addresses limitations of the SFLS model, offering stable and accurate g-function calculations.

Abstract: The calculation of g-functions is essential for the design and simulation of
geothermal boreholes. However, existing methods, such as the stacked finite
line source (SFLS) model, face challenges regarding computational efficiency
and accuracy, particularly with fine-grained discretization. This paper
introduces a novel discretization method to address these limitations. We
reformulate the g-function calculation under the uniform borehole wall
temperature boundary condition as the solution to spatio-temporal integral
equations. The SFLS model is identified as a special case using stepwise
approximation of the heat extraction rate. Our proposed method employs the
Gauss-Legendre quadrature to approximate the spatial integrals with a weighted
sum of function values at strategically chosen points. This transforms the
time-consuming segment-to-segment integral calculations in SFLS model into
simpler and analytical point-to-point response factors. Furthermore, we
identify that the governing integral equations are of the Fredholm first kind,
leading to ill-conditioned linear systems that can cause g-function to diverge
at high discretization orders. To address this, a regularization technique is
implemented to ensure stable and convergent solutions. Numerical tests
demonstrate that the proposed method is significantly more efficient, achieving
comparable or improved accuracy at speeds 20 to 200 times faster than the SFLS
model with optimized nonuniform discretization schemes.

</details>


### [21] [An efficient and robust high-order compact ALE gas-kinetic scheme for unstructured meshes](https://arxiv.org/abs/2508.11283)
*Yibo Wang,Xing Ji,Liang Pan*

Main category: physics.comp-ph

TL;DR: An efficient high-order compact ALE gas-kinetic scheme is developed for compressible moving grids, using memory-reduction reconstruction and least square methods to improve computational efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: The need for high-order spatial accuracy in ALE calculations leads to inefficiency due to frequent matrix inversions. This paper addresses this by developing a more efficient method.

Method: The scheme uses memory-reduction reconstruction to form quadratic polynomials, least square methods for derivatives, and gradient compression to reduce oscillations. It combines with two-stage fourth-order time discretization.

Result: The method achieves a 7x speedup by reducing matrix inversions and maintains high accuracy, robustness, and geometric conservation law preservation.

Conclusion: The proposed scheme efficiently handles compressible moving grids and boundaries, significantly improving computational speed and accuracy.

Abstract: For the arbitrary-Lagrangian-Eulerian (ALE) calculations, the geometric
information needs to be calculated at each time step due to the movement of
mesh. To achieve the high-order spatial accuracy, a large number of matrix
inversions are needed, which affect the efficiency of computation dramatically.
In this paper, an efficient and robust high-order compact ALE gas-kinetic
scheme is developed for the compressible moving grids and moving boundary
problems. The memory-reduction reconstruction is used to construct a quadratic
polynomial on the target cell, where both structured and unstructured meshes
can be used. Taking derivatives of the candidate polynomial, the quadratic
terms can be obtained by the least square method using the average gradient
values of the cell itself and its adjacent cells. Moving the quadratic terms to
right-hand side of the constrains for cell averaged value, the linear terms of
the polynomial can be determined by the least square method as well. The
gradient compression factor is adopted to suppress the spurious oscillations
near discontinuities. Combined with the two-stage fourth-order time
discretization, a high-order compact gas-kinetic scheme is developed for ALE
computation. In the process of mesh movement, the inversions of lower order
matrix are needed for the least square method, which makes a 7x speedup and
improves the efficiency greatly. In the computation, the grid velocity can be
given by the mesh adaptation method and the cell centered Lagrangian nodal
solver. Numerical examples are presented to evaluate the accuracy, efficiency,
robustness and the preservation of geometric conservation law of the current
scheme.

</details>


### [22] [DPI-SPR: A Differentiable Physical Inversion for Shadow Profile Reconstruction Framework in Forward Scatter Radar](https://arxiv.org/abs/2508.11470)
*ShuQi Lei,Gan Yu,Yuan Tian,XiaoWei Shao*

Main category: physics.comp-ph

TL;DR: DPI-SPR is a novel end-to-end imaging method for forward scatter radar, using differentiable physical inversion to reconstruct shadow profiles from noisy signals, achieving robust performance even at low SNR.


<details>
  <summary>Details</summary>
Motivation: Existing FSR imaging methods degrade under low SNR due to noise sensitivity and ill-posed inversion.

Method: Proposes DPI-SPR, an end-to-end framework using a differentiable forward scattering model (DFSM) and optimization of SWRF parameters with logarithmic loss and physics-based regularization.

Result: Achieves high-precision reconstruction from noisy signals at SNR as low as 8dB, outperforming existing methods.

Conclusion: DPI-SPR sets a new benchmark for robust FSR imaging, addressing noise challenges effectively.

Abstract: Forward scatter radar (FSR) has emerged as an effective imaging modality for
target detection, utilizing forward scattering (FS) signals to reconstruct
two-dimensional shadow profile images of objects. However, real-world FS
signals are inevitably corrupted by noise. Due to the ill-posed nature of
electromagnetic inversion and its high sensitivity to noise, existing imaging
methods often suffer from degraded performance or even complete failure under
low signal-to-noise ratio (SNR) conditions. To address this challenge, we
propose DPI-SPR (Differentiable Physical Inversion for Shadow Profile
Reconstruction), an end-to-end imaging paradigm built upon the Secondary
Wave-Source Response Field (SWRF). The core concept of this paradigm is to
reformulate the imaging problem as an optimization problem of continuous and
learnable SWRF parameters. To this end, we develop a fully Differentiable
Forward Scattering model (DFSM). Leveraging this model, the proposed inversion
framework integrates a robust logarithmic loss with physics-based
regularization constraints, enabling accurate gradient propagation from
observation errors to the SWRF parameters associated with the shadow profile.
Extensive simulation experiments have been conducted to verify the
effectiveness and robustness of the inversion proposed framework. The results
show that our method achieves high-precision profile reconstruction directly
from limited and noisy reference signals, even at an SNR as low as 8dB, setting
a new benchmark for robust imaging in FSR scenarios.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [23] [Bridging the Gap between Collisional and Collisionless Plasma Shocks: A Simulation Study using OSIRIS](https://arxiv.org/abs/2508.11097)
*Yossef Nissim Kindi,Asaf Pe'er,Antoine Bret,Luís O. Silva,Kevin M. Schoeffler*

Main category: physics.plasm-ph

TL;DR: The paper explores shock waves in plasmas, bridging collisional and collisionless regimes through simulations, identifying the ion plasma parameter as a key metric for shock width transition.


<details>
  <summary>Details</summary>
Motivation: To understand the transition between collisional and collisionless shock waves in plasmas and their implications for astrophysical environments.

Method: Particle-in-cell simulations using OSIRIS with a Coulomb-collision module, varying collisionality parameters.

Result: A smooth transition in shock width occurs when the ion plasma parameter $N_D \approx 1$, aligning with asymptotic predictions for both collisional and collisionless regimes.

Conclusion: The ion plasma parameter is a practical metric for shock width transition, with implications for shock breakout and particle acceleration in astrophysics.

Abstract: Shock waves in plasmas can be characterized by the mechanisms behind their
formation. When binary collisions are frequent, dissipation is collision-driven
and the shock width is a few mean free paths. In contrast, collisionless shocks
rely on collective plasma processes to establish dissipation on scales well
below the mean free path. We bridge these regimes with particle-in-cell
simulations using OSIRIS with a Coulomb-collision module, varying parameters
that control collisionality. We find a smooth transition of the shock width in
the intermediate region where the ion plasma parameter $N_D \approx 1$. Our
results recover the asymptotic predictions: a collisional-regime width
consistent with the Mott-Smith ansatz with a BGK operator, and the
collisionless limit consistent with Tidman's classical formalism. The ion
plasma parameter thus serves as a practical metric for identifying when shocks
shift from fluid-like, mean-free-path scales to collisionless,
sub-mean-free-path scales. We discuss implications for astrophysical
environments, where shock breakout changes the shock width and marks the onset
of efficient particle acceleration.

</details>


### [24] [Generation of Ultrabrilliant Positron Beam via Superponderomotive Injection in Laser Wakefield Acceleration](https://arxiv.org/abs/2508.11148)
*Ting Sun,Zhen-Ke Dou,Ya-Qing Huang,Feng Wan,Qian Zhao,Jian-Xing Li*

Main category: physics.plasm-ph

TL;DR: A novel superponderomotive injection method for positron acceleration in laser wakefields is proposed, enabling high-charge, bright positron beams for advanced applications.


<details>
  <summary>Details</summary>
Motivation: Overcoming the challenge of generating wakefield positron beams via inherent injection in plasma-based acceleration due to its ultrahigh gradient and ultrashort duration.

Method: Superponderomotive injection in the blowout regime, using transverse laser fields to trap positrons into a modulated wakefield for energy gain. Demonstrated via particle-in-cell simulations with a donut wake and pair jet collision.

Result: Produces multi-cycle positron beams with hundreds of pC charge and brightness of $10^{13}~\rm{Am}^{-2}$, feasible with current laser-plasma setups.

Conclusion: The method is promising for next-gen colliders, astrophysics modeling, and ultrafast material diagnostics.

Abstract: Plasma-based acceleration of positrons attracts extensive interests due to
the ultrahigh accelerating gradient and ultrashort duration, while generating
wakefield positron beam by the inherent injection is still a great challenge.
Here, we put forward a superponderomotive injection method of positrons in the
blowout regime of laser wakefield acceleration. In this method, transverse
laser fields facilitate the trapping of positrons into the laser-modulated
longitudinal wakefield which dominates the subsequent energy gaining, that is
distinct from the electrostatic induced superponderomotive electrons.
Particle-in-cell simulations demonstrate that this method can be implemented by
the collision between a donut wake and a pair jet, resulting in the multi-cycle
positron beam with hundreds of pC charge and brightness of
$10^{13}~\rm{Am}^{-2}$. Such a collisional setup is available in the current
laser-plasma experiments, and the generated positron beam is favorable for
developing next generation of electron-positron collider, modeling the
laboratory astrophysics and performing the ultrafast material diagnostics
research.

</details>


### [25] [Pump with broadband probe experiments for single-shot measurements of plasma conditions and crossed-beam energy transfer](https://arxiv.org/abs/2508.11156)
*A. Longman,R. Muir,D. Mittelberger,E. Grace,C. Goyon,G. Swadling,G. Kemp,T. Chapman,S. Maricle,N. Vanartsdalen,A. Linder,T. Dumbacher,K. Zoromski,B. Stuart,F. Albert,J. Heebner,P. Michel*

Main category: physics.plasm-ph

TL;DR: A new method for measuring plasma conditions using monochromatic pump-broadband probe laser interactions, offering high signal enhancement and non-perturbative measurements.


<details>
  <summary>Details</summary>
Motivation: To improve plasma parameter measurement techniques by leveraging crossed-beam energy transfer, reducing laser intensity requirements and enabling non-perturbative measurements.

Method: Utilizes crossed-beam energy transfer between a broadband probe and monochromatic pump laser, mediated by plasma ion acoustic waves, capturing the energy transfer spectrum in a single shot.

Result: Achieves signal enhancements over 9 orders of magnitude compared to Thomson scattering, enabling inference of plasma density, temperatures, and flow velocity.

Conclusion: This technique is a powerful tool for advancing plasma studies, particularly in inertial confinement fusion research.

Abstract: A novel technique for measuring plasma conditions using monochromatic
pump-broadband probe laser interactions has been experimentally demonstrated.
Originally proposed in [J. Ludwig et al., Phys. Plasmas \textbf{26}, 113108
(2019)], this method utilizes crossed-beam energy transfer between the
broadband probe and the pump, mediated by plasma ion acoustic waves. The
complete energy transfer spectrum can be captured in a single shot, enabling
the inference of plasma parameters such as density, electron and ion
temperatures, and flow velocity. Compared to Thomson scattering, this technique
offers signal enhancements typically larger than 9 orders of magnitude,
significantly reducing the required probe laser intensity and facilitating
interactions that are linear and measurements that are non-perturbative of the
plasma. Furthermore, it provides a powerful tool for advancing studies of
crossed-beam energy transfer under conditions relevant to inertial confinement
fusion experiments.

</details>


### [26] [Scaling Laws in Plasma Channels for Laser Wakefield Accelerators](https://arxiv.org/abs/2508.11238)
*Tianliang Zhang,Jianyi Liu,Shuang Liu,Ran Li,Fei Li,Jianfei Hua,Wei Lu*

Main category: physics.plasm-ph

TL;DR: Preformed plasma channels guide high-power lasers for multi-GeV electron beams, with above-threshold ionization heating enabling robust channel creation. Hydrodynamic expansion dominates density evolution, and scaling laws link channel parameters to formation conditions for optimized design.


<details>
  <summary>Details</summary>
Motivation: To enable efficient generation of multi-GeV electron beams for applications like free-electron lasers and particle colliders by understanding and optimizing plasma channel formation.

Method: Combined timescale analysis and numerical simulations to study hydrodynamic expansion's role in density profile evolution during ATI channel formation.

Result: Hydrodynamic expansion maintains effective laser-guiding structures across varying gas densities. Scaling laws show linear on-axis density with initial gas density and exponential matching radius dependence.

Conclusion: The findings provide a predictive framework for designing and optimizing plasma channels in high-efficiency, high-energy laser wakefield accelerators.

Abstract: Preformed plasma channels are essential for guiding high-power laser pulses
over extended distances in laser wakefield accelerators, enabling the
generation of multi-GeV electron beams for applications such as free-electron
lasers and particle colliders. Above-threshold ionization heating provides a
robust mechanism for creating laser-matched plasma channels across a wide
parameter range, owing to its density- and geometry-independent heating effect.
Establishing predictive scaling laws between channel parameters and formation
conditions is critical for designing channels optimized for electron
acceleration across energies spanning hundreds of MeV to tens of GeV. Through
combined timescale analysis and numerical simulations, hydrodynamic expansion
is identified as the dominant mechanism governing density profile evolution
during ATI channel formation. Remarkably, this process maintains effective
laser-guiding channel structures across a wide range of initial gas density, as
evidenced by the persistent profile similarity observed despite these
significant parameter variations. For parabolic channels matched to Gaussian
laser drivers, rigorous scaling laws are established that, the on-axis density
scales linearly with the initial gas density, while the matching radius has an
exponential dependence on both the initial gas density and the ionization laser
radius. These findings provide a systematic framework for the predictive design
and optimization of plasma channels in high-efficiency and high-energy LWFA
applications.

</details>


### [27] [Accelerated Solvers for Neutral Particle Dynamics in Plasma Simulation](https://arxiv.org/abs/2508.11564)
*Margherita Guido,Daniel Kressner,Davide Mancini,Paolo Ricci*

Main category: physics.plasm-ph

TL;DR: Low-rank linear algebra techniques, specifically hierarchical matrix approximations, are used to enhance the efficiency of boundary turbulence simulations in tokamaks, reducing computation time and memory by over 90%.


<details>
  <summary>Details</summary>
Motivation: Understanding and optimizing fusion reactor performance requires efficient simulation of turbulence in tokamak boundary regions, particularly for neutral particles.

Method: Hierarchical matrix approximations are applied to solve dense linear systems from discretized integral equations in a kinetic model, implemented in the GBS simulation code.

Result: The method achieves over 90% reduction in computation time and memory, enabling high-resolution simulations of neutral particles.

Conclusion: Low-rank linear algebra techniques significantly improve the efficiency of boundary simulations, facilitating better understanding and optimization of fusion reactors.

Abstract: The simulation of turbulence in the boundary region of a tokamak is crucial
for understanding and optimizing the performance of fusion reactors. In this
work, the use of low-rank linear algebra techniques is shown to enhance the
efficiency of boundary simulations, specifically by accelerating the solution
of a kinetic model for the neutral particles. Solving the kinetic model
deterministically using the method of characteristics requires the solution of
integral equations, which typically result in dense linear systems upon
discretization. We employ hierarchical matrix approximations to significantly
reduce the computational cost of assembling and solving the linear systems,
leading to substantial savings in both time and memory. The hierarchical matrix
method is implemented and tested within the GBS simulation code for boundary
simulations, achieving over 90\% reduction in computation time and memory, and
enabling simulations with unprecedented spatial resolution for neutral
particles.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [28] [Efficient computation of thermal radiation from biperiodic layered systems using the T-matrix method](https://arxiv.org/abs/2508.11590)
*Martin Gabbert,Markus Nyman,Lukas Rebholz,Carsten Rockstuhl,Ivan Fernandez-Corbaton*

Main category: physics.optics

TL;DR: A method combining the directional Kirchhoff law with T-matrix calculations is presented for efficient computation of thermal radiation from metasurfaces, validated by experimental data and outperforming other methods in speed.


<details>
  <summary>Details</summary>
Motivation: The need for efficient numerical tools to handle the diverse and computationally intensive simulations required for understanding metasurface thermal radiation control.

Method: Combines the directional Kirchhoff law with T-matrix based calculations for efficient computation.

Result: Accurately reproduces experimental data from a platinum square plate metasurface and predicts highly circularly polarised emissivity from a chiral metasurface. Outperforms rigorous coupled wave analysis in CPU-time efficiency.

Conclusion: The presented method is efficient and accurate for simulating thermal radiation from metasurfaces, offering computational advantages over existing approaches.

Abstract: Metasurfaces are becoming important tools for the control of thermal
radiation. Understanding their functional possibilities on computational
grounds requires evaluating the response of the biperiodic layered system for
many degrees of freedom, including several radiation directions and
polarisations, while varying lattice spacing, thicknesses, and/or materials of
homogeneous layers, over a range of frequencies. The diverse set of cases that
need to be considered in simulations prompts for efficient numerical tools to
handle them. To respond to this need, we present a method for computing the
thermal radiation from metasurfaces that combines the directional Kirchhoff law
with efficient T-matrix based calculations. We show that such a method can
accurately reproduce experimental data from a metasurface made of platinum
square plates. Additionally, we predict highly circularly polarised emissivity
from a chiral metasurface. When comparing CPU-times, the method outperforms
other approaches such as rigorous coupled wave analysis already at the modest
number of 61 cases per frequency.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [29] [Nested Operator Inference for Adaptive Data-Driven Learning of Reduced-order Models](https://arxiv.org/abs/2508.11542)
*Nicole Aretz,Karen Willcox*

Main category: cs.LG

TL;DR: A nested Operator Inference (OpInf) method improves reduced-order models (ROMs) by prioritizing dominant modes, achieving lower error and faster computation.


<details>
  <summary>Details</summary>
Motivation: To enhance ROM learning by leveraging hierarchy in reduced spaces and enabling dynamic updates.

Method: Nested OpInf iteratively constructs initial guesses for ROMs, warm-starts from learned models, and prioritizes dominant modes.

Result: Four times smaller error than standard OpInf in heat conduction; 3% error and 19,000x speed-up in ice sheet modeling.

Conclusion: Nested OpInf is efficient and versatile for physics-informed ROMs, outperforming standard methods.

Abstract: This paper presents a data-driven, nested Operator Inference (OpInf) approach
for learning physics-informed reduced-order models (ROMs) from snapshot data of
high-dimensional dynamical systems. The approach exploits the inherent
hierarchy within the reduced space to iteratively construct initial guesses for
the OpInf learning problem that prioritize the interactions of the dominant
modes. The initial guess computed for any target reduced dimension corresponds
to a ROM with provably smaller or equal snapshot reconstruction error than with
standard OpInf. Moreover, our nested OpInf algorithm can be warm-started from
previously learned models, enabling versatile application scenarios involving
dynamic basis and model form updates. We demonstrate the performance of our
algorithm on a cubic heat conduction problem, with nested OpInf achieving a
four times smaller error than standard OpInf at a comparable offline time.
Further, we apply nested OpInf to a large-scale, parameterized model of the
Greenland ice sheet where, despite model form approximation errors, it learns a
ROM with, on average, 3% error and computational speed-up factor above 19,000.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [30] [Multi-QIDA method for VQE state preparation in molecular systems](https://arxiv.org/abs/2508.11270)
*Fabio Tarocco,Davide Materia,Leonardo Ratini,Leonardo Guidoni*

Main category: quant-ph

TL;DR: The paper introduces Multi-QIDA, a method to construct shallow quantum circuits for molecular systems using Quantum Mutual Information (QMI), addressing scalability and optimization challenges in VQE.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of VQE, such as scalability and barren plateaus, by leveraging QMI to build efficient quantum circuits for molecular systems.

Method: Multi-QIDA uses QMI matrices from quantum chemistry, employs Minimum/Maximum spanning trees to reduce correlators, and iteratively optimizes circuits with SO(4) gates for enhanced expressibility.

Result: Multi-QIDA successfully recovers missing correlations in molecular systems while maintaining computational efficiency, outperforming traditional hardware-efficient ansatze.

Conclusion: Multi-QIDA provides a scalable and efficient approach for quantum chemistry applications, addressing key challenges in VQE and enabling broader use of quantum algorithms.

Abstract: The development of quantum algorithms and their application to quantum
chemistry has introduced new opportunities for solving complex molecular
problems that are computationally infeasible for classical methods. In quantum
chemistry, the Variational Quantum Eigensolver (VQE) is a hybrid
quantum-classical algorithm designed to estimate ground-state energies of
molecular systems. Despite its promise, VQE faces challenges such as
scalability issues, high circuit depths, and barren plateaus that make the
optimization of the variational wavefunction. To mitigate these challenges, the
Quantum Information Driven Ansatz (QIDA) leverages Quantum Mutual Information
(QMI) to construct compact, correlation-driven circuits. In this work, we go
back to the original field of application of QIDA, by applying the already
defined Multi-Threshold Quantum Information Driven Ansatz (Multi-QIDA)
methodology on Molecular Systems. to systematically construct shallow, layered
quantum circuits starting from approximate QMI matrices obtained by Quantum
Chemistry calculations. The Multi-QIDA approach combines efficient creation of
the QMI map, reduction of the number of correlators required by exploiting
Minimum/Maximum spanning tress, and an iterative layer-wise VQE optimization
routine. These enhancements allow the method to recover missing correlations in
molecular systems while maintaining computational efficiency. Additionally, the
approach incorporates alternative gate constructions, such as SO(4)
correlators, to enhance the circuit expressibility without significantly
increasing the circuit complexity. We benchmark Multi-QIDA on systems ranging
from small molecules like H2O, BeH2, and NH3 in Iterative Natural Orbitals
(INOs) basis set, to active-space models such as H2O-6-31G-CAS(4,4) and
N2-cc-pVTZ-CAS(6,6), comparing it to traditional hardware-efficient ansatze.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [31] [SO-PIFRNN: Self-optimization physics-informed Fourier-features randomized neural network for solving partial differential equations](https://arxiv.org/abs/2508.10921)
*Jiale Linghu,Weifeng Gao,Hao Dong,Yufeng Nie*

Main category: cs.NE

TL;DR: The paper introduces SO-PIFRNN, a framework combining physics-informed neural networks with hyperparameter optimization to improve PDE-solving accuracy. It uses a bi-level optimization approach and Fourier-features for better frequency capture.


<details>
  <summary>Details</summary>
Motivation: To enhance the accuracy and efficiency of solving PDEs by integrating hyperparameter optimization and Fourier-features in neural networks.

Method: Bi-level optimization: outer-level uses MSC-PSO for hyperparameter tuning, inner-level uses least squares for weight determination. Introduces Fourier basis activation and derivative neural networks.

Result: SO-PIFRNN shows superior accuracy and frequency capture in solving multiscale, high-order, high-dimensional, and nonlinear PDEs.

Conclusion: The framework effectively improves PDE-solving performance through innovative optimization and neural network design.

Abstract: This study proposes a self-optimization physics-informed Fourier-features
randomized neural network (SO-PIFRNN) framework, which significantly improves
the numerical solving accuracy of PDEs through hyperparameter optimization
mechanism. The framework employs a bi-level optimization architecture: the
outer-level optimization utilizes a multi-strategy collaborated particle swarm
optimization (MSC-PSO) algorithm to search for optimal hyperparameters of
physics-informed Fourier-features randomized neural network, while the
inner-level optimization determines the output layer weights of the neural
network via the least squares method. The core innovation of this study is
embodied in the following three aspects: First, the Fourier basis function
activation mechanism is introduced in the hidden layer of neural network, which
significantly enhances the ability of the network to capture multi-frequency
components of the solution. Secondly, a novel derivative neural network method
is proposed, which improves the calculation accuracy and efficiency of PIFRNN
method. Finally, the MSC-PSO algorithm of the hybrid optimization strategy is
designed to improve the global search ability and convergence accuracy through
the synergistic effect of dynamic parameter adjustment, elitist and mutation
strategies. Through a series of numerical experiments, including multiscale
equations in complex regions, high-order equations, high-dimensional equations
and nonlinear equations, the validity of SO-PIFRNN is verified. The
experimental results affirm that SO-PIFRNN exhibits superior approximation
accuracy and frequency capture capability.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [32] [BRIDGES Lectures: Flows of geometric structures, especially $\mathrm{G}_2$-structures](https://arxiv.org/abs/2508.11604)
*Spiro Karigiannis*

Main category: math.DG

TL;DR: The paper discusses short-time existence and uniqueness for geometric flows, focusing on Ricci flow and G₂-structures, with recent results on heat-type flows of G₂-structures.


<details>
  <summary>Details</summary>
Motivation: To explore and establish short-time existence and uniqueness for geometric flows, particularly for G₂-structures, extending known techniques like the DeTurck trick.

Method: Introduces geometric structures and flows, discusses parabolicity, and applies the DeTurck trick to Ricci flow and G₂-structures. Surveys various G₂-structure flows and recent results.

Result: Clarifies differences in short-time existence for G₂ Laplacian flow and presents a classification of heat-type flows of G₂-structures with a sufficient condition for STE and uniqueness.

Conclusion: The work extends understanding of geometric flows, particularly G₂-structures, and provides tools for analyzing their short-time behavior.

Abstract: The BRIDGES meeting in gauge theory, extremal structures, and stability was
held June 2024 at l'Institut d'\'Etudes Scientifiques de Carg\`ese in Corsica,
organized by Daniele Faenzi, Eveline Legendre, Eric Loubeau, and Henrique S\'a
Earp. The first week was a summer school consisting of four independent but
related lecture series by Oscar Garc\'ia Prada, Spiro Karigiannis, Laurent
Manivel, and Ruxandra Moraru. The present document consists of notes for the
lecture series by Spiro Karigiannis on "Flows of geometric structures,
especially $\mathrm{G}_2$-structures". Some assistance in the preparation of
these notes by the author was provided by several participants of the summer
school. See the Comments field for more information.
  The main theme is short time existence (STE) and uniqueness for geometric
flows. We first introduce geometric structures on manifolds and geometric flows
of such structures. We discuss some qualitative features of geometric flows,
and consider the notions of strong and weak parabolicity. We focus on the Ricci
flow, explaining carefully the DeTurck trick to establish short-time existence
and uniqueness, an argument which we then extend to a general class of
geometric flows of Riemannian metrics, previewing similar ideas for flows of
$\mathrm{G}_2$-structures. Finally, we consider geometric flows of
$\mathrm{G}_2$-structures. We review the basics of $\mathrm{G}_2$-geometry and
survey several different geometric flows of $\mathrm{G}_2$-structures. In
particular, we clarify in what sense STE results for the $\mathrm{G}_2$
Laplacian flow differ from STE results for other geometric flows. We conclude
with a summary of some recent results by the author with Dwivedi and
Gianniotis, including a classification of all possible heat-type flows of
$\mathrm{G}_2$-structures, and a sufficient condition for such a flow to admit
STE and uniqueness by a modified DeTurck trick.

</details>


<div id='physics.ins-det'></div>

# physics.ins-det [[Back]](#toc)

### [33] [The DeepFMKit Python package: A toolbox for simulating and analyzing deep frequency modulation interferometers](https://arxiv.org/abs/2508.11195)
*Miguel Dovale-Álvarez*

Main category: physics.ins-det

TL;DR: DeepFMKit is an open-source Python library for simulating and analyzing DFMI systems, featuring a physics engine, parameter estimation algorithms, and an experimentation framework.


<details>
  <summary>Details</summary>
Motivation: The complexity of designing and optimizing DFMI systems necessitates a tool for end-to-end simulation and analysis to improve precision interferometry.

Method: DeepFMKit combines a high-fidelity physics engine with interchangeable parameter estimation algorithms (NLS and EKF) and an experimentation framework for large-scale analyses.

Result: The library enables systematic characterization of DFMI systems, aiding in prototyping, error investigation, and development acceleration.

Conclusion: DeepFMKit provides a modular, powerful computational tool for advancing precision interferometry research.

Abstract: Deep Frequency Modulation Interferometry (DFMI) is an emerging laser
interferometry technique for high-precision metrology, offering picometer-level
displacement measurements and the potential for absolute length determination
with sub-wavelength accuracy. However, the design and optimization of DFMI
systems involve a complex interplay between interferometer physics, laser
technology, multiple noise sources, and the choice of data processing
algorithm. To address this, we present DeepFMKit, a new open-source Python
library for the end-to-end simulation and analysis of DFMI systems. The
framework features a high-fidelity physics engine that rigorously models key
physical effects such as time-of-flight delays in dynamic interferometers,
arbitrary laser modulation waveforms, and colored noise from user-defined
$1/f^\alpha$ spectral densities. This engine is coupled with a suite of
interchangeable parameter estimation algorithms, including a highly-optimized,
parallelized frequency-domain Non-linear Least Squares (NLS) for
high-throughput offline analysis, and multiple time-domain Extended Kalman
Filter (EKF) implementations for real-time state tracking, featuring both
random walk and integrated random walk (constant velocity) process models.
Furthermore, DeepFMKit includes a high-throughput experimentation framework for
automating large-scale parameter sweeps and Monte Carlo analyses, enabling
systematic characterization of system performance. DeepFMKit's modular,
object-oriented architecture facilitates the rapid configuration of virtual
experiments, providing a powerful computational tool for researchers to
prototype designs, investigate systematic errors, and accelerate the
development of precision interferometry.

</details>


<div id='math.MG'></div>

# math.MG [[Back]](#toc)

### [34] [Explicit solutions to Christoffel-Minkowski problems and Hessian equations under rotational symmetries](https://arxiv.org/abs/2508.11600)
*Fabian Mussnig,Jacopo Ulivelli*

Main category: math.MG

TL;DR: The paper presents an explicit solution to the Christoffel-Minkowski problem for convex bodies of revolution, using first moments over spherical caps and an explicit formula for the support function. It also addresses mixed area measures and solves mixed Monge-Ampère equations under radial symmetry.


<details>
  <summary>Details</summary>
Motivation: To solve the Christoffel-Minkowski problem for convex bodies of revolution and address broader existence problems for mixed area measures, leveraging radial symmetry.

Method: Constructs explicit convex solutions to mixed Monge-Ampère equations on ℝⁿ under radial symmetry, with conditions on the measure expressed via open balls. Includes a special case for the Dirichlet problem for k-Hessian equations.

Result: An explicit representation formula for the support function of the resulting convex body is derived, and conditions on the prescribed measure are simplified to first moments over spherical caps.

Conclusion: The approach successfully solves the problem for convex bodies of revolution and extends to mixed area measures, providing a framework for similar problems under radial symmetry.

Abstract: An explicit solution to the Christoffel-Minkowski problem for convex bodies
of revolution is presented. The conditions on the prescribed measure involve
only first moments over spherical caps, and the support function of the
resulting convex body is given by an explicit representation formula in terms
of the measure. More generally, existence problems for mixed area measures are
addressed. The approach relies on constructing explicit convex solutions to
mixed Monge-Amp\`ere equations on $\mathbb{R}^n$ under the assumption of radial
symmetry, with the conditions on the measure being expressed through its values
on open balls. As a special case, the Dirichlet problem for $k$-Hessian
equations on $\mathbb{R}^n$ is treated.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [35] [Solid-Angle Nearest-Neighbor Method for Size-Disperse Systems of Spheres](https://arxiv.org/abs/2508.11220)
*Nydia Roxana Varela-Rosales,Michael Engel*

Main category: cond-mat.soft

TL;DR: SANNR, a generalization of SANN, improves neighbor detection in polydisperse systems by incorporating particle radii, outperforming Voronoi, Laguerre, and SANN methods.


<details>
  <summary>Details</summary>
Motivation: Accurate neighbor detection is crucial in particle-based simulations, especially in systems with size disparities, where existing methods like Voronoi and SANN falter.

Method: SANNR extends SANN by integrating particle radii into the solid-angle criterion, enabling robust neighbor detection in size-diverse mixtures.

Result: SANNR matches Laguerre tessellation in accuracy while maintaining SANN's geometric continuity, enhancing detection of local and global order in complex systems like AB$_{13}$.

Conclusion: SANNR provides a parameter-free, adaptable solution for neighbor identification in polydisperse and multicomponent systems, improving structural analysis.

Abstract: Identifying nearest neighbors accurately is essential in particle-based
simulations, from analyzing local structure to detecting phase transitions.
While parameter-free methods such as Voronoi tessellation and the solid-angle
nearest-neighbor (SANN) algorithm are effective in monodisperse systems, they
become less reliable in mixtures with large size disparities. We introduce
SANNR, a generalization of SANN that incorporates particle radii into the
solid-angle criterion for robust, size-sensitive neighbor detection. We compare
SANNR against Voronoi, Laguerre, and SANN in binary and size-disperse sphere
mixtures. Using Wasserstein distance metrics, we show that SANNR closely
matches size-aware Laguerre tessellation while preserving the geometric
continuity of SANN. Applied to the crystallization of the complex AB$_{13}$
phase, SANNR improves detection of local bond-orientational order and better
captures the emergence of global symmetry. SANNR thus offers a smooth,
parameter-free, and extensible framework for neighbor detection in polydisperse
and multicomponent systems.

</details>


### [36] [pylimer-tools: A Python Package for Generating and Analyzing Bead-Spring Polymer Networks](https://arxiv.org/abs/2508.11509)
*Tim Bernhard,Fabian Schwarz,Andrei A. Gusev*

Main category: cond-mat.soft

TL;DR: pylimer-tools is a Python package for computational polymer network studies, offering MC-based network generation, structural/mechanical analysis, and support for external simulations like LAMMPS.


<details>
  <summary>Details</summary>
Motivation: Addresses the need for specialized computational tools in polymer science, particularly for bead-spring networks.

Method: Uses Monte Carlo procedures for network generation, Force Balance for energy reduction, and includes DPD simulator with slip-spring modeling.

Result: Provides functionality for loop detection, strand fraction computation, and shear modulus calculation.

Conclusion: pylimer-tools is a versatile toolkit for polymer network analysis, combining performance (C++) with accessibility (Python bindings).

Abstract: The Python package pylimer-tools is a comprehensive toolkit for computational
studies of polymer networks, particularly bead-spring networks. The package
provides functionality to generate polymer networks using Monte Carlo (MC)
procedures and analyze their structural and mechanical properties. Key features
include detection of loops, reduction of the network to its ground state energy
both with and without entanglements by the Force Balance procedure, and
thereafter computing the soluble and dangling fractions of network strands, as
well as the equilibrium shear modulus. The toolkit supports analysis of
structures generated both internally and by external simulation software such
as LAMMPS. The package implements theoretical frameworks including
Miller-Macosko theory and provides a dissipative particle dynamics (DPD)
simulator with slip-spring entanglement modeling. Built with C++ for
performance and exposed through Python bindings, pylimer-tools addresses the
need for specialized tools in computational polymer science.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [37] [The Effect of Flow Parameters and Wall Models on Gas-Surface Interactions: A Numerical Investigation of dsmcFoam](https://arxiv.org/abs/2508.11403)
*M. B. Agir,N. H. Crisp,K. L. Smith,P. C. E. Roberts,M. Newsam,M. Griffiths,S Vaidya*

Main category: physics.flu-dyn

TL;DR: The paper investigates gas-surface interactions in atmosphere-breathing electric propulsion systems using simulations to evaluate flow and wall parameters.


<details>
  <summary>Details</summary>
Motivation: To accurately model particle-surface interactions for efficient propulsion system operation in diverse environments.

Method: A parametric study with a simulation matrix analyzes flow and wall parameters using a simplified 2D test geometry and the dsmcFoam solver.

Result: The study reveals limitations in default boundary conditions for predicting gas-surface interactions, suggesting the need for new conditions.

Conclusion: New boundary conditions are required to accurately model gas-surface interactions in propulsion systems.

Abstract: Atmosphere-breathing electric propulsion systems harness atmospheric
particles as propellant, enabling efficient operation across diverse
environmental conditions. To accurately simulate the captured gas flow through
the modules, particle-surface interactions must be carefully modelled. To
initiate this research, a parametric study is conducted using an extensive
simulation matrix to investigate the effects of flow parameters, such as
velocity, temperature, species, and angle of attack, and wall model parameters
(diffuse fraction/accommodation coefficient) on gas-surface interactions. A
simplified test geometry was created to run 2D simulations, where the flow
interacts with an adjacent wall positioned perpendicular to one of the inlet
patches. In this study, changes in reflection patterns, force density on the
surface, and flow properties in the vicinity of the wall are investigated under
varying flow and wall conditions using the current boundary conditions of the
dsmcFoam solver. Furthermore, the capabilities of dsmcFoam's default boundary
conditions in predicting gas-surface interaction physics are evaluated using
the results of the simulation matrix. The findings highlight the need for new
boundary conditions to accurately replicate interaction physics across various
aspects.

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [38] [Polarized Emission of Intrabinary Shocks in Spider Pulsars from Global 3D Kinetic Simulations](https://arxiv.org/abs/2508.11625)
*Andrew G. Sullivan,Jorge Cortés,Lorenzo Sironi*

Main category: astro-ph.HE

TL;DR: The paper presents the first 3D kinetic simulations of spider pulsar intrabinary shocks, predicting their polarized emission properties and comparing them with observations.


<details>
  <summary>Details</summary>
Motivation: To understand particle acceleration and emission in spider pulsar systems, where relativistic shocks form due to collisions between pulsar winds and stellar outflows.

Method: First global 3D kinetic simulations of intrabinary shocks, analyzing emission spectra, light curves, and polarization patterns under varying conditions.

Result: Reproduced observed double-peaked light curves and predicted high polarization degrees (≥15%), increasing with magnetic field strength.

Conclusion: The findings can be tested with future X-ray polarization observations of spider pulsars.

Abstract: In spider pulsar systems, a relativistic intrabinary shock forms when the
pulsar wind collides with the massive outflow driven off the pulsar's low-mass
stellar companion. The shock is a site of non-thermal particle acceleration,
likely via shock-driven magnetic reconnection, and produces synchrotron
emission. These shocks are among the few systems in which global scales can be
reasonably captured with kinetic simulations, enabling first-principles
particle acceleration and emission studies. We perform the first global 3D
kinetic simulations of spider pulsar intrabinary shocks and predict their
polarized emission properties. We report emission spectra, light curves, and
polarization patterns as a function of the stripe-averaged magnetic field,
cooling strength, and viewing inclination. At $90^\circ$ inclination and for a
low stripe-averaged magnetic field, we reproduce the double peaked light curve
observed in spider systems. We predict a significant polarization degree
$\gtrsim15\%$, which monotonically increases with the stripe-averaged field
strength. Our results can be applied to and tested by forthcoming X-ray
polarization observations of spider pulsars.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [39] [Full-Wave Modeling of Transcranial Ultrasound using Volume-Surface Integral Equations and CT-Derived Heterogeneous Skull Data](https://arxiv.org/abs/2508.11100)
*Alberto Almuna-Morales,Danilo Aballay,Pierre Gélat,Reza Haqshenas,Elwin van 't Wout*

Main category: physics.med-ph

TL;DR: The paper presents an efficient numerical method for modeling full-wave acoustic propagation through the skull in transcranial ultrasound therapy, validated against high-resolution models and showing significant beam distortion effects.


<details>
  <summary>Details</summary>
Motivation: Transcranial ultrasound therapy faces challenges like beam distortion and focal shift due to the skull's heterogeneity and attenuation. Existing computational models rely on CT images but are limited by coarse resolution.

Method: The paper introduces a volume-surface integral equation-based numerical method to model acoustic propagation through the skull, validated against a high-resolution boundary element model.

Result: The method is highly accurate on coarse meshes, enabling direct use of CT voxel data. Simulations reveal significant beam distortion and focal shift compared to homogeneous cases.

Conclusion: The proposed method efficiently addresses the limitations of CT-based models, improving accuracy for transcranial ultrasound therapy planning.

Abstract: Transcranial ultrasound therapy uses focused acoustic energy to induce
therapeutic bioeffects in the brain. Ultrasound is transmitted through the
skull, which is highly attenuating and heterogeneous, causing beam distortion,
reducing focal pressure, and shifting the target location. Computational models
are frequently used for predicting beam aberration, assessing cranial heating,
and correcting the phase of ultrasound transducers. These models often rely on
computed tomography (CT) images to build patient-specific geometries and
estimate skull acoustic properties. However, the coarse voxel resolution of CT
limits accuracy for differential equation solvers. This paper presents an
efficient numerical method based on volume-surface integral equations to model
full-wave acoustic propagation through heterogeneous skull tissue. We have
shown that this approach is highly accurate on relatively coarse meshes
compared to the minimum wavelength, enabling direct use of CT voxel data. The
method is validated against a high-resolution boundary element model using an
averaged skull representation. Simulations with a CT-based skull model and a
bowl transducer show significant beam distortion and attenuation, with a focal
shift of several millimeters from the homogeneous case.

</details>


<div id='hep-ex'></div>

# hep-ex [[Back]](#toc)

### [40] [It's not a FAD: first results in using Flows for unsupervised Anomaly Detection at 40 MHz at the Large Hadron Collider](https://arxiv.org/abs/2508.11594)
*Francesco Vaselli,Maurizio Pierini,Maciej Mikolaj Glowacki,Thea Aarrestad,Katya Govorkova,Vladimir Loncar,Dimitrios Danopoulos,Felice Pantaleo*

Main category: hep-ex

TL;DR: A Continuous Normalizing Flow (CNF) model is adapted for FPGA deployment in anomaly detection at the LHC, using a novel hardware-friendly anomaly score.


<details>
  <summary>Details</summary>
Motivation: To enable real-time anomaly detection in high-rate environments like the LHC's L1 trigger systems, overcoming the computational complexity of traditional CNF methods.

Method: Proposes a squared norm of the model's vector field output as a simpler anomaly score, trained via Flow Matching on Standard Model-like data and synthesized for FPGA using hls4ml.

Result: The model effectively detects beyond-the-Standard-Model signatures with low latency (few hundred nanoseconds) and minimal FPGA resource usage.

Conclusion: CNFs are viable for real-time anomaly detection at 40 MHz, offering a new tool for data-driven discovery in high-energy physics.

Abstract: We present the first implementation of a Continuous Normalizing Flow (CNF)
model for unsupervised anomaly detection within the realistic, high-rate
environment of the Large Hadron Collider's L1 trigger systems. While CNFs
typically define an anomaly score via a probabilistic likelihood, calculating
this score requires solving an Ordinary Differential Equation, a procedure too
complex for FPGA deployment. To overcome this, we propose a novel,
hardware-friendly anomaly score defined as the squared norm of the model's
vector field output. This score is based on the intuition that anomalous events
require a larger transformation by the flow. Our model, trained via Flow
Matching on Standard Model-like data, is synthesized for an FPGA using the
hls4ml library. We demonstrate that our approach effectively identifies a
variety of beyond-the-Standard-Model signatures with performance comparable to
existing machine learning-based triggers. The algorithm achieves a latency of a
few hundred nanoseconds and requires minimal FPGA resources, establishing CNFs
as a viable new tool for real-time, data-driven discovery at 40 MHz.

</details>
