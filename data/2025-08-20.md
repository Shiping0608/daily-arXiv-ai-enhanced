<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 14]
- [math.AP](#math.AP) [Total: 20]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 5]
- [astro-ph.SR](#astro-ph.SR) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [math.DG](#math.DG) [Total: 2]
- [math.FA](#math.FA) [Total: 1]
- [physics.optics](#physics.optics) [Total: 2]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 2]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [math.SP](#math.SP) [Total: 1]
- [math.PR](#math.PR) [Total: 1]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [cond-mat.supr-con](#cond-mat.supr-con) [Total: 1]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [physics.chem-ph](#physics.chem-ph) [Total: 2]
- [math-ph](#math-ph) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [cs.DC](#cs.DC) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Data-driven optimized high-order WENO schemes with low-dissipation and low-dispersion](https://arxiv.org/abs/2508.13190)
*Jinrui Zhou,Yiqi Gu,Song Jiang,Hua Shen,Liwei Xu,Guanyu Zhou*

Main category: math.NA

TL;DR: Data-driven optimized WENO schemes using neural networks to improve spectral properties while maintaining shock-capturing capabilities


<details>
  <summary>Details</summary>
Motivation: Classical WENO schemes have suboptimal spectral properties that limit their ability to capture high-frequency waves and small-scale features, despite achieving optimal convergence order for smooth solutions and non-oscillatory behavior for discontinuities

Method: Propose WENO5-JS/Z-NN schemes that introduce a neural network-parameterized compensation term to the weight function. The neural network is trained to minimize reconstructed errors over spatial stencils while incorporating TVD constraints and anti-dissipation penalization in the loss function to enhance shock-capturing and stability

Result: The new schemes maintain discontinuity capture ability while providing higher resolution for fine-scale flow features. ADR analysis shows the schemes match the exact spectrum more accurately over a broader range of wavenumbers compared to WENO5-JS/Z

Conclusion: The data-driven optimized approach successfully improves spectral properties of WENO schemes while preserving their essential non-oscillatory characteristics and stability for high-frequency wave simulation

Abstract: Classical high-order weighted essentially non-oscillatory (WENO) schemes are
designed to achieve optimal convergence order for smooth solutions and to
maintain non-oscillatory behaviors for discontinuities. However, their spectral
properties are not optimal, which limits the ability to capture high-frequency
waves and small-scale features. In this paper, we propose a data-driven
optimized method to improve the spectral properties of the WENO schemes. By
analyzing the approximate dispersion relation (ADR), the spectral error of the
schemes can be bounded by the reconstructed errors of a series of trigonometric
functions with different wavenumbers. Therefore, we propose the new schemes
WENO5-JS/Z-NN that introduce a compensation term parameterized by a neural
network to the weight function of the WENO5-JS/Z schemes. The neural network is
trained such that the generated weights can minimize the reconstructed errors
over a large number of spatial stencils, and furthermore, improve the spectral
accuracy. Meanwhile, the Total Variation Diminishing (TVD) constraint and
anti-dissipation penalization are incorporated into the loss function to
enhance the shock-capturing capability and preserve stability in simulating
high-frequency waves. Compared to WENO5-JS/Z, our schemes maintain the ability
to capture discontinuities while providing higher resolution for fine-scale
flow features. The ADR indicates that the new schemes can match the exact
spectrum more accurately over a broader range of wavenumbers.

</details>


### [2] [On the usage of $2$-node lines in $n$-correct and $GC_n$ sets](https://arxiv.org/abs/2508.13289)
*Hakop Hakopian,Gagik Vardanyan,Navasard Vardanyan*

Main category: math.NA

TL;DR: Maximum number of used 2-node lines sharing a common node in an n-correct set is n, and this leads to Carnicer-Gasca sets with n+1 maximal lines.


<details>
  <summary>Details</summary>
Motivation: To study interpolation properties of bivariate polynomials on n-correct sets and determine the maximum possible number of used 2-node lines sharing a common node.

Method: Analysis of n-correct sets in the plane, studying lines passing through nodes and their relationship with fundamental polynomials of nodes.

Result: The maximum number of used 2-node lines sharing a common node B equals n. If there are n such lines, then the set contains exactly n maximal lines not passing through B, and if it's a GC_n set, there's an additional maximal line through B making it a Carnicer-Gasca set.

Conclusion: The structure of n-correct sets with maximum used 2-node lines leads to Carnicer-Gasca sets, which can be constructed with prescribed sets of used 2-node lines.

Abstract: An $n$-correct set $\mathcal{X}$ in the plane is a set of nodes admitting
unique
  interpolation with bivariate polynomials of total degree at most $n$. A
$k$-node line is a line passing through exactly $k$ nodes of $\mathcal{X}.$ A
line can pass through at most $n+1$ nodes of an $n$-correct set. An
$(n+1)$-node line is called maximal line (C. de Boor, 2007). We say that a node
$A\in\mathcal{X}$ uses a line $\ell,$ if $\ell$ is a factor of the fundamental
polynomial of the node $A.$
  Let $\mathcal{X}$ be an $n$-correct set. One of the main problems we study in
this paper is to determine the maximum possible number of used $2$-node lines
that share a common node $B \in\mathcal{X}.$ We show that this number equals
$n$. Moreover, if there are $n$ such $2$-node lines, then $\mathcal{X}$
contains exactly $n$ maximal lines not passing through the common node $B$.
Furthermore, if $\mathcal{X}$ is $GC_n$ set, there exists an additional maximal
line passing through $B$. Hence, in this case, $\mathcal{X}$ has $n+1$ maximal
lines and is Carnicer~Gasca set of degree $n$. Note that Carnicer~Gasca sets of
degree $n$ with a prescribed set of $n$ used $2$-node lines can be readily
constructed.

</details>


### [3] [Surface Stokes Without Inf-Sup Condition](https://arxiv.org/abs/2508.13342)
*Ricardo H. Nochetto,Mansur Shakipov*

Main category: math.NA

TL;DR: A new FEM method for surface Stokes equations using elliptic reformulation and lifted parametric approach, achieving robust error estimates without discrete inf-sup condition.


<details>
  <summary>Details</summary>
Motivation: To develop a numerical method for surface Stokes equations that circumvents the usual discrete inf-sup condition and provides robust error estimates for velocity-pressure pairs.

Method: Reformulate surface Stokes equations as a nonsymmetric indefinite elliptic problem governed by two Laplacians, then use lifted parametric FEM with no geometric error assumption.

Result: Proved well-posedness, quasi-best approximation in robust mesh-dependent H1-norm for any polynomial degree, and optimal L2 error estimates for velocity and pressure with mesh size depending only on Weingarten map.

Conclusion: The proposed method is both accurate and practical, working effectively for velocity-pressure pairs with equal and disparate polynomial degrees as demonstrated in numerical experiments.

Abstract: For a $d$-dimensional hypersurface of class $C^3$ without boundary, we
reformulate the surface Stokes equations as a nonsymmetric indefinite elliptic
problem governed by two Laplacians. We then use this elliptic reformulation as
a basis for a numerical method based on lifted parametric FEM. Assuming no
geometric error for simplicity, we prove its well-posedness, quasi-best
approximation in a robust mesh-dependent $H^1$-norm for any polynomial degree,
as well as an optimal $L^2$ error estimate for both velocity and pressure. This
entails a sufficiently small mesh size that solely depends on the Weingarten
map and circumvents the usual discrete inf-sup condition. We present numerical
experiments for velocity-pressure pairs with equal and disparate polynomial
degrees, demonstrating that the proposed method is both accurate and practical.

</details>


### [4] [A convergence proof for a finite element discretization of Chorin's projection method of the incompressible Navier-Stokes equations](https://arxiv.org/abs/2508.13416)
*Franziska Weber*

Main category: math.NA

TL;DR: Convergence proof for Chorin's projection method with finite elements for Navier-Stokes equations, showing numerical approximations converge to weak solutions without additional regularity assumptions.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous mathematical convergence of the widely used Chorin's projection method combined with finite element discretization for incompressible Navier-Stokes equations, particularly without requiring additional regularity beyond standard assumptions.

Method: Two-step projection method: prediction step computes intermediate velocity field, projection step enforces incompressibility by projecting onto divergence-free subspace. Uses discrete energy inequality for a priori estimates and new compactness result in L^2([0,T]×Ω).

Result: Proved convergence (up to subsequence) of numerical approximations to Leray-Hopf weak solutions of Navier-Stokes equations. Established precompactness in L^2 space and successfully passed to limit as discretization parameters vanish.

Conclusion: The paper provides a complete convergence proof for Chorin's projection method with finite elements, handling the challenge of different a priori bounds for intermediate and projected velocity fields through careful integration of estimates.

Abstract: We study Chorin's projection method combined with a finite element spatial
discretization for the time-dependent incompressible Navier-Stokes equations.
The scheme advances the solution in two steps: a prediction step, which
computes an intermediate velocity field that is generally not divergence-free,
and a projection step, which enforces incompressibility by projecting this
velocity onto the divergence-free subspace. We establish convergence, up to a
subsequence, of the numerical approximations generated by this scheme to a
Leray-Hopf weak solution of the Navier-Stokes equations, without any additional
regularity assumptions beyond square-integrable initial data. A discrete energy
inequality yields a priori estimates, which we combine with a new compactness
result to prove precompactness of the approximations in
$L^2([0,T]\times\Omega)$, where $[0,T]$ is the time interval and $\Omega$ is
the spatial domain. Passing to the limit as the discretization parameters
vanish, we obtain a weak solution of the Navier-Stokes equations. A central
difficulty is that different a priori bounds are available for the intermediate
and projected velocity fields; our compactness argument carefully integrates
these estimates to complete the convergence proof.

</details>


### [5] [Stabilization of BiCGSTAB by the generalized residual cutting method](https://arxiv.org/abs/2508.13536)
*Toshihiko Abe*

Main category: math.NA

TL;DR: Generalized Residual Cutting (GRC) method stabilizes BiCGSTAB algorithm to prevent convergence failures in solving large sparse nonsymmetric linear systems.


<details>
  <summary>Details</summary>
Motivation: BiCGSTAB, while robust and widely used for solving large sparse nonsymmetric linear systems, sometimes fails to converge due to stagnation or breakdown issues, despite being more stable than the original BiCG method.

Method: Apply the Generalized Residual Cutting (GRC) method, which was developed from the Residual Cutting method for elliptic PDEs, to stabilize the BiCGSTAB iterative algorithm and prevent convergence failures.

Result: GRC successfully stabilizes BiCGSTAB, allowing it to avoid convergence failures (stagnation or breakdown) that sometimes occur in the standard implementation.

Conclusion: The combination of GRC with BiCGSTAB enhances the robustness of the iterative solver for large sparse nonsymmetric linear systems, making it more reliable for scientific computing and engineering applications.

Abstract: The residual cutting (RC) method has been proposed as an outer-inner loop
iteration for efficiently solving large and sparse linear systems of equations
arising in solving numerically problems of elliptic partial differential
equations. Then based on RC the generalized residual cutting (GRC) method has
been introduced, which can be applied to more general sparse linear systems
problems. In this paper, we show that GRC can stabilize the BiCGSTAB, which is
also an iterative algorithm for solving large, sparse, and nonsymmetric linear
systems, and widely used in scientific computing and engineering simulations,
due to its robustness. BiCGSTAB converges faster and more smoothly than the
original BiCG method, by reducing irregular convergence behavior by stabilizing
residuals. However, it sometimes fails to converge due to stagnation or
breakdown. We attempt to emphasize its robustness by further stabililizing it
by GRC, avoiding such failures.

</details>


### [6] [Hybrid solver methods for ODEs: Machine-Learning combined with standard methods](https://arxiv.org/abs/2508.13538)
*Jürgen Geiser*

Main category: math.NA

TL;DR: Combining standard numerical methods with machine learning to accelerate ODE/PDE solvers


<details>
  <summary>Details</summary>
Motivation: To enhance traditional numerical methods for solving ODEs and PDEs by integrating machine learning techniques to accelerate the solution process

Method: Integration of standard discretization methods (Runge-Kutta, finite difference) with machine learning algorithms, particularly feedforward networks, through minimization problems

Result: Demonstrated that machine learning methods can accelerate the solver process for ODEs

Conclusion: Machine learning techniques can be effectively combined with traditional numerical methods to improve computational efficiency in solving differential equations

Abstract: In this article, we consider combined standard and machine learning methods
to solve ODEs and PDEs. We deal with the minimisation problems for the machine
learning algorithms and standard discretization methods, which are related to
Runge-Kutta methods and finite difference methods. We show, that we could solve
the ODEs with additional ML methods, e.g., feedforward network, such that it
will accelerate the solver process.

</details>


### [7] [A stability-enhanced nonstandard finite difference framework for solving one and two-dimensional nonlocal differential equations](https://arxiv.org/abs/2508.13542)
*Shweta Kumari,Mani Mehra*

Main category: math.NA

TL;DR: NSFD schemes enhance stability of explicit finite difference methods for time-fractional diffusion equations using nonstandard L1 approximation for Caputo derivatives, showing expanded stability regions and improved accuracy.


<details>
  <summary>Details</summary>
Motivation: Standard finite difference schemes suffer from limited stability regions when applied explicitly to partial differential equations, particularly for time-fractional diffusion equations.

Method: Proposed nonstandard L1 approximation for Caputo fractional derivative, constructed NSFD schemes for 1D/2D TFDEs, analyzed stability using boundary locus and discrete energy methods, validated through numerical simulations.

Result: The NSFD schemes demonstrated expanded stability regions compared to standard FD schemes, maintained convergence, and showed improved accuracy and stability in numerical experiments across various denominator functions.

Conclusion: Nonstandard finite difference schemes provide an effective approach to enhance stability of explicit methods for time-fractional diffusion equations while maintaining accuracy and convergence properties.

Abstract: Standard finite difference (SFD) schemes often suffer from limited stability
regions, especially when applied in explicit setup to partial differential
equations. To address this challenge, this study investigates the efficacy of
nonstandard finite difference (NSFD) schemes in enhancing stability of explicit
SFD schemes for 1D and 2D Caputo-type time-fractional diffusion equations
(TFDEs). A nonstandard L1approximation is proposed for the Caputo fractional
derivative, and its local truncation error is derived analytically. This
nonstandard L1 formulation is used to construct the NSFD scheme for a
Caputo-type time-fractional initial value problem. The absolute stability of
the resulting scheme is rigorously examined using the boundary locus method,
and its performance is validated through numerical simulations on test examples
for various choices of denominator functions. Based on this framework, two
explicit NSFD schemes are developed for 1D and 2D cases of the Caputo-type
TFDE. Their stability is further assessed through the discrete energy method,
with particular focus on the expansion of stability region relative to SFD
schemes. The convergence of the proposed NSFD schemes is also established.
Finally, a comprehensive set of numerical experiments is conducted to
demonstrate the accuracy and stability advantages of proposed methods, with
results presented through tables and graphical illustrations.

</details>


### [8] [A Cubed Sphere Fast Multipole Method](https://arxiv.org/abs/2508.13550)
*Anthony Chen,Robert Krasny*

Main category: math.NA

TL;DR: New Fast Multipole Method for spherical pairwise particle interactions using barycentric Lagrange interpolation on cubed sphere grid cells, kernel-independent approach with applications to Poisson/biharmonic equations, vorticity, and tidal calculations.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient method for summing pairwise particle interactions on spherical surfaces that arise from discretizing integral transforms and convolutions, with applications in geophysical and mathematical problems.

Method: Uses barycentric Lagrange interpolation on a quadtree structure composed of cubed sphere grid cells, creating a kernel-independent scheme that only requires kernel evaluations at points on the sphere.

Result: Successfully applied to Poisson and biharmonic equations on the sphere, barotropic vorticity equation on rotating sphere, and self-attraction/loading potential in tidal calculations. Both serial and parallel implementations tested.

Conclusion: The method provides an efficient, kernel-independent approach for spherical pairwise interactions with broad applicability to geophysical and mathematical problems involving spherical domains.

Abstract: This work describes a new version of the Fast Multipole Method for summing
pairwise particle interactions that arise from discretizing integral transforms
and convolutions on the sphere. The kernel approximations use barycentric
Lagrange interpolation on a quadtree composed of cubed sphere grid cells. The
scheme is kernel-independent and requires kernel evaluations only at points on
the sphere. Results are presented for the Poisson and biharmonic equations on
the sphere, barotropic vorticity equation on a rotating sphere, and
self-attraction and loading potential in tidal calculations. A tree code
version is also described for comparison, and both schemes are tested in serial
and parallel calculations.

</details>


### [9] [A kernel compression method for distributed-order fractional partial differential equations](https://arxiv.org/abs/2508.13631)
*Jonas Beddrich,Barbara Wohlmuth*

Main category: math.NA

TL;DR: A kernel compression method for solving distributed-order fractional PDEs by approximating non-local history terms with exponential sums, enabling efficient solution as local-in-time PDE systems with optimal temporal error decay.


<details>
  <summary>Details</summary>
Motivation: To efficiently solve distributed-order fractional PDEs which are computationally expensive due to their non-local nature, by transforming them into equivalent local-in-time systems that are more tractable.

Method: Discretize the fractional derivative order integral, approximate integral kernels with exponential sums using an improved algorithm, use implicit Runge-Kutta temporal discretization, and exploit PDE structure to solve single PDE per time step with linear updates for auxiliary variables.

Result: Achieved double precision accuracy with <100 exponential terms, solved linear and nonlinear DOFPDEs in 2D/3D with up to 40 million spatial degrees of freedom, demonstrating optimal temporal error decay with graded meshes.

Conclusion: The method provides a flexible and robust numerical scheme that effectively handles the computational challenges of distributed-order fractional PDEs by converting non-local problems into local systems while maintaining high accuracy and efficiency.

Abstract: We propose a kernel compression method for solving Distributed-Order (DO)
Fractional Partial Differential Equations (DOFPDEs) at the cost of solving
corresponding local-in-time PDEs. The key concepts are (1) discretization of
the integral over the order of the fractional derivative and (2) approximation
of linear combinations of integral kernels with exponential sums, expressing
the non-local history term as a sum of auxiliary variables that solve a weakly
coupled, local in time system of PDEs. For the second step, we introduce an
improved algorithm that approximates the occurring integral kernels with double
precision accuracy using only a moderate number (<100) of exponential terms.
After temporal discretization using implicit Runge--Kutta methods, we exploit
the inherent structure of the PDE system to obtain the solution at each time
step by solving a single PDE. At the same time, the auxiliary variables are
computed by a linear update, not even requiring a matrix-vector multiplication.
Choosing temporal meshes with a grading factor corresponding to the convergence
order of the Runge--Kutta schemes, we achieve the optimal decay of the temporal
discretization error. The flexibility and robustness of our numerical scheme
are illustrated by recreating well-studied test cases and solving linear and
nonlinear DOFPDEs in 2D and 3D with up to 40 million spatial degrees of
freedom.

</details>


### [10] [A convergent Fourier spectral Galerkin method for the fractional Camassa-Holm equation](https://arxiv.org/abs/2508.13683)
*Mukul Dwivedi,Andreas Rupp*

Main category: math.NA

TL;DR: Fourier spectral Galerkin method for fractional Camassa-Holm equation preserves mass/energy invariants, achieves spectral accuracy with optimal error estimates and exponential convergence for smooth solutions.


<details>
  <summary>Details</summary>
Motivation: To develop a numerical method for the fractional Camassa-Holm equation that preserves its fundamental invariants (mass and energy) while providing high accuracy and convergence properties.

Method: Fourier spectral Galerkin method with periodic boundary conditions for fractional Camassa-Holm equation involving fractional Laplacian (α ∈ [1,2]), preserving mass and energy invariants.

Result: Semi-discrete scheme preserves invariants; achieves optimal error estimates O(N^{-r}) for H^r initial data (r ≥ α+2) and exponential convergence O(e^{-cN}) for smooth solutions; numerical experiments confirm orbital stability and optimal convergence.

Conclusion: The Fourier spectral Galerkin method is effective for fractional Camassa-Holm equation, preserving invariants while providing spectral accuracy and optimal convergence rates, validated by numerical experiments.

Abstract: We analyze a Fourier spectral Galerkin method for the fractional Camassa-Holm
(fCH) equation involving a fractional Laplacian of exponent $\alpha \in [1,2]$
with periodic boundary conditions. The semi-discrete scheme preserves both mass
and energy invariants of the fCH equation. For the fractional
Benjamin-Bona-Mahony reduction, we establish existence and uniqueness of
semi-discrete solutions and prove strong convergence to the unique solution in
$ C^1([0, T];H^{\alpha}_{\mathrm{per}}(I))$ for given $T>0$. For the general
fCH equation, we demonstrate spectral accuracy in spatial discretization with
optimal error estimates $\mathcal{O}(N^{-r})$ for initial data $u_0 \in H^r(I)$
with $r \geq \alpha + 2$ and exponential convergence $\mathcal{O}(e^{-cN})$ for
smooth solutions. Numerical experiments validate orbital stability of solitary
waves achieving optimal convergence, confirming theoretical findings.

</details>


### [11] [Theory and internal structure of ADER-DG method for ordinary differential equations](https://arxiv.org/abs/2508.13824)
*I. S. Popov*

Main category: math.NA

TL;DR: Analysis of ADER-DG method's approximation properties, convergence, and stability for ODE systems, showing high-order accuracy and multiple stability properties.


<details>
  <summary>Details</summary>
Motivation: To investigate the theoretical properties of the ADER-DG method when applied to ODE systems, including its approximation capabilities, convergence behavior, and various stability characteristics.

Method: The ADER-DG method generates an implicit Runge-Kutta method and uses polynomials of degree N for numerical solutions. The analysis examines approximation order, superconvergence properties, and multiple stability criteria including A-stability, AN-stability, L-stability, B-stability, BN-stability, and algebraic stability.

Result: The method achieves approximation order 2N+1 at grid nodes using degree N polynomials, demonstrates superconvergence, and shows subgrid resolution with order N+1 for local solutions. It exhibits comprehensive stability properties across multiple stability criteria.

Conclusion: The ADER-DG method demonstrates excellent theoretical properties for ODE systems with high-order accuracy, superconvergence, and robust stability across multiple stability definitions, with applications confirming theoretical expectations.

Abstract: Investigation of the approximation properties, convergence, and stability of
the ADER-DG method for solving an ODE system is carried out. The ADER-DG method
generates a new implicit RK method, which is similar in its properties to the
original ADER-DG method. The ADER-DG method has an approximation order $2N+1$
when using polynomials of degree $N$ for the numerical solution at grid nodes,
and demonstrates superconvergence. The local solution obtained by the local DG
predictor has an approximation order $N+1$ and has a subgrid resolution. The
ADER-DG method is $A$- and $AN$-stable, $L$-stable, $B$- and $BN$-stable, and
algebraically stable. Applications of the ADER-DG method demonstrated
compliance with the expected theoretical results.

</details>


### [12] [Analysis-Aware Defeaturing of Dirichlet Features](https://arxiv.org/abs/2508.13886)
*Philipp Weder,Annalisa Buffa*

Main category: math.NA

TL;DR: This paper extends mathematical defeaturing framework to handle Dirichlet boundary conditions in Poisson problems, providing explicit error estimators for feature removal that only require boundary integrals.


<details>
  <summary>Details</summary>
Motivation: Current defeaturing operators use heuristic criteria and ignore PDE solution impact. Industrial simulation pipelines need rigorous mathematical methods to simplify geometries while maintaining solution accuracy.

Method: Extends Buffa, Chanon, and Vazquez (2022) framework to features with Dirichlet BCs. Derives a posteriori error estimators with explicit dependence on feature size, using only boundary integrals over feature boundaries.

Result: Developed error estimators successfully validated through 2D and 3D numerical experiments, showing validity and efficiency for defeaturing operations.

Conclusion: The proposed mathematical framework provides rigorous error control for defeaturing operations with Dirichlet boundary conditions, enabling more reliable geometry simplification in industrial simulations.

Abstract: Feature removal from computational geometries, or defeaturing, is an integral
part of industrial simulation pipelines. Defeaturing simplifies the otherwise
costly or even impossible meshing process, speeds up the simulation, and lowers
its memory footprint. Current defeaturing operators are often based on
heuristic criteria and ignore the impact of the simplifications on the PDE
solution. This work extends the mathematically rigorous framework developed by
Buffa, Chanon, and V\'azquez (2022) to features subject to Dirichlet boundary
conditions in Poisson problems. We derive a posteriori error estimators for
negative features in the interior or on the boundary of the computational
domain. The estimators' dependence on the feature size is explicit, and their
evaluation only involves boundary integrals over the feature boundary.
Numerical experiments in two and three dimensions showcase the validity and
efficiency of the estimators.

</details>


### [13] [Convergence analysis of a balancing domain decomposition method for an elliptic optimal control problem with HDG discretizations](https://arxiv.org/abs/2508.13997)
*Sijing Liu,Jinjin Zhang*

Main category: math.NA

TL;DR: BDDC algorithm applied to HDG discretization of elliptic optimal control problems shows robustness with regularization parameter and good scalability properties.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient domain decomposition method for solving nonsymmetric positive definite linear systems arising from HDG discretization of elliptic distributed optimal control problems, addressing challenges with regularization parameter sensitivity and scalability.

Method: Applied balancing domain decomposition by constraints (BDDC) algorithm to hybridizable discontinuous Galerkin (HDG) discretization, using BDDC-preconditioned GMRES solver for the nonsymmetric system.

Result: Convergence analysis shows algorithm robustness with respect to regularization parameter when subdomain size is sufficiently small. Number of iterations is independent of number of subdomains and only slightly dependent on subdomain problem size. Numerical experiments confirm theoretical findings.

Conclusion: The BDDC preconditioner is effective for HDG discretizations of elliptic optimal control problems, providing parameter robustness and good parallel scalability properties.

Abstract: In this work, a balancing domain decomposition by constraints (BDDC)
algorithm is applied to the nonsymmetric positive definite linear system
arising from the hybridizable discontinuous Galerkin (HDG) discretization of an
elliptic distributed optimal control problem. Convergence analysis for the BDDC
preconditioned generalized minimal residual (GMRES) solver demonstrates that,
when the subdomain size is small enough, the algorithm is robust with respect
to the regularization parameter, and the number of iterations is independent of
the number of subdomains and depends only slightly on the subdomain problem
size. Numerical experiments are performed to confirm the theoretical results.

</details>


### [14] [Convergence analysis of the dynamically regularized Lagrange multiplier method for the incompressible Navier-Stokes equations](https://arxiv.org/abs/2508.14007)
*Cao-Kha Doan,Thi-Thao-Phuong Hoang,Lili Ju,Rihui Lan*

Main category: math.NA

TL;DR: Temporal convergence analysis of Dynamically Regularized Lagrange Multiplier method for incompressible Navier-Stokes equations with optimal error estimates and numerical validation.


<details>
  <summary>Details</summary>
Motivation: To provide rigorous temporal convergence analysis for the recently introduced DRLM method, which incorporates kinetic energy evolution through dynamic regularization.

Method: Applied backward Euler method with explicit treatment of nonlinear convection term; established unique solvability and optimal error estimates using uniform bound on Lagrange multiplier and mathematical induction.

Result: Proved unique solvability of first-order DRLM scheme and established optimal error estimates for velocity and pressure; numerical results confirmed theoretical convergence rates and error bounds decaying with regularization parameter.

Conclusion: The DRLM method demonstrates strong theoretical convergence properties with optimal error estimates that are validated numerically, showing the effectiveness of the dynamic regularization approach.

Abstract: This paper is concerned with temporal convergence analysis of the recently
introduced Dynamically Regularized Lagrange Multiplier (DRLM) method for the
incompressible Navier-Stokes equations. A key feature of the DRLM approach is
the incorporation of the kinetic energy evolution through a quadratic dynamic
equation involving a time-dependent Lagrange multiplier and a regularization
parameter. We apply the backward Euler method with an explicit treatment of the
nonlinear convection term and show the unique solvability of the resulting
first-order DRLM scheme. Optimal error estimates for the velocity and pressure
are established based on a uniform bound on the Lagrange multiplier and
mathematical induction. Numerical results confirm the theoretical convergence
rates and error bounds that decay with respect to the regularization parameter.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [15] [The vanishing viscosity process for an eikonal equation in the radially symmetric setting](https://arxiv.org/abs/2508.13230)
*Fanchen Meng*

Main category: math.AP

TL;DR: Study of vanishing viscosity method for eikonal equation with radial symmetry, providing explicit formulas and proving convergence with ϵ|logϵ| rate.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of viscous solutions approaching the eikonal equation limit and establish convergence rates for radially symmetric cases.

Method: Construct explicit formulas for viscous solutions u^ϵ and limiting solution u under radial symmetry assumption, then prove qualitative and quantitative convergence.

Result: Proved u^ϵ → u as ϵ→0+ with local convergence rate of order ϵ|logϵ|, and discussed uniqueness of viscosity solutions.

Conclusion: The vanishing viscosity method converges for radially symmetric eikonal equations with explicit convergence rate, providing insights into solution uniqueness.

Abstract: We study the vanishing viscosity method for the eikonal equation $|Du|=V$ in
$B(0,1)$ with homogeneous Dirichlet boundary value condition. By assuming $V$
is radially symmetric and restricting attention to radially symmetric
solutions, we construct explicit formulas for both the viscous solution
$u^{\epsilon}$ and the limiting solution $u$. We prove $u^{\epsilon}\rightarrow
u$ as $\epsilon \rightarrow 0^+$ qualitatively and quantitatively derive an
$\epsilon |\log \epsilon|$ type local convergence rate. Finally, we discuss the
uniqueness of viscosity solutions for the eikonal equation and give some
examples.

</details>


### [16] [The Hardy spaces $\mathcal{H}^{p}_{FIO}(\mathbb{R}^{n})$ for Fourier integral operators for $p<1$](https://arxiv.org/abs/2508.13243)
*Naijia Liu,Jan Rozendaal,Liang Song*

Main category: math.AP

TL;DR: Extension of Hardy spaces for Fourier integral operators to 0<p<1 range, establishing properties like interpolation, duality, invariance, embeddings, characterizations, and molecular decompositions.


<details>
  <summary>Details</summary>
Motivation: To extend the theory of Hardy spaces for Fourier integral operators beyond the 1≤p≤∞ range to include 0<p<1, enabling broader applications in PDE analysis.

Method: Introduce and construct Hardy spaces H^p_FIO(R^n) for 0<p<1, then systematically establish their mathematical properties through theoretical analysis including complex interpolation, duality proofs, invariance under Fourier integral operators, Sobolev embeddings, equivalent characterizations, and molecular decomposition.

Result: Successfully constructed Hardy spaces for Fourier integral operators in the 0<p<1 range and proved various fundamental properties including interpolation behavior, duality relations, operator invariance, embedding theorems, and decomposition structures.

Conclusion: The developed Hardy spaces H^p_FIO(R^n) for 0<p<1 provide a complete extension of the theory and are applied in companion work to establish sharp regularity results for wave equations with rough coefficients in H^1 and bmo spaces.

Abstract: We introduce the Hardy spaces $\mathcal{H}^{p}_{FIO}(\mathbb{R}^{n})$ for
Fourier integral operators for $0<p<1$, thereby extending earlier constructions
for $1\leq p\leq \infty$. We then establish various properties of these spaces,
including their behavior under complex interpolation and duality, and their
invariance under Fourier integral operators. We also obtain Sobolev embeddings,
equivalent characterizations, and a molecular decomposition. These spaces are
used in the companion article arXiv:2502.02511 to determine the sharp
$\mathcal{H}^{1}(\mathbb{R}^{n})$ and $\mathrm{bmo}(\mathbb{R}^{n})$ regularity
of wave equations with rough coefficients.

</details>


### [17] [An Elliptic-Parabolic Free Boundary Problem with Discontinuous Data](https://arxiv.org/abs/2508.13299)
*Dennis Kriventsov,María Soria-Carro*

Main category: math.AP

TL;DR: Analysis of a 1+1D elliptic-parabolic free boundary problem modeling fluid flow in partially saturated porous media, showing optimal regularity results for solutions and free boundaries with bounded data.


<details>
  <summary>Details</summary>
Motivation: To understand how jump discontinuities in boundary and initial data affect the regularity of solutions and free boundaries in porous media flow problems, serving as foundation for higher-dimensional studies.

Method: Mathematical analysis of the 1+1 dimensional elliptic-parabolic free boundary problem, examining weak solutions and interface regularity properties.

Result: Weak solutions are Lipschitz in space and C^{1/2} in time in unsaturated regions; free boundary is locally a C^{1/2} graph, with this regularity being optimal.

Conclusion: The study provides optimal regularity results for 1D free boundary problems and establishes groundwork for analyzing local regularization in higher-dimensional elliptic-parabolic free boundaries.

Abstract: We consider an elliptic-parabolic free boundary problem that models the fluid
flow through a partially saturated porous medium. The free boundary arises as
the interface separating the saturated and unsaturated regions. Our main goal
is to investigate, for the 1+1 dimensional model, how jump discontinuities on
the boundary and initial data influence the regularity of both the solution and
the free boundary. We show that if the data is merely bounded, then weak
solutions are Lipschitz in space and $C^{1/2}$ in time in the unsaturated
region. Moreover, the free boundary is locally the graph of a $C^{1/2}$
function, and this regularity is optimal. We view this analysis as a stepping
stone towards the study of local regularization for higher-dimensional
elliptic-parabolic free boundaries.

</details>


### [18] [D-tensor paraproducts and its caricatures](https://arxiv.org/abs/2508.13322)
*Oluwadamilola Fasina*

Main category: math.AP

TL;DR: Generalization of 2-tensor paraproduct decomposition to d-tensors with Taylor expansion approximation and Calderon-Zygmund type decomposition


<details>
  <summary>Details</summary>
Motivation: Extend previous 2-tensor paraproduct decomposition results to higher-dimensional d-tensors and establish theoretical foundations for approximation methods

Method: Develop d-tensor paraproduct decomposition showing A(f) can be approximated by sum of A^α(P^{j}(f))v^α(f) terms, forming a basis for linear functionals, with residual analysis

Result: Successfully generalized decomposition to d-tensors, proved residual Δ(A,f) ∈ Λ^{2α}, obtained Calderon-Zygmund type decomposition, and validated with computational examples for d=2,3

Conclusion: The d-tensor paraproduct decomposition provides a complete theoretical framework with approximation guarantees and basis representation properties, supported by computational evidence

Abstract: We generalize the $2$-tensor paraproduct decomposition result of
arXiv:2503.12629 to $d$-tensors and expound on its caricatures. In particular,
we show that for $A \in C^{d}, f \in \Lambda^{\alpha}([0,1]^d)$, $A(f)$ has an
approximation $\tilde{A}(f) = (\sum_{\alpha=1}^d A^{\alpha}(P^{j_1,j_2, \ldots,
j_d}(f)) \tilde{\mathbf{v}}^{\alpha}(f) ) $ for a fixed sequence of scales,
$\mathbf{j} = (j_1, j_2, \ldots, j_d)$, and the series $\sum_{\alpha=1}^d
A^{\alpha}(P^{j_1,j_2, \ldots, j_d}(f)) \tilde{\mathbf{v}}^{\alpha}(f) $ is a
taylor expansion of $A$. Additionally, we show the sequence of operators
$(\tilde{\mathbf{v}}^1(f),\tilde{\mathbf{v}}^2(f), \ldots,
\tilde{\mathbf{v}}^d(f))$ form a basis for the subspace comprised of linear
functionals of the form $A(f)$, and the residual, $\Delta(A,f) = \tilde{A}(f) -
A(f) \in \Lambda^{2\alpha}([0,1]^d)$. Consequentially, we show one can obtain a
Calderon-Zygmund type decomposition, $\Delta(A,f)_\lambda +
\tilde{A}(f)_{\lambda}$ with the obtained paraproduct decomposition $A(f) =
\Delta(A,f) + \tilde{A}(f)$. Our theoretical findings are supported by
computational examples for d=2,3.

</details>


### [19] [Estimates for pseudo-differential operators on the torus revisited. III](https://arxiv.org/abs/2508.13338)
*Duván Cardona,Manuel Martínez*

Main category: math.AP

TL;DR: This paper completes the authors' work on continuity properties of toroidal pseudo-differential operators, proving pointwise estimates using Fefferman-Stein sharp maximal function and Hardy-Littlewood maximal function, and establishing boundedness theorems for weighted Lebesgue spaces on the torus.


<details>
  <summary>Details</summary>
Motivation: To extend and complete the analysis of continuity properties for toroidal pseudo-differential operators that was started in previous manuscripts, and to provide boundedness results in the context of global symbolic analysis on the torus.

Method: The authors use techniques from Park and Tomita, proving pointwise estimates with Fefferman-Stein sharp maximal function and Hardy-Littlewood maximal function, combined with properties of Muckenhoupt's weight class A_p.

Result: Boundedness theorems for pseudo-differential operators between weighted Lebesgue spaces L^p(w) on the torus, along with continuity results on Sobolev spaces W^s_p and Besolev spaces B^s_{p,q}.

Conclusion: The paper successfully extends Euclidean case results to the toroidal setting, providing comprehensive boundedness results for toroidal pseudo-differential operators and completing the authors' previous work on this topic.

Abstract: This paper finishes the goal of the authors started in two previous
manuscripts dedicated to revisiting the continuity properties of toroidal
pseudo-differential operators with symbols in the H\"ormander classes. Here we
prove pointwise estimates in terms of the Fefferman-Stein sharp maximal
function and of the Hardy-Littlewood maximal function. Combining these
estimates with the properties of Muckenhoupt's weight class $A_p$ we obtain
boundedness theorems for pseudo-differential operators between weighted
Lebesgue spaces on the torus $L^p(w)$. These results are given in the context
of the global symbolic analysis defined on $\mathbb{T}^n\times \mathbb{Z}^n$ as
developed by Ruzhansky and Turunen by using discrete Fourier analysis, and
extend those of Park and Tomita available in the Euclidean case. Moreover, we
include continuity results on Sobolev spaces $W^s_p$ and on Besov spaces
$B^s_{p,q}$ on the torus. Our techniques are taken from Park and Tomita
\cite{park-tomita} and we consider its toroidal extension here for the
completeness of the boundedness of toroidal pseudo-differential operators with
respect to the current literature.

</details>


### [20] [A Hele-Shaw problem with interior and free boundary oscillation: well-posedness and homogenization](https://arxiv.org/abs/2508.13441)
*Olga Turanova,Yuming Paul Zhang*

Main category: math.AP

TL;DR: Analysis of a 1D Hele-Shaw free boundary problem with heterogeneities on both boundary and interior, establishing well-posedness and stochastic homogenization results.


<details>
  <summary>Details</summary>
Motivation: To address free boundary problems with heterogeneities appearing both on the free boundary and within the interior domain, which presents unique mathematical challenges not fully covered by existing literature.

Method: Introduces novel viscosity flows concept for well-posedness and comparison principle, and develops a new approximation approach combining interior homogenization with free boundary propagation for stochastic homogenization.

Result: Established well-posedness and comparison principle for the heterogeneous free boundary problem, and proved stochastic homogenization under stationary ergodic coefficients assumption, with results being novel even for periodic settings.

Conclusion: The paper provides fundamental mathematical foundations for heterogeneous free boundary problems through innovative viscosity flow concepts and effective homogenization techniques that handle both interior and boundary heterogeneities simultaneously.

Abstract: We investigate a Hele-Shaw type free boundary problem in one spatial
dimension, where heterogeneities appear both on the free boundary and within
the interior of the positivity set. Our contributions are twofold. First, we
establish well-posedness and a comparison principle for the problem by
introducing a novel notion of viscosity flows. Second, under the assumption
that the coefficients are stationary ergodic, we prove a stochastic
homogenization result. Our results are new even in the periodic setting. To
derive the effective free boundary velocity, we use a new approximation that
accounts for both interior homogenization and free boundary propagation.

</details>


### [21] [Non-radial solutions for the critical quasi-linear Hénon equation involving $p$-Laplacian in $\R^N$](https://arxiv.org/abs/2508.13539)
*Wei Dai,Lixiu Duan,Changfeng Gui,Yuan Li*

Main category: math.AP

TL;DR: This paper extends classical results on non-radial solutions for Hénon equations from the Laplace operator (p=2) to the p-Laplace operator (1<p<N), proving existence of non-radial solutions at critical parameter values α(k) with specific asymptotic behavior.


<details>
  <summary>Details</summary>
Motivation: To generalize the classical work on Hénon equations involving Laplace operators to the more challenging nonlinear p-Laplace operator setting, overcoming difficulties like nonlinearity, absence of Kelvin transforms, and lack of Green representation formulas.

Method: Careful study of the linearized problem combined with approximation methods and bifurcation theory to analyze the D^{1,p}-critical quasi-linear Hénon equation with p-Laplacian.

Result: Proved that when α takes critical values α(k) for k≥2, the equation admits non-radial solutions with specific asymptotic behavior u~|x|^{-(N-p)/(p-1)} and |∇u|~|x|^{-(N-1)/(p-1)} at infinity.

Conclusion: Successfully extended classical Laplace operator results to p-Laplace operators, providing a significant generalization while overcoming substantial technical challenges inherent to the nonlinear p-Laplacian framework.

Abstract: In this paper, we investigate the following $D^{1,p}$-critical quasi-linear
H\'enon equation involving $p$-Laplacian \begin{equation*}\label{00} \left\{
\begin{aligned} &-\Delta_p u=|x|^{\alpha}u^{p_\al^*-1}, & x\in \R^N, \\ &u>0, &
x\in \R^N, \end{aligned} \right. \end{equation*} where $N\geq2$, $1<p<N$,
$p_\al^*:=\frac{p(N+\al)}{N-p}$ and $\alpha>0$. By carefully studying the
linearized problem and applying the approximation method and bifurcation
theory, we prove that, when the parameter $\al$ takes the critical values
$\al(k):=\frac{p\sqrt{(N+p-2)^2+4(k-1)(p-1)(k+N-1)}-p(N+p-2)}{2(p-1)}$ for
$k\geq2$, the above quasi-linear H\'enon equation admits non-radial solutions
$u$ such that $u\sim |x|^{-\frac{N-p}{p-1}}$ and $|\nabla u|\sim
|x|^{-\frac{N-1}{p-1}}$ at $\infty$. One should note that, $\alpha(k)=2(k-1)$
for $k\geq2$ when $p=2$. Our results successfully extend the classical work of
F. Gladiali, M. Grossi, and S. L. N. Neves in \cite{GGN} concerning the Laplace
operator (i.e., the case $p=2$) to the more general setting of the nonlinear
$p$-Laplace operator ($1<p<N$). We overcome a series of crucial difficulties,
including the nonlinear feature of the $p$-Laplacian $\Delta_p$, the absence of
Kelvin type transforms and the lack of the Green integral representation
formula.

</details>


### [22] [The asymptotic behavior of simple eigenvalues of particle-in-well systems](https://arxiv.org/abs/2508.13545)
*Peter Hintz,Aaron Moser*

Main category: math.AP

TL;DR: Study of higher-dimensional quantum wells showing smooth dependence of eigenvalues/eigenfunctions on well depth parameter h, with explicit first-order expansion at h=0


<details>
  <summary>Details</summary>
Motivation: Extend classical 1D particle-in-a-well quantum mechanics problem to higher dimensions with smooth domains, analyzing spectral properties dependence on well depth

Method: Two-step approach: 1) Construct high-order quasimodes on resolved space to capture boundary structure, 2) Correct quasimodes to true eigenfunctions via fixed point argument

Result: Proved smooth dependence of simple eigenvalues and eigenfunctions on square root of inverse well depth h, provided explicit first-order eigenvalue expansion at h=0

Conclusion: Successfully extended classical quantum well analysis to higher dimensions, establishing smooth spectral dependence on well depth parameter with precise asymptotic behavior

Abstract: The particle in a well in dimension one is a classical problem in quantum
mechanics. We study higher-dimensional analogues of the problem, where the well
is a smooth domain in $\mathbb{R}^d$. We show that simple eigenvalues and
eigenfunctions of the corresponding Schr\"odinger operator depend smoothly on
the square root $h$ of the inverse depth of the well and provide an explicit
first-order expansion of the eigenvalues at $h = 0$.
  Our proof consists of two steps. In the first step, we construct
$\mathcal{O}(h^\infty)$ quasimodes (approximate eigenfunctions) on a resolution
of $[0, 1)_h\times\mathbb{R}^d$ which allows us to capture fine structure near
the boundary of the well. The second step corrects these quasimodes to true
eigenfunctions via a fixed point argument.

</details>


### [23] [Geometric spectral properties of electromagnetic waveguides](https://arxiv.org/abs/2508.13591)
*Philippe Briet,Maxence Cassier,Thomas Ourmières-Bonafos,Michele Zaccaron*

Main category: math.AP

TL;DR: Analysis of how geometric deformations (bending and twisting) affect the spectrum of Maxwell operators in waveguides, showing conditions for preserving essential spectrum and creating discrete eigenvalues within spectral gaps.


<details>
  <summary>Details</summary>
Motivation: To understand how geometric deformations in waveguides impact the electromagnetic spectrum, particularly how bending and twisting can create discrete eigenvalues within spectral gaps that weren't present in straight waveguides.

Method: Uses Birman-Schwinger-type principle to analyze asymptotic behavior of curvature and twist, provides sufficient conditions for spectrum preservation, and studies cross-section geometry effects both analytically and numerically.

Result: Establishes conditions where geometric deformations preserve essential spectrum while creating discrete eigenvalues within spectral gaps, with localization results and stability analysis under cross-section shape deformations.

Conclusion: Geometric deformations significantly impact waveguide spectra, with specific conditions determining whether essential spectrum is preserved and discrete eigenvalues are created, particularly relevant for rectangular cross-section waveguides.

Abstract: Consider a reference homogeneous and isotropic electromagnetic waveguide with
a simply connected cross-section embedded in a perfect conductor. In this
setting, when the waveguide is straight, the spectrum of the associated
self-adjoint Maxwell operator with a constant twist (which may be zero) lies on
the real line and is symmetric with respect to zero and exhibits a spectral gap
around the origin. Moreover, the spectrum is purely essential, and contains 0
which is an eigenvalue of infinite multiplicity. In this work, we present new
results on the effects of geometric deformations, specifically bending and
twisting, on the spectrum of the Maxwell operator. More precisely, we provide,
on the one hand, sufficient conditions on the asymptotic behavior of curvature
and twist that ensure the preservation of the essential spectrum of the
reference waveguide. Our approach relies on a Birman-Schwinger-type principle,
which may be of independent interest and applicable in other contexts. On the
other hand, we give sufficient conditions (involving in particular the
geometrical shape of the cross-section of the waveguide) so that the
geometrical deformation creates discrete spectrum (namely isolated eigenvalues
of finite multiplicity) within the gap of the essential spectrum. In addition,
we give some results on the localization of these discrete eigenvalues. The
sufficient condition involving the cross-section is then studied both
analytically and numerically. Finally, we examine its stability under shape
deformations of the cross-section, focusing in particular on the case of a
waveguide with a rectangular cross-section.

</details>


### [24] [Global well-posedness of the inviscid resistive isentropic compressible MHD system](https://arxiv.org/abs/2508.13627)
*Jinkai Li,Liening Qiao*

Main category: math.AP

TL;DR: Global well-posedness and large time behavior established for inviscid resistive isentropic compressible MHD system on 3D torus with small perturbations around constant state satisfying Diophantine condition.


<details>
  <summary>Details</summary>
Motivation: Address the challenging problem of proving global solvability for inviscid compressible systems which lack dissipation mechanisms.

Method: Three-tier dissipative energy design with different Sobolev norms: high order for magnetic field, intermediate for density, low order for velocity field. Key observation is dissipation of spatial density derivatives perpendicular to magnetic field direction.

Result: First global well-posedness result for isentropic setting, demonstrating weak stabilizing effects of magnetic field on inviscid isentropic flows through generated dissipation mechanisms.

Conclusion: Magnetic field interaction with velocity field generates dissipation mechanism that stabilizes the system, enabling global solutions despite absence of viscous dissipation.

Abstract: Due to the absence of dissipation mechanism to the inviscid compressible
systems, it is a challenging problem to prove their global solvability. In this
paper, we are concerned with the initial-boundary value problem to the inviscid
and resistive isentropic compressible magnetohydrodynamic (MHD) system on three
dimensional torus $\mathbb T^3$. Global well-posedness and large time behavior
of solutions are established in the first time for the isentropic setting,
under the condition that the initial data $(\rho_0, u_0, H_0)$ is a small
perturbation around the constant state $(1, 0, w)$, with $w$ satisfying the
Diophantine condition. The main observation of this paper is that the spatial
derivatives of the density along directions perpendicular to $w$ are
dissipated. Such dissipation mechanism is generated from the interaction
between the velocity field and the background magnetic field. This verifies the
weak stabilizing effects of the magnetic filed on the dynamics in the scenario
of inviscid isentropic flows. Due to different dissipation mechanisms for the
density, velocity, and magnetic field, three ties of dissipative energies are
designed, that is, high order Sobolev norms of the perturbed magnetic field,
intermediate order Sobolev norms of the perturbed density, and low order
Sobolev norms of the velocity field.

</details>


### [25] [Contractive transport maps from $\mathbb{S}^2$ to nearly spherical surfaces with positive Ricci curvature](https://arxiv.org/abs/2508.13688)
*Jordan Serres*

Main category: math.AP

TL;DR: Every nearly spherical, positively curved surface is a contractive, volume-preserving image of a round sphere


<details>
  <summary>Details</summary>
Motivation: To establish a fundamental geometric relationship between nearly spherical surfaces with positive curvature and round spheres, providing a characterization theorem in differential geometry

Method: Combines three mathematical tools: Ricci flow on surfaces, the Kim-Milman construction, and a multiscale Bakry-Émery criterion to prove the main result

Result: Successfully proves that any nearly spherical surface with positive curvature can be obtained as a contractive, volume-preserving transformation of a round sphere

Conclusion: This establishes a deep geometric connection between nearly spherical positively curved surfaces and round spheres, with implications for understanding surface geometry and transformation properties

Abstract: We prove that every nearly spherical, positively curved surface is the
contractive, volume-preserving image of a round sphere. The proof combines
three main tools: the Ricci flow on surfaces, the Kim-Milman construction, and
a multiscale Bakry-\'Emery criterion.

</details>


### [26] [Global well-posedness for a time-fractional doubly nonlinear equation](https://arxiv.org/abs/2508.13694)
*Goro Akagi,Giacomo Enrico Sodini,Ulisse Stefanelli*

Main category: math.AP

TL;DR: Analysis of time-fractional parabolic equations with doubly nonlinear structure and maximal monotone graphs, establishing existence of global weak solutions via regularization and Galerkin methods.


<details>
  <summary>Details</summary>
Motivation: To study time-fractional parabolic equations with doubly nonlinear structure, featuring nonlinear terms both inside and outside the time differential operator, with maximal monotone graphs without growth restrictions and Lipschitz perturbations.

Method: Regularization and Galerkin approximation method to establish the existence of global weak solutions for the doubly nonlinear time-fractional parabolic equation.

Result: Existence of global weak solutions is obtained for the time-fractional parabolic equation with doubly nonlinear structure and maximal monotone graphs. Uniqueness is also discussed under additional assumptions.

Conclusion: The paper successfully establishes existence results for doubly nonlinear time-fractional parabolic equations with maximal monotone graphs, providing a foundation for further analysis of such complex nonlinear fractional models.

Abstract: We consider a time-fractional parabolic equation of doubly nonlinear type,
featuring nonlinear terms both inside and outside the differential operator in
time. The main nonlinearities are maximal monotone graphs, without restrictions
on the growth. In addition, a Lipschitz continuous perturbation is considered.
The existence of global weak solutions is obtained via a regularization and
Galerkin approximation method. Uniqueness is also discussed under some
additional assumptions.

</details>


### [27] [Geodesic convexity and strengthened functional inequalities in submanifolds of Wasserstein space](https://arxiv.org/abs/2508.13698)
*Louis-Pierre Chaintron,Daniel Lacker*

Main category: math.AP

TL;DR: The paper presents a general principle for proving geodesic convexity of functionals on submanifolds of Wasserstein spaces, showing that if the EVI gradient flow exists and leaves the submanifold invariant, then the functional is geodesically convex on that submanifold.


<details>
  <summary>Details</summary>
Motivation: To study geodesic convexity of energy and entropy functionals on non-geodesically convex submanifolds of Wasserstein spaces, which has applications in improving functional inequalities and understanding concentration of measure phenomena.

Method: Develops a general metric space principle that requires no knowledge of submanifold geodesic structure, using existence of EVI gradient flows that preserve the submanifold to prove convexity. Applies this to various settings including sphere-like submanifolds and couplings of log-concave marginals.

Result: Provides new proofs of known results (Carlen-Gangbo theorem) and new convexity results (λ-convexity on couplings of λ-log-concave marginals). Develops sufficient conditions for geodesic existence in Wasserstein submanifolds and systematically improves Talagrand and HWI inequalities.

Conclusion: The general principle offers a powerful tool for establishing convexity on submanifolds without detailed geodesic knowledge, leading to both theoretical advances and potential applications in concentration of measure estimates for conditioned empirical measures.

Abstract: We study the geodesic convexity of various energy and entropy functionals
restricted to (non-geodesically convex) submanifolds of Wasserstein spaces with
their induced geometry. We prove a variety of convexity results by means of a
simple general principle, which holds in the metric space setting, and which
crucially requires no knowledge of the structure of geodesics in the
submanifold: If the EVI gradient flow of a functional exists and leaves the
submanifold invariant, then the restriction of the functional to the
submanifold is geodesically convex. This leads to short new proofs of several
known results, such as one of Carlen and Gangbo on strong convexity of entropy
on sphere-like submanifolds, and several new results, such as the
$\lambda$-convexity of entropy on the space of couplings of
$\lambda$-log-concave marginals. Along the way, we develop sufficient
conditions for existence of geodesics in Wasserstein submanifolds. Submanifold
convexity results lead systematically to improvements of Talagrand and HWI
inequalities which we speculate to be closely related to concentration of
measure estimates for conditioned empirical measures, and we prove one rigorous
result in this direction in the Carlen-Gangbo setting.

</details>


### [28] [Reaction enhancement by flux-limited chemotaxis](https://arxiv.org/abs/2508.13704)
*Jing An,Alexander Kiselev,Yao Yao*

Main category: math.AP

TL;DR: Analysis of chemotaxis effects on reaction times using flux-limited model to address biological realism and broader parameter regimes


<details>
  <summary>Details</summary>
Motivation: Chemotaxis improves efficiency in biological processes like immune response and reproduction, but classical models have limitations like over-concentration and radial data restrictions

Method: Employed flux-limited chemotaxis model for two reacting densities, building on previous Keller-Segel approach but addressing speed limitations of biological agents

Result: Rigorous derivation of scaling laws showing how chemotaxis affects typical reaction time scale

Conclusion: The flux-limited model is more biologically reasonable and covers broader parameter regimes than classical chemotaxis models

Abstract: Chemotaxis plays a crucial role in a variety of processes in biology and
ecology. Quite often it acts to improve efficiency of biological reactions; one
example is the immune system signalling, where infected tissues release
chemokines attracting monocytes to fight invading bacteria. Another example is
reproduction, where eggs release pheromones that attract sperm. In this paper,
we analyze a system of two reacting densities, one of which is chemotactic on
another. Since the speed of any biological agents is limited, we employ flux
limited chemotaxis model. Our main result is the rigorous derivation of the
scaling laws showing how presence of chemotaxis affects the typical reaction
time scale. This work builds on the results of \cite{kiselev2022chemotaxis},
which employed a classical Keller-Segel chemotaxis term (not flux limited) -
leading to the effect of possible over concentration and restricting the
results to radial data. The model presented here is more reasonable
biologically and covers broader parameter regimes.

</details>


### [29] [Asymptotic minimality of one-dimensional transition profiles in Aviles-Giga type models: an approach via 1-currents](https://arxiv.org/abs/2508.13753)
*Radu Ignat,Roger Moser*

Main category: math.AP

TL;DR: Analysis of Modica-Mortola functionals for 2D vector fields with vanishing divergence, showing transition layers become discontinuities and deriving energy concentration estimates using geometric measure theory.


<details>
  <summary>Details</summary>
Motivation: To understand the asymptotic behavior of Modica-Mortola type functionals for vector fields when divergence converges to zero, which creates Aviles-Giga type models with degenerate transition layers.

Method: Studied energy concentration at discontinuities using geometric variational problems involving ℝ²-valued 1-currents from geometric measure theory to analyze transition profiles.

Result: Derived energy concentration estimates and established criteria determining when energetically favorable transition profiles are essentially one-dimensional.

Conclusion: The analysis provides geometric variational framework for understanding energy concentration in degenerate transition layers and identifies conditions for one-dimensional transition profiles.

Abstract: For vector fields on a two-dimensional domain, we study the asymptotic
behaviour of Modica-Mortola (or Allen-Cahn) type functionals under the
assumption that the divergence converges to $0$ at a certain rate, which
effectively produces a model of Aviles-Giga type. This problem will typically
give rise to transition layers, which degenerate into discontinuities in the
limit. We analyse the energy concentration at these discontinuities and the
corresponding transition profiles.
  We derive an estimate for the energy concentration in terms of a novel
geometric variational problem involving the notion of $\mathbb{R}^2$-valued
$1$-currents from geometric measure theory. This in turn leads to criteria,
under which the energetically favourable transition profiles are essentially
one-dimensional.

</details>


### [30] [A double-phase Neumann problem with $p=1$](https://arxiv.org/abs/2508.13772)
*Alexandros Matsoukas,Nikos Yannakakis*

Main category: math.AP

TL;DR: Existence and variational characterization of solutions for double-phase Neumann problems with p=1 via limit of p>1 solutions


<details>
  <summary>Details</summary>
Motivation: Study double-phase Neumann problems with non-homogeneous boundary conditions where the lowest exponent p equals 1, which presents mathematical challenges requiring special treatment

Method: Establish solution existence as the limit of solutions to corresponding double-phase problems with p>1, and provide variational characterization of this limit

Result: Proved existence of solutions for the p=1 case through limiting process from p>1 problems, and obtained variational characterization of the limiting solution

Conclusion: The approach successfully handles the challenging p=1 case by connecting it to better-understood p>1 problems through limit analysis and variational methods

Abstract: We study a double-phase Neumann problem with non-homogeneous boundary
conditions, where the lowest exponent $p$ is equal to 1. The existence of a
solution is established as the limit of solutions to corresponding double-phase
problems with $p>1$. We also provide a variational characterization of the
limit.

</details>


### [31] [Sharp Hardy and spectral gap inequalities on special irreversible Finsler manifolds](https://arxiv.org/abs/2508.13793)
*Sándor Kajántó*

Main category: math.AP

TL;DR: This paper provides the first examples of sharp Hardy-type inequalities on irreversible Finsler manifolds with finite reversibility, addressing a gap in the literature where sharpness was only known for reversible cases.


<details>
  <summary>Details</summary>
Motivation: Previous research showed Hardy-type inequalities fail on manifolds with infinite reversibility, but there was no evidence of sharpness on irreversible manifolds with finite reversibility. The authors aimed to find specific examples where these inequalities remain sharp.

Method: The authors constructed families of Finsler metric measure manifolds and used a Finslerian extension of the method of Riccati pairs to prove sharpness of Hardy inequalities. They established sufficient conditions for sharpness of more complex Hardy-type inequalities.

Result: The paper successfully provides two concrete examples where classical/weighted Hardy inequality (with non-positive flag curvature) and McKean-type spectral gap estimate (with strong negative flag curvature) are sharp on irreversible Finsler manifolds.

Conclusion: This work fills an important gap by demonstrating that Hardy-type inequalities can be sharp on irreversible Finsler manifolds with finite reversibility, using novel construction methods and Riccati pair techniques adapted to the Finsler setting.

Abstract: The sharpness of various Hardy-type inequalities is well-understood in the
reversible Finsler setting; while infinite reversibility implies the failure of
these functional inequalities, cf. Krist\'aly, Huang, and Zhao [Trans. Am.
Math. Soc., 2020]. However, in the remaining case of irreversible manifolds
with finite reversibility, there is no evidence on the sharpness of Hardy-type
inequalities. In fact, we are not aware of any particular examples where the
sharpness persists. In this paper we present two such examples involving two
celebrated inequalities: the classical/weighted Hardy inequality (assuming
non-positive flag curvature) and the McKean-type spectral gap estimate
(assuming strong negative flag curvature). In both cases, we provide a family
of Finsler metric measure manifolds on which these inequalities are sharp. We
also establish some sufficient conditions, which guarantee the sharpness of
more involved Hardy-type inequalities on these spaces. Our relevant technical
tool is a Finslerian extension of the method of Riccati pairs (for proving
Hardy inequalities), which also inspires the main ideas of our constructions.

</details>


### [32] [GENERIC formulation and small-angle limit for Kinetic wave equations](https://arxiv.org/abs/2508.13871)
*Manh Hong Duong,Zihui He*

Main category: math.AP

TL;DR: Formulating wave kinetic equations into GENERIC framework and deriving small-angle limit for four-wave equation similar to Boltzmann-Landau grazing limit


<details>
  <summary>Details</summary>
Motivation: To establish a connection between wave kinetic equations and the GENERIC (General Equation for Non-Equilibrium Reversible-Irreversible Coupling) framework, and to derive a small-angle limit analogous to the classical grazing collision limit in kinetic theory

Method: Formulating three-wave and four-wave kinetic equations within the GENERIC framework, and formally deriving a small-angle limit for the four-wave equation that parallels the Boltzmann-to-Landau grazing collision limit

Result: Successfully formulated both three-wave and four-wave kinetic equations into the GENERIC structure, derived the small-angle limit for the four-wave equation, and demonstrated that the limiting system maintains the GENERIC structure

Conclusion: The work provides a systematic framework for wave kinetic equations through GENERIC formalism and establishes a rigorous connection between four-wave kinetics and small-angle scattering limits, analogous to established results in particle kinetic theory

Abstract: In this paper, we formulate the three-wave and four-wave kinetic equations
into the GENERIC framework and formally derive a small-angle limit for the
four-wave equation. This limit is akin to the well-known grazing limit from the
kinetic Boltzmann equation to the kinetic Landau equation. We also show the
GENERIC structure of the limiting system.

</details>


### [33] [Effective theories for incompressible magnetoelastic shallow shells](https://arxiv.org/abs/2508.13916)
*Emanuele Tasso,Tobias Unterberger*

Main category: math.AP

TL;DR: Analysis of asymptotic behavior of thin magnetoelastic shallow shells using Γ-convergence, combining variational methods with degree theory and geometric arguments.


<details>
  <summary>Details</summary>
Motivation: To characterize the asymptotic behavior of thin magnetoelastic shallow shells, extending previous work on incompressible magnetoelastic plates and elastic shallow shells.

Method: Combination of variational methods (Γ-convergence) with degree theory, fixed-point arguments, and geometric analysis. Uses approximation by rigid movements for deformations and careful consideration of deformed domain geometry for magnetizations.

Result: Achieved compactness up to rigid motions and characterized the asymptotic behavior through Γ-convergence.

Conclusion: Successfully extended analytical framework from previous works to thin magnetoelastic shallow shells, providing rigorous mathematical characterization of their asymptotic behavior.

Abstract: We characterize the asymptotic behaviour, in the sense of
$\Gamma$-convergence, of a thin magnetoelastic shallow shell. The compactness
is achieved up to rigid motions. For deformations, it relies on an
approximation by rigid movements, whereas for magnetizations it is based on a
careful consideration of the geometry of the deformed domain. The result is
obtained by a combination of variational methods ($\Gamma$-convergence) with
degree theory, fixed-point and geometrical arguments. The proof strategy relies
on an adaptation of an analogous result for incompressible magnetoelastic
plates from M. Bresciani in arXiv:2007.14122 and an application of results by
I.Velcic on elastic shallow shells in arXiv:1102.2647.

</details>


### [34] [On the one-dimensional piston model with Large Velocity Variations](https://arxiv.org/abs/2508.13971)
*Dian Hu,Qianfeng Li,Yongqian Zhang*

Main category: math.AP

TL;DR: Analysis of 1D piston expanding into rarefied gas using asymptotic methods to prove global existence of piecewise smooth solutions with significant velocity variations.


<details>
  <summary>Details</summary>
Motivation: Investigate the dynamics of a one-dimensional piston expanding into static rarefied gas and understand the flow structure in the vanishing-density limit.

Method: Asymptotic analysis in vanishing initial density limit, derivation of sharp estimates for piston-shock distance, characteristic speed separation, and reflection coefficients. Application of method of characteristics.

Result: Proved global-in-time existence of piecewise smooth solutions with significant velocity variations. Revealed a stable mechanism operating in the vanishing-density limit.

Conclusion: The analysis successfully demonstrates the existence and stability of solutions for piston expansion in rarefied gas, providing insights into characteristic wave interactions and shock front dynamics.

Abstract: This paper investigates the dynamics of a one-dimensional piston expanding
into a static rarefied gas. Using asymptotic analysis in the limit of vanishing
initial density, we derive sharp estimates for the piston-shock distance, the
separation of characteristic speeds, and the reflection coefficient associated
with characteristic waves interacting with the leading shock front. Based on
these estimates, we apply the method of characteristics to prove the
global-in-time existence of piecewise smooth solutions. The resulting flow
structure exhibits significant velocity variations. The analysis reveals a
stable mechanism that operates in the vanishing-density limit of the piston
model.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [35] [Very High-order Compact Gas-kinetic Scheme With Discontinuity Feedback Factor](https://arxiv.org/abs/2508.13705)
*Junlei Mu,Hong Zhang,Xing Ji,Yang Zhang,Gang Chen,Kun Xu*

Main category: physics.comp-ph

TL;DR: A robust high-order compact gas-kinetic scheme with adaptive stencil extension and discontinuity feedback factor that overcomes limitations of traditional WENO and compact methods, achieving high CFL numbers (0.5+ for 9th-order) and excellent shock resolution.


<details>
  <summary>Details</summary>
Motivation: Existing high-order methods have critical limitations - WENO schemes suffer from reduced robustness at higher orders and require expensive smoothness indicators, while compact methods like DG and FR have poor time-marching efficiency with very low CFL numbers (<0.05).

Method: Combines compact gas-kinetic scheme (CGKS) with adaptive stencil extension reconstruction with discontinuity feedback factor (ASE-DFF). Uses unified framework for arbitrarily high-order compact scheme without sacrificing CFL number, and discontinuity feedback factor to eliminate expensive smoothness indicator calculations.

Result: Achieves CFL number above 0.5 for up to 9th-order cases (vs <0.05 for conventional methods), maintains first-order robustness near discontinuities, and delivers high-resolution results for flows with strong shocks and rarefaction waves.

Conclusion: Provides a practically impactful solution for high-fidelity compressible flow simulation that balances computational efficiency, high-order accuracy, and robustness in challenging flow regimes.

Abstract: This paper presents a robust and efficient very high-order scheme for
compressible flow simulation, addressing critical limitations of existing
high-order methods. The proposed scheme combines the compact gas-kinetic scheme
(CGKS) with an adaptive stencil extension reconstruction with discontinuity
feedback factor (ASE-DFF), achieving significant improvements in both
robustness and computational efficiency. Traditional weighted essentially
non-oscillatory (WENO) schemes suffer from reduced robustness at higher order
and require costly smoothness indicators for large stencils. Meanwhile, compact
methods based on Discontinuous Galerkin (DG) and Flux Reconstruction (FR)
struggle with poor time-marching efficiency. In contrast, the ASE-DFF-CGKS
introduces two key innovations: (1) a unified framework enabling arbitrarily
high-order compact gas-kinetic scheme without sacrificing large CFL number, and
(2) a discontinuity feedback factor that eliminates the need for expensive
smoothness indicator calculations while essentially keeping first-order
robustness near discontinuities. The scheme's advantages are demonstrated
through benchmark simulations. It maintains a CFL number above 0.5 for up to
9th-order case, unlike conventional compact methods that restrict a CFL less
than 0.05. Also it delivers high-resolution results for flow involving strong
shock and rarefaction wave. This work provides a practically impactful solution
for high-fidelity compressible flow simulation, balancing computational
efficiency, high-order accuracy and robustness in challenging flow regimes.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [36] [Resistive diffusion and radiative cooling effects in magnetized oblique shocks](https://arxiv.org/abs/2508.13310)
*R. Datta,E. Neill,E. Freeman,E. S. Lavine,S. Chowdhry,L. Horan IV,W. M. Potter,D. A. Hammer,B. R. Kusse,J. D. Hare*

Main category: physics.plasm-ph

TL;DR: Experimental investigation of magnetized oblique shocks in plasma flows showing shallower shock angles and higher density compression than predicted by standard models, requiring inclusion of resistive diffusion and radiative cooling effects.


<details>
  <summary>Details</summary>
Motivation: Magnetized oblique shocks are important in astrophysical systems, fusion experiments, and aerospace applications, but previous experiments focused on quasi-parallel shocks while perpendicular-type shocks with magnetic field compression need investigation.

Method: Experiments on COBRA pulsed power facility using aluminum exploding wire arrays to generate supersonic plasma flows deflected by angled obstacles. Shock imaging via laser shadowgraphy and interferometry, with flow velocity and temperature measured by optical Thomson scattering.

Result: Observed shocks exhibit shallower shock angles and higher density compression compared to canonical Rankine-Hugoniot predictions. Results are best explained by a model incorporating both resistive diffusion and radiative cooling effects.

Conclusion: Perpendicular-type magnetized oblique shocks require models that account for both resistive diffusion and radiative cooling, as these effects significantly impact shock behavior and compression characteristics in experimental conditions.

Abstract: Magnetized oblique shocks are of interest in various plasmas, including in
astrophysical systems, magneto-inertial confinement fusion experiments, and in
aerospace applications. Through experiments on the COBRA pulsed power facility
(Cornell University, 1~MA peak current, 100~ns rise time), we investigate
oblique shock formation in a system with a magnetic field, and where both
radiative cooling and resistive diffusion are important. Compared to previous
pulsed power experiments, which have investigated quasi-parallel oblique
shocks, here we consider perpendicular-type shocks, which can support magnetic
field compression. In our experiments, supersonic, super-Alfv\'enic,
collisional plasma flows, generated using an aluminum exploding wire array, are
deflected by angled obstacles to generate oblique shocks. The shocks are imaged
using laser shadowgraphy and Mach-Zehnder interferometry, while optical Thomson
scattering provides measurements of the flow velocity and temperature. The
shocks exhibit shallower shock angles and higher density compression, when
compared to canonical Rankine-Hugoniot predictions. These results are best
described by a model that includes both resistive diffusion and radiative
cooling, consistent with the values of the cooling parameter and the resistive
diffusion length in the experiment.

</details>


### [37] [Free electron charging of microdroplets in a plasma at atmospheric pressure](https://arxiv.org/abs/2508.13372)
*Nourhan Hendawy,Harold McQuaid,Somhairle Mag Uidhir,David Rutherford,Declan Diver,Davide Mariotti,Paul Maguire*

Main category: physics.plasm-ph

TL;DR: Microdroplets in low-temperature plasma acquire high charges (up to 2.5E5 electrons) and generate H2O2 at remarkable rates, with plasma enabling controlled charge levels for systematic study.


<details>
  <summary>Details</summary>
Motivation: To investigate how microdroplets interact with low-temperature plasma environments and understand the exceptional chemical properties observed in gas phase microdroplets, particularly focusing on charge acquisition and reactive species generation.

Method: Injecting microdroplets (average 15 um diameter) into a low-temperature flowing gas plasma at atmospheric pressure, measuring acquired charge, simulating particle behavior, and quantifying H2O2 formation in the liquid.

Result: Measured charges up to 2.5E5 electrons per droplet, simulations show 40% lower charge for solid particles, surface electric fields up to 1E7 V m-1 for smallest droplets, and H2O2 generation rates exceeding 275 M s-1 with concentrations up to 33 mM.

Conclusion: Plasma exposure enables controlled charging of microdroplets and enhances their exceptional chemical characteristics, though the complex interplay of mechanisms requires further systematic study using this controlled environment approach.

Abstract: Gas phase microdroplets have recently demonstrated exceptional chemical
properties via suspected mechanisms such as contact electrification and surface
charge pinning leading, in turn, to high electric fields at the surface. In
this work, microdroplets are injected into a low temperature flowing gas plasma
at atmospheric pressure and during flight they are subject to an excess free
electron flux. We report the first measurements of particle charge acquired in
a fully collisional plasma with average values up to 2.5E5 electrons, dependent
on absorbed power, for droplets with an average diameter of 15 um. Simulations
indicate that for similar plasma conditions, the acquired charge on solid
particles would be ~40% lower, at 1.5 x 105 electrons, with the difference
likely due in part to low mobility water cluster ion formation around the
evaporating droplet. Simulation also indicates surface electric fields up to
1E7 V m-1 for the smallest droplets (3 um). The formation of H2O2 was observed
in the liquid with measured values up to 33 mM, equivalent to a remarkable
generation rate of over 275 M s-1. The microdroplet system involves a complex
interplay of various potential mechanisms, yet to be elucidated. The inclusion
of a controlled plasma environment offers the capability for creating known
charge levels and defined exposure times to allow more systematic study as well
as enhancing their already exceptional characteristics.

</details>


### [38] [Impulsive excitation of a solid by extreme contrast, high intensity femtosecond laser pulses](https://arxiv.org/abs/2508.13649)
*Sagar Dam,Jian Fuh Ong,Sk Rakeeb,Ameya Parab,Aparajit C,Anandam,Amit D Lad,Yash M Ved,M Krishnamurthy,Kazuo A Tanaka,G Ravindra Kumar*

Main category: physics.plasm-ph

TL;DR: Ultra-fast dynamics of high-intensity femtosecond laser-solid interaction reveals extreme pressure and long-lived inward shock formation through pump-probe experiments and hydrodynamic simulations.


<details>
  <summary>Details</summary>
Motivation: To understand the complex interaction dynamics between high-intensity extreme contrast femtosecond lasers and solid materials, particularly the generation of extreme pressures and shock phenomena at ultra-fast timescales.

Method: Used pump-probe experiments with simultaneous Doppler spectrometry and reflectivity measurements, combined with hydrodynamic simulations to model and explain the observed physical phenomena.

Result: Discovered the presence of extreme pressure in solid density regions that triggers a long-lived (~15 ps) strong inward shock, with hydrodynamic simulations accurately replicating these experimental observations.

Conclusion: The study successfully reveals and explains the underlying physics of ultra-fast laser-solid interactions, demonstrating that extreme contrast femtosecond lasers can generate sustained shock phenomena in solids that persist for picosecond timescales.

Abstract: We present the ultra-fast dynamics of the interaction between a
high-intensity extreme contrast (expected to be around 1e-18 at hundreds of
picoseconds timescale) femtosecond laser and a solid. Simultaneous measurements
of probe Doppler spectrometry and reflectivity in pump-probe experiments reveal
the presence of extreme pressure in the solid density region, which triggers a
long-lived (about 15 ps) strong inward shock. Hydrodynamic simulations
accurately replicate these observations, providing a detailed explanation of
the underlying physics

</details>


### [39] [A Bayesian approach to time-domain Photonic Doppler Velocimetry](https://arxiv.org/abs/2508.13695)
*J. R. Allison,R. Bordas,J. Read,G. Burdiak,V. Beltrán,N. Joiner,H. Doyle,N. Hawker,J. Skidmore,T. Ao,A. Porwitzky,D. Dolan,B. Farfan,C. Johnson,A. Hansen*

Main category: physics.plasm-ph

TL;DR: Bayesian approach for Photonic Doppler Velocimetry analysis that infers velocity directly from oscilloscope data without spectrograms, validated against traditional STFT methods with consistent results but longer computation times.


<details>
  <summary>Details</summary>
Motivation: Standard PDV analysis using short-time Fourier transform requires user-chosen window parameters and can be affected by low signal-to-noise regions, motivating a more direct inference method.

Method: Bayesian inference approach that directly analyzes PDV oscilloscope traces using carefully chosen prior distributions for model parameters, avoiding spectrogram generation.

Result: Successfully recovered shock-front velocities in quartz consistent with STFT-based approach, with improved interpolation across low signal-to-noise regions, though computationally intensive (several hours for convergence).

Conclusion: The Bayesian method serves as a complementary approach to STFT-based analysis, requiring careful model specification and posterior predictive checks, but providing direct velocity inference without user-defined window parameters.

Abstract: Photonic Doppler Velocimetry (PDV) is an established technique for measuring
the velocities of fast-moving surfaces in high-energy-density experiments. In
the standard approach to PDV analysis, a short-time Fourier transform (STFT) is
used to generate a spectrogram from which the velocity history of the target is
inferred. The user chooses the form, duration and separation of the window
function. Here we present a Bayesian approach to infer the velocity directly
from the PDV oscilloscope trace, without using the spectrogram for analysis.
This is clearly a difficult inference problem due to the highly-periodic nature
of the data, but we find that with carefully chosen prior distributions for the
model parameters we can accurately recover the injected velocity from synthetic
data. We validate this method using PDV data collected at the STAR two-stage
light gas gun at Sandia National Laboratories, recovering shock-front
velocities in quartz that are consistent with those inferred using the
STFT-based approach, and are interpolated across regions of low signal-to-noise
data. Although this method does not rely on the same user choices as the STFT,
we caution that it can be prone to misspecification if the chosen model is not
sufficient to capture the velocity behavior. Analysis using posterior
predictive checks can be used to establish if a better model is required,
although more complex models come with additional computational cost, often
taking more than several hours to converge when sampling the Bayesian
posterior. We therefore recommend it be viewed as a complementary method to
that of the STFT-based approach.

</details>


### [40] [Numerical simulations of a RF-RF hybrid plasma torch with argon at atmospheric pressure](https://arxiv.org/abs/2508.13858)
*Loann Terraz,Biruk Alemu,Santiago Eizaguirre*

Main category: physics.plasm-ph

TL;DR: Numerical analysis of RF-RF hybrid torch operation showing how coil distance and high-frequency power affect minimum sustaining current, temperature profiles, velocities, and heat transfer.


<details>
  <summary>Details</summary>
Motivation: To understand the operational characteristics and optimize the performance of RF-RF hybrid torches operating at different frequencies by studying key parameters under varying conditions.

Method: Used COMSOL Multiphysics modeling software to simulate argon plasma at atmospheric pressure, analyzing coil distance variations and high-frequency power changes on sustaining current requirements.

Result: Obtained detailed evolution of radial temperature profiles, axial velocities, and heat convection at medium-frequency coil end, with comparison to total heat conduction to plasma confinement tube wall.

Conclusion: The study provides valuable numerical insights into RF-RF hybrid torch behavior, demonstrating how coil configuration and power settings influence plasma sustainability and heat transfer characteristics.

Abstract: We report numerical results regarding the minimum sustaining coil excitation
current for a RF-RF hybrid torch operating at two different frequencies. The
first coil is excited at a high-frequency, while the second coil is set at a
medium frequency. The filling gas is argon, at atmospheric pressure. We use the
modeling software COMSOL Multiphysics to describe the evolution of key
parameters when: (i) the distance between the two coils changes, (ii) the power
of the high frequency coil changes. We discuss the radial temperature profiles,
the axial velocities and the heat convected at the end of the medium-frequency
coil. The latter is compared with the total heat conduction to the plasma
confinement tube wall.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [41] [A Monte Carlo simulation on the scattering coefficients of solar radio wave propagation](https://arxiv.org/abs/2508.13494)
*Jiazhen Gan,Chuanbing Wang*

Main category: astro-ph.SR

TL;DR: Comparison of quasilinear diffusion coefficients with ray-tracing simulations for radio wave scattering in solar-terrestrial environment, showing agreement for weak scattering but discrepancies in strong scattering conditions between 2D and 3D density fluctuations.


<details>
  <summary>Details</summary>
Motivation: Radio wave scattering by small-scale density fluctuations significantly affects solar radio burst observations, and current quasilinear theory may not accurately model strong scattering conditions.

Method: Comprehensive comparison between quasilinear diffusion coefficients and ray-tracing simulations of photon trajectories in numerically generated broadband isotropic density fluctuation fields in both 2D and 3D configurations.

Result: For weak scattering, simulations agree with quasilinear theory. For strong scattering (radio frequency near electron plasma frequency or large density fluctuations), quasilinear theory underestimates scattering in 2D but overestimates in 3D. Group velocity correction improves accuracy for 3D cases.

Conclusion: Quasilinear theory has limitations in strong scattering regimes, requiring corrections for accurate modeling. The study provides improved quantification of radio wave scattering strength through density fluctuations with physical mechanisms explained.

Abstract: Radio waves undergo scattering by small-scale density fluctuations during
propagation through the solar-terrestrial environment, substantially affecting
the observed characteristics of solar radio bursts. This scattering process can
be effectively modeled as photon diffusion in phase space. In this study, we
present a comprehensive comparison between the quasilinear diffusion
coefficients and those calculated by ray-tracing the photon trajectories in
numerically generated, broadband, isotropic density fluctuation fields in both
two-dimensional (2D) and three-dimensional (3D) configurations. The comparative
analysis demonstrates that for weak scattering, the simulated diffusion
coefficients agree well with the quasilinear theoretical predictions. However,
when the radio frequency approaches the electron plasma frequency and/or the
density fluctuation amplitude becomes significant, photons experience strong
scattering. Under such conditions, the quasilinear theory tends to
underestimate the scattering strength of photons induced by 2D density
fluctuations while overestimating the scattering strength in 3D cases.
Furthermore, we implement a group velocity correction to the theoretical
diffusion coefficients, based on the effective propagation speed averaged over
all test photons. The corrected coefficients provide an accurate quantification
of the scattering strength for radio waves propagating through 3D density
fluctuations. The physical mechanisms underlying these phenomena are elucidated
in the discussion.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [42] [GRAFT: Gradient-Aware Fast MaxVol Technique for Dynamic Data Sampling](https://arxiv.org/abs/2508.13653)
*Ashish Jha,Anh huy Phan,Razan Dibo,Valentin Leplat*

Main category: cs.LG

TL;DR: GRAFT is an efficient in-training subset selection method that reduces computational costs and environmental impact by selecting diverse, representative examples from low-rank feature representations instead of using full batches.


<details>
  <summary>Details</summary>
Motivation: Training modern neural networks on large datasets is computationally intensive and environmentally costly due to high energy consumption and CO2 emissions. There's a need for methods that can reduce these costs while maintaining training effectiveness.

Method: GRAFT extracts low-rank feature representations for each batch, applies a Fast MaxVol sampler to select a small diverse subset that spans the batch's dominant subspace, and dynamically adjusts subset size using a gradient-approximation criterion.

Result: GRAFT matches or exceeds recent selection baselines in both accuracy and efficiency across multiple benchmarks, while significantly reducing wall-clock time, energy consumption, and CO2 emissions.

Conclusion: GRAFT provides a favorable trade-off between accuracy, efficiency, and environmental impact, making it a scalable solution for reducing the computational and environmental costs of neural network training.

Abstract: Training modern neural networks on large datasets is computationally and
environmentally costly. We introduce GRAFT, a scalable in-training subset
selection method that (i) extracts a low-rank feature representation for each
batch, (ii) applies a Fast MaxVol sampler to select a small, diverse subset
that spans the batch's dominant subspace, and (iii) dynamically adjusts the
subset size using a gradient-approximation criterion. By operating in low-rank
subspaces and training on carefully chosen examples instead of full batches,
GRAFT preserves the training trajectory while reducing wall-clock time, energy
consumption, and $\mathrm{CO}_2$ emissions. Across multiple benchmarks, GRAFT
matches or exceeds recent selection baselines in both accuracy and efficiency,
providing a favorable trade-off between accuracy, efficiency, and emissions.

</details>


### [43] [GDNSQ: Gradual Differentiable Noise Scale Quantization for Low-bit Neural Networks](https://arxiv.org/abs/2508.14004)
*Sergey Salishev,Ian Akhremchik*

Main category: cs.LG

TL;DR: A novel quantization method that treats neural networks as noisy channels, using differentiable STE with learnable parameters and exterior-point penalty to achieve competitive accuracy even at extreme W1A1 quantization while maintaining efficiency.


<details>
  <summary>Details</summary>
Motivation: Quantization reduces neural network capacity as bit-width decreases, creating bottlenecks. The paper aims to track capacity dynamics and identify quantization bottlenecks by framing fine-tuning as a constrained optimization problem.

Method: Uses a fully differentiable Straight-Through Estimator (STE) with learnable bit-width, noise scale and clamp bounds. Enforces target bit-width via exterior-point penalty and stabilizes training with metric smoothing through distillation.

Result: Achieves competitive accuracy down to the extreme W1A1 (1-bit weights, 1-bit activations) quantization setting while retaining the computational efficiency of STE-based approaches.

Conclusion: The proposed method successfully addresses quantization bottlenecks through a constrained optimization framework with learnable parameters, demonstrating that simple yet effective techniques can achieve extreme quantization without sacrificing too much accuracy.

Abstract: Quantized neural networks can be viewed as a chain of noisy channels, where
rounding in each layer reduces capacity as bit-width shrinks; the
floating-point (FP) checkpoint sets the maximum input rate. We track capacity
dynamics as the average bit-width decreases and identify resulting quantization
bottlenecks by casting fine-tuning as a smooth, constrained optimization
problem. Our approach employs a fully differentiable Straight-Through Estimator
(STE) with learnable bit-width, noise scale and clamp bounds, and enforces a
target bit-width via an exterior-point penalty; mild metric smoothing (via
distillation) stabilizes training. Despite its simplicity, the method attains
competitive accuracy down to the extreme W1A1 setting while retaining the
efficiency of STE.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [44] [A Screw Approach to the Approximation of the Local Geometry of the Configuration Space and of the set of Configurations of Certain Rank of Lower Pair Linkages](https://arxiv.org/abs/2508.13802)
*Andreas Mueller*

Main category: math.DG

TL;DR: Higher-order local mobility analysis for multi-loop linkages using Taylor expansion of geometric constraints and joint screws, addressing limitations of smooth motion assumptions in existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing higher-order mobility analysis methods assume smooth motions, limiting their generality. The paper aims to develop a more comprehensive approach that can handle constraint singularities and non-smooth motions in multi-loop linkages.

Method: Uses higher-order Taylor series expansion of geometric constraint mapping with recursive algebraic expressions in terms of joint screws. Includes local approximation of configurations with certain rank and explicit expressions for differentials of Jacobian minors.

Result: The approach successfully analyzes complex singularities including bifurcation singularities in 4-bar linkages and cusps in c-space for planar three-loop linkages that cannot be handled by existing methods.

Conclusion: The presented higher-order local mobility analysis method provides a more general framework that can handle constraint singularities and complex c-space geometries beyond the limitations of smooth motion assumptions in previous approaches.

Abstract: A motion of a mechanism is a curve in its configuration space (c-space).
Singularities of the c-space are kinematic singularities of the mechanism. Any
mobility analysis of a particular mechanism amounts to investigating the
c-space geometry at a given configuration. A higher-order analysis is necessary
to determine the finite mobility. To this end, past research lead to approaches
using higher-order time derivatives of loop closure constraints assuming
(implicitly) that all possible motions are smooth. This continuity assumption
limits the generality of these methods. In this paper an approach to the
higher-order local mobility analysis of lower pair multi-loop linkages is
presented. This is based on a higher-order Taylor series expansion of the
geometric constraint mapping, for which a recursive algebraic expression in
terms of joint screws is presented. An exhaustive local analysis includes
analysis of the set of constraint singularities (configurations where the
constraint Jacobian has certain corank). A local approximation of the set of
configurations with certain rank is presented, along with an explicit
expression for the differentials of Jacobian minors in terms of instantaneous
joint screws. The c-space and the set of points of certain corank are therewith
locally approximated by an algebraic variety determined algebraically from the
mechanism's screw system. Results are shown for a simple planar 4-bar linkage,
which exhibits a bifurcation singularity, and for a planar three-loop linkage
exhibiting a cusp in c-space. The latter cannot be treated by the higher-order
local analysis methods proposed in the literature.

</details>


### [45] [The Bernstein problem for Sobolev intrinsic graphs in the Heisenberg group](https://arxiv.org/abs/2508.13717)
*Sebastiano Nicolussi Golo,Francesco Serra Cassano,Mattia Vedovato*

Main category: math.DG

TL;DR: Stable intrinsic graphs in the Heisenberg group must be intrinsic planes under appropriate integrability conditions, extending previous Lipschitz results.


<details>
  <summary>Details</summary>
Motivation: To extend the understanding of stable minimal surfaces in sub-Riemannian geometry beyond the Lipschitz class, particularly in the Heisenberg group where previous results were limited to Lipschitz functions.

Method: Study entire, locally Sobolev intrinsic graphs that are stable for the sub-Riemannian area in the first Heisenberg group. Apply appropriate integrability conditions for the derivatives to analyze stability properties.

Result: Under the specified integrability conditions, any such stable intrinsic graph must be an intrinsic plane - specifically, a coset of a two-dimensional subgroup. This generalizes previous results from the Lipschitz class to a broader function class.

Conclusion: The research successfully extends the classification of stable minimal surfaces in the Heisenberg group, showing that intrinsic planes are the only possible stable configurations under given conditions, even beyond the Lipschitz framework.

Abstract: In the first Heisenberg group, we study entire, locally Sobolev intrinsic
graphs that are stable for the sub-Riemannian area. We show that, under
appropriate integrability conditions for the derivatives, the intrinsic graph
must be an intrinsic plane, i.e., a coset of a two dimensional subgroup. This
result extends \cite{arXiv:1809.04586} beyond the Lipschitz class.

</details>


<div id='math.FA'></div>

# math.FA [[Back]](#toc)

### [46] [Nearly Optimal Bounds on the Fourier sampling numbers of Besov Spaces](https://arxiv.org/abs/2508.13991)
*Jonathan W. Siegel*

Main category: math.FA

TL;DR: Optimal recovery of functions from Fourier samples on d-dimensional torus using Besov space smoothness assumptions, with characterization via Fourier sampling numbers and asymptotic gap analysis.


<details>
  <summary>Details</summary>
Motivation: To understand the fundamental limits of recovering functions from Fourier coefficient samples, particularly for functions with classical smoothness properties in Besov spaces, and to characterize optimal recovery error.

Method: Theoretical analysis using restricted Gelfand widths (Fourier sampling numbers), asymptotic analysis of sampling numbers, construction of nearly optimal Fourier measurements and recovery algorithms, and novel lower bound proofs.

Result: Determined correct asymptotics of Fourier sampling numbers (up to logarithmic factors) for regime s/d > 1 - 1/p, identified asymptotic gap between Fourier sampling numbers and Gelfand widths for specific parameter ranges (q=1, p0<p≤2), and provided practical implications for edge recovery.

Conclusion: The paper establishes fundamental limits for Fourier-based function recovery, provides optimal measurement strategies, and reveals important gaps between different complexity measures, with practical applications in edge-preserving reconstruction.

Abstract: Let $\mathbb{T}^d$ denote the $d$-dimensional torus. We consider the problem
of optimally recovering a target function $f^*:\mathbb{T}^d\rightarrow
\mathbb{C}$ from samples of its Fourier coefficients. We make classical
smoothness assumptions on $f^*$, specifically that $f^*$ lies in a Besov space
$B^s_\infty(L_q)$ with $s > 0$ and $1\leq q\leq \infty$, and measure recovery
error in the $L_p$-norm with $1\leq p\leq \infty$. Abstractly, the optimal
recovery error is characterized by a `restricted' version of the Gelfand
widths, which we call the Fourier sampling numbers. Up to logarithmic factors,
we determine the correct asymptotics of the Fourier sampling numbers in the
regime $s/d > 1 - 1/p$. We also give a description of nearly optimal Fourier
measurements and recovery algorithms in each of these cases. In the other
direction, we prove a novel lower bound showing that there is an asymptotic gap
between the Fourier sampling numbers and the Gelfand widths when $q = 1$ and
$p_0 < p\leq 2$ with $p_0 \approx 1.535$. Finally, we discuss the practical
implications of our results, which imply a sharper recovery of edges, and
provide numerical results demonstrating this phenomenon.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [47] [AI-Augmented Photon-Trapping Spectrometer-on-a-Chip on Silicon Platform with Extended Near-Infrared Sensitivity](https://arxiv.org/abs/2508.13521)
*Ahasan Ahamed,Htet Myat,Amita Rawat,Lisa N McPhillips,M Saif Islam*

Main category: physics.optics

TL;DR: Compact AI-enhanced silicon spectrometer with photon-trapping photodiodes achieves high-resolution NIR hyperspectral imaging up to 1100nm using only 16 detectors and neural network reconstruction.


<details>
  <summary>Details</summary>
Motivation: Traditional spectrometers fail beyond 950nm due to poor detector efficiency and noise, limiting NIR applications. There's a need for compact, noise-resilient spectrometers for portable real-time applications.

Method: Monolithically fabricated silicon photodiodes with photon-trapping surface textures for improved NIR responsivity, combined with a fully connected neural network for spectral reconstruction from 16 engineered detectors.

Result: Achieves <0.05 RMSE and 8nm resolution over 640-1100nm range, maintains >30dB SNR even with 40dB added noise, 0.4mm2 footprint, 50dB dynamic range, 57ps response time, and >7000 photodiode gain.

Conclusion: The AI-augmented silicon spectrometer enables high-performance hyperspectral sensing in CMOS-compatible platform, suitable for biomedical imaging, environmental monitoring, and remote sensing applications.

Abstract: We present a compact, noise-resilient reconstructive spectrometer-on-a-chip
that achieves high-resolution hyperspectral imaging across an extended
near-infrared (NIR) range up to 1100nm. The device integrates monolithically
fabricated silicon photodiodes enhanced with photon-trapping surface textures
(PTST), enabling improved responsivity in the low-absorption NIR regime.
Leveraging a fully connected neural network, we demonstrate accurate spectral
reconstruction from only 16 uniquely engineered detectors, achieving <0.05 RMSE
and 8nm resolution over a wide spectral range of 640nm to 1100nm. Our system
outperforms conventional spectrometers, maintaining signal-to-noise ratio above
30dB even with 40dB of added detector noise; extending functionality to longer
wavelengths up to 1100nm, while the traditional spectrometers fail to perform
beyond 950nm due to poor detector efficiency and noise performance. With a
footprint of 0.4mm2, dynamic range of 50dB, ultrafast time response (57ps), and
high photodiode gain (>7000), this AI-augmented silicon spectrometer is
well-suited for portable, real-time, and low-light applications in biomedical
imaging, environmental monitoring, and remote sensing. The results establish a
pathway toward fully integrated, high-performance hyperspectral sensing in a
CMOS-compatible platform.

</details>


### [48] [Quasi-Minnaert Resonances in High-contrast acoustic Structures and Applications to Invisibility Cloaking](https://arxiv.org/abs/2508.13659)
*Weisheng Zhou,Huaian Diao,Hongyu Liu*

Main category: physics.optics

TL;DR: Novel quasi-Minnaert resonance in acoustic wave propagation through high-contrast media, featuring boundary localization and surface resonance with continuous spectral spectrum, enabling invisibility cloaking effects.


<details>
  <summary>Details</summary>
Motivation: To investigate acoustic wave phenomena in high-contrast media that differ significantly from homogeneous backgrounds, particularly exploring resonance effects that could enable advanced acoustic technologies like cloaking.

Method: Used layer potential theory and rigorous asymptotic analysis to study wave coupling with high-contrast material structures, validated through extensive numerical experiments with radial geometries (disks, spheres) and general-shaped geometries (hearts, Cassini ovals, clovers).

Result: Demonstrated that quasi-Minnaert resonances occur through specific coupling between high-contrast structures and incident waves, showing boundary localization and surface resonance phenomena, and numerically confirmed these resonances induce invisibility cloaking effects.

Conclusion: Quasi-Minnaert resonances represent a significant advancement in acoustic wave manipulation with continuous spectral spectrum, offering promising applications in mathematical material science and acoustic cloaking technology development.

Abstract: This paper investigates a novel quasi-Minnaert resonance phenomenon in
acoustic wave propagation through high-contrast medium in both two and three
dimensions, occurring in the sub-wavelength regime. These media are
characterized by physical properties significantly distinct from those of a
homogeneous background. The quasi-Minnaert resonance is defined by two primary
features: boundary localization, where the $L^2$-norms of the internal total
field and the external scattered field exhibit pronounced concentration near
the boundary, and surface resonance, marked by highly oscillatory behavior of
the fields near the boundary. In contrast to classical Minnaert resonances,
which are associated with a discrete spectral spectrum tied to physical
parameters, quasi-Minnaert resonances exhibit analogous physical phenomena but
with a continuous spectral spectrum. Using layer potential theory and rigorous
asymptotic analysis, we demonstrate that the coupling between a high-contrast
material structure, particularly with radial geometries, and a carefully
designed incident wave is critical for inducing quasi-Minnaert resonances.
Extensive numerical experiments, involving radial geometries (e.g., unit disks
and spheres) and general-shaped geometries (e.g., hearts, Cassini ovals, and
clovers in $\mathbb{R}^2$, and spheres in $\mathbb{R}^3$), validate the
occurrence of these resonances. Furthermore, we numerically demonstrate that
quasi-Minnaert resonances induce an invisibility cloaking effect in the
high-contrast medium. These findings have significant implications for
mathematical material science and the development of acoustic cloaking
technologies.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [49] [Generative Super-Resolution of Turbulent Flows via Stochastic Interpolants](https://arxiv.org/abs/2508.13770)
*Martin Schiødt,Nikolaj Takata Mücke,Clara Marika Velte*

Main category: physics.flu-dyn

TL;DR: Stochastic interpolants enable patch-wise super-resolution of turbulent flows, outperforming other generative models in reconstructing fine-scale dynamics from low-resolution data.


<details>
  <summary>Details</summary>
Motivation: Turbulent flows have complex multiscale features that are challenging to capture due to limited experimental resolution and high computational costs of simulations, leaving fine-scale dynamics unresolved in practical scenarios.

Method: Uses stochastic interpolants applied iteratively over local patches of flow fields to perform super-resolution, enabling efficient reconstruction without processing the full domain simultaneously.

Result: Patch-wise approach produces physically consistent super-resolved flow snapshots, accurately recovering key statistical quantities like kinetic energy spectrum and dissipation rate, with higher quality than full-field reconstruction.

Conclusion: Stochastic interpolants are established as a viable tool for super-resolving turbulent flows and show potential for future applications in flow reconstruction.

Abstract: Capturing the intricate multiscale features of turbulent flows remains a
fundamental challenge due to the limited resolution of experimental data and
the computational cost of high-fidelity simulations. In many practical
scenarios only coarse representations of the flows are feasible, leaving
crucial fine-scale dynamics unresolved. This study addresses that limitation by
leveraging generative models to perform super-resolution of velocity fields and
reconstruct the unresolved scales from low-resolution conditionals. In
particular, the recently formalized stochastic interpolants are employed to
super-resolve a case study of two-dimensional turbulence. Key to our approach
is the iterative application of stochastic interpolants over local patches of
the flow field, that enables efficient reconstruction without the need to
process the full domain simultaneously. The patch-wise strategy is shown to
yield physically consistent super-resolved flow snapshots, and key statistical
quantities -- such as the kinetic energy spectrum and the spatially averaged
dissipation rate -- are accurately recovered. Moreover, compared with
full-field reconstruction, the patch-wise approach produces higher-quality
super-resolutions, and, in general, stochastic interpolants are observed to
outperform contesting generative models across a range of metrics. These
results establish stochastic interpolants as a viable tool for super-resolving
turbulent flows and highlight their potential for future applications.

</details>


### [50] [OpenLB-UQ: An Uncertainty Quantification Framework for Incompressible Fluid Flow Simulations](https://arxiv.org/abs/2508.13867)
*Mingliang Zhong,Adrian Kummerländer,Shota Ito,Mathias J. Krause,Martin Frank,Stephan Simonis*

Main category: physics.flu-dyn

TL;DR: OpenLB-UQ integrates uncertainty quantification into the OpenLB lattice Boltzmann library using non-intrusive stochastic collocation methods and Monte Carlo sampling, validated on benchmark cases with promising scalability and accuracy.


<details>
  <summary>Details</summary>
Motivation: Uncertainty quantification is crucial for assessing reliability in computational fluid dynamics simulations due to input parameter uncertainties, but existing tools need efficient integration with large-scale flow simulation libraries.

Method: Leverages OpenLB's efficiency for large-scale flow sampling with integrated UQ module using non-intrusive stochastic collocation methods based on generalized polynomial chaos and Monte Carlo sampling.

Result: Extensive validation shows expected convergence rates, promising scalability, robust statistical accuracy, and computational efficiency on benchmark cases including 2D Taylor-Green vortex flows and flow past a cylinder.

Conclusion: OpenLB-UQ successfully enhances OpenLB's capabilities, providing researchers with a scalable framework for uncertainty quantification in incompressible fluid flow simulations and beyond.

Abstract: Uncertainty quantification (UQ) is crucial in computational fluid dynamics to
assess the reliability and robustness of simulations, given the uncertainties
in input parameters. OpenLB is an open-source lattice Boltzmann method library
designed for efficient and extensible simulations of complex fluid dynamics on
high-performance computers. In this work, we leverage the efficiency of OpenLB
for large-scale flow sampling with a dedicated and integrated UQ module. To
this end, we focus on non-intrusive stochastic collocation methods based on
generalized polynomial chaos and Monte Carlo sampling. The OpenLB-UQ framework
is extensively validated in convergence tests with respect to statistical
metrics and sample efficiency using selected benchmark cases, including
two-dimensional Taylor--Green vortex flows with up to four-dimensional
uncertainty and a flow past a cylinder. Our results confirm the expected
convergence rates and show promising scalability, demonstrating robust
statistical accuracy as well as computational efficiency. OpenLB-UQ enhances
the capability of the OpenLB library, offering researchers a scalable framework
for UQ in incompressible fluid flow simulations and beyond.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [51] [Extraction of the self energy and Eliashberg function from angle resolved photoemission spectroscopy using the \textsc{xARPES} code](https://arxiv.org/abs/2508.13845)
*Thomas P. van Waas,Christophe Berthod,Jan Berges,Nicola Marzari,J. Hugo Dil,Samuel Poncé*

Main category: cond-mat.mtrl-sci

TL;DR: Novel method for extracting self-energies from angle-resolved photoemission spectroscopy data using maximum-entropy method and Bayesian inference, enabling consistent analysis of curved dispersions without manual parameter assignment.


<details>
  <summary>Details</summary>
Motivation: Existing methods for decomposing spectral functions rely on linearization of bands and manual assignment of self-energy magnitudes, which limits accuracy and consistency in analyzing anisotropic many-body interactions.

Method: Extended maximum-entropy method to Eliashberg-function extraction with Bayesian inference, optimizing dispersion parameters and magnitudes of electron-electron and electron-impurity interactions. Developed Python code xARPES to implement these functionalities.

Result: Successfully identified phonon modes of 2D electron liquid on TiO2-terminated SrTiO3 and achieved unprecedented agreement between two Eliashberg functions of Li-doped graphene extracted from separate dispersions. Demonstrated superior performance compared to state-of-the-art approaches on model data.

Conclusion: The novel methodology provides a consistent framework for extracting self-energies from curved dispersions, enabling more accurate analysis of many-body interactions in angle-resolved photoemission spectroscopy data.

Abstract: Angle-resolved photoemission spectroscopy is a powerful experimental
technique for studying anisotropic many-body interactions through the electron
spectral function. Existing attempts to decompose the spectral function into
non-interacting dispersions and electron-phonon, electron-electron, and
electron-impurity self-energies rely on linearization of the bands and manual
assignment of self-energy magnitudes. Here, we show how self-energies can be
extracted consistently for curved dispersions. We extend the maximum-entropy
method to Eliashberg-function extraction with Bayesian inference, optimizing
the parameters describing the dispersions and the magnitudes of
electron-electron and electron-impurity interactions. We compare these novel
methodologies with state-of-the-art approaches on model data, then demonstrate
their applicability with two high-quality experimental data sets. With the
first set, we identify the phonon modes of a two-dimensional electron liquid on
TiO$_2$-terminated SrTiO$_3$. With the second set, we obtain unprecedented
agreement between two Eliashberg functions of Li-doped graphene extracted from
separate dispersions. We release these functionalities in the novel Python code
\textsc{xARPES}.

</details>


<div id='math.SP'></div>

# math.SP [[Back]](#toc)

### [52] [Hot spots in domains of constant curvature](https://arxiv.org/abs/2508.13353)
*Lawford Hatcher*

Main category: math.SP

TL;DR: The paper proves constant-curvature versions of hot spots conjecture results in 2D, showing it holds for non-acute geodesic triangles in negative curvature, establishes properties of eigenfunctions on constant curvature triangles, and shows second Neumann eigenfunctions have finite critical points in most simply connected polygons.


<details>
  <summary>Details</summary>
Motivation: To extend the hot spots conjecture and related eigenfunction analysis from Euclidean geometry to constant curvature spaces (both positive and negative curvature), providing geometric insights in non-Euclidean settings.

Method: Mathematical proof techniques involving geometric analysis on constant curvature surfaces, studying Laplace eigenfunctions on geodesic triangles and polygons with Dirichlet-Neumann boundary conditions, and employing Killing field monotonicity properties.

Result: Proved hot spots conjecture for non-acute geodesic triangles in constant negative curvature; showed first mixed Dirichlet-Neumann eigenfunctions have no non-vertex critical points and are monotonic with respect to some Killing field; demonstrated second Neumann eigenfunctions have finitely many critical points in most simply connected polygons.

Conclusion: The hot spots conjecture and related eigenfunction properties can be successfully extended to constant curvature geometries, with most results holding analogously to the Euclidean case, though with some specific exceptions in certain geometric configurations.

Abstract: We prove constant-curvature analogues of several results regarding the hot
spots conjecture in dimension two. Our main theorem shows that the hot spots
conjecture holds for all non-acute geodesic triangles of constant negative
curvature. We also prove that, under certain circumstances, on constant
(positive or negative) curvature triangles, first mixed Dirichlet-Neumann
Laplace eigenfunctions have no non-vertex critical points. Moreover, we show
that each of these eigenfunctions is monotonic with respect to some Killing
field. Finally, we show that for general simply connected polygons of non-zero
constant curvature--with exactly one family of exceptions--second Neumann
eigenfunctions of the Laplacian have at most finitely many critical points.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [53] [Propagation of Singularities for the Damped Stochastic Klein-Gordon Equation](https://arxiv.org/abs/2508.13671)
*Hongyi Chen,Cheuk Yin Lee*

Main category: math.PR

TL;DR: Random singularities with law of iterated logarithm behavior exist and propagate in 1+1 dimensional damped stochastic Klein-Gordon equation, similar to stochastic wave equation.


<details>
  <summary>Details</summary>
Motivation: To investigate the connection between damped stochastic Klein-Gordon equations and microlocal analysis, particularly wavefront set descriptions of singularities determined by highest order terms of the linear operator.

Method: Analysis of the 1+1 dimensional damped stochastic Klein-Gordon equation, with proofs that differ significantly from those for the wave equation. The approach shows that proving results for the critically damped equation implies them for the general equation.

Result: The paper demonstrates that random singularities associated with the law of iterated logarithm exist and propagate in the same manner as in the stochastic wave equation.

Conclusion: The findings provide evidence for connections to microlocal analysis and show that despite identical results to the wave equation, the proofs are substantially different and more intuitive from a PDE perspective.

Abstract: For the $1+1$ dimensional damped stochastic Klein-Gordon equation, we show
that random singularities associated with the law of the iterated logarithm
exist and propogate in the same way as the stochastic wave equation. This
provides evidence for possible connections to microlocal analysis, ie. the
exact regularity and singularities described in this paper should admit
wavefront set type descriptions whose propagation is determined by the highest
order terms of the linear operator. Despite the results being exactly the same
as those of the wave equation, our proofs are significantly different than the
proofs for the wave equation. Miraculously, proving our results for the
critically damped equation implies them for the general equation, which
significantly simplifies the problem. Even after this simplification, many
important parts of the proof are significantly different than (and we think are
more intuitive from the PDE viewpoint compared to) existing proofs for the wave
equation.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [54] [Physics-Informed Neural Networks for Programmable Origami Metamaterials with Controlled Deployment](https://arxiv.org/abs/2508.13559)
*Sukheon Kang,Youngkwon Kim,Jinkyu Yang,Seunghwa Ryu*

Main category: cond-mat.soft

TL;DR: A physics-informed neural network framework for forward prediction and inverse design of conical Kresling origami without training data, enabling programmable energy landscapes and hierarchical deployment sequences.


<details>
  <summary>Details</summary>
Motivation: Origami-inspired structures offer lightweight, deployable systems but face design challenges due to complex nonlinear mechanics, multistability, and precise deployment force control requirements.

Method: Physics-informed neural network (PINN) framework that embeds mechanical equilibrium equations directly into learning, enabling both forward prediction and inverse design without pre-collected training data.

Result: The model accurately predicts complete energy landscapes, enables freeform programming of entire energy curves with specified stable-state heights and energy barriers, and successfully designs hierarchical assemblies with sequential deployment.

Conclusion: This work establishes a versatile, data-free approach for programming complex mechanical energy landscapes in origami metamaterials, with broad applications in aerospace systems, morphing structures, and soft robotics.

Abstract: Origami-inspired structures provide unprecedented opportunities for creating
lightweight, deployable systems with programmable mechanical responses.
However, their design remains challenging due to complex nonlinear mechanics,
multistability, and the need for precise control of deployment forces. Here, we
present a physics-informed neural network (PINN) framework for both forward
prediction and inverse design of conical Kresling origami (CKO) without
requiring pre-collected training data. By embedding mechanical equilibrium
equations directly into the learning process, the model predicts complete
energy landscapes with high accuracy while minimizing non-physical artifacts.
The inverse design routine specifies both target stable-state heights and
separating energy barriers, enabling freeform programming of the entire energy
curve. This capability is extended to hierarchical CKO assemblies, where
sequential layer-by-layer deployment is achieved through programmed barrier
magnitudes. Finite element simulations and experiments on physical prototypes
validate the designed deployment sequences and barrier ratios, confirming the
robustness of the approach. This work establishes a versatile, data-free route
for programming complex mechanical energy landscapes in origami-inspired
metamaterials, offering broad potential for deployable aerospace systems,
morphing structures, and soft robotic actuators.

</details>


<div id='cond-mat.supr-con'></div>

# cond-mat.supr-con [[Back]](#toc)

### [55] [Uniform electron benchmark for the first-principles $GW_{0}$-Eliashberg theory](https://arxiv.org/abs/2508.13779)
*Ryosuke Akashi,Hiroshi Shinaoka*

Main category: cond-mat.supr-con

TL;DR: Numerical study of Eliashberg equations for phonon-mediated superconductivity using consistent GW0 approximation, revealing interplay between electron-phonon interactions and plasmon effects on superconducting transition temperature.


<details>
  <summary>Details</summary>
Motivation: To investigate the numerical behavior of Eliashberg equations and understand how electron-phonon interactions and screened Coulomb effects influence superconductivity, providing benchmarks for first-principles calculations.

Method: Used consistent GW0 approximation with full wavenumber and frequency dependences of screened Coulomb interaction and phonon-mediated attraction. Applied intermediate representation and Fourier convolution techniques for efficient computation at low temperatures on uniform electron gas with model Einstein phonons.

Result: Revealed interplay between electron-phonon ω-mass and k-mass renormalizations in determining normal-state effective mass, spectral weight, and superconducting transition temperature. Identified electron density regimes where plasmon effect enhances or suppresses phonon-mediated superconductivity beyond static Coulomb effects.

Conclusion: The comprehensive Eliashberg calculations provide valuable benchmarks for first-principles superconducting calculations that treat screened Coulomb interaction effects non-empirically, with comparisons to density functional theory for superconductors.

Abstract: We investigate the numerical behavior of the Eliashberg equations for
phonon-mediated superconductivity, incorporating normal-state self-energy
calculations within the consistent $GW_{0}$ approximation. We account for the
full wavenumber and frequency dependences of both the screened Coulomb
interaction and phonon-mediated attraction. We present results for the
prototypical uniform electron gas system with model Einstein phonons at
temperatures of a few kelvin. At extremely low temperatures, we efficiently
execute the required convolutions of Green's functions and interactions in
Matsubara frequency and wavenumber using intermediate representation and
Fourier convolution techniques. In particular, we elucidate the interplay
between electron-phonon $\omega$-mass and $k$-mass renormalizations of the
electronic self-energy in determining the normal-state effective mass, spectral
weight and the superconducting transition temperature. The electron density
regimes where the plasmon effect enhances or suppresses the phonon-mediated
superconductivity on top of the static Coulomb effect are revealed. We compare
our comprehensive Eliashberg calculation results with those from density
functional theory for superconductors, where the functionals have been
constructed with reference to Eliashberg theory. Our model, methods, and
results provide a valuable benchmark for first-principles superconducting
calculations that treat screened Coulomb interaction effects non-empirically.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [56] [Quantification of the cascading tipping probability from the AMOC to the Amazon rainforest](https://arxiv.org/abs/2508.13383)
*Valérian Jacques-Dumas,Henk A. Dijkstra*

Main category: physics.ao-ph

TL;DR: Study shows AMOC collapse is necessary for Amazon rainforest collapse due to drying effect, using coupled climate model and rare-event algorithm.


<details>
  <summary>Details</summary>
Motivation: Investigate how AMOC collapse influences Amazon rainforest dynamics under climate change, as both are critical Earth system components at risk of tipping.

Method: Used coupled conceptual AMOC-Amazon model tuned with CESM simulations, employing rare-event algorithm to efficiently sample collapse scenarios.

Result: AMOC collapse is a necessary condition for Amazon rainforest collapse in the basin center due to significant drying effect; quantified conditional probability of tipping cascade.

Conclusion: The study demonstrates critical interdependence between AMOC and Amazon systems, with AMOC collapse being prerequisite for Amazon collapse, highlighting cascading tipping risks in climate system.

Abstract: The Amazon rainforest and the AMOC are two key components of the Earth system
and may both collapse under climate change. Due to its influence on
precipitation patterns, a collapsed AMOC influences the dynamics of the Amazon
rainforest. We investigate this effect using a coupled conceptual AMOC-Amazon
model. The Amazon model is based on empirical hydrological data controlled by
the AMOC strength. The AMOC model and its influence on the Amazon are tuned
using a simulated AMOC collapse in the Community Earth System Model (CESM).
Since the collapse of both systems is very rare, we study it using a
``rare-event'' algorithm, which samples such events much more efficiently than
direct numerical simulation. This algorithm also allows us to track many
observables of interest of the coupled model. We find that in the centre of the
Amazon basin an AMOC collapse is a necessary condition for the Amazon
rainforest to collapse, due to its important drying effect. Moreover, we are
able to quantify the importance of the AMOC in this tipping cascade by
computing the conditional probability that the collapse of the Amazon
rainforest follows that of the AMOC, given that the Amazon rainforest turns
into a savannah within $200$ years.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [57] [Qudit-based scalable quantum algorithm for solving the integer programming problem](https://arxiv.org/abs/2508.13906)
*Kapil Goswami,Peter Schmelcher,Rick Mukherjee*

Main category: quant-ph

TL;DR: A scalable quantum algorithm using multiple qudits for integer programming that achieves quantum speed-up with time complexity O(d^{n/2}) compared to classical brute force O(d^n).


<details>
  <summary>Details</summary>
Motivation: Integer programming is NP-hard and classical algorithms struggle with exponential complexity. Existing quantum approaches are resource-inefficient when encoding integers into qubits, and previous qudit-based methods lacked scalability to larger problems.

Method: Extends qudit-based encoding with a circuit-based scalable algorithm using multiple interacting qudits. Includes a distillation function to separate feasible/infeasible regions, phase-amplitude encoding for cost function, and quantum phase estimation with multi-controlled single-qubit rotation for optimization.

Result: Proves optimal solution has maximum measurement probability. Achieves time complexity O(d^{n/2} + m·n²·log d + n/ε_QPE) for n variables with d integer values and m constraints, representing a d^{n/2} reduction compared to classical brute force O(d^n).

Conclusion: The quantum algorithm provides a scalable solution for integer programming with demonstrated quantum speed-up, overcoming resource inefficiency and scalability limitations of previous quantum approaches.

Abstract: Integer programming (IP) is an NP-hard combinatorial optimization problem
that is widely used to represent a diverse set of real-world problems spanning
multiple fields, such as finance, engineering, logistics, and operations
research. It is a hard problem to solve using classical algorithms, as its
complexity increases exponentially with problem size. Most quantum algorithms
for solving IP are highly resource inefficient because they encode integers
into qubits. In [1], the issue of resource inefficiency was addressed by
mapping integer variables to qudits. However, [1] has limited practical value
due to a lack of scalability to multiple qudits to encode larger problems. In
this work, by extending upon the ideas of [1], a circuit-based scalable quantum
algorithm is presented using multiple interacting qudits for which we show a
quantum speed-up. The quantum algorithm consists of a distillation function
that efficiently separates the feasible from the infeasible regions, a
phase-amplitude encoding for the cost function, and a quantum phase estimation
coupled with a multi-controlled single-qubit rotation for optimization. We
prove that the optimal solution has the maximum probability of being measured
in our algorithm. The time complexity for the quantum algorithm is shown to be
$O(d^{n/2} + m\cdot n^2\cdot \log{d} + n/\epsilon_{QPE})$ for a problem with
the number of variables $n$ taking $d$ integer values, satisfying $m$
constraints with a precision of $\epsilon_{QPE}$. Compared to the classical
time complexity of brute force $O(d^n)$ and the best classical exact algorithm
$O((\log{n})^{3n})$, it incurs a reduction of $d^{n/2}$ in the time complexity
in terms of $n$ for solving a general polynomial IP problem.

</details>


### [58] [Quantum Sampling and Moment Estimation for Transformed Gaussian Random Fields](https://arxiv.org/abs/2508.13879)
*Matthias Deiml,Daniel Peterseim*

Main category: quant-ph

TL;DR: Quantum algorithm for efficient sampling of transformed Gaussian random fields with polylogarithmic time complexity in accuracy tolerance.


<details>
  <summary>Details</summary>
Motivation: To overcome the input bottleneck in quantum computation by generating microstructure fields directly on quantum devices from statistical parameters, enabling efficient representation of bounded Gaussian random fields essential for modeling microstructures in PDEs.

Method: Enhanced classical moving average method combined with pointwise transformations for boundedness, using amplitude estimation and quantum pseudorandom number generation for observable estimation.

Result: Achieves quantum state preparation in O(polylog tol⁻¹) time and total complexity O(tol⁻¹ polylog tol⁻¹) for estimating linear/nonlinear observables including mixed/higher-order moments, validated through numerical experiments.

Conclusion: The method provides efficient quantum representation of transformed Gaussian fields with superior scaling compared to classical approaches, enabling practical quantum computation applications in microstructure modeling and PDE coefficient field generation.

Abstract: We present a quantum algorithm for efficiently sampling transformed Gaussian
random fields on $d$-dimensional domains, based on an enhanced version of the
classical moving average method. Pointwise transformations enforcing
boundedness are essential for using Gaussian random fields in quantum
computation and arise naturally, for example, in modeling coefficient fields
representing microstructures in partial differential equations. Generating this
microstructure from its few statistical parameters directly on the quantum
device bypasses the input bottleneck. Our method enables an efficient quantum
representation of the resulting random field and prepares a quantum state
approximating it to accuracy $\mathtt{tol} > 0$ in time
$\mathcal{O}(\operatorname{polylog} \mathtt{tol}^{-1})$. Combined with
amplitude estimation and a quantum pseudorandom number generator, this leads to
algorithms for estimating linear and nonlinear observables, including mixed and
higher-order moments, with total complexity $\mathcal{O}(\mathtt{tol}^{-1}
\operatorname{polylog} \mathtt{tol}^{-1})$. We illustrate the theoretical
findings through numerical experiments on simulated quantum hardware.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [59] [Towards Routine Condensed Phase Simulations with Delta-Learned Coupled Cluster Accuracy: Application to Liquid Water](https://arxiv.org/abs/2508.13391)
*Niamh O'Neill,Benjamin X. Shi,William Baldwin,William C. Witt,Gábor Csányi,Julian D. Gale,Angelos Michaelides,Christoph Schran*

Main category: physics.chem-ph

TL;DR: A practical approach combining machine learning potentials with local correlation approximations enables routine CCSD(T)-level simulations of liquid water with quantum nuclear effects, achieving experimental agreement for structural, transport properties and density predictions.


<details>
  <summary>Details</summary>
Motivation: Simulating liquid water accurately requires precise CCSD(T) electronic structure methods but current approaches are computationally expensive and not routine. Recent MLP efforts show promise but remain challenging due to large datasets and high computational costs.

Method: Combines machine learning potentials (MLPs) with local correlation approximations to enable CCSD(T)-level simulations. Includes nuclear quantum effects and extends to constant pressure simulations for predicting bulk properties like density.

Result: Achieves agreement with experiments for structural and transport properties. Successfully predicts water's density maximum in close agreement with experiment. Enables routine CCSD(T)-based MLP development for condensed phase systems.

Conclusion: Provides a practical blueprint for routinely developing CCSD(T)-based machine learning potentials for condensed phase simulations, overcoming previous computational limitations while maintaining high accuracy comparable to experimental data.

Abstract: Simulating liquid water to an accuracy that matches its wealth of available
experimental data requires both precise electronic structure methods and
reliable sampling of nuclear (quantum) motion. This is challenging because
applying the electronic structure method of choice - coupled cluster theory
with single, double and perturbative triple excitations [CCSD(T)] - to
condensed phase systems is currently limited by its computational cost and
complexity. Recent tour-de-force efforts have demonstrated that this accuracy
can indeed bring simulated liquid water into close agreement with experiment
using machine learning potentials (MLPs). However, achieving this remains far
from routine, requiring large datasets and significant computational cost. In
this work, we introduce a practical approach that combines developments in MLPs
with local correlation approximations to enable routine CCSD(T)-level
simulations of liquid water. When combined with nuclear quantum effects, we
achieve agreement to experiments for structural and transport properties.
Crucially, this approach extends beyond constant volume to constant pressure
simulations, allowing fundamental properties such as the density to now be
predicted by MLP-based CCSD(T) models. Importantly, the approach also handles
constant pressure simulations, enabling MLP-based CCSD(T) models to predict
isothermal-isobaric bulk properties, such as water's density maximum in close
agreement with experiment. Encompassing tests across electronic structure,
datasets and MLP architecture, this work provides a practical blueprint towards
routinely developing CCSD(T)-based MLPs for the condensed phase.

</details>


### [60] [Gold-Standard Chemical Database 138 (GSCDB138): A diverse set of accurate energy differences for assessing and developing density functionals](https://arxiv.org/abs/2508.13468)
*Jiashu Liang,Martin Head-Gordon*

Main category: physics.chem-ph

TL;DR: GSCDB138 is a comprehensive benchmark library of 138 datasets with 8383 entries covering various chemical properties, providing updated reference values and serving as a rigorous platform for density functional approximation validation and training.


<details>
  <summary>Details</summary>
Motivation: To create a rigorously curated benchmark for validating density functional approximations (DFAs) by updating legacy data, removing low-quality entries, and adding new property-focused datasets to address gaps in existing benchmarks.

Method: Curated 138 datasets (8383 entries) covering main-group and transition-metal reaction energies, barrier heights, non-covalent interactions, dipole moments, polarizabilities, electric-field response energies, and vibrational frequencies. Updated legacy data from GMTKN55 and MGCDB84 to current best reference values, removed redundant/spin-contaminated/low-quality points, and added new property-focused sets.

Result: Testing 29 popular DFAs revealed expected Jacob's-ladder hierarchy with exceptions: r2SCAN-D4 rivals hybrids for frequencies, electric-field errors poorly correlate with ground-state energetics. ωB97M-V and ωB97X-V are most balanced hybrid meta-GGA and hybrid GGA respectively; B97M-V and revPBE-D4 lead meta-GGA and GGA classes. Double hybrids reduce mean errors by ~25% vs best hybrids but require careful treatment.

Conclusion: GSCDB138 provides a comprehensive, openly documented platform for stringent DFA validation and training next-generation non-empirical and machine-learned functionals, addressing the need for rigorous benchmarking across diverse chemical properties.

Abstract: We present GSCDB138, a rigorously curated benchmark library of 138 data sets
(8383 entries) covering main-group and transition-metal reaction energies and
barrier heights, non-covalent interactions, dipole moments, polarizabilities,
electric-field response energies, and vibrational frequencies. Legacy data from
GMTKN55 and MGCDB84 have been updated to today's best reference values;
redundant, spincontaminated, or low-quality points were removed, and many new,
property-focused sets were added. Testing 29 popular density-functional
approximations shows the expected Jacob's-ladder hierarchy overall, yet reveals
interesting exceptions: r2SCAN-D4 (meta-GGA) rivals hybrids for frequencies,
and electric-field errors correlate poorly with ground-state energetics.
{\omega}B97M-V and {\omega}B97X-V are the most balanced hybrid meta-GGA and
hybrid GGA, respectively; B97M-V and revPBE-D4 lead the metaGGA and GGA
classes. Double hybrids lower mean errors by about 25 % versus the best hybrids
but demand careful frozen-core, basis-set, and multi-reference treatment.
GSCDB138 offers a comprehensive, openly documented platform for stringent DFA
validation and for training the next generation of non-empirical and
machine-learned functionals.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [61] [On Modeling and Solving the Boltzmann Equation](https://arxiv.org/abs/2508.13232)
*Liliane Basso Barichello*

Main category: math-ph

TL;DR: Overview of linear Boltzmann equation solutions in 1D/2D spatial dimensions, focusing on discrete ordinates approximation for neutron/photon transport and rarefied gas dynamics applications.


<details>
  <summary>Details</summary>
Motivation: The Boltzmann equation's theoretical complexity and wide applicability in scientific/technological problems requiring numerical simulations justify research interest and the need for concise, accurate solution methods.

Method: Discrete ordinates approximation of the linear Boltzmann equation, using the ADO (Analytical Discrete Ordinates) method to provide concise and accurate solutions for numerical simulations.

Result: The work establishes connections between different phenomena described by the model and demonstrates the versatility of the ADO methodology in providing fundamental solutions for applications including nuclear safeguards, reactor shielding, optical tomography, and micro-electro-mechanical systems.

Conclusion: The ADO method proves to be a versatile analytical methodology capable of providing concise and accurate solutions to the linear Boltzmann equation, making it fundamental for numerical simulations across various scientific and technological applications.

Abstract: The Boltzmann equation has been a driving force behind significant
mathematical research over the years. Its challenging theoretical complexity,
combined with a wide variety of current scientific and technological problems
that require numerical simulations based on this model, justifies such
interest. This work provides a brief overview of studies and advances related
to the solution of the linear Boltzmann equation in one- and two-dimensional
spatial dimensions. In particular, relevant aspects of the discrete ordinates
approximation of the model are highlighted for neutron and photon transport
applications, including nuclear safeguards, nuclear reactor shielding problems,
and optical tomography. In addition, a short discussion on rarefied gas
dynamics problems, which are relevant, for instance, in the studies of
micro-electro-mechanical systems, and their connection with the linearized
Boltzmann equation, is presented. A primary goal of the work is to establish as
much as possible the connections between the different phenomena described by
the model and the versatility of the analytical methodology, the ADO method, in
providing concise and accurate solutions, which are fundamental for numerical
simulations.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [62] [Calibrated Semantic Diffusion: A p-Laplacian Synthesis with Learnable Dissipation, Quantified Constants, and Graph-Aware Calibration](https://arxiv.org/abs/2508.13658)
*Faruk Alpay,Hamdi Alakkad*

Main category: math.OC

TL;DR: A calibrated diffusion framework combining linear Laplacian smoothing, nonlinear graph p-Laplacian flows, and learnable dissipation for controllable graph-based diffusion dynamics.


<details>
  <summary>Details</summary>
Motivation: To develop a general model for graph-based diffusion with controllable dynamics that addresses limitations of fixed-parameter models in handling graphs with varying spectral properties.

Method: Synthesizes linear Laplacian smoothing, nonlinear graph p-Laplacian flows, and a learnable dissipation term from strongly convex potential. Proposes constructive calibration algorithm (SGPS) with formal guarantees.

Result: Quantified two-regime decay analysis for p>2 with p-dependent transient bounds, formalization of "non-synonymy" impossibility principle, explicit closed-form lower bounds for graph p-gap, and sharp constants for discrete-time/stochastic stability.

Conclusion: The framework provides stronger theoretical bounds and practical calibration methods for graph diffusion, with empirical validation confirming tightness of theoretical results.

Abstract: We develop a calibrated diffusion framework by synthesizing three established
concepts: linear Laplacian smoothing, nonlinear graph p-Laplacian flows, and a
learnable dissipation term derived from a strongly convex potential. This
synthesis provides a general model for graph-based diffusion with controllable
dynamics. Our key theoretical results include a quantified two-regime decay
analysis for $p>2$, which provides stronger, p-dependent transient bounds not
captured by standard ISS templates, and the first formalization of a
"non-synonymy" impossibility principle, which proves that fixed-parameter
models cannot meet universal performance targets across graphs with varying
spectral properties. To address this, we propose a constructive calibration
algorithm (SGPS) with formal guarantees for achieving target rates and mass. We
derive explicit, closed-form lower bounds for the graph p-gap on canonical
graphs a notable improvement over prior implicit estimates and provide sharp
constants for discrete-time and stochastic stability, including a
contextualized restatement of the necessary and sufficient Euler step-size and
a strengthened analysis of the stochastic noise floor. Illustrative,
small-scale empirical validations confirm the tightness of key theoretical
bounds.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [63] [LAMMPS-KOKKOS: Performance Portable Molecular Dynamics Across Exascale Architectures](https://arxiv.org/abs/2508.13523)
*Anders Johansson,Evan Weinberg,Christian R. Trott,Megan J. McCarthy,Stan G. Moore*

Main category: cs.DC

TL;DR: LAMMPS molecular dynamics code integrates Kokkos library for performance portability across modern heterogeneous computing systems, demonstrating strong scaling on exascale machines with various interatomic potentials.


<details>
  <summary>Details</summary>
Motivation: To adapt the widely-used LAMMPS molecular dynamics code to modern heterogeneous computing environments and achieve performance portability across different hardware architectures including GPUs from various vendors.

Method: Integration of the Kokkos performance portability library into the existing C++ LAMMPS codebase, with performance analysis of pairwise, many-body reactive, and machine-learned force-field interatomic potentials across different GPU architectures.

Result: Achieved strong scaling performance on all current US exascale machines (OLCF Frontier, ALCF Aurora, NNSA El Capitan) for the three types of interatomic potentials, with detailed analysis of FLOPS throughput, memory bandwidths, cache capabilities, and thread-atomic operation performance.

Conclusion: The integration of Kokkos successfully enables LAMMPS to achieve performance portability across modern heterogeneous computing systems, making it capable of efficiently utilizing current and future exascale computing resources.

Abstract: Since its inception in 1995, LAMMPS has grown to be a world-class molecular
dynamics code, with thousands of users, over one million lines of code, and
multi-scale simulation capabilities. We discuss how LAMMPS has adapted to the
modern heterogeneous computing landscape by integrating the Kokkos performance
portability library into the existing C++ code. We investigate performance
portability of simple pairwise, many-body reactive, and machine-learned
force-field interatomic potentials. We present results on GPUs across different
vendors and generations, and analyze performance trends, probing FLOPS
throughput, memory bandwidths, cache capabilities, and thread-atomic operation
performance. Finally, we demonstrate strong scaling on all current US exascale
machines -- OLCF Frontier, and ALCF Aurora, and NNSA El Capitan -- for the
three potentials.

</details>
