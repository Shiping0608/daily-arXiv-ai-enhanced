<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 6]
- [math.AP](#math.AP) [Total: 18]
- [physics.comp-ph](#physics.comp-ph) [Total: 4]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 3]
- [math.PR](#math.PR) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 2]
- [stat.ML](#stat.ML) [Total: 2]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]
- [physics.atom-ph](#physics.atom-ph) [Total: 1]
- [astro-ph.SR](#astro-ph.SR) [Total: 1]
- [cond-mat.mes-hall](#cond-mat.mes-hall) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 3]
- [astro-ph.HE](#astro-ph.HE) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [math.SP](#math.SP) [Total: 2]
- [physics.optics](#physics.optics) [Total: 1]
- [math.OC](#math.OC) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [A Hybrid Discontinuous Galerkin Neural Network Method for Solving Hyperbolic Conservation Laws with Temporal Progressive Learning](https://arxiv.org/abs/2508.16032)
*Yan Shen,Jingrun Chen,Keke Wu*

Main category: math.NA

TL;DR: Hybrid framework combining discontinuous Galerkin discretizations with progressive neural networks for hyperbolic conservation laws, achieving better shock capture and temporal consistency than standard methods.


<details>
  <summary>Details</summary>
Motivation: Traditional methods and PINNs struggle with capturing sharp discontinuities and maintaining temporal consistency in hyperbolic conservation laws.

Method: Couples DG discretizations with temporally progressive neural network architecture using structure-preserving weak-form loss (DG residuals + Rankine-Hugoniot conditions) and causality-respecting progressive training across temporal subintervals.

Result: Outperforms standard PINNs, PINNs-WE, and first-order DG schemes in accuracy and robustness for Burgers and Euler equations, particularly in shock and steep gradient capture.

Conclusion: Combining classical discretization techniques with machine learning shows promise for developing robust and accurate solvers for nonlinear hyperbolic systems.

Abstract: For hyperbolic conservation laws, traditional methods and physics-informed
neural networks (PINNs) often encounter difficulties in capturing sharp
discontinuities and maintaining temporal consistency. To address these
challenges, we introduce a hybrid computational framework by coupling
discontinuous Galerkin (DG) discretizations with a temporally progressive
neural network architecture. Our method incorporates a structure-preserving
weak-form loss -- combining DG residuals and Rankine-Hugoniot jump conditions
-- with a causality-respecting progressive training strategy. The proposed
framework trains neural networks sequentially across temporally decomposed
subintervals, leveraging pseudo-label supervision to ensure temporal coherence
and solution continuity. This approach mitigates error accumulation and
enhances the model's capacity to resolve shock waves and steep gradients
without explicit limiters. Besides, a theoretical analysis establishes error
bounds for the proposed framework, demonstrating convergence toward the
physical solution under mesh refinement and regularized training. Numerical
experiments on Burgers and Euler equations show that our method consistently
outperforms standard PINNs, PINNs-WE, and first-order DG schemes in both
accuracy and robustness, particularly in capturing shocks and steep gradients.
These results highlight the promise of combining classical discretization
techniques with machine learning to develop robust and accurate solvers for
nonlinear hyperbolic systems.

</details>


### [2] [Using the Immersed Penalized Boundary Method with Splines to Solve PDE's on Curved Domains in 3D](https://arxiv.org/abs/2508.16060)
*Aussie Greene,Larry L. Schumaker*

Main category: math.NA

TL;DR: This paper extends the immersed penalized boundary method (IPBM) to trivariate spline spaces for solving 3D elliptic boundary-value problems on curved domains.


<details>
  <summary>Details</summary>
Motivation: Second-order elliptic boundary-value problems on curved 3D domains are common in practice, and there's a need for effective numerical methods to solve them. The IPBM method has shown promise for 2D problems, and this work aims to extend it to 3D.

Method: The paper uses the immersed penalized boundary method (IPBM) with trivariate spline spaces to handle boundary-value problems on curved 3D domains.

Result: The paper demonstrates how IPBM methods can be successfully applied to solve boundary-value problems in three-dimensional curved domains using trivariate spline spaces.

Conclusion: IPBM methods with trivariate spline spaces provide an effective approach for solving elliptic PDEs on curved 3D domains, extending the successful 2D methodology to three dimensions.

Abstract: Second-order elliptic boundary-value problems defined on curved domains in 2D
and 3D arise frequently in practice. A lot of work has gone into developing
numerical methods for solving such problems. One of the newest and most
promising methods is the $\textit{immersed penalized boundary method}$ (IPBM)
introduced in [Schumaker, L. L., Solving elliptic PDE's on domains with curved
boundaries with an immersed penalized boundary method, J. Sci. Comp. ${\bf
80(3)}$ (2019), 1369--1394]. For a comprehensive discussion of the use of these
methods with various bivariate spline spaces, see the recent book [Schumaker,
L. L.: $\textit{Spline Functions: More Computational Methods}$, SIAM
(Philadelphia), 2024]. The purpose of this paper is to show how to use IPBM
methods with trivariate spline spaces to solve boundary-value problems on
curved domains in 3D.

</details>


### [3] [A kernel-free boundary integral method for elliptic interface problems on surfaces](https://arxiv.org/abs/2508.16061)
*Pengsong Yin,Wenjun YIng,Yulin Zhang,Han Zhou*

Main category: math.NA

TL;DR: A kernel-free boundary integral method for solving elliptic equations on surfaces, eliminating the need for explicit kernel functions and using Cartesian grid multigrid solvers instead.


<details>
  <summary>Details</summary>
Motivation: To develop a more efficient and accurate method for solving boundary value and interface problems for elliptic PDEs on surfaces without requiring explicit analytical kernel expressions.

Method: Replaces numerical integration of layer potentials with interpolation of solutions to equivalent interface problems, solved using fast multigrid solvers on Cartesian grids. Second-order implementation for 3D surfaces.

Result: The method demonstrates efficiency and accuracy for both boundary value and interface problems through numerical experiments.

Conclusion: The kernel-free boundary integral method provides an effective approach for solving elliptic equations on surfaces, offering advantages over traditional methods that require explicit kernel functions.

Abstract: This work presents a generalized boundary integral method for elliptic
equations on surfaces, encompassing both boundary value and interface problems.
The method is kernel-free, implying that the explicit analytical expression of
the kernel function is not required when solving the boundary integral
equations. The numerical integration of single- and double-layer potentials or
volume integrals at the boundary is replaced by interpolation of the solution
to an equivalent interface problem, which is then solved using a fast multigrid
solver on Cartesian grids. This paper provides detailed implementation of the
second-order version of the kernel-free boundary integral method for elliptic
PDEs defined on an embedding surface in $\mathbb{R}^3$ and presents numerical
experiments to demonstrate the efficiency and accuracy of the method for both
boundary value and interface problems.

</details>


### [4] [$\ell_{1}^{2}-η\ell_{2}^{2}$ sparsity regularization for nonlinear ill-posed problems](https://arxiv.org/abs/2508.16163)
*Long Li,Liang Ding*

Main category: math.NA

TL;DR: Analysis of ℓ₁²-ηℓ₂² sparsity regularization for nonlinear ill-posed inverse problems, showing better theoretical results for 0<η<1 than η=1, with proven sparsity and convergence rates.


<details>
  <summary>Details</summary>
Motivation: To investigate the well-posedness and effectiveness of ℓ₁²-ηℓ₂² sparsity regularization in nonlinear ill-posed inverse problems, addressing limitations when η=1 and providing theoretical guarantees for sparsity and convergence.

Method: Theoretical analysis of ℓ₁²-ηℓ₂² regularization properties, examination of coercivity and Radon-Riesz properties, development of convergence rate proofs for 0<η<1, and proposal of iterative half variation algorithm for practical implementation.

Result: Proved that all minimizers exhibit sparsity, established O(δ¹/²) and O(δ) convergence rates for 0<η<1, showed weaker theoretical outcomes for η=1 due to lack of coercivity, and demonstrated numerical effectiveness of the proposed iterative algorithm.

Conclusion: The ℓ₁²-ηℓ₂² regularization with 0<η<1 provides strong theoretical guarantees for sparsity and convergence in nonlinear ill-posed inverse problems, outperforming the η=1 case, with practical implementation through the iterative half variation algorithm.

Abstract: In this study, we investigate the
$\left\|\cdot\right\|_{\ell_{1}}^{2}-\eta\left\|\cdot\right\|_{\ell_{2}}^{2}$
sparsity regularization with $0< \eta\leq 1$, in the context of nonlinear
ill-posed inverse problems. We focus on the examination of the well-posedness
associated with this regularization approach. Notably, the case where $\eta=1$
presents weaker theoretical outcomes than $0< \eta<1$, primarily due to the
absence of coercivity and the Radon-Riesz property associated with the
regularization term. Under specific conditions pertaining to the nonlinearity
of the operator $F$, we establish that every minimizer of the
$\left\|\cdot\right\|_{\ell_{1}}^{2}-\eta\left\|\cdot\right\|_{\ell_{2}}^{2}$
regularization exhibits sparsity. Moreover, for the case where $0<\eta<1$, we
demonstrate convergence rates of $\mathcal{O}\left(\delta^{1/2}\right)$ and
$\mathcal{O}\left(\delta\right)$ for the regularized solution, concerning a
sparse exact solution, under differing yet widely accepted conditions related
to the nonlinearity of $F$. Additionally, we present the iterative half
variation algorithm as an effective method for addressing the
$\left\|\cdot\right\|_{\ell_{1}}^{2}-\eta\left\|\cdot\right\|_{\ell_{2}}^{2}$
regularization in the domain of nonlinear ill-posed equations. Numerical
results provided corroborate the effectiveness of the proposed methodology.

</details>


### [5] [Numerical solution of the time fractional nonlinear Fisher-KPP diffusion-reaction equation using the local domain boundary element method](https://arxiv.org/abs/2508.16241)
*Theodore V. Gortsas*

Main category: math.NA

TL;DR: Extension of Local Domain Boundary Element Method (LD-BEM) for solving nonlinear time fractional Fisher-KPP equations in 2D problems with reduced computational cost compared to conventional BEM.


<details>
  <summary>Details</summary>
Motivation: Time fractional Fisher-KPP equations model biological/chemical systems with memory effects, but solving them is challenging due to nonlinearity and fractional derivative nonlocality. Conventional BEM has high computational cost and memory requirements.

Method: Extended Local Domain Boundary Element Method (LD-BEM) to handle nonlinear time fractional Fisher-KPP problems, addressing computational efficiency issues of conventional BEM.

Result: Successfully implemented numerical method for examining various 2D Fisher-KPP problems using different fractional derivative definitions with improved computational efficiency.

Conclusion: The LD-BEM extension provides an efficient computational approach for solving challenging nonlinear time fractional Fisher-KPP equations in two dimensions.

Abstract: The Fisher-KPP partial differential equation has been employed in science to
model various biological, chemical, and thermal phenomena. Time fractional
extensions of Fisher's equation have also appeared in the literature, aiming to
model systems with memory. The solution of the time fractional Fisher-KPP
equation is challenging due to the interplay between the nonlinearity and the
nonlocality imposed by the fractional derivatives. An accurate method that for
the solution of time fractional diffusion problems is the Boundary Element
Method (BEM). The conventional BEM has a high computational cost and memory
requirements since it leads to dense coefficient matrices. For nonlinear
transient problems, its efficiency is further reduced due to the appearance of
volume integrals. In the present work an extension of the recently proposed
Local Domain Boundary Element Method (LD-BEM) is presented for the solution of
nonlinear time fractional Fisher-KPP problems. The implemented numerical method
is used to examine various two-dimensional problems related to the Fisher-KPP
equation using different definitions of the fractional derivative.

</details>


### [6] [A Nodal Discontinuous Galerkin Method with Low-Rank Velocity Space Representation for the Multi-Scale BGK Model](https://arxiv.org/abs/2508.16564)
*Andres Galindo-Olarte,Joseph Nakao,Mirjeta Pasha,Jing-Mei Qiu,William Taitano*

Main category: math.NA

TL;DR: Hybrid algorithm combining low-rank velocity decomposition with full-rank spatial representation for Boltzmann-BGK equation, using DG spatial discretization and IMEX time integration with multiscale handling.


<details>
  <summary>Details</summary>
Motivation: Extend low-rank techniques to realistic Boltzmann equation settings where structured representations like conformal geometries are impractical in engineering applications.

Method: Nodal discontinuous Galerkin for spatial discretization, low-rank decomposition over velocity grid, IMEX Runge-Kutta time integration, and multiscale implicit integrator using auxiliary moment equation for vanishing collision time.

Result: Demonstrated algorithm's accuracy order, reduced computational complexity, and robustness on canonical gas kinetics problems with increasing complexity.

Conclusion: Establishes foundation for applying modern low-rank techniques to solve Boltzmann equation in practical engineering scenarios without requiring structured geometric representations.

Abstract: A novel hybrid algorithm is presented for the Boltzmann-BGK equation, in
which a low-rank decomposition is applied solely in the velocity subspace,
while a full-rank representation is maintained in the physical (position)
space. This approach establishes a foundation for extending modern low-rank
techniques to solve the Boltzmann equation in realistic settings, particularly
where structured representations -- such as conformal geometries -- may not be
feasible in practical engineering applications. A nodal discontinuous Galerkin
method is employed for spatial discretization, coupled with a low-rank
decomposition over the velocity grid, as well as implicit-explicit Runge-Kutta
methods for time integration. To handle the limit of vanishing collision time,
a multiscale implicit integrator based on an auxiliary moment equation is
utilized. The algorithm's order of accuracy, reduced computational complexity,
and robustness are demonstrated on a suite of canonical gas kinetics problems
with increasing complexity.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [7] [Spherical solutions to the Klein-Gordon equation in the expanding universe](https://arxiv.org/abs/2508.15812)
*Karen Yagdjian*

Main category: math.AP

TL;DR: Explicit wave function formula for spherically symmetric fields in FLRW universe with de Sitter-generated scale factor, applied to test decay of pionic atom field.


<details>
  <summary>Details</summary>
Motivation: To derive explicit solutions for wave functions in cosmological contexts and apply them to study field decay phenomena in quantum systems.

Method: Developed explicit formula for wave function of spherically symmetric fields in FLRW universe with scale factor from de Sitter universe, solving Klein-Gordon equation.

Result: Obtained explicit solutions that can be used to analyze time decay behavior of fields, specifically demonstrated for pionic atom field decay.

Conclusion: The explicit wave function solutions provide a valuable tool for studying field dynamics in cosmological settings and quantum decay processes.

Abstract: We produce an explicit formula for the wave function of the spherically
symmetric fields emitted to the FLRW universe with the scale factor generated
by the de Sitter universe. As an application of these explicitly written
solutions of the Klein-Gordon equation, we test the decay in time of the field
generated by a pionic atom.

</details>


### [8] [Physical blowups via buffered time change in a mean-field neural network](https://arxiv.org/abs/2508.15961)
*Nikolaos Papadopoulos,Thibaud Taillefumier*

Main category: math.AP

TL;DR: The paper analyzes blowup singularities in mean-field neural models with integrate-and-fire neurons, proposing a physical definition for blowup solutions via time change techniques and showing uniqueness when neurons have distributed refractory periods.


<details>
  <summary>Details</summary>
Motivation: To resolve ambiguity in admissible blowup solutions for McKean-Vlasov equations describing neural networks, as multiple solutions exist similar to shock solutions in nonlinear conservation laws, making it unclear which notion of blowup solutions should be adopted.

Method: Define physical blowup dynamics as solutions to a fixed-point problem based on time change associated with McKean-Vlasov equations. Introduce a buffering mechanism that regularizes blowup dynamics while conserving principles of finite-dimensional particle systems, then recover physical solutions in vanishing buffering limit.

Result: Physical blowup solutions are shown to be unique, globally defined, and avoid eternal blowup phenomenon when neurons exhibit nonzero, distributed refractory periods. The approach provides unambiguous characterization of blowup singularities.

Conclusion: The proposed formulation unambiguously defines physical blowup dynamics for mean-field neural models, with uniqueness and global existence guaranteed under realistic modeling assumptions of distributed refractory periods, resolving previous ambiguities in blowup solution selection.

Abstract: Idealized networks of integrate-and-fire neurons with impulse-like
interactions obey McKean-Vlasov diffusion equations in the mean-field limit.
These equations are prone to blowups: for a strong enough interaction coupling,
the mean-field rate of interaction diverges in finite time with a finite
fraction of neurons spiking simultaneously, thereby marking a macroscopic
synchronous event. Characterizing these blowup singularities analytically is
the key to understanding the emergence and persistence of spiking synchrony in
mean-field neural models. Such a treatment is possible via time change
techniques for a Poissonian variation of the classically considered
integrate-and-fire dynamics. However, just as for shock solutions for nonlinear
conservation laws, there are several admissible blowup solutions to the
corresponding McKean-Vlasov equations. Because of this ambiguity, it is unclear
which notion of blowup solutions shall be adopted. Here, we unambiguously
define physical blowup dynamics as solutions to a fixed-point problem bearing
on the time change associated to the McKean-Vlasov equation. To justify the
physicality of this formulation, we introduce a buffering mechanism that
regularizes blowup dynamics, while satisfying the same conservation principles
as the finite-dimensional particle-system dynamics. We then show that physical
blowup dynamics are recovered from these regularized solutions in the limit of
vanishing buffering. Our approach also shows that these physical solutions are
unique, globally defined, and avoid the so-called eternal blowup phenomenon, as
long as the neurons exhibit nonzero, distributed refractory periods, a
reasonable modeling assumption.

</details>


### [9] [Scattering for radial bounded solutions of focusing supercritical wave equations in four dimensions](https://arxiv.org/abs/2508.15996)
*Guher Camliyurt,Carlos E. Kenig*

Main category: math.AP

TL;DR: Global existence and scattering for radial solutions of focusing wave equation with energy supercritical nonlinearity in dimension 4, under boundedness in critical Sobolev space


<details>
  <summary>Details</summary>
Motivation: Study the long-time behavior of solutions to energy supercritical wave equations, which are challenging due to the lack of conservation laws and potential finite-time blow-up

Method: Analysis of radial solutions to the focusing wave equation with energy supercritical nonlinearity in 4D, using boundedness in critical Sobolev space as a key condition

Result: Proves that any radial solution remaining bounded in the critical Sobolev space is global in time and scatters to free waves as time approaches positive and negative infinity

Conclusion: Boundedness in the critical Sobolev space serves as a sufficient condition to ensure global existence and scattering behavior for radial solutions of energy supercritical wave equations in 4D

Abstract: We consider the focusing wave equation with energy supercritical nonlinearity
in dimension four. We prove that any radial solution that remains bounded in
the critical Sobolev space is global and scatters to free waves as $t \to \pm
\infty$.

</details>


### [10] [Regularity of the free boundary in an unstable parabolic problem](https://arxiv.org/abs/2508.15997)
*Mark Allen,Gilles Bokolo-Tamba*

Main category: math.AP

TL;DR: Analysis of free boundary regularity in an unstable parabolic combustion model, showing C^{1,α/2} smoothness for advancing heat fronts.


<details>
  <summary>Details</summary>
Motivation: To understand the mathematical properties of free boundaries in combustion models, particularly the regularity and smoothness of the interface where heat advances in unstable parabolic systems.

Method: Studied the physical scenario of advancing heat in an unstable parabolic problem derived from combustion modeling, employing mathematical analysis techniques to examine free boundary regularity.

Result: Proved that the free boundary is a C^{1,α/2} hypersurface, establishing the smoothness and regularity properties of the interface in this combustion context.

Conclusion: The free boundary in unstable parabolic combustion problems exhibits C^{1,α/2} regularity when heat advances, providing important mathematical characterization of interface behavior in such physical systems.

Abstract: We study the free boundary in an unstable parabolic problem arising from a
model in combustion. We consider the physical situation in which the heat
advances and prove that the free boundary is a $C^{1,\alpha/2}$ hypersurface.

</details>


### [11] [Multiscale Analysis of a Kinetic Model of Confined Suspensions of Self-Propelled Rods](https://arxiv.org/abs/2508.16003)
*Leonid Berlyand,Spencer Dang,Pierre-Emmanuel Jabin,Mykhailo Potomkin*

Main category: math.AP

TL;DR: Rigorous mathematical justification of a kinetic model for confined active matter using multi-scale analysis to bridge microscopic dynamics and macroscopic wall accumulation phenomena.


<details>
  <summary>Details</summary>
Motivation: Active matter systems exhibit complex dynamics near boundaries, with significant portions of their behavior occurring at confining walls. Previous work developed a kinetic framework but lacked rigorous mathematical foundation.

Method: Multi-scale analysis of a classical kinetic system with vanishing translational diffusion. Two distinct derivations were employed to obtain the degenerate governing equation as a singular limit. The analysis considered permanently wall-confined active rods for clarity.

Result: Established well-posedness of the system and successfully derived the reduced kinetic model through rigorous mathematical justification. The model explicitly treats wall accumulation using coupled probability distribution functions.

Conclusion: This work provides a solid mathematical foundation for reduced kinetic models of confined active matter, enabling better understanding of the coupling between boundary dynamics and bulk behavior in active systems.

Abstract: The behavior of active matter under confinement poses significant challenges
due to the intricate coupling between dynamics near boundaries and those in the
bulk. A defining feature of active matter systems is that a substantial portion
of their dynamics takes place near confining boundaries. In our previous work,
we developed a kinetic framework that enables direct computation of the
probability distribution functions for both the position and orientation of
active rods. A distinguishing aspect of this approach is its explicit treatment
of wall accumulation through the use of two coupled probability distribution
functions: one describing the bulk population and the other representing rods
accumulated at the boundary. Another novel feature is the structure of the
governing equation, which is degenerate: it is second-order in one non-temporal
variable and first-order in another. The main focus of this paper is to
rigorously justify this model via multi-scale analysis. We first establish
well-posedness of the system and then employ two distinct multi-scale
derivations to obtain the model as a singular limit of a more classical kinetic
system in the regime of vanishing translational diffusion. For analytical
clarity, we consider the case in which active rods, once accumulated at the
wall, remain permanently confined there. This work provides a rigorous
mathematical foundation for reduced kinetic models of confined active matter,
bridging microscopic dynamics and macroscopic accumulation phenomena.

</details>


### [12] [Ground state and multiple solutions for modified autonomous fourth-order elliptic equations with Berestycki-Lions type conditions](https://arxiv.org/abs/2508.16010)
*Lifeng Yin,Fan Wang*

Main category: math.AP

TL;DR: Existence of ground state and infinitely many solutions for modified fourth-order elliptic equation in R^N (4<N≤6) with Berestycki-Lions type nonlinearity.


<details>
  <summary>Details</summary>
Motivation: Address the lack of results for autonomous modified fourth-order elliptic equations under weak nonlinearity conditions.

Method: Novel approach combining Jeanjean's technique with Pohozaev-Palais-Smale sequence construction for ground state; minimax methods on topologically constrained comparison functional for infinite multiplicity when f is odd.

Result: Proved existence of ground state solution and infinite multiplicity of radially symmetric solutions when f is odd.

Conclusion: Resolves the gap in results for this autonomous problem under nearly weakest possible nonlinearity conditions.

Abstract: This article establishes the existence of a ground state and infinitely many
solutions for the modified fourth-order elliptic equation:
  \[
  \begin{aligned}
  \left\{
  \begin{array}{ll}
  \Delta^2 u - \Delta u + u - \frac{1}{2}u\Delta(u^2) = f(u), & \text{in }
\mathbb{R}^N,
  u \in H^2(\mathbb{R}^N),
  \end{array}
  \right.
  \end{aligned}
  \]
  where $4 < N \leq 6$ and$f:\mathbb{R}\rightarrow\mathbb{R}$ is a nonlinearity
of Berestycki-Lions type.
  For the ground state solution, we develop a novel approach that combines
Jeanjean's technique with a Pohozaev-Palais-Smale sequence construction. When
$f$ is odd, we prove infinite multiplicity of radially symmetric solutions via
minimax methods on a topologically constrained comparison functional. This work
resolves the lack of results for this autonomous problem under almost the
weakest nonlinearity conditions.

</details>


### [13] [Inaccuracy of Ensemble-Based Covariance Propagation, Beyond Sampling Error](https://arxiv.org/abs/2508.16567)
*Shay Gilpin*

Main category: math.AP

TL;DR: Ensemble-based data assimilation methods show significant covariance propagation errors when correlation length scales approach grid resolution, despite accurate state propagation, revealing a fundamental discrepancy between discrete and continuum covariance dynamics.


<details>
  <summary>Details</summary>
Motivation: To investigate the assumption that accurate discrete state propagation in ensemble Kalman filters implies accurate covariance propagation, particularly when correlation length scales are near grid resolution.

Method: Numerical experiments and analytical analysis comparing discrete covariance propagation with known exact continuum covariance dynamics for advective systems.

Result: Ensemble covariances show remarkably large errors (at least one order of magnitude larger than mean state errors) when correlation lengths approach grid scale, beyond typical sampling or discretization errors.

Conclusion: There is a fundamental problem in data assimilation schemes that use the same discrete model for both state and covariance propagation, as covariance errors cannot be fixed by standard methods like inflation and localization.

Abstract: Modern data assimilation schemes typically use the same discrete dynamical
model to evolve the state estimate in time also to approximate the evolution,
or propagation, of the estimation error covariance. Ensemble-based methods,
such as the ensemble Kalman filter, approximate the evolution of the covariance
through the propagation of individual ensemble members. Thus, it is tacitly
assumed that if the discrete state propagation and resulting mean state
estimates are accurate, then the ensemble-based discrete covariance propagation
will be accurate as well, apart from sampling errors due to limited ensemble
size. Through a series of numerical experiments supported by analytical
results, we demonstrate that this assumption is false when correlation length
scales approach grid resolution. We show for states that satisfy advective
dynamics, that while the discrete state propagation and ensemble mean state
estimates are accurate, the corresponding ensemble covariances can be
remarkably inaccurate, well beyond that expected from sampling errors or
typical numerical discretization errors. The underlying problem is a
fundamental discrepancy between discrete covariance propagation and the
continuum covariance dynamics, which we can identify because the exact
continuum covariance dynamics are known. Errors in the ensemble covariances,
which can be at least one order of magnitude larger than those of the mean
state when correlation lengths begin to approach grid scale, cannot be
rectified by the usual methods, such as covariance inflation and localization.
This work brings to light a fundamental problem for data assimilation schemes
that propagate covariances using the same discrete dynamical model used to
propagate the state.

</details>


### [14] [Quantization of blow-up masses for the Finsler $N$-Liouville equation](https://arxiv.org/abs/2508.16080)
*Xia Huang,Yuan Li,Dong Ye,Feng Zhou*

Main category: math.AP

TL;DR: Quantification of blow-up masses for Finsler N-Liouville equation, generalizing previous results for Liouville equations and N-Laplacian cases.


<details>
  <summary>Details</summary>
Motivation: Blow-up phenomena quantization is crucial for PDE analysis. This work extends classical results from Li-Shafrir (1994), Wang-Xia (2012), and Esposito-Lucia (2024) to the Finsler N-Liouville equation context.

Method: Analysis of the Finsler N-Liouville equation -Q_N u_n = V_n e^{u_n} in Ω ⊂ ℝ^N (N ≥ 2), quantifying blow-up masses through mathematical analysis techniques.

Result: The paper provides quantified blow-up mass results for the Finsler N-Liouville equation, establishing generalizations of previous classical results in this area.

Conclusion: This work successfully generalizes quantization results for blow-up phenomena to the Finsler N-Liouville equation framework, extending the scope of previous classical theorems in PDE analysis.

Abstract: The quantization results for blow-up phenomena play crucial roles in the
analysis of partial differential equations. Here we quantify the blow-up masses
to the following Finsler $N$-Liouville equation
$$-Q_{N}u_{n}=V_{n}e^{u_{n}}\quad\mbox{in}~ \Omega\subset \mathbb{R}^{N}, N \ge
2.$$ Our study generalizes the classical result of Li-Shafrir [Indiana Univ.
Math.J.,1994] for Liouville equation, Wang-Xia's work for anisotropic Liouville
equation in $\mathbb{R}^2$ [JDE, 2012], and Esposito-Lucia's for the
$N$-Laplacian case in $\mathbb{R}^N$ ($N \geq 3$) in their recent paper [CVPDE,
2024].

</details>


### [15] [Strichartz and local smoothing estimates for the fractional Schrödinger equations over fractal time](https://arxiv.org/abs/2508.16102)
*Jin Bong Lee,Sanghyuk Lee,Luz Roncal*

Main category: math.AP

TL;DR: Strichartz estimates for fractional Schrödinger operators over fractal time sets using Assouad dimension concepts, with extensions to measures satisfying dimensional growth conditions and related inhomogeneous estimates.


<details>
  <summary>Details</summary>
Motivation: To extend Strichartz-type estimates for fractional Schrödinger operators to fractal time sets, capturing their geometric nature through Assouad dimension concepts and providing estimates consistent with fractal dimensions.

Method: Employ notions of bounded Assouad characteristic and Assouad spectrum to handle fractal time sets. Prove estimates for measures satisfying α-dimensional growth conditions, establish inhomogeneous estimates, and derive L² local smoothing estimates.

Result: Obtained Strichartz estimates that naturally extend known results for fractional Schrödinger operators while precisely accounting for fractal dimensions, even when dealing with rough fractal sets.

Conclusion: The work successfully extends fractional Schrödinger operator estimates to fractal settings using Assouad dimension concepts, maintaining natural consistency with associated fractal dimensions despite the roughness of the sets involved.

Abstract: We obtain Strichartz-type estimates for the fractional Schr\"odinger operator
$f \mapsto e^{it(-\Delta)^{\gamma/2}} f$ over a time set $E$ of fractal
dimension. To obtain those estimates capturing fractal nature of $E$, we employ
the notions in the spirit of the Assouad dimension, such as, bounded Assouad
characteristic and Assouad specturm. We also prove the estimate $$ \|
e^{it(-\Delta)^{\gamma/2}} f \|_{L_t^q(\mathrm{d}\mu; L_x^r(\mathbb{R}^d))} \le
C \|f\|_{H^s}, $$ where $\mu$ is a measure satisfying an $\alpha$-dimensional
growth condition. In addition, we establish related inhomogeneous estimates and
$L^2$ local smoothing estimates. A surprising feature of our work is that,
despite dealing with rough fractal sets, we extend the known estimates for the
fractional Schr\"odinger operators in a natural way, precisely consistent with
the associated fractal dimensions.

</details>


### [16] [Nonlocal Harnack inequality in a disconnected region](https://arxiv.org/abs/2508.16103)
*Se-Chan Lee*

Main category: math.AP

TL;DR: Harnack inequality for nonlocal equations in disconnected regions, comparing solutions across different connected components - a purely nonlocal phenomenon


<details>
  <summary>Details</summary>
Motivation: To establish mathematical tools for analyzing nonlocal equations in disconnected domains, capturing phenomena that don't exist in local PDE theory

Method: Two approaches: one based on localized maximum principle and another using Poisson kernel estimates

Result: Successfully established Harnack inequality that compares solution values across disconnected components

Conclusion: This work provides fundamental analytical tools for nonlocal equations in disconnected regions, revealing unique nonlocal phenomena without local counterparts

Abstract: We establish a Harnack inequality for weak solutions of nonlocal equations in
a disconnected region. The inequality compares the value of a solution on one
connected component with its value on another, capturing a purely nonlocal
phenomenon with no local analogue. We provide two different approaches: one
based on the localized maximum principle and another on the Poisson kernel
estimates.

</details>


### [17] [Limiting behavior of principal eigenvalues for a class of elliptic operators with degenerate large advection](https://arxiv.org/abs/2508.16108)
*S. Cano-Casanova,J. López-Gómez,M. Molina-Meyer*

Main category: math.AP

TL;DR: The paper extends Chen and Lou's Theorem 1.2 to cases where the advection function m(x) is highly degenerate at its maximum, showing principal eigenvalue behavior is independent of the zero order.


<details>
  <summary>Details</summary>
Motivation: To generalize previous results that required m''(x_0)<0, allowing for more degenerate cases where m(x) has higher order zeros at its maximum point.

Method: Mathematical analysis of principal eigenvalues in degenerate advection problems, extending existing theorems to handle highly degenerate functions m(x).

Result: The limiting behavior of the principal eigenvalue remains valid even when m(x) is highly degenerate at its maximum, independent of the order of the zero.

Conclusion: Chen and Lou's Theorem 1.2 holds under more general conditions, removing the strict concavity requirement m''(x_0)<0 at the maximum point.

Abstract: In this paper we show that, in a number of circumstances, Theorem 1.2 of Chen
and Lou \cite{ChLo} remains valid even when the function $m(x)$ of the
advection term $-2sm'(x)$ is highly degenerate at its maximum. Our main result
establishes that the limiting behavior of the principal eigenvalue is
independent of the order of the zero of $m(x)-m(x_0)$ at $x=x_0$, where
$m(x_0)=\|m\|_\infty$. Theorem 1.2 of Chen and Lou \cite{ChLo} imposed
$m''(x_0)<0$.

</details>


### [18] [Spectral density estimates of surface-localized eigenmodes for transmission eigenvalue problems](https://arxiv.org/abs/2508.16203)
*Yan Jiang,Hongyu Liu,Kai Zhang,Haoran Zheng*

Main category: math.AP

TL;DR: Transmission eigenfunctions exhibit surface-localization near domain boundaries, with sharp spectral density bounds showing this behavior occurs in a significant proportion of cases, connecting geometric rigidity to spectral properties.


<details>
  <summary>Details</summary>
Motivation: To understand the distinctive spectral patterns of transmission eigenfunctions in wave scattering theory, particularly their boundary-localizing behavior discovered in prior studies, and to establish rigorous connections between geometric properties and spectral characteristics.

Method: Derived sharp spectral density estimates (both lower and upper bounds) to quantify the proportion of transmission eigenfunctions exhibiting surface-localization behavior, primarily analyzed within a radially symmetric framework.

Result: Established that a significant proportion of transmission eigenfunctions manifest surface-localizing behavior near domain boundaries, with rigorous bounds on the spectral density.

Conclusion: The study provides rigorous theoretical insights into transmission eigenfunctions' geometric-spectral relationships, advances new perspectives in wave scattering theory, and offers meaningful implications for inverse scattering applications.

Abstract: This paper investigates a distinctive spectral pattern exhibited by
transmission eigenfunctions in wave scattering theory. Building upon the
discovery in [7, 8] that these eigenfunctions localize near the domain
boundary, we derive sharp spectral density estimates--establishing both lower
and upper bounds--to demonstrate that a significant proportion of transmission
eigenfunctions manifest this surface-localizing behavior. Our analysis
elucidates the connection between the geometric rigidity of eigenfunctions and
their spectral properties. Though primarily explored within a radially
symmetric framework, this study provides rigorous theoretical insights,
advances new perspectives in this emerging field, and offers meaningful
implications for inverse scattering theory.

</details>


### [19] [Existence and concentration phenomenon of multiple solutions for the fractional logarithmic Schrödinger-Poisson system via penalization method](https://arxiv.org/abs/2508.16229)
*Jiao Luo,Zhipeng Yang*

Main category: math.AP

TL;DR: Existence of multiple solutions for fractional logarithmic Schrodinger-Poisson system with concentration around local minima of potential.


<details>
  <summary>Details</summary>
Motivation: Study multiplicity of solutions for fractional logarithmic Schrodinger-Poisson systems with small parameter epsilon, addressing challenges in energy functional analysis.

Method: Introduce new Banach space to make energy functional C^1, use Lusternik-Schnirelmann category theory to study solution multiplicity, prove concentration around local minima.

Result: For sufficiently small epsilon > 0, the system has positive ground state solution and each positive solution concentrates around local minimum points of potential V.

Conclusion: The fractional logarithmic Schrodinger-Poisson system exhibits multiple solutions that concentrate near local minima of the potential function when epsilon is sufficiently small.

Abstract: This paper concerns the existence of multiple solutions for the fractional
logarithmic Schr\"odinger-Possion system of the form
  \begin{equation*}
  \begin{cases}
  {\varepsilon}^{2\alpha} (-\Delta )^{\alpha}u+V(x) u+\phi u=u \log
u^{2}+u^{q-1}, & \text{in}\quad \mathbb{R}^{3},
  {\varepsilon}^{2\alpha} (-\Delta )^{\alpha}\phi=u^2, & \text{in}\quad
\mathbb{R}^{3}.
  \end{cases}
  \end{equation*} where $\varepsilon>0$ is a small parameter, $q \in (4,
2_\alpha^*)$ with $\alpha\in(\frac{3}{4},1)$, $V: \mathbb{R}^{3} \rightarrow
\mathbb{R}$ is a continuous function that satisfies some local potential
hypothesis. By introducing a new Banach space, the energy functional become
$C^{1}$, which create the conditions for studying the multiplicity of solutions
involving Lusternik-Schnirelmann category. We prove that for $\varepsilon>0$
small enough, the system has a positive ground state solution and each positive
solution concentrates around a local minimum point of $V$.

</details>


### [20] [Diverse regularities for nonlocal parabolic De Giorgi classes I](https://arxiv.org/abs/2508.16247)
*Simone Ciani,Kenta Nakamura*

Main category: math.AP

TL;DR: This paper establishes Harnack inequalities for nonlocal parabolic De Giorgi classes, extending results to nonlinear fractional heat equations and proving structural properties like local boundedness, weak Harnack inequalities, and Hölder continuity.


<details>
  <summary>Details</summary>
Motivation: To extend Harnack inequality theory to nonlocal parabolic equations and demonstrate that recent results by Kassmann and Weidner are structural properties valid beyond specific equations.

Method: Develops nonlocal parabolic De Giorgi classes, derives local boundedness, proves weak Harnack inequalities under minimal tail conditions, establishes measure propagation lemmas, and provides full Harnack inequality proofs.

Result: New Harnack inequalities and Hölder continuity results for nonnegative solutions in nonlocal parabolic De Giorgi classes, including a Liouville-type rigidity property. Results are novel even for linear cases.

Conclusion: The techniques establish that Harnack inequalities are structural properties of the energy class rather than equation-dependent, opening avenues for extensions to various natural directions in nonlocal analysis.

Abstract: In this paper we establish diverse Harnack inequalities for elements of a
specific energy class, called the nonlocal parabolic (p-homogenous) De Giorgi
class. This class encompasses the nonlinear parabolic counterpart of the
seminal work of M. Cozzi (J. Funct. Anal., 2017) and embodies local weak
solutions to the fractional heat equation. More precisely, we first derive the
local boundedness and subsequently prove under minimal tail conditions several
weak Harnack inequalities, measure theoretical propagation lemmas, and a full
Harnack inequality for nonnegative members of the aforementioned class.
Finally, we present a full proof of the local H\"older modulus of continuity,
thereby establishing a Liouville-type rigidity property. The results are new
even for the linear case, thereby showing that the recent achievements of
Kassmann and Weidner (Duke Math. J., 2024) are structural properties, valid
regardless of any equation. The techniques and ideas presented in this paper
will open the door for further extensions to many natural directions.

</details>


### [21] [Kirchhoff-type equations involving the Fractional $(p,q)-$Laplacian](https://arxiv.org/abs/2508.16281)
*Lisbeth Carrero,Pedro Hernández-Llanos*

Main category: math.AP

TL;DR: Existence and nonexistence of solutions for Kirchhoff-type fractional (p-q)-Laplacian problems using variational methods


<details>
  <summary>Details</summary>
Motivation: Study complex nonlinear fractional problems involving two different fractional Laplacian operators with Kirchhoff-type terms, which arise in various physical phenomena and require advanced mathematical analysis

Method: Variational methods including direct minimization of energy functional for first solution and Mountain Pass Theorem for second solution; also prove nonexistence for small lambda values

Result: Established existence of at least two weak solutions: one via direct minimization and another via Mountain Pass Theorem; proved nonexistence result for small positive lambda values

Conclusion: The paper successfully demonstrates both existence and nonexistence results for this complex fractional (p-q)-Laplacian problem, providing complete mathematical characterization of solution behavior depending on parameter lambda

Abstract: In this paper, we study the existence and nonexistence of solutions for the
following Kirchhoff-type fractional $(p\text{-}q)$-Laplacian problem:
  \begin{equation*} \begin{cases}
  M\left([u]^p_{p,s_1}\right)(-\Delta)^{s_1}_p u +
M\left([u]^q_{q,s_2}\right)(-\Delta)^{s_2}_q u = \lambda\big[a(x)|u|^{p-2}u +
b(x)|u|^{q-2}u\big] + h(x), & \text{in } \Omega, \\ u = 0, & \text{on }
\mathbb{R}^N \setminus \Omega, \end{cases} \end{equation*}
  where $\Omega \subset \mathbb{R}^N$ ($N \geq 1$) is a bounded domain with
smooth boundary, $0 < s_1 < s_2 < 1$, and $s_1 p < N$. We assume $1 < q \leq p
< \theta p < p^{*}_{s_1} := \dfrac{Np}{N - s_1 p}$, and $\lambda \in
\mathbb{R}$. The functions $a(x), b(x)$, and $h(x)$ are non-negative, with $a,
b \in L^\infty(\Omega)$ and $h \in L^q(\Omega)$.
  Using variational methods, we establish the existence of at least two weak
solutions. The first solution is obtained via the direct minimization of the
associated energy functional, and the second is obtained by applying the
Mountain Pass Theorem. We also prove a nonexistence result for small values of
the parameter $\lambda > 0$.

</details>


### [22] [Sharp remainder terms of weighted Hardy-Poincaré and Heisenberg-Pauli-Weyl inequalities related to the Baouendi-Grushin operator](https://arxiv.org/abs/2508.16380)
*Yerkin Shaimerdenov,Nurgissa Yessirkegenov,Amir Zhangirbayev*

Main category: math.AP

TL;DR: Sharp remainder terms for Hardy-Poincaré inequalities with non-radial weights in Baouendi-Grushin vector fields, unifying and improving previous results with explicit constants.


<details>
  <summary>Details</summary>
Motivation: To obtain precise remainder terms for Hardy-Poincaré type inequalities with general weights in the Baouendi-Grushin setting, extending beyond previous radial weight limitations.

Method: Developed analytical techniques for Baouendi-Grushin vector fields that handle non-radial weights, enabling unification and improvement of existing results with explicit constants for complex-valued functions.

Result: Achieved sharp remainder terms for weighted Hardy-type inequalities, recovered sharp remainder formula for L^p-Poincaré inequality, introduced Grushin p-Bessel pairs concept for radial weights, and established sharp remainder term for Heisenberg-Pauli-Weyl inequality with sharp constant.

Conclusion: The method provides a unified framework that significantly advances the theory of Hardy-Poincaré inequalities in both Grushin and Euclidean settings, offering explicit constants and handling complex-valued functions with non-radial weights.

Abstract: In this paper, we obtain sharp remainder terms for the Hardy-Poincar\'e type
inequality with general non-radial weights in the setting of Baouendi-Grushin
vector fields (see Theorem 2.5). It is worth emphasizing that all of our
results are new both in the Grushin and standard Euclidean setting. The method
employed allows us to not only unify, but also improve the results of Kombe and
Yener [KY18] for any $1<p<\infty$ while holding true for complex-valued
functions and providing explicit constants (Corollary 2.7). As a result, we are
able to obtain sharp remainder terms to many known weighted Hardy-type
inequalities (see Section 3.1). Aside from weighted Hardy-type inequalities, we
also recover sharp remainder formula for the $L^{p}$-Poincar\'e inequality
(Corollary 3.5). In the special case of radial weights, we are naturally able
to introduce the notion of Grushin $p$-Bessel pairs (see Definition 2.9).
Finally, we are able to apply the technique to establish the sharp remainder
term of the Heisenberg-Pauli-Weyl inequality in $L^{p}$ (Corollary 3.13) that
includes the sharp constant.

</details>


### [23] [Lipschitz regularity for parabolic double phase equations with gradient nonlinearity](https://arxiv.org/abs/2508.16391)
*Abhrojyoti Sen,Jarkko Siltakoski*

Main category: math.AP

TL;DR: This paper establishes local Lipschitz regularity in space and sharp Hölder estimates in time for viscosity solutions to parabolic double phase equations using the Ishii-Lions method.


<details>
  <summary>Details</summary>
Motivation: To analyze the regularity properties of solutions to parabolic double phase equations, which combine p-Laplacian and q-Laplacian type operators with variable coefficients, and to understand the relationship between viscosity and weak solutions.

Method: Employing the Ishii-Lions method to prove local Lipschitz regularity in space and obtaining Hölder estimates in time. Also establishing equivalence between bounded viscosity solutions and weak solutions under appropriate regularity assumptions on coefficients.

Result: Proved local Lipschitz regularity in space and sharp Hölder estimates in time for viscosity solutions. Established equivalence between bounded viscosity solutions and weak solutions when the coefficient a has sufficient regularity.

Conclusion: The paper successfully demonstrates regularity results for parabolic double phase equations and provides a connection between different solution concepts (viscosity and weak solutions) for these types of equations.

Abstract: We establish the local Lipschitz regularity in space for the viscosity
solutions to the parabolic double phase equation of the form \[
\smash{\partial_{t}u-\operatorname{div} \left(|Du|^{p-2}D u+a(z)|D u|^{q-2}D
u\right)=f(z, Du)} \] by employing the Ishii-Lions method. In addition, we
obtain H\"{o}lder estimate in time which turns out to be sharp in the
degenerate regime. Here, $1< p\leq q<\infty,$ and the coefficient $a\geq 0$ is
assumed to be bounded, locally Lipschitz continuous in space, and continuous in
time. Furthermore, the non-homogeneity $f$ is assumed to be continuous on
$\Omega\times \mathbb{R}\times \mathbb{R}^N,$ and to satisfy a suitable
gradient growth condition. We also establish the equivalence between bounded
viscosity solutions and weak solutions, under appropriate additional regularity
assumption on the coefficient $a.$

</details>


### [24] [Global existence and uniqueness for Hibler's visco-plastic sea-ice model](https://arxiv.org/abs/2508.16537)
*Stefan Dingel,Karoline Disser*

Main category: math.AP

TL;DR: Global existence and uniqueness of weak solutions for Hibler's original visco-plastic sea-ice model with local cut-off for all strain rates.


<details>
  <summary>Details</summary>
Motivation: Hibler's model is standardly used in global climate simulations but lacks rigorous mathematical foundation, with previous results mainly concerning local-in-time well-posedness of regularized variants.

Method: Consider Hibler's original model with local cut-off for arbitrarily small and large strain rates, addressing degeneracy and plasticity of the stress tensor in this range.

Result: Prove global existence and uniqueness of weak solutions to the momentum equations of Hibler's visco-plastic model.

Conclusion: Provides rigorous mathematical foundation for Hibler's original sea-ice dynamics model used in climate simulations, extending beyond previous local-in-time results for regularized variants.

Abstract: In this paper, we prove global existence and uniqueness of weak solutions to
the momentum equations of Hibler's visco-plastic model for the dynamics of the
arctic sea-ice covers. Although Hibler's model is standardly used in global
climate simulations, there are only few rigorous mathematical results so far
that mainly concern local-in-time well-posedness of globally regularized
variants. Here, we consider Hibler's original model with local cut-off for
arbitrarily small and large strain rates. Degeneracy and plasticity of the
stress tensor hold in this range.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [25] [Coupled Continuous-Discontinuous Galerkin Finite Element Solver for Compound Flood Simulations](https://arxiv.org/abs/2508.15948)
*Chayanon Wichitrnithed,Eirik Valseth,Shintaro Bunya,Ethan J. Kubatko,Clint Dawson*

Main category: physics.comp-ph

TL;DR: A coupled DG-CG discretization method for shallow water equations is developed to accurately model compound flooding from tropical cyclones, integrating rainfall and runoff with storm surge.


<details>
  <summary>Details</summary>
Motivation: Recent tropical cyclones like Hurricane Harvey (2017) have shown that runoff interacting with storm surge creates amplified flooding effects that cannot be modeled by simple superposition of individual flood sources, requiring more accurate numerical simulations.

Method: Developed a locally conservative coupled Discontinuous Galerkin-Continuous Galerkin (DG-CG) discretization of shallow water equations integrated into ADCIRC, with modified continuity equation to include spatially and temporally variable rainfall using parametric rainfall models.

Result: The method demonstrated conservation properties and robustness through numerical tests including small-scale laboratory-based cases and large-scale Hurricane Harvey experiments in the Gulf of Mexico, showing effective compound flood modeling capabilities.

Conclusion: The developed coupled DG-CG approach shows strong potential for accurate compound flood modeling by effectively capturing the interaction between rainfall runoff and storm surge in tropical cyclone events.

Abstract: Several recent tropical cyclones, e.g., Hurricane Harvey (2017), have lead to
significant rainfall and resulting runoff. When the runoff interacts with storm
surge, the resulting floods can be greatly amplified and lead to effects that
cannot be correctly modeled by simple superposition of its distinctive sources.
In an effort to develop accurate numerical simulations of runoff, surge, and
compounding floods, we develop a locally conservative coupled DG-CG
discretization of the shallow water equations and integrate it into the
Advanced Circulation Model (ADCIRC). We also modify the continuity equation to
include spatially and temporally variable rainfall into the model using
parametric rainfall models. We demonstrate the capabilities of the scheme
though a sequence of physically relevant numerical tests, including small scale
test cases based on laboratory measurements and large scale experiments with
Hurricane Harvey in the Gulf of Mexico. The results highlight the conservation
properties and robustness of the developed method and show the potential of
compound flood modeling using our approach.

</details>


### [26] [graph framework: A Domain Specific Compiler for Building Physics Applications](https://arxiv.org/abs/2508.15967)
*M. Cianciosa,D. Batchelor,W. Elwasif*

Main category: physics.comp-ph

TL;DR: A graph computation framework that compiles physics equations to optimized kernel code for CPUs, Apple GPUs, and NVIDIA GPUs, demonstrated on RF ray tracing in fusion energy.


<details>
  <summary>Details</summary>
Motivation: Address hardware incompatibility issues between different GPU vendors and enable scientific codes to run across multiple accelerator platforms without vendor lock-in.

Method: Developed a graph computation framework that abstracts physics equations from hardware-specific implementations, compiling them to optimized kernel code for multiple platforms including CPUs, Apple GPUs, and NVIDIA GPUs.

Result: Created a framework capable of generating optimized kernel code for diverse hardware platforms, successfully demonstrated through application to Radio Frequency (RF) ray tracing problems in fusion energy research.

Conclusion: The framework provides a solution to hardware vendor incompatibility problems, enabling scientific applications to leverage multiple accelerator platforms while maintaining performance and avoiding dependency on Python-based machine learning frameworks.

Abstract: Modern supercomputers are increasingly relying on Graphic Processing Units
(GPUs) and other accelerators to achieve exa-scale performance at reasonable
energy usage. The challenge of exploiting these accelerators is the
incompatibility between different vendors. A scientific code written using CUDA
will not operate on a AMD gpu. Frameworks that can abstract the physics from
the accelerator kernel code are needed to exploit the current and future
hardware. In the world of machine learning, several auto differentiation
frameworks have been developed that have the promise of abstracting the math
from the compute hardware. However in practice, these framework often lag in
supporting non-CUDA platforms. Their reliance on python makes them challenging
to embed within non python based applications. In this paper we present the
development of a graph computation framework which compiles physics equations
to optimized kernel code for the central processing unit (CPUs), Apple GPUs,
and NVidia GPUs. The utility of this framework will be demonstrated for a Radio
Frequency (RF) ray tracing problems in fusion energy.

</details>


### [27] [Training a Foundation Model for Materials on a Budget](https://arxiv.org/abs/2508.16067)
*Teddy Koker,Tess Smidt*

Main category: physics.comp-ph

TL;DR: Nequix is a compact E(3)-equivariant potential that achieves state-of-the-art accuracy with significantly reduced computational costs, requiring only 500 A100-GPU hours for training and offering fast inference speeds.


<details>
  <summary>Details</summary>
Motivation: Foundation models for materials modeling are advancing but remain computationally expensive, making state-of-the-art methods inaccessible to many research groups due to high training costs.

Method: Pairs a simplified NequIP design with modern training practices including equivariant root-mean-square layer normalization and the Muon optimizer, built in JAX with 700K parameters.

Result: Ranks third overall on Matbench-Discovery and MDR Phonon benchmarks while requiring less than one quarter of the training cost of most other methods, with order-of-magnitude faster inference speed than the top-ranked model.

Conclusion: Nequix demonstrates that compact models with efficient training practices can achieve competitive accuracy in materials modeling while dramatically reducing computational requirements, making advanced methods more accessible.

Abstract: Foundation models for materials modeling are advancing quickly, but their
training remains expensive, often placing state-of-the-art methods out of reach
for many research groups. We introduce Nequix, a compact E(3)-equivariant
potential that pairs a simplified NequIP design with modern training practices,
including equivariant root-mean-square layer normalization and the Muon
optimizer, to retain accuracy while substantially reducing compute
requirements. Built in JAX, Nequix has 700K parameters and was trained in 500
A100-GPU hours. On the Matbench-Discovery and MDR Phonon benchmarks, Nequix
ranks third overall while requiring less than one quarter of the training cost
of most other methods, and it delivers an order-of-magnitude faster inference
speed than the current top-ranked model. We release model weights and fully
reproducible codebase at https://github.com/atomicarchitects/nequix

</details>


### [28] [Application of a Pressured-Based OpenFOAM Solver for Rotating Detonation Engines](https://arxiv.org/abs/2508.16105)
*Keunjae Kwak,Hyoungwoo Kim,Je Ir Ryu,Donh-Hyuk Shin*

Main category: physics.comp-ph

TL;DR: Developed OpenFOAM-based simulation framework for rotating detonation engines with adaptive mesh refinement and dynamic load balancing, achieving 11.2x computational cost reduction while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Rotating detonation engines (RDEs) show promise for improved efficiency in power generation and aircraft propulsion, but require accurate and cost-efficient simulation frameworks to facilitate further research and development.

Method: Used multicomponentFluid solver in OpenFOAM v12 with validation against 1D planar detonation and 2D RDE simulations. Implemented adaptive mesh refinement (AMR) and dynamic load balancing (DLB) for optimization. Proposed detonation velocity correction method for fair comparison with CJ detonation velocity.

Result: Simulation results agreed well with validation data both qualitatively (pressure distribution, temperature field) and quantitatively (detonation velocity, mass flux, specific impulse, thrust). Combined AMR and DLB reduced computational costs by up to 11.2 times, with DLB showing particularly important impact.

Conclusion: The developed framework provides both high accuracy and cost efficiency for RDE simulations, enabling more accessible research and development of rotating detonation engine technology through significant computational cost reductions.

Abstract: This study aims to develop a simulation framework for rotating detonation
engines (RDEs) using multicomponentFluid solver in OpenFOAM v12 and to
demonstrate reducing the computational costs by adaptive mesh refinement (AMR)
and dynamic load balancing (DLB). RDEs have been extensively studied for
improvements in efficiency for power generation and aircraft propulsion
systems. A well-established framework, showing both high accuracy and cost
efficiency, is required to facilitate further research and development in RDEs.
The multicomponentFluid solver is validated against two problems:
one-dimensional planar detonation simulation and two-dimensional RDE
simulation, in which the present study's results are compared to reference
results of experiments and simulations, respectively. In the problems, the
present simulation results agree well with the validation data both
qualitatively (e.g., pressure distribution and temperature field) and
quantitatively (e.g., detonation velocity, mass flux, and specific impulse and
thrust). In the two-dimensional RDE simulation, we propose a detonation
velocity correction method for fair comparison with Chapman-Jouguet (CJ)
detonation velocity. Moreover, the two-dimensional RDE simulation is optimized
using AMR and DLB. By adopting both, computational costs decrease by up to 11.2
times. The effect of each of them is examined as well, which highlights the
importance of DLB.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [29] [QED cascade initiation via reflection of a multipetawatt laser pulse from a self-organized parabolic plasma mirror](https://arxiv.org/abs/2508.15984)
*M. A. Serebryakov,E. N. Nerush,L. Ji,X. Geng,I. Yu. Kostyukov*

Main category: physics.plasm-ph

TL;DR: A simple setup using a single multipetawatt laser pulse reflecting off a deformed solid target can initiate QED cascades at 7 PW threshold, with 27 PW producing clear avalanche signatures including exponential growth and 15+ positron generations.


<details>
  <summary>Details</summary>
Motivation: To demonstrate a simpler method for observing self-sustained QED cascades without requiring complex multi-laser synchronization, making experimental observation more feasible.

Method: Using superposition of an incident multipetawatt laser pulse and its reflection from a solid target that deforms into a parabolic mirror, focusing the reflected radiation to initiate QED cascades.

Result: At 7 PW threshold power, cascade initiation occurs. With 27 PW laser pulse, clear avalanche signatures emerge: exponential growth, over 15 positron generations with similar energy spectra.

Conclusion: Avalanche-type QED cascades can be observed using a single laser channel without precise spatio-temporal synchronization, simplifying experimental requirements compared to previous assumptions.

Abstract: The self-sustained or avalanche-type cascade is an intriguing prediction of
strong-field quantum electrodynamics (QED) that has yet to be observed in
laboratories. It is accompanied by the conversion of electromagnetic energy
into gamma photons and electron-positron ($e^-e^+$) pairs, whose number
increases exponentially over time. We investigate a simple configuration to
initiate a QED cascades: it is based on the superposition of an incident
multipetawatt laser pulse and its reflection from a solid target. The incident
laser pulse <<deforms>> the initially flat target surface, creating a parabolic
mirror that focuses the reflected radiation. For the considered setup the
threshold laser power is about $7\,\text{PW}$. With a $27\,\text{PW}$ laser
pulse, positron production exhibits clear signatures of an avalanche-type
cascade, including exponential growth and more than 15 positron generations
with similar energy spectra. Therefore, observing an avalanche-type QED cascade
does not require the use of multiple laser channels with precise
spatio-temporal synchronization, as previously supposed.

</details>


### [30] [Resonantly Driven Electron Bernstein Waves in Magnetized Low-Pressure Capacitive Discharges](https://arxiv.org/abs/2508.16208)
*Deepak Gautam,Sarveshwar Sharma,Igor Kaganovich,Bhooshan Paradkar*

Main category: physics.plasm-ph

TL;DR: Investigation of capacitively coupled plasma discharges in mildly magnetized regime where electron Bernstein waves propagate into bulk plasma, analyzed using PIC-MCC simulations.


<details>
  <summary>Details</summary>
Motivation: To understand the physics of CCP discharges in the mildly magnetized regime (1 ≤ f_ce/f_rf < 2) where electron Bernstein waves are excited and propagate into the plasma bulk.

Method: Using particle-in-cell Monte Carlo collisions (PIC-MCC) simulations to analyze the detailed physics of CCP operation in this specific magnetic field regime.

Result: As magnetic field increases, notable changes in discharge characteristics occur with EBWs observed to propagate along plasma density gradient inside the bulk plasma.

Conclusion: The mildly magnetized regime exhibits distinctive EBW excitation and propagation behavior that significantly impacts CCP discharge characteristics, requiring detailed simulation analysis.

Abstract: The physics of capacitively coupled plasma (CCP) discharges is investigated
in a mildly magnetized regime, defined by $1 \le f_{ce}/f_{rf} < 2$, where
$f_{ce}$ and $f_{rf}$ denote the electron cyclotron frequency and the applied
radio-frequency (RF), respectively. A distinctive feature of this regime is the
excitation of electron Bernstein waves (EBWs) that propagate into the bulk
plasma. As the applied magnetic field increases, notable changes in the
discharge characteristics occur, with EBWs observed to propagate along the
plasma density gradient inside the bulk. The underlying physics of CCP
operation in this regime is analyzed in detail using particle-in-cell Monte
Carlo collisions (PIC-MCC) simulations.

</details>


### [31] [Bayesian Optimisation of Breit-Wheeler Pair Production in Simulated Laser Experiments](https://arxiv.org/abs/2508.16533)
*Christopher Arran,Stuart Morris,Christopher P. Ridgers*

Main category: physics.plasm-ph

TL;DR: Optimizing laser parameters for electron-positron pair production using Monte Carlo simulations and Gaussian Process Regression to overcome laser jitter challenges.


<details>
  <summary>Details</summary>
Motivation: Laser wakefield acceleration produces gamma rays for pair production, but practical difficulties like laser pointing and timing jitter make many-photon collisions extremely challenging to achieve.

Method: Used simulated Monte-Carlo experiments with efficient sampling algorithm for infrequent pair production, employed Gaussian Process Regression to explore multi-dimensional parameter space and find optimal conditions by varying laser spot size, colliding beam energy, and stand-off distance.

Result: Optimal stand-off distance increases with laser jitter degree; best pair production conditions differ from optimal gamma ray energy conditions; with 100J laser energy, achievable rates of ~1 pair per 100 electrons even with 10s of microns and femtoseconds jitter.

Conclusion: The methodology enables practical optimization of laser parameters for electron-positron pair production despite significant jitter, demonstrating feasibility of achieving measurable pair production rates in near-term all-optical experiments.

Abstract: High laser intensities enable the production of electron-positron pairs from
bright gamma rays passing through strong fields. Potentially the most promising
approach for all-optical experiments in the near term uses dense but higher
divergence electron beams from laser wakefield acceleration to produce gamma
rays through inverse Compton scattering. Achieving many-photon collisions
between these gamma rays and the high intensity laser pulse in practice is
extremely difficult, however, due to significant shot-to-shot jitter in laser
pointing and timing.
  We model these practical difficulties using simulated Monte-Carlo
experiments. By using a more efficient algorithm for sampling infrequent pair
production with particle splitting, we enable the exploration of a
multi-dimensional parameter space. Using Gaussian Process Regression we then
efficiently find optimal conditions for maximising pair production by changing
the laser spot size, the energy in the colliding beam, and the stand-off
distance between the laser wakefield accelerator and the focus of the colliding
laser pulse. We find that the optimal stand-off distance increases with the
degree of laser jitter and that the best conditions for producing
electron-positron pairs are not the same as the best conditions for maximising
the energy in the gamma rays. With \unit[100]{J} of laser energy, we estimate
rates of pair production of around 1 pair per 100 electrons are achievable even
with jitter of 10s of microns and 10s of femtoseconds.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [32] [An $L^0$-approach to stochastic evolution equations](https://arxiv.org/abs/2508.16458)
*Øyvind Stormark Auestad*

Main category: math.PR

TL;DR: Framework for analyzing pathwise time regularity and numerical approximation of L^0-valued stochastic evolution equations using BDG inequalities and Kolmogorov continuity test.


<details>
  <summary>Details</summary>
Motivation: To develop a comprehensive framework for studying pathwise properties of stochastic evolution equations, particularly focusing on time regularity and numerical approximation methods for equations with only stochastically integrable processes.

Method: Developed two Burkholder-Davis-Gundy type inequalities for Itô integrals with stochastically integrable processes: one in metrics for convergence in probability, and a modified version for pathwise properties. Combined with refined Kolmogorov continuity test to derive Hölder regularity and pathwise convergence rates.

Result: Obtained powerful method for deriving Hölder regularity of Itô integrals in general form and simple way to derive pathwise convergence rates for numerical approximations. Verified findings through numerical experiments on linear parabolic stochastic evolution equations with generalized Whittle-Matérn type noise.

Conclusion: The framework provides effective tools for analyzing pathwise time regularity and numerical approximation of stochastic evolution equations, with applications demonstrated for specific classes of equations and validated through numerical experiments.

Abstract: We introduce a framework for studying pathwise time regularity and numerical
approximation of $L^0$-valued stochastic evolution equations. At the core of
our framework are two Burkholder--Davis--Gundy type inequalities accommodating
It\^o integrals with respect to only stochastically integrable processes. The
first of these inequalities is formulated in suitable metrics which metrize
convergence in probability on the space of integrands and integrals. The second
is a modified version, tailored for deriving pathwise properties of the
integral. By combining it with a refined version of the Kolmogorov continuity
test, we obtain a powerful method for deriving H\"older regularity of It\^o
integrals in their most general form. Moreover, it provides a simple and
powerful way of deriving rates of pathwise convergence of numerical
approximations of stochastic evolution equations. Both applications are
illustrated for a class of linear parabolic stochastic evolution equations with
generalized Whittle--Mat\'ern type noise, and our findings are verified by
numerical experiments from this setting.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [33] [Double excitations in molecules](https://arxiv.org/abs/2508.16262)
*Namana Venkatareddy,Victor Ghosh,H. R. Krishnamurthy,Manish Jain*

Main category: physics.chem-ph

TL;DR: The paper develops two screened configuration interaction methods (scrCISD and scrCIS(D)) to study double excitations in organic molecules, finding that scrCIS(D) performs better for excitation energies while scrCISD works well for triplet pair binding energies.


<details>
  <summary>Details</summary>
Motivation: Double excitations are important for understanding singlet fission and photophysics in organic molecules like polyenes, but existing methods struggle to accurately describe states with both single and double excitation character.

Method: Developed two CI-based methods: screened configuration interaction singles and doubles (scrCISD) and screened configuration interaction singles with perturbative doubles (scrCIS(D)), applied to an effective many-body Hamiltonian that incorporates screening.

Result: scrCISD systematically underestimates excitation energies compared to best theoretical estimates, while scrCIS(D) shows good agreement. scrCISD also accurately calculates binding energies of correlated triplet pair states in pentacene dimers.

Conclusion: The scrCIS(D) method is more accurate for excitation energies, while scrCISD remains useful for calculating binding energies of doubly excited states, providing valuable tools for studying photophysical processes in organic molecules.

Abstract: Double excitations in organic molecules have garnered significant interest as
a result of their importance in singlet fission and photophysics. These
excitations play a crucial role in understanding the photoexcitation processes
in polyenes. To describe photoexcited states with both single and double
excitation character, we use a first-principles many-body theory that combines
the GW / Bethe-Salpeter equation and the configuration interaction (CI)
methods. Specifically, we develop and employ two CI-based methods: screened
configuration interaction singles and doubles (scrCISD) and screened
configuration interaction singles with perturbative doubles (scrCIS(D)),
applied to an effective many-body Hamiltonian that incorporates screening. We
apply these methods to Thiel's set of molecules, which exhibit excited states
predominantly characterized by single excitations with a partial double
excitation character. Our results indicate that the scrCISD method
systematically underestimates the excitation energies compared to the best
theoretical estimates, while the scrCIS(D) method shows good agreement with
these estimates. Furthermore, we used the scrCISD method to calculate the
binding energies of the dominantly doubly excited correlated triplet pair
states, $\mathrm{TT^1}$, in pentacene dimers, finding that the $\mathrm{TT^1}$
binding energies agree well with empirical calculations.

</details>


### [34] [Universal Multistate Kinetic Models for the In-Silico Discovery of Thermally Activated Delayed Fluorescence Emitters](https://arxiv.org/abs/2508.16436)
*Yue He,Daniel Escudero*

Main category: physics.chem-ph

TL;DR: KinLuv - extended multistate kinetic model incorporating higher excited states (S2, T2) and Herzberg-Teller vibronic coupling for accurate TADF emitter modeling


<details>
  <summary>Details</summary>
Motivation: Conventional three-state model (S0, S1, T1) fails to capture complex photophysical behaviors in TADF emitters, lacking vibronic coupling effects and high-lying excited states

Method: Developed KinLuv model that includes S2, T2 states and accounts for Herzberg-Teller vibronic coupling in rate constant calculations

Result: Successfully predicted PLQY and prompt/delayed fluorescence lifetimes for DOBNA and DiKTA emitters, matching experimental results

Conclusion: Incorporating HT vibronic coupling and higher excited states is essential for quantitative TADF modeling and designing high-performance emitters

Abstract: Many thermally activated delayed fluorescence (TADF) emitters exhibit complex
photophysical behaviors that cannot be fully captured by the conventional three
state model (S0, S1, T1). The lack of kinetic models that incorporate vibronic
coupling effects and high lying excited states has long limited the systematic
understanding and rational design of these materials. To address this, we
developed KinLuv, an extended multistate kinetic model that not only includes
higher lying excited states (S2, T2) but also accounts for the Herzberg Teller
(HT) vibronic coupling in the rate constant calculations. Applied to two
representative TADF emitters, i.e., DOBNA and DiKTa, KinLuv successfully
predicts photoluminescence quantum yields (PLQY) and prompt/delayed
fluorescence lifetimes in good agreement with reported experimental results.
These findings highlight that incorporating HT vibronic coupling effects and
higher lying excited states is essential for quantitatively modeling TADF
mechanisms and guiding the design of high performance emitters.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [35] [Underdamped Langevin MCMC with third order convergence](https://arxiv.org/abs/2508.16485)
*Maximilian Scott,Dáire O'Kane,Andraž Jelinčič,James Foster*

Main category: stat.ML

TL;DR: Proposed a new numerical method for underdamped Langevin diffusion with non-asymptotic analysis in 2-Wasserstein distance for strongly log-concave distributions. Achieves improved convergence rates when third derivatives are Lipschitz continuous.


<details>
  <summary>Details</summary>
Motivation: To develop a more efficient gradient-only method for underdamped Langevin diffusion that can leverage higher-order smoothness assumptions for improved convergence rates in sampling from strongly log-concave distributions.

Method: A new numerical method for underdamped Langevin diffusion (ULD) with non-asymptotic analysis. The method assumes Lipschitz continuous gradients and Hessians, and optionally Lipschitz continuous third derivatives for enhanced performance.

Result: Achieves 2-Wasserstein error ε in O(√d/ε) and O(√d/√ε) steps under standard assumptions. With third derivative Lipschitz continuity, achieves O(√d/ε^(1/3)) steps - the first gradient-only ULD method with third-order convergence. Competitive performance in Bayesian logistic regression on real datasets.

Conclusion: The proposed algorithm matches complexity of existing Langevin MCMC methods under standard assumptions but provides superior convergence when third derivatives are Lipschitz continuous, making it the first gradient-only ULD method with third-order convergence.

Abstract: In this paper, we propose a new numerical method for the underdamped Langevin
diffusion (ULD) and present a non-asymptotic analysis of its sampling error in
the 2-Wasserstein distance when the $d$-dimensional target distribution
$p(x)\propto e^{-f(x)}$ is strongly log-concave and has varying degrees of
smoothness. Precisely, under the assumptions that the gradient and Hessian of
$f$ are Lipschitz continuous, our algorithm achieves a 2-Wasserstein error of
$\varepsilon$ in $\mathcal{O}(\sqrt{d}/\varepsilon)$ and
$\mathcal{O}(\sqrt{d}/\sqrt{\varepsilon})$ steps respectively. Therefore, our
algorithm has a similar complexity as other popular Langevin MCMC algorithms
under matching assumptions. However, if we additionally assume that the third
derivative of $f$ is Lipschitz continuous, then our algorithm achieves a
2-Wasserstein error of $\varepsilon$ in
$\mathcal{O}(\sqrt{d}/\varepsilon^{\frac{1}{3}})$ steps. To the best of our
knowledge, this is the first gradient-only method for ULD with third order
convergence. To support our theory, we perform Bayesian logistic regression
across a range of real-world datasets, where our algorithm achieves competitive
performance compared to an existing underdamped Langevin MCMC algorithm and the
popular No U-Turn Sampler (NUTS).

</details>


### [36] [A Sharp KL-Convergence Analysis for Diffusion Models under Minimal Assumptions](https://arxiv.org/abs/2508.16306)
*Nishant Jain,Tong Zhang*

Main category: stat.ML

TL;DR: Improved convergence analysis for diffusion models with better dependence on discretization step size, reducing steps from O(d log²(1/δ)/ε²) to O(d log³/²(1/δ)/ε) to achieve O(ε²) KL divergence error.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion model convergence guarantees have suboptimal dependence on ε (inverse quadratic) and discretization step size. The paper aims to provide refined analysis with improved error bounds.

Method: Models generation as composition of reverse ODE step followed by smaller noising step. Leverages Wasserstein-type error control from ODE step, converted to KL divergence bound via noise addition. Novel analysis for discretizing Probability Flow ODE without smoothness assumptions.

Result: Achieves O(d log³/²(1/δ)/ε) steps to approximate target distribution corrupted with Gaussian noise of variance δ within O(ε²) KL divergence, improving from previous O(d log²(1/δ)/ε²) steps.

Conclusion: The refined analysis provides significantly better convergence guarantees for diffusion models, with improved dependence on ε and step size, enabling more efficient sampling with fewer discretization steps.

Abstract: Diffusion-based generative models have emerged as highly effective methods
for synthesizing high-quality samples. Recent works have focused on analyzing
the convergence of their generation process with minimal assumptions, either
through reverse SDEs or Probability Flow ODEs. The best known guarantees,
without any smoothness assumptions, for the KL divergence so far achieve a
linear dependence on the data dimension $d$ and an inverse quadratic dependence
on $\varepsilon$. In this work, we present a refined analysis that improves the
dependence on $\varepsilon$. We model the generation process as a composition
of two steps: a reverse ODE step, followed by a smaller noising step along the
forward process. This design leverages the fact that the ODE step enables
control in Wasserstein-type error, which can then be converted into a KL
divergence bound via noise addition, leading to a better dependence on the
discretization step size. We further provide a novel analysis to achieve the
linear $d$-dependence for the error due to discretizing this Probability Flow
ODE in absence of any smoothness assumptions. We show that
$\tilde{O}\left(\tfrac{d\log^{3/2}(\frac{1}{\delta})}{\varepsilon}\right)$
steps suffice to approximate the target distribution corrupted with Gaussian
noise of variance $\delta$ within $O(\varepsilon^2)$ in KL divergence,
improving upon the previous best result, requiring
$\tilde{O}\left(\tfrac{d\log^2(\frac{1}{\delta})}{\varepsilon^2}\right)$ steps.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [37] [M2C: An Open-Source Software for Multiphysics Simulation of Compressible Multi-Material Flows and Fluid-Structure Interactions](https://arxiv.org/abs/2508.16387)
*Xuning Zhao,Wentao Ma,Shafquat Islam,Aditya Narkhede,Kevin Wang*

Main category: physics.flu-dyn

TL;DR: M2C is an open-source multiphysics simulation software for extreme conditions fluid flows and fluid-structure interactions using finite volume methods, level set tracking, and exact Riemann solvers.


<details>
  <summary>Details</summary>
Motivation: To develop a comprehensive open-source tool for simulating complex multiphysics phenomena under extreme conditions like high pressures, temperatures, shock waves, and large deformations that require accurate multi-material interface handling.

Method: Finite volume method for compressible Navier-Stokes equations, level set method for multi-material interfaces, embedded boundary method for fluid-structure interfaces, FIVER for advective fluxes, coupled with Aero-S structural solver using partitioned procedure, parallelized with MPI.

Result: Developed a high-performance C++ code with verified accuracy through benchmark cases (Riemann problems, interface evolution, bubble dynamics, ionization) and demonstrated applications including laser-induced cavitation, explosion mitigation, and hypervelocity impact.

Conclusion: M2C provides a robust open-source framework for accurate simulation of extreme multiphysics phenomena with verified performance across various benchmark cases and practical applications.

Abstract: M2C (Multiphysics Modeling and Computation) is an open-source software for
simulating multi-material fluid flows and fluid-structure interactions under
extreme conditions, such as high pressures, high temperatures, shock waves, and
large interface deformations. It employs a finite volume method to solve the
compressible Navier-Stokes equations and supports a wide range of thermodynamic
equations of state. M2C incorporates models of laser radiation and absorption,
phase transition, and ionization, coupled with continuum dynamics.
Multi-material interfaces are evolved using a level set method, while
fluid-structure interfaces are tracked using an embedded boundary method.
Advective fluxes across interfaces are computed using FIVER (FInite Volume
method based on Exact multi-material Riemann problems). For two-way
fluid-structure interaction, M2C is coupled with the open-source structural
dynamics solver Aero-S using a partitioned procedure. The M2C code is written
in C++ and parallelized with MPI for high-performance computing. The source
package includes a set of example problems for demonstration and user training.
Accuracy is verified through benchmark cases such as Riemann problems,
interface evolution, single-bubble dynamics, and ionization response. Several
multiphysics applications are also presented, including laser-induced thermal
cavitation, explosion and blast mitigation, and hypervelocity impact.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [38] [Conditionally adaptive augmented Lagrangian method for physics-informed learning of forward and inverse problems using artificial neural networks](https://arxiv.org/abs/2508.15695)
*Qifeng Hu,Shamsulhaq Basir,Inanc Senocak*

Main category: cs.LG

TL;DR: Enhanced PECANN framework with multiple penalty parameters, expectation-based constraint enforcement, Fourier features, time-windowing strategy, and adaptive penalty updates for improved PDE learning.


<details>
  <summary>Details</summary>
Motivation: To improve the capability of physics and equality constrained artificial neural networks (PECANN) for learning solutions of canonical partial differential equations with better robustness, efficiency, and applicability to demanding scientific computing problems.

Method: Generalized augmented Lagrangian method with multiple penalty parameters, reformulated constraint enforcement as expectations, incorporated Fourier feature mappings, introduced time-windowing strategy for long-time evolution, and developed conditionally adaptive penalty update (CAPU) strategy.

Result: PECANN-CAPU achieves competitive accuracy across various problems including transonic rarefaction, reversible advection, high-wavenumber Helmholtz/Poisson equations, and inverse heat source identification, outperforming established methods and recent Kolmogorov-Arnold network approaches.

Conclusion: The collective advances substantially improve PECANN's robustness, efficiency, and applicability to demanding PDE problems in scientific computing, demonstrating competitive performance across diverse test cases.

Abstract: We present several advances to the physics and equality constrained
artificial neural networks (PECANN) framework that substantially improve its
capability to learn solutions of canonical partial differential equations
(PDEs). First, we generalize the augmented Lagrangian method (ALM) to support
multiple independent penalty parameters, enabling simultaneous enforcement of
heterogeneous constraints. Second, we reformulate pointwise constraint
enforcement and Lagrange multipliers as expectations over constraint terms,
reducing memory overhead and permitting efficient mini-batch training. Third,
to address PDEs with oscillatory, multi-scale features, we incorporate Fourier
feature mappings and show that a single mapping suffices where multiple
mappings or more costly architectures were required in related methods. Fourth,
we introduce a time-windowing strategy for long-time evolution in which the
terminal state of each window is enforced as an initial-condition constraint
for the next, ensuring continuity without discrete time models. Crucially, we
propose a conditionally adaptive penalty update (CAPU) strategy for ALM, which
preserves the principle that larger constraint violations incur stronger
penalties. CAPU accelerates the growth of Lagrange multipliers for selectively
challenging constraints, enhancing constraint enforcement during training. We
demonstrate the effectiveness of PECANN-CAPU on problems including the
transonic rarefaction problem, reversible advection of a passive by a vortex,
high-wavenumber Helmholtz and Poisson equations, and inverse identification of
spatially varying heat sources. Comparisons with established methods and recent
Kolmogorov-Arnold network approaches show that PECANN-CAPU achieves competitive
accuracy across all cases. Collectively, these advances improve PECANN's
robustness, efficiency, and applicability to demanding problems in scientific
computing.

</details>


<div id='physics.atom-ph'></div>

# physics.atom-ph [[Back]](#toc)

### [39] [Trapping of electrons and $^{40}\textrm{Ca}^+$ ions in a dual-frequency Paul trap](https://arxiv.org/abs/2508.16407)
*Vladimir Mikhailovskii,Natalija Sheth,Guofeng Qu,Michal Hejduk,Niklas Vilhelm Lausti,K. T. Satyajith,Christian Smorra,Günther Werth,Neha Yadav,Qian Yu,Clemens Matthiesen,Hartmut Häffner,Ferdinand Schmidt-Kaler,Hendrik Bekker,Dmitry Budker*

Main category: physics.atom-ph

TL;DR: Operation and performance characterization of a dual-frequency Paul trap capable of storing electrons and calcium ions simultaneously using two quadrupole fields at different frequencies (1.6 GHz and 2 MHz).


<details>
  <summary>Details</summary>
Motivation: To develop and characterize a dual-frequency Paul trap for potential use in synthesizing antihydrogen from antiprotons and positrons, addressing challenges in co-trapping oppositely charged species.

Method: Loading and storing electrons or calcium ions under various conditions while applying two simultaneous quadrupole fields at different frequencies, followed by detection using an electron multiplier tube. Extensive numerical simulations were conducted to support experimental findings.

Result: Tens of electrons or ions can be trapped for up to 10 milliseconds, with a small fraction remaining trapped for hundreds of milliseconds. Electron trapping decreases rapidly with increased slow field amplitude, while ion trapping shows no dependence on fast field amplitude.

Conclusion: The dual-frequency Paul trap successfully demonstrates trapping capabilities for both electrons and ions, though challenges remain for co-trapping oppositely charged species and extending particle trap duration for antihydrogen synthesis applications.

Abstract: We demonstrate the operation of a dual-frequency Paul trap and characterize
its performance by storing either electrons or calcium ions while applying two
quadrupole fields simultaneously which oscillate at $\Omega_\textrm{fast} =
2\pi \times 1.6$ GHz and $\Omega_\textrm{slow} = 2\pi \times 2$ MHz. The
particles are loaded and stored in the trap under various conditions followed
by detection employing an electron multiplier tube. We find that tens of
electrons or ions can be trapped for up to ten milliseconds and a small
fraction remains trapped even after hundreds of milliseconds. During
dual-frequency operation we find that while the number of trapped electrons
rapidly decreases with increase of the $\Omega_\textrm{slow}$ field amplitude,
the number of trapped ions shows no dependence on the $\Omega_\textrm{fast}$
field amplitude as supported by our extensive numerical simulations. We aim to
use a similar trap for synthesising antihydrogen from antiprotons and
positrons. Accordingly, we discuss open challenges such as the co-trapping of
oppositely charged species and particle trap duration.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [40] [Identification of Nonlinear Damping of Transverse Loop Oscillations by KHI-induced Turbulence](https://arxiv.org/abs/2508.16349)
*Sihui Zhong,Andrew Hillier,Iñigo Arregui*

Main category: astro-ph.SR

TL;DR: Analytical model for nonlinear damping of kink oscillations in coronal loops shows better fit than linear models, with damping time inversely proportional to velocity disturbance over loop radius.


<details>
  <summary>Details</summary>
Motivation: To understand the strong damping of large-amplitude kink oscillations in coronal loops, which observational evidence suggests is nonlinear but challenging to directly identify.

Method: Developed analytic formula for nonlinear standing kink oscillations dissipated by turbulence, used MCMC fitting with Bayesian inference, and performed Bayesian model comparison on observed data.

Result: Nonlinear function fits observed decaying kink oscillations better than traditional linear models. Initial damping time τ is inversely proportional to Vi/R. Analysis of two events shows one favors nonlinear model while the other fits linear model.

Conclusion: The analytical approximation of nonlinear damping due to turbulence provides a valid and reliable description of large-amplitude decaying kink oscillations in coronal loops.

Abstract: Kink oscillations in coronal loops have been extensively studied for their
potential contributions to coronal heating and their role in plasma diagnostics
through coronal seismology. A key focus is the strong damping of
large-amplitude kink oscillations, which observational evidence suggests is
nonlinear. However, directly identifying the nonlinearity is a challenge. This
work presents an analytic formula describing nonlinear standing kink
oscillations dissipated by turbulence, characterised by a time-varying damping
rate and period drift. We investigate how the damping behaviour depends on the
driving amplitude and loop properties, showing that the initial damping time
$\tau$ is inversely proportional to the velocity disturbance over the loop
radius, $V_i/R$. Using MCMC fitting with Bayesian inference, the nonlinear
function better fits an observed decaying kink oscillation than traditional
linear models, including exponential damping, suggesting its nonlinear nature.
By applying a Bayesian model comparison, we establish regimes in which
nonlinear and linear resonant absorption mechanisms dominate based on the
relationship between the damping rate $\tau/P$ and $V_i/R$. Additionally,
analysis of two specific events reveals that while one favours the nonlinear
model, the other is better explained by the linear model. Our results suggest
that this analytical approximation of nonlinear damping due to turbulence
provides a valid and reliable description of large-amplitude decaying kink
oscillations in coronal loops.

</details>


<div id='cond-mat.mes-hall'></div>

# cond-mat.mes-hall [[Back]](#toc)

### [41] [Advancing Quantum Transport Calculations: An Effective Medium Theory with Plane-Wave Basis and PAW Potentials in Eigenstates](https://arxiv.org/abs/2507.07366)
*Yi-Cheng Lin,Ken-Ming Lin,Yu-Chang Chen*

Main category: cond-mat.mes-hall

TL;DR: Effective medium theory implemented in VASP using PAW method with plane wave basis for transmission coefficient calculation, compared with NEGF-DFT using LCAO basis, showing minor discrepancies due to basis set differences.


<details>
  <summary>Details</summary>
Motivation: To develop an effective medium theory framework that avoids overcompleteness issues in non-equilibrium transport theories and enables decomposition of transmission coefficients into individual eigenstate contributions.

Method: Implemented EMT-PW in VASP using PAW method with plane wave basis set. Derived transmission coefficient through three approaches: current density relation, field operator method, and nonquilibrium Green's function formalism. Compared with NEGF-DFT using NanoDCAL package with LCAO basis.

Result: Minor discrepancies observed between EMT-PW and NEGF-DFT results, attributed to differences in basis sets, pseudopotentials, and lead region treatment. EMT-PW avoids overcompleteness issues and allows eigenstate decomposition of transmission.

Conclusion: EMT-PW combined with effective gate model provides powerful tool for analyzing current characteristics in nanodevices under gate voltages, offering robust foundation for exploring quantum statistics and current correlations within second quantization framework.

Abstract: We present an effective medium theory based on density functional theory that
is implemented in VASP using the PAW method with a plane wave basis set. The
transmission coefficient is derived through three complementary approaches: the
current density relation J=nqv, the field operator method, and the
nonquilibrium Green's function formalism. We compare transmission coefficients
calculated using EMT-PW with results from NEGF-DFT, based on the NanoDCAL
package utilizing a linear combination of atomic orbitals (LCAO) basis set, for
both periodic and nonperiodic boundary conditions. The minor discrepancies
observed are attributed to differences in basis sets, pseudopotentials, and the
treatment of lead regions. Notably, the EMT-PW framework avoids the common
issue of overcompleteness encountered in non-equilibrium transport theories and
allows for the decomposition of the total transmission coefficient into
contributions from individual eigenstates. Furthermore, when combined with an
effective gate model, EMT-PW is shown to be a powerful tool for analyzing
current characteristics in nanodevices under applied gate voltages. By
leveraging one-electron wavefunctions in eigenstates, this method provides a
robust foundation for exploring the quantum statistics of electrons and current
quantum correlations within the second quantization framework.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [42] [A simulation-based training framework for machine-learning applications in ARPES](https://arxiv.org/abs/2508.15983)
*MengXing Na,Chris Zhou,Sydney K. Y. Dufresne,Matteo Michiardi,Andrea Damascelli*

Main category: cond-mat.mtrl-sci

TL;DR: Aurelia - an open-source synthetic ARPES spectra simulator that generates training data for machine learning models to automate ARPES data analysis tasks like spectra quality assessment.


<details>
  <summary>Details</summary>
Motivation: Recent advances in ARPES technology create multi-dimensional datasets that present challenges in data processing and analysis. Machine learning could help but lacks sufficient training data, particularly for deep learning applications.

Method: Developed an open-source synthetic ARPES spectra simulator called aurelia to generate large training datasets. Used this to train a convolutional neural network for evaluating ARPES spectra quality during sample alignment.

Result: The simulation-trained model outperformed human analysis in assessing spectra quality and could quickly identify optimal measurement regions with high precision.

Conclusion: Simulated ARPES spectra can effectively serve as proxies for experimental spectra when training machine learning models, overcoming the data scarcity problem in ARPES analysis automation.

Abstract: In recent years, angle-resolved photoemission spectroscopy (ARPES) has
advanced significantly in its ability to probe more observables and
simultaneously generate multi-dimensional datasets. These advances present new
challenges in data acquisition, processing, and analysis. Machine learning (ML)
models can drastically reduce the workload of experimentalists; however, the
lack of training data for ML -- and in particular deep learning -- is a
significant obstacle. In this work, we introduce an open-source synthetic ARPES
spectra simulator - aurelia - for the purpose of generating the large datasets
necessary to train ML models. As a demonstration, we train a convolutional
neural network to evaluate ARPES spectra quality -- a critical task performed
during the initial sample alignment phase of the experiment. We benchmark the
simulation-trained model against actual experimental data and find that it can
assess the spectra quality more accurately than human analysis, and swiftly
identify the optimal measurement region with high precision. Thus, we establish
that simulated ARPES spectra can be an effective proxy for experimental spectra
in training ML models.

</details>


### [43] [FIRE-GNN: Force-informed, Relaxed Equivariance Graph Neural Network for Rapid and Accurate Prediction of Surface Properties](https://arxiv.org/abs/2508.16012)
*Circe Hsu,Claire Schlesinger,Karan Mudaliar,Jordan Leung,Robin Walters,Peter Schindler*

Main category: cond-mat.mtrl-sci

TL;DR: FIRE-GNN is a new graph neural network that combines surface-normal symmetry breaking and machine learning interatomic potential force information to achieve state-of-the-art accuracy for work function prediction with 0.065 eV MAE.


<details>
  <summary>Details</summary>
Motivation: First principles calculations like DFT are accurate for predicting work function and cleavage energy but computationally expensive, making comprehensive screening of materials surfaces infeasible.

Method: Developed FIRE-GNN (Force-Informed, Relaxed Equivariance Graph Neural Network) that integrates surface-normal symmetry breaking and MLIP-derived force information. Benchmarked against recent invariant and equivariant architectures.

Result: Achieved twofold reduction in mean absolute error (0.065 eV) over previous state-of-the-art for work function prediction. Consistently outperforms competing models and shows good out-of-distribution generalization.

Conclusion: FIRE-GNN enables accurate and rapid predictions of work function and cleavage energy across vast chemical space, facilitating discovery of materials with tuned surface properties for electronic emission, semiconductors, and catalysis applications.

Abstract: The work function and cleavage energy of a surface are critical properties
that determine the viability of materials in electronic emission applications,
semiconductor devices, and heterogeneous catalysis. While first principles
calculations are accurate in predicting these properties, their computational
expense combined with the vast search space of surfaces make a comprehensive
screening approach with density functional theory (DFT) infeasible. Here, we
introduce FIRE-GNN (Force-Informed, Relaxed Equivariance Graph Neural Network),
which integrates surface-normal symmetry breaking and machine learning
interatomic potential (MLIP)-derived force information, achieving a twofold
reduction in mean absolute error (down to 0.065 eV) over the previous
state-of-the-art for work function prediction. We additionally benchmark recent
invariant and equivariant architectures, analyze the impact of symmetry
breaking, and evaluate out-of-distribution generalization, demonstrating that
FIRE-GNN consistently outperforms competing models for work function
predictions. This model enables accurate and rapid predictions of the work
function and cleavage energy across a vast chemical space and facilitates the
discovery of materials with tuned surface properties

</details>


### [44] [Machine Learning Time Propagators for Time-Dependent Density Functional Theory Simulations](https://arxiv.org/abs/2508.16554)
*Karan Shah,Attila Cangi*

Main category: cond-mat.mtrl-sci

TL;DR: Novel neural operator approach accelerates real-time TDDFT electron dynamics simulations using autoregressive neural operators as density propagators, achieving superior accuracy and speed over traditional solvers.


<details>
  <summary>Details</summary>
Motivation: Time-dependent density functional theory (TDDFT) is widely used for electron dynamics but computationally expensive. There's a need for faster methods to enable real-time modeling of laser-irradiated molecules with varying experimental parameters.

Method: Uses autoregressive neural operators as time-propagators for electron density in real-time TDDFT, with physics-informed constraints, featurization, and high-resolution training data.

Result: Achieves superior accuracy and computational speed compared to traditional numerical solvers, demonstrated on 1D diatomic molecules under various laser parameters.

Conclusion: This neural operator approach has potential for enabling real-time, on-the-fly modeling of laser-irradiated molecules and materials with varying experimental conditions.

Abstract: Time-dependent density functional theory (TDDFT) is a widely used method to
investigate electron dynamics under external time-dependent perturbations such
as laser fields. In this work, we present a novel approach to accelerate
electron dynamics simulations based on real time TDDFT using autoregressive
neural operators as time-propagators for the electron density. By leveraging
physics-informed constraints and featurization, and high-resolution training
data, our model achieves superior accuracy and computational speed compared to
traditional numerical solvers. We demonstrate the effectiveness of our model on
a class of one-dimensional diatomic molecules under the influence of a range of
laser parameters. This method has potential in enabling real-time, on-the-fly
modeling of laser-irradiated molecules and materials with varying experimental
parameters.

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [45] [On the hypothesis of a bi-isotropic plasma permeating the interstellar space](https://arxiv.org/abs/2508.15879)
*Filipe S. Ribeiro,Pedro D. S. Silva,Rodolfo Casana,Manoel M. Ferreira Jr*

Main category: astro-ph.HE

TL;DR: Study of electromagnetic wave propagation in magnetized chiral plasma, revealing exotic optical signatures and constraining chiral parameter using pulsar data.


<details>
  <summary>Details</summary>
Motivation: To understand electromagnetic wave behavior in interstellar magnetized chiral plasma and establish observational constraints on magnetoelectric parameters using astrophysical data.

Method: Maxwell equations with bi-isotropic constitutive relations for cold uniform collisionless plasma model, analyzing rotatory power and dichroism coefficients, then applying astrophysical pulsar data for parameter constraints.

Result: Finite chiral parameter induces double rotatory power sign reversal (exotic optical signature), modified right-handed helicon propagation at low frequencies, and chiral parameter constraints of order 10^-16 to 10^-22 from pulsar dispersion and rotation measures.

Conclusion: Interstellar medium as bi-isotropic cold plasma exhibits unique electromagnetic modes with observable optical signatures, and astrophysical observations provide stringent upper limits on magnetoelectric chiral parameters.

Abstract: In this work, we study the propagation of electromagnetic waves in a
magnetized chiral plasma that pervades the interstellar space. The Maxwell
equations, supplemented by bi-isotropic constitutive relations, are rewritten
to describe a cold, uniform, and collisionless plasma model that yields new
collective electromagnetic modes for distinct pairs of refractive indices
associated with right- and left-handed circularly polarized waves. We have
investigated the optical behavior through the rotatory power (RP) and dichroism
coefficient, reporting that the finite chiral parameter induces double RP sign
reversal, an exotic optical signature that takes place in chiral dielectrics
and rotating plasmas. In the low-frequency regime, a modified propagating
helicon with right-handed circular polarization is obtained. Next, supposing
that the interstellar medium behaves as a bi-isotropic cold plasma, we employ
Astrophysical data of radio pulsars to achieve upper limits on the
magnetoelectric parameters magnitude. In particular, by using dispersion
measure and rotation measure data from five pulsars, we constrain the magnitude
of the chiral parameter to the order of $10^{-16}$ and $10^{-22}$,
respectively.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [46] [Quantum Fisher information as a witness of non-Markovianity and criticality in the spin-boson model](https://arxiv.org/abs/2508.16413)
*Daniele Parlato,Grazia Di Bello,Fabrizio Pavan,Giulio De Filippis,Carmine Antonio Perroni*

Main category: quant-ph

TL;DR: Quantum Fisher information matrix analysis in spin-boson model reveals entanglement, quantum phase transitions, non-Markovian effects, and enables criticality-enhanced quantum sensing.


<details>
  <summary>Details</summary>
Motivation: To understand how quantum Fisher information, a key quantity in quantum metrology, behaves in open quantum systems and how it can reveal quantum resources and critical phenomena.

Method: Used numerically exact methods to calculate static and dynamical quantum Fisher information matrix elements for the spin-boson model with respect to spin-bath couplings and magnetic field strengths.

Result: Coupling-coupling matrix elements serve as entanglement witness and detect Berezinskii-Kosterlitz-Thouless quantum phase transition through non-monotonic behavior. Time-dependent elements reveal non-Markovian effects and coherent-to-incoherent regime transition.

Conclusion: Quantum Fisher information matrix non-monotonic features signal changes in quantum resources (entanglement, coherence), quantify non-Markovian behavior, and enable criticality-enhanced quantum sensing in open quantum systems.

Abstract: The quantum Fisher information, the quantum analogue of the classical Fisher
information, is a central quantity in quantum metrology and quantum sensing
because of its connection to parameter estimation and fidelity susceptibility.
Using numerically exact methods applied to a paradigmatic open quantum system,
the spin-boson model, we calculate both static and dynamical quantum Fisher
information matrix elements with respect to spin-bath couplings and magnetic
field strengths. As the spin-bath interaction increases, we first show that the
coupling-coupling matrix elements relative to the ground state of the
Hamiltonian serve as a genuine witness of bipartite entanglement and the
Berezinskii-Kosterlitz-Thouless quantum phase transition through their
non-monotonic behavior. Furthermore, we demonstrate that the time-dependent
matrix elements can reveal non-Markovian effects as well as the transition from
the coherent to incoherent regime at the Toulouse point. In the paradigmatic
spin-boson model, the non-monotonic features of the quantum Fisher information
matrix signal changes in quantum resources such as entanglement and coherence,
quantify non-Markovian behavior, and enable criticality-enhanced quantum
sensing, thereby shedding light on key features of open quantum systems.

</details>


<div id='math.SP'></div>

# math.SP [[Back]](#toc)

### [47] [Is the optimal magnetic rectangle a square?](https://arxiv.org/abs/2508.16152)
*David Krejcirik*

Main category: math.SP

TL;DR: The paper investigates how the lowest eigenvalue of magnetic Dirichlet Laplacian depends on rectangle geometry under magnetic fields, conjecturing that squares minimize this eigenvalue under area/perimeter constraints.


<details>
  <summary>Details</summary>
Motivation: To understand the geometric dependence of magnetic Dirichlet Laplacian eigenvalues, particularly whether squares serve as optimal shapes under magnetic fields, unlike the magnetic-free case which has explicit solutions.

Method: Established lower and upper bounds for the eigenvalue, analyzed the problem for weak magnetic fields, and related the conjecture to eigenvalue simplicity and symmetry properties of minimizers in a non-convex minimization problem.

Result: Confirmed the conjecture (square as global minimizer) for weak magnetic fields, and established connections between the conjecture's validity and eigenvalue simplicity/symmetry properties.

Conclusion: The square appears to be the optimal shape minimizing the lowest magnetic Dirichlet Laplacian eigenvalue under area/perimeter constraints, particularly for weak fields, with deeper connections to spectral properties and symmetry.

Abstract: We are concerned with the dependence of the lowest eigenvalue of the magnetic
Dirichlet Laplacian on the geometry of rectangles, subject to homogeneous
fields. We conjecture that the square is a global minimiser both under the area
or perimeter constraints. Contrary to the well-known magnetic-free analogue,
the present spectral problem does not admit explicit solutions. By establishing
lower and upper bound to the eigenvalue, we establish the conjecture for weak
magnetic fields. Moreover, we relate the validity of the conjecture to the
simplicity of the eigenvalue and symmetries of minimisers of a non-convex
minimisation problem.

</details>


### [48] [Sharp bounds on the failure of the hot spots conjecture](https://arxiv.org/abs/2508.16321)
*Jaume de Dios Pont,Alexander W. Hsu,Mitchell A. Taylor*

Main category: math.SP

TL;DR: This paper analyzes the hot spots ratio, which measures the failure of Rauch's hot spots conjecture. The authors identify the maximal possible ratio over connected Lipschitz domains in R^d, showing it converges to √e as d→∞, matching previous upper bounds. They prove extremizing sets don't exist for d≥2 and extremizing sequences converge to balls at quantitative rates. The paper also provides sharp bounds on where Neumann eigenfunctions exceed boundary values, showing the conjecture becomes asymptotically true in measure as d→∞.


<details>
  <summary>Details</summary>
Motivation: To understand the degree of failure of Rauch's hot spots conjecture across different dimensions and domain types, and to identify the maximal possible hot spots ratio while studying the asymptotic behavior of this conjecture.

Method: Mathematical analysis of the hot spots ratio over connected Lipschitz domains in R^d, using techniques from spectral theory, geometric analysis, and asymptotic methods to study convergence properties and extremal behavior.

Result: The maximal hot spots ratio converges to √e as d→∞, matching previous upper bounds. For d≥2, extremizing sets do not exist but extremizing sequences converge quantitatively to balls. The paper provides sharp bounds on eigenfunction behavior and shows the hot spots conjecture becomes asymptotically true in measure.

Conclusion: The hot spots conjecture fails maximally with ratio approaching √e in high dimensions, but extremal domains approach balls and the conjecture holds asymptotically in measure, providing a nuanced understanding of Rauch's conjecture across different dimensional settings.

Abstract: The hot spots ratio of a domain $\Omega\subset \mathbb{R}^d$ measures the
degree of failure of Rauch's hot spots conjecture on that domain. We identify
the largest possible value of this ratio over all connected Lipschitz domains
$\Omega\subset \mathbb{R}^d$, for any dimension $d$. As $d\to \infty$, we show
that this maximal ratio converges to $\sqrt{e}$, which asymptotically matches
the previous best known upper bound by Mariano, Panzo and Wang. For $d\ge 2$,
we show that sets extremizing the hot spots ratio do not exist, and extremizing
sequences must converge to a ball at a quantitative rate. We then give a sharp
bound on the measure of the set for which the first Neumann eigenfunction
exceeds its maximal boundary value. From this we deduce that the hot spots
conjecture is asymptotically true "in measure'' as $d\to \infty$.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [49] [Generating Cylindrical Vector γ Rays via Beam-Target Interactions: Towards Structured Light at High Energies](https://arxiv.org/abs/2508.16220)
*Yue Cao,Kun Xue,Si-Man Liu,Zhong-Peng Li,Li-Xiang Hu,Xin-Yu Liu,Zhen-Ke Dou,Feng Wan,Qian Zhao,Tong-Pu Yu,Jian-Xing Li*

Main category: physics.optics

TL;DR: Novel method generates structured gamma rays (cylindrical vector beams) using relativistic electron beams hitting multifoil targets, achieving ~60% radial polarization through nonlinear Compton scattering of coherent transition radiation.


<details>
  <summary>Details</summary>
Motivation: Structured gamma rays are valuable for sub-nuclear imaging and polarization-sensitive applications, but conventional optical methods struggle at gamma-ray energies.

Method: Uses dense electron beams striking multifoil targets to generate coherent transition radiation (CTR), then converts this to gamma photons through nonlinear Compton scattering while preserving polarization via phase matching.

Result: Achieves radial polarization degrees approaching 60% in gamma rays, and these can decay into azimuthally spin-polarized positrons via nonlinear Breit-Wheeler process.

Conclusion: Extends structured light concept to gamma-ray regime, enabling new applications in nuclear structure probing, fundamental symmetry tests, and extreme condition studies.

Abstract: Structured {\gamma} rays, particularly cylindrical vector {\gamma} rays,
offer promising tools for sub-nuclear imaging and polarization-sensitive probes
in fundamental research and applications, but conventional optical methods face
great challenges at such photon energy. Here, we put forward a novel method
generating such {\gamma} rays through relativistic beam-target interactions.
For instance, radially polarized {\gamma} rays can be generated by using a
dense electron beam striking a multifoil target. We find that the radial
polarization is transferred from the generated coherent transition radiation
(CTR) fields to $\gamma$ photons through nonlinear Compton scattering, with the
high polarization preserved by phase matching. Three-dimensional spin-resolved
simulations demonstrate radial polarization degrees approaching 60\%.
Furthermore, these {\gamma} rays can decay into azimuthally spin-polarized
positrons via the nonlinear Breit-Wheeler process, with their spins aligning
along the CTR magnetic field. Our work extends the concept of structured light
into the {\gamma}-ray regime, offering new prospects for broad fields such as
nuclear structure probing, fundamental symmetries tests, polarization-sensitive
studies in extreme conditions, and laboratory astrophysical observations.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [50] [A User Manual for cuHALLaR: A GPU Accelerated Low-Rank Semidefinite Programming Solver](https://arxiv.org/abs/2508.15951)
*Jacob Aguirre,Diego Cifuentes,Vincent Guigues,Renato D. C. Monteiro,Victor Hugo Nascimento,Arnesh Sujanani*

Main category: math.OC

TL;DR: Julia interface for HALLaR and cuHALLaR SDP solvers with support for SDPA and HSLR data formats


<details>
  <summary>Details</summary>
Motivation: Provide a user-friendly Julia interface to leverage the speed and numerical stability of established HALLaR and cuHALLaR semidefinite programming solvers for large-scale problems

Method: Developed a Julia-based interface that allows loading custom data files, configuring solver options, and executing experiments directly from Julia, with support for both SDPA-compatible formats and enhanced HSLR structure formats

Result: Created a functional interface that includes example problems such as SDP relaxations of Matrix Completion and Maximum Stable Set problems, enabling users to work with large-scale SDPs efficiently

Conclusion: The Julia interface successfully bridges the gap between powerful precompiled SDP solvers and the Julia ecosystem, making advanced semidefinite programming capabilities more accessible to researchers and practitioners

Abstract: We present a Julia-based interface to the precompiled HALLaR and cuHALLaR
binaries for large-scale semidefinite programs (SDPs). Both solvers are
established as fast and numerically stable, and accept problem data in formats
compatible with SDPA and a new enhanced data format taking advantage of Hybrid
Sparse Low-Rank (HSLR) structure. The interface allows users to load custom
data files, configure solver options, and execute experiments directly from
Julia. A collection of example problems is included, including the SDP
relaxations of the Matrix Completion and Maximum Stable Set problems.

</details>
