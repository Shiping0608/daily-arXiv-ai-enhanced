<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 12]
- [math.AP](#math.AP) [Total: 12]
- [physics.comp-ph](#physics.comp-ph) [Total: 3]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 3]
- [gr-qc](#gr-qc) [Total: 1]
- [cond-mat.mes-hall](#cond-mat.mes-hall) [Total: 1]
- [astro-ph.SR](#astro-ph.SR) [Total: 1]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [physics.optics](#physics.optics) [Total: 2]
- [quant-ph](#quant-ph) [Total: 3]
- [physics.chem-ph](#physics.chem-ph) [Total: 2]
- [physics.class-ph](#physics.class-ph) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Neural operators for solving nonlinear inverse problems](https://arxiv.org/abs/2508.19347)
*Otmar Scherzer,Thi Lan Nhi Vu,Jikai Yan*

Main category: math.NA

TL;DR: Analysis of Tikhonov regularization using neural operators for solving ill-posed operator equations, with extensions to Sobolev and Lebesgue spaces and network structure optimization.


<details>
  <summary>Details</summary>
Motivation: To solve infinite dimensional operator equations where operators are specified indirectly through input-output training pairs rather than physical laws, requiring robust regularization methods for ill-posed problems.

Method: Tikhonov regularization with neural operators as surrogates, balancing approximation errors, regularization parameters, and noise. Extends neural operator approximation properties to Sobolev and Lebesgue spaces.

Result: Provides analytical framework for solving ill-posed operator equations using neural operators, with extended approximation capabilities beyond continuous functions to more general function spaces.

Conclusion: Neural operators combined with Tikhonov regularization offer an effective approach for solving inverse problems involving indirectly specified operators, with proper network structure being crucial for success.

Abstract: We consider solving a probably infinite dimensional operator equation, where
the operator is not modeled by physical laws but is specified indirectly via
training pairs of the input-output relation of the operator. Neural operators
have proven to be efficient to approximate operators with such information. In
this paper, we analyze Tikhonov regularization with neural operators as
surrogates for solving ill-posed operator equations. The analysis is based on
balancing approximation errors of neural operators, regularization parameters,
and noise. Moreover, we extend the approximation properties of neural operators
from sets of continuous functions to Sobolev and Lebesgue spaces, which is
crucial for solving inverse problems. Finally, we address the problem of
finding an appropriate network structure of neural operators.

</details>


### [2] [Numerical Optimization for Tensor Disentanglement](https://arxiv.org/abs/2508.19409)
*Julia Wei,Alec Dektor,Chungen Shen,Zaiwen Wen,Chao Yang*

Main category: math.NA

TL;DR: This paper presents optimization methods for tensor disentangling - identifying transformations that reduce bond dimensions in tensor networks by exploiting gauge freedom through orthogonal matrix optimization and rank minimization.


<details>
  <summary>Details</summary>
Motivation: Tensor networks provide compact representations of high-dimensional data, but there's a need for efficient methods to reduce bond dimensions through gauge freedom exploitation to enable more scalable computations in quantum physics, PDEs, and machine learning.

Method: Formulates tensor disentangling as optimization over orthogonal matrices to minimize matricized rank. Uses Riemannian optimization and a joint optimization framework alternating between optimizing orthogonal transformations and low-rank approximations. Introduces binary search strategy for optimal rank discovery.

Result: Numerical experiments on random tensors and approximate isometric tensor network states show the effectiveness of the proposed optimization methods and the potential for hybrid approaches combining different methods.

Conclusion: The presented optimization framework provides competitive alternatives for tensor disentangling, with the binary search strategy effectively handling unknown optimal rank scenarios, offering practical solutions for reducing bond dimensions in tensor networks.

Abstract: Tensor networks provide compact and scalable representations of
high-dimensional data, enabling efficient computation in fields such as quantum
physics, numerical partial differential equations (PDEs), and machine learning.
This paper focuses on tensor disentangling, the task of identifying
transformations that reduce bond dimensions by exploiting gauge freedom in the
network. We formulate this task as an optimization problem over orthogonal
matrices acting on a single tensor's indices, aiming to minimize the rank of
its matricized form. We present Riemannian optimization methods and a joint
optimization framework that alternates between optimizing the orthogonal
transformation for a fixed low-rank approximation and optimizing the low-rank
approximation for a fixed orthogonal transformation, offering a competitive
alternative when the target rank is known. To seek the often unknown optimal
rank, we introduce a binary search strategy integrated with the disentangling
procedure. Numerical experiments on random tensors and tensors in an
approximate isometric tensor network state are performed to compare different
optimization methods and explore the possibility of combining different methods
in a hybrid approach.

</details>


### [3] [A deep first-order system least squares method for the obstacle problem](https://arxiv.org/abs/2508.19412)
*Gabriel Acosta,Eugenia Belén,Francisco M. Bersetche,Juan Pablo Borthagaray*

Main category: math.NA

TL;DR: Deep learning approach for obstacle problems using FOSLS framework, reformulating as convex minimization with simultaneous approximation of solution, gradient, and Lagrange multiplier. Provides mesh-free scalable method for high dimensions with theoretical guarantees.


<details>
  <summary>Details</summary>
Motivation: To develop a flexible, mesh-free alternative for obstacle problems that scales efficiently to high-dimensional settings, overcoming limitations of traditional methods.

Method: First-order system least-squares (FOSLS) framework reformulating obstacle problem as convex minimization task. Simultaneously approximates solution, gradient, and Lagrange multiplier using deep learning approach.

Result: Numerical experiments in dimensions up to 20 demonstrate robustness and scalability, even on non-Lipschitz domains. Method shows practical effectiveness in high-dimensional settings.

Conclusion: The proposed deep learning FOSLS approach provides an effective mesh-free method for obstacle problems with strong theoretical foundations (coercivity, Lipschitz continuity, convergence guarantees) and practical scalability to high dimensions.

Abstract: We propose a deep learning approach to the obstacle problem inspired by the
first-order system least-squares (FOSLS) framework. This method reformulates
the problem as a convex minimization task; by simultaneously approximating the
solution, gradient, and Lagrange multiplier, our approach provides a flexible,
mesh-free alternative that scales efficiently to high-dimensional settings. Key
theoretical contributions include the coercivity and local Lipschitz continuity
of the proposed least-squares functional, along with convergence guarantees via
$\Gamma$-convergence theory under mild regularity assumptions. Numerical
experiments in dimensions up to 20 demonstrate the method's robustness and
scalability, even on non-Lipschitz domains.

</details>


### [4] [A Square-Root Free Algorithm for Computing Real Givens Rotations](https://arxiv.org/abs/2508.19431)
*Carlos F. Borges*

Main category: math.NA

TL;DR: A square-root-free algorithm for constructing real Givens rotations that is competitive with square-root-based methods on FMA-supported hardware and applicable to a wider range of algorithms.


<details>
  <summary>Details</summary>
Motivation: To develop an accurate alternative to square-root-based Givens rotation algorithms that avoids square root operations while maintaining performance and broad applicability.

Method: Developed a square-root-free algorithm for constructing real Givens rotations, leveraging fused multiply-add (FMA) hardware operations. The algorithm constructs Givens rotations directly without square roots.

Result: The algorithm is competitive with square-root-based methods on processors with hardware FMA support. Accuracy was validated through simulation studies.

Conclusion: This square-root-free approach provides an efficient and accurate alternative to traditional square-root-based Givens rotation algorithms, with broader applicability across various algorithms that use Givens rotations.

Abstract: We develop an accurate square-root-free algorithm for constructing real
Givens rotations. On processors that support the fused multiply-add operation
in hardware, the algorithm is competitive with square-root based algorithms
using a hardware square-root. Unlike the square-root-free algorithms in
\cite{Hsieh1993,GENTLEMAN1973,Barlow1987,Hammarling1974ANO,Ling1989EfficientLL,Hanson,Hanson2},
our approach will construct the Givens rotation directly and is therefore
applicable to a much wider variety of algorithms that use Givens rotations. We
investigate the accuracy of the algorithm by simulation.

</details>


### [5] [A new class of regularized preconditioners for double saddle-point problems](https://arxiv.org/abs/2508.19469)
*Achraf Badahmane*

Main category: math.NA

TL;DR: Novel 3x3 block preconditioners for double saddle-point problems from Maxwell equations and liquid crystal modeling, addressing limitations of existing methods under high Reynolds numbers.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of recent preconditioners under high Reynolds numbers, which were not explored in original studies, and demonstrate superior performance in such regimes.

Method: Introduces a new class of three-by-three block preconditioners specifically designed for double saddle-point problems from time-dependent Maxwell equations and liquid crystal director modeling.

Result: Thorough analysis of convergence and spectral properties shows the proposed preconditioner outperforms existing methods under high Reynolds numbers, with theoretical bounds verified.

Conclusion: The novel three-by-three block preconditioner class effectively addresses high Reynolds number scenarios where previous methods fail, demonstrating superior efficiency and performance for double saddle-point problems.

Abstract: The block structure of double saddle-point problems has prompted extensive
research into efficient preconditioners. This paper introduces a novel class of
three-by-three block preconditioners tailored for such systems from the
time-dependent Maxwell equations or liquid crystal director modeling. The main
motivation of this work is to highlight the limitations of recent
preconditioners under high Reynolds numbers, as the original studies did not
explore this scenario, and to demonstrate that our preconditioner outperforms
the existing ones in such regimes. We thoroughly analyze the convergence and
spectral properties of the proposed preocnditioner. We illustrate the
efficiency of the proposed preconditioners, and verify the theoretical bounds.

</details>


### [6] [NLAFormer: Transformers Learn Numerical Linear Algebra Operations](https://arxiv.org/abs/2508.19557)
*Zhantao Ma,Yihang Gao,Michael K. Ng*

Main category: math.NA

TL;DR: NLAFormer is a transformer-based architecture designed to learn numerical linear algebra operations without simulating computer control flow, reducing input size and layer requirements while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: To leverage transformers' effectiveness in modeling complex relationships for learning numerical linear algebra operations more efficiently than existing approaches like loop-transformers.

Method: Proposes NLAFormer architecture that uses linear algebra arguments to express operations like pointwise computation, shifting, transposition, inner product, and matrix operations without simulating computer control flow.

Result: The approach significantly reduces both input matrix size and number of required layers while being able to learn complex algorithms like conjugate gradient method for solving symmetric positive definite linear systems.

Conclusion: NLAFormer demonstrates that transformers can effectively learn numerical linear algebra operations with improved efficiency and performance, as validated through experimental results.

Abstract: Transformers are effective and efficient at modeling complex relationships
and learning patterns from structured data in many applications. The main aim
of this paper is to propose and design NLAFormer, which is a transformer-based
architecture for learning numerical linear algebra operations: pointwise
computation, shifting, transposition, inner product, matrix multiplication, and
matrix-vector multiplication. Using a linear algebra argument, we demonstrate
that transformers can express such operations. Moreover, the proposed approach
discards the simulation of computer control flow adopted by the
loop-transformer, significantly reducing both the input matrix size and the
number of required layers. By assembling linear algebra operations, NLAFormer
can learn the conjugate gradient method to solve symmetric positive definite
linear systems. Experiments are conducted to illustrate the numerical
performance of NLAFormer.

</details>


### [7] [Energy-Equidistributed Moving Sampling Physics-informed Neural Networks for Solving Conservative Partial Differential Equations](https://arxiv.org/abs/2508.19561)
*Qinjiao Gao,Longzhe Xu,Dongjiang Wang,Ran Zhang*

Main category: math.NA

TL;DR: Novel energy-equidistributed adaptive sampling framework for multi-dimensional conservative PDEs using energy density as monitor function, integrated with deep neural networks to create EEMS-PINNs that combine physics-informed learning with energy-adaptive mesh optimization.


<details>
  <summary>Details</summary>
Motivation: To develop a framework that maintains solution accuracy in long-time simulations while preserving conserved energy, addressing the challenge of dynamic energy evolution tracking during temporal integration.

Method: Introduces location-based and velocity-based formulations of Energy-Equidistributed moving mesh PDEs (EMMPDEs) using energy density function as monitor function, integrated with deep neural networks to establish Energy-Equidistributed Moving Sampling Physics-Informed Neural Networks (EEMS-PINNs).

Result: Extensive numerical experiments demonstrate EEMS-PINNs effectively maintain solution accuracy in long-time simulations while preserving conserved energy, with robust performance in non-conservative systems.

Conclusion: The proposed framework successfully integrates physics-informed learning with energy-adaptive mesh optimization, providing stable and accurate performance for both conservative and non-conservative systems in long-time simulations.

Abstract: This paper presents a novel Energy-Equidistributed adaptive sampling
framework for multi-dimensional conservative PDEs, introducing both
location-based and velocity-based formulations of Energy-Equidistributed moving
mesh PDEs (EMMPDEs). The framework utilizes the energy density function as the
monitor function, ensuring that mesh adaptation dynamically tracks energy
evolution during temporal integration. These theoretical developments are
integrated with deep neural networks to establish the Energy-Equidistributed
Moving Sampling Physics-Informed Neural Networks (EEMS-PINNs), which integrate
physics-informed learning with energy-adaptive mesh optimization. Extensive
numerical experiments demonstrate that EEMS-PINNs effectively maintain solution
accuracy in long-time simulations while preserving conserved energy. The
framework's robustness is further evidenced by its stable performance in
non-conservative systems. The code for this paper can be found at
https://github.com/sufe-Ran-Zhang/EMMPDE.

</details>


### [8] [Eighth-Order Accurate Methods for Boundary Value Problems Arising from the Lane-Emden Equation](https://arxiv.org/abs/2508.19729)
*Dang Quang A,Nguyen Thanh Huong,Vu Vinh Quang*

Main category: math.NA

TL;DR: High-order numerical methods for solving singular Lane-Emden boundary value problems with eighth-order accuracy and convergence.


<details>
  <summary>Details</summary>
Motivation: The Lane-Emden equation frequently arises in astrophysics and nonlinear models but presents challenges due to singularity at one endpoint, requiring robust numerical methods.

Method: Established existence/uniqueness of solution, proposed continuous iterative method, discretized using trapezoidal quadrature with correction terms, derived three discrete iterative schemes for specific cases.

Result: Methods achieve eighth-order accuracy and convergence, numerical experiments validate theoretical findings and demonstrate superiority over existing methods.

Conclusion: Efficient tools for solving Lane-Emden boundary value problems that can be extended to higher-order nonlinear singular models like Emden-Fowler equations.

Abstract: This paper presents high-order numerical methods for solving boundary value
problems associated with the Lane-Emden equation, which frequently arises in
astrophysics and various nonlinear models. A major challenge in studying this
equation lies in its singularity at one endpoint. Prior to constructing the
numerical methods, we establish the existence and uniqueness of the solution
and propose a continuous iterative method. This continuous method is then
discretized using the trapezoidal quadrature rule enhanced with correction
terms. As a result, we derive three discrete iterative schemes tailored for
three specific cases of the Lane-Emden equation. We rigorously prove that the
proposed methods achieve eighth-order accuracy and convergence. A series of
numerical experiments is conducted to validate the theoretical findings and
demonstrate the accuracy and convergence order of the proposed schemes, which
outperform existing methods. These schemes thus provide efficient tools for
solving Lane-Emden boundary value problems and can be readily extended to
higher-order nonlinear singular models, such as Emden-Fowler equations, which
arise in many applications.

</details>


### [9] [An adaptive finite element discretization based parallel orbital-updating method for eigenvalue problems](https://arxiv.org/abs/2508.19832)
*Xiaoying Dai,Yan Li,Bin Yang,Aihui Zhou*

Main category: math.NA

TL;DR: Mathematical justification for parallel orbital-updating method for clustered eigenvalue problems of linear PDE operators, including convergence and error estimates.


<details>
  <summary>Details</summary>
Motivation: Solving eigenvalue problems of partial differential operators requires many highly accurate eigenpair approximations, which is computationally challenging. The adaptive finite element parallel orbital-updating method has shown efficiency in electronic structure calculations but lacks mathematical foundation.

Method: Provides mathematical analysis and justification for the adaptive finite element discretization based parallel orbital-updating method for clustered eigenvalue problems of linear partial differential operators.

Result: Establishes convergence properties and provides error estimates for the numerical approximations obtained through this method.

Conclusion: The paper provides the necessary mathematical foundation to support the effectiveness of the parallel orbital-updating approach for solving clustered eigenvalue problems in PDEs, validating its use in computational applications like electronic structure calculations.

Abstract: It is significant and challenging to solve eigenvalue problems of partial
differential operators when many highly accurate eigenpair approximations are
required. The adaptive finite element discretization based parallel
orbital-updating method, which can significantly reduce the computational cost
and enhance the parallel scalability, has been shown to be efficient in
electronic structure calculations. In this paper, we provide a mathematical
justification for the method for clustered eigenvalue problems of linear
partial differential operators, including the convergence and error estimates
of the numerical approximations.

</details>


### [10] [Dominant H-Eigenvectors of Tensor Kronecker Products Do Not Decouple](https://arxiv.org/abs/2508.19902)
*Ayush Kulkarni,Charles Colley,David F. Gleich*

Main category: math.NA

TL;DR: Counterexample showing Kronecker product of tensors' dominant H-eigenvector doesn't decouple like matrices/Z-eigenvectors, with H-eigenvalues exceeding component products.


<details>
  <summary>Details</summary>
Motivation: To address an open question about whether the dominant H-eigenvector of a Kronecker product of tensors decouples into a product of eigenvectors like in matrices and Z-eigenvector cases.

Method: Constructed a counterexample demonstrating that for H-eigenvectors, the dominant eigenvector of a Kronecker product does not decouple into individual tensor eigenvectors.

Result: Showed that the largest H-eigenvalue can actually exceed the product of the H-eigenvalues of the component tensors, contrary to matrix and Z-eigenvector behavior.

Conclusion: H-eigenvectors behave fundamentally differently from matrices and Z-eigenvectors in Kronecker products, with non-decoupling properties and potential eigenvalue amplification.

Abstract: We illustrate a counterexample to an open question related to the dominant
H-eigenvector of a Kronecker product of tensors. For matrices and
Z-eigenvectors of tensors, the dominant eigenvector of a Kronecker product
decouples into a product of eigenvectors of the tensors underlying the
Kronecker product. This does not occur for H-eigenvectors and indeed, the
largest H-eigenvalue can exceed the product of the H-eigenvalues of the
component tensors.

</details>


### [11] [Performance evaluation of high-order compact and second-order gas-kinetic schemes in compressible flow simulations](https://arxiv.org/abs/2508.19911)
*Yaqing Yang,Fengxiang Zhao,Kun Xu*

Main category: math.NA

TL;DR: Fifth-order compact gas-kinetic scheme (CGKS-5th) achieves comparable resolution to second-order scheme at 10x lower computational cost, with superior accuracy in turbulent flows with shocks and vortices.


<details>
  <summary>Details</summary>
Motivation: Trade-off between accuracy, robustness, and computational cost in complex flow simulations; need to evaluate practical performance of high-order schemes for scale-resolving simulations.

Method: Comparative evaluation of fifth-order compact gas-kinetic scheme (CGKS-5th) vs conventional second-order gas-kinetic scheme (GKS-2nd) across subsonic to supersonic test cases; implemented multi-GPU parallelization with CUDA and MPI.

Result: CGKS-5th achieves similar resolution as GKS-2nd at approximately 10x lower computational cost; delivers significantly higher accuracy and resolution under equivalent computational resources, especially for turbulent flows with shocks and small-scale vortices.

Conclusion: First clear verification of high-order compact gas-kinetic schemes' advantages for viscous flows with discontinuities; demonstrates superior computational efficiency and accuracy for complex flow simulations.

Abstract: The trade-off among accuracy, robustness, and computational cost remains a
key challenge in simulating complex flows. Second-order schemes are
computationally efficient but lack the accuracy required for resolving
intricate flow structures, particularly in turbulence. High-order schemes,
especially compact high-order schemes, offer superior accuracy and resolution
at a relatively modest computational cost. To clarify the practical performance
of high-order schemes in scale-resolving simulations, this study evaluates two
representative gas-kinetic schemes: the newly developed fifth-order compact
gas-kinetic scheme (CGKS-5th) and the conventional second-order gas-kinetic
scheme (GKS-2nd). Test cases ranging from subsonic to supersonic flows are used
to quantitatively assess their accuracy and efficiency. The results demonstrate
that CGKS-5th achieves comparable resolution to GKS-2nd at roughly an order of
magnitude lower computational cost. Under equivalent computational resources,
CGKS-5th delivers significantly higher accuracy and resolution, particularly in
turbulent flows involving shocks and small-scale vortices. This study provides
the first clear verification of the advantages of high-order compact
gas-kinetic schemes in simulating viscous flows with discontinuities.
Additionally, multi-GPU parallelization using CUDA and MPI is implemented to
enable large-scale applications.

</details>


### [12] [High-order nonuniform time-stepping and MBP-preserving linear schemes for the time-fractional Allen-Cahn equation](https://arxiv.org/abs/2508.19965)
*Bingyin Zhang,Hongfei Fu,Hong Wag*

Main category: math.NA

TL;DR: High-order linear stabilized schemes for time-fractional Allen-Cahn equation that preserve energy stability and maximum-bound principle using novel prediction strategy and auxiliary functional.


<details>
  <summary>Details</summary>
Motivation: To develop nonuniform time-stepping schemes that can simultaneously preserve both discrete energy stability and maximum-bound principle for the time-fractional Allen-Cahn equation, which is challenging due to the nonlinear potential.

Method: Developed a new prediction strategy for second-order MBP-preserving predicted solution, introduced nonnegative auxiliary functional for stabilization term design, and proposed L1 and L2-1_σ schemes with improved versions using unbalanced stabilization terms.

Result: L1 scheme unconditionally preserves discrete MBP, L2-1_σ scheme requires mild time-step restriction, and improved L2-1_σ scheme enhances MBP preservation for large time steps. Numerical examples validate accuracy and physics-preserving properties.

Conclusion: The proposed linear stabilized schemes successfully achieve both energy stability and maximum-bound principle preservation for time-fractional Allen-Cahn equation through innovative prediction strategies and stabilization techniques, demonstrating effectiveness in maintaining physical properties.

Abstract: In this paper, we present a class of nonuniform time-stepping, high-order
linear stabilized schemes that can preserve both the discrete energy stability
and maximum-bound principle (MBP) for the time-fractional Allen-Cahn equation.
To this end, we develop a new prediction strategy to obtain a second-order and
MBP-preserving predicted solution, which is then used to handle the nonlinear
potential explicitly. Additionally, we introduce an essential nonnegative
auxiliary functional that enables the design of an appropriate stabilization
term to dominate the predicted nonlinear potential, and thus to preserve the
discrete MBP. Combining the newly developed prediction strategy and auxiliary
functional, we propose two unconditionally energy-stable linear stabilized
schemes, L1 and L2-$1_\sigma$ schemes. We show that the L1 scheme
unconditionally preserves the discrete MBP, whereas the L2-$1_\sigma$ scheme
requires a mild time-step restriction. Furthermore, we develop an improved
L2-$1_\sigma$ scheme with enhanced MBP preservation for large time steps,
achieved through a novel unbalanced stabilization term that leverages the
boundedness and monotonicity of the auxiliary functional. Representative
numerical examples validate the accuracy, effectiveness, and physics-preserving
of the proposed methods.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [13] [Higher-order homogenization for equations of linearized elasticity using the operator-asymptotic approach](https://arxiv.org/abs/2508.19388)
*Yi-Sheng Lim,Josip Žubrinić*

Main category: math.AP

TL;DR: Extension of operator-asymptotic approach to 3D linearized elasticity for higher-order convergence rates using Bloch approximation


<details>
  <summary>Details</summary>
Motivation: To improve homogenization of periodic composite media by achieving higher-order convergence rates beyond the original operator-asymptotic approach

Method: Extends Lim-Žubrinić's operator-asymptotic approach to 3D linearized elasticity, employs Bloch approximation for vector-valued functions with compact Fourier support

Result: Achieves error of order ε^{n+1} in L^2 and ε^n in H^1 norms for any n, significantly improving convergence rates

Conclusion: The extended operator-asymptotic approach with Bloch approximation provides superior convergence rates in homogenization of periodic elastic composites

Abstract: The operator-asymptotic approach was introduced by Lim-\v{Z}ubrini\'c in
[Asymptotic Analysis. 141(4), p. 211-256 (2025)] for the homogenization of an
$\varepsilon\mathbb{Z}^d$-periodic composite media. In this article, we
consider the setting of three-dimensional linearized elasticity, and extend the
approach to obtain higher-order convergence rates. In particular, we consider
the so-called ``Bloch approximation'' for vector-valued functions with compact
Fourier support, and demonstrate that under such data, the approach provides an
expansion that yields an error of order $\varepsilon^{n+1}$ in $L^2$ and
$\varepsilon^n$ in $H^1$, for any $n$.

</details>


### [14] [Maximal estimates for orthonormal systems of wave equations](https://arxiv.org/abs/2508.19446)
*Shinya Kinoshita,Hyerim Ko,Shobu Shiraki*

Main category: math.AP

TL;DR: Maximal estimates for wave operators extended to orthonormal families using geometric analysis and Schatten 2 estimates, with partial progress in low dimensions.


<details>
  <summary>Details</summary>
Motivation: To extend classical maximal estimates for wave operators to orthonormal families of initial data, addressing limitations in existing theory.

Method: Geometric analysis of wave operator kernels within Schatten 2 estimates framework, utilizing Wolff's geometric lemma on intersection patterns of thickened spheres.

Result: Partial progress achieved on maximal estimates for orthonormal systems in low dimensions through novel geometric approach.

Conclusion: The geometric framework provides a promising approach for extending maximal wave operator estimates to orthonormal families, with successful partial results in low-dimensional cases.

Abstract: This paper investigates maximal estimates of the wave operators for
orthonormal families of initial data. We extend the classical maximal estimates
for the wave operator by making partial progress on maximal estimates for
orthonormal systems in low dimensions. Our novel approach is based on a
geometric analysis of the kernel of wave operators within the framework of
Schatten $2$ estimates. In particular, we exploit Wolff's geometric lemma on
the intersection patterns of thickened spheres.

</details>


### [15] [Maximal estimates for orthonormal systems of wave equations with sharp regularity](https://arxiv.org/abs/2508.19451)
*Hyerim Ko,Sanghyuk Lee,Shobu Shiraki*

Main category: math.AP

TL;DR: Optimal maximal estimates for wave equation with orthonormal initial data, achieving sharp regularity in 3D and improved Schatten exponents in higher dimensions.


<details>
  <summary>Details</summary>
Motivation: To establish optimal maximal estimates for the wave equation with orthonormal initial data, improving upon previous results by Kinoshita-Ko-Shiraki and achieving sharp regularity bounds across different dimensions.

Method: Novel analysis of a key integral arising in the case β=2, which refines existing techniques to achieve optimal estimates through improved integral analysis.

Result: Optimal results with sharp regularity exponent up to endpoint in d=3; sharp bounds for Schatten exponent β∈[2,∞] when d≥4 and β∈[1,2] when d=2, improving previous estimates.

Conclusion: The new approach based on refined integral analysis enables optimal maximal estimates for wave equations with orthonormal data across various dimensions, achieving sharp regularity and improved Schatten exponents beyond previous results.

Abstract: We study maximal estimates for the wave equation with orthonormal initial
data. In dimension $d=3$, we establish optimal results with the sharp
regularity exponent up to the endpoint. In higher dimensions $d \ge 4$ and also
in $d=2$, we obtain sharp bounds for the Schatten exponent (summability index)
$\beta\in [2, \infty]$ when $d\ge4$, and $\beta\in[1, 2]$ when $d=2$, improving
upon the previous estimates due to Kinoshita--Ko--Shiraki. Our approach is
based on a novel analysis of a key integral arising in the case $\beta=2$,
which allows us to refine existing techniques and achieve the optimal
estimates.

</details>


### [16] [Global regularity of Leray-Hopf weak solutions to 3D Navier-Stokes equations](https://arxiv.org/abs/2508.19590)
*Myong-Hwan Ri*

Main category: math.AP

TL;DR: Leray-Hopf weak solutions to 3D Navier-Stokes equations with H^1/2 initial data are regular and belong to L^1(0,1; H^1/2(R^3))


<details>
  <summary>Details</summary>
Motivation: To establish regularity of weak solutions to the 3D Navier-Stokes equations, which is a fundamental open problem in fluid dynamics

Method: Construct a supercritical space with inverse logarithmic weight in frequency domain, obtain energy estimates for high frequency parts, and use re-scaling argument to estimate critical norms

Result: Proved that any Leray-Hopf weak solution with H^1/2 initial data is regular and belongs to L^1(0,1; H^1/2(R^3))

Conclusion: The paper provides a new approach to prove regularity for 3D Navier-Stokes equations using supercritical spaces and re-scaling techniques

Abstract: We show that any Leray-Hopf weak solution to 3D Navier-Stokes equations with
initial values u0 2 H1=2(R3) belong to L1(0; 1; H1=2(R3)) and thus it is
regular. For the proof, flrst, we construct a supercritical space, the norm of
which is compared to the homogeneous Sobolev H_ 1=2-norm in that it has inverse
logarithmic weight very sparsely in the frequency domain. Then we obtain the
energy estimates of high frequency parts of the solution which involve the
supercritical norm on the right-hand side. Finally, we superpose the energy
norm of high frequency parts of the solution to get estimates of the critical
norms of the weak solution via the re-scaling argument.

</details>


### [17] [Backward Harnack inequality and Hamilton estimates for heat type equations](https://arxiv.org/abs/2508.19680)
*Juanling Lu,Yuting Wu,Qi S. Zhang*

Main category: math.AP

TL;DR: A backward-in-time Harnack inequality for positive solutions of the heat equation on compact manifolds, allowing comparison of solution values at different space-time points in both time directions.


<details>
  <summary>Details</summary>
Motivation: To extend Harnack inequalities beyond the usual forward-in-time versions and remove restrictions like boundedness or vanishing boundary conditions that limit applicability.

Method: Based on Hamilton's gradient estimates for the heat equation, the authors develop a backward-in-time Harnack inequality that works on compact manifolds without additional solution constraints.

Result: A novel Harnack inequality that enables comparison of solution values at different space-time points in both forward and backward time directions, expanding the scope beyond traditional Harnack inequalities.

Conclusion: This backward-in-time Harnack inequality represents a significant extension of classical results and is expected to have important applications given the fundamental role of Harnack inequalities in analysis.

Abstract: Based on gradient estimates for the heat equation by Hamilton, we discover a
backward in time Harnack inequality for positive solutions on compact manifolds
without further restrictions such as boundedness or vanishing boundary value
for solutions. Contrary to the usual Harnack inequality, it allows comparison
of the values of a solution at two different space time points, in both
directions of time. In view of the importance of the usual Harnack inequality,
further application is expected.

</details>


### [18] [Controllability of a One-Dimensional Dynamic Debonding Model](https://arxiv.org/abs/2508.19755)
*Nicola De Nitti,Arick Shao*

Main category: math.AP

TL;DR: Analysis of boundary controllability for a 1D dynamic debonding model coupling wave equation with Griffith fracture criterion, with characterization of reachable states and exact control construction.


<details>
  <summary>Details</summary>
Motivation: To understand how to control the debonding/fracture propagation process in a dynamic system governed by wave mechanics and Griffith's fracture criterion, which has applications in material science and engineering.

Method: Investigates a one-dimensional dynamic debonding model that couples the wave equation with Griffith's fracture propagation criterion, focusing on boundary controllability analysis.

Result: Provides precise characterizations of reachable target states in both C⁰,¹ and C¹ regularity settings, and constructs exact controls to achieve these target states.

Conclusion: The study successfully establishes controllability properties for the dynamic debonding system, offering mathematical foundations for controlling fracture propagation in wave-mediated debonding processes.

Abstract: We investigate a one-dimensional dynamic debonding model, introduced by
Freund (1990), in which the wave equation is coupled with a Griffith criterion
governing the propagation of the fracture. In particular, we study the boundary
controllability of the system to a prescribed target state. Our main results
provide precise characterizations of the reachable target states, in both \(
C^{ 0, 1 } \) and \( C^1 \) regularity settings, and construct exact controls
toward these target states.

</details>


### [19] [Unconditional Uniqueness of 5th Order KP Equations](https://arxiv.org/abs/2508.19779)
*James Patterson*

Main category: math.AP

TL;DR: Unconditional uniqueness for 5th order KP equations using energy estimates and X^{s,b} methods with L^4 Strichartz estimates


<details>
  <summary>Details</summary>
Motivation: Study the 5th order Kadomstev-Petviashvili equations on the real line to establish unconditional uniqueness of solutions

Method: Adapt energy estimate argument from previous work, apply short-time X^{s,b} methods, use multilinear interpolation to access L^4 Strichartz estimates for improved derivative gain

Result: Achieved unconditional uniqueness with regularity arbitrarily close to L^2, which is almost sharp in the context of unconditional uniqueness results

Conclusion: The combined approach of energy estimates, X^{s,b} methods, and L^4 Strichartz estimates provides an effective framework for proving unconditional uniqueness for 5th order KP equations

Abstract: In this paper we study the $5$th Order Kadomstev-Petviashvili (KP) equations
posed on the real line. In particular we adapt the energy estimate argument
from Guo-Molinet (arXiv:2404.12364v1 [math.AP]) to conclude unconditional
uniqueness of the solution to data map for $5$th order KP type equations.
Applying short-time $X^{s,b}$ methods to improve classical energy estimates
provides more than sufficient decay when considering estimates on the interior
of the time interval $[0,T]$. The issue is how we deal with the boundary. By
abusing symmetry we can apply multilinear interpolation to gain access to $L^4$
Strichartz estimates, which provide improved derivative gain. When taken
together, the regularity of our resultant function space can be arbitrarily
close to $L^2$, which in the context of unconditional uniqueness results is
almost sharp.

</details>


### [20] [Nonlocal singular problem and associated Sobolev type inequality with extremal in the Heisenberg group](https://arxiv.org/abs/2508.19811)
*Prashanta Garain*

Main category: math.AP

TL;DR: Analysis of fractional p-Laplace equations with variable exponent singular nonlinearity in Heisenberg groups, establishing existence, regularity, uniqueness, and connections to Sobolev-type inequalities.


<details>
  <summary>Details</summary>
Motivation: To study fractional p-Laplace equations with singular nonlinearities in non-Euclidean geometric settings (Heisenberg groups), extending classical results to more complex frameworks and providing new insights even for the standard p=2 case.

Method: Mathematical analysis approach establishing weak solution existence and regularity, proving uniqueness for constant singular exponents, characterizing extremals of Sobolev-type inequalities, and demonstrating connections between singular problem solutions and these extremals.

Result: Successfully established existence and regularity of weak solutions, proved uniqueness for constant singular exponents, characterized extremals of related Sobolev-type inequalities, and demonstrated connections between solutions and extremals - providing novel insights even in classical cases.

Conclusion: The paper provides comprehensive analysis of fractional p-Laplace equations with singular nonlinearities in Heisenberg groups, offering new mathematical tools and results that extend classical theory and have implications for both non-Euclidean and Euclidean settings.

Abstract: We study a fractional $p$-Laplace equation involving a variable exponent
singular nonlinearity in the framework of the Heisenberg group. We first
establish the existence and regularity of weak solutions. In the case of a
constant singular exponent, we further prove the uniqueness of solutions and
characterize the extremals of a related Sobolev-type inequality. Additionally,
we demonstrate a connection between the solutions of the singular problem and
these extremals. To the best of our knowledge, these findings provide new
insights even in the classical case $p=2$.

</details>


### [21] [Existence for accreting viscoelastic solids at large strains](https://arxiv.org/abs/2508.19923)
*Andrea Chiesa,Ulisse Stefanelli*

Main category: math.AP

TL;DR: Analysis of viscoelastic solid growth at large strains with accreted material accumulating unstressed at boundaries, using backstrain modeling and fictitious compliant material regularization.


<details>
  <summary>Details</summary>
Motivation: To address the complex problem of accretive growth in viscoelastic solids at large strains, where material accumulates at boundaries in unstressed states and growth is deformation-driven, requiring proper modeling of incompatible strain buildup.

Method: Revisits an existing model, incorporates additional backstrain to model progressive incompatible strain buildup, and regularizes the model using a fictitious compliant material surrounding the accreting body.

Result: Demonstrates the existence of solutions to the coupled accretion and viscoelastic equilibrium problem, providing mathematical validation for the proposed modeling approach.

Conclusion: The proposed framework successfully addresses accretive growth in viscoelastic solids by incorporating backstrain modeling and regularization techniques, establishing existence proofs for the coupled mechanical problem.

Abstract: By revisiting a model proposed in [45], we address the accretive growth of a
viscoelastic solid at large strains. The accreted material is assumed to
accumulate at the boundary of the body in an unstressed state. The growth
process is driven by the deformation state of the solid. The progressive
build-up of incompatible strains in the material is modeled by considering an
additional backstrain. The model is regularized by postulating the presence of
a fictitious compliant material surrounding the accreting body. We show the
existence of solutions to the coupled accretion and viscoelastic equilibrium
problem.

</details>


### [22] [Quantitative stability for the conformally invariant Chang-Gui inequality on the exponentiation of functions on the sphere](https://arxiv.org/abs/2508.19930)
*Monideep Ghosh,Debabrata Karmakar*

Main category: math.AP

TL;DR: Analysis of a Trudinger-Moser-Onofri inequality variant on the 2-sphere, establishing sharp constant α≥2/3, classifying extremals, and proving quantitative stability using conformal invariance.


<details>
  <summary>Details</summary>
Motivation: To extend and deepen the understanding of a recent inequality variant introduced by Chang and Gui on the 2-sphere, particularly focusing on the characterization of extremal functions and stability properties.

Method: Exploited the conformal invariance property of the associated functional to characterize all extremal solutions in terms of conformal maps of the 2-sphere, and established a sharp quantitative stability result in the gradient norm.

Result: Confirmed that the inequality holds for α≥2/3, with trivial extremals for α>2/3 and nontrivial extremals at the critical value α=2/3. Provided complete characterization of extremals using conformal maps and proved sharp quantitative stability.

Conclusion: The conformal invariance property provides a powerful tool for understanding extremal functions and stability properties of this inequality variant, leading to a complete classification and sharp quantitative results.

Abstract: In this work, we focus on a recent variant of the Trudinger-Moser-Onofri
inequality introduced by S. Y. Alice Chang and Changfeng Gui \cite{CG-2023}:
\begin{align*} \alpha\int_{\mathbb{S}^2}|\nabla_{\mathbb{S}^2}u|^2 {\rm
d}\omega+2 \int_{\mathbb{S}^2} u {\rm d}\omega
-\frac{1}{2}\ln\left[\left(\int_{\mathbb{s}^2}e^{2u}{\rm
d}\omega\right)^2-\sum_{i=1}^3\left(\int_{\mathbb{s}^2}\omega_i e^{2u}{\rm d}
\omega\right)^2\right] \geq 0 \end{align*} holds on $H^1(\mathbb{S}^2)$ if and
only if $\alpha \geq \frac{2}{3}$. In this regime, the infimum is attained only
by trivial functions when $\alpha > \frac{2}{3},$ whereas for the critical
value $\alpha = \frac{2}{3}$ nontrivial extremals exist, and Chang-Gui further
provided a complete classification of such solutions.
  Building upon their result, we found a nice conformal invariance of the
associated functional. Exploiting this invariance, we were able to characterize
the full family of extremals in terms of conformal maps of $\mathbb{S}^2$ and,
moreover, establish a sharp quantitative stability result in the gradient norm.

</details>


### [23] [Symmetry-breaking bifurcation of periodic solutions for a free-boundary tumor model](https://arxiv.org/abs/2508.19954)
*Wenhua He,Mingxin Wang,Ruixiang Xing*

Main category: math.AP

TL;DR: Analysis of free boundary multi-layer tumor model with periodic external nutrients, examining flat solution stability and periodic bifurcations.


<details>
  <summary>Details</summary>
Motivation: To understand tumor growth dynamics under periodic nutrient supply and investigate stability conditions and symmetry-breaking periodic solutions in free boundary tumor models.

Method: Mathematical analysis of a simplified tumor model with three parameters, studying flat solutions, stability classification, and periodic bifurcations from flat periodic solutions using periodicity and symmetry arguments.

Result: Complete classification of stability conditions: zero flat solution is globally stable when threshold concentration ≥ mean external nutrients; otherwise, unique positive periodic flat solution exists as global attractor. First demonstration of periodic bifurcations in free boundary tumor problems.

Conclusion: The model provides fundamental insights into tumor growth under periodic nutrient conditions, establishing stability criteria and revealing rich periodic solution structures through symmetry-breaking bifurcations.

Abstract: In this paper, we consider a free boundary multi-layer tumor model that
incorporates a $T-$periodic provision of external nutrients $\Phi(t)$. The
simplified model contains three parameters: the mean of periodic external
nutrients $\Phi(t)$, the threshold concentration $\widetilde{\sigma}$ for
proliferation and the cell to cell adhesiveness coefficient $\gamma$. We first
study the flat solution and give a complete classification about $\frac{1}{T}
\int_0^T \Phi(t) d t$ and $\widetilde{\sigma}$ according to global stability of
zero equilibrium solution or global stability of the positive periodic
solution. Precisely, (i) a zero flat solution is globally stable under the flat
perturbations if and only if $\widetilde{\sigma} \geqslant \frac{1}{T} \int_0^T
\Phi(t) d t$; (ii) If $\widetilde{\sigma}<\frac{1}{T} \int_0^T \Phi(t) d t$,
then there exists a unique positive flat solution $\left(\sigma_*(y, t), p_*(y,
t), { \rho_*(t)}\right)$ with period $T$ and it is a global attractor of all
positive flat solutions for all $\gamma>0$. We further investigate periodic
solutions bifurcating from the flat periodic solution $\left(\sigma_*(y, t),
p_*(y, t), { \rho_*(t)}\right)$. By periodicity and symmetry, we not only give
symmetry-breaking periodic solutions for all positive parameter $\gamma_j$, but
also show the existence of a plethora of periodic bifurcations. For the free
boundary tumor problem, this is the first result of the existence of periodic
bifurcations.

</details>


### [24] [On the relationship between the semiclassical and standard pseudodifferential algebras](https://arxiv.org/abs/2508.20081)
*András Vasy*

Main category: math.AP

TL;DR: Relationship between semiclassical and standard pseudodifferential algebras, implications for elliptic estimates with large spectral parameters, and connection to second microlocalization.


<details>
  <summary>Details</summary>
Motivation: To clarify the precise mathematical relationship between semiclassical and standard pseudodifferential operator algebras, which is fundamental in microlocal analysis and spectral theory.

Method: Theoretical analysis and mathematical exploration of the structural relationships between different pseudodifferential operator algebras, examining their properties and interconnections.

Result: Establishes the precise relationship between semiclassical and standard pseudodifferential algebras, provides implications for elliptic estimates with large spectral parameters (including pseudodifferential spectral families), and explains connections to second microlocalization.

Conclusion: The paper provides important clarifications on the connections between different pseudodifferential operator frameworks, offering insights that are valuable for spectral theory and microlocal analysis applications, particularly for problems involving large spectral parameters.

Abstract: In this short paper we discuss the precise relationship between the
semiclassical and standard pseudodifferential algebras and explore implications
such as for large spectral parameter elliptic estimates, even in the case of
pseudodifferential spectral familes. We also explain the connection between
second microlocalization and this relationship.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [25] [Boltz-ABFE: Free Energy Perturbation without Crystal Structures](https://arxiv.org/abs/2508.19385)
*Stephan Thaler,Zhiyi Wu,William G. Glass,Richard T. Bradshaw,Prudencio Tossou,Geoffrey P. F. Wood*

Main category: physics.comp-ph

TL;DR: Boltz-ABFE combines structure prediction (Boltz-2) with absolute free energy perturbation (FEP) to estimate binding affinities without experimental crystal structures, expanding FEP's applicability to early drug discovery.


<details>
  <summary>Details</summary>
Motivation: FEP requires accurate protein-ligand complex structures, limiting its use in early drug discovery where experimental structures are unavailable. Structure prediction models like Boltz-2 could overcome this limitation.

Method: Combined Boltz-2 structure prediction with absolute FEP protocol to create Boltz-ABFE pipeline. Investigated predicted structure quality and developed automated approaches to improve structures for molecular dynamics simulations.

Result: Demonstrated effectiveness on four protein targets from FEP+ benchmark set. Showed feasibility of absolute FEP simulations without experimental crystal structures.

Conclusion: Boltz-ABFE significantly expands FEP's applicability domain, enabling accurate structure-based affinity estimation in early-stage drug discovery without experimental structures.

Abstract: Free energy perturbation (FEP) is considered the gold-standard simulation
method for estimating small molecule binding affinity, a quantity of vital
importance to drug discovery. The accuracy of FEP critically depends on an
accurate model of the protein-ligand complex as an initial condition for the
underlying molecular dynamics simulation. This requirement has limited the
impact of FEP in earlier stages of the discovery process, where appropriate
experimental crystal structures are rarely available. The latest generation of
structure prediction models, such as Boltz-2, promise to overcome this
limitation by predicting protein-ligand complex structures. In this work, we
combine Boltz-2 with our own absolute FEP protocol to build Boltz-ABFE, a
robust pipeline for estimating the absolute binding free energies (ABFE) in the
absence of experimental crystal structures. We investigate the quality of the
structures predicted by Boltz-2, propose automated approaches to improve
structures for use in molecular dynamics simulations, and demonstrate the
effectiveness of the Boltz-ABFE pipeline for four protein targets from the FEP+
benchmark set. Demonstrating the feasibility of absolute FEP simulations
without experimental crystal structures, Boltz-ABFE significantly expands the
domain of applicability of FEP, paving the way towards accelerated early-stage
drug discovery via accurate, structure-based affinity estimation.

</details>


### [26] [Current-Driven Symmetry Breaking and Spin-Orbit Polarization in Chiral Wires](https://arxiv.org/abs/2508.19519)
*Uiseok Jeong,Daniel Hill,Binghai Yan,Angel Rubio,Carsten A. Ullrich,Noejung Park*

Main category: physics.comp-ph

TL;DR: Chiral molecular systems exhibit current-induced spin polarization above critical current thresholds through dynamic symmetry breaking, not inherent chirality effects.


<details>
  <summary>Details</summary>
Motivation: To resolve whether geometric chirality inherently induces spin polarization or simply modulates spin transport in chiral molecular systems.

Method: Employed ab initio real-time time-dependent density functional theory (rt-TDDFT) to directly simulate charge current, spin, and orbital interplay beyond perturbative treatments.

Result: Above critical current threshold, time-reversal symmetry constraints are dynamically lifted, producing pronounced spin and orbital polarizations despite symmetric Hamiltonian. Spin/orbital angular momenta dynamics correlate with loss of translational momentum.

Conclusion: Current-driven symmetry lowering in chiral systems causes redistribution of angular degrees of freedom, with implications for chirality-induced spin selectivity and spintronic device design.

Abstract: The spin dynamics of electrons in chiral molecular systems remains a topic of
intense interest, particularly regarding whether geometric chirality inherently
induces spin polarization or simply modulates spin transport. In this work, we
employ ab initio real-time time-dependent density functional theory (rt-TDDFT)
to directly simulate the interplay between charge current, spin, and orbital.
This real-time tracking goes beyond perturbative treatments, and we analyze how
nonequilibrium currents effectively lift the symmetry constraints of
screw-rotation and time-reversal symmetry. We find that above a critical
current threshold, time-reversal symmetry constraints are dynamically lifted --
leading to pronounced spin and orbital polarizations, even when the underlying
Hamiltonian remains symmetric. Notably, the emergence of spin and orbital
angular momenta dynamics correlates with a loss of translational (linear)
momentum, suggesting a redistribution of angular degrees of freedom as an
intrinsic consequence of a current-driven symmetry lowering in a chiral system,
with implications for chirality-induced spin selectivity and spintronic device
design.

</details>


### [27] [Universal Generalization Theory for Physical Intuitions from Small Artificial Neural Networks](https://arxiv.org/abs/2508.19537)
*Jingruo Peng,Shuze Zhu*

Main category: physics.comp-ph

TL;DR: Small neural networks can develop strong physical intuitions by learning from few samples using a variational principle-based training algorithm, mastering brachistochrone and quantum harmonic oscillator problems.


<details>
  <summary>Details</summary>
Motivation: To understand how human brains develop physical intuitions from limited observations and simulate this process in artificial neural networks.

Method: Developed a training algorithm adapted from the variational principle in physics, applied to small artificial neural networks learning from few highly similar samples.

Result: Neural networks successfully mastered brachistochrone and quantum harmonic oscillator problems, with a derived unified generalization theory showing a threshold network size requirement.

Conclusion: The variational principle is the governing mechanism for artificial physical intuitions, offering insights into how both humans and AI can formulate strong physical understanding.

Abstract: Physical intuitions are native functions from human brains, yet the
understanding of how physical intuitions are formulated has remained elusive.
In this Letter, we propose a mechanism that simulates how human brain can
quickly develop intuitional understandings from limited observations.
Conceiving a training algorithm adapted from the well-known variational
principle in physics, small artificial neural networks can possess strong
physical intuitions that master the problems of brachistochrone and quantum
harmonic oscillators, by learning from a few highly similar samples. Our
simulations suggest that the variational principle is the governing mechanism
for artificial physical intuitions. A unified generalization theory is derived,
which hinges upon a variational operation on the Euler-Lagrange equation. Our
theory also rationalizes that there is a threshold of artificial neural network
size below which satisfactory physical intuitions are not possible. Our work
offers insights into how strong physical intuition can be formulated as humans
or as artificial intelligences.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [28] [On the numerical simulations of electric field-enhanced solid-state electro-aerodynamic thrusters enabled with an insulated electrode](https://arxiv.org/abs/2508.19265)
*Hisaichi Shibata,Takahiro Nozaki*

Main category: physics.plasm-ph

TL;DR: Electric field-enhanced solid-state electro-aerodynamic propulsion system improves thrust density while maintaining efficiency by using auxiliary electrodes that induce ionization without discharging.


<details>
  <summary>Details</summary>
Motivation: Previous decoupled thrusters were found infeasible due to electric field line constraints when ignoring diffusion effects, requiring improved driving methods for additional electrodes.

Method: Proposed using auxiliary electrodes that don't discharge but induce ionization and attachment, creating an electric field-enhanced system. Validated through numerical simulations.

Result: The new concept demonstrated improved thrust density while maintaining the thrust-to-power ratio compared to previous approaches.

Conclusion: Electric field-enhanced solid-state electro-aerodynamic propulsion represents a feasible improvement over decoupled thrusters, offering better performance without sacrificing efficiency.

Abstract: A solid-state electro-aerodynamic propulsion system applies a high electric
potential difference between two electrodes to ionize air within the resulting
electric field, accelerating the ions to generate thrust. Previously, the
so-called ``decoupled thrusters'' have been proposed, in which ion generation
and acceleration are spatially decoupled. Here, we argue that such decoupling
is infeasible from the perspective of electric field lines if the diffusion
effects are ignored, and consequently the driving method (e.g., dielectric
barrier discharge) for the additional electrodes can be improved. Specifically,
the use of auxiliary electrodes that do not discharge but induce ionization and
attachment can be appropriate. This concept was named the electric
field-enhanced solid-state electro-aerodynamic propulsion system. Furthermore,
through numerical simulations, this concept showed improvement in thrust
density while maintaining the thrust-to-power ratio.

</details>


### [29] [Transition of blue-core helicon discharge](https://arxiv.org/abs/2508.19662)
*L. Chang,S. J. Zhang,J. T. Wu,Y. W. Zhang,C. Wang,Y. Peng,S. S. Gao,C. J. Sun,Q. Wang,C. F. Sang,S. C. Thakur,S. Isayama,S. J. You*

Main category: physics.plasm-ph

TL;DR: Study examines blue-core helicon discharge transition characteristics using experimental diagnostics and electromagnetic simulations, revealing opposite density/temperature trends, radial oscillations, and wave field changes during transition.


<details>
  <summary>Details</summary>
Motivation: To investigate the transitional characteristics of blue-core helicon discharge, which has not been particularly focused on before, using advanced diagnostics and simulations to understand the underlying physics.

Method: Used Multiple Plasma Simulation Linear Device (MPS-LD) with Langmuir probe, optical emission spectrometer, high-speed camera, and electromagnetic solver based on Maxwell's equations and cold-plasma dielectric tensor to measure parameters and compute wave fields.

Result: Found opposite jump directions for electron density (increases) and temperature (decreases), localized radial density profile near axis, W-shaped temperature profile, pressure-dependent behavior, radial oscillations with azimuthal instabilities, and significant wave field changes during transition.

Conclusion: This work provides comprehensive characteristics of transitional blue-core discharge that are important for both physics understanding and practical applications, revealing detailed transitional behaviors not previously available.

Abstract: This study explores the transitional characteristics of blue-core helicon
discharge, which to our knowledge was not particularly focused on before.
Parameters are measured on recently built advanced linear plasma device, i.e.
Multiple Plasma Simulation Linear Device (MPS-LD) by various diagnostics
including Langmuir probe, optical emission spectrometer, and standard
high-speed camera. It is found that the jump direction of electron density
(from low level to high level) is opposite to that of electron temperature
(from high level to low level). Electron density increases significantly and
the radial profile becomes localized near the axis when the blue-core
transition occurs. With increased field strength, electron density increases
whereas electron temperature drops. The radial profile of electron temperature
looks like a ``W" shape, i.e. minimizing around the edge of blue-core column.
Electron density increases with background pressure, while electron temperature
peaks around certain pressure value. High-speed videos show that the plasma
column oscillates radially and experiences azimuthal instabilities with high
rate once entered blue-core mode. An electromagnetic solver (EMS) based on
Maxwell's equations and a cold-plasma dielectric tensor is also employed to
compute the wave field and power absorption during blue-core transition, to
provide more details that are valuable for understanding the transitional
physics but not yet available in experiment. The results show that wave field
in both radial and axial directions changes significantly during the
transition, its structure differs from antenna to downstream, and the power
dependence of wave magnetic field is overall opposite to that of wave electric
field. This work presents comprehensive characteristics of the transitional
blue-core discharge and is important to both physics understanding and
practical applications.

</details>


### [30] [Molecular dynamics of nondegenerate hydrogen plasma using improved Kelbg pseudopotential with electron finite-size correction](https://arxiv.org/abs/2508.19820)
*G. S. Demyanov,P. R. Levashov*

Main category: physics.plasm-ph

TL;DR: Improved semiclassical molecular dynamics for hydrogen plasma using modified Kelbg pseudopotential that accounts for finite electron size, resolving nonphysical cluster formation at low temperatures but still underestimating energy compared to path integral Monte Carlo.


<details>
  <summary>Details</summary>
Motivation: To address the nonphysical cluster formation in hydrogen plasma simulations at temperatures below 50 kK reported by Filinov (2004), by developing an improved semiclassical approach that accounts for finite electron size.

Method: Semiclassical molecular dynamics simulation using an improved Kelbg pseudopotential that incorporates finite electron size effects. The method also utilizes angular-averaged Ewald potential for long-range interactions and computes thermodynamic limits.

Result: The modified approach successfully resolves the nonphysical cluster formation problem at temperatures below 50 kK. However, energy is still underestimated at low temperatures compared to path integral Monte Carlo results. The study provides analysis of radial distribution functions, composition, ionization degree, energy, and pressure dependencies on plasma coupling parameter.

Conclusion: Accounting for finite electron size in the Kelbg pseudopotential eliminates nonphysical cluster formation in hydrogen plasma simulations, representing a significant improvement over previous methods, though energy underestimation at low temperatures remains an issue requiring further investigation.

Abstract: This paper is devoted to semiclassical molecular dynamics simulation of
nondegenerate hydrogen plasma using an improved Kelbg pseudopotential. The main
novelty of our method is accounting for the finite size of electrons. This
modification resolves the nonphysical cluster formation at temperatures below
50 kK, which was first reported by A.V. Filinov [Phys. Rev. E 70, 046411
(2004)]. However, the energy still appears to be underestimated at low
temperatures, as indicated by comparisons with the recent path integral Monte
Carlo calculations [Phys. Plasmas 31, 110501 (2024)]. Using the presented
method, we analyze the dependence of radial distribution functions,
composition, ionization degree, energy, and pressure on the plasma coupling
parameter, while maintaining a fixed degeneracy parameter. Additionally, we
demonstrate the impact of incorporating long-range interactions on the energy
$N$-dependence by utilizing the angular-averaged Ewald potential. Finally, we
compute the thermodynamic limits for energy and pressure.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [31] [Time-periodic solutions of the conformally invariant wave equation on the Einstein cylinder](https://arxiv.org/abs/2508.19717)
*Ficek Filip,Maciej Maliborski*

Main category: gr-qc

TL;DR: Time-periodic solutions of cubic conformal wave equation on Einstein cylinder show complex bifurcation patterns


<details>
  <summary>Details</summary>
Motivation: Explore time-periodic solutions of Einstein equations with negative cosmological constant as initial step

Method: Combination of numerical and perturbative techniques

Result: Discovery of intricate bifurcation patterns in time-periodic solutions

Conclusion: Complex bifurcation structures found in periodic solutions of conformal wave equation on Einstein cylinder

Abstract: As a first step in exploring time-periodic solutions of the Einstein
equations with a negative cosmological constant, we study the cubic conformal
wave equation on the Einstein cylinder. Using a combination of numerical and
perturbative techniques, we discover that time-periodic solutions form
intricate bifurcation patterns.

</details>


<div id='cond-mat.mes-hall'></div>

# cond-mat.mes-hall [[Back]](#toc)

### [32] [Tunable quantum anomalous Hall effect in fullerene monolayers](https://arxiv.org/abs/2508.19849)
*Leonard Werner Pingen,Jiaqi Wu,Bo Peng*

Main category: cond-mat.mes-hall

TL;DR: Proposes molecular engineering approach using C26 fullerenes on honeycomb lattice to achieve quantum anomalous Hall effect with tunable Chern numbers under accessible conditions.


<details>
  <summary>Details</summary>
Motivation: Despite theoretical predictions, experimental realization of quantum anomalous Hall effect remains elusive. Need alternative materials design approaches beyond conventional solid-state systems.

Method: Engineering customized molecular building blocks - 2D honeycomb lattice of C26 fullerenes that exhibits ferromagnetic ground state breaking time-reversal symmetry. System is tunable via magnetic degrees of freedom and applied strain.

Result: Achieves rich phase diagram with Chern numbers C = +/-2, +/-1, 0. Demonstrates ferromagnetic ground state and tunable quantum anomalous Hall physics.

Conclusion: Provides versatile platform for tunable QAH physics under accessible conditions and offers experimentally feasible approach for chemical synthesis of molecular networks with QAHE.

Abstract: Nearly four decades after its theoretical prediction, the search for material
realizations of quantum anomalous Hall effect (QAHE) remains a highly active
field of research. Many materials have been predicted to exhibit quantum
anomalous Hall (QAH) physics under feasible conditions but the experimental
verification remains widely elusive. In this work, we propose an alternative
approach towards QAH materials design by engineering customized molecular
building blocks. We demonstrate this ansatz for a two-dimensional (2D)
honeycomb lattice of C26 fullerenes, which exhibits a ferromagnetic ground
state and thus breaks time-reversal symmetry. The molecular system is found to
be highly tunable with respect to its magnetic degrees of freedom and applied
strain, giving rise to a rich phase diagram with Chern numbers C= +/-2, +/-1,
0. Our proposal offers a versatile platform to realize tunable QAH physics
under accessible conditions and provides an experimentally feasible approach
for chemical synthesis of molecular networks with QAHE.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [33] [Numerical simulations of oscillations for axisymmetric solar backgrounds with differential rotation and gravity](https://arxiv.org/abs/2508.19386)
*Ha Pham,Florian Faucher,Damien Fournier,Hélène Barucq,Laurent Gizon*

Main category: astro-ph.SR

TL;DR: Implementation and analysis of Hybridizable Discontinuous Galerkin method for solving stellar oscillation equations with realistic solar gravity and rotation, focusing on stabilization requirements for accurate numerical results.


<details>
  <summary>Details</summary>
Motivation: To improve local helioseismology techniques by developing robust numerical methods for modeling solar oscillations with realistic gravitational and rotational effects, which are essential for accurate imaging and inversion of the Sun's interior dynamics.

Method: Used Hybridizable Discontinuous Galerkin (HDG) method to solve stellar oscillation equations derived from an equivalent variant of Galbrun's equation, working with Lagrangian displacement and pressure perturbation as unknowns. The system was solved under differential rotation and axisymmetric assumption using azimuthal decomposition.

Result: The method successfully handled the mathematical challenges posed by solar buoyancy frequency profiles, with numerical power spectra accurately reproducing observed effects of solar rotation on acoustic waves. Stabilization was shown to be crucial for solution accuracy, depending on frequencies relative to buoyancy frequency and Dirac source position.

Conclusion: The HDG method is robust for solving stellar oscillation equations with realistic solar backgrounds, but requires careful stabilization choices to handle the mixed elliptic/hyperbolic nature of the wave operator influenced by gravity effects through the buoyancy frequency profile.

Abstract: Local helioseismology comprises of imaging and inversion techniques employed
to reconstruct the dynamic and interior of the Sun from correlations of
oscillations observed on the surface, all of which require modeling solar
oscillations and computing Green's kernels. In this context, we implement and
investigate the robustness of the Hybridizable Discontinuous Galerkin (HDG)
method in solving the equation modeling stellar oscillations for realistic
solar backgrounds containing gravity and differential rotation. While a common
choice for modeling stellar oscillations is the Galbrun's equation, our working
equations are derived from an equivalent variant, involving less regularity in
its coefficients, working with Lagrangian displacement and pressure
perturbation as unknowns. Under differential rotation and axisymmetric
assumption, the system is solved in azimuthal decomposition with the HDG
method. Compared to no-gravity approximations, the mathematical nature of the
wave operator is now linked to the profile of the solar buoyancy frequency N
which encodes gravity, and leads to distinction into regions of elliptic or
hyperbolic behavior of the wave operator at zero attenuation. While small
attenuation is systematically included to guarantee theoretical well-posedness,
the above phenomenon affects the numerical solutions in terms of amplitude and
oscillation pattern, and requires a judicious choice of stabilization. We
investigate the stabilization of the HDG discretization scheme, and demonstrate
its importance to ensure the accuracy of numerical results, which is shown to
depend on frequencies relative to N, and on the position of the Dirac source.
As validations, the numerical power spectra reproduce accurately the observed
effects of the solar rotation on acoustic waves.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [34] [LEMONS: An open-source platform to generate non-circuLar, anthropometry-based pEdestrian shapes and simulate their Mechanical interactiONS in two dimensions](https://arxiv.org/abs/2508.19865)
*Oscar Dufour,Alexandre Nicolas,Maxime Stapelle*

Main category: cond-mat.soft

TL;DR: LEMONS is an open-source software for simulating dense pedestrian crowds with both online interface and C++ library, supporting 2D/3D modeling and Python integration.


<details>
  <summary>Details</summary>
Motivation: To provide researchers and practitioners with an accessible, flexible tool for modeling and simulating dense pedestrian crowds with realistic mechanical interactions and customizable decision-making models.

Method: Developed an open-source platform featuring: 1) intuitive online interface for crowd generation using anthropometric data, 2) C++ library for mechanical contact computations with agents/obstacles, 3) Python scripting integration for custom decision-making models and velocity specifications.

Result: A comprehensive simulation tool that produces videos of pedestrian crowd simulations, enabling users to model dense crowds with mechanical accuracy while maintaining flexibility for custom implementations.

Conclusion: LEMONS provides a versatile, open-source solution for pedestrian crowd simulation that combines user-friendly interfaces with powerful computational capabilities and extensive customization options through Python integration.

Abstract: We offer a collection of videos showing simulations of pedestrian crowds made
with the LEMONS software. LEMONS is an open-source computational tool designed
for modelling dense crowds. The platform features an intuitive online
interface, enabling users to generate 2D and 3D pedestrian crowds based on
anthropometric data. Additionally, it features a C++ library that computes
mechanical contacts with other agents and obstacles, and evolves the crowd's
configuration. Both the online platform and the library can readily be called
from Python scripts, providing users with complete flexibility to implement
their own decision-making models, such as specifying the desired velocities of
individuals within the crowd.__Documentation is also provided.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [35] [Kolmogorov-Arnold Representation for Symplectic Learning: Advancing Hamiltonian Neural Networks](https://arxiv.org/abs/2508.19410)
*Zongyu Wu,Ruichen Xu,Luoyao Chen,Georgios Kementzidis,Siyao Wang,Yuefan Deng*

Main category: cs.LG

TL;DR: KAR-HNN replaces MLPs with univariate transformations in Hamiltonian Neural Networks to reduce energy drift and improve stability while maintaining physical consistency.


<details>
  <summary>Details</summary>
Motivation: Existing HNN implementations using MLPs cause hypersensitivity to hyperparameters and struggle with complex energy landscapes, leading to energy drift and poor long-term stability.

Method: Proposes Kolmogorov-Arnold Representation-based Hamiltonian Neural Network that uses localized univariate transformations instead of MLPs to better capture high-frequency and multi-scale dynamics while preserving symplectic structure.

Result: Tested on four benchmark problems (spring-mass, simple pendulum, two- and three-body problems), showing reduced energy drift and improved long-term predictive stability compared to MLP-based HNNs.

Conclusion: KAR-HNN is effective for accurate and stable modeling of realistic physical processes, especially in high dimensions with few known parameters, due to its improved energy conservation and stability properties.

Abstract: We propose a Kolmogorov-Arnold Representation-based Hamiltonian Neural
Network (KAR-HNN) that replaces the Multilayer Perceptrons (MLPs) with
univariate transformations. While Hamiltonian Neural Networks (HNNs) ensure
energy conservation by learning Hamiltonian functions directly from data,
existing implementations, often relying on MLPs, cause hypersensitivity to the
hyperparameters while exploring complex energy landscapes. Our approach
exploits the localized function approximations to better capture high-frequency
and multi-scale dynamics, reducing energy drift and improving long-term
predictive stability. The networks preserve the symplectic form of Hamiltonian
systems, and thus maintain interpretability and physical consistency. After
assessing KAR-HNN on four benchmark problems including spring-mass, simple
pendulum, two- and three-body problem, we foresee its effectiveness for
accurate and stable modeling of realistic physical processes often at high
dimensions and with few known parameters.

</details>


### [36] [$\mathcal{C}^1$-approximation with rational functions and rational neural networks](https://arxiv.org/abs/2508.19672)
*Erion Morina,Martin Holler*

Main category: cs.LG

TL;DR: The paper demonstrates that regular functions can be approximated in the C¹-norm using rational functions and rational neural networks, with specific rates for network width/depth and rational function degree.


<details>
  <summary>Details</summary>
Motivation: To provide theoretical foundations for rational function and rational neural network approximations, particularly relevant for symbolic regression in physical law learning applications.

Method: The authors develop approximation theory for rational functions and rational neural networks in the C¹-norm, analyzing approximation rates with respect to network architecture parameters (width, depth) and rational function degree.

Result: The paper establishes that suitably regular functions can be effectively approximated in the C¹-norm by both rational functions and rational neural networks, with specific approximation rates. The results extend to architectures like EQL÷ and ParFam that are important for symbolic regression.

Conclusion: The theoretical results provide rigorous mathematical support for using rational approximations and rational neural networks in applications requiring C¹-approximation, particularly in symbolic regression for learning physical laws through architectures like EQL÷ and ParFam.

Abstract: We show that suitably regular functions can be approximated in the
$\mathcal{C}^1$-norm both with rational functions and rational neural networks,
including approximation rates with respect to width and depth of the network,
and degree of the rational functions. As consequence of our results, we further
obtain $\mathcal{C}^1$-approximation results for rational neural networks with
the $\text{EQL}^\div$ and ParFam architecture, both of which are important in
particular in the context of symbolic regression for physical law learning.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [37] [Exploiting nonlinear incoherent image formation through linear volume metaoptics for inference](https://arxiv.org/abs/2508.19436)
*Nan Zhang,Arvin Keshvari,Ata Shakeri,Zin Lin*

Main category: physics.optics

TL;DR: Depth maps of 3D opaque scenes are encoded in imaging optics response functions, creating nonlinear depth-dependent images through engineered spatio-spectral dispersions in volume metaoptics.


<details>
  <summary>Details</summary>
Motivation: To demonstrate that imaging optics can directly encode 3D depth information and leverage volume metaoptics for nonlinear sensing and processing of 3D scenes.

Method: Engineered strong spatio-spectral dispersions in volume metaoptics to create complex images that respond to depth maps, with the optics response function encoding 2D depth information.

Result: The optics creates images that depend nonlinearly on depth maps, and volume metaoptics can generate complex depth-dependent imagery through controlled dispersions.

Conclusion: Linear volume metaoptics can nonlinearly sense and process 3D opaque scenes through engineered spatio-spectral dispersions that encode depth information in the optical response.

Abstract: We showed that a 2D depth map representing an incoherent 3D opaque scene is
directly encoded in the response function of an imaging optics. As a result,
the optics creates an image that depends nonlinearly on the depth map.
Furthermore, strong spatio-spectral dispersions in volume metaoptics can be
engineered to create a complex image in response to a depth map. We hypothesize
that this complexity will allow the linear volume metaoptics to nonlinearly
sense and process 3D opaque scenes.

</details>


### [38] [Fourier Feature Networks for High-Fidelity Prediction of Perturbed Optical Fields](https://arxiv.org/abs/2508.19751)
*Joshua R. Jandrell,Mitchell A. Cox*

Main category: physics.optics

TL;DR: Using Fourier features as additional network inputs enables accurate modeling of highly oscillatory complex-valued functions in optical systems, achieving order-of-magnitude error reduction with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Standard MLPs struggle with learning highly oscillatory complex-valued functions due to spectral bias, making them ineffective for modeling optical field perturbations.

Method: Incorporating Fourier features (predefined sinusoids dependent on perturbation) as additional network inputs to reframe the problem as finding linear combinations of basis functions.

Result: Achieved mean complex correlation of 0.995 with ground truth, reducing prediction error by an order of magnitude while using 85% fewer parameters compared to standard MLP.

Conclusion: This Fourier Feature Network approach provides a general and robust method for accurately modeling oscillatory physical systems, particularly effective for optical transmission matrix prediction.

Abstract: Modelling the effects of perturbations on optical fields often requires
learning highly oscillatory complex-valued functions. Standard multi-layer
perceptrons (MLPs) struggle with this task due to an inherent spectral bias,
preventing them from fitting high-frequency sinusoids. To overcome this, we
incorporate Fourier features - a set of predefined sinusoids dependent on the
perturbation - as an additional network input. This reframes the learning
problem from approximating a complex function to finding a linear combination
of basis functions. We demonstrate this method by training a Fourier Feature
Network to predict the transmission matrix of a multimode fibre under
mechanical compression. Compared to a standard MLP, our network reduces
prediction error in the output field's amplitude and phase by an order of
magnitude, achieving a mean complex correlation of 0.995 with the ground truth,
despite using 85% fewer parameters. This approach offers a general and robust
method for accurately modelling a wide class of oscillatory physical systems.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [39] [Quantum Resource Management in the NISQ Era: Challenges, Vision, and a Runtime Framework](https://arxiv.org/abs/2508.19276)
*Marcos Guillermo Lammers,Federico Hernán Holik,Alejandro Fernández*

Main category: quant-ph

TL;DR: Analysis of quantum resource management challenges in NISQ era, proposing runtime-aware quantum software development with Qonscious framework prototype.


<details>
  <summary>Details</summary>
Motivation: Current NISQ quantum devices have severe limitations (limited qubits, high error rates, short coherence times) requiring efficient resource management for practical quantum algorithm deployment.

Method: Analyzed role of quantum resources in NISQ devices, proposed runtime-aware development approach, and created Qonscious prototype framework for conditional execution based on dynamic resource evaluation.

Result: Developed a proof-of-concept framework that enables resource-aware quantum programming, addressing key challenges like limited introspection and temporal constraints in current platforms.

Conclusion: This work strengthens Quantum Resource Estimation field and moves toward scalable, reliable, resource-aware quantum software development for practical NISQ applications.

Abstract: Quantum computers represent a radical technological advancement in the way
information is processed by using the principles of quantum mechanics to solve
very complex problems that exceed the capabilities of classical systems.
However, in the current NISQ era (Noisy Intermediate-Scale Quantum devices),
the available hardware presents several limitations, such as a limited number
of qubits, high error rates, and reduced coherence times. Efficient management
of quantum resources, both physical (qubits, error rates, connectivity) and
logical (quantum gates, algorithms, error correction), becomes particularly
relevant in the design and deployment of quantum algorithms. In this work, we
analyze the role of resources in the various uses of NISQ devices today,
identifying their relevance and implications for software engineering focused
on the use of quantum computers. We propose a vision for runtime-aware quantum
software development, identifying key challenges to its realization, such as
limited introspection capabilities and temporal constraints in current
platforms. As a proof of concept, we introduce Qonscious, a prototype framework
that enables conditional execution of quantum programs based on dynamic
resource evaluation. With this contribution, we aim to strengthen the field of
Quantum Resource Estimation (QRE) and move towards the development of scalable,
reliable, and resource-aware quantum software.

</details>


### [40] [Fourier transform-based linear combination of Hamiltonian simulation](https://arxiv.org/abs/2508.19596)
*Xi Huang,Dong An*

Main category: quant-ph

TL;DR: New Fourier-based LCHS formalism simplifies kernel construction, achieves 1.81x reduction in quantum differential equation algorithms and 8.27x reduction in circuit depth at epsilon ≤ 10^-8 error, and extends to unstable dynamics simulation.


<details>
  <summary>Details</summary>
Motivation: Existing LCHS formalism requires finding kernel functions subject to complicated technical conditions on a half complex plane, making implementation difficult and limiting practical applications.

Method: Establish an alternative LCHS formalism based on Fourier transform that completely removes technical requirements beyond the real axis, providing simple and flexible kernel construction. Construct a different family of LCHS kernel functions.

Result: Achieved 1.81 times reduction in quantum differential equation algorithms based on LCHS, and 8.27 times reduction in quantum circuit depth at truncation error of ε ≤ 10^-8. Successfully extended LCHS scope to simulate linear unstable dynamics for short/intermediate time periods.

Conclusion: The Fourier-based LCHS formalism provides a simpler, more flexible approach with significant performance improvements in quantum differential equation algorithms and extends applicability to unstable dynamics simulation.

Abstract: Linear combination of Hamiltonian simulation (LCHS) connects the general
linear non-unitary dynamics with unitary operators and serves as the
mathematical backbone of designing near-optimal quantum linear differential
equation algorithms. However, the existing LCHS formalism needs to find a
kernel function subject to complicated technical conditions on a half complex
plane. In this work, we establish an alternative formalism of LCHS based on the
Fourier transform. Our new formalism completely removes the technical
requirements beyond the real axis, providing a simple and flexible way of
constructing LCHS kernel functions. Specifically, we construct a different
family of the LCHS kernel function, providing a $1.81$ times reduction in the
quantum differential equation algorithms based on LCHS, and an $8.27$ times
reduction in its quantum circuit depth at a truncation error of $\epsilon \le
10^{-8}$. Additionally, we extend the scope of the LCHS formula to the scenario
of simulating linear unstable dynamics for a short or intermediate time period.

</details>


### [41] [Direct probing of the simulation complexity of open quantum many-body dynamics](https://arxiv.org/abs/2508.19959)
*Lucia Vilchez-Estevez,Alexander Yosifov,Jinzhao Sun*

Main category: quant-ph

TL;DR: This paper analyzes the computational complexity of simulating open quantum systems, showing that dissipation affects correlation length and mixing time differently, and reveals a separation between quantum and classical resource scaling.


<details>
  <summary>Details</summary>
Motivation: To understand whether simulating open quantum systems can be comparable or even less complex than simulating closed systems, and to investigate how dissipation affects computational complexity in both quantum and classical simulation methods.

Method: The study investigates computational complexity using both quantum and classical methods, focusing on correlation length and mixing time parameters. Classical complexity is characterized by bond dimension and operator entanglement entropy, with numerical tensor network simulations.

Result: Dissipation affects correlation length and mixing time in distinct ways at different timescales. Classical tensor network simulations show that classical complexity does not decrease with stronger dissipation, revealing a separation between quantum and classical resource scaling.

Conclusion: There exists a fundamental separation between how quantum and classical simulation methods scale with dissipation strength, suggesting that quantum simulations may have advantages for certain open quantum system problems despite the apparent complexity introduced by environmental interactions.

Abstract: Simulating open quantum systems is key to understanding non-equilibrium
processes, as persistent influence from the environment induces dissipation and
can give rise to steady-state phase transitions. A common strategy is to embed
the system-environment into a larger unitary framework, but this obscures the
intrinsic complexity of the reduced system dynamics. Here, we investigate the
computational complexity of simulating open quantum systems, focusing on two
physically relevant parameters, correlation length and mixing time, and explore
whether it can be comparable (or even lower) to that of simulating their closed
counterparts. In particular, we study the role of dissipation in simulating
open-system dynamics using both quantum and classical methods, where the
classical complexity is characterised by the bond dimension and operator
entanglement entropy. Our results show that dissipation affects correlation
length and mixing time in distinct ways at intermediate and long timescales.
Moreover, we observe numerically that in classical tensor network simulations,
classical complexity does not decrease with stronger dissipation, revealing a
separation between quantum and classical resource scaling.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [42] [A Field-Theoretical Paradigm via Hierarchical Coarse-Graining: I. Generalized Mode Theory](https://arxiv.org/abs/2508.20025)
*Jaehyeok Jin,Yining Han,Gregory A. Voth*

Main category: physics.chem-ph

TL;DR: Systematic bottom-up approach to derive field-theoretical models from atomistic molecular liquids using hierarchical coarse-graining and generalized mode theory.


<details>
  <summary>Details</summary>
Motivation: Molecular simulations are computationally prohibitive beyond molecular scale, and existing field-theoretical approaches are typically top-down, lacking bottom-up derivation from atomistic systems.

Method: Hierarchical approach using molecular coarse-grained models as intermediate step, Hubbard-Stratonovich transformation for canonical/grand canonical ensembles, generalized mode theory with additional fields, and efficient perturbative approach for Fourier modes.

Result: Developed a systematic strategy to bridge particle-based and field-theoretical modeling, preserving microscopic structural correlations while reducing computational complexity.

Conclusion: This work provides a critical step toward integrating complex molecular interactions into field-theoretic descriptions, extending applicability beyond conventional limitations.

Abstract: Multiscale computer simulations facilitate the efficient exploration of large
spatiotemporal scales in chemical and physical systems. However, molecular
simulations predominantly rely on particle-based representations, which become
computationally prohibitive when applied beyond the molecular scale.
Field-theoretical simulations have emerged as an alternative to overcome these
limitations. Despite their applicability in mesoscopic simulations, these
approaches are typically derived from top-down principles, leaving a gap in the
bottom-up derivation of field-theoretical models for targeted molecular
systems. This work presents a systematic strategy for constructing statistical
field models for molecular liquids from the atomistic level, marking a critical
step toward bridging particle-based and field-theoretical modeling techniques.
By introducing molecular coarse-grained models as an intermediate step, this
hierarchical approach reduces the complexity of the
atomistic-to-field-theoretical transformation while preserving important
microscopic structural correlations. We systematically derive field-theoretical
models in reciprocal space for both canonical and grand canonical ensembles
using the Hubbard-Stratonovich transformation. By incorporating additional
fields for both positive and negative Fourier modes, our generalized mode
theory extends the applicability of bottom-up field-theoretical models beyond
the conventional Hubbard-Stratonovich transformation, which can be limited to
positive Fourier modes. Furthermore, we introduce an efficient perturbative
approach for approximating Fourier modes of molecular interactions, reducing
computational cost and allowing for integrating complex molecular interactions
into the field-theoretic description.

</details>


### [43] [CHEMSMART: Chemistry Simulation and Modeling Automation Toolkit for High-Efficiency Computational Chemistry Workflows](https://arxiv.org/abs/2508.20042)
*Xinglong Zhang,Huiwen Tan,Jingyi Liu,Zihan Li,Lewen Wang,Benjamin W. J. Chen*

Main category: physics.chem-ph

TL;DR: CHEMSMART is an open-source Python framework that automates quantum chemistry workflows for catalysis and molecular modeling, integrating job preparation, execution, analysis, and visualization.


<details>
  <summary>Details</summary>
Motivation: Address inefficiencies in manual workflow management for computational chemistry by providing seamless interoperability between quantum chemistry packages and cheminformatics platforms.

Method: Modular architecture with automated job submission for geometry optimization, transition state searches, thermochemical analysis, and non-covalent interaction plotting. Includes auxiliary scripts for file conversion and data organization.

Result: A robust, user-friendly platform available on GitHub that streamlines quantum chemistry workflows and ensures reproducible computational chemistry research.

Conclusion: CHEMSMART successfully provides an efficient framework for computational chemistry workflows, with future plans to expand software compatibility, incorporate QM/MM and classical MD, and align with FAIR data principles for enhanced reproducibility.

Abstract: CHEMSMART (Chemistry Simulation and Modeling Automation Toolkit) is an
open-source, Python-based framework designed to streamline quantum chemistry
workflows for homogeneous catalysis and molecular modeling. By integrating job
preparation, submission, execution, results analysis, and visualization,
CHEMSMART addresses the inefficiencies of manual workflow management in
computational chemistry by ensuring seamless interoperability with quantum
chemistry packages and cheminformatics platforms. Its modular architecture
supports automated job submission and execution tasks for geometry
optimization, transition state searches, thermochemical analysis, and
non-covalent interaction plotting, while auxiliary scripts facilitate file
conversion, data organization, and electronic structure analysis. Future
developments aim to expand compatibility with additional software, incorporate
QM/MM and classical MD, and align with FAIR data principles for enhanced
reproducibility and data reuse. Available on GitHub, CHEMSMART empowers
researchers with a robust, user-friendly platform for efficient and
reproducible computational chemistry.

</details>


<div id='physics.class-ph'></div>

# physics.class-ph [[Back]](#toc)

### [44] [SNIC bifurcation and its Application to MEMS](https://arxiv.org/abs/2508.19285)
*Joshua Shay Kricheli*

Main category: physics.class-ph

TL;DR: Method to extract mechanical frequency combs using nonlinear beam dynamics and injection pulling effects, modeled via Duffing equation and reduced to Adler's equation for analytical solution.


<details>
  <summary>Details</summary>
Motivation: Develop a practical method for generating frequency combs in MEMS devices for various applications through mechanical means rather than optical approaches.

Method: Use nonlinear beam dynamics perturbed by external harmonic driver to achieve injection pulling and SNIC bifurcation. Model with Duffing equation, reduce to slow evolution model, and solve using Adler's equation with numerical simulations.

Result: Successful generation of frequency combs through mechanical means, with numerical simulations confirming theoretical predictions and validating the analytical approach.

Conclusion: The proposed method effectively extracts frequency combs in mechanical systems using nonlinear dynamics and injection pulling, providing a viable alternative to optical frequency comb generation for MEMS applications.

Abstract: This project focuses on a method to extract a frequency comb in mechanical
means, for general interest and numerous practical applications in MEMS. The
method of execution is the implementation of a beam that is exhibiting
non-linear dynamics that is perturbed and analyzed for its transverse
vibrations. The perturbation is an external harmonic driver with a chosen small
amplitude and frequency (which is slightly detuned from the beam
eigenfrequency), that when engaged with the unperturbed beam oscillations,
causes it reach a state of "injection pulling" - an effect that occurs when one
harmonic oscillator is coupled with a second one and causes it to oscillate in
a frequency near its own. This causes the beam to reach SNIC bifurcation,
rendering a frequency comb as desired. Theoretical analysis showed that the
problem can be modelled using a non-linear equation of the beam, that
translates to a form of the non-linear Duffing equation. While a solution to
the dynamics function of the beam is hard to obtain in practice due to
mathematical difficulties, a slow evolution model is suggested that is composed
of functions of a amplitude and phase. Using several additional mathematical
assumptions, the amplitude is seen to be related to the phase, while the phase
equation solution is seen to be of the form of Adler's equation. These
assumptions ultimately reduce the entire behaviour of the beam to a relatively
simple solution to the Adler equation, which has a known analytical solution.
Computerized numerical simulations are run on it to check the results and
compare them to the theory and desired outcome. The results agreed with the
theory and produce the expected frequency comb, showing the assumptions to be
valid in extracting the comb.

</details>
