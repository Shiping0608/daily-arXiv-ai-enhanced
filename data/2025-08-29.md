<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 17]
- [math.AP](#math.AP) [Total: 12]
- [physics.comp-ph](#physics.comp-ph) [Total: 2]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 5]
- [cs.AI](#cs.AI) [Total: 1]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [math-ph](#math-ph) [Total: 1]
- [physics.space-ph](#physics.space-ph) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 2]
- [astro-ph.SR](#astro-ph.SR) [Total: 1]
- [math.CA](#math.CA) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 2]
- [quant-ph](#quant-ph) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 2]
- [cs.LG](#cs.LG) [Total: 1]
- [math.DG](#math.DG) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Multi-Order Monte Carlo IMEX hierarchies for uncertainty quantification in multiscale hyperbolic systems](https://arxiv.org/abs/2508.20187)
*Giulia Bertaglia,Walter Boscheri,Lorenzo Pareschi*

Main category: math.NA

TL;DR: Novel Multi-Order Monte Carlo method for uncertainty quantification in multiscale PDEs using Implicit-Explicit Runge-Kutta integrators, avoiding costly hierarchical re-meshing of traditional Multi-Level Monte Carlo.


<details>
  <summary>Details</summary>
Motivation: Traditional Multi-Level Monte Carlo methods require expensive hierarchical re-meshing and face computational challenges with hyperbolic systems, kinetic equations, and low Mach number flows with stiff relaxation.

Method: Constructs multi-order hierarchy by varying spatial and temporal discretization orders within Monte Carlo framework using Implicit-Explicit Runge-Kutta time integrators to satisfy asymptotic-preserving property.

Result: Achieves substantial reduction of both error and variance while maintaining asymptotic consistency in the asymptotic limit, as demonstrated by numerical experiments.

Conclusion: The Multi-Order Monte Carlo approach provides efficient variance reduction and naturally adapts to multiple scales, making it particularly suitable for challenging multiscale time-dependent PDE problems.

Abstract: We introduce a novel Multi-Order Monte Carlo approach for uncertainty
quantification in the context of multiscale time-dependent partial differential
equations. The new framework leverages Implicit-Explicit Runge-Kutta time
integrators to satisfy the asymptotic-preserving property across different
discretization orders of accuracy. In contrast to traditional Multi-Level Monte
Carlo methods, which require costly hierarchical re-meshing, our method
constructs a multi-order hierarchy by varying both spatial and temporal
discretization orders within the Monte Carlo framework. This enables efficient
variance reduction while naturally adapting to the multiple scales inherent in
the problem. The proposed method is particularly well-suited for hyperbolic
systems with stiff relaxation, kinetic equations, and low Mach number flows,
where standard Multi-Level Monte Carlo techniques often encounter computational
challenges. Numerical experiments demonstrate that the novel Multi-Order Monte
Carlo approach achieves substantial reduction of both error and variance while
maintaining asymptotic consistency in the asymptotic limit.

</details>


### [2] [Operator learning meets inverse problems: A probabilistic perspective](https://arxiv.org/abs/2508.20207)
*Nicholas H. Nelsen,Yunan Yang*

Main category: math.NA

TL;DR: Survey of operator learning methods for solving inverse problems, focusing on end-to-end approaches that map data directly to solutions without explicit forward models.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive overview of the intersection between operator learning and inverse problems, addressing the challenge of approximating mappings between infinite-dimensional function spaces for scientific computing applications.

Method: Covers probabilistic and deterministic approaches to inverse problems, measure-centric formulations, operator learning components (data generation, loss functions, architectures), and end-to-end inverse operator learning paradigms.

Result: Presents structure-aware architectures for both point predictions and posterior estimates, with theoretical foundations for linear and nonlinear inverse problems, and discusses prior/regularizer estimation using operator learning.

Conclusion: Operator learning provides powerful frameworks for solving inverse problems, particularly through end-to-end approaches that handle noise challenges and enable direct mapping from data to solutions without requiring explicit forward models.

Abstract: Operator learning offers a robust framework for approximating mappings
between infinite-dimensional function spaces. It has also become a powerful
tool for solving inverse problems in the computational sciences. This chapter
surveys methodological and theoretical developments at the intersection of
operator learning and inverse problems. It begins by summarizing the
probabilistic and deterministic approaches to inverse problems, and pays
special attention to emerging measure-centric formulations that treat observed
data or unknown parameters as probability distributions. The discussion then
turns to operator learning by covering essential components such as data
generation, loss functions, and widely used architectures for representing
function-to-function maps. The core of the chapter centers on the end-to-end
inverse operator learning paradigm, which aims to directly map observed data to
the solution of the inverse problem without requiring explicit knowledge of the
forward map. It highlights the unique challenge that noise plays in this
data-driven inversion setting, presents structure-aware architectures for both
point predictions and posterior estimates, and surveys relevant theory for
linear and nonlinear inverse problems. The chapter also discusses the
estimation of priors and regularizers, where operator learning is used more
selectively within classical inversion algorithms.

</details>


### [3] [Automated Runge-Kutta-Nyström time stepping for finite element methods in Irksome](https://arxiv.org/abs/2508.20255)
*Robert C. Kirby,Scott P. MacLachlan,Pablo D. Brubeck*

Main category: math.NA

TL;DR: Irksome library now supports second-order temporal derivatives in PDEs using Runge-Kutta-Nyström methods, generating smaller algebraic systems and better performance compared to first-order approaches.


<details>
  <summary>Details</summary>
Motivation: To enable direct discretization of second-order time derivatives in PDEs without rewriting them as first-order systems, providing computational advantages and maintaining problem formulation fidelity.

Method: Extends Irksome library based on UFL to generate stage-coupled variational problems for Runge-Kutta-Nyström methods, with Firedrake generating code and PETSc interface for solving.

Result: Numerical results show fully implicit Runge-Kutta-Nyström methods can be competitive with explicit methods for wave equations, and effective for mixed-order systems like poroelasticity even near incompressible limits.

Conclusion: The approach provides computational efficiency, smaller systems, and maintains problem formulation integrity while being essential for higher-order spatial derivative wave equations and effective for complex mixed-order systems.

Abstract: Irksome is a library based on the Unified Form Language (UFL) that automates
the application of Runge-Kutta time-stepping methods for finite element spatial
discretizations of partial differential equations (PDEs). This paper describes
recent updates to Irksome that allow users to express semidiscrete forms of
PDEs that contain second-order temporal derivatives, whence it generates
stage-coupled variational problems to be solved at each time step for
Runge-Kutta-Nystr\"om methods. Firedrake then generates code for these
variational problems and provides a rich interface to PETSc for solving them.
Directly discretizing second-order time derivatives with Runge-Kutta-Nystr\"om
methods provides several advantages relative to discretizing a rewritten
first-order system with a standard Runge-Kutta method. Besides working with an
interface closer to the problem formulation in UFL, avoiding these auxiliary
variables means that Runge-Kutta-Nystr\"om methods lead to smaller algebraic
systems and better run-time. Our numerical results indicate that, with
effective preconditioning, fully implicit Runge-Kutta-Nystr\"om methods can be
made competitive with more traditional explicit methods for wave equations.
They are also (essentially) required to discretize wave-type equations with
higher-order spatial derivatives. We also provide numerical experiments for
fully dynamic poroelasticity, a system of mixed temporal order, where our
time-stepping and algebraic solvers perform effectively even as we approach the
incompressible limit.

</details>


### [4] [Randomized Krylov methods for inverse problems](https://arxiv.org/abs/2508.20269)
*Julianne Chung,Silvia Gazzola*

Main category: math.NA

TL;DR: Randomized Krylov subspace methods for large-scale linear inverse problems using sketched inner products and Golub-Kahan approach, extending randomized GMRES with hybrid projection and automatic regularization.


<details>
  <summary>Details</summary>
Motivation: To efficiently compute regularized solutions for large-scale linear inverse problems by leveraging randomized techniques to handle high-dimensional vectors and rectangular matrices.

Method: Develop randomized Golub-Kahan approach using sketched inner products, propose iterative solvers based on randomized factorizations, and combine with hybrid projection methods and Tikhonov regularization with automatic parameter selection.

Result: Numerical results from image deblurring and seismic tomography demonstrate the potential benefits of the proposed randomized approaches.

Conclusion: The randomized Krylov subspace methods effectively extend capabilities for solving inverse problems with rectangular matrices and show promise through numerical experiments in practical applications.

Abstract: In this paper we develop randomized Krylov subspace methods for efficiently
computing regularized solutions to large-scale linear inverse problems.
Building on the recently developed randomized Gram-Schmidt process, where
sketched inner products are used to estimate inner products of high-dimensional
vectors, we propose a randomized Golub-Kahan approach that works for general
rectangular matrices. We describe new iterative solvers based on the randomized
Golub-Kahan approach and show how they can be used for solving inverse problems
with rectangular matrices, thus extending the capabilities of the recently
proposed randomized GMRES method. We also consider hybrid projection methods
that combine iterative projection methods, based on both the randomized Arnoldi
and randomized Golub-Kahan factorizations, with Tikhonov regularization, where
regularization parameters can be selected automatically during the iterative
process. Numerical results from image deblurring and seismic tomography show
the potential benefits of these approaches.

</details>


### [5] [An Efficient Exponential Sum Approximation of Power-Law Kernels for Solving Fractional Differential Equation](https://arxiv.org/abs/2508.20311)
*Renu Chaudhary,Kai Diethelm,Afshin Farhadi,Fred A. Fuchs*

Main category: math.NA

TL;DR: A framework for approximating weakly singular power-law kernels using finite exponential sums, with compression techniques and application to solving fractional differential equations.


<details>
  <summary>Details</summary>
Motivation: To develop efficient computational methods for fractional integral and differential operators by approximating singular kernels with exponential sums, enabling solution with standard ODE solvers.

Method: Uses Laplace transform substitution and trapezoidal rule to create exponential sum approximations, applies Prony's method for compression, and embeds compressed kernels into Riemann-Liouville integrals with two solution strategies: piecewise constant interpolation and transformation to ODE systems.

Result: Developed a computationally feasible framework that pre-specifies computational cost while minimizing approximation error, enabling efficient solution of fractional differential equations with standard ODE solvers.

Conclusion: The proposed exponential-sum approximation with compression techniques provides accurate and efficient solutions for fractional differential equations, demonstrating good convergence properties through numerical experiments.

Abstract: In this work, we present a comprehensive framework for approximating the
weakly singular power-law kernel $t^{\alpha-1}$ of fractional integral and
differential operators, where $\alpha \in (0,1)$ and $t \in [\delta,T]$ with
$0<\delta<T<\infty$, using a finite sum of exponentials. This approximation
method begins by substituting an exponential function into the Laplace
transform of the power function, followed by the application of the trapezoidal
rule to approximate the resulting integral. To ensure computational
feasibility, the integral limits are truncated, leading to a finite exponential
sum representation of the kernel. In contrast to earlier approaches, we
pre-specify the admitted computational cost (measured in terms of the number of
exponentials) and minimize the approximation error. Furthermore, to reduce the
computational cost while maintaining accuracy, we present a two-stage algorithm
based on Prony's method that compresses the exponential sum. The compressed
kernel is then embedded into the Riemann-Liouville fractional integral and
applied to solve fractional differential equations. To this end, we discuss two
solution strategies, namely (a) method based on piecewise constant
interpolation and (b) a transformation of the original fractional differential
equation into a system of first-order ordinary differential equations (ODEs).
This reformulation makes the problem solvable by standard ODE solvers with low
computational cost while retaining the accuracy benefits of the
exponential-sum-approximation. Finally, we apply the proposed strategies to
solve some well-known fractional differential equations and demonstrate the
advantages, accuracy, and the experimental order of convergence of the methods
through numerical results.

</details>


### [6] [Artificial neural network solver for Fokker-Planck and Koopman eigenfunctions](https://arxiv.org/abs/2508.20339)
*Max Kreider,Peter J. Thomas,Yao Li*

Main category: math.NA

TL;DR: Neural network solver for computing Koopman and Fokker-Planck eigenfunctions of SDEs, using operator-based loss to reduce data dependency.


<details>
  <summary>Details</summary>
Motivation: Eigenfunctions of Fokker-Planck and Koopman operators are powerful for analyzing stochastic differential equations but are numerically challenging to compute due to high-dimensional PDEs on unbounded domains.

Method: Data-driven artificial neural network solver that incorporates the differential operator into the loss function, improving accuracy and reducing dependence on large training datasets.

Result: Demonstrated successful application on numerical examples in two, three, and four dimensions, showing the approach works across different dimensional spaces.

Conclusion: The proposed neural network approach provides an effective method for computing eigenfunctions of stochastic operators with improved accuracy and reduced data requirements compared to traditional methods.

Abstract: For a stochastic differential equation (SDE) that is an It\^{o} diffusion or
Langevin equation, the Fokker-Planck operator governs the evolution of the
probability density, while its adjoint, the infinitesimal generator of the
stochastic Koopman operator, governs the evolution of system observables, in
the mean. The eigenfunctions of these operators provide a powerful framework to
analyze SDEs, and have shown to be particularly useful for systems of
stochastic oscillators. However, computing these eigenfunctions typically
requires solving high-dimensional PDEs on unbounded domains, which is
numerically challenging. Building on previous work, we propose a data-driven
artificial neural network solver for Koopman and Fokker-Planck eigenfunctions.
Our approach incorporates the differential operator into the loss function,
improving accuracy and reducing dependence on large amounts of accurate
training data. We demonstrate our approach on several numerical examples in
two, three, and four dimensions.

</details>


### [7] [Numerical Method for Space-Time Fractional Diffusion: A Stochastic Approach](https://arxiv.org/abs/2508.20361)
*Tengteng Cui,Chengtao Sheng,Bihao Su,Zhi Zhou*

Main category: math.NA

TL;DR: A stochastic Monte Carlo method using Feynman-Kac formula and walk-on-spheres technique to solve space-time fractional diffusion models efficiently, especially in high dimensions and complex geometries.


<details>
  <summary>Details</summary>
Motivation: Space-time fractional diffusion models face significant computational challenges due to memory effects of time-fractional derivatives and nonlocal nature of spatial fractional Laplacians, leading to high computational costs and storage demands in high-dimensional settings.

Method: Proposes a Monte Carlo method combining simulation of monotone path of stable subordinator in time with walk-on-spheres method for stable Levy jumping process in space, based on Feynman-Kac formula.

Result: Rigorous error bounds derived in terms of simulation paths and time step size. Numerical experiments confirm theoretical bounds and demonstrate computational efficiency, particularly in complex geometries and high-dimensional spaces.

Conclusion: The method shows robustness across various fractional orders, especially for small fractional values, addressing limitations of traditional numerical methods for space-time fractional diffusion problems.

Abstract: In this paper, we develop and analyze a stochastic algorithm for solving
space-time fractional diffusion models, which are widely used to describe
anomalous diffusion dynamics. These models pose substantial numerical
challenges due to the memory effect of the time-fractional derivative and the
nonlocal nature of the spatial fractional Laplacian and the, leading to
significant computational costs and storage demands, particularly in
high-dimensional settings. To overcome these difficulties, we propose a Monte
Carlo method based on the Feynman--Kac formula for space-time fractional
models. The novel algorithm combines the simulation of the monotone path of a
stable subordinator in time with the ``walk-on-spheres'' method that
efficiently simulates the stable Levy jumping process in space. We rigorously
derive error bounds for the proposed scheme, explicitly expressed in terms of
the number of simulation paths and the time step size. Numerical experiments
confirm the theoretical error bounds and demonstrate the computational
efficiency of the method, particularly in domains with complex geometries or
high-dimensional spaces. Furthermore, both theoretical and numerical results
emphasize the robustness of the proposed approach across a range of fractional
orders, particularly for small fractional values, a capability often absent in
traditional numerical methods.

</details>


### [8] [D3PINNs: A Novel Physics-Informed Neural Network Framework for Staged Solving of Time-Dependent Partial Differential Equations](https://arxiv.org/abs/2508.20440)
*Xun Yang,Guanqiu Ma*

Main category: math.NA

TL;DR: D3PINNs framework combines PINNs with dynamic domain decomposition to solve time-dependent PDEs by reducing them to ODEs, maintaining PINN efficiency while improving time-dependent solution capture.


<details>
  <summary>Details</summary>
Motivation: To enhance Physics-Informed Neural Networks' ability to solve time-dependent partial differential equations more effectively while retaining computational efficiency.

Method: First obtain approximate solution using PINNs with domain decomposition, then reduce PDE to ODE by replacing solution terms with approximation, finally solve time-varying solution using classical ODE numerical methods.

Result: The framework successfully captures dynamic solutions of time-dependent PDEs and numerical experiments validate its effectiveness.

Conclusion: D3PINNs provide an efficient and flexible approach that enhances PINNs' capability for solving time-dependent PDE problems while maintaining computational advantages.

Abstract: In this paper, we propose a novel framework, Dynamic Domain Decomposition
Physics-Informed Neural Networks (D3PINNs), for solving time-dependent partial
differential equations (PDEs). In this framework, solutions of time-dependent
PDEs are dynamically captured. First, an approximate
  solution is obtained by the Physics-Informed Neural Networks (PINNs)
containing the domain decomposition, then the time derivative terms in the PDE
will be retained and the other terms associated with the solution will be
replaced with the approximate solution. As a result, the PDE reduces to an
ordinary differential equations (ODEs). Finally, the time-varying solution will
be solved by the classical numerical methods for ODEs. D3PINNs retain the
computational efffciency and ffexibility inherent to PINNs and enhance the
ability for capturing solutions of time-dependent PDEs. Numerical experiments
validate the effectiveness of the proposed methods.

</details>


### [9] [A Chebyshev--Jackson series based block SS--RR algorithm for computing partial eigenpairs of real symmetric matrices](https://arxiv.org/abs/2508.20456)
*Zhongxiao Jia,Tianhang Liu*

Main category: math.NA

TL;DR: A new algorithm using Chebyshev-Jackson series expansion for eigenpair computation that avoids solving expensive linear systems, proving convergence and showing better efficiency than traditional contour integral methods.


<details>
  <summary>Details</summary>
Motivation: To improve computational efficiency for eigenpair computations of large symmetric matrices by avoiding expensive solutions of large shifted complex linear systems required in traditional contour integral methods.

Method: Uses Chebyshev-Jackson series expansion to approximate moments instead of numerical quadrature, involving only matrix-vector products. Proves pointwise convergence of CJ series expansions for higher order moments.

Result: The CJ-SS-RR algorithm demonstrates better efficiency than the contour integral-based block SS-RR algorithm with trapezoidal rule in numerical experiments.

Conclusion: The Chebyshev-Jackson series expansion provides an effective alternative to numerical quadrature for moment approximation in eigenpair computations, offering improved computational efficiency while maintaining theoretical convergence guarantees.

Abstract: This paper considers eigenpair computations of large symmetric matrices with
the desired eigenvalues lying in a given interval using the contour
integral-based block SS--RR method, a Rayleigh--Ritz projection onto a certain
subspace generated by moment matrices. Instead of using a numerical quadrature
to approximately compute the moments by solving a number of large shifted
complex linear systems at each iteration, we make use of the Chebyshev--Jackson
(CJ) series expansion to approximate the moments, which only involves
matrix-vector products and avoids expensive solutions of the linear systems. We
prove that the CJ series expansions pointwise converge to the moments as the
series degree increases, but at different convergence rates depending on point
positions and moment orders. These extend the available convergence results on
the zeroth moment of CJ series expansions to higher order ones. Based on the
results established, we develop a CJ--SS--RR algorithm. Numerical experiments
illustrate that the new algorithm is more efficient than the contour
integral-based block SS--RR algorithm with the trapezoidal rule.

</details>


### [10] [Tsunami modeling with dynamic seafloors: a high-order solver validated with shallow water benchmarks](https://arxiv.org/abs/2508.20596)
*Thomas Melkior,Harsha S Bhat,Faisal Amlani*

Main category: math.NA

TL;DR: A high-order pseudo-spectral algorithm using Fourier continuation is developed to model tsunami generation with time-dependent seafloor motion, addressing limitations of conventional instantaneous displacement models.


<details>
  <summary>Details</summary>
Motivation: Conventional tsunami models neglect dynamic ground movement and assume instantaneous seafloor displacement, missing important physical effects of time-dependent earthquake rupture and bathymetry motion that can affect tsunami generation.

Method: Pseudo-spectral algorithm based on Fourier continuation methodology for solving nonlinear shallow water equations with time-dependent seafloor displacement and velocity, providing high-order convergence and minimal numerical dispersion errors.

Result: The solver achieves high accuracy with mild CFL constraints, effectively eliminates numerical pollution errors, and enables efficient resolution of different space-time scales for dynamic earthquake-generated tsunamis.

Conclusion: The proposed Fourier continuation-based approach demonstrates the importance of incorporating time-dependent seafloor behavior in tsunami hazard assessment and enables realistic parametric studies of earthquake effects on tsunami generation.

Abstract: Recent scientific studies have suggested that, in certain physical
configurations, the time-dependent behavior of earthquake rupture and seafloor
(bathymetry) motion can leave observable near-field signatures in tsunami wave
generation and propagation. However, dynamic ground movement is often neglected
in conventional tsunami models, which commonly assume instantaneous ground
displacement (sourcing). This work introduces a pseudo-spectral algorithm for
the solution of the nonlinear shallow water equations with timedependent
seafloor displacement and velocity. Based on a Fourier continuation (FC)
methodology for the accurate trigonometric interpolation of a non-periodic
function, the solver provides high-order convergence in space and time; mild
(linear) Courant-Friedrichs-Lewy (CFL) constraints for explicit time
integration; and results that are effectively free of numerical dispersion (or
''pollution'') errors. Such properties enable the efficient and robust
resolution of the different space-time scales involved modeling tsunamis
generated by dynamic earthquake ground motion (including over long distances).
Numerical experiments attesting to accuracy and computational performance are
presented with direct comparisons to high-order finite difference
methodologies. The solver is physically validated by a number of classical and
semi-classical benchmark cases based on simulated or experimental data.
Additionally, a seismologically realistic, first-of-its-kind parametric study
based on earthquake speed is introduced, whose results-easily facilitated by
the FC-based approach proposed herein, with minimal numerical tuning-further
demonstrate the potential importance of (and the motivation herein for)
incorporating time-dependent seafloor behavior in quantitative tsunami hazard
assessment.

</details>


### [11] [A comparative study of finite element methods for a class of harmonic map heat flow problems](https://arxiv.org/abs/2508.20590)
*Nam Anh Nguyen,Arnold Reusken*

Main category: math.NA

TL;DR: Systematic comparison of three finite element methods for harmonic map heat flow from 2D disk to 3D sphere, including convergence validation and efficiency analysis with discrete inf-sup stability for one method.


<details>
  <summary>Details</summary>
Motivation: To provide a unified framework for comparing different finite element discretization approaches for harmonic map heat flow problems, which are important in various mathematical and physical applications.

Method: Review and systematic comparison of three finite element discretization methods in a unified framework, with numerical tests to validate convergence rates and computational efficiency.

Result: Numerical tests validated convergence rates for smooth solutions and provided comparative analysis of computational efficiency across the three methods.

Conclusion: The study successfully compared three finite element methods, derived discrete inf-sup stability for one method, and provided practical insights into their convergence behavior and computational performance for harmonic map heat flow problems.

Abstract: In this paper, we review and systematically compare three finite element
discretization methods for a harmonic map heat flow problem from the unit disk
in $\mathbb{R}^2$ to the unit sphere in $\mathbb{R}^3$ in an unified framework.
Numerical tests validate the convergence rates in a regime of smooth solutions
and are used to compare the methods in terms of computational efficiency. For
one of the methods a discrete inf-sup stability result is derived.

</details>


### [12] [High-order fully well-balanced numerical methods for one-dimensional blood flow with discontinuous properties, friction and gravity](https://arxiv.org/abs/2508.20638)
*Ernesto Pimentel-García,Lucas O. Müller,Carlos Parés*

Main category: math.NA

TL;DR: High-order numerical schemes for blood flow models with discontinuous properties and source terms, using generalized hydrostatic reconstruction and numerical steady state preservation.


<details>
  <summary>Details</summary>
Motivation: To develop accurate numerical methods for blood flow modeling that can handle discontinuous mechanical properties and algebraic source terms (friction, gravity) while maintaining well-balanced properties.

Method: Generalized Hydrostatic Reconstruction for parameter discontinuities, numerical reconstruction operator for steady state identification and preservation, extended to vessel networks.

Result: Methods successfully applied to single- and multiple-vessel tests including a 118-vessel network, demonstrating superior performance over naive discretizations.

Conclusion: The presented well-balanced, high-order schemes effectively handle blood flow models with discontinuities and source terms, outperforming conventional approaches in complex vascular networks.

Abstract: We present well-balanced, high-order, semi-discrete numerical schemes for
one-dimensional blood flow models with discontinuous mechanical properties and
algebraic source terms representing friction and gravity. While discontinuities
in model parameters are handled using the Generalized Hydrostatic
Reconstruction, the presence of algebraic source terms implies that steady
state solutions cannot always be computed analytically. In fact, steady states
are defined by an ordinary differential equation that needs to be integrated
numerically. Therefore, we resort on a numerical reconstruction operator to
identify and, where appropriate, preserve steady states with an accuracy that
depends on the reconstruction operator's numerical scheme. We extend our
methods to deal with networks of vessels and show numerical results for single-
and multiple-vessel tests, including a network of 118 vessels, demonstrating
the capacity of the presented methods to outperform naive discretizations of
the equations under study.

</details>


### [13] [Curvilinear coordinates and curvature in radiative transport](https://arxiv.org/abs/2508.20852)
*Johannes Krotz,Ryan G. McClarren*

Main category: math.NA

TL;DR: General formulation of streaming term in radiative transport equations using curvilinear coordinates with local orthonormal frames, showing how geometry and curvature affect particle transport.


<details>
  <summary>Details</summary>
Motivation: To understand how coordinate system choices and geometric curvature influence the streaming operator in transport problems, particularly for domains with complex geometry.

Method: Parametrize angular variables using local orthonormal frames to express directional derivatives in terms of curvature-related quantities from spatial manifolds.

Result: Derived a general expression that provides geometric interpretation of streaming operator components and shows interaction between coordinate choices and curvature.

Conclusion: The framework offers intuitive insights for simplifying angular dependence and guides coordinate system selection to balance analytical tractability with computational efficiency.

Abstract: We derive a general expression for the streaming term in radiative transport
equa- tions and other transport problems when formulated in curvilinear
coordinates, emphasizing coordinate systems adapted to the geometry of the
domain and the directional dependence of particle transport. By parametrizing
the angular vari- able using a local orthonormal frame, we express directional
derivatives in terms of curvature-related quantities that reflect the geometry
of underlying spatial man- ifolds. Our formulation highlights how the
interaction between coordinate choices and curvature influences the streaming
operator, offering geometric interpretations of its components. The resulting
framework offers intuitive insight into when and how angular dependence can be
simplified and may guide the selection of coordinate systems that balance
analytical tractability and computational efficiency.

</details>


### [14] [Uniform error analysis of a rectangular Morley finite element method on a Shishkin mesh for a 4th-order singularly perturbed boundary value problem](https://arxiv.org/abs/2508.20857)
*Xiangyun Meng,Martin Stynes*

Main category: math.NA

TL;DR: Finite element method using rectangular Morley elements on Shishkin mesh for singularly perturbed reaction-diffusion problem achieves superior O(N^{-3/2}) convergence rate compared to previous Adini element approach.


<details>
  <summary>Details</summary>
Motivation: Solve singularly perturbed reaction-diffusion problems with boundary layers more efficiently, improving upon existing methods that had slower convergence rates.

Method: Discretization using rectangular Morley finite elements on a Shishkin mesh, with analysis in an energy-type norm natural for the problem.

Result: Proves O(ε^{1/2}N^{-1} + εN^{-1}lnN + N^{-3/2}) convergence rate. In critical regime ε≈N^{-1}, achieves O(N^{-3/2}) rate, superior to previous O(N^{-1/2}) rate with Adini elements.

Conclusion: The proposed Morley element method on Shishkin mesh provides significantly better convergence performance for singularly perturbed reaction-diffusion problems with boundary layers.

Abstract: The singularly perturbed reaction-diffusion problem $\varepsilon^2\Delta^2 u
- \mathrm{div}\left(c\nabla u\right) = f$ is considered on the unit square
$\Omega$ in $\mathbb{R}^2$ with homogenous Dirichlet boundary conditions. Its
solution typically contains boundary layers on all sides of~$\Omega$. It is
discretised by a finite element method that uses rectangular Morley elements on
a Shishkin mesh. In an associated energy-type norm that is natural for this
problem, we prove an $O(\varepsilon^{1/2}N^{-1}+\varepsilon N^{-1}\ln N +
N^{-3/2})$ rate of convergence for the error in the computed solution, where
$N$~is the number of mesh intervals in each coordinate direction. Thus in the
most troublesome regime when $\varepsilon \approx N^{-1}$, our method is proved
to attain an $O(N^{-3/2})$ rate of convergence, which is shown to be sharp by
our numerical experiments and is superior to the $O(N^{-1/2})$ rate that is
proved in Meng & Stynes, Adv. Comput. Math. 2019 when Adini finite elements are
used to solve the same problem on the same mesh.

</details>


### [15] [Fast numerical derivatives based on multi-interval Fourier extension](https://arxiv.org/abs/2508.20876)
*Zhenyu Zhao,Yanfei Wang,Xinran Liu*

Main category: math.NA

TL;DR: Efficient algorithm for stable numerical differentiation from noisy data using Fourier extension approximations with adaptive domain partitioning and recursive bisection.


<details>
  <summary>Details</summary>
Motivation: Numerical differentiation from noisy data is an ill-posed problem that requires stable and efficient methods to handle functions with rapid oscillations or boundary activity.

Method: Combines multi-interval Fourier extension approximations with adaptive domain partitioning strategy, using recursive bisection to select locally-resolved Fourier fits and precomputed differentiation operators.

Result: Significant improvements over existing methods, achieving accurate derivative reconstruction for challenging functions while maintaining computational efficiency.

Conclusion: Provides a robust framework for ill-posed differentiation problems with efficient computation and automatic refinement for complex function behaviors.

Abstract: We present a computationally efficient algorithm for stable numerical
differentiation from noisy, uniformly-sampled data on a bounded interval. The
method combines multi-interval Fourier extension approximations with an
adaptive domain partitioning strategy: a global precomputation of local Fourier
sampling matrices and their thin SVDs is reused throughout a recursive
bisection procedure that selects locally-resolved Fourier fits. Each accepted
subinterval stores a compact set of Fourier coefficients that are subsequently
used to reconstruct the derivative via a precomputed differentiation operator.
The stopping criterion balances fitting error and an explicit noise-level
bound, and the algorithm automatically refines the partition where the function
exhibits rapid oscillations or boundary activity. Numerical experiments
demonstrate significant improvements over existing methods, achieving accurate
derivative reconstruction for challenging functions. The approach provides a
robust framework for ill-posed differentiation problems while maintaining
computational efficiency.

</details>


### [16] [Lattice Random Walk Discretisations of Stochastic Differential Equations](https://arxiv.org/abs/2508.20883)
*Samuel Duffield,Maxwell Aifer,Denis Melanson,Zach Belateche,Patrick J. Coles*

Main category: math.NA

TL;DR: A lattice random walk discretization scheme for SDEs using binary/ternary increments instead of floating-point computations, offering hardware compatibility, Gaussian sampling elimination, and robustness to quantization errors.


<details>
  <summary>Details</summary>
Motivation: Traditional SDE discretizations require complex floating-point computations and Gaussian sampling. This work aims to simplify computations using binary/ternary random values while maintaining convergence properties.

Method: Lattice random walk discretization that samples 1-2 bit binary or ternary increments at each step, avoiding floating-point arithmetic and directly manipulating probability distributions of bitstreams.

Result: Proven weak convergence and demonstrated advantages through experiments on various SDEs, including state-of-the-art diffusion models, showing robustness to quantization errors and handling of non-Lipschitz drifts.

Conclusion: The proposed scheme provides a significant departure from traditional methods, offering hardware compatibility, computational simplicity, and robust performance while eliminating Gaussian sampling requirements.

Abstract: We introduce a lattice random walk discretisation scheme for stochastic
differential equations (SDEs) that samples binary or ternary increments at each
step, suppressing complex drift and diffusion computations to simple 1 or 2 bit
random values. This approach is a significant departure from traditional
floating point discretisations and offers several advantages; including
compatibility with stochastic computing architectures that avoid floating-point
arithmetic in place of directly manipulating the underlying probability
distribution of a bitstream, elimination of Gaussian sampling requirements,
robustness to quantisation errors, and handling of non-Lipschitz drifts. We
prove weak convergence and demonstrate the advantages through experiments on
various SDEs, including state-of-the-art diffusion models.

</details>


### [17] [Optimization on the Extended Tensor-Train Manifold with Shared Factors](https://arxiv.org/abs/2508.20928)
*Alexander Molozhavenko,Maxim Rakhuba*

Main category: math.NA

TL;DR: This paper develops Riemannian optimization methods for Extended Tensor Train (ETT) decompositions with factor sharing constraints, proving smooth manifold structure and demonstrating effectiveness on tensor approximation and eigenvalue problems.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of tensor decompositions with factor sharing constraints, which break the multilinear structure but enable more compact representations and parameter sharing in tensor models.

Method: Developed Riemannian optimization framework for ETT format with factor sharing, including quasi-optimal retraction operations and tangent space projection using automatic differentiation.

Result: Proved that the underlying manifold with factor sharing constraints is smooth, and demonstrated practical effectiveness through tensor approximation tasks and multidimensional eigenvalue problems.

Conclusion: Riemannian optimization provides a natural and effective approach for handling factor sharing constraints in ETT decompositions, enabling efficient computation while maintaining mathematical rigor.

Abstract: This paper studies tensors that admit decomposition in the Extended Tensor
Train (ETT) format, with a key focus on the case where some decomposition
factors are constrained to be equal. This factor sharing introduces additional
challenges, as it breaks the multilinear structure of the decomposition.
Nevertheless, we show that Riemannian optimization methods can naturally handle
such constraints and prove that the underlying manifold is indeed smooth. We
develop efficient algorithms for key Riemannian optimization components,
including a retraction operation based on quasi-optimal approximation in the
new format, as well as tangent space projection using automatic
differentiation. Finally, we demonstrate the practical effectiveness of our
approach through tensor approximation tasks and multidimensional eigenvalue
problem.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [18] [Besov spaces associated with the Harmonic oscillator](https://arxiv.org/abs/2508.20436)
*Reika Fukuizumi,Tsukasa Iwabuchi*

Main category: math.AP

TL;DR: Introduction and exploration of Besov spaces associated with harmonic oscillators, covering fundamental concepts, embeddings, and bilinear estimates.


<details>
  <summary>Details</summary>
Motivation: To establish and systematically study Besov spaces in the context of harmonic oscillators, providing a mathematical framework for analyzing related functional spaces and their properties.

Method: Mathematical analysis and theoretical development of Besov spaces associated with harmonic oscillators, including investigation of embedding properties and bilinear estimates.

Result: Comprehensive summary and exploration of fundamental concepts, embedding properties, and bilinear estimates for Besov spaces in harmonic oscillator settings.

Conclusion: Successful introduction and thorough exploration of harmonic oscillator-associated Besov spaces, establishing foundational knowledge and mathematical properties for this specialized functional space.

Abstract: The Besov space associated with the harmonic oscillator is introduced and
thoroughly explored in this paper. It provides a comprehensive summary of the
fundamental concepts of the Besov spaces, their embedding properties, bilinear
estimates, and related topics.

</details>


### [19] [Equilibria of aggregation-diffusion models with nonlinear potentials](https://arxiv.org/abs/2508.20523)
*Francesco Bozzola,Edoardo Mainini*

Main category: math.AP

TL;DR: Analysis of a nonlinear diffusion-aggregation model with (s,p) Riesz potential, examining radial stationary states and their connection to Hardy-Littlewood-Sobolev inequalities and free energy minimization.


<details>
  <summary>Details</summary>
Motivation: To understand the competition between nonlinear diffusion (porous medium type) and nonlocal aggregation drift, particularly focusing on the novel choice of nonlinear (s,p) Riesz potential for describing aggregation effects.

Method: Investigation of radial stationary states, analysis of their relation to extremals of Hardy-Littlewood-Sobolev inequalities, and study of global minimizers of homogeneous free energy functionals. Examination of the limit as fractional parameter s approaches zero.

Result: Established connections between radial stationary states and extremals of Hardy-Littlewood-Sobolev inequalities. Showed that when aggregation doesn't dominate diffusion, these states relate to global minimizers of free energy. Described asymptotic behavior as s→0 where nonlocal interaction becomes backward diffusion.

Conclusion: The model provides a framework for understanding nonlinear diffusion-competing aggregation systems, with stationary states linked to fundamental inequalities and free energy minimization, offering insights into the transition to backward diffusion behavior.

Abstract: We consider an evolution model with nonlinear diffusion of porous medium type
in competition with a nonlocal drift term favoring mass aggregation. The
distinguishing trait of the model is the choice of a nonlinear $(s,p)$ Riesz
potential for describing the overall aggregation effect. We investigate radial
stationary states of the dynamics, showing their relation with extremals of
suitable Hardy-Littlewood-Sobolev inequalities. In the case that aggregation
does not dominate over diffusion, radial stationary states also relate to
global minimizers of a homogeneous free energy functional featuring the $(s,p)$
energy associated to the nonlinear potential. In the limit as the fractional
parameter $s$ tends to zero, the nonlocal interaction term becomes a backward
diffusion and we describe the asymptotic behavior of the stationary states.

</details>


### [20] [Non-Archimedean Neumann problem: weak and strong solutions](https://arxiv.org/abs/2508.20548)
*Alexandra V. Antoniouk,Anatoly N. Kochubei*

Main category: math.AP

TL;DR: Analysis of Neumann problem for fractional differentiation operator in non-Archimedean local fields using weak and strong solution approaches


<details>
  <summary>Details</summary>
Motivation: To extend the study of fractional differential equations to non-Archimedean local fields and develop solution methods for Neumann problems with Vladimirov-Taibleson operators

Method: Weak solutions following Dipierro-Ros-Oton-Valdinoci (2017) approach; strong solutions using ultrametric identities specific to the operator

Result: Development of solution frameworks for Neumann problems with fractional differentiation operators in non-Archimedean settings

Conclusion: The paper establishes analytical methods for solving fractional differential equations in non-Archimedean local fields, contributing to the extension of fractional calculus beyond classical settings

Abstract: We consider the Neumann problem for the equation with the
Vladimirov-Taibleson fractional differentiation operator over a non-Archimedean
local field. We study weak solutions following the method by Dipierro, Ros-Oton
and Valdinoci (2017). Our investigation of strong solutions is based on the
ultrametric identities for the operator under consideration.

</details>


### [21] [Regularity Analysis for Two Coupled Second Order Evolution Equations](https://arxiv.org/abs/2508.20624)
*Chenxi Deng,Zhaobin Kuang,Zhuangyi Liu,Qiong Zhang*

Main category: math.AP

TL;DR: Analysis of regularity properties for a semigroup from coupled second-order evolution equations with indirect damping, showing analyticity or Gevrey class regularity in different parameter regions.


<details>
  <summary>Details</summary>
Motivation: To provide a complete and sharp characterization of the regularity properties of the semigroup associated with coupled second-order evolution equations with indirect damping, building on recent stability studies.

Method: Derived asymptotic expressions of eigenvalues of the generator, partitioned parameter space into disjoint regions, and estimated the resolvent of the generator on the imaginary axis.

Result: Successfully partitioned parameter space into regions where the semigroup exhibits either analyticity or Gevrey class regularity, providing a complete regularity characterization.

Conclusion: The study offers a comprehensive understanding of the regularity properties of the semigroup for this system, with sharp characterizations across different parameter regions.

Abstract: We investigate the regularity of the strongly continuous semigroup associated
with a system of two coupled second order evolution equations with indirect
damping, whose stability was recently studied by Hao et al. By deriving the
asymptotic expression of the eigenvalues the generator, we partition the
parameter space into several disjoint regions, where the semigroup exhibits
either analyticity or Gevrey class regularity. Together with the estimate of
the resolvent of the generator on the imaginary axis, we give a complete and
sharp regularity characterization for this system.

</details>


### [22] [Homogenized phase transition model for perforated ferromagnetic media](https://arxiv.org/abs/2508.20690)
*Catherine Choquet,Mohamed Ouhadan,Mouhcine Tilioua*

Main category: math.AP

TL;DR: Homogenization of phase-field model for paramagnetic-ferromagnetic transition in perforated 3D materials using two-scale convergence to derive effective macroscopic equations and Curie temperature tensor.


<details>
  <summary>Details</summary>
Motivation: To rigorously predict effective equations governing phase transitions in perforated 3D magnetic materials, capturing how microstructure affects macroscopic magnetic properties and enabling calculation of Curie temperature tensor.

Method: Uses two-scale convergence with dilation operator for homogenization, employs compensated compactness and Vitali compactness for nonlinearities in phase-field Ginzburg-Landau model coupled with Maxwell system and temperature equation.

Result: Developed homogenization framework that overcomes limitations of standard compactness arguments, enabling explicit calculation of macroscopic Curie temperature tensor that incorporates material geometry and magnetic permeability effects.

Conclusion: The work provides a rigorous mathematical foundation for predicting macroscopic magnetic behavior in perforated materials, with practical applications in designing materials with tailored magnetic properties through microstructure control.

Abstract: This work presents a rigorous prediction of the effective equations governing
the paramagnetic-ferromagnetic phase transition in a perforated
three-dimensional body. Assuming a periodic distribution of perforations, we
investigate the asymptotic behavior of solutions to the equations describing
the thermodynamic and electromagnetic properties of the material as the period
of the microstructure tends to zero. The microscopic model is a phase-field
model within the Ginzburg-Landau framework for second-order phase transitions,
where the phase-field is directly related to the magnetization vector. This
model couples a nonlinear equation for the magnetization with the quasi-static
Maxwell system and another nonlinear equation for the temperature. The primary
mathematical challenge lies in homogenizing these equations, which exhibit a
complex doubly non-linear structure. Additionally, the extension operators used
within the homogenization framework precludes the application of standard
Aubin--Lions compactness arguments. Our analysis employs two-scale convergence
in conjunction with a two-scale decomposition based on an appropriate dilation
operator. The nonlinearities are primarily addressed by means of a variant of
compensated compactness and a Vitali compactness argument. From the perspective
of practical applications, this work enables the explicit calculation of a
Curie temperature tensor, capturing at the macroscopic scale the coupled effect
of the material's geometric structure and its magnetic permeability tensor.

</details>


### [23] [Large-friction and incompressible limits for pressureless Euler/isentropic Navier-Stokes flows](https://arxiv.org/abs/2508.20730)
*Hai-Liang Li,Ling-Yun Shou,Yue Zhang*

Main category: math.AP

TL;DR: Analysis of two-phase flow system limits: large-friction convergence to drift-flux model with √τ rate, incompressible limit to Transport-Navier-Stokes system, and combined limits for ill-prepared data.


<details>
  <summary>Details</summary>
Motivation: To study the mathematical behavior of two-phase flow systems (Euler-NS) under extreme conditions - large friction and incompressible limits - and establish rigorous convergence results between different physical models.

Method: Established uniform regularity estimates with respect to friction coefficient τ, proved global existence of solutions for Cauchy problem, analyzed convergence rates, and studied asymptotic behavior using critical Besov spaces and perturbation techniques.

Result: Proved strong convergence of Euler-NS to drift-flux model with explicit √τ convergence rate, established incompressible limit to Transport-Navier-Stokes system, and justified combined limits (τ=ε→0) globally in time for ill-prepared initial data.

Conclusion: The paper provides complete mathematical justification for singular limit processes in two-phase flow systems, demonstrating robust convergence properties and establishing global validity of these limits even for challenging ill-prepared initial conditions.

Abstract: We investigate the large-friction and incompressible limits for a two-phase
flow (Euler-NS) system which couples the pressureless Euler equations and the
isentropic compressible Navier-Stokes equations through a drag force term with
the friction coefficient $\frac{1}{\tau}>0$ in $\mathbb{R}^{d}$ ($d\geq2$). We
establish the uniform regularity estimates with respect to $\tau$ so that the
solution of the Cauchy problem for the Euler-NS system exists globally in time,
provided that the initial data are uniformly close to the equilibrium state in
a critical Besov space. These uniform estimates allow us to rigorously justify
the strong convergence of the Euler-NS system to a one-velocity two-phase
drift-flux (DF) model as $\tau \to 0$, with an explicit convergence rate of
order $\sqrt{\tau}$. We also study the large-time asymptotic behavior of
solutions for the Euler-NS system, uniformly with respect to $\tau$. Moreover,
when the Mach number $\varepsilon>0$ is considered, we prove the incompressible
limit of the DF model toward the Transport-Navier-Stokes (TNS) system as
$\varepsilon\rightarrow 0$, and justify the combined large-friction and
incompressible limit for the Euler-NS system toward the TNS system in the
regime $\tau=\varepsilon\rightarrow 0$. Each singular limit process is globally
valid in time for {\emph{ill-prepared}} initial data.

</details>


### [24] [Existence of transmission eigenvalues for biharmonic scattering by a clamped planar region](https://arxiv.org/abs/2508.20768)
*Isaac Harris,Andreas Kleefeld,Heejin Lee*

Main category: math.AP

TL;DR: Study of clamped transmission eigenvalue problem from elastic plate scattering, proving existence of infinitely many real eigenvalues and their relationship to Dirichlet/Neumann eigenvalues.


<details>
  <summary>Details</summary>
Motivation: To analyze a new transmission eigenvalue problem derived from scattering of impenetrable clamped obstacles in thin elastic plates, which presents unique analytical and computational challenges compared to other models.

Method: Studied the variational formulation equivalent to the clamped transmission eigenvalue problem, which is a system of homogeneous PDEs defined throughout ℝ², and investigated relationships with Dirichlet and Neumann eigenvalues of the negative Laplacian.

Result: Proved the existence of infinitely many real clamped transmission eigenvalues through analysis of the equivalent variational formulation.

Conclusion: The clamped transmission eigenvalue problem exhibits infinitely many real eigenvalues and has connections to classical Dirichlet and Neumann eigenvalue problems, providing new insights into scattering problems for elastic plates.

Abstract: In this paper, we study the so-called clamped transmission eigenvalue
problem. This is a new transmission eigenvalue problem that is derived from the
scattering of an impenetrable clamped obstacle in a thin elastic plate. The
scattering problem is modeled by a biharmonic wave operator given by the
Kirchhoff--Love infinite plate problem in the frequency domain. These
scattering problems have not been studied to the extent of other models. Unlike
other transmission eigenvalue problems, the problem studied here is a system of
homogeneous PDEs defined in all of $\mathbb{R}^2$. This provides unique
analytical and computational difficulties when studying the clamped
transmission eigenvalue problem. We are able to prove that there exist
infinitely many real clamped transmission eigenvalues. This is done by studying
the equivalent variational formulation. We also investigate the relationship of
the clamped transmission eigenvalues to the Dirichlet and Neumann eigenvalues
of the negative Laplacian for the bounded scattering obstacle.

</details>


### [25] [On a nonlocal superconductivity problem](https://arxiv.org/abs/2508.20841)
*Damião J. Araújo,Aelson Sobral*

Main category: math.AP

TL;DR: Interior gradient Hölder regularity estimates for viscosity solutions of degenerate nonlocal free boundary problems in superconductivity models


<details>
  <summary>Details</summary>
Motivation: Extend nonlocal counterpart to local free boundary problems studied by Caffarelli et al., addressing models where no PDE governs moving sets with vanishing gradients

Method: Analysis of degenerate nonlocal free boundary problems using viscosity solution framework, where test functions only require nonzero gradient

Result: Main results establish interior gradient Hölder regularity estimates for viscosity solutions

Conclusion: Successfully extends regularity theory to nonlocal setting for superconductivity-related free boundary problems with degenerate structure

Abstract: This paper investigates degenerate nonlocal free boundary problems arising in
the context of superconductivity, extending the nonlocal counterpart to the
work of Caffarelli, Salazar, and Shahgholian \cite{CS02, CSS04} in the local
setting. In these models, no partial differential equation governs the moving
sets where the gradient vanishes, meaning that test functions are only required
to have a nonzero gradient. Our main results provide interior gradient H\"older
regularity estimates for viscosity solutions.

</details>


### [26] [Homogenisation of phase-field functionals with linear growth](https://arxiv.org/abs/2508.20845)
*Francesco Colasanto,Matteo Focardi,Caterina Ida Zeppieri*

Main category: math.AP

TL;DR: Homogenization analysis of elliptic Ambrosio-Tortorelli functionals with linear gradient growth, showing convergence to free-discontinuity energy with jump-dependent surface term.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous homogenization procedures for image-segmentation models, particularly analyzing the impact of fine-scale oscillations and phase-field regularizations in elliptic functionals with linear gradient growth.

Method: Analysis of Ambrosio-Tortorelli type elliptic functionals with linear growth in gradient variable, considering random fine-scale oscillations and phase-field regularizations under mild assumptions including stationary random integrands.

Result: The functionals homogenize to a free-discontinuity energy where the surface term explicitly depends on the jump amplitude of the limit variable, contrasting with classical superlinear growth cases.

Conclusion: A rigorous homogenization framework is established for image-segmentation models with linear gradient growth, demonstrating convergence to jump-dependent free-discontinuity energies under general conditions including random settings.

Abstract: We propose a first rigorous homogenisation procedure in image-segmentation
models by analysing the relative impact of (possibly random) fine-scale
oscillations and phase-field regularisations for a family of elliptic
functionals of Ambrosio and Tortorelli type, when the regularised volume term
grows \emph{linearly} in the gradient variable. In contrast to the more
classical case of superlinear growth, we show that our functionals homogenise
to a free-discontinuity energy whose surface term explicitly depends on the
jump amplitude of the limit variable. The convergence result as above is
obtained under very mild assumptions which allow us to treat, among other, the
case of \emph{stationary random integrands}.

</details>


### [27] [Energy decay for evolution equations with glassy type memory](https://arxiv.org/abs/2508.20921)
*Paola Loreti,Daniela Sforza*

Main category: math.AP

TL;DR: Analysis of energy decay in integro-differential equations with glassy memory kernels, providing explicit estimates connecting kernel decay to energy decay.


<details>
  <summary>Details</summary>
Motivation: To study energy decay in integro-differential evolution equations with glassy memory kernels, a class not previously analyzed, and establish explicit relationships between kernel decay rates and energy decay rates.

Method: Detailed mathematical analysis of integro-differential equations with glassy memory kernels, employing analytical techniques to derive explicit decay estimates.

Result: Obtained explicit estimates showing the connection between the decay constant of the memory kernel function and the decay constant of the energy in the system.

Conclusion: The study successfully analyzes glassy memory kernels in integro-differential equations and provides explicit decay estimates, filling a gap in previous research on this class of memory kernels.

Abstract: In this paper, we address the question of estimating the energy decay of
integro-differential evolution equations with glassy memory. This class of
memory kernel was not analyzed in previous studies. Moreover, a detailed
analysis provides an explicit estimate of the connection between the kernel
function's decay constant and the energy's decay constant.

</details>


### [28] [Breather solutions to nonlinear Maxwell equations with retarded material laws](https://arxiv.org/abs/2508.20938)
*Sebastian Ohrem*

Main category: math.AP

TL;DR: Existence of breathers (time-periodic, localized solutions) in Kerr-type optical slab waveguides with nonlinear electric response and retarded cubic terms, proven using variational methods under spectral gap conditions.


<details>
  <summary>Details</summary>
Motivation: To demonstrate the existence of breathers in optical waveguides with Kerr-type nonlinear materials that have both instantaneous and temporally retarded responses to electric fields.

Method: Variational method that relies on the assumption that an effective operator related to the linear part of Maxwell's equations has a spectral gap about 0. Analysis of slab waveguides with inhomogeneous bounded coefficients.

Result: Proved existence of time-periodic, real-valued breather solutions that are localized perpendicular to the waveguide and travel along one direction of the waveguide. Provided examples of material coefficients (including nonperiodic materials) where the required spectral gap exists.

Conclusion: Breathers can exist in Kerr-type optical slab waveguides with retarded nonlinear responses when the linear operator has a spectral gap, expanding the understanding of nonlinear wave propagation in complex optical materials.

Abstract: We consider Maxwell's equations for Kerr-type optical materials, which are
magnetically inactive and have a nonlinear response to electric fields. This
response consists of a linear plus a cubic term, which are both inhomogeneous
with bounded coefficients. The cubic term is temporally retarded while the
linear term has instantaneous and retarded contributions. For slab waveguides
we show existence of breathers, which are time-periodic, real-valued solutions
that are localized in the direction perpendicular to the waveguide, and
moreover they are traveling along one direction of the waveguide. We find these
breathers using a variational method which relies on the assumption that an
effective operator related to the linear part of Maxwell's equations has a
spectral gap about $0$. We also give examples of material coefficients,
including nonperiodic materials, where such a spectral gap is present.

</details>


### [29] [A local and nonlocal coupling model involving the $p$-Laplacian](https://arxiv.org/abs/2508.20951)
*Uriel Kaufmann,Raúl Vidal*

Main category: math.AP

TL;DR: Extends previous results to p-Laplacian operator, coupling local and nonlocal operators through source terms, establishing existence and uniqueness via energy minimization.


<details>
  <summary>Details</summary>
Motivation: To generalize existing mathematical results from previous work to the more complex p-Laplacian operator case, specifically combining local and nonlocal operators.

Method: Couples local p-Laplacian operator with nonlocal p-Laplacian operator through source terms, analyzes the resulting energy functional, and uses direct minimization approach.

Result: Establishes existence and uniqueness of solutions for the coupled p-Laplacian system.

Conclusion: Successfully extends previous mathematical framework to p-Laplacian case with proven existence and uniqueness results through energy functional minimization.

Abstract: In this paper we extend some results presented in \cite{julio} to the case of
the $p$-Laplacian operator. More precisely, we consider a model that couples a
local $p$-Laplacian operator with a nonlocal $p$-Laplacian operator through
source terms in the equation. The resulting problem is associated with an
energy functional. We establish the existence and uniqueness of a solution,
which is obtained via the direct minimization of the corresponding energy
functional.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [30] [Self-consistent clustering analysis for homogenisation of heterogeneous plates](https://arxiv.org/abs/2508.20446)
*Menglei Li,Haolin Li,Bing Wang,Bing Wang*

Main category: physics.comp-ph

TL;DR: A reduced-order model for periodic micro-structured plates using self-consistent clustering analysis coupled with Lippmann-Schwinger equation for efficient multiscale homogenization.


<details>
  <summary>Details</summary>
Motivation: To enable rapid multiscale homogenization of heterogeneous plate structures with periodic micro-structures while maintaining accuracy and significantly reducing computational costs compared to traditional methods.

Method: Developed a plate-specific SCA scheme with offline-online strategy combining Green's functions with k-means data compression, and online self-consistent update exploiting weak sensitivity of reference medium. Handles both linear and nonlinear problems in classical plate theory and first-order shear deformation theory.

Result: The model matches FFT-based direct numerical simulation accuracy while reducing computational cost by over an order of magnitude across all tested cases including linear isotropic perforated plates, woven composites, and nonlinear elasto-plastic cases with damage.

Conclusion: The proposed reduced-order model successfully enables efficient and accurate multiscale analysis of heterogeneous plate structures, making it suitable for practical engineering applications requiring rapid homogenization of complex micro-structured plates.

Abstract: This work introduces a reduced-order model for plate structures with periodic
micro-structures by coupling self-consistent clustering analysis (SCA) with the
Lippmann-Schwinger equation, enabling rapid multiscale homogenisation of
heterogeneous plates. A plate-specific SCA scheme is derived for the first time
and features two key elements: (i) an offline-online strategy that combines
Green's functions with k-means data compression, and (ii) an online
self-consistent update that exploits the weak sensitivity of the reference
medium. The framework handles both linear and nonlinear problems in classical
plate theory and first-order shear deformation theory, and its performance is
verified on linear isotropic perforated plates and woven composites, as well as
on non-linear elasto-plastic perforated plates and woven composites with
damage. Across all cases the proposed model matches the accuracy of FFT-based
direct numerical simulation while reducing computational cost by over an order
of magnitude.

</details>


### [31] [Spectral density characteristics of self-organized structuring in phase-synchronized oscillator ensembles](https://arxiv.org/abs/2508.21012)
*Magnus F Ivarsen*

Main category: physics.comp-ph

TL;DR: Self-organized turbulence emerges from complex interactions rather than linear instabilities, with oscillators showing universal scaling laws in spectral density that match natural observations.


<details>
  <summary>Details</summary>
Motivation: To understand how spontaneous structures form in nature through complexity rather than traditional linear instability theory, particularly in astrophysical and geophysical plasmas.

Method: Simulating ensembles of oscillators with phase synchronization through propagating Kuramoto-interaction fields to analyze spectral characteristics of self-organized structures.

Result: The spectral density of emergent structures exhibits universal scaling laws consistent with natural observations, showing complex physical interactions can achieve outcomes through self-organization principles.

Conclusion: Spontaneously generated structures provide alternative explanations to reductionist approaches for understanding structure formation in coupled plasma systems.

Abstract: Self-organized turbulence represents a way for structuring in nature to arise
through sheer complexity rather than through linear instability theory.
Simulating ensembles of oscillators that undergo phase synchronization through
a propagating Kuramoto-interaction field, we present the spectral
characteristics of spontaneous, self-organized structures of locally coupled
oscillators. We demonstrate that the spectral density of emergent structures
can exhibit universal scaling laws, in line with expectations from nature,
indicating that observed statistical outcomes of complex physical interactions
can be achieved through a more general principle of self-organization. We
suggest that spontaneously generated structures may provide nuance to the
reigning reductionist explanations for the observed structure in coupled
systems of astrophysical and geophysical plasmas.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [32] [Isotope Enrichment and Element Separation by Self Generated Magnetic Centrifuge Fields in Ultrafast Laser Ablation](https://arxiv.org/abs/2508.20299)
*Peter P. Pronko,Paul A. Van Rompay*

Main category: physics.plasm-ph

TL;DR: A model explaining nickel isotope enrichment in laser ablation plumes through magnetic centrifuge effects and cyclotron rotation of plasma ions.


<details>
  <summary>Details</summary>
Motivation: To explain the anomalously large enrichment of nickel isotopes observed in ablation plumes from ultrafast laser irradiation of solid surfaces.

Method: Developed a self-consistent model based on spontaneous creation of a magnetic centrifuge in the ablation plume, cyclotron rotation of plasma ions (~10^9 rad/s), and Ion Bernstein Wave coupling for resonant orbits.

Result: Extracted Gaussian radial magnetic field distribution for Ni isotopes with peak field of 53 Megagauss and average of 20 Megagauss. Found cyclotron rotations dominate over hydrodynamic rotation, with profound enrichment resonance for specific charge states.

Conclusion: Cyclotron rotation of ions in the magnetic centrifuge is the dominant mechanism for nickel isotope enrichment, with IBW coupling providing electrostatic acceleration to resonant orbits.

Abstract: A self-consistent model is developed to explain the anomalously large
enrichment of nickel isotopes observed in ablation plumes from ultrafast laser
irradiation of solid surfaces. The model is based on the spontaneous creation
of a magnetic centrifuge in the ablation plume and the associated cyclotron
rotation of plasma ions with rotation rates on the order of 10^9 radians per
second. Mass separation occurs around the radial coordinate of cylindrical
symmetry with longitudinal axis normal to the ablating surface. A Gaussian
shaped radial magnetic field distribution is extracted for Ni isotopes with
peak field of 53 Megagauss and average field of 20 Megagauss. In addition to
cyclotron rotation of ions, a rigid rotor model is also presented that is
associated with the hydrodynamic rotation of the entire plasma and is shown to
be of little consequence for the isotope enrichment. Cyclotron rotations
dominate the process. A profound resonance of enrichment is observed for
specific cyclotron charge states and is shown to be associated with Ion
Bernstein Wave (IBW) coupling that provides electrostatic acceleration to these
resonant orbits.

</details>


### [33] [Efficient ion re-acceleration in laboratory-produced interpenetrating collisionless shocks](https://arxiv.org/abs/2508.20303)
*W. Yao,I. Cohen,P. Suarez Gerona,H. Ahmed,A. F. A. Bott,S. N. Chen,M. Cook,R. Lelièvre,P. Martin,T. Waltenspiel,P. Antici,J. Béard,M. Borghesi,D. Caprioli,A. Ciardi,E. d'Humières,M. François,L. Gremillet,A. Marcowith,M. Miceli,T. Seebaruth,S. Orlando,J. Fuchs*

Main category: physics.plasm-ph

TL;DR: Laser experiment shows colliding magnetized shocks boost proton energy and efficiency, supporting cosmic ray acceleration theories.


<details>
  <summary>Details</summary>
Motivation: To test the hypothesis that shock-shock collisions in stellar clusters could explain high-energy cosmic ray spectrum through additional particle acceleration.

Method: Laser-based experiment creating magnetized plasma conditions similar to astrophysical environments, combined with numerical kinetic simulations.

Result: Interpenetrating collisionless shocks significantly boost energy of ambient protons and improve overall acceleration efficiency; protons are reaccelerated via bouncing motion in convective electric fields.

Conclusion: Colliding-shock platform provides controlled laboratory setting to test diffusive shock acceleration mechanism for cosmic ray origins.

Abstract: Although the origin of cosmic rays (CRs) remains an open question,
collisionless magnetized shock waves are widely regarded as key sites for
particle acceleration. Recent theories further suggest that shock-shock
collisions in stellar clusters could provide the additional acceleration needed
to explain the observed high-energy CR spectrum. Here, we investigate this
hypothesis through a laser-based experiment that creates magnetized plasma
conditions similar to astrophysical environments. Our results demonstrate that
interpenetrating collisionless shocks can significantly boost the energy of
ambient protons previously energized by the individual shocks, while also
improving the overall acceleration efficiency. Numerical kinetic simulations
corroborate these findings, revealing that protons are reaccelerated via their
bouncing motion in the convective electric fields of the colliding magnetized
flows. By allowing to highly energize ambient protons, our novel
colliding-shock platform opens the prospect to test the long-discussed
mechanism of diffusive shock acceleration in a controlled laboratory setting.

</details>


### [34] [Assessment of the Runaway Electrons induced damage to the Tokamak First Wall](https://arxiv.org/abs/2508.20502)
*L. Singh,M. De Bastiani,R. Bonifetto,F. Subba,D. Borgogno*

Main category: physics.plasm-ph

TL;DR: Comparison of Beryllium vs Tungsten damage from Runaway Electrons using FLUKA simulations and thermal modeling to analyze penetration depth and melting effects.


<details>
  <summary>Details</summary>
Motivation: To assess and compare the damage caused by Runaway Electrons on First Wall tiles made of Beryllium and Tungsten materials in fusion reactors.

Method: Used FLUKA code with realistic RE energy distributions from ASDEX Upgrade experiment to simulate impacts, combined with finite element thermal modeling in FreeFem++ to analyze thermal response and melting.

Result: Found clear relationship between beam impact angle and deposited energy - higher angles lead to deeper electron penetration and greater energy deposition. Different RE current values significantly influence material temperature evolution and melting thickness.

Conclusion: The study provides quantitative analysis of RE damage on fusion reactor wall materials, demonstrating Tungsten and Beryllium's different responses to electron penetration and thermal loads, which is crucial for first wall design and material selection.

Abstract: The study assessed the damage caused by Runaway Electrons (RE) on First Wall
tiles, comparing the effects on Beryllium and Tungsten. This was done by using
realistic RE energy distribution functions to replicate RE impacts through the
FLUKA code. These energy distribution functions are based on the ASDEX Upgrade
experiment # 39012. The parametric analysis carried out with FLUKA in the
presence of magnetic fields indicated a clear relationship between the beam
impact angle and the material deposited energy, demonstrating that higher
impact angles lead to deeper electron penetration and greater deposited
energies. A finite element model based on apparent heat capacity formulation in
FreeFem++ was developed to analyze the material thermal response to such
thermal loads using volumetric energy density profiles from FLUKA simulations
as input. Different RE current values were simulated to show its influence on
the evolution of the material temperature and melting thickness

</details>


### [35] [Monte Carlo simulation method for incoherent Thomson scattering spectra from arbitrary electron distribution functions](https://arxiv.org/abs/2508.20627)
*Kentaro Sakai,Kentaro Tomita,Takeo Hoshi,Ryo Yasuhara*

Main category: physics.plasm-ph

TL;DR: Monte Carlo simulation method for calculating incoherent Thomson scattering spectra in high-temperature plasmas using macro-particles and random sampling from electron distribution functions.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient computational method for calculating incoherent Thomson scattering spectra that can handle arbitrary electron distribution functions while reducing computational costs.

Method: Treats scattering as superposition of individual photon-electron interactions, uses macro-particles from particle-in-cell simulations to reduce computational cost, and randomly samples electron velocities from distribution functions.

Result: Simulated spectra for relativistic Maxwellian and kappa distribution functions show good agreement with both analytical and numerical results, validating the method's reliability.

Conclusion: The Monte Carlo simulation method can reliably reproduce incoherent Thomson scattering spectra and is applicable to arbitrary electron distribution functions with appropriate sampling schemes.

Abstract: We developed a Monte Carlo simulation method to calculate incoherent Thomson
scattering spectra in high temperature plasmas. The basic idea is to treat the
entire scattering process as the superposition of individual photon-electron
interactions. We introduce macro-particles, referred from particle-in-cell
simulations, to reduce the computational cost, and obtain scattered spectra
within a reasonable computational time. Since the velocity of the interacting
electron is randomly sampled from an electron distribution function, the method
can be applied to arbitrary electron distribution functions provided an
appropriate sampling scheme is available. We present simulation results for
relativistic Maxwellian and kappa distribution functions, and compare them with
both analytical and numerical spectra for validation. The simulated spectra
show good agreement with both analytical and numerical results, demonstrating
that the Monte Carlo simulation method can reliably reproduce incoherent
Thomson scattering spectra.

</details>


### [36] [Automated simulation-based design via multi-fidelity active learning and optimisation for laser direct drive implosions](https://arxiv.org/abs/2508.20878)
*A. J. Crilly,P. W. Moloney,D. Shi,E. A. Ferdinandi*

Main category: physics.plasm-ph

TL;DR: Machine learning framework using neural network surrogates trained on 1D and 2D simulations to optimize inertial fusion designs robust to hydrodynamic instabilities, achieving high gain for laser drivers.


<details>
  <summary>Details</summary>
Motivation: Inertial fusion design requires balancing computational efficiency (1D simulations) with accuracy in capturing hydrodynamic instabilities (expensive 2D simulations). Current 1D designs are computationally cheap but don't account for instabilities, while 2D simulations are too expensive for extensive design exploration.

Method: Developed ensemble neural network surrogate models trained on both 1D and 2D simulation data to capture robust design space. Used Bayesian optimization with the surrogate to find optimal designs for 25 kJ laser driver, then performed hydrodynamic scaling to 2 MJ driver.

Result: Successfully found optimal designs that are robust to hydrodynamic instabilities. Achieved high gain for 2 MJ laser driver using 2D simulations that include alpha heating effects.

Conclusion: The machine learning framework effectively bridges the gap between computationally cheap 1D simulations and expensive 2D simulations, enabling efficient design of inertial fusion experiments that are robust to hydrodynamic instabilities while maintaining computational feasibility.

Abstract: The design of inertial fusion experiments is a complex task as driver energy
must be delivered in a precise manner to a structured target to achieve a fast,
but hydrodynamically stable, implosion. Radiation-hydrodynamics simulation
codes are an essential tool in this design process. However, multi-dimensional
simulations that capture hydrodynamic instabilities are more computationally
expensive than optimistic, 1D, spherically symmetric simulations which are
often the primary design tool. In this work, we develop a machine learning
framework that aims to effectively use information from a large number of 1D
simulations to inform design in the presence of hydrodynamic instabilities. We
use an ensemble of neural network surrogate models trained on both 1D and 2D
data to capture the space of good designs, i.e. those that are robust to
hydrodynamic instabilities. We use this surrogate to perform Bayesian
optimisation to find optimal designs for a 25 kJ laser driver. We perform
hydrodynamic scaling on these designs to confirm the achievement of high gain
for a 2 MJ laser driver, using 2D simulations including alpha heating effects.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [37] [Re4: Scientific Computing Agent with Rewriting, Resolution, Review and Revision](https://arxiv.org/abs/2508.20729)
*Ao Cheng,Lei Zhang,Guowei He*

Main category: cs.AI

TL;DR: A collaborative agent framework using three LLMs (Consultant, Reviewer, Programmer) with a 'rewriting-resolution-review-revision' chain for scientific computing problems, improving bug-free code generation and reducing non-physical solutions.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of generating reliable, bug-free code for scientific computing problems from natural language descriptions using LLMs, overcoming limitations of single-model approaches.

Method: Three-agent framework: Consultant rewrites problems with domain knowledge, Programmer generates executable code, and Reviewer provides iterative feedback for self-debugging and refinement through a review mechanism.

Result: Significantly improved bug-free code generation rate and reduced non-physical solutions compared to single-model approaches. Enhanced execution success rate for latest reasoning models in solving PDEs, ill-conditioned linear systems, and data-driven physical analysis.

Conclusion: The collaborative agent framework establishes automatic code generation and review as a promising paradigm for scientific computing, providing highly reliable autonomous code generation based on natural language descriptions.

Abstract: Large language models (LLMs) serve as an active and promising field of
generative artificial intelligence and have demonstrated abilities to perform
complex tasks in multiple domains, including mathematical and scientific
reasoning. In this work, we construct a novel agent framework for solving
representative problems in scientific computing. The proposed agent,
incorporating a "rewriting-resolution-review-revision" logical chain via three
reasoning LLMs (functioning as the Consultant, Reviewer, and Programmer,
respectively), is integrated in a collaborative and interactive manner. The
Consultant module endows the agent with knowledge transfer capabilities to link
problems to professional domain insights, thereby rewriting problem
descriptions through text augmentation. The Programmer module is responsible
for generating and executing well-structured code to deliver the problem
resolution. The Reviewer module equips the agent with the capacity for
self-debugging and self-refinement through interactive feedback with code
runtime outputs. By leveraging the end-to-end review mechanism, the executable
code provided by the Programmer attains the iterative revision. A comprehensive
evaluation is conducted on the performance of the proposed agent framework in
solving PDEs, ill-conditioned linear systems, and data-driven physical analysis
problems. Compared to single-model, this collaborative framework significantly
improves the bug-free code generation rate and reduces the occurrence of
non-physical solutions, thereby establishing a highly reliable framework for
autonomous code generation based on natural language descriptions. The review
mechanism improved the average execution success (bug-free code and non-NaN
solutions) rate of the latest reasoning models. In summary, our agent framework
establishes automatic code generation and review as a promising scientific
computing paradigm.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [38] [Enhanced premelting at the ice-rubber interface using all-atom molecular dynamics simulation](https://arxiv.org/abs/2508.20448)
*Takumi Kojima,Ikki Yasuda,Takumi Sato,Noriyoshi Arai,Kenji Yasuoka*

Main category: cond-mat.soft

TL;DR: Molecular dynamics simulations reveal how hydrophobic rubber enhances ice premelting by disrupting water structure while suppressing water mobility through confinement, creating a mixed rubber-water interface near melting point.


<details>
  <summary>Details</summary>
Motivation: Understanding the molecular tribology of ice-rubber interfaces is critical for applications like tires and shoe outsoles, but the fundamental mechanisms remain unclear.

Method: All-atom molecular dynamics simulations of styrene-butadiene rubber in contact with basal face of ice across temperatures from 254 to 269 K.

Result: Rubber enhances structural disorder of interfacial water (promoting premelting) but suppresses water mobility through confinement. Near melting point, rubber chains penetrate premelting layer forming mixed rubber-water region that couples dynamics.

Conclusion: Nanoscale roughness and morphology of hydrophobic polymers disrupt ice hydrogen-bond networks to enhance premelting, providing insights for designing polymer materials with controlled ice adhesion and friction.

Abstract: The ice-rubber interface is critical in applications such as tires and shoe
outsoles, yet its molecular tribology remains unclear. Using all-atom molecular
dynamics simulations, we studied premelting layers at the basal face of ice in
contact with styrene-butadiene rubber from 254 to 269 K. Despite its
hydrophobicity, rubber enhances structural disorder of interfacial water,
promoting premelting. In contrast, water mobility is suppressed by confinement
from polymer chains, leading to glassy dynamics distinct from the ice-vapor
interface. Near the melting point, rubber chains become more flexible and
penetrate the premelting layer, forming a mixed rubber-water region that
couples the dynamics of both components. These results suggest that nanoscale
roughness and morphology of hydrophobic polymers disrupt ice hydrogen-bond
networks, thereby enhancing premelting. Our findings provide molecular-level
insight into ice slipperiness and inform the design of polymer materials with
controlled ice adhesion and friction.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [39] [Bohr--Sommerfeld rules for systems](https://arxiv.org/abs/2508.21013)
*Simon Becker,Setsuro Fujiié,Jens Wittsten*

Main category: math-ph

TL;DR: Complete formulation of Bohr-Sommerfeld quantization rule for semiclassical 2x2 systems with eigenvalue crossings, providing geometric phase corrections.


<details>
  <summary>Details</summary>
Motivation: To address eigenvalue crossings in Dirac-type operators and extend Bohr-Sommerfeld quantization to 2x2 systems with crossings within closed phase space curves.

Method: Building on scalar Bohr-Sommerfeld rules and semiclassical treatments of Harper operator, deriving additional contributions to quantization conditions for general self-adjoint 2x2 systems.

Result: Derived concise expressions that give explicit geometric phase corrections and clarify when these phases take quantized values.

Conclusion: Provides a complete formulation of quantization rules for 2x2 systems with eigenvalue crossings, offering geometric insights into phase corrections and quantization conditions.

Abstract: We present a complete, self-contained formulation of the Bohr--Sommerfeld
quantization rule for a semiclassical self-adjoint $2 \times 2$ system on the
real line, arising from a simple closed curve in phase space. We focus on the
case where the principal symbol exhibits eigenvalue crossings within the domain
enclosed by the curve -- a situation commonly encountered in Dirac-type
operators. Building on earlier work on scalar Bohr--Sommerfeld rules and
semiclassical treatments of the Harper operator near rational flux quanta, we
identify additional contributions to the quantization condition, and derive
concise expressions for general self-adjoint $2 \times 2$ systems. The
resulting formulas give explicit geometric phase corrections and clarify when
these phases take quantized values.

</details>


<div id='physics.space-ph'></div>

# physics.space-ph [[Back]](#toc)

### [40] [On the Regulation of the Solar Wind Helium Abundance by the Hydrogen Compressibility](https://arxiv.org/abs/2508.20391)
*B. L. Alterman,R. D'Amicis*

Main category: physics.space-ph

TL;DR: Solar wind helium abundance behavior differs between fast and slow wind, with unexpected large gradients in fast wind linked to compressive fluctuations and non-Alfvenic cross helicity.


<details>
  <summary>Details</summary>
Motivation: To understand why helium abundance gradients unexpectedly increase in fast solar wind with decreasing cross helicity, challenging traditional solar wind origin models.

Method: Analysis of solar wind compressibility (|δn/n|) and cross helicity (|σc|) relationships with helium abundance gradients, comparing results with Alterman & D'Amicis (2025) findings.

Result: Found two subsets of enhanced helium abundance: one with large compressibility likely from non-standard sources, and established upper bounds (|σc| ≲ 0.65 for non-Alfvenic, |δn/n| ≲ 0.15 for incompressible fluctuations).

Conclusion: Compressibility is essential for characterizing solar wind helium abundance and may regulate it, revealing new insights beyond traditional solar wind origin models.

Abstract: Traditionally, fast solar wind is considered to originate in solar source
regions that are continuously open to the heliosphere and slow wind originates
in regions that are intermittently open to it. In fast wind, the gradient of
the solar wind helium abundance ($A_\mathrm{He}$) with increasing solar wind
speed ($v_\mathrm{sw}$) is $\sim0$ and $A_\mathrm{He}$ is fixed at $\sim50\%$
of the photospheric value. In slow wind, this gradient is large,
$A_\mathrm{He}$ is highly variable, and it doesn't exceed this $\sim50\%$
value. Although the normalized cross helicity in fast wind typically approaches
1, this is not universally true and Alterman & D'Amicis (2025) show that
$\nabla_{v_\mathrm{sw}} \! A_\mathrm{He}$ in fast wind unexpectedly increases
with decreasing $\left|\sigma_c\right|$. We show that these large gradients are
due to the presence of compressive fluctuations. Accounting for the solar
wind's compressibility ($\left|\delta n/n\right|$), there are two subsets of
enhanced $A_\mathrm{He}$ in excess of typical fast wind values. The subset with
a large compressibility is likely from neither continuously nor intermittently
open sources. The portion of the solar wind speed distribution over which these
fluctuations are most significant corresponds to the range of Alfv\'en
wave-poor solar wind from continuously open source regions, which is likely
analogous to the Alfv\'enic slow wind. Mapping the results of this work to
Alterman & D'Amicis (2025) and vice versa shows that, in any given
$\left|\delta n/n\right|$ quantile, $\left|\sigma_c\right| \lesssim 0.65$, an
upper bound on non-Alfv\'enic cross helicity. Similarly, $\left|\delta
n/n\right| \lesssim 0.15$ in any given $\left|\sigma_c\right|$ quantile, is an
upper bound on incompressible fluctuations. We conclude that $\left|\delta
n/n\right|$ is essential for characterizing the solar wind helium abundance and
possibly regulating it.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [41] [Altermagnetic Shastry-Sutherland fullerene networks](https://arxiv.org/abs/2508.21056)
*Jiaqi Wu,Alaric Sanders,Rundong Yuan,Bo Peng*

Main category: cond-mat.mtrl-sci

TL;DR: Carbon fullerene C40 monolayers form altermagnetic Shastry-Sutherland lattice with tunable quantum phases including spin liquid states accessible via strain.


<details>
  <summary>Details</summary>
Motivation: To explore molecular building blocks for realizing exotic quantum phases, using charge-neutral carbon fullerene molecules to design altermagnetic monolayers with tunable quantum magnetism.

Method: Designed altermagnetic C40 monolayers in Shastry-Sutherland lattice configuration, utilizing resonance structure of unpaired electrons to create effective spin-1/2 clusters, and analyzed electronic band structure and magnon bands.

Result: Achieved d-wave splitting of spin-polarized electronic band structure, strong chiral-split magnon bands, and demonstrated tunable phase transitions between altermagnetic, quantum spin liquid, plaquette, and dimer phases via moderate bi-axial strains.

Conclusion: Magnetic fullerene monolayers serve as a versatile and tunable platform for exotic quantum magnetism and spintronic applications, with easily accessible quantum phases through strain engineering.

Abstract: Molecular building blocks provide a versatile platform for realising exotic
quantum phases. Using charge neutral, pure carbon fullerene molecules as an
example, we design altermagnetic C$_{40}$ monolayers in Shastry-Sutherland
lattice. The resonance structure of one unpaired electron leads to an effective
spin-1/2 cluster on both long sides of the molecule, which, after rotating into
a 2D rutile-like crystal structure, forms altermagnetic ground state. We show
$d$-wave spitting of the spin-polarised electronic band structure and strong
chiral-split magnon bands. Most interestingly, the effective spin-1/2 clusters
form the Shastry-Sutherland model with a rich phase diagram including
altermagenti, quantum spin liquid, plaquette, and dimer phases, which can be
easily accessed to via moderate bi-axial strains. Our findings present magnetic
fullerene monolayers as a tunable platform for exotic quantum magnetism and
spintronic applications.

</details>


### [42] [Ultrafast solid-state chemical synthesis of BaTiO3 initiated by gyrotron microwave radiation](https://arxiv.org/abs/2508.20714)
*S. V. Sintsov,N. V. Chekmarev,K. I. Rybakov,A. A. Sorokin,E. I. Preobrazhenskii,A. V. Vodopyanov*

Main category: cond-mat.mtrl-sci

TL;DR: Microwave synthesis of barium titanate using 24 GHz gyrotron achieves 90% yield in 1.5-7 minutes through localized thermal instabilities.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient solid-state synthesis method for barium titanate using continuous microwave radiation to significantly reduce reaction time and improve yield.

Method: Used 24 GHz gyrotron in multimode cavity reactor with stoichiometric mixture of ultrafine barium carbonate and titanium dioxide powders. Developed numerical model solving Maxwell and heat conduction equations to analyze power absorption.

Result: Synthesis completed in 1.5-7 minutes with up to 90% product yield. Specific absorbed power reached 670 W/cm³ under 400 W input power in localized thermal instability domains.

Conclusion: Continuous microwave radiation enables rapid, high-yield barium titanate synthesis through localized thermal effects, with numerical modeling confirming efficient power absorption in instability zones.

Abstract: This work presents the results of a study on the solid-state synthesis of
barium titanate under continuous microwave radiation from a 24 GHz gyrotron in
a multimode cavity reactor. It is shown that in localized domains where
fine-scale thermal instabilities develop, initiated by microwave radiation
within the initial stoichiometric reaction mixture of ultrafine barium
carbonate and titanium dioxide powders, the synthesis can proceed within 1,5 -
7 minutes, achieving a target product yield of up to 90%. Based on a developed
realistic numerical model of the multimode reactor, involving an iterative
solution of stationary Maxwell and heat conduction equations, it is
demonstrated that the specific absorbed power in the domains where fine-scale
thermal instabilities develop can reach 670 W/cm3 under an input microwave
power of 400 W.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [43] [Characterizing the Impact of Alfvén Wave Forcing in Interplanetary Space on the Distribution of near-Earth Solar Wind Speeds](https://arxiv.org/abs/2504.18350)
*B. L. Alterman*

Main category: astro-ph.SR

TL;DR: Solar wind's near-Sun kinetic energy flux can predict fastest speeds observed near Earth, with Alfvén wave forcing identified as the primary acceleration mechanism during interplanetary propagation.


<details>
  <summary>Details</summary>
Motivation: Early solar wind acceleration models cannot account for the fastest non-transient solar wind speeds observed near-Earth, requiring identification of the energy deposition mechanisms and their dependence on source region topology.

Method: Used near-Sun observations of solar wind's kinetic energy flux to predict expected kinetic energy flux near-Earth, applied a recently developed model of solar wind evolution in the inner heliosphere.

Result: The solar wind's near-Sun kinetic energy flux is sufficient to predict the distribution of fastest non-transient speeds observed near Earth, with acceleration primarily due to continuous Alfvén wave forcing during propagation.

Conclusion: Alfvén wave forcing during solar wind transit is a consequence of source region topology, allowing statistical mapping of near-Earth observations to their source regions using solar wind's Alfvénicity.

Abstract: Broadly, solar wind source regions can be classified by their magnetic
topology as intermittently and continuously open to the heliosphere. Early
models of solar wind acceleration do not account for the fastest, non-transient
solar wind speeds observed near-Earth and energy must be deposited into the
solar wind after it leaves the Sun. Alfv\'en wave energy deposition and thermal
pressure gradients are likely candidates and the relative contribution of each
acceleration mechanism likely depends on the source region. Although solar wind
speed is a rough proxy for solar wind source region, it cannot unambiguously
identify source region topology.
  Using near-Sun observations of the solar wind's kinetic energy flux, we
predict the expected kinetic energy flux near-Earth. This predicted kinetic
energy flux corresponds to the range of solar wind speeds observed in the fast
solar wind and infer that the solar wind's near-Sun kinetic energy flux is
sufficient to predict the distribution of fastest, non-transient speeds
observed near Earth. Applying a recently developed model of solar wind
evolution in the inner heliosphere, we suggest that the acceleration required
to generate this distribution of fastest, non-transient speeds is likely due to
the continuous deposition of energy by Alfv\'en wave forcing during the solar
wind's propagation through interplanetary space. We infer that the solar wind's
Alfv\'enicity can statistically map near-Earth observations to their source
regions because the Alfv\'en wave forcing that the solar wind experiences in
transit is a consequence of the source region topology.

</details>


<div id='math.CA'></div>

# math.CA [[Back]](#toc)

### [44] [Monodromy Equivalence for Lamé-type Equations I: Finite-gap Structures and Cone Spherical Metrics](https://arxiv.org/abs/2508.20486)
*Ting-Jung Kuo,Xuanpu Liang,Ping-Hsiang Wu*

Main category: math.CA

TL;DR: Analysis of generalized Lamé-type equations showing monodromy equivalence with classical Lamé equation, leading to finite-gap structure classification and applications in constructing cone spherical metrics with large conical singularities.


<details>
  <summary>Details</summary>
Motivation: Motivated by the finite-gap structure of the classical Lamé equation and its importance in mathematical physics, the paper investigates generalized Lamé-type equations to establish deeper connections and applications.

Method: Establishes monodromy equivalence between classical Lamé equation and generalized Lamé-type equation for the fundamental case n=1, then applies this equivalence to derive finite-gap structures and classify spectral curves.

Result: Obtained complete classification of spectral curves σ₁ and σ₂ for τ∈iℝ>0, and constructed a family of cone spherical metrics with three large conical singularities (each with cone angle >2π) that exhibits an explicitly described blow-up configuration.

Conclusion: The monodromy equivalence provides a powerful tool for understanding finite-gap structures of generalized Lamé-type equations and enables explicit construction and analysis of cone spherical metrics with specific geometric properties.

Abstract: Motivated by the finite-gap structure of the classical Lam\'{e} equation
(1.2) and its central role in mathematical physics, generalized Lam\'{e}-type
equations (1.12) are investigated. For the fundamental case $n=1$, a monodromy
equivalence between the classical Lam\'{e} equation (1.18) and the generalized
Lam\'{e}-type equation (1.19) is established. Two main applications are
obtained: (i) the finite-gap structure of \ (1.19) is derived, together with a
complete classification of the spectral curves $\sigma_{1}$ and $\sigma_{2}$
for $\tau\in i\mathbb{R}_{>0}$; and (ii) the monodromy equivalence is applied
to the construction of cone spherical metrics with three large conical
singularities, each with cone angle exceeding $2\pi$. A family of such metrics
is shown to exhibits a blow-up configuration, which is described explicitly in
terms of the monodromy data.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [45] [Ising energy model for the stochastic prediction of tumor islets](https://arxiv.org/abs/2508.20804)
*Lucas Amoudruz,Gregory Buti,Luciano Rivetti,Ali Ajdari,Gregory Sharp,Petros Koumoutsakos,Simon Spohn,Anca L Grosu,Thomas Bortfeld*

Main category: physics.med-ph

TL;DR: A physics-based Ising model for prostate cancer tumor spread that simulates cell movement, duplication, and death to predict tumor islet distribution and shapes observed in histology.


<details>
  <summary>Details</summary>
Motivation: Cancer diagnosis faces challenges with microscopic tumor infiltration invisible on standard imaging, requiring histological detection. There's a need for models that can predict tumor spread patterns for treatment planning.

Method: Developed an Ising-like Hamiltonian model with three elementary events (cell movement, duplication, death) capturing neighbor correlations. Parameters fitted to clinical data from 23 prostate cancer patients.

Result: The model effectively describes tumor islet size and number distributions, producing regularly shaped spherical islets that mimic histological observations due to Ising interaction acting as surface tension.

Conclusion: This straightforward physical model successfully captures histological tumor spread patterns and can calculate tumor involvement probabilities in specific prostate sub-volumes for radiation treatment planning applications.

Abstract: A major challenge in diagnosing and treating cancer is the infiltrative
growth of tumors into surrounding tissues.
  This microscopic spread of the disease is invisible on most diagnostic
imaging modalities and can often only be detected histologically in biopsies.
  The purpose of this paper is to develop a physically based model of tumor
spread that captures the histologically observed behavior in terms of seeding
small tumor islets in prostate cancer.
  The model is based on three elementary events: a tumor cell can move,
duplicate, or die.
  The propensity of each event is given by an Ising-like Hamiltonian that
captures correlations between neighboring cells.
  The model parameters were fitted to clinical data obtained from surgical
specimens taken from 23 prostate cancer patients.
  The results demonstrate that this straightforward physical model effectively
describes the distribution of the size and the number of tumor islets in
prostate cancer.
  The simulated tumor islets exhibit a regular, approximately spherical shape,
correctly mimicking the shapes observed in histology.
  This is due to the Ising interaction term between neighboring cells acting as
a surface tension that gives rise to regularly shaped islets.
  The model addresses the important clinical need of calculating the
probability of tumor involvement in specific sub-volumes of the prostate, which
is required for radiation treatment planning and other applications.

</details>


### [46] [Start-to-end modelling of laser-plasma acceleration, beam transport and dose deposition of very high-energy electrons for radiotherapy](https://arxiv.org/abs/2508.20678)
*Rajakrishna Kalvala,Anton Golovanov,Arnaud Courvoisier,Tomer Friling,Eyal Kroupp,Lidan Grishko,Victor Malka*

Main category: physics.med-ph

TL;DR: Study demonstrates that polychromatic very high-energy electron beams from laser-plasma accelerators can deliver precise radiotherapy doses when properly collimated and arranged in multifield arrays.


<details>
  <summary>Details</summary>
Motivation: To explore the feasibility of using laser-plasma accelerator-generated VHEE beams for radiotherapy, addressing limitations of traditional photon-based radiotherapy with enhanced dose distribution capabilities.

Method: Used particle-in-cell simulations for laser-plasma interaction to obtain realistic electron beams, filtered and collimated them with beamlines, arranged into pencil beam arrays, and conducted GEANT4 simulations for dose deposition in water and heterogeneous phantoms with multifield irradiation setups.

Result: Polychromatic VHEE beams from laser-plasma accelerators, when collimated using compact beamline of quadrupoles and dipoles, can deliver on-axis dose with enhanced precision and uniformity.

Conclusion: Laser-plasma accelerators show transformative potential for advancing radiotherapy modalities, paving the way for further research and clinical implementation.

Abstract: Radiotherapy using very high-energy electron (VHEE) beams generated by a
laser-plasma accelerator has garnered significant interest due to its dose
distribution capabilities and potential to address limitations of traditional
photon-based radiotherapy. To explore the feasibility of such approach, the
presented study uses the parameters of OONA, the commercial <1.3\,J, <25\,fs
pulse duration laser recently installed at the Weizmann Institute of Science.
Through particle-in-cell simulations of laser-plasma interaction, realistic
electron beams were obtained. After filtering and collimation with a beamline
and arranging them into an array of pencil beams, their radiotherapeutic
potential was investigated. GEANT4 simulations were used to calculate the dose
deposition in water phantoms and heterogeneous phantoms with bone inserts,
considering realistic beam parameters for laser-plasma accelerators. Multifield
irradiation setup and the dose distribution at the isocenter through different
incidence angles were studied, simulating an intensity-modulated delivery. Our
findings demonstrate that polychromatic VHEE beams generated from laser-plasma
accelerators, when collimated using a compact beamline of quadrupoles and
dipoles, can deliver an on-axis dose with enhanced precision and uniformity.
This study highlights the transformative potential of laser-plasma accelerators
in advancing radiotherapy modalities, paving the way for further research and
clinical implementation.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [47] [Spectral Gaps with Quantum Counting Queries and Oblivious State Preparation](https://arxiv.org/abs/2508.21002)
*Almudena Carrera Vazquez,Aleksandros Sobczyk*

Main category: quant-ph

TL;DR: Quantum algorithm for approximating spectral gaps with logarithmic qubits and quadratic complexity in N, providing speed-up over classical methods for large gaps.


<details>
  <summary>Details</summary>
Motivation: Approximating spectral gaps is crucial for many scientific and engineering applications, but classical algorithms have high complexity (O(N^ω)). Quantum computing offers potential speed-up.

Method: Uses quantum algorithm with random initial state preparation to efficiently count eigenvalues below threshold. Operates in QRAM model with polylogarithmic complexity factors.

Result: Achieves additive error εΔ_k approximation with complexity O(N²/ε²Δ_k² polylog(N,1/Δ_k,1/ε,1/δ)), providing speed-up over classical O(N^ω) for large spectral gaps.

Conclusion: Quantum algorithm enables efficient spectral gap approximation with quadratic complexity in N, demonstrating quantum advantage for this important eigenproblem with applications across science and engineering.

Abstract: Approximating the $k$-th spectral gap $\Delta_k=|\lambda_k-\lambda_{k+1}|$
and the corresponding midpoint $\mu_k=\frac{\lambda_k+\lambda_{k+1}}{2}$ of an
$N\times N$ Hermitian matrix with eigenvalues
$\lambda_1\geq\lambda_2\geq\ldots\geq\lambda_N$, is an important special case
of the eigenproblem with numerous applications in science and engineering. In
this work, we present a quantum algorithm which approximates these values up to
additive error $\epsilon\Delta_k$ using a logarithmic number of qubits.
Notably, in the QRAM model, its total complexity (queries and gates) is bounded
by $O\left( \frac{N^2}{\epsilon^{2}\Delta_k^2}\mathrm{polylog}\left(
N,\frac{1}{\Delta_k},\frac{1}{\epsilon},\frac{1}{\delta}\right)\right)$, where
$\epsilon,\delta\in(0,1)$ are the accuracy and the success probability,
respectively. For large gaps $\Delta_k$, this provides a speed-up against the
best-known complexities of classical algorithms, namely, $O \left(
N^{\omega}\mathrm{polylog} \left(
N,\frac{1}{\Delta_k},\frac{1}{\epsilon}\right)\right)$, where $\omega\lesssim
2.371$ is the matrix multiplication exponent. A key technical step in the
analysis is the preparation of a suitable random initial state, which
ultimately allows us to efficiently count the number of eigenvalues that are
smaller than a threshold, while maintaining a quadratic complexity in $N$. In
the black-box access model, we also report an $\Omega(N^2)$ query lower bound
for deciding the existence of a spectral gap in a binary (albeit non-symmetric)
matrix.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [48] [A Deep-Learning Enhanced Gappy Proper Orthogonal Decomposition Method for Conjugate Heat Transfer Problem](https://arxiv.org/abs/2508.20633)
*Arash Hajisharifi,Rahul Halder,Michele Girfoglio,Giovanni Stabile,Gianluigi Rozza*

Main category: physics.flu-dyn

TL;DR: Developed a non-intrusive ROM using ANN-enhanced Gappy POD method to reconstruct temperature fields in refrigerators, achieving 1°C prediction error and 5000x speed-up with sparse training data.


<details>
  <summary>Details</summary>
Motivation: To create an efficient reduced order model for large-scale industrial applications that can accurately reconstruct full temperature fields from limited sensor data, addressing computational challenges in complex thermal systems.

Method: Combined Artificial Neural Network with Gappy Proper Orthogonal Decomposition (GPOD) method, validated using Conjugated Heat Transfer simulations and experimental data from a domestic refrigerator under various operating conditions.

Result: Achieved prediction accuracy within 1 degree centigrade and computational speed-up of 5000 times compared to full-order models, even with very sparse training datasets.

Conclusion: The ANN-enhanced GPOD approach outperforms conventional GPOD methods for large-scale industrial applications, providing accurate temperature field reconstruction with significant computational efficiency gains.

Abstract: The current study aims to develop a non-intrusive Reduced Order Model (ROM)
to reconstruct the full temperature field for a large-scale industrial
application based on both numerical and experimental datasets. The proposed
approach is validated against a domestic refrigerator. At the full order level,
air circulation and heat transfer in fluid and between fluid and surrounding
solids in the fridge were numerically studied using the Conjugated Heat
Transfer (CHT) method to explore both the natural and forced convection-based
fridge model followed by a parametric study-based on the ambient temperature,
fridge fan velocity, and evaporator temperature. The main novelty of the
current work is the introduction of a stable Artificial Neural Network (ANN)
enhanced Gappy Proper Orthogonal Decomposition (GPOD) method which shows better
performance than the conventional GPOD approach in such large-scale industrial
applications. The full-order model is validated with the experimental results
and the prediction accuracy of the surrogate model associated with different
reduced-order approaches is compared with the benchmark numerical results or
high-fidelity results. In our current work, we show that a prediction error of
one degree centigrade and computational speed-up of 5000 is achieved even at a
very sparse training dataset using the proposed deep-learning enhanced GPOD
approach.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [49] [Machine learning topological defect formation](https://arxiv.org/abs/2508.20347)
*Fumika Suzuki,Ying Wai Li,Wojciech H. Zurek*

Main category: cond-mat.stat-mech

TL;DR: Machine learning can predict final topological defect configurations from short time evolution data near critical points, following Kibble-Zurek scaling laws.


<details>
  <summary>Details</summary>
Motivation: The Kibble-Zurek mechanism suggests defect formation is determined early in phase transitions, motivating whether ML can predict final defect patterns from early-stage order parameter evolution.

Method: Using a Recurrent Neural Network to analyze the time evolution of the order parameter over short intervals near the critical point during second-order phase transitions.

Result: ML successfully predicts final topological defect configurations based on early time evolution data, and this predictability follows the same power law scaling as described by the Kibble-Zurek mechanism.

Conclusion: Machine learning can accurately forecast the final configuration of topological defects from limited early-stage data, demonstrating that defect formation patterns are established shortly after crossing the critical point.

Abstract: According to the Kibble-Zurek mechanism (KZM), the density of topological
defects created during a second-order phase transition is determined by the
correlation length at the freeze-out time. This suggests that the final
configuration of topological defects in such a transition is largely
established during the impulse regime, soon after the critical point is
traversed. Motivated by this, we conjecture that machine learning (ML) can
predict the final configuration of topological defects based on the time
evolution of the order parameter over a short interval in the vicinity of the
critical point, well before the order parameter settles into the emerging new
minima resulting from spontaneous symmetry breaking. Furthermore, we show that
the predictability of ML also follows the power law scaling dictated by KZM. We
demonstrate these using a Recurrent Neural Network.

</details>


### [50] [Lee-Yang-zero ratio method in three-dimensional Ising model](https://arxiv.org/abs/2508.20422)
*Tatsuya Wada,Masakiyo Kitazawa,Kazuyuki Kanaya*

Main category: cond-mat.stat-mech

TL;DR: The paper applies the Lee-Yang-zero ratio method to determine the critical point in the 3D Ising model, showing it's as effective as the Binder-cumulant method but with better suppression of finite-size scaling violations and non-linearity.


<details>
  <summary>Details</summary>
Motivation: To develop and validate the Lee-Yang-zero ratio method as an alternative approach for precisely locating critical points in statistical physics models, particularly the 3D Ising model.

Method: Monte Carlo simulations of the three-dimensional Ising model using the Lee-Yang-zero ratio method, with comparisons to the conventional Binder-cumulant method. Also proposes an alternative single Lee-Yang zero method.

Result: The LYZR method proves equally powerful as Binder-cumulant for critical point determination, with advantages in suppressing finite-size scaling violations and non-linearity near criticality. Precise universal values of LYZRs at critical point achieved.

Conclusion: The Lee-Yang-zero ratio method is a robust alternative to traditional approaches for critical point analysis, offering improved performance characteristics while maintaining accuracy, with potential applications across various statistical physics models.

Abstract: By performing Monte Carlo simulations of the three-dimensional Ising model,
we apply the recently proposed Lee-Yang-zero ratio (LYZR) method to determine
the location of the critical point in this model. We demonstrate that the LYZR
method is as powerful as the conventional Binder-cumulant method in studying
the critical point, while the LYZR method has the advantage of suppressing the
violation of the finite-size scaling and non-linearity near the critical point.
We also achieve a precise determination of the values of the LYZRs at the
critical point, which are universal numbers. In addition, we propose an
alternative method that uses only a single Lee-Yang zero and show that it is
also useful for the search for the critical point.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [51] [Objective Value Change and Shape-Based Accelerated Optimization for the Neural Network Approximation](https://arxiv.org/abs/2508.20290)
*Pengcheng Xie,Zihao Zhou,Zijian Zhou*

Main category: cs.LG

TL;DR: This paper introduces VC (value change), a novel metric to measure neural network approximation difficulty and characterize local performance. It reveals VC-tendency and minority-tendency phenomena in error evolution, and proposes a variation-based distance metric and preprocessing framework for neural network approximation.


<details>
  <summary>Details</summary>
Motivation: Neural networks often suffer from unpredictable local performance that hinders reliability in critical applications. There is a need for quantifiable measures to understand local value changes and stability in neural network approximation.

Method: The authors propose VC metric to measure local value changes, investigate its theoretical properties, identify VC-tendency and minority-tendency phenomena, develop a variation-based distance metric, and create a preprocessing framework for neural network approximation.

Result: Numerical results from real-world experiments and PDE-related scientific problems support the discovered phenomena and demonstrate the effectiveness of the proposed preprocessing acceleration method.

Conclusion: VC provides a valuable metric for characterizing neural network approximation behavior, revealing important trends in error evolution, and enabling improved preprocessing methods that enhance approximation performance and reliability.

Abstract: This paper introduce a novel metric of an objective function f, we say VC
(value change) to measure the difficulty and approximation affection when
conducting an neural network approximation task, and it numerically supports
characterizing the local performance and behavior of neural network
approximation. Neural networks often suffer from unpredictable local
performance, which can hinder their reliability in critical applications. VC
addresses this issue by providing a quantifiable measure of local value changes
in network behavior, offering insights into the stability and performance for
achieving the neural-network approximation. We investigate some fundamental
theoretical properties of VC and identified two intriguing phenomena in neural
network approximation: the VC-tendency and the minority-tendency. These trends
respectively characterize how pointwise errors evolve in relation to the
distribution of VC during the approximation process.In addition, we propose a
novel metric based on VC, which measures the distance between two functions
from the perspective of variation. Building upon this metric, we further
propose a new preprocessing framework for neural network approximation.
Numerical results including the real-world experiment and the PDE-related
scientific problem support our discovery and pre-processing acceleration
method.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [52] [The Hasse Principle for Geometric Variational Problems: An Illustration via Area-minimizing Submanifolds](https://arxiv.org/abs/2508.21045)
*Zhenhua Liu*

Main category: math.DG

TL;DR: The paper extends the Hasse principle from number theory to area-minimizing submanifolds, showing that integral homology information can be recovered from real and mod n homology data.


<details>
  <summary>Details</summary>
Motivation: To apply the Hasse principle concept from Diophantine equations to geometric variational problems, specifically area-minimizing submanifolds, to uncover new properties and behaviors.

Method: By establishing that the Hasse principle holds for area-minimizing submanifolds, allowing reconstruction of integral homology information from real and mod n homology solutions.

Result: Several surprising conclusions: area-minimizing submanifolds in mod n homology are asymptotically much smoother than expected, and they are not generically calibrated.

Conclusion: The Hasse principle likely applies to all geometric variational problems formulated on chain spaces over different coefficients, including Almgren-Pitts min-max, mean curvature flow, and other minimization problems.

Abstract: The Hasse principle in number theory states that information about integral
solutions to Diophantine equations can be pieced together from real solutions
and solutions modulo prime powers. We show that the Hasse principle holds for
area-minimizing submanifolds: information about area-minimizing submanifolds in
integral homology can be fully recovered from those in real homology and mod
$n$ homology for all $n\in \mathbb{Z}_{\ge 2}.$ As a consequence we derive
several surprising conclusions, including: area-minimizing submanifolds in mod
$n$ homology are asymptotically much smoother than expected and area-minimizing
submanifolds are not generically calibrated. We conjecture that the Hasse
principle holds for all geometric variational problems that can be formulated
on chain space over different coeffiicients, e.g., Almgren-Pitts min-max, mean
curvature flow, Song's spherical Plateau problem, minimizers of elliptic and
other general functionals, etc.

</details>
