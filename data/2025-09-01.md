<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 14]
- [math.AP](#math.AP) [Total: 9]
- [physics.comp-ph](#physics.comp-ph) [Total: 3]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 2]
- [quant-ph](#quant-ph) [Total: 2]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 2]
- [math.OC](#math.OC) [Total: 1]
- [math.FA](#math.FA) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [math.PR](#math.PR) [Total: 1]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [physics.app-ph](#physics.app-ph) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Preconditioned pseudo-time continuation for parameterized inverse problems](https://arxiv.org/abs/2508.21155)
*Joseph Hart,Alen Alexanderian,Bart van Bloemen Waanders*

Main category: math.NA

TL;DR: A method using pseudo-time continuation with adaptive quasi-Newton Hessian preconditioning to efficiently solve parametrized variational inverse problems constrained by PDEs for uncertainty quantification.


<details>
  <summary>Details</summary>
Motivation: To address the computational challenge of solving multiple optimization problems for different auxiliary parameter values in PDE-constrained inverse problems, which is crucial for uncertainty quantification but traditionally requires solving many separate optimization problems.

Method: Leverages pseudo-time continuation to solve an initial value problem that evolves the optimal solution along an auxiliary parameter path, using an adaptive quasi-Newton Hessian preconditioner that exploits properties of the pseudo-time continuation process.

Result: The proposed preconditioner achieves reliable and efficient computation for parametrized variational inverse problems, as demonstrated through two nonlinear inverse problems.

Conclusion: The adaptive quasi-Newton Hessian preconditioner combined with pseudo-time continuation provides an effective framework for accelerating the solution of PDE-constrained inverse problems across varying parameter values, enabling more efficient uncertainty quantification.

Abstract: We consider parametrized variational inverse problems that are constrained by
partial differential equations (PDEs). We seek to efficiently compute the
solution of the inverse problem when auxiliary model parameters, which appear
in the governing PDE, are varied. Computing the solution of the inverse problem
for different auxiliary parameter values is crucial for uncertainty
quantification. This, however, is computationally challenging since it requires
solving many optimization problems for different realizations of the auxiliary
parameters. We leverage pseudo-time continuation and solve an initial value
problem to evolve the optimal solution along an auxiliary parameter path. This
article introduces the use of an adaptive quasi-Newton Hessian preconditioner
to accelerate the computation. Our proposed preconditioner exploits properties
of the pseudo-time continuation process to achieve reliable and efficient
computation. We elaborate our proposed framework and elucidate its properties
for two nonlinear inverse problems.

</details>


### [2] [Entropy stable finite difference (ESFD) methods via entropy correction artificial viscosity (ECAV) and knapsack limiting (KL) techniques](https://arxiv.org/abs/2508.21226)
*Brian Christner,Jesse Chan*

Main category: math.NA

TL;DR: This paper introduces knapsack limiting and artificial viscosity techniques for finite difference discretizations, creating high-order accurate, entropy stable, and hyperparameter-free schemes that preserve positivity for compressible Euler and Navier-Stokes equations.


<details>
  <summary>Details</summary>
Motivation: Entropy stable methods provide robust computational fluid dynamics solutions by satisfying discrete entropy inequalities (2nd law of thermodynamics), but existing approaches need adaptation for finite difference discretizations to achieve high-order accuracy, entropy stability, and positivity preservation without performance tradeoffs.

Method: The authors apply two recent strategies to finite difference discretizations: 1) knapsack limiting - blending low-order (positivity preserving, entropy stable) with high-order schemes, and 2) entropy correction artificial viscosity - adding viscosity to satisfy cell entropy inequality in high-order schemes.

Result: The proposed finite difference schemes achieve high-order accuracy in smooth conditions, maintain entropy stability, are hyperparameter-free, and the knapsack limiting scheme provably preserves positivity for compressible Euler and Navier-Stokes equations without significant performance degradation compared to state-of-the-art stabilized schemes.

Conclusion: The successful adaptation of knapsack limiting and artificial viscosity techniques to finite difference discretizations provides robust, high-order accurate, entropy stable, and positivity preserving computational fluid dynamics methods that operate without hyperparameter tuning and maintain competitive performance.

Abstract: Entropy stable methods have become increasingly popular in the field of
computational fluid dynamics. They often work by satisfying some form of a
discrete entropy inequality: a discrete form of the 2nd law of thermodynamics.
Schemes which satisfy a (semi-)discrete entropy inequality typically behave
much more robustly, and do so in a way that is hyperparameter free. Recently, a
new strategy was introduced to construct entropy stable discontinuous Galerkin
methods: knapsack limiting, which blends together a low order, positivity
preserving, and entropy stable scheme with a high order accurate scheme, in
order to produce a high order accurate, entropy stable, and positivity
preserving scheme. Another recent strategy introduces an entropy correction
artificial viscosity into a high order scheme, aiming to satisfy a cell entropy
inequality.
  In this work, we introduce the techniques of knapsack limiting and artificial
viscosity for finite difference discretizations. The proposed schemes preserve
high order accuracy in sufficiently smooth conditions, are entropy stable, and
are hyperparameter free. Moreover, the proposed knapsack limiting scheme
provably preserves positivity for the compressible Euler and Navier-Stokes
equations. Both schemes achieve this goal without significant performance
tradeoffs compared to state of the art stabilized schemes.

</details>


### [3] [Machine-precision energy conservative quadrature hyperreduction of Lagrangian hydrodynamics](https://arxiv.org/abs/2508.21279)
*Chris Vales,Siu Wun Cheung,Dylan M. Copeland,Youngsoo Choi*

Main category: math.NA

TL;DR: Energy conservative quadrature-based model reduction for compressible Euler equations using data-driven reduced basis and hyperreduction with exact discrete total energy conservation.


<details>
  <summary>Details</summary>
Motivation: To develop a structure-preserving reduced order model for Lagrangian hydrodynamics that maintains exact energy conservation while achieving computational efficiency.

Method: Projection-based reduced model using data-driven reduced basis functions and hyperreduction via Empirical Quadrature Procedure (EQP), with a strongly energy conservative variant that enforces exact discrete total energy conservation.

Result: Numerical experiments on four benchmark problems show total energy conservation to near machine precision while maintaining accuracy comparable to basic EQP formulation.

Conclusion: The energy conservative EQP (CEQP) method is an effective structure-preserving hyperreduction strategy for reduced simulation of nonlinear Lagrangian hydrodynamics.

Abstract: We present an energy conservative, quadrature based model reduction framework
for the compressible Euler equations of Lagrangian hydrodynamics. Building on a
high order finite element discretization of the governing equations, we develop
a projection based reduced model using data driven reduced basis functions and
hyperreduction via the empirical quadrature procedure (EQP). We introduce a
strongly energy conservative variant of EQP that enforces exact discrete total
energy conservation during the hyperreduction process. Numerical experiments
for four benchmark problems -- Sedov blast, Gresho vortex, triple point and
Taylor-Green vortex -- demonstrate that the numerical implementation of our
proposed method conserves total energy to near machine precision while
maintaining accuracy comparable to the basic EQP formulation. These results
establish the energy conservative EQP (CEQP) method as an effective structure
preserving hyperreduction strategy for the reduced simulation of nonlinear
Lagrangian hydrodynamics.

</details>


### [4] [Generalized quantum singular value transformation with application in quantum bi-conjugate gradient method](https://arxiv.org/abs/2508.21390)
*Yu-Qiu Liu,Hefeng Wang,Hua Xiang*

Main category: math.NA

TL;DR: This paper extends generalized quantum signal processing (GQSP) to handle general matrices (called GQSVT) and applies it to develop a quantum bi-conjugate gradient (BiCG) algorithm with shallow circuit depth and minimal ancilla qubits.


<details>
  <summary>Details</summary>
Motivation: Quantum signal processing (QSP) has parity restrictions on achievable polynomials, while GQSP removes these restrictions but only works with unitary matrices. The authors aim to extend GQSP to general matrices and demonstrate practical applications.

Method: The authors extend GQSP to handle general matrices, calling it generalized quantum singular value transformation (GQSVT). They then implement a quantum BiCG method using GQSVT and swap test techniques.

Result: The developed quantum BiCG algorithm achieves relatively shallow circuit depth and requires only a small number of ancilla qubits, making it more practical for quantum computing implementations.

Conclusion: The extension of GQSP to GQSVT enables efficient quantum algorithms for general matrices, with the quantum BiCG method serving as a practical application that demonstrates improved circuit efficiency and reduced resource requirements.

Abstract: Quantum signal processing (QSP) and generalized quantum signal processing
(GQSP) are essential tools for implementing the block encoding of matrix
functions. The achievable polynomials of QSP have restrictions on parity, while
GQSP eliminates these restrictions. In this paper, we further investigate GQSP
and present a quantum bi-conjugate gradient (BiCG) algorithm as an application.
First, we extend GQSP, which constructs functions of unitary matrices, to
general matrices. We refer to this extension as generalized quantum singular
value transformation (GQSVT). Subsequently, we implement the quantum BiCG
method, utilizing GQSVT and swap test, which has a relatively shallow circuit
depth and requires a small number of ancilla qubits.

</details>


### [5] [A Biologically Motivated Finite Difference Approach for Simulating Singularly Perturbed Vertical Motion in Human Gait](https://arxiv.org/abs/2508.21410)
*Shubhangini Gupta,Sourav Banerjee,Tamal Pramanick*

Main category: math.NA

TL;DR: A simulation-based numerical method for solving singularly perturbed second-order differential equations from human gait modeling, using domain decomposition and time-rescaling to handle boundary layers with second-order accuracy.


<details>
  <summary>Details</summary>
Motivation: Standard numerical methods fail to resolve boundary layer behavior in singularly perturbed differential equations from human gait models that include gravity, damping, and leg stiffness factors.

Method: Domain decomposition technique dividing the problem into inner and outer regions, time-rescaling transformation for inner region, mixed finite difference framework with Thomas algorithm for solving tridiagonal systems.

Result: Second-order accuracy in space demonstrated through convergence analysis, with validated accuracy, stability, and efficiency through experiments on modified human gait models.

Conclusion: The framework serves as a fundamental tool for biomechanical simulation and provides foundation for future research incorporating nonlinearities, time delays, and real-world walking data.

Abstract: In this study, we present a simulation-based numerical method for solving a
class of singularly perturbed second-order differential equations that come
from a simplified biologically motivated model of human gait. Important
physical factors such as gravity, damping, and leg stiffness are included in
the model, which also depicts the vertical motion of the center of mass of the
body during walking or running. Most of the time, standard numerical methods
are ineffective in resolving boundary layer behavior that occurs due to the
small perturbation parameter in the governing equation. We use a domain
decomposition technique to divide the problem domain into inner and outer
regions to tackle this difficulty. The boundary layer resolves the steep
gradients. We applied a time-rescaling transformation to the inner region. Each
subdomain is discretized, and the resulting tridiagonal systems are efficiently
solved using the Thomas algorithm within the mixed finite difference framework.
A detailed convergence analysis demonstrates second-order accuracy in space.
The numerical results validate the proposed scheme's accuracy, stability, and
efficiency through experiments based on modified human gait models. The
framework serves as a fundamental tool for biomechanical simulation. The
modeling is a foundation for future research, incorporating nonlinearities,
time delays, and real-world scenarios data on how people walk.

</details>


### [6] [Computing Radially-Symmetric Solutions of the Ultra-Relativistic Euler Equations with Entropy-Stable Discontinuous Galerkin Methods](https://arxiv.org/abs/2508.21427)
*Ferdinand Thein,Hendrik Ranocha*

Main category: math.NA

TL;DR: Derived entropy-stable flux for ultra-relativistic Euler equations using main field/entropy variables and potentials, with 2D/3D simulation results.


<details>
  <summary>Details</summary>
Motivation: To address shock wave formation and pressure blow-up in ultra-relativistic gases by developing entropy-stable numerical methods for better simulation accuracy.

Method: Derived main field (entropy variables) and corresponding potentials, then developed entropy-stable flux formulation for ultra-relativistic Euler equations.

Result: Successfully implemented entropy-stable flux and demonstrated its performance through 2D and 3D simulations of various test cases.

Conclusion: The developed entropy-stable flux provides improved numerical stability for simulating ultra-relativistic Euler equations, particularly for problems involving shock waves and pressure blow-up.

Abstract: The ultra--relativistic Euler equations describe gases in the relativistic
case when the thermal energy dominates. These equations for an ideal gas are
given in terms of the pressure, the spatial part of the dimensionless
four-velocity, and the particle density. Kunik et al.\ (2024,
https://doi.org/10.1016/j.jcp.2024.113330) proposed genuine multi--dimensional
benchmark problems for the ultra--relativistic Euler equations. In particular,
they compared full two-dimensional discontinuous Galerkin simulations for
radially symmetric problems with solutions computed using a specific
one-dimensional scheme. Of particular interest in the solutions are the
formation of shock waves and a pressure blow-up. In the present work we derive
an entropy-stable flux for the ultra--relativistic Euler equations. Therefore,
we derive the main field (or entropy variables) and the corresponding
potentials. We then present the entropy-stable flux and conclude with
simulation results for different test cases both in 2D and in 3D.

</details>


### [7] [Inverse Random Source Problem for the Helmholtz Equation from Statistical Phaseless Data](https://arxiv.org/abs/2508.21478)
*Qiao-Ping Chen,Hongyu Liu,Zejun Sun,Li-Li Wang,Guang-Hui Zheng*

Main category: math.NA

TL;DR: This paper addresses non-uniqueness in reconstructing random sources from phaseless Helmholtz equation data using reference sources, phase retrieval formulas, and Bayesian methods with proven stability.


<details>
  <summary>Details</summary>
Motivation: To overcome the fundamental challenge of non-uniqueness in reconstructing random sources from statistical phaseless measurements in the 2D Helmholtz equation, which is crucial for applications where only intensity data is available.

Method: Introduces artificial point sources as references, derives phase retrieval formulas for field statistics, analyzes uniqueness/stability, formulates Fredholm integral equations for inverse problem, and employs Bayesian methods for uncertainty quantification.

Result: The method successfully reconstructs random sources from phaseless data, with rigorous proofs of uniqueness and stability for both phase retrieval and inverse problems, validated by numerical experiments.

Conclusion: The proposed reference source technique combined with Bayesian inference provides an effective and stable approach for solving the inverse random source problem with phaseless data, overcoming the inherent non-uniqueness challenge.

Abstract: This paper investigates the problem of reconstructing a random source from
statistical phaseless data for the two-dimensional Helmholtz equation. The
major challenge of this problem is non-uniqueness, which we overcome through a
reference source technique. Firstly, we introduce some artificially added point
sources into the inverse random source system and derive phase retrieval (PR)
formulas for the expectation and variance of the radiated fields. This paper
rigorously analyze the uniqueness and stability of the recovered statistics of
the radiated fields. Afterwards, since the direct problem has a unique mild
solution, by examining the expectation and variance of this solution and
combined with the phase retrieval formulas, we derive the Fredholm integral
equations to solve the inverse random source problem (IRSP). We prove the
stability of the corresponding integral equations. To quantify the uncertainty
of the random source, we utilize the Bayesian method to reconstruct the random
source and establish the well-posedness of the posterior distribution. Finally,
numerical experiments demonstrate the effectiveness of the proposed method and
validate the theoretical results.

</details>


### [8] [The Derivative of Kemeny's Constant as a Centrality Measure in Undirected Graphs](https://arxiv.org/abs/2508.21506)
*Dario A. Bini,Beatrice Meini,Federico Poloni*

Main category: math.NA

TL;DR: This paper introduces directional derivatives of Kemeny's constant to define edge/non-edge centrality measures and sensitivity analysis for graph connectivity.


<details>
  <summary>Details</summary>
Motivation: To develop quantitative measures that assess how individual edges and non-edges affect graph connectivity as measured by Kemeny's constant, which captures the average random walk time between vertices.

Method: Introduces two concepts of directional derivative of Kemeny's constant with respect to edges, provides explicit expressions using the inverse of modified graph Laplacian, develops algorithms for computation, and validates on road networks and link prediction.

Result: Successfully defines centrality measures for edges and non-edges, establishes connections to existing measures, provides computational algorithms, and demonstrates applications including handling cut-edges and one-path graphs with explicit weight expressions.

Conclusion: The proposed directional derivative framework provides effective tools for analyzing edge importance in graph connectivity, with practical applications in network analysis and link prediction, particularly valuable for understanding sensitivity to edge modifications.

Abstract: Kemeny's constant quantifies a graph's connectivity by measuring the average
time for a random walker to reach any other vertex. We introduce two concepts
of the directional derivative of Kemeny's constant with respect to an edge and
use them to define centrality measures for edges and non-edges in the graph.
Additionally, we present a sensitivity measure of Kemeny's constant. An
explicit expression for these quantities involving the inverse of the modified
graph Laplacian is provided, which is valid even for cut-edges. These measures
are connected to the one introduced in [Altafini et al., SIMAX 2023], and
algorithms for their computation are included. The benefits of these measures
are discussed, along with applications to road networks and link prediction
analysis. For one-path graphs, an explicit expression for these measures is
given in terms of the edge weights.

</details>


### [9] [Random domain decomposition for parabolic PDEs on graphs](https://arxiv.org/abs/2508.21557)
*Martín Hernández*

Main category: math.NA

TL;DR: RBM applied to parabolic PDEs on graphs without discretization, using randomized domain decomposition. Proves mean-square convergence with first-order accuracy in step size. Shows significant memory and computation savings.


<details>
  <summary>Details</summary>
Motivation: Complex systems like gas pipeline networks require solving PDEs on graphs, which demands high computational and memory resources. RBM offers stochastic decomposition to address these challenges efficiently.

Method: Apply Random Batch Method at PDE level for parabolic equations on graphs without space/time discretization. Use non-overlapping domain decomposition with randomized coefficients and source terms.

Result: Proven mean-square convergence to true PDE solution with first-order accuracy in RBM step size. Numerical experiments confirm convergence rate and show substantial reductions in memory usage and computational time compared to full graph solving.

Conclusion: RBM provides an effective stochastic approach for solving parabolic PDEs on graphs, offering computational efficiency and memory savings while maintaining convergence properties across different time discretization schemes.

Abstract: The simulation of complex systems, such as gas transport in large pipeline
networks, often involves solving PDEs posed on intricate graph structures. Such
problems require considerable computational and memory resources. The Random
Batch Method (RBM) has shown promise in addressing these challenges via
stochastic decomposition techniques. In this paper, we apply the RBM at the PDE
level for parabolic equations on graphs, without assuming any preliminary
discretization in space or time. We consider a non-overlapping domain
decomposition in which the PDE coefficients and source terms are randomized. We
prove that the resulting RBM-based scheme converges, in the mean-square sense
and uniformly in time, to the true PDE solution with first-order accuracy in
the RBM step size. Numerical experiments confirm this convergence rate and
demonstrate substantial reductions in both memory usage and computational time
compared to solving on the full graph. Moreover, these advantages persist
across different time discretization schemes.

</details>


### [10] [Conforming and discontinuous discretizations of non-isothermal Darcy-Forchheimer flows](https://arxiv.org/abs/2508.21630)
*Stefano Bonetti,Michele Botti,Paola F. Antonietti*

Main category: math.NA

TL;DR: Two numerical schemes for Darcy-Forchheimer fluid flow coupled with advection-diffusion temperature modeling, using different discontinuous Galerkin approaches with fixed-point linearization for nonlinearities.


<details>
  <summary>Details</summary>
Motivation: To develop and analyze unified numerical discretization methods for coupled Darcy-Forchheimer fluid flow and temperature distribution problems, addressing nonlinearities through efficient computational approaches.

Method: Two approaches: 1) Fully discontinuous Galerkin discretization, 2) Raviart-Thomas space for velocity with discontinuous pressure/temperature. Both use fixed-point linearization strategy for nonlinear treatment and iterative splitting solution.

Result: Unified stability analysis shows convergence under mild data requirements. Extensive 2D/3D simulations demonstrate error decay and practical performance in physically sound test cases.

Conclusion: Both proposed numerical schemes effectively handle the coupled nonlinear problem with proven stability and convergence properties, showing practical applicability through comprehensive simulations.

Abstract: We present and analyze in a unified setting two schemes for the numerical
discretization of a Darcy-Forchheimer fluid flow model coupled with an
advection-diffusion equation modeling the temperature distribution in the
fluid. The first approach is based on fully discontinuous Galerkin
discretization spaces. In contrast, in the second approach, the velocity is
approximated in the Raviart-Thomas space, and the pressure and temperature are
still piecewise discontinuous. A fixed-point linearization strategy, naturally
inducing an iterative splitting solution, is proposed for treating the
nonlinearities of the problem. We present a unified stability analysis and
prove the convergence of the iterative algorithm under mild requirements on the
problem data. A wide set of two- and three-dimensional simulations is presented
to assess the error decay and demonstrate the practical performance of the
proposed approaches in physically sound test cases.

</details>


### [11] [Analogy between Learning With Error Problem and Ill-Posed Inverse Problems](https://arxiv.org/abs/2508.21653)
*Gaurav Mittal*

Main category: math.NA

TL;DR: The paper establishes an analogy between lattice-based learning with error (LWE) problems and ill-posed inverse problems, showing LWE is a structured inverse problem. It proposes symmetric and public key encryption schemes based on this connection.


<details>
  <summary>Details</summary>
Motivation: To explore the connection between LWE problems and ill-posed inverse problems, and leverage this analogy to develop new encryption schemes with provable security properties.

Method: 1. Demonstrate LWE as a structured inverse problem 2. Design symmetric encryption based on ill-posed problems 3. Construct public key encryption combining the symmetric scheme with CRYSTALS-Kyber KEM

Result: Established theoretical connection between LWE and inverse problems, proposed two encryption schemes with security analysis.

Conclusion: The analogy between LWE and ill-posed inverse problems provides a new perspective for cryptographic scheme design, enabling the development of encryption schemes with discussed security properties.

Abstract: In this work, we unveil an analogy between well-known lattice based learning
with error problem and ill-posed inverse problems. We show that LWE problem is
a structured inverse problem. Further, we propose a symmetric encryption scheme
based on ill-posed problems and thoroughly discuss its security. Finally, we
propose a public key encryption scheme based on our symmetric encryption scheme
and CRYSTALS-Kyber KEM (key encapsulation mechanism) and discuss its security.

</details>


### [12] [Quantitative evaluations of stability and convergence for solutions of semilinear Klein--Gordon equation](https://arxiv.org/abs/2508.21659)
*Takuya Tsuchiya,Makoto Nakamura*

Main category: math.NA

TL;DR: Simulation study of semilinear Klein-Gordon equation with power-law nonlinearity, proposing quantitative evaluation methods for stability and convergence of numerical solutions, and investigating thresholds through amplitude and mass variations.


<details>
  <summary>Details</summary>
Motivation: To develop reliable quantitative methods for assessing stability and convergence in numerical solutions of semilinear Klein-Gordon equations with power-law nonlinear terms, which are important in various physical applications.

Method: Performed simulations of the semilinear Klein-Gordon equation with power-law nonlinearity, varied initial value amplitude and mass parameters to investigate thresholds, and proposed quantitative evaluation methods for stability and convergence analysis.

Result: Developed quantitative evaluation methods for stability and convergence, identified thresholds through parameter variations, and proposed appropriate values for these thresholds based on simulation results.

Conclusion: The study successfully established quantitative methods for evaluating numerical solution stability and convergence in semilinear Klein-Gordon equations, providing practical threshold values through systematic parameter variation analysis.

Abstract: We perform some simulations of the semilinear Klein--Gordon equation with a
power-law nonlinear term and propose each of the quantitative evaluation
methods for the stability and convergence of numerical solutions. We also
investigate each of the thresholds in the methods by varying the amplitude of
the initial value and the mass, and propose appropriate values.

</details>


### [13] [Trajectory learning for ensemble forecasts via the continuous ranked probability score: a Lorenz '96 case study](https://arxiv.org/abs/2508.21664)
*Sagy Ephrati,James Woodfield*

Main category: math.NA

TL;DR: CRPS-based trajectory learning enables accurate and sharp ensemble forecasts using stochastic parametrizations in the Lorenz '96 system, outperforming derivative-fitting methods.


<details>
  <summary>Details</summary>
Motivation: To develop effective ensemble forecast methods using trajectory learning with CRPS as a loss function, particularly for data assimilation applications requiring accuracy over short lead times.

Method: Employed continuous ranked probability score (CRPS) as loss function for trajectory learning, developed and trained both additive and multiplicative stochastic parametrizations using the two-scale Lorenz '96 system as case study.

Result: CRPS-based trajectory learning produced parametrizations that are both accurate and sharp, straightforward to calibrate, and outperformed derivative-fitting-based parametrizations in short-term forecasts.

Conclusion: The approach shows promise for data assimilation applications due to its accuracy over short lead times, demonstrating feasibility of trajectory learning for ensemble forecasts using CRPS.

Abstract: This paper demonstrates the feasibility of trajectory learning for ensemble
forecasts by employing the continuous ranked probability score (CRPS) as a loss
function. Using the two-scale Lorenz '96 system as a case study, we develop and
train both additive and multiplicative stochastic parametrizations to generate
ensemble predictions. Results indicate that CRPS-based trajectory learning
produces parametrizations that are both accurate and sharp. The resulting
parametrizations are straightforward to calibrate and outperform
derivative-fitting-based parametrizations in short-term forecasts. This
approach is particularly promising for data assimilation applications due to
its accuracy over short lead times.

</details>


### [14] [Low-Rank Regularized Convex-Non-Convex Problems for Image Segmentation or Completion](https://arxiv.org/abs/2508.21765)
*Mohamed El Guide,Anas El Hachimi,Khalide Jbilou,Lothar Reichel*

Main category: math.NA

TL;DR: Convex-non-convex formulation for image segmentation and completion using low-rank and smoothness regularization, solved with ADMM.


<details>
  <summary>Details</summary>
Motivation: To develop an effective approach for image segmentation and completion that combines both low-rank structure promotion and smoothness enforcement in a unified optimization framework.

Method: Proposes a convex-non-convex functional minimization with two regularization terms (low-rank and smoothness), solved using alternating direction method of multipliers (ADMM) with convergence analysis.

Result: The method demonstrates performance through numerical experiments, showing effectiveness in image segmentation and completion tasks.

Conclusion: The proposed convex-non-convex formulation with ADMM provides an effective solution for image segmentation and completion problems with proven convergence properties.

Abstract: This work proposes a novel convex-non-convex formulation of the image
segmentation and the image completion problems. The proposed approach is based
on the minimization of a functional involving two distinct regularization
terms: one promotes low-rank structure in the solution, while the other one
enforces smoothness. To solve the resulting optimization problem, we employ the
alternating direction method of multipliers (ADMM). A detailed convergence
analysis of the algorithm is provided, and the performance of the methods is
demonstrated through a series of numerical experiments.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [15] [Propagation in the Fisher-KPP equation with Mixed Operator](https://arxiv.org/abs/2508.21151)
*Begoña Barrios,Bryan Pichucho,Alexander Quaas*

Main category: math.AP

TL;DR: Analysis of Fisher-KPP equation with mixed local-nonlocal diffusion operator, showing fractional Laplacian dominates classical Laplacian in determining propagation rates and tail behavior.


<details>
  <summary>Details</summary>
Motivation: To extend previous work on Fisher-KPP equations to mixed diffusion settings combining classical and fractional Laplacian operators, and to understand the long-time dynamics and spreading behavior of such equations.

Method: Construction and detailed study of heat kernel associated with mixed operator, development of mild solutions theory, establishment of comparison principle in weighted function spaces.

Result: Non-existence of traveling waves demonstrated, large-time spreading rate characterized. Fractional Laplacian dominates classical Laplacian, especially in initial layer where it dictates exponential propagation rate and solution tail thickness.

Conclusion: The fractional component of mixed diffusion operators plays a dominant role in determining the asymptotic spreading behavior and propagation characteristics of Fisher-KPP equations.

Abstract: Our investigation focuses on the asymptotic spreading behavior of the
Fisher-KPP equation with a mixed local-nonlocal operator in the diffusion (see
the work by X. Cabr\'e and J.-M. Roquejoffre, 2013, ref.[8]) to the setting of
mixed diffusion, which involves both the classical and the fractional Laplacian
in order to analyze the long-time dynamics of the equation. A key step in our
approach involves the construction and detailed study of the heat kernel
associated with the mixed operator, which we use to develop a theory of mild
solutions and establish a comparison principle in suitable weighted function
spaces.
  This framework allows us to rigorously establish the non-existence of
traveling waves and characterize the large-time spreading rate of solutions. We
show that the influence of the fractional Laplacian dominates over the
classical Laplacian, especially in the initial layer, where it dictates the
exponential propagation rate and the thickness of the solution tails.

</details>


### [16] [Asymptotic expansions for the transmission eigenvalues of periodic scatterers of bounded support](https://arxiv.org/abs/2508.21174)
*Fioralba Cakoni,Shari Moskow*

Main category: math.AP

TL;DR: First-order corrections for transmission eigenvalues in scatterers with periodic refractive index variations, using two-scale asymptotics for biharmonic homogenization with boundary correctors.


<details>
  <summary>Details</summary>
Motivation: To understand how periodic variations in refractive index affect transmission eigenvalues, which are important for inverse scattering problems and material characterization.

Method: Two-scale asymptotics applied to a fourth-order PDE with periodic coefficients, deriving boundary corrector functions for the biharmonic homogenization problem.

Result: Derived first-order corrections to transmission eigenvalues that incorporate boundary corrector functions, with convergence estimates established.

Conclusion: Periodic refractive index variations produce measurable corrections to transmission eigenvalues through boundary effects, providing analytical framework for inverse scattering applications.

Abstract: We consider the transmission eigenvalues for a bounded scatterer with a
periodically varying index of refraction, and derive the first order
corrections to the limiting transmission eigenvalues. We assume the scatterer
contrast to be of one sign, in which case the transmission eigenvalue problem
can be written in terms of operators corresponding to a fourth order PDE with
periodic coefficients. We perform two-scale asymptotics for this biharmonic
type homogenization problem and show convergence estimates which require a
boundary corrector function, and this boundary corrector function appears in
the formula for the transmission eigenvalues correction.

</details>


### [17] [Pattern formation and nonlinear waves close to a 1:1 resonant Turing and Turing--Hopf instability](https://arxiv.org/abs/2508.21183)
*Bastian Hilder,Christian Kuehn*

Main category: math.AP

TL;DR: Analysis of pattern-forming systems near simultaneous Turing and Turing-Hopf instabilities with 1:1 spatial resonance using coupled Swift-Hohenberg equations and amplitude equations.


<details>
  <summary>Details</summary>
Motivation: To understand the dynamics of pattern-forming systems when both Turing and Turing-Hopf instabilities occur simultaneously with the same critical wave number, which is a complex scenario in pattern formation.

Method: Used coupled Swift-Hohenberg equations with dispersive terms and general nonlinearities. Derived coupled complex Ginzburg-Landau equations with singular advection as amplitude equations. Applied geometric singular perturbation theory, center manifold reduction, and normal form theory.

Result: Successfully derived amplitude equations with error estimates, constructed space-time periodic solutions and fast-traveling front solutions connecting different patterns, and obtained global spatially periodic solutions.

Conclusion: The approach provides a framework for analyzing spatial transitions between different patterns in complex pattern-forming systems with simultaneous instabilities, with applications to various physical and biological systems.

Abstract: In this paper, we analyse the dynamics of a pattern-forming system close to
simultaneous Turing and Turing--Hopf instabilities, which have a 1:1 spatial
resonance, that is, they have the same critical wave number. For this, we
consider a system of coupled Swift--Hohenberg equations with dispersive terms
and general, smooth nonlinearities. Close to the onset of instability, we
derive a system of two coupled complex Ginzburg--Landau equations with a
singular advection term as amplitude equations and justify the approximation by
providing error estimates. We then construct space-time periodic solutions to
the amplitude equations, as well as fast-travelling front solutions, which
connect different space-time periodic states. This yields the existence of
solutions to the pattern-forming system on a finite, but long time interval,
which model the spatial transition between different patterns. The construction
is based on geometric singular perturbation theory exploiting the fast
travelling speed of the fronts. Finally, we construct global, spatially
periodic solutions to the pattern-forming system by using centre manifold
reduction, normal form theory and a variant of singular perturbation theory to
handle fast oscillatory higher-order terms.

</details>


### [18] [Quantitative estimates for the relative isoperimetric problem and its gradient flow outside convex bodies in the plane](https://arxiv.org/abs/2508.21198)
*Elena Mäder-Baumdicker,Robin Neumayer,Jiewon Park,Melanie Rupflin*

Main category: math.AP

TL;DR: Quantitative results for relative isoperimetric problems outside convex bodies in the plane, including Lojasiewicz estimates, gradient flow convergence rates, and stability for minimizers with explicit constants and optimal rates.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous quantitative analysis of relative isoperimetric problems outside convex bodies, providing explicit constants and optimal rates that hold under nondegeneracy conditions of a simple 2D variational problem.

Method: Proving three inter-related results: (1) Lojasiewicz estimates and quantitative rigidity for critical points, (2) rates of convergence for gradient flow, and (3) quantitative stability for minimizers. Uses a novel flow approach to prove stability for minimizers in isoperimetric problems.

Result: Obtained explicit constants and optimal exponents/rates for all three results. The proofs are interconnected and the approach works whenever a two-dimensional auxiliary variational problem for circular arcs outside the convex body is nondegenerate.

Conclusion: The paper provides comprehensive quantitative analysis of relative isoperimetric problems with explicit bounds and optimal rates, introducing a novel flow-based approach for proving stability of minimizers in this context for the first time.

Abstract: We prove three related quantitative results for the relative isoperimetric
problem outside a convex body $\Omega$ in the plane: (1) {\L}ojasiewicz
estimates and quantitative rigidity for critical points, (2) rates of
convergence for the gradient flow, and (3) quantitative stability for
minimizers. These results come with explicit constants and optimal
exponents/rates, and hold whenever a simple two-dimensional auxiliary
variational problem for circular arcs outside of $\Omega$ is nondegenerate. The
proofs are inter-related, and in particular, for the first time in the context
of isoperimetric problems, a flow approach is used to prove quantitative
stability for minimizers.

</details>


### [19] [Propagation of smallness near codimension two for gradients of harmonic functions](https://arxiv.org/abs/2508.21214)
*Benjamin Foster,Josep Gallegos*

Main category: math.AP

TL;DR: Harmonic functions with bounded gradient show propagation of smallness: if gradient is ε-small on a set with positive (n-2+δ)-dimensional Hausdorff content, then gradient remains small throughout half-ball with bound Cε^α.


<details>
  <summary>Details</summary>
Motivation: To improve upon previous results by Logunov and Malinnikova and achieve the sharp threshold for dimension of smallness sets from which propagation of smallness can occur for harmonic functions.

Method: Analysis of harmonic functions in unit ball with bounded gradient, studying how smallness of gradient on sets with specific Hausdorff dimension propagates throughout the domain.

Result: Proved that if gradient is ε-small on set E with positive (n-2+δ)-dimensional Hausdorff content, then sup|∇u| ≤ Cε^α in half-ball, with constants depending only on n, δ and Hausdorff content.

Conclusion: The result establishes optimal dimension threshold (n-2+δ) for propagation of smallness in harmonic functions, improving previous work and reaching sharp dimensional requirements.

Abstract: Let $u$ be a harmonic function in the unit ball $B_1 \subset \mathbb R^n$,
normalized so that its gradient has magnitude at most 1 on the unit ball. We
show that if the gradient of $u$ is $\epsilon$-small in size on a set $E\subset
B_{1/2}$ with positive $(n-2+\delta)$-dimensional Hausdorff content for some
$\delta>0$, then $\sup_{B_{1/2}} |\nabla u| \leq C \epsilon^\alpha$ with
$C,\alpha>0$ depending only on $n,\delta$ and the $(n-2+\delta)$-Hausdorff
content of $E$. This is an improvement over a similar result of Logunov and
Malinnikova that required $\delta>1-c_n$ for a small dimensional constant $c_n$
and reaches the sharp threshold for the dimension of the smallness sets from
which propagation of smallness can occur.

</details>


### [20] [1D quasi-solutions of the 2D Chern-Simons-Schr{ö}dinger system](https://arxiv.org/abs/2508.21464)
*Nicolas Rougerie,Qiyun Yang*

Main category: math.AP

TL;DR: Study of 2D abelian anyons using Schrodinger-Chern-Simons model with anisotropic trapping, leading to effective 1D quintic NLS equation.


<details>
  <summary>Details</summary>
Motivation: To understand the dynamics of 2D abelian anyon systems and derive simplified effective equations through dimensional reduction.

Method: Used a mean-field model combining Schrodinger matter field with Chern-Simons gauge field, applied strongly anisotropic trapping potential (wave-guide), and traced out the tight confinement direction to obtain 1D effective dynamics.

Result: Derived that the effective dynamics in the loose direction of the wave-guide is governed by the classical 1D quintic nonlinear Schrodinger (NLS) equation.

Conclusion: The dimensional reduction technique successfully transforms the complex 2D anyon system into a more tractable 1D quintic NLS equation, providing a simplified framework for studying anyon dynamics.

Abstract: We study a mean-field model for a system of 2D abelian anyons, given by the
dynamics of a Schr{\"o}dinger matter field coupled to a Chern-Simons gauge
field. We derive an effective 1D equation by adding a strongly anisotropic
trapping potential (wave-guide) acting on the Schr{\"o}dinger field, and
tracing out the tight confinement direction. The effective dynamics in the
loose direction of the wave-guide turns out to be governed by the classical 1D
quintic NLS equation.

</details>


### [21] [Sharp stability in hypercontractivity estimates and logarithmic Sobolev inequalities](https://arxiv.org/abs/2508.21552)
*Zoltán M. Balogh,Alexandru Kristály*

Main category: math.AP

TL;DR: Stability analysis of hypercontractivity estimates for Hopf-Lax semigroup and Euclidean L^p-logarithmic Sobolev inequalities with sharp optimal exponent 1/2.


<details>
  <summary>Details</summary>
Motivation: To establish stability results in hypercontractivity estimates and apply them to deduce stability for Euclidean L^p-logarithmic Sobolev inequalities, building on recent stability results for Prékopa-Leindler inequality.

Method: Uses recent stability results for Prékopa-Leindler inequality from Böröczky and De (2021), Figalli and Ramos (2024), and Figalli, van Hintum, and Tiba (2025) as main tools to prove stability in hypercontractivity estimates.

Result: Under mild assumptions, most stability results are sharp with optimal exponent 1/2 in both hypercontractivity and L^p-logarithmic Sobolev deficits. The approach also works for Gaussian hypercontractivity and logarithmic Sobolev inequality.

Conclusion: The paper provides sharp stability results for hypercontractivity estimates and L^p-logarithmic Sobolev inequalities, demonstrating the broad applicability of the approach to both Euclidean and Gaussian settings.

Abstract: We prove stability results in hypercontractivity estimates for the Hopf--Lax
semigroup in $\mathbb R^n$ and apply them to deduce stability results for the
Euclidean $L^p$-logarithmic Sobolev inequality for any $p>1$. As a main tool,
we use recent stability results for the Pr\'ekopa--Leindler inequality, due to
B\"or\"oczky and De (2021), Figalli and Ramos (2024) and Figalli, van Hintum,
and Tiba (2025). Under mild assumptions on the functions, most of our stability
results turn out to be sharp, as they are reflected in the optimal exponent
$1/2$ both in the hypercontractivity and $L^p$-logarithmic Sobolev deficits,
respectively. This approach also works for establishing stability of Gaussian
hypercontractivity estimates and Gaussian logarithmic Sobolev inequality,
respectively.

</details>


### [22] [F-equivalence for parabolic systems and applications to the stabilization of nonlinear PDE](https://arxiv.org/abs/2508.21605)
*Vincent Boulard,Amaury Hayat*

Main category: math.AP

TL;DR: Optimal conditions for F-equivalence in parabolic control systems, enabling exponential stabilization through feedback control, with applications to multi-dimensional systems including heat, Kuramoto-Sivashinsky, and Navier-Stokes equations.


<details>
  <summary>Details</summary>
Motivation: Extend finite-dimensional F-equivalence results to infinite-dimensional parabolic systems, particularly challenging for spatial dimensions larger than one, to enable rapid stabilization of complex systems.

Method: Introduce extended framework for F-equivalence of parabolic operators, establish optimal conditions for existence of F-equivalence pair (T,K), prove uniqueness under approximate controllability, and construct feedback operators for rapid stabilization.

Result: Main result provides optimal conditions for F-equivalence pair existence, shows uniqueness when system is approximately controllable, and enables construction of feedback operators for rapid stabilization of semilinear parabolic systems including multi-dimensional cases.

Conclusion: The framework successfully addresses limitations of existing approaches and provides a method for rapid stabilization of various parabolic systems, demonstrating practical applicability through examples of heat equation, Kuramoto-Sivashinsky equation, Navier-Stokes equations, and quasilinear heat equation.

Abstract: We consider the $F$-equivalence problem for parabolic systems: under which
conditions a control system, governed by a parabolic operator $A$ and a control
operator $B$, can be made equivalent to an arbitrarily exponentially stable
evolution system through an appropriate control feedback law? While this
problem has been resolved for finite-dimensional systems fifty years ago, good
conditions for infinite-dimensional systems remain a challenge, especially for
systems in spatial dimension larger than one. Our main result establishes
optimal conditions for the existence of an $F$-equivalence pair $(T,K)$ for a
given parabolic control system $(A,B)$. We introduce an extended framework for
$F$-equivalence of parabolic operators, addressing key limitations of existing
approaches, and we prove that the pair $(T,K)$ is unique if and only if $(A,B)$
is approximately controllable. As a consequence, this provides a method to
construct feedback operators for the rapid stabilization of semilinear
parabolic systems, possibly multi-dimensional in space. We provide several
illustrative examples, including the rapid stabilization of the heat equation,
the Kuramoto-Sivashinsky equation, the Navier-Stokes equations and the
quasilinear heat equation.

</details>


### [23] [Scattering for the non-radial inhomogeneous Hartree equation with a potential](https://arxiv.org/abs/2508.21822)
*Carlos M. Guzmán,Suerlan Silva,Gabriel Peçanha*

Main category: math.AP

TL;DR: Scattering proof for focusing generalized inhomogeneous Hartree equation with potential and weight |x|^{-b} in intercritical case for nonradial data under mass-potential condition.


<details>
  <summary>Details</summary>
Motivation: Extend scattering results to include inhomogeneous weight |x|^{-b} and potential V, generalizing previous works that didn't account for these complexities.

Method: Adapt Murphy's strategy with Tao's scattering criterion and localized Morawetz estimates. Establish global well-posedness for small data using admissible Strichartz pairs to handle the potential.

Result: Proved scattering for nonradial initial data in intercritical case under generalized mass-energy threshold condition.

Conclusion: Successfully extended scattering theory to cover inhomogeneous Hartree equations with potential, overcoming challenges from the weight and perturbed operator -Δ + V.

Abstract: In this work, we consider the focusing generalized inhomogeneous Hartree
equation with potential \[ i u_t + \Delta u - V(x)u + \left(I_{\gamma} *
|x|^{-b}|u|^{p}\right)|x|^{-b}|u|^{p-2}u = 0, \] where $0<\gamma<3$ and
$0<b<\frac{1+\gamma}{2}$. We prove scattering in the intercritical case for
nonradial initial data, under a mass-potential condition that generalizes the
usual mass-energy threshold. The main new points compared to previous works are
the inhomogeneous weight $|x|^{-b}$ and the presence of a potential $V$, which
lead us to study the perturbed operator $-\Delta + V$.
  Our proof follows the general strategy of Murphy, but we need to adapt
several steps to deal with the weight and the potential. We use Tao's
scattering criterion together with localized Morawetz estimates in this
setting. As a preliminary step, we establish global well-posedness for small
data, which, in the presence of $V$, requires careful analysis using
appropriate admissible Strichartz pairs.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [24] [Fast Methods For Multisite Charge Transfer Processes I: Constrained, State Averaged CASSCF(1,M) and CASSCF(2M-1,M) Simulations](https://arxiv.org/abs/2508.21136)
*Tian Qiu,Joseph E. Subotnik*

Main category: physics.comp-ph

TL;DR: Developed eDSCn/hDSCn method for multi-fragment charge transfer using constrained CASSCF with dynamic weights and efficient DIIS-SQP optimization.


<details>
  <summary>Details</summary>
Motivation: To enable efficient treatment of charge transfer between multiple molecular fragments (n>2) and study complex multi-state charge transfer processes.

Method: Dynamically-weighted state-averaged constrained CASSCF approach with generalized constraints for charge transfer, using DIIS-SQP algorithm for efficient optimization.

Result: Successfully reproduces exponential decay of diabatic couplings with distance in SSH chain, enables efficient nonadiabatic dynamics simulations.

Conclusion: The method provides an efficient framework for studying multi-fragment charge transfer problems and enables complex nonadiabatic dynamics simulations.

Abstract: We design a dynamically-weighted state-averaged constrained CASSCF to treat
\ul{e}lectrons or \ul{h}oles moving between $n$ molecular fragments (where $n$
can be larger than 2). Within such a so-called eDSCn/hDSCn approach, we
consider configurations that are mutually single excitations of each other, and
we apply a generalized set of constraints to tailor the method for studying
charge transfer problems. The constrained optimization problem is efficiently
solved using a DIIS-SQP algorithm, thus maintaining computational efficiency.
We demonstrate the method for a finite Su-Schrieffer-Heeger (SSH) chain,
successfully reproducing the expected exponential decay of diabatic couplings
with distance. When combined with a gradient, the current extension immediately
enables efficient nonadiabatic dynamics simulations of complex multi-state
charge transfer processes.

</details>


### [25] [Fast Methods For Multisite Charge Transfer Processes II: Analytic Nuclear Gradients and Nonadiabatic Dynamics For cCASSCF(1,M) and cCASSCF(2M-1,M) Wavefunctions](https://arxiv.org/abs/2508.21139)
*Tian Qiu,Joseph E. Subotnik*

Main category: physics.comp-ph

TL;DR: Analytic nuclear gradients and derivative couplings for constrained CASSCF with small active spaces for electron/hole transfer modeling


<details>
  <summary>Details</summary>
Motivation: To enable efficient nonadiabatic dynamics simulations of charge transfer processes by providing analytic derivatives for constrained CASSCF methods

Method: Used Lagrangian formalism to differentiate both CASSCF energy and constraints, ensuring globally smooth surfaces. Implemented efficient algorithm for nuclear gradients and derivative couplings

Result: Successfully derived and implemented analytic derivatives. Applied to surface-hopping simulations of proton coupled electron transfer in phenoxyl-phenol system

Conclusion: The developed method provides an efficient approach for studying charge transfer dynamics and can be immediately applied to nonadiabatic simulations of electron/hole transfer processes

Abstract: We derive and implement analytic nuclear gradients and derivative couplings
for a constrained Complete Active Space Self-Consistent Field with a small
active space designed to model electron or hole transfer. Using a Lagrangian
formalism, we are able to differentiate both the CASSCF energy and the
constraint (which is required for globally smooth surfaces), and the resulting
efficient algorithm can be immediately applied to nonadiabatic dynamics
simulations of charge transfer processes. Here, we run initial surface-hopping
simulations of a proton coupled electron transfer event for a phenoxyl-phenol
system.

</details>


### [26] [GPU-acceleration of the Discontinuous Galerkin Shallow Water Equations Solver (DG-SWEM) using CUDA and OpenACC](https://arxiv.org/abs/2508.21208)
*Chayanon Wichitrnithed,Eirik Valseth,Clint Dawson*

Main category: physics.comp-ph

TL;DR: Porting of DG-SWEM coastal ocean solver to GPU using CUDA Fortran and OpenACC, comparing performance and maintainability on NVIDIA Grace Hopper vs CPU.


<details>
  <summary>Details</summary>
Motivation: Discontinuous Galerkin methods for coastal ocean circulation and storm surge simulation exhibit high data parallelism due to loose element coupling, making them naturally suitable for GPU acceleration to improve computational performance.

Method: Two separate GPU porting approaches: CUDA Fortran and OpenACC. For OpenACC, used Unified Memory to maintain single codebase maintainability. Tested on NVIDIA Grace Hopper chip and compared GPU performance per node against MPI version on single CPU node (144 cores).

Result: Performance comparison between GPU implementations (CUDA Fortran and OpenACC) and traditional CPU-based MPI version on realistic use cases, evaluating computational efficiency on modern GPU architecture.

Conclusion: GPU acceleration using both CUDA Fortran and OpenACC approaches provides significant performance benefits for discontinuous Galerkin coastal ocean solvers, with OpenACC offering additional maintainability advantages through Unified Memory while preserving single codebase.

Abstract: This paper presents a porting of DG-SWEM, a discontinuous Galerkin solver for
coastal ocean circulation, and in particular storm surge, to GPU using two
separate approaches: CUDA Fortran and OpenACC. Time-explicit discontinuous
Galerkin methods have been shown to exhibit a large amount of data parallelism
due to the loose coupling between elements, and thus are naturally mapped to
the GPU architecture. For each porting approach, we discuss the code design,
ease of programming, and performance when running on realistic use cases.
Specifically for the OpenACC version, we also aim to preserve maintainability
within the same codebase through using Unified Memory. We test the codes on
NVIDIA's Grace Hopper chip and compare the GPU performance on each node to the
MPI version on a single CPU node (144 cores).

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [27] [Coherent attosecond pulses generated by a relativistic electron beam interacting with an intense laser at a grazing angle](https://arxiv.org/abs/2508.21293)
*H. Peng,T. W. Huang,C. N. Wu,K. Jiang,R. Li,C. Riconda,S. Weber,C. T. Zhou*

Main category: physics.plasm-ph

TL;DR: Coherent attosecond radiation generation through grazing angle interaction between laser pulses and relativistic electron beams, producing ultrashort pulse trains at Cherenkov angle.


<details>
  <summary>Details</summary>
Motivation: Achieving coherent radiation from relativistic electron beams interacting with intense laser fields has been challenging due to difficulties in precisely controlling the phase matching of radiating electrons.

Method: Using grazing angle interaction between laser pulses and relativistic electron beams, theoretical modeling, 3D particle-in-cell (PIC) simulations, and far-field time-domain radiation simulations.

Result: Demonstrated production of coherent attosecond radiation with electrons oscillating in laser field and modulated with superluminal phase, generating coherent ultrashort pulse trains at Cherenkov angle.

Conclusion: The proposed scheme enables feasible development of high-repetition-rate, compact, and high-energy attosecond pulse sources.

Abstract: The interaction between relativistic electron beams and intense laser fields
has been extensively studied for generating high-energy radiation. However,
achieving coherent radiation from such interactions needs to precisely control
the phase matching of the radiationg electrons, which has proven to be
exceptionally challenging. In this study, we demonstrate that coherent
attosecond radiation can be produced when a laser pulse interacts at grazing
angle with a relativistic electron beam. The electrons oscillate in the laser
field and are modulated with a superluminal phase, coherent ultrashort pulse
trains are produced in the far field at the Cherenkov angle. This is verified
by theoretical modeling and numerical simulations, including three-dimensional
particle-in-cell (PIC) simulations and far-field time-domain radiation
simulations. Based on our proposed scheme, high-repetition-rate, compact, and
high-energy attosecond pulse sources are feasible.

</details>


### [28] [Quantifying Reconnection and it's Dynamical Role in 2D Magnetic Rayleigh-Taylor Turbulence](https://arxiv.org/abs/2508.21532)
*Manohar Teja Kalluri,Andrew Hillier,Ben Snow*

Main category: physics.plasm-ph

TL;DR: Magnetic reconnection plays a crucial role in magnetic Rayleigh-Taylor instability evolution, facilitating plume merger and enabling continued growth, accounting for up to 80% of magnetic-to-kinetic energy transfer in weak field regimes.


<details>
  <summary>Details</summary>
Motivation: Understanding the role of magnetic reconnection in magnetic Rayleigh-Taylor instability evolution and energy dynamics, which remains poorly understood despite known occurrence during MRTI development.

Method: High-resolution 2D simulations with a robust automated reconnection detection algorithm and statistical analysis across various magnetic field strengths.

Result: Reconnection facilitates plume merger, relieves magnetic tension, and enables continued instability growth. It accounts for up to 80% of magnetic-to-kinetic energy transfer in weak field regimes but only ~3% of magnetic energy dissipation.

Conclusion: Magnetic reconnection is a critical mechanism regulating large-scale MRTI dynamics with significant implications for astrophysical plasmas and turbulent mixing in magnetized flows.

Abstract: Magnetic Rayleigh-Taylor instability (MRTI) governs material transport and
mixing in astrophysical and laboratory plasmas under the influence of gravity
and magnetic fields. While magnetic reconnection is known to occur during MRTI
evolution, its role in the evolution and energy dynamics remains poorly
understood. Here, we present a comprehensive analysis of the role of
reconnection in the two-dimensional MRTI dynamics, using high-resolution
simulations. We establish that reconnection, through facilitating plume merger,
relieving magnetic tension, and enabling continued instability growth, forms an
essential component for the long-term instability evolution. To quantify the
role of reconnection in energy dynamics, we develop a robust automated
reconnection detection algorithm and perform a statistical analysis across a
range of magnetic field strengths. We find that reconnection accounts for up to
$80\%$ of the magnetic-to-kinetic energy transfer in the weak magnetic field
regime, while contributing minimally ($\approx 3\%$) to magnetic energy
dissipation. Our results establish magnetic reconnection as a critical
mechanism that regulates large-scale MRTI dynamics, with implications for
astrophysical plasmas and turbulent mixing in magnetized flows.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [29] [LREI: A fast numerical solver for quantum Landau-Lifshitz equations](https://arxiv.org/abs/2508.21200)
*Davoud Mirzaei,Behnam Hashemi,Vahid Azimi-Mousolou*

Main category: quant-ph

TL;DR: LREI is an efficient low-rank algorithm for solving quantum spin dynamics equations that reduces computational complexity from O(N³) to O(r²N) and memory from O(N²) to O(rN), enabling simulation of large spin systems previously infeasible.


<details>
  <summary>Details</summary>
Motivation: Quantum Landau-Lifshitz and Landau-Lifshitz-Gilbert equations govern spin dynamics in open quantum systems, but traditional methods face exponential growth in computational cost with system size, making large spin simulations impractical.

Method: Exploits low-rank structure of density matrix and Hamiltonian sparsity using low-rank factor representation and Krylov subspace methods for partial eigendecompositions. Uses Householder reflectors to handle zero eigenvalue invariant subspace without forming large matrices.

Result: Achieves significant computational savings - 20-spin system simulations (with million-dimensional density matrices) now take only seconds on standard laptops instead of being infeasible. Both Runge-Kutta and Adams-Bashforth methods preserve physical properties.

Conclusion: LREI enables practical simulation of much larger quantum spin systems, providing a powerful tool for comparing q-LL and q-LLG dynamics, testing model validity, and studying quantum features like correlations and entanglement across different regimes.

Abstract: We develop LREI (Low-Rank Eigenmode Integration), a memory- and
time-efficient scheme for solving quantum Landau-Lifshitz (q-LL) and quantum
Landau-Lifshitz-Gilbert (q-LLG) equations, which govern spin dynamics in open
quantum systems. Although system size grows exponentially with the number of
spins, our approach exploits the low-rank structure of the density matrix and
the sparsity of Hamiltonians to avoid full matrix computations. By representing
density matrices via low-rank factors and applying Krylov subspace methods for
partial eigendecompositions, we reduce the per-step complexity of Runge-Kutta
and Adams-Bashforth schemes from $\mathcal{O}(N^3)$ to $\mathcal{O}(r^2N)$,
where $N = 2^n$ is the Hilbert space dimension for $n$ spins and $r \ll N$ the
effective rank. Similarly, memory costs shrink from $\mathcal{O}(N^2)$ to
$\mathcal{O}(rN)$, since no full $N\times N$ matrices are formed. A key advance
is handling the invariant subspace of zero eigenvalues. By using Householder
reflectors built for the dominant eigenspace, we perform the solution entirely
without large matrices. For example, a time step of a twenty-spin system, with
density matrix size over one million, now takes only seconds on a standard
laptop. Both Runge-Kutta and Adams-Bashforth methods are reformulated to
preserve physical properties of the density matrix throughout evolution. This
low-rank algorithm enables simulations of much larger spin systems, which were
previously infeasible, providing a powerful tool for comparing q-LL and q-LLG
dynamics, testing each model validity, and probing how quantum features such as
correlations and entanglement evolve across different regimes of system size
and damping.

</details>


### [30] [Block Encoding of Sparse Matrices via Coherent Permutation](https://arxiv.org/abs/2508.21667)
*Abhishek Setty*

Main category: quant-ph

TL;DR: A unified framework for efficient block encoding of arbitrary sparse matrices using combinatorial optimization and coherent permutation operators to overcome key implementation challenges.


<details>
  <summary>Details</summary>
Motivation: Block encoding of sparse matrices is crucial for quantum algorithms like singular value transformation and Hamiltonian simulation, but efficient gate-level implementation for arbitrary sparse matrices remains a major challenge due to multi-controlled X gates overhead, amplitude reordering issues, and hardware connectivity constraints.

Method: Novel connection with combinatorial optimization for systematic control qubit assignment to achieve nearest-neighbor connectivity, combined with coherent permutation operators that preserve superposition while enabling amplitude reordering.

Result: Significant reductions in circuit depth and control overhead for structured sparse matrices, demonstrating practical circuit implementations that bridge the gap between theoretical formulations and hardware realization.

Conclusion: The framework provides efficient gate-level constructions for arbitrary sparse matrix block encoding, overcoming key obstacles and enabling practical implementation of quantum algorithms that rely on sparse matrix operations.

Abstract: Block encoding of sparse matrices underpins powerful quantum algorithms such
as quantum singular value transformation, Hamiltonian simulation, and quantum
linear solvers, but its efficient gate-level implementation for arbitrary
sparse matrices remains a major challenge. We introduce a unified framework
that overcomes the key obstacles of multi-controlled X gates overhead,
amplitude reordering, and hardware connectivity, enabling efficient block
encoding for arbitrary sparse matrices with explicit gate-level constructions.
Central to our approach are a novel connection with combinatorial optimization,
which enables systematic assignment of control qubits to achieve
nearest-neighbor connectivity, and coherent permutation operators that preserve
superposition while enabling amplitude reordering. We demonstrate our methods
on structured sparse matrices, showing significant reductions in circuit depth
and control overhead, thereby bridging the gap between theoretical formulations
and practical circuit implementations for quantum algorithms.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [31] [Bayesian perspectives for quantum states and application to ab initio quantum chemistry](https://arxiv.org/abs/2508.21729)
*Yannic Rath,Massimo Bortone,George H. Booth*

Main category: cond-mat.str-el

TL;DR: Review of machine learning-inspired approaches for quantum chemical problems, focusing on Gaussian Process States for efficient many-electron wavefunction representation and Bayesian modeling frameworks.


<details>
  <summary>Details</summary>
Motivation: Strong correlation in chemical systems presents formidable challenges for simulation, requiring high accuracy and efficient representations of many-electron states beyond traditional methods.

Method: Application of Gaussian Process States derived from machine learning principles, using Bayesian modeling frameworks to represent many-body wavefunctions in second quantization.

Result: Development of novel tools for computing ab initio chemical properties and informing machine learning models to extract correlation patterns from classical data.

Conclusion: Machine learning-inspired representations, particularly Gaussian Process States with Bayesian frameworks, provide a unified approach for quantum chemical problems and offer new capabilities for both quantum simulation and classical data analysis.

Abstract: The quantum many-electron problem is not just at the heart of condensed
matter phenomena, but also essential for first-principles simulation of
chemical phenomena. Strong correlation in chemical systems are prevalent and
present a formidable challenge in the simulation of these systems, while
predictive phenomena in this domain often also requires a demanding level of
accuracy to inform chemical behavior. Efficient representations of the
many-electron states of chemical systems are therefore also being inspired by
machine learning principles to provide an alternative to established
approaches. In this chapter, we review recent progress in this endeavor for
quantum chemical problems represented in second quantization, and the
particular challenges present in this field. In particular, we focus on the
application of Gaussian Process States emerging from efficient representations
of the many-body wavefunction with rigorous Bayesian modeling frameworks,
allowing for the unification of multiple paradigms under a common umbrella. We
show how such models (and other representations derived from machine learning)
can be used as novel tools to compute ab initio chemical properties, while in
turn also informing the design of machine learning models to extract
correlation patterns in classical data.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [32] [Control of growth morphology of deposited fcc metals through tuning substrate-metal interactions](https://arxiv.org/abs/2508.21492)
*Samuel Aldana,Michael Nolan*

Main category: cond-mat.mtrl-sci

TL;DR: Kinetic Monte Carlo simulations show that substrate interaction strength and thermal annealing can control thin film morphology (2D vs 3D growth) for fcc metals, with Au, Pd and Pt being most sensitive. Strong substrate interactions reduce roughness and improve interconnect metrics.


<details>
  <summary>Details</summary>
Motivation: Precise control over thin film morphology is critical for optimizing material properties in applications like CMOS interconnects and catalysis, where growth mode (2D layer-by-layer vs 3D island formation) determines key functional properties.

Method: Extensive kinetic Monte Carlo simulations on six fcc metals (Ag, Au, Cu, Ni, Pd, Pt) growing in (111) direction, evaluating growth modes under homoepitaxial scenarios by varying substrate-metal interaction strengths through modified activation energies for migration, combined with thermal vacuum annealing within BEOL thermal budget.

Result: Modulation of substrate interaction strength effectively promotes island formation or layer-by-layer growth. Au, Pd and Pt show highest sensitivity to substrate interactions. Strong interactions decrease RMS roughness, substrate exposure, island number and aspect ratios, while increasing flat surface areas and coordination numbers. Thermal annealing further improves interconnect metrics when strong metal-substrate interactions are used.

Conclusion: Substrate interaction strength control combined with thermal annealing provides effective strategies for morphology control in thin film deposition, enabling optimization of growth modes and improving interconnect-relevant metrics for technological applications.

Abstract: Precise control over thin film morphology is critical for optimizing material
properties across diverse technological applications, as the growth mode
(whether 2D layer-by-layer or 3D island formation)determines key functional
properties such as electrical conductivity in CMOS interconnect applications
and catalytic activity, where island distribution and size dictate performance.
To explore the role of the substrate on the morphology of deposited metals, we
present extensive kinetic Monte Carlo simulations on six fcc metals growing in
the (111) direction: Ag, Au, Cu, Ni, Pd and Pt. Our simulation framework
enables screening and evaluation of their growth mode under homoepitaxial
growth scenarios and proposes morphology control strategies by variation of
substrate-metal interaction strengths, modeled by modifying the activation
energies for upward and downward migration, combined with thermal vacuum
annealing within typical back end of line (BEOL) integration thermal budget.
Our simulation results demonstrate that modulation of the substrate interaction
strength can be effectively employed to promote island formation or
layer-by-layer growth modes overcoming limitations in achieving large flat
surface areas. Au, Pd and Pt exhibit the highest sensitivity to substrate
interaction strength variations, followed by Ag, showing that strongly
interacting substrates decrease the root mean square (RMS) roughness,
(uncovered) substrate exposure, island number and island aspect ratios, with
moderate increases in flat surface areas and atomic coordination numbers.
Additionally, interconnect relevant metrics are improved through thermal vacuum
annealing particularly when sufficiently strong metal-substrate interactions
are employed, reducing surface roughness, achieving larger flat surface areas,
merging and smoothing islands, and decreasing defect density...

</details>


### [33] [Surface Stability Modeling with Universal Machine Learning Interatomic Potentials: A Comprehensive Cleavage Energy Benchmarking Study](https://arxiv.org/abs/2508.21663)
*Ardavan Mehdizadeh,Peter Schindler*

Main category: cond-mat.mtrl-sci

TL;DR: Benchmark study shows that training data composition, not architectural complexity, is the key factor for accurate cleavage energy predictions in universal machine learning interatomic potentials. Models trained on non-equilibrium configurations (OMat24 dataset) achieve 6% error and 87% accuracy on surface termination identification.


<details>
  <summary>Details</summary>
Motivation: Despite the success of universal MLIPs in predicting bulk properties, there has been no systematic evaluation of their ability to predict cleavage energies - a critical property for fracture, catalysis, surface stability, and interfacial phenomena.

Method: Comprehensive benchmark of 19 state-of-the-art universal MLIPs using a DFT database of 36,718 slab structures spanning elemental, binary, and ternary metallic compounds. Evaluated diverse architectural paradigms across chemical compositions, crystal systems, thickness, and surface orientations.

Result: Training data composition dominates performance: OMat24-trained models achieve <6% MAPE and 87% accuracy on stable surface terminations. Architecturally identical models trained on equilibrium-only data show 5x higher errors, while surface-adsorbate trained models fail catastrophically with 17x degradation. Simpler architectures with appropriate data match complex transformers' accuracy with 10-100x speedup.

Conclusion: The community should focus on strategic training data generation that captures relevant physical phenomena rather than architectural complexity, as appropriate training data enables accurate cleavage energy predictions even with simpler, faster models.

Abstract: Machine learning interatomic potentials (MLIPs) have revolutionized
computational materials science by bridging the gap between quantum mechanical
accuracy and classical simulation efficiency, enabling unprecedented
exploration of materials properties across the periodic table. Despite their
remarkable success in predicting bulk properties, no systematic evaluation has
assessed how well these universal MLIPs (uMLIPs) can predict cleavage energies,
a critical property governing fracture, catalysis, surface stability, and
interfacial phenomena. Here, we present a comprehensive benchmark of 19
state-of-the-art uMLIPs for cleavage energy prediction using our previously
established density functional theory (DFT) database of 36,718 slab structures
spanning elemental, binary, and ternary metallic compounds. We evaluate diverse
architectural paradigms, analyzing their performance across chemical
compositions, crystal systems, thickness, and surface orientations. Our results
reveal that training data composition dominates architectural sophistication:
models trained on the Open Materials 2024 (OMat24) dataset, which emphasizes
non-equilibrium configurations, achieve mean absolute percentage errors below
6% and correctly identify the thermodynamically most stable surface
terminations in 87% of cases, without any explicit surface energy training. In
contrast, architecturally identical models trained on equilibrium-only datasets
show five-fold higher errors, while models trained on surface-adsorbate data
fail catastrophically with a 17-fold degradation. Remarkably, simpler
architectures trained on appropriate data achieve comparable accuracy to
complex transformers while offering 10-100x computational speedup. These
findings show that the community should focus on strategic training data
generation that captures the relevant physical phenomena.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [34] [Mean Field Games of Controls with Dirichlet \& Neumann Boundary Conditions](https://arxiv.org/abs/2508.21642)
*P. Jameson Graber,Kyle Rosengartner*

Main category: math.OC

TL;DR: Analysis of mean field games with controls under Dirichlet/Neumann boundary conditions, proving well-posedness with smallness conditions or monotone couplings


<details>
  <summary>Details</summary>
Motivation: To establish mathematical foundations for mean field games of controls where player costs depend on joint state-control distributions, particularly addressing boundary condition scenarios

Method: Theoretical analysis of mean field game systems with Dirichlet and Neumann boundary conditions, employing smallness conditions and monotone coupling assumptions

Result: Proved well-posedness (existence and uniqueness) of such mean field game systems under specified boundary conditions with either sufficient smallness requirements or monotone coupling properties

Conclusion: The paper provides rigorous mathematical guarantees for the solvability of mean field games of controls with boundary conditions, establishing important theoretical foundations for this class of problems

Abstract: In a mean field game of controls, a large population of identical players
seek to minimize a cost that depends on the joint distribution of the states of
the players and their controls. We consider the classes of mean field games of
controls in which the value function and the distribution of player states
satisfy either Dirichlet or Neumann boundary conditions. We prove that such
systems are well-posed either with sufficient smallness conditions or in the
case of monotone couplings.

</details>


<div id='math.FA'></div>

# math.FA [[Back]](#toc)

### [35] [Fractional Heat Semigroup Characterization of Distances from Functions in Lipschitz Spaces to Their Subspaces](https://arxiv.org/abs/2508.21269)
*Feng Dai,Eero Saksman,Dachun Yang,Wen Yuan,Yangyang Zhang*

Main category: math.FA

TL;DR: Characterizes distance from functions in inhomogeneous Lipschitz spaces to non-dense subspaces using fractional semigroups and introduces critical indices via admissible set functions.


<details>
  <summary>Details</summary>
Motivation: To develop a general framework for quantifying the approximation distance from functions in Lipschitz spaces to various subspaces (Sobolev, Besov, etc.) using fractional semigroup techniques.

Method: Uses fractional semigroup operators and introduces admissible set functions to define critical indices that measure the size of 'bad' point sets where semigroup estimates fail.

Result: Establishes equivalence between the critical index ε and the distance dist(f,V) for broad classes of subspaces V in Lipschitz spaces.

Conclusion: Provides a unified approach to characterize approximation distances in Lipschitz spaces through fractional semigroup analysis and admissible set functions.

Abstract: Let $\Lambda_s$ denote the inhomogeneous Lipschitz space of order
$s\in(0,\infty)$ on $\mathbb{R}^n$. This article characterizes the distance
$d(f, V)_{\Lambda_s}: = \inf_{g\in V} \|f-g\|_{\Lambda_s}$ from a function
$f\in \Lambda_s$ to a non-dense subspace $V\subset \Lambda_s$ via the
fractional semigroup $\{T_{\alpha, t}: =e^{-t (-\Delta)^{\alpha/2}}: t\in (0,
\infty)\}$ for any $\alpha\in(0,\infty)$. Given an integer $ r >s/\alpha$, a
uniformly bounded continuous function $f$ on $\mathbb{R}^n$ belongs to the
space $\Lambda_s$ if and only if there exists a constant $\lambda\in(0,\infty)$
such that \begin{align*} \left|(-\Delta)^{\frac {\alpha r}2} (T_{\alpha,
t^\alpha } f)(x) \right|\leq \lambda t^{s -r\alpha }\ \ \text{for any
$x\in\mathbb{R}^n$ and $t\in (0, 1]$}.\end{align*} The least such constant is
denoted by $\lambda_{ \alpha, r, s}(f)$. For each $f\in \Lambda_s$ and
$0<\varepsilon< \lambda_{\alpha,r, s}(f)$, let $$ D_{\alpha,
r}(s,f,\varepsilon):=\left\{ (x,t)\in \mathbb{R}^n\times (0,1]:\ \left|
(-\Delta)^{\frac {\alpha r}2} (T_{\alpha, t^\alpha} f)(x) \right|> \varepsilon
t^{s -r \alpha }\right\}$$ be the set of ``bad'' points. To quantify its size,
we introduce a class of extended nonnegative \emph{admissible set functions}
$\nu$ on the Borel $\sigma$-algebra $\mathcal{B}(\mathbb{R}^n\times [0, 1])$
and define, for any admissible function $\nu$, the \emph{critical index} $
\varepsilon_{\alpha, r, s,\nu}(f):=\inf\{\varepsilon\in(0,\infty):\
\nu(D_{\alpha, r}(s,f,\varepsilon))<\infty\}.$ Our result shows that, for a
broad class of subspaces $V\subset \Lambda_s$, including intersections of
$\Lambda_s$ with Sobolev, Besov, Triebel--Lizorkin, and Besov-type spaces,
there exists an admissible function $\nu$ depending on $V$ such that
$\varepsilon_{\alpha, r, s,\nu}(f)\sim \mathrm{dist}(f, V)_{\Lambda_s}.$

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [36] [A Mixture of Experts Gating Network for Enhanced Surrogate Modeling in External Aerodynamics](https://arxiv.org/abs/2508.21249)
*Mohammad Amin Nabian,Sanjay Choudhry*

Main category: cs.LG

TL;DR: A meta-learning Mixture of Experts framework that combines three state-of-the-art neural network architectures (DoMINO, X-MeshGraphNet, FigConvNet) with a gating network to achieve superior aerodynamic prediction accuracy in automotive CFD simulations.


<details>
  <summary>Details</summary>
Motivation: High computational cost of CFD simulations is a bottleneck in automotive design. While ML-based surrogate models show promise, no single architecture demonstrates universal superiority, and the field has diverse specialized models.

Method: Proposes a Mixture of Experts model with a gating network that dynamically combines predictions from three heterogeneous surrogate models. Uses entropy regularization to prevent model collapse and ensure balanced expert contributions. Trained on DrivAerML dataset.

Result: Achieves significant reduction in L-2 prediction error, outperforming both ensemble average and the most accurate individual expert model across all evaluated physical quantities (surface pressure and wall shear stress fields).

Conclusion: The MoE framework is a powerful strategy for creating robust and accurate composite surrogate models by synergistically combining complementary strengths of specialized architectures in automotive aerodynamics.

Abstract: The computational cost associated with high-fidelity CFD simulations remains
a significant bottleneck in the automotive design and optimization cycle. While
ML-based surrogate models have emerged as a promising alternative to accelerate
aerodynamic predictions, the field is characterized by a diverse and rapidly
evolving landscape of specialized neural network architectures, with no single
model demonstrating universal superiority. This paper introduces a novel
meta-learning framework that leverages this architectural diversity as a
strength. We propose a Mixture of Experts (MoE) model that employs a dedicated
gating network to dynamically and optimally combine the predictions from three
heterogeneous, state-of-the-art surrogate models: DoMINO, a decomposable
multi-scale neural operator; X-MeshGraphNet, a scalable multi-scale graph
neural network; and FigConvNet, a factorized implicit global convolution
network. The gating network learns a spatially-variant weighting strategy,
assigning credibility to each expert based on its localized performance in
predicting surface pressure and wall shear stress fields. To prevent model
collapse and encourage balanced expert contributions, we integrate an entropy
regularization term into the training loss function. The entire system is
trained and validated on the DrivAerML dataset, a large-scale, public benchmark
of high-fidelity CFD simulations for automotive aerodynamics. Quantitative
results demonstrate that the MoE model achieves a significant reduction in L-2
prediction error, outperforming not only the ensemble average but also the most
accurate individual expert model across all evaluated physical quantities. This
work establishes the MoE framework as a powerful and effective strategy for
creating more robust and accurate composite surrogate models by synergistically
combining the complementary strengths of specialized architectures.

</details>


### [37] [Convergence of Stochastic Gradient Methods for Wide Two-Layer Physics-Informed Neural Networks](https://arxiv.org/abs/2508.21571)
*Bangti Jin,Longjun Wu*

Main category: cs.LG

TL;DR: This paper establishes linear convergence guarantees for stochastic gradient descent/flow in training over-parameterized two-layer Physics Informed Neural Networks (PINNs) for solving partial differential equations, extending previous results that only analyzed gradient descent.


<details>
  <summary>Details</summary>
Motivation: PINNs are widely used for solving PDEs, typically trained with stochastic gradient descent methods. However, existing convergence analyses focused only on gradient descent, leaving a gap in understanding the convergence behavior of stochastic optimization methods which introduce dynamic randomness.

Method: The authors analyze stochastic gradient descent/flow for over-parameterized two-layer PINNs with general activation functions. The key challenge is handling dynamic randomness from stochastic optimization, and the analysis focuses on ensuring positive definiteness of suitable Gram matrices during training.

Result: The paper establishes linear convergence in the high probability sense for stochastic gradient descent/flow when training over-parameterized two-layer PINNs, extending previous gradient descent results to stochastic optimization methods.

Conclusion: This work provides theoretical guarantees for stochastic optimization methods in training PINNs, offering insights into the optimization dynamics and ensuring that neural networks trained by stochastic algorithms maintain convergence properties similar to deterministic methods.

Abstract: Physics informed neural networks (PINNs) represent a very popular class of
neural solvers for partial differential equations. In practice, one often
employs stochastic gradient descent type algorithms to train the neural
network. Therefore, the convergence guarantee of stochastic gradient descent is
of fundamental importance. In this work, we establish the linear convergence of
stochastic gradient descent / flow in training over-parameterized two layer
PINNs for a general class of activation functions in the sense of high
probability. These results extend the existing result [18] in which gradient
descent was analyzed. The challenge of the analysis lies in handling the
dynamic randomness introduced by stochastic optimization methods. The key of
the analysis lies in ensuring the positive definiteness of suitable Gram
matrices during the training. The analysis sheds insight into the dynamics of
the optimization process, and provides guarantees on the neural networks
trained by stochastic algorithms.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [38] [Survival probability for jump processes in unbounded domains on metric measure spaces](https://arxiv.org/abs/2508.21158)
*Phanuel Mariano,Jing Wang*

Main category: math.PR

TL;DR: Analysis of survival probability decay rates for symmetric jump processes in unbounded domains with positive bottom spectrum, providing probabilistic interpretation and geometric conditions.


<details>
  <summary>Details</summary>
Motivation: To understand the large time behavior of survival probabilities for symmetric jump processes in unbounded domains and establish connections between spectral properties and geometric domain characteristics.

Method: Prove asymptotic upper and lower bounds with explicit constants using the bottom of spectrum λ(D), apply to symmetric jump processes in general metric measure spaces, and analyze α-stable processes in uniformly C¹¹ domains.

Result: Established probabilistic interpretation and equivalent geometric condition for λ(D)>0, showed sharp exponential decay rate in horn-shaped domains, and provided examples of applicable unbounded domains.

Conclusion: The study successfully connects spectral properties with geometric domain characteristics for symmetric jump processes, providing explicit bounds and probabilistic interpretations that hold across various unbounded domain types.

Abstract: We study the large time behavior of the survival probability
$\mathbb{P}_x\left(\tau_D>t\right)$ for symmetric jump processes in unbounded
domains with a positive bottom of the spectrum. We prove asymptotic upper and
lower bounds with explicit constants in terms of the bottom of the spectrum
$\lambda(D)$. Our main result applies to symmetric jump processes in general
metric measure spaces. For $\alpha$-stable processes in unbounded uniformly
$C^{1,1}$ domains, our results provide a probabilistic interpretation and an
equivalent geometric condition for $\lambda(D)>0$. In the case of increasing
horn-shaped domains, the exponential rate of decay for the survival probability
is sharp. We also present examples of unbounded domains where our results
apply.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [39] [Faster Linear Algebra Algorithms with Structured Random Matrices](https://arxiv.org/abs/2508.21189)
*Chris Camaño,Ethan N. Epperly,Raphael A. Meyer,Joel A. Tropp*

Main category: cs.DS

TL;DR: This paper introduces the Oblivious Subspace Injection (OSI) property as a new framework for analyzing structured random matrices in randomized linear algebra algorithms, providing theoretical guarantees and practical implementations for faster low-rank approximation and least-squares regression.


<details>
  <summary>Details</summary>
Motivation: Despite extensive research on randomized algorithms using structured random matrices for dimension reduction, fundamental questions remain about their design and analysis. Practitioners need faster, near-optimal algorithms for linear algebra tasks with theoretical guarantees.

Method: Develops the OSI property framework that factors analysis into: (1) proving algorithms work with OSI matrices, and (2) proving specific random matrix models satisfy OSI. Analyzes standard randomized algorithms under OSI assumption and identifies OSI examples including sparse matrices, trigonometric transforms, and tensor product structures.

Result: Theoretical results show faster, near-optimal runtimes for fundamental linear algebra tasks. Provides implementation guidance and empirical evidence demonstrating exemplary performance on synthetic problems and scientific applications.

Conclusion: The OSI framework provides a unified approach to analyze structured random matrices, enabling faster randomized linear algebra algorithms with strong theoretical guarantees and practical performance benefits across various applications.

Abstract: To achieve the greatest possible speed, practitioners regularly implement
randomized algorithms for low-rank approximation and least-squares regression
with structured dimension reduction maps. Despite significant research effort,
basic questions remain about the design and analysis of randomized linear
algebra algorithms that employ structured random matrices.
  This paper develops a new perspective on structured dimension reduction,
based on the oblivious subspace injection (OSI) property. The OSI property is a
relatively weak assumption on a random matrix that holds when the matrix
preserves the length of vectors on average and, with high probability, does not
annihilate any vector in a low-dimensional subspace. With the OSI abstraction,
the analysis of a randomized linear algebra algorithm factors into two parts:
(i) proving that the algorithm works when implemented with an OSI; and (ii)
proving that a given random matrix model has the OSI property.
  This paper develops both parts of the program. First, it analyzes standard
randomized algorithms for low-rank approximation and least-squares regression
under the OSI assumption. Second, it identifies many examples of OSIs,
including random sparse matrices, randomized trigonometric transforms, and
random matrices with tensor product structure. These theoretical results imply
faster, near-optimal runtimes for several fundamental linear algebra tasks. The
paper also provides guidance on implementation, along with empirical evidence
that structured random matrices offer exemplary performance for a range of
synthetic problems and contemporary scientific applications.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [40] [Data-Driven Bifurcation Handling in Physics-Based Reduced-Order Vascular Hemodynamic Models](https://arxiv.org/abs/2508.21165)
*Natalia L. Rubio,Eric F. Darve,Alison L. Marsden*

Main category: cs.CE

TL;DR: Machine learning-enhanced 0D hemodynamic model improves cardiovascular flow predictions at bifurcations while maintaining computational efficiency


<details>
  <summary>Details</summary>
Motivation: Traditional 3D finite-element simulations are computationally expensive for clinical use, while standard reduced-order models lack accuracy at vessel bifurcations where complex flow physics occur

Method: Developed a resistor-resistor-inductor (RRI) model using neural networks to predict pressure-flow relationships from bifurcation geometry, incorporating non-dimensionalization and flow split prediction, integrated into 0D models via optimization-based solution

Result: Substantial accuracy improvements: reduced inlet pressure errors from 54 mmHg (45%) to 25 mmHg (17%) across vascular trees and Reynolds numbers 0-5,500, with particular effectiveness at high Reynolds numbers and extensive networks

Conclusion: Hybrid machine learning-numerical approach enables accurate real-time hemodynamic modeling for clinical decision support, uncertainty quantification, and digital twins in cardiovascular biomedical engineering

Abstract: Three-dimensional (3D) finite-element simulations of cardiovascular flows
provide high-fidelity predictions to support cardiovascular medicine, but their
high computational cost limits clinical practicality. Reduced-order models
(ROMs) offer computationally efficient alternatives but suffer reduced
accuracy, particularly at vessel bifurcations where complex flow physics are
inadequately captured by standard Poiseuille flow assumptions. We present an
enhanced numerical framework that integrates machine learning-predicted
bifurcation coefficients into zero-dimensional (0D) hemodynamic ROMs to improve
accuracy while maintaining computational efficiency. We develop a
resistor-resistor-inductor (RRI) model that uses neural networks to predict
pressure-flow relationships from bifurcation geometry, incorporating linear and
quadratic resistances along with inductive effects. The method employs
non-dimensionalization to reduce training data requirements and apriori flow
split prediction for improved bifurcation characterization. We incorporate the
RRI model into a 0D model using an optimization-based solution strategy. We
validate the approach in isolated bifurcations and vascular trees, across
Reynolds numbers from 0 to 5,500, defining ROM accuracy by comparison to 3D
finite element simulation. Results demonstrate substantial accuracy
improvements: averaged across all trees and Reynolds numbers, the RRI method
reduces inlet pressure errors from 54 mmHg (45%) for standard 0D models to 25
mmHg (17%), while a simplified resistor-inductor (RI) variant achieves 31 mmHg
(26%) error. The enhanced 0D models show particular effectiveness at high
Reynolds numbers and in extensive vascular networks. This hybrid numerical
approach enables accurate, real-time hemodynamic modeling for clinical decision
support, uncertainty quantification, and digital twins in cardiovascular
biomedical engineering.

</details>


<div id='physics.app-ph'></div>

# physics.app-ph [[Back]](#toc)

### [41] [When Energy and Information Revolutions Meet 2D Janus](https://arxiv.org/abs/2508.21425)
*Long Zhang,Ziqi Ren,Li Sun,Yihua Gao,Deli Wang,Junjie He,Guoying Gao*

Main category: physics.app-ph

TL;DR: This review paper provides a comprehensive analysis of 2D Janus materials, covering their theoretical predictions, experimental preparations, properties, and applications in energy and information fields.


<details>
  <summary>Details</summary>
Motivation: Addressing global concerns of energy depletion, environmental issues, and quantum limitations in post-Moore era information storage by exploring 2D Janus materials with unique asymmetric properties.

Method: Systematic review summarizing theoretical predictions, experimental preparations, modulation strategies, and recent advances in modifiable properties across optics, catalysis, piezoelectricity, electrochemistry, thermoelectricity, magnetism, and electronics.

Result: Comprehensive repository of 2D Janus family with focus on experimentally realized hexagonal and trigonal structures, highlighting their pressure-dependent and non-linear optical responses, piezoelectricity, valley polarization, and Rashba spin splitting.

Conclusion: The review serves as a valuable resource for designing, fabricating, regulating, and applying 2D Janus systems, promoting both academic investigations and industrial applications in energy and information fields.

Abstract: The depletion of energy sources, worsening environmental issues, and the
quantum limitations of integrated circuits for information storage in the
post-Moore era, are pressing global concerns. Fortunately, two-dimensional (2D)
Janus materials, possessing broken spatial symmetry, with emerging
pressure-dependent and non-linear optical response, piezoelectricity, valley
polarization, Rashba spin splitting and more, have established a substantial
platform for exploring and applying modifiable physical, chemical and
biological properties in material science and offered a promising solution for
these energy and information issues. To furnish researchers with a
comprehensive repository of 2D Janus family, this review systematically
summarizes their theoretical predictions, experimental preparations, and
modulation strategies. It also retrospectively outlines the recent advances in
modifiable properties, applications, and inherent mechanisms in optics,
catalysis, piezoelectricity, electrochemistry, thermoelectricity, magnetism,
and electronics, with a focus on experimentally realized hexagonal and trigonal
Janus structures. Additionally, their current research state is summarized, and
potential opportunities and challenges that may arise are highlighted. Overall,
this review aims to serve as a valuable resource for designing, fabricating,
regulating, and applying 2D Janus systems, both theoretically and
experimentally. This review will strongly promote the advanced academic
investigations and industrial applications of 2D Janus materials in energy and
information fields.

</details>
