{"id": "2508.21155", "pdf": "https://arxiv.org/pdf/2508.21155", "abs": "https://arxiv.org/abs/2508.21155", "authors": ["Joseph Hart", "Alen Alexanderian", "Bart van Bloemen Waanders"], "title": "Preconditioned pseudo-time continuation for parameterized inverse problems", "categories": ["math.NA", "cs.NA"], "comment": null, "summary": "We consider parametrized variational inverse problems that are constrained by\npartial differential equations (PDEs). We seek to efficiently compute the\nsolution of the inverse problem when auxiliary model parameters, which appear\nin the governing PDE, are varied. Computing the solution of the inverse problem\nfor different auxiliary parameter values is crucial for uncertainty\nquantification. This, however, is computationally challenging since it requires\nsolving many optimization problems for different realizations of the auxiliary\nparameters. We leverage pseudo-time continuation and solve an initial value\nproblem to evolve the optimal solution along an auxiliary parameter path. This\narticle introduces the use of an adaptive quasi-Newton Hessian preconditioner\nto accelerate the computation. Our proposed preconditioner exploits properties\nof the pseudo-time continuation process to achieve reliable and efficient\ncomputation. We elaborate our proposed framework and elucidate its properties\nfor two nonlinear inverse problems.", "AI": {"tldr": "A method using pseudo-time continuation with adaptive quasi-Newton Hessian preconditioning to efficiently solve parametrized variational inverse problems constrained by PDEs for uncertainty quantification.", "motivation": "To address the computational challenge of solving multiple optimization problems for different auxiliary parameter values in PDE-constrained inverse problems, which is crucial for uncertainty quantification but traditionally requires solving many separate optimization problems.", "method": "Leverages pseudo-time continuation to solve an initial value problem that evolves the optimal solution along an auxiliary parameter path, using an adaptive quasi-Newton Hessian preconditioner that exploits properties of the pseudo-time continuation process.", "result": "The proposed preconditioner achieves reliable and efficient computation for parametrized variational inverse problems, as demonstrated through two nonlinear inverse problems.", "conclusion": "The adaptive quasi-Newton Hessian preconditioner combined with pseudo-time continuation provides an effective framework for accelerating the solution of PDE-constrained inverse problems across varying parameter values, enabling more efficient uncertainty quantification."}}
{"id": "2508.21226", "pdf": "https://arxiv.org/pdf/2508.21226", "abs": "https://arxiv.org/abs/2508.21226", "authors": ["Brian Christner", "Jesse Chan"], "title": "Entropy stable finite difference (ESFD) methods via entropy correction artificial viscosity (ECAV) and knapsack limiting (KL) techniques", "categories": ["math.NA", "cs.NA", "68Q25, 68R10, 68U05"], "comment": null, "summary": "Entropy stable methods have become increasingly popular in the field of\ncomputational fluid dynamics. They often work by satisfying some form of a\ndiscrete entropy inequality: a discrete form of the 2nd law of thermodynamics.\nSchemes which satisfy a (semi-)discrete entropy inequality typically behave\nmuch more robustly, and do so in a way that is hyperparameter free. Recently, a\nnew strategy was introduced to construct entropy stable discontinuous Galerkin\nmethods: knapsack limiting, which blends together a low order, positivity\npreserving, and entropy stable scheme with a high order accurate scheme, in\norder to produce a high order accurate, entropy stable, and positivity\npreserving scheme. Another recent strategy introduces an entropy correction\nartificial viscosity into a high order scheme, aiming to satisfy a cell entropy\ninequality.\n  In this work, we introduce the techniques of knapsack limiting and artificial\nviscosity for finite difference discretizations. The proposed schemes preserve\nhigh order accuracy in sufficiently smooth conditions, are entropy stable, and\nare hyperparameter free. Moreover, the proposed knapsack limiting scheme\nprovably preserves positivity for the compressible Euler and Navier-Stokes\nequations. Both schemes achieve this goal without significant performance\ntradeoffs compared to state of the art stabilized schemes.", "AI": {"tldr": "This paper introduces knapsack limiting and artificial viscosity techniques for finite difference discretizations, creating high-order accurate, entropy stable, and hyperparameter-free schemes that preserve positivity for compressible Euler and Navier-Stokes equations.", "motivation": "Entropy stable methods provide robust computational fluid dynamics solutions by satisfying discrete entropy inequalities (2nd law of thermodynamics), but existing approaches need adaptation for finite difference discretizations to achieve high-order accuracy, entropy stability, and positivity preservation without performance tradeoffs.", "method": "The authors apply two recent strategies to finite difference discretizations: 1) knapsack limiting - blending low-order (positivity preserving, entropy stable) with high-order schemes, and 2) entropy correction artificial viscosity - adding viscosity to satisfy cell entropy inequality in high-order schemes.", "result": "The proposed finite difference schemes achieve high-order accuracy in smooth conditions, maintain entropy stability, are hyperparameter-free, and the knapsack limiting scheme provably preserves positivity for compressible Euler and Navier-Stokes equations without significant performance degradation compared to state-of-the-art stabilized schemes.", "conclusion": "The successful adaptation of knapsack limiting and artificial viscosity techniques to finite difference discretizations provides robust, high-order accurate, entropy stable, and positivity preserving computational fluid dynamics methods that operate without hyperparameter tuning and maintain competitive performance."}}
{"id": "2508.21279", "pdf": "https://arxiv.org/pdf/2508.21279", "abs": "https://arxiv.org/abs/2508.21279", "authors": ["Chris Vales", "Siu Wun Cheung", "Dylan M. Copeland", "Youngsoo Choi"], "title": "Machine-precision energy conservative quadrature hyperreduction of Lagrangian hydrodynamics", "categories": ["math.NA", "cs.NA", "physics.comp-ph", "physics.flu-dyn"], "comment": "24 pages, 1 figure", "summary": "We present an energy conservative, quadrature based model reduction framework\nfor the compressible Euler equations of Lagrangian hydrodynamics. Building on a\nhigh order finite element discretization of the governing equations, we develop\na projection based reduced model using data driven reduced basis functions and\nhyperreduction via the empirical quadrature procedure (EQP). We introduce a\nstrongly energy conservative variant of EQP that enforces exact discrete total\nenergy conservation during the hyperreduction process. Numerical experiments\nfor four benchmark problems -- Sedov blast, Gresho vortex, triple point and\nTaylor-Green vortex -- demonstrate that the numerical implementation of our\nproposed method conserves total energy to near machine precision while\nmaintaining accuracy comparable to the basic EQP formulation. These results\nestablish the energy conservative EQP (CEQP) method as an effective structure\npreserving hyperreduction strategy for the reduced simulation of nonlinear\nLagrangian hydrodynamics.", "AI": {"tldr": "Energy conservative quadrature-based model reduction for compressible Euler equations using data-driven reduced basis and hyperreduction with exact discrete total energy conservation.", "motivation": "To develop a structure-preserving reduced order model for Lagrangian hydrodynamics that maintains exact energy conservation while achieving computational efficiency.", "method": "Projection-based reduced model using data-driven reduced basis functions and hyperreduction via Empirical Quadrature Procedure (EQP), with a strongly energy conservative variant that enforces exact discrete total energy conservation.", "result": "Numerical experiments on four benchmark problems show total energy conservation to near machine precision while maintaining accuracy comparable to basic EQP formulation.", "conclusion": "The energy conservative EQP (CEQP) method is an effective structure-preserving hyperreduction strategy for reduced simulation of nonlinear Lagrangian hydrodynamics."}}
{"id": "2508.21390", "pdf": "https://arxiv.org/pdf/2508.21390", "abs": "https://arxiv.org/abs/2508.21390", "authors": ["Yu-Qiu Liu", "Hefeng Wang", "Hua Xiang"], "title": "Generalized quantum singular value transformation with application in quantum bi-conjugate gradient method", "categories": ["math.NA", "cs.NA", "quant-ph"], "comment": null, "summary": "Quantum signal processing (QSP) and generalized quantum signal processing\n(GQSP) are essential tools for implementing the block encoding of matrix\nfunctions. The achievable polynomials of QSP have restrictions on parity, while\nGQSP eliminates these restrictions. In this paper, we further investigate GQSP\nand present a quantum bi-conjugate gradient (BiCG) algorithm as an application.\nFirst, we extend GQSP, which constructs functions of unitary matrices, to\ngeneral matrices. We refer to this extension as generalized quantum singular\nvalue transformation (GQSVT). Subsequently, we implement the quantum BiCG\nmethod, utilizing GQSVT and swap test, which has a relatively shallow circuit\ndepth and requires a small number of ancilla qubits.", "AI": {"tldr": "This paper extends generalized quantum signal processing (GQSP) to handle general matrices (called GQSVT) and applies it to develop a quantum bi-conjugate gradient (BiCG) algorithm with shallow circuit depth and minimal ancilla qubits.", "motivation": "Quantum signal processing (QSP) has parity restrictions on achievable polynomials, while GQSP removes these restrictions but only works with unitary matrices. The authors aim to extend GQSP to general matrices and demonstrate practical applications.", "method": "The authors extend GQSP to handle general matrices, calling it generalized quantum singular value transformation (GQSVT). They then implement a quantum BiCG method using GQSVT and swap test techniques.", "result": "The developed quantum BiCG algorithm achieves relatively shallow circuit depth and requires only a small number of ancilla qubits, making it more practical for quantum computing implementations.", "conclusion": "The extension of GQSP to GQSVT enables efficient quantum algorithms for general matrices, with the quantum BiCG method serving as a practical application that demonstrates improved circuit efficiency and reduced resource requirements."}}
{"id": "2508.21151", "pdf": "https://arxiv.org/pdf/2508.21151", "abs": "https://arxiv.org/abs/2508.21151", "authors": ["Bego\u00f1a Barrios", "Bryan Pichucho", "Alexander Quaas"], "title": "Propagation in the Fisher-KPP equation with Mixed Operator", "categories": ["math.AP"], "comment": "21 Pages, 2 figures", "summary": "Our investigation focuses on the asymptotic spreading behavior of the\nFisher-KPP equation with a mixed local-nonlocal operator in the diffusion (see\nthe work by X. Cabr\\'e and J.-M. Roquejoffre, 2013, ref.[8]) to the setting of\nmixed diffusion, which involves both the classical and the fractional Laplacian\nin order to analyze the long-time dynamics of the equation. A key step in our\napproach involves the construction and detailed study of the heat kernel\nassociated with the mixed operator, which we use to develop a theory of mild\nsolutions and establish a comparison principle in suitable weighted function\nspaces.\n  This framework allows us to rigorously establish the non-existence of\ntraveling waves and characterize the large-time spreading rate of solutions. We\nshow that the influence of the fractional Laplacian dominates over the\nclassical Laplacian, especially in the initial layer, where it dictates the\nexponential propagation rate and the thickness of the solution tails.", "AI": {"tldr": "Analysis of Fisher-KPP equation with mixed local-nonlocal diffusion operator, showing fractional Laplacian dominates classical Laplacian in determining propagation rates and tail behavior.", "motivation": "To extend previous work on Fisher-KPP equations to mixed diffusion settings combining classical and fractional Laplacian operators, and to understand the long-time dynamics and spreading behavior of such equations.", "method": "Construction and detailed study of heat kernel associated with mixed operator, development of mild solutions theory, establishment of comparison principle in weighted function spaces.", "result": "Non-existence of traveling waves demonstrated, large-time spreading rate characterized. Fractional Laplacian dominates classical Laplacian, especially in initial layer where it dictates exponential propagation rate and solution tail thickness.", "conclusion": "The fractional component of mixed diffusion operators plays a dominant role in determining the asymptotic spreading behavior and propagation characteristics of Fisher-KPP equations."}}
{"id": "2508.21136", "pdf": "https://arxiv.org/pdf/2508.21136", "abs": "https://arxiv.org/abs/2508.21136", "authors": ["Tian Qiu", "Joseph E. Subotnik"], "title": "Fast Methods For Multisite Charge Transfer Processes I: Constrained, State Averaged CASSCF(1,M) and CASSCF(2M-1,M) Simulations", "categories": ["physics.comp-ph"], "comment": null, "summary": "We design a dynamically-weighted state-averaged constrained CASSCF to treat\n\\ul{e}lectrons or \\ul{h}oles moving between $n$ molecular fragments (where $n$\ncan be larger than 2). Within such a so-called eDSCn/hDSCn approach, we\nconsider configurations that are mutually single excitations of each other, and\nwe apply a generalized set of constraints to tailor the method for studying\ncharge transfer problems. The constrained optimization problem is efficiently\nsolved using a DIIS-SQP algorithm, thus maintaining computational efficiency.\nWe demonstrate the method for a finite Su-Schrieffer-Heeger (SSH) chain,\nsuccessfully reproducing the expected exponential decay of diabatic couplings\nwith distance. When combined with a gradient, the current extension immediately\nenables efficient nonadiabatic dynamics simulations of complex multi-state\ncharge transfer processes.", "AI": {"tldr": "Developed eDSCn/hDSCn method for multi-fragment charge transfer using constrained CASSCF with dynamic weights and efficient DIIS-SQP optimization.", "motivation": "To enable efficient treatment of charge transfer between multiple molecular fragments (n>2) and study complex multi-state charge transfer processes.", "method": "Dynamically-weighted state-averaged constrained CASSCF approach with generalized constraints for charge transfer, using DIIS-SQP algorithm for efficient optimization.", "result": "Successfully reproduces exponential decay of diabatic couplings with distance in SSH chain, enables efficient nonadiabatic dynamics simulations.", "conclusion": "The method provides an efficient framework for studying multi-fragment charge transfer problems and enables complex nonadiabatic dynamics simulations."}}
{"id": "2508.21293", "pdf": "https://arxiv.org/pdf/2508.21293", "abs": "https://arxiv.org/abs/2508.21293", "authors": ["H. Peng", "T. W. Huang", "C. N. Wu", "K. Jiang", "R. Li", "C. Riconda", "S. Weber", "C. T. Zhou"], "title": "Coherent attosecond pulses generated by a relativistic electron beam interacting with an intense laser at a grazing angle", "categories": ["physics.plasm-ph"], "comment": null, "summary": "The interaction between relativistic electron beams and intense laser fields\nhas been extensively studied for generating high-energy radiation. However,\nachieving coherent radiation from such interactions needs to precisely control\nthe phase matching of the radiationg electrons, which has proven to be\nexceptionally challenging. In this study, we demonstrate that coherent\nattosecond radiation can be produced when a laser pulse interacts at grazing\nangle with a relativistic electron beam. The electrons oscillate in the laser\nfield and are modulated with a superluminal phase, coherent ultrashort pulse\ntrains are produced in the far field at the Cherenkov angle. This is verified\nby theoretical modeling and numerical simulations, including three-dimensional\nparticle-in-cell (PIC) simulations and far-field time-domain radiation\nsimulations. Based on our proposed scheme, high-repetition-rate, compact, and\nhigh-energy attosecond pulse sources are feasible.", "AI": {"tldr": "Coherent attosecond radiation generation through grazing angle interaction between laser pulses and relativistic electron beams, producing ultrashort pulse trains at Cherenkov angle.", "motivation": "Achieving coherent radiation from relativistic electron beams interacting with intense laser fields has been challenging due to difficulties in precisely controlling the phase matching of radiating electrons.", "method": "Using grazing angle interaction between laser pulses and relativistic electron beams, theoretical modeling, 3D particle-in-cell (PIC) simulations, and far-field time-domain radiation simulations.", "result": "Demonstrated production of coherent attosecond radiation with electrons oscillating in laser field and modulated with superluminal phase, generating coherent ultrashort pulse trains at Cherenkov angle.", "conclusion": "The proposed scheme enables feasible development of high-repetition-rate, compact, and high-energy attosecond pulse sources."}}
{"id": "2508.21410", "pdf": "https://arxiv.org/pdf/2508.21410", "abs": "https://arxiv.org/abs/2508.21410", "authors": ["Shubhangini Gupta", "Sourav Banerjee", "Tamal Pramanick"], "title": "A Biologically Motivated Finite Difference Approach for Simulating Singularly Perturbed Vertical Motion in Human Gait", "categories": ["math.NA", "cs.NA", "65L11, 65L70, 65L80 } 65L11, 65L70, 65L80 } 65L11, 65L70, 65L80"], "comment": null, "summary": "In this study, we present a simulation-based numerical method for solving a\nclass of singularly perturbed second-order differential equations that come\nfrom a simplified biologically motivated model of human gait. Important\nphysical factors such as gravity, damping, and leg stiffness are included in\nthe model, which also depicts the vertical motion of the center of mass of the\nbody during walking or running. Most of the time, standard numerical methods\nare ineffective in resolving boundary layer behavior that occurs due to the\nsmall perturbation parameter in the governing equation. We use a domain\ndecomposition technique to divide the problem domain into inner and outer\nregions to tackle this difficulty. The boundary layer resolves the steep\ngradients. We applied a time-rescaling transformation to the inner region. Each\nsubdomain is discretized, and the resulting tridiagonal systems are efficiently\nsolved using the Thomas algorithm within the mixed finite difference framework.\nA detailed convergence analysis demonstrates second-order accuracy in space.\nThe numerical results validate the proposed scheme's accuracy, stability, and\nefficiency through experiments based on modified human gait models. The\nframework serves as a fundamental tool for biomechanical simulation. The\nmodeling is a foundation for future research, incorporating nonlinearities,\ntime delays, and real-world scenarios data on how people walk.", "AI": {"tldr": "A simulation-based numerical method for solving singularly perturbed second-order differential equations from human gait modeling, using domain decomposition and time-rescaling to handle boundary layers with second-order accuracy.", "motivation": "Standard numerical methods fail to resolve boundary layer behavior in singularly perturbed differential equations from human gait models that include gravity, damping, and leg stiffness factors.", "method": "Domain decomposition technique dividing the problem into inner and outer regions, time-rescaling transformation for inner region, mixed finite difference framework with Thomas algorithm for solving tridiagonal systems.", "result": "Second-order accuracy in space demonstrated through convergence analysis, with validated accuracy, stability, and efficiency through experiments on modified human gait models.", "conclusion": "The framework serves as a fundamental tool for biomechanical simulation and provides foundation for future research incorporating nonlinearities, time delays, and real-world walking data."}}
{"id": "2508.21174", "pdf": "https://arxiv.org/pdf/2508.21174", "abs": "https://arxiv.org/abs/2508.21174", "authors": ["Fioralba Cakoni", "Shari Moskow"], "title": "Asymptotic expansions for the transmission eigenvalues of periodic scatterers of bounded support", "categories": ["math.AP"], "comment": null, "summary": "We consider the transmission eigenvalues for a bounded scatterer with a\nperiodically varying index of refraction, and derive the first order\ncorrections to the limiting transmission eigenvalues. We assume the scatterer\ncontrast to be of one sign, in which case the transmission eigenvalue problem\ncan be written in terms of operators corresponding to a fourth order PDE with\nperiodic coefficients. We perform two-scale asymptotics for this biharmonic\ntype homogenization problem and show convergence estimates which require a\nboundary corrector function, and this boundary corrector function appears in\nthe formula for the transmission eigenvalues correction.", "AI": {"tldr": "First-order corrections for transmission eigenvalues in scatterers with periodic refractive index variations, using two-scale asymptotics for biharmonic homogenization with boundary correctors.", "motivation": "To understand how periodic variations in refractive index affect transmission eigenvalues, which are important for inverse scattering problems and material characterization.", "method": "Two-scale asymptotics applied to a fourth-order PDE with periodic coefficients, deriving boundary corrector functions for the biharmonic homogenization problem.", "result": "Derived first-order corrections to transmission eigenvalues that incorporate boundary corrector functions, with convergence estimates established.", "conclusion": "Periodic refractive index variations produce measurable corrections to transmission eigenvalues through boundary effects, providing analytical framework for inverse scattering applications."}}
{"id": "2508.21139", "pdf": "https://arxiv.org/pdf/2508.21139", "abs": "https://arxiv.org/abs/2508.21139", "authors": ["Tian Qiu", "Joseph E. Subotnik"], "title": "Fast Methods For Multisite Charge Transfer Processes II: Analytic Nuclear Gradients and Nonadiabatic Dynamics For cCASSCF(1,M) and cCASSCF(2M-1,M) Wavefunctions", "categories": ["physics.comp-ph"], "comment": null, "summary": "We derive and implement analytic nuclear gradients and derivative couplings\nfor a constrained Complete Active Space Self-Consistent Field with a small\nactive space designed to model electron or hole transfer. Using a Lagrangian\nformalism, we are able to differentiate both the CASSCF energy and the\nconstraint (which is required for globally smooth surfaces), and the resulting\nefficient algorithm can be immediately applied to nonadiabatic dynamics\nsimulations of charge transfer processes. Here, we run initial surface-hopping\nsimulations of a proton coupled electron transfer event for a phenoxyl-phenol\nsystem.", "AI": {"tldr": "Analytic nuclear gradients and derivative couplings for constrained CASSCF with small active spaces for electron/hole transfer modeling", "motivation": "To enable efficient nonadiabatic dynamics simulations of charge transfer processes by providing analytic derivatives for constrained CASSCF methods", "method": "Used Lagrangian formalism to differentiate both CASSCF energy and constraints, ensuring globally smooth surfaces. Implemented efficient algorithm for nuclear gradients and derivative couplings", "result": "Successfully derived and implemented analytic derivatives. Applied to surface-hopping simulations of proton coupled electron transfer in phenoxyl-phenol system", "conclusion": "The developed method provides an efficient approach for studying charge transfer dynamics and can be immediately applied to nonadiabatic simulations of electron/hole transfer processes"}}
{"id": "2508.21532", "pdf": "https://arxiv.org/pdf/2508.21532", "abs": "https://arxiv.org/abs/2508.21532", "authors": ["Manohar Teja Kalluri", "Andrew Hillier", "Ben Snow"], "title": "Quantifying Reconnection and it's Dynamical Role in 2D Magnetic Rayleigh-Taylor Turbulence", "categories": ["physics.plasm-ph", "astro-ph.SR", "physics.flu-dyn", "physics.geo-ph"], "comment": "22 pages, 27 figures", "summary": "Magnetic Rayleigh-Taylor instability (MRTI) governs material transport and\nmixing in astrophysical and laboratory plasmas under the influence of gravity\nand magnetic fields. While magnetic reconnection is known to occur during MRTI\nevolution, its role in the evolution and energy dynamics remains poorly\nunderstood. Here, we present a comprehensive analysis of the role of\nreconnection in the two-dimensional MRTI dynamics, using high-resolution\nsimulations. We establish that reconnection, through facilitating plume merger,\nrelieving magnetic tension, and enabling continued instability growth, forms an\nessential component for the long-term instability evolution. To quantify the\nrole of reconnection in energy dynamics, we develop a robust automated\nreconnection detection algorithm and perform a statistical analysis across a\nrange of magnetic field strengths. We find that reconnection accounts for up to\n$80\\%$ of the magnetic-to-kinetic energy transfer in the weak magnetic field\nregime, while contributing minimally ($\\approx 3\\%$) to magnetic energy\ndissipation. Our results establish magnetic reconnection as a critical\nmechanism that regulates large-scale MRTI dynamics, with implications for\nastrophysical plasmas and turbulent mixing in magnetized flows.", "AI": {"tldr": "Magnetic reconnection plays a crucial role in magnetic Rayleigh-Taylor instability evolution, facilitating plume merger and enabling continued growth, accounting for up to 80% of magnetic-to-kinetic energy transfer in weak field regimes.", "motivation": "Understanding the role of magnetic reconnection in magnetic Rayleigh-Taylor instability evolution and energy dynamics, which remains poorly understood despite known occurrence during MRTI development.", "method": "High-resolution 2D simulations with a robust automated reconnection detection algorithm and statistical analysis across various magnetic field strengths.", "result": "Reconnection facilitates plume merger, relieves magnetic tension, and enables continued instability growth. It accounts for up to 80% of magnetic-to-kinetic energy transfer in weak field regimes but only ~3% of magnetic energy dissipation.", "conclusion": "Magnetic reconnection is a critical mechanism regulating large-scale MRTI dynamics with significant implications for astrophysical plasmas and turbulent mixing in magnetized flows."}}
{"id": "2508.21427", "pdf": "https://arxiv.org/pdf/2508.21427", "abs": "https://arxiv.org/abs/2508.21427", "authors": ["Ferdinand Thein", "Hendrik Ranocha"], "title": "Computing Radially-Symmetric Solutions of the Ultra-Relativistic Euler Equations with Entropy-Stable Discontinuous Galerkin Methods", "categories": ["math.NA", "cs.NA", "math-ph", "math.MP", "65M06, 65M20, 65M70, 35L45"], "comment": null, "summary": "The ultra--relativistic Euler equations describe gases in the relativistic\ncase when the thermal energy dominates. These equations for an ideal gas are\ngiven in terms of the pressure, the spatial part of the dimensionless\nfour-velocity, and the particle density. Kunik et al.\\ (2024,\nhttps://doi.org/10.1016/j.jcp.2024.113330) proposed genuine multi--dimensional\nbenchmark problems for the ultra--relativistic Euler equations. In particular,\nthey compared full two-dimensional discontinuous Galerkin simulations for\nradially symmetric problems with solutions computed using a specific\none-dimensional scheme. Of particular interest in the solutions are the\nformation of shock waves and a pressure blow-up. In the present work we derive\nan entropy-stable flux for the ultra--relativistic Euler equations. Therefore,\nwe derive the main field (or entropy variables) and the corresponding\npotentials. We then present the entropy-stable flux and conclude with\nsimulation results for different test cases both in 2D and in 3D.", "AI": {"tldr": "Derived entropy-stable flux for ultra-relativistic Euler equations using main field/entropy variables and potentials, with 2D/3D simulation results.", "motivation": "To address shock wave formation and pressure blow-up in ultra-relativistic gases by developing entropy-stable numerical methods for better simulation accuracy.", "method": "Derived main field (entropy variables) and corresponding potentials, then developed entropy-stable flux formulation for ultra-relativistic Euler equations.", "result": "Successfully implemented entropy-stable flux and demonstrated its performance through 2D and 3D simulations of various test cases.", "conclusion": "The developed entropy-stable flux provides improved numerical stability for simulating ultra-relativistic Euler equations, particularly for problems involving shock waves and pressure blow-up."}}
{"id": "2508.21183", "pdf": "https://arxiv.org/pdf/2508.21183", "abs": "https://arxiv.org/abs/2508.21183", "authors": ["Bastian Hilder", "Christian Kuehn"], "title": "Pattern formation and nonlinear waves close to a 1:1 resonant Turing and Turing--Hopf instability", "categories": ["math.AP", "nlin.PS", "35B32, 35B34, 35B36, 34E15, 34C37, 37L10"], "comment": "45 pages, 16 Figures", "summary": "In this paper, we analyse the dynamics of a pattern-forming system close to\nsimultaneous Turing and Turing--Hopf instabilities, which have a 1:1 spatial\nresonance, that is, they have the same critical wave number. For this, we\nconsider a system of coupled Swift--Hohenberg equations with dispersive terms\nand general, smooth nonlinearities. Close to the onset of instability, we\nderive a system of two coupled complex Ginzburg--Landau equations with a\nsingular advection term as amplitude equations and justify the approximation by\nproviding error estimates. We then construct space-time periodic solutions to\nthe amplitude equations, as well as fast-travelling front solutions, which\nconnect different space-time periodic states. This yields the existence of\nsolutions to the pattern-forming system on a finite, but long time interval,\nwhich model the spatial transition between different patterns. The construction\nis based on geometric singular perturbation theory exploiting the fast\ntravelling speed of the fronts. Finally, we construct global, spatially\nperiodic solutions to the pattern-forming system by using centre manifold\nreduction, normal form theory and a variant of singular perturbation theory to\nhandle fast oscillatory higher-order terms.", "AI": {"tldr": "Analysis of pattern-forming systems near simultaneous Turing and Turing-Hopf instabilities with 1:1 spatial resonance using coupled Swift-Hohenberg equations and amplitude equations.", "motivation": "To understand the dynamics of pattern-forming systems when both Turing and Turing-Hopf instabilities occur simultaneously with the same critical wave number, which is a complex scenario in pattern formation.", "method": "Used coupled Swift-Hohenberg equations with dispersive terms and general nonlinearities. Derived coupled complex Ginzburg-Landau equations with singular advection as amplitude equations. Applied geometric singular perturbation theory, center manifold reduction, and normal form theory.", "result": "Successfully derived amplitude equations with error estimates, constructed space-time periodic solutions and fast-traveling front solutions connecting different patterns, and obtained global spatially periodic solutions.", "conclusion": "The approach provides a framework for analyzing spatial transitions between different patterns in complex pattern-forming systems with simultaneous instabilities, with applications to various physical and biological systems."}}
{"id": "2508.21208", "pdf": "https://arxiv.org/pdf/2508.21208", "abs": "https://arxiv.org/abs/2508.21208", "authors": ["Chayanon Wichitrnithed", "Eirik Valseth", "Clint Dawson"], "title": "GPU-acceleration of the Discontinuous Galerkin Shallow Water Equations Solver (DG-SWEM) using CUDA and OpenACC", "categories": ["physics.comp-ph"], "comment": null, "summary": "This paper presents a porting of DG-SWEM, a discontinuous Galerkin solver for\ncoastal ocean circulation, and in particular storm surge, to GPU using two\nseparate approaches: CUDA Fortran and OpenACC. Time-explicit discontinuous\nGalerkin methods have been shown to exhibit a large amount of data parallelism\ndue to the loose coupling between elements, and thus are naturally mapped to\nthe GPU architecture. For each porting approach, we discuss the code design,\nease of programming, and performance when running on realistic use cases.\nSpecifically for the OpenACC version, we also aim to preserve maintainability\nwithin the same codebase through using Unified Memory. We test the codes on\nNVIDIA's Grace Hopper chip and compare the GPU performance on each node to the\nMPI version on a single CPU node (144 cores).", "AI": {"tldr": "Porting of DG-SWEM coastal ocean solver to GPU using CUDA Fortran and OpenACC, comparing performance and maintainability on NVIDIA Grace Hopper vs CPU.", "motivation": "Discontinuous Galerkin methods for coastal ocean circulation and storm surge simulation exhibit high data parallelism due to loose element coupling, making them naturally suitable for GPU acceleration to improve computational performance.", "method": "Two separate GPU porting approaches: CUDA Fortran and OpenACC. For OpenACC, used Unified Memory to maintain single codebase maintainability. Tested on NVIDIA Grace Hopper chip and compared GPU performance per node against MPI version on single CPU node (144 cores).", "result": "Performance comparison between GPU implementations (CUDA Fortran and OpenACC) and traditional CPU-based MPI version on realistic use cases, evaluating computational efficiency on modern GPU architecture.", "conclusion": "GPU acceleration using both CUDA Fortran and OpenACC approaches provides significant performance benefits for discontinuous Galerkin coastal ocean solvers, with OpenACC offering additional maintainability advantages through Unified Memory while preserving single codebase."}}
{"id": "2508.21478", "pdf": "https://arxiv.org/pdf/2508.21478", "abs": "https://arxiv.org/abs/2508.21478", "authors": ["Qiao-Ping Chen", "Hongyu Liu", "Zejun Sun", "Li-Li Wang", "Guang-Hui Zheng"], "title": "Inverse Random Source Problem for the Helmholtz Equation from Statistical Phaseless Data", "categories": ["math.NA", "cs.NA"], "comment": null, "summary": "This paper investigates the problem of reconstructing a random source from\nstatistical phaseless data for the two-dimensional Helmholtz equation. The\nmajor challenge of this problem is non-uniqueness, which we overcome through a\nreference source technique. Firstly, we introduce some artificially added point\nsources into the inverse random source system and derive phase retrieval (PR)\nformulas for the expectation and variance of the radiated fields. This paper\nrigorously analyze the uniqueness and stability of the recovered statistics of\nthe radiated fields. Afterwards, since the direct problem has a unique mild\nsolution, by examining the expectation and variance of this solution and\ncombined with the phase retrieval formulas, we derive the Fredholm integral\nequations to solve the inverse random source problem (IRSP). We prove the\nstability of the corresponding integral equations. To quantify the uncertainty\nof the random source, we utilize the Bayesian method to reconstruct the random\nsource and establish the well-posedness of the posterior distribution. Finally,\nnumerical experiments demonstrate the effectiveness of the proposed method and\nvalidate the theoretical results.", "AI": {"tldr": "This paper addresses non-uniqueness in reconstructing random sources from phaseless Helmholtz equation data using reference sources, phase retrieval formulas, and Bayesian methods with proven stability.", "motivation": "To overcome the fundamental challenge of non-uniqueness in reconstructing random sources from statistical phaseless measurements in the 2D Helmholtz equation, which is crucial for applications where only intensity data is available.", "method": "Introduces artificial point sources as references, derives phase retrieval formulas for field statistics, analyzes uniqueness/stability, formulates Fredholm integral equations for inverse problem, and employs Bayesian methods for uncertainty quantification.", "result": "The method successfully reconstructs random sources from phaseless data, with rigorous proofs of uniqueness and stability for both phase retrieval and inverse problems, validated by numerical experiments.", "conclusion": "The proposed reference source technique combined with Bayesian inference provides an effective and stable approach for solving the inverse random source problem with phaseless data, overcoming the inherent non-uniqueness challenge."}}
{"id": "2508.21198", "pdf": "https://arxiv.org/pdf/2508.21198", "abs": "https://arxiv.org/abs/2508.21198", "authors": ["Elena M\u00e4der-Baumdicker", "Robin Neumayer", "Jiewon Park", "Melanie Rupflin"], "title": "Quantitative estimates for the relative isoperimetric problem and its gradient flow outside convex bodies in the plane", "categories": ["math.AP", "math.DG"], "comment": null, "summary": "We prove three related quantitative results for the relative isoperimetric\nproblem outside a convex body $\\Omega$ in the plane: (1) {\\L}ojasiewicz\nestimates and quantitative rigidity for critical points, (2) rates of\nconvergence for the gradient flow, and (3) quantitative stability for\nminimizers. These results come with explicit constants and optimal\nexponents/rates, and hold whenever a simple two-dimensional auxiliary\nvariational problem for circular arcs outside of $\\Omega$ is nondegenerate. The\nproofs are inter-related, and in particular, for the first time in the context\nof isoperimetric problems, a flow approach is used to prove quantitative\nstability for minimizers.", "AI": {"tldr": "Quantitative results for relative isoperimetric problems outside convex bodies in the plane, including Lojasiewicz estimates, gradient flow convergence rates, and stability for minimizers with explicit constants and optimal rates.", "motivation": "To establish rigorous quantitative analysis of relative isoperimetric problems outside convex bodies, providing explicit constants and optimal rates that hold under nondegeneracy conditions of a simple 2D variational problem.", "method": "Proving three inter-related results: (1) Lojasiewicz estimates and quantitative rigidity for critical points, (2) rates of convergence for gradient flow, and (3) quantitative stability for minimizers. Uses a novel flow approach to prove stability for minimizers in isoperimetric problems.", "result": "Obtained explicit constants and optimal exponents/rates for all three results. The proofs are interconnected and the approach works whenever a two-dimensional auxiliary variational problem for circular arcs outside the convex body is nondegenerate.", "conclusion": "The paper provides comprehensive quantitative analysis of relative isoperimetric problems with explicit bounds and optimal rates, introducing a novel flow-based approach for proving stability of minimizers in this context for the first time."}}
{"id": "2508.21200", "pdf": "https://arxiv.org/pdf/2508.21200", "abs": "https://arxiv.org/abs/2508.21200", "authors": ["Davoud Mirzaei", "Behnam Hashemi", "Vahid Azimi-Mousolou"], "title": "LREI: A fast numerical solver for quantum Landau-Lifshitz equations", "categories": ["quant-ph", "cond-mat.mes-hall", "cond-mat.mtrl-sci", "cs.NA", "math.NA", "physics.comp-ph"], "comment": "23 pages and 12 figures", "summary": "We develop LREI (Low-Rank Eigenmode Integration), a memory- and\ntime-efficient scheme for solving quantum Landau-Lifshitz (q-LL) and quantum\nLandau-Lifshitz-Gilbert (q-LLG) equations, which govern spin dynamics in open\nquantum systems. Although system size grows exponentially with the number of\nspins, our approach exploits the low-rank structure of the density matrix and\nthe sparsity of Hamiltonians to avoid full matrix computations. By representing\ndensity matrices via low-rank factors and applying Krylov subspace methods for\npartial eigendecompositions, we reduce the per-step complexity of Runge-Kutta\nand Adams-Bashforth schemes from $\\mathcal{O}(N^3)$ to $\\mathcal{O}(r^2N)$,\nwhere $N = 2^n$ is the Hilbert space dimension for $n$ spins and $r \\ll N$ the\neffective rank. Similarly, memory costs shrink from $\\mathcal{O}(N^2)$ to\n$\\mathcal{O}(rN)$, since no full $N\\times N$ matrices are formed. A key advance\nis handling the invariant subspace of zero eigenvalues. By using Householder\nreflectors built for the dominant eigenspace, we perform the solution entirely\nwithout large matrices. For example, a time step of a twenty-spin system, with\ndensity matrix size over one million, now takes only seconds on a standard\nlaptop. Both Runge-Kutta and Adams-Bashforth methods are reformulated to\npreserve physical properties of the density matrix throughout evolution. This\nlow-rank algorithm enables simulations of much larger spin systems, which were\npreviously infeasible, providing a powerful tool for comparing q-LL and q-LLG\ndynamics, testing each model validity, and probing how quantum features such as\ncorrelations and entanglement evolve across different regimes of system size\nand damping.", "AI": {"tldr": "LREI is an efficient low-rank algorithm for solving quantum spin dynamics equations that reduces computational complexity from O(N\u00b3) to O(r\u00b2N) and memory from O(N\u00b2) to O(rN), enabling simulation of large spin systems previously infeasible.", "motivation": "Quantum Landau-Lifshitz and Landau-Lifshitz-Gilbert equations govern spin dynamics in open quantum systems, but traditional methods face exponential growth in computational cost with system size, making large spin simulations impractical.", "method": "Exploits low-rank structure of density matrix and Hamiltonian sparsity using low-rank factor representation and Krylov subspace methods for partial eigendecompositions. Uses Householder reflectors to handle zero eigenvalue invariant subspace without forming large matrices.", "result": "Achieves significant computational savings - 20-spin system simulations (with million-dimensional density matrices) now take only seconds on standard laptops instead of being infeasible. Both Runge-Kutta and Adams-Bashforth methods preserve physical properties.", "conclusion": "LREI enables practical simulation of much larger quantum spin systems, providing a powerful tool for comparing q-LL and q-LLG dynamics, testing model validity, and studying quantum features like correlations and entanglement across different regimes."}}
{"id": "2508.21506", "pdf": "https://arxiv.org/pdf/2508.21506", "abs": "https://arxiv.org/abs/2508.21506", "authors": ["Dario A. Bini", "Beatrice Meini", "Federico Poloni"], "title": "The Derivative of Kemeny's Constant as a Centrality Measure in Undirected Graphs", "categories": ["math.NA", "cs.NA"], "comment": null, "summary": "Kemeny's constant quantifies a graph's connectivity by measuring the average\ntime for a random walker to reach any other vertex. We introduce two concepts\nof the directional derivative of Kemeny's constant with respect to an edge and\nuse them to define centrality measures for edges and non-edges in the graph.\nAdditionally, we present a sensitivity measure of Kemeny's constant. An\nexplicit expression for these quantities involving the inverse of the modified\ngraph Laplacian is provided, which is valid even for cut-edges. These measures\nare connected to the one introduced in [Altafini et al., SIMAX 2023], and\nalgorithms for their computation are included. The benefits of these measures\nare discussed, along with applications to road networks and link prediction\nanalysis. For one-path graphs, an explicit expression for these measures is\ngiven in terms of the edge weights.", "AI": {"tldr": "This paper introduces directional derivatives of Kemeny's constant to define edge/non-edge centrality measures and sensitivity analysis for graph connectivity.", "motivation": "To develop quantitative measures that assess how individual edges and non-edges affect graph connectivity as measured by Kemeny's constant, which captures the average random walk time between vertices.", "method": "Introduces two concepts of directional derivative of Kemeny's constant with respect to edges, provides explicit expressions using the inverse of modified graph Laplacian, develops algorithms for computation, and validates on road networks and link prediction.", "result": "Successfully defines centrality measures for edges and non-edges, establishes connections to existing measures, provides computational algorithms, and demonstrates applications including handling cut-edges and one-path graphs with explicit weight expressions.", "conclusion": "The proposed directional derivative framework provides effective tools for analyzing edge importance in graph connectivity, with practical applications in network analysis and link prediction, particularly valuable for understanding sensitivity to edge modifications."}}
{"id": "2508.21214", "pdf": "https://arxiv.org/pdf/2508.21214", "abs": "https://arxiv.org/abs/2508.21214", "authors": ["Benjamin Foster", "Josep Gallegos"], "title": "Propagation of smallness near codimension two for gradients of harmonic functions", "categories": ["math.AP", "31B05"], "comment": "14 pages", "summary": "Let $u$ be a harmonic function in the unit ball $B_1 \\subset \\mathbb R^n$,\nnormalized so that its gradient has magnitude at most 1 on the unit ball. We\nshow that if the gradient of $u$ is $\\epsilon$-small in size on a set $E\\subset\nB_{1/2}$ with positive $(n-2+\\delta)$-dimensional Hausdorff content for some\n$\\delta>0$, then $\\sup_{B_{1/2}} |\\nabla u| \\leq C \\epsilon^\\alpha$ with\n$C,\\alpha>0$ depending only on $n,\\delta$ and the $(n-2+\\delta)$-Hausdorff\ncontent of $E$. This is an improvement over a similar result of Logunov and\nMalinnikova that required $\\delta>1-c_n$ for a small dimensional constant $c_n$\nand reaches the sharp threshold for the dimension of the smallness sets from\nwhich propagation of smallness can occur.", "AI": {"tldr": "Harmonic functions with bounded gradient show propagation of smallness: if gradient is \u03b5-small on a set with positive (n-2+\u03b4)-dimensional Hausdorff content, then gradient remains small throughout half-ball with bound C\u03b5^\u03b1.", "motivation": "To improve upon previous results by Logunov and Malinnikova and achieve the sharp threshold for dimension of smallness sets from which propagation of smallness can occur for harmonic functions.", "method": "Analysis of harmonic functions in unit ball with bounded gradient, studying how smallness of gradient on sets with specific Hausdorff dimension propagates throughout the domain.", "result": "Proved that if gradient is \u03b5-small on set E with positive (n-2+\u03b4)-dimensional Hausdorff content, then sup|\u2207u| \u2264 C\u03b5^\u03b1 in half-ball, with constants depending only on n, \u03b4 and Hausdorff content.", "conclusion": "The result establishes optimal dimension threshold (n-2+\u03b4) for propagation of smallness in harmonic functions, improving previous work and reaching sharp dimensional requirements."}}
{"id": "2508.21557", "pdf": "https://arxiv.org/pdf/2508.21557", "abs": "https://arxiv.org/abs/2508.21557", "authors": ["Mart\u00edn Hern\u00e1ndez"], "title": "Random domain decomposition for parabolic PDEs on graphs", "categories": ["math.NA", "cs.NA", "math.AP", "35R02, 65C99, 65M55, 65M75, 68Q25"], "comment": null, "summary": "The simulation of complex systems, such as gas transport in large pipeline\nnetworks, often involves solving PDEs posed on intricate graph structures. Such\nproblems require considerable computational and memory resources. The Random\nBatch Method (RBM) has shown promise in addressing these challenges via\nstochastic decomposition techniques. In this paper, we apply the RBM at the PDE\nlevel for parabolic equations on graphs, without assuming any preliminary\ndiscretization in space or time. We consider a non-overlapping domain\ndecomposition in which the PDE coefficients and source terms are randomized. We\nprove that the resulting RBM-based scheme converges, in the mean-square sense\nand uniformly in time, to the true PDE solution with first-order accuracy in\nthe RBM step size. Numerical experiments confirm this convergence rate and\ndemonstrate substantial reductions in both memory usage and computational time\ncompared to solving on the full graph. Moreover, these advantages persist\nacross different time discretization schemes.", "AI": {"tldr": "RBM applied to parabolic PDEs on graphs without discretization, using randomized domain decomposition. Proves mean-square convergence with first-order accuracy in step size. Shows significant memory and computation savings.", "motivation": "Complex systems like gas pipeline networks require solving PDEs on graphs, which demands high computational and memory resources. RBM offers stochastic decomposition to address these challenges efficiently.", "method": "Apply Random Batch Method at PDE level for parabolic equations on graphs without space/time discretization. Use non-overlapping domain decomposition with randomized coefficients and source terms.", "result": "Proven mean-square convergence to true PDE solution with first-order accuracy in RBM step size. Numerical experiments confirm convergence rate and show substantial reductions in memory usage and computational time compared to full graph solving.", "conclusion": "RBM provides an effective stochastic approach for solving parabolic PDEs on graphs, offering computational efficiency and memory savings while maintaining convergence properties across different time discretization schemes."}}
{"id": "2508.21464", "pdf": "https://arxiv.org/pdf/2508.21464", "abs": "https://arxiv.org/abs/2508.21464", "authors": ["Nicolas Rougerie", "Qiyun Yang"], "title": "1D quasi-solutions of the 2D Chern-Simons-Schr{\u00f6}dinger system", "categories": ["math.AP", "cond-mat.mes-hall", "cond-mat.quant-gas", "math-ph", "math.MP"], "comment": null, "summary": "We study a mean-field model for a system of 2D abelian anyons, given by the\ndynamics of a Schr{\\\"o}dinger matter field coupled to a Chern-Simons gauge\nfield. We derive an effective 1D equation by adding a strongly anisotropic\ntrapping potential (wave-guide) acting on the Schr{\\\"o}dinger field, and\ntracing out the tight confinement direction. The effective dynamics in the\nloose direction of the wave-guide turns out to be governed by the classical 1D\nquintic NLS equation.", "AI": {"tldr": "Study of 2D abelian anyons using Schrodinger-Chern-Simons model with anisotropic trapping, leading to effective 1D quintic NLS equation.", "motivation": "To understand the dynamics of 2D abelian anyon systems and derive simplified effective equations through dimensional reduction.", "method": "Used a mean-field model combining Schrodinger matter field with Chern-Simons gauge field, applied strongly anisotropic trapping potential (wave-guide), and traced out the tight confinement direction to obtain 1D effective dynamics.", "result": "Derived that the effective dynamics in the loose direction of the wave-guide is governed by the classical 1D quintic nonlinear Schrodinger (NLS) equation.", "conclusion": "The dimensional reduction technique successfully transforms the complex 2D anyon system into a more tractable 1D quintic NLS equation, providing a simplified framework for studying anyon dynamics."}}
{"id": "2508.21425", "pdf": "https://arxiv.org/pdf/2508.21425", "abs": "https://arxiv.org/abs/2508.21425", "authors": ["Long Zhang", "Ziqi Ren", "Li Sun", "Yihua Gao", "Deli Wang", "Junjie He", "Guoying Gao"], "title": "When Energy and Information Revolutions Meet 2D Janus", "categories": ["physics.app-ph", "cond-mat.mes-hall", "cond-mat.mtrl-sci", "physics.chem-ph", "physics.comp-ph"], "comment": "114 pages, 21 figures, and 7 tables", "summary": "The depletion of energy sources, worsening environmental issues, and the\nquantum limitations of integrated circuits for information storage in the\npost-Moore era, are pressing global concerns. Fortunately, two-dimensional (2D)\nJanus materials, possessing broken spatial symmetry, with emerging\npressure-dependent and non-linear optical response, piezoelectricity, valley\npolarization, Rashba spin splitting and more, have established a substantial\nplatform for exploring and applying modifiable physical, chemical and\nbiological properties in material science and offered a promising solution for\nthese energy and information issues. To furnish researchers with a\ncomprehensive repository of 2D Janus family, this review systematically\nsummarizes their theoretical predictions, experimental preparations, and\nmodulation strategies. It also retrospectively outlines the recent advances in\nmodifiable properties, applications, and inherent mechanisms in optics,\ncatalysis, piezoelectricity, electrochemistry, thermoelectricity, magnetism,\nand electronics, with a focus on experimentally realized hexagonal and trigonal\nJanus structures. Additionally, their current research state is summarized, and\npotential opportunities and challenges that may arise are highlighted. Overall,\nthis review aims to serve as a valuable resource for designing, fabricating,\nregulating, and applying 2D Janus systems, both theoretically and\nexperimentally. This review will strongly promote the advanced academic\ninvestigations and industrial applications of 2D Janus materials in energy and\ninformation fields.", "AI": {"tldr": "This review paper provides a comprehensive analysis of 2D Janus materials, covering their theoretical predictions, experimental preparations, properties, and applications in energy and information fields.", "motivation": "Addressing global concerns of energy depletion, environmental issues, and quantum limitations in post-Moore era information storage by exploring 2D Janus materials with unique asymmetric properties.", "method": "Systematic review summarizing theoretical predictions, experimental preparations, modulation strategies, and recent advances in modifiable properties across optics, catalysis, piezoelectricity, electrochemistry, thermoelectricity, magnetism, and electronics.", "result": "Comprehensive repository of 2D Janus family with focus on experimentally realized hexagonal and trigonal structures, highlighting their pressure-dependent and non-linear optical responses, piezoelectricity, valley polarization, and Rashba spin splitting.", "conclusion": "The review serves as a valuable resource for designing, fabricating, regulating, and applying 2D Janus systems, promoting both academic investigations and industrial applications in energy and information fields."}}
{"id": "2508.21630", "pdf": "https://arxiv.org/pdf/2508.21630", "abs": "https://arxiv.org/abs/2508.21630", "authors": ["Stefano Bonetti", "Michele Botti", "Paola F. Antonietti"], "title": "Conforming and discontinuous discretizations of non-isothermal Darcy-Forchheimer flows", "categories": ["math.NA", "cs.NA", "65N12, 65N30, 74F05, 76S05"], "comment": null, "summary": "We present and analyze in a unified setting two schemes for the numerical\ndiscretization of a Darcy-Forchheimer fluid flow model coupled with an\nadvection-diffusion equation modeling the temperature distribution in the\nfluid. The first approach is based on fully discontinuous Galerkin\ndiscretization spaces. In contrast, in the second approach, the velocity is\napproximated in the Raviart-Thomas space, and the pressure and temperature are\nstill piecewise discontinuous. A fixed-point linearization strategy, naturally\ninducing an iterative splitting solution, is proposed for treating the\nnonlinearities of the problem. We present a unified stability analysis and\nprove the convergence of the iterative algorithm under mild requirements on the\nproblem data. A wide set of two- and three-dimensional simulations is presented\nto assess the error decay and demonstrate the practical performance of the\nproposed approaches in physically sound test cases.", "AI": {"tldr": "Two numerical schemes for Darcy-Forchheimer fluid flow coupled with advection-diffusion temperature modeling, using different discontinuous Galerkin approaches with fixed-point linearization for nonlinearities.", "motivation": "To develop and analyze unified numerical discretization methods for coupled Darcy-Forchheimer fluid flow and temperature distribution problems, addressing nonlinearities through efficient computational approaches.", "method": "Two approaches: 1) Fully discontinuous Galerkin discretization, 2) Raviart-Thomas space for velocity with discontinuous pressure/temperature. Both use fixed-point linearization strategy for nonlinear treatment and iterative splitting solution.", "result": "Unified stability analysis shows convergence under mild data requirements. Extensive 2D/3D simulations demonstrate error decay and practical performance in physically sound test cases.", "conclusion": "Both proposed numerical schemes effectively handle the coupled nonlinear problem with proven stability and convergence properties, showing practical applicability through comprehensive simulations."}}
{"id": "2508.21552", "pdf": "https://arxiv.org/pdf/2508.21552", "abs": "https://arxiv.org/abs/2508.21552", "authors": ["Zolt\u00e1n M. Balogh", "Alexandru Krist\u00e1ly"], "title": "Sharp stability in hypercontractivity estimates and logarithmic Sobolev inequalities", "categories": ["math.AP"], "comment": "28 pages", "summary": "We prove stability results in hypercontractivity estimates for the Hopf--Lax\nsemigroup in $\\mathbb R^n$ and apply them to deduce stability results for the\nEuclidean $L^p$-logarithmic Sobolev inequality for any $p>1$. As a main tool,\nwe use recent stability results for the Pr\\'ekopa--Leindler inequality, due to\nB\\\"or\\\"oczky and De (2021), Figalli and Ramos (2024) and Figalli, van Hintum,\nand Tiba (2025). Under mild assumptions on the functions, most of our stability\nresults turn out to be sharp, as they are reflected in the optimal exponent\n$1/2$ both in the hypercontractivity and $L^p$-logarithmic Sobolev deficits,\nrespectively. This approach also works for establishing stability of Gaussian\nhypercontractivity estimates and Gaussian logarithmic Sobolev inequality,\nrespectively.", "AI": {"tldr": "Stability analysis of hypercontractivity estimates for Hopf-Lax semigroup and Euclidean L^p-logarithmic Sobolev inequalities with sharp optimal exponent 1/2.", "motivation": "To establish stability results in hypercontractivity estimates and apply them to deduce stability for Euclidean L^p-logarithmic Sobolev inequalities, building on recent stability results for Pr\u00e9kopa-Leindler inequality.", "method": "Uses recent stability results for Pr\u00e9kopa-Leindler inequality from B\u00f6r\u00f6czky and De (2021), Figalli and Ramos (2024), and Figalli, van Hintum, and Tiba (2025) as main tools to prove stability in hypercontractivity estimates.", "result": "Under mild assumptions, most stability results are sharp with optimal exponent 1/2 in both hypercontractivity and L^p-logarithmic Sobolev deficits. The approach also works for Gaussian hypercontractivity and logarithmic Sobolev inequality.", "conclusion": "The paper provides sharp stability results for hypercontractivity estimates and L^p-logarithmic Sobolev inequalities, demonstrating the broad applicability of the approach to both Euclidean and Gaussian settings."}}
{"id": "2508.21492", "pdf": "https://arxiv.org/pdf/2508.21492", "abs": "https://arxiv.org/abs/2508.21492", "authors": ["Samuel Aldana", "Michael Nolan"], "title": "Control of growth morphology of deposited fcc metals through tuning substrate-metal interactions", "categories": ["cond-mat.mtrl-sci", "cond-mat.mes-hall", "physics.comp-ph"], "comment": "58 pages, 10 figures", "summary": "Precise control over thin film morphology is critical for optimizing material\nproperties across diverse technological applications, as the growth mode\n(whether 2D layer-by-layer or 3D island formation)determines key functional\nproperties such as electrical conductivity in CMOS interconnect applications\nand catalytic activity, where island distribution and size dictate performance.\nTo explore the role of the substrate on the morphology of deposited metals, we\npresent extensive kinetic Monte Carlo simulations on six fcc metals growing in\nthe (111) direction: Ag, Au, Cu, Ni, Pd and Pt. Our simulation framework\nenables screening and evaluation of their growth mode under homoepitaxial\ngrowth scenarios and proposes morphology control strategies by variation of\nsubstrate-metal interaction strengths, modeled by modifying the activation\nenergies for upward and downward migration, combined with thermal vacuum\nannealing within typical back end of line (BEOL) integration thermal budget.\nOur simulation results demonstrate that modulation of the substrate interaction\nstrength can be effectively employed to promote island formation or\nlayer-by-layer growth modes overcoming limitations in achieving large flat\nsurface areas. Au, Pd and Pt exhibit the highest sensitivity to substrate\ninteraction strength variations, followed by Ag, showing that strongly\ninteracting substrates decrease the root mean square (RMS) roughness,\n(uncovered) substrate exposure, island number and island aspect ratios, with\nmoderate increases in flat surface areas and atomic coordination numbers.\nAdditionally, interconnect relevant metrics are improved through thermal vacuum\nannealing particularly when sufficiently strong metal-substrate interactions\nare employed, reducing surface roughness, achieving larger flat surface areas,\nmerging and smoothing islands, and decreasing defect density...", "AI": {"tldr": "Kinetic Monte Carlo simulations show that substrate interaction strength and thermal annealing can control thin film morphology (2D vs 3D growth) for fcc metals, with Au, Pd and Pt being most sensitive. Strong substrate interactions reduce roughness and improve interconnect metrics.", "motivation": "Precise control over thin film morphology is critical for optimizing material properties in applications like CMOS interconnects and catalysis, where growth mode (2D layer-by-layer vs 3D island formation) determines key functional properties.", "method": "Extensive kinetic Monte Carlo simulations on six fcc metals (Ag, Au, Cu, Ni, Pd, Pt) growing in (111) direction, evaluating growth modes under homoepitaxial scenarios by varying substrate-metal interaction strengths through modified activation energies for migration, combined with thermal vacuum annealing within BEOL thermal budget.", "result": "Modulation of substrate interaction strength effectively promotes island formation or layer-by-layer growth. Au, Pd and Pt show highest sensitivity to substrate interactions. Strong interactions decrease RMS roughness, substrate exposure, island number and aspect ratios, while increasing flat surface areas and coordination numbers. Thermal annealing further improves interconnect metrics when strong metal-substrate interactions are used.", "conclusion": "Substrate interaction strength control combined with thermal annealing provides effective strategies for morphology control in thin film deposition, enabling optimization of growth modes and improving interconnect-relevant metrics for technological applications."}}
{"id": "2508.21653", "pdf": "https://arxiv.org/pdf/2508.21653", "abs": "https://arxiv.org/abs/2508.21653", "authors": ["Gaurav Mittal"], "title": "Analogy between Learning With Error Problem and Ill-Posed Inverse Problems", "categories": ["math.NA", "cs.CR", "cs.NA", "94A60, 65J22"], "comment": null, "summary": "In this work, we unveil an analogy between well-known lattice based learning\nwith error problem and ill-posed inverse problems. We show that LWE problem is\na structured inverse problem. Further, we propose a symmetric encryption scheme\nbased on ill-posed problems and thoroughly discuss its security. Finally, we\npropose a public key encryption scheme based on our symmetric encryption scheme\nand CRYSTALS-Kyber KEM (key encapsulation mechanism) and discuss its security.", "AI": {"tldr": "The paper establishes an analogy between lattice-based learning with error (LWE) problems and ill-posed inverse problems, showing LWE is a structured inverse problem. It proposes symmetric and public key encryption schemes based on this connection.", "motivation": "To explore the connection between LWE problems and ill-posed inverse problems, and leverage this analogy to develop new encryption schemes with provable security properties.", "method": "1. Demonstrate LWE as a structured inverse problem 2. Design symmetric encryption based on ill-posed problems 3. Construct public key encryption combining the symmetric scheme with CRYSTALS-Kyber KEM", "result": "Established theoretical connection between LWE and inverse problems, proposed two encryption schemes with security analysis.", "conclusion": "The analogy between LWE and ill-posed inverse problems provides a new perspective for cryptographic scheme design, enabling the development of encryption schemes with discussed security properties."}}
{"id": "2508.21605", "pdf": "https://arxiv.org/pdf/2508.21605", "abs": "https://arxiv.org/abs/2508.21605", "authors": ["Vincent Boulard", "Amaury Hayat"], "title": "F-equivalence for parabolic systems and applications to the stabilization of nonlinear PDE", "categories": ["math.AP", "93D15, 35K58"], "comment": null, "summary": "We consider the $F$-equivalence problem for parabolic systems: under which\nconditions a control system, governed by a parabolic operator $A$ and a control\noperator $B$, can be made equivalent to an arbitrarily exponentially stable\nevolution system through an appropriate control feedback law? While this\nproblem has been resolved for finite-dimensional systems fifty years ago, good\nconditions for infinite-dimensional systems remain a challenge, especially for\nsystems in spatial dimension larger than one. Our main result establishes\noptimal conditions for the existence of an $F$-equivalence pair $(T,K)$ for a\ngiven parabolic control system $(A,B)$. We introduce an extended framework for\n$F$-equivalence of parabolic operators, addressing key limitations of existing\napproaches, and we prove that the pair $(T,K)$ is unique if and only if $(A,B)$\nis approximately controllable. As a consequence, this provides a method to\nconstruct feedback operators for the rapid stabilization of semilinear\nparabolic systems, possibly multi-dimensional in space. We provide several\nillustrative examples, including the rapid stabilization of the heat equation,\nthe Kuramoto-Sivashinsky equation, the Navier-Stokes equations and the\nquasilinear heat equation.", "AI": {"tldr": "Optimal conditions for F-equivalence in parabolic control systems, enabling exponential stabilization through feedback control, with applications to multi-dimensional systems including heat, Kuramoto-Sivashinsky, and Navier-Stokes equations.", "motivation": "Extend finite-dimensional F-equivalence results to infinite-dimensional parabolic systems, particularly challenging for spatial dimensions larger than one, to enable rapid stabilization of complex systems.", "method": "Introduce extended framework for F-equivalence of parabolic operators, establish optimal conditions for existence of F-equivalence pair (T,K), prove uniqueness under approximate controllability, and construct feedback operators for rapid stabilization.", "result": "Main result provides optimal conditions for F-equivalence pair existence, shows uniqueness when system is approximately controllable, and enables construction of feedback operators for rapid stabilization of semilinear parabolic systems including multi-dimensional cases.", "conclusion": "The framework successfully addresses limitations of existing approaches and provides a method for rapid stabilization of various parabolic systems, demonstrating practical applicability through examples of heat equation, Kuramoto-Sivashinsky equation, Navier-Stokes equations, and quasilinear heat equation."}}
{"id": "2508.21663", "pdf": "https://arxiv.org/pdf/2508.21663", "abs": "https://arxiv.org/abs/2508.21663", "authors": ["Ardavan Mehdizadeh", "Peter Schindler"], "title": "Surface Stability Modeling with Universal Machine Learning Interatomic Potentials: A Comprehensive Cleavage Energy Benchmarking Study", "categories": ["cond-mat.mtrl-sci", "cs.LG", "physics.comp-ph"], "comment": "70 pages total (main paper + supplementary information), 4 figures in\n  main text, multiple supplementary figures and tables", "summary": "Machine learning interatomic potentials (MLIPs) have revolutionized\ncomputational materials science by bridging the gap between quantum mechanical\naccuracy and classical simulation efficiency, enabling unprecedented\nexploration of materials properties across the periodic table. Despite their\nremarkable success in predicting bulk properties, no systematic evaluation has\nassessed how well these universal MLIPs (uMLIPs) can predict cleavage energies,\na critical property governing fracture, catalysis, surface stability, and\ninterfacial phenomena. Here, we present a comprehensive benchmark of 19\nstate-of-the-art uMLIPs for cleavage energy prediction using our previously\nestablished density functional theory (DFT) database of 36,718 slab structures\nspanning elemental, binary, and ternary metallic compounds. We evaluate diverse\narchitectural paradigms, analyzing their performance across chemical\ncompositions, crystal systems, thickness, and surface orientations. Our results\nreveal that training data composition dominates architectural sophistication:\nmodels trained on the Open Materials 2024 (OMat24) dataset, which emphasizes\nnon-equilibrium configurations, achieve mean absolute percentage errors below\n6% and correctly identify the thermodynamically most stable surface\nterminations in 87% of cases, without any explicit surface energy training. In\ncontrast, architecturally identical models trained on equilibrium-only datasets\nshow five-fold higher errors, while models trained on surface-adsorbate data\nfail catastrophically with a 17-fold degradation. Remarkably, simpler\narchitectures trained on appropriate data achieve comparable accuracy to\ncomplex transformers while offering 10-100x computational speedup. These\nfindings show that the community should focus on strategic training data\ngeneration that captures the relevant physical phenomena.", "AI": {"tldr": "Benchmark study shows that training data composition, not architectural complexity, is the key factor for accurate cleavage energy predictions in universal machine learning interatomic potentials. Models trained on non-equilibrium configurations (OMat24 dataset) achieve 6% error and 87% accuracy on surface termination identification.", "motivation": "Despite the success of universal MLIPs in predicting bulk properties, there has been no systematic evaluation of their ability to predict cleavage energies - a critical property for fracture, catalysis, surface stability, and interfacial phenomena.", "method": "Comprehensive benchmark of 19 state-of-the-art universal MLIPs using a DFT database of 36,718 slab structures spanning elemental, binary, and ternary metallic compounds. Evaluated diverse architectural paradigms across chemical compositions, crystal systems, thickness, and surface orientations.", "result": "Training data composition dominates performance: OMat24-trained models achieve <6% MAPE and 87% accuracy on stable surface terminations. Architecturally identical models trained on equilibrium-only data show 5x higher errors, while surface-adsorbate trained models fail catastrophically with 17x degradation. Simpler architectures with appropriate data match complex transformers' accuracy with 10-100x speedup.", "conclusion": "The community should focus on strategic training data generation that captures relevant physical phenomena rather than architectural complexity, as appropriate training data enables accurate cleavage energy predictions even with simpler, faster models."}}
{"id": "2508.21659", "pdf": "https://arxiv.org/pdf/2508.21659", "abs": "https://arxiv.org/abs/2508.21659", "authors": ["Takuya Tsuchiya", "Makoto Nakamura"], "title": "Quantitative evaluations of stability and convergence for solutions of semilinear Klein--Gordon equation", "categories": ["math.NA", "cs.NA", "math.AP", "quant-ph"], "comment": "7 pages, 4 figures, 2 tables", "summary": "We perform some simulations of the semilinear Klein--Gordon equation with a\npower-law nonlinear term and propose each of the quantitative evaluation\nmethods for the stability and convergence of numerical solutions. We also\ninvestigate each of the thresholds in the methods by varying the amplitude of\nthe initial value and the mass, and propose appropriate values.", "AI": {"tldr": "Simulation study of semilinear Klein-Gordon equation with power-law nonlinearity, proposing quantitative evaluation methods for stability and convergence of numerical solutions, and investigating thresholds through amplitude and mass variations.", "motivation": "To develop reliable quantitative methods for assessing stability and convergence in numerical solutions of semilinear Klein-Gordon equations with power-law nonlinear terms, which are important in various physical applications.", "method": "Performed simulations of the semilinear Klein-Gordon equation with power-law nonlinearity, varied initial value amplitude and mass parameters to investigate thresholds, and proposed quantitative evaluation methods for stability and convergence analysis.", "result": "Developed quantitative evaluation methods for stability and convergence, identified thresholds through parameter variations, and proposed appropriate values for these thresholds based on simulation results.", "conclusion": "The study successfully established quantitative methods for evaluating numerical solution stability and convergence in semilinear Klein-Gordon equations, providing practical threshold values through systematic parameter variation analysis."}}
{"id": "2508.21822", "pdf": "https://arxiv.org/pdf/2508.21822", "abs": "https://arxiv.org/abs/2508.21822", "authors": ["Carlos M. Guzm\u00e1n", "Suerlan Silva", "Gabriel Pe\u00e7anha"], "title": "Scattering for the non-radial inhomogeneous Hartree equation with a potential", "categories": ["math.AP"], "comment": "20 pages", "summary": "In this work, we consider the focusing generalized inhomogeneous Hartree\nequation with potential \\[ i u_t + \\Delta u - V(x)u + \\left(I_{\\gamma} *\n|x|^{-b}|u|^{p}\\right)|x|^{-b}|u|^{p-2}u = 0, \\] where $0<\\gamma<3$ and\n$0<b<\\frac{1+\\gamma}{2}$. We prove scattering in the intercritical case for\nnonradial initial data, under a mass-potential condition that generalizes the\nusual mass-energy threshold. The main new points compared to previous works are\nthe inhomogeneous weight $|x|^{-b}$ and the presence of a potential $V$, which\nlead us to study the perturbed operator $-\\Delta + V$.\n  Our proof follows the general strategy of Murphy, but we need to adapt\nseveral steps to deal with the weight and the potential. We use Tao's\nscattering criterion together with localized Morawetz estimates in this\nsetting. As a preliminary step, we establish global well-posedness for small\ndata, which, in the presence of $V$, requires careful analysis using\nappropriate admissible Strichartz pairs.", "AI": {"tldr": "Scattering proof for focusing generalized inhomogeneous Hartree equation with potential and weight |x|^{-b} in intercritical case for nonradial data under mass-potential condition.", "motivation": "Extend scattering results to include inhomogeneous weight |x|^{-b} and potential V, generalizing previous works that didn't account for these complexities.", "method": "Adapt Murphy's strategy with Tao's scattering criterion and localized Morawetz estimates. Establish global well-posedness for small data using admissible Strichartz pairs to handle the potential.", "result": "Proved scattering for nonradial initial data in intercritical case under generalized mass-energy threshold condition.", "conclusion": "Successfully extended scattering theory to cover inhomogeneous Hartree equations with potential, overcoming challenges from the weight and perturbed operator -\u0394 + V."}}
{"id": "2508.21729", "pdf": "https://arxiv.org/pdf/2508.21729", "abs": "https://arxiv.org/abs/2508.21729", "authors": ["Yannic Rath", "Massimo Bortone", "George H. Booth"], "title": "Bayesian perspectives for quantum states and application to ab initio quantum chemistry", "categories": ["cond-mat.str-el", "physics.chem-ph", "physics.comp-ph", "quant-ph"], "comment": "To appear in \"Machine Learning in Condensed Matter Physics -\n  Significance, Challenges, and Future Directions\", a Springer Series in\n  Solid-State Sciences", "summary": "The quantum many-electron problem is not just at the heart of condensed\nmatter phenomena, but also essential for first-principles simulation of\nchemical phenomena. Strong correlation in chemical systems are prevalent and\npresent a formidable challenge in the simulation of these systems, while\npredictive phenomena in this domain often also requires a demanding level of\naccuracy to inform chemical behavior. Efficient representations of the\nmany-electron states of chemical systems are therefore also being inspired by\nmachine learning principles to provide an alternative to established\napproaches. In this chapter, we review recent progress in this endeavor for\nquantum chemical problems represented in second quantization, and the\nparticular challenges present in this field. In particular, we focus on the\napplication of Gaussian Process States emerging from efficient representations\nof the many-body wavefunction with rigorous Bayesian modeling frameworks,\nallowing for the unification of multiple paradigms under a common umbrella. We\nshow how such models (and other representations derived from machine learning)\ncan be used as novel tools to compute ab initio chemical properties, while in\nturn also informing the design of machine learning models to extract\ncorrelation patterns in classical data.", "AI": {"tldr": "Review of machine learning-inspired approaches for quantum chemical problems, focusing on Gaussian Process States for efficient many-electron wavefunction representation and Bayesian modeling frameworks.", "motivation": "Strong correlation in chemical systems presents formidable challenges for simulation, requiring high accuracy and efficient representations of many-electron states beyond traditional methods.", "method": "Application of Gaussian Process States derived from machine learning principles, using Bayesian modeling frameworks to represent many-body wavefunctions in second quantization.", "result": "Development of novel tools for computing ab initio chemical properties and informing machine learning models to extract correlation patterns from classical data.", "conclusion": "Machine learning-inspired representations, particularly Gaussian Process States with Bayesian frameworks, provide a unified approach for quantum chemical problems and offer new capabilities for both quantum simulation and classical data analysis."}}
{"id": "2508.21664", "pdf": "https://arxiv.org/pdf/2508.21664", "abs": "https://arxiv.org/abs/2508.21664", "authors": ["Sagy Ephrati", "James Woodfield"], "title": "Trajectory learning for ensemble forecasts via the continuous ranked probability score: a Lorenz '96 case study", "categories": ["math.NA", "cs.LG", "cs.NA", "65M75, 37M05"], "comment": "19 pages, 9 figures. All comments are welcome!", "summary": "This paper demonstrates the feasibility of trajectory learning for ensemble\nforecasts by employing the continuous ranked probability score (CRPS) as a loss\nfunction. Using the two-scale Lorenz '96 system as a case study, we develop and\ntrain both additive and multiplicative stochastic parametrizations to generate\nensemble predictions. Results indicate that CRPS-based trajectory learning\nproduces parametrizations that are both accurate and sharp. The resulting\nparametrizations are straightforward to calibrate and outperform\nderivative-fitting-based parametrizations in short-term forecasts. This\napproach is particularly promising for data assimilation applications due to\nits accuracy over short lead times.", "AI": {"tldr": "CRPS-based trajectory learning enables accurate and sharp ensemble forecasts using stochastic parametrizations in the Lorenz '96 system, outperforming derivative-fitting methods.", "motivation": "To develop effective ensemble forecast methods using trajectory learning with CRPS as a loss function, particularly for data assimilation applications requiring accuracy over short lead times.", "method": "Employed continuous ranked probability score (CRPS) as loss function for trajectory learning, developed and trained both additive and multiplicative stochastic parametrizations using the two-scale Lorenz '96 system as case study.", "result": "CRPS-based trajectory learning produced parametrizations that are both accurate and sharp, straightforward to calibrate, and outperformed derivative-fitting-based parametrizations in short-term forecasts.", "conclusion": "The approach shows promise for data assimilation applications due to its accuracy over short lead times, demonstrating feasibility of trajectory learning for ensemble forecasts using CRPS."}}
{"id": "2508.21158", "pdf": "https://arxiv.org/pdf/2508.21158", "abs": "https://arxiv.org/abs/2508.21158", "authors": ["Phanuel Mariano", "Jing Wang"], "title": "Survival probability for jump processes in unbounded domains on metric measure spaces", "categories": ["math.PR", "math.AP", "math.SP"], "comment": "20 pages, 3 figures", "summary": "We study the large time behavior of the survival probability\n$\\mathbb{P}_x\\left(\\tau_D>t\\right)$ for symmetric jump processes in unbounded\ndomains with a positive bottom of the spectrum. We prove asymptotic upper and\nlower bounds with explicit constants in terms of the bottom of the spectrum\n$\\lambda(D)$. Our main result applies to symmetric jump processes in general\nmetric measure spaces. For $\\alpha$-stable processes in unbounded uniformly\n$C^{1,1}$ domains, our results provide a probabilistic interpretation and an\nequivalent geometric condition for $\\lambda(D)>0$. In the case of increasing\nhorn-shaped domains, the exponential rate of decay for the survival probability\nis sharp. We also present examples of unbounded domains where our results\napply.", "AI": {"tldr": "Analysis of survival probability decay rates for symmetric jump processes in unbounded domains with positive bottom spectrum, providing probabilistic interpretation and geometric conditions.", "motivation": "To understand the large time behavior of survival probabilities for symmetric jump processes in unbounded domains and establish connections between spectral properties and geometric domain characteristics.", "method": "Prove asymptotic upper and lower bounds with explicit constants using the bottom of spectrum \u03bb(D), apply to symmetric jump processes in general metric measure spaces, and analyze \u03b1-stable processes in uniformly C\u00b9\u00b9 domains.", "result": "Established probabilistic interpretation and equivalent geometric condition for \u03bb(D)>0, showed sharp exponential decay rate in horn-shaped domains, and provided examples of applicable unbounded domains.", "conclusion": "The study successfully connects spectral properties with geometric domain characteristics for symmetric jump processes, providing explicit bounds and probabilistic interpretations that hold across various unbounded domain types."}}
{"id": "2508.21765", "pdf": "https://arxiv.org/pdf/2508.21765", "abs": "https://arxiv.org/abs/2508.21765", "authors": ["Mohamed El Guide", "Anas El Hachimi", "Khalide Jbilou", "Lothar Reichel"], "title": "Low-Rank Regularized Convex-Non-Convex Problems for Image Segmentation or Completion", "categories": ["math.NA", "cs.NA"], "comment": null, "summary": "This work proposes a novel convex-non-convex formulation of the image\nsegmentation and the image completion problems. The proposed approach is based\non the minimization of a functional involving two distinct regularization\nterms: one promotes low-rank structure in the solution, while the other one\nenforces smoothness. To solve the resulting optimization problem, we employ the\nalternating direction method of multipliers (ADMM). A detailed convergence\nanalysis of the algorithm is provided, and the performance of the methods is\ndemonstrated through a series of numerical experiments.", "AI": {"tldr": "Convex-non-convex formulation for image segmentation and completion using low-rank and smoothness regularization, solved with ADMM.", "motivation": "To develop an effective approach for image segmentation and completion that combines both low-rank structure promotion and smoothness enforcement in a unified optimization framework.", "method": "Proposes a convex-non-convex functional minimization with two regularization terms (low-rank and smoothness), solved using alternating direction method of multipliers (ADMM) with convergence analysis.", "result": "The method demonstrates performance through numerical experiments, showing effectiveness in image segmentation and completion tasks.", "conclusion": "The proposed convex-non-convex formulation with ADMM provides an effective solution for image segmentation and completion problems with proven convergence properties."}}
{"id": "2508.21269", "pdf": "https://arxiv.org/pdf/2508.21269", "abs": "https://arxiv.org/abs/2508.21269", "authors": ["Feng Dai", "Eero Saksman", "Dachun Yang", "Wen Yuan", "Yangyang Zhang"], "title": "Fractional Heat Semigroup Characterization of Distances from Functions in Lipschitz Spaces to Their Subspaces", "categories": ["math.FA", "math.AP", "math.CA", "Primary 46E35, Secondary 26A16, 35K08, 42C40, 42E35"], "comment": "46 pages; Submitted", "summary": "Let $\\Lambda_s$ denote the inhomogeneous Lipschitz space of order\n$s\\in(0,\\infty)$ on $\\mathbb{R}^n$. This article characterizes the distance\n$d(f, V)_{\\Lambda_s}: = \\inf_{g\\in V} \\|f-g\\|_{\\Lambda_s}$ from a function\n$f\\in \\Lambda_s$ to a non-dense subspace $V\\subset \\Lambda_s$ via the\nfractional semigroup $\\{T_{\\alpha, t}: =e^{-t (-\\Delta)^{\\alpha/2}}: t\\in (0,\n\\infty)\\}$ for any $\\alpha\\in(0,\\infty)$. Given an integer $ r >s/\\alpha$, a\nuniformly bounded continuous function $f$ on $\\mathbb{R}^n$ belongs to the\nspace $\\Lambda_s$ if and only if there exists a constant $\\lambda\\in(0,\\infty)$\nsuch that \\begin{align*} \\left|(-\\Delta)^{\\frac {\\alpha r}2} (T_{\\alpha,\nt^\\alpha } f)(x) \\right|\\leq \\lambda t^{s -r\\alpha }\\ \\ \\text{for any\n$x\\in\\mathbb{R}^n$ and $t\\in (0, 1]$}.\\end{align*} The least such constant is\ndenoted by $\\lambda_{ \\alpha, r, s}(f)$. For each $f\\in \\Lambda_s$ and\n$0<\\varepsilon< \\lambda_{\\alpha,r, s}(f)$, let $$ D_{\\alpha,\nr}(s,f,\\varepsilon):=\\left\\{ (x,t)\\in \\mathbb{R}^n\\times (0,1]:\\ \\left|\n(-\\Delta)^{\\frac {\\alpha r}2} (T_{\\alpha, t^\\alpha} f)(x) \\right|> \\varepsilon\nt^{s -r \\alpha }\\right\\}$$ be the set of ``bad'' points. To quantify its size,\nwe introduce a class of extended nonnegative \\emph{admissible set functions}\n$\\nu$ on the Borel $\\sigma$-algebra $\\mathcal{B}(\\mathbb{R}^n\\times [0, 1])$\nand define, for any admissible function $\\nu$, the \\emph{critical index} $\n\\varepsilon_{\\alpha, r, s,\\nu}(f):=\\inf\\{\\varepsilon\\in(0,\\infty):\\\n\\nu(D_{\\alpha, r}(s,f,\\varepsilon))<\\infty\\}.$ Our result shows that, for a\nbroad class of subspaces $V\\subset \\Lambda_s$, including intersections of\n$\\Lambda_s$ with Sobolev, Besov, Triebel--Lizorkin, and Besov-type spaces,\nthere exists an admissible function $\\nu$ depending on $V$ such that\n$\\varepsilon_{\\alpha, r, s,\\nu}(f)\\sim \\mathrm{dist}(f, V)_{\\Lambda_s}.$", "AI": {"tldr": "Characterizes distance from functions in inhomogeneous Lipschitz spaces to non-dense subspaces using fractional semigroups and introduces critical indices via admissible set functions.", "motivation": "To develop a general framework for quantifying the approximation distance from functions in Lipschitz spaces to various subspaces (Sobolev, Besov, etc.) using fractional semigroup techniques.", "method": "Uses fractional semigroup operators and introduces admissible set functions to define critical indices that measure the size of 'bad' point sets where semigroup estimates fail.", "result": "Establishes equivalence between the critical index \u03b5 and the distance dist(f,V) for broad classes of subspaces V in Lipschitz spaces.", "conclusion": "Provides a unified approach to characterize approximation distances in Lipschitz spaces through fractional semigroup analysis and admissible set functions."}}
{"id": "2508.21165", "pdf": "https://arxiv.org/pdf/2508.21165", "abs": "https://arxiv.org/abs/2508.21165", "authors": ["Natalia L. Rubio", "Eric F. Darve", "Alison L. Marsden"], "title": "Data-Driven Bifurcation Handling in Physics-Based Reduced-Order Vascular Hemodynamic Models", "categories": ["cs.CE", "cs.LG", "cs.NA", "math.NA", "76Z05", "J.2"], "comment": "32 pages, 13 figures", "summary": "Three-dimensional (3D) finite-element simulations of cardiovascular flows\nprovide high-fidelity predictions to support cardiovascular medicine, but their\nhigh computational cost limits clinical practicality. Reduced-order models\n(ROMs) offer computationally efficient alternatives but suffer reduced\naccuracy, particularly at vessel bifurcations where complex flow physics are\ninadequately captured by standard Poiseuille flow assumptions. We present an\nenhanced numerical framework that integrates machine learning-predicted\nbifurcation coefficients into zero-dimensional (0D) hemodynamic ROMs to improve\naccuracy while maintaining computational efficiency. We develop a\nresistor-resistor-inductor (RRI) model that uses neural networks to predict\npressure-flow relationships from bifurcation geometry, incorporating linear and\nquadratic resistances along with inductive effects. The method employs\nnon-dimensionalization to reduce training data requirements and apriori flow\nsplit prediction for improved bifurcation characterization. We incorporate the\nRRI model into a 0D model using an optimization-based solution strategy. We\nvalidate the approach in isolated bifurcations and vascular trees, across\nReynolds numbers from 0 to 5,500, defining ROM accuracy by comparison to 3D\nfinite element simulation. Results demonstrate substantial accuracy\nimprovements: averaged across all trees and Reynolds numbers, the RRI method\nreduces inlet pressure errors from 54 mmHg (45%) for standard 0D models to 25\nmmHg (17%), while a simplified resistor-inductor (RI) variant achieves 31 mmHg\n(26%) error. The enhanced 0D models show particular effectiveness at high\nReynolds numbers and in extensive vascular networks. This hybrid numerical\napproach enables accurate, real-time hemodynamic modeling for clinical decision\nsupport, uncertainty quantification, and digital twins in cardiovascular\nbiomedical engineering.", "AI": {"tldr": "Machine learning-enhanced 0D hemodynamic model improves cardiovascular flow predictions at bifurcations while maintaining computational efficiency", "motivation": "Traditional 3D finite-element simulations are computationally expensive for clinical use, while standard reduced-order models lack accuracy at vessel bifurcations where complex flow physics occur", "method": "Developed a resistor-resistor-inductor (RRI) model using neural networks to predict pressure-flow relationships from bifurcation geometry, incorporating non-dimensionalization and flow split prediction, integrated into 0D models via optimization-based solution", "result": "Substantial accuracy improvements: reduced inlet pressure errors from 54 mmHg (45%) to 25 mmHg (17%) across vascular trees and Reynolds numbers 0-5,500, with particular effectiveness at high Reynolds numbers and extensive networks", "conclusion": "Hybrid machine learning-numerical approach enables accurate real-time hemodynamic modeling for clinical decision support, uncertainty quantification, and digital twins in cardiovascular biomedical engineering"}}
{"id": "2508.21189", "pdf": "https://arxiv.org/pdf/2508.21189", "abs": "https://arxiv.org/abs/2508.21189", "authors": ["Chris Cama\u00f1o", "Ethan N. Epperly", "Raphael A. Meyer", "Joel A. Tropp"], "title": "Faster Linear Algebra Algorithms with Structured Random Matrices", "categories": ["cs.DS", "cs.NA", "math.NA", "65F55, 65F20, 68W20", "G.1.3"], "comment": null, "summary": "To achieve the greatest possible speed, practitioners regularly implement\nrandomized algorithms for low-rank approximation and least-squares regression\nwith structured dimension reduction maps. Despite significant research effort,\nbasic questions remain about the design and analysis of randomized linear\nalgebra algorithms that employ structured random matrices.\n  This paper develops a new perspective on structured dimension reduction,\nbased on the oblivious subspace injection (OSI) property. The OSI property is a\nrelatively weak assumption on a random matrix that holds when the matrix\npreserves the length of vectors on average and, with high probability, does not\nannihilate any vector in a low-dimensional subspace. With the OSI abstraction,\nthe analysis of a randomized linear algebra algorithm factors into two parts:\n(i) proving that the algorithm works when implemented with an OSI; and (ii)\nproving that a given random matrix model has the OSI property.\n  This paper develops both parts of the program. First, it analyzes standard\nrandomized algorithms for low-rank approximation and least-squares regression\nunder the OSI assumption. Second, it identifies many examples of OSIs,\nincluding random sparse matrices, randomized trigonometric transforms, and\nrandom matrices with tensor product structure. These theoretical results imply\nfaster, near-optimal runtimes for several fundamental linear algebra tasks. The\npaper also provides guidance on implementation, along with empirical evidence\nthat structured random matrices offer exemplary performance for a range of\nsynthetic problems and contemporary scientific applications.", "AI": {"tldr": "This paper introduces the Oblivious Subspace Injection (OSI) property as a new framework for analyzing structured random matrices in randomized linear algebra algorithms, providing theoretical guarantees and practical implementations for faster low-rank approximation and least-squares regression.", "motivation": "Despite extensive research on randomized algorithms using structured random matrices for dimension reduction, fundamental questions remain about their design and analysis. Practitioners need faster, near-optimal algorithms for linear algebra tasks with theoretical guarantees.", "method": "Develops the OSI property framework that factors analysis into: (1) proving algorithms work with OSI matrices, and (2) proving specific random matrix models satisfy OSI. Analyzes standard randomized algorithms under OSI assumption and identifies OSI examples including sparse matrices, trigonometric transforms, and tensor product structures.", "result": "Theoretical results show faster, near-optimal runtimes for fundamental linear algebra tasks. Provides implementation guidance and empirical evidence demonstrating exemplary performance on synthetic problems and scientific applications.", "conclusion": "The OSI framework provides a unified approach to analyze structured random matrices, enabling faster randomized linear algebra algorithms with strong theoretical guarantees and practical performance benefits across various applications."}}
{"id": "2508.21642", "pdf": "https://arxiv.org/pdf/2508.21642", "abs": "https://arxiv.org/abs/2508.21642", "authors": ["P. Jameson Graber", "Kyle Rosengartner"], "title": "Mean Field Games of Controls with Dirichlet \\& Neumann Boundary Conditions", "categories": ["math.OC", "math.AP"], "comment": null, "summary": "In a mean field game of controls, a large population of identical players\nseek to minimize a cost that depends on the joint distribution of the states of\nthe players and their controls. We consider the classes of mean field games of\ncontrols in which the value function and the distribution of player states\nsatisfy either Dirichlet or Neumann boundary conditions. We prove that such\nsystems are well-posed either with sufficient smallness conditions or in the\ncase of monotone couplings.", "AI": {"tldr": "Analysis of mean field games with controls under Dirichlet/Neumann boundary conditions, proving well-posedness with smallness conditions or monotone couplings", "motivation": "To establish mathematical foundations for mean field games of controls where player costs depend on joint state-control distributions, particularly addressing boundary condition scenarios", "method": "Theoretical analysis of mean field game systems with Dirichlet and Neumann boundary conditions, employing smallness conditions and monotone coupling assumptions", "result": "Proved well-posedness (existence and uniqueness) of such mean field game systems under specified boundary conditions with either sufficient smallness requirements or monotone coupling properties", "conclusion": "The paper provides rigorous mathematical guarantees for the solvability of mean field games of controls with boundary conditions, establishing important theoretical foundations for this class of problems"}}
{"id": "2508.21249", "pdf": "https://arxiv.org/pdf/2508.21249", "abs": "https://arxiv.org/abs/2508.21249", "authors": ["Mohammad Amin Nabian", "Sanjay Choudhry"], "title": "A Mixture of Experts Gating Network for Enhanced Surrogate Modeling in External Aerodynamics", "categories": ["cs.LG", "cs.AI", "cs.NA", "math.NA", "physics.flu-dyn"], "comment": null, "summary": "The computational cost associated with high-fidelity CFD simulations remains\na significant bottleneck in the automotive design and optimization cycle. While\nML-based surrogate models have emerged as a promising alternative to accelerate\naerodynamic predictions, the field is characterized by a diverse and rapidly\nevolving landscape of specialized neural network architectures, with no single\nmodel demonstrating universal superiority. This paper introduces a novel\nmeta-learning framework that leverages this architectural diversity as a\nstrength. We propose a Mixture of Experts (MoE) model that employs a dedicated\ngating network to dynamically and optimally combine the predictions from three\nheterogeneous, state-of-the-art surrogate models: DoMINO, a decomposable\nmulti-scale neural operator; X-MeshGraphNet, a scalable multi-scale graph\nneural network; and FigConvNet, a factorized implicit global convolution\nnetwork. The gating network learns a spatially-variant weighting strategy,\nassigning credibility to each expert based on its localized performance in\npredicting surface pressure and wall shear stress fields. To prevent model\ncollapse and encourage balanced expert contributions, we integrate an entropy\nregularization term into the training loss function. The entire system is\ntrained and validated on the DrivAerML dataset, a large-scale, public benchmark\nof high-fidelity CFD simulations for automotive aerodynamics. Quantitative\nresults demonstrate that the MoE model achieves a significant reduction in L-2\nprediction error, outperforming not only the ensemble average but also the most\naccurate individual expert model across all evaluated physical quantities. This\nwork establishes the MoE framework as a powerful and effective strategy for\ncreating more robust and accurate composite surrogate models by synergistically\ncombining the complementary strengths of specialized architectures.", "AI": {"tldr": "A meta-learning Mixture of Experts framework that combines three state-of-the-art neural network architectures (DoMINO, X-MeshGraphNet, FigConvNet) with a gating network to achieve superior aerodynamic prediction accuracy in automotive CFD simulations.", "motivation": "High computational cost of CFD simulations is a bottleneck in automotive design. While ML-based surrogate models show promise, no single architecture demonstrates universal superiority, and the field has diverse specialized models.", "method": "Proposes a Mixture of Experts model with a gating network that dynamically combines predictions from three heterogeneous surrogate models. Uses entropy regularization to prevent model collapse and ensure balanced expert contributions. Trained on DrivAerML dataset.", "result": "Achieves significant reduction in L-2 prediction error, outperforming both ensemble average and the most accurate individual expert model across all evaluated physical quantities (surface pressure and wall shear stress fields).", "conclusion": "The MoE framework is a powerful strategy for creating robust and accurate composite surrogate models by synergistically combining complementary strengths of specialized architectures in automotive aerodynamics."}}
{"id": "2508.21571", "pdf": "https://arxiv.org/pdf/2508.21571", "abs": "https://arxiv.org/abs/2508.21571", "authors": ["Bangti Jin", "Longjun Wu"], "title": "Convergence of Stochastic Gradient Methods for Wide Two-Layer Physics-Informed Neural Networks", "categories": ["cs.LG", "cs.NA", "math.NA", "stat.ML"], "comment": "24 pages", "summary": "Physics informed neural networks (PINNs) represent a very popular class of\nneural solvers for partial differential equations. In practice, one often\nemploys stochastic gradient descent type algorithms to train the neural\nnetwork. Therefore, the convergence guarantee of stochastic gradient descent is\nof fundamental importance. In this work, we establish the linear convergence of\nstochastic gradient descent / flow in training over-parameterized two layer\nPINNs for a general class of activation functions in the sense of high\nprobability. These results extend the existing result [18] in which gradient\ndescent was analyzed. The challenge of the analysis lies in handling the\ndynamic randomness introduced by stochastic optimization methods. The key of\nthe analysis lies in ensuring the positive definiteness of suitable Gram\nmatrices during the training. The analysis sheds insight into the dynamics of\nthe optimization process, and provides guarantees on the neural networks\ntrained by stochastic algorithms.", "AI": {"tldr": "This paper establishes linear convergence guarantees for stochastic gradient descent/flow in training over-parameterized two-layer Physics Informed Neural Networks (PINNs) for solving partial differential equations, extending previous results that only analyzed gradient descent.", "motivation": "PINNs are widely used for solving PDEs, typically trained with stochastic gradient descent methods. However, existing convergence analyses focused only on gradient descent, leaving a gap in understanding the convergence behavior of stochastic optimization methods which introduce dynamic randomness.", "method": "The authors analyze stochastic gradient descent/flow for over-parameterized two-layer PINNs with general activation functions. The key challenge is handling dynamic randomness from stochastic optimization, and the analysis focuses on ensuring positive definiteness of suitable Gram matrices during training.", "result": "The paper establishes linear convergence in the high probability sense for stochastic gradient descent/flow when training over-parameterized two-layer PINNs, extending previous gradient descent results to stochastic optimization methods.", "conclusion": "This work provides theoretical guarantees for stochastic optimization methods in training PINNs, offering insights into the optimization dynamics and ensuring that neural networks trained by stochastic algorithms maintain convergence properties similar to deterministic methods."}}
{"id": "2508.21667", "pdf": "https://arxiv.org/pdf/2508.21667", "abs": "https://arxiv.org/abs/2508.21667", "authors": ["Abhishek Setty"], "title": "Block Encoding of Sparse Matrices via Coherent Permutation", "categories": ["quant-ph", "cs.DS", "cs.NA", "math.NA"], "comment": null, "summary": "Block encoding of sparse matrices underpins powerful quantum algorithms such\nas quantum singular value transformation, Hamiltonian simulation, and quantum\nlinear solvers, but its efficient gate-level implementation for arbitrary\nsparse matrices remains a major challenge. We introduce a unified framework\nthat overcomes the key obstacles of multi-controlled X gates overhead,\namplitude reordering, and hardware connectivity, enabling efficient block\nencoding for arbitrary sparse matrices with explicit gate-level constructions.\nCentral to our approach are a novel connection with combinatorial optimization,\nwhich enables systematic assignment of control qubits to achieve\nnearest-neighbor connectivity, and coherent permutation operators that preserve\nsuperposition while enabling amplitude reordering. We demonstrate our methods\non structured sparse matrices, showing significant reductions in circuit depth\nand control overhead, thereby bridging the gap between theoretical formulations\nand practical circuit implementations for quantum algorithms.", "AI": {"tldr": "A unified framework for efficient block encoding of arbitrary sparse matrices using combinatorial optimization and coherent permutation operators to overcome key implementation challenges.", "motivation": "Block encoding of sparse matrices is crucial for quantum algorithms like singular value transformation and Hamiltonian simulation, but efficient gate-level implementation for arbitrary sparse matrices remains a major challenge due to multi-controlled X gates overhead, amplitude reordering issues, and hardware connectivity constraints.", "method": "Novel connection with combinatorial optimization for systematic control qubit assignment to achieve nearest-neighbor connectivity, combined with coherent permutation operators that preserve superposition while enabling amplitude reordering.", "result": "Significant reductions in circuit depth and control overhead for structured sparse matrices, demonstrating practical circuit implementations that bridge the gap between theoretical formulations and hardware realization.", "conclusion": "The framework provides efficient gate-level constructions for arbitrary sparse matrix block encoding, overcoming key obstacles and enabling practical implementation of quantum algorithms that rely on sparse matrix operations."}}
