<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 14]
- [math.AP](#math.AP) [Total: 9]
- [physics.comp-ph](#physics.comp-ph) [Total: 3]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 2]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 2]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.DS](#cs.DS) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [math.FA](#math.FA) [Total: 1]
- [math.PR](#math.PR) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [physics.app-ph](#physics.app-ph) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Preconditioned pseudo-time continuation for parameterized inverse problems](https://arxiv.org/abs/2508.21155)
*Joseph Hart,Alen Alexanderian,Bart van Bloemen Waanders*

Main category: math.NA

TL;DR: A method using pseudo-time continuation with adaptive quasi-Newton Hessian preconditioning to efficiently solve parametrized PDE-constrained inverse problems for uncertainty quantification.


<details>
  <summary>Details</summary>
Motivation: Solving inverse problems for different auxiliary parameter values is computationally expensive but crucial for uncertainty quantification, requiring solving many optimization problems.

Method: Leverage pseudo-time continuation to solve an initial value problem that evolves optimal solutions along parameter paths, using an adaptive quasi-Newton Hessian preconditioner to accelerate computation.

Result: The proposed preconditioner exploits properties of the pseudo-time continuation process to achieve reliable and efficient computation, demonstrated on two nonlinear inverse problems.

Conclusion: The framework provides an efficient approach for solving parametrized variational inverse problems constrained by PDEs, particularly useful for uncertainty quantification applications.

Abstract: We consider parametrized variational inverse problems that are constrained by
partial differential equations (PDEs). We seek to efficiently compute the
solution of the inverse problem when auxiliary model parameters, which appear
in the governing PDE, are varied. Computing the solution of the inverse problem
for different auxiliary parameter values is crucial for uncertainty
quantification. This, however, is computationally challenging since it requires
solving many optimization problems for different realizations of the auxiliary
parameters. We leverage pseudo-time continuation and solve an initial value
problem to evolve the optimal solution along an auxiliary parameter path. This
article introduces the use of an adaptive quasi-Newton Hessian preconditioner
to accelerate the computation. Our proposed preconditioner exploits properties
of the pseudo-time continuation process to achieve reliable and efficient
computation. We elaborate our proposed framework and elucidate its properties
for two nonlinear inverse problems.

</details>


### [2] [Entropy stable finite difference (ESFD) methods via entropy correction artificial viscosity (ECAV) and knapsack limiting (KL) techniques](https://arxiv.org/abs/2508.21226)
*Brian Christner,Jesse Chan*

Main category: math.NA

TL;DR: This paper introduces knapsack limiting and artificial viscosity techniques for finite difference discretizations to achieve entropy stability, high-order accuracy, and positivity preservation in computational fluid dynamics.


<details>
  <summary>Details</summary>
Motivation: Entropy stable methods provide robust, hyperparameter-free solutions in computational fluid dynamics, but existing approaches like discontinuous Galerkin methods need adaptation for finite difference discretizations to achieve similar benefits.

Method: The authors adapt knapsack limiting (blending low-order positivity-preserving schemes with high-order schemes) and entropy correction artificial viscosity techniques from discontinuous Galerkin methods to finite difference discretizations.

Result: The proposed finite difference schemes preserve high-order accuracy in smooth conditions, are entropy stable, hyperparameter-free, and the knapsack limiting scheme provably preserves positivity for compressible Euler and Navier-Stokes equations without significant performance tradeoffs.

Conclusion: The work successfully extends entropy stabilization techniques to finite difference methods, providing robust, high-order accurate, and positivity-preserving schemes for computational fluid dynamics applications.

Abstract: Entropy stable methods have become increasingly popular in the field of
computational fluid dynamics. They often work by satisfying some form of a
discrete entropy inequality: a discrete form of the 2nd law of thermodynamics.
Schemes which satisfy a (semi-)discrete entropy inequality typically behave
much more robustly, and do so in a way that is hyperparameter free. Recently, a
new strategy was introduced to construct entropy stable discontinuous Galerkin
methods: knapsack limiting, which blends together a low order, positivity
preserving, and entropy stable scheme with a high order accurate scheme, in
order to produce a high order accurate, entropy stable, and positivity
preserving scheme. Another recent strategy introduces an entropy correction
artificial viscosity into a high order scheme, aiming to satisfy a cell entropy
inequality.
  In this work, we introduce the techniques of knapsack limiting and artificial
viscosity for finite difference discretizations. The proposed schemes preserve
high order accuracy in sufficiently smooth conditions, are entropy stable, and
are hyperparameter free. Moreover, the proposed knapsack limiting scheme
provably preserves positivity for the compressible Euler and Navier-Stokes
equations. Both schemes achieve this goal without significant performance
tradeoffs compared to state of the art stabilized schemes.

</details>


### [3] [Machine-precision energy conservative quadrature hyperreduction of Lagrangian hydrodynamics](https://arxiv.org/abs/2508.21279)
*Chris Vales,Siu Wun Cheung,Dylan M. Copeland,Youngsoo Choi*

Main category: math.NA

TL;DR: Energy conservative quadrature-based model reduction for compressible Euler equations using data-driven reduced basis and hyperreduction with exact discrete total energy conservation.


<details>
  <summary>Details</summary>
Motivation: To develop a structure-preserving reduced order model for Lagrangian hydrodynamics that maintains exact energy conservation while achieving computational efficiency.

Method: Projection-based reduced model using data-driven reduced basis functions and hyperreduction via empirical quadrature procedure (EQP) with energy conservation enforcement.

Result: Numerical experiments on four benchmark problems show total energy conservation to near machine precision while maintaining accuracy comparable to basic EQP formulation.

Conclusion: The energy conservative EQP (CEQP) method is an effective structure-preserving hyperreduction strategy for nonlinear Lagrangian hydrodynamics simulation.

Abstract: We present an energy conservative, quadrature based model reduction framework
for the compressible Euler equations of Lagrangian hydrodynamics. Building on a
high order finite element discretization of the governing equations, we develop
a projection based reduced model using data driven reduced basis functions and
hyperreduction via the empirical quadrature procedure (EQP). We introduce a
strongly energy conservative variant of EQP that enforces exact discrete total
energy conservation during the hyperreduction process. Numerical experiments
for four benchmark problems -- Sedov blast, Gresho vortex, triple point and
Taylor-Green vortex -- demonstrate that the numerical implementation of our
proposed method conserves total energy to near machine precision while
maintaining accuracy comparable to the basic EQP formulation. These results
establish the energy conservative EQP (CEQP) method as an effective structure
preserving hyperreduction strategy for the reduced simulation of nonlinear
Lagrangian hydrodynamics.

</details>


### [4] [Generalized quantum singular value transformation with application in quantum bi-conjugate gradient method](https://arxiv.org/abs/2508.21390)
*Yu-Qiu Liu,Hefeng Wang,Hua Xiang*

Main category: math.NA

TL;DR: Extends GQSP to general matrices as GQSVT and applies it to implement quantum BiCG algorithm with shallow circuits and few ancilla qubits.


<details>
  <summary>Details</summary>
Motivation: QSP and GQSP have polynomial parity restrictions; need to extend these techniques to general matrices and develop practical quantum algorithms with efficient resource usage.

Method: Extends GQSP to handle general matrices (GQSVT), then implements quantum BiCG method using GQSVT and swap test techniques.

Result: Developed GQSVT framework and demonstrated quantum BiCG algorithm with relatively shallow circuit depth and minimal ancilla qubit requirements.

Conclusion: GQSVT successfully generalizes quantum signal processing to arbitrary matrices, enabling efficient quantum algorithms like BiCG with practical circuit implementations.

Abstract: Quantum signal processing (QSP) and generalized quantum signal processing
(GQSP) are essential tools for implementing the block encoding of matrix
functions. The achievable polynomials of QSP have restrictions on parity, while
GQSP eliminates these restrictions. In this paper, we further investigate GQSP
and present a quantum bi-conjugate gradient (BiCG) algorithm as an application.
First, we extend GQSP, which constructs functions of unitary matrices, to
general matrices. We refer to this extension as generalized quantum singular
value transformation (GQSVT). Subsequently, we implement the quantum BiCG
method, utilizing GQSVT and swap test, which has a relatively shallow circuit
depth and requires a small number of ancilla qubits.

</details>


### [5] [A Biologically Motivated Finite Difference Approach for Simulating Singularly Perturbed Vertical Motion in Human Gait](https://arxiv.org/abs/2508.21410)
*Shubhangini Gupta,Sourav Banerjee,Tamal Pramanick*

Main category: math.NA

TL;DR: A simulation-based numerical method for solving singularly perturbed second-order differential equations from human gait models, using domain decomposition and time-rescaling to handle boundary layers with second-order accuracy.


<details>
  <summary>Details</summary>
Motivation: Standard numerical methods fail to resolve boundary layer behavior in singularly perturbed differential equations from human gait models, which include important physical factors like gravity, damping, and leg stiffness.

Method: Domain decomposition technique dividing the problem into inner and outer regions, time-rescaling transformation for the inner region, discretization with mixed finite difference framework, and solving tridiagonal systems using Thomas algorithm.

Result: The method achieves second-order accuracy in space, validated through numerical experiments on modified human gait models, demonstrating accuracy, stability, and efficiency.

Conclusion: The framework serves as a fundamental tool for biomechanical simulation and provides a foundation for future research incorporating nonlinearities, time delays, and real-world walking data.

Abstract: In this study, we present a simulation-based numerical method for solving a
class of singularly perturbed second-order differential equations that come
from a simplified biologically motivated model of human gait. Important
physical factors such as gravity, damping, and leg stiffness are included in
the model, which also depicts the vertical motion of the center of mass of the
body during walking or running. Most of the time, standard numerical methods
are ineffective in resolving boundary layer behavior that occurs due to the
small perturbation parameter in the governing equation. We use a domain
decomposition technique to divide the problem domain into inner and outer
regions to tackle this difficulty. The boundary layer resolves the steep
gradients. We applied a time-rescaling transformation to the inner region. Each
subdomain is discretized, and the resulting tridiagonal systems are efficiently
solved using the Thomas algorithm within the mixed finite difference framework.
A detailed convergence analysis demonstrates second-order accuracy in space.
The numerical results validate the proposed scheme's accuracy, stability, and
efficiency through experiments based on modified human gait models. The
framework serves as a fundamental tool for biomechanical simulation. The
modeling is a foundation for future research, incorporating nonlinearities,
time delays, and real-world scenarios data on how people walk.

</details>


### [6] [Computing Radially-Symmetric Solutions of the Ultra-Relativistic Euler Equations with Entropy-Stable Discontinuous Galerkin Methods](https://arxiv.org/abs/2508.21427)
*Ferdinand Thein,Hendrik Ranocha*

Main category: math.NA

TL;DR: Derived entropy-stable flux for ultra-relativistic Euler equations using main field/entropy variables and potentials, with 2D/3D simulation results.


<details>
  <summary>Details</summary>
Motivation: To address shock wave formation and pressure blow-up in ultra-relativistic gases by developing entropy-stable numerical methods, building on previous benchmark problems by Kunik et al.

Method: Derived main field (entropy variables) and corresponding potentials for ultra-relativistic Euler equations, then developed an entropy-stable flux formulation. Validated with 2D and 3D simulations.

Result: Successfully developed entropy-stable flux formulation and demonstrated its performance through simulations of various test cases in both 2D and 3D configurations.

Conclusion: The proposed entropy-stable flux provides a robust numerical framework for solving ultra-relativistic Euler equations, effectively handling shock formation and pressure blow-up phenomena.

Abstract: The ultra--relativistic Euler equations describe gases in the relativistic
case when the thermal energy dominates. These equations for an ideal gas are
given in terms of the pressure, the spatial part of the dimensionless
four-velocity, and the particle density. Kunik et al.\ (2024,
https://doi.org/10.1016/j.jcp.2024.113330) proposed genuine multi--dimensional
benchmark problems for the ultra--relativistic Euler equations. In particular,
they compared full two-dimensional discontinuous Galerkin simulations for
radially symmetric problems with solutions computed using a specific
one-dimensional scheme. Of particular interest in the solutions are the
formation of shock waves and a pressure blow-up. In the present work we derive
an entropy-stable flux for the ultra--relativistic Euler equations. Therefore,
we derive the main field (or entropy variables) and the corresponding
potentials. We then present the entropy-stable flux and conclude with
simulation results for different test cases both in 2D and in 3D.

</details>


### [7] [Inverse Random Source Problem for the Helmholtz Equation from Statistical Phaseless Data](https://arxiv.org/abs/2508.21478)
*Qiao-Ping Chen,Hongyu Liu,Zejun Sun,Li-Li Wang,Guang-Hui Zheng*

Main category: math.NA

TL;DR: This paper presents a method for reconstructing random sources from phaseless data in 2D Helmholtz equations using reference sources and Bayesian inference to overcome non-uniqueness issues.


<details>
  <summary>Details</summary>
Motivation: The main challenge in reconstructing random sources from statistical phaseless data is non-uniqueness, which this research aims to overcome through innovative techniques.

Method: Introduces artificial point sources as references, derives phase retrieval formulas for field statistics, analyzes uniqueness/stability, uses Fredholm integral equations, and applies Bayesian methods for uncertainty quantification.

Result: The method successfully reconstructs random sources with proven stability and well-posed posterior distributions, validated through numerical experiments.

Conclusion: The proposed approach effectively solves the inverse random source problem for 2D Helmholtz equations with phaseless data, providing both theoretical guarantees and practical validation.

Abstract: This paper investigates the problem of reconstructing a random source from
statistical phaseless data for the two-dimensional Helmholtz equation. The
major challenge of this problem is non-uniqueness, which we overcome through a
reference source technique. Firstly, we introduce some artificially added point
sources into the inverse random source system and derive phase retrieval (PR)
formulas for the expectation and variance of the radiated fields. This paper
rigorously analyze the uniqueness and stability of the recovered statistics of
the radiated fields. Afterwards, since the direct problem has a unique mild
solution, by examining the expectation and variance of this solution and
combined with the phase retrieval formulas, we derive the Fredholm integral
equations to solve the inverse random source problem (IRSP). We prove the
stability of the corresponding integral equations. To quantify the uncertainty
of the random source, we utilize the Bayesian method to reconstruct the random
source and establish the well-posedness of the posterior distribution. Finally,
numerical experiments demonstrate the effectiveness of the proposed method and
validate the theoretical results.

</details>


### [8] [The Derivative of Kemeny's Constant as a Centrality Measure in Undirected Graphs](https://arxiv.org/abs/2508.21506)
*Dario A. Bini,Beatrice Meini,Federico Poloni*

Main category: math.NA

TL;DR: This paper introduces directional derivatives of Kemeny's constant to define edge/non-edge centrality measures and sensitivity analysis, providing explicit formulas using graph Laplacian inverses that work even for cut-edges.


<details>
  <summary>Details</summary>
Motivation: To quantify how individual edges affect graph connectivity by measuring their impact on Kemeny's constant, which represents the average random walk time between vertices.

Method: Defined two concepts of directional derivative of Kemeny's constant with respect to edges, derived explicit expressions using modified graph Laplacian inverses, developed computational algorithms, and applied to road networks and link prediction.

Result: Obtained explicit formulas for edge centrality measures that remain valid for cut-edges, established connections to existing measures, and provided specific expressions for one-path graphs in terms of edge weights.

Conclusion: The introduced directional derivative measures provide effective tools for analyzing edge importance in graph connectivity, with practical applications in network analysis and link prediction problems.

Abstract: Kemeny's constant quantifies a graph's connectivity by measuring the average
time for a random walker to reach any other vertex. We introduce two concepts
of the directional derivative of Kemeny's constant with respect to an edge and
use them to define centrality measures for edges and non-edges in the graph.
Additionally, we present a sensitivity measure of Kemeny's constant. An
explicit expression for these quantities involving the inverse of the modified
graph Laplacian is provided, which is valid even for cut-edges. These measures
are connected to the one introduced in [Altafini et al., SIMAX 2023], and
algorithms for their computation are included. The benefits of these measures
are discussed, along with applications to road networks and link prediction
analysis. For one-path graphs, an explicit expression for these measures is
given in terms of the edge weights.

</details>


### [9] [Random domain decomposition for parabolic PDEs on graphs](https://arxiv.org/abs/2508.21557)
*Martín Hernández*

Main category: math.NA

TL;DR: RBM applied to parabolic PDEs on graphs via stochastic domain decomposition, achieving first-order convergence with significant computational/memory savings.


<details>
  <summary>Details</summary>
Motivation: Simulating complex systems like gas pipeline networks requires solving PDEs on intricate graphs, which demands substantial computational and memory resources that current methods struggle with.

Method: Apply Random Batch Method (RBM) at PDE level for parabolic equations on graphs using non-overlapping domain decomposition with randomized coefficients and source terms, without preliminary space/time discretization.

Result: Proven mean-square convergence to true PDE solution with first-order accuracy in RBM step size. Numerical experiments confirm convergence rate and show substantial reductions in memory usage and computational time compared to full graph solving.

Conclusion: RBM provides an effective stochastic approach for solving parabolic PDEs on graphs, offering computational efficiency and memory savings while maintaining mathematical convergence guarantees across different time discretization schemes.

Abstract: The simulation of complex systems, such as gas transport in large pipeline
networks, often involves solving PDEs posed on intricate graph structures. Such
problems require considerable computational and memory resources. The Random
Batch Method (RBM) has shown promise in addressing these challenges via
stochastic decomposition techniques. In this paper, we apply the RBM at the PDE
level for parabolic equations on graphs, without assuming any preliminary
discretization in space or time. We consider a non-overlapping domain
decomposition in which the PDE coefficients and source terms are randomized. We
prove that the resulting RBM-based scheme converges, in the mean-square sense
and uniformly in time, to the true PDE solution with first-order accuracy in
the RBM step size. Numerical experiments confirm this convergence rate and
demonstrate substantial reductions in both memory usage and computational time
compared to solving on the full graph. Moreover, these advantages persist
across different time discretization schemes.

</details>


### [10] [Conforming and discontinuous discretizations of non-isothermal Darcy-Forchheimer flows](https://arxiv.org/abs/2508.21630)
*Stefano Bonetti,Michele Botti,Paola F. Antonietti*

Main category: math.NA

TL;DR: Two numerical schemes for Darcy-Forchheimer flow coupled with advection-diffusion temperature modeling, using different discretization approaches with fixed-point linearization for nonlinearities.


<details>
  <summary>Details</summary>
Motivation: To develop efficient numerical methods for simulating coupled fluid flow and heat transfer problems involving Darcy-Forchheimer flow with temperature-dependent properties, which are common in porous media applications.

Method: Two discretization approaches: 1) fully discontinuous Galerkin spaces, and 2) Raviart-Thomas space for velocity with discontinuous pressure/temperature. Both use fixed-point linearization iterative splitting for nonlinearities.

Result: Unified stability analysis and convergence proof under mild data requirements. Comprehensive 2D/3D simulations demonstrate error decay and practical performance in physically sound test cases.

Conclusion: Both proposed numerical schemes are effective for Darcy-Forchheimer flow with thermal coupling, with proven stability and convergence, validated through extensive numerical experiments.

Abstract: We present and analyze in a unified setting two schemes for the numerical
discretization of a Darcy-Forchheimer fluid flow model coupled with an
advection-diffusion equation modeling the temperature distribution in the
fluid. The first approach is based on fully discontinuous Galerkin
discretization spaces. In contrast, in the second approach, the velocity is
approximated in the Raviart-Thomas space, and the pressure and temperature are
still piecewise discontinuous. A fixed-point linearization strategy, naturally
inducing an iterative splitting solution, is proposed for treating the
nonlinearities of the problem. We present a unified stability analysis and
prove the convergence of the iterative algorithm under mild requirements on the
problem data. A wide set of two- and three-dimensional simulations is presented
to assess the error decay and demonstrate the practical performance of the
proposed approaches in physically sound test cases.

</details>


### [11] [Analogy between Learning With Error Problem and Ill-Posed Inverse Problems](https://arxiv.org/abs/2508.21653)
*Gaurav Mittal*

Main category: math.NA

TL;DR: LWE problem is analogous to ill-posed inverse problems, enabling new encryption schemes based on this connection.


<details>
  <summary>Details</summary>
Motivation: To establish a connection between lattice-based learning with error (LWE) problems and ill-posed inverse problems, and leverage this analogy to develop new encryption schemes.

Method: Demonstrated that LWE is a structured inverse problem, proposed a symmetric encryption scheme based on ill-posed problems, and extended it to a public key encryption scheme using CRYSTALS-Kyber KEM.

Result: Successfully established the analogy between LWE and ill-posed inverse problems, and developed both symmetric and public key encryption schemes with security analysis.

Conclusion: The work provides a novel perspective on LWE problems through the lens of inverse problems, offering new cryptographic constructions with potential security benefits.

Abstract: In this work, we unveil an analogy between well-known lattice based learning
with error problem and ill-posed inverse problems. We show that LWE problem is
a structured inverse problem. Further, we propose a symmetric encryption scheme
based on ill-posed problems and thoroughly discuss its security. Finally, we
propose a public key encryption scheme based on our symmetric encryption scheme
and CRYSTALS-Kyber KEM (key encapsulation mechanism) and discuss its security.

</details>


### [12] [Quantitative evaluations of stability and convergence for solutions of semilinear Klein--Gordon equation](https://arxiv.org/abs/2508.21659)
*Takuya Tsuchiya,Makoto Nakamura*

Main category: math.NA

TL;DR: Simulation study of semilinear Klein-Gordon equation with power-law nonlinearity, proposing quantitative evaluation methods for stability and convergence of numerical solutions, and investigating thresholds through amplitude and mass variations.


<details>
  <summary>Details</summary>
Motivation: To develop reliable quantitative evaluation methods for assessing the stability and convergence of numerical solutions to the semilinear Klein-Gordon equation with power-law nonlinear terms, which is important for accurate numerical simulations in physics and engineering applications.

Method: Performed simulations of the semilinear Klein-Gordon equation with power-law nonlinearity, varied the amplitude of initial values and mass parameters to investigate thresholds, and proposed quantitative evaluation methods for stability and convergence analysis.

Result: Developed quantitative evaluation methods for stability and convergence assessment, identified thresholds through systematic parameter variation studies, and proposed appropriate values for these thresholds based on simulation results.

Conclusion: The study successfully established quantitative evaluation frameworks for numerical solution analysis of semilinear Klein-Gordon equations, providing practical threshold values that can guide future numerical simulations and ensure solution reliability.

Abstract: We perform some simulations of the semilinear Klein--Gordon equation with a
power-law nonlinear term and propose each of the quantitative evaluation
methods for the stability and convergence of numerical solutions. We also
investigate each of the thresholds in the methods by varying the amplitude of
the initial value and the mass, and propose appropriate values.

</details>


### [13] [Trajectory learning for ensemble forecasts via the continuous ranked probability score: a Lorenz '96 case study](https://arxiv.org/abs/2508.21664)
*Sagy Ephrati,James Woodfield*

Main category: math.NA

TL;DR: CRPS-based trajectory learning enables accurate ensemble forecasts using stochastic parametrizations in Lorenz '96 system


<details>
  <summary>Details</summary>
Motivation: To develop effective ensemble forecasting methods using trajectory learning with continuous ranked probability score as loss function for improved short-term predictions

Method: Employed CRPS as loss function to train both additive and multiplicative stochastic parametrizations in two-scale Lorenz '96 system for ensemble predictions

Result: CRPS-based trajectory learning produces accurate and sharp parametrizations that outperform derivative-fitting-based approaches in short-term forecasts and are easy to calibrate

Conclusion: This approach shows strong promise for data assimilation applications due to its accuracy over short lead times and effective ensemble forecasting capabilities

Abstract: This paper demonstrates the feasibility of trajectory learning for ensemble
forecasts by employing the continuous ranked probability score (CRPS) as a loss
function. Using the two-scale Lorenz '96 system as a case study, we develop and
train both additive and multiplicative stochastic parametrizations to generate
ensemble predictions. Results indicate that CRPS-based trajectory learning
produces parametrizations that are both accurate and sharp. The resulting
parametrizations are straightforward to calibrate and outperform
derivative-fitting-based parametrizations in short-term forecasts. This
approach is particularly promising for data assimilation applications due to
its accuracy over short lead times.

</details>


### [14] [Low-Rank Regularized Convex-Non-Convex Problems for Image Segmentation or Completion](https://arxiv.org/abs/2508.21765)
*Mohamed El Guide,Anas El Hachimi,Khalide Jbilou,Lothar Reichel*

Main category: math.NA

TL;DR: Convex-non-convex formulation for image segmentation and completion using low-rank and smoothness regularization, solved with ADMM


<details>
  <summary>Details</summary>
Motivation: To develop an effective approach for image segmentation and completion problems that combines both low-rank structure promotion and smoothness enforcement in a unified optimization framework

Method: Proposes a convex-non-convex functional minimization with two regularization terms (low-rank and smoothness), solved using alternating direction method of multipliers (ADMM)

Result: The method demonstrates performance through numerical experiments and includes detailed convergence analysis of the algorithm

Conclusion: The proposed convex-non-convex formulation with ADMM provides an effective solution for image segmentation and completion tasks by leveraging both low-rank and smoothness properties

Abstract: This work proposes a novel convex-non-convex formulation of the image
segmentation and the image completion problems. The proposed approach is based
on the minimization of a functional involving two distinct regularization
terms: one promotes low-rank structure in the solution, while the other one
enforces smoothness. To solve the resulting optimization problem, we employ the
alternating direction method of multipliers (ADMM). A detailed convergence
analysis of the algorithm is provided, and the performance of the methods is
demonstrated through a series of numerical experiments.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [15] [Propagation in the Fisher-KPP equation with Mixed Operator](https://arxiv.org/abs/2508.21151)
*Begoña Barrios,Bryan Pichucho,Alexander Quaas*

Main category: math.AP

TL;DR: Analysis of Fisher-KPP equation with mixed local-nonlocal diffusion operator, showing fractional Laplacian dominates classical Laplacian in determining propagation rates and tail behavior.


<details>
  <summary>Details</summary>
Motivation: To understand the asymptotic spreading behavior and long-time dynamics of the Fisher-KPP equation with mixed diffusion involving both classical and fractional Laplacian operators.

Method: Construction and study of heat kernel for mixed operator, development of mild solutions theory, establishment of comparison principle in weighted function spaces.

Result: Non-existence of traveling waves, characterization of large-time spreading rate, demonstration that fractional Laplacian dominates classical Laplacian in determining exponential propagation rate and tail thickness.

Conclusion: The fractional diffusion component plays a dominant role in the spreading dynamics, particularly in the initial layer, controlling both propagation speed and solution tail behavior.

Abstract: Our investigation focuses on the asymptotic spreading behavior of the
Fisher-KPP equation with a mixed local-nonlocal operator in the diffusion (see
the work by X. Cabr\'e and J.-M. Roquejoffre, 2013, ref.[8]) to the setting of
mixed diffusion, which involves both the classical and the fractional Laplacian
in order to analyze the long-time dynamics of the equation. A key step in our
approach involves the construction and detailed study of the heat kernel
associated with the mixed operator, which we use to develop a theory of mild
solutions and establish a comparison principle in suitable weighted function
spaces.
  This framework allows us to rigorously establish the non-existence of
traveling waves and characterize the large-time spreading rate of solutions. We
show that the influence of the fractional Laplacian dominates over the
classical Laplacian, especially in the initial layer, where it dictates the
exponential propagation rate and the thickness of the solution tails.

</details>


### [16] [Asymptotic expansions for the transmission eigenvalues of periodic scatterers of bounded support](https://arxiv.org/abs/2508.21174)
*Fioralba Cakoni,Shari Moskow*

Main category: math.AP

TL;DR: First-order corrections to transmission eigenvalues for scatterers with periodic refractive index variations, using two-scale asymptotics for biharmonic homogenization with boundary correctors.


<details>
  <summary>Details</summary>
Motivation: To analyze transmission eigenvalues in bounded scatterers with periodically varying refractive indices, particularly when the contrast has one sign, enabling formulation as a fourth-order PDE problem.

Method: Two-scale asymptotics for biharmonic-type homogenization problem, incorporating boundary corrector functions to derive convergence estimates and eigenvalue corrections.

Result: Derived first-order corrections to limiting transmission eigenvalues, with the boundary corrector function appearing explicitly in the correction formula.

Conclusion: The method successfully provides first-order eigenvalue corrections for periodic scatterers through biharmonic homogenization with boundary correctors, establishing convergence estimates.

Abstract: We consider the transmission eigenvalues for a bounded scatterer with a
periodically varying index of refraction, and derive the first order
corrections to the limiting transmission eigenvalues. We assume the scatterer
contrast to be of one sign, in which case the transmission eigenvalue problem
can be written in terms of operators corresponding to a fourth order PDE with
periodic coefficients. We perform two-scale asymptotics for this biharmonic
type homogenization problem and show convergence estimates which require a
boundary corrector function, and this boundary corrector function appears in
the formula for the transmission eigenvalues correction.

</details>


### [17] [Pattern formation and nonlinear waves close to a 1:1 resonant Turing and Turing--Hopf instability](https://arxiv.org/abs/2508.21183)
*Bastian Hilder,Christian Kuehn*

Main category: math.AP

TL;DR: Analysis of pattern-forming systems near simultaneous Turing and Turing-Hopf instabilities with 1:1 spatial resonance, using coupled Swift-Hohenberg equations and deriving amplitude equations.


<details>
  <summary>Details</summary>
Motivation: To understand the dynamics of pattern-forming systems when both Turing and Turing-Hopf instabilities occur simultaneously with the same critical wave number, which is relevant for studying pattern transitions in various physical and biological systems.

Method: Use coupled Swift-Hohenberg equations with dispersive terms and general nonlinearities. Derive coupled complex Ginzburg-Landau equations as amplitude equations near instability onset. Construct space-time periodic solutions and fast-travelling front solutions using geometric singular perturbation theory. Apply center manifold reduction and normal form theory for global solutions.

Result: Successfully derived amplitude equations with error estimates, constructed space-time periodic solutions and fast-travelling fronts that connect different patterns, and obtained solutions modeling spatial pattern transitions on long time intervals.

Conclusion: The approach provides a framework for analyzing pattern transitions in systems with simultaneous instabilities, demonstrating the existence of various solution types including periodic patterns and connecting fronts, with applications to understanding pattern formation dynamics.

Abstract: In this paper, we analyse the dynamics of a pattern-forming system close to
simultaneous Turing and Turing--Hopf instabilities, which have a 1:1 spatial
resonance, that is, they have the same critical wave number. For this, we
consider a system of coupled Swift--Hohenberg equations with dispersive terms
and general, smooth nonlinearities. Close to the onset of instability, we
derive a system of two coupled complex Ginzburg--Landau equations with a
singular advection term as amplitude equations and justify the approximation by
providing error estimates. We then construct space-time periodic solutions to
the amplitude equations, as well as fast-travelling front solutions, which
connect different space-time periodic states. This yields the existence of
solutions to the pattern-forming system on a finite, but long time interval,
which model the spatial transition between different patterns. The construction
is based on geometric singular perturbation theory exploiting the fast
travelling speed of the fronts. Finally, we construct global, spatially
periodic solutions to the pattern-forming system by using centre manifold
reduction, normal form theory and a variant of singular perturbation theory to
handle fast oscillatory higher-order terms.

</details>


### [18] [Quantitative estimates for the relative isoperimetric problem and its gradient flow outside convex bodies in the plane](https://arxiv.org/abs/2508.21198)
*Elena Mäder-Baumdicker,Robin Neumayer,Jiewon Park,Melanie Rupflin*

Main category: math.AP

TL;DR: Quantitative analysis of relative isoperimetric problems outside convex bodies in the plane, proving three related results with explicit constants and optimal rates.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous quantitative estimates for relative isoperimetric problems, addressing critical points, gradient flow convergence, and stability of minimizers with precise mathematical bounds.

Method: Using a flow approach combined with analysis of a two-dimensional auxiliary variational problem for circular arcs outside convex bodies, proving Łojasiewicz estimates, quantitative rigidity, and convergence rates.

Result: Proved three quantitative results: (1) Łojasiewicz estimates and quantitative rigidity for critical points, (2) rates of convergence for gradient flow, and (3) quantitative stability for minimizers - all with explicit constants and optimal exponents/rates.

Conclusion: The flow approach successfully proves quantitative stability for minimizers in isoperimetric problems for the first time, with results holding when the auxiliary variational problem is nondegenerate, providing comprehensive quantitative analysis with optimal parameters.

Abstract: We prove three related quantitative results for the relative isoperimetric
problem outside a convex body $\Omega$ in the plane: (1) {\L}ojasiewicz
estimates and quantitative rigidity for critical points, (2) rates of
convergence for the gradient flow, and (3) quantitative stability for
minimizers. These results come with explicit constants and optimal
exponents/rates, and hold whenever a simple two-dimensional auxiliary
variational problem for circular arcs outside of $\Omega$ is nondegenerate. The
proofs are inter-related, and in particular, for the first time in the context
of isoperimetric problems, a flow approach is used to prove quantitative
stability for minimizers.

</details>


### [19] [Propagation of smallness near codimension two for gradients of harmonic functions](https://arxiv.org/abs/2508.21214)
*Benjamin Foster,Josep Gallegos*

Main category: math.AP

TL;DR: Harmonic functions with gradient bounded by 1 in unit ball show that if gradient is ε-small on a set with positive (n-2+δ)-dimensional Hausdorff content, then maximum gradient in half-ball is bounded by Cε^α.


<details>
  <summary>Details</summary>
Motivation: To improve previous results on propagation of smallness for harmonic functions by reaching the sharp threshold for the dimension of smallness sets, overcoming limitations of prior work that required δ>1-c_n.

Method: Analysis of harmonic functions in the unit ball with bounded gradient, studying how small gradient values on sets with specific dimensional Hausdorff content propagate to control the maximum gradient in a smaller region.

Result: Established that sup_{B_{1/2}} |∇u| ≤ Cε^α, where C and α depend only on n, δ and the (n-2+δ)-Hausdorff content of E, achieving the sharp dimensional threshold for propagation of smallness.

Conclusion: This work provides an optimal result for propagation of smallness in harmonic functions, extending and improving previous bounds by reaching the critical dimension n-2+δ for the smallness sets.

Abstract: Let $u$ be a harmonic function in the unit ball $B_1 \subset \mathbb R^n$,
normalized so that its gradient has magnitude at most 1 on the unit ball. We
show that if the gradient of $u$ is $\epsilon$-small in size on a set $E\subset
B_{1/2}$ with positive $(n-2+\delta)$-dimensional Hausdorff content for some
$\delta>0$, then $\sup_{B_{1/2}} |\nabla u| \leq C \epsilon^\alpha$ with
$C,\alpha>0$ depending only on $n,\delta$ and the $(n-2+\delta)$-Hausdorff
content of $E$. This is an improvement over a similar result of Logunov and
Malinnikova that required $\delta>1-c_n$ for a small dimensional constant $c_n$
and reaches the sharp threshold for the dimension of the smallness sets from
which propagation of smallness can occur.

</details>


### [20] [1D quasi-solutions of the 2D Chern-Simons-Schr{ö}dinger system](https://arxiv.org/abs/2508.21464)
*Nicolas Rougerie,Qiyun Yang*

Main category: math.AP

TL;DR: Study of 2D abelian anyons using Schrodinger-Chern-Simons model, deriving effective 1D quintic NLS equation through anisotropic trapping potential.


<details>
  <summary>Details</summary>
Motivation: To understand the dynamics of 2D abelian anyon systems and derive simplified effective equations that capture their essential behavior in confined geometries.

Method: Used a mean-field model combining Schrodinger matter field with Chern-Simons gauge field, applied strongly anisotropic trapping potential (wave-guide), and traced out the tight confinement direction to obtain 1D effective dynamics.

Result: The effective dynamics in the loose direction of the wave-guide is governed by the classical 1D quintic nonlinear Schrodinger (NLS) equation.

Conclusion: The study successfully derived a simplified 1D effective equation from a complex 2D anyon system, showing that anisotropic confinement leads to quintic NLS dynamics, providing insights into reduced-dimensional anyon behavior.

Abstract: We study a mean-field model for a system of 2D abelian anyons, given by the
dynamics of a Schr{\"o}dinger matter field coupled to a Chern-Simons gauge
field. We derive an effective 1D equation by adding a strongly anisotropic
trapping potential (wave-guide) acting on the Schr{\"o}dinger field, and
tracing out the tight confinement direction. The effective dynamics in the
loose direction of the wave-guide turns out to be governed by the classical 1D
quintic NLS equation.

</details>


### [21] [Sharp stability in hypercontractivity estimates and logarithmic Sobolev inequalities](https://arxiv.org/abs/2508.21552)
*Zoltán M. Balogh,Alexandru Kristály*

Main category: math.AP

TL;DR: Stability analysis for hypercontractivity and logarithmic Sobolev inequalities using Hopf-Lax semigroup and Prékopa-Leindler inequality stability results, achieving sharp optimal exponent 1/2.


<details>
  <summary>Details</summary>
Motivation: To establish stability results in hypercontractivity estimates and extend them to prove stability for Euclidean L^p-logarithmic Sobolev inequalities, building on recent advances in Prékopa-Leindler inequality stability.

Method: Using the Hopf-Lax semigroup in R^n and applying recent stability results for the Prékopa-Leindler inequality from multiple sources (Böröczky & De 2021, Figalli & Ramos 2024, Figalli et al. 2025) under mild function assumptions.

Result: Proved sharp stability results with optimal exponent 1/2 in both hypercontractivity and L^p-logarithmic Sobolev deficits. The approach also successfully establishes stability for Gaussian hypercontractivity and logarithmic Sobolev inequality.

Conclusion: The method provides sharp stability bounds for fundamental inequalities in analysis, demonstrating the effectiveness of combining Hopf-Lax semigroup techniques with recent Prékopa-Leindler stability results across both Euclidean and Gaussian settings.

Abstract: We prove stability results in hypercontractivity estimates for the Hopf--Lax
semigroup in $\mathbb R^n$ and apply them to deduce stability results for the
Euclidean $L^p$-logarithmic Sobolev inequality for any $p>1$. As a main tool,
we use recent stability results for the Pr\'ekopa--Leindler inequality, due to
B\"or\"oczky and De (2021), Figalli and Ramos (2024) and Figalli, van Hintum,
and Tiba (2025). Under mild assumptions on the functions, most of our stability
results turn out to be sharp, as they are reflected in the optimal exponent
$1/2$ both in the hypercontractivity and $L^p$-logarithmic Sobolev deficits,
respectively. This approach also works for establishing stability of Gaussian
hypercontractivity estimates and Gaussian logarithmic Sobolev inequality,
respectively.

</details>


### [22] [F-equivalence for parabolic systems and applications to the stabilization of nonlinear PDE](https://arxiv.org/abs/2508.21605)
*Vincent Boulard,Amaury Hayat*

Main category: math.AP

TL;DR: Optimal conditions for F-equivalence of parabolic control systems, enabling rapid stabilization through unique feedback operators for multi-dimensional systems.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of finding good conditions for F-equivalence in infinite-dimensional parabolic systems, especially in spatial dimensions larger than one, which remains unresolved despite being solved for finite-dimensional systems 50 years ago.

Method: Introduce an extended framework for F-equivalence of parabolic operators, establish optimal conditions for existence of F-equivalence pair (T,K), and prove uniqueness when the system is approximately controllable.

Result: Main result provides optimal conditions for existence of F-equivalence pair, with uniqueness guaranteed for approximately controllable systems, enabling construction of feedback operators for rapid stabilization.

Conclusion: The framework allows rapid stabilization of various semilinear parabolic systems including heat equation, Kuramoto-Sivashinsky equation, Navier-Stokes equations, and quasilinear heat equation in multi-dimensional spaces.

Abstract: We consider the $F$-equivalence problem for parabolic systems: under which
conditions a control system, governed by a parabolic operator $A$ and a control
operator $B$, can be made equivalent to an arbitrarily exponentially stable
evolution system through an appropriate control feedback law? While this
problem has been resolved for finite-dimensional systems fifty years ago, good
conditions for infinite-dimensional systems remain a challenge, especially for
systems in spatial dimension larger than one. Our main result establishes
optimal conditions for the existence of an $F$-equivalence pair $(T,K)$ for a
given parabolic control system $(A,B)$. We introduce an extended framework for
$F$-equivalence of parabolic operators, addressing key limitations of existing
approaches, and we prove that the pair $(T,K)$ is unique if and only if $(A,B)$
is approximately controllable. As a consequence, this provides a method to
construct feedback operators for the rapid stabilization of semilinear
parabolic systems, possibly multi-dimensional in space. We provide several
illustrative examples, including the rapid stabilization of the heat equation,
the Kuramoto-Sivashinsky equation, the Navier-Stokes equations and the
quasilinear heat equation.

</details>


### [23] [Scattering for the non-radial inhomogeneous Hartree equation with a potential](https://arxiv.org/abs/2508.21822)
*Carlos M. Guzmán,Suerlan Silva,Gabriel Peçanha*

Main category: math.AP

TL;DR: Scattering proof for generalized inhomogeneous Hartree equation with potential and weight, extending previous results to nonradial data under mass-potential condition.


<details>
  <summary>Details</summary>
Motivation: Extend scattering theory to include inhomogeneous weight |x|^{-b} and potential V(x) in Hartree equation, generalizing mass-energy threshold condition.

Method: Adapt Murphy's strategy with Tao's scattering criterion and localized Morawetz estimates. Establish global well-posedness for small data using admissible Strichartz pairs.

Result: Proved scattering in intercritical case for nonradial initial data under generalized mass-potential condition.

Conclusion: Successfully handled inhomogeneous weight and potential perturbations, extending scattering results to more general Hartree equations.

Abstract: In this work, we consider the focusing generalized inhomogeneous Hartree
equation with potential \[ i u_t + \Delta u - V(x)u + \left(I_{\gamma} *
|x|^{-b}|u|^{p}\right)|x|^{-b}|u|^{p-2}u = 0, \] where $0<\gamma<3$ and
$0<b<\frac{1+\gamma}{2}$. We prove scattering in the intercritical case for
nonradial initial data, under a mass-potential condition that generalizes the
usual mass-energy threshold. The main new points compared to previous works are
the inhomogeneous weight $|x|^{-b}$ and the presence of a potential $V$, which
lead us to study the perturbed operator $-\Delta + V$.
  Our proof follows the general strategy of Murphy, but we need to adapt
several steps to deal with the weight and the potential. We use Tao's
scattering criterion together with localized Morawetz estimates in this
setting. As a preliminary step, we establish global well-posedness for small
data, which, in the presence of $V$, requires careful analysis using
appropriate admissible Strichartz pairs.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [24] [Fast Methods For Multisite Charge Transfer Processes I: Constrained, State Averaged CASSCF(1,M) and CASSCF(2M-1,M) Simulations](https://arxiv.org/abs/2508.21136)
*Tian Qiu,Joseph E. Subotnik*

Main category: physics.comp-ph

TL;DR: A new dynamically-weighted state-averaged constrained CASSCF method (eDSCn/hDSCn) is developed for studying charge transfer between multiple molecular fragments, with efficient DIIS-SQP optimization and demonstrated success on SSH chains.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient method for treating electrons or holes moving between multiple molecular fragments (n>2) in charge transfer problems, which requires handling configurations that are mutually single excitations of each other.

Method: Dynamically-weighted state-averaged constrained CASSCF approach with generalized constraints tailored for charge transfer, solved efficiently using DIIS-SQP algorithm.

Result: Successfully demonstrated on finite Su-Schrieffer-Heeger chains, reproducing expected exponential decay of diabatic couplings with distance.

Conclusion: The method enables efficient nonadiabatic dynamics simulations of complex multi-state charge transfer processes when combined with gradients.

Abstract: We design a dynamically-weighted state-averaged constrained CASSCF to treat
\ul{e}lectrons or \ul{h}oles moving between $n$ molecular fragments (where $n$
can be larger than 2). Within such a so-called eDSCn/hDSCn approach, we
consider configurations that are mutually single excitations of each other, and
we apply a generalized set of constraints to tailor the method for studying
charge transfer problems. The constrained optimization problem is efficiently
solved using a DIIS-SQP algorithm, thus maintaining computational efficiency.
We demonstrate the method for a finite Su-Schrieffer-Heeger (SSH) chain,
successfully reproducing the expected exponential decay of diabatic couplings
with distance. When combined with a gradient, the current extension immediately
enables efficient nonadiabatic dynamics simulations of complex multi-state
charge transfer processes.

</details>


### [25] [Fast Methods For Multisite Charge Transfer Processes II: Analytic Nuclear Gradients and Nonadiabatic Dynamics For cCASSCF(1,M) and cCASSCF(2M-1,M) Wavefunctions](https://arxiv.org/abs/2508.21139)
*Tian Qiu,Joseph E. Subotnik*

Main category: physics.comp-ph

TL;DR: Analytic nuclear gradients and derivative couplings for constrained CASSCF with small active spaces for electron/hole transfer modeling


<details>
  <summary>Details</summary>
Motivation: To enable efficient nonadiabatic dynamics simulations of charge transfer processes by providing analytic gradients and couplings for constrained CASSCF methods

Method: Used Lagrangian formalism to differentiate both CASSCF energy and constraints, ensuring globally smooth surfaces. Implemented efficient algorithm for nuclear gradients and derivative couplings

Result: Successfully derived and implemented analytic nuclear gradients and derivative couplings. Applied to surface-hopping simulations of proton coupled electron transfer in phenoxyl-phenol system

Conclusion: The developed method provides an efficient approach for modeling charge transfer dynamics and can be immediately applied to nonadiabatic simulations of electron/hole transfer processes

Abstract: We derive and implement analytic nuclear gradients and derivative couplings
for a constrained Complete Active Space Self-Consistent Field with a small
active space designed to model electron or hole transfer. Using a Lagrangian
formalism, we are able to differentiate both the CASSCF energy and the
constraint (which is required for globally smooth surfaces), and the resulting
efficient algorithm can be immediately applied to nonadiabatic dynamics
simulations of charge transfer processes. Here, we run initial surface-hopping
simulations of a proton coupled electron transfer event for a phenoxyl-phenol
system.

</details>


### [26] [GPU-acceleration of the Discontinuous Galerkin Shallow Water Equations Solver (DG-SWEM) using CUDA and OpenACC](https://arxiv.org/abs/2508.21208)
*Chayanon Wichitrnithed,Eirik Valseth,Clint Dawson*

Main category: physics.comp-ph

TL;DR: Porting of DG-SWEM coastal ocean solver to GPU using CUDA Fortran and OpenACC, comparing code design, programming ease, and performance on NVIDIA Grace Hopper vs CPU MPI version.


<details>
  <summary>Details</summary>
Motivation: Discontinuous Galerkin methods exhibit high data parallelism due to loose element coupling, making them naturally suitable for GPU architectures to accelerate coastal ocean circulation and storm surge simulations.

Method: Two separate GPU porting approaches: CUDA Fortran and OpenACC. OpenACC version uses Unified Memory to maintain codebase maintainability. Performance tested on NVIDIA Grace Hopper chip compared to MPI version on single CPU node (144 cores).

Result: Performance comparison between GPU implementations (CUDA Fortran and OpenACC) and CPU MPI version on realistic use cases, evaluating computational efficiency on modern GPU architecture.

Conclusion: Demonstrates successful GPU porting strategies for discontinuous Galerkin coastal ocean solvers, providing insights into code design trade-offs and performance benefits of GPU acceleration for scientific computing applications.

Abstract: This paper presents a porting of DG-SWEM, a discontinuous Galerkin solver for
coastal ocean circulation, and in particular storm surge, to GPU using two
separate approaches: CUDA Fortran and OpenACC. Time-explicit discontinuous
Galerkin methods have been shown to exhibit a large amount of data parallelism
due to the loose coupling between elements, and thus are naturally mapped to
the GPU architecture. For each porting approach, we discuss the code design,
ease of programming, and performance when running on realistic use cases.
Specifically for the OpenACC version, we also aim to preserve maintainability
within the same codebase through using Unified Memory. We test the codes on
NVIDIA's Grace Hopper chip and compare the GPU performance on each node to the
MPI version on a single CPU node (144 cores).

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [27] [Coherent attosecond pulses generated by a relativistic electron beam interacting with an intense laser at a grazing angle](https://arxiv.org/abs/2508.21293)
*H. Peng,T. W. Huang,C. N. Wu,K. Jiang,R. Li,C. Riconda,S. Weber,C. T. Zhou*

Main category: physics.plasm-ph

TL;DR: Coherent attosecond radiation achieved through grazing-angle interaction between laser pulse and relativistic electron beam, enabling compact high-energy attosecond sources.


<details>
  <summary>Details</summary>
Motivation: Generating coherent radiation from relativistic electron beams and intense laser fields has been challenging due to difficulties in precise phase matching control of radiating electrons.

Method: Laser pulse interacts at grazing angle with relativistic electron beam, causing electrons to oscillate and be modulated with superluminal phase, producing coherent ultrashort pulse trains at Cherenkov angle. Verified through theoretical modeling, 3D particle-in-cell simulations, and far-field time-domain radiation simulations.

Result: Successful demonstration of coherent attosecond radiation production through the proposed grazing-angle interaction scheme.

Conclusion: The proposed scheme enables feasible development of high-repetition-rate, compact, and high-energy attosecond pulse sources.

Abstract: The interaction between relativistic electron beams and intense laser fields
has been extensively studied for generating high-energy radiation. However,
achieving coherent radiation from such interactions needs to precisely control
the phase matching of the radiationg electrons, which has proven to be
exceptionally challenging. In this study, we demonstrate that coherent
attosecond radiation can be produced when a laser pulse interacts at grazing
angle with a relativistic electron beam. The electrons oscillate in the laser
field and are modulated with a superluminal phase, coherent ultrashort pulse
trains are produced in the far field at the Cherenkov angle. This is verified
by theoretical modeling and numerical simulations, including three-dimensional
particle-in-cell (PIC) simulations and far-field time-domain radiation
simulations. Based on our proposed scheme, high-repetition-rate, compact, and
high-energy attosecond pulse sources are feasible.

</details>


### [28] [Quantifying Reconnection and it's Dynamical Role in 2D Magnetic Rayleigh-Taylor Turbulence](https://arxiv.org/abs/2508.21532)
*Manohar Teja Kalluri,Andrew Hillier,Ben Snow*

Main category: physics.plasm-ph

TL;DR: Magnetic reconnection plays a critical role in magnetic Rayleigh-Taylor instability evolution, facilitating plume merger and enabling continued growth, accounting for up to 80% of magnetic-to-kinetic energy transfer in weak field regimes.


<details>
  <summary>Details</summary>
Motivation: Magnetic reconnection occurs during MRTI evolution but its role in energy dynamics and instability evolution remains poorly understood, particularly in astrophysical and laboratory plasmas.

Method: High-resolution 2D simulations with a robust automated reconnection detection algorithm, performing statistical analysis across various magnetic field strengths.

Result: Reconnection accounts for up to 80% of magnetic-to-kinetic energy transfer in weak magnetic field regimes, but only contributes minimally (~3%) to magnetic energy dissipation.

Conclusion: Magnetic reconnection is established as a critical mechanism regulating large-scale MRTI dynamics, with significant implications for astrophysical plasmas and turbulent mixing in magnetized flows.

Abstract: Magnetic Rayleigh-Taylor instability (MRTI) governs material transport and
mixing in astrophysical and laboratory plasmas under the influence of gravity
and magnetic fields. While magnetic reconnection is known to occur during MRTI
evolution, its role in the evolution and energy dynamics remains poorly
understood. Here, we present a comprehensive analysis of the role of
reconnection in the two-dimensional MRTI dynamics, using high-resolution
simulations. We establish that reconnection, through facilitating plume merger,
relieving magnetic tension, and enabling continued instability growth, forms an
essential component for the long-term instability evolution. To quantify the
role of reconnection in energy dynamics, we develop a robust automated
reconnection detection algorithm and perform a statistical analysis across a
range of magnetic field strengths. We find that reconnection accounts for up to
$80\%$ of the magnetic-to-kinetic energy transfer in the weak magnetic field
regime, while contributing minimally ($\approx 3\%$) to magnetic energy
dissipation. Our results establish magnetic reconnection as a critical
mechanism that regulates large-scale MRTI dynamics, with implications for
astrophysical plasmas and turbulent mixing in magnetized flows.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [29] [Bayesian perspectives for quantum states and application to ab initio quantum chemistry](https://arxiv.org/abs/2508.21729)
*Yannic Rath,Massimo Bortone,George H. Booth*

Main category: cond-mat.str-el

TL;DR: Review of machine learning-inspired approaches for quantum chemical simulations, focusing on Gaussian Process States for efficient many-electron wavefunction representation and Bayesian modeling frameworks.


<details>
  <summary>Details</summary>
Motivation: Strong electron correlation in chemical systems presents challenges for accurate simulation, requiring new efficient representations beyond traditional methods.

Method: Application of Gaussian Process States derived from machine learning principles to represent many-body wavefunctions using Bayesian modeling frameworks in second quantization.

Result: Development of novel tools for computing ab initio chemical properties and informing machine learning models to extract correlation patterns from classical data.

Conclusion: Machine learning-inspired representations provide promising alternatives to established quantum chemical approaches, unifying multiple paradigms under Bayesian frameworks for improved accuracy in strongly correlated systems.

Abstract: The quantum many-electron problem is not just at the heart of condensed
matter phenomena, but also essential for first-principles simulation of
chemical phenomena. Strong correlation in chemical systems are prevalent and
present a formidable challenge in the simulation of these systems, while
predictive phenomena in this domain often also requires a demanding level of
accuracy to inform chemical behavior. Efficient representations of the
many-electron states of chemical systems are therefore also being inspired by
machine learning principles to provide an alternative to established
approaches. In this chapter, we review recent progress in this endeavor for
quantum chemical problems represented in second quantization, and the
particular challenges present in this field. In particular, we focus on the
application of Gaussian Process States emerging from efficient representations
of the many-body wavefunction with rigorous Bayesian modeling frameworks,
allowing for the unification of multiple paradigms under a common umbrella. We
show how such models (and other representations derived from machine learning)
can be used as novel tools to compute ab initio chemical properties, while in
turn also informing the design of machine learning models to extract
correlation patterns in classical data.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [30] [Control of growth morphology of deposited fcc metals through tuning substrate-metal interactions](https://arxiv.org/abs/2508.21492)
*Samuel Aldana,Michael Nolan*

Main category: cond-mat.mtrl-sci

TL;DR: Kinetic Monte Carlo simulations show that substrate interaction strength and thermal annealing can control thin film morphology (2D vs 3D growth) in six fcc metals, enabling optimization for interconnect and catalytic applications.


<details>
  <summary>Details</summary>
Motivation: Precise control over thin film morphology is critical for optimizing material properties in technological applications like CMOS interconnects and catalysis, where growth mode determines key functional properties.

Method: Extensive kinetic Monte Carlo simulations on six fcc metals (Ag, Au, Cu, Ni, Pd, Pt) growing in (111) direction, varying substrate-metal interaction strengths by modifying activation energies for migration, combined with thermal vacuum annealing within BEOL thermal budget.

Result: Modulation of substrate interaction strength effectively promotes island formation or layer-by-layer growth. Au, Pd and Pt show highest sensitivity. Strong interactions decrease RMS roughness, substrate exposure, island number and aspect ratios, while increasing flat surface areas and coordination numbers. Thermal annealing further improves interconnect metrics when combined with strong interactions.

Conclusion: Substrate interaction strength control combined with thermal annealing provides effective strategies for morphology control in metal thin films, enabling optimization for specific applications by overcoming limitations in achieving desired surface characteristics.

Abstract: Precise control over thin film morphology is critical for optimizing material
properties across diverse technological applications, as the growth mode
(whether 2D layer-by-layer or 3D island formation)determines key functional
properties such as electrical conductivity in CMOS interconnect applications
and catalytic activity, where island distribution and size dictate performance.
To explore the role of the substrate on the morphology of deposited metals, we
present extensive kinetic Monte Carlo simulations on six fcc metals growing in
the (111) direction: Ag, Au, Cu, Ni, Pd and Pt. Our simulation framework
enables screening and evaluation of their growth mode under homoepitaxial
growth scenarios and proposes morphology control strategies by variation of
substrate-metal interaction strengths, modeled by modifying the activation
energies for upward and downward migration, combined with thermal vacuum
annealing within typical back end of line (BEOL) integration thermal budget.
Our simulation results demonstrate that modulation of the substrate interaction
strength can be effectively employed to promote island formation or
layer-by-layer growth modes overcoming limitations in achieving large flat
surface areas. Au, Pd and Pt exhibit the highest sensitivity to substrate
interaction strength variations, followed by Ag, showing that strongly
interacting substrates decrease the root mean square (RMS) roughness,
(uncovered) substrate exposure, island number and island aspect ratios, with
moderate increases in flat surface areas and atomic coordination numbers.
Additionally, interconnect relevant metrics are improved through thermal vacuum
annealing particularly when sufficiently strong metal-substrate interactions
are employed, reducing surface roughness, achieving larger flat surface areas,
merging and smoothing islands, and decreasing defect density...

</details>


### [31] [Surface Stability Modeling with Universal Machine Learning Interatomic Potentials: A Comprehensive Cleavage Energy Benchmarking Study](https://arxiv.org/abs/2508.21663)
*Ardavan Mehdizadeh,Peter Schindler*

Main category: cond-mat.mtrl-sci

TL;DR: Systematic benchmark reveals that training data composition, not architectural complexity, is the key factor for accurate cleavage energy prediction in universal machine learning interatomic potentials. Models trained on non-equilibrium configurations achieve 6% error while equilibrium-only trained models show 5x higher errors.


<details>
  <summary>Details</summary>
Motivation: Despite MLIPs' success in predicting bulk properties, there has been no systematic evaluation of their ability to predict cleavage energies - a critical property for fracture, catalysis, surface stability, and interfacial phenomena.

Method: Comprehensive benchmark of 19 state-of-the-art universal MLIPs using a DFT database of 36,718 slab structures across elemental, binary, and ternary metallic compounds. Evaluated diverse architectural paradigms across chemical compositions, crystal systems, thickness, and surface orientations.

Result: Training data composition dominates performance: OMat24-trained models achieve <6% MAPE and identify most stable surfaces in 87% of cases without explicit training. Equilibrium-only trained models show 5x higher errors. Simpler architectures with appropriate data match complex transformers' accuracy with 10-100x speedup.

Conclusion: The community should focus on strategic training data generation that captures relevant physical phenomena rather than architectural complexity, as data quality significantly outperforms model sophistication for cleavage energy prediction.

Abstract: Machine learning interatomic potentials (MLIPs) have revolutionized
computational materials science by bridging the gap between quantum mechanical
accuracy and classical simulation efficiency, enabling unprecedented
exploration of materials properties across the periodic table. Despite their
remarkable success in predicting bulk properties, no systematic evaluation has
assessed how well these universal MLIPs (uMLIPs) can predict cleavage energies,
a critical property governing fracture, catalysis, surface stability, and
interfacial phenomena. Here, we present a comprehensive benchmark of 19
state-of-the-art uMLIPs for cleavage energy prediction using our previously
established density functional theory (DFT) database of 36,718 slab structures
spanning elemental, binary, and ternary metallic compounds. We evaluate diverse
architectural paradigms, analyzing their performance across chemical
compositions, crystal systems, thickness, and surface orientations. Our results
reveal that training data composition dominates architectural sophistication:
models trained on the Open Materials 2024 (OMat24) dataset, which emphasizes
non-equilibrium configurations, achieve mean absolute percentage errors below
6% and correctly identify the thermodynamically most stable surface
terminations in 87% of cases, without any explicit surface energy training. In
contrast, architecturally identical models trained on equilibrium-only datasets
show five-fold higher errors, while models trained on surface-adsorbate data
fail catastrophically with a 17-fold degradation. Remarkably, simpler
architectures trained on appropriate data achieve comparable accuracy to
complex transformers while offering 10-100x computational speedup. These
findings show that the community should focus on strategic training data
generation that captures the relevant physical phenomena.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [32] [Data-Driven Bifurcation Handling in Physics-Based Reduced-Order Vascular Hemodynamic Models](https://arxiv.org/abs/2508.21165)
*Natalia L. Rubio,Eric F. Darve,Alison L. Marsden*

Main category: cs.CE

TL;DR: Machine learning-enhanced 0D hemodynamic model improves accuracy at vessel bifurcations while maintaining computational efficiency for cardiovascular flow simulations.


<details>
  <summary>Details</summary>
Motivation: 3D finite-element simulations are computationally expensive for clinical use, while standard reduced-order models lack accuracy at vessel bifurcations where complex flow physics occur.

Method: Developed a resistor-resistor-inductor (RRI) model using neural networks to predict pressure-flow relationships from bifurcation geometry, incorporating non-dimensionalization and flow split prediction, integrated into 0D models with optimization-based solution.

Result: Substantial accuracy improvements: RRI method reduced inlet pressure errors from 54 mmHg (45%) to 25 mmHg (17%) across various Reynolds numbers and vascular trees compared to 3D simulations.

Conclusion: The hybrid machine learning-enhanced 0D modeling approach enables accurate, real-time hemodynamic modeling suitable for clinical decision support and cardiovascular biomedical engineering applications.

Abstract: Three-dimensional (3D) finite-element simulations of cardiovascular flows
provide high-fidelity predictions to support cardiovascular medicine, but their
high computational cost limits clinical practicality. Reduced-order models
(ROMs) offer computationally efficient alternatives but suffer reduced
accuracy, particularly at vessel bifurcations where complex flow physics are
inadequately captured by standard Poiseuille flow assumptions. We present an
enhanced numerical framework that integrates machine learning-predicted
bifurcation coefficients into zero-dimensional (0D) hemodynamic ROMs to improve
accuracy while maintaining computational efficiency. We develop a
resistor-resistor-inductor (RRI) model that uses neural networks to predict
pressure-flow relationships from bifurcation geometry, incorporating linear and
quadratic resistances along with inductive effects. The method employs
non-dimensionalization to reduce training data requirements and apriori flow
split prediction for improved bifurcation characterization. We incorporate the
RRI model into a 0D model using an optimization-based solution strategy. We
validate the approach in isolated bifurcations and vascular trees, across
Reynolds numbers from 0 to 5,500, defining ROM accuracy by comparison to 3D
finite element simulation. Results demonstrate substantial accuracy
improvements: averaged across all trees and Reynolds numbers, the RRI method
reduces inlet pressure errors from 54 mmHg (45%) for standard 0D models to 25
mmHg (17%), while a simplified resistor-inductor (RI) variant achieves 31 mmHg
(26%) error. The enhanced 0D models show particular effectiveness at high
Reynolds numbers and in extensive vascular networks. This hybrid numerical
approach enables accurate, real-time hemodynamic modeling for clinical decision
support, uncertainty quantification, and digital twins in cardiovascular
biomedical engineering.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [33] [Faster Linear Algebra Algorithms with Structured Random Matrices](https://arxiv.org/abs/2508.21189)
*Chris Camaño,Ethan N. Epperly,Raphael A. Meyer,Joel A. Tropp*

Main category: cs.DS

TL;DR: This paper introduces the Oblivious Subspace Injection (OSI) property as a unified framework for analyzing structured random matrices in randomized linear algebra algorithms, providing theoretical guarantees and practical implementations for faster low-rank approximation and least-squares regression.


<details>
  <summary>Details</summary>
Motivation: Despite extensive research on structured random matrices for randomized linear algebra, basic questions remain about their design and analysis. Practitioners need faster, near-optimal algorithms for fundamental linear algebra tasks using structured dimension reduction maps.

Method: Develops the OSI property framework that factors analysis into: (1) proving algorithms work with OSI matrices, and (2) proving specific random matrix models satisfy OSI. Analyzes standard randomized algorithms under OSI assumption and identifies various OSI examples including sparse matrices, trigonometric transforms, and tensor product structures.

Result: Theoretical results show faster, near-optimal runtimes for fundamental linear algebra tasks. Provides implementation guidance and empirical evidence demonstrating exemplary performance for synthetic problems and scientific applications using structured random matrices.

Conclusion: The OSI framework provides a unified approach to analyze structured random matrices, enabling faster randomized linear algebra algorithms with theoretical guarantees and practical performance benefits across various applications.

Abstract: To achieve the greatest possible speed, practitioners regularly implement
randomized algorithms for low-rank approximation and least-squares regression
with structured dimension reduction maps. Despite significant research effort,
basic questions remain about the design and analysis of randomized linear
algebra algorithms that employ structured random matrices.
  This paper develops a new perspective on structured dimension reduction,
based on the oblivious subspace injection (OSI) property. The OSI property is a
relatively weak assumption on a random matrix that holds when the matrix
preserves the length of vectors on average and, with high probability, does not
annihilate any vector in a low-dimensional subspace. With the OSI abstraction,
the analysis of a randomized linear algebra algorithm factors into two parts:
(i) proving that the algorithm works when implemented with an OSI; and (ii)
proving that a given random matrix model has the OSI property.
  This paper develops both parts of the program. First, it analyzes standard
randomized algorithms for low-rank approximation and least-squares regression
under the OSI assumption. Second, it identifies many examples of OSIs,
including random sparse matrices, randomized trigonometric transforms, and
random matrices with tensor product structure. These theoretical results imply
faster, near-optimal runtimes for several fundamental linear algebra tasks. The
paper also provides guidance on implementation, along with empirical evidence
that structured random matrices offer exemplary performance for a range of
synthetic problems and contemporary scientific applications.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [34] [Mean Field Games of Controls with Dirichlet \& Neumann Boundary Conditions](https://arxiv.org/abs/2508.21642)
*P. Jameson Graber,Kyle Rosengartner*

Main category: math.OC

TL;DR: Analysis of mean field games with controls under Dirichlet/Neumann boundary conditions, proving well-posedness with smallness conditions or monotone couplings


<details>
  <summary>Details</summary>
Motivation: To establish rigorous mathematical foundations for mean field games of controls with boundary conditions, addressing the challenge of analyzing systems where player costs depend on joint state and control distributions

Method: Mathematical analysis of mean field game systems with Dirichlet and Neumann boundary conditions, employing techniques to prove well-posedness through smallness conditions and monotonicity assumptions

Result: Proved that such mean field game systems are well-posed either when sufficient smallness conditions are satisfied or in cases with monotone couplings

Conclusion: The paper provides important theoretical guarantees for mean field games of controls with boundary conditions, establishing conditions under which these complex systems have well-defined solutions

Abstract: In a mean field game of controls, a large population of identical players
seek to minimize a cost that depends on the joint distribution of the states of
the players and their controls. We consider the classes of mean field games of
controls in which the value function and the distribution of player states
satisfy either Dirichlet or Neumann boundary conditions. We prove that such
systems are well-posed either with sufficient smallness conditions or in the
case of monotone couplings.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [35] [A Mixture of Experts Gating Network for Enhanced Surrogate Modeling in External Aerodynamics](https://arxiv.org/abs/2508.21249)
*Mohammad Amin Nabian,Sanjay Choudhry*

Main category: cs.LG

TL;DR: A meta-learning Mixture of Experts framework that combines three state-of-the-art CFD surrogate models (DoMINO, X-MeshGraphNet, FigConvNet) using a gating network to achieve superior aerodynamic prediction accuracy.


<details>
  <summary>Details</summary>
Motivation: High computational cost of CFD simulations is a bottleneck in automotive design. While ML-based surrogates offer acceleration, no single architecture demonstrates universal superiority across diverse scenarios.

Method: Proposes a Mixture of Experts model with a gating network that dynamically combines predictions from three heterogeneous neural networks. Uses entropy regularization to prevent model collapse and ensure balanced expert contributions. Trained on DrivAerML dataset.

Result: Achieves significant reduction in L-2 prediction error, outperforming both ensemble average and the most accurate individual expert model across all evaluated physical quantities (surface pressure and wall shear stress fields).

Conclusion: The MoE framework establishes an effective strategy for creating robust composite surrogate models by synergistically combining complementary strengths of specialized architectures in automotive aerodynamics.

Abstract: The computational cost associated with high-fidelity CFD simulations remains
a significant bottleneck in the automotive design and optimization cycle. While
ML-based surrogate models have emerged as a promising alternative to accelerate
aerodynamic predictions, the field is characterized by a diverse and rapidly
evolving landscape of specialized neural network architectures, with no single
model demonstrating universal superiority. This paper introduces a novel
meta-learning framework that leverages this architectural diversity as a
strength. We propose a Mixture of Experts (MoE) model that employs a dedicated
gating network to dynamically and optimally combine the predictions from three
heterogeneous, state-of-the-art surrogate models: DoMINO, a decomposable
multi-scale neural operator; X-MeshGraphNet, a scalable multi-scale graph
neural network; and FigConvNet, a factorized implicit global convolution
network. The gating network learns a spatially-variant weighting strategy,
assigning credibility to each expert based on its localized performance in
predicting surface pressure and wall shear stress fields. To prevent model
collapse and encourage balanced expert contributions, we integrate an entropy
regularization term into the training loss function. The entire system is
trained and validated on the DrivAerML dataset, a large-scale, public benchmark
of high-fidelity CFD simulations for automotive aerodynamics. Quantitative
results demonstrate that the MoE model achieves a significant reduction in L-2
prediction error, outperforming not only the ensemble average but also the most
accurate individual expert model across all evaluated physical quantities. This
work establishes the MoE framework as a powerful and effective strategy for
creating more robust and accurate composite surrogate models by synergistically
combining the complementary strengths of specialized architectures.

</details>


### [36] [Convergence of Stochastic Gradient Methods for Wide Two-Layer Physics-Informed Neural Networks](https://arxiv.org/abs/2508.21571)
*Bangti Jin,Longjun Wu*

Main category: cs.LG

TL;DR: Establishes linear convergence of stochastic gradient descent for over-parameterized two-layer Physics Informed Neural Networks (PINNs) with general activation functions, extending previous gradient descent results.


<details>
  <summary>Details</summary>
Motivation: PINNs are popular neural solvers for PDEs, typically trained with stochastic gradient descent methods. However, convergence guarantees for stochastic optimization in PINNs were lacking compared to deterministic gradient descent.

Method: Analyzed stochastic gradient descent/flow for over-parameterized two-layer PINNs, focusing on maintaining positive definiteness of Gram matrices during training to handle dynamic randomness from stochastic optimization.

Result: Proved linear convergence in high probability for stochastic gradient descent in training PINNs with general activation functions, extending previous gradient descent convergence results.

Conclusion: The analysis provides theoretical guarantees for stochastic optimization methods in PINNs, offering insights into optimization dynamics and ensuring neural networks trained by stochastic algorithms have convergence guarantees.

Abstract: Physics informed neural networks (PINNs) represent a very popular class of
neural solvers for partial differential equations. In practice, one often
employs stochastic gradient descent type algorithms to train the neural
network. Therefore, the convergence guarantee of stochastic gradient descent is
of fundamental importance. In this work, we establish the linear convergence of
stochastic gradient descent / flow in training over-parameterized two layer
PINNs for a general class of activation functions in the sense of high
probability. These results extend the existing result [18] in which gradient
descent was analyzed. The challenge of the analysis lies in handling the
dynamic randomness introduced by stochastic optimization methods. The key of
the analysis lies in ensuring the positive definiteness of suitable Gram
matrices during the training. The analysis sheds insight into the dynamics of
the optimization process, and provides guarantees on the neural networks
trained by stochastic algorithms.

</details>


<div id='math.FA'></div>

# math.FA [[Back]](#toc)

### [37] [Fractional Heat Semigroup Characterization of Distances from Functions in Lipschitz Spaces to Their Subspaces](https://arxiv.org/abs/2508.21269)
*Feng Dai,Eero Saksman,Dachun Yang,Wen Yuan,Yangyang Zhang*

Main category: math.FA

TL;DR: Characterizes distance from functions in inhomogeneous Lipschitz spaces to non-dense subspaces using fractional semigroups and critical indices from 'bad' point sets.


<details>
  <summary>Details</summary>
Motivation: To develop a quantitative method for measuring the distance between functions in Lipschitz spaces and various subspaces (Sobolev, Besov, etc.) using fractional semigroup techniques.

Method: Uses fractional semigroup operators and defines 'bad' point sets D_α,r(s,f,ε) where fractional derivatives exceed threshold. Introduces admissible set functions ν to measure these sets and defines critical index ε_α,r,s,ν(f).

Result: Shows that for broad classes of subspaces V ⊂ Λ_s, there exists an admissible function ν such that the critical index ε_α,r,s,ν(f) is equivalent to the distance dist(f, V)_Λ_s.

Conclusion: Provides a novel characterization of distances in Lipschitz spaces through fractional semigroup analysis and critical indices, applicable to various function space intersections including Sobolev and Besov spaces.

Abstract: Let $\Lambda_s$ denote the inhomogeneous Lipschitz space of order
$s\in(0,\infty)$ on $\mathbb{R}^n$. This article characterizes the distance
$d(f, V)_{\Lambda_s}: = \inf_{g\in V} \|f-g\|_{\Lambda_s}$ from a function
$f\in \Lambda_s$ to a non-dense subspace $V\subset \Lambda_s$ via the
fractional semigroup $\{T_{\alpha, t}: =e^{-t (-\Delta)^{\alpha/2}}: t\in (0,
\infty)\}$ for any $\alpha\in(0,\infty)$. Given an integer $ r >s/\alpha$, a
uniformly bounded continuous function $f$ on $\mathbb{R}^n$ belongs to the
space $\Lambda_s$ if and only if there exists a constant $\lambda\in(0,\infty)$
such that \begin{align*} \left|(-\Delta)^{\frac {\alpha r}2} (T_{\alpha,
t^\alpha } f)(x) \right|\leq \lambda t^{s -r\alpha }\ \ \text{for any
$x\in\mathbb{R}^n$ and $t\in (0, 1]$}.\end{align*} The least such constant is
denoted by $\lambda_{ \alpha, r, s}(f)$. For each $f\in \Lambda_s$ and
$0<\varepsilon< \lambda_{\alpha,r, s}(f)$, let $$ D_{\alpha,
r}(s,f,\varepsilon):=\left\{ (x,t)\in \mathbb{R}^n\times (0,1]:\ \left|
(-\Delta)^{\frac {\alpha r}2} (T_{\alpha, t^\alpha} f)(x) \right|> \varepsilon
t^{s -r \alpha }\right\}$$ be the set of ``bad'' points. To quantify its size,
we introduce a class of extended nonnegative \emph{admissible set functions}
$\nu$ on the Borel $\sigma$-algebra $\mathcal{B}(\mathbb{R}^n\times [0, 1])$
and define, for any admissible function $\nu$, the \emph{critical index} $
\varepsilon_{\alpha, r, s,\nu}(f):=\inf\{\varepsilon\in(0,\infty):\
\nu(D_{\alpha, r}(s,f,\varepsilon))<\infty\}.$ Our result shows that, for a
broad class of subspaces $V\subset \Lambda_s$, including intersections of
$\Lambda_s$ with Sobolev, Besov, Triebel--Lizorkin, and Besov-type spaces,
there exists an admissible function $\nu$ depending on $V$ such that
$\varepsilon_{\alpha, r, s,\nu}(f)\sim \mathrm{dist}(f, V)_{\Lambda_s}.$

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [38] [Survival probability for jump processes in unbounded domains on metric measure spaces](https://arxiv.org/abs/2508.21158)
*Phanuel Mariano,Jing Wang*

Main category: math.PR

TL;DR: Analysis of survival probability decay rates for symmetric jump processes in unbounded domains with positive spectrum bottom, providing probabilistic interpretation and geometric conditions.


<details>
  <summary>Details</summary>
Motivation: To understand the large time behavior of survival probabilities in unbounded domains for symmetric jump processes, particularly establishing connections between spectral properties and geometric domain characteristics.

Method: Proved asymptotic upper and lower bounds with explicit constants using the bottom of spectrum λ(D), applied to symmetric jump processes in general metric measure spaces, with specific focus on α-stable processes in uniformly C¹¹ domains.

Result: Established probabilistic interpretation and equivalent geometric condition for λ(D)>0, demonstrated sharp exponential decay rates in horn-shaped domains, and provided examples of applicable unbounded domains.

Conclusion: The study successfully links spectral properties with geometric domain characteristics for symmetric jump processes, providing explicit bounds and interpretations that advance understanding of survival probability decay in unbounded settings.

Abstract: We study the large time behavior of the survival probability
$\mathbb{P}_x\left(\tau_D>t\right)$ for symmetric jump processes in unbounded
domains with a positive bottom of the spectrum. We prove asymptotic upper and
lower bounds with explicit constants in terms of the bottom of the spectrum
$\lambda(D)$. Our main result applies to symmetric jump processes in general
metric measure spaces. For $\alpha$-stable processes in unbounded uniformly
$C^{1,1}$ domains, our results provide a probabilistic interpretation and an
equivalent geometric condition for $\lambda(D)>0$. In the case of increasing
horn-shaped domains, the exponential rate of decay for the survival probability
is sharp. We also present examples of unbounded domains where our results
apply.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [39] [LREI: A fast numerical solver for quantum Landau-Lifshitz equations](https://arxiv.org/abs/2508.21200)
*Davoud Mirzaei,Behnam Hashemi,Vahid Azimi-Mousolou*

Main category: quant-ph

TL;DR: LREI enables efficient quantum spin dynamics simulations by exploiting low-rank density matrix structure and Hamiltonian sparsity, reducing computational complexity from O(N³) to O(r²N) and memory from O(N²) to O(rN).


<details>
  <summary>Details</summary>
Motivation: Traditional methods for solving quantum Landau-Lifshitz equations become computationally infeasible as system size grows exponentially with spin count, requiring memory-efficient approaches for large quantum spin systems.

Method: Uses low-rank factorization of density matrices, Krylov subspace methods for partial eigendecompositions, and Householder reflectors to handle invariant subspaces without forming full matrices. Reformulates Runge-Kutta and Adams-Bashforth schemes to preserve physical properties.

Result: Achieves dramatic speedup - a 20-spin system (1M+ matrix size) now takes seconds per time step on a standard laptop instead of being computationally prohibitive.

Conclusion: LREI enables previously infeasible simulations of large quantum spin systems, providing a powerful tool for comparing q-LL/q-LLG dynamics and studying quantum features like entanglement across different system regimes.

Abstract: We develop LREI (Low-Rank Eigenmode Integration), a memory- and
time-efficient scheme for solving quantum Landau-Lifshitz (q-LL) and quantum
Landau-Lifshitz-Gilbert (q-LLG) equations, which govern spin dynamics in open
quantum systems. Although system size grows exponentially with the number of
spins, our approach exploits the low-rank structure of the density matrix and
the sparsity of Hamiltonians to avoid full matrix computations. By representing
density matrices via low-rank factors and applying Krylov subspace methods for
partial eigendecompositions, we reduce the per-step complexity of Runge-Kutta
and Adams-Bashforth schemes from $\mathcal{O}(N^3)$ to $\mathcal{O}(r^2N)$,
where $N = 2^n$ is the Hilbert space dimension for $n$ spins and $r \ll N$ the
effective rank. Similarly, memory costs shrink from $\mathcal{O}(N^2)$ to
$\mathcal{O}(rN)$, since no full $N\times N$ matrices are formed. A key advance
is handling the invariant subspace of zero eigenvalues. By using Householder
reflectors built for the dominant eigenspace, we perform the solution entirely
without large matrices. For example, a time step of a twenty-spin system, with
density matrix size over one million, now takes only seconds on a standard
laptop. Both Runge-Kutta and Adams-Bashforth methods are reformulated to
preserve physical properties of the density matrix throughout evolution. This
low-rank algorithm enables simulations of much larger spin systems, which were
previously infeasible, providing a powerful tool for comparing q-LL and q-LLG
dynamics, testing each model validity, and probing how quantum features such as
correlations and entanglement evolve across different regimes of system size
and damping.

</details>


### [40] [Block Encoding of Sparse Matrices via Coherent Permutation](https://arxiv.org/abs/2508.21667)
*Abhishek Setty*

Main category: quant-ph

TL;DR: A unified framework for efficient block encoding of arbitrary sparse matrices using combinatorial optimization and coherent permutation operators to overcome key implementation challenges.


<details>
  <summary>Details</summary>
Motivation: Block encoding of sparse matrices is crucial for quantum algorithms like singular value transformation and Hamiltonian simulation, but efficient gate-level implementations for arbitrary sparse matrices remain challenging due to multi-controlled X gates overhead, amplitude reordering issues, and hardware connectivity constraints.

Method: Novel connection with combinatorial optimization for systematic control qubit assignment to achieve nearest-neighbor connectivity, combined with coherent permutation operators that preserve superposition while enabling amplitude reordering.

Result: Significant reductions in circuit depth and control overhead for structured sparse matrices, providing explicit gate-level constructions that bridge theoretical formulations with practical circuit implementations.

Conclusion: The framework enables efficient block encoding for arbitrary sparse matrices, overcoming key obstacles and making quantum algorithms more practical for real-world implementation.

Abstract: Block encoding of sparse matrices underpins powerful quantum algorithms such
as quantum singular value transformation, Hamiltonian simulation, and quantum
linear solvers, but its efficient gate-level implementation for arbitrary
sparse matrices remains a major challenge. We introduce a unified framework
that overcomes the key obstacles of multi-controlled X gates overhead,
amplitude reordering, and hardware connectivity, enabling efficient block
encoding for arbitrary sparse matrices with explicit gate-level constructions.
Central to our approach are a novel connection with combinatorial optimization,
which enables systematic assignment of control qubits to achieve
nearest-neighbor connectivity, and coherent permutation operators that preserve
superposition while enabling amplitude reordering. We demonstrate our methods
on structured sparse matrices, showing significant reductions in circuit depth
and control overhead, thereby bridging the gap between theoretical formulations
and practical circuit implementations for quantum algorithms.

</details>


<div id='physics.app-ph'></div>

# physics.app-ph [[Back]](#toc)

### [41] [When Energy and Information Revolutions Meet 2D Janus](https://arxiv.org/abs/2508.21425)
*Long Zhang,Ziqi Ren,Li Sun,Yihua Gao,Deli Wang,Junjie He,Guoying Gao*

Main category: physics.app-ph

TL;DR: This review paper provides a comprehensive analysis of 2D Janus materials, covering their theoretical predictions, experimental preparations, modulation strategies, and applications in energy and information fields.


<details>
  <summary>Details</summary>
Motivation: Addressing global concerns of energy depletion, environmental issues, and quantum limitations in post-Moore era integrated circuits by exploring 2D Janus materials with unique asymmetric properties.

Method: Systematic review summarizing theoretical predictions, experimental preparations, modulation strategies, and recent advances in modifiable properties of 2D Janus materials, focusing on experimentally realized hexagonal and trigonal structures.

Result: The review establishes a comprehensive repository of 2D Janus family materials, outlining their applications in optics, catalysis, piezoelectricity, electrochemistry, thermoelectricity, magnetism, and electronics.

Conclusion: 2D Janus materials offer promising solutions for energy and information challenges, and this review serves as a valuable resource for designing, fabricating, regulating, and applying these materials in both theoretical and experimental contexts.

Abstract: The depletion of energy sources, worsening environmental issues, and the
quantum limitations of integrated circuits for information storage in the
post-Moore era, are pressing global concerns. Fortunately, two-dimensional (2D)
Janus materials, possessing broken spatial symmetry, with emerging
pressure-dependent and non-linear optical response, piezoelectricity, valley
polarization, Rashba spin splitting and more, have established a substantial
platform for exploring and applying modifiable physical, chemical and
biological properties in material science and offered a promising solution for
these energy and information issues. To furnish researchers with a
comprehensive repository of 2D Janus family, this review systematically
summarizes their theoretical predictions, experimental preparations, and
modulation strategies. It also retrospectively outlines the recent advances in
modifiable properties, applications, and inherent mechanisms in optics,
catalysis, piezoelectricity, electrochemistry, thermoelectricity, magnetism,
and electronics, with a focus on experimentally realized hexagonal and trigonal
Janus structures. Additionally, their current research state is summarized, and
potential opportunities and challenges that may arise are highlighted. Overall,
this review aims to serve as a valuable resource for designing, fabricating,
regulating, and applying 2D Janus systems, both theoretically and
experimentally. This review will strongly promote the advanced academic
investigations and industrial applications of 2D Janus materials in energy and
information fields.

</details>
