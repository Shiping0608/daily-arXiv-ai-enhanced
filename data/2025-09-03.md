<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 36]
- [math.AP](#math.AP) [Total: 40]
- [physics.comp-ph](#physics.comp-ph) [Total: 13]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 12]
- [cs.LG](#cs.LG) [Total: 4]
- [cs.CV](#cs.CV) [Total: 1]
- [math.OC](#math.OC) [Total: 5]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 2]
- [math.RA](#math.RA) [Total: 1]
- [physics.atom-ph](#physics.atom-ph) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [astro-ph.SR](#astro-ph.SR) [Total: 1]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [stat.ML](#stat.ML) [Total: 3]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.CE](#cs.CE) [Total: 2]
- [physics.optics](#physics.optics) [Total: 1]
- [nlin.AO](#nlin.AO) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 2]
- [math.CA](#math.CA) [Total: 1]
- [gr-qc](#gr-qc) [Total: 1]
- [math.FA](#math.FA) [Total: 2]
- [hep-th](#hep-th) [Total: 1]
- [math.PR](#math.PR) [Total: 2]
- [math.DG](#math.DG) [Total: 3]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [A quasi-Trefftz space for a second order time-harmonic Maxwell's equation](https://arxiv.org/abs/2509.00193)
*Lise-Marie Imbert-Gérard*

Main category: math.NA

TL;DR: First study of local Taylor-based polynomial quasi-Trefftz spaces for systems of PDEs, specifically for electromagnetic wave propagation in inhomogeneous media using second-order Maxwell's equations.


<details>
  <summary>Details</summary>
Motivation: To develop quasi-Trefftz methods (a family of Discontinuous Galerkin methods) that rely on equation-dependent function spaces for solving systems of partial differential equations, particularly for electromagnetic applications with variable coefficients.

Method: Uses local Taylor-based polynomial quasi-Trefftz spaces and an adequate Helmholtz decomposition for spaces of homogeneous polynomial vector fields to construct quasi-Trefftz functions.

Result: Obtained explicit dimension of the proposed quasi-Trefftz space and developed a procedure to construct quasi-Trefftz functions for the second-order formulation of Maxwell's equations with variable coefficients.

Conclusion: Successfully established the foundation for quasi-Trefftz methods applied to systems of PDEs, providing both theoretical dimension analysis and practical construction methods for electromagnetic wave propagation problems in inhomogeneous media.

Abstract: Quasi-Trefftz methods are a family of Discontinuous Galerkin methods relying
on equation-dependent function spaces. This work is the first study of the
notion of local Taylor-based polynomial quasi-Trefftz space for a system of
Partial Differential Equations (PDEs). These discrete spaces are introduced
here for electro-magnetic wave propagation in inhomogeneous media, governed by
a second order formulation of Maxwell's equation with variable coefficients.
Thanks to an adequate Helmholtz decomposition for spaces of homogeneous
polynomial vector fields, the outcome is the explicit dimension of the proposed
quasi-Trefftz space as well as a procedure to construct quasi-Trefftz
functions.

</details>


### [2] [WoSNN: Stochastic Solver for PDEs with Machine Learning](https://arxiv.org/abs/2509.00204)
*Silei Song,Arash Fahim,Michael Mascagni*

Main category: math.NA

TL;DR: WoS-NN integrates machine learning with Walk-on-Spheres method to solve elliptic PDEs with Dirichlet boundary conditions, achieving 75% error reduction using only 8% of path samples compared to traditional WoS.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient stochastic solver for elliptic PDEs that combines the strengths of traditional Walk-on-Spheres method with machine learning for faster and more accurate global solutions.

Method: Integration of neural networks with Walk-on-Spheres method and space discretization approaches to create WoS-NN, a meshless method that works well with irregular regions.

Result: WoS-NN provides accurate field estimations with 75% error reduction while using only 8% of path samples compared to conventional WoS method, saving computational time and resources.

Conclusion: The proposed WoS-NN method successfully combines machine learning with traditional stochastic solvers to achieve superior performance in solving elliptic PDEs with significant computational efficiency gains.

Abstract: Solving elliptic partial differential equations (PDEs) is a fundamental step
in various scientific and engineering studies. As a classic stochastic solver,
the Walk-on-Spheres (WoS) method is a well-established and efficient algorithm
that provides accurate local estimates for PDEs. In this paper, by integrating
machine learning techniques with WoS and space discretization approaches, we
develop a novel stochastic solver, WoS-NN. This new method solves elliptic
problems with Dirichlet boundary conditions, facilitating precise and rapid
global solutions and gradient approximations. The method inherits excellent
characteristics from the original WoS method, such as being meshless and robust
to irregular regions. By integrating neural networks, WoS-NN also gives instant
local predictions after training without re-sampling, which is especially
suitable for intense requests on a static region. A typical experimental result
demonstrates that the proposed WoS-NN method provides accurate field
estimations, reducing errors by around $75\%$ while using only $8\%$ of path
samples compared to the conventional WoS method, which saves abundant
computational time and resource consumption.

</details>


### [3] [Superconvergence Extraction of Upwind Discontinuous Galerkin Method Solving the Radiative Transfer Equation](https://arxiv.org/abs/2509.00296)
*Andres Galindo-Olarte,Zhichao Peng,Jennifer K. Ryan*

Main category: math.NA

TL;DR: The paper analyzes superconvergence of upwind DG methods for radiative transfer equations and applies SIAC filters to enhance accuracy orders with minimal computational overhead, achieving significant efficiency gains.


<details>
  <summary>Details</summary>
Motivation: To improve the accuracy and computational efficiency of discontinuous Galerkin methods for solving radiative transfer equations, particularly for low-dimensional macroscopic moments that are of practical interest.

Method: Theoretical analysis of superconvergence properties combined with application of Smooth-Increasing Accuracy-Conserving (SIAC) filters on upwind discontinuous Galerkin methods for both steady-state and time-dependent radiative transfer equations.

Result: Proved (2k+2)-th order superconvergence for steady-state problems and (2k+1/2)-th order for time-dependent problems. Numerical experiments confirmed post-filter convergence orders of 2k+2 (steady-state) and 2k+1 (time-dependent), with substantial computational efficiency improvements: 2.22× accuracy improvement and 19.94× time reduction for time-dependent problems, and 4-9× acceleration for steady-state problems.

Conclusion: SIAC filtering effectively enhances both accuracy and computational efficiency for DG methods in radiative transfer problems, providing substantial performance gains without accuracy loss, making it a valuable technique for practical applications.

Abstract: We theoretically analyze the superconvergence of the upwind discontinuous
Galerkin (DG) method for both the steady-state and time-dependent radiative
transfer equation (RTE), and apply the Smooth-Increasing Accuracy-Conserving
(SIAC) filters to enhance the accuracy order. Direct application of SIAC
filters on low-dimensional macroscopic moments, often the quantities of
practical interest, can effectively improve the approximation accuracy with
marginal computational overhead.
  Using piecewise $k$-th order polynomials for the approximation and assuming
constant cross sections, we prove $(2k+2)$-th order superconvergence for the
steady-state problem at Radau points on each element and $(2k+1/2)$-th order
superconvergence for the global $L^2$ and negative-order Sobolev norms for the
time-dependent problem.
  Numerical experiments confirm the efficacy of the filtering, demonstrating
post-filter convergence orders of $2k+2$ for steady-state and $2k+1$ for
time-dependent problems. More significantly, the SIAC filter delivers
substantial gains in computational efficiency. For a time-dependent problem, we
observed an approximately $2.22 \times$ accuracy improvement and a $19.94
\times$ reduction in computational time. For the steady-state problems, the
filter achieved a $4$--$9 \times$ acceleration without any loss of accuracy.

</details>


### [4] [Development of numerical methods for nonlinear hybrid stochastic functional differential equations with infinite delay](https://arxiv.org/abs/2509.00475)
*Guozhen Li,Xiaoyue Li,Xuerong Mao*

Main category: math.NA

TL;DR: Explicit truncated Euler-Maruyama schemes for nonlinear hybrid stochastic functional differential equations with infinite delay, achieving 2pth moment boundedness, strong convergence, and 1/2 order convergence rate with exponential stability analysis.


<details>
  <summary>Details</summary>
Motivation: To develop explicit numerical approximations for complex nonlinear hybrid stochastic functional differential equations with infinite delay, which are challenging to solve analytically and require reliable computational methods.

Method: Proposed explicit truncated Euler-Maruyama schemes, analyzed 2pth moment boundedness and strong convergence of numerical solutions, established 1/2 order convergence rate under stronger conditions, and examined exponential stability properties.

Result: Successfully obtained 2pth moment boundedness and strong convergence of numerical solutions, achieved 1/2 order convergence rate, and demonstrated exponential stability including moment and almost sure exponential stability.

Conclusion: The explicit truncated Euler-Maruyama schemes provide effective numerical solutions for nonlinear hybrid stochastic functional differential equations with infinite delay, with proven convergence properties and stability guarantees.

Abstract: This paper focuses on explicit numerical approximations for nonlinear hybrid
stochastic functional differential equations with infinite delay. Precisely,
explicit truncated Euler-Maruyama schemes are proposed, $2p$th $(p \ge 1)$
moment boundedness and strong convergence of the numerical solutions are
obtained. Under slightly stronger conditions, the $1/2$ order convergence rate
is established. Furthermore, the exponential stability, including moment and
almost sure exponential stability, is examined. Finally, an example is provided
to illustrate our results.

</details>


### [5] [A Mixed Precision Eigensolver Based on the Jacobi Algorithm](https://arxiv.org/abs/2509.00495)
*Zhengbo Zhou*

Main category: math.NA

TL;DR: Mixed precision Jacobi algorithm using single precision for approximate eigenvectors and double precision Newton-Schulz iteration for orthogonalization, achieving 30% speedup over traditional Jacobi.


<details>
  <summary>Details</summary>
Motivation: Accelerate the classic Jacobi algorithm for symmetric matrix spectral decomposition by leveraging mixed precision arithmetic while maintaining accuracy.

Method: Compute approximate eigenvectors in single precision, orthogonalize using Newton-Schulz iteration in double precision, then apply cyclic-by-row Jacobi algorithm on the transformed matrix.

Result: The mixed precision approach requires less than 10 Jacobi iterations due to quadratic convergence and achieves roughly 30% time reduction compared to standard Jacobi algorithm.

Conclusion: Mixed precision arithmetic combined with cyclic-by-row Jacobi and Newton-Schulz orthogonalization provides significant computational savings while maintaining accuracy for symmetric matrix spectral decomposition.

Abstract: The classic method for computing the spectral decomposition of a real
symmetric matrix, the Jacobi algorithm, can be accelerated by using mixed
precision arithmetic. The Jacobi algorithm is aiming to reduce the off-diagonal
entries iteratively using Givens rotations. We investigate how to use the low
precision to speed up this algorithm based on the approximate spectral
decomposition in low precision.
  We first study two different index choosing techniques, classical and
cyclic-by-row, for the Jacobi algorithm. Numerical testing suggests that
cyclic-by-row is more efficient. Then we discuss two different methods of
orthogonalizing an almost orthogonal matrix: the QR factorization and the polar
decomposition. For polar decomposition, we speed up the Newton iteration by
using the one-step Schulz iteration. Based on numerical testing, using the
polar decomposition approach (Newton--Schulz iteration) is not only faster but
also more accurate than using the QR factorization.
  A mixed precision algorithm for computing the spectral decomposition of a
real symmetric matrix at double precision is provided. In doing so we compute
the approximate eigenvector matrix $Q_\ell$ of $A$ in single precision using
$\texttt{eig}$ and $\texttt{single}$ in MATLAB. We then use the Newton--Schulz
iteration to orthogonalize the eigenvector matrix $Q_\ell$ into an orthogonal
matrix $Q_d$ in double precision. Finally, we apply the cyclic-by-row Jacobi
algorithm on $Q_d^TAQ_d$ and obtain the spectral decomposition of $A$. At this
stage, we will see, from the testings, the cyclic-by-row Jacobi algorithm only
need less than 10 iterations to converge by utilizing the quadratic
convergence. The new mixed precision algorithm requires roughly 30\% of the
time used by the Jacobi algorithm on its own.

</details>


### [6] [On discrete Sobolev inequalities for nonconforming finite elements under a semi-regular mesh condition](https://arxiv.org/abs/2509.00505)
*Hiroki Ishizaka*

Main category: math.NA

TL;DR: Discrete L^q-L^p Sobolev inequality for Crouzeix-Raviart and discontinuous Crouzeix-Raviart finite elements on anisotropic meshes in 2D/3D, with mesh-robust constant depending only on domain and semi-regular parameter.


<details>
  <summary>Details</summary>
Motivation: To provide a robust mathematical foundation for stability and error analysis of nonconforming and discontinuous Galerkin methods on highly anisotropic meshes, where traditional isotropic approaches fail.

Method: Uses anisotropy-sensitive trace inequality with element height, two-step affine/Piola mapping, Raviart--Thomas interpolation stability, and discrete integration-by-parts with weighted jump/trace terms on faces.

Result: Derived discrete Sobolev inequality applicable to all (q,p) pairs satisfying local Sobolev embedding (including q ≤ p), with constant robust against mesh aspect ratios and interior angles.

Conclusion: The inequality provides a mesh-robust foundation for analyzing nonconforming and discontinuous Galerkin methods on anisotropic meshes, enabling reliable numerical simulations on complex geometries.

Abstract: We derive a discrete $ L^q-L^p$ Sobolev inequality tailored for the
Crouzeix--Raviart and discontinuous Crouzeix--Raviart finite element spaces on
anisotropic meshes in both two and three dimensions. Subject to a semi-regular
mesh condition, this discrete Sobolev inequality is applicable to all pairs
$(q,p)$ that align with the local Sobolev embedding, including scenarios where
$q \leq p$. Importantly, the constant is influenced solely by the domain and
the semi-regular parameter, ensuring robustness against variations in aspect
ratios and interior angles of the mesh. The proof employs an
anisotropy-sensitive trace inequality that leverages the element height, a
two-step affine/Piola mapping approach, the stability of the Raviart--Thomas
interpolation, and a discrete integration-by-parts identity augmented with
weighted jump/trace terms on faces. This Sobolev inequality serves as a
mesh-robust foundation for the stability and error analysis of nonconforming
and discontinuous Galerkin methods on highly anisotropic meshes.

</details>


### [7] [The adaptive EM schemes for McKean-Vlasov SDEs with common noise in finite and infinite horizons](https://arxiv.org/abs/2509.00521)
*Hu Liu,Shuaibin Gao,Junhao Hu*

Main category: math.NA

TL;DR: Adaptive Euler-Maruyama schemes for McKean-Vlasov SDEs with common noise, achieving L^p convergence rates under superlinear growth conditions.


<details>
  <summary>Details</summary>
Motivation: To develop efficient numerical approximation methods for McKean-Vlasov stochastic differential equations with common noise, particularly when dealing with superlinear growth conditions in drift and diffusion coefficients.

Method: Adaptive Euler-Maruyama schemes are employed to approximate solutions of McKean-Vlasov SDEs with common noise, analyzing convergence under superlinear growth conditions.

Result: The paper establishes L^p convergence rates in both finite and infinite time horizons, with rates depending on particle number and step size. Numerical examples validate the theoretical results.

Conclusion: The adaptive EM schemes provide effective approximation for McKean-Vlasov SDEs with common noise, with proven convergence rates under challenging superlinear growth conditions.

Abstract: This paper is dedicated to investigating the adaptive Euler-Maruyama (EM)
schemes for the approximation of McKean-Vlasov stochastic differential
equations (SDEs) with common noise. When the drift and diffusion coefficients
both satisfy the superlinear growth conditions, the $L^p$ convergence rates in
finite and infinite horizons are revealed, which reacts to the particle number
and step size. Subsequently, there is an illustration of the theory results by
means of two numerical examples.

</details>


### [8] [Stabilization techniques for immersogeometric analysis of plate and shell problems in explicit dynamics](https://arxiv.org/abs/2509.00522)
*Giuliano Guarino,Yannis Voet,Pablo Antolin,Annalisa Buffa*

Main category: math.NA

TL;DR: Extension of previous work to enable stable immersogeometric analysis of plates and shells using lumped mass matrices, addressing time step constraints in explicit dynamics.


<details>
  <summary>Details</summary>
Motivation: Finite element plate/shell formulations face stringent time step constraints in explicit dynamics due to high-order PDEs and structural slenderness, worsened by badly cut elements in immersed discretizations. Lumped mass matrices can increase critical time step but may cause spurious oscillations.

Method: Based on polynomial extensions technique from previous work, this approach allows stable immersogeometric analysis with lumped mass matrices while maintaining accuracy comparable to boundary-fitted discretizations.

Result: The method enables stable analysis of plate and shell problems using lumped mass matrices in immersed finite element settings, overcoming the spurious oscillation issues.

Conclusion: This extension provides a stable framework for immersogeometric analysis of slender structures with lumped mass matrices, achieving accuracy levels similar to traditional boundary-fitted approaches while addressing time step constraints.

Abstract: Finite element plate and shell formulations are ubiquitous in structural
analysis for modeling all kinds of slender structures, both for static and
dynamic analyses. The latter are particularly challenging as the high order
nature of the underlying partial differential equations and the slenderness of
the structures all impose a stringent constraint on the critical time step in
explicit dynamics. Unfortunately, badly cut elements in immersed finite element
discretizations further aggravate the issue. While lumping the mass matrix
often increases the critical time step, it might also trigger spurious
oscillations in the approximate solution thereby compromising the numerical
solution. In this article, we extend our previous work in
\cite{voet2025stabilization} to allow stable immersogeometric analysis of plate
and shell problems with lumped mass matrices. This technique is based on
polynomial extensions and restores a level of accuracy comparable to
boundary-fitted discretizations.

</details>


### [9] [Numerical solution of 2D boundary value problems on merged Voronoi-Delaunay meshes](https://arxiv.org/abs/2509.00557)
*M. M. Chernyshov,P. N. Vabishchevich*

Main category: math.NA

TL;DR: Novel mesh approach combining Delaunay triangulations and Voronoi partitions to create orthodiagonal quadrilateral cells for solving multidimensional boundary value problems.


<details>
  <summary>Details</summary>
Motivation: To develop a more natural and consistent computational framework for finite-volume approximations of conservation laws using irregular meshes, particularly for anisotropic media problems.

Method: Employ meshes with nodes at both Delaunay triangulation vertices and Voronoi partition generators, creating merged Voronoi-Delaunay meshes with orthodiagonal quadrilateral cells for approximating scalar/vector functions and gradient/divergence operators.

Result: The approach enables convenient approximation of mathematical operators and demonstrates capability through solution of a steady-state diffusion-reaction problem in anisotropic medium.

Conclusion: The Voronoi-Delaunay mesh framework provides an effective computational technology for multidimensional boundary value problems with irregular meshes and finite-volume approximations.

Abstract: Computational technologies for the approximate solution of multidimensional
boundary value problems often rely on irregular computational meshes and
finite-volume approximations. In this framework, the discrete problem
represents the corresponding conservation law for control volumes associated
with the nodes of the mesh. This approach is most naturally and consistently
implemented using Delaunay triangulations together with Voronoi diagrams as
control volumes. In this paper, we employ meshes with nodes located both at the
vertices of Delaunay triangulations and at the generators of Voronoi
partitions. The cells of the merged Voronoi-Delaunay mesh are orthodiagonal
quadrilaterals. On such meshes, scalar and vector functions, as well as
invariant gradient and divergence operators of vector calculus, can be
conveniently approximated. We illustrate the capabilities of this approach by
solving a steady-state diffusion-reaction problem in an anisotropic medium.

</details>


### [10] [A level-set based finite difference method for the ground state Bose-Einstein condensates in smooth bounded domains](https://arxiv.org/abs/2509.00668)
*Hwi Lee,Yingjie Liu*

Main category: math.NA

TL;DR: A level-set finite difference method for computing Bose-Einstein condensate ground states in curved domains using Cartesian grids with ghost points and PDE-based extension techniques.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient computational method for solving Bose-Einstein condensate ground state problems in domains with complex curved boundaries, overcoming the challenges of traditional interpolation methods near irregular boundaries.

Method: Uses normalized gradient flow with Cartesian grid discretization, PDE-based extension technique for third-order accurate ghost point values near boundaries, and explicit ghost value mapping for seamless integration with implicit time-stepping methods.

Result: The method effectively handles domains with curved boundaries and corners, demonstrating third-order accuracy and computational efficiency while avoiding complex interpolation procedures.

Conclusion: The proposed level-set based finite difference method provides an accurate and efficient approach for computing ground states of Bose-Einstein condensates in complex domains, with potential applications to problems involving higher-order interaction terms.

Abstract: We present a level-set based finite difference method to calculate the ground
states of Bose Einstein condensates in domains with curved boundaries. Our
method draws on the variational and level set approaches, benefiting from both
of their long-standing success. More specifically, we use the normalized
gradient flow, where the spatial discretization is based on the simple
Cartesian grid with fictitious values in the outer vicinity of the domains. We
develop a PDE-based extension technique that systematically and automatically
constructs ghost point values with third-order accuracy near irregular
boundaries, effectively circumventing the computational complexity of
interpolation in these regions. Another novel aspect of our work is the
application of the PDE-based extension technique to a nodal basis function,
resulting in an explicit ghost value mapping that can be seamlessly
incorporated into implicit time-stepping methods where the extended function
values are treated as unknowns at the next time step. We present numerical
examples to demonstrate the effectiveness of our method, including its
application to domains with corners and to problems involving higher-order
interaction terms.

</details>


### [11] [Robust and fast iterative method for the elliptic Monge-Ampère equation](https://arxiv.org/abs/2509.00794)
*R. N. Köhle,K. T. W. Menting,K. Mitra,J. H. M. ten Thije Boonkkamp*

Main category: math.NA

TL;DR: Fast iterative scheme for elliptic Monge-Ampère equation using linearization and fixed-point iteration with Poisson solves


<details>
  <summary>Details</summary>
Motivation: The Monge-Ampère equation is nonlinear and degenerate but has important applications in optimal transport, geometric optics, and differential geometry, requiring robust numerical methods

Method: Linearizes the equation using fixed-point iteration (L-scheme), solving a Poisson problem in each step with weighted residual as right-hand side

Result: Proven contraction in H² and L∞ for weight greater than largest Hessian eigenvalue; outperforms Newton's method in speed and stability in test cases

Conclusion: The proposed iterative scheme is robust against discretization, nonlinearities, and degeneracies, with performance enhanced by preconditioners or Green's functions

Abstract: This paper introduces a fast and robust iterative scheme for the elliptic
Monge-Amp\`ere equation with Dirichlet boundary conditions. The Monge-Amp\`ere
equation is a nonlinear and degenerate equation, with applications in optimal
transport, geometric optics, and differential geometry. The proposed method
linearises the equation and uses a fixed-point iteration (L-scheme), solving a
Poisson problem in each step with a weighted residual as the right-hand side.
This algorithm is robust against discretisation, nonlinearities, and
degeneracies. For a weight greater than the largest eigenvalue of the Hessian,
contraction in $H^2$ and $L^\infty$ is proven for both classical and
generalised solutions, respectively. The method's performance can be enhanced
by using preconditioners or Green's functions. Test cases demonstrate that the
scheme outperforms Newton's method in speed and stability.

</details>


### [12] [An Inexact Low-Rank Source Iteration for Steady-State Radiative Transfer Equation with Diffusion Synthetic Acceleration](https://arxiv.org/abs/2509.00805)
*Wei Guo,Zhichao Peng*

Main category: math.NA

TL;DR: Inexact low-rank source iteration with diffusion synthetic acceleration for solving multidimensional radiative transfer equations, achieving 90x speedup and 100x storage reduction.


<details>
  <summary>Details</summary>
Motivation: To solve multidimensional steady-state radiative transfer equations more efficiently by reducing computational resources through low-rank representations.

Method: Uses low-rank matrix or hierarchical Tucker tensor format for angular flux representation, preconditioned low-rank conjugate gradient method with diffusion preconditioner, and adaptive inexact strategy with dynamic tolerance relaxation.

Result: Achieves errors of 10^-4 to 10^-5 relative to full-rank solutions, reduces degrees of freedom by up to 100x, and achieves over 90x speedup in diffusion-dominated cases while remaining competitive in transport-dominated problems.

Conclusion: First low-rank SI-DSA framework for multidimensional steady-state RTE that provides substantial computational and storage efficiency gains across various problem types.

Abstract: We propose an inexact low-rank source iteration with diffusion synthetic
acceleration (SI-DSA) for solving the multidimensional steady-state radiative
transfer equation (RTE) in the second-order formulation. The angular flux is
represented in either a low-rank matrix or hierarchical Tucker tensor (HTT)
format, enabling substantial reductions in computational resources. Each SI
step is solved using a preconditioned low-rank conjugate gradient (CG) method
with a diffusion preconditioner. To further improve efficiency, we introduce an
adaptive inexact strategy that dynamically relaxes the inner CG tolerance
during early SI iterations. The method exploits the tensor-product structure of
the discretized operators to perform all matrix-vector operations in low-rank
form. Numerical experiments on 2D2V benchmark problems, including
diffusion-dominated, transport-dominated, and multiscale problems, demonstrate
that the proposed approach achieves errors on the order of $10^{-4}$ to
$10^{-5}$ relative to full-rank reference solutions, while reducing the degrees
of freedom by up to two orders of magnitude. In the diffusion-dominated case,
the low-rank solver achieves speedups exceeding $90\times$ over its full-rank
counterpart and remains competitive in solving challenging transport-dominated
and multiscale problems while providing substantial storage savings. To our
knowledge, this work provides the first low-rank SI-DSA framework for
multidimensional steady-state RTE.

</details>


### [13] [On the numerical computation of $R_0$ in periodic environments](https://arxiv.org/abs/2509.00847)
*Dimitri Breda,Simone De Reggi,Jordi Ripoll*

Main category: math.NA

TL;DR: A novel spectral method using evolution semigroups to approximate R0 in periodic epidemic models via Fourier/Chebyshev discretization, achieving spectral accuracy with validation on multi-group and vector-borne disease models.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for computing the basic reproduction number R0 in time-periodic population models can be complex to implement. The paper aims to develop a more straightforward method that characterizes R0 via evolution semigroups and provides efficient numerical approximation.

Method: Characterize the Next-Generation Operator via evolution semigroups, identify birth/infection and transition operators, discretize them using Fourier or Chebyshev collocation methods, and solve a generalized matrix eigenvalue problem to obtain R0.

Result: The method achieves spectral accuracy with convergence order depending on model coefficient regularity. Validated on multi-group epidemic models with periodic contact rates and seasonal vector-borne disease models, showing improved implementation ease compared to existing approaches.

Conclusion: The proposed evolution semigroup approach provides an efficient and accurate method for computing R0 in periodic epidemic models, with easy implementation and adaptability to compute type-reproduction numbers as well.

Abstract: We propose a novel approach to approximate the basic reproduction number
$R_0$ as spectral radius of the Next-Generation Operator in time-periodic
population models by characterizing the latter via evolution semigroups. Once
birth/infection and transition operators are identified, we discretize them via
either Fourier or Chebyshev collocation methods. Then $R_0$ is obtained by
solving a generalized matrix eigenvalue problem. The order of convergence of
the approximating reproduction numbers to the true one is shown to depend on
the regularity of the model coefficients, and spectral accuracy is proved. We
validate the theoretical results by discussing applications to epidemiology,
viz. a large-size multi-group epidemic model with periodic contact rates, and a
vector-borne disease model with seasonal vector recruitment. We illustrate how
the method facilitates implementation compared to existing approaches and how
it can be easily adapted to also compute type-reproduction numbers.

</details>


### [14] [Sufficient conditions for strong discrete maximum principles in finite element solutions of linear and semilinear elliptic equations](https://arxiv.org/abs/2509.00932)
*Andrei Draganescu,L. Ridgway Scott*

Main category: math.NA

TL;DR: Novel technique proves global strong discrete maximum principles for finite element discretizations when standard matrix-based conditions fail, using macroelement extension and connectivity arguments.


<details>
  <summary>Details</summary>
Motivation: Existing matrix-based sufficient conditions for discrete maximum principles are often not satisfied, requiring alternative approaches to prove these fundamental properties for finite element methods.

Method: Extends strong form of discrete maximum principle from macroelements to entire domain using connectivity arguments, applied to elliptic equations with pathological meshes and semilinear cases.

Result: Successfully establishes global strong discrete maximum principles for finite element discretizations that would otherwise fail standard matrix-based verification conditions.

Conclusion: The connectivity-based approach provides a powerful alternative to matrix-based conditions for proving discrete maximum principles, enabling analysis of challenging cases including pathological meshes and semilinear elliptic equations.

Abstract: We introduce a novel technique for proving global strong discrete maximum
principles for finite element discretizations of linear and semilinear elliptic
equations for cases when the common, matrix-based sufficient conditions are not
satisfied. The basic argument consists of extending the strong form of discrete
maximum principle from macroelements to the entire domain via a connectivity
argument. The method is applied to discretizations of elliptic equations with
certain pathological meshes, and to semilinear elliptic equations.

</details>


### [15] [Deep Tangent Bundle (DTB) method: a Deep Neural Network approach to compute solutions of PDES](https://arxiv.org/abs/2509.00957)
*Hao Wu,Haomin Zhou*

Main category: math.NA

TL;DR: The Deep Tangent Bundle (DTB) method is a numerical framework for solving high-dimensional evolutionary PDEs using deep neural networks and tangent bundle approximation without nonconvex optimization.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of solving evolutionary partial differential equations in high dimensions by leveraging the expressive power of deep neural networks while avoiding complex optimization problems.

Method: Uses the tangent bundle of an adaptively updated deep neural network to approximate the vector field in spatial variables, combined with traditional time discretization schemes.

Result: The method demonstrates simplicity, flexibility, and efficiency for various high-dimensional PDEs through several numerical examples.

Conclusion: DTB provides an effective approach for high-dimensional PDE solutions by combining neural network expressiveness with tangent bundle simplicity, avoiding nonconvex optimization challenges.

Abstract: We develop a numerical framework, the Deep Tangent Bundle (DTB) method, that
is suitable for computing solutions of evolutionary partial differential
equations (PDEs) in high dimensions. The main idea is to use the tangent bundle
of an adaptively updated deep neural network (DNN) to approximate the vector
field in the spatial variables while applying the traditional schemes for time
discretization. The DTB method takes advantage of the expression power of DNNs
and the simplicity of the tangent bundle approximation. It does not involve
nonconvex optimization. Several numerical examples demonstrate that the DTB is
simple, flexible, and efficient for various PDEs of higher dimensions.

</details>


### [16] [A concurrent global-local numerical method for multiscale parabolic equations](https://arxiv.org/abs/2509.01059)
*Yulei Liao,Yang Liu,Pingbing Ming*

Main category: math.NA

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper presents a concurrent global-local numerical method for solving
multiscale parabolic equations in divergence form. The proposed method employs
hybrid coefficient to provide accurate macroscopic information while preserving
essential microscopic details within specified local defects. Both the
macroscopic and microscopic errors have been improved compared to existing
results, eliminating the factor of $\Delta t^{-1/2}$ when the diffusion
coefficient is time-independent. Numerical experiments demonstrate that the
proposed method effectively captures both global and local solution behaviors.

</details>


### [17] [A Structure-Preserving Numerical Method for Harmonic Maps Between High-genus Surfaces](https://arxiv.org/abs/2509.01256)
*Zhipeng Zhu,Wai Yeung Lam,Lok Ming Lui*

Main category: math.NA

TL;DR: A structure-preserving algorithm for computing harmonic maps between closed surfaces of genus ≥2 using hyperbolic geometry and generalized cotangent weights.


<details>
  <summary>Details</summary>
Motivation: To develop a framework for comparing surfaces with non-trivial topology by minimizing distortion, addressing the limitation that conformal or isometric maps may not exist between surfaces with different geometries.

Method: Minimizes Dirichlet energy over geodesic realizations of surface graphs into target hyperbolic surfaces within a fixed homotopy class, using canonical edge weights derived from hyperbolic metrics that generalize Euclidean cotangent weights.

Result: The algorithm preserves injectivity and ensures that isometries remain harmonic in the discrete theory, reflecting classical harmonic map behavior.

Conclusion: The developed framework provides optimal homeomorphisms between surfaces with negative curvature and generalizes classical harmonic map theory to the discrete hyperbolic setting.

Abstract: Motivated by geometry processing for surfaces with non-trivial topology, we
study discrete harmonic maps between closed surfaces of genus at least two.
Harmonic maps provide a natural framework for comparing surfaces by minimizing
distortion. Unlike conformal or isometric maps-which may not exist between
surfaces with different geometries-harmonic maps always exist within a fixed
homotopy class and yield optimal homeomorphisms when the target surface has
negative curvature. We develop a structure-preserving algorithm to compute
harmonic maps from a triangulated surface to a reference hyperbolic surface.
The method minimizes Dirichlet energy over geodesic realizations of the surface
graph into the target hyperbolic surface in the homotopy class of a
homeomorphism. A central feature of our framework is the use of canonical edge
weights derived from the hyperbolic metric, which generalize the classical
cotangent weights from the Euclidean setting. These weights preserve
injectivity and ensure that isometries remain harmonic in the discrete theory,
reflecting their classical behavior.

</details>


### [18] [Linear, decoupled, second-order and structure-preserving scheme for Carreau fluid equations coupled with steric Poisson-Nernst-Planck model](https://arxiv.org/abs/2509.01270)
*Wenxing Zhu,Mingyang Pan,Dongdong He*

Main category: math.NA

TL;DR: A linear, decoupled second-order accurate scheme for Carreau fluid equations coupled with steric Poisson-Nernst-Planck model, preserving positivity and energy dissipation.


<details>
  <summary>Details</summary>
Motivation: To study ionic steric effects in complex fluid systems by developing a structure-preserving numerical scheme that can handle nonlinear coupling while maintaining physical properties.

Method: Uses logarithmic transformation for ion concentration positivity, introduces nonlocal auxiliary variable for free energy to handle nonlinear coupling terms, and employs finite element approximations with second-order time accuracy.

Result: The scheme is proven to be mass conservative, positivity-preserving for ion concentration, and energy dissipative at discrete level. Numerical simulations confirm stability and accuracy.

Conclusion: The developed scheme successfully handles ionic steric effects in Carreau-SPNP systems while preserving essential physical properties, providing an effective tool for numerical investigation of steric effects.

Abstract: In this paper, to study ionic steric effects, we present a linear, decoupled,
second-order accurate in time and structure-preserving scheme with finite
element approximations for Carreau fluid equations coupled with steric
Poisson-Nernst-Planck (SPNP) model. The logarithmic transformation for the ion
concentration is used to preserve positivity property. To deal with the
nonlinear coupling terms in fluid equation, a nonlocal auxiliary variable with
respect to the free energy of SPNP equations and its associated ordinary
differential equation are introduced. The obtained system is equivalent to the
original system. The fully discrete scheme is proved to be mass conservative,
positivity-preserving for ion concentration and energy dissipative at discrete
level. Some numerical simulations are provided to demonstrate its stability and
accuracy. Moreover, the ionic steric effects are numerically investigated.

</details>


### [19] [Linear, decoupled, positivity preserving, positive-definiteness preserving and energy stable schemes for the diffusive Oldroyd-B coupled with PNP model](https://arxiv.org/abs/2509.01278)
*Wenxing Zhu,Mingyang Pan,Dongdong He*

Main category: math.NA

TL;DR: A first-order finite element scheme for viscoelastic electrohydrodynamic modeling using Poisson-Nernst-Planck equations and Oldroyd-B model with logarithmic transformations to preserve positivity and energy stability.


<details>
  <summary>Details</summary>
Motivation: To develop a numerical scheme that can accurately handle viscoelastic electrohydrodynamic problems while preserving physical properties like positive-definiteness of conformation tensor and positivity of ion concentrations, particularly addressing the high Weissenberg number problem.

Method: Uses finite element method with logarithmic transformations for conformation tensor and ion concentrations, introduces nonlocal auxiliary variable and splitting technique for decoupled scheme, ensuring mass conservation and energy stability at discrete level.

Result: The scheme demonstrates convergence rates and robust performance in numerical examples, successfully handles high Weissenberg number problem at moderately high Weissenberg numbers, and captures elastic effects in electro-convection phenomena.

Conclusion: The proposed first-order finite element scheme provides an effective and stable numerical approach for viscoelastic electrohydrodynamic modeling, preserving essential physical properties while addressing challenging high Weissenberg number scenarios.

Abstract: In this paper, we present a first-order finite element scheme for the
viscoelastic electrohydrodynamic model. The model incorporates the
Poisson-Nernst-Planck equations to describe the transport of ions and the
Oldroyd-B constitutive model to capture the behavior of viscoelastic fluids. To
preserve the positive-definiteness of the conformation tensor and the
positivity of ion concentrations, we employ both logarithmic transformations.
The decoupled scheme is achieved by introducing a nonlocal auxiliary variable
and using the splitting technique. The proposed schemes are rigorously proven
to be mass conservative and energy stable at the fully discrete level. To
validate the theoretical analysis, we present numerical examples that
demonstrate the convergence rates and the robust performance of the schemes.
The results confirm that the proposed methods accurately handle the high
Weissenberg number problem (HWNP) at moderately high Weissenberg numbers.
Finally, the flow structure influenced by the elastic effect within the
electro-convection phenomena has been studied.

</details>


### [20] [Quasi-optimal error estimates for the approximation of stable stationary states of the elastic energy of inextensible curves](https://arxiv.org/abs/2509.01287)
*Sören Bartels,Balázs Kovács,Dominik Schneider*

Main category: math.NA

TL;DR: Local existence and quasi-optimal error estimate for piecewise cubic minimizers of bending energy with improved discretized inextensibility constraint enforcement.


<details>
  <summary>Details</summary>
Motivation: Previous discretization methods only enforced inextensibility constraints at nodes, leading to suboptimal convergence rates. This research aims to improve convergence by better constraint enforcement.

Method: Enforce inextensibility constraint at both nodes and midpoints of each subinterval. Use inverse function theorem to prove existence and error estimate for stationary states of bending energy.

Result: Achieved quasi-optimal convergence rates for piecewise cubic minimizers. Numerical simulations verified the theoretical results experimentally.

Conclusion: The improved discretization method with constraint enforcement at both nodes and midpoints provides superior convergence performance compared to previous approaches that only enforced constraints at nodes.

Abstract: We establish local existence and a quasi-optimal error estimate for piecewise
cubic minimizers to the bending energy under a discretized inextensibility
constraint. In previous research a discretization is used where the
inextensibility constraint is only enforced at the nodes of the discretization.
We show why this discretization leads to suboptimal convergence rates and we
improve on it by also enforcing the constraint in the midpoints of each
subinterval. We then use the inverse function theorem to prove existence and an
error estimate for stationary states of the bending energy that yields
quasi-optimal convergence. We use numerical simulations to verify the
theoretical results experimentally.

</details>


### [21] [Curvature-Based Optimal Polynomial Geometric Interpolation of Circular Arcs](https://arxiv.org/abs/2509.01353)
*Ema Češek,Aleš Vavpetič*

Main category: math.NA

TL;DR: Optimal approximation of circular arcs using parametric polynomial curves with curvature error minimization, analyzed for parabolic G⁰, cubic G¹, and quartic G² interpolation cases.


<details>
  <summary>Details</summary>
Motivation: To develop efficient and accurate methods for approximating circular arcs using low-degree parametric polynomial curves while minimizing curvature error, which is important for CAD/CAM systems and geometric modeling.

Method: Uses parametric polynomial curves of low degrees (parabolic, cubic, quartic) with prescribed geometric continuity (G⁰, G¹, G²) at boundary points. Analyzes optimal approximation based on curvature error minimization rather than traditional radial error approaches.

Result: Provides analysis and comparison of approximation quality for different polynomial degrees and continuity conditions. Shows that curvature-based approximation can yield different results compared to radial error-based methods.

Conclusion: Curvature error minimization provides an alternative and potentially superior approach to circular arc approximation compared to traditional radial error methods, with different polynomial degrees and continuity conditions offering trade-offs between accuracy and computational complexity.

Abstract: The problem of the optimal approximation of circular arcs by parametric
polynomial curves is considered. The optimality relates to the curvature error.
Parametric polynomial curves of low degree are used and a geometric continuity
is prescribed at the boundary points of the circular arc. Analysis is done for
cases of parabolic $G^0$, cubic $G^1$ and quartic $G^2$ interpolation. The
comparison of the approximation of circular arcs based on curvature with the
approximation based on radial error is provided.

</details>


### [22] [A geometrically robust unfitted boundary algebraic equation method based on discrete potentials and local basis functions](https://arxiv.org/abs/2509.01380)
*Qing Xia*

Main category: math.NA

TL;DR: Unfitted boundary algebraic equation method using lattice Green's functions and discrete potential theory for solving elliptic PDEs in complex geometries with dimension reduction and geometric flexibility.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient method for solving elliptic partial differential equations in complex geometries that avoids traditional mesh generation challenges and handles both smooth and non-smooth boundaries.

Method: Uses lattice Green's functions on infinite regular grids with discrete potential theory to construct single/double layer potentials (discrete boundary integral method). Employs local basis functions on cut cells and difference potentials framework with FFT-based solvers for efficient computation.

Result: Method achieves dimension reduction, geometric flexibility, mesh-independent conditioning, and small-cut stability. Numerical experiments show accuracy and robustness for ellipses, diamonds with varying aspect ratios, sharp corners, and potential flows in unbounded domains.

Conclusion: The developed unfitted BAE method provides an effective approach for elliptic PDEs in complex geometries with theoretical stability, convergence guarantees, and practical advantages over traditional methods.

Abstract: We present an unfitted boundary algebraic equation (BAE) method for solving
elliptic partial differential equations in complex geometries. The method
employs lattice Green's functions on infinite regular grids combined with
discrete potential theory to construct single and double layer potentials,
which is a discrete analog to boundary integral method. Local basis functions
on cut cells accommodate arbitrary boundary conditions and seamlessly integrate
with the boundary algebraic equations. The difference potentials framework
enables efficient treatment of nonhomogeneous terms and fast computation of
layer potentials via FFT-based solvers. We establish theoretical stability and
convergence through a novel interpolation operator framework. Key advantages of
the developed method include: dimension reduction, geometric flexibility,
mesh-independent conditioning, small-cut stability, and uniform treatment of
smooth and non-smooth geometries. Numerical experiments validate accuracy and
robustness across ellipses and diamonds with varying aspect ratios and sharp
corners, and an application of potential flows in unbounded domains.

</details>


### [23] [Global convergence of adaptive least-squares finite element methods for nonlinear PDEs](https://arxiv.org/abs/2509.01531)
*Philipp Bringmann,Dirk Praetorius*

Main category: math.NA

TL;DR: Weighted least-squares minimization for Zarantonello fixed-point iteration updates, enabling conforming FEM discretization with built-in error estimators and adaptive mesh refinement.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient computational framework for quasilinear PDEs with strongly monotone and Lipschitz continuous nonlinearity using the established Zarantonello fixed-point iteration scheme.

Method: Proposes a weighted least-squares minimization approach for computing updates in the Zarantonello scheme, allowing conforming finite element discretization with arbitrary polynomial degrees. Includes an adaptive Uzawa-type algorithm with outer linearization and inner adaptive mesh-refinement loops.

Result: The method provides built-in a posteriori discretization error estimators and proves R-linear convergence of linearization iterates for arbitrary initial guesses. Numerical experiments demonstrate the algorithm's performance.

Conclusion: The weighted least-squares formulation enhances the robustness of the Zarantonello damping parameter and enables efficient adaptive computation for quasilinear PDEs with strong theoretical convergence guarantees.

Abstract: The Zarantonello fixed-point iteration is an established linearization scheme
for quasilinear PDEs with strongly monotone and Lipschitz continuous
nonlinearity. This paper presents a weighted least-squares minimization for the
computation of the update of this scheme. The resulting formulation allows for
a conforming finite element discretization of the primal and dual variable of
the PDE with arbitrary polynomial degree. The least-squares functional provides
a built-in a posteriori discretization error estimator in each linearization
step motivating an adaptive Uzawa-type algorithm with an outer linearization
loop and an inner adaptive mesh-refinement loop. We prove R-linear convergence
of the linearization iterates for arbitrary initial guesses. Particular focus
is on the role of the weights in the least-squares functional of the linearized
problem and their influence on the robustness of the Zarantonello damping
parameter. Numerical experiments illustrate the performance of the proposed
algorithm.

</details>


### [24] [User Manual for Model-based Imaging Inverse Problem](https://arxiv.org/abs/2509.01572)
*Xiaodong Wang*

Main category: math.NA

TL;DR: A user manual explaining model-based optimization for imaging inverse problems, targeting beginners without prior optimization knowledge, organized into four key questions about problem definition, optimization rationale, solution methods, and practical implementation.


<details>
  <summary>Details</summary>
Motivation: To provide accessible guidance on complex imaging inverse problems for individuals lacking background in convex optimization or inverse problem theory, focusing on logical thinking rather than rigorous mathematical notation.

Method: Structured manual divided into four sections addressing: 1) defining inverse imaging problems, 2) explaining why optimization is used, 3) detailing solution methods for optimization problems, and 4) implementing optimization algorithms in real imaging systems.

Result: A systematic educational resource that clarifies the mathematical rationale behind imaging inverse problems through a question-driven approach, making complex optimization concepts more accessible to beginners.

Conclusion: This manual serves as a practical guide for understanding and solving imaging inverse problems through model-based optimization, emphasizing logical progression and practical implementation over mathematical rigor.

Abstract: This user manual is intended to provide a detailed description on model-based
optimization for imaging inverse problem. Theseproblems can be particularly
complex and challenging, especially for individuals without prior exposure to
convex optimization orinverse problem theory, like myself. In light of this, I
am writing this manual to clarify and systematically organize the
mathematicalrationale underlying imaging inverse problems. This manual might
not be accurate in mathmatical notion but more focus on the logicalthinking on
how to solve and proceed to solve the problems. If you want to think deep about
something, try to raise questions! Thismanual is seaprated into four sections,
aiming to answer the following four questions: (1) What is inverse imaging
problem? (2) Why optimization is used to solve the inverse imaging problem? (3)
How to solve the optimization problem? (4) How to implement the optimization
algorithm in real imaging system?

</details>


### [25] [An efficient spline-based scheme on Shishkin-type meshes for solving singularly perturbed coupled systems with Robin boundary conditions](https://arxiv.org/abs/2509.01575)
*Kousalya Ramanujam,Vembu Shanthi*

Main category: math.NA

TL;DR: A numerical scheme for weakly coupled singularly perturbed reaction-diffusion equations with Robin boundary conditions using specialized meshes to handle boundary layers, achieving second-order convergence.


<details>
  <summary>Details</summary>
Motivation: To solve systems of singularly perturbed reaction-diffusion equations with different small parameters and Robin boundary conditions, where solutions exhibit complex overlapping boundary layers that require specialized numerical treatment.

Method: Employed standard Shishkin mesh and proposed a modified Bakhvalov-Shishkin mesh. Used cubic spline approximation at boundary conditions and central difference scheme at interior points to resolve the boundary layers.

Result: The scheme achieves almost second-order convergence (up to logarithmic factor) on Shishkin mesh and exact second-order convergence on the modified Bakhvalov-Shishkin mesh, validated by numerical results.

Conclusion: The proposed numerical approach effectively handles the challenging boundary layer problems in weakly coupled singularly perturbed systems with Robin boundary conditions, demonstrating high-order convergence on specialized meshes.

Abstract: In this paper, we investigate a weakly coupled system of singularly perturbed
linear reaction-diffusion equations with Robin boundary conditions, where the
leading terms are multiplied by small positive parameters that may differ in
magnitude. The solution to this system exhibits overlapping and interacting
boundary layers. To appropriately resolve these layers, we employ a standard
Shishkin mesh and propose a novel modification of Bakhvalov-Shishkin mesh. A
cubic spline approximation is applied at the boundary conditions, while a
central difference scheme is used at the interior points. The problem is then
solved on both meshes. It is demonstrated that the proposed scheme achieves
almost second-order convergence upto a logarithmic factor on the Shishkin mesh
and exact second-order convergence on the modified Bakhvalov-Shishkin mesh. We
present numerical results to validate the accuracy of our findings.

</details>


### [26] [A Million-Point Fast Trajectory Optimization Solver](https://arxiv.org/abs/2509.01855)
*A. Javeed,D. P. Kouri,D. Ridzal,J. D. Steinman,I. M. Ross*

Main category: math.NA

TL;DR: This paper presents a breakthrough method for solving million-point trajectory optimization problems at unprecedented speed on small processors using Birkhoff-theoretic discretization, matrix-free Krylov methods, and near-perfect preconditioning.


<details>
  <summary>Details</summary>
Motivation: To overcome the computational challenges of solving large-scale trajectory optimization problems with millions of grid points, which are traditionally considered computationally prohibitive.

Method: Uses three key mathematical advancements: (1) Birkhoff-theoretic discretization of optimal control problems, (2) matrix-free linear algebra with Krylov-subspace methods, and (3) a near-perfect Birkhoff preconditioner that enables O(1) iteration speed. Leverages FFT techniques for O(N log N) Birkhoff matrix-vector products.

Result: Achieves unprecedented computational scale and speed, demonstrated through numerical experiments on practical astrodynamics problems with million-point grids.

Conclusion: The proposed algorithmic framework enables solving previously intractable large-scale trajectory optimization problems efficiently on small form-factor processors, representing a significant advancement in computational optimal control.

Abstract: One might argue that solving a trajectory optimization problem over a million
grid points is preposterous. How about solving such a problem at an incredibly
fast computational time? On a small form-factor processor? Algorithmic details
that make possible this trifecta of breakthroughs are presented in this paper.
The computational mathematics that deliver these advancements are: (i) a
Birkhoff-theoretic discretization of optimal control problems, (ii) matrix-free
linear algebra leveraging Krylov-subspace methods, and (iii) a near-perfect
Birkhoff preconditioner that helps achieve $\mathcal{O}(1)$ iteration speed
with respect to the grid size,~$N$. A key enabler of this high performance is
the computation of Birkhoff matrix-vector products at $\mathcal{O}(N\log(N))$
time using fast Fourier transform techniques that eliminate traditional
computational bottlenecks. A numerical demonstration of this unprecedented
scale and speed is illustrated for a practical astrodynamics problem.

</details>


### [27] [The Conjugate Function Method for Surfaces with Elaborate Topological Types](https://arxiv.org/abs/2509.01978)
*H. Hakula,A. Rasila,Y. Zheng*

Main category: math.NA

TL;DR: Generalization and refinement of conjugate function method for accurate conformal mapping computation on simply/multiply connected planar domains and Riemann surfaces using high-order finite elements


<details>
  <summary>Details</summary>
Motivation: To address the challenge of accurate and efficient construction of conjugate problems for multiply connected domains, enabling highly accurate computations of conformal mappings on surfaces with complex geometries and singularities

Method: High-order finite element methods applied to the conjugate function method, generalized for both simply and multiply connected planar domains and Riemann surfaces

Result: Highly accurate computations achieved for domains with complex boundary geometry containing strong singularities and cusps, demonstrated through extensive numerical experiments with error estimates

Conclusion: The refined conjugate function method provides effective numerical computation of conformal mappings with high accuracy on various domain types, including challenging multiply connected cases with complex geometries

Abstract: The conjugate function method is an algorithm for numerical computation of
conformal mappings for simply and multiply connected domains on surfaces. In
this paper the conjugate function method is generalized and refined to achieve
the same level of accuracy on simply and multiply connected planar domains and
Riemann surfaces. The main challenge addressed here is the accurate and
efficient construction of the conjugate problem for multiply connected domains.
The method relies on high-order finite element methods which allow for highly
accurate computations of mappings on surfaces, including domains of complex
boundary geometry containing strong singularities and cusps. The efficacy of
the proposed method is illustrated via an extensive set of numerical
experiments with error estimates.

</details>


### [28] [CLINN: Conservation Law Informed Neural Network for Approximating Discontinuous Solutions](https://arxiv.org/abs/2509.02091)
*Weiheng Zeng,Ruoxi Lu,Tiegang Liu*

Main category: math.NA

TL;DR: CLINN is a novel neural network framework that improves upon PINN by incorporating conservation law constraints and adaptive refinement to better handle discontinuities in conservation laws.


<details>
  <summary>Details</summary>
Motivation: Physics-informed Neural Networks (PINN) struggle with conservation properties and discontinuity resolution when solving conservation laws, requiring a more robust approach.

Method: Proposes Conservation Law-informed Neural Network (CLINN) that integrates boundedness constraint, implicit solution form, and Rankine-Hugoniot condition into loss function, plus residual-based adaptive refinement (RAR) for discontinuity handling.

Result: CLINN achieves superior accuracy in resolving solution profiles and discontinuity locations with reduced oscillations, showing up to 99.2% reduction in mean squared error compared to conventional PINN.

Conclusion: CLINN effectively addresses PINN's limitations in conservation law problems by enforcing exact conservation properties and improving discontinuity resolution through adaptive refinement strategies.

Abstract: Physics-informed Neural Network (PINN) faces significant challenges when
approximating solutions to conservation laws, particularly in ensuring
conservation and accurately resolving discontinuities. To address these
limitations, we propose Conservation Law-informed Neural Network (CLINN), a
novel framework that incorporates the boundedness constraint, implicit solution
form, and Rankine-Hugoniot condition of scalar conservation laws into the loss
function, thereby enforcing exact conservation properties. Furthermore, we
integrate a residual-based adaptive refinement (RAR) strategy to dynamically
prioritize training near discontinuities, substantially improving the network's
ability to capture sharp gradients. Numerical experiments are conducted on
benchmark problems, including the inviscid Burgers equation, the
Lighthill-Whitham-Richards (LWR) traffic flow model, and the Buckley-Leverett
problem. Results demonstrate that CLINN achieves superior accuracy in resolving
solution profiles and discontinuity locations while reducing numeral
oscillations. Compared to conventional PINN, CLINN yields a maximum reduction
of 99.2% in mean squared error (MSE).

</details>


### [29] [High-Order Schemes for Hyperbolic Conservation Laws Using Young Measures](https://arxiv.org/abs/2509.02107)
*Shaoshuai Chu,Michael Herty*

Main category: math.NA

TL;DR: High-order numerical schemes for random hyperbolic conservation laws using linear programming, extending first-order methods to second and fifth-order with improved discontinuity resolution.


<details>
  <summary>Details</summary>
Motivation: To improve upon existing first-order schemes for solving random hyperbolic conservation laws by developing higher-order numerical methods that better resolve discontinuities.

Method: Developed second-order scheme using piecewise linear reconstructions and fifth-order scheme using finite-difference alternative weighted essentially non-oscillatory (A-WENO) framework, both building on linear programming approach for generalized measure-valued solutions.

Result: The high-order extensions significantly improve resolution of discontinuities in numerical experiments on various hyperbolic conservation laws including random Burgers equation, isentropic Euler equations, and deterministic problems.

Conclusion: The proposed high-order schemes successfully extend the linear programming approach to random hyperbolic conservation laws, providing improved accuracy and discontinuity resolution compared to first-order methods.

Abstract: We develop high-order numerical schemes to solve random hyperbolic
conservation laws using linear programming. The proposed schemes are high-order
extensions of the existing first-order scheme introduced in [{\sc S. Chu, M.
Herty, M. Luk\'a\v{c}ov\'a-Medvi{\softd}ov\'a, and Y. Zhou}, solving random
hyperbolic conservation laws using linear programming], where a novel
structure-preserving numerical method using a concept of generalized,
measure-valued solutions to solve random hyperbolic systems of conservation
laws is proposed, yielding a linear partial differential equation concerning
the Young measure and allowing the computation of approximations based on
linear programming problems. The second-order extension is obtained using
piecewise linear reconstructions of the one-sided point values of the unknowns.
The fifth-order scheme is developed using the finite-difference alternative
weighted essentially non-oscillatory (A-WENO) framework. These extensions
significantly improve the resolution of discontinuities, as demonstrated by a
series of numerical experiments on both random (Burgers equation, isentropic
Euler equations) and deterministic (discontinuous flux, pressureless gas
dynamics, Burgers equation with non-atomic support) hyperbolic conservation
laws.

</details>


### [30] [Achieving wavenumber robustness in domain decomposition for heterogeneous Helmholtz equation: an overview of spectral coarse spaces](https://arxiv.org/abs/2509.02131)
*Victorita Dolean,Mark Fry,Matthias Langer,Emile Parolin,Pierre-Henri Tournier*

Main category: math.NA

TL;DR: Analysis of coarse space methods for Helmholtz equation domain decomposition, comparing GenEO and harmonic approaches for high-frequency wave propagation problems.


<details>
  <summary>Details</summary>
Motivation: Solving time-harmonic wave problems in heterogeneous media at high frequencies is computationally challenging, requiring robust domain decomposition methods that maintain performance as wavenumber increases.

Method: Examination and comparison of different coarse space families (GenEO type and harmonic coarse spaces) for two-level domain decomposition methods, using spectral information and multiscale approaches.

Result: Numerical experiments show coarse space effectiveness depends on specific problem and configuration, revealing trade-offs between computational cost, robustness, and practical applicability.

Conclusion: No single coarse space method is universally best; performance varies by problem type, highlighting the need for careful selection based on specific computational requirements and problem characteristics.

Abstract: Solving time-harmonic wave propagation problems in the frequency domain
within heterogeneous media poses significant mathematical and computational
challenges, particularly in the high-frequency regime. Among the available
numerical approaches, domain decomposition methods are widely regarded as
effective due to their suitability for parallel computing and their capacity to
maintain robustness with respect to physical parameters, such as the
wavenumber. These methods can achieve near-constant time-to-solution as the
wavenumber increases, though often at the expense of a computationally
intensive coarse correction step. This work focuses on identifying the best
algorithms and numerical strategies for benchmark problems modelled by the
Helmholtz equation. Specifically, we examine and compare several coarse spaces
which are part of different families, e.g. GenEO (Generalised Eigenvalue
Overlap) type coarse spaces and harmonic coarse spaces, that underpin two-level
domain decomposition methods. By leveraging spectral information and multiscale
approaches, we aim to provide a comprehensive overview of the strengths and
weaknesses of these methods. Numerical experiments demonstrate that the
effectiveness of these coarse spaces depends on the specific problem and
numerical configuration, highlighting the trade-offs between computational
cost, robustness, and practical applicability.

</details>


### [31] [Higher Order Unfitted Space-Time Methods for Transport Problems](https://arxiv.org/abs/2509.02253)
*Erik Burman,Fabian Heimann*

Main category: math.NA

TL;DR: Unfitted Space-Time FEM for scalar transport on moving domains with boundary moving at same velocity as concentration. Uses continuous Galerkin on fixed background mesh and tensor product elements.


<details>
  <summary>Details</summary>
Motivation: To develop an accurate numerical method for scalar transport equations on domains with moving boundaries, where the boundary motion is governed by the same velocity field as the interior transport.

Method: Unfitted Space-Time Finite Element method with continuous Galerkin on fixed background mesh and tensor product elements (discontinuous along time boundaries). Second-order spatially accurate geometry approximation.

Result: Established stability in problem-specific norm and proved high-order a priori error bounds. Numerical examples validate theoretical findings.

Conclusion: The proposed method provides stable and high-order accurate numerical solution for scalar transport problems on moving domains with consistent velocity fields.

Abstract: In this article, we present an Unfitted Space-Time Finite Element method for
the scalar transport equation posed on moving domains. We consider the case of
the domain boundary being transported by the same velocity field as the scalar
concentration inside the physical domain. A standard continuous Galerkin Finite
element space is considered on a fixed background mesh, as well as tensor
product Space-Time elements, which can be discontinuous along time slice
boundaries. For the computational geometry, we opt for a spatially second-order
accurate approximation variant in the mathematical analysis. In particular, we
establish stability in a problem-specific norm and prove a priori error bounds
of high order. Numerical examples illustrate these theoretical findings.

</details>


### [32] [Paving the way to a $\operatorname{T}$-coercive method for the wave equation](https://arxiv.org/abs/2509.02288)
*Daniel Hoonhout,Richard Löscher,Carolina Urzúa-Torres*

Main category: math.NA

TL;DR: Introduces a space-time transformation operator T for T-coercivity in wave equation weak formulations, providing unconditionally stable Galerkin-Bubnov methods with μ-independent error estimates.


<details>
  <summary>Details</summary>
Motivation: To develop stable variational formulations for wave equations that avoid parameter-dependent stability issues and simplify error analysis by keeping trial spaces as standard Sobolev spaces.

Method: Proposes a transformation operator T_μ that establishes T_μ-coercivity for weak formulations, modifying only the test space while maintaining standard trial spaces. Uses ODE model u'' + μu = f linked to wave equations via Fourier expansion.

Result: Achieves unconditionally stable Galerkin-Bubnov formulation with error estimates independent of parameter μ. Numerical examples validate the theoretical results.

Conclusion: The μ-dependent transformation operator successfully establishes T-coercivity, providing stable formulations for wave equations with simplified analysis framework applicable to space-time problems.

Abstract: In this paper, we take a first step toward introducing a space-time
transformation operator $\operatorname{T}$ that establishes
$\operatorname{T}$-coercivity for the weak variational formulation of the wave
equation in space and time on bounded Lipschitz domains. As a model problem, we
study the ordinary differential equation (ODE) $u'' + \mu u = f$ for $\mu>0$,
which is linked to the wave equation via a Fourier expansion in space. For its
weak formulation, we introduce a transformation operator $\operatorname{T}_\mu$
that establishes $\operatorname{T}_\mu$-coercivity of the bilinear form
yielding an unconditionally stable Galerkin-Bubnov formulation with error
estimates independent of $\mu$. The novelty of the current approach is the
explicit dependence of the transformation on $\mu$ which, when extended to the
framework of partial differential equations, yields an operator acting in both
time and space. We pay particular attention to keeping the trial space as a
standard Sobolev space, simplifying the error analysis, while only the test
space is modified. The theoretical results are complemented by numerical
examples.

</details>


### [33] [Correction of weighted and shifted seven-step BDF for parabolic equations with nonsmooth data](https://arxiv.org/abs/2509.02307)
*Minghua Chen,Jiankang Shi,Fan Yu,Zhi Zhou*

Main category: math.NA

TL;DR: The paper proposes correction schemes for weighted and shifted BDF methods to restore kth-order convergence for parabolic equations with nonsmooth data, overcoming order reduction issues.


<details>
  <summary>Details</summary>
Motivation: Standard seven-step BDF is unstable for parabolic equations, and while weighted/shifted BDF methods improve stability, they suffer from severe order reduction with nonsmooth data like discontinuous source terms or rough initial values.

Method: Design proper correction time-stepping schemes for k-step weighted and shifted BDF convolution quadrature (k≤7) to handle parabolic problems with nonsmooth data.

Result: The proposed correction schemes successfully restore the desired kth-order convergence rate even with discontinuous source terms and nonsmooth initial data.

Conclusion: Numerical experiments confirm that the correction methods effectively recover optimal convergence rates for parabolic equations with challenging nonsmooth data conditions.

Abstract: It is well known that the seven-step backward difference formula (BDF) is
unstable for the parabolic equations, since it is not even zero-stable.
However, a linear combination of two non zero-stable schemes, namely the
seven-step BDF and its shifted counterpart, can yield $A(\alpha)$ stable. Based
on this observation, the authors [Akrivis, Chen, and Yu, IMA J. Numer. Anal.,
DOI:10.1093/imanum/drae089] propose the weighted and shifted seven-step BDF
methods for the parabolic equations, which stability regions are larger than
the standard BDF. Nonetheless, this approach is not directly applicable for the
parabolic equations with nonsmooth data, which may suffer from severe order
reduction. This motivates us to design proper correction time-stepping schemes
to restore the desired $k$th-order convergence rate of the $k$-step weighted
and shifted BDF ($k\leq 7$) convolution quadrature for the parabolic problems.
We prove that the desired $k$th-order convergence can be recovered even if the
source term is discontinuous and the initial value is nonsmooth data. Numerical
experiments illustrate the theoretical results.

</details>


### [34] [Multi-stage PDE-based image processing techniques for noisy MRI scans](https://arxiv.org/abs/2509.02342)
*Ksenia Slepova,Ivan Etoku Oiye,Martin B. van Gijzen*

Main category: math.NA

TL;DR: A multi-stage PDE-based approach combining image denoising and segmentation using nonlinear diffusion for noise removal and clustering/region growing for segmentation.


<details>
  <summary>Details</summary>
Motivation: PDE-based methods have shown reliable results for both image denoising and segmentation, but combining them effectively into a unified optimization framework can improve performance.

Method: Three-stage approach: 1) Compute raw image from noisy data, 2) Filter noise using anisotropic diffusion coupled with optimization incorporating diffusion coefficient from presegmented image, 3) Final segmentation using clustering and region growing.

Result: The approach was demonstrated on both ground truth images and MR measurements from an experimental, inexpensive scanner, showing effective performance.

Conclusion: The proposed multi-stage PDE-based framework successfully integrates denoising and segmentation into a unified optimization problem, providing reliable results for image processing applications.

Abstract: Image denoising and image segmentation play essential roles in image
processing. Partial differential equations (PDE)-based methods have proven to
show reliable results when incorporated in both denoising and segmentation of
images. In our work, we discuss a multi-stage PDE-based image processing
approach. It relies upon the nonlinear diffusion for noise removal and
clustering and region growing for segmentation. In the first stage of the
approach, the raw image is computed from noisy measurement data. The second
stage aims to filter out the noise using anisotropic diffusion. We couple these
stages into one optimisation problem which allows us to incorporate a diffusion
coefficient based on a presegmented image. The third stage performs the final
segmentation of the image. We demonstrate our approach on both images for which
the ground truth is known and on MR measurements made by an experimental,
inexpensive scanner.

</details>


### [35] [A Convolutional Hierarchical Deep-learning Neural Network (C-HiDeNN) Framework for Non-linear Finite Element/Meshfree Analysis](https://arxiv.org/abs/2509.02435)
*Yingjian Liu,Monish Yadav Pabbala,Jiachen Guo,Chanwook Park,Gino Domel,Wing Kam Liu,Dong Qian*

Main category: math.NA

TL;DR: C-HiDeNN is a convolutional hierarchical deep neural network framework that enhances nonlinear finite element and meshfree analysis through convolution operators and expanded optimization parameters, achieving higher accuracy than standard FEM with reduced computational costs.


<details>
  <summary>Details</summary>
Motivation: To improve upon existing HiDeNN framework by introducing convolution operations and additional optimization parameters to enhance approximation capabilities for nonlinear finite element and meshfree analysis, addressing the need for higher accuracy with lower computational costs.

Method: Extends HiDeNN with convolution operators and introduces additional optimization parameters including polynomial order 'p', dilation parameter 'a', patch size 's', and nodal position 'X'. These parameters serve as weights and biases within the C-HiDeNN patch structure, enabling adaptive refinement in regions requiring high resolution.

Result: The framework demonstrates significantly higher accuracy compared to standard Finite Element Method (FEM) while substantially reducing computational costs in numerical examples of nonlinear finite element and meshfree analysis.

Conclusion: C-HiDeNN provides an effective convolutional hierarchical deep learning framework that successfully enhances nonlinear computational mechanics analysis through improved approximation capabilities and adaptive optimization parameters, offering superior accuracy with reduced computational burden.

Abstract: We present a framework for the Convolutional Hierarchical Deep Neural Network
(C-HiDeNN) tailored for nonlinear finite element and meshfree analysis.
Building upon the structured foundation of HiDeNN, which includes the
evaluation of shape function derivatives, adaptivity, and material derivatives,
C-HiDeNN introduces a convolution operator to enhance the HiDeNN approximation.
A distinctive feature of C-HiDeNN is its expanded set of optimization
parameters, such as the polynomial order 'p,' dilation parameter 'a,' patch
size 's,' and nodal position 'X'. These parameters function as the weights and
biases within the C-HiDeNN patch. In addition, C-HiDeNN can be prescribed in
regions where high resolution is desired to adaptively improve prediction
accuracy. To demonstrate the effectiveness of this framework, we provide
numerical examples in the context of nonlinear finite element and meshfree
analysis. The results show that our approach achieves significantly higher
accuracy compared to the standard Finite Element Method (FEM) while
substantially reducing computational costs.

</details>


### [36] [Fractional differential equations: non-constant coefficients, simulation and model reduction](https://arxiv.org/abs/2509.02465)
*Ruben Aylwin,Göksu Oruc,Karsten Urban*

Main category: math.NA

TL;DR: Analysis of boundary value problems with Riemann-Liouville fractional derivatives (order 1-2) using variational formulation, finite element discretization, and reduced basis method with greedy algorithm for parametric coefficients.


<details>
  <summary>Details</summary>
Motivation: To address boundary value problems involving fractional derivatives with non-constant diffusion and reaction coefficients, developing efficient computational methods for parametric analysis.

Method: Derived variational formulation, analyzed well-posedness, implemented finite element discretization, and applied reduced basis method with greedy algorithm for parametric coefficients.

Result: Established well-posedness of continuous problem and discretization, demonstrated convergence properties dependent on fractional order s, and presented numerical validation.

Conclusion: The reduced basis method with greedy algorithm effectively handles parametric fractional derivative problems, with convergence behavior directly influenced by the fractional order parameter s.

Abstract: We consider boundary value problems with Riemann-Liouville fractional
derivatives of order $s\in (1, 2)$ with non-constant diffusion and reaction
coefficients. A variational formulation is derived and analyzed leading to the
well-posedness of the continuous problem and its Finite Element discretization.
Then, the Reduced Basis Method through a greedy algorithm for parametric
diffusion and reaction coefficients is analyzed. Its convergence properties,
and in particular the decay of the Kolmogorov $n$-width, are seen to depend on
the fractional order $s$. Finally, numerical results confirming our findings
are presented.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [37] [Boundary Value Problems for the Magnetic Laplacian in Semiclassical Analysis](https://arxiv.org/abs/2509.00292)
*Zhongwei Shen*

Main category: math.AP

TL;DR: Magnetic Laplacian analysis in semiclassical setting with uniform nontangential maximal function estimates for Neumann/Dirichlet problems in Lipschitz domains under finite type magnetic field assumption.


<details>
  <summary>Details</summary>
Motivation: Extend Jerison and Kenig's classical result for Laplacian in Lipschitz domains to magnetic Laplacian in semiclassical setting, providing new insights even for smooth domains.

Method: Study L^2 Neumann and Dirichlet problems for magnetic Laplacian P^h(A) = (hD + A)^2 in bounded Lipschitz domains, assuming magnetic field ∇×A is of finite type on closure.

Result: Established uniform nontangential maximal function estimates for (hD + A)u for 0 < h < h_0, extending classical Laplacian results to magnetic case.

Conclusion: Successfully generalized Jerison-Kenig's Lipschitz domain results to semiclassical magnetic Laplacian, providing new uniform estimates that hold even for smooth domains.

Abstract: This paper is concerned with the magnetic Laplacian $P^h (\A)=(h D+\A)^2$ in
semiclassical analysis, where $h$ is a semiclassical parameter. We study the
$L^2$ Neumann and Dirichlet problems for the equation $P^h(\A)u=0$ in a bounded
Lipschitz domain $\Omega$. Under the assumption that the magnetic field $\nabla
\times \A$ is of finite type on $\overline{\Omega}$, we establish the
nontangential maximal function estimates for $(h D+\A)u$, which are uniform for
$0< h< h_0$. This extends a well-known result due to D. Jerison and C. Kenig
for the Laplacian in Lipschitz domains to the magnetic Laplacian in the
semiclassical setting. Our results are new even for smooth domains.

</details>


### [38] [Non-symmetric solutions to an overdetermined problem for the Helmholtz equation in the plane](https://arxiv.org/abs/2509.00455)
*Miles H. Wheeler*

Main category: math.AP

TL;DR: Construction of smooth bounded domains in R² (other than disks) where the overdetermined elliptic problem with Dirichlet and Neumann boundary conditions has non-trivial solutions, providing counterexamples to a conjecture by Willms and Gladwell.


<details>
  <summary>Details</summary>
Motivation: To challenge and disprove the conjecture by Willms and Gladwell [WG94] that only disks can support non-trivial solutions to the overdetermined boundary value problem with both Dirichlet and Neumann conditions specified.

Method: Construct explicit examples of smooth bounded domains in the plane (R²) that are not disks, and demonstrate that these domains admit solutions to the overdetermined problem with non-zero constants λ, b, c.

Result: Successful construction of counterexample domains showing that non-disk domains can indeed support solutions to the overdetermined boundary value problem, contradicting the previous conjecture.

Conclusion: The conjecture that only disks can have solutions to this overdetermined problem is false, as demonstrated by the constructed counterexamples of smooth bounded domains in R².

Abstract: In this note we construct smooth bounded domains $\Omega \subset \mathbb
R^2$, other than disks, for which the overdetermined problem $$
  \left\{
  \begin{alignedat}{2}
  \Delta u + \lambda u &= 0 &\qquad& \text{ in } \Omega, \newline
  u &= b &\qquad& \text{ on } \partial \Omega, \newline
  \frac{\partial u}{\partial n} &= c &\qquad& \text{ on } \partial \Omega
  \end{alignedat}
  \right. $$ has a solution for some constants $\lambda,b,c \ne 0$. These
appear to be the first counterexamples to a conjecture of Willms and Gladwell
[WG94].

</details>


### [39] [$p$-biharmonic Kirchhoff equations with critical Choquard nonlinearity](https://arxiv.org/abs/2509.00470)
*Divya Goel,Sarika Goyal,Diksha Saini*

Main category: math.AP

TL;DR: Study of p-biharmonic critical Choquard-Kirchhoff equation with Hardy-Littlewood-Sobolev critical exponent. Proves concentration compactness principle and establishes existence/multiplicity of solutions via variational methods.


<details>
  <summary>Details</summary>
Motivation: To address the p-biharmonic Choquard-Kirchhoff equation with critical exponent, which combines nonlocal Choquard terms with Kirchhoff-type nonlocal coefficients, presenting mathematical challenges in analysis.

Method: First proves concentration compactness principle for p-biharmonic Choquard-type equations, then uses variational methods combined with concentration-compactness techniques to analyze existence and multiplicity of solutions.

Result: Establishes existence and multiplicity of solutions with respect to parameters λ and α for different values of r. Results are novel even for the p-Laplacian case.

Conclusion: The paper successfully develops new mathematical tools (concentration compactness principle) and obtains significant results for critical p-biharmonic Choquard-Kirchhoff equations, extending previous work on related problems.

Abstract: In this article, we deal with the following involving $p$-biharmonic critical
Choquard-Kirchhoff equation
  $$
  \left(a+b\left(\int_{\mathbb R^N}|\Delta u|^p dx\right)^{\theta-1}\right)
\Delta_{p}^{2}u = \alpha \left(|x|^{-\mu}*u^{p^*_\mu}\right)|u|^{p^*_\mu-2}u+
\lambda f(x) |u|^{r-2} u \; \text{in}\; \mathbb R^N,
  $$
  where $a\geq 0$, $b> 0$, $0<\mu<N$, $N>2p$, $p\geq 2$, $\theta\geq1$,
$\alpha$ and $\lambda$ are positive real parameters, $p_{\mu}^{*}=
\frac{p(2N-\mu)}{2(N-2p)}$ is the upper critical exponent in the sense of
Hardy-Littlewood-Sobolev inequality. The function $f \in L^{t}(\mathbb R^N)$
with $t= \frac{p^{*}}{(p^* -r)}$ if $p<r<p^*:=\frac{Np}{N-2p}$ and $t=\infty$
if $r\geq p^{*}$. We first prove the concentration compactness principle for
the $p$-biharmonic Choquard-type equation. Then using the variational method
together with the concentration-compactness, we established the existence and
multiplicity of solutions to the above problem with respect to parameters
$\lambda$ and \(\alpha\) for different values of $r$. The results obtained here
are new even for $p-$Laplacian.

</details>


### [40] [Caccioppoli-type inequalities for the Dunkl-$A$-Laplacian and their application to nonexistence result](https://arxiv.org/abs/2509.00565)
*Athulya P,Sandeep Kumar Verma*

Main category: math.AP

TL;DR: Introduces A-Laplacian in Dunkl framework, derives Caccioppoli inequalities for Dunkl-Orlicz-Sobolev spaces, and establishes nonexistence conditions for solutions to Dunkl-differential inequalities.


<details>
  <summary>Details</summary>
Motivation: To extend the theory of A-Laplacian operators to the Dunkl framework, which generalizes classical differential operators by incorporating reflection groups and multiplicity functions, enabling analysis in non-Euclidean settings.

Method: Defines A-Laplacian operator Δ_{k,A}(u) = div_k(A(∇_k u)) using Dunkl-gradient operator. Derives local and global Caccioppoli-type inequalities for Dunkl-Orlicz-Sobolev spaces satisfying Dunkl-differential inequalities.

Result: Obtains Caccioppoli inequalities that provide control over solutions. Establishes sufficient conditions ensuring nonexistence of nonzero solutions to the Dunkl-differential inequality -Δ_{k,A}(u) ≥ bΦ(u)χ_{u>0}.

Conclusion: The developed framework and inequalities provide powerful tools for analyzing Dunkl-type differential operators and establishing nonexistence results for solutions to associated differential inequalities in Dunkl-Orlicz-Sobolev spaces.

Abstract: For a suitable function $A:\mathbb{R}^n\to \mathbb{R}^n$, we introduce the
$A$-Laplacian in the Dunkl framework as $\Delta_{k,A}(u)
=\text{div}_k(A(\nabla_ku))$, where $\nabla_k$ is the Dunkl-gradient operator
associated with the multiplicity function $k$ and the root system
$\mathcal{R}$. We derive the local and global Caccioppoli-type inequality for
an element $u$ in the Dunkl-Orlicz-Sobolev space, satisfying the
Dunkl-differential inequality $$ -\Delta_{k, A}(u) \geq b\Phi(u)\chi_{\{u>0\}}.
$$ Using the Caccioppoli inequality, we establish a sufficient condition for
the nonexistence of a nonzero solution $u$ to the Dunkl-differential
inequality.

</details>


### [41] [$C^{\infty}$ Regularity for the free boundary of one-phase Fractional Laplacian problem](https://arxiv.org/abs/2509.00609)
*Runcao Lyu*

Main category: math.AP

TL;DR: The paper extends regularity results for free boundary problems with fractional Laplacian from C¹,α to C∞ smoothness.


<details>
  <summary>Details</summary>
Motivation: Previous work by De Silva, Savin, and Sire established that flat boundaries are C¹,α regular for fractional Laplacian free boundary problems. The authors aim to improve this regularity to C∞ smoothness, generalizing the known result for the half-Laplacian case.

Method: The authors work with a one-phase free boundary problem involving fractional Laplacian (-Δ)^s for 0<s<1. They build upon existing results and techniques to achieve higher regularity.

Result: Successfully proved that flat boundaries in the fractional Laplacian free boundary problem are C∞ smooth, extending the previous C¹,α regularity result.

Conclusion: The research demonstrates that optimal regularity for free boundary problems with fractional Laplacian operators is C∞, significantly improving upon prior results and completing the regularity theory for these problems.

Abstract: We consider a one-phase free boundary problem involving fractional Laplacian
$(-\Delta)^s$, $0<s<1$. D. De Silva, O. Savin, and Y. Sire proved that the flat
boundaries are $C^{1,\alpha}$. We raise the regularity to $C^{\infty}$,
extending the result known for $(-\Delta)^{1/2}$ by D. De Silva and O. Savin.

</details>


### [42] [Improved stability threshold for 2D Navier-Stokes Couette flow in a infinite channel](https://arxiv.org/abs/2509.00694)
*Tao Liang,Jiahong Wu,Xiaoping Zhai*

Main category: math.AP

TL;DR: Nonlinear stability of 2D Navier-Stokes around Couette flow with Navier slip boundary conditions, establishing sharp stability threshold of ν¹ᐟ² without logarithmic loss.


<details>
  <summary>Details</summary>
Motivation: To improve the understanding of hydrodynamic stability by establishing the precise critical perturbation size for nonlinear stability of Couette shear flow, removing the logarithmic loss from previous work.

Method: Analysis of 2D Navier-Stokes equations in channel domain with Navier slip boundary conditions, studying perturbations of initial vorticity measured in anisotropic Sobolev spaces.

Result: Proved quantitative stability threshold of order ν¹ᐟ² for perturbations, sharpening previous result of ν¹ᐟ²(1+ln(1/ν))⁻¹ᐟ² by removing the logarithmic factor.

Conclusion: The natural scaling ν¹ᐟ² is identified as the critical size of perturbations for nonlinear stability in this setting, providing the optimal stability threshold.

Abstract: We study the nonlinear stability of the two-dimensional Navier-Stokes
equations around the Couette shear flow in the channel domain
$\mathbb{R}\times[-1,1]$ subject to Navier slip boundary conditions. We
establish a quantitative stability threshold for perturbations of the initial
vorticity $\omega_{in}$, showing that stability holds for perturbations of
order $\nu^{1/2}$ measured in an anisotropic Sobolev space. This sharpens the
recent work of Arbon and Bedrossian [Comm. Math. Phys., 406 (2025), Paper No.
129] who proved stability under the threshold $\nu^{1/2}(1+\ln(1/\nu))^{-1/2}$.
Our result removes the logarithmic loss and identifies the natural scaling
$\nu^{1/2}$ as the critical size of perturbations for nonlinear stability in
this setting.

</details>


### [43] [$Γ$-convergence and stochastic homogenization for functionals in the $\mathcal{A}$-free setting](https://arxiv.org/abs/2509.00726)
*Gianni Dal Maso,Rita Ferreira,Irene Fonseca*

Main category: math.AP

TL;DR: Compactness result for Γ-convergence of integral functionals on A-free vector fields, enabling homogenization without periodicity assumptions by taking limits of minimization problems on large cubes.


<details>
  <summary>Details</summary>
Motivation: To study homogenization problems for integral functionals defined on A-free vector fields without requiring periodicity assumptions, which extends classical homogenization theory to more general settings.

Method: Prove compactness for Γ-convergence, then obtain homogenized integrand by taking limits of minimum values from minimization problems on large cubes as side length tends to infinity, assuming limit values are independent of cube center.

Result: Established that homogenized integrand can be derived through limit procedures on large cubes, and under stochastic periodicity assumptions, solved stochastic homogenization problem using subadditive ergodic theorem.

Conclusion: The approach provides a framework for homogenization of A-free vector field functionals without periodicity, with applications to stochastic homogenization through ergodic methods.

Abstract: We obtain a compactness result for $\Gamma$-convergence of integral
functionals defined on $\mathcal{A}$-free vector fields. This is used to study
homogenization problems for these functionals without periodicity assumptions.
More precisely, we prove that the homogenized integrand can be obtained by
taking limits of minimum values of suitable minimization problems on large
cubes, when the side length of these cubes tends to $+\infty$, assuming that
these limit values do not depend on the center of the cube. Under the usual
stochastic periodicity assumptions, this result is then used to solve the
stochastic homogenization problem by means of the subadditive ergodic theorem.

</details>


### [44] [Orbital Stability of First Laplacian Eigenstates for the Euler Equation on Flat 2-Tori](https://arxiv.org/abs/2509.00750)
*Guodong Wang*

Main category: math.AP

TL;DR: Extension of orbital stability results for first Laplacian eigenstates from rectangular/square tori to arbitrary flat tori, including hexagonal tori, under Euler dynamics.


<details>
  <summary>Details</summary>
Motivation: Previous work showed orbital stability of first eigenstates on rectangular/square tori, but the stability on arbitrary flat tori (particularly hexagonal) remained an open problem that needed generalization.

Method: Using Burton's stability criterion framework with two key steps: (1) variational characterization for equimeasurable classes in first eigenspace, (2) analysis of translational orbits within each equimeasurable class, involving complex polynomial equations for hexagonal torus symmetry.

Result: Successfully proved orbital stability for first eigenstates on arbitrary flat tori, obtaining the first family of orbitally stable sinusoidal Euler flows on hexagonal tori.

Conclusion: The framework extends stability results to general flat tori, with the hexagonal case presenting particularly challenging mathematical problems involving symmetry analysis and polynomial systems.

Abstract: On a two-dimensional flat torus, the Laplacian eigenfunctions can be
expressed explicitly in terms of sinusoidal functions. For a rectangular or
square torus, it is known that every first eigenstate is orbitally stable up to
translation under the Euler dynamics. In this paper, we extend this result to
flat tori of arbitrary shape. As a consequence, we obtain for the first time a
family of orbitally stable sinusoidal Euler flows on a hexagonal torus. The
proof is carried out within the framework of Burton's stability criterion and
consists of two key ingredients: (i) establishing a suitable variational
characterization for each equimeasurable class in the first eigenspace, and
(ii) analyzing the number of translational orbits within each equimeasurable
class. The second ingredient, particularly for the case of a hexagonal torus,
is very challenging, as it requires analyzing a sophisticated system of
polynomial equations related to the symmetry of the torus and the structure of
the first eigenspace.

</details>


### [45] [Sharp gradient stability for a class of Hardy-Sobolev-Maz'ya inequalities](https://arxiv.org/abs/2509.00814)
*Wei Dai,Jingze Fu,An Zhang*

Main category: math.AP

TL;DR: Stability analysis for Hardy-Sobolev-Maz'ya inequalities with non-radial extremal functions, proving non-degeneracy and sharp global quantitative stability.


<details>
  <summary>Details</summary>
Motivation: To address the stability problem for Hardy-Sobolev-Maz'ya inequalities that have non-radial extremal functions, particularly dealing with challenges posed by partial (stronger) singular weights.

Method: Refined techniques from previous works and introduced new ideas to handle cylindrical symmetry of non-radial extremal functions and partial singular weight structure. Key innovations include new compact embedding with strong singularity and refined spectral inequalities.

Result: Proved that Euler-Lagrange equations are non-degenerate and obtained sharp global quantitative stability for the inequalities.

Conclusion: The paper successfully addresses substantial challenges from partial singular weights and non-radial extremal functions, providing important stability results with novel technical approaches.

Abstract: This paper investigates the stability problem for a class of
Hardy-Sobolev-Maz'ya inequalities with non-radial extremal functions. We prove
that the Euler-Lagrange equations are non-degenerate and obtain a sharp global
quantitative stability. The presence of partial (stronger) singular weight
brings substantial new challenges, requiring us to significantly refine the
techniques from \cite{DT1,FN,FZ} and introduce some new ideas to handle both
the cylindrical symmetry of non-radial extremal functions and the partial
(stronger) singular weight structure. Key technical innovations include new
compact embedding with strong singularity and new refined spectral inequalities
that are crucial for our analysis.

</details>


### [46] [Superlinear problems involving nonlinear superposition operators of mixed fractional order](https://arxiv.org/abs/2509.00817)
*Souvik Bhowmick,Sekhar Ghosh,Vishvesh Kumar*

Main category: math.AP

TL;DR: Study of elliptic problems with nonlinear superpositions of fractional operators and superlinear nonlinearities, establishing existence of infinitely many weak solutions using variational methods.


<details>
  <summary>Details</summary>
Motivation: To develop a unified variational framework for analyzing elliptic problems involving complex superpositions of fractional operators with different orders and signs, which haven't been systematically studied before.

Method: Constructed a suitable variational setting and applied the Fountain Theorem to establish the existence of infinitely many weak solutions. The approach covers various superlinear growth assumptions including the classical Ambrosetti-Rabinowitz condition.

Result: Proved the existence of infinitely many weak solutions for this broad class of operators. The results are novel even for special cases like superpositions of fractional p-Laplacians or combinations with p-Laplacians.

Conclusion: Provides a unified variational framework that systematically accommodates a broad class of operators including finite sums of fractional p-Laplacians with different orders and operators with fractional Laplacians having 'wrong' signs.

Abstract: In this work, we study a class of elliptic problems involving nonlinear
superpositions of fractional operators of the form \[ A_{\mu,p}u :=
\int_{[0,1]} (-\Delta)_{p}^{s} u \, d\mu(s), \] where $\mu$ is a signed measure
on $[0,1]$, coupled with nonlinearities of superlinear type. Our analysis
covers a variety of superlinear growth assumptions, beginning with the
classical Ambrosetti--Rabinowitz condition. Within this framework, we construct
a suitable variational setting and apply the Fountain Theorem to establish the
existence of infinitely many weak solutions. The results obtained are novel
even in the special cases of superpositions of fractional $p$-Laplacians, or
combinations of the fractional $p$-Laplacian with the $p$-Laplacian. More
generally, our approach applies to finite sums of fractional $p$-Laplacians
with different orders, as well as to operators in which fractional Laplacians
appear with ``wrong'' signs. A distinctive contribution of the paper lies in
providing a unified variational framework that systematically accommodates this
broad class of operators.

</details>


### [47] [Non-minimizing Axially Symmetric Cavity Flow](https://arxiv.org/abs/2509.00927)
*Masoud Bayrami,Morteza Fotouhi,Parisa Vosooqnejad*

Main category: math.AP

TL;DR: Analysis of critical points in axially symmetric cavity flow problems with free boundaries, extending to jet flow problems.


<details>
  <summary>Details</summary>
Motivation: While cavity flow problems have been extensively studied for minimizers and stable solutions, there is limited understanding of critical points of the energy functional in axially symmetric cases with free boundaries.

Method: The paper investigates properties of critical points in axially symmetric cavity flow problems with free boundaries, relating them to known variational solutions.

Result: The approach provides insights into critical points beyond minimizers and stable solutions, and naturally extends to jet flow problems.

Conclusion: This work fills a gap in understanding critical points in cavity flow problems and demonstrates applicability to related jet flow scenarios.

Abstract: Cavity flow problems in two dimensions, as well as in the axially symmetric
three-dimensional case, have been extensively studied in the literature from a
qualitative perspective. While numerous results exist concerning minimizers or
stable solutions-particularly regarding the regularity of the free boundary and
the analysis of singularities-much less is known about the critical points of
the corresponding energy functional. In this paper, we focus on investigating
the properties of such critical points in the axially symmetric cavity flow
problem with a free boundary, in relation to the known variational solutions.
Moreover, our approach extends naturally to the case of jet flow problems.

</details>


### [48] [On the regularity of continuous solutions to multidimensional scalar conservation laws with bounded source](https://arxiv.org/abs/2509.00977)
*Fabio Ancona,Laura Caravenna,Alexander J. Cliffe,Elio Marconi*

Main category: math.AP

TL;DR: Hölder regularity of continuous isentropic solutions for multi-dimensional scalar balance laws with bounded source terms and general nonlinear flux assumptions


<details>
  <summary>Details</summary>
Motivation: To establish regularity properties for solutions to multi-dimensional scalar balance laws, which is important for understanding the behavior and stability of such solutions in various physical applications

Method: Exploiting the kinetic formulation of the balance law to prove regularity results

Result: Proved Hölder regularity for continuous isentropic solutions under bounded source terms and general nonlinear flux assumptions

Conclusion: The kinetic formulation provides an effective framework for establishing regularity properties in multi-dimensional scalar balance laws with general nonlinear flux conditions

Abstract: We prove the H\"older regularity of continuous isentropic solutions to
multi-dimensional scalar balance laws when the source term is bounded and the
flux satisfies general assumptions of nonlinearity. The results are achieved by
exploiting the kinetic formulation of the balance law.

</details>


### [49] [The Euler equations with variable coefficients](https://arxiv.org/abs/2509.01067)
*Benjamin Ingimarson,Igor Kukavica,Amjad Tuffaha*

Main category: math.AP

TL;DR: Local existence for Euler equations with variable coefficients in bounded domains, optimal regularity r>2.5, plus Beale-Kato-Majda criterion for r=3 relating blow-up to BMO vorticity norm.


<details>
  <summary>Details</summary>
Motivation: To establish existence theory for Euler equations with space-time dependent variable coefficients on bounded domains, extending classical results to more general settings with optimal regularity conditions.

Method: Mathematical analysis of Euler equations with variable coefficients, using techniques from partial differential equations and functional analysis to prove local-in-time existence under optimal regularity conditions.

Result: Proved local existence for initial data v0 ∈ H^r with optimal condition r > 2.5. For r = 3, established a Beale-Kato-Majda type criterion linking H^r norm blow-up to the BMO norm of variable vorticity ζ.

Conclusion: The paper provides optimal regularity conditions for local existence of Euler equations with variable coefficients and extends the classical Beale-Kato-Majda criterion to this more general setting, offering important theoretical foundations for studying fluid dynamics in variable coefficient environments.

Abstract: We establish local-in-time existence for the Euler equations on a bounded
domain with space-time dependent variable coefficients, given initial data $v_0
\in H^r$ under the optimal regularity condition $r > 2.5$. In the case $r = 3$,
we further prove a Beale-Kato-Majda criterion that relates blow-up in the $H^r$
norm to the BMO norm of the variable vorticity $\zeta$.

</details>


### [50] [A generalization of Savin's small perturbation theorem for fully nonlinear elliptic equations and applications](https://arxiv.org/abs/2509.01138)
*Zhenyu Fan*

Main category: math.AP

TL;DR: Generalization of Savin's small perturbation theorem to nonhomogeneous fully nonlinear equations with Hölder small perturbations, with application to partial regularity for sigma-k Hessian equations.


<details>
  <summary>Details</summary>
Motivation: To extend Savin's perturbation results beyond homogeneous equations to more general nonhomogeneous fully nonlinear equations, enabling broader applications in PDE theory.

Method: Generalize Savin's small perturbation theorem to handle nonhomogeneous fully nonlinear equations F(D²u, Du, u, x)=f where coefficients and right-hand side terms are Hölder small perturbations.

Result: Successfully extended the perturbation theorem and established a partial regularity result specifically for the sigma-k Hessian equation σ_k(D²u)=f.

Conclusion: The generalization provides a framework for analyzing regularity properties of nonhomogeneous fully nonlinear equations and yields concrete results for sigma-k Hessian equations, expanding the scope of perturbation methods in PDE analysis.

Abstract: In this note, we generalize Savin's small perturbation theorem to
nonhomogeneous fully nonlinear equations $F(D^2u, Du, u,x)=f$ provided the
coefficients and the right-hand side terms are H\"older small perturbations. As
an application, we establish a partial regularity result for the sigma-$k$
Hessian equation $\sigma_{k}(D^2u)=f$.

</details>


### [51] [On finite-energy solutions of Kazan-Warner equations on the lattice graph](https://arxiv.org/abs/2509.01155)
*Huyuan Chen,Bobo hua*

Main category: math.AP

TL;DR: Analysis of finite-energy solutions to Kazdan-Warner equations on 2D lattice graphs, with existence results for positive parameter case and layer structure characterization for negative parameter case.


<details>
  <summary>Details</summary>
Motivation: To address the open problem of finite-energy solutions to Liouville equations on discrete lattice structures and understand the solution behavior for different parameter regimes.

Method: Mathematical analysis of the discrete Kazdan-Warner equation on 2D integer lattice graphs, employing techniques from discrete analysis and potential theory to study solution existence and structure.

Result: For ε=1, proved existence of continuous family of finite-energy solutions for certain κ values. For ε=-1 with β>4π/κ, demonstrated layer structure of solution set and derived extremal solution.

Conclusion: The study provides partial resolution to the Liouville equation existence problem and reveals structural properties of solutions in different parameter regimes on discrete lattice graphs.

Abstract: We investigate finite-energy solutions to Kazdan-Warner type equations in
2-dimensional integer lattice graph $$ - \Delta u= \varepsilon e^{\kappa u}
+\beta\delta_0\quad {\rm in}\ \mathbb{Z}^2,$$ where $\varepsilon=\pm1$,
$\kappa>0$ and $\beta\in\mathbb{R}$.
  When $\varepsilon=1$, we prove the existence of a continuous family of
finite-energy solutions for some parameter $\kappa$. This provides a partial
resolution of the open problem on the existence of finite-energy solutions to
the Liouville equation.
  When $\varepsilon=-1$ and $\beta>\frac{4\pi}{\kappa}$, we prove that the set
of finite-energy solutions exhibits a layer structure. Moreover, we derive the
extremal solution in this case.

</details>


### [52] [Regularity and dynamics of weak solutions for one-dimensional compressible Navier-Stokes equations with vacuum](https://arxiv.org/abs/2509.01196)
*Jin Tan,Yan-Lin Wang,Lan Zhang*

Main category: math.AP

TL;DR: Global existence and regularity of finite-energy weak solutions for 1D compressible Navier-Stokes equations with vacuum and general initial data, including density patches and vacuum bubbles.


<details>
  <summary>Details</summary>
Motivation: Extend D. Hoff's weak solution theory for compressible Navier-Stokes equations with bounded density to handle vacuum states and general initial conditions in one dimension.

Method: Establish global a priori time estimates for 1D CNS with H^1 initial velocity and bounded initial density (including vacuum states), analyze quantitative dynamics of vacuum states.

Result: Proved global existence and regularity of finite-energy weak solutions, showed exponential decay of velocity and density to equilibrium, obtained quantitative dynamics of density patches and vacuum bubbles.

Conclusion: Successfully developed weak solution theory for 1D compressible Navier-Stokes equations that handles vacuum states and provides complete quantitative description of solution behavior including exponential convergence to equilibrium.

Abstract: In the spirit of D. Hoff's weak solution theory for the compressible
Navier-Stokes equations (CNS) with bounded density, in this paper we establish
the global existence and regularity properties of finite-energy weak solutions
to an initial boundary value problem of one-dimensional CNS with general
initial data and vacuum. The core of our proof is a global in time a priori
estimate for one-dimensional CNS that holds for any $H^1$ initial velocity and
bounded initial density not necessarily strictly positive: it could be a
density patch or a vacuum bubble. We also establish that the velocity and
density decay exponentially to equilibrium. As a by-product, we obtain the
quantitative dynamics of aforementioned two vacuum states.

</details>


### [53] [Global Existence, Hamiltonian Conservation and Vanishing Viscosity for the Surface Quasi-Geostrophic Equation](https://arxiv.org/abs/2509.01268)
*Luigi De Rosa,Mickaël Latocca,Jaemin Park*

Main category: math.AP

TL;DR: Global weak solutions for surface quasi-geostrophic equation with constant Hamiltonian, obtained via vanishing viscosity limit and non-concentration propagation


<details>
  <summary>Details</summary>
Motivation: To prove existence of global weak solutions for the surface quasi-geostrophic equation with initial data in L^{4/3} space, maintaining constant Hamiltonian over time

Method: Vanishing viscosity limit approach, propagating non-concentration of L^{4/3} norm from initial data to deduce strong compactness in Hamiltonian norm

Result: Existence of global-in-time weak solutions θ ∈ L^∞_t L^{4/3}_x with constant Hamiltonian (H^{-1/2} norm), general no anomalous dissipation results under minimal Onsager supercritical assumptions

Conclusion: Successful construction of global weak solutions through careful handling of non-concentration properties, extending results beyond classical strong compactness settings

Abstract: For any initial datum $\theta_0\in L^{\frac{4}{3}}_x$ it is proved the
existence of a global-in-time weak solution $\theta \in L^\infty_t
L^{\frac43}_x$ to the surface quasi-geostrophic equation whose Hamiltonian,
i.e. the $\dot{H}^{-\frac{1}{2}}_x$ norm, is constant in time. The solution is
obtained as a vanishing viscosity limit. Outside the classical strong
compactness setting, the main idea is to propagate in time the
non-concentration of the $L^{\frac{4}{3}}_x$ norm of the initial data, from
which strong compactness in the Hamiltonian norm is deduced. General no
anomalous dissipation results under minimal Onsager supercritical assumptions
are also obtained.

</details>


### [54] [The continuous version of the generalized exchange-driven growth model](https://arxiv.org/abs/2509.01316)
*Prasanta K. Barik,Fernando P. da Costa,João T. Pinto,Rafael Sasportes*

Main category: math.AP

TL;DR: Existence and uniqueness of weak solutions for continuous generalized exchange-driven growth model with mass conservation properties.


<details>
  <summary>Details</summary>
Motivation: To extend the discrete generalized exchange-driven growth model to a continuous framework and establish mathematical foundations for this coagulation variant where smaller particles detach from larger ones and merge with other particles.

Method: Examined existence of weak solutions under suitable reaction rates, established uniqueness under additional rate conditions, and proved mass conservation properties for coagulation rates with linear bounds.

Result: Proved existence of weak solutions, established uniqueness under specific conditions, and demonstrated that solutions conserve both mass and total number of particles for linearly bounded coagulation rates.

Conclusion: The continuous version of the generalized exchange-driven growth model has well-defined mathematical properties including existence, uniqueness under certain conditions, and conservation laws, providing a solid foundation for further study of this coagulation variant.

Abstract: In this article, we discuss the continuous version of the generalized
exchange-driven growth model which is a variant of the coagulation model in
which a smaller size particle is detached from a bigger one and merges with
another particle. This new model is a continuous extension of the generalized
exchange-driven growth model originally formulated in a discrete context [4].
In this work, we examine the existence of weak solutions to the continuous
version of the generalized exchange-driven growth model under a suitable
reaction rate. Under an additional condition on the reaction rates, a
uniqueness result is established. Finally, we prove that solutions satisfy the
mass-conserving property and the conservation of the total number of particles
for coagulation rates with linear bounds.

</details>


### [55] [Regularizing effect of the natural growth term in quasilinear problems with sign-changing nonlinearities](https://arxiv.org/abs/2509.01355)
*José Carmona Tapia,Paolo Malanchini,Antonio J. Martínez Aparicio,Pedro J. Martínez-Aparicio*

Main category: math.AP

TL;DR: Study of existence/nonexistence of solutions to a Dirichlet problem with p-Laplacian and gradient-dependent nonlinearity, focusing on area conditions and regularization effects.


<details>
  <summary>Details</summary>
Motivation: To understand how gradient terms affect the solvability of nonlinear elliptic PDEs with p-Laplacian operators, particularly examining the interplay between the nonlinearity f and gradient coefficient g.

Method: Mathematical analysis of the Dirichlet problem using area conditions involving f and g, studying sufficient and necessary conditions for existence of nonnegative solutions with specific maximum bounds.

Result: An area condition is both sufficient and necessary for solution existence when f(0)≥0. The gradient term g has a regularizing effect - more negative g strengthens regularization. For any fixed λ, there exists g such that a nonnegative solution exists with maximum in (α,β].

Conclusion: The gradient term plays a crucial regularizing role in the p-Laplacian Dirichlet problem, with negative g values enhancing solution existence regardless of f's shape, and area conditions provide complete characterization of solvability.

Abstract: We investigate the existence and nonexistence of solutions to the Dirichlet
problem \begin{equation*} \tag{$P$} \label{pba} \left\{ \begin{alignedat}{2}
-\Delta_p u + g(u) |\nabla u|^p &= \lambda f(u) \quad &&\mbox{in} \;\; \Omega,
\\ u &= 0 \quad &&\mbox{on} \;\; \partial\Omega, \end{alignedat} \right.
\end{equation*} where $\Omega\subset \mathbb{R}^N$ is a smooth bounded domain,
$p\in (1,\infty)$, $\lambda>0$ and $g\in C(\mathbb{R})$. Our main assumption is
that $:f \mathbb{R}\to \mathbb{R}$ is a continuous function such that $f(s)>0$
for all $s\in (\alpha,\beta)$, where $0<\alpha<\beta$ are two zeros of $f$.
  If $f(0)\geq 0$, we show that an area condition involving $f$ and $g$ is both
sufficient and necessary in order to have a pair $(\lambda,u)\in
\mathbb{R}^+\times C_0^1(\overline{\Omega})$, with $u\geq 0$ and
$\|u\|_{C(\overline{\Omega})}\in (\alpha,\beta]$, solving~\eqref{pba}.
  We also study how the presence of the gradient term affects the existence of
solution. Roughly speaking, the more negative $g$ is, the stronger its
regularizing effect on~\eqref{pba}. We prove that, regardless of the shape of
$f$, for any fixed $\lambda$, there always exists a function $g$ such
that~\eqref{pba} admits a nonnegative solution with maximum in
$(\alpha,\beta]$.

</details>


### [56] [Nonlinear Fisher information, corresponding functional inequalities and applications](https://arxiv.org/abs/2509.01475)
*Tomasz Cieślak,Kentaro Fujie,Tatsuya Hosono*

Main category: math.AP

TL;DR: Analysis of nonlinear Fisher information evolution in quasilinear heat equations, extension of functional inequalities, and applications to 1D Keller-Segel systems with global existence results.


<details>
  <summary>Details</summary>
Motivation: To extend the concept of Fisher information to nonlinear settings and establish functional inequalities for quasilinear heat equations, with applications to biological systems modeled by Keller-Segel equations.

Method: Studied evolution of nonlinear Fisher information along quasilinear heat equation, developed nonlinear version of functional inequality, applied to 1D critical quasilinear fully parabolic Keller-Segel system, and analyzed Fisher information for p-Laplace equation.

Result: Obtained global existence of solutions for critical nonlinear diffusion/nonlinear sensitivity 1D fully parabolic Keller-Segel system for certain diffusion types, and established nonlinear functional inequalities.

Conclusion: The nonlinear Fisher information framework provides powerful tools for analyzing quasilinear parabolic equations and establishes global existence results for critical biological systems with specific diffusion properties.

Abstract: We study the evolution of the nonlinear version of the Fisher information
along the quasilinear heat equation. We also provide a nonlinear version of a
recent functional inequality (Cie\'slak--Fuest--Hajduk--Sier\.z\k{e}ga, 2024),
corresponding to the nonlinear heat equation. Next, applications of our version
of nonlinear Fisher information to the 1D critical quasilinear fully parabolic
Keller--Segel system are given. In particular, the global existence of
solutions to the critical nonlinear diffusion/nonlinear sensitivity 1D fully
parabolic Keller--Segel system is obtained for certain type of diffusion. Last,
but not least, we also study the version of the Fisher information along the
$p$-Laplace equation.

</details>


### [57] [Scattering norm estimate near the threshold for the energy-subcrtical NLS](https://arxiv.org/abs/2509.01505)
*Zuyu Ma*

Main category: math.AP

TL;DR: Asymptotic estimates for scattering norms in energy-subcritical Schrödinger equations as mass-energy approaches threshold


<details>
  <summary>Details</summary>
Motivation: Previous works established scattering behavior below mass-energy threshold, but no quantitative estimates near the threshold were available

Method: Establish asymptotic estimates for upper bounds of scattering norms as mass-energy approaches the critical threshold M(Q)^{1-s_c}E(Q)^{s_c}

Result: Generalizes Duyckaerts-Merle's work on energy-critical case to energy-subcritical Schrödinger equations

Conclusion: Provides quantitative scattering norm estimates near the mass-energy threshold, extending previous qualitative results

Abstract: We consider the focusing energy-subcritical Schr\"odinger equations. In
earlier works by Holmer-Roudenko \cite{holmer}, Duyckaerts-Holmer-Roudenko
\cite{duyckaerts2}, Fang-Xie-Cazenave \cite{fang}, Guevara \cite{guevara} and
later by Dodson-Murphy \cite{dodson1,dodson2} and Arora-Dodson-Murphy
\cite{arora}, they proved that scattering is the only dynamical behavior if the
$H^1$ initial data satisfies
$M(u_0)^{1-s_c}E(u_0)^{s_c}<M(Q)^{1-s_c}E(Q)^{s_c}$ and $\| u\|^{1-s_c}_{L^2}\|
u\|^{s_c}_{\dot{H}^1}<\| Q\|^{1-s_c}_{L^2}\|Q\|^{s_c}_{\dot{H}^1}$, where $Q$
is the ground state. In this paper, we establish asymptotic estimates for the
upper bound of the scattering norms as $M(u_0)^{1-s_c}E(u_0)^{s_c}$ approaches
the threshold mass-energy threshold $M(Q)^{1-s_c}E(Q)^{s_c}$, which generalizes
the work of Duyckaerts-Merle \cite{duyckaerts} on the energy-critical
Schr\"odinger equation($s_c=1$).

</details>


### [58] [Sharp unconditional well-posedness of the 2-$d$ periodic cubic hyperbolic nonlinear Schrödinger equation](https://arxiv.org/abs/2509.01650)
*Engin Başakoğlu,Tadahiro Oh,Yuzhao Wang*

Main category: math.AP

TL;DR: Sharp semilinear local well-posedness and unconditional uniqueness for 2D periodic cubic hyperbolic NLS in Fourier-Lebesgue spaces using Fourier restriction norm method and normal form approach.


<details>
  <summary>Details</summary>
Motivation: To establish precise local well-posedness and unconditional uniqueness results for the two-dimensional periodic cubic hyperbolic nonlinear Schrödinger equation in Fourier-Lebesgue spaces, extending previous work on the standard NLS.

Method: Employed Fourier restriction norm method for local well-posedness and adapted normal form approach for unconditional uniqueness, with sharp counting estimates for the hyperbolic Schrödinger equation as a key ingredient.

Result: Achieved sharp semilinear local well-posedness (modulo endpoint case) including almost scaling-critical Fourier-Lebesgue spaces, and established sharp unconditional uniqueness within the semilinear regime.

Conclusion: Successfully obtained sharp results for both local well-posedness and unconditional uniqueness of 2D periodic cubic hyperbolic NLS, with the additional benefit of proving sharp unconditional uniqueness for the standard 2D periodic cubic NLS in Fourier-Lebesgue spaces for p ≥ 3.

Abstract: We study semilinear local well-posedness of the two-dimensional periodic
cubic hyperbolic nonlinear Schr\"odinger equation (HNLS) in Fourier-Lebesgue
spaces. By employing the Fourier restriction norm method, we first establish
sharp semilinear local well-posedness of HNLS in Fourier-Lebesgue spaces
(modulo the endpoint case), including almost scaling-critical Fourier-Lebesgue
spaces. Then, by adapting the normal form approach, developed by the second
author with Guo and Kwon (2013) and by the second and third authors (2021), to
the current hyperbolic setting, we establish sharp unconditional uniqueness of
HNLS within the semilinear local well-posedness regime. As a key ingredient to
both results, we establish sharp counting estimates for the hyperbolic
Schr\"odinger equation. As a byproduct of our analysis, we also obtain sharp
unconditional uniqueness of the (usual) two-dimensional periodic cubic
nonlinear Schr\"odinger equation in Fourier--Lebesgue spaces for $p \ge 3$.

</details>


### [59] [Well-Posedness and Finite Time Singularity for Touching g-SQG Patches on the Plane](https://arxiv.org/abs/2509.01687)
*Junekey Jeon,Andrej Zlatos*

Main category: math.AP

TL;DR: Local well-posedness and singularity formation for g-SQG patch model on plane with α∈(0,1/6], allowing touching patches


<details>
  <summary>Details</summary>
Motivation: Study the generalized surface quasi-geostrophic (g-SQG) patch model to understand local existence and finite-time singularity formation in fluid dynamics

Method: Bypass auxiliary contour equations and directly track patch boundary curves instead of their parametrizations

Result: Proved sharp results: patch boundaries have L² curvatures, singularity occurs when L²-norms blow up in finite time

Conclusion: The approach provides sharp analysis of g-SQG patch dynamics with direct boundary tracking method

Abstract: We prove local well-posedness as well as singularity formation for the g-SQG
patch model on the plane (so on a domain without a boundary), with
$\alpha\in(0,\frac 16]$ and patches being allowed to touch each other. We do
this by bypassing any auxiliary contour equations and tracking patch boundary
curves directly instead of their parametrizations. In our results, which are
sharp in terms of $\alpha$, the patch boundaries have $L^2$ curvatures and a
singularity occurs when at least one of these $L^2$-norms blows up in finite
time.

</details>


### [60] [Quadratic Growth Model with Discontinuity: A Link between Monostable and Bistable Traveling Waves](https://arxiv.org/abs/2509.01715)
*Wonhyung Choi,Junsik Bae,Yong-Jung Kim*

Main category: math.AP

TL;DR: Classification of traveling waves and stationary solutions in a reaction-diffusion equation with Allee effects and finite-time extinction discontinuity.


<details>
  <summary>Details</summary>
Motivation: To understand population dynamics with Allee-type effects where sub-threshold populations face finite-time extinction, which creates free boundaries in wave profiles unlike classical models.

Method: Analysis of a reaction-diffusion equation with quadratic polynomial reaction term featuring a discontinuity at zero, classifying traveling waves and stationary solutions through wave speed parameter.

Result: A complete scenario connecting monostable and bistable traveling waves through wave speed parameter, providing a unified framework for their dynamics.

Conclusion: The discontinuity-induced free boundary distinguishes this model from classical equations and enables a unified classification of wave dynamics in population systems with Allee effects.

Abstract: We classify traveling waves and stationary solutions of a reaction-diffusion
equation arising in population dynamics with Allee-type effects. The reaction
term is given by a quadratic polynomial with a discontinuity at zero, which
captures finite-time extinction for sub-threshold populations. This
discontinuity induces a free boundary in the wave profile, a phenomenon that
distinguishes the model from the classical logistic or Allen-Cahn equations. A
complete scenario is presented that connects monostable and bistable traveling
waves through the wave speed parameter, thereby providing a unified framework
for their dynamics.

</details>


### [61] [Embedding Results and Fractional $p(x,.)$-Laplacian Problem in Unbounded Domains](https://arxiv.org/abs/2509.01783)
*Abdelkrim Barbara,Ahmed Bousmaha,Mohammed Shimi*

Main category: math.AP

TL;DR: New continuous embedding theorem for fractional Sobolev spaces with variable exponents into Lebesgue spaces on unbounded domains, applied to prove existence of nontrivial solutions for nonlocal elliptic problems.


<details>
  <summary>Details</summary>
Motivation: Address analytical challenges from nonlocal behavior of fractional p(x,·)-Laplacian and lack of compactness due to unbounded domains in variable exponent function spaces.

Method: Prove new continuous embedding theorem, then use variational methods combined with the embedding result to study nonlocal elliptic problems.

Result: Established embedding theorem and proved existence of nontrivial weak solutions under suitable growth and regularity conditions on the nonlinearity.

Conclusion: Successfully developed embedding results and variational techniques to handle nonlocal operators with variable exponents on unbounded domains, overcoming compactness issues.

Abstract: In this paper, we prove a new continuous embedding theorem for fractional
Sobolev spaces with variable exponents into variable exponent Lebesgue spaces
on unbounded domains. As an application, we study a class of nonlocal elliptic
problems driven by the fractional $p(x, \cdot)$-Laplacian operator. Using
variational methods combined with the established embedding result, we prove
the existence of nontrivial weak solutions under suitable growth and regularity
conditions on the nonlinearity.\\ A significant analytical challenge addressed
in this work arises from both the nonlocal behavior of the fractional $p(x,
\cdot)$-Laplacian and the lack of compactness induced by the unboundedness of
the domain.

</details>


### [62] [Rate of convergence of the vanishing viscosity method for Hamilton-Jacobi equations with Neumann boundary conditions](https://arxiv.org/abs/2509.01973)
*Alessandro Goffi*

Main category: math.AP

TL;DR: Study of convergence rates for Hamilton-Jacobi equations with Neumann boundary conditions in small noise limit, showing O(√ε) rates for Lipschitz Hamiltonians on convex domains and enhanced O(ε) and O(ε^β) rates for quadratic Hamiltonians.


<details>
  <summary>Details</summary>
Motivation: To understand the quantitative behavior of small noise limits in Hamilton-Jacobi equations with Neumann boundary conditions, particularly how convergence rates depend on data regularity and domain geometry.

Method: Uses recent L^1 contraction estimates for Fokker-Planck equations with bounded velocity fields on unbounded domains to derive differential Harnack estimates for the corresponding Neumann heat flow.

Result: Achieved O(√ε) convergence rate for locally Lipschitz Hamiltonians on convex Euclidean domains, and improved rates of O(ε) and O(ε^β) for β∈(1/2,1) for quadratic Hamiltonians.

Conclusion: The study provides quantitative convergence rates for small noise limits in Hamilton-Jacobi equations, demonstrating that better rates can be obtained for specific Hamiltonian structures and leveraging modern PDE techniques.

Abstract: We study the quantitative small noise limit in the $L^\infty$ norm of certain
time-dependent Hamilton-Jacobi equations equipped with Neumann boundary
conditions, depending on the regularity of the data and the geometric
properties of the domain. We first provide a $\mathcal{O}(\sqrt{\eps})$ rate of
convergence for Hamilton-Jacobi equations with locally Lipschitz Hamiltonians
posed on convex domains of the Euclidean space. We then enhance this speed of
convergence in the case of quadratic Hamiltonians proving one-side rates of
order $\mathcal{O}(\eps)$ and $\mathcal{O}(\eps^\beta)$, $\beta\in(1/2,1)$. The
results exploit recent $L^1$ contraction estimates for Fokker-Planck equations
with bounded velocity fields on unbounded domains used to derive differential
Harnack estimates for the corresponding Neumann heat flow.

</details>


### [63] [Global existence of the irrotational Euler-Norstrom equations with a positive cosmological constant: The gravitational field equation](https://arxiv.org/abs/2509.02023)
*Uwe Brauer,Lavi Karp*

Main category: math.AP

TL;DR: Global existence of classical solutions for nonlinear irrotational Euler-Nordstroem system with linear equation of state and cosmological constant on 3D torus


<details>
  <summary>Details</summary>
Motivation: To prove global existence of classical solutions for the coupled Euler-Nordstroem system with gravitational scalar field and cosmological constant, focusing on spatially periodic deviations from background metric

Method: Two-part approach: 1) Analysis of Nordstroem equation with source term from irrotational fluid, 2) Analysis of full coupled system. Developed specialized energy functional and energy estimates tailored to the wave equation, as traditional symmetric hyperbolic system techniques were inadequate due to fractional-order nonlinearity

Result: Established framework for proving global existence of classical solutions in Sobolev spaces H^m(T^3) by developing appropriate energy methods for the semi-linear wave equation

Conclusion: Successfully developed specialized energy estimates to handle the fractional-order nonlinearity in the Euler-Nordstroem system, enabling proof of global existence for classical solutions on the three-dimensional torus

Abstract: Our objective is to demonstrate the global existence of classical solutions
for the nonlinear irrotational Euler-Nordstroem system, which includes a linear
equation of state and a cosmological constant. In this framework, the
gravitational field is represented by a single scalar function that satisfies a
specific semi-linear wave equation. We focus on spatially periodic deviations
from the background metric, which is why we study the semi-linear wave equation
on the three-dimensional torus $\mathbb{T}^3$ within the Sobolev spaces
$H^m(\mathbb{T}^3)$. This work is divided into two parts. First, we examine the
Nordstroem equation with a source term generated by an irrotational fluid
governed by a linear equation of state. In the second part, we analyze the full
coupled system. One reason for this separation is that an irrotational fluid
with a linear equation of state introduces a source term for the Nordstroem
equation containing a nonlinear term of fractional order. This nonlinearity
precludes the direct application of the techniques used in our earlier work
\cite{Brauer_Karp_23}, where we relied on symmetric hyperbolic systems, energy
estimates, and homogeneous Sobolev spaces. Instead, we develop an appropriate
energy functional and establish the corresponding energy estimates tailored to
the wave equation under consideration.

</details>


### [64] [Relativistic BGK model for reactive gas mixtures](https://arxiv.org/abs/2509.02037)
*Seung-Yeon Cho,Byung-Hoon Hwang,Myeong-Su Lee,Seok-Bae Yun*

Main category: math.AP

TL;DR: A BGK-type kinetic model for relativistic reactive gas mixtures that provides a computationally tractable alternative to the Boltzmann equation while maintaining physical consistency, conservation laws, and proper equilibrium behavior.


<details>
  <summary>Details</summary>
Motivation: To develop a computationally efficient yet physically accurate model for relativistic reactive gas mixtures that can serve as an alternative to the complex Boltzmann equation while preserving key physical properties.

Method: Constructed a BGK-type relaxation operator that ensures conservation laws are satisfied and the system relaxes to the proper Jüttner equilibrium distribution characterized by common temperature, velocity, and chemical potentials obeying mass action law.

Result: The model correctly satisfies conservation laws, relaxes to proper Jüttner equilibrium, obeys an H-theorem with the same entropy functional as the original Boltzmann equation, and numerical simulations confirm preservation of conserved quantities and entropy decay.

Conclusion: The proposed BGK-type kinetic model successfully provides a computationally tractable framework for relativistic reactive gas mixtures that maintains physical consistency, conservation properties, and proper thermodynamic behavior while being more practical than the full Boltzmann equation.

Abstract: We propose a BGK-type kinetic model for relativistic reactive gas mixtures.
This model serves as a computationally tractable yet physically consistent
alternative to the corresponding Boltzmann equation. The relaxation operator is
constructed to ensure that the model correctly satisfies the conservation laws
and relaxes to the proper equilibrium: a J\"{u}ttner distribution characterized
by a common temperature, velocity, and chemical potentials that obey the law of
mass action. Furthermore, we prove that the model satisfies an H-theorem with
the same entropy functional as the original Boltzmann equation. Finally,
numerical simulations are presented, which confirm that the model preserves the
conserved quantities and exhibits entropy decay towards the proper J\"{u}ttner
equilibrium.

</details>


### [65] [Tangential touch between free and fixed boundaries for the fully nonlinear Alt-Phillips problem](https://arxiv.org/abs/2509.02064)
*Yamin Wang,Hui Yu*

Main category: math.AP

TL;DR: Free boundary intersects fixed boundary tangentially where Dirichlet data vanish for fully nonlinear Alt-Phillips problem with γ∈(1,2)


<details>
  <summary>Details</summary>
Motivation: Study the behavior of free boundaries in fully nonlinear Alt-Phillips problems, particularly focusing on the intersection points between free and fixed boundaries

Method: Analysis of fully nonlinear Alt-Phillips problem with parameter γ in the range (1,2), examining boundary intersection behavior

Result: Free boundary meets fixed boundary tangentially at points where Dirichlet boundary conditions vanish

Conclusion: This tangential intersection result is novel even for the Laplacian case when γ∈(1,2), providing new insights into free boundary behavior

Abstract: For the fully nonlinear Alt-Phillips problem with parameter $\gamma\in(1,2)$,
we show that the free boundary intersects the fixed boundary tangentially where
the Dirichlet data vanish.
  For this range of $\gamma$, this result is new even when the operator is the
Laplacian.

</details>


### [66] [Superexponential dissipation enhancement on $\mathbb{T}^d$](https://arxiv.org/abs/2509.02081)
*Keefer Rowan*

Main category: math.AP

TL;DR: Construction of incompressible velocity fields that cause faster-than-exponential dissipation in advection-diffusion equations across different dimensions (2D, 3D, 4D) with varying velocity field regularities.


<details>
  <summary>Details</summary>
Motivation: To demonstrate the existence of velocity fields that can produce accelerated dissipation rates beyond exponential decay in advection-diffusion processes, exploring the relationship between spatial dimension, velocity field regularity, and dissipation rates.

Method: Construct specific incompressible velocity fields with different regularity properties (L∞, W1,∞, C∞) in 2D, 3D, and 4D, then analyze particular solutions to the advection-diffusion equation that exhibit the accelerated dissipation behavior.

Result: Successfully constructed velocity fields: in 2D (L∞) producing double exponential decay e^{-C^{-1}e^{C^{-1}t}}, in 3D (W1,∞) producing t² exponential decay e^{-C^{-1}t²}, and in 4D (C∞) producing some superexponential decay rate.

Conclusion: The dimension and regularity of velocity fields significantly impact dissipation rates in advection-diffusion equations, with higher dimensions and smoother fields enabling increasingly rapid (superexponential) dissipation behavior.

Abstract: We construct incompressible velocity fields that exhibit faster than
exponential dissipation for particular solutions to the advection-diffusion
equation on $\mathbb{T}^d$. In 2D, we construct a velocity field in
$L^\infty_{t,x}$ and exhibit a solution that decays with double exponential
rate $e^{-C^{-1} e^{C^{-1}t}}$. In 3D, we construct a velocity field in
$L^\infty_t W^{1,\infty}_x$ and exhibit a solution that decays with rate
$e^{-C^{-1} t^2}$. In 4D, we construct a velocity field in $L^\infty_t
C^\infty_x$ and exhibit a solution that decays with *some* superexponential
rate.

</details>


### [67] [Non existence of solutions for a slightly super-critical elliptic problem with non-power nonlinearity](https://arxiv.org/abs/2509.02140)
*Mohamed Ben Ayed,Habib Fourti*

Main category: math.AP

TL;DR: The paper extends previous results by showing that the slightly supercritical elliptic problem with logarithmic perturbation has no single-peaked solutions for small enough ε under a new assumption.


<details>
  <summary>Details</summary>
Motivation: To extend the non-existence result for single-peaked solutions from the standard supercritical elliptic problem to the logarithmic perturbation case, addressing a gap in the understanding of such nonlinear elliptic equations.

Method: Mathematical analysis of the elliptic equation with logarithmic perturbation term, building on previous work by Ben Ayed et al. and introducing a new assumption to handle the logarithmic nonlinearity.

Result: The authors prove that for sufficiently small ε > 0, the slightly supercritical elliptic problem with logarithmic term has no single-peaked solutions.

Conclusion: The logarithmic perturbation preserves the non-existence property of single-peaked solutions observed in the standard supercritical case, extending previous mathematical results to this more complex nonlinear setting.

Abstract: In this paper, we are concerned with the following elliptic equation $$ (
SC_\varepsilon ) \qquad
  \begin{cases} -\Delta u = |u|^{4/(n-2)}u [\ln (e+|u|)]^\varepsilon & \hbox{
in } \Omega,\\ u = 0 & \hbox{ on }\partial \Omega, \end{cases} $$ where $\Omega
$ is a smooth bounded open domain in $\mathbb{R}^n, \ n\geq 3$ and $\varepsilon
>0$. In Comm. Contemp. Math. (2003), Ben Ayed et al. showed that the slightly
supercritical usual elliptic problem has no single peaked solution. Here we
extend their result for problem $( SC_\varepsilon )$ when $\varepsilon$ is
small enough, and that by assuming a new assumption.

</details>


### [68] [Well-posedness and scattering of odd solutions for the defocusing INLS in one dimension](https://arxiv.org/abs/2509.02158)
*Zhi-Yuan Cui,Yuan Li,Dun Zhao*

Main category: math.AP

TL;DR: This paper establishes local and global well-posedness and scattering results for the defocusing inhomogeneous nonlinear Schrödinger equation in one dimension with odd initial data in H^1(R), overcoming the singularity issue using Hardy inequality for odd functions.


<details>
  <summary>Details</summary>
Motivation: Previous studies on the defocusing inhomogeneous nonlinear Schrödinger equation have been limited in one dimension due to difficulties handling the singularity factor |x|^{-b}, with most results established in H^s(R) with s<1 rather than H^1(R).

Method: The authors use the one-dimensional Hardy inequality for odd functions to overcome the singularity, combined with Strichartz estimates, the concentration-compactness/rigidity method by Kenig-Merle, and techniques for handling initial data far from the origin by Miao-Murphy-Zheng.

Result: Local well-posedness is established for 0<α<∞, and global well-posedness and scattering are proven for 4-2b<α<∞ (mass-supercritical case) for odd initial data in H^1(R).

Conclusion: The results fill a significant gap in the theory of well-posedness and energy scattering for the inhomogeneous nonlinear Schrödinger equation in one dimension, providing the first comprehensive treatment for odd initial data in H^1(R).

Abstract: We consider the defocusing inhomogeneous nonlinear Schr\"{o}dinger equation
  $i\partial_tu+\Delta u= |x|^{-b}|u|^{\alpha}u,$
  where $0<b<1$ and $0<\alpha<\infty$. This problem has been extensively
studied for initial data in $H^1(\R^N)$ with $N\geq 2$. However, in the
one-dimensional setting, due to the difficulty in dealing with the singularity
factor $|x|^{-b}$, the well-posedness and scattering in $H^1(\R)$ are scarce,
and almost known results have been established in $H^s(\R)$ with $s<1$. In this
paper, we focus on the odd initial data in $H^1(\R)$. For this case, we
establish local well-posedness for $0<\alpha<\infty$, as well as global
well-posedness and scattering for $4-2b<\alpha<\infty$, which corresponds to
the mass-supercritical case. The key ingredient is the application of the
one-dimensional Hardy inequality for odd functions to overcome the singularity
induced by $|x|^{-b}$. Our proof is based on the Strichartz estimates and
employs the concentration-compactness/rigidity method developed by Kenig-Merle
as well as the technique for handling initial data living far from the origin,
as proposed by Miao-Murphy-Zheng. Our results fill a gap in the theory of
well-posedness and energy scattering for the inhomogeneous nonlinear
Schr\"{o}dinger equation in one dimension.

</details>


### [69] [On analysis of problems of mathematical physics with non-Lipschitz boundaries](https://arxiv.org/abs/2509.02176)
*Anna Rozanova-Pierrat*

Main category: math.AP

TL;DR: Review of mathematical physics methods for domains with irregular boundaries using measure-free and L2 approaches, with generalization of Poincaré-Steklov operators and numerical validation.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of solving mathematical physics problems on domains with irregular boundaries that may have varying Hausdorff dimensions, requiring specialized analytical frameworks.

Method: Two frameworks: measure-free approach using trace operator spaces for extension domains, and L2-approach based on d-upper regular boundary measure. Both handle boundaries with Hausdorff dimensions in (n-2, n).

Result: Successful generalization of Poincaré-Steklov/Dirichlet-to-Neumann operators for both contexts. Numerical example demonstrates stability of localized eigenfunctions for elliptic operators with Robin boundary conditions.

Conclusion: The paper establishes effective mathematical frameworks for handling irregular boundary problems and validates the convergence of spectral problems through numerical evidence, advancing computational methods for complex domain geometries.

Abstract: We review recent advances in solving problems of mathematical physics on
domains with irregular boundaries in Rn. We distinguish two frameworks: a
measure-free approach in the image of the trace operator spaces for extension
domains and an L2-approach depending on a d-upper regular boundary measure. In
both cases, the domains can have boundaries with different Hausdorff dimensions
inside the interval (n -- 2, n). The generalization of the
Poincar{\'e}-Steklov/Dirichlet-to-Neumann operator for these two contexts is
given. To illustrate the established convergence of spectral problems for
elliptic operators with Robin boundary conditions, we give a numerical example
of the stability of localized eigenfunctions, using results of M. Graffin.

</details>


### [70] [From Nash to Cournot--Nash via $Γ$-convergence](https://arxiv.org/abs/2509.02205)
*Jo{ã}o Miguel Machado,Guilherme Mazanti,Laurent Pfeiffer*

Main category: math.AP

TL;DR: Analysis of convergence from N-player games to continuum models, focusing on Nash equilibria convergence to Cournot-Nash equilibria and potential function structure.


<details>
  <summary>Details</summary>
Motivation: To understand how finite-player game equilibria converge to continuum limit models as the number of players approaches infinity, particularly for games with potential structure.

Method: Characterize equilibria via stationarity conditions under low regularity assumptions, analyze pairwise interaction games, and prove Γ-convergence of potential functions.

Result: Demonstrated characterization of equilibria through stationarity conditions, showed N-player games maintain potential structure, and proved convergence of potential functions to the limit game's potential.

Conclusion: The work establishes rigorous convergence results from finite to continuum player models, providing mathematical foundation for analyzing large population games through potential function methods.

Abstract: This work addresses the issue of the convergence of an $N$-player game
towards a limit model involving a continuum of players, as the number of agents
$N$ goes to infinity. More precisely, we investigate the convergence of Nash
equilibria to a Cournot--Nash equilibrium of the limit model. When the cost
function of the players is the first variation of some potential function,
equilibria can be characterized by a stationarity condition, satisfied in
particular by the minimizers of the potential. We demonstrate such a
characterization under low regularity assumptions. Then we focus on the case
where the players interact in a pairwise fashion; in this case we show that the
original sequence of $N$-player games also admit a potential structure and
prove that their corresponding potential functions converge in the sense of
$\Gamma$-convergence to the potential function of the limit game.

</details>


### [71] [Stability of viscous shock for the Navier-Stokes-Fourier system: outflow and impermeable wall problems](https://arxiv.org/abs/2509.02215)
*Xushan Huang,Hobin Lee,HyeonSeop Oh*

Main category: math.AP

TL;DR: Stability analysis of viscous shocks in 1D Navier-Stokes-Fourier system for outflow and impermeable wall boundary problems, showing convergence to shock profiles with small perturbations.


<details>
  <summary>Details</summary>
Motivation: To establish time-asymptotic stability of viscous shock solutions in the Navier-Stokes-Fourier system for boundary value problems, particularly addressing outflow and impermeable wall cases which haven't been previously studied.

Method: Employed the method of a-contraction with shifts in Eulerian coordinates to provide a unified approach for both outflow and impermeable wall problems, avoiding free boundary issues that arise in Lagrangian coordinates.

Result: Proved that solutions converge asymptotically to viscous shock profiles (up to a dynamical shift) when initial perturbations and shock amplitude are sufficiently small.

Conclusion: This represents the first stability result for viscous shocks in initial-boundary value problems of the Navier-Stokes-Fourier system for outflow and impermeable wall cases, demonstrating successful application of a-contraction methods in Eulerian coordinates.

Abstract: We investigate the time-asymptotic stability of solutions to the
one-dimensional Navier-Stokes-Fourier system in the half space, focusing on the
outflow and impermeable wall problems. When the asymptotic profile determined
by the prescribed constant states at the boundary and at the far field is a
viscous shock, we show that the solution converges asymptotically to the
viscous shock profile, up to a dynamical shift, provided the initial
perturbation and the shock amplitude are sufficiently small. In order to obtain
our results, we employ the method of a-contraction with shifts. Although the
impermeable wall problem is technically simpler to analyze in Lagrangian mass
coordinates, the outflow problem leads to a free boundary in that framework.
Therefore, we use Eulerian coordinates to provide a unified approach to both
problems. This is the first result on the time-asymptotic stability of viscous
shocks for initial-boundary value problems of the Navier-Stokes-Fourier system
for the outflow and impermeable wall cases.

</details>


### [72] [A numerical study of stability for solitary waves of a quasi-linear Schr{ö}dinger equation](https://arxiv.org/abs/2509.02236)
*Meriem Bahhi,Jonas Lampart,Christian Klein,Simona Rota Nodari*

Main category: math.AP

TL;DR: Analysis of stability of solitary waves in quasi-linear Schrödinger equations with saturation effects and power nonlinearities, examining mass-energy relations and dynamic behavior across different dimensions and exponents.


<details>
  <summary>Details</summary>
Motivation: To understand the stability properties of solitary waves in quasi-linear Schrödinger equations, which combine saturation effects from quasi-linear terms with power nonlinearities, and to determine how stability depends on nonlinearity exponents and spatial dimensions.

Method: Analytical determination of asymptotic behavior of L²-mass as function of frequency near critical frequencies, followed by numerical investigation of mass-energy relations and dynamic evolution of perturbations to solitary waves.

Result: Found that stability depends on exponent and dimension - some cases show all solitary waves stable, others show emergence of both stable and unstable branches. Perturbations of unstable waves may converge to stable solutions with similar mass or disperse.

Conclusion: The study provides analytical and numerical evidence for stability conjectures in quasi-linear Schrödinger equations, demonstrating complex dynamic behavior including convergence to stable solutions and dispersion, with implications depending on nonlinearity exponents and spatial dimensions.

Abstract: We discuss the (in)stability of solitary waves for a quasi-linear
Schr{\"o}dinger equation. The equation contains a quasi-linear term,
responsible for a saturation effect, as well as a power nonlinearity. For
different exponents of the nonlinearity, we determine analytically the
asymptotic behavior of the $L^2$-mass of the solution as a function of the
frequency close to the critical frequencies, which leads to natural conjectures
concerning their stability. Depending on the exponent and the dimension, we
expect all solitary waves to be stable, or the emergence of both a stable and
an unstable branch of solutions. We investigate our conjectures numerically,
and find compatible results both for the mass-energy relation and the dynamics.
We observe that perturbations of solitary waves on the unstable branch may
converge dynamically to the stable solution of a similar mass, or disperse.
More general initial conditions show a similar behavior.

</details>


### [73] [On nondivergence form linear parabolic and elliptic equations with degenerate coefficients](https://arxiv.org/abs/2509.02286)
*Hongjie Dong,Junhee Ryu*

Main category: math.AP

TL;DR: Unique solvability established for degenerate parabolic/elliptic equations in weighted mixed-norm Sobolev spaces, with optimal function space analysis.


<details>
  <summary>Details</summary>
Motivation: To establish well-posedness and optimal regularity results for degenerate PDEs with coefficients having limited regularity, particularly those with x_d^2 degeneracy in the upper half space.

Method: Analysis of degenerate parabolic and elliptic equations in nondivergence form with coefficients x_d^2a_{ij}, where a_{ij} are bounded, uniformly nondegenerate, and measurable in (t,x_d) except a_dd which is measurable in t or x_d. Uses weighted mixed-norm Sobolev spaces and weighted small mean oscillations conditions.

Result: Proved unique solvability in weighted mixed-norm Sobolev spaces for the class of degenerate equations. Investigated and established the optimality of the associated function spaces.

Conclusion: The paper provides complete well-posedness theory and optimal regularity results for degenerate parabolic and elliptic equations with coefficients having specific measurable and oscillation properties in the upper half space.

Abstract: We establish the unique solvability in weighted mixed-norm Sobolev spaces for
a class of degenerate parabolic and elliptic equations in the upper half space.
The operators are in nondivergence form, with the leading coefficients given by
$x_d^2a_{ij}$, where $a_{ij}$ is bounded, uniformly nondegenerate, and
measurable in $(t,x_d)$ except $a_{dd}$, which is measurable in $t$ or $x_d$.
In the remaining spatial variables, they have weighted small mean oscillations.
In addition, we investigate the optimality of the function spaces associated
with our results.

</details>


### [74] [Probabilistic well-posedness of dispersive PDEs beyond variance blowup I: Benjamin-Bona-Mahony equation](https://arxiv.org/abs/2509.02344)
*Guopeng Li,Jiawei Li,Tadahiro Oh,Nikolay Tzvetkov*

Main category: math.AP

TL;DR: Extends probabilistic well-posedness theory beyond variance blowup for BBM equation with Gaussian random initial data using renormalization techniques.


<details>
  <summary>Details</summary>
Motivation: To develop a framework for handling nonlinear dispersive PDEs with random initial data when traditional variance blowup occurs, enabling analysis beyond this limitation.

Method: Introduces vanishing multiplicative renormalization constants on initial data and studies convergence in law of solutions to stochastic BBM forced by spatial white noise derivatives. Also considers frequency-truncated Gaussian initial data and fractional derivatives of space-time white noise.

Result: Shows that renormalized solutions converge to solutions of stochastic BBM forced by spatial white noise derivatives. For frequency-truncated cases, convergence to linear stochastic BBM with full Gaussian initial data is established, even for arbitrarily low regularity data.

Conclusion: Renormalization techniques successfully extend probabilistic well-posedness theory beyond variance blowup for BBM equation, providing convergence results to stochastic versions forced by white noise derivatives for various initial data regularities.

Abstract: We investigate a possible extension of probabilistic well-posedness theory of
nonlinear dispersive PDEs with random initial data beyond variance blowup. As a
model equation, we study the Benjamin-Bona-Mahony equation (BBM) with Gaussian
random initial data. By introducing a suitable vanishing multiplicative
renormalization constant on the initial data, we show that solutions to BBM
with the renormalized Gaussian random initial data beyond variance blowup
converge in law to a solution to the stochastic BBM forced by the derivative of
a spatial white noise. By considering alternative renormalization, we show that
solutions to the renormalized BBM with the frequency-truncated Gaussian initial
data converges in law to a solution to the linear stochastic BBM with the full
Gaussian initial data, forced by the derivative of a spatial white noise. This
latter result holds for the Gaussian random initial data of arbitrarily low
regularity. We also establish analogous results for the stochastic BBM forced
by a fractional derivative of a space-time white noise.

</details>


### [75] [On the complex moment problem as a dynamic inverse problem for a discrete system](https://arxiv.org/abs/2509.02443)
*A. S. Mikhaylov,V. S. Mikhaylov*

Main category: math.AP

TL;DR: The paper connects the complex moment problem to dynamic inverse problems for discrete systems with complex Jacobi matrices, showing how inverse problem characterizations provide sufficient conditions for solving moment problems.


<details>
  <summary>Details</summary>
Motivation: To establish a relationship between the complex moment problem (constructing measures from moments) and dynamic inverse problems for discrete systems, leveraging insights from one domain to solve problems in the other.

Method: Relates the complex moment problem to the dynamic inverse problem for discrete systems associated with complex Jacobi matrices, using characterizations of dynamic inverse data.

Result: Demonstrates that characterization of dynamic inverse data in solving inverse problems provides sufficient conditions for solving the complex moment problem.

Conclusion: The connection between complex moment problems and dynamic inverse problems offers a new approach and sufficient conditions for constructing positive Borel measures from given moments.

Abstract: We consider the complex moment problem, that is the problem of constructing a
positive Borel measure on $\mathbb{C}$ from a given set of moments. We relate
this problem to the dynamic inverse problem for the discrete system associated
with the complex Jacobi matrix. We show how the characterization of dynamic
inverse data in solving the inverse problem provides sufficient conditions for
solving the complex moment problem.

</details>


### [76] [Sharp boundary regularity properties for hypoelliptic kinetic equations](https://arxiv.org/abs/2509.02536)
*Yuzhe Zhu*

Main category: math.AP

TL;DR: Sharp boundary regularity analysis for kinetic Fokker-Planck equations with inflow boundary conditions, quantifying hypoelliptic regularization effects.


<details>
  <summary>Details</summary>
Motivation: To establish precise boundary regularity results for kinetic Fokker-Planck equations, particularly understanding how solutions behave near boundaries under different coefficient conditions and boundary types.

Method: Mathematical analysis of kinetic Fokker-Planck equations with prescribed inflow boundary conditions, examining both rough and regular coefficients, and characterizing solution behaviors on grazing and incoming boundaries.

Result: For rough coefficients: characterized solution behaviors on grazing/incoming boundaries; derived explicit exponential infinite-order vanishing estimate near incoming boundaries without influxes/sources. For regular coefficients: obtained optimal Hölder regularity on grazing boundaries and general Schauder-type estimates away from boundaries.

Conclusion: The paper provides comprehensive boundary regularity results for kinetic Fokker-Planck equations, quantifying the hypoelliptic regularization effect and establishing optimal regularity estimates under various coefficient and boundary conditions.

Abstract: We establish sharp boundary regularity results for solutions to kinetic
Fokker-Planck equations under prescribed inflow boundary conditions, providing
precise quantification of the boundary hypoelliptic regularization effect. For
equations with rough coefficients, we characterize the behaviours for solutions
on grazing and incoming boundaries. In particular, in the absence of influxes
and sources, an explicit exponential infinite-order vanishing estimate is
derived near incoming boundaries. When the coefficients are regular, we
obtained the optimal H\"older regularity on grazing boundaries and general
Schauder-type estimates away from them.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [77] [AFSI: Automated Fluid-Structure Interaction Solver Development for Nonlinear Solid Mechanics](https://arxiv.org/abs/2509.00014)
*Pengfei Ma,Li Cai,Xuan Wang,Hao Gao*

Main category: physics.comp-ph

TL;DR: AFSI is an open-source FSI solver that extends FEniCS with immersed boundary framework for simulating large deformations in hyperelastic materials without expensive remeshing.


<details>
  <summary>Details</summary>
Motivation: To enable simulation of large deformations in hyperelastic materials (like cardiac tissue) while avoiding costly remeshing operations typically required in traditional FSI approaches.

Method: Uses immersed boundary framework coupling Lagrangian solid representation with Eulerian fluid description. Implemented as hybrid Python/C++ architecture - Python for user interface/geometry definition, C++ for performance-critical tasks like assembly and linear solvers.

Result: Developed a concise modular Python API that facilitates FSI simulation setup and allows easy modification of discretization strategies while leveraging FEniCS's variational formulations and post-processing tools.

Conclusion: AFSI combines FEniCS flexibility with robust immersed boundary formulation, enabling rapid prototyping of complex nonlinear solid-fluid interaction problems for biomechanical systems and highly deformable structures in flow.

Abstract: AFSI is a novel, open-source fluid-structure interaction (FSI) solver
  that extends the capabilities of the FEniCS finite element library through
  an immersed boundary (IB) framework. Designed to simulate large deformations
  in hyperelastic materials (such as cardiac tissue), AFSI avoids the need for
expensive remeshing by coupling a Lagrangian representation of the solid with
an Eulerian description of the surrounding fluid. This approach retains the
full expressiveness of FEniCS's variational formulations, function spaces, and
time integration schemes.
  Implemented in a hybrid Python/C++ architecture, AFSI allows users to define
geometries, constitutive models (e.g., the Holzapfel-Ogden law for myocardium),
and strain energy functions directly in Python, while delegating
performance-critical tasks such as assembly and linear solvers to optimized C++
backends. Its concise and modular Python API facilitates the setup of FSI
simulations, enabling users to easily modify discretization strategies or
analyze results using standard FEniCS post-processing tools.
  By combining the flexibility of FEniCS with a robust immersed boundary
formulation, AFSI empowers rapid prototyping of complex nonlinear solid-fluid
interaction problems, making it a powerful tool for simulating biomechanical
systems and other applications involving highly deformable structures in flow.

</details>


### [78] [Generalization vs. Memorization in Autoregressive Deep Learning: Or, Examining Temporal Decay of Gradient Coherence](https://arxiv.org/abs/2509.00024)
*James Amarel,Nicolas Hengartner,Robyn Miller,Kamaljeet Singh,Siddharth Mansingh,Arvind Mohan,Benjamin Migliori,Emily Casleton,Alexei Skurikhin,Earl Lawrence,Gerd J. Kunde*

Main category: physics.comp-ph

TL;DR: The paper analyzes autoregressive PDE surrogate models using influence functions to distinguish genuine generalization from memorization, revealing limitations in standard models and providing insights for improved surrogate design.


<details>
  <summary>Details</summary>
Motivation: Foundation models for PDE surrogates promise scientific acceleration through extrapolation and adaptation, but reliably achieving genuine generalization remains challenging. Current evaluation metrics cannot clearly distinguish true generalization from memorization.

Method: The authors apply the influence function formalism to systematically characterize how autoregressive PDE surrogates assimilate and propagate information from diverse physical scenarios.

Result: The analysis reveals fundamental limitations of standard models and training routines, showing they often fail to achieve genuine generalization despite appearing to perform well.

Conclusion: The influence function approach provides actionable insights for designing improved PDE surrogates that can reliably achieve genuine generalization, which is crucial for producing novel scientific insights and robust deployment.

Abstract: Foundation models trained as autoregressive PDE surrogates hold significant
promise for accelerating scientific discovery through their capacity to both
extrapolate beyond training regimes and efficiently adapt to downstream tasks
despite a paucity of examples for fine-tuning. However, reliably achieving
genuine generalization - a necessary capability for producing novel scientific
insights and robustly performing during deployment - remains a critical
challenge. Establishing whether or not these requirements are met demands
evaluation metrics capable of clearly distinguishing genuine model
generalization from mere memorization.
  We apply the influence function formalism to systematically characterize how
autoregressive PDE surrogates assimilate and propagate information derived from
diverse physical scenarios, revealing fundamental limitations of standard
models and training routines in addition to providing actionable insights
regarding the design of improved surrogates.

</details>


### [79] [Generative Latent Space Dynamics of Electron Density](https://arxiv.org/abs/2509.00169)
*Yuan Chiang,Youngsoo Choi,Daniel Osei-Kuffuor*

Main category: physics.comp-ph

TL;DR: A generative framework combining 3D convolutional autoencoder with latent diffusion model to predict electron density trajectories from quantum simulations, enabling stable long-term predictions while preserving statistical properties.


<details>
  <summary>Details</summary>
Motivation: Modeling time-dependent electron density evolution is crucial for understanding quantum mechanical behaviors in condensed matter and enabling predictive simulations in spectroscopy and ultrafast science, but current machine learning methods only handle static density prediction.

Method: Combines 3D convolutional autoencoder with latent diffusion model to encode electron densities into compact latent space and predict future states by sampling from learned conditional distribution. Uses scaled Jensen-Shannon divergence regularization to preserve statistical fidelity.

Result: Accurately captures both spatial correlations and log-normal-like statistical structure of electron density in liquid lithium at 800 K, enabling stable long-horizon rollouts without drift or collapse.

Conclusion: The framework has potential to accelerate quantum dynamics simulation and overcome key challenges faced by current spatiotemporal machine learning methods as surrogates for quantum mechanical simulators.

Abstract: Modeling the time-dependent evolution of electron density is essential for
understanding quantum mechanical behaviors of condensed matter and enabling
predictive simulations in spectroscopy, photochemistry, and ultrafast science.
Yet, while machine learning methods have advanced static density prediction,
modeling its spatiotemporal dynamics remains largely unexplored. In this work,
we introduce a generative framework that combines a 3D convolutional
autoencoder with a latent diffusion model (LDM) to learn electron density
trajectories from ab-initio molecular dynamics (AIMD) simulations. Our method
encodes electron densities into a compact latent space and predicts their
future states by sampling from the learned conditional distribution, enabling
stable long-horizon rollouts without drift or collapse. To preserve statistical
fidelity, we incorporate a scaled Jensen-Shannon divergence regularization that
aligns generated and reference density distributions. On AIMD trajectories of
liquid lithium at 800 K, our model accurately captures both the spatial
correlations and the log-normal-like statistical structure of the density. The
proposed framework has the potential to accelerate the simulation of quantum
dynamics and overcome key challenges faced by current spatiotemporal machine
learning methods as surrogates of quantum mechanical simulators.

</details>


### [80] [Particle swarm optimization of divertor targets for heat load control](https://arxiv.org/abs/2509.00206)
*H. Frerichs*

Main category: physics.comp-ph

TL;DR: Using FLARE for fast heat load approximation and particle swarm optimization to optimize ITER outer divertor target geometry with baffle heat load constraints.


<details>
  <summary>Details</summary>
Motivation: Divertor targets in fusion devices need to handle extreme heat loads, requiring optimized geometry designs that can be efficiently evaluated.

Method: Utilizes FLARE for fast heat load approximation based on field line reconstruction from unstructured flux tube mesh, combined with particle swarm optimization (PSO) for divertor target geometry optimization.

Result: The optimal divertor target configuration for ITER depends on assumptions made about the background plasma in the heat load proxy simulations.

Conclusion: PSO with FLARE provides an effective approach for divertor optimization, but results are sensitive to plasma background assumptions, highlighting the need for careful parameter selection in heat load simulations.

Abstract: Divertor targets in magnetic confinement fusion devices must be designed to
handle extreme heat loads. Fast approximation of heat loads with FLARE based on
field line reconstruction from an unstructured flux tube mesh is utilized in
particle swarm optimization (PSO) of the divertor target geometry. Optimization
of the outer divertor target in ITER is evaluated with a constraint for the
head loads onto baffles. The optimal configuration is found to depend on
assumptions for the background plasma in the heat load proxy simulation.

</details>


### [81] [rhodent: A Python package for analyzing real-time TDDFT response](https://arxiv.org/abs/2509.00459)
*Jakub Fojt,Tuomas P. Rossi,Paul Erhart*

Main category: physics.comp-ph

TL;DR: rhodent is a modular Python package for processing rt-TDDFT calculation outputs, enabling calculation of various observables and decompositions, with support for multiple codes and efficient frequency-dependent analysis.


<details>
  <summary>Details</summary>
Motivation: Existing rt-TDDFT codes have different numerical methods, exchange-correlation functionals, and analysis capabilities, creating inconvenience for users who need features from different codes.

Method: Developed a modular Python package that processes rt-TDDFT outputs to calculate hot-carrier distributions, energies, induced densities, dipole moments, and their decompositions. Supports gpaw code and can be extended to others. Uses linear response assumption for efficient narrow-band laser response calculation from broad-band perturbations.

Result: The package successfully processes rt-TDDFT calculations and demonstrates capabilities through examples with Al/Ag clusters and organic molecules, enabling efficient analysis of frequency-dependent excitations.

Conclusion: rhodent provides a unified, extensible solution for rt-TDDFT analysis, overcoming limitations of individual codes and significantly speeding up frequency-dependent excitation analysis.

Abstract: Real-time time-dependent density functional theory (rt-TDDFT) is a
well-established method for studying the dynamic response of matter in the
femtosecond or optical range. In this method, the Kohn-Sham (KS) wave functions
are propagated forward in time, and in principle, one can extract any
observable at any given time. Alternatively, by taking a Fourier transform,
spectroscopic quantities can be extracted. There are many publicly available
codes implementing rt-TDDFT, which differ in their numeric solution of the KS
equations, their available exchange-correlation functionals, and in their
analysis capabilities. For users of rt-TDDFT, this is an inconvenient situation
because they may need to use a numerical method that is available in one code,
but an analysis method available in another. Here, we introduce rhodent, a
modular Python package for processing the output of rt-TDDFT calculations. Our
package can be used to calculate hot-carrier distributions, energies, induced
densities, and dipole moments, and various decompositions thereof. In its
current version, rhodent handles calculation results from the gpaw code, but
can readily be extended to support other rt-TDDFT codes. Additionally, under
the assumption of linear response, rhodent can be used to calculate the
response to a narrow-band laser, from the response to a broad-band
perturbation, greatly speeding up the analysis of frequency-dependent
excitations. We demonstrate the capabilities of rhodent via a set of examples,
for systems consisting of Al and Ag clusters and organic molecules.

</details>


### [82] [Statistics of Residual Stress in Random Microstructures: Mean-Field Estimates and Full-Field Validations](https://arxiv.org/abs/2509.00739)
*Tarkes Dora Pallicity*

Main category: physics.comp-ph

TL;DR: This paper presents a mean-field homogenization (MFH) method to efficiently estimate second moments of local stress fields in thermo-elastic composites, validated against computationally intensive full-field simulations.


<details>
  <summary>Details</summary>
Motivation: Local field fluctuations are critical for predicting failure and inelastic behavior in random composites, but full-field methods are computationally expensive and limited in scalability to complex microstructures.

Method: Developed analytical expressions based on Hill-Mandel condition to calculate second moments of local fields for linear thermo-elastic problems using MFH, relying on derivatives of Hill's polarization tensor from previous work.

Result: MFH successfully captured essential features of stress distributions compared to full-field simulations, which showed non-Gaussian stress components and Weibull-like distributions for equivalent residual stress.

Conclusion: The mean-field approach with assumed Gaussian distribution provides efficient and reasonably accurate estimation of field fluctuations, offering a practical alternative to computationally intensive full-field methods.

Abstract: Fluctuations of local fields are crucial for the prediction of failure in
random composites across different scales as well as estimating the inelastic
behaviour of it. This can be quantified statistically through second moments of
the local fields, which can be quickly estimated using mean field
homogenization (MFH). However, the exact fluctuation field can be estimated
using full-field methods though it comes at the cost of intensive computational
resources and limited scalability to complex microstructures. In this work, MFH
is used to estimate the statistical variation of the field quantities and then
cross-verified with full-field methods for a linear-thermoelastic
homogenization problem. An analytical expression to calculate the second
moments of the local fields for a linear thermo-elastic problem using MFH is
obtained based on the Hill-Mandel condition. The expressions fundamentally rely
on the solution of linear elastic problem which in turn depends on the
derivatives of Hill's polarization tensor. Solution of this derivative term has
been analytically and semianalytically derived in previous work [1]. The
statistical distribution of residual stress tensor components and equivalent
stress in particulate and unidirectional fibrous composites, arising purely due
to differential thermal expansion, is computed and compared with full-field
homogenization. Full-field simulations indicated a non-Gaussian distribution of
stress components, whereas Weibull-like distributions for equivalent residual
stress. Nevertheless, the assumed Gaussian distribution in mean-field estimates
captures the essential features.

</details>


### [83] [Self-supervised neural operator for solving partial differential equations](https://arxiv.org/abs/2509.00867)
*Wen You,Shaoqian Zhou,Xuhui Meng*

Main category: physics.comp-ph

TL;DR: Self-supervised neural operator (SNO) that generates training data on-the-fly without numerical solvers, using physics-informed sampling and encoder-only Transformer for PDE solving.


<details>
  <summary>Details</summary>
Motivation: Neural operators require costly high-fidelity data from numerical solvers, limiting applications in complex systems. Need self-supervised approach to eliminate dependency on solver data.

Method: Three-part architecture: 1) Physics-informed sampler based on Bayesian PINNs for data generation, 2) Function encoder for compact representations, 3) Encoder-only Transformer for operator learning mapping boundary conditions to PDE solutions.

Result: High accuracy achieved on 1D/2D nonlinear PDEs and fluid dynamics problems. Lightweight finetuning (O(100) variables) improves predictions with few hundred steps.

Conclusion: Provides new route toward pretrained foundation models as efficient PDE surrogates without numerical solver dependency.

Abstract: Neural operators (NOs) provide a new paradigm for efficiently solving partial
differential equations (PDEs), but their training depends on costly
high-fidelity data from numerical solvers, limiting applications in complex
systems. We propose a self-supervised neural operator (SNO) that generates
accurate and diverse training data on the fly without numerical solvers. SNO
consists of three parts: a physics-informed sampler (PI-sampler) based on
Bayesian PINNs for efficient data generation, a function encoder (FE) for
compact input-output representations, and an encoder-only Transformer for
operator learning, mapping boundary/initial conditions, source terms, and
geometries to PDE solutions. We validate SNO on 1D steady/unsteady nonlinear
reaction-diffusion equations, a 2D nonlinear PDE with varying geometries, and
vortex-induced vibration of a flexible cylinder in fluid dynamics. SNO achieves
high accuracy in all cases, and lightweight finetuning (O(100) trainable
variables) further improves predictions with only a few hundred steps. This
work provides a new route toward pretrained foundation models as efficient PDE
surrogates.

</details>


### [84] [Multiscale light-matter dynamics in quantum materials: from electrons to topological superlattices](https://arxiv.org/abs/2509.00966)
*Taufeq Mohammed Razakh,Thomas Linker,Ye Luo,Nariman Piroozan,John Pennycook,Nalini Kumar,Albert Musaelian,Anders Johansson,Boris Kozinsky,Rajiv K. Kalia,Priya Vashishta,Fuyuki Shimojo,Shinnosuke Hattori,Ken-ichi Nomura,Aiichiro Nakano*

Main category: physics.comp-ph

TL;DR: A new computational paradigm using hardware heterogeneity and low-precision arithmetic achieves massive speedups for simulating light-matter dynamics in topological quantum materials, enabling the first study of light-induced switching in topological superlattices.


<details>
  <summary>Details</summary>
Motivation: To overcome the challenge of simulating multiple field and particle equations for light, electrons, and atoms over vast spatiotemporal scales on modern heterogeneous computers with low-precision focus, particularly for topological quantum materials.

Method: Divide-conquer-recombine algorithms that partition problems into physical subproblems with small dynamic ranges, mapped onto best-matching hardware units, combined with metamodel-space algebra to minimize communication and precision requirements.

Result: Achieved 152x and 3,780x speedups over state-of-the-art for 15.4 million-electron and 1.23 trillion-atom PbTiO3 material simulations, reaching 1.87 EFLOP/s performance on 60,000 GPUs of Aurora supercomputer.

Conclusion: The approach enables the first study of light-induced switching of topological superlattices, paving the way for future ferroelectric 'topotronics' devices with ultralow-power and ultrafast operation.

Abstract: Light-matter dynamics in topological quantum materials enables
ultralow-power, ultrafast devices. A challenge is simulating multiple field and
particle equations for light, electrons, and atoms over vast spatiotemporal
scales on Exaflop/s computers with increased heterogeneity and low-precision
focus. We present a paradigm shift that solves the
multiscale/multiphysics/heterogeneity challenge harnessing hardware
heterogeneity and low-precision arithmetic. Divide-conquer-recombine algorithms
divide the problem into not only spatial but also physical subproblems of small
dynamic ranges and minimal mutual information, which are mapped onto
best-characteristics-matching hardware units, while metamodel-space algebra
minimizes communication and precision requirements. Using 60,000 GPUs of
Aurora, DC-MESH (divide-and-conquer Maxwell-Ehrenfest-surface hopping) and
XS-NNQMD (excited-state neural-network quantum molecular dynamics) modules of
MLMD (multiscale light-matter dynamics) software were 152- and 3,780-times
faster than the state-of-the-art for 15.4 million-electron and 1.23
trillion-atom PbTiO3 material, achieving 1.87 EFLOP/s for the former. This
enabled the first study of light-induced switching of topological superlattices
for future ferroelectric 'topotronics'.

</details>


### [85] [Learning by training: emergent return-point memory from cyclically tuning disordered sphere packings](https://arxiv.org/abs/2509.01296)
*Mengjie Zu,Carl P. Goodrich*

Main category: physics.comp-ph

TL;DR: Systems adapt to changing environments through cyclic inverse design, evolving toward a marginally absorbing manifold that encodes memory of training ranges, resembling return-point memory in driven systems.


<details>
  <summary>Details</summary>
Motivation: To understand how environmental variation influences adaptation, what is learned during the process, and whether memory of past conditions is retained in adaptive systems.

Method: Using athermal disordered systems subjected to cyclic inverse design to attain target elastic properties across a chosen range, and analyzing the formation of marginally absorbing manifolds.

Result: Systems evolve toward a marginally absorbing manifold (MAM) that encodes memory of the training range, closely resembling return-point memory observed in cyclically driven systems.

Conclusion: The proposed model provides a simple and broadly applicable physical framework for understanding how adaptive systems learn under environmental change and retain memory of past experiences.

Abstract: Many living and artificial systems improve their fitness or performance by
adapting to changing environments or diverse training data. However, it remains
unclear how such environmental variation influences adaptation, what is learned
in the process, and whether memory of past conditions is retained. In this
work, we investigate these questions using athermal disordered systems that are
subject to cyclic inverse design, enabling them to attain target elastic
properties spanning a chosen range. We demonstrate that such systems evolve
toward a marginally absorbing manifold (MAM), which encodes memory of the
training range that closely resembles return-point memory observed in
cyclically driven systems. We further propose a general mechanism for the
formation of MAMs and the corresponding memory that is based on gradient
discontinuities in the trained quantities. Our model provides a simple and
broadly applicable physical framework for understanding how adaptive systems
learn under environmental change and how they retain memory of past
experiences.

</details>


### [86] [Explaining Optomechanical Libration Spectra: A Stochastic Simulation Approach](https://arxiv.org/abs/2509.01636)
*Ankush Gogoi,Vikram Pakrashi,Joanna A. Zielinska*

Main category: physics.comp-ph

TL;DR: Ito-Taylor expansion framework for simulating rotational optomechanics experiments, explaining nonlinear orientation dynamics and spectral features in nanodumbbell systems.


<details>
  <summary>Details</summary>
Motivation: To develop a practical computational framework for modeling complex rotational dynamics in optomechanics experiments, particularly to explain observed spectral features that existing models couldn't capture.

Method: Developed an Ito-Taylor expansion based stochastic simulation framework to model nonlinear orientation dynamics of optically levitated cylindrically symmetric nanodumbbells.

Result: Successfully reproduced and explained shoulder-like features in power spectral density of libration, showing they arise from interplay between confined libration and thermally driven rotation around symmetry axis.

Conclusion: The Ito-Taylor expansion framework provides an effective computational approach for simulating and understanding complex rotational dynamics in optomechanical systems, particularly for explaining previously unexplained spectral features.

Abstract: We present a practical and computationally effective Ito-Taylor expansion
based stochastic simulation framework for modeling rotational optomechanics
experiments. By developing a model using this framework, we could capture the
nonlinear orientation dynamics of an optically levitated, nearly cylindrically
symmetric nanodumbbell. It successfully reproduces and explains shoulder-like
features observed in the power spectral density of libration, which we show
arising from the interplay between confined libration and thermally driven
rotation around the particle symmetry axis.

</details>


### [87] [Inference of epidemic networks: the effect of different data types](https://arxiv.org/abs/2509.01871)
*Oscar Fajardo-Fontiveros,Carl J. E. Suster,Eduardo G. Altmann*

Main category: physics.comp-ph

TL;DR: The paper develops computational methods to estimate epidemic transmission probabilities by combining different data types (location, genetic, temporal) using MCMC sampling, and shows that outbreak management depends critically on the data used for inference.


<details>
  <summary>Details</summary>
Motivation: To understand how epidemic network properties change based on available outbreak data types, and to develop methods for estimating transmission probabilities and unobserved infection parameters.

Method: Introduced mathematical and computational methods using generative models that combine location, genetic, and temporal infection data. Developed a Markov Chain Monte Carlo method to sample transmission trees according to their probability, enabling statistical estimation of network properties.

Result: The approach was validated against analytically solvable examples and applied to COVID-19 data from Australia. Results showed that important network properties for outbreak management depend sensitively on the type of data used in the inference process.

Conclusion: The methodology provides probabilistic estimations of epidemic network properties and demonstrates that outbreak management strategies should consider the specific types of data available, as different data sources significantly impact the inferred transmission network characteristics.

Abstract: We investigate how the properties of epidemic networks change depending on
the availability of different types of data on a disease outbreak. This is
achieved by introducing mathematical and computational methods that estimate
the probability of transmission trees by combining generative models that
jointly determine the number of infected hosts, the probability of infection
between them depending on location and genetic information, and their time of
infection and sampling. We introduce a suitable Markov Chain Monte Carlo method
that we show to sample trees according to their probability. Statistics
performed over the sampled trees lead to probabilistic estimations of network
properties and other quantities of interest, such as the number of unobserved
hosts and the depth of the infection tree. We confirm the validity of our
approach by comparing the numerical results with analytically solvable
examples. Finally, we apply our methodology to data from COVID-19 in Australia.
We find that network properties that are important for the management of the
outbreak depend sensitively on the type of data used in the inference.

</details>


### [88] [Probing the partition function for temperature-dependent potentials with nested sampling](https://arxiv.org/abs/2509.02361)
*Lune Maillard,Philippe Depondt,Fabio Finocchi,Simon Huppert,Thomas Plé,Julien Salomon,Martino Trassinelli*

Main category: physics.comp-ph

TL;DR: A new method using extended partition function with temperature as sampling parameter enables single-run nested sampling for temperature-dependent potentials, outperforming traditional approaches.


<details>
  <summary>Details</summary>
Motivation: Traditional nested sampling requires separate runs for each temperature when dealing with temperature-dependent potential energies, leading to massive computational time increases.

Method: Introduces an extended partition function where temperature is treated as an additional sampling parameter, allowing nested sampling to be performed in a single run for temperature-dependent systems.

Result: The method successfully computes quantum partition functions for harmonic potentials and Lennard-Jones clusters at low temperatures, outperforming traditional nested sampling across multiple temperature ranges.

Conclusion: The extended partition function approach restores the single-run efficiency of nested sampling for temperature-dependent systems, providing significant computational advantages over conventional methods.

Abstract: Thermodynamic properties can be in principle derived from the partition
function, which, in many-atom systems, is hard to evaluate as it involves a sum
on the accessible microscopic states. Recently, the partition function has been
computed via nested sampling, relying on Bayesian statistics, which is able to
provide the density of states as a function of the energy in a single run,
independently of the temperature. This appealing property is lost whenever the
potential energy that appears in the partition function is
temperature-dependent: for instance, mean-field effective potential energies or
the quantum partition function in the path-integral formalism. For these cases,
the nested sampling must be carried out at each temperature, which results in a
massive increase of computational time. Here, we introduce and implement a new
method, that is based on an extended partition function where the temperature
is considered as an additional parameter to be sampled. The extended partition
function can be evaluated by nested sampling in a single run, so to restore
this highly desirable property even for temperature-dependent effective
potential energies. We apply this original method to compute the quantum
partition function for harmonic potentials and Lennard-Jones clusters at low
temperatures and show that it outperforms the straightforward application of
nested sampling for each temperature within several temperature ranges.

</details>


### [89] [Bryne: sustainable prototyping of finite element models](https://arxiv.org/abs/2509.02378)
*Benjamin Terschanski,Robert Klöfkorn,Andreas Dedner,Julia Kowalski*

Main category: physics.comp-ph

TL;DR: Bryne is an open-source Python framework that extends dune-fem to create reusable, metadata-enriched FEM solvers for sustainable multiphysics simulations.


<details>
  <summary>Details</summary>
Motivation: Modern FEM software enables rapid prototyping but lacks sustainability - solvers are hard to reuse beyond initial setups, and managing complex metadata across parameter spaces is challenging without extensive coding.

Method: Object-oriented framework built on dune-fem Python API that translates minimal solvers into human-readable, metadata-enriched simulations with simulation drivers and model coupling interfaces for operator-split multiphysics.

Result: Enables building reproducible infrastructure for complex simulation setups without sacrificing backend flexibility, demonstrated through convection-coupled phase-change simulation.

Conclusion: Bryne bridges the gap between rapid prototyping and sustainable simulation building by providing structured framework for reusable, metadata-rich FEM solvers in multiphysics applications.

Abstract: Open-source simulation frameworks are evolving rapidly to provide accessible
tools for the numerical solution of partial differential equations. Modern
finite element (FEM) software such as FEniCS, Firedrake, or dune-fem alleviates
the need for modelers to recode the discretization and linear solver backend
for each application and enables rapid prototyping of solvers. However, while
it has become easier to build prototype FEM models, creating a solver reusable
beyond its specific initial simulation setup remains difficult. Moreover,
simulation setups typically cover an ample input parameter space, and tracking
complex metadata on research project time scales has become a challenge. This
implies the need to supplement model development with a coding-intensive
complementary workstream, seldom developed for sustainable reuse. To address
these issues, we introduce our open-source Python package Bryne. Bryne is an
object-oriented framework for FEM solvers built with the dune-fem Python API.
In this article, we describe how it helps to evolve rapid-prototyping solver
development into sustainable simulation building. First, we show how to
translate a minimal dune-fem solver into a Bryne FEM model to build
human-readable, metadata-enriched simulations. Bryne then offers a simulation
driver and model coupling interfaces to combine implemented solvers in
operator-split multiphysics simulations. The resulting reproducibility-enabled
infrastructure allows users to tackle complex simulation setups without
sacrificing backend flexibility. We demonstrate the workflow on a
convection-coupled phase-change simulation, where a discontinuous Galerkin flow
solver is coupled with a solver for solidification phase change.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [90] [PRISM: A MATLAB-Based Application for Structured Probe Data Management and Visualization in Tokamak Diagnostics](https://arxiv.org/abs/2509.00358)
*Priyanka Verma,Subhojit Bose,Harshita Raj,Joydeep Ghosh*

Main category: physics.plasm-ph

TL;DR: PRISM is a MATLAB-based GUI application that automates probe registration, metadata management, and 2D/3D visualization for tokamak experiments, replacing manual documentation methods.


<details>
  <summary>Details</summary>
Motivation: Traditional manual documentation methods (handwritten logbooks, spreadsheets) for tokamak diagnostic instruments lead to inefficiencies and human errors, requiring a more systematic digital solution.

Method: Developed a MATLAB-based application using App Designer with a graphical user interface for structured probe registration, metadata storage, and spatial visualization in tokamak geometries.

Result: Successfully tested with ADITYA-U tokamak data, enabling accurate information entry, easy metadata retrieval, and probe setup visualization. The tool is flexible and not limited to specific setups.

Conclusion: PRISM provides an efficient digital solution for probe management in tokamak experiments and has potential for future integration with digital twins and real-time control systems.

Abstract: The successful operation of tokamak experiments requires accurate
documentation, tracking, and visualization of diagnostic instruments,
particularly electrical probes. Traditionally, this metadata is maintained
manually through handwritten logbooks or semi-digital spreadsheets, leading to
inefficiencies and human errors. In response to these challenges, we present
PRISM (Probe Registration and Information System for Monitoring)-a MATLAB-based
application developed using App Designer. PRISM provides a graphical user
interface (GUI) that facilitates structured probe registration, metadata
storage, and both 2D and 3D spatial visualization in tokamak geometries. Tested
with data from the ADITYA-U tokamak, PRISM helps users enter information
accurately, retrieve metadata easily, and visualize probe setups. The tool is
built in a flexible way, is not limited to a specific setup, and could
potentially support future developments such as digital twins and real-time
control systems.

</details>


### [91] [Ultrashort Time-Integrated Diagnosis of Laser-Heated Deuterium Ions in Dense Plasma via Fusion Neutron Spectra](https://arxiv.org/abs/2509.00659)
*Jie Feng,Hao Xu,Mingxuan Wei,Mingyang Zhu,Xichen Hu,Bingzhan Shi,Fuyuan Wu,Weijun Zhou,Wenchao Yan,Guoqiang Zhang,Jinguang Wang,Yifei Li,Xin Lu,Liming Chen*

Main category: physics.plasm-ph

TL;DR: A new method using femtosecond laser pulses and neutron detection to measure deuterium ion dynamics in dense plasma with picosecond time resolution.


<details>
  <summary>Details</summary>
Motivation: Ultrashort time-integrated diagnosis of ions is crucial for high energy density physics research but extremely challenging to measure experimentally.

Method: Irradiate heavy water stream with hundred Hertz repetitive intense femtosecond laser pulses, detect neutrons from D(D,n)3He reaction using single Time-of-Flight detector to accumulate high-resolution energy spectrum.

Result: Successfully calculated temperature and angular distribution of deuterium ions transported in plasma, with results well verified by particle-in-cell simulations.

Conclusion: This method provides a new way for picosecond time-integrated ion dynamics diagnosis in plasma, with potential applications in understanding ion transport processes and laser plasma ion acceleration studies.

Abstract: The ultrashort time-integrated diagnosis of ions plays a vital role in high
energy density physics research. However, it is extremely challenging to
measure in experiment. Here, we demonstrate a reliable approach for
investigating the dynamics of deuterium ions in dense plasma. By irradiating a
heavy water stream with the hundred Hertz repetitive intense femtosecond laser
pulses, the neutrons from D(D,n)3He reaction can be detected via a single
Time-of-Flight detector to accumulate the spectrum with a fine
energy-resolution. This spectrum has been utilized to calculate the temperature
and angular distribution of deuterium ions transported in plasma. And the
calculated results are well verified by particle-in-cell simulations of
deuterium ions dynamics. Our method paves a new way for diagnosing ions
picoseconds time-integrated dynamics in plasma and holds great potential for
understanding the ions transport process in high-energy density matters and
studying laser plasma ion acceleration.

</details>


### [92] [Simulation study of neutral tungsten emissions for fusion applications](https://arxiv.org/abs/2509.00878)
*Ritu Dey,Ayushi Agrawal,Reetesh Kumar Gangwar,Deepti Sharma,Rajesh Srivastava,Malay B. Chowdhuri,Joydeep Ghosh*

Main category: physics.plasm-ph

TL;DR: Electron-impact excitation cross-sections and rate coefficients for neutral tungsten transitions calculated using relativistic distorted wave approach.


<details>
  <summary>Details</summary>
Motivation: To provide accurate electron-impact excitation data for neutral tungsten transitions relevant to tokamak plasma observations.

Method: Used relativistic distorted wave approach within flexible atomic code, with energy levels corrected to match NIST database. Computed cross-sections for electron energies up to 30 keV.

Result: Calculated excitation cross-sections and rate coefficients for three tungsten transitions (400.87 nm, 429.46 nm, and 430.21 nm).

Conclusion: Provides valuable atomic data for tungsten transitions observed in tokamak plasmas, with NIST-corrected energy levels ensuring accuracy.

Abstract: The article reports electron-impact excitation cross-sections and rate
coefficients for neutral tungsten for three transitions (400.87 nm, 429.46 nm,
and 430.21 nm) using the relativistic distorted wave approach within the
flexible atomic code. Some of these lines are also observed in tokamak plasma.
Cross-sections are computed for incident electron energy up to 30 keV. The
energy levels in flexible atomic code were corrected to match the NIST
database. The electron impact excitation rate coefficients are also provided.

</details>


### [93] [Local extraction of three-dimensional magnetic reconnection X-lines](https://arxiv.org/abs/2509.00915)
*Maximilian M. Richter,Patricio A. Muñoz,Felix Spanier*

Main category: physics.plasm-ph

TL;DR: New framework for identifying magnetic reconnection in 3D turbulent plasmas using bifurcation lines as X-lines and introducing quasi X-lines for guide field configurations, with local reconnection rate estimation and magnetic shear layer measurement.


<details>
  <summary>Details</summary>
Motivation: Identifying fast magnetic reconnection events in 3D turbulent plasma environments remains challenging, requiring new methods beyond traditional global approaches and electric field/current density reliance.

Method: Applied bifurcation lines from fluid visualization to magnetic fields to identify X-lines, introduced quasi X-lines for guide field configurations, developed local reconnection rate estimation technique, and measured magnetic shear layers as second invariant of shear strain tensor.

Result: Obtained reconnection rate distribution with local maximum near normalized value 0.1, validated across various plasma simulation models including kinetic PIC and resistive MHD.

Conclusion: The framework provides efficient tools for quantitative study of magnetic reconnection in complex magnetic field topologies, enabling better exploration of magnetic field dynamics in turbulent plasma environments.

Abstract: Magnetic reconnection is one of the most important magnetic energy conversion
processes observed in laboratory and space plasmas. It describes the breaking
and joining of magnetic field lines, leading to the release of magnetic energy
and the acceleration of charged particles. Finding regions where fast
reconnection occurs is key to understanding this process. However, identifying
such reconnection events within a turbulent environment in three dimensions
remains a challenge. In this work, we develop a new framework for identifying
magnetic reconnection using 3D turbulent plasma simulations. First, we apply
bifurcation lines from fluid visualization to magnetic fields and show that
they can be identified with X-lines of magnetic reconnection. For reconnection
configurations with magnetic guide fields, we introduce a novel concept of
quasi X-lines (QXL). Using the spatial information of X-lines in numerical
simulations, we present a local technique to estimate the reconnection rate,
obtaining a distribution that features a local maximum near the normalized
value 0.1. Additionally, we provide an alternative tool to highlight current
sheets in turbulent plasma by measuring magnetic shear layers as the second
invariant of the shear strain tensor. These methods, avoiding traditional
reliance on global methods, electric fields and current density, offer a new
perspective to the quantitative study of magnetic reconnection in plasmas with
complex magnetic field topologies. Validated across various plasma simulation
models, including kinetic particle-in-cell (PIC) and resistive
magnetohydrodynamics (MHD), our approach enables efficient exploration of
magnetic field dynamics in turbulent plasma environments.

</details>


### [94] [Multi-diagnostic characterization of inductively coupled discharges with tailored waveform substrate bias for precise control of plasma etching](https://arxiv.org/abs/2509.01171)
*Jonas Giesekus,Anton Pletzer,Florian Beckfeld,Katharina Noesges,Claudia Bock,Julian Schulze*

Main category: physics.plasm-ph

TL;DR: Low frequency tailored pulse-wave-shaped bias voltage enables precise control of ion energy distribution for selective etching of silicon dioxide and amorphous silicon with monoenergetic ions.


<details>
  <summary>Details</summary>
Motivation: Precise control of ion energy distribution functions is crucial for selectivity, sputter rate control, and minimizing substrate damage in nanoscale plasma processes.

Method: Applied 100 kHz tailored pulse-wave-shaped bias voltage to substrate electrode in ICP reactor, investigated effects on IEDF, electron density, dynamics, and etch rates of SiO2 and a-Si in commercial 200mm RIE reactor.

Result: Achieved monoenergetic IEDFs with FWHM below 10 eV (20-100 eV range) in both argon and SF6. Determined Ar ion sputter thresholds: 23 eV for a-Si and 37 eV for SiO2. Enabled selective etching by tailoring IEDF within narrow energy window.

Conclusion: Tailored waveform substrate bias provides precise ion energy control for selective material processing without affecting electron density or power absorption dynamics, enabling new capabilities in nanoscale plasma etching.

Abstract: Precise control of ion energy distribution functions (IEDF) is crucial for
selectivity as well as control over sputter rate and substrate damage in
nanoscale plasma processes. In this work, a low frequency (100 kHz) tailored
pulse-wave-shaped bias voltage waveform is applied to the substrate electrode
of an inductively coupled plasma (ICP) and its effects on the IEDF, electron
density, electron dynamics and the etch rates of silicon dioxide as well as
amorphous silicon are investigated in a commercial 200 mm reactive ion etching
(RIE) reactor. While the tailored waveform substrate bias hardly affects the
electron density above the substrate and the spatio-temporally resolved
electron power absorption dynamics, it is found to affect the ion flux to the
substrate at high ICP source powers. Monoenergetic IEDFs with a full width at
half maximum (FWHM) below 10 eV are realized with mean ion energies ranging
from 20 eV to 100 eV in both argon and SF6. Such monoenergetic IEDFs are used
to determine the Ar ion sputter threshold energies of amorphous silicon and
silicon dioxide to be 23 eV and 37 eV, respectively, and to realize selective
etching of these two materials by Ar ion sputtering based on tailoring the IEDF
to ensure that all incident ions are within this narrow ion energy selectivity
window.

</details>


### [95] [Ionization/dissociation-driven first order phase transitions: on a new class of first order phase transitions](https://arxiv.org/abs/2509.01273)
*Genri Norman,Ilnur Saitov*

Main category: physics.plasm-ph

TL;DR: Comparison of plasma chemical models and DFT simulations reveals plasma nature of hydrogen's phase transition to conducting state, proposing a new class of ionization-driven first-order phase transitions.


<details>
  <summary>Details</summary>
Motivation: To understand the nature of phase transition in fluid hydrogen when it transforms into a conducting state, and to compare different modeling approaches for this phenomenon.

Method: Compared various models including chemical models of plasma and first-principle simulations using density functional theory (DFT) to describe hydrogen's phase transition.

Result: Results indicate the plasma nature of the phase transition in warm dense hydrogen, suggesting it belongs to a new class of first-order phase transitions.

Conclusion: Proposed concept of ionization or dissociation-driven phase transitions as a new class, with plasma phase transition in fluid hydrogen being associated with this category.

Abstract: In this work, we compare various models for describing the phase transition
of the fluid hydrogen into a conducting state, including both chemical models
of plasma and first-principle simulations within the framework of the density
functional theory (DFT). The comparison of the results indicates the plasma
nature of the phase transition in warm dense hydrogen. We propose a concept of
a new class of first-order phase transitions: ionization or dissociation-driven
phase transitions, with which the plasma phase transition in fluid hydrogen can
be associated.

</details>


### [96] [Effects of reversed magnetic shear on the plasma rotation stabilization of resistive wall modes in tokamaks](https://arxiv.org/abs/2509.01502)
*Sui Wan,Ping Zhu,Linjin Zheng*

Main category: physics.plasm-ph

TL;DR: Reversed magnetic shear worsens resistive wall mode stabilization in tokamaks, requiring higher rotation speeds and narrowing stability windows compared to positive shear configurations.


<details>
  <summary>Details</summary>
Motivation: To understand how reversed magnetic shear affects plasma rotation stabilization of resistive wall modes (RWMs) in tokamak fusion devices, particularly for advanced configurations like CFETR.

Method: Used AEGIS code to analyze MHD equilibria from circular cross-sections to realistic CFETR-like scenarios with various magnetic shear profiles, examining n=1 RWM stability under toroidal rotation.

Result: Strongly reversed magnetic shear broadens the unstable beta_N window and significantly reduces rotational stabilization, requiring notably higher toroidal rotation frequency thresholds for complete RWM suppression.

Conclusion: Reversed magnetic shear imposes more stringent requirements for effective toroidal rotation stabilization of n=1 RWMs in advanced tokamak configurations.

Abstract: Effects of reversed magnetic shear on the plasma rotation stabilization of
resistive wall modes in tokamaks are investigated using the AEGIS code. MHD
equilibria in toroidal configuration from circular cross-sections to realistic
CFETR-like scenarios with various magnetic shear profiles are considered. Two
critical aspects of the $n=1$ RWM are examined: the influence of toroidal
rotation on the unstable regime and the toroidal rotation frequency thresholds
required for complete stabilization. It is found that strongly reversed
magnetic shear consistently broadens the unstable $\beta_{\rm N}$ window in
both circular and CFETR equilibria when toroidal rotation is included.
Furthermore, reversed magnetic shear significantly reduces the rotational
stabilization, resulting in narrower stability windows and notably higher
toroidal rotation frequency thresholds required for complete RWM suppression
compared to the cases with positive shear only. These results clearly
demonstrate that the reversed magnetic shear in the advanced tokamak
configuration imposes more stringent requirement for the effective toroidal
rotation stabilization of the $n=1$ RWM.

</details>


### [97] [Forward and Backward Electron Acceleration by Radially Polarized Ultra-Intense Laser Focus Seeded By Field Ionization of High Charge States of Neon](https://arxiv.org/abs/2509.01741)
*Nour El Houda Hissi,Gregory K. Ngirmang,Joseph Smith,Enam A. Chowdhury*

Main category: physics.plasm-ph

TL;DR: Computational study shows backward electron acceleration in radially polarized laser beams due to strong longitudinal electric fields, with GeV energies achieved for inner shell neon electrons.


<details>
  <summary>Details</summary>
Motivation: With advances in large aperture phase optics enabling ultra-intense laser plasma experiments with complex polarization states, this research investigates electron acceleration phenomena in radially polarized Petawatt-class lasers.

Method: Numerically solved relativistically invariant Lorentz equations with semi-classical tunneling ionization and Monte Carlo sampling of focal volume. Conducted parametric study of laser parameters and performed 3D particle-in-cell (PIC) simulations for validation.

Result: Electrons reached GeV energies, particularly those ionized from neon inner shells near pulse peak. Counterintuitive backward acceleration observed due to radial polarization creating strong longitudinal electric field Ez that initially pushes electrons backward when ionized at specific phases.

Conclusion: Radially polarized laser beams produce unique backward electron acceleration phenomena through strong longitudinal electric fields, with significant energy gains observed at longer wavelengths, opening new possibilities for laser-plasma interaction experiments.

Abstract: Thanks to the fabrication of large aperture phase optics, ultra-intense
relativistic laser plasma interaction (RLPI) experiments with complex
polarization states are becoming feasible. In this work, we perform a
computational investigation of direct acceleration of electrons produced during
ionization of underdense neon gas using a tightly focused and radially
polarized Petawatt-class short pulse lasers by numerically solving the
relativistically invariant Lorentz equations, incorporating semi-classical
tunneling ionization and Monte Carlo type sampling of the focal volume. The
accelerated electrons energy gain increases at longer laser wavelengths and GeV
energies are reached for electrons ionized from the neon inner shells, which
are field ionized near the peak of the pulse. Backward acceleration of
electrons is observed for a range of initial positions and phases of ionization
of neon charge states. This apparent counterintuitive phenomenon is directly
linked to the radial polarization state of the incident laser beam that results
in a strong longitudinal electric field Ez when tightly focused, where
electrons ionized near the focal center at the phase when Ez is pointed toward
the forward propagation direction experiences an initial push in the backward
direction. A parametric study of the phenomenon by varying laser parameters is
presented, and a 3D particle in cell (PIC) simulation is considered to confirm
the existence of this phenomenon.

</details>


### [98] [Real-Time Applicability of Emulated Virtual Circuits for Tokamak Plasma Shape Control](https://arxiv.org/abs/2509.01789)
*Pedro Cavestany,Alasdair Ross,Adriano Agnello,Aran Garrod,Nicola C. Amorisco,George K. Holt,Kamran Pentland,James Buchanan*

Main category: physics.plasm-ph

TL;DR: Machine learning emulators for tokamak plasma control achieve 5-10% accuracy with neural networks trained on 100k-1M synthetic equilibria, and vessel currents can be inferred via linear regression with few-Ampere residuals.


<details>
  <summary>Details</summary>
Motivation: To improve real-time magnetic control of tokamak plasmas by quantifying emulator accuracy and addressing unmeasured vessel currents for robust shape control.

Method: Used neural network emulators with ~100k parameters trained on large synthetic equilibrium datasets, compared Jacobians with finite differences on exact solutions, and employed linear regression on trailing coil current measurements to infer shaping currents.

Result: Achieved 5-10% agreement on geometric targets, demonstrated that shaping currents can be inferred with few-Ampere residuals, and showed emulators can achieve few-millisecond latency.

Conclusion: Machine learning emulators can be developed for robust real-time plasma shape control in existing and future tokamaks with appropriate training data and current inference methods.

Abstract: Machine learning has recently been adopted to emulate sensitivity matrices
for real-time magnetic control of tokamak plasmas. However, these approaches
would benefit from a quantification of possible inaccuracies. We report on two
aspects of real-time applicability of emulators. First, we quantify the
agreement of target displacement from VCs computed via Jacobians of the shape
emulators with those from finite differences Jacobians on exact Grad-Shafranov
solutions. Good agreement ($\approx$5-10%) can be achieved on a selection of
geometric targets using combinations of neural network emulators with
$\approx10^5$ parameters. A sample of $\approx10^{5}-10^{6}$ synthetic
equilibria is essential to train emulators that are not over-regularised or
overfitting. Smaller models trained on the shape targets may be further
fine-tuned to better fit the Jacobians. Second, we address the effect of vessel
currents that are not directly measured in real-time and are typically subsumed
into effective "shaping currents" when designing virtual circuits. We
demonstrate that shaping currents can be inferred via simple linear regression
on a trailing window of active coil current measurements with residuals of only
a few Amp\`eres, enabling a choice for the most appropriate shaping currents at
any point in a shot. While these results are based on historic shot data and
simulations tailored to MAST-U, they indicate that emulators with
few-millisecond latency can be developed for robust real-time plasma shape
control in existing and upcoming tokamaks.

</details>


### [99] [Hot-Ion Modes in Globus-M2 and Saturation of Energy Confinement Time Scaling in Spherical Tokamaks with Toroidal Magnetic Field of 1 T and Above](https://arxiv.org/abs/2509.02214)
*G. S. Kurskiev,V. B. Minaev,N. V. Sakharov,V. K. Gusev,Yu. V. Petrov,I. V. Miroshnikov,F. V. Chernyshev,N. N. Bakharev,E. O. Kiselev,A. Yu. Telnova,E. E. Tkachenko,N. S. Zhiltsov,Globus- M2 Team*

Main category: physics.plasm-ph

TL;DR: Spherical tokamaks can achieve extremely high ion temperatures (up to 50 million K) with proper torque transfer and magnetic field optimization, but electron thermal insulation improvements plateau above 1T magnetic fields.


<details>
  <summary>Details</summary>
Motivation: To explore how spherical tokamaks can achieve extremely high ion temperatures through optimized magnetic field configurations and turbulence control for improved plasma confinement.

Method: Experimental study using the Globus-M2 spherical tokamak with minor radius of 0.22m, varying toroidal magnetic fields from 0.5T to 1T, and comparing results with ST40 tokamak operating at 2T. High-energy atomic beam heating was used to study plasma thermal energy confinement.

Result: Significant improvement in plasma thermal energy confinement observed when increasing toroidal magnetic field from 0.5T to 1T. Ion temperatures up to 50 million Kelvin achieved with proper torque transfer to stabilize turbulence. However, comparison with ST40 at 2T indicates no further strong improvement in thermal insulation at higher magnetic fields.

Conclusion: Spherical tokamaks can reach extremely high ion temperatures through optimized magnetic fields and turbulence control, but electron thermal insulation improvements reach a plateau around 1T, suggesting diminishing returns for further magnetic field increases.

Abstract: In a small spherical tokamak with minor radius of 0.22 m and toroidal
magnetic field of 1 T, it is possible to heat ions of a sufficiently dense
plasma to an extremely high temperature up to 50 million Kelvin. To do this, it
is necessary to transfer a sufficiently large torque to stabilize ion-scale
turbulence and achieve ion heat transport at neoclassical level reaching
extremely low values of plasma collisionality. It is also necessary to ensure
good thermal insulation of electrons, which is always determined by turbulent
transport. In a spherical tokamak, the toroidal magnetic field has a strong
beneficial effect on suppressing turbulent electron heat fluxes. In the
Globus-M2 tokamak, when heating plasma with high-energy atomic beams, a
significant improvement in the plasma thermal energy confinement is observed
with an increase in the toroidal magnetic field from 0.5 to 1 T. The comparison
of our results with experiments on ST40 tokamak operating with toroidal
magnetic field of 2 T indicates that further strong improvement of thermal
insulation of spherical tokamak plasma in the region of higher magnetic fields
is not expected.

</details>


### [100] [One--Component Plasma Equation of State Revisited via Angular--Averaged Ewald Potential](https://arxiv.org/abs/2509.02390)
*G. S. Demyanov,P. R. Levashov*

Main category: physics.plasm-ph

TL;DR: Analytic fits for classical one-component plasma internal energy using Monte-Carlo data across coupling parameters Γ=0.01-170, extending previous datasets and validating two functional forms.


<details>
  <summary>Details</summary>
Motivation: To provide accurate analytical fits for one-component plasma internal energy over a wide range of coupling strengths, building upon previous Monte-Carlo data and ensuring correct weak-coupling behavior.

Method: Extended Monte-Carlo dataset using angular-averaged Ewald potential with additional strong coupling points (Γ=120,150,170), then fitted two established functional forms: Caillol's 5-parameter equation and Potekhin-Chabrier equation that enforces Debye-Hückel limit.

Result: Fits reproduce MC data within statistical uncertainties while recovering correct weak-coupling behavior. Provided coefficients, validity ranges, and comparisons with prior analytical and simulation results.

Conclusion: Successfully developed accurate analytic fits for OCP internal energy across broad coupling parameter range, validating both functional forms against extended Monte-Carlo dataset and ensuring proper asymptotic behavior.

Abstract: We present analytic fits of classical one--component plasma (OCP) internal
energy over a wide range of coupling parameter $0.01\le\Gamma\le 170$ using
Monte--Carlo data in the thermodynamic limit. We extend the dataset obtained in
[Demyanov and Levashov, Phys. Rev. E 106, 015204 (2022)] using the
angular--averaged Ewald potential with additional points at strong coupling
($\Gamma=120,\ 150,\ 170$). We then fit two frequently used functional forms
for the OCP equation of state: (i) a five-parameter equation by Caillol [J.
Chem. Phys. 111, 6538--6547 (1999)] and (ii) the equation by Potekhin and
Chabrier [Phys. Rev. E 62, 8554 (2000)] that enforces the Debye--H\"uckel
limit. The presented fits reproduce our MC data within statistical
uncertainties, recovering the correct weak-coupling behavior. Coefficients,
recommended validity ranges, and comparisons to prior analytical and simulation
results are provided.

</details>


### [101] [SOLPS-ITER Numerical Simulations of ITER-scale Snowflake Divertors: Low-Field-Side SF-/SF+ and High-Field-Side SF-/SF+ Configurations](https://arxiv.org/abs/2509.02455)
*H. S. Wu,F. Subba,M. R. K. Wigram,O. Pan,R. Lo Frano,A. Pucciarelli,R. Zanino*

Main category: physics.plasm-ph

TL;DR: Study of four Snowflake divertor configurations for ITER-size tokamak shows LFS SF- divertor compresses recycling neutrals causing outer target detachment, while SF+ configurations show potential for X-point radiator formation.


<details>
  <summary>Details</summary>
Motivation: Provide insights on Snowflake divertor design for future fusion devices by investigating impacts of magnetic geometry and divertor target geometry on plasma behavior and power exhaust performance.

Method: Used SOLS-ITER edge plasma code to simulate four SF configurations with systematic variation of secondary X-point positions and comparison of flat vs ITER-like divertor shapes through upstream density scans.

Result: LFS SF- divertor with closed inner target structure compresses recycling neutrals into LFS SOL/PFR regions, causing volumetric dissipation and outer target detachment. SF+ configurations show strong ionization/recombination near primary X-point, potentially beneficial for X-point radiator formation.

Conclusion: LFS SF- divertor design shows promising detachment characteristics, while SF+ configurations may enable X-point radiator formation, though impurity seeding studies are needed for further validation.

Abstract: With edge plasma code SOLS-ITER, we study four Snowflake (SF) configurations
for an ITER-size tokamak, with toroidal magnetic field BT=5T, major radius R=5m
and plasma current Ip=10MA. Our aim is to provide insights on SF divertor
design for future devices. In this work, the impacts of magnetic geometry and
divertor target geometry in the four types of SF configurations on plasma
behavior and power exhaust performance are investigated in detail.
Low-recycling regime, high-recycling and detachment in the four types of SF
divertors are obtained through an upstream density scan. The secondary X-point
positions of SF divertors are systematically varied to examine their impact.
For Low-Field-Side (LFS) SF- and High-Field-Side (HFS) SF- divertors the
observed power splitting, induced by the secondary X-point, is consistent with
experimental observations. The effect of target geometry is studied by
comparing the flat target plates with the ITER-like divertor shape. The overall
simulation results reveal a notable consequence of the LFS SF- divertor: closed
structure of the inner target with high inclined plate can compress recycling
neutrals originating from the HFS divertor region into the LFS SOL and PFR
regions. This results in considerable volumetric dissipation through strong
ionization and recombination, causing the connected outer target region to
detach. This feature can be considered in the design of the LFS SF- divertor
for future devices. For the LFS and HFS SF+ divertors, the region between the
two X-points exhibits strong ionization and recombination sources close to the
primary X-point. This feature might be beneficial for the formation of an
X-point radiator, but would require further impurity seeding simulation study.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [102] [Equivariant U-Shaped Neural Operators for the Cahn-Hilliard Phase-Field Model](https://arxiv.org/abs/2509.01293)
*Xiao Xue,M. F. P. ten Eikelder,Tianyue Yang,Yiqing Li,Kan He,Shuo Wang,Peter V. Coveney*

Main category: cs.LG

TL;DR: E-UNO: equivariant U-shaped neural operator for phase separation modeling that combines spectral convolution with multi-resolution architecture and translation equivariance, outperforming standard neural operators.


<details>
  <summary>Details</summary>
Motivation: Numerical solvers for Cahn-Hilliard equation are computationally expensive and lack flexibility, while current neural operators fail to capture multiscale behavior and physical symmetries.

Method: Uses equivariant U-shaped neural operator with global spectral convolution, multi-resolution architecture, and translation equivariance regulation to learn phase-field evolution from short historical dynamics.

Result: Outperforms standard Fourier neural operator and U-shaped neural operator baselines, especially on fine-scale and high-frequency structures, with better generalization and less training data.

Conclusion: E-UNO establishes an efficient surrogate for complex phase-field systems by encoding symmetry and scale hierarchy for physically consistent dynamics.

Abstract: Phase separation in binary mixtures, governed by the Cahn-Hilliard equation,
plays a central role in interfacial dynamics across materials science and soft
matter. While numerical solvers are accurate, they are often computationally
expensive and lack flexibility across varying initial conditions and
geometries. Neural operators provide a data-driven alternative by learning
solution operators between function spaces, but current architectures often
fail to capture multiscale behavior and neglect underlying physical symmetries.
Here we show that an equivariant U-shaped neural operator (E-UNO) can learn the
evolution of the phase-field variable from short histories of past dynamics,
achieving accurate predictions across space and time. The model combines global
spectral convolution with a multi-resolution U-shaped architecture and
regulates translation equivariance to align with the underlying physics. E-UNO
outperforms standard Fourier neural operator and U-shaped neural operator
baselines, particularly on fine-scale and high-frequency structures. By
encoding symmetry and scale hierarchy, the model generalizes better, requires
less training data, and yields physically consistent dynamics. This establishes
E-UNO as an efficient surrogate for complex phase-field systems.

</details>


### [103] [LUCIE-3D: A three-dimensional climate emulator for forced responses](https://arxiv.org/abs/2509.02061)
*Haiwen Guan,Troy Arcomano,Ashesh Chattopadhyay,Romit Maulik*

Main category: cs.LG

TL;DR: LUCIE-3D is a lightweight 3D climate emulator that captures atmospheric vertical structure, responds to climate forcings like CO2, and maintains computational efficiency while reproducing key climate patterns and dynamics.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient 3D climate model that captures vertical atmospheric structure and responds to climate change forcings while maintaining computational efficiency for rapid experimentation and research applications.

Method: Built on LUCIE-2D framework using Spherical Fourier Neural Operator (SFNO) backbone, trained on 30 years of ERA5 reanalysis data across 8 vertical σ-levels, with CO2 as forcing variable and optional sea surface temperature integration.

Result: Successfully reproduces climatological means, variability, long-term climate change signals (surface warming, stratospheric cooling), captures key dynamical processes (equatorial Kelvin waves, MJO, annular modes), and shows credible extreme event statistics.

Conclusion: LUCIE-3D provides a stable, physically consistent, and accessible tool for rapid climate experimentation with applications in coupled climate dynamics, paleoclimate research, and future Earth system emulation, training efficiently in under 5 hours on 4 GPUs.

Abstract: We introduce LUCIE-3D, a lightweight three-dimensional climate emulator
designed to capture the vertical structure of the atmosphere, respond to
climate change forcings, and maintain computational efficiency with long-term
stability. Building on the original LUCIE-2D framework, LUCIE-3D employs a
Spherical Fourier Neural Operator (SFNO) backbone and is trained on 30 years of
ERA5 reanalysis data spanning eight vertical {\sigma}-levels. The model
incorporates atmospheric CO2 as a forcing variable and optionally integrates
prescribed sea surface temperature (SST) to simulate coupled ocean--atmosphere
dynamics. Results demonstrate that LUCIE-3D successfully reproduces
climatological means, variability, and long-term climate change signals,
including surface warming and stratospheric cooling under increasing CO2
concentrations. The model further captures key dynamical processes such as
equatorial Kelvin waves, the Madden--Julian Oscillation, and annular modes,
while showing credible behavior in the statistics of extreme events. Despite
requiring longer training than its 2D predecessor, LUCIE-3D remains efficient,
training in under five hours on four GPUs. Its combination of stability,
physical consistency, and accessibility makes it a valuable tool for rapid
experimentation, ablation studies, and the exploration of coupled climate
dynamics, with potential applications extending to paleoclimate research and
future Earth system emulation.

</details>


### [104] [A Class of Random-Kernel Network Models](https://arxiv.org/abs/2509.01090)
*James Tian*

Main category: cs.LG

TL;DR: Random-kernel networks use deterministic kernel composition for depth with randomness only in the outermost layer, showing deeper networks can approximate functions with fewer samples than shallow ones.


<details>
  <summary>Details</summary>
Motivation: To explore whether deeper neural network architectures can achieve better sample efficiency than shallow networks when using random feature models, specifically examining if depth provides advantages in Monte Carlo sampling requirements.

Method: Develop multilayer random-kernel networks where depth is created through deterministic kernel composition, with randomness introduced only in the outermost layer. Prove theoretical results comparing sample complexity between deep and shallow architectures.

Result: Established a depth separation theorem showing that deeper random-kernel network constructions can approximate certain functions with fewer Monte Carlo samples than any shallow counterpart.

Conclusion: Depth in neural network architectures provides concrete advantages in sample complexity, with deeper random-kernel networks demonstrating superior efficiency over shallow models for function approximation tasks.

Abstract: We introduce random-kernel networks, a multilayer extension of random feature
models where depth is created by deterministic kernel composition and
randomness enters only in the outermost layer. We prove that deeper
constructions can approximate certain functions with fewer Monte Carlo samples
than any shallow counterpart, establishing a depth separation theorem in sample
complexity.

</details>


### [105] [Efficient Transformer-Inspired Variants of Physics-Informed Deep Operator Networks](https://arxiv.org/abs/2509.01679)
*Zhi-Feng Wei,Wenqian Chen,Panos Stinis*

Main category: cs.LG

TL;DR: Transformer-inspired DeepONet variants with bidirectional cross-conditioning between branch and trunk networks achieve improved accuracy and training efficiency compared to existing DeepONet architectures.


<details>
  <summary>Details</summary>
Motivation: To develop more efficient and accurate operator learning frameworks for PDEs that maintain the simplicity of vanilla DeepONet while achieving the accuracy of modified DeepONet through better information exchange between network components.

Method: Proposed Transformer-inspired DeepONet variants that introduce bidirectional cross-conditioning - injecting query-point information into branch network and input-function information into trunk network, enabling dynamic dependencies while preserving efficiency.

Result: Experiments on four PDE benchmarks (advection, diffusion-reaction, Burgers', Korteweg-de Vries equations) show variants match or surpass modified DeepONet accuracy with improved training efficiency. Best variant for each equation aligns with equation characteristics.

Conclusion: Cross-conditioning effectiveness depends on equation characteristics and underlying physics. The proposed variants provide robust, efficient, and accurate alternatives to existing DeepONet architectures, validated through rigorous statistical analyses.

Abstract: Operator learning has emerged as a promising tool for accelerating the
solution of partial differential equations (PDEs). The Deep Operator Networks
(DeepONets) represent a pioneering framework in this area: the "vanilla"
DeepONet is valued for its simplicity and efficiency, while the modified
DeepONet achieves higher accuracy at the cost of increased training time. In
this work, we propose a series of Transformer-inspired DeepONet variants that
introduce bidirectional cross-conditioning between the branch and trunk
networks in DeepONet. Query-point information is injected into the branch
network and input-function information into the trunk network, enabling dynamic
dependencies while preserving the simplicity and efficiency of the "vanilla"
DeepONet in a non-intrusive manner. Experiments on four PDE benchmarks --
advection, diffusion-reaction, Burgers', and Korteweg-de Vries equations --
show that for each case, there exists a variant that matches or surpasses the
accuracy of the modified DeepONet while offering improved training efficiency.
Moreover, the best-performing variant for each equation aligns naturally with
the equation's underlying characteristics, suggesting that the effectiveness of
cross-conditioning depends on the characteristics of the equation and its
underlying physics. To ensure robustness, we validate the effectiveness of our
variants through a range of rigorous statistical analyses, among them the
Wilcoxon Two One-Sided Test, Glass's Delta, and Spearman's rank correlation.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [106] [TransMatch: A Transfer-Learning Framework for Defect Detection in Laser Powder Bed Fusion Additive Manufacturing](https://arxiv.org/abs/2509.01754)
*Mohsen Asghari Ilani,Yaser Mike Banad*

Main category: cs.CV

TL;DR: TransMatch framework combines transfer learning and semi-supervised few-shot learning to detect surface defects in LPBF additive manufacturing with high accuracy using limited labeled data.


<details>
  <summary>Details</summary>
Motivation: Surface defects in Laser Powder Bed Fusion pose significant risks to structural integrity, and there's a scarcity of labeled AM defect data that limits traditional detection methods.

Method: TransMatch merges transfer learning and semi-supervised few-shot learning to leverage both labeled and unlabeled novel-class images, overcoming limitations of previous meta-learning approaches.

Result: Achieved 98.91% accuracy with minimal loss on 8,284 image dataset, with high precision, recall, and F1-scores for multiple defect classes including cracks, pinholes, holes, and spatter.

Conclusion: TransMatch represents a significant advancement in additive manufacturing defect detection, offering a practical and scalable solution for quality assurance across industrial applications.

Abstract: Surface defects in Laser Powder Bed Fusion (LPBF) pose significant risks to
the structural integrity of additively manufactured components. This paper
introduces TransMatch, a novel framework that merges transfer learning and
semi-supervised few-shot learning to address the scarcity of labeled AM defect
data. By effectively leveraging both labeled and unlabeled novel-class images,
TransMatch circumvents the limitations of previous meta-learning approaches.
Experimental evaluations on a Surface Defects dataset of 8,284 images
demonstrate the efficacy of TransMatch, achieving 98.91% accuracy with minimal
loss, alongside high precision, recall, and F1-scores for multiple defect
classes. These findings underscore its robustness in accurately identifying
diverse defects, such as cracks, pinholes, holes, and spatter. TransMatch thus
represents a significant leap forward in additive manufacturing defect
detection, offering a practical and scalable solution for quality assurance and
reliability across a wide range of industrial applications.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [107] [AS-BOX: Additional Sampling Method for Weighted Sum Problems with Box Constraints](https://arxiv.org/abs/2509.00547)
*Nataša Krejić,Nataša Krklec Jerinkić,Tijana Ostojić,Nemanja Vučićević*

Main category: math.OC

TL;DR: AS-BOX is a stochastic optimization method for box-constrained weighted finite-sum problems that combines projected gradients with adaptive batch sizing and nonmonotone line search, ensuring convergence and competitive performance.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient stochastic optimization method for box-constrained problems that can dynamically adjust sample sizes while maintaining theoretical guarantees and practical performance.

Method: Combines projected gradient directions with adaptive variable sample size strategies and nonmonotone line search. Dynamically adjusts batch size based on progress with the additional sampling function and structural consistency of the projected direction.

Result: The method establishes almost sure convergence under standard assumptions and provides complexity bounds. Numerical experiments show efficiency and competitiveness compared to state-of-the-art algorithms.

Conclusion: AS-BOX provides a theoretically sound and practically effective approach for stochastic optimization with box constraints, demonstrating both convergence guarantees and competitive numerical performance.

Abstract: A class of optimization problems characterized by a weighted finite-sum
objective function subject to box constraints is considered. We propose a novel
stochastic optimization method, named AS-BOX (\text{A}ddi\-ti\-onal
\text{S}ampling for \text{BOX} constraints), that combines projected gradient
directions with adaptive variable sample size strategies and nonmonotone line
search. The method dynamically adjusts the batch size based on progress with
respect to the additional sampling function and on structural consistency of
the projected direction, enabling practical adaptivity of AS-BOX, while
ensuring theoretical support. We establish almost sure convergence under
standard assumptions and provide complexity bounds. Numerical experiments
demonstrate the efficiency and competitiveness of the proposed method compared
to state-of-the-art algorithms.

</details>


### [108] [Active-Set Identification in Noisy and Stochastic Optimization](https://arxiv.org/abs/2509.00888)
*Frank E. Curtis,Daniel P. Robinson,Lara Zebiane*

Main category: math.OC

TL;DR: Active-set identification for constrained optimization extended to noisy function and derivative evaluations, with theoretical guarantees and practical demonstrations.


<details>
  <summary>Details</summary>
Motivation: Traditional active-set identification methods require exact function and derivative values and assume smoothness and constraint qualifications. This work addresses the practical need to handle noisy evaluations in real-world optimization problems.

Method: Proposed two strategies for active-set identification that work with deterministic or stochastic noise in both objective and constraint functions and their derivatives. Theoretical analysis under mild conditions shows correct identification when close to local minimizer with small noise.

Result: The strategies successfully identify active constraints near optimal solutions even with noise. Guarantees are provided for use within stochastic algorithms. Demonstrated effectiveness through simple examples and a constrained neural-network training task.

Conclusion: The proposed methods extend active-set identification to noisy optimization settings, providing theoretical foundations and practical utility for real-world constrained optimization problems with imperfect function evaluations.

Abstract: Identifying active constraints from a point near an optimal solution is
important both theoretically and practically in constrained continuous
optimization, as it can help identify optimal Lagrange multipliers and
essentially reduces an inequality-constrained problem to an
equality-constrained one. Traditional active-set identification guarantees have
been proved under assumptions of smoothness and constraint qualifications, and
assume exact function and derivative values. This work extends these results to
settings when both objective and constraint function and derivative values have
deterministic or stochastic noise. Two strategies are proposed that, under mild
conditions, are proved to identify the active set of a local minimizer
correctly when a point is close enough to the local minimizer and the noise is
sufficiently small. Guarantees are also stated for the use of active-set
identification strategies within a stochastic algorithm. We demonstrate our
findings with two simple illustrative examples and a more realistic constrained
neural-network training task.

</details>


### [109] [Convergence Rates of Time Discretization in Extended Mean Field Control](https://arxiv.org/abs/2509.00904)
*Christoph Reisinger,Wolfgang Stockinger,Maria Olympia Tsianni,Yufei Zhang*

Main category: math.OC

TL;DR: Piecewise constant control approximation achieves order 1/2 cost approximation and order 1/4 control approximation for linear-convex extended mean field control problems, with improved first-order convergence under sufficient regularity.


<details>
  <summary>Details</summary>
Motivation: To analyze the accuracy of piecewise constant control approximations for extended mean field control problems where dynamics and costs depend on joint state-control distributions.

Method: Proving 1/2-Hölder continuity of optimal controls for linear-convex extended MFC problems, then using this regularity to establish approximation rates for optimal cost and control.

Result: For linear-convex extended MFC: optimal cost approximated with order 1/2, optimal control with order 1/4. For general extended MFC: value functions converge with first-order rate under sufficient regularity.

Conclusion: Piecewise constant approximations provide effective numerical schemes for extended MFC problems, with convergence rates matching classical control problems and consistent with numerical observations.

Abstract: Piecewise constant control approximation provides a practical framework for
designing numerical schemes of continuous-time control problems. We analyze the
accuracy of such approximations for extended mean field control (MFC) problems,
where the dynamics and costs depend on the joint distribution of states and
controls. For linear-convex extended MFC problems, we show that the optimal
control is $1/2$-H\"older continuous in time. Using this regularity, we prove
that the optimal cost of the continuous-time problem can be approximated by
piecewise constant controls with order $1/2$, while the optimal control itself
can be approximated with order $1/4$. For general extended MFC problems, we
further show that, under sufficient regularity of the value functions, the
value functions converge with an improved first-order rate, matching the
best-known rate for classical control problems without mean field interaction,
and consistent with the numerical observations for MFC of Cucker-Smale models.

</details>


### [110] [Dynamic control of stochastic matching systems in heavy traffic: An effective computational method for high-dimensional problems](https://arxiv.org/abs/2509.00809)
*Baris Ata,Yaosheng Xu*

Main category: math.OC

TL;DR: Proposes a deep neural network-based computational method for solving high-dimensional bipartite matching systems in heavy-traffic regimes, outperforming existing benchmarks.


<details>
  <summary>Details</summary>
Motivation: Bipartite matching systems with uncertainty require effective dynamic control policies to maximize value while minimizing congestion costs, but existing methods struggle with high-dimensional problems.

Method: Focuses on balanced, high-volume systems in heavy-traffic regime, derives approximating Brownian control problem, and develops computational method using deep neural networks.

Result: The proposed policy outperforms benchmark policies in test problems and is computationally feasible for dimensions up to 100 or more.

Conclusion: Deep neural network technology provides an effective and computationally feasible approach for solving high-dimensional bipartite matching control problems in heavy-traffic settings.

Abstract: Bipartite matching systems arise in many settings where agents or tasks from
two distinct sets must be paired dynamically under compatibility constraints.
We consider a high-dimensional bipartite matching system under uncertainty and
seek an effective dynamic control policy that maximizes the expected discounted
total value generated by the matches minus the congestion-related costs. To
derive a tractable approximation, we focus attention on balanced, high-volume
systems, i.e., the heavy-traffic regime, and derive an approximating Brownian
control problem. We then develop a computational method that relies on deep
neural network technology for solving this problem. To show the effectiveness
of the policy derived from our computational method, we compare it to the
benchmark policies available in the extant literature in the context of the
original matching problem. In the test problems attempted thus far, our
proposed policy outperforms the benchmarks, and its derivation is
computationally feasible for dimensions up to 100 or more.

</details>


### [111] [Cancer Detection via Electrical Impedance Tomography and Optimal Control of Elliptic PDEs](https://arxiv.org/abs/2509.02050)
*Ugur G. Abdulla,Jose H. Rodrigues*

Main category: math.OC

TL;DR: Computational method for early cancer detection using EIT and optimal control, increasing measurement data through permutations to improve tumor identification resolution.


<details>
  <summary>Details</summary>
Motivation: Early detection of cancerous tumors is critical for treatment success. The method leverages the fact that cancerous tissue has significantly higher electrical conductivity than healthy tissue, using Electrical Impedance Tomography (EIT) for non-invasive identification.

Method: Formulates an inverse EIT problem as a PDE-constrained optimal control problem. Introduces novel permutation approach to increase input data from m to m² measurements by adding "voltage-to-current" measurements. Uses gradient projection method (GPM) based on Fréchet differentiability in Besov-Hilbert spaces.

Result: Numerical simulations in 2D and 3D models demonstrate a sharp increase in resolution for identifying cancerous tumors when increasing the number of measurements from m to m² through the permutation approach.

Conclusion: The proposed permutation method effectively enhances tumor identification capabilities in EIT by significantly increasing measurement data while maintaining the same number of unknown parameters, providing improved resolution for early cancer detection.

Abstract: We pursue a computational analysis of the biomedical problem on the
identification of the cancerous tumor at an early stage of development based on
the Electrical Impedance Tomography (EIT) and optimal control of elliptic
partial differential equations. Relying on the fact that the electrical
conductivity of the cancerous tumor is significantly higher than the
conductivity of the healthy tissue, we consider an inverse EIT problem on the
identification of the conductivity map in the complete electrode model based on
the $m$ current-to-voltage measurements on the boundary electrodes. A
variational formulation as a PDE-constrained optimal control problem is
introduced based on the novel idea of increasing the size of the input data by
adding "voltage-to-current" measurements through various permutations of the
single "current-to-voltage" measurement. The idea of permutation preserves the
size of the unknown parameters on the expense of increase of the number of PDE
constraints. We apply a gradient projection (GPM) method based on the Fr\'echet
differentiability in Besov-Hilbert spaces. Numerical simulations of 2D and 3D
model examples demonstrate the sharp increase of the resolution of the
cancerous tumor by increasing the number of measurements from $m$ to $m^2$

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [112] [Interpolation-supplemented lattice Boltzmann simulation of thermal convection on non-uniform meshes](https://arxiv.org/abs/2509.01099)
*Ao Xu,Zheng Zhao,Ben-Rui Xu,Li-Sheng Jiang*

Main category: physics.flu-dyn

TL;DR: ISLBM extends lattice Boltzmann method with quadratic interpolation for buoyancy-driven thermal convection on non-uniform meshes, achieving high accuracy and 1-3 orders of magnitude higher efficiency compared to spectral element and finite volume methods.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient and accurate method for simulating thermal convection on non-uniform meshes while maintaining algorithmic simplicity and parallel scalability, particularly for high Rayleigh number flows.

Method: Interpolation-supplemented lattice Boltzmann method (ISLBM) incorporating quadratic interpolation during streaming step, enabling flexible mesh refinement near boundaries. Tested on 2D/3D side-heated cavities at Ra=10^6-10^8 and Ra=10^5-10^7 respectively with Pr=0.71.

Result: ISLBM accurately captures thermal/velocity boundary layers with close agreement to reference data. Achieves nearly third-order accuracy for global quantities, second-order for local fields. Shows 1-3 orders magnitude higher efficiency than Nek5000 and OpenFOAM. GPU performance reaches 60-70% of uniform mesh throughput.

Conclusion: ISLBM provides a robust foundation for high-fidelity thermal convection simulations on non-uniform meshes with excellent computational efficiency, demonstrating potential for future turbulent flow applications.

Abstract: We present a systematic evaluation of an interpolation-supplemented lattice
Boltzmann method (ISLBM) for simulating buoyancy-driven thermal convection on
non-uniform meshes. The ISLBM extends the standard lattice Boltzmann framework
by incorporating quadratic interpolation during the streaming step, enabling
flexible mesh refinement near solid boundaries while maintaining algorithmic
simplicity and parallel scalability. The method is implemented for a
two-dimensional side-heated cavity at high Rayleigh numbers $10^6 \leq Ra \leq
10^8$, and for a three-dimensional side-heated cavity at $10^5 \leq Ra \leq
10^7$, with the Prandtl number fixed at $Pr=0.71$. Benchmark results show that
the ISLBM accurately captures thermal and velocity boundary layers, yielding
Nusselt and Reynolds numbers in close agreement with high-fidelity reference
data. Grid-convergence studies demonstrate nearly third-order accuracy for
global quantities and about second-order for local fields. We further assess
the computational performance of the in-house LBM solver against two
open-source solvers: Nek5000 based on the spectral element method, and OpenFOAM
based on the finite volume method. Performance metrics, including million
lattice updates per second (MLUPS) and wall-clock time per dimensionless time
unit (WCTpDT), indicate that the ISLBM offers one to three orders of magnitude
higher efficiency in large-scale simulations. On GPU architectures, the ISLBM
retains high computational performance: throughput on non-uniform meshes
reaches 60 -- 70\% of that on uniform meshes in terms of MLUPS, while the cost
in WCTpDT is about three times higher. These results highlight the potential of
interpolation-based LBM approaches for high-fidelity simulations of thermal
convection on non-uniform meshes, providing a robust foundation for future
extensions to turbulent flows.

</details>


### [113] [A semi-analytical approach to characterize high-frequency three-dimensional wave propagation through clamp-on flowmeters](https://arxiv.org/abs/2509.00020)
*Sabiju Valiya Valappil,Alejandro M. Aragon,Johannes F. L. Goosen*

Main category: physics.flu-dyn

TL;DR: A hybrid computational approach combining FEA, ray tracing, analytical methods, and 2D DG analysis with scaling factors to efficiently model 3D wave propagation in clamp-on ultrasonic flowmeters at high frequencies.


<details>
  <summary>Details</summary>
Motivation: Traditional 3D transient analysis of clamp-on ultrasonic flowmeters using finite element analysis is computationally prohibitive at high frequencies, creating a need for more efficient modeling approaches.

Method: Separated the system into fluid and solid domains, analyzed fluid domain with FEA at low frequencies and ray tracing at high frequencies, characterized solid domain analytically via geometric projection, then performed 2D DG analysis and applied scaling factors to capture 3D effects.

Result: The hybrid approach successfully captured 3D wave propagation behavior, producing clearer output signals that enable straightforward identification of fluid signals that would be nearly impossible with traditional methods.

Conclusion: The proposed multi-domain hybrid computational framework provides an efficient and accurate alternative to computationally expensive 3D FEA for high-frequency wave propagation analysis in clamp-on ultrasonic flowmeters.

Abstract: Wave propagation analysis at high frequencies is essential for applications
involving ultrasound waves, such as clamp-on ultrasonic flowmeters. However, it
is extremely challenging to perform a 3D transient analysis of a clamp-on
flowmeter using standard tools such as finite element analysis (FEA) due to the
enormous associated computational cost. In this study, we separate the clamp-on
flowmeter into different domains and analyze them separately. Wave propagation
in the fluid domain is analyzed via FEA at low frequencies (100 kHz, 200 kHz,
and 500 kHz) and using ray tracing at high frequencies (1 MHz). The behavior in
the solid domain (wedges and pipe wall) is analytically characterized via
geometric projection. All these individual analyses provide us with different
scaling factors with which the waves in the respective domains scale when 3D
effects are considered. The complete clamp-on system is then analyzed in 2D via
the Discontinuous Galerkin (DG) method to obtain the response at the receiver.
The receiving signal is then scaled using the aforementioned scaling factors to
accurately capture the wave propagation behavior of the clamp-on system in 3D.
The output signal from the 2D analysis then becomes much clearer so that the
fluid signal can be identified straightforwardly, which would be nearly
impossible otherwise.

</details>


<div id='math.RA'></div>

# math.RA [[Back]](#toc)

### [114] [Notes on Simplifying the Construction of Barabanov Norms](https://arxiv.org/abs/2509.02230)
*Victor Kozyakin*

Main category: math.RA

TL;DR: This paper presents methods for constructing Barabanov norms to compute joint/generalized spectral radii of matrix sets, combining theoretical foundations with practical algorithms.


<details>
  <summary>Details</summary>
Motivation: To develop simpler, more practical approaches for computing joint and generalized spectral radii using extremal norms, making these mathematical tools more accessible for everyday use.

Method: Uses the Dranishnikov-Konyagin theorem on invariant bodies to construct Barabanov norms, and describes a modified max-relaxation algorithm along with additional techniques for simplification.

Result: Provides a theoretical framework and practical algorithm for constructing Barabanov norms, enabling easier computation of spectral radii for matrix sets.

Conclusion: The paper successfully bridges theoretical mathematics with practical computation, offering improved methods for determining growth rates of matrix products through extremal norm construction.

Abstract: To answer the question about the growth rate of matrix products, the concepts
of joint and generalized spectral radius were introduced in the 1960s. A common
tool for finding the joint/generalized spectral radius is the so-called
extremal norms and, in particular, the Barabanov norm. The goal of this paper
is to try to combine the advantages of different approaches based on the
concept of extremality in order to obtain results that are simpler for everyday
use. It is shown how the Dranishnikov-Konyagin theorem on the existence of a
special invariant body for a set of matrices can be used to construct a
Barabanov norm. A modified max-relaxation algorithm for constructing Barabanov
norms, which follows from this theorem, is described. Additional techniques are
also described that simplify the construction of Barabanov norms under the
assumption that

</details>


<div id='physics.atom-ph'></div>

# physics.atom-ph [[Back]](#toc)

### [115] [Corrections to radiative rates between atomic configurations](https://arxiv.org/abs/2509.00207)
*Jean-Christophe Pain,Djamel Benredjem*

Main category: physics.atom-ph

TL;DR: Extension of Klapisch's method to include energy distribution effects in opacity calculations, applied to iron experiments at solar and stellar envelope conditions, with Kirchhoff's law validation.


<details>
  <summary>Details</summary>
Motivation: Radiative opacity calculations for hot dense matter are challenging due to immense number of energy levels and transitions. Standard methods neglect energy distributions within configurations, limiting accuracy.

Method: Extended Klapisch's formalism to include energy shift and variance of Unresolved Transition Arrays, applied to iron opacity calculations for Sandia experiments (solar conditions) and laser experiments (stellar envelopes).

Result: Improved opacity calculations accounting for level energy distributions within configurations, with specific applications to L-shell and M-shell transitions in iron under different temperature conditions.

Conclusion: The extended formalism provides more accurate opacity calculations by including energy distribution effects, with proposed prescription to ensure Kirchhoff's law validity in LTE-approaching plasmas for both standard and corrected cases.

Abstract: The computation of radiative opacity or emissivity of hot dense matter is a
challenging task. It requires accounting for an immense number of energy levels
and lines across various excitation and ionization states. Whether in local
thermodynamic equilibrium (LTE) or non-LTE plasmas, statistical methods provide
significant assistance. Many computational codes are based on the Detailed
Configuration Accounting approximation, which involves averaged rates between
configurations. In that approach, only the mean energies of the configurations
are considered, and the effects of the energy distributions of the levels
within the initial and final configurations are typically neglected. A long
time ago, Klapisch proposed a method to correct the rates. The corresponding
formalism includes the energy shift and variance of the Unresolved Transition
Array, as well as the average energies of the configurations. We extend this
formalism and investigate its impact on opacity calculations in two specific
cases: first, the iron experiment conducted at Sandia National Laboratories
under conditions similar to those at the base of the Sun's convective zone,
dominated by L-shell 2p-$n$d transitions, and second, laser experiments--still
for iron--at much lower temperature. The latter measurements shed light on our
understanding of the envelopes of $\beta$-Cephei-type stars, where the relevant
transitions are intra-M-shell $\Delta n=0$ (3-3) transitions, specifically
3s-3p and 3p-3d, in the XUV range. The issue of ensuring the validity of
Kirchhoff's law when plasmas approach LTE is also addressed, and a prescription
is proposed, applying both to the standard configuration-to-configuration case
and to the aforementioned corrections, which account for the energy
distribution of the levels within a configuration.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [116] [Assessing the Advantages and Limitations of Quantum Neural Networks in Regression Tasks](https://arxiv.org/abs/2509.00854)
*Gubio G. de Limaa,Tiago de S. Farias,Alexandre C. Ricardo,Celso Jorge Villa Boas*

Main category: quant-ph

TL;DR: QNNs show significant advantage over classical neural networks for specific quantum machine learning tasks (7 orders of magnitude lower error for sinusoidal functions), but are not universally superior, reinforcing the No Free Lunch theorem.


<details>
  <summary>Details</summary>
Motivation: To determine under what conditions quantum neural networks (QNNs) provide concrete benefits over classical neural networks (CNNs) and address methodological challenges in fair comparisons.

Method: Qualitative and quantitative analyses of classical and quantum models applied to regression problems using two target functions with contrasting properties.

Result: QNNs excelled at approximating sinusoidal functions with errors up to seven orders of magnitude lower than classical counterparts, but showed limited performance in other cases.

Conclusion: QNNs are highly effective for certain specific tasks but not universally superior, reinforcing that no single model outperforms all others across every problem domain.

Abstract: The development of quantum neural networks (QNNs) has attracted considerable
attention due to their potential to surpass classical models in certain machine
learning tasks. Nonetheless, it remains unclear under which conditions QNNs
provide concrete benefits over classical neural networks (CNNs). This study
addresses this question by performing both qualitative and quantitative
analyses of classical and quantum models applied to regression problems, using
two target functions with contrasting properties. Additionally, the work
explores the methodological difficulties inherent in making fair comparisons
between QNNs and CNNs. The findings reveal a distinct advantage of QNNs in a
specific quantum machine learning context. In particular, QNNs excelled at
approximating the sinusoidal function, achieving errors up to seven orders of
magnitude lower than their classical counterparts. However, their performance
was limited in other cases, emphasizing that QNNs are highly effective for
certain tasks but not universally sPuperior. These results reinforce the
principles of the ``No Free Lunch'' theorem, highlighting that no single model
outperforms all others across every problem domain.

</details>


### [117] [Large time-step discretisation of adiabatic quantum dynamics](https://arxiv.org/abs/2509.00171)
*Dong An,Pedro C. S. Costa,Dominic W. Berry*

Main category: quant-ph

TL;DR: Digital adiabatic quantum computing can use much larger time steps than expected, reducing complexity. Uniform time steps independent of error tolerance and evolution time are possible, with evidence of exponential convergence for first-order Trotter methods.


<details>
  <summary>Details</summary>
Motivation: Digital implementation of adiabatic quantum computing requires efficient Hamiltonian simulation, which often introduces computational overhead and complex quantum control logic. The goal is to simplify this process and reduce complexity.

Method: The paper shows that time step sizes in time discretization can be much larger than expected. It demonstrates that uniform time step sizes independent of tolerated error and evolution time can be chosen for sufficiently accurate simulation, with boundary cancellation conditions that exponentially suppress continuous diabatic errors.

Result: The analysis reveals an exponential convergence of even first-order Trotter methods with uniform time step size. When applied to adiabatic unstructured search, the Trotterized adiabatic approach matches the Grover lower bound, doesn't require prior knowledge of marked states, and performs comparably to quantum approximate optimization algorithms.

Conclusion: The findings significantly reduce the complexity of digital adiabatic quantum computing by allowing larger, uniform time steps while maintaining accuracy, making the approach more practical and efficient for quantum computation.

Abstract: Adiabatic quantum computing is a general framework for preparing eigenstates
of Hamiltonians on quantum devices. However, its digital implementation
requires an efficient Hamiltonian simulation subroutine, which may introduce
extra computational overhead or complicated quantum control logic. In this
work, we show that the time step sizes in time discretization can be much
larger than expected, and the overall complexity is greatly reduced.
Remarkably, regardless of the general convergence order of the numerical
method, we can choose a uniform time step size independent of tolerated error
and evolution time for sufficiently accurate simulation. Furthermore, with the
boundary cancellation condition where the continuous diabatic errors are
exponentially suppressed, we provide strong evidence on an exponential
convergence of even first-order Trotter with uniform time step size. We apply
our analysis to the example of adiabatic unstructured search and show several
preferable features of the Trotterized adiabatic approach: it can match the
Grover lower bound, it does not require a priori knowledge on the number of
marked states, and its performance can be asymptotically comparable with that
of the quantum approximate optimization algorithm.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [118] [Solar Flare Ion Temperatures](https://arxiv.org/abs/2509.01237)
*Alexander J. B. Russell,Vanessa Polito,Paola Testa,Bart De Pontieu,Sergey A. Belov*

Main category: astro-ph.SR

TL;DR: Ion temperatures in solar flares are several times higher than electron temperatures, reaching up to 60 MK, which helps explain excess nonthermal broadening in spectral lines.


<details>
  <summary>Details</summary>
Motivation: To address the long-standing issue of excess nonthermal broadening in solar flare spectral lines by investigating ion temperature characteristics during flare onset and above-the-loop regions.

Method: Analysis of spectral line Doppler widths, examination of universal temperature scaling relations from solar wind and magnetospheric reconnection events, and calculation of thermal equilibration times for different flare densities.

Result: Ion temperatures are found to be several times higher than local electron temperatures, potentially reaching 60 MK or greater in the hot onset phase and above-the-loop regions.

Conclusion: High ion temperatures represent a substantial component of spectral line widths, providing a significant contribution to solving the problem of excess nonthermal broadening in flare lines.

Abstract: This paper proposes that the ion temperature is several times the local
electron temperature in the hot onset phase and at the above-the-loop region of
solar flares. The paper considers: the evidence of spectral line Doppler widths
("non-thermal" broadening); evidence for "universal" ion and electron
temperature increase scaling relations for magnetic reconnection in the solar
wind, Earth's magnetopause, Earth's magnetotail and numerical simulations; and
thermal equilibration times for onset and above-the-loop densities, which are
much longer than previous estimates based on soft X-ray flare loops. We
conclude that the ion temperature is likely to reach 60 MK or greater and that
it may represent a substantial part of spectral line widths, significantly
contributing to solving the long-standing issue of the excess nonthermal
broadening in flare lines.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [119] [Emergent Rotational Order and Re-entrant Global Order of Vicsek Agents in a Complex Noise Environment](https://arxiv.org/abs/2509.01036)
*Mohd Yasir Khan*

Main category: cond-mat.soft

TL;DR: Study of Vicsek model agents in complex noise environments shows emergent rotational order and U-shaped global order patterns, with velocity-dependent escape dynamics and environmental heterogeneity effects on collective behaviors.


<details>
  <summary>Details</summary>
Motivation: To understand how complex noise environments and environmental heterogeneity impact collective dynamics in active matter systems, using the Vicsek model as a platform to study flocking and swarming phenomena.

Method: Modified Vicsek model with a noiseless circular region surrounded by a noisy outer region, featuring mutually repelling interactions. Varied outer noise intensity and analyzed phase/susceptibility plots, escape rates, and order parameters.

Result: Emergent rotational order peaks at higher noise levels; global order follows U-shaped curve; higher velocities enhance escape rates from circular region; gradual noise increase reduces both global and rotational order.

Conclusion: Environmental heterogeneity and sudden noise changes significantly impact collective dynamics, with applications for predicting and manipulating active agent behaviors in biological and synthetic swarming systems.

Abstract: Noisy pursuit in complex environments drives emergent collective behaviors in
active matter systems. A compelling platform to study the impact of environment
cues is provided by the standard Vicsek model for studying flocking and
swarming phenomena. In this study, we explore the collective dynamics of Vicsek
agents in a complex noise environment, featuring a noiseless circular region
($\eta_{\text{c}} = 0.0$) surrounded by a noisy outer region ($\eta_{\text{b}}
= 1.0$, tunable), with a mutually repelling interactions. By varying the outer
noise intensity, we observe an emergent rotational order ($\phi_r$) that peaks
at higher noise levels ($\eta_{\text{b}}\sim 1$), as revealed by phase and
susceptibility plots. Global order follows ($\phi$) follows a `U' shaped curve,
$\phi \sim 0.965$ at $\eta_b=0$, dies down to $\phi \sim 0.57$ at $\eta_b=0.9$
and re-enters at $\eta_b > 1$ and peaks $\phi \sim 0.960$ at $\eta_b=1.5$. The
latter rise attributing to $\phi_r$ increase. Higher particle velocities
enhance escape rates ($\kappa$) from the circular region, with slower-moving
agents exhibiting greater virtual confinement. We quantify escape dynamics
through time-averaged and first-passage escape rates, demonstrating
velocity-dependent retention on the probability of finding the bi-motility
agent flocks at a give time resulting in segregation and trapping. Introducing
a gradual noise increase from the circle's center to the outer region reduces
both global ($\phi$) and rotational ($\phi_r$) order, underscoring the impact
of environmental heterogeneity and sudden annealing over gradual change. These
findings offer insights into predicting and manipulating active agent dynamics
in heterogeneous environments, with applications in biological and synthetic
swarming systems.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [120] [The Nondecreasing Rank](https://arxiv.org/abs/2509.00265)
*Andrew McCormack*

Main category: stat.ML

TL;DR: Introduces nondecreasing (ND) rank for matrices/tensors, which requires vector monotonicity in outer product representations. Develops theory on ND rank properties and presents algorithm for low ND rank approximations with applications to pig weight and COVID mental health data.


<details>
  <summary>Details</summary>
Motivation: To develop a tensor factorization framework that incorporates monotonicity constraints, which is relevant for analyzing ordered data where variables naturally exhibit non-decreasing patterns (e.g., growth measurements, survey responses with ordered categories).

Method: Introduces ND rank concept, develops theoretical properties (typical, maximum, border ranks), and proposes a variant of hierarchical alternating least squares algorithm for finding low ND rank approximations.

Result: Shows equivalence between ND factorization and nonnegative factorization for certain poset orderings, demonstrates that not all monotonic tensors have finite ND rank, and provides successful applications to real datasets including pig weight measurements and COVID-19 mental health survey data.

Conclusion: The ND rank provides a useful constrained factorization approach for ordered data analysis, with theoretical foundations established and practical algorithm developed that successfully captures monotonic patterns in real-world applications.

Abstract: In this article the notion of the nondecreasing (ND) rank of a matrix or
tensor is introduced. A tensor has an ND rank of r if it can be represented as
a sum of r outer products of vectors, with each vector satisfying a
monotonicity constraint. It is shown that for certain poset orderings finding
an ND factorization of rank $r$ is equivalent to finding a nonnegative rank-r
factorization of a transformed tensor. However, not every tensor that is
monotonic has a finite ND rank. Theory is developed describing the properties
of the ND rank, including typical, maximum, and border ND ranks. Highlighted
also are the special settings where a matrix or tensor has an ND rank of one or
two. As a means of finding low ND rank approximations to a data tensor we
introduce a variant of the hierarchical alternating least squares algorithm.
Low ND rank factorizations are found and interpreted for two datasets
concerning the weight of pigs and a mental health survey during the COVID-19
pandemic.

</details>


### [121] [Beyond Universal Approximation Theorems: Algorithmic Uniform Approximation by Neural Networks Trained with Noisy Data](https://arxiv.org/abs/2509.00924)
*Anastasis Kratsios,Tin Sum Cheng,Daniel Roy*

Main category: stat.ML

TL;DR: This paper bridges the gap between theoretical universal approximation theorems and practical machine learning by introducing a randomized training algorithm that constructs neural networks from noisy data, achieving minimax-optimal parameter complexity while exhibiting real-world network behaviors.


<details>
  <summary>Details</summary>
Motivation: Classical universal approximation theorems operate in theoretical vacuums assuming noiseless data and free parameter selection, isolating them from practical machine learning goals of reliable generalization from noisy observations.

Method: The authors introduce an architecture-specific randomized training algorithm that constructs uniform approximators from N noisy training samples on d-dimensional cubes, achieving minimax-optimal trainable parameters.

Result: The trained neural networks attain minimax-optimal parameter complexity (subject to logarithmic factors), exhibit sub-linear parametric complexity on related tasks, exactly interpolate training data, and maintain reasonable Lipschitz regularity.

Conclusion: This work brings state-of-the-art UATs closer to practical machine learning, shifting the open question from algorithmic implementability with noisy samples to whether stochastic gradient descent can achieve comparable guarantees.

Abstract: At its core, machine learning seeks to train models that reliably generalize
beyond noisy observations; however, the theoretical vacuum in which
state-of-the-art universal approximation theorems (UATs) operate isolates them
from this goal, as they assume noiseless data and allow network parameters to
be chosen freely, independent of algorithmic realism. This paper bridges that
gap by introducing an architecture-specific randomized training algorithm that
constructs a uniform approximator from $N$ noisy training samples on the
$d$-dimensional cube $[0,1]^d$. Our trained neural networks attain the
minimax-optimal quantity of \textit{trainable} (non-random) parameters, subject
to logarithmic factors which vanish under the idealized noiseless sampling
assumed in classical UATs.
  Additionally, our trained models replicate key behaviours of real-world
neural networks, absent in standard UAT constructions, by: (1) exhibiting
sub-linear parametric complexity when fine-tuning on structurally related and
favourable out-of-distribution tasks, (2) exactly interpolating the training
data, and (3) maintaining reasonable Lipschitz regularity (after the initial
clustering attention layer). These properties bring state-of-the-art UATs
closer to practical machine learning, shifting the central open question from
algorithmic implementability with noisy samples to whether stochastic gradient
descent can achieve comparable guarantees.

</details>


### [122] [Lipschitz-Guided Design of Interpolation Schedules in Generative Models](https://arxiv.org/abs/2509.01629)
*Yifan Chen,Eric Vanden-Eijnden,Jiawei Xu*

Main category: stat.ML

TL;DR: Optimizing interpolation schedules in generative models by minimizing averaged squared Lipschitzness rather than kinetic energy, achieving exponential improvements in numerical efficiency and reduced mode collapse without requiring neural network retraining.


<details>
  <summary>Details</summary>
Motivation: While all scalar interpolation schedules achieve identical statistical efficiency after optimal diffusion coefficient tuning, their numerical efficiency differs substantially, motivating focus on numerical properties of drift fields rather than statistical criteria.

Method: Propose averaged squared Lipschitzness minimization as a principled criterion for numerical optimization, derive transfer formula for schedule conversion at inference time without retraining, and validate on Gaussian distributions, Gaussian mixtures, and high-dimensional invariant distributions.

Result: Exponential improvements in Lipschitz constants over standard linear schedules for Gaussian distributions, reduced mode collapse in few-step sampling for Gaussian mixtures, and robust performance improvements across resolutions on stochastic Allen-Cahn and Navier-Stokes equations.

Conclusion: Numerical optimization through Lipschitzness minimization provides superior schedule design for generative models, enabling efficient inference without retraining and demonstrating consistent performance gains across various distribution types and resolutions.

Abstract: We study the design of interpolation schedules in the stochastic interpolants
framework for flow and diffusion-based generative models. We show that while
all scalar interpolation schedules achieve identical statistical efficiency
under Kullback-Leibler divergence in path space after optimal diffusion
coefficient tuning, their numerical efficiency can differ substantially. This
observation motivates focusing on numerical properties of the resulting drift
fields rather than statistical criteria for schedule design. We propose
averaged squared Lipschitzness minimization as a principled criterion for
numerical optimization, providing an alternative to kinetic energy minimization
used in optimal transport approaches. A transfer formula is derived that
enables conversion between different schedules at inference time without
retraining neural networks. For Gaussian distributions, our optimized schedules
achieve exponential improvements in Lipschitz constants over standard linear
schedules, while for Gaussian mixtures, they reduce mode collapse in few-step
sampling. We also validate our approach on high-dimensional invariant
distributions from stochastic Allen-Cahn equations and Navier-Stokes equations,
demonstrating robust performance improvements across resolutions.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [123] [Dipolar Nematic State in Relaxor Ferroelectrics](https://arxiv.org/abs/2509.01464)
*Yuan-Jinsheng Liu,Tyler C. Sterling,Shi Liu*

Main category: cond-mat.mtrl-sci

TL;DR: Universal dipolar nematic state discovered in relaxor ferroelectrics through large-scale MD simulations, challenging conventional polar cluster models and providing unified framework for understanding relaxor properties.


<details>
  <summary>Details</summary>
Motivation: Relaxor ferroelectrics show exceptional dielectric and electromechanical properties but their microscopic origins remain unclear due to hierarchical polar structures and chemical complexity. Existing models lack first-principles predictive capability.

Method: Large-scale molecular dynamics simulations using a universal first-principles-based machine-learning interatomic potential to investigate atomic-scale polar dynamics in Pb-, Bi-, and Ba-based relaxors.

Result: Discovered a universal dipolar nematic state with long-range orientational order but no local alignment. Introduced universal order parameter based on skewness of local polarization autocorrelation functions that captures thermal evolution across all systems. Found robust structural memory under electric field cycling.

Conclusion: Establishes a unified microscopic framework for relaxors with a broadly applicable statistical approach for understanding complex disordered materials, explaining key phenomena like diffuse phase transitions and giant piezoelectricity.

Abstract: Relaxor ferroelectrics exhibit exceptional dielectric and electromechanical
properties, yet their microscopic origins remain elusive due to the interplay
of hierarchical polar structures and chemical complexity. While models based on
polar nanoregions or nanodomains offer valuable phenomenological insights, they
often lack the first-principles predictive capability necessary for
quantitatively describing functional properties such as piezoelectric
coefficients. Here, we use large-scale molecular dynamics simulations, enabled
by a universal first-principles-based machine-learning interatomic potential,
to investigate atomic-scale polar dynamics in canonical Pb-, Bi-, and Ba-based
relaxors. Across all systems, we uncover a universal dipolar nematic state,
characterized by long-range orientational order of local polarizations without
local alignment, challenging conventional polar cluster-based paradigms. We
introduce a universal order parameter, derived from the skewness of the
distributions of the local polarization autocorrelation functions, that
captures the thermal evolution of both lead-based and lead-free systems within
a single master curve. This nematic order, and its robust structural memory
under electric field cycling, underpins key relaxor phenomena, including
diffuse phase transition, frequency-dependent dielectric dispersion, and
reversible giant piezoelectricity. Our findings establish a unified microscopic
framework for relaxors and present a broadly applicable statistical approach to
understanding complex disordered materials.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [124] [RAMS: Residual-based adversarial-gradient moving sample method for scientific machine learning in solving partial differential equations](https://arxiv.org/abs/2509.01234)
*Weihang Ouyang,Min Zhu,Wei Xiong,Si-Wei Liu,Lu Lu*

Main category: cs.CE

TL;DR: RAMS is a novel adaptive sampling method that moves samples along adversarial gradient directions to maximize PDE residuals, improving efficiency for high-dimensional SciML problems.


<details>
  <summary>Details</summary>
Motivation: Existing sampling methods for physics-informed neural networks and neural operators are computationally expensive for high-dimensional PDEs and operator learning tasks, creating a need for more efficient adaptive sampling approaches.

Method: Proposed Residual-based Adversarial-gradient Moving Sample (RAMS) method that optimizes sample placement by moving samples along adversarial gradient directions to maximize PDE residuals, compatible with existing sampling frameworks.

Result: Extensive experiments show RAMS effectively improves performance for high-dimensional PINNs and both physics-informed and data-driven operator learning problems, demonstrating computational efficiency.

Conclusion: RAMS represents the first efficient adaptive sampling approach for operator learning and marks a significant advancement in scientific machine learning by providing an effective solution for high-dimensional problems.

Abstract: Physics-informed neural networks (PINNs) and neural operators, two leading
scientific machine learning (SciML) paradigms, have emerged as powerful tools
for solving partial differential equations (PDEs). Although increasing the
training sample size generally enhances network performance, it also increases
computational costs for physics-informed or data-driven training. To address
this trade-off, different sampling strategies have been developed to sample
more points in regions with high PDE residuals. However, existing sampling
methods are computationally demanding for high-dimensional problems, such as
high-dimensional PDEs or operator learning tasks. Here, we propose a
residual-based adversarial-gradient moving sample (RAMS) method, which moves
samples according to the adversarial gradient direction to maximize the PDE
residual via gradient-based optimization. RAMS can be easily integrated into
existing sampling methods. Extensive experiments, ranging from PINN applied to
high-dimensional PDEs to physics-informed and data-driven operator learning
problems, have been conducted to demonstrate the effectiveness of RAMS.
Notably, RAMS represents the first efficient adaptive sampling approach for
operator learning, marking a significant advancement in the SciML field.

</details>


### [125] [Kernel manifolds: nonlinear-augmentation dimensionality reduction using reproducing kernel Hilbert spaces](https://arxiv.org/abs/2509.00224)
*Alejandro N. Diaz,Jacob T. Needels,Irina K. Tezaur,Patrick J. Blonigan*

Main category: cs.CE

TL;DR: Generalizes quadratic manifold dimensionality reduction using kernel methods to learn optimal nonlinear corrections from reproducing kernel Hilbert spaces, overcoming limitations of linear approaches with better accuracy and flexibility.


<details>
  <summary>Details</summary>
Motivation: To overcome approximation accuracy limitations of purely linear dimensionality reduction approaches by developing a more flexible nonlinear augmentation framework that can impose arbitrary nonlinear structure.

Method: Develops kernel methods-based nonlinear-augmentation dimensionality reduction that learns optimal nonlinear corrections from user-defined reproducing kernel Hilbert spaces, generalizing feature map-based approaches.

Result: The approach allows imposing arbitrary nonlinear structure, includes existing methods as special cases, has low training cost, and shows monotonically decreasing error with increasing latent dimension.

Conclusion: The kernel-based nonlinear augmentation provides a flexible and effective framework for dimensionality reduction that outperforms traditional linear methods and recent quadratic manifold approaches across various test problems.

Abstract: This paper generalizes recent advances on quadratic manifold (QM)
dimensionality reduction by developing kernel methods-based
nonlinear-augmentation dimensionality reduction. QMs, and more generally
feature map-based nonlinear corrections, augment linear dimensionality
reduction with a nonlinear correction term in the reconstruction map to
overcome approximation accuracy limitations of purely linear approaches. While
feature map-based approaches typically learn a least-squares optimal polynomial
correction term, we generalize this approach by learning an optimal nonlinear
correction from a user-defined reproducing kernel Hilbert space. Our approach
allows one to impose arbitrary nonlinear structure on the correction term,
including polynomial structure, and includes feature map and radial basis
function-based corrections as special cases. Furthermore, our method has
relatively low training cost and has monotonically decreasing error as the
latent space dimension increases. We compare our approach to proper orthogonal
decomposition and several recent QM approaches on data from several example
problems.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [126] [Demonstration of a tandem lens for producing shaped laser-ionized plasmas for plasma wakefield acceleration](https://arxiv.org/abs/2509.01747)
*R. Ariniello,V. Lee,M. D. Litos*

Main category: physics.optics

TL;DR: A tandem lens optical system using two diffractive optics creates meter-long Bessel focus for plasma wakefield acceleration applications, enabling tailored plasma density profiles through controlled on-axis intensity.


<details>
  <summary>Details</summary>
Motivation: To develop an optical system that produces optimal plasma sources for plasma wakefield acceleration by controlling the on-axis intensity profile to tailor plasma density ramps for better electron beam matching.

Method: Uses a tandem lens setup with two diffractive optics to focus high-power ultrafast lasers, creating shaped on-axis intensity profiles that generate meter-long Bessel focus. Includes algorithm for calculating lens phases and detailed performance calculations.

Result: Successfully demonstrated the optical system capable of producing meter-long Bessel focus with controlled on-axis intensity, enabling tailored plasma density profiles at plasma entrance and exit.

Conclusion: The tandem diffractive optics approach provides an effective method for creating optimized plasma sources for wakefield acceleration applications through precise control of laser intensity profiles.

Abstract: We demonstrate a tandem lens optical setup, comprising two diffractive
optics, that focuses a high-power ultrafast laser with a shaped on-axis
intensity profile, producing a meter-long Bessel focus. The intended use of the
optical setup is to produce a laser-ionized plasma source for plasma wakefield
acceleration. By controlling the on-axis intensity, the density profile of the
plasma ramps at the entrance and exit of the plasma can be tailored to optimize
matching of the electron beam into the plasma. In addition to demonstrating the
optical system, we describe the algorithm used to calculate the lens phases and
present detailed calculations of the lenses' expected performance.

</details>


<div id='nlin.AO'></div>

# nlin.AO [[Back]](#toc)

### [127] [Optimal information injection and transfer mechanisms for active matter reservoir computing](https://arxiv.org/abs/2509.01799)
*Mario U. Gaimann,Miriam Klopotek*

Main category: nlin.AO

TL;DR: Active matter systems can serve as effective reservoirs for computing when driven by chaotic inputs, with attractive forces triggering emergent structural patterns that enhance computational performance through improved fading memory and nonlinearity.


<details>
  <summary>Details</summary>
Motivation: To understand how biological complex systems and active matter can process information efficiently as reservoir computing substrates, and to test whether such systems compute independently of the driving method.

Method: Using simulations of active matter systems driven by chaotically moving input signals, switching between repulsive and attractive driving forces, and performing reservoir computing across various non-equilibrium active matter phases.

Result: Attractive driving forces completely change computation methods while maintaining similar predictive performance. Nonlinear driving improves computation by decoupling agent dynamics, triggering emergent structural boundaries and coherent speed gradients. Liquid droplet formations show particularly good performance with consistently convex performance landscapes.

Conclusion: Active matter systems, especially those forming liquid droplets, are well-suited for reservoir computing due to their robustness, adaptivity, and emergent regulatory mechanisms that enhance computational properties like fading memory and nonlinearity.

Abstract: Reservoir computing (RC) is a state-of-the-art machine learning method that
makes use of the power of dynamical systems (the reservoir) for real-time
inference. When using biological complex systems as reservoir substrates, it
serves as a testbed for basic questions about bio-inspired computation -- of
how self-organization generates proper spatiotemporal patterning. Here, we use
a simulation of an active matter system, driven by a chaotically moving input
signal, as a reservoir. So far, it has been unclear whether such complex
systems possess the capacity to process information efficiently and
independently of the method by which it was introduced. We find that when
switching from a repulsive to an attractive driving force, the system
completely changes the way it computes, while the predictive performance
landscapes remain nearly identical. The nonlinearity of the driver's injection
force improves computation by decoupling the single-agent dynamics from that of
the driver. Triggered are the (re-)growth, deformation, and active motion of
smooth structural boundaries (interfaces), and the emergence of coherent
gradients in speed -- features found in many soft materials and biological
systems. The nonlinear driving force activates emergent regulatory mechanisms,
which manifest enhanced morphological and dynamic diversity -- arguably
improving fading memory, nonlinearity, expressivity, and thus, performance. We
further perform RC in a broad variety of non-equilibrium active matter phases
that arise when tuning internal (repulsive) forces for information transfer.
Overall, we find that active matter agents forming liquid droplets are
particularly well suited for RC. The consistently convex shape of the
predictive performance landscapes, together with the observed phenomenological
richness, conveys robustness and adaptivity.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [128] [Migration as a Probe: A Generalizable Benchmark Framework for Specialist vs. Generalist Machine-Learned Force Fields in Doped Materials](https://arxiv.org/abs/2509.00090)
*Yi Cao,Paulette Clancy*

Main category: physics.chem-ph

TL;DR: Benchmarking framework compares specialist (from-scratch) vs fine-tuned foundation MLFFs for Cr-intercalated Sb2Te3, showing fine-tuning improves kinetic accuracy but may degrade long-range physics representations.


<details>
  <summary>Details</summary>
Motivation: To determine whether building specialist models from scratch or adapting generalist foundation models is better for MLFFs, addressing trade-offs in data efficiency, accuracy, and OOD risks.

Method: Developed benchmarking framework using MACE architecture on Cr-intercalated Sb2Te3, employing migration pathways via NEB trajectories to test interpolation/extrapolation, assessing equilibrium, kinetic, and mechanical properties.

Result: All models captured equilibrium structures but diverged on non-equilibrium processes. Task-specific fine-tuning substantially improved kinetic accuracy over from-scratch and zero-shot models, but degraded long-range physics representations. Training paradigms produced distinct latent encodings.

Conclusion: Provides practical guide for MLFF development, highlights migration-based probes as efficient diagnostics, and suggests pathways for uncertainty-aware active learning strategies.

Abstract: Machine-learned force fields (MLFFs), particularly pre-trained foundation
models, promise to bring ab initio-level accuracy to the length and time scales
of molecular dynamics. Yet this shift raises a central question: is it better
to build a specialist model from scratch or adapt a generalist foundation model
for a specific system? The trade-offs in data efficiency, predictive accuracy,
and risks of out-of-distribution (OOD) failure remain unclear. Here, we present
a benchmarking framework that contrasts bespoke (from scratch) and fine-tuned
foundation models in a test case of a technologically relevant 2D material,
Cr-intercalated Sb2Te3, using the MACE architecture. Our framework employs
migration pathways, evaluated through nudged elastic band (NEB) trajectories,
as a diagnostic probe that tests both interpolation and extrapolation. We
assess accuracy for equilibrium, kinetic (atomic migration), and mechanical
(interlayer sliding) tasks. While all models capture equilibrium structures,
predictions for non-equilibrium processes diverge. Task-specific fine-tuning
substantially improves kinetic accuracy compared with both from-scratch and
zero-shot models, but can degrade learned representations of long-range
physics. Analysis of internal representations shows that training paradigms
yield distinct, non-overlapping latent encodings of system physics. This work
offers a practical guide for MLFF development, highlights migration-based
probes as efficient diagnostics, and suggests pathways toward uncertainty-aware
active learning strategies.

</details>


### [129] [Effective approximations for Hartree-Fock exchange potential](https://arxiv.org/abs/2509.00733)
*Fei Xu*

Main category: physics.chem-ph

TL;DR: A generalized framework for approximate Fock exchange operators in Hartree-Fock theory using low-rank decomposition and adjustable variables, achieving near-exact accuracy with significantly improved computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Address computational bottlenecks of nonlocal Hartree-Fock exchange potential in large-scale applications while maintaining quantum mechanical exchange effects.

Method: Low-rank decomposition with adjustable variables to ensure accuracy for occupied orbitals, plus two-level nested self-consistent field iteration strategy to decouple exchange operator stabilization and electron density refinement.

Result: Numerical experiments show near-identical energies compared to exact exchange operator and NWChem references, with substantial computational efficiency improvements.

Conclusion: The proposed approximate exchange operators provide an effective solution to computational challenges of nonlocal exchange while maintaining high accuracy in Hartree-Fock calculations.

Abstract: The Hartree-Fock exchange potential is fundamental for capturing quantum
mechanical exchange effects but faces critical challenges in large-scale
applications due to its nonlocal and computationally intensive nature. This
study introduces a generalized framework for constructing approximate Fock
exchange operators in Hartree-Fock theory, addressing the computational
bottlenecks caused by the nonlocal nature. By employing low-rank decomposition
and incorporating adjustable variables, the proposed method ensures high
accuracy for occupied orbitals while maintaining Hermiticity and structural
consistency with the exact Fock exchange operator. Meanwhile, a two-level
nested self-consistent field iteration strategy is developed to decouple the
exchange operator stabilization (outer loop) and electron density refinement
(inner loop), significantly reducing computational costs. Numerical experiments
on several molecules demonstrate that the approximate exchange operators
achieve near-identical energies compared to that of the exact exchange operator
and the NWChem references, with substantial improvements in computational
efficiency.

</details>


<div id='math.CA'></div>

# math.CA [[Back]](#toc)

### [130] [Sharp microlocal Kakeya--Nikodym estimates for Hörmander operators and spectral projectors](https://arxiv.org/abs/2509.01116)
*Chuanwei Gao,Shukun Wu,Yakun Xi*

Main category: math.CA

TL;DR: Sharp microlocal Kakeya-Nikodym estimates for Hörmander operators and spectral projectors, with applications to various L^p estimates in odd dimensions.


<details>
  <summary>Details</summary>
Motivation: To establish precise microlocal Kakeya-Nikodym bounds for Hörmander operators with positive-definite Carleson-Sjölin phases and spectral projectors on Riemannian manifolds, completing the analysis in odd-dimensional cases.

Method: Develop sharp microlocal Kakeya-Nikodym estimates for Hörmander operators and spectral projectors, then apply these to derive various L^q to L^p estimates.

Result: Obtained sharp L^q→L^p estimates for Hörmander operators in odd dimensions, along with estimates for Fourier extension operators, Bochner-Riesz operators, Laplace eigenfunctions, and Hecke-Maass forms.

Conclusion: The established microlocal Kakeya-Nikodym estimates provide a powerful framework that yields sharp results across multiple areas of harmonic analysis and spectral theory, particularly completing the odd-dimensional case analysis.

Abstract: We establish sharp microlocal Kakeya--Nikodym estimates for H\"ormander
operators with positive-definite Carleson--Sj\"olin phases and for spectral
projectors on smooth, compact Riemannian manifolds. As an application, we
obtain sharp $L^q\to L^p$ estimates for the aforementioned H\"ormander
operators in odd dimensions, thereby completing the analysis in the
odd-dimensional case. Further applications include $L^q\to L^p$ estimates for
the Fourier extension operator, $L^p$ estimates for the Bochner--Riesz
operator, microlocal Kakeya--Nikodym estimates for Laplace eigenfunctions, and
$L^p$ estimates for Hecke--Maass forms on compact $3$-dimensional arithmetic
hyperbolic manifolds.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [131] [The trace-free Einstein tensor is not variational for the metric as field variable](https://arxiv.org/abs/2509.02490)
*Arian L. von Blanckenburg,Domenico Giulini,Philip K. Schwartz*

Main category: gr-qc

TL;DR: The trace-free Einstein tensor cannot be derived from any local action functional, even without requiring diffeomorphism invariance.


<details>
  <summary>Details</summary>
Motivation: To investigate whether the trace-free Einstein tensor can be obtained from variational principles of local actions, extending beyond the well-known restriction that applies to diffeomorphism-invariant actions.

Method: The authors employ mathematical analysis of variational principles and action functionals, examining the conditions under which the trace-free Einstein tensor could arise as Euler-Lagrange equations from local actions.

Result: The study demonstrates that the trace-free Einstein tensor cannot be obtained from any local action functional, regardless of whether the action is diffeomorphism-invariant or not.

Conclusion: This result establishes a fundamental limitation: the trace-free Einstein tensor is not variational from any local action, which has implications for gravitational theories and variational formulations of Einstein's equations.

Abstract: It is well-known that the trace-free Einstein tensor of a pseudo-Riemannian
metric cannot arise by variation of a local diffeomorphism-invariant action
functional with the (inverse) metric as field variable. We show that this
statement remains true even for general local actions, without the assumption
of diffeomorphism invariance.

</details>


<div id='math.FA'></div>

# math.FA [[Back]](#toc)

### [132] [On Hardy spaces associated with the twisted Laplacian and sharp estimates for the corresponding wave operator](https://arxiv.org/abs/2509.00327)
*Riju Basak,K. Jotsaroop*

Main category: math.FA

TL;DR: Characterization of Hardy spaces H^p_L(C^n) for 0<p<1 associated with twisted Laplacian, extending previous work for p=1. Using atomic decomposition, proves sharp boundedness of wave operator from H^p_L to L^p spaces.


<details>
  <summary>Details</summary>
Motivation: Generalize Hardy space characterizations from p=1 case to 0<p<1 range for twisted Laplacian, and establish sharp boundedness results for wave operators on these Hardy spaces.

Method: Proves equivalent characterizations of H^p_L(C^n) spaces using atomic decomposition approach based on twisted convolution. Applies this atomic characterization to analyze wave operator boundedness.

Result: Obtains sharp boundedness condition: wave operator L^{-δ/2}e^{±it√L} is bounded from H^p_L(C^n) to L^p(C^n) for 0<p≤1 when δ ≥ (2n-1)(1/p-1/2).

Conclusion: Successfully extends Hardy space theory to 0<p<1 case for twisted Laplacian and establishes optimal boundedness results for wave operators on these function spaces.

Abstract: We prove various equivalent characterisations of the Hardy space
$H^p_{\mathcal{L}}(\mathbb{C}^n)$ for $0<p<1$ associated with the twisted
Laplacian $\mathcal{L}$ which generalises the result of [MPR81] for the case
$p=1$. Using the atomic characterisation of $H^p_{\mathcal{L}}(\mathbb{C}^n)$
corresponding to the twisted convolution, we prove sharp boundedness result for
the wave operator $\mathcal{L}^{-\delta/2}e^{\pm it\sqrt{\mathcal{L}}}$ for a
fixed $t>0$ on $H^p_{\mathcal{L}}(\mathbb{C}^n)$. More precisely we prove that
it is a bounded operator from $H^p_{\mathcal{L}}(\mathbb{C}^n)$ to
$L^p(\mathbb{C}^n)$ for $ 0<p\leq 1$ and $\delta\geq
(2n-1)\left(1/p-1/2\right)$.

</details>


### [133] [Birkhoff-Kellogg type results in product spaces and their application to differential systems](https://arxiv.org/abs/2509.01392)
*Alessandro Calamai,Gennaro Infante,Jorge Rodríguez-López*

Main category: math.FA

TL;DR: New Birkhoff-Kellogg theorem extension in product spaces providing component-wise eigenvalues with nontrivial eigenvectors having norm localization, applied to PDE/ODE systems.


<details>
  <summary>Details</summary>
Motivation: Extend classical Birkhoff-Kellogg invariant-direction theorem to handle operator systems in product spaces, seeking component-wise eigenvalues rather than scalar eigenvalues with better localization properties.

Method: Develop theoretical framework for operator systems in product spaces, proving existence of component-wise eigenvalues with corresponding eigenvectors having all nontrivial components and norm localization properties.

Result: Successfully established new version of Birkhoff-Kellogg theorem that provides component-wise eigenvalues with localized eigenvectors, applicable to nonlinear eigenvalue problems in differential equations.

Conclusion: The new theorem offers improved localization properties for eigenvectors and provides qualitative solution properties for systems of PDEs and ODEs, demonstrated through explicit examples.

Abstract: We provide a new version of the well-known Birkhoff-Kellogg
invariant-direction Theorem in product spaces. Our results concern operator
systems and give the existence of component-wise eigenvalues, instead of scalar
eigenvalues as in the classical case, that have corresponding eigenvectors with
all components nontrivial and localized by their norm. We also show that, when
applied to nonlinear eigenvalue problems for differential equations, this
localization property of the eigenvectors provides, in turn, qualitative
properties of the solutions. This is illustrated in two context of systems of
PDEs and ODEs. We illustrate the applicability of our theoretical results with
two explicit examples.

</details>


<div id='hep-th'></div>

# hep-th [[Back]](#toc)

### [134] [Canonical Forms as Dual Volumes](https://arxiv.org/abs/2509.02239)
*Elia Mazzucchelli,Prashanth Raman*

Main category: hep-th

TL;DR: Dual volume representations of canonical forms for positive geometries using Laplace transforms of measures on convex dual sets, identifying completely monotone geometries characterized by hyperbolic polynomials.


<details>
  <summary>Details</summary>
Motivation: To establish connections between positive geometries and probability measures through dual volume representations, linking to PDEs, hyperbolicity, convexity, and optimization.

Method: Expressing rational canonical functions as Laplace transforms of measures on convex duals of semialgebraic sets, focusing on hyperbolic polynomial boundaries and hyperbolicity regions.

Result: Identified completely monotone geometries including simplex-like minimal spectrahedra with Wishart-related measures, computed explicit measures for projective plane geometries with transcendental period evaluations.

Conclusion: Dual volume perspective reinterprets positive geometries by replacing logarithmic forms with probability measures, creating new connections across multiple mathematical disciplines.

Abstract: We study dual volume representations of canonical forms for positive
geometries in projective spaces, expressing their rational canonical functions
as Laplace transforms of measures supported on the convex dual of the
semialgebraic set. When the measure is non-negative, we term the geometry
completely monotone, reflecting the property of its canonical function. We
identify a class of positive geometries whose canonical functions admit such
dual volume representations, characterized by the algebraic boundary cut out by
a hyperbolic polynomial, for which the geometry is a hyperbolicity region. In
particular, simplex-like minimal spectrahedra are completely monotone, with
representing measures related to the Wishart distribution, capturing volumes of
spectrahedra or their boundaries. We explicitly compute these measures for
positive geometries in the projective plane bounded by lines and conics or by a
nodal cubic, revealing periods evaluating to transcendental functions. This
dual volume perspective reinterprets positive geometries by replacing
logarithmic differential forms with probability measures on the dual, forging
new connections to partial differential equations, hyperbolicity, convexity,
positivity, algebraic statistics, and convex optimization.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [135] [Spectral bounds and heat kernel upper estimates for Dirichlet forms](https://arxiv.org/abs/2509.01115)
*Aobo Chen,Zhenyu Yu*

Main category: math.PR

TL;DR: Characterizes heat kernel bounds using Harnack-type inequalities and spectral methods for Dirichlet forms with variable scale functions, generalizing previous work.


<details>
  <summary>Details</summary>
Motivation: To extend heat kernel analysis to more general Dirichlet forms where scale functions vary with position, building upon existing work by Mariano and Wang.

Method: Uses Harnack-type inequalities on exit times and spectral bounds to characterize upper bounds of heat kernels for regular Dirichlet forms without killing part.

Result: Shows that the Harnack-type inequality is preserved under quasi-symmetric changes of metric on uniformly perfect metric spaces.

Conclusion: Generalizes previous results and provides a framework for analyzing heat kernels in more flexible geometric settings with variable scale functions.

Abstract: We use a Harnack-type inequality on exit times and spectral bounds to
characterize upper bounds of the heat kernel associated with any regular
Dirichlet form without killing part, where the scale function may vary with
position. We further show that this Harnack-type inequality is preserved under
quasi-symmetric changes of metric on uniformly perfect metric spaces. This
generalised the work of Mariano and Wang [Stochastic Process. Appl. 189 (2025)
104707)].

</details>


### [136] [Quantitative positivity of transition densities for random perturbations of Hamiltonian systems](https://arxiv.org/abs/2509.02448)
*Shimaa Elesaely,David P. Herzog,Kyle L. Liss*

Main category: math.PR

TL;DR: The paper establishes uniform pointwise lower bounds for transition densities of perturbed Hamiltonian systems, providing quantitative minorization estimates without requiring knowledge of invariant measures.


<details>
  <summary>Details</summary>
Motivation: To develop a general framework for analyzing diffusion processes from random perturbations of conservative Hamiltonian systems, particularly for physically relevant systems where invariant measures are inaccessible.

Method: Under abstract hypotheses including Hamiltonian structural assumptions, weak Lyapunov structure, and quantitative hypoellipticity, the authors prove sharp uniform pointwise lower bounds on transition densities in the small noise limit.

Result: The general theorem yields quantitative minorization estimates for various models including Langevin dynamics, oscillator chains, and finite-dimensional fluid models like Navier-Stokes truncations and Lorenz '96 system.

Conclusion: The approach provides sharp exponential convergence rates to equilibrium without requiring explicit knowledge of invariant measures, making it broadly applicable to physically relevant systems.

Abstract: We study a class of diffusion processes arising from random perturbations of
conservative Hamiltonian systems. Under a set of abstract hypotheses --
including basic structural assumptions on the Hamiltonian, a weak Lyapunov
structure, and a quantitative notion of hypoellipticity -- we prove that
transition densities satisfy a sharp, uniform pointwise lower bound over
Hamiltonian sublevel sets in the small noise limit $\epsilon \to 0$. By
applying our general theorem, we obtain quantitative minorization estimates for
a variety of models including Langevin dynamics, chains of oscillators coupled
to heat bathes at different temperatures, and finite-dimensional fluid models
such as stochastically forced Galerkin truncations of the Navier-Stokes
equations and the Lorenz '96 system. As a corollary, assuming a stronger
Lyapunov structure, our main result yields a sharp exponential rate of
convergence to equilibrium for $0 < \epsilon \ll 1$ in a weighted total
variation norm. A central feature of our approach is that it does not require
knowledge of the explicit form of the invariant measure, nor even its
existence, and hence is broadly applicable to deduce minorization for
physically relevant systems where invariant measures are inaccessible.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [137] [On the stability of Ricci flow on hyperbolic 3-manifolds of finite volume](https://arxiv.org/abs/2509.00188)
*Ruojing Jiang,Franco Vargas Pallete*

Main category: math.DG

TL;DR: Normalized Ricci-DeTurck flow converges exponentially to hyperbolic metric on finite-volume 3-manifolds when initial metric is sufficiently close to hyperbolic metric, using interpolation theory.


<details>
  <summary>Details</summary>
Motivation: To establish stability of hyperbolic metrics under Ricci flow and provide tools for studying minimal surface entropy, which quantifies growth of closed minimal surfaces by genus.

Method: Applied normalized Ricci-DeTurck flow with interpolation theory to analyze convergence in weighted Hölder norms on hyperbolic 3-manifolds of finite volume.

Result: Proved that for initial metrics sufficiently close to hyperbolic metric, the flow exists for all time and converges exponentially fast to the hyperbolic metric.

Conclusion: The convergence result provides a valuable tool for investigating minimal surface entropy and understanding the stability of hyperbolic metrics under Ricci flow.

Abstract: On a hyperbolic 3-manifold of finite volume, we prove that if the initial
metric is sufficiently close to the hyperbolic metric $h_0$, then the
normalized Ricci-DeTurck flow exists for all time and converges exponentially
fast to $h_0$ in a weighted H\"older norm. A key ingredient of our approach is
the application of interpolation theory.
  Furthermore, this result is a valuable tool for investigating minimal surface
entropy, which quantifies the growth rate of the number of closed minimal
surfaces in terms of genus. We explore this in [17].

</details>


### [138] [Regularity of cylindrical singular sets of mean curvature flow](https://arxiv.org/abs/2509.01707)
*Ao Sun,Zhihan Wang,Jinxin Xue*

Main category: math.DG

TL;DR: Analysis of k-cylindrical singular sets in mean curvature flow, showing they are locally contained in k-dimensional C^{2,α}-submanifolds after removing lower-dimensional parts, with curvature determined by asymptotic flow profiles.


<details>
  <summary>Details</summary>
Motivation: To understand the structure and behavior of k-cylindrical singular sets in mean curvature flow, which are important for characterizing flow singularities and their geometric properties.

Method: Based on a new L^2-distance non-concentration property introduced in previous work, modified into a relative version to handle low eigenmodes that don't decay fast enough and don't contribute to curvature.

Result: Proved that k-cylindrical singular sets are locally contained in k-dimensional C^{2,α}-submanifolds after removing lower-dimensional parts, with curvature determined by asymptotic flow profiles. Also provided detailed asymptotic profile and graphical radius estimates.

Conclusion: The study provides a comprehensive understanding of k-cylindrical singular sets in mean curvature flow, establishing their submanifold structure and connecting their curvature to the asymptotic behavior of the flow at singularities.

Abstract: In this paper, we study the $k$-cylindrical singular set of mean curvature
flow in $\mathbb R^{n+1}$ for each $1\leq k\leq n-1$. We prove that they are
locally contained in a $k$-dimensional $C^{2,\alpha}$-submanifold after
removing some lower-dimensional parts. Moreover, if the $k$-cylindrical
singular set is a $k$-submanifold, then its curvature is determined by the
asymptotic profile of the flow at these singularities. As a byproduct, we
provide a detailed asymptotic profile and graphical radius estimate at these
singularities. The proof is based on a new $L^2$-distance non-concentration
property that we introduced in [SWX25], modified into a relative version that
allows us to modulo those low eigenmodes that are not decaying fast enough and
do not contribute to the curvature of the singular set.

</details>


### [139] [Rigidity in the Ginzburg--Landau approximation of harmonic spheres](https://arxiv.org/abs/2509.02389)
*Matilde Gianocca*

Main category: math.DG

TL;DR: Degree-one harmonic maps from S² to S² cannot all be approximated by Ginzburg-Landau critical points; only rotations can be approximated.


<details>
  <summary>Details</summary>
Motivation: To understand which harmonic maps from the 2-sphere to itself can be obtained as limits of Ginzburg-Landau critical points, particularly investigating the approximation capabilities of these energy minimizers.

Method: Prove a rigidity theorem showing that for small ε and energy below 8π-γ, the only critical points of the Ginzburg-Landau energy are rotations (up to conjugation), using energy estimates and geometric analysis.

Result: Only rotations (degree-one harmonic maps that are rotations) can be approximated by Ginzburg-Landau critical points; other harmonic maps cannot arise as such limits.

Conclusion: There is a fundamental limitation in approximating harmonic maps via Ginzburg-Landau theory - only a specific subclass (rotations) can be captured, revealing structural constraints in this approximation scheme.

Abstract: We prove that not every harmonic map from $S^{2}$ to $S^{2}$ can arise as a
limit of Ginzburg--Landau critical points. More precisely, we show that the
only degree-one harmonic maps that can be approximated in this way are
rotations.
  This conclusion follows from a rigidity theorem: we show that for every
$\gamma>0$ and $\varepsilon$ small enough, the only critical points
$u_\varepsilon:S^{2}\to\mathbb R^{3}$ of the Ginzburg--Landau energy
$E_\varepsilon$ with energy below $8\pi-\gamma$ are (up to conjugation)
rotations, that is
$u_\varepsilon(x)=\sqrt{1-2\varepsilon^{2}}\;R_\varepsilon\,x$.

</details>
