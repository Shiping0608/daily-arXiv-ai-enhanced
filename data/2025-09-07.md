<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 11]
- [math.AP](#math.AP) [Total: 12]
- [physics.comp-ph](#physics.comp-ph) [Total: 3]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 9]
- [math-ph](#math-ph) [Total: 1]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [stat.CO](#stat.CO) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [math.SP](#math.SP) [Total: 1]
- [math.FA](#math.FA) [Total: 2]
- [cs.LG](#cs.LG) [Total: 1]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 2]
- [quant-ph](#quant-ph) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [math.CV](#math.CV) [Total: 1]
- [q-bio.PE](#q-bio.PE) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Stabilization-free virtual element method for 2D third medium contact](https://arxiv.org/abs/2509.03630)
*Bing-Bing Xu,Peter Wriggers*

Main category: math.NA

TL;DR: SFVEM enables third medium contact simulation without stabilization terms, simplifying complex contact problems with large deformations.


<details>
  <summary>Details</summary>
Motivation: Traditional VEM requires complex stabilization terms and second-order schemes for third medium contact, making implementation difficult.

Method: Developed stabilization-free virtual element method (SFVEM) that eliminates need for additional stabilization terms, simplifies regularization term construction, and uses high-order projection operators.

Result: SFVEM successfully solves complex 2D third medium contact problems, demonstrating effectiveness and applicability without traditional stabilization requirements.

Conclusion: SFVEM provides a simplified and effective approach for third medium contact simulation, overcoming limitations of traditional VEM while maintaining flexibility for arbitrary polygonal elements.

Abstract: The third medium contact has been proven to be an effective approach for
simulating contact problems involving large deformations. Unlike traditional
contact algorithms, the third medium contact introduces a third medium between
two contacting bodies, thereby avoiding the complex treatment of the contact
constraints. The approach has been successfully applied in different
applications in the framework of the finite element method (FEM). As a
generalization of the finite element method, the virtual element method (VEM)
can handle arbitrary polygonal elements, providing greater flexibility for
modeling third medium contact. However, due to the introduction of the
projection operator, VEM requires additional stabilization terms to control the
rank of the stiffness matrix. Moreover, the regularization term in the third
medium contact formulation requires a second-order numerical scheme, which
further complicates the application of VEM to such problems. In this work, the
stabilization-free virtual element method (SFVEM) is developed and applied to
solve the third medium contact problems. Different from the traditional VEM,
SFVEM does not require additional stabilization terms, which simplifies the
construction of the regularization term in third medium contact. Building upon
the traditional second-order FEM framework, we present the specific format of
SFVEM for solving third medium contact, including the construction of
high-order projection operator and the tangent stiffness matrix. Numerical
examples are provided to demonstrate the effectiveness and applicability of
SFVEM in solving complex 2D third medium contact problems.

</details>


### [2] [ARDO: A Weak Formulation Deep Neural Network Method for Elliptic and Parabolic PDEs Based on Random Differences of Test Functions](https://arxiv.org/abs/2509.03757)
*Wei Cai,Andrew Qing He*

Main category: math.NA

TL;DR: ARDO method: derivative-free deep learning approach for PDEs using weak adversarial formulation with random difference operators on test functions


<details>
  <summary>Details</summary>
Motivation: To develop a fully derivative-free framework for solving PDEs with neural networks, particularly addressing the challenge of computing derivatives with respect to solution networks in traditional methods

Method: Uses weak adversarial formulation that transfers random difference operators onto test functions instead of computing derivatives of the solution neural network

Result: Creates a derivative-free framework suitable for Fokker-Planck type second-order elliptic and parabolic PDEs

Conclusion: ARDO method provides an effective derivative-free alternative for solving PDEs with deep learning, offering particular advantages for complex second-order PDE problems

Abstract: We propose ARDO method for solving PDEs and PDE-related problems with deep
learning techniques. This method uses a weak adversarial formulation but
transfers the random difference operator onto the test function. The main
advantage of this framework is that it is fully derivative-free with respect to
the solution neural network. This framework is particularly suitable for
Fokker-Planck type second-order elliptic and parabolic PDEs.

</details>


### [3] [Error and long-term analysis of two-step symmetric methods for relativistic charged-particle dynamics](https://arxiv.org/abs/2509.03886)
*Ting Li,Bin Wang,Ruili Zhang*

Main category: math.NA

TL;DR: Two-step symmetric methods for relativistic charged-particle dynamics that preserve geometric structures like energy, mass shell, and phase-space volume with second-order accuracy.


<details>
  <summary>Details</summary>
Motivation: To develop numerical methods that maintain important geometric properties of relativistic charged-particle systems over long time periods, ensuring structural preservation in simulations.

Method: Developed four two-step symmetric methods based on splitting schemes - one exactly preserves mass shell and phase-space volume, three modified versions with rigorous backward error analysis for long-time near-conservation.

Result: All methods achieve second-order accuracy and successfully preserve geometric structures, with numerical experiments confirming theoretical conservation properties.

Conclusion: The proposed two-step symmetric methods effectively conserve geometric structures in relativistic charged-particle dynamics, providing reliable long-time simulation capabilities with proven accuracy and preservation properties.

Abstract: In this work, we consider the error estimates and the long-time conservation
or near-conservation of geometric structures, including energy, mass shell and
phase-space volume, for four two-step symmetric methods applied to relativistic
charged-particle dynamics. We begin by introducing a two-step symmetric
numerical method based on a splitting scheme that exactly preserves the mass
shell and the phase-space volume of the relativistic system. Building on this
formulation, we develop three additional two-step symmetric methods with
further modifications, for which the long-time near-conservation of energy and
mass shell can be rigorously established through the backward error analysis.
All methods are shown to achieve second-order accuracy. The theoretical results
are illustrated and complemented by numerical experiments.

</details>


### [4] [Fluid boundary conditions in kinetic-diffusion Monte Carlo](https://arxiv.org/abs/2509.03942)
*Thijs Steel,Vince Maes,Giovanni Samaey*

Main category: math.NA

TL;DR: Extension of KDMC method to handle boundary conditions without switching to slower purely kinetic methods, achieving 500x speedup with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: The KDMC method is efficient for simulating neutral particles in fusion reactors but becomes slow near boundaries where it must switch to purely kinetic methods.

Method: Extended the KDMC method to accurately incorporate boundary conditions directly within the hybrid fluid-kinetic framework, eliminating the need for switching to purely kinetic approaches.

Result: Experimental results show up to 500 times speedup compared to traditional KDMC that switches to purely kinetic methods, while maintaining acceptable accuracy levels.

Conclusion: The extended KDMC method successfully addresses the boundary condition limitation, providing dramatic performance improvements for fusion reactor simulations without significant accuracy compromise.

Abstract: The Kinetic-Diffusion Monte Carlo (KDMC) method is a powerful tool for
simulating neutral particles in fusion reactors. It is a hybrid fluid-kinetic
method that is significantly faster than pure kinetic methods at the cost of a
small bias due to fluid approximations. Unfortunately, when simulating
particles close to a boundary, it needs to switch to a purely kinetic method,
which is significantly slower. In this paper, we will extend the method so that
it can accurately take boundary conditions into account without switching to a
purely kinetic method. Experiments show that this extension can lead to a
speedup of up to 500 times compared to a KDMC method that switches to a purely
kinetic method, while not sacrificing too much accuracy.

</details>


### [5] [A unified stabilized virtual element method for the generalized Oseen equation: stability and robustness](https://arxiv.org/abs/2509.04113)
*Sudheer Mishra,E Natarajan*

Main category: math.NA

TL;DR: A novel local projection stabilized virtual element method for generalized Oseen problem using equal-order elements on polygonal meshes, providing stability and optimal error estimates.


<details>
  <summary>Details</summary>
Motivation: To address stability issues in convection-dominated regimes and enable use of equal-order element pairs on general polygonal meshes for generalized Oseen problems.

Method: Local projection based stabilization techniques for conforming virtual element method with equal-order element pairs, ensuring discrete inf-sup condition in energy norm.

Result: Method demonstrates stability for generalized Oseen, Brinkman, and Stokes equations without additional conditions. Optimal error estimates show uniform convergence, especially for small diffusion cases. Quasi-robust parameter performance.

Conclusion: The proposed method offers simplified construction, easier implementation than residual-based techniques, avoids element pair coupling, and is validated through numerical experiments across diffusion and convection-dominated regimes.

Abstract: In this thesis, we investigate a novel local projection based stabilized
conforming virtual element method for the generalized Oseen problem using
equal-order element pairs on general polygonal meshes. To ensure the stability,
particularly in the presence of convection-dominated regimes and the
utilization of equal-order element pairs, we introduce local projections based
stabilization techniques. We demonstrate the discrete inf-sup condition in the
energy norm. Moreover, the stability of the proposed method also guarantees the
stability properties for the Brinkman equation and the Stokes equation without
introducing any additional conditions. Furthermore, we derive an optimal error
estimates in the energy norm that underline the uniform convergence in the
energy norm for the generalized Oseen problem with small diffusion. In
addition, the error estimates remain valid and uniform for the Brinkman
equation and the Stokes equation. Additionally, the convergence study shows
that the proposed method is quasi-robust with respect to parameters. The
proposed method offers several advantages, including simplicity in
construction, easier implementation compared to residual-based stabilization
techniques, and avoiding coupling between element pairs. We validate our
theoretical findings through a series of numerical experiments, including
diffusion-dominated and convection-dominated regimes.

</details>


### [6] [A posteriori error estimates and space-adaptive mesh refinements for time-dependent scattering problems](https://arxiv.org/abs/2509.04217)
*Théophile Chaumont-Frelet,Heiko Gimperlein,Ignacio Labarca-Figueroa,Jörg Nick*

Main category: math.NA

TL;DR: A posteriori error analysis for time-dependent acoustic scattering using boundary integral equations with convolution quadrature and boundary elements, featuring reliable residual-based error estimators and adaptive mesh refinement with improved temporal convergence.


<details>
  <summary>Details</summary>
Motivation: To develop reliable error estimation and adaptive refinement methods for time-dependent acoustic scattering problems, which are challenging due to the coupling of temporal and spatial discretizations in boundary integral formulations.

Method: Discretization using convolution quadrature method in time and boundary elements in space, with residual-type error estimator for reliability analysis. A temporal shift modification is introduced to recover full classical convergence order for boundary densities.

Result: Proved reliability of residual-type error estimator, demonstrated that adaptive scheme yields asymptotically optimal meshes for 2D acoustic scattering problems, and achieved improved temporal convergence order of 2m-1 with the modified convolution quadrature method.

Conclusion: The proposed error estimation and adaptive refinement approach provides effective tools for solving time-dependent acoustic scattering problems, with the temporal shift modification significantly improving convergence rates while maintaining computational efficiency.

Abstract: This work studies a posteriori error estimates and their use for
time-dependent acoustic scattering problems, formulated as a time-dependent
boundary integral equation based on a single-layer ansatz. The integral
equation is discretized by the convolution quadrature method in time and by
boundary elements in space. We prove the reliability of an error estimator of
residual type and study the resulting space-adaptive mesh refinements.
Moreover, we present a simple modification of the convolution quadrature method
based on temporal shifts, which recovers, for the boundary densities, the full
classical temporal convergence order $2m-1$ of the temporal convolution
quadrature method based on the $m$-stage convolution quadrature
semi-discretization. We numerically observe that the adaptive scheme yields
asymptotically optimal meshes for an acoustic scattering problem in two
dimensions.

</details>


### [7] [An explicit splitting SAV scheme for the kinetic Langevin dynamics](https://arxiv.org/abs/2509.04251)
*Lei Dai,Yingsong Jiang,Xiaojie Wang*

Main category: math.NA

TL;DR: A novel explicit splitting scalar auxiliary variable (SSAV) scheme for kinetic Langevin dynamics that handles non-globally Lipschitz continuous gradients with superlinear growth, providing strong convergence and effective sampling of invariant distributions.


<details>
  <summary>Details</summary>
Motivation: Kinetic Langevin dynamics has important applications in molecular dynamics and Hamiltonian Monte Carlo sampling, but existing methods struggle with non-globally Lipschitz continuous gradients with superlinear growth. There is a need for efficient, robust explicit schemes that can reproduce the energy structure of the original dynamics.

Method: The authors propose a splitting scalar auxiliary variable (SSAV) scheme, which is an explicit method that maintains the energy structure of the original dynamics. The scheme uses an energy argument to prove exponential integrability and establishes order-one strong convergence without requiring global monotonicity conditions.

Result: The SSAV scheme demonstrates polynomial growth of moments with respect to time length, enabling weak error estimates of order one with polynomial (not exponential) dependence on time. Despite polynomial growth, the explicit scheme remains computationally effective for approximating invariant distributions with exponential ergodicity.

Conclusion: Numerical experiments confirm the theoretical findings and show the superiority of the SSAV algorithm in sampling applications. The method provides an efficient and robust explicit scheme for kinetic Langevin dynamics with challenging gradient conditions.

Abstract: The kinetic Langevin dynamics finds diverse applications in various
disciplines such as molecular dynamics and Hamiltonian Monte Carlo sampling. In
this paper, a novel splitting scalar auxiliary variable (SSAV) scheme is
proposed for the dynamics, where the gradient of the potential $U$ is possibly
non-globally Lipschitz continuous with superlinear growth. As an explicit
scheme, the SSAV method is efficient, robust and is able to reproduce the
energy structure of the original dynamics. By an energy argument, the SSAV
scheme is proved to possess an exponential integrability property, which is
crucial to establishing the order-one strong convergence without the global
monotonicity condition. Moreover, moments of the numerical approximations are
shown to have polynomial growth with respect to the time length. This helps us
to obtain weak error estimates of order one, with error constants polynomially
(not exponentially) depending on the time length. Despite the obtained
polynomial growth, the explicit scheme is shown to be computationally effective
for the approximation of the invariant distribution of the dynamics with
exponential ergodicity. Numerical experiments are presented to confirm the
theoretical findings and to show the superiority of the algorithm in sampling.

</details>


### [8] [Error analysis for learning the time-stepping operator of evolutionary PDEs](https://arxiv.org/abs/2509.04256)
*Ke Chen,Meenakshi Krishnan,Haizhao Yang*

Main category: math.NA

TL;DR: Theoretical framework for analyzing neural network approximation of PDE solution operators with explicit error bounds and generalization guarantees.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks offer data-driven alternatives to classical PDE solvers that face stability and computational cost challenges, but lack rigorous theoretical analysis.

Method: Derive explicit error estimates for feedforward neural networks approximating time-stepping operators, establish Lipschitz continuity properties, and identify low-complexity structures in PDE operators.

Result: Obtained generalization bounds showing efficient learnability without curse of dimensionality for various PDE classes including reaction-diffusion, parabolic, and conservation law equations.

Conclusion: The framework provides rigorous theoretical foundation for neural network-based PDE solvers and extends to multi-input operator learning settings.

Abstract: Deep neural networks (DNNs) have recently emerged as effective tools for
approximating solution operators of partial differential equations (PDEs)
including evolutionary problems. Classical numerical solvers for such PDEs
often face challenges of balancing stability constraints and the high
computational cost of iterative solvers. In contrast, DNNs offer a data-driven
alternative through direct learning of time-stepping operators to achieve this
balancing goal. In this work, we provide a rigorous theoretical framework for
analyzing the approximation of these operators using feedforward neural
networks (FNNs). We derive explicit error estimates that characterize the
dependence of the approximation error on the network architecture -- namely its
width and depth -- as well as the number of training samples. Furthermore, we
establish Lipschitz continuity properties of time-stepping operators associated
with classical numerical schemes and identify low-complexity structures
inherent in these operators for several classes of PDEs, including
reaction-diffusion equations, parabolic equations with external forcing, and
scalar conservation laws. Leveraging these structural insights, we obtain
generalization bounds that demonstrate efficient learnability without incurring
the curse of dimensionality. Finally, we extend our analysis from single-input
operator learning to a general multi-input setting, thereby broadening the
applicability of our results.

</details>


### [9] [Projection-based stabilization for high-order incompressible flow solvers](https://arxiv.org/abs/2509.04352)
*Antonio Blanco-Casares,Vishal Kumar,Daniel Mira,Oriol Lehmkuhl*

Main category: math.NA

TL;DR: Novel stabilization strategy for incompressible Navier-Stokes equations using fractional step method with high-order accuracy on unstructured meshes.


<details>
  <summary>Details</summary>
Motivation: To achieve high accuracy while ensuring convergence and compatibility with high-order elements on unstructured meshes for incompressible flow simulations.

Method: Fractional step method with carefully defined boundary conditions and a stabilization strategy that shares the same principle as natural stabilization in fractional step methods, relying on the difference between gradient operator and its projection.

Result: Numerical dissipation diminishes with increasing polynomial order, and test cases confirm convergence under both mesh refinement and increasing polynomial order.

Conclusion: The proposed stabilization method is effective, seamlessly integrated, and enables high-order temporal accuracy for incompressible Navier-Stokes equations on unstructured meshes.

Abstract: This work presents a novel stabilization strategy for the Galerkin
formulation of the incompressible Navier-Stokes equations, developed to achieve
high accuracy while ensuring convergence and compatibility with high-order
elements on unstructured meshes. The numerical algorithm employs a fractional
step method with carefully defined boundary conditions to obtain a consistent
pressure field, enabling high-order temporal accuracy. The proposed
stabilization is seamlessly integrated into the algorithm and shares the same
underlying principle as the natural stabilization inherent in the fractional
step method, both rely on the difference between the gradient operator and its
projection. The numerical dissipation associated to the stabilization term is
found to diminish with increasing polynomial order of the elements. Numerical
test cases confirm the effectiveness of the method, demonstrating convergence
under mesh refinement and increasing polynomial order.

</details>


### [10] [Randomized biorthogonalization through a two-sided Gram-Schmidt process](https://arxiv.org/abs/2509.04386)
*Laura Grigori,Lorenzo Piccinini,Igor Simunec*

Main category: math.NA

TL;DR: Randomized two-sided Gram-Schmidt process for biorthogonalization using sketching matrices to reduce computational cost and improve numerical stability compared to classical methods.


<details>
  <summary>Details</summary>
Motivation: To develop a computationally efficient and numerically stable alternative to the classical two-sided Gram-Schmidt process for biorthogonalizing matrices, addressing issues of high computational cost and numerical instability in deterministic approaches.

Method: Proposes a randomized algorithm that uses sketching matrices with oblivious subspace embedding properties to replace exact biorthogonality conditions with sketch-based approximations, maintaining range preservation while reducing computational requirements.

Result: The randomized approach demonstrates lower computational cost, better numerical stability, and often produces computed bases with smaller condition numbers compared to deterministic methods. Multiple implementations are analyzed and numerically compared.

Conclusion: The randomized two-sided Gram-Schmidt process provides an effective and efficient alternative to classical biorthogonalization methods, with applications in nonsymmetric Lanczos algorithms for eigenvalue and eigenvector approximation.

Abstract: We propose and analyze a randomized two-sided Gram-Schmidt process for the
biorthogonalization of two given matrices $X, Y \in\mathbb{R}^{n\times m}$. The
algorithm aims to find two matrices $Q, P \in\mathbb{R}^{n\times m}$ such that
${\rm range}(X) = {\rm range}(Q)$, ${\rm range}(Y) = {\rm range}(P)$ and
$(\Omega Q)^T \Omega P = I$, where $\Omega \in\mathbb{R}^{s \times n}$ is a
sketching matrix satisfying an oblivious subspace $\varepsilon$-embedding
property; in other words, the biorthogonality condition on the columns of $Q$
and $P$ is replaced by an equivalent condition on their sketches. This
randomized approach is computationally less expensive than the classical
two-sided Gram-Schmidt process, has better numerical stability, and the
condition number of the computed bases $Q, P$ is often smaller than in the
deterministic case. Several different implementations of the randomized
algorithm are analyzed and compared numerically. The randomized two-sided
Gram-Schmidt process is applied to the nonsymmetric Lancozs algorithm for the
approximation of eigenvalues and both left and right eigenvectors.

</details>


### [11] [An Arbitrary-Order Moving-Mesh Finite Element Algorithm for One-Dimensional Implicit Moving Boundary Problems](https://arxiv.org/abs/2509.04409)
*Matthew E Hubbard,Thomas J Radley*

Main category: math.NA

TL;DR: A high-order moving-mesh finite element method for moving boundary problems with implicit boundary velocity dependence, using conservative ALE approach and achieving arbitrary-order accuracy without geometric conservation law requirements.


<details>
  <summary>Details</summary>
Motivation: To develop an accurate and efficient method for moving boundary problems where boundary velocity depends on the interior solution, addressing challenges in maintaining high-order accuracy while handling mesh movement.

Method: Uses conservative arbitrary Lagrangian-Eulerian approach with two-stage algorithm: computes velocity field for mesh movement (boundary and internal), then solves PDE on updated mesh. Employs high-order approximation for boundary velocities and linear mesh velocity variation internally.

Result: The method achieves arbitrary-order accuracy in space and time, is explicit in time, requires only linear system inversions per time-step, operates fully in physical domain without reference mapping, and doesn't need discrete geometric conservation law.

Conclusion: The proposed moving-mesh finite element method successfully handles moving boundary problems with implicit boundary velocity dependence while maintaining high-order accuracy through a conservative ALE framework and linear mesh velocity assumption.

Abstract: We present a one-dimensional high-order moving-mesh finite element method for
moving boundary problems where the boundary velocity depends implicitly on the
solution in the interior of the domain. The method employs a conservative
arbitrary Lagrangian-Eulerian approach to predict the evolution of the
approximate solution. It retains the order of accuracy of the underlying
fixed-mesh method by (i) computing the boundary velocities using a high-order
approximation to a distributed local conservation principle which generates a
Lagrangian `flow' velocity and (ii) assuming a continuous piecewise linear
variation of the mesh velocity field in the interior of the computational
domain.
  Within each time-step the algorithm consists of two stages: the computation
of a velocity field with which to move both the domain boundary and the
internal mesh, and the approximation of the solution to the PDE on the updated
mesh. Both internal and boundary velocities are generated within the same
framework, though it would be simple to replace the internal mesh velocity
field by applying a more traditional mesh movement strategy (for example, one
which seeks to equidistribute an error indicator at each time-step): the
high-order accuracy should be retained as long the discrete velocity which is
used in the solution update is assumed to vary linearly within mesh elements.
The proposed method is explicit in time, requires only the inversion of linear
systems of equations within each time-step, and is implemented fully in the
physical domain, so no mapping to a reference domain is employed. It attains
arbitrary-order accuracy in both space and time without the need to satisfy a
discrete geometric conservation law.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [12] [Asymptotic Stability of multi-solitons for $1$d Supercritical NLS](https://arxiv.org/abs/2509.03637)
*Gong Chen,Abdon Moutinho*

Main category: math.AP

TL;DR: Asymptotic stability of multi-solitons for 1D supercritical NLS on finite-codimension manifolds when soliton velocities are sufficiently separated and k > 11/4


<details>
  <summary>Details</summary>
Motivation: Previous work showed solitary waves are unstable, but Krieger and Schlag established asymptotic stability on codimension-one center-stable manifold. This paper extends to multi-soliton configurations.

Method: Uses linear estimates developed for one-dimensional matrix charge transfer models from previous work to prove asymptotic stability

Result: Proves asymptotic stability of multi-solitons on finite-codimension manifold for k > 11/4 when soliton velocities are sufficiently separated

Conclusion: Extends previous stability results from single solitary waves to multi-soliton configurations under specific conditions on nonlinearity strength and velocity separation

Abstract: Consider the one-dimensional $L^2$ supercritical nonlinear Schr\"odinger
equation \begin{equation} i\partial_{t}\psi+\partial^{2}_{x}\psi+\vert
\psi\vert^{2k}\psi=0 \text{, $k>2$}. \end{equation} It is well known that
solitary waves for this equation are unstable. In the pioneering work of
Krieger and Schlag \cite{KriegerSchlag}, the asymptotic stability of a solitary
wave was established on a codimension-one center-stable manifold. In the
present paper, using linear estimates developed for one-dimensional matrix
charge transfer models in our previous work, \cite{dispanalysis1}, we prove
asymptotic stability of multi-solitons on a finite-codimension manifold for
$k>\frac{11}{4}$, provided that the soliton velocities are sufficiently
separated.

</details>


### [13] [Global hypoellipticity for a class of time-periodic operators on asymptotically Euclidean manifolds](https://arxiv.org/abs/2509.03745)
*Fernando de Ávila Silva,Matteo Bonino,Sandro Coriasco*

Main category: math.AP

TL;DR: Introduction of time-periodic Gevrey-Sobolev-Kato spaces on asymptotically Euclidean manifolds with Fourier expansion characterization and application to global hypoellipticity of time-periodic evolution equations.


<details>
  <summary>Details</summary>
Motivation: To develop specialized function spaces for studying time-periodic phenomena on asymptotically Euclidean manifolds and apply them to understand global hypoellipticity properties of evolution equations.

Method: Define time-periodic Gevrey-Sobolev-Kato spaces, characterize them through Fourier expansions associated with suitable elliptic operators, and apply this framework to analyze global hypoellipticity.

Result: Development of a mathematical framework for characterizing function spaces and establishing global hypoellipticity results for time-periodic evolution equations on asymptotically Euclidean manifolds.

Conclusion: The introduced time-periodic Gevrey-Sobolev-Kato spaces provide an effective tool for studying global hypoellipticity problems in time-periodic evolution equations on asymptotically Euclidean manifolds.

Abstract: We introduce time-periodic Gevrey-Sobolev-Kato spaces on asymptotically
Euclidean manifolds and study their characterisation throughout Fourier
expansions associated with suitable elliptic operators. As an application, we
study the global hypoellipticity problem for a naturally associated class of
time-periodic evolution equations.

</details>


### [14] [Inverse problem for semidiscrete stochastic parabolic operators in arbitrary dimensions](https://arxiv.org/abs/2509.03760)
*Rodrigo Lecaros,Ariel A. Pérez,Manuel F. Prado*

Main category: math.AP

TL;DR: Analysis of inverse problems for semidiscrete stochastic parabolic equations using Carleman estimates to establish stability results for source identification and Cauchy problems.


<details>
  <summary>Details</summary>
Motivation: To solve two types of inverse problems in semidiscrete stochastic parabolic equations - identifying random source terms and reconstructing solutions from boundary data, which are important for practical applications involving stochastic processes.

Method: Established two Carleman estimates for semidiscrete stochastic parabolic operators (homogeneous and nonhomogeneous boundary conditions) and applied them to analyze the inverse problems.

Result: Obtained Lipschitz stability for the semidiscrete inverse source problem and Hölder stability for the semidiscrete Cauchy inverse problem.

Conclusion: The Carleman estimate approach successfully provides stability guarantees for both inverse problems in semidiscrete stochastic parabolic equations across arbitrary dimensions.

Abstract: In this paper, we study two types of inverse problems for space semidiscrete
stochastic parabolic equations in arbitrary dimensions. The first problem
concerns a semidiscrete inverse source problem, which involves determining the
random source term of the white noise in the stochastic parabolic equation
using observation data of the solution at the terminal time and the trace of
the discrete spatial derivative of the solution on a subdomain at the boundary
of the space. The second problem addresses a semidiscrete Cauchy inverse
problem, which involves determining the solution of the stochastic parabolic
equation in a subdomain of space-time, from the data of the solution and the
trace of the discrete spatial derivative on a subdomain of the boundary of
space and time. To address these problems, we establish two Carleman estimates
for a semidiscrete stochastic parabolic operator in arbitrary dimensions: one
for the homogeneous boundary condition and the other for the nonhomogeneous
boundary condition. Applying these Carleman estimates, we obtain Lipschitz and
H\"older stability for the first and second inverse problems, respectively.

</details>


### [15] [Hölder Stable Recovery of the Source in Space-Time Fractional Wave Equations](https://arxiv.org/abs/2509.03779)
*Kuang Huang,Zhiyuan Li,Zhidong Zhang,Zhi Zhou*

Main category: math.AP

TL;DR: Recovery of spatially dependent source in 1D space-time fractional wave equation using single endpoint boundary measurements, with bi-orthogonal basis approach for Mittag-Leffler functions to overcome non-orthogonality issues.


<details>
  <summary>Details</summary>
Motivation: The main challenge is that eigenfunctions of the Dirichlet eigenvalue problem do not form an orthogonal system due to the fractional derivative in space, making source recovery difficult with boundary measurements.

Method: Introduce a bi-orthogonal basis for Mittag-Leffler functions to establish uniqueness and Hölder-type stability results, then employ Tikhonov regularization method for numerical solution of the inverse source problem.

Result: Established uniqueness and stability results provided measurement time is sufficiently large. Numerical examples demonstrate accuracy and efficiency of the proposed method, validating theoretical findings.

Conclusion: The bi-orthogonal basis approach successfully addresses the non-orthogonality challenge in fractional wave equations, enabling effective source recovery with boundary measurements through Tikhonov regularization.

Abstract: We study the recovery of a spatially dependent source in a one-dimensional
space-time fractional wave equation using boundary measurement data collected
at a single endpoint. The main challenge arises from the fact that the
eigenfunctions of the Dirichlet eigenvalue problem do not form an orthogonal
system, due to the presence of a fractional derivative in space. To address
this difficulty, we introduce a bi-orthogonal basis for the Mittag-Leffler
functions and use it to establish uniqueness and H\"older-type stability
results, provided the measurement time is sufficiently large. A Tikhonov
regularization method is then employed to numerically solve the inverse source
problem. Several numerical examples are presented to demonstrate the accuracy
and efficiency of the proposed method and to validate our theoretical findings.

</details>


### [16] [Energy decay and blow-up of viscoelastic wave equations with polynomial nonlinearity and damping](https://arxiv.org/abs/2509.03799)
*Qingqing Peng,Yikan Liu*

Main category: math.AP

TL;DR: Analysis of energy decay and finite-time blow-up in viscoelastic wave equations with polynomial nonlinearity and weak damping, establishing decay conditions and blow-up time bounds.


<details>
  <summary>Details</summary>
Motivation: To study the stability and blow-up behavior of solutions to viscoelastic wave equations with polynomial nonlinearity and weak damping, providing quantitative bounds for energy decay and blow-up times.

Method: Established explicit decay results by imposing polynomial conditions on the relaxation function for small initial energy. Used Levine's convexity method to derive upper bounds for blow-up time when initial energy is below potential well depth, and provided lower bounds for blow-up time.

Result: Obtained general decay results for solutions under polynomial relaxation function conditions. Derived quantitative upper and lower bounds for blow-up times when solutions exhibit finite-time blow-up behavior.

Conclusion: The paper successfully provides comprehensive analysis of both energy decay and finite-time blow-up phenomena in viscoelastic wave equations, offering explicit conditions and bounds that contribute to understanding the stability and singularity formation in such systems.

Abstract: This paper is concerned with the energy decay and the finite time blow-up of
the solution to a viscoelastic wave equation with polynomial nonlinearity and
weak damping. We establish explicit and general decay results for the solutions
by imposing polynomial conditions on the relaxation function, provided that the
initial energy is sufficiently small. Furthermore, we derive an upper bound for
the blow-up time when the initial energy is less than the depth of the
potential well by utilizing Levine's convexity method. Additionally, we provide
a lower bound for the blow-up time if the solution blows up.

</details>


### [17] [A note on the non-$L^1$ asymptotic completeness of the Vlasov-Maxwell system](https://arxiv.org/abs/2509.04025)
*Emile Breton*

Main category: math.AP

TL;DR: Small data solutions to Vlasov-Maxwell system do not exhibit linear scattering behavior under generic charge conditions, demonstrating non-L^1 asymptotic completeness.


<details>
  <summary>Details</summary>
Motivation: To investigate the scattering properties and asymptotic behavior of solutions to the Vlasov-Maxwell system, specifically whether small data solutions exhibit linear scattering.

Method: The proof utilizes the Lorentz invariance property of the Vlasov-Maxwell equations to analyze the asymptotic completeness and scattering behavior of solutions.

Result: The study shows that under generic asymptotic conditions on the charge, small data solutions do not verify linear scattering, establishing non-L^1 asymptotic completeness of the system.

Conclusion: The Vlasov-Maxwell system lacks linear scattering properties for small data solutions, with the Lorentz invariance playing a crucial role in demonstrating this non-scattering behavior.

Abstract: We prove that under a generic asymptotic condition on the charge, the small
data solutions to the Vlasov-Maxwell system do not verify linear scattering. In
other words, we show the non-$L^1$ asymptotic completeness of the system. The
proof makes use of the Lorentz invariance of the equations.

</details>


### [18] [Existence and large radial solutions for an elliptic system under finite new Keller-Osserman integral conditions](https://arxiv.org/abs/2509.04099)
*Dragos-Patru Covei*

Main category: math.AP

TL;DR: Existence and behavior of entire positive radial solutions for semilinear elliptic systems with finite Keller-Osserman conditions and integrable weights.


<details>
  <summary>Details</summary>
Motivation: Extend classical Keller-Osserman theory to coupled elliptic systems with general nonlinearities and weight functions, addressing the existence and qualitative properties of positive radial solutions.

Method: Develop a novel subharmonic functional approach tailored to reciprocal integral conditions, establishing existence of infinitely many solutions, closedness of admissible central values, and blow-up behavior at boundary points.

Result: Proved existence of infinitely many entire positive radial solutions for nonempty sets of central values, demonstrated closedness of admissible central value sets, and showed blow-up at infinity for boundary solutions.

Conclusion: The framework successfully extends Keller-Osserman theory to coupled systems with general nonlinearities and weights, providing comprehensive existence and qualitative results through a specialized functional approach.

Abstract: We investigate the existence and qualitative behavior of entire positive
radial solutions to the semilinear elliptic system% \begin{equation*} \Delta
u=p(|x|)\,g(v),\qquad \Delta v=q(|x|)\,f(u),\qquad x\in \mathbb{R}% ^{n},\
n\geq 3, \end{equation*}% under finite Keller--Osserman-type integral
conditions on the nonlinearities $f$ and $g$, and integrability constraints on
the radial weights $p$ and $q$% . The nonlinearities are assumed continuous on
$[0,\infty )$, differentiable on $(0,\infty )$, vanish at the origin, and are
strictly positive elsewhere, with% \begin{equation*} \int_{1}^{\infty
}\frac{dt}{g(f(t))}<\infty ,\qquad \int_{1}^{\infty }\frac{%
dt}{f(g(t))}<\infty . \end{equation*}% The weights satisfy $\int_{0}^{\infty
}s\,p(s)\,ds<\infty $, $% \int_{0}^{\infty }s\,q(s)\,ds<\infty $, and $\min
(p,q)$ is not compactly supported. Within this framework, we establish: (i) the
existence of infinitely many entire positive radial solutions for a nonempty
set of central values; (ii) closedness of the set of admissible central values;
and (iii) largeness (blow-up at infinity) of solutions corresponding to
boundary points of this set. The approach is based on a novel subharmonic
functional tailored to the reciprocal integral conditions, extending classical
Keller--Osserman theory to a broad class of coupled systems with general
nonlinearities and weight functions.

</details>


### [19] [Vlasov equation coupled with non-Newtonian fluids with discontinuous-in-time stress tensor](https://arxiv.org/abs/2509.04110)
*Jakub Woźnicki*

Main category: math.AP

TL;DR: Existence of weak solutions for dilute particle systems coupled with non-Newtonian fluids with variable growth exponents, without time regularity assumptions on the growth parameter.


<details>
  <summary>Details</summary>
Motivation: To analyze the mathematical framework for particle-fluid systems where the fluid exhibits non-Newtonian behavior with spatially and temporally variable properties, particularly relevant for materials whose properties change instantaneously (e.g., under electric field switching).

Method: Study the system of PDEs describing dilute particle flow coupled with incompressible non-Newtonian fluid via drag force. The Cauchy stress tensor is monotone with asymptotically (s-1)-growth, where s depends on space and time variables without time smoothness assumptions, only spatial log-Hölder continuity.

Result: Proved long-time and large-data existence of weak solutions under the condition that s ≥ (3d+2)/(d+2), where d is the spatial dimension.

Conclusion: The analysis establishes mathematical existence results for complex particle-fluid systems with variable non-Newtonian properties, providing a foundation for studying materials with instantaneous property changes under external fields.

Abstract: We analyze the system of equations describing the flow of a dilute particle
system coupled with an incompressible non-Newtonian fluid in a bounded domain.
In this setting, both PDEs are connected via a drag force, or the friction
force. We are interested in a thin spray regime, which means that we neglect
the inter-particle interactions. When it comes to the fluid system, the Cauchy
stress tensor is supposed to be a monotone mapping and has asymptotically
$(s-1)$-growth with the parameter $s$ depending on the spatial and time
variable. We do not assume any smoothness of $s$ with respect to time variable
and assume the log-H\"{o}lder continuity with respect to spatial variable. An
example of materials, which satisfy those assumptions, are those whose
properties are instantaneous, e.g. changed by the switched electric field. We
show the long time and the large data existence of weak solution provided that
$s\ge\frac{3d+2}{d+2}$.

</details>


### [20] [On the local existence for the characteristic initial value problem for the Einstein-Dirac system](https://arxiv.org/abs/2509.04167)
*Peng Zhao,Xiaoning Wu*

Main category: math.AP

TL;DR: Semi-global existence proof for Einstein-Dirac system using characteristic initial value formulation and Luk's strategy without symmetry assumptions


<details>
  <summary>Details</summary>
Motivation: To investigate the characteristic initial value problem for Einstein-Dirac system governing gravity-spin-1/2 field interactions and prove existence results without symmetry restrictions

Method: Apply Luk's strategy with double null foliation, promote symmetric spinorial derivatives to independent variables, derive Weyl-curvature-free evolution system, use spinor-specific techniques for energy estimates

Result: Construction of smooth solutions in rectangular region to future of two intersecting null hypersurfaces with characteristic initial data

Conclusion: Successfully proved semi-global existence for Einstein-Dirac system by eliminating curvature coupling in energy estimates and closing bootstrap at optimal derivative levels

Abstract: In this paper, we investigate the characteristic initial value problem for
the Einstein-Dirac system, a model governing the interaction between gravity
and spin-$1/2$ fields. We apply Luk's strategy \cite{Luk12} and prove a
semi-global existence result for this coupled Einstein-Dirac system without
imposing symmetry conditions. More precisely, we construct smooth solutions in
a rectangular region to the future of two intersecting null hypersurfaces, on
which characteristic initial data are specified. The key novelty is to promote
the symmetric spinorial derivatives of the Dirac field to independent variables
and to derive a commuted "Weyl-curvature-free" evolution system for them. This
eliminates the coupling to the curvature in the energy estimates and closes the
bootstrap at the optimal derivative levels. The analysis relies on a double
null foliation and incorporates spinor-specific techniques essential to
handling the structure of the Dirac field.

</details>


### [21] [On the geometric properties of multi-operator two-phase elliptic measure](https://arxiv.org/abs/2509.04189)
*Max Goering,Anna Skorobogatova*

Main category: math.AP

TL;DR: Structural characterization of boundaries using two-phase elliptic measure in multi-operator setting, extending previous results and addressing Bishop's question about Oksendal's conjecture.


<details>
  <summary>Details</summary>
Motivation: Extend existing boundary characterization results to multi-operator settings and provide insights into Bishop's question regarding Oksendal's conjecture under mutual absolute continuity assumptions.

Method: Reduction to multi-operator two-phase free-boundary problem combined with extension of Preiss's Density Theorem tools.

Result: Structural characterization achieved for boundaries using two-phase elliptic measure in multi-operator context, with partial answer to Bishop's question.

Conclusion: The paper successfully extends boundary analysis techniques to multi-operator settings and makes progress on Oksendal's conjecture under specific conditions.

Abstract: We provide a structural characterization of a given boundary using two-phase
elliptic measure in a multi-operator setting, extending to this novel setting
results of Kenig, Preiss & Toro, Toro & Zhao and Azzam & Mourgoglou, including
a partial answer to Bishop's question regarding the validity of Oksendal's
conjecture under the assumption of mutual absolute continuity of the elliptic
measures. Our techniques rely on a reduction to a multi-operator two-phase
free-boundary problem combined with an extension of the powerful tools
introduced by Preiss in his Density Theorem.

</details>


### [22] [Recovery of Sturm-Liouville operators from partial boundary spectral data and applications](https://arxiv.org/abs/2509.04289)
*Ali Feizmohammadi,Yavar Kian*

Main category: math.AP

TL;DR: Unique reconstruction of potential from partial spectral data (fraction of Dirichlet eigenvalues + normal derivatives at endpoints) with applications to inverse coefficient problems for PDEs, providing optimal time measurement bounds.


<details>
  <summary>Details</summary>
Motivation: Address inverse Sturm-Liouville problems from partial spectral data and develop applications for wave-based imaging and Schrödinger equation inverse problems without requiring analyticity or infinite-time data.

Method: Uses Kahane's interpolation theorem to analyze endpoint time traces of solutions, enabling recovery from finite-time measurements. Also presents spectral interpolation theorem for 1D Schrödinger operators.

Result: Shows unique reconstruction of potential from partial spectral data. Provides optimal time measurement bounds for inverse coefficient determination problems in passive wave imaging and active Schrödinger equation imaging.

Conclusion: Novel approach using interpolation theorems enables practical inverse problems without restrictive assumptions, with applications in imaging and optimal measurement time bounds.

Abstract: We study the inverse Sturm-Liouville problem on a finite interval from
partial knowledge of spectral data. Specifically, we show that the potential
can be uniquely reconstructed from the knowledge of a fraction of Dirichlet
eigenvalues together with the normal derivatives of the corresponding
eigenfunctions at both endpoints. We present two novel applications of our
spectral result in inverse coefficient determination problems for evolutionary
PDEs that include passive wave-based imaging of a medium and active imaging for
the time-dependent Schr\"odinger equation with unknown internal sources. Our
results yield optimal time measurement bounds for such inverse coefficient
determination problems. A central innovation is the use of Kahane's
interpolation theorem to analyze endpoint time traces of solutions, enabling
the recovery without requiring analyticity assumptions or infinite-time data,
as in previous approaches. Finally, in the appendix, we present a spectral
interpolation theorem for one-dimensional Schr\"odinger operators, which may be
of independent interest.

</details>


### [23] [Energy critical fourth-order Schrödinger equation system with power-type nonlinearities in the radial case](https://arxiv.org/abs/2509.04428)
*Maicon Hespanha,Renzo Scarpelli*

Main category: math.AP

TL;DR: Analysis of focusing fourth-order Schrodinger equations with radial data and power-type nonlinearities, establishing local well-posedness, ground states, blow-up conditions, and scattering results.


<details>
  <summary>Details</summary>
Motivation: To generalize the analysis of energy-critical fourth-order Schrodinger systems with radial initial data and establish comprehensive results including well-posedness, existence of ground states, blow-up conditions, and scattering behavior.

Method: Established hypotheses on nonlinearities, proved local well-posedness in H^2 space, used virial argument for blow-up results, and employed concentration-compactness/rigidity method for scattering analysis.

Result: Proved local well-posedness, existence of ground state solutions, blow-up for negative energy or kinetic energy exceeding ground state, and scattering for solutions with energy below ground state levels.

Conclusion: The paper provides a complete theoretical framework for energy-critical fourth-order Schrodinger systems, characterizing solution behavior based on energy levels relative to ground states through rigorous mathematical analysis.

Abstract: In this paper, we study a system of focusing fourth-order Schr\"odinger
equations in the energy-critical setting with radial initial data and general
power-type nonlinearities. The main idea is to generalize the analysis of such
systems: we first establish several hypotheses on the nonlinearities and prove
their implications. These implications are then used to establish a local
well-posedness result in $H^2(\mathbb{R}^d)$ and and to prove the existence of
ground state solutions. Using a virial argument, we demonstrate a blow-up
result for initial data with negative energy or with kinetic energy exceeding
that of the ground state. Finally, employing the
concentration-compactness/rigidity method, we prove a scattering result for
solutions whose energy and kinetic energy are below those of the ground state.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [24] [Accurate and scalable deep Maxwell solvers using multilevel iterative methods](https://arxiv.org/abs/2509.03622)
*Chenkai Mao,Jonathan A. Fan*

Main category: physics.comp-ph

TL;DR: Neural network surrogates combined with iterative algorithms to solve PDE problems with high accuracy and scalability, using subdomain neural operators with Robin-type boundary conditions as flexible preconditioners.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of using neural networks as surrogate PDE solvers with high accuracy and scalability for problems featuring different scales, resolutions, and boundary conditions.

Method: Developed a subdomain neural operator model supporting arbitrary Robin-type boundary condition inputs, used as flexible preconditioners for iterative subdomain problem solving, and constructed global coarse spaces for accelerated large-scale PDE solving via iterative multilevel domain decomposition.

Result: Successfully trained a single network to simulate large-scale 2D Maxwell's equations problems with varying sizes, resolutions, wavelengths, and dielectric media distributions, and demonstrated utility in accurate inverse design of multi-wavelength nanophotonic devices.

Conclusion: Presents a promising path for building accurate and scalable multi-physics surrogate solvers for large practical problems by combining neural network surrogates with iterative algorithms and domain decomposition methods.

Abstract: Neural networks have promise as surrogate partial differential equation (PDE)
solvers, but it remains a challenge to use these concepts to solve problems
with high accuracy and scalability. In this work, we show that neural network
surrogates can combine with iterative algorithms to accurately solve PDE
problems featuring different scales, resolutions, and boundary conditions. We
develop a subdomain neural operator model that supports arbitrary Robin-type
boundary condition inputs, and we show that it can be utilized as a flexible
preconditioner to iteratively solve subdomain problems with bounded accuracy.
We further show that our subdomain models can facilitate the construction of
global coarse spaces to enable accelerated, large scale PDE problem solving
based on iterative multilevel domain decomposition. With two-dimensional
Maxwell's equations as a model system, we train a single network to simulate
large scale problems with different sizes, resolutions, wavelengths, and
dielectric media distribution. We further demonstrate the utility of our
platform in performing the accurate inverse design of multi-wavelength
nanophotonic devices. Our work presents a promising path to building accurate
and scalable multi-physics surrogate solvers for large practical problems.

</details>


### [25] [A Highly Scalable TDMA for GPUs and Its Application to Flow Solver Optimization](https://arxiv.org/abs/2509.03933)
*Seungchan Kim,Jihoo Kim,Sanghyun Ha,Donghyun You*

Main category: physics.comp-ph

TL;DR: Pipelined-TDMA algorithm for multi-GPU systems that overlaps communication with computation to overcome scalability bottlenecks in conventional tridiagonal matrix algorithms, achieving near-ideal scaling on up to 64 GPUs.


<details>
  <summary>Details</summary>
Motivation: To resolve scalability bottlenecks caused by the sequential structure of conventional divide-and-conquer TDMA algorithms on multi-GPU systems, particularly addressing inter-GPU communication and low-occupancy computation issues.

Method: Developed a pipelined TDMA that processes multiple tridiagonal systems concurrently, overlapping communication with computation and executing GPU kernels simultaneously. Optimized batch size to balance GPU occupancy and pipeline efficiency, using 1D slab-type domain decomposition.

Result: Achieved ideal weak scaling up to 64 GPUs with 1 billion grid cells per GPU, reached 74.7% of ideal performance in strong scaling for 4-billion-cell problems. Integrated into ADI-based fractional-step method, accelerated TDMA component by 4.37x and overall flow solver by 1.31x in 9-billion-cell simulation on 64 GPUs.

Conclusion: The Pipelined-TDMA successfully hides non-scalable execution time (communication and low-occupancy computation) behind scalable compute stages, effectively removing the scalability bottleneck in Poisson solvers for flow simulations on large multi-GPU systems.

Abstract: A tridiagonal matrix algorithm (TDMA), Pipelined-TDMA, is developed for
multi-GPU systems to resolve the scalability bottlenecks caused by the
sequential structure of conventional divide-and-conquer TDMA. The proposed
method pipelines multiple tridiagonal systems, overlapping communication with
computation and executing GPU kernels concurrently to hide non-scalable stages
behind scalable compute stages. To maximize performance, the batch size is
optimized to strike a balance between GPU occupancy and pipeline efficiency:
larger batches improve throughput for solving tridiagonal systems, while
excessively large batches reduce pipeline utilization. Performance evaluations
on up to 64 NVIDIA A100 GPUs using a one-dimensional (1D) slab-type domain
decomposition confirm that, except for the terminal phase of the pipeline, the
proposed method successfully hides most of the non-scalable execution
time-specifically inter-GPU communication and low-occupancy computation. The
solver achieves ideal weak scaling up to 64 GPUs with one billion grid cells
per GPU and reaches 74.7 percent of ideal performance in strong scaling tests
for a 4-billion-cell problem, relative to a 4-GPU baseline. The optimized TDMA
is integrated into an ADI-based fractional-step method to remove the
scalability bottleneck in the Poisson solver of the flow solver (Ha et al.,
2021). In a 9-billion-cell simulation on 64 GPUs, the TDMA component in the
Poisson solver is accelerated by 4.37x, contributing to a 1.31x overall speedup
of the complete flow solver.

</details>


### [26] [Enhanced Sampling in the Age of Machine Learning: Algorithms and Applications](https://arxiv.org/abs/2509.04291)
*Kai Zhu,Enrico Trizio,Jintu Zhang,Renling Hu,Linlong Jiang,Tingjun Hou,Luigi Bonati*

Main category: physics.comp-ph

TL;DR: Review of machine learning-enhanced sampling methods in molecular dynamics, focusing on data-driven collective variables, improved biasing schemes, and applications across biomolecular processes, catalysis, and phase transitions.


<details>
  <summary>Details</summary>
Motivation: Molecular dynamics simulations are constrained by long timescales of rare events, requiring enhanced sampling methods that can benefit from integration with machine learning techniques.

Method: Comprehensive review of machine learning integration with enhanced sampling methods, particularly data-driven construction of collective variables, improved biasing schemes, and novel strategies using reinforcement learning and generative approaches.

Result: Machine learning techniques are reshaping the field of enhanced sampling, enabling more effective exploration of complex molecular systems and rare events across various application domains.

Conclusion: Future directions aim at developing more automated strategies for rare-event sampling through continued integration of machine learning with molecular dynamics simulations.

Abstract: Molecular dynamics simulations hold great promise for providing insight into
the microscopic behavior of complex molecular systems. However, their
effectiveness is often constrained by long timescales associated with rare
events. Enhanced sampling methods have been developed to address these
challenges, and recent years have seen a growing integration with machine
learning techniques. This review provides a comprehensive overview of how they
are reshaping the field, with a particular focus on the data-driven
construction of collective variables. Furthermore, these techniques have also
improved biasing schemes and unlocked novel strategies via reinforcement
learning and generative approaches. In addition to methodological advances, we
highlight applications spanning different areas such as biomolecular processes,
ligand binding, catalytic reactions, and phase transitions. We conclude by
outlining future directions aimed at enabling more automated strategies for
rare-event sampling.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [27] [Thermodynamically consistent modelling and simulation of two-fluid magnetohydrodynamic equations](https://arxiv.org/abs/2509.03520)
*Ting Xiao,Qiaolin He*

Main category: physics.plasm-ph

TL;DR: A thermodynamically consistent two-fluid MHD model using Helmholtz free energy framework that ensures energy conservation and entropy increase, with a numerical scheme maintaining thermodynamic laws.


<details>
  <summary>Details</summary>
Motivation: To develop a two-fluid magnetohydrodynamic model that strictly satisfies both energy conservation and entropy increase principles, addressing thermodynamic consistency in plasma modeling.

Method: Constructed a model based on Helmholtz free energy framework with convex-concave characteristics, deriving thermodynamic quantities self-consistently and developing a temporally discrete numerical scheme with thermodynamic consistency.

Result: The proposed method satisfies both the first law (global energy conservation) and second law (non-decreasing total entropy) of thermodynamics, with numerical experiments validating its effectiveness in capturing key plasma phenomena.

Conclusion: The thermodynamically consistent two-fluid MHD model successfully maintains fundamental thermodynamic principles while providing accurate plasma simulations, with proven mathematical properties and practical computational applicability.

Abstract: In this paper, we proposes a thermodynamically consistent two-fluid
magnetohydrodynamic model based on the Helmholtz free energy framework, which
strictly satisfies both the principles of energy conservation and entropy
increase in the two-fluid model. By constructing the convexe-concave
characteristics of the free energy density (convexity with respect to plasma
number density and concavity with respect to temperature), the model
self-consistently derives key thermodynamic quantities such as chemical
potential, entropy density, and internal energy. Based on the proposed modeling
framework and the convex-concave properties of the Helmholtz free energy
density, we develop a temporally discrete numerical scheme with thermodynamic
consistency. We rigorously prove that the proposed method satisfies the first
law of thermodynamics (global energy conservation) and the second law
(non-decreasing total entropy). Additionally, we provide spatiotemporal error
estimates for the 2D degenerate system used in practical computations.
Numerical experiments validate the effectiveness of the proposed method in
capturing key plasma phenomena.

</details>


### [28] [Is There an Exact Magnetic-Moment for Charged Particle Motion in a Homogeneous, Time-Dependent Magnetic Field?](https://arxiv.org/abs/2509.03641)
*Michael Updike,Joshua Burby*

Main category: physics.plasm-ph

TL;DR: Nonperturbative guiding-center model provides exact alternative to full-orbit simulations, showing magnetic moment contains parametric resonances that destroy long-term conservation, refuting previous claims about all-time invariance.


<details>
  <summary>Details</summary>
Motivation: To provide an exact alternative to traditional guiding-center theory and full-orbit simulations for charged particle dynamics, particularly in cases where traditional methods may fail.

Method: Using the nonperturbative guiding-center model to analyze charged particle motion in homogeneous, time-varying magnetic fields, constructing exact magnetic moment invariant and comparing with adiabatic invariant series.

Result: Demonstrated that the exact magnetic moment contains information about parametric resonances that destroy conservation over very long times, unlike the perturbative invariant.

Conclusion: The nonperturbative model refutes previous claims about the all-time invariance of magnetic moment, showing parametric resonances prevent long-term conservation.

Abstract: The nonperturbative guiding-center model provides an exact alternative to
full-orbit simulations of charged particle dynamics in situations where
traditional guiding-center theory may fail. We demonstrate that the charged
particle motion in a homogeneous, time-varying magnetic field is a solvable
example of the nonperturbative guiding-center model. This entails showing that
the exact magnetic moment of Qin and Davidson can be constructed to be
asymptotic to the adiabatic invariant series of Kruskal. In contrast to the
perturbative invariant, the exact invariant contains information about
parametric resonances. These resonances destroy the conservation of the usual
magnetic moment over very long times. This refutes some previous claims about
the all-time invariance of the magnetic moment.

</details>


### [29] [Pressure dependence of magnetron sputtering: 2D-RZ particle-in-cell and 1D fluid modeling](https://arxiv.org/abs/2509.03664)
*Joseph G. Theis,Gregory R. Werner,Thomas G. Jenkins,Daniel Main,John R. Cary*

Main category: physics.plasm-ph

TL;DR: Reproduction of DC magnetron sputtering voltage-pressure dependence using 2D-RZ PIC simulation and 1D fluid modeling, with theoretical explanation that voltage decrease compensates for increased neutral density by lowering electron energy rather than electron recapture.


<details>
  <summary>Details</summary>
Motivation: The voltage-pressure (V-P) dependence in DC magnetron sputtering is fundamental to device performance but has not been previously reproduced with simulation or satisfactorily explained, despite being consistently observed in experiments.

Method: Used 2D-RZ particle-in-cell (PIC) simulation to reproduce experimental V-P dependence, then developed a steady-state 1D-axial fluid model of sheath and presheath informed by PIC results.

Result: Successfully reproduced V-P dependence curves matching past experiments. Found voltage decrease with pressure is due to compensation for increased neutral density by lowering plasma electron energy (reducing ionization probability) rather than electron recapture at cathode.

Conclusion: The V-P dependence is governed by constant global ionization rate requirement, where voltage adjusts to maintain ionization despite pressure changes. Electron reflection at cathode only affects sheath properties, not bulk plasma or presheath.

Abstract: We reproduce the consistently-seen experimental voltage versus pressure (V-P)
dependence of DC magnetron sputtering (DCMS) with 2D-RZ particle-in-cell (PIC)
simulation. Informed by PIC simulation, we develop a steady-state, 1D-axial
fluid model of the sheath and presheath that also reproduces this V-P
dependence. The V-P dependence is the relationship between the steady-state
voltage needed to maintain a constant discharge current and the neutral gas
pressure. V-P dependence is fundamental to device performance, but has not
previously been reproduced with simulation or satisfactorily explained. In this
work, we compare the V-P curve of our simulated device and fluid model with
past experiments and then present a theoretical explanation for this V-P
dependence. We find that the decrease in voltage with increasing pressure is
not due to electron recapture at the cathode. Rather, the constant current
dictates a constant global ionization rate, so the voltage decrease compensates
for the increase in neutral gas density by lowering the energy of the plasma
electrons, which decreases their ionization probability. The PIC simulations
also reveal that the presheath and bulk plasma are unaffected by the electron
reflection coefficient at the cathode; the only effect of increasing reflection
is a reduction in the sheath voltage and width. In addition to the potential
structure, we explore how pressure affects the plasma density, particle drifts,
and particle energy distributions.

</details>


### [30] [Plasma wakefield: from accelerators to black holes](https://arxiv.org/abs/2509.03880)
*Pisin Chen,Yung-Kun Liu*

Main category: physics.plasm-ph

TL;DR: Review of plasma wakefield acceleration evolution from invention to applications in particle physics, astrophysics, and analog gravity experiments like AnaBHEL for studying black hole information paradox.


<details>
  <summary>Details</summary>
Motivation: To commemorate the 2024 S. Chandrasekhar Prize and provide a comprehensive retrospective on the development and impact of plasma wakefield acceleration technology.

Method: Historical review tracing conceptual evolution, theoretical breakthroughs, and establishment of theoretical foundations of plasma wakefield acceleration.

Result: Plasma wakefield acceleration has profound applications beyond particle accelerators, enabling laboratory studies of gravity effects via Einstein's equivalence principle and investigations of black hole physics.

Conclusion: The field has evolved to enable hybrid schemes and next-generation colliders, with ongoing experiments like AnaBHEL aiming to address fundamental questions about black hole evaporation and information loss paradox.

Abstract: Commemorating the 2024 S. Chandrasekhar Prize, this review provides a
retrospective on the genesis and evolution of plasma wakefield acceleration. It
traces the journey from prehistory and the invention of the Plasma Wakefield
Accelerator (PWFA), the establishment of its theoretical cornerstones, to its
profound reverberations across fundamental physics, including astrophysics and
analog gravity. The narrative emphasizes conceptual evolution, key theoretical
breakthroughs, and future outlook, culminating in a vision for hybrid schemes
and next-generation colliders. In addition to application to particle
accelerators and high energy collider physics, it is found that plasma
wakefield, with its ultra-intense acceleration, can also be applied to
investigate gravity effects in the laboratory based on Einstein's equivalence
principle. A specific example is accelerating flying relativistic plasma
mirrors to investigate the celebrated black hole Hawking evaporation and the
associated information loss paradox. We describe an ongoing experiment, AnaBHEL
(Analog Black Hole Evaporation via Lasers), which aims at shedding some lights
on the black hole information loss paradox.

</details>


### [31] [Lagrangian features of turbulent transport in tokamak plasmas](https://arxiv.org/abs/2509.04132)
*D. I. Palade*

Main category: physics.plasm-ph

TL;DR: Study shows ion turbulent transport in tokamak plasmas exhibits approximate ergodicity and stationarity despite compressible Eulerian drifts, with broad initial distributions enabling ergodic mixing and minimal transport impact from initial constraints.


<details>
  <summary>Details</summary>
Motivation: To understand the Lagrangian properties of ion turbulent transport driven by drift-type turbulence in tokamak plasmas, particularly investigating whether ergodic behavior emerges despite compressible and inhomogeneous Eulerian gyrocenter drifts.

Method: Numerical simulations conducted using the T3ST code to analyze Lagrangian transport properties, examining phase-space distributions and their effects on transport characteristics.

Result: Reveals approximate ergodicity, stationarity, and time-symmetry in transport behavior. Broad initial phase-space distributions support ergodic mixing, and relatively minor constraints on initial distributions have negligible effects on transport levels.

Conclusion: Despite the compressible and inhomogeneous nature of Eulerian gyrocenter drifts, ion turbulent transport in tokamak plasmas demonstrates robust ergodic properties when supported by broad initial distributions, with transport levels being largely insensitive to initial distribution constraints.

Abstract: This study investigates the Lagrangian properties of ion turbulent transport
driven by drift-type turbulence in tokamak plasmas. Despite the compressible
and inhomogeneous nature of Eulerian gyrocenter drifts, numerical simulations
with the T3ST code reveal approximate ergodicity, stationarity, and
time-symmetry. These characteristics are attributed to broad initial
phase-space distributions that support ergodic mixing. Moreover, relatively
minor constraints on the initial distributions are found to have negligible
effects on transport levels.

</details>


### [32] [Energy Confinement Time Scaling for the Negative Triangularity Scenario in DIII-D](https://arxiv.org/abs/2509.04279)
*P. Lunia*

Main category: physics.plasm-ph

TL;DR: New scaling law developed from DIII-D negative triangularity campaign shows stronger plasma current dependence and more severe power degradation than H-mode scaling


<details>
  <summary>Details</summary>
Motivation: To create a specific scaling law from DIII-D negative triangularity campaign data that accurately reflects confinement properties while addressing dataset limitations

Method: Regression analysis on single-machine dataset with measures to minimize sampling bias and provide realistic uncertainty estimates

Result: The new power law demonstrates robustly stronger dependence on plasma current and more severe power degradation compared to IPB98(y,2) H-mode scaling law

Conclusion: The DIII-D negative triangularity campaign produces energy confinement properties that meet or exceed H-mode scaling expectations, with distinct current and power dependence characteristics

Abstract: Results from the 2023 negative triangularity campaign on DIII-D demonstrate
encouraging energy confinement properties, similar to or exceeding the scaling
of the IPB98(y,2) law. This paper describes the procedure with which a new
scaling law was regressed specifically from the data from the DIII-D campaign.
Given the relatively small size of the single-machine dataset, measures were
taken to minimize sampling bias and give a realistic estimate of the large
uncertainties from the regression. The resulting power law shows a robustly
stronger dependence on plasma current and more severe power degradation as
compared to the H-mode scaling law.

</details>


### [33] [Assessing time-dependent temperature profile predictions using reduced transport models for high performing NSTX plasmas](https://arxiv.org/abs/2509.04359)
*J. B. Lestz,G. Avdeeva,T. F. Neiser,M. V. Gorelenkova,F. D. Halpern,S. M. Kaye,J. McClenaghan,A. Y. Pankin,K. E. Thome*

Main category: physics.plasm-ph

TL;DR: TRANSP simulations compare MMM and TGLF transport models on NSTX data. MMM shows better agreement with experimental temperature profiles despite lower computational cost than TGLF.


<details>
  <summary>Details</summary>
Motivation: To evaluate how well modern reduced transport models can reproduce experimentally observed temperature profiles in spherical tokamaks using NSTX discharge data.

Method: Time-dependent predictive simulations using 1.5D tokamak integrated modeling code TRANSP on NSTX discharges, comparing Multi-Mode Model (MMM) and Trapped Gyro-Landau Fluid (TGLF) models. Also used neural net surrogate models for TGLF with time slice flux matching transport solver.

Result: MMM had median overpredictions of 28% for Te and 27% for Ti. TGLF overpredicted Te by 46% with high variance and underpredicted Ti by 25%. TGLF performance worsened with increasing beta. Electrostatic TGLF substantially overpredicted both temperatures. Neural net surrogates showed better agreement than TRANSP TGLF.

Conclusion: MMM provides reasonable agreement with experimental temperature profiles, motivating further detailed examination for predictive modeling of NSTX-U scenarios. Electromagnetic turbulence is crucial in high beta spherical tokamak regimes.

Abstract: Time-dependent, predictive simulations were performed with the 1.5D tokamak
integrated modeling code TRANSP on a large set of well-analyzed, high
performing discharges from the National Spherical Torus Experiment (NSTX) in
order to evaluate how well modern reduced transport models can reproduce
experimentally observed temperature profiles in spherical tokamaks. Overall, it
is found that simulations using the Multi-Mode Model (MMM) more consistently
agree with the NSTX observations than those using the Trapped Gyro-Landau Fluid
(TGLF) model, despite TGLF requiring orders of magnitude greater computational
cost. When considering all examined discharges, MMM has median overpredictions
of electron temperature ($T_e$) and ion temperature ($T_i$) profiles of 28% and
27%, respectively, relative to the experiment. TGLF overpredicts $T_e$ by 46%,
with much larger variance than MMM, and underpredicts $T_i$ by 25%. As $\beta$
is increased across NSTX discharges, TGLF predicts lower $T_e$ and significant
flattening of the $T_i$ profile, conflicting with NSTX observations. When using
an electrostatic version of TGLF, both $T_e$ and $T_i$ are substantially
overpredicted, underscoring the importance of electromagnetic turbulence in the
high $\beta$ spherical tokamak regime. Additionally, calculations with neural
net surrogate models for TGLF were performed outside of TRANSP with a time
slice flux matching transport solver, finding better agreement with experiment
than the TRANSP simulations, highlighting the impact of different transport
solvers and simulation techniques. Altogether, the reasonable agreement with
experiment of temperature profiles predicted by MMM motivates a more detailed
examination of the sensitivities of the TRANSP simulations with MMM to
different NSTX plasma regimes in a companion paper, in preparation for
self-consistent, time-dependent predictive modeling of NSTX-U scenarios.

</details>


### [34] [Sensitivities of time-dependent temperature profile predictions for NSTX with the Multi-Mode Model](https://arxiv.org/abs/2509.04360)
*J. B. Lestz,G. Avdeeva,S. M. Kaye,M. V. Gorelenkova,F. D. Halpern,J. McClenaghan,A. Y. Pankin,K. E. Thome*

Main category: physics.plasm-ph

TL;DR: MMM transport model applied to NSTX discharges shows reasonable temperature profile predictions, outperforming TGLF, but tends to overpredict confinement with overly steep profiles


<details>
  <summary>Details</summary>
Motivation: To evaluate MMM's sensitivities across various plasma conditions and understand its regime-dependent performance for future NSTX-U simulations

Method: Time-dependent predictive simulations using 1.5D tokamak integrated modeling code TRANSP on hundreds of milliseconds of NSTX discharge data

Result: MMM predicted reasonable Te and Ti profiles, with electron transport dominated by ETG modes at low beta/high collisionality, transitioning to mixed modes at higher beta/lower collisionality. MMM overpredicts confinement leading to overly steep profiles but shows robustness to start time

Conclusion: MMM provides useful context for regime-dependent transport predictions, with better agreement for plasmas having broader Te profiles, higher beta, and longer confinement times

Abstract: The Multi-Mode Model (MMM) for turbulent transport was applied to a large set
of well-analyzed discharges from the National Spherical Torus Experiment (NSTX)
in order to evaluate its sensitivities to a wide range of plasma conditions.
MMM calculations were performed for hundreds of milliseconds in each discharge
by performing time-dependent predictive simulations with the 1.5D tokamak
integrated modeling code TRANSP. A closely related study concluded that MMM
predicted electron ($T_e$) and ion ($T_i$) temperature profiles that were in
reasonable agreement with NSTX observations, generally outperforming a
different reduced transport model, TGLF, motivating a more thorough
investigation of the characteristics of the MMM predictions. The simulations
with MMM have electron energy transport dominated by electron temperature
gradient modes for relatively low plasma $\beta$ and high collisionality,
transitioning to a mixture of different modes for higher $\beta$ and lower
collisionality. The thermal ion diffusivity predicted by MMM is much smaller
than the neoclassical contribution, in line with previous experimental analysis
of NSTX. Nonetheless, the $T_e$ and $T_i$ profiles are coupled via collisional
energy exchange and thus sensitive to which transport channels are predicted.
The simulations with MMM are robust to the simulation start time, converging to
remarkably similar temperatures later during the discharge. MMM typically
overpredicts confinement relative to NSTX observations, leading to the
prediction of overly steep profiles. Plasmas with spatially broader $T_e$
profiles, higher $\beta$, and longer energy confinement times tend to be
predicted by MMM with better agreement with the experiment. These findings
provide useful context for understanding the regime-dependent tendencies of MMM
in anticipation of self-consistent, time-dependent predictive simulations of
NSTX-U discharges.

</details>


### [35] [Estimation of Effective Viscosity to Quantify Collisional Behavior in Collisionless Plasma](https://arxiv.org/abs/2509.04374)
*Subash Adhikari,Carlos Gonzalez,Yan Yang,Sean Oughton,Francesco Pecora,Riddhi Bandyopadhyay,William H. Matthaeus*

Main category: physics.plasm-ph

TL;DR: Study estimates effective viscosity in collisionless plasma turbulence using PIC simulations and shows MHD/two-fluid models with this viscosity produce similar global behavior to kinetic simulations.


<details>
  <summary>Details</summary>
Motivation: To understand dissipation mechanisms in nearly collisionless plasma where traditional viscosity and resistivity definitions don't apply, but viscous-like energy conversion has been observed.

Method: Used 2.5D kinetic particle-in-cell (PIC) simulation of collisionless plasma turbulence with scale-filtering approach to estimate effective viscosity at each scale, then compared with MHD and two-fluid simulations using this viscosity.

Result: Found striking similarity in global behavior between MHD/two-fluid simulations (with effective viscosity from PIC) and kinetic/PIC counterpart. Also explored scale dependence of effective viscosity.

Conclusion: The approach of using effective viscosity estimates from kinetic simulations can successfully model collisionless plasma turbulence in MHD/two-fluid frameworks, with implications for space plasma studies.

Abstract: While dissipation in collisional plasma is defined in terms of viscosity and
resistivity, the exact functional form of dissipation i.e., the so-called
dissipation function in nearly collisionless plasma is unknown. Nevertheless,
previous studies have suggested that there exists viscous-like energy
conversion in collisionless plasma with scaling characteristics analogous to
collisional plasma, and in particular that the average dissipation is
proportional to the square of the rate of strain as in hydrodynamics. In this
study, using 2.5D kinetic particle-in-cell (PIC) simulation of collisionless
plasma turbulence, we provide an estimate of effective viscosity at each scale,
obtained via a scale-filtering approach. We then compare the turbulent dynamics
of the PIC simulation with that from MHD and two-fluid simulations in which
with the viscosity is equal to the effective viscosity estimate obtained from
the PIC simulation. We find that the global behavior in these MHD and two-fluid
simulations has a striking similarity with that in its kinetic/PIC counterpart.
In addition, we explore the scale dependence of the effective viscosity, and
discuss implications of this approach for space plasmas.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [36] [Analysis of nonlinear resonances in resonator crystals: Tight-binding approximation and existence of subwavelength soliton-like solutions](https://arxiv.org/abs/2509.04184)
*Habib Ammari,Jiayu Qiu*

Main category: math-ph

TL;DR: Mathematical framework for confining waves at subwavelength scales in periodic nonlinear resonator systems, proving existence of soliton-like localized waves and validating tight-binding approximation.


<details>
  <summary>Details</summary>
Motivation: To understand physical mechanisms for subwavelength wave confinement in periodic systems of nonlinear resonators and enable study of topological properties in such systems.

Method: Developed discrete approximation using linear capacitance operator to characterize nonlinear subwavelength resonances, and proved existence of soliton-like localized waves in both full- and half-space crystals.

Result: Successfully provided mathematical framework for subwavelength wave confinement, validated tight-binding approximation for crystals of subwavelength resonators, and established foundation for studying topological properties.

Conclusion: The framework enables future research on topological properties of periodic nonlinear resonator lattices, including nonlinearity-induced topological edge states and interplay between nonlinearity and disorder.

Abstract: This work provides a mathematical framework for elucidating physical
mechanisms for confining waves at subwavelength scales in periodic systems of
nonlinear resonators. A discrete approximation in terms of the linear
capacitance operator is provided to characterize the nonlinear subwavelength
resonances. Moreover, the existence of subwavelength soliton-like localized
waves in periodic systems of nonlinear resonators is proven. As a by-product, a
tight-binding approximation of the capacitance operator is shown to be valid
for crystals of subwavelength resonators. Both full- and half-space crystals
are considered. The framework developed in this work opens the door to the
study of topological properties of periodic lattices of subwavelength nonlinear
resonators, such as the emergence of nonlinearity-induced topological edge
states, and to elucidate the interplay between nonlinearity and disorder.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [37] [Shape spectra of elastic shells with surface-adsorbed semiflexible polymers](https://arxiv.org/abs/2509.04138)
*Hadiya Abdul Hameed,Jaroslaw Paturej,Aykut Erbas*

Main category: cond-mat.soft

TL;DR: Semiflexible polymers adsorbed on spherical shells can induce shape distortions and nematic ordering, with strong localization causing shrinkage and weak localization leading to elliptic shapes through surface ordering.


<details>
  <summary>Details</summary>
Motivation: Understand how high density of surface-adsorbed semiflexible polymers affects morphology of spherical biological shells like cell nuclei and vesicles, which often deviate from perfect spheres due to complex molecular interactions.

Method: Coarse-grained molecular dynamics simulations of pressurized bead-spring shells with varying attraction strength between chains and shell surface, chain concentration, and polymerization degree.

Result: Strong surface localization induces severe shape distortions and shrinkage depending on chain length and concentration. Weak localization leads to nematically ordered phases that cause elliptic shell shapes when chains are comparable to or longer than confinement radius.

Conclusion: Findings provide a strategy to control synthetic shell shapes by manipulating polymer localization and length, and suggest mechanisms for non-spherical shapes in biological systems.

Abstract: The shape of biological shells, such as cell nuclei, membranes, and vesicles,
often deviates from a perfect sphere due to an interplay of complex
interactions with a myriad of molecular structures. In particular, semiflexible
biopolymers adsorbed to the surfaces of such shells seem to affect their
morphological properties. While the effect of a single, long, semiflexible
chain is relatively well characterized, the mechanisms by which a high density
of such surface-adsorbed polymers can alter the morphology of a spherical, soft
confinement, akin to biological shells, remain relatively poorly understood.
Here, we use coarse-grained molecular dynamics simulations to investigate how
surface adsorption of many semiflexible polymers affects the morphology of a
pressurized bead-spring shell, which is spherical in the absence of these
chains. By varying the attraction strength between the chains and the shell
surface, chain concentration, and the polymerization degree of chains, we
demonstrate that strong surface localization of the chains can induce severe
shape distortions and shrinkage, depending on the chain length and
concentration. Conversely, weak localization does not induce significant shape
fluctuations, yet nematically ordered phases appear on the surface. Notably,
these ordered phases lead to elliptic shell shapes for chains with sizes
comparable to or longer than the radius of the confinement when the elastic
shell is composed of extensible, harmonic bonds. Overall, our findings offer a
strategy to control the shape of synthetic shells by manipulating peripheral
localization and length of semiflexible polymers while suggesting a mechanism
for non-spherical shapes appearing in some biological systems.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [38] [Exploiting correlations in multi-coincidence Coulomb explosion patterns for differentiating molecular structures using machine learning](https://arxiv.org/abs/2509.03776)
*Anbu Selvam Venkatachalam,Loren Greenman,Joshua Stallbaumer,Artem Rudenko,Daniel Rolles,Huynh Van Sa Lam*

Main category: physics.chem-ph

TL;DR: New machine learning approach for Coulomb explosion imaging enables automated analysis of high-dimensional molecular data to distinguish isomers and identify reaction pathways.


<details>
  <summary>Details</summary>
Motivation: Coulomb explosion imaging generates rich high-dimensional data but current methods struggle to visualize correlations and exploit the full information content, limiting its potential for analyzing complex molecular dynamics.

Method: Developed a machine-learning-based analysis approach for CEI data that detects up to eight ionic fragments in coincidence, enabling automated pattern recognition and correlation analysis in high-dimensional momentum-space data.

Result: Successfully applied the method to distinguish dichloroethylene isomers, demonstrating robust molecular structure identification and providing background-free data with rich structural information.

Conclusion: This approach establishes a scalable framework for extracting insightful information from CEI data and paves the way for channel-specific analysis of ultrafast structural dynamics, particularly for disentangling mixed reaction pathways and detecting weak channels.

Abstract: Coulomb explosion imaging (CEI) is a powerful technique for capturing the
real-time motion of individual atoms during ultrafast photochemical reactions.
CEI generates high-dimensional data with naturally embedded correlations that
allow mapping the coordinated motion of nuclei in molecules. This enables
reliable separation of competing reaction pathways and makes this approach
uniquely suited for characterizing weak reaction channels. However, rich
information contained in experimental CEI patterns remains largely
underexploited due to challenges in visualizing correlations between multiple
observables in multi-dimensional parameter space. Here we present a new
approach to CEI of intermediate-sized polyatomic molecules, detecting up to
eight ionic fragments in coincidence and leveraging machine-learning-based
analysis to identify patterns and correlations in the resulting
high-dimensional momentum-space data, enabling robust molecular structure
identification and differentiation. Our approach provides high-dimensional
background-free data encoding exceptionally rich structural information and
establishes an automated, scalable framework for extracting insightful
information from the data. As a demonstration, we apply this method to image
and distinguish dichloroethylene isomers, showcasing its potential for broader
applications in molecular imaging. Our results pave the way for
channel-specific analysis of ultrafast structural dynamics in chemically
relevant systems, particularly for disentangling mixed reaction pathways and
detecting contributions from weak channels and minority species.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [39] [Prob-GParareal: A Probabilistic Numerical Parallel-in-Time Solver for Differential Equations](https://arxiv.org/abs/2509.03945)
*Guglielmo Gattiglio,Lyudmila Grigoryeva,Massimiliano Tamborrino*

Main category: stat.CO

TL;DR: Prob-GParareal is a probabilistic extension of GParareal that uses Gaussian processes to provide uncertainty quantification for parallel-in-time solutions of differential equations, with theoretical analysis and numerical validation on benchmark problems.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in developing probabilistic counterparts to established Parallel-in-Time (PinT) methods by providing uncertainty quantification for differential equation solutions.

Method: Uses Gaussian processes to model the Parareal correction function, enabling propagation of numerical uncertainty across time. Also introduces Prob-nnGParareal variant with nearest-neighbors GPs for improved performance.

Result: Theoretical analysis of computational complexity and error bounds. Numerical demonstration of accuracy and robustness on five benchmark ODE systems (chaotic, stiff, bifurcation problems) and a PDE example.

Conclusion: Prob-GParareal successfully provides probabilistic uncertainty quantification while maintaining compatibility with existing Parareal frameworks, offering a flexible and scalable approach for probabilistic PinT solutions.

Abstract: We introduce Prob-GParareal, a probabilistic extension of the GParareal
algorithm designed to provide uncertainty quantification for the
Parallel-in-Time (PinT) solution of (ordinary and partial) differential
equations (ODEs, PDEs). The method employs Gaussian processes (GPs) to model
the Parareal correction function, as GParareal does, further enabling the
propagation of numerical uncertainty across time and yielding probabilistic
forecasts of system's evolution. Furthermore, Prob-GParareal accommodates
probabilistic initial conditions and maintains compatibility with classical
numerical solvers, ensuring its straightforward integration into existing
Parareal frameworks. Here, we first conduct a theoretical analysis of the
computational complexity and derive error bounds of Prob-GParareal. Then, we
numerically demonstrate the accuracy and robustness of the proposed algorithm
on five benchmark ODE systems, including chaotic, stiff, and bifurcation
problems. To showcase the flexibility and potential scalability of the proposed
algorithm, we also consider Prob-nnGParareal, a variant obtained by replacing
the GPs in Parareal with the nearest-neighbors GPs, illustrating its increased
performance on an additional PDE example. This work bridges a critical gap in
the development of probabilistic counterparts to established PinT methods.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [40] [An analog-electronic implementation of a harmonic oscillator recurrent neural network](https://arxiv.org/abs/2509.04064)
*Pedro Carvalho,Bernd Ulmann,Wolf Singer,Felix Effenberger*

Main category: q-bio.NC

TL;DR: Analog electronic implementation of oscillatory neural networks (HORNs) successfully replicates digital model dynamics but requires retraining of readout layer due to precision differences, achieving comparable classification performance to digital counterpart.


<details>
  <summary>Details</summary>
Motivation: To explore the feasibility of implementing oscillatory recurrent neural networks (HORNs) in analog-electronic hardware while maintaining computational performance, as previous implementations focused on attractor dynamics rather than transient dynamics.

Method: Used digital twin approach - trained a four-node HORN in silico for sequential MNIST classification, then transferred parameters to analog electronic implementation. Employed custom error metrics and compared performance with both original and retrained readout layers.

Result: Analog system successfully replicated digital model dynamics in most cases, but only 28.39% agreement when using digital readout layer due to precision differences. With retrained linear readout, classification performance recovered to digital twin levels, indicating preserved information content.

Conclusion: Analog electronic circuits can effectively implement oscillatory neural networks for computation, demonstrating energy-efficient analog systems that exploit brain-inspired transient dynamics, though readout layers need hardware-specific training.

Abstract: Oscillatory recurrent networks, such as the Harmonic Oscillator Recurrent
Network (HORN) model, offer advantages in parameter efficiency, learning speed,
and robustness relative to traditional non-oscillating architectures. Yet,
while many implementations of physical neural networks exploiting attractor
dynamics have been studied, implementations of oscillatory models in
analog-electronic hardware that utilize the networks' transient dynamics so far
are lacking. This study explores the feasibility of implementing HORNs in
analog-electronic hardware while maintaining the computational performance of
the digital counterpart. Using a digital twin approach, we trained a four-node
HORN in silico for sequential MNIST classification and transferred the trained
parameters to an analog electronic implementation. A set of custom error
metrics indicated that the analog system is able to successfully replicate the
dynamics of the digital model in most test cases. However, despite the overall
well-matching dynamics, when using the readout layer of the digital model on
the data generated by the analog system, we only observed $28.39\%$ agreement
with the predictions of the digital model. An analysis shows that this mismatch
is due to a precision difference between the analog hardware and the
floating-point representation exploited by the digital model to perform
classification tasks. When the analog system was utilized as a reservoir with a
re-trained linear readout, its classification performance could be recovered to
that of the digital twin, indicating preserved information content within the
analog dynamics. This proof-of-concept establishes that analog electronic
circuits can effectively implement oscillatory neural networks for computation,
providing a demonstration of energy-efficient analog systems that exploit
brain-inspired transient dynamics for computation.

</details>


<div id='math.SP'></div>

# math.SP [[Back]](#toc)

### [41] [Absence of eigenvalues of electromagnetic Dirac operators](https://arxiv.org/abs/2509.03627)
*Naiara Arrizabalaga,Lucrezia Cossetti,Matias Morales*

Main category: math.SP

TL;DR: Generalization of multiplier method for electromagnetic Dirac operators to establish conditions for absence of point spectrum, extending previous magnetic-only results to include electric fields and Coulomb-type potentials.


<details>
  <summary>Details</summary>
Motivation: To extend the method of multipliers for Dirac operators beyond purely magnetic cases, covering combined electromagnetic fields and addressing massless cases with Coulomb-type potentials.

Method: Developed the method of multipliers for electromagnetic Dirac operators, establishing sufficient conditions on magnetic and electric fields that guarantee absence of point spectrum.

Result: Successfully generalized previous results from purely magnetic cases to include electric fields, with specific coverage of Coulomb-type potentials in the massless case.

Conclusion: The method provides sufficient conditions for absence of point spectrum in electromagnetic Dirac operators, extending prior work and covering important Coulomb-type potential cases.

Abstract: In this work, we develop the method of multipliers for electromagnetic Dirac
operators and establish sufficient conditions on the magnetic and electric
fields that guarantee the absence of point spectrum. Our results can be viewed
as a generalization of those obtained by Cossetti, Fanelli and
Krej\v{c}i\v{r}\'ik (2020), which treated the purely magnetic case. We also
emphasize that, in the massless case, our approach covers Coulomb-type
potentials of the form $V(x) = \frac{1}{|x|} \big(\nu \mathbb{I} + \mu \beta +
i \delta \beta \big(\boldsymbol{\alpha}\cdot \frac{x}{|x|} \big) \big).$

</details>


<div id='math.FA'></div>

# math.FA [[Back]](#toc)

### [42] [BMO-Interpolations and Jump Detection for Functions in $BV\cap BMO$](https://arxiv.org/abs/2509.04176)
*Paz Hashash,Arkady Poliakovsky*

Main category: math.FA

TL;DR: Interpolation results for BMO in Lorentz spaces, extended to Besov spaces, BV space, and fractional Sobolev spaces, with applications to geometric properties of jump sets in BV ∩ BMO functions.


<details>
  <summary>Details</summary>
Motivation: To establish interpolation theorems that connect BMO (Bounded Mean Oscillation) with various function spaces including Lorentz, Besov, BV (Bounded Variation), and fractional Sobolev spaces, enabling better understanding of function regularity and geometric properties.

Method: Proving interpolation results for BMO in Lorentz spaces first, then deriving interpolation theorems for Besov spaces, BV space, and fractional Sobolev spaces from these foundational results.

Result: Successful development of interpolation theorems across multiple function space categories, with practical application to obtain geometric information about jump sets of functions belonging to both BV and BMO spaces.

Conclusion: The interpolation framework provides powerful tools for analyzing function regularity and geometric properties, particularly for functions with bounded variation that also exhibit bounded mean oscillation behavior.

Abstract: In this article, we prove interpolation results for $BMO$ in Lorentz spaces.
From these results, we derive interpolation theorems in Besov spaces, the space
$BV$, and fractional Sobolev spaces. As an application, we obtain geometric
information about the jump set of functions in $BV \cap BMO$.

</details>


### [43] [Polynomial Stability of Non-Linearly Damped Contraction Semigroups](https://arxiv.org/abs/2509.04275)
*Lassi Paunonen,David Seifert*

Main category: math.FA

TL;DR: Analysis of stability properties for semi-linear systems with rational decay rates for classical solutions under non-uniform observability and non-linearity conditions, applied to wave equations and Euler-Bernoulli beams.


<details>
  <summary>Details</summary>
Motivation: To establish stability properties and decay rates for abstract semi-linear systems, providing a framework that can be applied to various physical systems with non-linear damping.

Method: Develops an abstract class analysis for semi-linear systems, requiring non-uniform observability estimates for the linear part and suitable conditions on the non-linearity to prove rational decay rates.

Result: Main result shows rational rates of decay for classical solutions, demonstrated through applications to one-dimensional wave equations with weak non-linear damping and Euler-Bernoulli beams with tip mass and non-linear damping.

Conclusion: The abstract framework successfully establishes stability with rational decay rates for semi-linear systems, validated by concrete applications to physical systems with non-linear damping mechanisms.

Abstract: We investigate the stability properties of an abstract class of semi-linear
systems. Our main result establishes rational rates of decay for classical
solutions assuming a certain non-uniform observability estimate for the linear
part and suitable conditions on the non-linearity. We illustrate the strength
of our abstract results by applying them to a one-dimensional wave equation
with weak non-linear damping and to an Euler-Bernoulli beam with a tip mass
subject to non-linear damping.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [44] [Learning functions through Diffusion Maps](https://arxiv.org/abs/2509.03758)
*Alvaro Almeida Gomez*

Main category: cs.LG

TL;DR: A data-driven method for function approximation on manifolds using Diffusion Maps, with dimensionality reduction via SVD and online updating for scalability, showing superior performance over neural networks and interpolation methods.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient method for approximating real-valued functions on smooth manifolds that can handle high-dimensional data and incorporate new data efficiently, addressing computational challenges in manifold learning.

Method: Builds on Diffusion Maps framework to construct smooth extensions to ambient space using diffusion geometry and Laplace-Beltrami operator connections. Uses SVD-based dimensionality reduction on distance matrices and implements online updating mechanism for new data incorporation.

Result: The proposed methodology outperforms classical feedforward neural networks and interpolation methods in both accuracy and efficiency, as demonstrated in numerical experiments including sparse CT reconstruction applications.

Conclusion: The method provides an effective and scalable approach for function approximation on manifolds, with practical applications in areas like medical imaging where handling high-dimensional data and incremental updates are crucial.

Abstract: We propose a data-driven method for approximating real-valued functions on
smooth manifolds, building on the Diffusion Maps framework under the manifold
hypothesis. Given pointwise evaluations of a function, the method constructs a
smooth extension to the ambient space by exploiting diffusion geometry and its
connection to the heat equation and the Laplace-Beltrami operator.
  To address the computational challenges of high-dimensional data, we
introduce a dimensionality reduction strategy based on the low-rank structure
of the distance matrix, revealed via singular value decomposition (SVD). In
addition, we develop an online updating mechanism that enables efficient
incorporation of new data, thereby improving scalability and reducing
computational cost.
  Numerical experiments, including applications to sparse CT reconstruction,
demonstrate that the proposed methodology outperforms classical feedforward
neural networks and interpolation methods in terms of both accuracy and
efficiency.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [45] [An exact multiple-time-step variational formulation for the committor and the transition rate](https://arxiv.org/abs/2509.03539)
*Chatipat Lorpaiboon,Jonathan Weare,Aaron R. Dinner*

Main category: cond-mat.stat-mech

TL;DR: New committor estimation method that eliminates lag time bias in transition rate calculations between stable states, providing more robust estimates than existing approaches.


<details>
  <summary>Details</summary>
Motivation: Existing committor estimation methods from trajectory data are biased when using practical lag times, as they only minimize transition rate expressions correctly at single time steps rather than arbitrary lag times.

Method: Developed an alternative expression for the transition rate that is minimized by the exact committor at any lag time, derived additional transition rate expressions, and related these to variational approaches based on mean-squared residuals.

Result: Numerical tests on benchmark systems show the new committor and transition rate estimates are significantly less sensitive to lag time choice compared to existing methods.

Conclusion: The proposed method provides more reliable committor estimation and transition rate calculations by eliminating lag time bias, with error analysis through dynamic mode decomposition supporting its robustness.

Abstract: For a transition between two stable states, the committor is the probability
that the dynamics leads to one stable state before the other. It can be
estimated from trajectory data by minimizing an expression for the transition
rate that depends on a lag time. We show that an existing such expression is
minimized by the exact committor only when the lag time is a single time step,
resulting in a biased estimate in practical applications. We introduce an
alternative expression that is minimized by the exact committor at any lag
time. Numerical tests on benchmark systems demonstrate that our committor and
resulting transition rate estimates are much less sensitive to the choice of
lag time. We derive an additional expression for the transition rate, relate
the transition rate expression to a variational approach for kinetic statistics
based on the mean-squared residual, and discuss further numerical
considerations with the aid of a decomposition of the error into dynamic modes.

</details>


### [46] [The Non-commutative Spaces of Higher-Order Networks of Bach's Solo Violin Compositions: Dimension, Curvature, and Distance through the Spectral Triplet](https://arxiv.org/abs/2509.04311)
*Sara Najem,Dima Mrad*

Main category: cond-mat.stat-mech

TL;DR: Introduces geometric measures for simplicial complexes using non-commutative algebra and Connes' spectral triplet formalism to compute dimensions, curvature, and distance as characterizing features.


<details>
  <summary>Details</summary>
Motivation: To go beyond pairwise network representations and capture higher-order interactions in complex systems using simplicial complexes, which require new geometric characterization methods.

Method: Leverages the non-commutativity of algebra in matrix representations of simplicial complexes and applies Connes' spectral triplet formalism to compute geometric properties.

Result: Developed novel geometric measures including dimensions, curvature, and distance metrics that can characterize simplicial complexes alongside traditional topological metrics.

Conclusion: The spectral triplet formalism provides a powerful framework for geometric analysis of simplicial complexes, enabling richer characterization of higher-order interactions in complex systems beyond what topological metrics alone can provide.

Abstract: Our work is concerned with simplicial complexes that describe higher-order
interactions in real complex systems. This description allows to go beyond the
pairwise node-to-node representation that simple networks provide and to
capture a hierarchy of interactions of different orders. The prime contribution
of this work is the introduction of geometric measures for these simplicial
complexes. We do so by noting the non-commutativity of the algebra associated
with their matrix representations and consequently we bring to bear the
spectral triplet formalism of Connes on these structures and then notions of
associated dimensions, curvature, and distance can be computed to serve as
characterizing features in addition to known topological metrics.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [47] [Forecasting Low-Dimensional Turbulence via Multi-Dimensional Hybrid Quantum Reservoir Computing](https://arxiv.org/abs/2509.04006)
*L. Salatino,L. Mariani,A. Giordano,F. D'Amore,C. Mastroianni,L. Pontieri,A. Vinci,C. Gencarelli,L. Primavera,F. Plastina,J. Settino,F. Carbone*

Main category: quant-ph

TL;DR: Hybrid quantum-classical reservoir computing using 5-qubit Ising Hamiltonian for chaotic dynamics forecasting in fluid systems, showing robust performance through parameter optimization.


<details>
  <summary>Details</summary>
Motivation: Complex dynamics prediction is challenging due to nonlinearities and multiscale interactions. Quantum reservoir computing exploits high-dimensional Hilbert space for better forecasting.

Method: Hybrid quantum-classical reservoir architecture with 5-qubit transverse-field Ising Hamiltonian, input-modulated dynamics, and temporal multiplexing for multivariate time series.

Result: Systematic parameter scanning identified optimal regions maximizing forecasting performance (Valid Prediction Time) for both Navier-Stokes truncation and Lorenz-63 systems.

Conclusion: The hybrid quantum approach provides a flexible platform for modeling complex nonlinear time series with observed robustness and reliable performance.

Abstract: The prediction of complex dynamics remains an open problem across many
domains of physics, where nonlinearities and multiscale interactions severely
limit the reliability of conventional forecasting methods. Quantum reservoir
computing (QRC) has emerged as a promising paradigm for information processing
by exploiting the high dimensionality of the Hilbert space, where the dynamics
of quantum systems take place. Here, we introduce a hybrid quantum-classical
reservoir architecture capable of handling multivariate time series through
quantum evolution combined with classical memory enhancement. Our model employs
a five-qubit transverse-field Ising Hamiltonian with input-modulated dynamics
and temporal multiplexing, enabling the encoding of input signals over multiple
timescales. We apply this framework to two paradigmatic models of chaotic
behavior in fluid dynamics, where multiscale dynamics and nonlinearities play a
dominant role: a low-dimensional truncation of the two-dimensional
Navier-Stokes equations and the Lorenz-63 system. By systematically scanning
the quantum system's parameter space, we identify regions that maximize
forecasting performance, as measured by the Valid Prediction Time. The observed
robustness and reliable performances for both dynamical systems suggest that
this hybrid quantum approach offers a flexible platform for modelling complex
nonlinear time series.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [48] [Electromechanical human heart modeling for predicting endocardial heart motion](https://arxiv.org/abs/2509.04024)
*Milad Hasani,Alireza Rezania,Sam Riahi*

Main category: physics.med-ph

TL;DR: A comprehensive biventricular electromechanical human heart model that integrates 3D geometry with hemodynamics using fluid-structure interaction, validated against MRI data for clinical applications.


<details>
  <summary>Details</summary>
Motivation: To create a clinically relevant heart model that accurately simulates the complex interactions between electrical excitation, mechanical contraction, and blood flow for better understanding of cardiac physiology and therapeutic interventions.

Method: Uses two-way fluid-structure interaction with 3D blood meshes, couples reaction-diffusion framework with voltage-dependent active stress, and incorporates innovative epicardial boundary conditions to mimic tissue stiffness and viscosity.

Result: Model successfully replicated physiological heart motion with high consistency in regional displacement patterns when validated against Cine MRI data. Right ventricle analysis showed basal and mid free walls experience largest motion.

Conclusion: The validated model serves as a robust tool for enhancing cardiac physiology understanding and optimizing therapeutic interventions before clinical implementation, particularly for motion-driven energy harvesting device placement.

Abstract: This work presents a biventricular electromechanical human heart model that
is comprehensive and clinically relevant, integrating a realistic 3D heart
geometry with both systemic and pulmonary hemodynamics. The model uses a
two-way fluid-structure-interaction (FSI) formulation with actual 3D blood
meshes to accurately investigate the effect of blood flow on the myocardium. It
couples a reaction-diffusion framework and a voltage-dependent active stress
term to replicate the link between electrical excitation and mechanical
contraction. Additionally, the model incorporates innovative epicardial
boundary conditions to mimic the stiffness and viscosity of neighboring
tissues. The model's ability to replicate physiological heart motion was
validated against Cine magnetic resonance imaging (MRI) data, which
demonstrated a high degree of consistency in regional displacement patterns.
The analysis of the right ventricle showed that the basal and mid free walls
experience the largest motion, making these regions ideal for implanting
motion-driven energy harvesting devices. This validated model is a robust tool
for enhancing our understanding of cardiac physiology and optimizing
therapeutic interventions before clinical implementation.

</details>


<div id='math.CV'></div>

# math.CV [[Back]](#toc)

### [49] [Quadrature Domains and the Faber Transform](https://arxiv.org/abs/2509.03777)
*Andrew J. Graven,Nikolai G. Makarov*

Main category: math.CV

TL;DR: Framework for reconstructing quadrature domains from quadrature functions using Faber transform, with classification of one-point domains and extension to weighted quadrature domains.


<details>
  <summary>Details</summary>
Motivation: To develop a systematic approach for reconstructing quadrature domains from their quadrature functions and extend the theory to weighted cases.

Method: Using Faber transform to derive formulae relating quadrature function h to Riemann map, then applying this to classify one-point quadrature domains and develop theory for power-weighted and log-weighted quadrature domains.

Result: Obtained complete classification of one-point quadrature domains with complex charge, developed reconstruction formulae for weighted quadrature domains, and provided illustrative examples.

Conclusion: The Faber transform provides an effective framework for reconstructing quadrature domains and their weighted generalizations from quadrature functions, with applications demonstrated through various examples.

Abstract: We present a framework for reconstructing any simply connected bounded or
unbounded, quadrature domain $\Omega$ from its quadrature function $h$. Using
the Faber transform, we derive formulae directly relating $h$ to the Riemann
map for $\Omega$. Through this approach, we obtain a complete classification of
one point quadrature domains with complex charge. We proceed to develop a
theory of weighted quadrature domains with respect to weights of the form
$\rho_a(w)=|w|^{2(a-1)}$ when $a > 0$ ("power-weighted" quadrature domains) and
the limiting case of $a=0$ ("log-weighted" quadrature domains). We obtain Faber
transform formulae for reconstructing these from their respective quadrature
functions as well. Several examples are presented throughout to illustrate this
approach both in the simply connected setting and in the presence of rotational
symmetry.

</details>


<div id='q-bio.PE'></div>

# q-bio.PE [[Back]](#toc)

### [50] [Sensitivity analysis of an epidemic model with a mass vaccination program of a homogeneous population](https://arxiv.org/abs/2509.04188)
*Ma. Cristina R. Bargo*

Main category: q-bio.PE

TL;DR: Sensitivity analysis of COVID-19 vaccination parameters shows vaccine rollout rate becomes more important than efficacy over time, and death ratio is key for reducing fatalities.


<details>
  <summary>Details</summary>
Motivation: To address vaccine hesitancy and determine which vaccination program parameters most influence infection and death outcomes in COVID-19 modeling.

Method: Used ordinary differential equation model with Latin hypercube sampling and partial rank correlation coefficient to analyze sensitivity of infection/death counts to transmission rate, latency period, infectious period, death ratio, vaccine efficacy, and rollout rate.

Result: Death count highly sensitive to death ratio; infection count initially sensitive to latency period but shifts to vaccine rollout rate over time; vaccine rollout rate becomes more significant than efficacy in long-term disease control.

Conclusion: Vaccine rollout speed is more critical than efficacy for controlling disease spread, and reducing death ratio through treatments/healthcare improvements is essential for minimizing fatalities.

Abstract: The COVID-19 pandemic forced the rapid development of vaccines and the
implementation of mass vaccination programs around the world. However, many
hesitated to take the vaccine due to concerns about its effectiveness. By
looking at an ordinary differential equation (ODE) model of disease spread that
incorporates a mass vaccination program, this study aims to determine the
sensitivity of the cumulative count of infected individuals ($W$) and the
cumulative death count ($D$) to the following model parameters: disease
transmission rate ($\beta$), reciprocal of the disease latency period
($\kappa$), reciprocal of the infectious period ($\gamma$), death ratio
($\alpha$), vaccine efficacy rate ($r$), and vaccine rollout rate ($\delta$).
This was implemented using Latin hypercube sampling and partial rank
correlation coefficient. Results show that $D$ is highly sensitive to $\alpha$
and shows increasing sensitivity to $\delta$ in the long run. On the other
hand, $W$ is highly sensitive to $\kappa$ at the beginning of the simulation,
but this weakens over time. In contrast, $W$ is not very sensitive to $\delta$
initially but becomes very significant in the long run. This supports the
importance of the vaccine rollout rate over the vaccine efficacy rate in
curbing the spread of the disease in the population. It is also worthwhile to
reduce the death ratio by developing a cure for the disease or improving the
healthcare system as a whole.

</details>
