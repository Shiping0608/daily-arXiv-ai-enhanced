<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 12]
- [math.AP](#math.AP) [Total: 20]
- [physics.comp-ph](#physics.comp-ph) [Total: 2]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 6]
- [eess.SP](#eess.SP) [Total: 1]
- [physics.ins-det](#physics.ins-det) [Total: 1]
- [quant-ph](#quant-ph) [Total: 3]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [physics.atom-ph](#physics.atom-ph) [Total: 1]
- [astro-ph.SR](#astro-ph.SR) [Total: 2]
- [math.ST](#math.ST) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 2]
- [math.OC](#math.OC) [Total: 1]
- [physics.space-ph](#physics.space-ph) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [math.FA](#math.FA) [Total: 2]
- [cs.CE](#cs.CE) [Total: 1]
- [astro-ph.GA](#astro-ph.GA) [Total: 1]
- [math.MG](#math.MG) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [PeTTO: Leveraging GPUs to Accelerate Topology Optimization with the Pseudo-Transient Methods](https://arxiv.org/abs/2509.06971)
*Mingyuan Yang,Qian Yu,Chao Yang*

Main category: math.NA

TL;DR: PeTTO is a GPU-accelerated topology optimization method that uses pseudo-transient methods and automatic differentiation for 40-50x speedup over CPUs.


<details>
  <summary>Details</summary>
Motivation: Bridge the gap between high-performance computing and topology optimization to enable faster and better designs for real-world problems by leveraging GPU acceleration.

Method: Integrates pseudo-transient methods with phase field approaches, transforming PDE-constrained optimization into time-dependent PDEs. Uses automatic differentiation for sensitivity calculations and hybrid pseudo-transient/accelerated methods for efficient equation solving.

Result: Achieves 40- to 50-fold speedup on single GPU vs desktop CPUs. Successfully demonstrated on mechanical and thermal problems, single-material and multi-material cases in both 2D and 3D.

Conclusion: PeTTO effectively bridges HPC and topology optimization, enabling significant computational speedups that can facilitate faster and improved designs for practical engineering applications.

Abstract: We present a Pseudo-Transient Topology Optimization (PeTTO) approach that can
leverage graphics processing units (GPUs) to efficiently solve single-material
and multi-material topology optimization problems. By integrating PeTTO with
phase field methods, the partial differential equations (PDEs) constrained
optimization problem in topology optimization is transformed into a set of time
dependent PDEs, which can be analyzed using the knowledge of transient physics.
The sensitivities with respect to the design variable are calculated with the
automatic differentiation which help avoid tedious and error-prone manual
derivations. The overall system of equations is efficiently solved using a
hybrid of the pseudo-transient method and the accelerated pseudo-transient
method, balancing the convergence rate and numerical stability. A variety of
numerical examples are presented to demonstrate the effectiveness and
efficiency of the proposed PeTTO approach. These examples cover different
physics scenarios including mechanical and thermal problems, as well as
single-material and multi-materials cases in both 2D and 3D. The numerical
results show a 40- to 50-fold speedup when running the same PeTTO code on a
single GPU compared to desktop CPUs. This work helps bridge the gap between
high-performance computing and topology optimization, potentially enabling
faster and better designs for real-world problems.

</details>


### [2] [Safe cross-entropy-based importance sampling for rare event simulations](https://arxiv.org/abs/2509.07160)
*Zhiwei Gao,George Karniadakis*

Main category: math.NA

TL;DR: Safe-ICE method improves failure probability estimation by using a weighted cross-entropy-penalized EM algorithm that automatically prunes redundant mixture components and introduces a novel two-component mixture with light and heavy-tailed distributions for better tail exploration.


<details>
  <summary>Details</summary>
Motivation: Conventional ICE methods using light-tailed mixture distributions suffer from slow convergence and instability when estimating very small failure probabilities, and require manual selection of mixture components which undermines stability.

Method: Developed a weighted cross-entropy-penalized EM algorithm that automatically prunes redundant components during iteration, and introduced a novel two-component mixture pairing light-tailed with heavy-tailed distributions for effective tail region exploration.

Result: Safe-ICE converges more rapidly and yields more accurate failure-probability estimates than standard ICE, while automatically identifying the appropriate number of mixture components without manual tuning.

Conclusion: The Safe-ICE method successfully addresses the limitations of conventional ICE by providing more stable and efficient failure probability estimation, particularly for extremely small probabilities, through automated component selection and improved tail exploration.

Abstract: The Improved Cross-Entropy (ICE) method is a powerful tool for estimating
failure probabilities in reliability analysis. Its core idea is to approximate
the optimal importance-sampling density by minimizing the forward
Kullback-Leibler divergence within a chosen parametric family-typically a
mixture model. However, conventional mixtures are often light-tailed, which
leads to slow convergence and instability when targeting very small failure
probabilities. Moreover, selecting the number of mixture components in advance
can be difficult and may undermine stability. To overcome these challenges, we
adopt a weighted cross-entropy-penalized expectation-maximization (EM)
algorithm that automatically prunes redundant components during the iterative
process, making the approach more stable. Furthermore, we introduce a novel
two-component mixture that pairs a light-tailed distribution with a
heavy-tailed one, enabling more effective exploration of the tail region and
thus accelerating convergence for extremely small failure probabilities. We
call the resulting method Safe-ICE and assess it on a variety of test problems.
Numerical results show that Safe-ICE not only converges more rapidly and yields
more accurate failure-probability estimates than standard ICE, but also
identifies the appropriate number of mixture components without manual tuning.

</details>


### [3] [Auxiliary space theory for the analysis of iterative methods for semidefinite linear systems](https://arxiv.org/abs/2509.07179)
*Jongho Park,Jinchao Xu*

Main category: math.NA

TL;DR: Auxiliary space theory provides unified framework for analyzing iterative methods for semidefinite linear systems by mapping them to equivalent elementary methods on larger auxiliary spaces, enabling sharp convergence estimates using basic linear algebra.


<details>
  <summary>Details</summary>
Motivation: To develop a unified theoretical framework for analyzing various iterative methods for solving semidefinite linear systems, addressing the need for sharp convergence estimates and generalization of existing results.

Method: Interpret iterative methods for original systems as equivalent elementary methods on auxiliary systems defined on larger spaces, derive convergence estimates using elementary linear algebra, establish identities for error propagation operators and condition numbers.

Result: Developed auxiliary space theory that generalizes and refines existing results, provides sharp convergence estimates, and is applicable to numerous advanced numerical methods in scientific computing.

Conclusion: The auxiliary space theory offers a powerful unified framework for analyzing iterative methods, demonstrated through successful application to subspace correction methods, Hiptmair-Xu preconditioners, and auxiliary grid methods with refined analyses.

Abstract: We present an auxiliary space theory that provides a unified framework for
analyzing various iterative methods for solving linear systems that may be
semidefinite. By interpreting a given iterative method for the original system
as an equivalent, yet more elementary, iterative method for an auxiliary system
defined on a larger space, we derive sharp convergence estimates using
elementary linear algebra. In particular, we establish identities for the error
propagation operator and the condition number associated with iterative
methods, which generalize and refine existing results. The proposed auxiliary
space theory is applicable to the analysis of numerous advanced numerical
methods in scientific computing. To illustrate its utility, we present three
examples -- subspace correction methods, Hiptmair--Xu preconditioners, and
auxiliary grid methods -- and demonstrate how the proposed theory yields
refined analyses for these cases.

</details>


### [4] [The spectrum of the Steklov-Helmholtz operator](https://arxiv.org/abs/2509.07249)
*Nilima Nigam,Kshitij Patil,Weiran Sun*

Main category: math.NA

TL;DR: A wavenumber-robust method for computing Steklov eigenpairs of Helmholtz operator using BIO-MOD approach that avoids ill-conditioning near exceptional wavenumbers through single layer ansatz and reduced SVD.


<details>
  <summary>Details</summary>
Motivation: The Steklov-Helmholtz eigenvalue problem becomes severely ill-conditioned when wavenumber approaches Dirichlet-Laplace eigenvalues, causing computational difficulties that need to be addressed.

Method: Reformulate using Dirichlet-to-Neumann map, employ single layer ansatz with reduced SVD to handle near-singular matrices, use spectral convergence for smooth domains and graded meshes for polygons.

Result: Successfully removes ill-conditioning near exceptional wavenumbers, achieves high accuracy (10 digits for polygons, arbitrary precision for smooth domains), enables shape optimization proving disk maximizes second eigenvalue.

Conclusion: BIO-MOD approach provides robust computational framework for Steklov-Helmholtz problems, enabling detailed study of spectral geometry and optimization with high accuracy even near problematic wavenumbers.

Abstract: We present a wavenumber-robust strategy for computing Steklov eigenpairs of
the Helmholtz operator $-\Delta -\mu^2$. As the wavenumber $\mu \rightarrow
\mu_D$ from below (where $\mu_D^2 $ is a Dirichlet- Laplace eigenvalue of
multiplicity $\ell$), the lowest $\ell$ Steklov-Helmholtz eigenvalues diverge
to $-\infty$. Computationally, the Steklov-Helmholtz eigenvalue problem becomes
severely ill-conditioned when $\mu \approx \mu_D$.
  We first reformulate the problem in terms of a suitably-defined
Dirichlet-to-Neumann map. We then use an indirect approach based on a single
layer ansatz. The discrete single layer matrix is nearly singular close to
exceptional wavenumbers, and we use a reduced singular value decomposition to
avoid the consequent ill-conditioning. For smooth domains, convergence of our
eigenvalue solver is spectral. We use this method (called the BIO-MOD approach)
for shape optimization of scale-invariant Steklov-Helmholtz problems and prove
that the disk maximizes the second eigenvalue under appropriate scaling. For
curvilinear polygons, we use polynomially-graded meshes rather than uniform
meshes. As a proof of concept, we also implemented BIO-MOD using RCIP
quadratures (using the ChunkIE implementation). The BIO-MOD approach
successfully removes ill-conditioning near exceptional wavenumbers, and very
high eigenvalue accuracy (up to 10 digits for polygons, arbitrary precision
accuracy for smooth domains) is observed.
  We deploy our approach to computationally study the spectral geometry of the
Steklov-Helmholtz operator, including some questions about spectral asymptotics
and spectral optimization.

</details>


### [5] [The Stability of Block Eliminations and Additive Modifications](https://arxiv.org/abs/2509.07305)
*Neil Lindquist,Piotr Luszczek,Jack Dongarra*

Main category: math.NA

TL;DR: Theoretical analysis of BEAM method's numerical stability, improving element growth bounds from cubic to quadratic and proposing better growth measures for block LU factorization.


<details>
  <summary>Details</summary>
Motivation: Existing theoretical analysis for the novel BEAM method is lacking, requiring stability analysis of both block LU factorization and additive modifications effects.

Method: Analyzed numerical stability of block LU factorization and additive modifications, improved previous element growth bounds, proposed alternative growth measures, and examined condition number impacts.

Result: Improved element growth analysis from cubic to quadratic bounds, developed better-aligned growth measures for block LU, and identified cases where BEAM avoids modifications while block LU remains stable.

Conclusion: BEAM method shows improved theoretical stability properties with better element growth bounds and can safely factor matrices without modifications in cases where regular block LU works.

Abstract: The block elimination with additive modifications (BEAM) method was recently
proposed as a alternative to LU with partial pivoting requiring less
communication. Because of the novelty of BEAM, the existing theoretical
analysis is lacking. To that end, we analyze both the numerical stability of
the underlying block LU factorization and the effects of additive
modifications. For the block LU factorization, we are able to improve the
previous results of Demmel et al. from being cubic in the element growth to
merely quadratic. Furthermore, we propose an alternative measure of element
growth that is better aligned with block LU; this new measure of growth allows
our analysis to apply to matrices that cannot be factored with pointwise LU. In
the second part, we analyzed the modifications produced by BEAM and the effect
they have on the condition number and growth factor. Finally, we show that BEAM
will not apply any modifications in some cases that regular block LU can safely
factor.

</details>


### [6] [Quasi-Monte Carlo integration over $\mathbb{R}^s$ with boundary-damping importance sampling](https://arxiv.org/abs/2509.07509)
*Zexin Pan,Du Ouyang,Zhijian He*

Main category: math.NA

TL;DR: New importance sampling method for quasi-Monte Carlo integration that designs adjustment factors and transport maps to handle improper proposals and maintain fast convergence in high dimensions.


<details>
  <summary>Details</summary>
Motivation: Traditional importance sampling with improper proposals creates severe adjustment factors that degrade QMC performance, especially in high-dimensional integration problems.

Method: Design adjustment factors with desired regularity properties first, then determine tractable transport maps from standard uniforms to proposals to damp boundary growth and enable effective QMC integration.

Result: The proposed importance sampling method achieves fast convergence rates independent of dimension s, making it suitable for high-dimensional problems.

Conclusion: This tailored importance sampling approach successfully reclaims QMC effectiveness by strategically designing adjustment factors and transport maps to handle improper proposals while maintaining dimensional independence.

Abstract: This paper proposes a new importance sampling (IS) that is tailored to
quasi-Monte Carlo (QMC) integration over $\mathbb{R}^s$. IS introduces a
multiplicative adjustment to the integrand by compensating the sampling from
the proposal instead of the target distribution. Improper proposals result in
severe adjustment factor for QMC. Our strategy is to first design a adjustment
factor to meet desired regularities and then determine a tractable transport
map from the standard uniforms to the proposal for using QMC quadrature points
as inputs. The transport map has the effect of damping the boundary growth of
the resulting integrand so that the effectiveness of QMC can be reclaimed.
Under certain conditions on the original integrand, our proposed IS enjoys a
fast convergence rate independently of the dimension $s$, making it amenable to
high-dimensional problems.

</details>


### [7] [High-order staggered Lagrangian hydrodynamics (II) : the artificial viscosity and hourglass control algorithm](https://arxiv.org/abs/2509.07636)
*Zhiyuan Sun,Jun Liu,Pei Wang*

Main category: math.NA

TL;DR: Extension of stiffness-based hourglass control and artificial viscosity algorithms for high-order staggered Lagrangian hydrodynamics, improving computational efficiency and implementation simplicity.


<details>
  <summary>Details</summary>
Motivation: To develop more efficient and easily implementable artificial viscosity and hourglass control methods for high-order staggered Lagrangian hydrodynamics, building on classical approaches.

Method: Extended stiffness-based hourglass control to high-order setting by enriching pressure field from Q^{m-1} to Q^m polynomial space, using unified framework. Applied artificial viscosity formulation from Dobrev et al. (2012) with explicit intermediate variable computation.

Result: Achieved concise viscosity formulation with explicitly computable intermediate variables, derived compact tensor viscosity form, and demonstrated accuracy and efficiency through 2D numerical experiments.

Conclusion: Proposed algorithms provide improved computational efficiency and easier implementation for high-order staggered Lagrangian hydrodynamics while maintaining accuracy, with classical methods emerging as special cases.

Abstract: In this article, we investigate the artificial viscosity and hourglass
control algorithms for high-order staggered Lagrangian hydrodynamics(SGH), as
proposed in~\cite[Sun et al., 2025]{Sun2025High}. Inspired by the subzonal
pressure method in classical staggered Lagrangian hydrodynamics, we extend the
stiffness-based hourglass control algorithm to the high-order setting,
enriching the pressure field from the $Q^{m-1}$ to the $Q^{m}$ polynomial
space. A unified framework for this hourglass control approach is established,
from which the classical subzonal pressure method naturally emerges as the
special case of the $Q^1-P^0$ space. The artificial viscosity follows the
formulation in~\cite[Dobrev et al., 2012]{Dobrev2012High}. We show that the
viscosity admits a concise form, with intermediate variables explicitly
computable, leading to improved computational efficiency and easier
implementation. Moreover, the tensor viscosity in classical staggered
Lagrangian hydrodynamics can be derived in a similarly compact and explicit
form. Numerical experiments on two-dimensional problems are presented to
demonstrate the accuracy and efficiency of the proposed algorithms.

</details>


### [8] [Physics-informed low-rank neural operators with application to parametric elliptic PDEs](https://arxiv.org/abs/2509.07687)
*Sebastian Schaffer,Lukas Exl*

Main category: math.NA

TL;DR: PILNO is a neural operator framework that combines low-rank kernel approximations with encoder-decoder architecture for efficient PDE solution approximation on point cloud data, using physics-informed training for both supervised and unsupervised settings.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient and scalable neural operator framework for approximating PDE solution operators that works on point cloud data, is independent of specific discretizations, and can handle both supervised and unsupervised learning scenarios.

Method: Combines low-rank kernel approximations with encoder-decoder architecture, trained using physics-informed penalty framework to ensure PDE constraints and boundary conditions are satisfied.

Result: Demonstrated effectiveness on diverse problems including function fitting, Poisson equation, screened Poisson equation with variable coefficients, and parameterized Darcy flow. The low-rank structure provides computational efficiency in high-dimensional parameter spaces.

Conclusion: PILNO establishes itself as a scalable and flexible surrogate modeling tool for PDEs, enabling fast, continuous one-shot predictions while remaining discretization-independent.

Abstract: We present the Physics-Informed Low-Rank Neural Operator (PILNO), a neural
operator framework for efficiently approximating solution operators of partial
differential equations (PDEs) on point cloud data. PILNO combines low-rank
kernel approximations with an encoder--decoder architecture, enabling fast,
continuous one-shot predictions while remaining independent of specific
discretizations. The model is trained using a physics-informed penalty
framework, ensuring that PDE constraints and boundary conditions are satisfied
in both supervised and unsupervised settings. We demonstrate its effectiveness
on diverse problems, including function fitting, the Poisson equation, the
screened Poisson equation with variable coefficients, and parameterized Darcy
flow. The low-rank structure provides computational efficiency in
high-dimensional parameter spaces, establishing PILNO as a scalable and
flexible surrogate modeling tool for PDEs.

</details>


### [9] [Realizability-preserving monolithic convex limiting in continuous Galerkin discretizations of the M1 model of radiative transfer](https://arxiv.org/abs/2509.07689)
*Paul Moujaes,Dmitri Kuzmin,Christian Bäumer*

Main category: math.NA

TL;DR: A finite element discretization of the M1 radiative transfer model with monolithic convex limiting to ensure physical realizability and invariant domain preservation.


<details>
  <summary>Details</summary>
Motivation: To develop a robust numerical method for radiative transfer that maintains physical admissibility (hyperbolicity and realizability) while achieving high-order accuracy, particularly for radiotherapy dose calculation applications.

Method: Continuous finite elements with a two-step monolithic convex limiting (MCL) procedure: first step enforces positivity of particle density and local bounds, second step constrains the first moment magnitude to ensure realizability. Uses bar states from Riemann problems and diagonally implicit treatment of reactive terms.

Result: The scheme is provably invariant domain preserving (IDP) and maintains high-order accuracy in smooth regions while preventing nonphysical behavior. Verified through numerical tests.

Conclusion: The developed methodology provides a robust simulation tool for dose calculation in radiotherapy that ensures physical realizability while maintaining numerical accuracy.

Abstract: We discretize the $M_1$ model of radiative transfer using continuous finite
elements and propose a tailor-made monolithic convex limiting (MCL) procedure
for enforcing physical realizability. The $M_1$ system of nonlinear balance
laws for the zeroth and first moments of a probability distribution function is
derived from the linear Boltzmann equation and equipped with an entropy-based
closure for the second moment. To ensure hyperbolicity and physical
admissibility, evolving moments must stay in an invariant domain representing a
convex set of realizable states. We first construct a low-order method that is
provably invariant domain preserving (IDP). Introducing intermediate states
that represent spatially averaged exact solutions of homogeneous Riemann
problems, we prove that these so-called bar states are realizable in any number
of space dimensions. This key auxiliary result enables us to show the IDP
property of a fully discrete scheme with a diagonally implicit treatment of
reactive terms. To achieve high resolution, we add nonlinear correction terms
that are constrained using a two-step MCL algorithm. In the first limiting
step, local bounds are imposed on each conserved variable to avoid spurious
oscillations and maintain positivity of the scalar-valued zeroth moment
(particle density). The second limiting step constrains the magnitude of the
vector-valued first moment to be realizable. The flux-corrected finite element
scheme is provably IDP. Its ability to prevent nonphysical behavior while
attaining high-order accuracy in smooth regions is verified in a series of
numerical tests. The developed methodology provides a robust simulation tool
for dose calculation in radiotherapy.

</details>


### [10] [Embedding structures in continua: linear models and finite element discretizations](https://arxiv.org/abs/2509.07735)
*David Portillo,Ignacio Romero*

Main category: math.NA

TL;DR: A general framework for coupling structural members (rigid bodies, beams, shells) with deformable continua using energy-based kinematic constraints, with proven well-posed finite element approximations.


<details>
  <summary>Details</summary>
Motivation: To develop models that can accurately describe the mechanical behavior of deformable continua containing embedded structural components, extending previous work from the Arlequin method.

Method: Extends continuum formulation using energy-based kinematic constraints to ensure compatibility between structural members and continua. Creates a general framework exploiting similarities across structural theories for energetic coupling.

Result: The problems and their finite element approximations are proven to be well-posed. Numerical examples demonstrate the approach's generality and robustness for bodies with inclusions, fibers, and embedded surfaces.

Conclusion: The proposed framework provides a robust and general method for modeling deformable continua with embedded structural members, with mathematically sound finite element implementations.

Abstract: This work describes models and numerical approximations that describe the
mechanical behavior of deformable continua with embedded structural members,
such as rigid bodies, beams, shells, etc. The continuum formulation extends an
idea first presented in the context of the Arlequin method and constrains the
kinematics of the two types of bodies to be compatible in the energy sense. In
the article, we exploit the shared similarities of all structural theories to
introduce a general framework for energetically coupling the latter with
continua. In addition, we show that the problems, as well as their finite
element approximations, are well-posed. Numerical examples of bodies with
inclusions, fibers, and embedded surfaces are provided to illustrate the
generality and robustness of the approach.

</details>


### [11] [Feature Understanding and Sparsity Enhancement via 2-Layered kernel machines (2L-FUSE)](https://arxiv.org/abs/2509.07806)
*Fabiana Camattari,Sabrina Guastavino,Francesco Marchetti,Emma Perracchione*

Main category: math.NA

TL;DR: A novel sparsity enhancement strategy using data-adaptive kernel metrics and 2-layered kernel machines for feature reduction in regression tasks, achieving minimal feature sets without performance loss.


<details>
  <summary>Details</summary>
Motivation: To develop a flexible, interpretable and accurate feature reduction scheme that identifies the most informative directions in feature space while maintaining predictive performance.

Method: Learning a data-adaptive kernel metric (shape matrix) through 2-layered kernel machines, then factorizing it via eigen-decomposition to identify informative feature directions.

Result: Achieves minimal yet highly informative feature sets without losing predictive performance, as demonstrated on synthetic and real geomagnetic storm datasets.

Conclusion: The proposed approach provides an effective sparsity enhancement strategy that offers both interpretability and accuracy in feature reduction for regression tasks.

Abstract: We propose a novel sparsity enhancement strategy for regression tasks, based
on learning a data-adaptive kernel metric, i.e., a shape matrix, through
2-Layered kernel machines. The resulting shape matrix, which defines a
Mahalanobis-type deformation of the input space, is then factorized via an
eigen-decomposition, allowing us to identify the most informative directions in
the space of features. This data-driven approach provides a flexible,
interpretable and accurate feature reduction scheme. Numerical experiments on
synthetic and applications to real datasets of geomagnetic storms demonstrate
that our approach achieves minimal yet highly informative feature sets without
losing predictive performance.

</details>


### [12] [Convergence analysis for the Barrett-Garcke-Nurnberg method of transport type](https://arxiv.org/abs/2509.07834)
*Genming Bai,Harald Garcke,Shravan Veerapaneni*

Main category: math.NA

TL;DR: Proposes a Barrett-Garcke-Nurnberg method for evolving geometries under general flows with transport-dominant nature, providing the first convergence proof for fully discrete methods in this context.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of establishing convergence proofs for curve evolution under general background velocity fields, which are transport-dominant rather than parabolic like mean curvature flow or surface diffusion.

Method: Derives discrete energy estimates of transport type on discretized polynomial surfaces using projection error framework, leveraging orthogonality structure for additional stability.

Result: Proves that the proposed method converges sub-optimally in the L2 norm, marking the first convergence proof for a fully discrete numerical method solving curve evolution driven by general flows.

Conclusion: The BGN method successfully handles transport-dominant curve evolution problems and provides theoretical convergence guarantees through novel energy estimates and projection error techniques.

Abstract: In this paper, we propose a Barrett-Garcke-Nurnberg (BGN) method for evolving
geometries under general flows and present the corresponding convergence
analysis. Specifically, we examine the scenario where a closed curve evolves
according to a prescribed background velocity field. Unlike mean curvature flow
and surface diffusion, where the evolution velocities inherently exhibit
parabolicity, this case is dominated by transport which poses a significant
difficulty in establishing convergence proofs. To address the challenges
imposed by this transport-dominant nature, we derive several discrete energy
estimates of the transport type on discretized polynomial surfaces within the
framework of the projection error. The use of the projection error is
indispensable as it provides crucial additional stability through its
orthogonality structure. We prove that the proposed method converges
sub-optimally in the L2 norm, and this is the first convergence proof for a
fully discrete numerical method solving the evolution of curves driven by
general flows.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [13] [Asymptotic models for the evolution of a circular biofilm](https://arxiv.org/abs/2509.07227)
*Diego Alonso-Orán,Ana Carpio,Rafael Granero-Belinchón*

Main category: math.AP

TL;DR: Analysis of thin poroelastic biofilm spreading using lubrication equations, deriving reduced nonlinear evolution equations with time-dependent coefficients through asymptotic expansion.


<details>
  <summary>Details</summary>
Motivation: To model and understand the spread of thin poroelastic biofilms at air/agar interfaces, which is important for studying microbial growth patterns and potential instabilities.

Method: Starting from a biofilm slab model, performed formal multi-scale asymptotic expansion to derive reduced nonlinear evolution equations with time-dependent coefficients. Established local-in-time existence and continuation criteria, then proved global existence under certain radius assumptions.

Result: Successfully derived reduced evolution equations, established mathematical existence results (both local and global under conditions), and numerical simulations revealed emergence of instabilities and potential finite-time singularities.

Conclusion: The study provides a mathematical framework for analyzing biofilm spreading, demonstrating both analytical existence results and numerical evidence of complex dynamic behaviors including instabilities and singularities.

Abstract: We study a class of lubrication-type equations modeling the spread of thin
poroelastic biofilms at air/agar interfaces. Starting from a biofilm slab
model, we perform a formal multi-scale asymptotic expansion to derive a reduced
nonlinear evolution equation with time-dependent coefficients. First, we
establish a local-in-time existence result as well as a continuation criteria.
Moreover, under suitable assumptions on the radius of the biofilm, we show the
existence of global in time solutions. We conclude with some numerical
simulations illustrating the emergence of instabilities and potential
singularities in finite time.

</details>


### [14] [Recent research on $(-1)$-homogeneous solutions of stationary Navier-Stokes equations](https://arxiv.org/abs/2509.07243)
*Li Li,Xukai Yan*

Main category: math.AP

TL;DR: Analysis of (-1)-homogeneous solutions to 3D stationary Navier-Stokes equations with singular rays, focusing on axisymmetric no-swirl cases and their singular behavior patterns.


<details>
  <summary>Details</summary>
Motivation: To systematically study and characterize the properties of (-1)-homogeneous solutions in three-dimensional stationary Navier-Stokes equations, particularly those exhibiting singular ray behavior, which is important for understanding fundamental mathematical properties of fluid dynamics equations.

Method: Exposition and analysis of recent research findings, with specific focus on axisymmetric solutions without swirl. Uses mathematical exposition and graphical visualization to illustrate different types of singular behavior patterns.

Result: Presents various typical types of singular behavior exhibited by these solutions, supported by graphical examples that demonstrate the different patterns of singularity formation in the flow field.

Conclusion: The paper provides a comprehensive overview of (-1)-homogeneous solutions with singular rays, highlighting their mathematical properties and demonstrating through visualization the diverse singular behaviors that can occur in axisymmetric no-swirl configurations of the stationary Navier-Stokes equations.

Abstract: We make an exposition of recent research on $(-1)$-homogeneous solutions of
the three-dimensional incompressible stationary Navier-Stokes equations with
singular rays. We also discuss properties of such solutions that are
axisymmetric with no swirl, and present graphs illustrating examples that
exhibit various typical types of singular behavior.

</details>


### [15] [The bidirectional NLS approximation for the one-dimensional Euler-Poisson system](https://arxiv.org/abs/2509.07371)
*Huimin Liu,Yurui Lu,Xueke Pu*

Main category: math.AP

TL;DR: This paper proves that solutions to the Euler-Poisson system can be approximated by two counter-propagating waves solving NLS equations under a multiple scale transformation, extending previous unidirectional results.


<details>
  <summary>Details</summary>
Motivation: To extend the earlier unidirectional NLS approximation results for ion-acoustic waves to bidirectional wave propagation, providing a more comprehensive mathematical framework.

Method: Using a multiple scale transformation and rigorous mathematical analysis to obtain uniform error estimates in Sobolev spaces for the one-dimensional Euler-Poisson system.

Result: Successfully demonstrated that solutions converge to two counter-propagating wave packets, each evolving independently as an NLS solution, with validity proven for physically relevant timescales of order O(ε⁻²).

Conclusion: This work provides the first rigorous construction and valid proof of bidirectional NLS approximation, significantly advancing the mathematical understanding of wave packet dynamics in dispersive systems.

Abstract: The nonlinear Schr\"{o}dinger (NLS) equation is known as a universal equation
describing the evolution of the envelopes of slowly modulated spatially and
temporarily oscillating wave packet in various dispersive systems. In this
paper, we prove that under a certain multiple scale transformation, solutions
to the Euler-Poisson system can be approximated by the sums of two
counter-propagating waves solving the NLS equations. It extends the earlier
results [Liu and Pu, Comm. Math. Phys., 371(2), (2019)357-398], which justify
the unidirectional NLS approximation to the Euler-Poisson system for the
ion-coustic wave. We demonstrate that the solutions could be convergent to two
counter-propagating wave packets, where each wave packet involves independently
as a solution of the NLS equation. We rigorously prove the validity of the NLS
approximation for the one-dimensional Euler-Poisson system by obtaining uniform
error estimates in Sobolev spaces. The NLS dynamics can be observed at a
physically relevant timespan of order $\mathcal{O}(\epsilon^{-2})$. As far as
we know, this result is the first construction and valid proof of the
bidirectional NLS approximation.

</details>


### [16] [Gradient Flows of Interfacial Energies: Curvature Agents and Incompressibility](https://arxiv.org/abs/2509.07380)
*Keith Promislow,Truong Vu,Brian Wetton*

Main category: math.AP

TL;DR: A framework for gradient flow of sharp-interface surface energies coupled to curvature active agents using penalty methods for incompressible flows.


<details>
  <summary>Details</summary>
Motivation: To develop methods for modeling surface energies that couple interface stretching/compression with local mass flux through curvature active agents.

Method: Penalty method to create locally incompressible gradient flows, with formal and rigorous convergence analysis including Γ-limit of Allen-Cahn type models.

Result: Established convergence of penalty method to incompressible flow for broad family of surface energies, with rigorous results for narrower class.

Conclusion: The framework successfully couples interface deformation with mass transport through curvature agents, with proven convergence properties.

Abstract: We present a framework for the gradient flow of sharp-interface surface
energies that couple to embedded curvature active agents. We use a penalty
method to develop families of locally incompressible gradient flows that couple
interface stretching or compression to local flux of interfacial mass. We
establish the convergence of the penalty method to an incompressible flow both
formally for a broad family of surface energies and rigorously for a more
narrow class of surface energies. We present an analysis, including a
$\Gamma$-limit, of an Allen-Cahn type model for a coupled surface agent
curvature energy.

</details>


### [17] [On the exponential convergence to equilibrium for ultrafast diffusion equations](https://arxiv.org/abs/2509.07382)
*Yi C. Huang,Xinhang Tong*

Main category: math.AP

TL;DR: Simple proof of exponential convergence for ultrafast diffusion equations using Poincaré inequality instead of optimal transport methods


<details>
  <summary>Details</summary>
Motivation: To simplify the proof of exponential convergence to equilibrium for ultrafast diffusion equations and extend results to higher dimensions

Method: Direct use of Poincaré inequality approach, eliminating the need for optimal transport arguments used in previous work

Result: Successfully extends convergence results to Gaussian measures in higher dimensions beyond the one-dimensional case

Conclusion: The simplified Poincaré inequality approach provides a more general framework for proving exponential convergence in ultrafast diffusion equations across multiple dimensions

Abstract: We propose a simple proof of the exponential convergence to equilibrium for
ultrafast diffusion equations in $\mathbb{R}^n$. Our approach, based on the
direct use of Poincar\'e inequality, gets rid of the optimal transport
arguments used in \cite{fathi2025} which are valid for Gaussian-excluded
one-dimensional weights. This simplification allows us to extend their results
to Gaussian measures in higher dimensions.

</details>


### [18] [Existence and stability of the Riemann solutions for a non-symmetric Keyfitz--Kranzer type model](https://arxiv.org/abs/2509.07391)
*Rahul Barthwal,Christian Rohde,Anupam Sen*

Main category: math.AP

TL;DR: Developed hyperbolic model for thin film flow with gravity and solute transport, analyzed as non-symmetric Keyfitz-Kranzer system. Found convex entropies, proved Riemann problem solutions with delta shocks, and confirmed structural stability under perturbations.


<details>
  <summary>Details</summary>
Motivation: To develop a mathematical model for thin film flow dynamics under gravity and solute transport effects, addressing the need for understanding first-order dynamics in such systems.

Method: Developed hyperbolic conservation law system, identified convex entropies in hyperbolic regions, included delta shocks, analyzed stability to perturbations, and validated with numerical schemes.

Result: Proved existence of unique Riemann problem solutions with delta shocks, demonstrated structural stability under initial data perturbations, and confirmed analytical results with numerical validation.

Conclusion: The developed hyperbolic model successfully captures thin film flow dynamics, with proven solution existence, stability properties, and validation through numerical methods, establishing structural robustness.

Abstract: In this article, we develop a new hyperbolic model governing the first-order
dynamics of a thin film flow under the influence of gravity and solute
transport. The obtained system turns out to be a non-symmetric Keyfitz-Kranzer
type system. We find an entire class of convex entropies in the regions where
the system remains strictly hyperbolic. By including delta shocks, we prove the
existence of unique solutions of the Riemann problem. We analyze their
stability with respect to the perturbation of the initial data and to the
gravity and surface tension parameters is analyzed. Moreover, we discuss the
large time behaviour of the solutions of the perturbed Riemann problem and
prove that the initial Riemann states govern it. Thus, we confirm the
structural stability of the Riemann solutions under the perturbation of initial
data. Finally, we validate our analytical results with well-established
numerical schemes for this new system of conservation laws.

</details>


### [19] [Blow-up for a Nonlocal Diffusion Equation with Time Regularly Varying Nonlinearity and Forcing](https://arxiv.org/abs/2509.07405)
*Rihab Ben Belgacem,Mohamed Majdoub*

Main category: math.AP

TL;DR: Analysis of blow-up phenomena for semilinear parabolic equations with mixed local-nonlocal diffusion and time-dependent coefficients, establishing sharp existence criteria and nonexistence results.


<details>
  <summary>Details</summary>
Motivation: Extend earlier results on nonlinear diffusion equations to the wider class of time-dependent coefficients in regularly varying functions, addressing a gap in understanding blow-up phenomena for such systems.

Method: Combines semigroup estimates for mixed diffusion operators, test function methods, and asymptotic properties of regularly varying functions to analyze both unforced and forced cases.

Result: Established sharp blow-up and global existence criteria via critical Fujita exponent, derived nonexistence conditions for global weak solutions, and provided sufficient smallness conditions ensuring global mild solutions.

Conclusion: First comprehensive study of blow-up phenomena for nonlinear diffusion equations with time-dependent coefficients in regularly varying functions, providing complete existence and nonexistence theory.

Abstract: We investigate the Cauchy problem for a semilinear parabolic equation driven
by a mixed local-nonlocal diffusion operator of the form \[ \partial_t u -
(\Delta - (-\Delta)^{\mathsf{s}})u = \mathsf{h}(t)|x|^{-b}|u|^p + t^\varrho
\mathbf{w}(x), \qquad (x,t)\in \mathbb{R}^N\times (0,\infty), \] where
$\mathsf{s}\in (0,1)$, $p>1$, $b\geq 0$, and $\varrho>-1$. The function
$\mathsf{h}(t)$ is assumed to belong to the generalized class of regularly
varying functions, while $\mathbf{w}$ is a prescribed spatial source. We first
revisit the unforced case and establish sharp blow-up and global existence
criteria in terms of the critical Fujita exponent, thereby extending earlier
results to the wider class of time-dependent coefficients. For the forced
problem, we derive nonexistence of global weak solutions under suitable growth
conditions on $\mathsf{h}$ and integrability assumptions on $\mathbf{w}$.
Furthermore, we provide sufficient smallness conditions on the initial data and
the forcing term ensuring global-in-time mild solutions. Our analysis combines
semigroup estimates for the mixed operator, test function methods, and
asymptotic properties of regularly varying functions. To our knowledge, this is
the first study addressing blow-up phenomena for nonlinear diffusion equations
with such a class of time-dependent coefficients.

</details>


### [20] [Nonlocal Harnack inequalities for nonlocal double phase equations I ; with positive bounded modulating coefficient with no Hölder condition](https://arxiv.org/abs/2509.07433)
*Yong-Cheol Kim*

Main category: math.AP

TL;DR: Nonlocal Harnack inequalities and local boundedness results for weak solutions to nonlocal double phase equations using De Giorgi-Nash-Moser theory


<details>
  <summary>Details</summary>
Motivation: To establish regularity properties and Harnack-type inequalities for solutions to nonlocal double phase equations, which combine two different growth behaviors in a nonlocal setting

Method: Applied De Giorgi-Nash-Moser theory to prove nonlocal Harnack inequalities for weak solutions, and derived local boundedness with explicit formulas and weak Harnack inequalities for weak supersolutions

Result: Successfully proved nonlocal Harnack inequalities for locally nonnegative weak solutions, obtained local boundedness with explicit formulas, and established weak Harnack inequalities for supersolutions

Conclusion: The De Giorgi-Nash-Moser theory provides an effective framework for establishing regularity results for nonlocal double phase equations, extending classical results to this more complex nonlocal setting

Abstract: In this paper, by applying the De Giorgi-Nash-Moser theory we prove nonlocal
Harnack inequalities for (locally nonnegative in $\Omega$) weak solutions to
nolocal double phase equations \begin{equation*}\begin{cases}\cL u =0 & \text{
in $\Omega$,} \\ u=g & \text{ in $\BR^n\s\Omega$ } \end{cases}\end{equation*}
where $\Omega\subset\BR^n$ ($n\ge 2$) is a bounded domain with Lipschitz
boundary, $\cL$ is the nonlocal double phase operator $\cL$ given by
\begin{equation*}\begin{split}\cL
u(x)=&\pv\int_{\BR^n}|u(x)-u(y)|^{p-2}(u(x)-u(y))K_{ps}(x,y)\,dy \\
&+\pv\int_{\BR^n}\fa(x,y)|u(x)-u(y)|^{q-2}(u(x)-u(y))K_{qt}(x,y)\,dy,
\end{split} \end{equation*} $0<\fa(x,y) = \fa(y,x) \le
\|\fa\|_{L^\iy(\BR^n\times\BR^n)} < \iy$ and $ps\ge qt$ for $0<s,t<1<p\le
q<\iy$. In addition, we get local boundedness with explicit formula and weak
Harnack inequalities for their weak supersolutions.

</details>


### [21] [Unveiling Biological Models Through Turing Patterns](https://arxiv.org/abs/2509.07458)
*Yuhan Li,Hongyu Liu,Catharine W. K. Lo*

Main category: math.AP

TL;DR: New amplitude-based approach uses spatial amplitude profiles of Turing patterns to recover all system parameters including diffusion constants and nonlinear functions, enabling complete biological system identification.


<details>
  <summary>Details</summary>
Motivation: Traditional inverse problems rely on non-biological boundary measurements, neglecting the rich information embedded in Turing patterns themselves that encode key biological mechanisms.

Method: Framework that leverages physical observables (amplitude of Turing patterns) from nature, using spatial amplitude profile of a single pattern to simultaneously recover all system parameters including wavelength, diffusion constants, and full nonlinear forms of chemotactic and kinetic coefficient functions.

Result: Demonstrated successful parameter identification on models of chemotactic bacteria, achieving complete recovery of system parameters from pattern amplitude data.

Conclusion: Establishes a biologically grounded, mathematically rigorous paradigm for reverse-engineering pattern formation mechanisms across diverse biological systems using amplitude-based approach.

Abstract: Turing patterns play a fundamental role in morphogenesis and population
dynamics, encoding key information about the underlying biological mechanisms.
Yet, traditional inverse problems have largely relied on non-biological data
such as boundary measurements, neglecting the rich information embedded in the
patterns themselves. Here we introduce a new research direction that directly
leverages physical observables from nature--the amplitude of Turing
patterns--to achieve complete parameter identification. We present a framework
that uses the spatial amplitude profile of a single pattern to simultaneously
recover all system parameters, including wavelength, diffusion constants, and
the full nonlinear forms of chemotactic and kinetic coefficient functions.
Demonstrated on models of chemotactic bacteria, this amplitude-based approach
establishes a biologically grounded, mathematically rigorous paradigm for
reverse-engineering pattern formation mechanisms across diverse biological
systems.

</details>


### [22] [Sharp multiscale control for high order nonlinear equations](https://arxiv.org/abs/2509.07517)
*Frédéric Robert*

Main category: math.AP

TL;DR: Analysis of high-order critical equation solutions on Riemannian manifolds with uniform Dirichlet energy bounds, proving sharp pointwise control by bubble sums.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of solution families to high-order critical equations on Riemannian manifolds under uniform energy constraints, particularly as the parameter approaches infinity.

Method: Analyze solutions to the high-order critical equation $P_\alpha u_\alpha = \Delta_g^k u_\alpha + \text{lot} = |u_\alpha|^{2^\star-2}u_\alpha$ with uniform Dirichlet energy bounds, proving pointwise control via explicit standard peaks.

Result: Established sharp pointwise control showing $|u_\alpha| \leq C\Vert u_\infty \Vert_\infty + C\sum_{i=1}^N B_{i,\alpha}$ where $u_\infty \in C^{2k}(M)$ and $B_{i,\alpha}$ are explicit standard peaks, uniformly as $\alpha\to+\infty$.

Conclusion: The study provides a precise characterization of solution behavior through bubble decomposition, demonstrating uniform control and convergence properties for high-order critical equations on Riemannian manifolds.

Abstract: We analyze the behavior of families $(u_\alpha)_{\alpha>0}$ of solutions to
the high-order critical equation $P_\alpha u_\alpha=\Delta_g^k u_\alpha
+\hbox{lot}=|u_\alpha|^{2^\star-2}u_\alpha$ on a Riemannian manifold $M$, with
a uniform bound on the Dirichlet energy. We prove a sharp pointwise control of
the $u_\alpha$'s by a sum of bubbles uniformly with respect to $\alpha\to
+\infty$, that is $|u_\alpha|\leq C\Vert u_\infty \Vert_\infty
+C\sum_{i=1}^NB_{i,\alpha}$ where $u_\infty \in C^{2k}(M)$ and the
$(B_{i,\alpha})_\alpha$, $i=1,...,N$ are explicit standard peaks.

</details>


### [23] [Fractional Sobolev logarithmic inequalities](https://arxiv.org/abs/2509.07559)
*Vivek Sahu*

Main category: math.AP

TL;DR: New logarithmic Sobolev inequalities established in fractional Sobolev spaces using interpolation approach without weights, extending classical results to nonlocal framework.


<details>
  <summary>Details</summary>
Motivation: To extend classical logarithmic Sobolev inequalities to the fractional Sobolev setting and provide new analytical tools for nonlocal analysis.

Method: Relies on an interpolation inequality (fractional Caffarelli-Kohn-Nirenberg inequality without weights) and relates optimal constant to corresponding variational problem.

Result: Established new logarithmic Sobolev inequalities in fractional Sobolev spaces, providing extension of classical results to nonlocal framework.

Conclusion: The approach successfully extends classical logarithmic Sobolev inequalities to fractional settings and offers new tools for analysis in fractional Sobolev spaces.

Abstract: We establish new logarithmic Sobolev inequalities in the fractional Sobolev
setting. Our approach relies on a interpolation inequality, which can be viewed
as a fractional Caffarelli-Kohn-Nirenberg inequality without weights. We
further relate the optimal constant in this interpolation inequality to a
corresponding variational problem. These results extend classical logarithmic
Sobolev inequalities to the nonlocal framework and provide new tools for
analysis in fractional Sobolev spaces.

</details>


### [24] [A multi-point maximum principle to prove global Harnack inequalities for Schrödinger operators](https://arxiv.org/abs/2509.07575)
*Ben Andrews,Daniel Hauer,Jessica Slegers*

Main category: math.AP

TL;DR: New methodology for proving global parabolic Harnack inequalities on Riemannian manifolds using multi-point maximum principle, avoiding higher-order derivatives and requiring only C^2 solutions.


<details>
  <summary>Details</summary>
Motivation: To develop a more direct proof approach for Harnack inequalities that avoids the need for gradient estimates and higher-order derivative requirements (C^4) in existing methods.

Method: Multi-point maximum principle argument to prove Harnack inequality directly, without establishing gradient estimates first. Applied to linear Schrödinger equation with nonnegative Ricci curvature and bounded below potential.

Result: Successfully proves global pointwise Harnack inequality with only C^2 solution requirement. For quadratic potential in Euclidean space, achieves sharper constants. Also extends to Schrödinger equations with gradient drift terms including Ornstein-Uhlenbeck operator.

Conclusion: The multi-point maximum principle provides a more efficient and less restrictive approach to proving Harnack inequalities, reducing solution smoothness requirements from C^4 to C^2 while achieving improved constants in special cases.

Abstract: In this article, we introduce a new methodology to prove global parabolic
Harnack inequalities on Riemannian manifolds. We focus on presenting a new
proof of the global pointwise Harnack inequality satisfied by positive
solutions of the linear Schr\"odinger equation on a Riemannian manifold $M$
with nonnegative Ricci curvature, where the potential term $V$ is bounded from
below. Our approach is based on a multi-point maximum principle argument.
Standard proofs of this result (see, for instance, Li-Yau [Acta Math, 1986])
rely on first establishing a gradient estimate. This requires the solution to
be at least $C^4$ on $M$. We instead prove the Harnack inequality directly,
which has the advantage of avoiding higher-order derivatives of the solution in
the proof, enabling us to assume it is only $C^2$ on $M$. In the particular
case that $V$ is the quadratic potential $V(x)=|x|^2$ and $M$ is the Euclidean
space $\mathbb{R}^d$, we prove a new Harnack inequality with sharper constants.
Finally, we treat positive solutions of the Schr\"odinger equation with a
gradient drift term, including applications to the Ornstein-Uhlenbeck operator
$\Delta - x\cdot \nabla$ with quadratic potential in $\mathbb{R}^d$.

</details>


### [25] [Normalized solution to Kirchhoff-fractional system involving critical Choquard nonlinearity](https://arxiv.org/abs/2509.07597)
*Divya Goel,Shilpa Gupta,Asmita Rai*

Main category: math.AP

TL;DR: Study of normalized solutions for fractional Kirchhoff-Choquard system with Riesz potential and L^2 mass constraints


<details>
  <summary>Details</summary>
Motivation: To investigate the existence of normalized solutions for a complex fractional Kirchhoff-Choquard system with critical exponents and mass constraints in different parameter regimes

Method: Mathematical analysis of the fractional PDE system using variational methods, critical point theory, and careful treatment of the Hardy-Littlewood-Sobolev critical exponents

Result: Establishes existence of normalized solutions for both L^2 subcritical case (2*2_{μ,*} < p + q < 4 + (4s-2μ)/N) and L^2 supercritical case (4 + (8s-2μ)/N < p + q < 2*2^*_{μ,s}) when α > 0

Conclusion: The paper successfully demonstrates the existence of normalized solutions for the fractional Kirchhoff-Choquard system in two distinct parameter regimes, contributing to the understanding of such nonlocal systems with mass constraints

Abstract: In this article, we explore the fractional Kirchhoff-Choquard system given by
  $$ \left\{ \begin{array}{lr} (a+b\int_{\mathbb{R}^N}|(-\Delta)^{\frac{s}{2}}
u|^2\;dx)(-\Delta)^su=\lambda_1u+(I_{\mu}*|v|^{{2^*_{\mu,s}}})|u|^{{2^*_{\mu,s}}-2}u
+\alpha p (I_{\mu}*|v|^{q})|u|^{p-2}u \;\text{in}\;\mathbb{R}^N,\\
(a+b\int_{\mathbb{R}^N}|(-\Delta)^{\frac{s}{2}}
v|^2\;dx)(-\Delta)^sv=\lambda_2v+
(I_{\mu}*|u|^{{2^*_{\mu,s}}})|v|^{{2^*_{\mu,s}}-2}u +\alpha
q(I_{\mu}*|u|^{p})|v|^{q-2}v \;\;\text{in}\;\mathbb{R}^N,\\
\int_{\mathbb{R}^N}|u|^2=d_1^2,\;\;\int_{\mathbb{R}^N}|v|^2=d_2^2. \end{array}
\right. $$
  where $N> 2s$, $s \in (0,1)$, $\mu \in (0, N)$, $\alpha \in\mathbb{R}$. Here,
$I_{\mu}:\mathbb{R}^N \to \mathbb{R}$ denotes the Riesz potential. We denote by
$2_{\mu,*}:=\frac{2N-\mu}{N}$ and $\frac{2N-\mu}{N-2s}:={2^*_{\mu,s}}$, the
lower and upper Hardy-Littlewood-Sobolev critical exponents, repectively, and
assume that $2_{\mu,*} < p,q< {2^*_{\mu,s}}$. Our primary focus is on the
existence of normalized solutions for the case $\alpha>0$ in two scenarios: the
$L^2$ subcritical case characterized by $22_{\mu,*}<p + q < 4 +
\frac{4s-2\mu}{N}$ and $L^2$ supercritical associated with
$4+\frac{8s-2\mu}{N}< p + q < 2{2^*_{\mu,s}}$.

</details>


### [26] [Normalized solutions for fractional Choquard equation with critical growth on bounded domain](https://arxiv.org/abs/2509.07618)
*Divya Goel,Asmita Rai*

Main category: math.AP

TL;DR: Existence of multiple positive solutions for critical fractional Choquard equation with perturbation on star-shaped bounded domains using minimization and mountain pass techniques.


<details>
  <summary>Details</summary>
Motivation: To establish multiplicity of positive solutions for critical fractional Choquard equations with perturbation terms, addressing the mathematical challenge of proving existence of multiple solutions in this complex nonlinear setting.

Method: Using minimization technique over an appropriate set for the first solution and the uniform mountain pass theorem for the second solution.

Result: Proved the existence of first and second positive solutions for the critical fractional Choquard equation with perturbation on star-shaped bounded domains.

Conclusion: The paper successfully demonstrates the multiplicity of positive solutions for the given critical fractional Choquard equation using variational methods and mountain pass techniques.

Abstract: In this work, we establish the multiplicity of positive solutions for the
following critical fractional Choquard equation with a perturbation on the
star-shaped bounded domain $$ \left\{ \begin{array}{lr}
  (-\Delta)^s u = \lambda u +\alpha|u|^{p-2}u+ \left( \int\limits_{\Omega}
\frac{|u(y)|^{2^{*}_{\mu ,s}}}{|x-y|^ \mu}\, dy\right) |u|^{2^{*}_{\mu
,s}-2}u\; \text{in} \; \Omega,\\ u>0\; \text{in}\; \Omega,\; \\ u = 0\;
\text{in} \; \mathbb{R}^{N}\backslash\Omega, \\ \int_{\Omega}|u|^2 dx=d,
\end{array} \right. $$ where, $s\in(0,1), N>2s$, $\alpha\in \mathbb{R}$, $d>0$,
$2<p<2^*_s:=\frac{2N}{N-2s}$ and $2^{*}_{\mu ,s}:=\frac{2N-\mu}{N-2s}$
represents fractional Hardy-Littlewood-Sobolev critical exponent. Using the
minimization technique over an appropriate set and the uniform mountain pass
theorem, we prove the existence of first and second solutions, respectively.

</details>


### [27] [A gradient estimate for the linearized translator equation](https://arxiv.org/abs/2509.07629)
*Kyeongsu Choi,Robert Haslhofer,Or Hershkovits*

Main category: math.AP

TL;DR: Develops analytic foundations for linearized translator equation in R^4, proving gradient estimates that substitute key missing estimates from lower dimensions.


<details>
  <summary>Details</summary>
Motivation: The Bernstein property fails in R^4, requiring new analytic tools to understand how singularity models of mean curvature flow fit together in moduli space.

Method: Proves a sharp gradient estimate for W_v (derivative of variation field) in the tip region, serving as substitute for fundamental quadratic concavity estimate and Hamilton's Harnack inequality.

Result: Obtains sharp bound for W_v and implies bound for W_tau, providing crucial control over derivatives in the tip region where previous methods fail.

Conclusion: The gradient estimate provides essential analytic foundations for studying linearized translator equation in R^4, enabling progress in understanding mean curvature flow singularities in this critical dimension.

Abstract: In this paper, we develop some analytic foundations for the linearized
translator equation in $\mathbb{R}^4$, i.e. in the first dimension where the
Bernstein property fails. This equation governs how the (noncompact)
singularity models of the mean curvature flow in $\mathbb{R}^4$ fit together in
a common moduli space. Here, we prove a gradient estimate, which gives a sharp
bound for $W_v$, namely for the derivative of the variation field $W$ in the
tip region. This serves as a substitute for the fundamental quadratic concavity
estimate from Angenent-Daskalopoulos-Sesum, which has been crucial for
controlling $Y_v$, namely the derivative of the profile function $Y$ in the tip
region. Moreover, together with interior estimates by virtue of the linearized
translator equation our gradient estimate implies a bound for $W_\tau$ as well.
Hence, our gradient estimate also serves as substitute Hamilton's Harnack
inequality, which has played an important role for controlling $Y_\tau$ in the
tip region.

</details>


### [28] [Singularidades para as soluções das Equações de Navier-Stokes and Euler e o Problema do Milênio](https://arxiv.org/abs/2509.07638)
*Milton C. Lopes Filho,Helena J. Nussenzveig Lopes*

Main category: math.AP

TL;DR: Overview of research history and current state regarding the Navier-Stokes Millennium Prize problem, fluid dynamics singularities, and Euler equations problem


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive survey of the research landscape surrounding one of the most important unsolved problems in mathematics - the Navier-Stokes existence and smoothness problem

Method: Plenary talk presentation format delivering a birds-eye view analysis of historical developments and current research status

Result: A Portuguese-language overview presented at the 2024 Brazilian Math Society meeting covering key developments in fluid dynamics singularity research

Conclusion: This talk serves as an accessible introduction to the complex mathematical challenges surrounding fluid dynamics singularities and the Navier-Stokes Millennium problem

Abstract: The purpose of this note is to offer a birds-eye view on the history and the
state-of-the-art in the research surrounding the Millenium Prize problem for
the Navier-Stokes equations, the general problem of singularities in fluid
dynamics and the corresponding problem for the Euler equations. This is the
content of a plenary talk delivered at the 2024 Biannual meeting of the
Brazilian Math Society by Helena Nussenzveig Lopes and it is written in
portuguese.

</details>


### [29] [Non-homogeneous Schrodinger systems with sign-changing and general nonlinearities: Infinitely many solutions](https://arxiv.org/abs/2509.07652)
*Guanwei Chen*

Main category: math.AP

TL;DR: Study of non-homogeneous nonlinear Schrödinger systems with sign-changing nonlinearities, using variational and perturbation methods to obtain infinitely many solutions.


<details>
  <summary>Details</summary>
Motivation: To address non-homogeneous nonlinear Schrödinger systems where the nonlinearity G may be sign-changing and can be super-quadratic or asymptotically-quadratic at infinity, overcoming difficulties in proving boundedness and asymptotic behaviors.

Method: Variational methods and perturbation methods are employed to construct solutions, with detailed analysis of approximate functionals and minimax value sequences.

Result: Infinitely many solutions are obtained for the system, with several typical examples provided to illustrate the main results.

Conclusion: The paper successfully extends conditions from homogeneous problems to non-homogeneous systems, handling sign-changing nonlinearities and providing new analytical techniques for such systems.

Abstract: In this paper, we study the non-homogeneous nonlinear Schr\"{o}dinger system
$$\left\{ \begin{array}{ll} -\triangle u_j+V_j(x)
u_j=g_j(x,u_1,\cdots,u_m)+h_j(x),& x\in \Omega,\\ \\ u_j:=u_j(x)=0,& x\in
\partial\Omega,\\ \\ j=1,2,\cdots,m, \end{array}\right. $$ where
$\Omega\subset\mathbb{R}^{N}$ ($N\ge2$) is a bounded smooth domain,
$(g_1,\cdots,g_m)$ is the gradient of $G(x,U)\in
C^1(\Omega\times\mathbb{R}^m,\mathbb{R})$, $G(x,U)$ may be sign-changing, and
it is super-quadratic or asymptotically-quadratic as $|U|\to\infty$. We obtain
infinitely many solutions by using variational methods and perturbation
methods, and we provide several typical examples to illustrate the main
results. The {\bf main novelties} are as follows. (1) The nonlinearity $G$ may
be sign-changing. (2) The nonlinearity $G$ is not only general, but also
super-quadratic or asymptotically-quadratic at infinity and zero. (3) The
nonlinearity $G$ is power-type or non-power-type. (4) We not only construct
some new conditions, but also apply some conditions used in homogeneous
problems to the study of non-homogeneous systems for the first time. The {\bf
main difficulties} come from the following three aspects. (1) The proof of
boundedness for $(PS)$ sequence of approximate functionals. (2) The detailed
analysis of the asymptotic behaviors of approximate functionals. (3) The
estimate of the upper and lower bounds for the minimax value sequence $\{c_k\}$
of the even function.

</details>


### [30] [Calibrated Reifenberg With Holes](https://arxiv.org/abs/2509.07731)
*Susanna Bertolini,Alessandro Preti,Daniele Valtorta*

Main category: math.AP

TL;DR: Study of calibrated Reifenberg theorem with holes, showing that sets approximable by calibrated planes at all scales imply measure bounds and rectifiability without additional β-number hypotheses.


<details>
  <summary>Details</summary>
Motivation: To extend Reifenberg theorem to include sets with holes while maintaining the property that approximation by calibrated planes implies rectifiability and measure bounds.

Method: Follows techniques from previous work but allows for holes in sets, using calibrated planes for approximation at all points and scales without requiring additional β-number conditions.

Result: Demonstrates that sets approximable by calibrated planes at all scales imply measure upper bounds and rectifiability, even when holes are present.

Conclusion: The calibrated Reifenberg theorem with holes provides a more general framework for establishing rectifiability and measure bounds without extra hypotheses on β-numbers, making the approach more self-contained and applicable to sets with holes.

Abstract: In this article, we study a calibrated version of Reifenberg theorem "with
holes". In particular we study sets that are suitably approximable at all
points and scales by calibrated planes and show that, without any additional
hypotheses on $\beta$-numbers, this implies measure upper bounds and
rectifiability. This article follows the main techniques introduced in a
previous article, but it allows for holes in the sets under consideration, and
is more self-contained.

</details>


### [31] [Duality estimates for subdiffusion problems including time-fractional porous medium type equations](https://arxiv.org/abs/2509.07862)
*Arlúcio Viana,Patryk Wolejko,Rico Zacher*

Main category: math.AP

TL;DR: Duality estimates for time-fractional subdiffusion problems, including porous medium equations, enabling uniqueness proofs and extending key estimates from classical to subdiffusive systems.


<details>
  <summary>Details</summary>
Motivation: To establish mathematical foundations for time-fractional subdiffusion problems, which are important in modeling anomalous diffusion phenomena, and to extend classical reaction-diffusion results to the fractional case.

Method: Proving duality estimates for time-fractional and general subdiffusion problems, considering both concrete equations with Laplacian operators and abstract problems in Hilbert space settings.

Result: Successfully derived duality estimates that can be used to prove uniqueness of weak solutions and extend key estimates from classical reaction-diffusion systems to subdiffusive cases.

Conclusion: The developed duality estimates provide powerful mathematical tools for analyzing time-fractional subdiffusion problems, bridging the gap between classical diffusion theory and fractional calculus applications.

Abstract: We prove duality estimates for time-fractional and more general subdiffusion
problems. An important example is given by subdiffusive porous medium type
equations. Our estimates can be used to prove uniqueness of weak solutions to
such problems, and they allow to extend a key estimate from classical
reaction-diffusion systems to the subdiffusive case. Besides concrete equations
involving a Laplacian, we also consider abstract problems in a Hilbert space
setting.

</details>


### [32] [Mean field control with absorption](https://arxiv.org/abs/2509.07877)
*Pierre Cardaliaguet,Joe Jackson,Panagiotis E. Souganidis*

Main category: math.AP

TL;DR: Analysis of mean field control with boundary absorption, establishing comparison principle for infinite-dimensional Hamilton-Jacobi equation and proving convergence from N-particle to limiting problem.


<details>
  <summary>Details</summary>
Motivation: To study mean field control problems where particles are absorbed at domain boundaries, addressing the hierarchical Hamilton-Jacobi equations that arise in N-particle systems and their convergence to an infinite-dimensional limiting problem.

Method: Established a comparison principle for the novel infinite-dimensional Hamilton-Jacobi equation defined on the space of sub-probability measures, and proved convergence of the N-particle problem value to the limiting problem value as N approaches infinity.

Result: Successfully developed mathematical framework for boundary-absorbed mean field control, with rigorous comparison principle and convergence proof for the hierarchical system to the infinite-dimensional limit.

Conclusion: The paper provides fundamental theoretical foundations for mean field control with boundary absorption, demonstrating the connection between finite-particle systems and their infinite-dimensional limits through Hamilton-Jacobi analysis.

Abstract: In this paper we study a mean field control problem in which particles are
absorbed when they reach the boundary of a smooth domain. The value of the
N-particle problem is described by a hierarchy of Hamilton-Jacobi equations
which are coupled through their boundary conditions. The value function of the
limiting problem; meanwhile, solves a Hamilton-Jacobi equation set on the space
of sub-probability measures on the smooth domain, i.e. the space of
non-negative measures with total mass at most one. Our main contributions are
(i) to establish a comparison principle for this novel infinite-dimensional
Hamilton-Jacobi equation and (ii) to prove that the value of the N-particle
problem converges in a suitable sense towards the value of the limiting problem
as N tends to infinity.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [33] [Investigation of particle dynamics and classification mechanism in a spiral jet mill through computational fluid dynamics and discrete element methods](https://arxiv.org/abs/2509.06965)
*Simone Bnà,Raffaele Ponzini,Mirko Cestari,Carlo Cavazzoni,Ciro Cottini,Andrea Benassi*

Main category: physics.comp-ph

TL;DR: CFD-DEM simulation study of jet-milling process reveals current limitations in predictive modeling and identifies missing elements for quantitative simulation.


<details>
  <summary>Details</summary>
Motivation: Predicting jet-milling outcomes from process parameters and material properties remains challenging due to technical difficulties in direct measurement of thermodynamics, flow properties, and particle statistics within mills.

Method: Coupled CFD-DEM (Computational Fluid Dynamics - Discrete Element Method) simulations were conducted and compared with recent modeling and experimental works to review current understanding of jet-mill physics.

Result: The study identifies that current simulation techniques lack the ability to fully describe non-isothermal, compressible, high Mach number fluid flow, proper particle-fluid and particle-particle interactions, and accurate fracture mechanics of particles upon collisions.

Conclusion: The paper analyzes missing elements and bottlenecks limiting jet-mill simulation techniques, and explores possible ways to overcome these limitations towards developing quantitative, predictive simulation capabilities for jet-milling processes.

Abstract: Predicting the outcome of jet-milling based on the knowledge of process
parameters and starting material properties is a task still far from being
accomplished. Given the technical difficulties in measuring thermodynamics,
flow properties and particle statistics directly in the mills, modelling and
simulations constitute alternative tools to gain insight in the process physics
and many papers have been recently published on the subject. An ideal
predictive simulation tool should combine the correct description of
non-isothermal, compressible, high Mach number fluid flow, the correct
particle-fluid and particle-particle interactions and the correct fracture
mechanics of particle upon collisions but it is not currently available. In
this paper we present our coupled CFD-DEM simulation results; while comparing
them with the recent modelling and experimental works we will review the
current understating of the jet-mill physics and particle classification.
Subsequently we analyze the missing elements and the bottlenecks currently
limiting the simulation technique as well as the possible ways to circumvent
them towards a quantitative, predictive simulation of jet-milling.

</details>


### [34] [A unified framework for data-driven construction of stochastic reduced models with state-dependent memory](https://arxiv.org/abs/2509.07264)
*Zhiyuan She,Liyao Lyu,Bryan Ronain Smith,Huan Lei*

Main category: physics.comp-ph

TL;DR: A unified framework for constructing stochastic reduced models with state-dependent memory for high-dimensional Hamiltonian systems, addressing non-Markovian effects and efficient phase space sampling.


<details>
  <summary>Details</summary>
Motivation: To accurately model heterogeneous non-Markovian effects where memory function depends on coarse-grained variables, and efficiently explore phase space for sampling both equilibrium and dynamical observables in reduced model construction.

Method: Uses consensus-based sampling to establish shared sampling strategy for constructing free energy function and collecting conditional two-point correlation functions. Formulates reduced dynamics as extended Markovian system with auxiliary variables to approximate state-dependent memory using two-point statistics.

Result: The constructed model yields generalized Langevin-type formulation with invariant distribution consistent with full dynamics. Demonstrated effectiveness on 2D CG model of alanine dipeptide molecule, showing state-dependent memory is essential for predicting non-equilibrium kinetic properties.

Conclusion: Accurately capturing state-dependent memory is crucial for predicting non-equilibrium kinetic properties, as standard generalized Langevin models with homogeneous kernels exhibit significant discrepancies.

Abstract: We present a unified framework for the data-driven construction of stochastic
reduced models with state-dependent memory for high-dimensional Hamiltonian
systems. The method addresses two key challenges: (\rmnum{1}) accurately
modeling heterogeneous non-Markovian effects where the memory function depends
on the coarse-grained (CG) variables beyond the standard homogeneous kernel,
and (\rmnum{2}) efficiently exploring the phase space to sample both
equilibrium and dynamical observables for reduced model construction.
Specifically, we employ a consensus-based sampling method to establish a shared
sampling strategy that enables simultaneous construction of the free energy
function and collection of conditional two-point correlation functions used to
learn the state-dependent memory. The reduced dynamics is formulated as an
extended Markovian system, where a set of auxiliary variables, interpreted as
non-Markovian features, is jointly learned to systematically approximate the
memory function using only two-point statistics. The constructed model yields a
generalized Langevin-type formulation with an invariant distribution consistent
with the full dynamics. We demonstrate the effectiveness of the proposed
framework on a two-dimensional CG model of an alanine dipeptide molecule.
Numerical results on the transition dynamics between metastable states show
that accurately capturing state-dependent memory is essential for predicting
non-equilibrium kinetic properties, whereas the standard generalized Langevin
model with a homogeneous kernel exhibits significant discrepancies.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [35] [Modulated deuteron spectra observed with the Magnetic Recoil neutron Spectrometer at the National Ignition Facility](https://arxiv.org/abs/2509.06999)
*Bao Nguyen,Yousef Lawrence,Christopher Wink,Timothy Mark Johnson,Niels Vanderloo,Benjamin Reichelt,Amber Hennessy,Daniel Thomas Casey,Dave Schlossberg,Nathan Masters,Jose Milovich,Ari Le,Stephen Craxton,Maria Gatu Johnson,Johan Frenje*

Main category: physics.plasm-ph

TL;DR: Analysis of energy modulations in neutron spectra measurements from fusion implosions shows two-stream instability causes modulations but doesn't significantly impact key performance metrics.


<details>
  <summary>Details</summary>
Motivation: Anomalous energy modulations observed in recoil deuterons during high-yield fusion experiments raised concerns about potential impact on measurement accuracy of key performance metrics like yield, ion temperature, and areal density.

Method: Used analytic calculations and particle-in-cell simulations to examine beam-plasma instabilities, and conducted statistical analysis of synthetic deuteron spectra to assess impact on measurements.

Result: Identified two-stream instability as the driving mechanism behind energy modulations, but found modulation-induced errors are within the measurement errors of yield, ion temperature, and areal density values.

Conclusion: The observed energy modulations do not have a significant impact on the Magnetic Recoil Spectrometer measurements of fusion performance metrics.

Abstract: The Magnetic Recoil Spectrometer (MRS) on the National Ignition Facility is
used to measure the neutron spectrum from deuterium-tritium fueled inertial
confinement fusion implosions via n-d elastic scattering and magnetic
dispersion of recoil deuterons. From the MRS-determined neutron spectrum, the
yield ($Y_n$), apparent ion temperature ($T_i$) and areal density ($\rho R$)
are determined. However, anomalous energy modulations in recoil deuterons have
been observed in several high-yield indirect drive experiments
($Y_n\sim10^{16}-10^{18}$). These observations raise concerns about their
potential impact on the MRS-inferred performance metrics. Analytic calculations
and particle-in-cell simulations are used to examine the possible beam-plasma
instabilities, which indicate the two-stream instability as the driving
mechanism behind energy modulations. Based on a statistical analysis of
synthetic deuteron spectra, the modulations-induced errors are found to be
within the errors of the determined $Y_n$, $T_i$ and $\rho R$ values and thus
do not have a significant impact on the MRS measurement.

</details>


### [36] [TGLF-SINN: Deep Learning Surrogate Model for Accelerating Turbulent Transport Modeling in Fusion](https://arxiv.org/abs/2509.07024)
*Yadi Cao,Futian Zhang,Wesley Liu,Tom Neiser,Orso Meneghini,Lawson Fuller,Sterling Smith,Raffi Nazikian,Brian Sammuli,Rose Yu*

Main category: physics.plasm-ph

TL;DR: TGLF-SINN is a neural network surrogate that accelerates turbulent transport predictions in tokamaks with reduced data requirements through feature engineering, physics-guided regularization, and Bayesian active learning.


<details>
  <summary>Details</summary>
Motivation: Whole device simulations requiring thousands of TGLF evaluations are computationally expensive, and traditional NN surrogates need large training datasets that are costly to obtain from gyrokinetic simulations.

Method: Three key innovations: (1) principled feature engineering to reduce prediction range, (2) physics-guided regularization of transport spectra for better generalization, and (3) Bayesian Active Learning to strategically select training samples based on model uncertainty.

Result: Achieves 12.4% lower LRMSE than baseline, requires only 25% of complete dataset to achieve comparable accuracy (LRMSE only 0.0165 higher than baseline), and provides 45x speedup over TGLF in flux matching applications.

Conclusion: TGLF-SINN demonstrates potential for training efficient surrogates for higher-fidelity models where data acquisition is costly and sparse, enabling faster whole device simulations while maintaining accuracy.

Abstract: The Trapped Gyro-Landau Fluid (TGLF) model provides fast, accurate
predictions of turbulent transport in tokamaks, but whole device simulations
requiring thousands of evaluations remain computationally expensive. Neural
network (NN) surrogates offer accelerated inference with fully differentiable
approximations that enable gradient-based coupling but typically require large
training datasets to capture transport flux variations across plasma
conditions, creating significant training burden and limiting applicability to
expensive gyrokinetic simulations. We propose \textbf{TGLF-SINN
(Spectra-Informed Neural Network)} with three key innovations: (1) principled
feature engineering that reduces target prediction range, simplifying the
learning task; (2) physics-guided regularization of transport spectra to
improve generalization under sparse data; and (3) Bayesian Active Learning
(BAL) to strategically select training samples based on model uncertainty,
reducing data requirements while maintaining accuracy. Our approach achieves
superior performance with significantly less training data. In offline
settings, TGLF-SINN reduces logarithmic root mean squared error (LRMSE) by 12.
4\% compared to the current baseline \base. Using only 25\% of the complete
dataset with BAL, we achieve LRMSE only 0.0165 higher than \base~and 0.0248
higher than our offline model (0.0583). In downstream flux matching
applications, our NN surrogate provides 45x speedup over TGLF while maintaining
comparable accuracy, demonstrating potential for training efficient surrogates
for higher-fidelity models where data acquisition is costly and sparse.

</details>


### [37] [Second harmonic generation by radially polarized laser beam propagating in homogeneous plasma](https://arxiv.org/abs/2412.15340)
*Shivani Aggarwal,Saumya Singh,Dinkar Mishra,Bhupesh Kumar,Pallavi Jha*

Main category: physics.plasm-ph

TL;DR: Radially polarized laser beams enable efficient second harmonic generation in homogeneous unmagnetized plasma through periodic oscillations along propagation axis, with enhanced SHG near beam axis.


<details>
  <summary>Details</summary>
Motivation: To investigate second harmonic generation from radially polarized laser beams in homogeneous unmagnetized plasma, unlike linearly/circularly polarized beams which require inhomogeneous or magnetized plasma.

Method: Used Lorentz force and continuity equations to derive radial/axial current densities, then wave equation for analytical SHG field amplitudes. Validated with Fourier Bessel Particle-In-Cell (FBPIC) simulations.

Result: SHG amplitudes show periodic oscillations with detuning length dependent on plasma density and laser parameters. Radial/axial contributions are highly enhanced near beam axis due to Gaussian profile.

Conclusion: Radially polarized beams facilitate efficient harmonic generation in homogeneous unmagnetized plasmas without requiring plasma inhomogeneity or magnetization.

Abstract: This study presents an investigation of second harmonic generation (SHG)
resulting from interaction of radially polarized laser beam propagating in
homogeneous, unmagnetized plasma. Lorentz force and continuity equations have
been used to derive the radial and axial current density components. Further,
using these densities in the wave equation leads to analytical expressions for
the SHG field amplitudes. These amplitudes exhibit periodic oscillations along
the propagation axis, characterized by detuning length dependent on plasma
density and laser parameters. Radial and axial contributions to SHG are found
to be highly enhanced near the beam axis due to the Gaussian beam profile of
the laser. The analytical findings are validated using Fourier Bessel
Particle-In-Cell (FBPIC) simulations. Notably, unlike linearly or circularly
polarized beams which require either inhomogeneous or magnetized plasma,
radially polarized beams facilitate efficient harmonic generation in
homogeneous unmagnetized plasmas.

</details>


### [38] [Measurement of ion acceleration and diffusion in a laser-driven magnetized plasma](https://arxiv.org/abs/2509.07880)
*J. T. Y. Chu,J. W. D. Halliday,C. Heaton,K. Moczulski,A. Blazevic,D. Schumacher,M. Metternich,H. Nazary,C. D. Arrowsmith,A. R. Bell,K. A. Beyer,A. F. A. Bott,T. Campbell,E. Hansen,D. Q. Lamb,F. Miniati,P. Neumayer,C. A. J. Palmer,B. Reville,A. Reyes,S. Sarkar,A. Scopatz,C. Spindloe,C. B. Stuart,H. Wen,P. Tzeferacos,R. Bingham,G. Gregori*

Main category: physics.plasm-ph

TL;DR: Chromium ion beam experiment shows wave-particle interactions drive acceleration and diffusion despite absence of strong fluid-scale turbulence, suggesting lower-hybrid drift instability as mechanism.


<details>
  <summary>Details</summary>
Motivation: To study ion beam behavior and acceleration mechanisms when fired through magnetized plasma jets, particularly investigating wave-particle interactions in controlled laboratory conditions.

Method: Used mono-energetic chromium ion beam (~450 MeV) fired through magnetized interaction region created by two counter-propagating laser-ablated plasma jets, with laser interferometry for turbulence analysis.

Result: Observed acceleration and diffusion of beam ions driven by wave-particle interactions, despite absence of strong fluid-scale turbulence. Suggests electrostatic, short-scale kinetic turbulence as driving mechanism.

Conclusion: Lower-hybrid drift instability identified as possible mechanism for particle acceleration through electrostatic kinetic turbulence in magnetized plasma environments.

Abstract: Here we present results from an experiment performed at the GSI Helmholtz
Centre for Heavy Ion Research. A mono-energetic beam of chromium ions with
initial energies of $\sim 450$ MeV was fired through a magnetized interaction
region formed by the collision of two counter-propagating laser-ablated plasma
jets. While laser interferometry revealed the absence of strong fluid-scale
turbulence, acceleration and diffusion of the beam ions was driven by
wave-particle interactions. A possible mechanism is particle acceleration by
electrostatic, short scale length kinetic turbulence, such as the lower-hybrid
drift instability.

</details>


### [39] [Wakefield generation and electron acceleration via propagation of radially polarized laser pulses in homogeneous plasma](https://arxiv.org/abs/2412.17709)
*Shivani Aggarwal,Saumya Singh,Dinkar Mishra,Bhupesh Kumar,Pallavi Jha*

Main category: physics.plasm-ph

TL;DR: Study shows radially polarized laser pulses generate stronger wakefields and higher electron energy gains compared to linearly polarized pulses in plasma.


<details>
  <summary>Details</summary>
Motivation: To investigate and compare wakefield generation and electron acceleration using radially polarized vs linearly polarized laser pulses in pre-ionized plasma.

Method: Analytical study using Lorentz force and continuity equations with perturbation technique and quasi-static approximation, validated with FBPIC simulation code. Test electron injection used to examine trapping and acceleration.

Result: Significant enhancement in longitudinal wakefield amplitude and electron energy gain with radially polarized laser pulses compared to linearly polarized pulses.

Conclusion: Radially polarized laser pulses are more effective for wakefield generation and electron acceleration in plasma than conventional linearly polarized pulses.

Abstract: The paper presents a study of wakefield generation and electron injection via
propagation of radially polarized laser pulses in homogeneous pre-ionized
plasma. The analytical study is based on Lorentz force and continuity
equations. Perturbation technique and quasi-static approximation are used for
evaluating the generated longitudinal wakefields. Trapping and acceleration of
electrons are examined by injecting a test electron in the generated
wakefields. The results are compared with those obtained via linearly polarized
laser pulses. The validation of analytical results is performed using the
Fourier-Bessel particle-in-cell (FBPIC) simulation code. It is seen that there
is a significant enhancement in amplitude of the longitudinal wakefield
generated and electron energy gain via radially polarized laser pulses as
compared to linearly polarized laser pulse case.

</details>


### [40] [Unifying Kappa Distribution Models for Non-Equilibrium Space Plasmas: A Superstatistical Approach Based on Moments](https://arxiv.org/abs/2509.07927)
*Abiam Tamburrini,Sergio Davis,Pablo S. Moya*

Main category: physics.plasm-ph

TL;DR: This paper establishes the validity of kappa distributions for non-equilibrium plasmas using superstatistics, introduces a new unified kappa distribution framework, and presents a moment-based velocity distribution that avoids temperature debates.


<details>
  <summary>Details</summary>
Motivation: Modeling velocity distributions in non-equilibrium steady-state plasmas is challenging, and existing kappa distributions have varying interpretations of temperature in out-of-equilibrium systems, requiring a unified fundamental framework.

Method: The study uses superstatistics to validate kappa distributions and introduces a new kappa distribution based on superstatistical parameters. It also develops a moment-based velocity distribution that relies on velocity moments rather than temperature.

Result: The research demonstrates that the general distribution depends on the thermal characteristics of the temperature distribution population and provides a unified framework connecting various kappa distributions with superstatistical temperature.

Conclusion: The findings enhance understanding of kappa distributions and offer a robust model for non-equilibrium space plasmas, providing a more fundamental framework that connects different distributions and bypasses traditional temperature debates.

Abstract: From the perspective of non-equilibrium statistical mechanics, modeling the
velocity distribution of particles in non-equilibrium, steady-state plasmas
presents a significant challenge. Under this context, a family of kappa
distributions has been widely used to capture the high-energy tails in space
plasmas. These distributions deviate from the canonical Maxwell-Boltzmann
statistics and vary significantly in their interpretation of the temperature of
an out-of-equilibrium system. In this letter, we establish the validity of any
kappa distribution from the standpoint of superstatistics. This study unifies
these models by introducing a new kappa distribution based on superstatistical
parameters, providing a more general and fundamental framework to connect these
distributions and the superstatistical temperature of a system. We demonstrate
that the general distribution depends on the thermal characteristics of the
modeled temperature distribution population. Furthermore, we present a
moment-based velocity distribution that bypasses the traditional temperature
debate, relying on the velocity moments. Our findings enhance the understanding
of kappa distributions and offer a robust model for non-equilibrium space
plasmas.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [41] [Asymmetric Modulation Design for Fluid-Antenna SWIPT Systems](https://arxiv.org/abs/2509.07610)
*Ahsan Mehmood,Ioannis Krikidis,Ghassan M. Kraidy*

Main category: eess.SP

TL;DR: Optimized modulation schemes for fluid antenna SWIPT systems that jointly maximize information rate and energy harvesting, outperforming conventional constellations.


<details>
  <summary>Details</summary>
Motivation: To improve the rate-energy region of simultaneous wireless information and power transfer systems by addressing the nonlinear characteristics of practical energy harvesting circuits.

Method: Formulated a dual-objective optimization problem using epsilon-constraint method to maximize discrete-input mutual information and harvested current, designing optimized constellations for various energy thresholds. Evaluated three FA port selection strategies: Best Port, Fixed Port, and Random Port.

Result: Significant performance gains in both information rate and energy harvesting compared to conventional constellations across all three port selection strategies.

Conclusion: The proposed optimized modulation schemes effectively enhance SWIPT system performance by jointly optimizing information transmission and energy harvesting capabilities.

Abstract: In this work, we propose the design of modulation schemes that improve the
rate-energy region of fluid antenna-assisted simultaneous wireless information
and power transfer (SWIPT) systems. By considering the nonlinear
characteristics of practical energy harvesting circuits, we formulate a
dual-objective rate-energy (RE) region optimization problem to jointly maximize
the discrete-input mutual information (DIMI) and harvested current. The problem
is solved using the epsilon-constraint method and optimized constellations are
designed for various energy harvesting thresholds. We then evaluate the
performance of the optimized constellations under three different fluid antenna
(FA) port selection strategies: (i) Best Port, (ii) Fixed Port, and (iii)
Random Port. Our simulation results demonstrate significant performance gains
of optimized constellations over conventional constellations in both
information rate and energy harvesting.

</details>


<div id='physics.ins-det'></div>

# physics.ins-det [[Back]](#toc)

### [42] [A Generalisable Generative Model for Multi-Detector Calorimeter Simulation](https://arxiv.org/abs/2509.07700)
*Piyush Raikwar,Anna Zaborowska,Peter McKeown,Renato Cardoso,Mikolaj Piorczynski,Kyongmin Yeo*

Main category: physics.ins-det

TL;DR: CaloDiT-2 is a diffusion transformer model that enables fast calorimeter shower simulations with strong generalization across multiple detectors, reducing data and training requirements by up to 25x and 20x respectively.


<details>
  <summary>Details</summary>
Motivation: Traditional Geant4 simulations for particle-detector interactions at collider experiments like LHC are computationally expensive. Generative ML methods can provide faster simulations but need to generalize across different detector geometries.

Method: Uses a diffusion model with transformer blocks, creates detector-agnostic data representations, employs pre-training on multiple detectors with rapid adaptation to new ones using the LEMURS dataset.

Result: Achieves one of the best accuracy-speed tradeoffs on CaloChallenge benchmarks, reduces data requirements by 25x and training time by 20x for new detectors, and becomes the first published pre-trained model for particle shower simulations integrated into Geant4.

Conclusion: CaloDiT-2 provides an efficient solution for fast calorimeter simulations with strong generalization capabilities, significantly reducing development effort for novel or evolving detector geometries while maintaining high accuracy.

Abstract: Collider experiments, such as those at the Large Hadron Collider, use the
Geant4 toolkit to simulate particle-detector interactions with high accuracy.
However, these experiments increasingly require larger amounts of simulated
data, leading to huge computing cost. Generative machine learning methods could
offer much faster calorimeter shower simulations by directly emulating detector
responses. In this work, we present CaloDiT-2, a diffusion model which uses
transformer blocks. As is the case for other models explored for this task, it
can be applied to specific geometries, however its true strength lies in its
generalisation capabilities. Our approach allows pre-training on multiple
detectors and rapid adaptation to new ones, which we demonstrate on the LEMURS
dataset. It reduces the effort required to develop accurate models for novel
detectors or detectors which are under development and have geometries that are
changed frequently, requiring up to 25x less data and 20x less training time.
To the best of our knowledge, this is the first pre-trained model to be
published that allows adaptation in the context of particle shower simulations,
with the model also included in the Geant4 toolkit. We also present results on
benchmarks on Dataset-2 from the community-hosted CaloChallenge, showing that
our models provide one of the best tradeoffs between accuracy and speed from
the published models. Our contributions include a mechanism for the creation of
detector-agnostic data representations, architectural modifications suitable
for the data modality, a pre-training and adaptation strategy, and publicly
released datasets and pre-trained models for broad use.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [43] [Quantum algorithms for general nonlinear dynamics based on the Carleman embedding](https://arxiv.org/abs/2509.07155)
*David Jennings,Kamil Korzekwa,Matteo Lostaglio,Andrew T Sornborger,Yigit Subasi,Guoming Wang*

Main category: quant-ph

TL;DR: This paper extends quantum algorithms for solving nonlinear differential equations beyond purely dissipative systems to a broader class of stable systems, including those with conserved quantities and non-resonant systems, enabling efficient quantum simulation for a wider range of nonlinear problems.


<details>
  <summary>Details</summary>
Motivation: Previous quantum algorithms for nonlinear differential equations were limited to purely dissipative systems with negative log-norm (R<1), excluding many important physical systems. This work aims to expand the scope of nonlinear systems that can be efficiently simulated on quantum computers.

Method: The authors correct technical issues in prior analysis and extend results to: 1) broader class of stable systems using quadratic Lyapunov functions, 2) systems with conserved polynomial quantities, and 3) non-resonant systems. They analyze the Carleman scheme convergence and obtain results related to Poincaré-Dulac theorem and Carleman matrix diagonalization.

Result: The paper demonstrates that efficient quantum algorithms exist for a much wider class of nonlinear systems than previously known, including proving BQP-completeness of nonlinear oscillator problems of exponential size.

Conclusion: This work significantly expands the applicability of quantum computing to nonlinear dynamics problems, making efficient quantum simulation possible for many important physical systems that were previously excluded from quantum algorithmic approaches.

Abstract: Important nonlinear dynamics, such as those found in plasma and fluid
systems, are typically hard to simulate on classical computers. Thus, if
fault-tolerant quantum computers could efficiently solve such nonlinear
problems, it would be a transformative change for many industries. In a recent
breakthrough [Liu et al., PNAS 2021], the first efficient quantum algorithm for
solving nonlinear differential equations was constructed, based on a single
condition $R<1$, where $R$ characterizes the ratio of nonlinearity to
dissipation. This result, however, is limited to the class of purely
dissipative systems with negative log-norm, which excludes application to many
important problems. In this work, we correct technical issues with this and
other prior analysis, and substantially extend the scope of nonlinear dynamical
systems that can be efficiently simulated on a quantum computer in a number of
ways. Firstly, we extend the existing results from purely dissipative systems
to a much broader class of stable systems, and show that every quadratic
Lyapunov function for the linearized system corresponds to an independent
$R$-number criterion for the convergence of the Carlemen scheme. Secondly, we
extend our stable system results to physically relevant settings where
conserved polynomial quantities exist. Finally, we provide extensive results
for the class of non-resonant systems. With this, we are able to show that
efficient quantum algorithms exist for a much wider class of nonlinear systems
than previously known, and prove the BQP-completeness of nonlinear oscillator
problems of exponential size. In our analysis, we also obtain several results
related to the Poincar\'{e}-Dulac theorem and diagonalization of the Carleman
matrix, which could be of independent interest.

</details>


### [44] [A Tensor Network Framework for Lindbladian Spectra and Steady States](https://arxiv.org/abs/2509.07709)
*Philipp Westhoff,Mattia Moroder,Ulrich Schollwöck,Sebastian Paeckel*

Main category: quant-ph

TL;DR: A tensor-network framework for computing steady states and low-lying excited states of open quantum many-body systems with high precision, enabling spectral analysis of non-Markovian environments.


<details>
  <summary>Details</summary>
Motivation: Quantum systems coupled to environments exhibit unique properties like unconventional non-equilibrium phases and environment-assisted entanglement preparation, requiring systematic analysis of quantum many-body phases out of equilibrium through Lindbladian eigenstates.

Method: Tensor-network-based framework utilizing complex-time Krylov spaces to solve non-Hermitian eigenvalue problems in open quantum systems, demonstrated on the interacting Bose-Hubbard model with dissipation-assisted hopping.

Result: High efficiency and accuracy demonstrated, enabling reliable finite-size scaling analysis of spectral gap and revealing anomalous relaxation phenomena in the studied system.

Conclusion: This method unlocks spectral analysis capabilities for generic open quantum many-body systems, including those with non-Markovian environments, overcoming limitations of conventional simulation approaches.

Abstract: Quantum systems coupled to (non-)Markovian environments attract increasing
attention due to their peculiar physical properties. Exciting prospects such as
unconventional non-equilibrium phases beyond the Mermin-Wagner limit, or the
environment-assisted, robust preparation of highly entangled states, demand a
systematic analysis of quantum many-body phases out of equilibrium. Akin to the
equilibrium case, this requires the computation of the low-lying eigenstates of
Lindbladians, a problem challenging conventional approaches for simulating
quantum many-body systems. Here, we undertake a first step to overcome this
limitation and introduce a tensor-network-based framework to compute
systematically not only steady states, but also low-lying excited states with
unprecedented precision for large, driven quantum many-body systems. Our
framework is based on recent advances utilizing complex-time Krylov spaces, and
we leverage these ideas to create a toolbox tailored to solve the challenging
non-Hermitian eigenvalue problem ubiquitous in open quantum systems. At the
example of the interacting Bose-Hubbard model driven by dissipation-assisted
hopping, we demonstrate the high efficiency and accuracy, enabling us to
perform a reliable finite-size scaling analysis of the spectral gap and
demonstrating the existence of anomalous relaxation. This method unlocks the
capability of spectral analysis of generic open quantum many-body systems,
suitable also for non-Markovian environments.

</details>


### [45] [An Egorov Theorem for Wasserstein Distances](https://arxiv.org/abs/2509.07185)
*Jordan Cotler,Felipe Hernández*

Main category: quant-ph

TL;DR: New Egorov's theorem in quantum mechanics using p-Wasserstein metric on Husimi functions, with p=1 giving low-regularity version and p>1 providing stronger estimates.


<details>
  <summary>Details</summary>
Motivation: To formulate a new version of Egorov's theorem in the Schrödinger picture using optimal transport metrics, extending classical results to quantum mechanics with varying regularity conditions.

Method: Using p-Wasserstein metric applied to Husimi functions of quantum states, with different p values (p=1 for low-regularity, p>1 for stronger estimates).

Result: Proved new Egorov's theorem formulation and derived optimal transport inequality analogous to Golse and Paul's result in mean-field many-body quantum mechanics.

Conclusion: The approach provides a flexible framework for Egorov-type theorems in quantum mechanics with varying regularity requirements through different p-Wasserstein metrics.

Abstract: We prove a new version of Egorov's theorem formulated in the Schr\"{o}dinger
picture of quantum mechanics, using the $p$-Wasserstein metric applied to the
Husimi functions of quantum states. The special case $p=1$ corresponds to a
"low-regularity" Egorov theorem, while larger values $p>1$ yield progressively
stronger estimates. As a byproduct of our analysis, we prove an optimal
transport inequality analogous to a result of Golse and Paul in the context of
mean-field many-body quantum mechanics.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [46] [Emergence of continuously varying critical exponents in coupled map lattice as an effect of quenched disorder](https://arxiv.org/abs/2509.07529)
*Priyanka D. Bhoyar,Govindan Rangarajan,Prashant M. gade*

Main category: cond-mat.stat-mech

TL;DR: A coupled map lattice model with quenched disorder shows continuously changing critical exponents at the absorbing phase transition, not matching any known universality class.


<details>
  <summary>Details</summary>
Motivation: To investigate how spatial disorder in asymmetric couplings affects the universality class of absorbing phase transitions in spatiotemporal systems.

Method: Developed a coupled map lattice model with quenched disorder where asymmetric coupling (p to right neighbor, 1-p to left neighbor) is introduced for 0 ≤ p ≤ 0.5.

Result: For p=0, system belongs to directed percolation universality class. For p>0, critical exponents change continuously and do not match any known universality class, showing power-law decay m(t) ∼ t^{-δ} at critical point.

Conclusion: The continuously changing critical exponents with disorder introduction may be related to changes in the eigenvalue spectrum of the connectivity matrix, representing a novel behavior in absorbing phase transitions.

Abstract: The transition to an absorbing phase in a spatiotemporal system is a
well-investigated nonequilibrium dynamic transition. The absorbing phase
transitions fall into a few universality classes, defined by the critical
exponents observed at the critical point. We present a coupled map lattice
(CML) model with quenched disorder in the couplings. In this model, spatial
disorders are introduced in the form of asymmetric coupling with a larger
coupling ($p$) to a neighbor on the right and a smaller coupling ($1-p$) to the
neighbor on the left, for $0 \le p \le0.5$. For $p=0$, the system belongs to
the directed percolation universality class. For $p>0$, we observe continuously
changing critical exponents at the critical point. The order parameter is the
fraction of turbulent sites $m(t)$. %sites that are not in the laminar region.
We observe a power-law decay, $m(t) \sim t^{-\delta}$, at the critical point
$\epsilon_c$, where $\epsilon$ is the diffusive coupling parameter. These
exponents change continuously and do not match any known universality class in
any limit. This could be related to changes in the eigenvalue spectrum of the
connectivity matrix as the disorder is introduced.

</details>


<div id='physics.atom-ph'></div>

# physics.atom-ph [[Back]](#toc)

### [47] [Benchmarking the Plane-Wave Born and Distorted Waves approximations for electron-impact collision strengthcomputations: the sample case of Sr II](https://arxiv.org/abs/2509.07684)
*Jérôme Deprince,Lucas Maison*

Main category: physics.atom-ph

TL;DR: This paper evaluates computational methods for modeling electron-impact excitation in Sr II, a key heavy ion in kilonova spectra, comparing Plane Wave Born and Distorted Waves approaches against R-matrix reference data.


<details>
  <summary>Details</summary>
Motivation: The discovery of gravitational waves from neutron star mergers (GW170817) and associated kilonovae confirmed these events as major sites for heavy element production via r-process. Accurate modeling of nebular-phase kilonova spectra requires detailed treatment of non-LTE effects and atomic processes.

Method: Two computational approaches were employed: Plane Wave Born approximation within pseudo-relativistic Hartree-Fock method, and Distorted Waves method using AUTOSTRUCTURE. The resulting collision strengths were compared against reference R-matrix data to evaluate accuracy.

Result: The study provides collision strength results and radiative parameters for forbidden transitions in Sr II, establishing benchmarks for computational approximations used in nebular-phase kilonova modeling.

Conclusion: The results provide essential benchmarks for approximation methods that could be used to compute atomic data for large-scale applications to all heavy elements in nebular-phase kilonova modeling.

Abstract: The discovery of gravitational waves from a neutron star merger in 2017
(GW170817) and the associated kilonova (AT2017gfo) confirmed these events as
key sites for heavy element production through the r-process. Subsequent
observations, including late-time spectra with \textit{JWST}, have highlighted
the need for accurate modeling of kilonova ejecta. In the photospheric phase,
atomic level populations can be estimated under LTE using Boltzmann and Saha
relations, but about a week after the merger the ejecta enters the nebular
phase where non-LTE effects dominate. Modeling nebular spectra therefore
requires a detailed treatment of radiative and collisional processes that
affect the population of atomic levels. This work focuses on electron-impact
excitation in Sr II, a heavy ion relevant for kilonova spectra. Two
computational approaches are employed: the Plane Wave Born approximation within
the pseudo-relativistic Hartree-Fock method, and a Distorted Waves method using
AUTOSTRUCTURE. The resulting collision strengths are compared against reference
R-matrix data to evaluate the accuracy of these approximations and their
suitability for large-scale applications to all heavy elements. In addition,
radiative parameters for forbidden transitions are computed. These results
provide an essential benchmark of approximations that could be used to compute
atomic data for nebular-phase kilonova modeling.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [48] [Imaging and Radio Signatures of Shock-Plasmoid Interaction](https://arxiv.org/abs/2509.07116)
*Pankaj Kumar,Judith T. Karpen,P. K. Manoharan,N. Gopalswamy*

Main category: astro-ph.SR

TL;DR: Study shows CME-driven shock interacting with plasmoid enhances particle acceleration efficiency, observed through radio bursts and white-light imaging.


<details>
  <summary>Details</summary>
Motivation: Understanding how shocks interact with coronal structures is crucial for understanding particle acceleration mechanisms in solar corona and inner heliosphere.

Method: Used simultaneous radio and white-light observations from LASCO, STEREO-A COR-2, Wind/WAVES and STEREO/WAVES to track plasmoid evolution, CME, and shock interaction at ≈7 solar radii.

Result: Detected interplanetary Type II radio burst as shock propagated through plasmoid, and Type III radio bursts during plasmoid-CME merging, indicating escaping electron beams from reconnection process.

Conclusion: Shock-plasmoid interactions clearly enhance particle acceleration efficiency associated with CMEs, with implications for electron acceleration in flare and heliospheric current sheets.

Abstract: Understanding how shocks interact with coronal structures is crucial for
understanding the mechanisms of particle acceleration in the solar corona and
inner heliosphere. Using simultaneous radio and white-light observations, we
investigate the interaction between a CME-driven shock and a plasmoid. LASCO
and STEREO-A COR-2 white-light images are analyzed to track the evolution of
the plasmoid, CME and its associated shock, while the Wind/WAVES and
STEREO/WAVES dynamic spectra provide complementary radio signatures of the
shock-plasmoid interaction at $\approx$7 R$_\odot$. An interplanetary Type II
radio burst was detected as the shock propagated through the plasmoid. The
merging of the plasmoid into the CME was accompanied by interplanetary Type III
radio bursts, suggesting escaping electron beams during the reconnection
process. These observations clearly demonstrate that shock-plasmoid
interactions can enhance the efficiency of particle acceleration associated
with CMEs, with implications for electron acceleration in flare and
heliospheric current sheets as well.

</details>


### [49] [CLEAN and multi-scale CLEAN for STIX in Solar Orbiter](https://arxiv.org/abs/2509.07167)
*Miriana Catalano,Anna Volpara,Paolo Massa,Michele Piana,Anna Maria Massone*

Main category: astro-ph.SR

TL;DR: Multi-scale CLEAN improves automated hard X-ray imaging with better resolution for STIX telescope data


<details>
  <summary>Details</summary>
Motivation: Standard CLEAN has limited automation and under-resolution issues for hard X-ray imaging with STIX telescope

Method: Developed a multi-scale version of CLEAN algorithm specifically for Spectrometer/Telescope for Imaging X-rays (STIX) data

Result: Multi-scale CLEAN provides reliable solution to automation and resolution limitations, performs well on complex experimental scenarios

Conclusion: Multi-scale CLEAN represents an effective improvement over traditional CLEAN for hard X-ray image reconstruction in solar observations

Abstract: CLEAN is a well-established deconvolution approach to Fourier imaging at both
radio wavelwengths and hard X-ray energies. However, specifically for hard
X-ray imaging, CLEAN suffers two significant drawbacks: a rather limited degree
of automation, and a tendency to under-resolution. This paper introduces a
multi-scale version of CLEAN specifically tailored to the reconstruction of
images from measurements observed by the Spectrometer/Telescope for Imaging
X-rays (STIX) on-board Solar Orbiter. Using synthetic STIX data, this study
shows that multi-scale CLEAN may represent a reliable solution to the two
previously mentioned CLEAN limitations. Further, this paper shows the
performances of CLEAN and its multi-scale release in reconstructing
experimental real scenarios characterized by complex emission morphologies.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [50] [A novel statistical workflow for nonstationary modelling of successive Fréchet extremes](https://arxiv.org/abs/2509.07296)
*Grace Burtenshaw,Joe Lane,Meagan Carney*

Main category: math.ST

TL;DR: Novel statistical framework for modeling successive extreme events in energy demand using dynamical systems theory, providing more robust estimates than traditional extreme value theory methods.


<details>
  <summary>Details</summary>
Motivation: Traditional extreme value theory struggles with temporally clustered extremes and has high uncertainty due to data scarcity and computational costs, making accurate estimation of successive extreme events challenging for energy planning.

Method: Leverages recent theoretical advances in successive extreme value modeling in dynamical systems, assuming time series data follows fat-tailed Fréchet distribution, providing a statistical workflow for robust return level estimates.

Result: The framework enables significantly more robust estimates of returns and magnitudes of successive extreme events compared to standard likelihood methods, as demonstrated on gas supply forecast scenarios from 2025-2050.

Conclusion: The introduced statistical framework addresses limitations of traditional EVT by providing improved modeling of successive extreme events, offering better accuracy for energy resource planning with benchmarked statistical measures.

Abstract: Accurate estimation of the frequency and magnitude of successive extreme
events in energy demand is critical for strategic resource planning.
Traditional approaches based on extreme value theory (EVT) are typically
limited to modelling isolated extreme events and struggle to capture the
dynamics of temporally clustered extremes, such as those driven by prolonged
extreme weather events. These limitations are exacerbated by the scarcity of
historical data and computational costs of longrun simulations leading to high
uncertainty in return level estimates for successive extremes. Here, we
introduce a novel statistical framework leveraging recent theoretical advances
in successive extreme value modelling in dynamical systems. Under reasonable
assumptions of the time series data (e.g. the data follow a fat-tailed
Fr\'{e}chet distribution), our tool allows for significantly more robust
estimates of returns and magnitudes of successive extreme events compared to
standard likelihood methods. We illustrate our statistical workflow on
scenarios of forecasted gas supply levels from 2025 to 2050. Common measures of
statistical accuracy are provided as benchmarks for comparison.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [51] [Grain Boundary Anisotropy and Its Influence on Helium Bubble Nucleation, Growth, and Decohesion in Polycrystalline Iron](https://arxiv.org/abs/2509.07197)
*Yang Zhang,Peter Hatton,Blas P. Uberuaga,Jason R. Trelewicz*

Main category: cond-mat.mtrl-sci

TL;DR: Study shows grain boundary character controls helium bubble evolution in nuclear materials through atomic-scale segregation energy landscapes and distinct stress-relief mechanisms.


<details>
  <summary>Details</summary>
Motivation: Helium bubble accumulation at grain boundaries degrades mechanical properties in nuclear reactor materials, but the complex evolution behaviors due to GB structural anisotropy remain poorly understood.

Method: Integrated accelerated molecular dynamics simulations with a novel atomic-scale metric called flexibility volume (V_f) that combines local atomic volume and vibrational properties to study helium segregation and bubble growth in body-centered cubic iron.

Result: V_f predicts deformation propensity; segregation energy landscape dictates initial helium clustering and bubble morphology. Low-energy channels in tilt Σ5 boundary enable 1D migration, while deep traps in twist Σ13 boundary promote larger, rounder bubbles. Two stress-relief mechanisms identified: loop punching in anisotropic tilt Σ5 boundary and interfacial decohesion in twist Σ11 boundary.

Conclusion: Establishes fundamental connection between GB crystallographic/energetical anisotropy and helium bubble evolution, providing critical insights for designing radiation-tolerant microstructures in nuclear materials.

Abstract: The accumulation of helium bubbles at grain boundaries (GBs) critically
degrades the mechanical integrity of structural materials in nuclear reactors.
While GBs act as sinks for radiation-induced defects, their inherent structural
anisotropy leads to complex helium bubble evolution behaviors that remain
poorly understood. This work integrates accelerated molecular dynamics
simulations and a novel atomic-scale metric, the flexibility volume (V_f), to
establish the interplay between GB character, helium segregation, and bubble
growth mechanisms in body-centered cubic iron. We demonstrate that V_f, which
incorporates both local atomic volume and vibrational properties, qualitatively
predicts deformation propensity. Our results reveal that the atomic-scale
segregation energy landscape dictates initial helium clustering and subsequent
bubble morphology, with low-energy channels in tilt {\Sigma}5 boundary
facilitating one-dimensional migration while isolated deep traps in twist
{\Sigma}13 boundary promote larger, rounder bubble morphology. Critically,
besides gradual bubble growth via trap mutation mechanism, we identify two
distinct stress-relief mechanisms: loop punching in anisotropic tilt {\Sigma}5
boundary and interfacial decohesion in twist {\Sigma}11 boundary, with the
dominant pathway determined by the interplay between bubble morphology and
local mechanical softness. This study establishes a fundamental connection
between GB crystallographic and energetical anisotropy and helium bubble
evolution, providing critical insights for designing radiation-tolerant
microstructures.

</details>


### [52] [Algorithmic differentiation for plane-wave DFT: materials design, error control and learning model parameters](https://arxiv.org/abs/2509.07785)
*Niklas Frederik Schmitz,Bruno Ploumhans,Michael F. Herbst*

Main category: cond-mat.mtrl-sci

TL;DR: AD-DFPT framework combines algorithmic differentiation and density-functional perturbation theory for automated gradient computation in DFT calculations without manual derivation.


<details>
  <summary>Details</summary>
Motivation: To enable accurate computation of derivatives for any DFT output quantity with respect to any input parameter without the need for manual derivation of gradient expressions.

Method: Combines algorithmic differentiation (AD) with density-functional perturbation theory (DFPT) to create an automated differentiation framework implemented in the Density-Functional ToolKit (DFTK).

Result: Successfully demonstrated broad applicability including inverse design of semiconductor band gaps, learning exchange-correlation functional parameters, and propagating DFT parameter uncertainties to relaxed structures.

Conclusion: The AD-DFPT framework opens promising research avenues for gradient-driven workflows in first-principles materials modeling by enabling automated and accurate derivative computations.

Abstract: We present a differentiation framework for plane-wave density-functional
theory (DFT) that combines the strengths of algorithmic differentiation (AD)
and density-functional perturbation theory (DFPT). In the resulting AD-DFPT
framework derivatives of any DFT output quantity with respect to any input
parameter (e.g. geometry, density functional or pseudopotential) can be
computed accurately without deriving gradient expressions by hand. We implement
AD-DFPT into the Density-Functional ToolKit (DFTK) and show its broad
applicability. Amongst others we consider the inverse design of a semiconductor
band gap, the learning of exchange-correlation functional parameters, or the
propagation of DFT parameter uncertainties to relaxed structures. These
examples demonstrate a number of promising research avenues opened by
gradient-driven workflows in first-principles materials modeling.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [53] [On Global Rates for Regularization Methods based on Secant Derivative Approximations](https://arxiv.org/abs/2509.07580)
*Coralia Cartis,Sadok Jerad*

Main category: math.OC

TL;DR: Inexact framework for high-order adaptive regularization methods using lower-order derivative approximations and lazy updates with proved iteration complexity bounds.


<details>
  <summary>Details</summary>
Motivation: To develop efficient high-order optimization methods that reduce computational costs by using approximate derivatives and lazy updates while maintaining strong convergence guarantees.

Method: Uses approximations of pth-order tensor based on lower-order derivatives, with high-order secant updates or lazy constant approximations between recalculations. Supports exact evaluations or finite difference approximations when refreshing.

Result: Proves O(max[ε₁^{-(p+1)/p}, ε₂^{(-p+1)/(p-1)}]) iteration bound for reaching (ε₁, ε₂) second-order stationary points. For p=2, achieves O(max[ε₁^{-3/2}, ε₂^{-3}]) bound using quasi-Newton approximations.

Conclusion: The framework provides computationally efficient high-order adaptive regularization methods with strong theoretical guarantees, particularly effective for second-order methods using quasi-Newton approximations.

Abstract: An inexact framework for high-order adaptive regularization methods is
presented, in which approximations may be used for the $p$th-order tensor,
based on lower-order derivatives. Between each recalculation of the $p$th-order
derivative approximation, a high-order secant equation can be used to update
the $p$th-order tensor as proposed in (Welzel 2024) or the approximation can be
kept constant in a lazy manner. When refreshing the $p$th-order tensor
approximation after $m$ steps, an exact evaluation of the tensor or a finite
difference approximation can be used with an explicit discretization stepsize.
For all the newly adaptive regularization variants, we prove an
$\mathcal{O}\left( \max[ \epsilon_1^{-(p+1)/p}, \, \epsilon_2^{(-p+1)/(p-1)} ]
\right)$ bound on the number of iterations needed to reach an $(\epsilon_1, \,
\epsilon_2)$ second-order stationary points. Discussions on the number of
oracle calls for each introduced variant are also provided.
  When $p=2$, we obtain a second-order method that uses quasi-Newton
approximations with an $\mathcal{O}\left(\max[\epsilon_1^{-3/2}, \, \,
\epsilon_2^{-3}]\right)$ iteration bound to achieve approximate second-order
stationarity.

</details>


<div id='physics.space-ph'></div>

# physics.space-ph [[Back]](#toc)

### [54] [Comparing Simulated and Observed Particle Energy Distributions through Magnetic Reconnection in Earth's Magnetotail](https://arxiv.org/abs/2509.07621)
*Nadja Reisinger,Fabio Bacchini*

Main category: physics.space-ph

TL;DR: Fully kinetic 2D simulations of magnetic reconnection match observed ion and electron energy distributions but underestimate high-energy electron tails, with upstream temperatures being critical while numerical parameters have little effect.


<details>
  <summary>Details</summary>
Motivation: Magnetic reconnection accelerates particles to high energies in Earth's magnetosphere, providing a natural laboratory to study this explosive process using Magnetospheric Multiscale mission observations.

Method: Performed fully kinetic 2D simulations of a reconnection event observed by the Magnetospheric Multiscale mission and compared resulting ion and electron energy distributions with actual observations.

Result: Simulations captured overall shape and evolution of non-thermal energy distributions for both species but generally underestimated the very-high-energy tail of electron spectrum. Upstream temperatures proved critical while numerical parameters had negligible effects.

Conclusion: 2D simulations have limitations in capturing observed particle energization accurately, highlighting the need for more realistic 3D simulations to better reproduce high-energy electron acceleration in magnetic reconnection events.

Abstract: Magnetic reconnection is an explosive process that accelerates particles to
high energies in Earth's magnetosphere, offering a unique natural laboratory to
study this phenomenon. We performed fully kinetic 2D simulations of a
reconnection event observed by the Magnetospheric Multiscale mission and
compared the resulting ion and electron energy distributions with observations.
The simulations capture the overall shape and evolution of non-thermal energy
distributions for both species, but generally underestimate the
very-high-energy tail of the electron spectrum. Variations in numerical
parameters have negligible effects on the resulting spectra, while the initial
upstream temperatures instead play a critical role in reproducing the observed
distributions. This work presents a novel analysis of particle acceleration in
fully kinetic modeling of reconnection directly informed by observed, realistic
parameters; highlights the limitations of 2D simulations and underlines the
need for more realistic simulations (e.g. employing 3D setups) to capture the
observed particle energization more accurately.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [55] [HYLU: Hybrid Parallel Sparse LU Factorization](https://arxiv.org/abs/2509.07690)
*Xiaoming Chen*

Main category: cs.AR

TL;DR: HYLU is a hybrid parallel LU factorization solver for sparse linear systems on multi-core shared-memory systems that outperforms Intel MKL PARDISO by 1.74-2.26X.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient general-purpose solver for sparse linear systems (Ax=b) that can adapt to various sparsity patterns and perform well on multi-core shared-memory architectures.

Method: Uses hybrid parallel LU factorization with integrated hybrid numerical kernels that adapt to different sparsity patterns of coefficient matrices.

Result: Outperforms Intel MKL PARDISO by geometric means of 1.74X for one-time solving and 2.26X for repeated solving across 34 sparse matrices from SuiteSparse Matrix Collection.

Conclusion: HYLU provides significant performance improvements over existing solvers and is available as open-source software for efficient sparse linear system solving.

Abstract: This article introduces HYLU, a hybrid parallel LU factorization-based
general-purpose solver designed for efficiently solving sparse linear systems
(Ax=b) on multi-core shared-memory architectures. The key technical feature of
HYLU is the integration of hybrid numerical kernels so that it can adapt to
various sparsity patterns of coefficient matrices. Tests on 34 sparse matrices
from SuiteSparse Matrix Collection reveal that HYLU outperforms Intel MKL
PARDISO in the numerical factorization phase by geometric means of 1.74X (for
one-time solving) and 2.26X (for repeated solving). HYLU can be downloaded from
https://github.com/chenxm1986/hylu.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [56] [Homogenization with Guaranteed Bounds via Primal-Dual Physically Informed Neural Networks](https://arxiv.org/abs/2509.07579)
*Liya Gaynutdinova,Martin Doškář,Ondřej Rokoš,Ivana Pultarová*

Main category: cs.LG

TL;DR: Dual formulation for PINNs improves reliability in homogenizing periodic thermo-conductive composites with discontinuous coefficients, providing guaranteed error bounds and better failure detection.


<details>
  <summary>Details</summary>
Motivation: Standard PINNs often fail when applied to materials with discontinuous coefficients like piecewise constant properties, lacking clear diagnostics for failure.

Method: Introduces dual formulation for PINN framework, comparing standard PINNs with smoothed approximations vs variational PINNs (VPINNs) using spectral and neural network-based test functions.

Result: Strong-form PINNs outperform VPINNs in controlled settings but are sensitive to material discontinuities. VPINNs handle piecewise constants directly but require careful test function selection. Dual formulation provides reliable convergence indicators.

Conclusion: Integration of dual formulation into PINN frameworks enhances applicability to homogenization problems in micromechanics by providing guaranteed error bounds and robust failure detection.

Abstract: Physics-informed neural networks (PINNs) have shown promise in solving
partial differential equations (PDEs) relevant to multiscale modeling, but they
often fail when applied to materials with discontinuous coefficients, such as
media with piecewise constant properties. This paper introduces a dual
formulation for the PINN framework to improve the reliability of the
homogenization of periodic thermo-conductive composites, for both strong and
variational (weak) formulations. The dual approach facilitates the derivation
of guaranteed upper and lower error bounds, enabling more robust detection of
PINN failure. We compare standard PINNs applied to smoothed material
approximations with variational PINNs (VPINNs) using both spectral and neural
network-based test functions. Our results indicate that while strong-form PINNs
may outperform VPINNs in controlled settings, they are sensitive to material
discontinuities and may fail without clear diagnostics. In contrast, VPINNs
accommodate piecewise constant material parameters directly but require careful
selection of test functions to avoid instability. Dual formulation serves as a
reliable indicator of convergence quality, and its integration into PINN
frameworks enhances their applicability to homogenization problems in
micromechanics.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [57] [Random Trajectory Models for Complex Phenomena](https://arxiv.org/abs/2509.06963)
*Jeffrey D. Picka*

Main category: cond-mat.soft

TL;DR: The paper introduces a random trajectory (RT) model framework for adding probabilistic components to deterministic simulations to account for between-realization variability in complex phenomena.


<details>
  <summary>Details</summary>
Motivation: Many deterministic models for complex phenomena fail to capture between-realization variability observed in experimental realizations, necessitating probabilistic extensions.

Method: Proposes RT model framework that treats models as code with adjustable coefficients rather than differential equations, adding probabilistic components to deterministic base models.

Result: Provides a necessary condition for easy model fitting and verification, but argues this condition is rarely checkable, making most code models effectively black boxes with limited explanatory power.

Conclusion: Validating useful code models requires extensive additional experimentation and statistical analysis beyond initial model construction, highlighting the challenges in establishing scientific validity for such models.

Abstract: Many models for complex phenomena use a model for strongly-interacting
elements on a small scale to generate larger-scale simulations of some aspects
of experimental realizations. These models may be agent-based (as in the case
of discrete element method models for granular flow) or based on
pattern-forming systems of PDEs (as in models for Raleigh-B\`enard convection
patterns). Often these models are purely deterministic, producing a single
simulation for each set of initial conditions. If observed realizations
demonstrate between-realization variability for important aspects of the
phenomenon, those aspects can be simulated by adding probabilistic components
to the deterministic models to create random trajectory (RT) models.
  The RT model framework provides probabilistic models which can be fit to data
and validated, together with a clear perspective on how difficult it can be to
establish any kind of validity for a fitted model. It treats models as code
with adjustable coefficients, rather than as systems of differential equations.
It provides a simply stated necessary condition for these code models to be
easily fit and verified, as well as an argument that this condition can almost
never be checked. When the necessary condition cannot be checked, the RT model
framework identifies the code models as black box models which may have the
capacity for emulating the joint distributions of small collections of
statistics observed on realizations, but which can only provide very weak
evidence for any form of explanation for the emergence of any aspect of the
phenomenon. The framework also provides a way to clearly understand why finding
a useful code model and scientifically validating it may require many
person-years of extra experimentation and statistical analysis undertaken after
the first output-producing code model is constructed and contributed.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [58] [Model Order Reduction for Quantum Molecular Dynamics](https://arxiv.org/abs/2509.07340)
*Siu Wun Cheung,Youngsoo Choi,Jean-Luc Fattebert,Daniel Osei-Kuffuor*

Main category: physics.chem-ph

TL;DR: Novel model order reduction approach for quantum molecular dynamics that learns low-dimensional representation of electronic solution manifold to avoid expensive iterative wavefunction optimization.


<details>
  <summary>Details</summary>
Motivation: Quantum molecular dynamics provides high predictive power but is computationally dominated by expensive iterative electronic structure calculations, creating a need for more efficient alternatives.

Method: Learning low-dimensional representation of electronic solution manifold within Kohn-Sham DFT framework, projecting ground state electronic density determination onto low-dimensional subspace instead of full iterative optimization.

Result: Demonstrated on water molecule with excellent agreement to high-fidelity simulations for both molecular geometry and dynamic properties, showing good generalizability.

Conclusion: The model order reduction approach successfully provides an efficient alternative to computationally expensive high-fidelity electronic structure calculations in quantum molecular dynamics.

Abstract: Molecular dynamics simulations are indispensable for exploring the behavior
of atoms and molecules. Grounded in quantum mechanical principles, quantum
molecular dynamics provides high predictive power but its computational cost is
dominated by iterative high-fidelity electronic structure calculations. We
propose a novel model order reduction approach as an alternative to
high-fidelity electronic structure calculation. By learning a low-dimensional
representation of the electronic solution manifold within the Kohn-Sham density
functional theory framework, our model order reduction approach determines the
ground state electronic density by projecting the problem onto a
low-dimensional subspace, thereby avoiding the computationally expensive
iterative optimization of electronic wavefunctions in the full space. We
demonstrate the capability of our method on a water molecule, showing excellent
agreement with high-fidelity simulations for both molecular geometry and
dynamic properties, highlighting the generalizability through carefully
designed parametrization and systematic sampling.

</details>


<div id='math.FA'></div>

# math.FA [[Back]](#toc)

### [59] [How smooth are restrictions of Besov functions?](https://arxiv.org/abs/2509.07420)
*Julien Brasseur*

Main category: math.FA

TL;DR: This paper completes previous work showing that Besov spaces only have the restriction property when q≤p, and proves that the characterization of partial maps belonging to Besov spaces of generalized smoothness is sharp.


<details>
  <summary>Details</summary>
Motivation: To complete the characterization of restriction properties in Besov spaces, building on previous work that showed Besov spaces B_{p,q}^s only enjoy restriction properties when q≤p, and that partial maps belong to generalized smoothness spaces under certain conditions.

Method: The authors build upon their previous construction showing that when p<q, functions exist in B_{p,q}^s whose restrictions fail to belong to the same space. They now prove that the characterization of partial maps belonging to Besov spaces of generalized smoothness B_{p,q}^{(s,Ψ)} is sharp.

Result: The paper demonstrates that the previously established characterization is sharp, meaning the conditions on the function Ψ (involving summability conditions with p and q) are both necessary and sufficient for the restriction property to hold in generalized Besov spaces.

Conclusion: This work completes the understanding of restriction properties in Besov spaces, showing that the necessary and sufficient conditions for partial maps to belong to generalized smoothness spaces are optimal and cannot be improved.

Abstract: In a previous work, we showed that Besov spaces do not enjoy the restriction
property unless $q\leq p$. Specifically, we proved that if $p<q$, then it is
always possible to construct a function $f\in B_{p,q}^s(\mathbb{R}^N)$ such
that $f(\cdot,y)\notin B_{p,q}^s(\mathbb{R}^d)$ for a.e. $y\in
\mathbb{R}^{N-d}$, while this "pathology" does not happen if $q\leq p$. We
showed that the partial maps belong, in fact, to the Besov space of generalised
smoothness $B_{p,q}^{(s,\Psi)}(\mathbb{R}^d)$ provided the function $\Psi$
satisfies a simple summability condition involving $p$ and $q$. This short note
completes the picture by showing that this characterisation is sharp.

</details>


### [60] [Variable Matrix-Weighted Besov Spaces](https://arxiv.org/abs/2509.07786)
*Dachun Yang,Wen Yuan,Zongze Zeng*

Main category: math.FA

TL;DR: This paper introduces matrix-weighted variable Besov spaces using matrix weights and reducing operators, proves their equivalence, establishes various characterizations (φ-transform, molecular, wavelet, atomic), and applies these to study trace operators and Calderón-Zygmund operators.


<details>
  <summary>Details</summary>
Motivation: To extend the theory of Besov spaces to the matrix-weighted variable exponent setting, building on previous work with matrix A_p(∞) weights, and develop comprehensive characterizations and operator theory for these spaces.

Method: The authors introduce matrix-weighted variable Besov spaces using matrix weights and reducing operators, prove their equivalence, establish φ-transform theorem, develop almost diagonal operator theory, derive molecular characterizations, and obtain wavelet and atomic decompositions.

Result: The paper shows that matrix-weighted variable Besov spaces defined via matrix weights or reducing operators are equivalent. It establishes multiple characterizations (φ-transform, molecular, wavelet, atomic) and applies these to prove results about trace operators and boundedness of Calderón-Zygmund operators.

Conclusion: The developed framework provides a comprehensive theory for matrix-weighted variable Besov spaces with various equivalent characterizations and applications to operator theory, particularly for trace operators and Calderón-Zygmund operators in this setting.

Abstract: In this article, applying matrix ${\mathcal A}_{p(\cdot),\infty}$ weights
introduced in our previous work, we introduce the matrix-weighted variable
Besov space via the matrix weight $W$ or the reducing operators ${\mathbb{A}}$
of order $p(\cdot)$ for $W$, Then we show that, defined either by the matrix
weight $W$ or the reducing operators ${\mathbb{A}}$ of order $p(\cdot)$ for
$W$, the matrix-weighted variable Besov spaces (respectively, the
matrix-weighted variable Besov sequence spaces) are both equal. Next, we
establish the $\varphi$-transform theorem for matrix-weighted variable Besov
spaces and, using this, find that the definition of matrix-weighted variable
Besov spaces is independent of the choice of $\varphi$. After that, for the
further discussion of variable Besov spaces, we establish the theorem of almost
diagonal operators and then, by using this, we establish the molecular
characterization. Then, with applying the molecular characterization, we obtain
the wavelet and atomic characterizations of matrix-weighted variable Besov
spaces. Finally, as an application, we consider some classical operators. By
using the wavelet characterization, we establish the trace operator and obtain
the theorem of trace operators. Moreover, with applying the molecular
characterization, we establish the theorem of Calder\'on--Zygmund operators on
matrix-weighted variable Besov spaces.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [61] [Generalized eigenvalue stabilization for immersed explicit dynamics](https://arxiv.org/abs/2509.07632)
*Tim Bürchner,Lars Radtke,Sascha Eisenträger,Alexander Düster,Ernst Rank,Stefan Kollmannsberger,Philipp Kopp*

Main category: cs.CE

TL;DR: A generalized eigenvalue stabilization (GEVS) strategy is proposed to address the adverse impact of poorly cut elements on critical time step size in explicit time integration for immersed finite element methods.


<details>
  <summary>Details</summary>
Motivation: Explicit time integration for immersed finite element discretizations suffers severely from the influence of poorly cut elements, which negatively affects the critical time step size of the global system.

Method: The method uses spectral basis functions (C^0 continuous Lagrangian interpolation polynomials on GLL points) combined with GLL quadrature, and applies GEVS stabilization to element mass matrices of cut elements. This is combined with the finite cell method to guarantee definiteness of system matrices.

Result: Numerical experiments show that the stabilization strategy achieves optimal convergence rates and recovers critical time step sizes equivalent to boundary-conforming discretizations, even with weakly enforced Dirichlet boundary conditions using Nitsche's method or penalty formulations.

Conclusion: The proposed GEVS approach effectively stabilizes cut elements in immersed finite element methods, enabling optimal performance in explicit time integration comparable to traditional boundary-conforming methods.

Abstract: Explicit time integration for immersed finite element discretizations
severely suffers from the influence of poorly cut elements. In this
contribution, we propose a generalized eigenvalue stabilization (GEVS) strategy
for the element mass matrices of cut elements to cure their adverse impact on
the critical time step size of the global system. We use spectral basis
functions, specifically $C^0$ continuous Lagrangian interpolation polynomials
defined on Gauss-Lobatto-Legendre (GLL) points, which, in combination with its
associated GLL quadrature rule, yield high-order convergent diagonal mass
matrices for uncut elements. Moreover, considering cut elements, we combine the
proposed GEVS approach with the finite cell method (FCM) to guarantee
definiteness of the system matrices. However, the proposed GEVS stabilization
can directly be applied to other immersed boundary finite element methods.
Numerical experiments demonstrate that the stabilization strategy achieves
optimal convergence rates and recovers critical time step sizes of equivalent
boundary-conforming discretizations. This also holds in the presence of weakly
enforced Dirichlet boundary conditions using either Nitsche's method or penalty
formulations.

</details>


<div id='astro-ph.GA'></div>

# astro-ph.GA [[Back]](#toc)

### [62] [Cosmic Rays on Galaxy Scales: Progress and Pitfalls for CR-MHD Dynamical Models](https://arxiv.org/abs/2509.07104)
*Philip F. Hopkins*

Main category: astro-ph.GA

TL;DR: Overview of cosmic ray magnetohydrodynamics modeling across micro, meso, and macro scales, highlighting pitfalls of older assumptions and recent progress in CR-MHD equations and global 3D models.


<details>
  <summary>Details</summary>
Motivation: To connect extragalactic, Galactic, and plasma CR transport modeling communities by providing a pedagogical overview of state-of-the-art CR-MHD modeling and addressing systematic errors from older assumptions.

Method: Pedagogical review compiling extragalactic and local interstellar medium observations, analyzing CR transport models from micro (< au) to macro (~kpc) scales, and presenting phenomenological models for future simulations.

Result: Identification of pitfalls in older CR modeling approaches and demonstration of how recent progress in rigorously-derived CR-MHD equations and global 3D models is rapidly constraining GeV CR transport in the circumgalactic medium.

Conclusion: Critical open questions remain for micro, meso, and macro-scale CR-MHD simulations, requiring continued development of novel models for intermittent scattering and new drivers to advance cosmic ray transport understanding.

Abstract: Recent years have seen many arguments for cosmic rays (CRs) as an important
influence on galactic and circumgalactic (CGM) physics, star and galaxy
formation. We present a pedagogical overview of state-of-the-art modeling of
CR-magnetohydrodynamics (CR-MHD) on macro scales (~kpc), highlighting their
fundamental dependence on the micro (< au) scales of CR gyro orbits and meso
(~pc) scales of CR mean-free-paths, intended to connect the extragalactic,
Galactic, and plasma CR transport modeling communities. We note the pitfalls
and systematic errors that arise from older assumptions in CR modeling,
including: use of a simple Fokker-Planck equation or ad-hoc two-moment
formalisms for transport; assumption of leaky boxes or plane-parallel or
shear-periodic boundaries for comparison to local interstellar medium (LISM)
observations; ignoring detailed LISM constraints on CR spectra (e.g. focusing
only on extragalactic observables or spectrally integrated models); assuming CR
transport is mediated by classical models of advection, streaming from
self-confinement (super-Alfvenic or Alfvenic), or extrinsic turbulence. We
emphasize recent progress addressing these: development of rigorously-derived
CR-MHD equations; use of global, 3D galaxy+halo models for LISM comparisons;
new methods for full-spectrum dynamics; novel models for intermittent
scattering and/or new drivers. We compile extragalactic+LISM observations to
show how ~GeV CR transport is being rapidly constrained in the CGM, and present
phenomenological models which can be used in future simulations. We conclude by
highlighting critical open questions for micro, meso, and macro-scale CR-MHD
simulations.

</details>


<div id='math.MG'></div>

# math.MG [[Back]](#toc)

### [63] [Semialgebric rank-one convex hulls: 2x2 triangular matrices and beyond](https://arxiv.org/abs/2509.07541)
*Chiara Meroni,Bogdan Raita*

Main category: math.MG

TL;DR: The rank-one convex hull of finitely many 2x2 triangular matrices is semialgebraic, defined by linear and quadratic polynomials, with explicit constructions for five-point configurations.


<details>
  <summary>Details</summary>
Motivation: To characterize the rank-one convex hull of finite sets of 2x2 triangular matrices and understand its algebraic structure.

Method: Mathematical proof showing the semialgebraic nature of the convex hull, with explicit constructions for five-point cases and analysis of directional convexity.

Result: The rank-one convex hull is semialgebraic (defined by linear and quadratic polynomials), and evidence suggests this characterization doesn't extend to directional convexity.

Conclusion: Finite sets of 2x2 triangular matrices have rank-one convex hulls with specific algebraic structure, but this property may not generalize to broader convexity concepts.

Abstract: We prove that the rank-one convex hull of finitely many $2\times 2$
triangular matrices is a semialgebraic set, defined by linear and quadratic
polynomials. We present explicit constructions for five-point configurations
and offer evidence suggesting that a similar characterization does not hold in
the more general setting of directional convexity.

</details>
