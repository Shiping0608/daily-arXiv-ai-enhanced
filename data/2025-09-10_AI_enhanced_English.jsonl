{"id": "2509.06971", "pdf": "https://arxiv.org/pdf/2509.06971", "abs": "https://arxiv.org/abs/2509.06971", "authors": ["Mingyuan Yang", "Qian Yu", "Chao Yang"], "title": "PeTTO: Leveraging GPUs to Accelerate Topology Optimization with the Pseudo-Transient Methods", "categories": ["math.NA", "cs.NA"], "comment": null, "summary": "We present a Pseudo-Transient Topology Optimization (PeTTO) approach that can\nleverage graphics processing units (GPUs) to efficiently solve single-material\nand multi-material topology optimization problems. By integrating PeTTO with\nphase field methods, the partial differential equations (PDEs) constrained\noptimization problem in topology optimization is transformed into a set of time\ndependent PDEs, which can be analyzed using the knowledge of transient physics.\nThe sensitivities with respect to the design variable are calculated with the\nautomatic differentiation which help avoid tedious and error-prone manual\nderivations. The overall system of equations is efficiently solved using a\nhybrid of the pseudo-transient method and the accelerated pseudo-transient\nmethod, balancing the convergence rate and numerical stability. A variety of\nnumerical examples are presented to demonstrate the effectiveness and\nefficiency of the proposed PeTTO approach. These examples cover different\nphysics scenarios including mechanical and thermal problems, as well as\nsingle-material and multi-materials cases in both 2D and 3D. The numerical\nresults show a 40- to 50-fold speedup when running the same PeTTO code on a\nsingle GPU compared to desktop CPUs. This work helps bridge the gap between\nhigh-performance computing and topology optimization, potentially enabling\nfaster and better designs for real-world problems.", "AI": {"tldr": "PeTTO is a GPU-accelerated topology optimization method that uses pseudo-transient methods and automatic differentiation for 40-50x speedup over CPUs.", "motivation": "Bridge the gap between high-performance computing and topology optimization to enable faster and better designs for real-world problems by leveraging GPU acceleration.", "method": "Integrates pseudo-transient methods with phase field approaches, transforming PDE-constrained optimization into time-dependent PDEs. Uses automatic differentiation for sensitivity calculations and hybrid pseudo-transient/accelerated methods for efficient equation solving.", "result": "Achieves 40- to 50-fold speedup on single GPU vs desktop CPUs. Successfully demonstrated on mechanical and thermal problems, single-material and multi-material cases in both 2D and 3D.", "conclusion": "PeTTO effectively bridges HPC and topology optimization, enabling significant computational speedups that can facilitate faster and improved designs for practical engineering applications."}}
{"id": "2509.07160", "pdf": "https://arxiv.org/pdf/2509.07160", "abs": "https://arxiv.org/abs/2509.07160", "authors": ["Zhiwei Gao", "George Karniadakis"], "title": "Safe cross-entropy-based importance sampling for rare event simulations", "categories": ["math.NA", "cs.NA", "stat.CO"], "comment": "18 pages, 11 figures", "summary": "The Improved Cross-Entropy (ICE) method is a powerful tool for estimating\nfailure probabilities in reliability analysis. Its core idea is to approximate\nthe optimal importance-sampling density by minimizing the forward\nKullback-Leibler divergence within a chosen parametric family-typically a\nmixture model. However, conventional mixtures are often light-tailed, which\nleads to slow convergence and instability when targeting very small failure\nprobabilities. Moreover, selecting the number of mixture components in advance\ncan be difficult and may undermine stability. To overcome these challenges, we\nadopt a weighted cross-entropy-penalized expectation-maximization (EM)\nalgorithm that automatically prunes redundant components during the iterative\nprocess, making the approach more stable. Furthermore, we introduce a novel\ntwo-component mixture that pairs a light-tailed distribution with a\nheavy-tailed one, enabling more effective exploration of the tail region and\nthus accelerating convergence for extremely small failure probabilities. We\ncall the resulting method Safe-ICE and assess it on a variety of test problems.\nNumerical results show that Safe-ICE not only converges more rapidly and yields\nmore accurate failure-probability estimates than standard ICE, but also\nidentifies the appropriate number of mixture components without manual tuning.", "AI": {"tldr": "Safe-ICE method improves failure probability estimation by using a weighted cross-entropy-penalized EM algorithm that automatically prunes redundant mixture components and introduces a novel two-component mixture with light and heavy-tailed distributions for better tail exploration.", "motivation": "Conventional ICE methods using light-tailed mixture distributions suffer from slow convergence and instability when estimating very small failure probabilities, and require manual selection of mixture components which undermines stability.", "method": "Developed a weighted cross-entropy-penalized EM algorithm that automatically prunes redundant components during iteration, and introduced a novel two-component mixture pairing light-tailed with heavy-tailed distributions for effective tail region exploration.", "result": "Safe-ICE converges more rapidly and yields more accurate failure-probability estimates than standard ICE, while automatically identifying the appropriate number of mixture components without manual tuning.", "conclusion": "The Safe-ICE method successfully addresses the limitations of conventional ICE by providing more stable and efficient failure probability estimation, particularly for extremely small probabilities, through automated component selection and improved tail exploration."}}
{"id": "2509.07179", "pdf": "https://arxiv.org/pdf/2509.07179", "abs": "https://arxiv.org/abs/2509.07179", "authors": ["Jongho Park", "Jinchao Xu"], "title": "Auxiliary space theory for the analysis of iterative methods for semidefinite linear systems", "categories": ["math.NA", "cs.NA", "65F08, 65F10, 65J05, 65N55"], "comment": "24 pages, 0 figures", "summary": "We present an auxiliary space theory that provides a unified framework for\nanalyzing various iterative methods for solving linear systems that may be\nsemidefinite. By interpreting a given iterative method for the original system\nas an equivalent, yet more elementary, iterative method for an auxiliary system\ndefined on a larger space, we derive sharp convergence estimates using\nelementary linear algebra. In particular, we establish identities for the error\npropagation operator and the condition number associated with iterative\nmethods, which generalize and refine existing results. The proposed auxiliary\nspace theory is applicable to the analysis of numerous advanced numerical\nmethods in scientific computing. To illustrate its utility, we present three\nexamples -- subspace correction methods, Hiptmair--Xu preconditioners, and\nauxiliary grid methods -- and demonstrate how the proposed theory yields\nrefined analyses for these cases.", "AI": {"tldr": "Auxiliary space theory provides unified framework for analyzing iterative methods for semidefinite linear systems by mapping them to equivalent elementary methods on larger auxiliary spaces, enabling sharp convergence estimates using basic linear algebra.", "motivation": "To develop a unified theoretical framework for analyzing various iterative methods for solving semidefinite linear systems, addressing the need for sharp convergence estimates and generalization of existing results.", "method": "Interpret iterative methods for original systems as equivalent elementary methods on auxiliary systems defined on larger spaces, derive convergence estimates using elementary linear algebra, establish identities for error propagation operators and condition numbers.", "result": "Developed auxiliary space theory that generalizes and refines existing results, provides sharp convergence estimates, and is applicable to numerous advanced numerical methods in scientific computing.", "conclusion": "The auxiliary space theory offers a powerful unified framework for analyzing iterative methods, demonstrated through successful application to subspace correction methods, Hiptmair-Xu preconditioners, and auxiliary grid methods with refined analyses."}}
{"id": "2509.07249", "pdf": "https://arxiv.org/pdf/2509.07249", "abs": "https://arxiv.org/abs/2509.07249", "authors": ["Nilima Nigam", "Kshitij Patil", "Weiran Sun"], "title": "The spectrum of the Steklov-Helmholtz operator", "categories": ["math.NA", "cs.NA"], "comment": null, "summary": "We present a wavenumber-robust strategy for computing Steklov eigenpairs of\nthe Helmholtz operator $-\\Delta -\\mu^2$. As the wavenumber $\\mu \\rightarrow\n\\mu_D$ from below (where $\\mu_D^2 $ is a Dirichlet- Laplace eigenvalue of\nmultiplicity $\\ell$), the lowest $\\ell$ Steklov-Helmholtz eigenvalues diverge\nto $-\\infty$. Computationally, the Steklov-Helmholtz eigenvalue problem becomes\nseverely ill-conditioned when $\\mu \\approx \\mu_D$.\n  We first reformulate the problem in terms of a suitably-defined\nDirichlet-to-Neumann map. We then use an indirect approach based on a single\nlayer ansatz. The discrete single layer matrix is nearly singular close to\nexceptional wavenumbers, and we use a reduced singular value decomposition to\navoid the consequent ill-conditioning. For smooth domains, convergence of our\neigenvalue solver is spectral. We use this method (called the BIO-MOD approach)\nfor shape optimization of scale-invariant Steklov-Helmholtz problems and prove\nthat the disk maximizes the second eigenvalue under appropriate scaling. For\ncurvilinear polygons, we use polynomially-graded meshes rather than uniform\nmeshes. As a proof of concept, we also implemented BIO-MOD using RCIP\nquadratures (using the ChunkIE implementation). The BIO-MOD approach\nsuccessfully removes ill-conditioning near exceptional wavenumbers, and very\nhigh eigenvalue accuracy (up to 10 digits for polygons, arbitrary precision\naccuracy for smooth domains) is observed.\n  We deploy our approach to computationally study the spectral geometry of the\nSteklov-Helmholtz operator, including some questions about spectral asymptotics\nand spectral optimization.", "AI": {"tldr": "A wavenumber-robust method for computing Steklov eigenpairs of Helmholtz operator using BIO-MOD approach that avoids ill-conditioning near exceptional wavenumbers through single layer ansatz and reduced SVD.", "motivation": "The Steklov-Helmholtz eigenvalue problem becomes severely ill-conditioned when wavenumber approaches Dirichlet-Laplace eigenvalues, causing computational difficulties that need to be addressed.", "method": "Reformulate using Dirichlet-to-Neumann map, employ single layer ansatz with reduced SVD to handle near-singular matrices, use spectral convergence for smooth domains and graded meshes for polygons.", "result": "Successfully removes ill-conditioning near exceptional wavenumbers, achieves high accuracy (10 digits for polygons, arbitrary precision for smooth domains), enables shape optimization proving disk maximizes second eigenvalue.", "conclusion": "BIO-MOD approach provides robust computational framework for Steklov-Helmholtz problems, enabling detailed study of spectral geometry and optimization with high accuracy even near problematic wavenumbers."}}
{"id": "2509.06965", "pdf": "https://arxiv.org/pdf/2509.06965", "abs": "https://arxiv.org/abs/2509.06965", "authors": ["Simone Bn\u00e0", "Raffaele Ponzini", "Mirko Cestari", "Carlo Cavazzoni", "Ciro Cottini", "Andrea Benassi"], "title": "Investigation of particle dynamics and classification mechanism in a spiral jet mill through computational fluid dynamics and discrete element methods", "categories": ["physics.comp-ph", "physics.app-ph", "physics.flu-dyn"], "comment": null, "summary": "Predicting the outcome of jet-milling based on the knowledge of process\nparameters and starting material properties is a task still far from being\naccomplished. Given the technical difficulties in measuring thermodynamics,\nflow properties and particle statistics directly in the mills, modelling and\nsimulations constitute alternative tools to gain insight in the process physics\nand many papers have been recently published on the subject. An ideal\npredictive simulation tool should combine the correct description of\nnon-isothermal, compressible, high Mach number fluid flow, the correct\nparticle-fluid and particle-particle interactions and the correct fracture\nmechanics of particle upon collisions but it is not currently available. In\nthis paper we present our coupled CFD-DEM simulation results; while comparing\nthem with the recent modelling and experimental works we will review the\ncurrent understating of the jet-mill physics and particle classification.\nSubsequently we analyze the missing elements and the bottlenecks currently\nlimiting the simulation technique as well as the possible ways to circumvent\nthem towards a quantitative, predictive simulation of jet-milling.", "AI": {"tldr": "CFD-DEM simulation study of jet-milling process reveals current limitations in predictive modeling and identifies missing elements for quantitative simulation.", "motivation": "Predicting jet-milling outcomes from process parameters and material properties remains challenging due to technical difficulties in direct measurement of thermodynamics, flow properties, and particle statistics within mills.", "method": "Coupled CFD-DEM (Computational Fluid Dynamics - Discrete Element Method) simulations were conducted and compared with recent modeling and experimental works to review current understanding of jet-mill physics.", "result": "The study identifies that current simulation techniques lack the ability to fully describe non-isothermal, compressible, high Mach number fluid flow, proper particle-fluid and particle-particle interactions, and accurate fracture mechanics of particles upon collisions.", "conclusion": "The paper analyzes missing elements and bottlenecks limiting jet-mill simulation techniques, and explores possible ways to overcome these limitations towards developing quantitative, predictive simulation capabilities for jet-milling processes."}}
{"id": "2509.06999", "pdf": "https://arxiv.org/pdf/2509.06999", "abs": "https://arxiv.org/abs/2509.06999", "authors": ["Bao Nguyen", "Yousef Lawrence", "Christopher Wink", "Timothy Mark Johnson", "Niels Vanderloo", "Benjamin Reichelt", "Amber Hennessy", "Daniel Thomas Casey", "Dave Schlossberg", "Nathan Masters", "Jose Milovich", "Ari Le", "Stephen Craxton", "Maria Gatu Johnson", "Johan Frenje"], "title": "Modulated deuteron spectra observed with the Magnetic Recoil neutron Spectrometer at the National Ignition Facility", "categories": ["physics.plasm-ph"], "comment": "Submitted to Journal of Plasma Physics", "summary": "The Magnetic Recoil Spectrometer (MRS) on the National Ignition Facility is\nused to measure the neutron spectrum from deuterium-tritium fueled inertial\nconfinement fusion implosions via n-d elastic scattering and magnetic\ndispersion of recoil deuterons. From the MRS-determined neutron spectrum, the\nyield ($Y_n$), apparent ion temperature ($T_i$) and areal density ($\\rho R$)\nare determined. However, anomalous energy modulations in recoil deuterons have\nbeen observed in several high-yield indirect drive experiments\n($Y_n\\sim10^{16}-10^{18}$). These observations raise concerns about their\npotential impact on the MRS-inferred performance metrics. Analytic calculations\nand particle-in-cell simulations are used to examine the possible beam-plasma\ninstabilities, which indicate the two-stream instability as the driving\nmechanism behind energy modulations. Based on a statistical analysis of\nsynthetic deuteron spectra, the modulations-induced errors are found to be\nwithin the errors of the determined $Y_n$, $T_i$ and $\\rho R$ values and thus\ndo not have a significant impact on the MRS measurement.", "AI": {"tldr": "Analysis of energy modulations in neutron spectra measurements from fusion implosions shows two-stream instability causes modulations but doesn't significantly impact key performance metrics.", "motivation": "Anomalous energy modulations observed in recoil deuterons during high-yield fusion experiments raised concerns about potential impact on measurement accuracy of key performance metrics like yield, ion temperature, and areal density.", "method": "Used analytic calculations and particle-in-cell simulations to examine beam-plasma instabilities, and conducted statistical analysis of synthetic deuteron spectra to assess impact on measurements.", "result": "Identified two-stream instability as the driving mechanism behind energy modulations, but found modulation-induced errors are within the measurement errors of yield, ion temperature, and areal density values.", "conclusion": "The observed energy modulations do not have a significant impact on the Magnetic Recoil Spectrometer measurements of fusion performance metrics."}}
{"id": "2509.07227", "pdf": "https://arxiv.org/pdf/2509.07227", "abs": "https://arxiv.org/abs/2509.07227", "authors": ["Diego Alonso-Or\u00e1n", "Ana Carpio", "Rafael Granero-Belinch\u00f3n"], "title": "Asymptotic models for the evolution of a circular biofilm", "categories": ["math.AP", "math-ph", "math.MP"], "comment": "22 pages", "summary": "We study a class of lubrication-type equations modeling the spread of thin\nporoelastic biofilms at air/agar interfaces. Starting from a biofilm slab\nmodel, we perform a formal multi-scale asymptotic expansion to derive a reduced\nnonlinear evolution equation with time-dependent coefficients. First, we\nestablish a local-in-time existence result as well as a continuation criteria.\nMoreover, under suitable assumptions on the radius of the biofilm, we show the\nexistence of global in time solutions. We conclude with some numerical\nsimulations illustrating the emergence of instabilities and potential\nsingularities in finite time.", "AI": {"tldr": "Analysis of thin poroelastic biofilm spreading using lubrication equations, deriving reduced nonlinear evolution equations with time-dependent coefficients through asymptotic expansion.", "motivation": "To model and understand the spread of thin poroelastic biofilms at air/agar interfaces, which is important for studying microbial growth patterns and potential instabilities.", "method": "Starting from a biofilm slab model, performed formal multi-scale asymptotic expansion to derive reduced nonlinear evolution equations with time-dependent coefficients. Established local-in-time existence and continuation criteria, then proved global existence under certain radius assumptions.", "result": "Successfully derived reduced evolution equations, established mathematical existence results (both local and global under conditions), and numerical simulations revealed emergence of instabilities and potential finite-time singularities.", "conclusion": "The study provides a mathematical framework for analyzing biofilm spreading, demonstrating both analytical existence results and numerical evidence of complex dynamic behaviors including instabilities and singularities."}}
{"id": "2509.07305", "pdf": "https://arxiv.org/pdf/2509.07305", "abs": "https://arxiv.org/abs/2509.07305", "authors": ["Neil Lindquist", "Piotr Luszczek", "Jack Dongarra"], "title": "The Stability of Block Eliminations and Additive Modifications", "categories": ["math.NA", "cs.NA", "15A23, 65F05"], "comment": null, "summary": "The block elimination with additive modifications (BEAM) method was recently\nproposed as a alternative to LU with partial pivoting requiring less\ncommunication. Because of the novelty of BEAM, the existing theoretical\nanalysis is lacking. To that end, we analyze both the numerical stability of\nthe underlying block LU factorization and the effects of additive\nmodifications. For the block LU factorization, we are able to improve the\nprevious results of Demmel et al. from being cubic in the element growth to\nmerely quadratic. Furthermore, we propose an alternative measure of element\ngrowth that is better aligned with block LU; this new measure of growth allows\nour analysis to apply to matrices that cannot be factored with pointwise LU. In\nthe second part, we analyzed the modifications produced by BEAM and the effect\nthey have on the condition number and growth factor. Finally, we show that BEAM\nwill not apply any modifications in some cases that regular block LU can safely\nfactor.", "AI": {"tldr": "Theoretical analysis of BEAM method's numerical stability, improving element growth bounds from cubic to quadratic and proposing better growth measures for block LU factorization.", "motivation": "Existing theoretical analysis for the novel BEAM method is lacking, requiring stability analysis of both block LU factorization and additive modifications effects.", "method": "Analyzed numerical stability of block LU factorization and additive modifications, improved previous element growth bounds, proposed alternative growth measures, and examined condition number impacts.", "result": "Improved element growth analysis from cubic to quadratic bounds, developed better-aligned growth measures for block LU, and identified cases where BEAM avoids modifications while block LU remains stable.", "conclusion": "BEAM method shows improved theoretical stability properties with better element growth bounds and can safely factor matrices without modifications in cases where regular block LU works."}}
{"id": "2509.07264", "pdf": "https://arxiv.org/pdf/2509.07264", "abs": "https://arxiv.org/abs/2509.07264", "authors": ["Zhiyuan She", "Liyao Lyu", "Bryan Ronain Smith", "Huan Lei"], "title": "A unified framework for data-driven construction of stochastic reduced models with state-dependent memory", "categories": ["physics.comp-ph"], "comment": null, "summary": "We present a unified framework for the data-driven construction of stochastic\nreduced models with state-dependent memory for high-dimensional Hamiltonian\nsystems. The method addresses two key challenges: (\\rmnum{1}) accurately\nmodeling heterogeneous non-Markovian effects where the memory function depends\non the coarse-grained (CG) variables beyond the standard homogeneous kernel,\nand (\\rmnum{2}) efficiently exploring the phase space to sample both\nequilibrium and dynamical observables for reduced model construction.\nSpecifically, we employ a consensus-based sampling method to establish a shared\nsampling strategy that enables simultaneous construction of the free energy\nfunction and collection of conditional two-point correlation functions used to\nlearn the state-dependent memory. The reduced dynamics is formulated as an\nextended Markovian system, where a set of auxiliary variables, interpreted as\nnon-Markovian features, is jointly learned to systematically approximate the\nmemory function using only two-point statistics. The constructed model yields a\ngeneralized Langevin-type formulation with an invariant distribution consistent\nwith the full dynamics. We demonstrate the effectiveness of the proposed\nframework on a two-dimensional CG model of an alanine dipeptide molecule.\nNumerical results on the transition dynamics between metastable states show\nthat accurately capturing state-dependent memory is essential for predicting\nnon-equilibrium kinetic properties, whereas the standard generalized Langevin\nmodel with a homogeneous kernel exhibits significant discrepancies.", "AI": {"tldr": "A unified framework for constructing stochastic reduced models with state-dependent memory for high-dimensional Hamiltonian systems, addressing non-Markovian effects and efficient phase space sampling.", "motivation": "To accurately model heterogeneous non-Markovian effects where memory function depends on coarse-grained variables, and efficiently explore phase space for sampling both equilibrium and dynamical observables in reduced model construction.", "method": "Uses consensus-based sampling to establish shared sampling strategy for constructing free energy function and collecting conditional two-point correlation functions. Formulates reduced dynamics as extended Markovian system with auxiliary variables to approximate state-dependent memory using two-point statistics.", "result": "The constructed model yields generalized Langevin-type formulation with invariant distribution consistent with full dynamics. Demonstrated effectiveness on 2D CG model of alanine dipeptide molecule, showing state-dependent memory is essential for predicting non-equilibrium kinetic properties.", "conclusion": "Accurately capturing state-dependent memory is crucial for predicting non-equilibrium kinetic properties, as standard generalized Langevin models with homogeneous kernels exhibit significant discrepancies."}}
{"id": "2509.07024", "pdf": "https://arxiv.org/pdf/2509.07024", "abs": "https://arxiv.org/abs/2509.07024", "authors": ["Yadi Cao", "Futian Zhang", "Wesley Liu", "Tom Neiser", "Orso Meneghini", "Lawson Fuller", "Sterling Smith", "Raffi Nazikian", "Brian Sammuli", "Rose Yu"], "title": "TGLF-SINN: Deep Learning Surrogate Model for Accelerating Turbulent Transport Modeling in Fusion", "categories": ["physics.plasm-ph", "cs.LG"], "comment": null, "summary": "The Trapped Gyro-Landau Fluid (TGLF) model provides fast, accurate\npredictions of turbulent transport in tokamaks, but whole device simulations\nrequiring thousands of evaluations remain computationally expensive. Neural\nnetwork (NN) surrogates offer accelerated inference with fully differentiable\napproximations that enable gradient-based coupling but typically require large\ntraining datasets to capture transport flux variations across plasma\nconditions, creating significant training burden and limiting applicability to\nexpensive gyrokinetic simulations. We propose \\textbf{TGLF-SINN\n(Spectra-Informed Neural Network)} with three key innovations: (1) principled\nfeature engineering that reduces target prediction range, simplifying the\nlearning task; (2) physics-guided regularization of transport spectra to\nimprove generalization under sparse data; and (3) Bayesian Active Learning\n(BAL) to strategically select training samples based on model uncertainty,\nreducing data requirements while maintaining accuracy. Our approach achieves\nsuperior performance with significantly less training data. In offline\nsettings, TGLF-SINN reduces logarithmic root mean squared error (LRMSE) by 12.\n4\\% compared to the current baseline \\base. Using only 25\\% of the complete\ndataset with BAL, we achieve LRMSE only 0.0165 higher than \\base~and 0.0248\nhigher than our offline model (0.0583). In downstream flux matching\napplications, our NN surrogate provides 45x speedup over TGLF while maintaining\ncomparable accuracy, demonstrating potential for training efficient surrogates\nfor higher-fidelity models where data acquisition is costly and sparse.", "AI": {"tldr": "TGLF-SINN is a neural network surrogate that accelerates turbulent transport predictions in tokamaks with reduced data requirements through feature engineering, physics-guided regularization, and Bayesian active learning.", "motivation": "Whole device simulations requiring thousands of TGLF evaluations are computationally expensive, and traditional NN surrogates need large training datasets that are costly to obtain from gyrokinetic simulations.", "method": "Three key innovations: (1) principled feature engineering to reduce prediction range, (2) physics-guided regularization of transport spectra for better generalization, and (3) Bayesian Active Learning to strategically select training samples based on model uncertainty.", "result": "Achieves 12.4% lower LRMSE than baseline, requires only 25% of complete dataset to achieve comparable accuracy (LRMSE only 0.0165 higher than baseline), and provides 45x speedup over TGLF in flux matching applications.", "conclusion": "TGLF-SINN demonstrates potential for training efficient surrogates for higher-fidelity models where data acquisition is costly and sparse, enabling faster whole device simulations while maintaining accuracy."}}
{"id": "2509.07243", "pdf": "https://arxiv.org/pdf/2509.07243", "abs": "https://arxiv.org/abs/2509.07243", "authors": ["Li Li", "Xukai Yan"], "title": "Recent research on $(-1)$-homogeneous solutions of stationary Navier-Stokes equations", "categories": ["math.AP"], "comment": null, "summary": "We make an exposition of recent research on $(-1)$-homogeneous solutions of\nthe three-dimensional incompressible stationary Navier-Stokes equations with\nsingular rays. We also discuss properties of such solutions that are\naxisymmetric with no swirl, and present graphs illustrating examples that\nexhibit various typical types of singular behavior.", "AI": {"tldr": "Analysis of (-1)-homogeneous solutions to 3D stationary Navier-Stokes equations with singular rays, focusing on axisymmetric no-swirl cases and their singular behavior patterns.", "motivation": "To systematically study and characterize the properties of (-1)-homogeneous solutions in three-dimensional stationary Navier-Stokes equations, particularly those exhibiting singular ray behavior, which is important for understanding fundamental mathematical properties of fluid dynamics equations.", "method": "Exposition and analysis of recent research findings, with specific focus on axisymmetric solutions without swirl. Uses mathematical exposition and graphical visualization to illustrate different types of singular behavior patterns.", "result": "Presents various typical types of singular behavior exhibited by these solutions, supported by graphical examples that demonstrate the different patterns of singularity formation in the flow field.", "conclusion": "The paper provides a comprehensive overview of (-1)-homogeneous solutions with singular rays, highlighting their mathematical properties and demonstrating through visualization the diverse singular behaviors that can occur in axisymmetric no-swirl configurations of the stationary Navier-Stokes equations."}}
{"id": "2509.07509", "pdf": "https://arxiv.org/pdf/2509.07509", "abs": "https://arxiv.org/abs/2509.07509", "authors": ["Zexin Pan", "Du Ouyang", "Zhijian He"], "title": "Quasi-Monte Carlo integration over $\\mathbb{R}^s$ with boundary-damping importance sampling", "categories": ["math.NA", "cs.NA", "41A63, 65D30, 97N40"], "comment": null, "summary": "This paper proposes a new importance sampling (IS) that is tailored to\nquasi-Monte Carlo (QMC) integration over $\\mathbb{R}^s$. IS introduces a\nmultiplicative adjustment to the integrand by compensating the sampling from\nthe proposal instead of the target distribution. Improper proposals result in\nsevere adjustment factor for QMC. Our strategy is to first design a adjustment\nfactor to meet desired regularities and then determine a tractable transport\nmap from the standard uniforms to the proposal for using QMC quadrature points\nas inputs. The transport map has the effect of damping the boundary growth of\nthe resulting integrand so that the effectiveness of QMC can be reclaimed.\nUnder certain conditions on the original integrand, our proposed IS enjoys a\nfast convergence rate independently of the dimension $s$, making it amenable to\nhigh-dimensional problems.", "AI": {"tldr": "New importance sampling method for quasi-Monte Carlo integration that designs adjustment factors and transport maps to handle improper proposals and maintain fast convergence in high dimensions.", "motivation": "Traditional importance sampling with improper proposals creates severe adjustment factors that degrade QMC performance, especially in high-dimensional integration problems.", "method": "Design adjustment factors with desired regularity properties first, then determine tractable transport maps from standard uniforms to proposals to damp boundary growth and enable effective QMC integration.", "result": "The proposed importance sampling method achieves fast convergence rates independent of dimension s, making it suitable for high-dimensional problems.", "conclusion": "This tailored importance sampling approach successfully reclaims QMC effectiveness by strategically designing adjustment factors and transport maps to handle improper proposals while maintaining dimensional independence."}}
{"id": "2412.15340", "pdf": "https://arxiv.org/pdf/2412.15340", "abs": "https://arxiv.org/abs/2412.15340", "authors": ["Shivani Aggarwal", "Saumya Singh", "Dinkar Mishra", "Bhupesh Kumar", "Pallavi Jha"], "title": "Second harmonic generation by radially polarized laser beam propagating in homogeneous plasma", "categories": ["physics.plasm-ph", "physics.comp-ph", "physics.flu-dyn"], "comment": null, "summary": "This study presents an investigation of second harmonic generation (SHG)\nresulting from interaction of radially polarized laser beam propagating in\nhomogeneous, unmagnetized plasma. Lorentz force and continuity equations have\nbeen used to derive the radial and axial current density components. Further,\nusing these densities in the wave equation leads to analytical expressions for\nthe SHG field amplitudes. These amplitudes exhibit periodic oscillations along\nthe propagation axis, characterized by detuning length dependent on plasma\ndensity and laser parameters. Radial and axial contributions to SHG are found\nto be highly enhanced near the beam axis due to the Gaussian beam profile of\nthe laser. The analytical findings are validated using Fourier Bessel\nParticle-In-Cell (FBPIC) simulations. Notably, unlike linearly or circularly\npolarized beams which require either inhomogeneous or magnetized plasma,\nradially polarized beams facilitate efficient harmonic generation in\nhomogeneous unmagnetized plasmas.", "AI": {"tldr": "Radially polarized laser beams enable efficient second harmonic generation in homogeneous unmagnetized plasma through periodic oscillations along propagation axis, with enhanced SHG near beam axis.", "motivation": "To investigate second harmonic generation from radially polarized laser beams in homogeneous unmagnetized plasma, unlike linearly/circularly polarized beams which require inhomogeneous or magnetized plasma.", "method": "Used Lorentz force and continuity equations to derive radial/axial current densities, then wave equation for analytical SHG field amplitudes. Validated with Fourier Bessel Particle-In-Cell (FBPIC) simulations.", "result": "SHG amplitudes show periodic oscillations with detuning length dependent on plasma density and laser parameters. Radial/axial contributions are highly enhanced near beam axis due to Gaussian profile.", "conclusion": "Radially polarized beams facilitate efficient harmonic generation in homogeneous unmagnetized plasmas without requiring plasma inhomogeneity or magnetization."}}
{"id": "2509.07880", "pdf": "https://arxiv.org/pdf/2509.07880", "abs": "https://arxiv.org/abs/2509.07880", "authors": ["J. T. Y. Chu", "J. W. D. Halliday", "C. Heaton", "K. Moczulski", "A. Blazevic", "D. Schumacher", "M. Metternich", "H. Nazary", "C. D. Arrowsmith", "A. R. Bell", "K. A. Beyer", "A. F. A. Bott", "T. Campbell", "E. Hansen", "D. Q. Lamb", "F. Miniati", "P. Neumayer", "C. A. J. Palmer", "B. Reville", "A. Reyes", "S. Sarkar", "A. Scopatz", "C. Spindloe", "C. B. Stuart", "H. Wen", "P. Tzeferacos", "R. Bingham", "G. Gregori"], "title": "Measurement of ion acceleration and diffusion in a laser-driven magnetized plasma", "categories": ["physics.plasm-ph"], "comment": null, "summary": "Here we present results from an experiment performed at the GSI Helmholtz\nCentre for Heavy Ion Research. A mono-energetic beam of chromium ions with\ninitial energies of $\\sim 450$ MeV was fired through a magnetized interaction\nregion formed by the collision of two counter-propagating laser-ablated plasma\njets. While laser interferometry revealed the absence of strong fluid-scale\nturbulence, acceleration and diffusion of the beam ions was driven by\nwave-particle interactions. A possible mechanism is particle acceleration by\nelectrostatic, short scale length kinetic turbulence, such as the lower-hybrid\ndrift instability.", "AI": {"tldr": "Chromium ion beam experiment shows wave-particle interactions drive acceleration and diffusion despite absence of strong fluid-scale turbulence, suggesting lower-hybrid drift instability as mechanism.", "motivation": "To study ion beam behavior and acceleration mechanisms when fired through magnetized plasma jets, particularly investigating wave-particle interactions in controlled laboratory conditions.", "method": "Used mono-energetic chromium ion beam (~450 MeV) fired through magnetized interaction region created by two counter-propagating laser-ablated plasma jets, with laser interferometry for turbulence analysis.", "result": "Observed acceleration and diffusion of beam ions driven by wave-particle interactions, despite absence of strong fluid-scale turbulence. Suggests electrostatic, short-scale kinetic turbulence as driving mechanism.", "conclusion": "Lower-hybrid drift instability identified as possible mechanism for particle acceleration through electrostatic kinetic turbulence in magnetized plasma environments."}}
{"id": "2509.07371", "pdf": "https://arxiv.org/pdf/2509.07371", "abs": "https://arxiv.org/abs/2509.07371", "authors": ["Huimin Liu", "Yurui Lu", "Xueke Pu"], "title": "The bidirectional NLS approximation for the one-dimensional Euler-Poisson system", "categories": ["math.AP"], "comment": "56pages", "summary": "The nonlinear Schr\\\"{o}dinger (NLS) equation is known as a universal equation\ndescribing the evolution of the envelopes of slowly modulated spatially and\ntemporarily oscillating wave packet in various dispersive systems. In this\npaper, we prove that under a certain multiple scale transformation, solutions\nto the Euler-Poisson system can be approximated by the sums of two\ncounter-propagating waves solving the NLS equations. It extends the earlier\nresults [Liu and Pu, Comm. Math. Phys., 371(2), (2019)357-398], which justify\nthe unidirectional NLS approximation to the Euler-Poisson system for the\nion-coustic wave. We demonstrate that the solutions could be convergent to two\ncounter-propagating wave packets, where each wave packet involves independently\nas a solution of the NLS equation. We rigorously prove the validity of the NLS\napproximation for the one-dimensional Euler-Poisson system by obtaining uniform\nerror estimates in Sobolev spaces. The NLS dynamics can be observed at a\nphysically relevant timespan of order $\\mathcal{O}(\\epsilon^{-2})$. As far as\nwe know, this result is the first construction and valid proof of the\nbidirectional NLS approximation.", "AI": {"tldr": "This paper proves that solutions to the Euler-Poisson system can be approximated by two counter-propagating waves solving NLS equations under a multiple scale transformation, extending previous unidirectional results.", "motivation": "To extend the earlier unidirectional NLS approximation results for ion-acoustic waves to bidirectional wave propagation, providing a more comprehensive mathematical framework.", "method": "Using a multiple scale transformation and rigorous mathematical analysis to obtain uniform error estimates in Sobolev spaces for the one-dimensional Euler-Poisson system.", "result": "Successfully demonstrated that solutions converge to two counter-propagating wave packets, each evolving independently as an NLS solution, with validity proven for physically relevant timescales of order O(\u03b5\u207b\u00b2).", "conclusion": "This work provides the first rigorous construction and valid proof of bidirectional NLS approximation, significantly advancing the mathematical understanding of wave packet dynamics in dispersive systems."}}
{"id": "2509.07636", "pdf": "https://arxiv.org/pdf/2509.07636", "abs": "https://arxiv.org/abs/2509.07636", "authors": ["Zhiyuan Sun", "Jun Liu", "Pei Wang"], "title": "High-order staggered Lagrangian hydrodynamics (II) : the artificial viscosity and hourglass control algorithm", "categories": ["math.NA", "cs.NA", "49N45, 65N21"], "comment": null, "summary": "In this article, we investigate the artificial viscosity and hourglass\ncontrol algorithms for high-order staggered Lagrangian hydrodynamics(SGH), as\nproposed in~\\cite[Sun et al., 2025]{Sun2025High}. Inspired by the subzonal\npressure method in classical staggered Lagrangian hydrodynamics, we extend the\nstiffness-based hourglass control algorithm to the high-order setting,\nenriching the pressure field from the $Q^{m-1}$ to the $Q^{m}$ polynomial\nspace. A unified framework for this hourglass control approach is established,\nfrom which the classical subzonal pressure method naturally emerges as the\nspecial case of the $Q^1-P^0$ space. The artificial viscosity follows the\nformulation in~\\cite[Dobrev et al., 2012]{Dobrev2012High}. We show that the\nviscosity admits a concise form, with intermediate variables explicitly\ncomputable, leading to improved computational efficiency and easier\nimplementation. Moreover, the tensor viscosity in classical staggered\nLagrangian hydrodynamics can be derived in a similarly compact and explicit\nform. Numerical experiments on two-dimensional problems are presented to\ndemonstrate the accuracy and efficiency of the proposed algorithms.", "AI": {"tldr": "Extension of stiffness-based hourglass control and artificial viscosity algorithms for high-order staggered Lagrangian hydrodynamics, improving computational efficiency and implementation simplicity.", "motivation": "To develop more efficient and easily implementable artificial viscosity and hourglass control methods for high-order staggered Lagrangian hydrodynamics, building on classical approaches.", "method": "Extended stiffness-based hourglass control to high-order setting by enriching pressure field from Q^{m-1} to Q^m polynomial space, using unified framework. Applied artificial viscosity formulation from Dobrev et al. (2012) with explicit intermediate variable computation.", "result": "Achieved concise viscosity formulation with explicitly computable intermediate variables, derived compact tensor viscosity form, and demonstrated accuracy and efficiency through 2D numerical experiments.", "conclusion": "Proposed algorithms provide improved computational efficiency and easier implementation for high-order staggered Lagrangian hydrodynamics while maintaining accuracy, with classical methods emerging as special cases."}}
{"id": "2412.17709", "pdf": "https://arxiv.org/pdf/2412.17709", "abs": "https://arxiv.org/abs/2412.17709", "authors": ["Shivani Aggarwal", "Saumya Singh", "Dinkar Mishra", "Bhupesh Kumar", "Pallavi Jha"], "title": "Wakefield generation and electron acceleration via propagation of radially polarized laser pulses in homogeneous plasma", "categories": ["physics.plasm-ph", "physics.comp-ph", "physics.flu-dyn"], "comment": null, "summary": "The paper presents a study of wakefield generation and electron injection via\npropagation of radially polarized laser pulses in homogeneous pre-ionized\nplasma. The analytical study is based on Lorentz force and continuity\nequations. Perturbation technique and quasi-static approximation are used for\nevaluating the generated longitudinal wakefields. Trapping and acceleration of\nelectrons are examined by injecting a test electron in the generated\nwakefields. The results are compared with those obtained via linearly polarized\nlaser pulses. The validation of analytical results is performed using the\nFourier-Bessel particle-in-cell (FBPIC) simulation code. It is seen that there\nis a significant enhancement in amplitude of the longitudinal wakefield\ngenerated and electron energy gain via radially polarized laser pulses as\ncompared to linearly polarized laser pulse case.", "AI": {"tldr": "Study shows radially polarized laser pulses generate stronger wakefields and higher electron energy gains compared to linearly polarized pulses in plasma.", "motivation": "To investigate and compare wakefield generation and electron acceleration using radially polarized vs linearly polarized laser pulses in pre-ionized plasma.", "method": "Analytical study using Lorentz force and continuity equations with perturbation technique and quasi-static approximation, validated with FBPIC simulation code. Test electron injection used to examine trapping and acceleration.", "result": "Significant enhancement in longitudinal wakefield amplitude and electron energy gain with radially polarized laser pulses compared to linearly polarized pulses.", "conclusion": "Radially polarized laser pulses are more effective for wakefield generation and electron acceleration in plasma than conventional linearly polarized pulses."}}
{"id": "2509.07927", "pdf": "https://arxiv.org/pdf/2509.07927", "abs": "https://arxiv.org/abs/2509.07927", "authors": ["Abiam Tamburrini", "Sergio Davis", "Pablo S. Moya"], "title": "Unifying Kappa Distribution Models for Non-Equilibrium Space Plasmas: A Superstatistical Approach Based on Moments", "categories": ["physics.plasm-ph"], "comment": null, "summary": "From the perspective of non-equilibrium statistical mechanics, modeling the\nvelocity distribution of particles in non-equilibrium, steady-state plasmas\npresents a significant challenge. Under this context, a family of kappa\ndistributions has been widely used to capture the high-energy tails in space\nplasmas. These distributions deviate from the canonical Maxwell-Boltzmann\nstatistics and vary significantly in their interpretation of the temperature of\nan out-of-equilibrium system. In this letter, we establish the validity of any\nkappa distribution from the standpoint of superstatistics. This study unifies\nthese models by introducing a new kappa distribution based on superstatistical\nparameters, providing a more general and fundamental framework to connect these\ndistributions and the superstatistical temperature of a system. We demonstrate\nthat the general distribution depends on the thermal characteristics of the\nmodeled temperature distribution population. Furthermore, we present a\nmoment-based velocity distribution that bypasses the traditional temperature\ndebate, relying on the velocity moments. Our findings enhance the understanding\nof kappa distributions and offer a robust model for non-equilibrium space\nplasmas.", "AI": {"tldr": "This paper establishes the validity of kappa distributions for non-equilibrium plasmas using superstatistics, introduces a new unified kappa distribution framework, and presents a moment-based velocity distribution that avoids temperature debates.", "motivation": "Modeling velocity distributions in non-equilibrium steady-state plasmas is challenging, and existing kappa distributions have varying interpretations of temperature in out-of-equilibrium systems, requiring a unified fundamental framework.", "method": "The study uses superstatistics to validate kappa distributions and introduces a new kappa distribution based on superstatistical parameters. It also develops a moment-based velocity distribution that relies on velocity moments rather than temperature.", "result": "The research demonstrates that the general distribution depends on the thermal characteristics of the temperature distribution population and provides a unified framework connecting various kappa distributions with superstatistical temperature.", "conclusion": "The findings enhance understanding of kappa distributions and offer a robust model for non-equilibrium space plasmas, providing a more fundamental framework that connects different distributions and bypasses traditional temperature debates."}}
{"id": "2509.07380", "pdf": "https://arxiv.org/pdf/2509.07380", "abs": "https://arxiv.org/abs/2509.07380", "authors": ["Keith Promislow", "Truong Vu", "Brian Wetton"], "title": "Gradient Flows of Interfacial Energies: Curvature Agents and Incompressibility", "categories": ["math.AP", "35A15, 35B25"], "comment": "28 pages, 5 figures", "summary": "We present a framework for the gradient flow of sharp-interface surface\nenergies that couple to embedded curvature active agents. We use a penalty\nmethod to develop families of locally incompressible gradient flows that couple\ninterface stretching or compression to local flux of interfacial mass. We\nestablish the convergence of the penalty method to an incompressible flow both\nformally for a broad family of surface energies and rigorously for a more\nnarrow class of surface energies. We present an analysis, including a\n$\\Gamma$-limit, of an Allen-Cahn type model for a coupled surface agent\ncurvature energy.", "AI": {"tldr": "A framework for gradient flow of sharp-interface surface energies coupled to curvature active agents using penalty methods for incompressible flows.", "motivation": "To develop methods for modeling surface energies that couple interface stretching/compression with local mass flux through curvature active agents.", "method": "Penalty method to create locally incompressible gradient flows, with formal and rigorous convergence analysis including \u0393-limit of Allen-Cahn type models.", "result": "Established convergence of penalty method to incompressible flow for broad family of surface energies, with rigorous results for narrower class.", "conclusion": "The framework successfully couples interface deformation with mass transport through curvature agents, with proven convergence properties."}}
{"id": "2509.07687", "pdf": "https://arxiv.org/pdf/2509.07687", "abs": "https://arxiv.org/abs/2509.07687", "authors": ["Sebastian Schaffer", "Lukas Exl"], "title": "Physics-informed low-rank neural operators with application to parametric elliptic PDEs", "categories": ["math.NA", "cs.NA", "physics.comp-ph", "stat.ML", "68T07, 65N80, 65F55, 35J05, 76S05"], "comment": "18 pages, 10 figures", "summary": "We present the Physics-Informed Low-Rank Neural Operator (PILNO), a neural\noperator framework for efficiently approximating solution operators of partial\ndifferential equations (PDEs) on point cloud data. PILNO combines low-rank\nkernel approximations with an encoder--decoder architecture, enabling fast,\ncontinuous one-shot predictions while remaining independent of specific\ndiscretizations. The model is trained using a physics-informed penalty\nframework, ensuring that PDE constraints and boundary conditions are satisfied\nin both supervised and unsupervised settings. We demonstrate its effectiveness\non diverse problems, including function fitting, the Poisson equation, the\nscreened Poisson equation with variable coefficients, and parameterized Darcy\nflow. The low-rank structure provides computational efficiency in\nhigh-dimensional parameter spaces, establishing PILNO as a scalable and\nflexible surrogate modeling tool for PDEs.", "AI": {"tldr": "PILNO is a neural operator framework that combines low-rank kernel approximations with encoder-decoder architecture for efficient PDE solution approximation on point cloud data, using physics-informed training for both supervised and unsupervised settings.", "motivation": "To develop an efficient and scalable neural operator framework for approximating PDE solution operators that works on point cloud data, is independent of specific discretizations, and can handle both supervised and unsupervised learning scenarios.", "method": "Combines low-rank kernel approximations with encoder-decoder architecture, trained using physics-informed penalty framework to ensure PDE constraints and boundary conditions are satisfied.", "result": "Demonstrated effectiveness on diverse problems including function fitting, Poisson equation, screened Poisson equation with variable coefficients, and parameterized Darcy flow. The low-rank structure provides computational efficiency in high-dimensional parameter spaces.", "conclusion": "PILNO establishes itself as a scalable and flexible surrogate modeling tool for PDEs, enabling fast, continuous one-shot predictions while remaining discretization-independent."}}
{"id": "2509.06963", "pdf": "https://arxiv.org/pdf/2509.06963", "abs": "https://arxiv.org/abs/2509.06963", "authors": ["Jeffrey D. Picka"], "title": "Random Trajectory Models for Complex Phenomena", "categories": ["cond-mat.soft", "physics.comp-ph"], "comment": "82 pages, 1 figure", "summary": "Many models for complex phenomena use a model for strongly-interacting\nelements on a small scale to generate larger-scale simulations of some aspects\nof experimental realizations. These models may be agent-based (as in the case\nof discrete element method models for granular flow) or based on\npattern-forming systems of PDEs (as in models for Raleigh-B\\`enard convection\npatterns). Often these models are purely deterministic, producing a single\nsimulation for each set of initial conditions. If observed realizations\ndemonstrate between-realization variability for important aspects of the\nphenomenon, those aspects can be simulated by adding probabilistic components\nto the deterministic models to create random trajectory (RT) models.\n  The RT model framework provides probabilistic models which can be fit to data\nand validated, together with a clear perspective on how difficult it can be to\nestablish any kind of validity for a fitted model. It treats models as code\nwith adjustable coefficients, rather than as systems of differential equations.\nIt provides a simply stated necessary condition for these code models to be\neasily fit and verified, as well as an argument that this condition can almost\nnever be checked. When the necessary condition cannot be checked, the RT model\nframework identifies the code models as black box models which may have the\ncapacity for emulating the joint distributions of small collections of\nstatistics observed on realizations, but which can only provide very weak\nevidence for any form of explanation for the emergence of any aspect of the\nphenomenon. The framework also provides a way to clearly understand why finding\na useful code model and scientifically validating it may require many\nperson-years of extra experimentation and statistical analysis undertaken after\nthe first output-producing code model is constructed and contributed.", "AI": {"tldr": "The paper introduces a random trajectory (RT) model framework for adding probabilistic components to deterministic simulations to account for between-realization variability in complex phenomena.", "motivation": "Many deterministic models for complex phenomena fail to capture between-realization variability observed in experimental realizations, necessitating probabilistic extensions.", "method": "Proposes RT model framework that treats models as code with adjustable coefficients rather than differential equations, adding probabilistic components to deterministic base models.", "result": "Provides a necessary condition for easy model fitting and verification, but argues this condition is rarely checkable, making most code models effectively black boxes with limited explanatory power.", "conclusion": "Validating useful code models requires extensive additional experimentation and statistical analysis beyond initial model construction, highlighting the challenges in establishing scientific validity for such models."}}
{"id": "2509.07104", "pdf": "https://arxiv.org/pdf/2509.07104", "abs": "https://arxiv.org/abs/2509.07104", "authors": ["Philip F. Hopkins"], "title": "Cosmic Rays on Galaxy Scales: Progress and Pitfalls for CR-MHD Dynamical Models", "categories": ["astro-ph.GA", "astro-ph.CO", "astro-ph.HE", "physics.plasm-ph", "physics.space-ph"], "comment": "14 figures+7 tables, intended for pedagogical presentation. 30 pages.\n  Submitted to the Open Journal of Astrophysics. Comments welcome", "summary": "Recent years have seen many arguments for cosmic rays (CRs) as an important\ninfluence on galactic and circumgalactic (CGM) physics, star and galaxy\nformation. We present a pedagogical overview of state-of-the-art modeling of\nCR-magnetohydrodynamics (CR-MHD) on macro scales (~kpc), highlighting their\nfundamental dependence on the micro (< au) scales of CR gyro orbits and meso\n(~pc) scales of CR mean-free-paths, intended to connect the extragalactic,\nGalactic, and plasma CR transport modeling communities. We note the pitfalls\nand systematic errors that arise from older assumptions in CR modeling,\nincluding: use of a simple Fokker-Planck equation or ad-hoc two-moment\nformalisms for transport; assumption of leaky boxes or plane-parallel or\nshear-periodic boundaries for comparison to local interstellar medium (LISM)\nobservations; ignoring detailed LISM constraints on CR spectra (e.g. focusing\nonly on extragalactic observables or spectrally integrated models); assuming CR\ntransport is mediated by classical models of advection, streaming from\nself-confinement (super-Alfvenic or Alfvenic), or extrinsic turbulence. We\nemphasize recent progress addressing these: development of rigorously-derived\nCR-MHD equations; use of global, 3D galaxy+halo models for LISM comparisons;\nnew methods for full-spectrum dynamics; novel models for intermittent\nscattering and/or new drivers. We compile extragalactic+LISM observations to\nshow how ~GeV CR transport is being rapidly constrained in the CGM, and present\nphenomenological models which can be used in future simulations. We conclude by\nhighlighting critical open questions for micro, meso, and macro-scale CR-MHD\nsimulations.", "AI": {"tldr": "Overview of cosmic ray magnetohydrodynamics modeling across micro, meso, and macro scales, highlighting pitfalls of older assumptions and recent progress in CR-MHD equations and global 3D models.", "motivation": "To connect extragalactic, Galactic, and plasma CR transport modeling communities by providing a pedagogical overview of state-of-the-art CR-MHD modeling and addressing systematic errors from older assumptions.", "method": "Pedagogical review compiling extragalactic and local interstellar medium observations, analyzing CR transport models from micro (< au) to macro (~kpc) scales, and presenting phenomenological models for future simulations.", "result": "Identification of pitfalls in older CR modeling approaches and demonstration of how recent progress in rigorously-derived CR-MHD equations and global 3D models is rapidly constraining GeV CR transport in the circumgalactic medium.", "conclusion": "Critical open questions remain for micro, meso, and macro-scale CR-MHD simulations, requiring continued development of novel models for intermittent scattering and new drivers to advance cosmic ray transport understanding."}}
{"id": "2509.07382", "pdf": "https://arxiv.org/pdf/2509.07382", "abs": "https://arxiv.org/abs/2509.07382", "authors": ["Yi C. Huang", "Xinhang Tong"], "title": "On the exponential convergence to equilibrium for ultrafast diffusion equations", "categories": ["math.AP"], "comment": "5pages", "summary": "We propose a simple proof of the exponential convergence to equilibrium for\nultrafast diffusion equations in $\\mathbb{R}^n$. Our approach, based on the\ndirect use of Poincar\\'e inequality, gets rid of the optimal transport\narguments used in \\cite{fathi2025} which are valid for Gaussian-excluded\none-dimensional weights. This simplification allows us to extend their results\nto Gaussian measures in higher dimensions.", "AI": {"tldr": "Simple proof of exponential convergence for ultrafast diffusion equations using Poincar\u00e9 inequality instead of optimal transport methods", "motivation": "To simplify the proof of exponential convergence to equilibrium for ultrafast diffusion equations and extend results to higher dimensions", "method": "Direct use of Poincar\u00e9 inequality approach, eliminating the need for optimal transport arguments used in previous work", "result": "Successfully extends convergence results to Gaussian measures in higher dimensions beyond the one-dimensional case", "conclusion": "The simplified Poincar\u00e9 inequality approach provides a more general framework for proving exponential convergence in ultrafast diffusion equations across multiple dimensions"}}
{"id": "2509.07689", "pdf": "https://arxiv.org/pdf/2509.07689", "abs": "https://arxiv.org/abs/2509.07689", "authors": ["Paul Moujaes", "Dmitri Kuzmin", "Christian B\u00e4umer"], "title": "Realizability-preserving monolithic convex limiting in continuous Galerkin discretizations of the M1 model of radiative transfer", "categories": ["math.NA", "cs.NA"], "comment": "PDF is 28 pages and contains figures consisting of 15 jpg-files and\n  one pdf file", "summary": "We discretize the $M_1$ model of radiative transfer using continuous finite\nelements and propose a tailor-made monolithic convex limiting (MCL) procedure\nfor enforcing physical realizability. The $M_1$ system of nonlinear balance\nlaws for the zeroth and first moments of a probability distribution function is\nderived from the linear Boltzmann equation and equipped with an entropy-based\nclosure for the second moment. To ensure hyperbolicity and physical\nadmissibility, evolving moments must stay in an invariant domain representing a\nconvex set of realizable states. We first construct a low-order method that is\nprovably invariant domain preserving (IDP). Introducing intermediate states\nthat represent spatially averaged exact solutions of homogeneous Riemann\nproblems, we prove that these so-called bar states are realizable in any number\nof space dimensions. This key auxiliary result enables us to show the IDP\nproperty of a fully discrete scheme with a diagonally implicit treatment of\nreactive terms. To achieve high resolution, we add nonlinear correction terms\nthat are constrained using a two-step MCL algorithm. In the first limiting\nstep, local bounds are imposed on each conserved variable to avoid spurious\noscillations and maintain positivity of the scalar-valued zeroth moment\n(particle density). The second limiting step constrains the magnitude of the\nvector-valued first moment to be realizable. The flux-corrected finite element\nscheme is provably IDP. Its ability to prevent nonphysical behavior while\nattaining high-order accuracy in smooth regions is verified in a series of\nnumerical tests. The developed methodology provides a robust simulation tool\nfor dose calculation in radiotherapy.", "AI": {"tldr": "A finite element discretization of the M1 radiative transfer model with monolithic convex limiting to ensure physical realizability and invariant domain preservation.", "motivation": "To develop a robust numerical method for radiative transfer that maintains physical admissibility (hyperbolicity and realizability) while achieving high-order accuracy, particularly for radiotherapy dose calculation applications.", "method": "Continuous finite elements with a two-step monolithic convex limiting (MCL) procedure: first step enforces positivity of particle density and local bounds, second step constrains the first moment magnitude to ensure realizability. Uses bar states from Riemann problems and diagonally implicit treatment of reactive terms.", "result": "The scheme is provably invariant domain preserving (IDP) and maintains high-order accuracy in smooth regions while preventing nonphysical behavior. Verified through numerical tests.", "conclusion": "The developed methodology provides a robust simulation tool for dose calculation in radiotherapy that ensures physical realizability while maintaining numerical accuracy."}}
{"id": "2509.07197", "pdf": "https://arxiv.org/pdf/2509.07197", "abs": "https://arxiv.org/abs/2509.07197", "authors": ["Yang Zhang", "Peter Hatton", "Blas P. Uberuaga", "Jason R. Trelewicz"], "title": "Grain Boundary Anisotropy and Its Influence on Helium Bubble Nucleation, Growth, and Decohesion in Polycrystalline Iron", "categories": ["cond-mat.mtrl-sci", "physics.comp-ph"], "comment": null, "summary": "The accumulation of helium bubbles at grain boundaries (GBs) critically\ndegrades the mechanical integrity of structural materials in nuclear reactors.\nWhile GBs act as sinks for radiation-induced defects, their inherent structural\nanisotropy leads to complex helium bubble evolution behaviors that remain\npoorly understood. This work integrates accelerated molecular dynamics\nsimulations and a novel atomic-scale metric, the flexibility volume (V_f), to\nestablish the interplay between GB character, helium segregation, and bubble\ngrowth mechanisms in body-centered cubic iron. We demonstrate that V_f, which\nincorporates both local atomic volume and vibrational properties, qualitatively\npredicts deformation propensity. Our results reveal that the atomic-scale\nsegregation energy landscape dictates initial helium clustering and subsequent\nbubble morphology, with low-energy channels in tilt {\\Sigma}5 boundary\nfacilitating one-dimensional migration while isolated deep traps in twist\n{\\Sigma}13 boundary promote larger, rounder bubble morphology. Critically,\nbesides gradual bubble growth via trap mutation mechanism, we identify two\ndistinct stress-relief mechanisms: loop punching in anisotropic tilt {\\Sigma}5\nboundary and interfacial decohesion in twist {\\Sigma}11 boundary, with the\ndominant pathway determined by the interplay between bubble morphology and\nlocal mechanical softness. This study establishes a fundamental connection\nbetween GB crystallographic and energetical anisotropy and helium bubble\nevolution, providing critical insights for designing radiation-tolerant\nmicrostructures.", "AI": {"tldr": "Study shows grain boundary character controls helium bubble evolution in nuclear materials through atomic-scale segregation energy landscapes and distinct stress-relief mechanisms.", "motivation": "Helium bubble accumulation at grain boundaries degrades mechanical properties in nuclear reactor materials, but the complex evolution behaviors due to GB structural anisotropy remain poorly understood.", "method": "Integrated accelerated molecular dynamics simulations with a novel atomic-scale metric called flexibility volume (V_f) that combines local atomic volume and vibrational properties to study helium segregation and bubble growth in body-centered cubic iron.", "result": "V_f predicts deformation propensity; segregation energy landscape dictates initial helium clustering and bubble morphology. Low-energy channels in tilt \u03a35 boundary enable 1D migration, while deep traps in twist \u03a313 boundary promote larger, rounder bubbles. Two stress-relief mechanisms identified: loop punching in anisotropic tilt \u03a35 boundary and interfacial decohesion in twist \u03a311 boundary.", "conclusion": "Establishes fundamental connection between GB crystallographic/energetical anisotropy and helium bubble evolution, providing critical insights for designing radiation-tolerant microstructures in nuclear materials."}}
{"id": "2509.07116", "pdf": "https://arxiv.org/pdf/2509.07116", "abs": "https://arxiv.org/abs/2509.07116", "authors": ["Pankaj Kumar", "Judith T. Karpen", "P. K. Manoharan", "N. Gopalswamy"], "title": "Imaging and Radio Signatures of Shock-Plasmoid Interaction", "categories": ["astro-ph.SR", "physics.plasm-ph", "physics.space-ph"], "comment": "ApJ Letters, 14 pages, 8 Figures", "summary": "Understanding how shocks interact with coronal structures is crucial for\nunderstanding the mechanisms of particle acceleration in the solar corona and\ninner heliosphere. Using simultaneous radio and white-light observations, we\ninvestigate the interaction between a CME-driven shock and a plasmoid. LASCO\nand STEREO-A COR-2 white-light images are analyzed to track the evolution of\nthe plasmoid, CME and its associated shock, while the Wind/WAVES and\nSTEREO/WAVES dynamic spectra provide complementary radio signatures of the\nshock-plasmoid interaction at $\\approx$7 R$_\\odot$. An interplanetary Type II\nradio burst was detected as the shock propagated through the plasmoid. The\nmerging of the plasmoid into the CME was accompanied by interplanetary Type III\nradio bursts, suggesting escaping electron beams during the reconnection\nprocess. These observations clearly demonstrate that shock-plasmoid\ninteractions can enhance the efficiency of particle acceleration associated\nwith CMEs, with implications for electron acceleration in flare and\nheliospheric current sheets as well.", "AI": {"tldr": "Study shows CME-driven shock interacting with plasmoid enhances particle acceleration efficiency, observed through radio bursts and white-light imaging.", "motivation": "Understanding how shocks interact with coronal structures is crucial for understanding particle acceleration mechanisms in solar corona and inner heliosphere.", "method": "Used simultaneous radio and white-light observations from LASCO, STEREO-A COR-2, Wind/WAVES and STEREO/WAVES to track plasmoid evolution, CME, and shock interaction at \u22487 solar radii.", "result": "Detected interplanetary Type II radio burst as shock propagated through plasmoid, and Type III radio bursts during plasmoid-CME merging, indicating escaping electron beams from reconnection process.", "conclusion": "Shock-plasmoid interactions clearly enhance particle acceleration efficiency associated with CMEs, with implications for electron acceleration in flare and heliospheric current sheets."}}
{"id": "2509.07391", "pdf": "https://arxiv.org/pdf/2509.07391", "abs": "https://arxiv.org/abs/2509.07391", "authors": ["Rahul Barthwal", "Christian Rohde", "Anupam Sen"], "title": "Existence and stability of the Riemann solutions for a non-symmetric Keyfitz--Kranzer type model", "categories": ["math.AP", "35L40 35L45 35L65 76A20"], "comment": null, "summary": "In this article, we develop a new hyperbolic model governing the first-order\ndynamics of a thin film flow under the influence of gravity and solute\ntransport. The obtained system turns out to be a non-symmetric Keyfitz-Kranzer\ntype system. We find an entire class of convex entropies in the regions where\nthe system remains strictly hyperbolic. By including delta shocks, we prove the\nexistence of unique solutions of the Riemann problem. We analyze their\nstability with respect to the perturbation of the initial data and to the\ngravity and surface tension parameters is analyzed. Moreover, we discuss the\nlarge time behaviour of the solutions of the perturbed Riemann problem and\nprove that the initial Riemann states govern it. Thus, we confirm the\nstructural stability of the Riemann solutions under the perturbation of initial\ndata. Finally, we validate our analytical results with well-established\nnumerical schemes for this new system of conservation laws.", "AI": {"tldr": "Developed hyperbolic model for thin film flow with gravity and solute transport, analyzed as non-symmetric Keyfitz-Kranzer system. Found convex entropies, proved Riemann problem solutions with delta shocks, and confirmed structural stability under perturbations.", "motivation": "To develop a mathematical model for thin film flow dynamics under gravity and solute transport effects, addressing the need for understanding first-order dynamics in such systems.", "method": "Developed hyperbolic conservation law system, identified convex entropies in hyperbolic regions, included delta shocks, analyzed stability to perturbations, and validated with numerical schemes.", "result": "Proved existence of unique Riemann problem solutions with delta shocks, demonstrated structural stability under initial data perturbations, and confirmed analytical results with numerical validation.", "conclusion": "The developed hyperbolic model successfully captures thin film flow dynamics, with proven solution existence, stability properties, and validation through numerical methods, establishing structural robustness."}}
{"id": "2509.07735", "pdf": "https://arxiv.org/pdf/2509.07735", "abs": "https://arxiv.org/abs/2509.07735", "authors": ["David Portillo", "Ignacio Romero"], "title": "Embedding structures in continua: linear models and finite element discretizations", "categories": ["math.NA", "cs.NA", "math-ph", "math.MP"], "comment": null, "summary": "This work describes models and numerical approximations that describe the\nmechanical behavior of deformable continua with embedded structural members,\nsuch as rigid bodies, beams, shells, etc. The continuum formulation extends an\nidea first presented in the context of the Arlequin method and constrains the\nkinematics of the two types of bodies to be compatible in the energy sense. In\nthe article, we exploit the shared similarities of all structural theories to\nintroduce a general framework for energetically coupling the latter with\ncontinua. In addition, we show that the problems, as well as their finite\nelement approximations, are well-posed. Numerical examples of bodies with\ninclusions, fibers, and embedded surfaces are provided to illustrate the\ngenerality and robustness of the approach.", "AI": {"tldr": "A general framework for coupling structural members (rigid bodies, beams, shells) with deformable continua using energy-based kinematic constraints, with proven well-posed finite element approximations.", "motivation": "To develop models that can accurately describe the mechanical behavior of deformable continua containing embedded structural components, extending previous work from the Arlequin method.", "method": "Extends continuum formulation using energy-based kinematic constraints to ensure compatibility between structural members and continua. Creates a general framework exploiting similarities across structural theories for energetic coupling.", "result": "The problems and their finite element approximations are proven to be well-posed. Numerical examples demonstrate the approach's generality and robustness for bodies with inclusions, fibers, and embedded surfaces.", "conclusion": "The proposed framework provides a robust and general method for modeling deformable continua with embedded structural members, with mathematically sound finite element implementations."}}
{"id": "2509.07340", "pdf": "https://arxiv.org/pdf/2509.07340", "abs": "https://arxiv.org/abs/2509.07340", "authors": ["Siu Wun Cheung", "Youngsoo Choi", "Jean-Luc Fattebert", "Daniel Osei-Kuffuor"], "title": "Model Order Reduction for Quantum Molecular Dynamics", "categories": ["physics.chem-ph", "cs.NA", "math.NA", "physics.comp-ph"], "comment": null, "summary": "Molecular dynamics simulations are indispensable for exploring the behavior\nof atoms and molecules. Grounded in quantum mechanical principles, quantum\nmolecular dynamics provides high predictive power but its computational cost is\ndominated by iterative high-fidelity electronic structure calculations. We\npropose a novel model order reduction approach as an alternative to\nhigh-fidelity electronic structure calculation. By learning a low-dimensional\nrepresentation of the electronic solution manifold within the Kohn-Sham density\nfunctional theory framework, our model order reduction approach determines the\nground state electronic density by projecting the problem onto a\nlow-dimensional subspace, thereby avoiding the computationally expensive\niterative optimization of electronic wavefunctions in the full space. We\ndemonstrate the capability of our method on a water molecule, showing excellent\nagreement with high-fidelity simulations for both molecular geometry and\ndynamic properties, highlighting the generalizability through carefully\ndesigned parametrization and systematic sampling.", "AI": {"tldr": "Novel model order reduction approach for quantum molecular dynamics that learns low-dimensional representation of electronic solution manifold to avoid expensive iterative wavefunction optimization.", "motivation": "Quantum molecular dynamics provides high predictive power but is computationally dominated by expensive iterative electronic structure calculations, creating a need for more efficient alternatives.", "method": "Learning low-dimensional representation of electronic solution manifold within Kohn-Sham DFT framework, projecting ground state electronic density determination onto low-dimensional subspace instead of full iterative optimization.", "result": "Demonstrated on water molecule with excellent agreement to high-fidelity simulations for both molecular geometry and dynamic properties, showing good generalizability.", "conclusion": "The model order reduction approach successfully provides an efficient alternative to computationally expensive high-fidelity electronic structure calculations in quantum molecular dynamics."}}
{"id": "2509.07621", "pdf": "https://arxiv.org/pdf/2509.07621", "abs": "https://arxiv.org/abs/2509.07621", "authors": ["Nadja Reisinger", "Fabio Bacchini"], "title": "Comparing Simulated and Observed Particle Energy Distributions through Magnetic Reconnection in Earth's Magnetotail", "categories": ["physics.space-ph", "astro-ph.EP", "physics.plasm-ph"], "comment": null, "summary": "Magnetic reconnection is an explosive process that accelerates particles to\nhigh energies in Earth's magnetosphere, offering a unique natural laboratory to\nstudy this phenomenon. We performed fully kinetic 2D simulations of a\nreconnection event observed by the Magnetospheric Multiscale mission and\ncompared the resulting ion and electron energy distributions with observations.\nThe simulations capture the overall shape and evolution of non-thermal energy\ndistributions for both species, but generally underestimate the\nvery-high-energy tail of the electron spectrum. Variations in numerical\nparameters have negligible effects on the resulting spectra, while the initial\nupstream temperatures instead play a critical role in reproducing the observed\ndistributions. This work presents a novel analysis of particle acceleration in\nfully kinetic modeling of reconnection directly informed by observed, realistic\nparameters; highlights the limitations of 2D simulations and underlines the\nneed for more realistic simulations (e.g. employing 3D setups) to capture the\nobserved particle energization more accurately.", "AI": {"tldr": "Fully kinetic 2D simulations of magnetic reconnection match observed ion and electron energy distributions but underestimate high-energy electron tails, with upstream temperatures being critical while numerical parameters have little effect.", "motivation": "Magnetic reconnection accelerates particles to high energies in Earth's magnetosphere, providing a natural laboratory to study this explosive process using Magnetospheric Multiscale mission observations.", "method": "Performed fully kinetic 2D simulations of a reconnection event observed by the Magnetospheric Multiscale mission and compared resulting ion and electron energy distributions with actual observations.", "result": "Simulations captured overall shape and evolution of non-thermal energy distributions for both species but generally underestimated the very-high-energy tail of electron spectrum. Upstream temperatures proved critical while numerical parameters had negligible effects.", "conclusion": "2D simulations have limitations in capturing observed particle energization accurately, highlighting the need for more realistic 3D simulations to better reproduce high-energy electron acceleration in magnetic reconnection events."}}
{"id": "2509.07405", "pdf": "https://arxiv.org/pdf/2509.07405", "abs": "https://arxiv.org/abs/2509.07405", "authors": ["Rihab Ben Belgacem", "Mohamed Majdoub"], "title": "Blow-up for a Nonlocal Diffusion Equation with Time Regularly Varying Nonlinearity and Forcing", "categories": ["math.AP"], "comment": "21 pages. Comments and suggestions are most welcome", "summary": "We investigate the Cauchy problem for a semilinear parabolic equation driven\nby a mixed local-nonlocal diffusion operator of the form \\[ \\partial_t u -\n(\\Delta - (-\\Delta)^{\\mathsf{s}})u = \\mathsf{h}(t)|x|^{-b}|u|^p + t^\\varrho\n\\mathbf{w}(x), \\qquad (x,t)\\in \\mathbb{R}^N\\times (0,\\infty), \\] where\n$\\mathsf{s}\\in (0,1)$, $p>1$, $b\\geq 0$, and $\\varrho>-1$. The function\n$\\mathsf{h}(t)$ is assumed to belong to the generalized class of regularly\nvarying functions, while $\\mathbf{w}$ is a prescribed spatial source. We first\nrevisit the unforced case and establish sharp blow-up and global existence\ncriteria in terms of the critical Fujita exponent, thereby extending earlier\nresults to the wider class of time-dependent coefficients. For the forced\nproblem, we derive nonexistence of global weak solutions under suitable growth\nconditions on $\\mathsf{h}$ and integrability assumptions on $\\mathbf{w}$.\nFurthermore, we provide sufficient smallness conditions on the initial data and\nthe forcing term ensuring global-in-time mild solutions. Our analysis combines\nsemigroup estimates for the mixed operator, test function methods, and\nasymptotic properties of regularly varying functions. To our knowledge, this is\nthe first study addressing blow-up phenomena for nonlinear diffusion equations\nwith such a class of time-dependent coefficients.", "AI": {"tldr": "Analysis of blow-up phenomena for semilinear parabolic equations with mixed local-nonlocal diffusion and time-dependent coefficients, establishing sharp existence criteria and nonexistence results.", "motivation": "Extend earlier results on nonlinear diffusion equations to the wider class of time-dependent coefficients in regularly varying functions, addressing a gap in understanding blow-up phenomena for such systems.", "method": "Combines semigroup estimates for mixed diffusion operators, test function methods, and asymptotic properties of regularly varying functions to analyze both unforced and forced cases.", "result": "Established sharp blow-up and global existence criteria via critical Fujita exponent, derived nonexistence conditions for global weak solutions, and provided sufficient smallness conditions ensuring global mild solutions.", "conclusion": "First comprehensive study of blow-up phenomena for nonlinear diffusion equations with time-dependent coefficients in regularly varying functions, providing complete existence and nonexistence theory."}}
{"id": "2509.07806", "pdf": "https://arxiv.org/pdf/2509.07806", "abs": "https://arxiv.org/abs/2509.07806", "authors": ["Fabiana Camattari", "Sabrina Guastavino", "Francesco Marchetti", "Emma Perracchione"], "title": "Feature Understanding and Sparsity Enhancement via 2-Layered kernel machines (2L-FUSE)", "categories": ["math.NA", "astro-ph.SR", "cs.NA", "stat.ML", "65D15, 41A05, 68Q32"], "comment": null, "summary": "We propose a novel sparsity enhancement strategy for regression tasks, based\non learning a data-adaptive kernel metric, i.e., a shape matrix, through\n2-Layered kernel machines. The resulting shape matrix, which defines a\nMahalanobis-type deformation of the input space, is then factorized via an\neigen-decomposition, allowing us to identify the most informative directions in\nthe space of features. This data-driven approach provides a flexible,\ninterpretable and accurate feature reduction scheme. Numerical experiments on\nsynthetic and applications to real datasets of geomagnetic storms demonstrate\nthat our approach achieves minimal yet highly informative feature sets without\nlosing predictive performance.", "AI": {"tldr": "A novel sparsity enhancement strategy using data-adaptive kernel metrics and 2-layered kernel machines for feature reduction in regression tasks, achieving minimal feature sets without performance loss.", "motivation": "To develop a flexible, interpretable and accurate feature reduction scheme that identifies the most informative directions in feature space while maintaining predictive performance.", "method": "Learning a data-adaptive kernel metric (shape matrix) through 2-layered kernel machines, then factorizing it via eigen-decomposition to identify informative feature directions.", "result": "Achieves minimal yet highly informative feature sets without losing predictive performance, as demonstrated on synthetic and real geomagnetic storm datasets.", "conclusion": "The proposed approach provides an effective sparsity enhancement strategy that offers both interpretability and accuracy in feature reduction for regression tasks."}}
{"id": "2509.07529", "pdf": "https://arxiv.org/pdf/2509.07529", "abs": "https://arxiv.org/abs/2509.07529", "authors": ["Priyanka D. Bhoyar", "Govindan Rangarajan", "Prashant M. gade"], "title": "Emergence of continuously varying critical exponents in coupled map lattice as an effect of quenched disorder", "categories": ["cond-mat.stat-mech", "physics.comp-ph"], "comment": null, "summary": "The transition to an absorbing phase in a spatiotemporal system is a\nwell-investigated nonequilibrium dynamic transition. The absorbing phase\ntransitions fall into a few universality classes, defined by the critical\nexponents observed at the critical point. We present a coupled map lattice\n(CML) model with quenched disorder in the couplings. In this model, spatial\ndisorders are introduced in the form of asymmetric coupling with a larger\ncoupling ($p$) to a neighbor on the right and a smaller coupling ($1-p$) to the\nneighbor on the left, for $0 \\le p \\le0.5$. For $p=0$, the system belongs to\nthe directed percolation universality class. For $p>0$, we observe continuously\nchanging critical exponents at the critical point. The order parameter is the\nfraction of turbulent sites $m(t)$. %sites that are not in the laminar region.\nWe observe a power-law decay, $m(t) \\sim t^{-\\delta}$, at the critical point\n$\\epsilon_c$, where $\\epsilon$ is the diffusive coupling parameter. These\nexponents change continuously and do not match any known universality class in\nany limit. This could be related to changes in the eigenvalue spectrum of the\nconnectivity matrix as the disorder is introduced.", "AI": {"tldr": "A coupled map lattice model with quenched disorder shows continuously changing critical exponents at the absorbing phase transition, not matching any known universality class.", "motivation": "To investigate how spatial disorder in asymmetric couplings affects the universality class of absorbing phase transitions in spatiotemporal systems.", "method": "Developed a coupled map lattice model with quenched disorder where asymmetric coupling (p to right neighbor, 1-p to left neighbor) is introduced for 0 \u2264 p \u2264 0.5.", "result": "For p=0, system belongs to directed percolation universality class. For p>0, critical exponents change continuously and do not match any known universality class, showing power-law decay m(t) \u223c t^{-\u03b4} at critical point.", "conclusion": "The continuously changing critical exponents with disorder introduction may be related to changes in the eigenvalue spectrum of the connectivity matrix, representing a novel behavior in absorbing phase transitions."}}
{"id": "2509.07433", "pdf": "https://arxiv.org/pdf/2509.07433", "abs": "https://arxiv.org/abs/2509.07433", "authors": ["Yong-Cheol Kim"], "title": "Nonlocal Harnack inequalities for nonlocal double phase equations I ; with positive bounded modulating coefficient with no H\u00f6lder condition", "categories": ["math.AP", "47G20, 45K05, 35J60, 35B65, 35D30 (60J76)"], "comment": "43 pages", "summary": "In this paper, by applying the De Giorgi-Nash-Moser theory we prove nonlocal\nHarnack inequalities for (locally nonnegative in $\\Omega$) weak solutions to\nnolocal double phase equations \\begin{equation*}\\begin{cases}\\cL u =0 & \\text{\nin $\\Omega$,} \\\\ u=g & \\text{ in $\\BR^n\\s\\Omega$ } \\end{cases}\\end{equation*}\nwhere $\\Omega\\subset\\BR^n$ ($n\\ge 2$) is a bounded domain with Lipschitz\nboundary, $\\cL$ is the nonlocal double phase operator $\\cL$ given by\n\\begin{equation*}\\begin{split}\\cL\nu(x)=&\\pv\\int_{\\BR^n}|u(x)-u(y)|^{p-2}(u(x)-u(y))K_{ps}(x,y)\\,dy \\\\\n&+\\pv\\int_{\\BR^n}\\fa(x,y)|u(x)-u(y)|^{q-2}(u(x)-u(y))K_{qt}(x,y)\\,dy,\n\\end{split} \\end{equation*} $0<\\fa(x,y) = \\fa(y,x) \\le\n\\|\\fa\\|_{L^\\iy(\\BR^n\\times\\BR^n)} < \\iy$ and $ps\\ge qt$ for $0<s,t<1<p\\le\nq<\\iy$. In addition, we get local boundedness with explicit formula and weak\nHarnack inequalities for their weak supersolutions.", "AI": {"tldr": "Nonlocal Harnack inequalities and local boundedness results for weak solutions to nonlocal double phase equations using De Giorgi-Nash-Moser theory", "motivation": "To establish regularity properties and Harnack-type inequalities for solutions to nonlocal double phase equations, which combine two different growth behaviors in a nonlocal setting", "method": "Applied De Giorgi-Nash-Moser theory to prove nonlocal Harnack inequalities for weak solutions, and derived local boundedness with explicit formulas and weak Harnack inequalities for weak supersolutions", "result": "Successfully proved nonlocal Harnack inequalities for locally nonnegative weak solutions, obtained local boundedness with explicit formulas, and established weak Harnack inequalities for supersolutions", "conclusion": "The De Giorgi-Nash-Moser theory provides an effective framework for establishing regularity results for nonlocal double phase equations, extending classical results to this more complex nonlocal setting"}}
{"id": "2509.07834", "pdf": "https://arxiv.org/pdf/2509.07834", "abs": "https://arxiv.org/abs/2509.07834", "authors": ["Genming Bai", "Harald Garcke", "Shravan Veerapaneni"], "title": "Convergence analysis for the Barrett-Garcke-Nurnberg method of transport type", "categories": ["math.NA", "cs.NA"], "comment": null, "summary": "In this paper, we propose a Barrett-Garcke-Nurnberg (BGN) method for evolving\ngeometries under general flows and present the corresponding convergence\nanalysis. Specifically, we examine the scenario where a closed curve evolves\naccording to a prescribed background velocity field. Unlike mean curvature flow\nand surface diffusion, where the evolution velocities inherently exhibit\nparabolicity, this case is dominated by transport which poses a significant\ndifficulty in establishing convergence proofs. To address the challenges\nimposed by this transport-dominant nature, we derive several discrete energy\nestimates of the transport type on discretized polynomial surfaces within the\nframework of the projection error. The use of the projection error is\nindispensable as it provides crucial additional stability through its\northogonality structure. We prove that the proposed method converges\nsub-optimally in the L2 norm, and this is the first convergence proof for a\nfully discrete numerical method solving the evolution of curves driven by\ngeneral flows.", "AI": {"tldr": "Proposes a Barrett-Garcke-Nurnberg method for evolving geometries under general flows with transport-dominant nature, providing the first convergence proof for fully discrete methods in this context.", "motivation": "To address the challenge of establishing convergence proofs for curve evolution under general background velocity fields, which are transport-dominant rather than parabolic like mean curvature flow or surface diffusion.", "method": "Derives discrete energy estimates of transport type on discretized polynomial surfaces using projection error framework, leveraging orthogonality structure for additional stability.", "result": "Proves that the proposed method converges sub-optimally in the L2 norm, marking the first convergence proof for a fully discrete numerical method solving curve evolution driven by general flows.", "conclusion": "The BGN method successfully handles transport-dominant curve evolution problems and provides theoretical convergence guarantees through novel energy estimates and projection error techniques."}}
{"id": "2509.07579", "pdf": "https://arxiv.org/pdf/2509.07579", "abs": "https://arxiv.org/abs/2509.07579", "authors": ["Liya Gaynutdinova", "Martin Do\u0161k\u00e1\u0159", "Ond\u0159ej Roko\u0161", "Ivana Pultarov\u00e1"], "title": "Homogenization with Guaranteed Bounds via Primal-Dual Physically Informed Neural Networks", "categories": ["cs.LG", "math.AP", "physics.comp-ph", "80-10", "I.6.4"], "comment": null, "summary": "Physics-informed neural networks (PINNs) have shown promise in solving\npartial differential equations (PDEs) relevant to multiscale modeling, but they\noften fail when applied to materials with discontinuous coefficients, such as\nmedia with piecewise constant properties. This paper introduces a dual\nformulation for the PINN framework to improve the reliability of the\nhomogenization of periodic thermo-conductive composites, for both strong and\nvariational (weak) formulations. The dual approach facilitates the derivation\nof guaranteed upper and lower error bounds, enabling more robust detection of\nPINN failure. We compare standard PINNs applied to smoothed material\napproximations with variational PINNs (VPINNs) using both spectral and neural\nnetwork-based test functions. Our results indicate that while strong-form PINNs\nmay outperform VPINNs in controlled settings, they are sensitive to material\ndiscontinuities and may fail without clear diagnostics. In contrast, VPINNs\naccommodate piecewise constant material parameters directly but require careful\nselection of test functions to avoid instability. Dual formulation serves as a\nreliable indicator of convergence quality, and its integration into PINN\nframeworks enhances their applicability to homogenization problems in\nmicromechanics.", "AI": {"tldr": "Dual formulation for PINNs improves reliability in homogenizing periodic thermo-conductive composites with discontinuous coefficients, providing guaranteed error bounds and better failure detection.", "motivation": "Standard PINNs often fail when applied to materials with discontinuous coefficients like piecewise constant properties, lacking clear diagnostics for failure.", "method": "Introduces dual formulation for PINN framework, comparing standard PINNs with smoothed approximations vs variational PINNs (VPINNs) using spectral and neural network-based test functions.", "result": "Strong-form PINNs outperform VPINNs in controlled settings but are sensitive to material discontinuities. VPINNs handle piecewise constants directly but require careful test function selection. Dual formulation provides reliable convergence indicators.", "conclusion": "Integration of dual formulation into PINN frameworks enhances applicability to homogenization problems in micromechanics by providing guaranteed error bounds and robust failure detection."}}
{"id": "2509.07458", "pdf": "https://arxiv.org/pdf/2509.07458", "abs": "https://arxiv.org/abs/2509.07458", "authors": ["Yuhan Li", "Hongyu Liu", "Catharine W. K. Lo"], "title": "Unveiling Biological Models Through Turing Patterns", "categories": ["math.AP", "physics.bio-ph", "q-bio.BM", "q-bio.CB", "35R30, 35B10, 35B36, 35K10, 35K55, 35K57, 35Q92, 92-10, 92C15,\n  92C37, 92C70, 92D25"], "comment": "22 pages keywords: inverse reaction-diffusion equations, Turing\n  patterns, Turing instability, periodic solutions, sinusoidal form", "summary": "Turing patterns play a fundamental role in morphogenesis and population\ndynamics, encoding key information about the underlying biological mechanisms.\nYet, traditional inverse problems have largely relied on non-biological data\nsuch as boundary measurements, neglecting the rich information embedded in the\npatterns themselves. Here we introduce a new research direction that directly\nleverages physical observables from nature--the amplitude of Turing\npatterns--to achieve complete parameter identification. We present a framework\nthat uses the spatial amplitude profile of a single pattern to simultaneously\nrecover all system parameters, including wavelength, diffusion constants, and\nthe full nonlinear forms of chemotactic and kinetic coefficient functions.\nDemonstrated on models of chemotactic bacteria, this amplitude-based approach\nestablishes a biologically grounded, mathematically rigorous paradigm for\nreverse-engineering pattern formation mechanisms across diverse biological\nsystems.", "AI": {"tldr": "New amplitude-based approach uses spatial amplitude profiles of Turing patterns to recover all system parameters including diffusion constants and nonlinear functions, enabling complete biological system identification.", "motivation": "Traditional inverse problems rely on non-biological boundary measurements, neglecting the rich information embedded in Turing patterns themselves that encode key biological mechanisms.", "method": "Framework that leverages physical observables (amplitude of Turing patterns) from nature, using spatial amplitude profile of a single pattern to simultaneously recover all system parameters including wavelength, diffusion constants, and full nonlinear forms of chemotactic and kinetic coefficient functions.", "result": "Demonstrated successful parameter identification on models of chemotactic bacteria, achieving complete recovery of system parameters from pattern amplitude data.", "conclusion": "Establishes a biologically grounded, mathematically rigorous paradigm for reverse-engineering pattern formation mechanisms across diverse biological systems using amplitude-based approach."}}
{"id": "2509.07155", "pdf": "https://arxiv.org/pdf/2509.07155", "abs": "https://arxiv.org/abs/2509.07155", "authors": ["David Jennings", "Kamil Korzekwa", "Matteo Lostaglio", "Andrew T Sornborger", "Yigit Subasi", "Guoming Wang"], "title": "Quantum algorithms for general nonlinear dynamics based on the Carleman embedding", "categories": ["quant-ph", "cs.DS", "cs.NA", "math.NA"], "comment": "70+78 pages, 4 figures. Comments welcome!", "summary": "Important nonlinear dynamics, such as those found in plasma and fluid\nsystems, are typically hard to simulate on classical computers. Thus, if\nfault-tolerant quantum computers could efficiently solve such nonlinear\nproblems, it would be a transformative change for many industries. In a recent\nbreakthrough [Liu et al., PNAS 2021], the first efficient quantum algorithm for\nsolving nonlinear differential equations was constructed, based on a single\ncondition $R<1$, where $R$ characterizes the ratio of nonlinearity to\ndissipation. This result, however, is limited to the class of purely\ndissipative systems with negative log-norm, which excludes application to many\nimportant problems. In this work, we correct technical issues with this and\nother prior analysis, and substantially extend the scope of nonlinear dynamical\nsystems that can be efficiently simulated on a quantum computer in a number of\nways. Firstly, we extend the existing results from purely dissipative systems\nto a much broader class of stable systems, and show that every quadratic\nLyapunov function for the linearized system corresponds to an independent\n$R$-number criterion for the convergence of the Carlemen scheme. Secondly, we\nextend our stable system results to physically relevant settings where\nconserved polynomial quantities exist. Finally, we provide extensive results\nfor the class of non-resonant systems. With this, we are able to show that\nefficient quantum algorithms exist for a much wider class of nonlinear systems\nthan previously known, and prove the BQP-completeness of nonlinear oscillator\nproblems of exponential size. In our analysis, we also obtain several results\nrelated to the Poincar\\'{e}-Dulac theorem and diagonalization of the Carleman\nmatrix, which could be of independent interest.", "AI": {"tldr": "This paper extends quantum algorithms for solving nonlinear differential equations beyond purely dissipative systems to a broader class of stable systems, including those with conserved quantities and non-resonant systems, enabling efficient quantum simulation for a wider range of nonlinear problems.", "motivation": "Previous quantum algorithms for nonlinear differential equations were limited to purely dissipative systems with negative log-norm (R<1), excluding many important physical systems. This work aims to expand the scope of nonlinear systems that can be efficiently simulated on quantum computers.", "method": "The authors correct technical issues in prior analysis and extend results to: 1) broader class of stable systems using quadratic Lyapunov functions, 2) systems with conserved polynomial quantities, and 3) non-resonant systems. They analyze the Carleman scheme convergence and obtain results related to Poincar\u00e9-Dulac theorem and Carleman matrix diagonalization.", "result": "The paper demonstrates that efficient quantum algorithms exist for a much wider class of nonlinear systems than previously known, including proving BQP-completeness of nonlinear oscillator problems of exponential size.", "conclusion": "This work significantly expands the applicability of quantum computing to nonlinear dynamics problems, making efficient quantum simulation possible for many important physical systems that were previously excluded from quantum algorithmic approaches."}}
{"id": "2509.07684", "pdf": "https://arxiv.org/pdf/2509.07684", "abs": "https://arxiv.org/abs/2509.07684", "authors": ["J\u00e9r\u00f4me Deprince", "Lucas Maison"], "title": "Benchmarking the Plane-Wave Born and Distorted Waves approximations for electron-impact collision strengthcomputations: the sample case of Sr II", "categories": ["physics.atom-ph", "astro-ph.HE", "physics.comp-ph"], "comment": "Proceedings of the 22nd International Conference on Atomic Processes\n  in Plasmas 2025 (Tokyo, Japan)", "summary": "The discovery of gravitational waves from a neutron star merger in 2017\n(GW170817) and the associated kilonova (AT2017gfo) confirmed these events as\nkey sites for heavy element production through the r-process. Subsequent\nobservations, including late-time spectra with \\textit{JWST}, have highlighted\nthe need for accurate modeling of kilonova ejecta. In the photospheric phase,\natomic level populations can be estimated under LTE using Boltzmann and Saha\nrelations, but about a week after the merger the ejecta enters the nebular\nphase where non-LTE effects dominate. Modeling nebular spectra therefore\nrequires a detailed treatment of radiative and collisional processes that\naffect the population of atomic levels. This work focuses on electron-impact\nexcitation in Sr II, a heavy ion relevant for kilonova spectra. Two\ncomputational approaches are employed: the Plane Wave Born approximation within\nthe pseudo-relativistic Hartree-Fock method, and a Distorted Waves method using\nAUTOSTRUCTURE. The resulting collision strengths are compared against reference\nR-matrix data to evaluate the accuracy of these approximations and their\nsuitability for large-scale applications to all heavy elements. In addition,\nradiative parameters for forbidden transitions are computed. These results\nprovide an essential benchmark of approximations that could be used to compute\natomic data for nebular-phase kilonova modeling.", "AI": {"tldr": "This paper evaluates computational methods for modeling electron-impact excitation in Sr II, a key heavy ion in kilonova spectra, comparing Plane Wave Born and Distorted Waves approaches against R-matrix reference data.", "motivation": "The discovery of gravitational waves from neutron star mergers (GW170817) and associated kilonovae confirmed these events as major sites for heavy element production via r-process. Accurate modeling of nebular-phase kilonova spectra requires detailed treatment of non-LTE effects and atomic processes.", "method": "Two computational approaches were employed: Plane Wave Born approximation within pseudo-relativistic Hartree-Fock method, and Distorted Waves method using AUTOSTRUCTURE. The resulting collision strengths were compared against reference R-matrix data to evaluate accuracy.", "result": "The study provides collision strength results and radiative parameters for forbidden transitions in Sr II, establishing benchmarks for computational approximations used in nebular-phase kilonova modeling.", "conclusion": "The results provide essential benchmarks for approximation methods that could be used to compute atomic data for large-scale applications to all heavy elements in nebular-phase kilonova modeling."}}
{"id": "2509.07517", "pdf": "https://arxiv.org/pdf/2509.07517", "abs": "https://arxiv.org/abs/2509.07517", "authors": ["Fr\u00e9d\u00e9ric Robert"], "title": "Sharp multiscale control for high order nonlinear equations", "categories": ["math.AP", "Primary 35J35, Secondary 35J60, 35B44, 35J08, 58J05"], "comment": "38 pages", "summary": "We analyze the behavior of families $(u_\\alpha)_{\\alpha>0}$ of solutions to\nthe high-order critical equation $P_\\alpha u_\\alpha=\\Delta_g^k u_\\alpha\n+\\hbox{lot}=|u_\\alpha|^{2^\\star-2}u_\\alpha$ on a Riemannian manifold $M$, with\na uniform bound on the Dirichlet energy. We prove a sharp pointwise control of\nthe $u_\\alpha$'s by a sum of bubbles uniformly with respect to $\\alpha\\to\n+\\infty$, that is $|u_\\alpha|\\leq C\\Vert u_\\infty \\Vert_\\infty\n+C\\sum_{i=1}^NB_{i,\\alpha}$ where $u_\\infty \\in C^{2k}(M)$ and the\n$(B_{i,\\alpha})_\\alpha$, $i=1,...,N$ are explicit standard peaks.", "AI": {"tldr": "Analysis of high-order critical equation solutions on Riemannian manifolds with uniform Dirichlet energy bounds, proving sharp pointwise control by bubble sums.", "motivation": "To understand the behavior of solution families to high-order critical equations on Riemannian manifolds under uniform energy constraints, particularly as the parameter approaches infinity.", "method": "Analyze solutions to the high-order critical equation $P_\\alpha u_\\alpha = \\Delta_g^k u_\\alpha + \\text{lot} = |u_\\alpha|^{2^\\star-2}u_\\alpha$ with uniform Dirichlet energy bounds, proving pointwise control via explicit standard peaks.", "result": "Established sharp pointwise control showing $|u_\\alpha| \\leq C\\Vert u_\\infty \\Vert_\\infty + C\\sum_{i=1}^N B_{i,\\alpha}$ where $u_\\infty \\in C^{2k}(M)$ and $B_{i,\\alpha}$ are explicit standard peaks, uniformly as $\\alpha\\to+\\infty$.", "conclusion": "The study provides a precise characterization of solution behavior through bubble decomposition, demonstrating uniform control and convergence properties for high-order critical equations on Riemannian manifolds."}}
{"id": "2509.07167", "pdf": "https://arxiv.org/pdf/2509.07167", "abs": "https://arxiv.org/abs/2509.07167", "authors": ["Miriana Catalano", "Anna Volpara", "Paolo Massa", "Michele Piana", "Anna Maria Massone"], "title": "CLEAN and multi-scale CLEAN for STIX in Solar Orbiter", "categories": ["astro-ph.SR", "astro-ph.IM", "cs.NA", "math.NA", "85-08"], "comment": null, "summary": "CLEAN is a well-established deconvolution approach to Fourier imaging at both\nradio wavelwengths and hard X-ray energies. However, specifically for hard\nX-ray imaging, CLEAN suffers two significant drawbacks: a rather limited degree\nof automation, and a tendency to under-resolution. This paper introduces a\nmulti-scale version of CLEAN specifically tailored to the reconstruction of\nimages from measurements observed by the Spectrometer/Telescope for Imaging\nX-rays (STIX) on-board Solar Orbiter. Using synthetic STIX data, this study\nshows that multi-scale CLEAN may represent a reliable solution to the two\npreviously mentioned CLEAN limitations. Further, this paper shows the\nperformances of CLEAN and its multi-scale release in reconstructing\nexperimental real scenarios characterized by complex emission morphologies.", "AI": {"tldr": "Multi-scale CLEAN improves automated hard X-ray imaging with better resolution for STIX telescope data", "motivation": "Standard CLEAN has limited automation and under-resolution issues for hard X-ray imaging with STIX telescope", "method": "Developed a multi-scale version of CLEAN algorithm specifically for Spectrometer/Telescope for Imaging X-rays (STIX) data", "result": "Multi-scale CLEAN provides reliable solution to automation and resolution limitations, performs well on complex experimental scenarios", "conclusion": "Multi-scale CLEAN represents an effective improvement over traditional CLEAN for hard X-ray image reconstruction in solar observations"}}
{"id": "2509.07559", "pdf": "https://arxiv.org/pdf/2509.07559", "abs": "https://arxiv.org/abs/2509.07559", "authors": ["Vivek Sahu"], "title": "Fractional Sobolev logarithmic inequalities", "categories": ["math.AP", "46E35, 26D15, 26A33"], "comment": "10 pages", "summary": "We establish new logarithmic Sobolev inequalities in the fractional Sobolev\nsetting. Our approach relies on a interpolation inequality, which can be viewed\nas a fractional Caffarelli-Kohn-Nirenberg inequality without weights. We\nfurther relate the optimal constant in this interpolation inequality to a\ncorresponding variational problem. These results extend classical logarithmic\nSobolev inequalities to the nonlocal framework and provide new tools for\nanalysis in fractional Sobolev spaces.", "AI": {"tldr": "New logarithmic Sobolev inequalities established in fractional Sobolev spaces using interpolation approach without weights, extending classical results to nonlocal framework.", "motivation": "To extend classical logarithmic Sobolev inequalities to the fractional Sobolev setting and provide new analytical tools for nonlocal analysis.", "method": "Relies on an interpolation inequality (fractional Caffarelli-Kohn-Nirenberg inequality without weights) and relates optimal constant to corresponding variational problem.", "result": "Established new logarithmic Sobolev inequalities in fractional Sobolev spaces, providing extension of classical results to nonlocal framework.", "conclusion": "The approach successfully extends classical logarithmic Sobolev inequalities to fractional settings and offers new tools for analysis in fractional Sobolev spaces."}}
{"id": "2509.07296", "pdf": "https://arxiv.org/pdf/2509.07296", "abs": "https://arxiv.org/abs/2509.07296", "authors": ["Grace Burtenshaw", "Joe Lane", "Meagan Carney"], "title": "A novel statistical workflow for nonstationary modelling of successive Fr\u00e9chet extremes", "categories": ["math.ST", "cs.NA", "math.NA", "stat.TH"], "comment": null, "summary": "Accurate estimation of the frequency and magnitude of successive extreme\nevents in energy demand is critical for strategic resource planning.\nTraditional approaches based on extreme value theory (EVT) are typically\nlimited to modelling isolated extreme events and struggle to capture the\ndynamics of temporally clustered extremes, such as those driven by prolonged\nextreme weather events. These limitations are exacerbated by the scarcity of\nhistorical data and computational costs of longrun simulations leading to high\nuncertainty in return level estimates for successive extremes. Here, we\nintroduce a novel statistical framework leveraging recent theoretical advances\nin successive extreme value modelling in dynamical systems. Under reasonable\nassumptions of the time series data (e.g. the data follow a fat-tailed\nFr\\'{e}chet distribution), our tool allows for significantly more robust\nestimates of returns and magnitudes of successive extreme events compared to\nstandard likelihood methods. We illustrate our statistical workflow on\nscenarios of forecasted gas supply levels from 2025 to 2050. Common measures of\nstatistical accuracy are provided as benchmarks for comparison.", "AI": {"tldr": "Novel statistical framework for modeling successive extreme events in energy demand using dynamical systems theory, providing more robust estimates than traditional extreme value theory methods.", "motivation": "Traditional extreme value theory struggles with temporally clustered extremes and has high uncertainty due to data scarcity and computational costs, making accurate estimation of successive extreme events challenging for energy planning.", "method": "Leverages recent theoretical advances in successive extreme value modeling in dynamical systems, assuming time series data follows fat-tailed Fr\u00e9chet distribution, providing a statistical workflow for robust return level estimates.", "result": "The framework enables significantly more robust estimates of returns and magnitudes of successive extreme events compared to standard likelihood methods, as demonstrated on gas supply forecast scenarios from 2025-2050.", "conclusion": "The introduced statistical framework addresses limitations of traditional EVT by providing improved modeling of successive extreme events, offering better accuracy for energy resource planning with benchmarked statistical measures."}}
{"id": "2509.07700", "pdf": "https://arxiv.org/pdf/2509.07700", "abs": "https://arxiv.org/abs/2509.07700", "authors": ["Piyush Raikwar", "Anna Zaborowska", "Peter McKeown", "Renato Cardoso", "Mikolaj Piorczynski", "Kyongmin Yeo"], "title": "A Generalisable Generative Model for Multi-Detector Calorimeter Simulation", "categories": ["physics.ins-det", "hep-ex", "physics.comp-ph"], "comment": null, "summary": "Collider experiments, such as those at the Large Hadron Collider, use the\nGeant4 toolkit to simulate particle-detector interactions with high accuracy.\nHowever, these experiments increasingly require larger amounts of simulated\ndata, leading to huge computing cost. Generative machine learning methods could\noffer much faster calorimeter shower simulations by directly emulating detector\nresponses. In this work, we present CaloDiT-2, a diffusion model which uses\ntransformer blocks. As is the case for other models explored for this task, it\ncan be applied to specific geometries, however its true strength lies in its\ngeneralisation capabilities. Our approach allows pre-training on multiple\ndetectors and rapid adaptation to new ones, which we demonstrate on the LEMURS\ndataset. It reduces the effort required to develop accurate models for novel\ndetectors or detectors which are under development and have geometries that are\nchanged frequently, requiring up to 25x less data and 20x less training time.\nTo the best of our knowledge, this is the first pre-trained model to be\npublished that allows adaptation in the context of particle shower simulations,\nwith the model also included in the Geant4 toolkit. We also present results on\nbenchmarks on Dataset-2 from the community-hosted CaloChallenge, showing that\nour models provide one of the best tradeoffs between accuracy and speed from\nthe published models. Our contributions include a mechanism for the creation of\ndetector-agnostic data representations, architectural modifications suitable\nfor the data modality, a pre-training and adaptation strategy, and publicly\nreleased datasets and pre-trained models for broad use.", "AI": {"tldr": "CaloDiT-2 is a diffusion transformer model that enables fast calorimeter shower simulations with strong generalization across multiple detectors, reducing data and training requirements by up to 25x and 20x respectively.", "motivation": "Traditional Geant4 simulations for particle-detector interactions at collider experiments like LHC are computationally expensive. Generative ML methods can provide faster simulations but need to generalize across different detector geometries.", "method": "Uses a diffusion model with transformer blocks, creates detector-agnostic data representations, employs pre-training on multiple detectors with rapid adaptation to new ones using the LEMURS dataset.", "result": "Achieves one of the best accuracy-speed tradeoffs on CaloChallenge benchmarks, reduces data requirements by 25x and training time by 20x for new detectors, and becomes the first published pre-trained model for particle shower simulations integrated into Geant4.", "conclusion": "CaloDiT-2 provides an efficient solution for fast calorimeter simulations with strong generalization capabilities, significantly reducing development effort for novel or evolving detector geometries while maintaining high accuracy."}}
{"id": "2509.07575", "pdf": "https://arxiv.org/pdf/2509.07575", "abs": "https://arxiv.org/abs/2509.07575", "authors": ["Ben Andrews", "Daniel Hauer", "Jessica Slegers"], "title": "A multi-point maximum principle to prove global Harnack inequalities for Schr\u00f6dinger operators", "categories": ["math.AP"], "comment": null, "summary": "In this article, we introduce a new methodology to prove global parabolic\nHarnack inequalities on Riemannian manifolds. We focus on presenting a new\nproof of the global pointwise Harnack inequality satisfied by positive\nsolutions of the linear Schr\\\"odinger equation on a Riemannian manifold $M$\nwith nonnegative Ricci curvature, where the potential term $V$ is bounded from\nbelow. Our approach is based on a multi-point maximum principle argument.\nStandard proofs of this result (see, for instance, Li-Yau [Acta Math, 1986])\nrely on first establishing a gradient estimate. This requires the solution to\nbe at least $C^4$ on $M$. We instead prove the Harnack inequality directly,\nwhich has the advantage of avoiding higher-order derivatives of the solution in\nthe proof, enabling us to assume it is only $C^2$ on $M$. In the particular\ncase that $V$ is the quadratic potential $V(x)=|x|^2$ and $M$ is the Euclidean\nspace $\\mathbb{R}^d$, we prove a new Harnack inequality with sharper constants.\nFinally, we treat positive solutions of the Schr\\\"odinger equation with a\ngradient drift term, including applications to the Ornstein-Uhlenbeck operator\n$\\Delta - x\\cdot \\nabla$ with quadratic potential in $\\mathbb{R}^d$.", "AI": {"tldr": "New methodology for proving global parabolic Harnack inequalities on Riemannian manifolds using multi-point maximum principle, avoiding higher-order derivatives and requiring only C^2 solutions.", "motivation": "To develop a more direct proof approach for Harnack inequalities that avoids the need for gradient estimates and higher-order derivative requirements (C^4) in existing methods.", "method": "Multi-point maximum principle argument to prove Harnack inequality directly, without establishing gradient estimates first. Applied to linear Schr\u00f6dinger equation with nonnegative Ricci curvature and bounded below potential.", "result": "Successfully proves global pointwise Harnack inequality with only C^2 solution requirement. For quadratic potential in Euclidean space, achieves sharper constants. Also extends to Schr\u00f6dinger equations with gradient drift terms including Ornstein-Uhlenbeck operator.", "conclusion": "The multi-point maximum principle provides a more efficient and less restrictive approach to proving Harnack inequalities, reducing solution smoothness requirements from C^4 to C^2 while achieving improved constants in special cases."}}
{"id": "2509.07709", "pdf": "https://arxiv.org/pdf/2509.07709", "abs": "https://arxiv.org/abs/2509.07709", "authors": ["Philipp Westhoff", "Mattia Moroder", "Ulrich Schollw\u00f6ck", "Sebastian Paeckel"], "title": "A Tensor Network Framework for Lindbladian Spectra and Steady States", "categories": ["quant-ph", "cond-mat.quant-gas", "physics.comp-ph"], "comment": "12 pages main text + 9 pages supplementary information, 7 figures\n  main text + 4 figures supplementary information", "summary": "Quantum systems coupled to (non-)Markovian environments attract increasing\nattention due to their peculiar physical properties. Exciting prospects such as\nunconventional non-equilibrium phases beyond the Mermin-Wagner limit, or the\nenvironment-assisted, robust preparation of highly entangled states, demand a\nsystematic analysis of quantum many-body phases out of equilibrium. Akin to the\nequilibrium case, this requires the computation of the low-lying eigenstates of\nLindbladians, a problem challenging conventional approaches for simulating\nquantum many-body systems. Here, we undertake a first step to overcome this\nlimitation and introduce a tensor-network-based framework to compute\nsystematically not only steady states, but also low-lying excited states with\nunprecedented precision for large, driven quantum many-body systems. Our\nframework is based on recent advances utilizing complex-time Krylov spaces, and\nwe leverage these ideas to create a toolbox tailored to solve the challenging\nnon-Hermitian eigenvalue problem ubiquitous in open quantum systems. At the\nexample of the interacting Bose-Hubbard model driven by dissipation-assisted\nhopping, we demonstrate the high efficiency and accuracy, enabling us to\nperform a reliable finite-size scaling analysis of the spectral gap and\ndemonstrating the existence of anomalous relaxation. This method unlocks the\ncapability of spectral analysis of generic open quantum many-body systems,\nsuitable also for non-Markovian environments.", "AI": {"tldr": "A tensor-network framework for computing steady states and low-lying excited states of open quantum many-body systems with high precision, enabling spectral analysis of non-Markovian environments.", "motivation": "Quantum systems coupled to environments exhibit unique properties like unconventional non-equilibrium phases and environment-assisted entanglement preparation, requiring systematic analysis of quantum many-body phases out of equilibrium through Lindbladian eigenstates.", "method": "Tensor-network-based framework utilizing complex-time Krylov spaces to solve non-Hermitian eigenvalue problems in open quantum systems, demonstrated on the interacting Bose-Hubbard model with dissipation-assisted hopping.", "result": "High efficiency and accuracy demonstrated, enabling reliable finite-size scaling analysis of spectral gap and revealing anomalous relaxation phenomena in the studied system.", "conclusion": "This method unlocks spectral analysis capabilities for generic open quantum many-body systems, including those with non-Markovian environments, overcoming limitations of conventional simulation approaches."}}
{"id": "2509.07597", "pdf": "https://arxiv.org/pdf/2509.07597", "abs": "https://arxiv.org/abs/2509.07597", "authors": ["Divya Goel", "Shilpa Gupta", "Asmita Rai"], "title": "Normalized solution to Kirchhoff-fractional system involving critical Choquard nonlinearity", "categories": ["math.AP", "35B33, 35J50, 35J60, 35R11, 45K05"], "comment": null, "summary": "In this article, we explore the fractional Kirchhoff-Choquard system given by\n  $$ \\left\\{ \\begin{array}{lr} (a+b\\int_{\\mathbb{R}^N}|(-\\Delta)^{\\frac{s}{2}}\nu|^2\\;dx)(-\\Delta)^su=\\lambda_1u+(I_{\\mu}*|v|^{{2^*_{\\mu,s}}})|u|^{{2^*_{\\mu,s}}-2}u\n+\\alpha p (I_{\\mu}*|v|^{q})|u|^{p-2}u \\;\\text{in}\\;\\mathbb{R}^N,\\\\\n(a+b\\int_{\\mathbb{R}^N}|(-\\Delta)^{\\frac{s}{2}}\nv|^2\\;dx)(-\\Delta)^sv=\\lambda_2v+\n(I_{\\mu}*|u|^{{2^*_{\\mu,s}}})|v|^{{2^*_{\\mu,s}}-2}u +\\alpha\nq(I_{\\mu}*|u|^{p})|v|^{q-2}v \\;\\;\\text{in}\\;\\mathbb{R}^N,\\\\\n\\int_{\\mathbb{R}^N}|u|^2=d_1^2,\\;\\;\\int_{\\mathbb{R}^N}|v|^2=d_2^2. \\end{array}\n\\right. $$\n  where $N> 2s$, $s \\in (0,1)$, $\\mu \\in (0, N)$, $\\alpha \\in\\mathbb{R}$. Here,\n$I_{\\mu}:\\mathbb{R}^N \\to \\mathbb{R}$ denotes the Riesz potential. We denote by\n$2_{\\mu,*}:=\\frac{2N-\\mu}{N}$ and $\\frac{2N-\\mu}{N-2s}:={2^*_{\\mu,s}}$, the\nlower and upper Hardy-Littlewood-Sobolev critical exponents, repectively, and\nassume that $2_{\\mu,*} < p,q< {2^*_{\\mu,s}}$. Our primary focus is on the\nexistence of normalized solutions for the case $\\alpha>0$ in two scenarios: the\n$L^2$ subcritical case characterized by $22_{\\mu,*}<p + q < 4 +\n\\frac{4s-2\\mu}{N}$ and $L^2$ supercritical associated with\n$4+\\frac{8s-2\\mu}{N}< p + q < 2{2^*_{\\mu,s}}$.", "AI": {"tldr": "Study of normalized solutions for fractional Kirchhoff-Choquard system with Riesz potential and L^2 mass constraints", "motivation": "To investigate the existence of normalized solutions for a complex fractional Kirchhoff-Choquard system with critical exponents and mass constraints in different parameter regimes", "method": "Mathematical analysis of the fractional PDE system using variational methods, critical point theory, and careful treatment of the Hardy-Littlewood-Sobolev critical exponents", "result": "Establishes existence of normalized solutions for both L^2 subcritical case (2*2_{\u03bc,*} < p + q < 4 + (4s-2\u03bc)/N) and L^2 supercritical case (4 + (8s-2\u03bc)/N < p + q < 2*2^*_{\u03bc,s}) when \u03b1 > 0", "conclusion": "The paper successfully demonstrates the existence of normalized solutions for the fractional Kirchhoff-Choquard system in two distinct parameter regimes, contributing to the understanding of such nonlocal systems with mass constraints"}}
{"id": "2509.07580", "pdf": "https://arxiv.org/pdf/2509.07580", "abs": "https://arxiv.org/abs/2509.07580", "authors": ["Coralia Cartis", "Sadok Jerad"], "title": "On Global Rates for Regularization Methods based on Secant Derivative Approximations", "categories": ["math.OC", "cs.NA", "math.NA"], "comment": null, "summary": "An inexact framework for high-order adaptive regularization methods is\npresented, in which approximations may be used for the $p$th-order tensor,\nbased on lower-order derivatives. Between each recalculation of the $p$th-order\nderivative approximation, a high-order secant equation can be used to update\nthe $p$th-order tensor as proposed in (Welzel 2024) or the approximation can be\nkept constant in a lazy manner. When refreshing the $p$th-order tensor\napproximation after $m$ steps, an exact evaluation of the tensor or a finite\ndifference approximation can be used with an explicit discretization stepsize.\nFor all the newly adaptive regularization variants, we prove an\n$\\mathcal{O}\\left( \\max[ \\epsilon_1^{-(p+1)/p}, \\, \\epsilon_2^{(-p+1)/(p-1)} ]\n\\right)$ bound on the number of iterations needed to reach an $(\\epsilon_1, \\,\n\\epsilon_2)$ second-order stationary points. Discussions on the number of\noracle calls for each introduced variant are also provided.\n  When $p=2$, we obtain a second-order method that uses quasi-Newton\napproximations with an $\\mathcal{O}\\left(\\max[\\epsilon_1^{-3/2}, \\, \\,\n\\epsilon_2^{-3}]\\right)$ iteration bound to achieve approximate second-order\nstationarity.", "AI": {"tldr": "Inexact framework for high-order adaptive regularization methods using lower-order derivative approximations and lazy updates with proved iteration complexity bounds.", "motivation": "To develop efficient high-order optimization methods that reduce computational costs by using approximate derivatives and lazy updates while maintaining strong convergence guarantees.", "method": "Uses approximations of pth-order tensor based on lower-order derivatives, with high-order secant updates or lazy constant approximations between recalculations. Supports exact evaluations or finite difference approximations when refreshing.", "result": "Proves O(max[\u03b5\u2081^{-(p+1)/p}, \u03b5\u2082^{(-p+1)/(p-1)}]) iteration bound for reaching (\u03b5\u2081, \u03b5\u2082) second-order stationary points. For p=2, achieves O(max[\u03b5\u2081^{-3/2}, \u03b5\u2082^{-3}]) bound using quasi-Newton approximations.", "conclusion": "The framework provides computationally efficient high-order adaptive regularization methods with strong theoretical guarantees, particularly effective for second-order methods using quasi-Newton approximations."}}
{"id": "2509.07785", "pdf": "https://arxiv.org/pdf/2509.07785", "abs": "https://arxiv.org/abs/2509.07785", "authors": ["Niklas Frederik Schmitz", "Bruno Ploumhans", "Michael F. Herbst"], "title": "Algorithmic differentiation for plane-wave DFT: materials design, error control and learning model parameters", "categories": ["cond-mat.mtrl-sci", "physics.comp-ph"], "comment": "20 pages, 9 figures", "summary": "We present a differentiation framework for plane-wave density-functional\ntheory (DFT) that combines the strengths of algorithmic differentiation (AD)\nand density-functional perturbation theory (DFPT). In the resulting AD-DFPT\nframework derivatives of any DFT output quantity with respect to any input\nparameter (e.g. geometry, density functional or pseudopotential) can be\ncomputed accurately without deriving gradient expressions by hand. We implement\nAD-DFPT into the Density-Functional ToolKit (DFTK) and show its broad\napplicability. Amongst others we consider the inverse design of a semiconductor\nband gap, the learning of exchange-correlation functional parameters, or the\npropagation of DFT parameter uncertainties to relaxed structures. These\nexamples demonstrate a number of promising research avenues opened by\ngradient-driven workflows in first-principles materials modeling.", "AI": {"tldr": "AD-DFPT framework combines algorithmic differentiation and density-functional perturbation theory for automated gradient computation in DFT calculations without manual derivation.", "motivation": "To enable accurate computation of derivatives for any DFT output quantity with respect to any input parameter without the need for manual derivation of gradient expressions.", "method": "Combines algorithmic differentiation (AD) with density-functional perturbation theory (DFPT) to create an automated differentiation framework implemented in the Density-Functional ToolKit (DFTK).", "result": "Successfully demonstrated broad applicability including inverse design of semiconductor band gaps, learning exchange-correlation functional parameters, and propagating DFT parameter uncertainties to relaxed structures.", "conclusion": "The AD-DFPT framework opens promising research avenues for gradient-driven workflows in first-principles materials modeling by enabling automated and accurate derivative computations."}}
{"id": "2509.07618", "pdf": "https://arxiv.org/pdf/2509.07618", "abs": "https://arxiv.org/abs/2509.07618", "authors": ["Divya Goel", "Asmita Rai"], "title": "Normalized solutions for fractional Choquard equation with critical growth on bounded domain", "categories": ["math.AP"], "comment": null, "summary": "In this work, we establish the multiplicity of positive solutions for the\nfollowing critical fractional Choquard equation with a perturbation on the\nstar-shaped bounded domain $$ \\left\\{ \\begin{array}{lr}\n  (-\\Delta)^s u = \\lambda u +\\alpha|u|^{p-2}u+ \\left( \\int\\limits_{\\Omega}\n\\frac{|u(y)|^{2^{*}_{\\mu ,s}}}{|x-y|^ \\mu}\\, dy\\right) |u|^{2^{*}_{\\mu\n,s}-2}u\\; \\text{in} \\; \\Omega,\\\\ u>0\\; \\text{in}\\; \\Omega,\\; \\\\ u = 0\\;\n\\text{in} \\; \\mathbb{R}^{N}\\backslash\\Omega, \\\\ \\int_{\\Omega}|u|^2 dx=d,\n\\end{array} \\right. $$ where, $s\\in(0,1), N>2s$, $\\alpha\\in \\mathbb{R}$, $d>0$,\n$2<p<2^*_s:=\\frac{2N}{N-2s}$ and $2^{*}_{\\mu ,s}:=\\frac{2N-\\mu}{N-2s}$\nrepresents fractional Hardy-Littlewood-Sobolev critical exponent. Using the\nminimization technique over an appropriate set and the uniform mountain pass\ntheorem, we prove the existence of first and second solutions, respectively.", "AI": {"tldr": "Existence of multiple positive solutions for critical fractional Choquard equation with perturbation on star-shaped bounded domains using minimization and mountain pass techniques.", "motivation": "To establish multiplicity of positive solutions for critical fractional Choquard equations with perturbation terms, addressing the mathematical challenge of proving existence of multiple solutions in this complex nonlinear setting.", "method": "Using minimization technique over an appropriate set for the first solution and the uniform mountain pass theorem for the second solution.", "result": "Proved the existence of first and second positive solutions for the critical fractional Choquard equation with perturbation on star-shaped bounded domains.", "conclusion": "The paper successfully demonstrates the multiplicity of positive solutions for the given critical fractional Choquard equation using variational methods and mountain pass techniques."}}
{"id": "2509.07610", "pdf": "https://arxiv.org/pdf/2509.07610", "abs": "https://arxiv.org/abs/2509.07610", "authors": ["Ahsan Mehmood", "Ioannis Krikidis", "Ghassan M. Kraidy"], "title": "Asymmetric Modulation Design for Fluid-Antenna SWIPT Systems", "categories": ["eess.SP", "cs.NA", "math.NA"], "comment": "Accepted for Publication in Globecomm Green Communication Conference", "summary": "In this work, we propose the design of modulation schemes that improve the\nrate-energy region of fluid antenna-assisted simultaneous wireless information\nand power transfer (SWIPT) systems. By considering the nonlinear\ncharacteristics of practical energy harvesting circuits, we formulate a\ndual-objective rate-energy (RE) region optimization problem to jointly maximize\nthe discrete-input mutual information (DIMI) and harvested current. The problem\nis solved using the epsilon-constraint method and optimized constellations are\ndesigned for various energy harvesting thresholds. We then evaluate the\nperformance of the optimized constellations under three different fluid antenna\n(FA) port selection strategies: (i) Best Port, (ii) Fixed Port, and (iii)\nRandom Port. Our simulation results demonstrate significant performance gains\nof optimized constellations over conventional constellations in both\ninformation rate and energy harvesting.", "AI": {"tldr": "Optimized modulation schemes for fluid antenna SWIPT systems that jointly maximize information rate and energy harvesting, outperforming conventional constellations.", "motivation": "To improve the rate-energy region of simultaneous wireless information and power transfer systems by addressing the nonlinear characteristics of practical energy harvesting circuits.", "method": "Formulated a dual-objective optimization problem using epsilon-constraint method to maximize discrete-input mutual information and harvested current, designing optimized constellations for various energy thresholds. Evaluated three FA port selection strategies: Best Port, Fixed Port, and Random Port.", "result": "Significant performance gains in both information rate and energy harvesting compared to conventional constellations across all three port selection strategies.", "conclusion": "The proposed optimized modulation schemes effectively enhance SWIPT system performance by jointly optimizing information transmission and energy harvesting capabilities."}}
{"id": "2509.07629", "pdf": "https://arxiv.org/pdf/2509.07629", "abs": "https://arxiv.org/abs/2509.07629", "authors": ["Kyeongsu Choi", "Robert Haslhofer", "Or Hershkovits"], "title": "A gradient estimate for the linearized translator equation", "categories": ["math.AP", "math.DG"], "comment": "19 pages", "summary": "In this paper, we develop some analytic foundations for the linearized\ntranslator equation in $\\mathbb{R}^4$, i.e. in the first dimension where the\nBernstein property fails. This equation governs how the (noncompact)\nsingularity models of the mean curvature flow in $\\mathbb{R}^4$ fit together in\na common moduli space. Here, we prove a gradient estimate, which gives a sharp\nbound for $W_v$, namely for the derivative of the variation field $W$ in the\ntip region. This serves as a substitute for the fundamental quadratic concavity\nestimate from Angenent-Daskalopoulos-Sesum, which has been crucial for\ncontrolling $Y_v$, namely the derivative of the profile function $Y$ in the tip\nregion. Moreover, together with interior estimates by virtue of the linearized\ntranslator equation our gradient estimate implies a bound for $W_\\tau$ as well.\nHence, our gradient estimate also serves as substitute Hamilton's Harnack\ninequality, which has played an important role for controlling $Y_\\tau$ in the\ntip region.", "AI": {"tldr": "Develops analytic foundations for linearized translator equation in R^4, proving gradient estimates that substitute key missing estimates from lower dimensions.", "motivation": "The Bernstein property fails in R^4, requiring new analytic tools to understand how singularity models of mean curvature flow fit together in moduli space.", "method": "Proves a sharp gradient estimate for W_v (derivative of variation field) in the tip region, serving as substitute for fundamental quadratic concavity estimate and Hamilton's Harnack inequality.", "result": "Obtains sharp bound for W_v and implies bound for W_tau, providing crucial control over derivatives in the tip region where previous methods fail.", "conclusion": "The gradient estimate provides essential analytic foundations for studying linearized translator equation in R^4, enabling progress in understanding mean curvature flow singularities in this critical dimension."}}
{"id": "2509.07632", "pdf": "https://arxiv.org/pdf/2509.07632", "abs": "https://arxiv.org/abs/2509.07632", "authors": ["Tim B\u00fcrchner", "Lars Radtke", "Sascha Eisentr\u00e4ger", "Alexander D\u00fcster", "Ernst Rank", "Stefan Kollmannsberger", "Philipp Kopp"], "title": "Generalized eigenvalue stabilization for immersed explicit dynamics", "categories": ["cs.CE", "cs.NA", "math.NA"], "comment": "23 pages, 13 figures", "summary": "Explicit time integration for immersed finite element discretizations\nseverely suffers from the influence of poorly cut elements. In this\ncontribution, we propose a generalized eigenvalue stabilization (GEVS) strategy\nfor the element mass matrices of cut elements to cure their adverse impact on\nthe critical time step size of the global system. We use spectral basis\nfunctions, specifically $C^0$ continuous Lagrangian interpolation polynomials\ndefined on Gauss-Lobatto-Legendre (GLL) points, which, in combination with its\nassociated GLL quadrature rule, yield high-order convergent diagonal mass\nmatrices for uncut elements. Moreover, considering cut elements, we combine the\nproposed GEVS approach with the finite cell method (FCM) to guarantee\ndefiniteness of the system matrices. However, the proposed GEVS stabilization\ncan directly be applied to other immersed boundary finite element methods.\nNumerical experiments demonstrate that the stabilization strategy achieves\noptimal convergence rates and recovers critical time step sizes of equivalent\nboundary-conforming discretizations. This also holds in the presence of weakly\nenforced Dirichlet boundary conditions using either Nitsche's method or penalty\nformulations.", "AI": {"tldr": "A generalized eigenvalue stabilization (GEVS) strategy is proposed to address the adverse impact of poorly cut elements on critical time step size in explicit time integration for immersed finite element methods.", "motivation": "Explicit time integration for immersed finite element discretizations suffers severely from the influence of poorly cut elements, which negatively affects the critical time step size of the global system.", "method": "The method uses spectral basis functions (C^0 continuous Lagrangian interpolation polynomials on GLL points) combined with GLL quadrature, and applies GEVS stabilization to element mass matrices of cut elements. This is combined with the finite cell method to guarantee definiteness of system matrices.", "result": "Numerical experiments show that the stabilization strategy achieves optimal convergence rates and recovers critical time step sizes equivalent to boundary-conforming discretizations, even with weakly enforced Dirichlet boundary conditions using Nitsche's method or penalty formulations.", "conclusion": "The proposed GEVS approach effectively stabilizes cut elements in immersed finite element methods, enabling optimal performance in explicit time integration comparable to traditional boundary-conforming methods."}}
{"id": "2509.07638", "pdf": "https://arxiv.org/pdf/2509.07638", "abs": "https://arxiv.org/abs/2509.07638", "authors": ["Milton C. Lopes Filho", "Helena J. Nussenzveig Lopes"], "title": "Singularidades para as solu\u00e7\u00f5es das Equa\u00e7\u00f5es de Navier-Stokes and Euler e o Problema do Mil\u00eanio", "categories": ["math.AP", "35Q30, 35Q31, 35Q35"], "comment": "in Portuguese language. Notas de uma palestra plen\\'{a}ria,\n  ministrada na XI Bienal de Matem\\'{a}tica, S\\~{a}o Carlos, Julho 2024. Uma\n  vers\\~{a}o em ingl\\^{e}s est\\'{a} submetida aos Anais do congresso", "summary": "The purpose of this note is to offer a birds-eye view on the history and the\nstate-of-the-art in the research surrounding the Millenium Prize problem for\nthe Navier-Stokes equations, the general problem of singularities in fluid\ndynamics and the corresponding problem for the Euler equations. This is the\ncontent of a plenary talk delivered at the 2024 Biannual meeting of the\nBrazilian Math Society by Helena Nussenzveig Lopes and it is written in\nportuguese.", "AI": {"tldr": "Overview of research history and current state regarding the Navier-Stokes Millennium Prize problem, fluid dynamics singularities, and Euler equations problem", "motivation": "To provide a comprehensive survey of the research landscape surrounding one of the most important unsolved problems in mathematics - the Navier-Stokes existence and smoothness problem", "method": "Plenary talk presentation format delivering a birds-eye view analysis of historical developments and current research status", "result": "A Portuguese-language overview presented at the 2024 Brazilian Math Society meeting covering key developments in fluid dynamics singularity research", "conclusion": "This talk serves as an accessible introduction to the complex mathematical challenges surrounding fluid dynamics singularities and the Navier-Stokes Millennium problem"}}
{"id": "2509.07690", "pdf": "https://arxiv.org/pdf/2509.07690", "abs": "https://arxiv.org/abs/2509.07690", "authors": ["Xiaoming Chen"], "title": "HYLU: Hybrid Parallel Sparse LU Factorization", "categories": ["cs.AR", "cs.DC", "cs.MS", "cs.NA", "math.NA"], "comment": null, "summary": "This article introduces HYLU, a hybrid parallel LU factorization-based\ngeneral-purpose solver designed for efficiently solving sparse linear systems\n(Ax=b) on multi-core shared-memory architectures. The key technical feature of\nHYLU is the integration of hybrid numerical kernels so that it can adapt to\nvarious sparsity patterns of coefficient matrices. Tests on 34 sparse matrices\nfrom SuiteSparse Matrix Collection reveal that HYLU outperforms Intel MKL\nPARDISO in the numerical factorization phase by geometric means of 1.74X (for\none-time solving) and 2.26X (for repeated solving). HYLU can be downloaded from\nhttps://github.com/chenxm1986/hylu.", "AI": {"tldr": "HYLU is a hybrid parallel LU factorization solver for sparse linear systems on multi-core shared-memory systems that outperforms Intel MKL PARDISO by 1.74-2.26X.", "motivation": "To develop an efficient general-purpose solver for sparse linear systems (Ax=b) that can adapt to various sparsity patterns and perform well on multi-core shared-memory architectures.", "method": "Uses hybrid parallel LU factorization with integrated hybrid numerical kernels that adapt to different sparsity patterns of coefficient matrices.", "result": "Outperforms Intel MKL PARDISO by geometric means of 1.74X for one-time solving and 2.26X for repeated solving across 34 sparse matrices from SuiteSparse Matrix Collection.", "conclusion": "HYLU provides significant performance improvements over existing solvers and is available as open-source software for efficient sparse linear system solving."}}
{"id": "2509.07652", "pdf": "https://arxiv.org/pdf/2509.07652", "abs": "https://arxiv.org/abs/2509.07652", "authors": ["Guanwei Chen"], "title": "Non-homogeneous Schrodinger systems with sign-changing and general nonlinearities: Infinitely many solutions", "categories": ["math.AP", "35J20, 35J65"], "comment": "24 pages, 0 figures", "summary": "In this paper, we study the non-homogeneous nonlinear Schr\\\"{o}dinger system\n$$\\left\\{ \\begin{array}{ll} -\\triangle u_j+V_j(x)\nu_j=g_j(x,u_1,\\cdots,u_m)+h_j(x),& x\\in \\Omega,\\\\ \\\\ u_j:=u_j(x)=0,& x\\in\n\\partial\\Omega,\\\\ \\\\ j=1,2,\\cdots,m, \\end{array}\\right. $$ where\n$\\Omega\\subset\\mathbb{R}^{N}$ ($N\\ge2$) is a bounded smooth domain,\n$(g_1,\\cdots,g_m)$ is the gradient of $G(x,U)\\in\nC^1(\\Omega\\times\\mathbb{R}^m,\\mathbb{R})$, $G(x,U)$ may be sign-changing, and\nit is super-quadratic or asymptotically-quadratic as $|U|\\to\\infty$. We obtain\ninfinitely many solutions by using variational methods and perturbation\nmethods, and we provide several typical examples to illustrate the main\nresults. The {\\bf main novelties} are as follows. (1) The nonlinearity $G$ may\nbe sign-changing. (2) The nonlinearity $G$ is not only general, but also\nsuper-quadratic or asymptotically-quadratic at infinity and zero. (3) The\nnonlinearity $G$ is power-type or non-power-type. (4) We not only construct\nsome new conditions, but also apply some conditions used in homogeneous\nproblems to the study of non-homogeneous systems for the first time. The {\\bf\nmain difficulties} come from the following three aspects. (1) The proof of\nboundedness for $(PS)$ sequence of approximate functionals. (2) The detailed\nanalysis of the asymptotic behaviors of approximate functionals. (3) The\nestimate of the upper and lower bounds for the minimax value sequence $\\{c_k\\}$\nof the even function.", "AI": {"tldr": "Study of non-homogeneous nonlinear Schr\u00f6dinger systems with sign-changing nonlinearities, using variational and perturbation methods to obtain infinitely many solutions.", "motivation": "To address non-homogeneous nonlinear Schr\u00f6dinger systems where the nonlinearity G may be sign-changing and can be super-quadratic or asymptotically-quadratic at infinity, overcoming difficulties in proving boundedness and asymptotic behaviors.", "method": "Variational methods and perturbation methods are employed to construct solutions, with detailed analysis of approximate functionals and minimax value sequences.", "result": "Infinitely many solutions are obtained for the system, with several typical examples provided to illustrate the main results.", "conclusion": "The paper successfully extends conditions from homogeneous problems to non-homogeneous systems, handling sign-changing nonlinearities and providing new analytical techniques for such systems."}}
{"id": "2509.07731", "pdf": "https://arxiv.org/pdf/2509.07731", "abs": "https://arxiv.org/abs/2509.07731", "authors": ["Susanna Bertolini", "Alessandro Preti", "Daniele Valtorta"], "title": "Calibrated Reifenberg With Holes", "categories": ["math.AP"], "comment": null, "summary": "In this article, we study a calibrated version of Reifenberg theorem \"with\nholes\". In particular we study sets that are suitably approximable at all\npoints and scales by calibrated planes and show that, without any additional\nhypotheses on $\\beta$-numbers, this implies measure upper bounds and\nrectifiability. This article follows the main techniques introduced in a\nprevious article, but it allows for holes in the sets under consideration, and\nis more self-contained.", "AI": {"tldr": "Study of calibrated Reifenberg theorem with holes, showing that sets approximable by calibrated planes at all scales imply measure bounds and rectifiability without additional \u03b2-number hypotheses.", "motivation": "To extend Reifenberg theorem to include sets with holes while maintaining the property that approximation by calibrated planes implies rectifiability and measure bounds.", "method": "Follows techniques from previous work but allows for holes in sets, using calibrated planes for approximation at all points and scales without requiring additional \u03b2-number conditions.", "result": "Demonstrates that sets approximable by calibrated planes at all scales imply measure upper bounds and rectifiability, even when holes are present.", "conclusion": "The calibrated Reifenberg theorem with holes provides a more general framework for establishing rectifiability and measure bounds without extra hypotheses on \u03b2-numbers, making the approach more self-contained and applicable to sets with holes."}}
{"id": "2509.07862", "pdf": "https://arxiv.org/pdf/2509.07862", "abs": "https://arxiv.org/abs/2509.07862", "authors": ["Arl\u00facio Viana", "Patryk Wolejko", "Rico Zacher"], "title": "Duality estimates for subdiffusion problems including time-fractional porous medium type equations", "categories": ["math.AP", "35R11 (primary), 45K05, 76S05"], "comment": "24 pages", "summary": "We prove duality estimates for time-fractional and more general subdiffusion\nproblems. An important example is given by subdiffusive porous medium type\nequations. Our estimates can be used to prove uniqueness of weak solutions to\nsuch problems, and they allow to extend a key estimate from classical\nreaction-diffusion systems to the subdiffusive case. Besides concrete equations\ninvolving a Laplacian, we also consider abstract problems in a Hilbert space\nsetting.", "AI": {"tldr": "Duality estimates for time-fractional subdiffusion problems, including porous medium equations, enabling uniqueness proofs and extending key estimates from classical to subdiffusive systems.", "motivation": "To establish mathematical foundations for time-fractional subdiffusion problems, which are important in modeling anomalous diffusion phenomena, and to extend classical reaction-diffusion results to the fractional case.", "method": "Proving duality estimates for time-fractional and general subdiffusion problems, considering both concrete equations with Laplacian operators and abstract problems in Hilbert space settings.", "result": "Successfully derived duality estimates that can be used to prove uniqueness of weak solutions and extend key estimates from classical reaction-diffusion systems to subdiffusive cases.", "conclusion": "The developed duality estimates provide powerful mathematical tools for analyzing time-fractional subdiffusion problems, bridging the gap between classical diffusion theory and fractional calculus applications."}}
{"id": "2509.07877", "pdf": "https://arxiv.org/pdf/2509.07877", "abs": "https://arxiv.org/abs/2509.07877", "authors": ["Pierre Cardaliaguet", "Joe Jackson", "Panagiotis E. Souganidis"], "title": "Mean field control with absorption", "categories": ["math.AP", "math.OC", "math.PR"], "comment": null, "summary": "In this paper we study a mean field control problem in which particles are\nabsorbed when they reach the boundary of a smooth domain. The value of the\nN-particle problem is described by a hierarchy of Hamilton-Jacobi equations\nwhich are coupled through their boundary conditions. The value function of the\nlimiting problem; meanwhile, solves a Hamilton-Jacobi equation set on the space\nof sub-probability measures on the smooth domain, i.e. the space of\nnon-negative measures with total mass at most one. Our main contributions are\n(i) to establish a comparison principle for this novel infinite-dimensional\nHamilton-Jacobi equation and (ii) to prove that the value of the N-particle\nproblem converges in a suitable sense towards the value of the limiting problem\nas N tends to infinity.", "AI": {"tldr": "Analysis of mean field control with boundary absorption, establishing comparison principle for infinite-dimensional Hamilton-Jacobi equation and proving convergence from N-particle to limiting problem.", "motivation": "To study mean field control problems where particles are absorbed at domain boundaries, addressing the hierarchical Hamilton-Jacobi equations that arise in N-particle systems and their convergence to an infinite-dimensional limiting problem.", "method": "Established a comparison principle for the novel infinite-dimensional Hamilton-Jacobi equation defined on the space of sub-probability measures, and proved convergence of the N-particle problem value to the limiting problem value as N approaches infinity.", "result": "Successfully developed mathematical framework for boundary-absorbed mean field control, with rigorous comparison principle and convergence proof for the hierarchical system to the infinite-dimensional limit.", "conclusion": "The paper provides fundamental theoretical foundations for mean field control with boundary absorption, demonstrating the connection between finite-particle systems and their infinite-dimensional limits through Hamilton-Jacobi analysis."}}
{"id": "2509.07185", "pdf": "https://arxiv.org/pdf/2509.07185", "abs": "https://arxiv.org/abs/2509.07185", "authors": ["Jordan Cotler", "Felipe Hern\u00e1ndez"], "title": "An Egorov Theorem for Wasserstein Distances", "categories": ["quant-ph", "math-ph", "math.AP", "math.MP"], "comment": "24 pages", "summary": "We prove a new version of Egorov's theorem formulated in the Schr\\\"{o}dinger\npicture of quantum mechanics, using the $p$-Wasserstein metric applied to the\nHusimi functions of quantum states. The special case $p=1$ corresponds to a\n\"low-regularity\" Egorov theorem, while larger values $p>1$ yield progressively\nstronger estimates. As a byproduct of our analysis, we prove an optimal\ntransport inequality analogous to a result of Golse and Paul in the context of\nmean-field many-body quantum mechanics.", "AI": {"tldr": "New Egorov's theorem in quantum mechanics using p-Wasserstein metric on Husimi functions, with p=1 giving low-regularity version and p>1 providing stronger estimates.", "motivation": "To formulate a new version of Egorov's theorem in the Schr\u00f6dinger picture using optimal transport metrics, extending classical results to quantum mechanics with varying regularity conditions.", "method": "Using p-Wasserstein metric applied to Husimi functions of quantum states, with different p values (p=1 for low-regularity, p>1 for stronger estimates).", "result": "Proved new Egorov's theorem formulation and derived optimal transport inequality analogous to Golse and Paul's result in mean-field many-body quantum mechanics.", "conclusion": "The approach provides a flexible framework for Egorov-type theorems in quantum mechanics with varying regularity requirements through different p-Wasserstein metrics."}}
{"id": "2509.07420", "pdf": "https://arxiv.org/pdf/2509.07420", "abs": "https://arxiv.org/abs/2509.07420", "authors": ["Julien Brasseur"], "title": "How smooth are restrictions of Besov functions?", "categories": ["math.FA", "math.AP", "math.CA"], "comment": "Preliminary version: a longer version with additional results will\n  follow. Comments are welcome", "summary": "In a previous work, we showed that Besov spaces do not enjoy the restriction\nproperty unless $q\\leq p$. Specifically, we proved that if $p<q$, then it is\nalways possible to construct a function $f\\in B_{p,q}^s(\\mathbb{R}^N)$ such\nthat $f(\\cdot,y)\\notin B_{p,q}^s(\\mathbb{R}^d)$ for a.e. $y\\in\n\\mathbb{R}^{N-d}$, while this \"pathology\" does not happen if $q\\leq p$. We\nshowed that the partial maps belong, in fact, to the Besov space of generalised\nsmoothness $B_{p,q}^{(s,\\Psi)}(\\mathbb{R}^d)$ provided the function $\\Psi$\nsatisfies a simple summability condition involving $p$ and $q$. This short note\ncompletes the picture by showing that this characterisation is sharp.", "AI": {"tldr": "This paper completes previous work showing that Besov spaces only have the restriction property when q\u2264p, and proves that the characterization of partial maps belonging to Besov spaces of generalized smoothness is sharp.", "motivation": "To complete the characterization of restriction properties in Besov spaces, building on previous work that showed Besov spaces B_{p,q}^s only enjoy restriction properties when q\u2264p, and that partial maps belong to generalized smoothness spaces under certain conditions.", "method": "The authors build upon their previous construction showing that when p<q, functions exist in B_{p,q}^s whose restrictions fail to belong to the same space. They now prove that the characterization of partial maps belonging to Besov spaces of generalized smoothness B_{p,q}^{(s,\u03a8)} is sharp.", "result": "The paper demonstrates that the previously established characterization is sharp, meaning the conditions on the function \u03a8 (involving summability conditions with p and q) are both necessary and sufficient for the restriction property to hold in generalized Besov spaces.", "conclusion": "This work completes the understanding of restriction properties in Besov spaces, showing that the necessary and sufficient conditions for partial maps to belong to generalized smoothness spaces are optimal and cannot be improved."}}
{"id": "2509.07541", "pdf": "https://arxiv.org/pdf/2509.07541", "abs": "https://arxiv.org/abs/2509.07541", "authors": ["Chiara Meroni", "Bogdan Raita"], "title": "Semialgebric rank-one convex hulls: 2x2 triangular matrices and beyond", "categories": ["math.MG", "math.AG", "math.AP", "14P10 (Primary) 49J45, 26B25 (Secondary)"], "comment": "18 pages, comments are welcome!", "summary": "We prove that the rank-one convex hull of finitely many $2\\times 2$\ntriangular matrices is a semialgebraic set, defined by linear and quadratic\npolynomials. We present explicit constructions for five-point configurations\nand offer evidence suggesting that a similar characterization does not hold in\nthe more general setting of directional convexity.", "AI": {"tldr": "The rank-one convex hull of finitely many 2x2 triangular matrices is semialgebraic, defined by linear and quadratic polynomials, with explicit constructions for five-point configurations.", "motivation": "To characterize the rank-one convex hull of finite sets of 2x2 triangular matrices and understand its algebraic structure.", "method": "Mathematical proof showing the semialgebraic nature of the convex hull, with explicit constructions for five-point cases and analysis of directional convexity.", "result": "The rank-one convex hull is semialgebraic (defined by linear and quadratic polynomials), and evidence suggests this characterization doesn't extend to directional convexity.", "conclusion": "Finite sets of 2x2 triangular matrices have rank-one convex hulls with specific algebraic structure, but this property may not generalize to broader convexity concepts."}}
{"id": "2509.07786", "pdf": "https://arxiv.org/pdf/2509.07786", "abs": "https://arxiv.org/abs/2509.07786", "authors": ["Dachun Yang", "Wen Yuan", "Zongze Zeng"], "title": "Variable Matrix-Weighted Besov Spaces", "categories": ["math.FA", "math.AP", "math.CA", "Primary 46E35, Secondary 47A56, 15A15, 46E40, 42B35"], "comment": null, "summary": "In this article, applying matrix ${\\mathcal A}_{p(\\cdot),\\infty}$ weights\nintroduced in our previous work, we introduce the matrix-weighted variable\nBesov space via the matrix weight $W$ or the reducing operators ${\\mathbb{A}}$\nof order $p(\\cdot)$ for $W$, Then we show that, defined either by the matrix\nweight $W$ or the reducing operators ${\\mathbb{A}}$ of order $p(\\cdot)$ for\n$W$, the matrix-weighted variable Besov spaces (respectively, the\nmatrix-weighted variable Besov sequence spaces) are both equal. Next, we\nestablish the $\\varphi$-transform theorem for matrix-weighted variable Besov\nspaces and, using this, find that the definition of matrix-weighted variable\nBesov spaces is independent of the choice of $\\varphi$. After that, for the\nfurther discussion of variable Besov spaces, we establish the theorem of almost\ndiagonal operators and then, by using this, we establish the molecular\ncharacterization. Then, with applying the molecular characterization, we obtain\nthe wavelet and atomic characterizations of matrix-weighted variable Besov\nspaces. Finally, as an application, we consider some classical operators. By\nusing the wavelet characterization, we establish the trace operator and obtain\nthe theorem of trace operators. Moreover, with applying the molecular\ncharacterization, we establish the theorem of Calder\\'on--Zygmund operators on\nmatrix-weighted variable Besov spaces.", "AI": {"tldr": "This paper introduces matrix-weighted variable Besov spaces using matrix weights and reducing operators, proves their equivalence, establishes various characterizations (\u03c6-transform, molecular, wavelet, atomic), and applies these to study trace operators and Calder\u00f3n-Zygmund operators.", "motivation": "To extend the theory of Besov spaces to the matrix-weighted variable exponent setting, building on previous work with matrix A_p(\u221e) weights, and develop comprehensive characterizations and operator theory for these spaces.", "method": "The authors introduce matrix-weighted variable Besov spaces using matrix weights and reducing operators, prove their equivalence, establish \u03c6-transform theorem, develop almost diagonal operator theory, derive molecular characterizations, and obtain wavelet and atomic decompositions.", "result": "The paper shows that matrix-weighted variable Besov spaces defined via matrix weights or reducing operators are equivalent. It establishes multiple characterizations (\u03c6-transform, molecular, wavelet, atomic) and applies these to prove results about trace operators and boundedness of Calder\u00f3n-Zygmund operators.", "conclusion": "The developed framework provides a comprehensive theory for matrix-weighted variable Besov spaces with various equivalent characterizations and applications to operator theory, particularly for trace operators and Calder\u00f3n-Zygmund operators in this setting."}}
