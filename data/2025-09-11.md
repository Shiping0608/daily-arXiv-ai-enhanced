<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 16]
- [math.AP](#math.AP) [Total: 13]
- [physics.comp-ph](#physics.comp-ph) [Total: 4]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 7]
- [math.ST](#math.ST) [Total: 1]
- [math.DG](#math.DG) [Total: 1]
- [math.CA](#math.CA) [Total: 1]
- [math.FA](#math.FA) [Total: 2]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [math.OC](#math.OC) [Total: 4]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [math.PR](#math.PR) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [astro-ph.HE](#astro-ph.HE) [Total: 1]
- [physics.acc-ph](#physics.acc-ph) [Total: 1]
- [math.SP](#math.SP) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Subdivision Schemes in Metric Spaces](https://arxiv.org/abs/2509.08070)
*Nira Dyn,Nir Sharon*

Main category: math.NA

TL;DR: A unified framework for nonlinear subdivision schemes in complete metric spaces with convergence theorems and proximity methods.


<details>
  <summary>Details</summary>
Motivation: Extend subdivision theory beyond Euclidean and manifold-valued data to handle data in general metric spaces.

Method: Develop formal refinement in CMS, prove convergence under contractivity, introduce two proximity notions to relate nonlinear schemes, and provide theorems with examples.

Result: Established convergence theorems and proximity methods that work for various metric spaces including compact sets in R^n, Wasserstein space, and geometric Hermite metric space.

Conclusion: The framework successfully extends subdivision theory to complete metric spaces, enabling analysis of nonlinear schemes without relying on linear approximations.

Abstract: We develop a unified framework for nonlinear subdivision schemes on complete
metric spaces (CMS). We begin with CMS preliminaries and formalize refinement
in CMS, retaining key structural properties, such as locality. We prove a
convergence theorem under contractivity and demonstrate its applicability. To
address schemes where contractivity is unknown, we introduce two notions of
proximity. Our proximity methods relate a nonlinear scheme to another nonlinear
scheme with known contractivity, rather than to a linear scheme, as in much of
the literature. Specifically, the first type proximity compares the two schemes
after a single refinement step and, as in the classical theory, yields
convergence from sufficiently dense initial data. The proximity of the second
type monitors alignment across all refinement levels and provides strong
convergence without density assumptions. We formulate and prove the
corresponding theorems, and illustrate them with various examples, such as
schemes over metric spaces of compact sets in $\R^n$ and schemes over the
Wasserstein space, as well as a geometric Hermite metric space. These results
extend subdivision theory beyond Euclidean and manifold-valued data for data in
metric spaces.

</details>


### [2] [Tensor-Train Operator Inference](https://arxiv.org/abs/2509.08071)
*Engin Danis,Duc Truong,Kim Ø. Rasmussen§,Boian S. Alexandrov*

Main category: math.NA

TL;DR: Tensor-train framework for nonintrusive operator inference that learns discrete operators from physical governing equations using three approaches with efficient tensor-train data representation.


<details>
  <summary>Details</summary>
Motivation: To enable efficient handling of extremely large datasets in physical simulations with reduced computational effort compared to standard methods.

Method: Three tensor-train approaches: full-order tensor-train operator inference, full-order quantized tensor-train operator inference, and reduced-order tensor-train operator inference using tensor-train format representation through compression or cross interpolation.

Result: Demonstrated effectiveness through numerical experiments in Computational Fluid Dynamics, showing advantages in accuracy and scalability compared to standard reduced-order operator inference methods.

Conclusion: Tensor-train representations provide significant computational efficiency and accuracy improvements for operator inference in physical governing equations, making them suitable for large-scale simulations.

Abstract: In this study, we present a tensor--train framework for nonintrusive operator
inference aimed at learning discrete operators and using them to predict
solutions of physical governing equations. Our framework comprises three
approaches: full--order tensor--train operator inference, full--order quantized
tensor--train operator inference, and reduced--order tensor--train operator
inference. In each case, snapshot data is represented in tensor--train
format--either through compression or cross interpolation--enabling the
efficient handling of extremely large datasets with significantly reduced
computational effort compared to standard methods. The effectiveness of each
approach is demonstrated through numerical experiments related to Computational
Fluid Dynamics and benchmarked against the standard reduced--order operator
inference method, highlighting the advantages of the tensor--train
representations in both accuracy and scalability.

</details>


### [3] [An Improved Robin-Robin Coupling Method for Parabolic-Parabolic Interface Problems](https://arxiv.org/abs/2509.08103)
*Erik Burman,Miguel A. Fernandez,Johnny Guzman,Sijing Liu*

Main category: math.NA

TL;DR: Modified Robin-Robin coupling method for parabolic-parabolic interface problems achieves higher order convergence without extra assumptions


<details>
  <summary>Details</summary>
Motivation: To improve the original loosely coupled, non-iterative Robin-Robin coupling method by modifying the first step to maintain higher order convergence of error difference quantities

Method: Modified the first step of the existing Robin-Robin coupling scheme to ensure several error difference quantities achieve higher order convergence without requiring additional assumptions

Result: Numerical results demonstrate that the modified scheme successfully maintains higher order convergence as theoretically predicted

Conclusion: The proposed modification to the Robin-Robin coupling method effectively achieves higher order convergence for error difference quantities in parabolic-parabolic interface problems without imposing additional requirements

Abstract: We consider a loosely coupled, non-iterative Robin-Robin coupling method
proposed and analyzed in [Numer. Algorithms, 99:921-948, 2025] for a
parabolic-parabolic interface problem. We modify the first step of the scheme
so that several error difference quantities remain higher order convergence
without requiring additional assumptions. Numerical results are presented to
support our findings.

</details>


### [4] [Unstructured to structured: geometric multigrid on complex geometries via domain remapping](https://arxiv.org/abs/2509.08109)
*Nicolas Nytko,Scott MacLachlan,J. David Moulton,Luke N. Olson,Andrew Reisner,Matthew West*

Main category: math.NA

TL;DR: A framework that maps complex unstructured mesh domains to regular computational domains using diffeomorphisms, enabling geometric multigrid solvers on GPUs with improved efficiency, and showing that the mapping can be learned via invertible neural networks.


<details>
  <summary>Details</summary>
Motivation: Geometric multigrid solvers work well on structured meshes but face challenges with complex unstructured meshes where constructing mesh hierarchies is difficult.

Method: Proposes using diffeomorphic mappings to transform complex domains into regular computational domains, allowing geometric-style multigrid methods. Also demonstrates that invertible neural networks can learn these mappings.

Result: Enables robust geometric multigrid on unstructured meshes, improves memory access patterns for efficiency, and facilitates scalability on massively parallel processors like GPUs.

Conclusion: The framework successfully bridges the gap between complex unstructured domains and efficient geometric multigrid solvers through diffeomorphic mappings, with neural networks providing automated mapping capabilities for geometries without analytic solutions.

Abstract: For domains that are easily represented by structured meshes, robust
geometric multigrid solvers can quickly provide the numerical solution to many
discretized elliptic PDEs. However, for complicated domains with unstructured
meshes, constructing suitable hierarchies of meshes becomes challenging. We
propose a framework for mapping computations from such complex domains to
regular computational domains via diffeomorphisms, enabling the use of robust
geometric-style multigrid. This mapping facilitates regular memory accesses
during solves, improving efficiency and scalability, especially on massively
parallel processors such as GPUs. Moreover, we show that the diffeomorphic
mapping itself may be approximately learned using an invertible neural network,
facilitating automated application to geometries where no analytic mapping is
readily available.

</details>


### [5] [The Closest Point Heat Method for Solving Eikonal Equations on Implicit Surfaces](https://arxiv.org/abs/2509.08158)
*Tony Wong,Shingyu Leung,Byungjoon Lee*

Main category: math.NA

TL;DR: CPHM is a novel method that combines the heat method with closest point techniques to solve surface Eikonal equations on smooth surfaces, reducing mesh dependence while maintaining efficiency.


<details>
  <summary>Details</summary>
Motivation: To extend the classical heat method to implicit surfaces while preserving its simplicity and computational efficiency, reducing dependence on surface meshes.

Method: Integrates closest point techniques with the classical heat method framework, using an embedding approach to handle implicit surfaces while maintaining intrinsic geometric properties.

Result: Numerical experiments on benchmark geometries confirm the method's accuracy, convergence, and effectiveness on complex shapes.

Conclusion: CPHM successfully extends the heat method to implicit surfaces while preserving efficiency and geometric properties, demonstrating practical utility for complex geometries.

Abstract: We introduce the Closest Point Heat Method (CPHM), a novel approach for
solving the surface Eikonal equation on general smooth surfaces. Building on
the strengths of the classical heat method, such as simplicity of
implementation and computational efficiency, CPHM integrates closest point
techniques to reduce dependence on surface meshes. This embedding framework
naturally extends the heat method to implicit surfaces while preserving both
its efficiency and intrinsic geometric properties. Numerical experiments on
benchmark geometries confirm the accuracy and convergence of the proposed
method and demonstrate its effectiveness on complex shapes.

</details>


### [6] [Collocation and Mass Matrix in Least-squares Isogeometric Analysis](https://arxiv.org/abs/2509.08192)
*Gengchen Li,Hongwei Lin*

Main category: math.NA

TL;DR: Numerical analysis of spectral properties of collocation and mass matrices in isogeometric least-squares collocation method for Poisson problem, focusing on condition number and singular values in relation to discretization parameters.


<details>
  <summary>Details</summary>
Motivation: To systematically analyze the spectral properties of IGA-L collocation and mass matrices to understand how discretization parameters affect matrix conditioning and system stability.

Method: Comprehensive numerical investigation examining spectral properties in relation to mesh size, degree, regularity, spatial dimension, and collocation point distribution.

Result: Provides estimations for condition number, maximum and minimum singular values relative to mesh size, degree, and regularity parameters.

Conclusion: The study offers insights into optimizing collocation point distribution to achieve better-conditioned linear systems in isogeometric least-squares collocation methods.

Abstract: In this paper, we conduct a systematic numerical analysis of the spectral
properties of the collocation and mass matrices in the isogeometric
least-squares collocation method (IGA-L), for the approximation of the Poisson
problem with homogeneous Dirichlet boundary conditions. This study primarily
focuses on the spectral properties of the IGA-L collocation and mass matrices
in relation to the isogeometric discretization parameters, such as the mesh
size, degree, regularity, spatial dimension, and the number and distribution of
the collocation points. Through a comprehensive numerical investigation, we
provide estimations for the condition number, as well as the maximum and
minimum singular values, in relation to the mesh size, degree and regularity.
Moreover, in this paper we also study the effect of the number and distribution
of the collocation points on the spectral properties of the collocation matrix,
providing insights into the optimization of the collocation points for
achieving better-conditioned linear systems.

</details>


### [7] [Non-asymptotic Error Analysis of Explicit Modified Euler Methods for Superlinear and Non-contractive SODEs](https://arxiv.org/abs/2509.08410)
*Zhihui Liu,Xiaojie Wang,Xiaoming Wu,Xiaoyan Zhang*

Main category: math.NA

TL;DR: A family of explicit modified Euler methods for super-linear SODEs with multiplicative noise that preserves Lyapunov structure and achieves time-independent weak convergence rates.


<details>
  <summary>Details</summary>
Motivation: To develop numerical methods for long-time approximations of super-linear stochastic differential equations with multiplicative noise that can preserve the same stability properties as the continuous system.

Method: Construction of explicit modified Euler methods (MEMs) that preserve Lyapunov structure, with analysis under non-contractive conditions to establish non-asymptotic error bounds in Wasserstein-1 distance.

Result: Achieved time-independent weak convergence rate and obtained O(τ|ln τ|) convergence rate between exact and numerical invariant measures.

Conclusion: The proposed modified Euler methods provide effective long-time approximations for super-linear SODEs with multiplicative noise while preserving stability properties and achieving good convergence rates.

Abstract: A family of explicit modified Euler methods (MEMs) is constructed for
long-time approximations of super-linear SODEs driven by multiplicative noise.
The proposed schemes can preserve the same Lyapunov structure as the continuous
problems. Under a non-contractive condition, we establish a non-asymptotic
error bound between the law of the numerical approximation and the target
distribution in Wasserstein-1 ($\mathcal{W}_1$) distance through a
time-independent weak convergence rate for the proposed schemes. As a
by-product of this weak error estimate, we obtain an $\mathcal{O}(\tau|\ln
\tau|)$ convergence rate between the exact and numerical invariant measures.

</details>


### [8] [Accuracy analysis and optimization of scale-independent third-order WENO-Z scheme with critical-point accuracy preservation](https://arxiv.org/abs/2509.08413)
*Yunchuan Wu,Xiuzheng Cheng,Yi Duan,Linsen Zhang,Qin Li,Pan Yan,Mengyu Wang*

Main category: math.NA

TL;DR: A new WENO3-ZES4 scheme is developed that maintains third-order accuracy at first-order critical points regardless of their position, using optimized smoothness indicators and p=1 parameter for better resolution.


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing WENO3-Z schemes that fail to maintain third-order accuracy when first-order critical points occur at arbitrary grid positions and suffer from poor numerical resolution due to large exponent parameters.

Method: Developed an accuracy-optimization lemma to enhance nonlinear weights accuracy, constructed local and global smoothness indicators with specific error orders, and used a resolution-optimization lemma to determine optimal parameters balancing resolution and robustness.

Result: The WENO3-ZES4 scheme consistently achieves third-order accuracy at first-order critical points regardless of position, shows good numerical resolution, and maintains preferable robustness in 1D and 2D validation tests.

Conclusion: The proposed WENO3-ZES4 scheme successfully overcomes order degradation at critical points while maintaining good resolution and robustness, representing a significant improvement over previous WENO3-Z approaches.

Abstract: To address the order degradation at critical points in the WENO3-Z scheme,
some improvements have been proposed , but these approaches generally fail to
consider the occurrence of critical points at arbitrary positions within grid
intervals, resulting in their inability to maintain third-order accuracy when a
first-order critical point (CP1) occurs. Also, most previous improved schemes
suffer from a relatively large exponent p of the ratio of global to local
smoothness indicators, which adversely affects the numerical resolution.
Concerning these limitations, introduced here is an accuracy-optimization lemma
demonstrating that the accuracy of nonlinear weights can be enhanced providing
that smoothness indicators satisfy specific conditions, thereby establishing a
methodology for elevating the accuracy of nonlinear weights. Leveraging this
lemma, a local smoothness indicator is constructed with error terms achieving
second-order in smooth regions and fourth-order at CP1, alongside a global
smoothness indicator yielding fourth-order accuracy in smooth regions and
fifth-order at CP1, enabling the derivation of new nonlinear weights that meet
accuracy requirements even when employing p=1. Furthermore, a
resolution-optimization lemma is proposed to analyze the relationship between
parameters in local smoothness indicators and resolution. By integrating
theoretical analysis with numerical practices, free parameters in
non-normalized weights and local smoothness indicators are determined under the
balance of numerical resolution and robustness, which leads to the development
of WENO3-ZES4, a new WENO3-Z improvement that preserves the optimal order at
CP1 especially with p=1. 1D and 2D validating tests show that the new scheme
consistently achieves third-order in the case of CP1 regardless of its position
and exhibits good resolution as well as preferable robustness.

</details>


### [9] [Strong convergence of fully discrete finite element schemes for the stochastic semilinear generalized Benjamin-Bona-Mahony equation driven by additive Wiener noise](https://arxiv.org/abs/2509.08453)
*Suprio Bhar,Mrinmay Biswas,Mangala Prasad*

Main category: math.NA

TL;DR: Analysis of semi-discrete and full discretization methods for Stochastic semilinear generalized Benjamin-Bona-Mahony equation with additive Wiener noise, using FEM for spatial and semi-implicit method for temporal discretization.


<details>
  <summary>Details</summary>
Motivation: To develop and analyze numerical methods for solving stochastic semilinear generalized Benjamin-Bona-Mahony equations driven by additive Wiener noise in bounded convex polygonal domains.

Method: Finite element method for spatial discretization and semi-implicit method for time discretization, with analysis of strong convergence rates for both spatial and temporal parameters.

Result: Derived strong convergence rates with respect to both spatial and temporal discretization parameters, supported by numerical experiments that validate theoretical bounds.

Conclusion: The proposed numerical methods provide effective discretization approaches for stochastic semilinear generalized Benjamin-Bona-Mahony equations with proven convergence rates and experimental validation.

Abstract: In this article, we have analyzed semi-discrete finite element approximation
and full discretization of the Stochastic semilinear generalized
Benjamin-Bona-Mahony equation in a bounded convex polygonal domain driven by
additive Wiener noise. We use the finite element method for spatial
discretization and the semi-implicit method for time discretization and derive
a strong convergence rate with respect to both parameters (spatial and
temporal). Numerical experiments have also been performed to support
theoretical bounds.

</details>


### [10] [Restarting the Numerical Flow Iteration through low rank tensor approximations](https://arxiv.org/abs/2509.08474)
*Rostislav-Paul Wilhelm,Katharina Kormann*

Main category: math.NA

TL;DR: Proposes a restart algorithm with low-rank approximation for numerical flow iteration to reduce computational cost from quadratic to linear while maintaining memory efficiency and preserving low dissipation properties.


<details>
  <summary>Details</summary>
Motivation: The numerical flow iteration method for Vlasov-Poisson equation becomes computationally expensive for large time steps due to characteristic curve reconstruction. A restart mechanism is needed to reduce computational complexity while maintaining memory efficiency.

Method: Develops an algorithm that reconstructs low-rank representations of solutions at restart times using blackbox approximation, reducing the time intervals for characteristic curve solving.

Result: The proposed method reduces computational complexity from quadratic to linear in number of time steps while maintaining memory complexity and preserving the low dissipation property of numerical flow iteration.

Conclusion: The restart algorithm with low-rank approximation successfully addresses computational cost issues in numerical flow iteration while maintaining its advantages over semi-Lagrangian methods in filament dissipation.

Abstract: The numerical flow iteration method has recently been proposed as a
memory-slim solution method for the Vlasov-Poisson equation. It stores the
temporal evolution of the electric field and reconstructs the solution in each
time step by following the characteristics backwards in time and reconstructing
the solution from the initial distribution. If the number of time steps gets
large, the computational cost of this reconstruction may get prohibitive. Given
a representation of the intermediate solution, the time intervals over which
the characteristic curves need to be solved backwards in time can be reduced by
restarting the numerical flow iteration after certain time intervals. In this
paper, we propose an algorithm that reconstructs a low-rank representation of
the solution at the restart times using the blackbox approximation. The
proposed algorithm reduces the computational complexity compared to the pure
numerical flow iteration from quadratic to linear in the number of times step
while still keeping its memory complexity. On the other hand, our numerical
results demonstrate that the methods preserves the property of the numerical
flow iteration of showing much less dissipation of filaments compared to the
semi-Lagrangian method.

</details>


### [11] [A posteriori error analysis and adaptivity of a space-time finite element method for the wave equation in second order formulation](https://arxiv.org/abs/2509.08537)
*Zhaonan Dong,Emmanuil H. Georgoulis,Lorenzo Mascotto,Zuodong Wang*

Main category: math.NA

TL;DR: A posteriori error bounds for space-time finite element methods for linear wave problems with arbitrary order discretization, supporting dynamic mesh modification and adaptive algorithms.


<details>
  <summary>Details</summary>
Motivation: To provide rigorous error analysis for space-time finite element methods that can handle dynamic mesh changes required by adaptive algorithms, ensuring reliable error control for wave propagation problems.

Method: Combines standard finite elements in space with continuous piecewise polynomials in time and upwind discontinuous Galerkin-type approximation for second temporal derivative. Uses carefully designed temporal and spatial reconstructions to prove error bounds.

Result: Established a posteriori error bounds in L∞(L²)-norm with explicit control on constants and orders. Numerical verification shows convergence of error estimator, including mesh change effects.

Conclusion: The proposed space-time adaptive algorithm with rigorous error bounds enables reliable simulation of wave problems with dynamic mesh modifications, providing explicit error control for practical applications.

Abstract: We establish rigorous \emph{a posteriori} error bounds for a space-time
finite element method of arbitrary order discretising linear wave problems in
second order formulation. The method combines standard finite elements in space
and continuous piecewise polynomials in time with an upwind discontinuous
Galerkin-type approximation for the second temporal derivative. The proposed
scheme accepts dynamic mesh modification, as required by space-time adaptive
algorithms, resulting in a discontinuous temporal discretisation when mesh
changes occur. We prove \emph{a posteriori} error bounds in the
$L^\infty(L^2)$-norm, using carefully designed temporal and spatial
reconstructions; explicit control on the constants (including the spatial and
temporal orders of the method) in those error bounds is shown. The convergence
behaviour of an error estimator is verified numerically, also taking into
account the effect of the mesh change. A space-time adaptive algorithm is
proposed and tested numerically.

</details>


### [12] [Error Analysis of Krylov Subspace approximation Based on IDR($s$) Method for Matrix Function Bilinear Forms](https://arxiv.org/abs/2509.08563)
*Qian Qian Xue,Xiao Qiang Yue,Xian-Ming Gu*

Main category: math.NA

TL;DR: This paper presents an efficient algorithm for computing matrix function bilinear forms u^T f(A)v using IDR(s) method with error analysis and posterior error estimation.


<details>
  <summary>Details</summary>
Motivation: Matrix function bilinear forms appear in many applications, but traditional methods have high computational complexity and storage requirements. The IDR(s) method offers dimension reduction while maintaining numerical stability.

Method: Uses IDR(s) method to compute u^T f(A)v, performs error analysis to derive error expansion, and develops posterior error estimation with a stopping criterion based on the leading error term.

Result: The method effectively reduces computational complexity and storage requirements while maintaining numerical stability. The error analysis shows the leading term provides reliable posterior error estimation.

Conclusion: The proposed approach improves computational efficiency and shows excellent performance for ill-posed and large-scale problems, making it suitable for practical applications requiring matrix function bilinear forms.

Abstract: The bilinear form u^\top f(A) v of matrix functions appears in many
application problems, where u, v \in R^n\), A \in R^{n * n}\), and f(z) is a
given analytic function.The IDR(s) method effectively reduces computational
complexity and storage requirements by introducing dimension reduction
techniques, while maintaining the numerical stability of the algorithm. This
paper studies the numerical algorithm and posterior error estimation for the
matrix function bilinear form u^{\top} f(A) v based on the IDR(s) method.
Through the error analysis of the IDR(s) algorithm, the corresponding error
expansion is derived, and it is verified that the leading term of the error
expansion serves as a reliable posterior error estimate. Based on this, in this
paper a corresponding stopping criterion is proposed. This approach is
dedicated to improving computational efficiency, especially by showing
excellent performance in handling ill-posed and large-scale problems.

</details>


### [13] [Real-time CBCT reconstructions using Krylov solvers in repeated scanning procedures](https://arxiv.org/abs/2509.08574)
*Fred Vickers Hastings,S M Ragib Shahriar Islam,Malena Sabaté Landman,Sepideh Hatamikia,Carola-Bibiane Schönlieb,Ander Biguri*

Main category: math.NA

TL;DR: New efficient iterative solver for real-time CBCT reconstruction using PICCS regularization and Krylov subspace methods, particularly effective for sequential scans with local changes like in image-guided surgery.


<details>
  <summary>Details</summary>
Motivation: Need for efficient real-time CBCT reconstruction in medical applications where sequential under-sampled scans are taken on the same object with only local changes, such as tumor monitoring or surgical procedures with limited measurements for patient safety.

Method: Prior Image Constrained Compressed Sensing (PICCS) regularization combined with Krylov subspace methods, leveraging existing good initial reconstructions from previous over-sampled scans to aid subsequent reconstructions.

Result: Method effectively reduces artifacts in both synthetic and real CT data, and demonstrates faster performance compared to other common alternatives in the same setting.

Conclusion: The PICCS framework with Krylov subspace methods provides an efficient and effective solution for real-time CBCT reconstruction in sequential scanning scenarios with local changes, offering improved artifact reduction and computational speed.

Abstract: This work introduces a new efficient iterative solver for the reconstruction
of real-time cone-beam computed tomography (CBCT), which is based on the Prior
Image Constrained Compressed Sensing (PICCS) regularization and leverages the
efficiency of Krylov subspace methods. In particular, we focus on the setting
where a sequence of under-sampled CT scans are taken on the same object with
only local changes (e.g. changes in a tumour size or the introduction of a
surgical tool). This is very common, for example, in image-guided surgery,
where the amount of measurements is limited to ensure the safety of the
patient. In this case, we can also typically assume that a (good) initial
reconstruction for the solution exists, coming from a previously over-sampled
scan, so we can use this information to aid the subsequent reconstructions. The
effectiveness of this method is demonstrated in both a synthetic scan and using
real CT data, where it can be observed that the PICCS framework is very
effective for the reduction of artifacts, and that the new method is faster
than other common alternatives used in the same setting.

</details>


### [14] [Quasi-optimal time-space discretizations for a class of nonlinear parabolic PDEs](https://arxiv.org/abs/2509.08645)
*Nina Beranek,Robin Smeets,Rob Stevenson*

Main category: math.NA

TL;DR: A method for solving parabolic evolution equations with Lipschitz continuous strongly monotone operators using an equivalent system formulation and inexact Uzawa algorithm, with quasi-optimality conditions for Galerkin approximations.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient numerical approach for parabolic evolution equations with Lipschitz continuous and strongly monotone spatial operators by constructing an equivalent system that enables the use of inexact Uzawa algorithms and ensures quasi-optimal Galerkin approximations.

Method: Introduce an additional variable to construct an equivalent system where the operator becomes a Lipschitz continuous mapping from Hilbert space Y×X to its dual with Lipschitz continuous inverse. Use Galerkin discretizations solvable with inexact Uzawa algorithm. Develop quasi-optimality conditions including an inf-sup condition and an a posteriori condition for when test spaces are sufficiently large.

Result: The method produces quasi-optimal Galerkin approximations under inf-sup conditions on test and trial subspaces. An a posteriori condition is developed that guarantees quasi-optimality when the test space is sufficiently large, circumventing the inf-sup condition restriction.

Conclusion: The proposed approach successfully enables efficient numerical solution of parabolic evolution equations through system reformulation, inexact Uzawa algorithms, and flexible quasi-optimality conditions that work with sufficiently large test spaces.

Abstract: We consider parabolic evolution equations with Lipschitz continuous and
strongly monotone spatial operators. By introducing an additional variable, we
construct an equivalent system where the operator is a Lipschitz continuous
mapping from a Hilbert space $Y \times X$ to its dual, with a Lipschitz
continuous inverse. Resulting Galerkin discretizations can be solved with an
inexact Uzawa type algorithm. Quasi-optimality of the Galerkin approximations
is guaranteed under an inf-sup condition on the selected `test' and `trial'
subspaces of $Y$ and $X$. To circumvent the restriction imposed by this inf-sup
condition, an a posteriori condition for quasi-optimality is developed that is
shown to be satisfied whenever the test space is sufficiently large.

</details>


### [15] [Entropy-Stable Discontinuous Spectral-Element Methods for the Spherical Shallow Water Equations in Covariant Form](https://arxiv.org/abs/2509.08790)
*Tristan Montoya,Andrés M. Rueda-Ramírez,Gregor J. Gassner*

Main category: math.NA

TL;DR: Discontinuous spectral-element methods for rotating shallow water equations on curved manifolds that are well-balanced, mass-conservative, and energy-conservative/dissipative using skew-symmetric splitting and flux-differencing framework.


<details>
  <summary>Details</summary>
Motivation: To develop structure-preserving numerical methods for atmospheric flows and geophysical fluid dynamics on curved manifolds like spheres, which maintain conservation properties and well-balancing for numerical weather prediction and climate modeling.

Method: Skew-symmetric splitting of tensor divergence in covariant form within a flux-differencing framework using tensor-product summation-by-parts operators on unstructured quadrilateral grids.

Result: The methods achieve semi-discrete mass and energy conservation, well-balancing for arbitrary continuous bottom topographies, and energy dissipation through appropriate numerical interface flux choices, while handling geometry analytically without approximating metric terms.

Conclusion: The proposed methodology shows promise for developing full dynamical cores for numerical weather prediction and climate modeling, with potential applications to other hyperbolic and advection-dominated systems on curved manifolds.

Abstract: We introduce discontinuous spectral-element methods of arbitrary order that
are well balanced, conservative of mass, and conservative or dissipative of
total energy (i.e., a mathematical entropy function) for a covariant flux
formulation of the rotating shallow water equations with variable bottom
topography on curved manifolds such as the sphere. The proposed methods are
based on a skew-symmetric splitting of the tensor divergence in covariant form,
which we implement and analyze within a general flux-differencing framework
using tensor-product summation-by-parts operators. Such schemes are proven to
satisfy semi-discrete mass and energy conservation on general unstructured
quadrilateral grids in addition to well balancing for arbitrary continuous
bottom topographies, with energy dissipation resulting from a suitable choice
of numerical interface flux. Furthermore, the proposed covariant formulation
permits an analytical representation of the geometry and associated metric
terms while satisfying the aforementioned entropy stability, conservation, and
well-balancing properties without the need to approximate the metric terms so
as to enforce discrete metric identities. Numerical experiments on cubed-sphere
grids are presented in order to verify the schemes' structure-preservation
properties as well as to assess their accuracy and robustness within the
context of several standard test cases characteristic of idealized atmospheric
flows. Our theoretical and numerical results support the further development of
the proposed methodology towards a full dynamical core for numerical weather
prediction and climate modelling, as well as broader applications to other
hyperbolic and advection-dominated systems of partial differential equations on
curved manifolds.

</details>


### [16] [On the Lebesgue Constant of Extended-Domain Spectral Methods for Elliptic PDEs](https://arxiv.org/abs/2509.08745)
*Po-Yi Wu*

Main category: math.NA

TL;DR: Extended-domain spectral method shows stability dichotomy: unstable for self-adjoint Poisson equation but stable for non-self-adjoint convection-diffusion due to convection regularization.


<details>
  <summary>Details</summary>
Motivation: To provide rigorous stability analysis of extended-domain spectral methods for complex geometries, particularly understanding stability properties for non-normal operators which were not fully understood.

Method: Rigorous stability analysis based on Lebesgue constant, theoretical proofs for both self-adjoint Poisson equation and non-self-adjoint convection-diffusion equation, extended to multiple dimensions and variable coefficients.

Result: Found fundamental stability dichotomy - method is unstable for Poisson equation (super-polynomial Lebesgue constant growth) but stable for convection-diffusion equation (polynomial Lebesgue constant bound due to convection regularization).

Conclusion: Provides complete theoretical foundation for extended-domain spectral methods, establishing precise stability conditions and revealing non-trivial interplay between operator components.

Abstract: The extended-domain method is an appealingly simple strategy for applying
spectral methods to complex geometries, but its theoretical stability
properties, particularly for non-normal operators, are not fully understood.
This paper provides a rigorous stability analysis based on the Lebesgue
constant and reveals a fundamental stability dichotomy at the heart of the
method. We first prove a surprising result: for the self-adjoint Poisson
equation, the method is unstable, with a Lebesgue constant that grows
super-polynomially due to the ill-conditioning of spectral differentiation. In
stark contrast, we prove that for the non-self-adjoint convection-diffusion
equation, the method becomes stable. We show that the first-order convection
term regularizes the operator, leading to a provably polynomial bound on the
Lebesgue constant. These results, extended here to multiple dimensions and
variable coefficients, provide a complete theoretical foundation for this
practical method, establishing the precise conditions under which it is stable
and highlighting a non-trivial interplay between operator components.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [17] [The non-relativistic limit of scattering states for the Vlasov equation with short-range interaction potentials](https://arxiv.org/abs/2509.08072)
*Younghun Hong,Stephen Pankavich*

Main category: math.AP

TL;DR: Analysis of Vlasov equations with short-range potentials, showing global solutions from small data scatter to free flow limits, with wave operators proving relativistic scattering states converge to non-relativistic ones as c→∞


<details>
  <summary>Details</summary>
Motivation: To understand the long-time dynamics of both relativistic and non-relativistic Vlasov equations with short-range interaction potentials and establish connections between their scattering behaviors

Method: Construct global-in-time solutions from small initial data, prove scattering along forward free flow, establish wave operators, and analyze convergence of relativistic scattering states to non-relativistic counterparts in the infinite speed of light limit

Result: Successfully constructed global solutions that scatter to well-behaved limits, proved existence of wave operators, and demonstrated for the first time that relativistic scattering states converge to non-relativistic ones as c→∞

Conclusion: The study provides complete scattering theory for Vlasov equations with short-range potentials and establishes fundamental connections between relativistic and non-relativistic scattering behaviors

Abstract: We study the relativistic and non-relativistic Vlasov equation driven by
short-range interaction potentials and identify the large time dynamics of
solutions. In particular, we construct global-in-time solutions launched from
small initial data and prove that they scatter along the forward free flow to
well-behaved limits as $t \to \infty$. Moreover, we prove the existence of wave
operators for such a regime and, upon constructing the aforementioned time
asymptotic limits, use the wave operator formulation to prove for the first
time that the relativistic scattering states converge to their non-relativistic
counterparts as $c \to \infty$.

</details>


### [18] [Minimizing solutions of degenerate Allen-Cahn equations with three wells in $\mathbb{R}^2$](https://arxiv.org/abs/2509.08111)
*Lia Bronsard,Étienne Sandier,Peter Sternberg*

Main category: math.AP

TL;DR: Characterization of minimizers for vector-valued Allen-Cahn equation in R² with three-well potential, showing they depend on one variable and cannot approach all three distinct potential well values.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of minimizers in the vector-valued Allen-Cahn equation when the potential has three wells and the associated metric violates the strict triangle inequality, which is a non-standard condition in this context.

Method: Analysis of minimizers of the vector-valued Allen-Cahn equation Δu = ∇W(u) in R², assuming W has three potential wells and the degenerate metric does not satisfy the strict triangle inequality. Using coordinate transformations to show dependence on a single variable.

Result: All minimizers depend on only one variable in a suitable coordinate system. No minimizing solutions can approach all three distinct values of the potential wells simultaneously.

Conclusion: The study reveals fundamental constraints on minimizer behavior in multi-well potential systems, showing that under these specific metric conditions, solutions are effectively one-dimensional and cannot connect all three potential wells.

Abstract: We characterize all minimizers of the vector-valued Allen-Cahn equation in
$\mathbb{R}^2$ under the assumption that the potential $W$ has three wells and
that the associated degenerate metric does not satisfy the usual strict
triangle inequality. These minimizers depend on one variable only in a suitable
coordinate system.
  In particular, we show that no minimizing solutions to $ \Delta u=\nabla
W(u)$ on $\mathbb{R}^2$ can approach the three distinct values of the potential
wells.

</details>


### [19] [Pathological solutions of Navier-Stokes equations on $\mathbb{T}^2$ with gradients in Hardy spaces](https://arxiv.org/abs/2509.08168)
*Jan Burczak,Antonio Hidalgo-Torné*

Main category: math.AP

TL;DR: Construction of multiple nonzero solutions to 2D Navier-Stokes equations with vorticity gradients in Hardy space H^p for p<1, establishing p=1 as uniqueness threshold.


<details>
  <summary>Details</summary>
Motivation: To understand the uniqueness/non-uniqueness regimes for 2D Navier-Stokes equations in terms of vorticity path space regularity, specifically identifying the critical threshold in Hardy spaces.

Method: Developed theory of Hardy spaces on periodic domains and constructed multiple solutions with vorticity gradients in H^p for p in (0,1) from arbitrary smooth initial data.

Result: Demonstrated that p=1 is the critical threshold value distinguishing between non-uniqueness (p<1) and uniqueness regimes in the path space C(H^p) for vorticity.

Conclusion: The study establishes a precise regularity threshold for solution uniqueness in 2D Navier-Stokes equations, with Hardy space H^1 serving as the boundary between unique and non-unique solution behavior.

Abstract: For an arbitrary smooth initial datum, we construct multiple nonzero
solutions to the $2$d Navier-Stokes equations, with their gradients in the
Hardy space $\mathcal{H}^p$ with any $p \in (0,1)$. Thus, in terms of the path
space $C(\mathcal{H}^p)$ for vorticity, $p=1$ is the threshold value
distinguishing between non-uniqueness and uniqueness regimes. In order to
obtain our result, we develop the needed theory of Hardy spaces on periodic
domains.

</details>


### [20] [Two-dimensional steady supersonic ramp flows of Bethe-Zel'dovich-Thompson fluids](https://arxiv.org/abs/2509.08212)
*Geng Lai*

Main category: math.AP

TL;DR: Analysis of stability of self-similar composite wave solutions in supersonic ramp flows of BZT fluids, focusing on sonic shocks and their unique properties compared to ideal gases.


<details>
  <summary>Details</summary>
Motivation: To study the stability of self-similar fan-shock-fan and shock-fan-shock composite waves in supersonic BZT fluid flows, which exhibit different behavior from ideal gases due to sonic shock properties.

Method: Established a priori estimates about shock types, solved sonic shock free boundary problems, used characteristic decomposition and hodograph transformation methods to handle sonic shock singularities, and derived structural conditions for curved sonic shocks.

Result: Found that sonic shocks in BZT fluids are envelopes of wave characteristics rather than characteristics themselves, resulting in non-C^1 smooth flow at shock boundaries, and successfully established existence conditions for curved sonic shocks.

Conclusion: The study provides stability analysis and mathematical framework for understanding composite wave solutions in BZT fluid flows, overcoming challenges posed by sonic shock singularities through advanced transformation methods.

Abstract: Two-dimensional steady supersonic ramp flows are important and well-studied
flow patterns in aerodynamics. Vimercati, Kluwick and Guardone [J. Fluid Mech.,
885 (2018) 445--468] constructed various self-similar composite wave solutions
to the supersonic flow of Bethe-Zel'dovich-Thompson (BZT) fluids past
compressible and rarefactive ramps. We study the stabilities of the
self-similar fan-shock-fan and shock-fan-shock composite waves constructed by
Vimercati et al. in that paper. %In order to study the stabilities of the
composite waves, we solve some classes of shock free boundary problems. In
contrast to ideal gases, the flow downstream (or upstream) of a shock of a BZT
fluid may possibly be sonic in the sense of the flow velocity relative to the
shock front. In order to study the stabilities of the composite waves, we
establish some a priori estimates about the type of the shocks and solve some
classes of sonic shock free boundary problems. We find that the sonic shocks
are envelopes of one out of the two families of wave characteristics, and not
characteristics. This results in a fact that the flow downstream (or upstream)
a sonic shock is not $C^1$ smooth up to the shock boundary. We use a
characteristic decomposition method and a hodograph transformation method to
overcome the difficulty cased by the singularity on sonic shocks, and derive
several groups of structural conditions to establish the existence of curved
sonic shocks.

</details>


### [21] [Error estimates in the non-relativistic limit for the two-dimensional cubic Klein-Gordon equation](https://arxiv.org/abs/2509.08271)
*Yong Lu,Fangzheng Huang*

Main category: math.AP

TL;DR: Study of non-relativistic limit of 2D cubic nonlinear Klein-Gordon equation showing convergence to cubic nonlinear Schrödinger equation with rate O(ε²) for small parameter ε.


<details>
  <summary>Details</summary>
Motivation: To understand the relationship between relativistic (Klein-Gordon) and non-relativistic (Schrödinger) quantum mechanical descriptions by examining their convergence behavior in the non-relativistic limit.

Method: Reformulate nonrelativistic limit problems as stability problems in geometric optics, construct approximate solutions to arbitrary order using geometric optics techniques, and combine with decay estimates of cubic Schrödinger equation.

Result: For defocusing case with high regularity initial data: error estimates C(1+t)^N ε² up to time ε^{-⅔/(N+1)}. For limited regularity initial data: error estimates C(1+t)^Mε up to time ε^{-⅓/(M+1)}.

Conclusion: The cubic nonlinear Klein-Gordon equation converges to the cubic nonlinear Schrödinger equation in the non-relativistic limit with proven convergence rates and long-time validity estimates.

Abstract: In this paper, we study the non-relativistic limit of the two-dimensional
cubic nonlinear Klein-Gordon equation with a small parameter $0<\varepsilon \ll
1$ which is inversely proportional to the speed of light. We show the cubic
nonlinear Klein-Gordon equation converges to the cubic nonlinear
Schr\"{o}dinger equation with a convergence rate of order $O(\varepsilon^2)$.
In particular, for the defocusing case with high regularity initial data, we
show error estimates of the form $C(1+t)^N \varepsilon^2$ at time $t$ up to a
long time of order $\varepsilon^{-\frac{2}{N+1}}$, while for initial data with
limited regularity, we also show error estimates of the form
$C(1+t)^M\varepsilon$ at time $t$ up to a long time of order
$\varepsilon^{-\frac{1}{M+1}}$. Here $N$ and $M$ are constants depending on
initial data. The idea of proof is to reformulate nonrelativistic limit
problems to stability problems in geometric optics, then employ the techniques
in geometric optics to construct approximate solutions up to an arbitrary
order, and finally, together with the decay estimates of the cubic
Schr\"{o}dinger equation, derive the error estimates.

</details>


### [22] [On anisotropic energy conservation criteria of incompressible fluids](https://arxiv.org/abs/2509.08348)
*Yanqing Wang,Wei Wei,Yulin Ye*

Main category: math.AP

TL;DR: Establishes anisotropic energy conservation criteria for 3D inviscid incompressible fluids, extending previous results by allowing one velocity component in larger space L³(0,T;B^{1/3}_{3,∞}). Also generalizes Lions's criteria for viscous flows with different integrability for horizontal and vertical velocity components.


<details>
  <summary>Details</summary>
Motivation: To extend and improve existing energy conservation results for 3D incompressible fluids by leveraging divergence-free conditions and anisotropic analysis, providing more flexible and comprehensive criteria.

Method: Uses divergence-free condition to establish anisotropic energy conservation classes. For inviscid flows, extends previous results to allow one velocity component in larger function space. For viscous flows, generalizes Lions's criteria by permitting different integrability conditions for horizontal and vertical velocity components.

Result: Developed new anisotropic energy conservation criteria that: 1) Allow one velocity component to belong to L³(0,T;B^{1/3}_{3,∞}) space for inviscid 3D fluids, extending Cheskidov et al.'s result; 2) Generalize Lions's energy conservation criteria for viscous flows with different integrability requirements for horizontal vs vertical velocity components.

Conclusion: The paper successfully establishes more flexible anisotropic energy conservation conditions for both inviscid and viscous 3D incompressible fluids, providing significant extensions to classical results and enabling different mathematical treatments for different velocity components.

Abstract: In this paper, by means of divergence-free condition, we establish an
anisotropic energy conservation class enabling one component of velocity in the
largest space $L^{3} (0,T; B^{1/3}_{3,\infty})$ for the 3D inviscid
incompressible fluids, which extends the celebrated result obtained by
Cheskidov, Constantin, Friedlander and Shvydkoy in [15, Nonlinearity 21
(2008)]. For viscous flows, we generalize famous Lions's energy conservation
criteria to allow the horizontal components and vertical part of velocity to
have different integrability.

</details>


### [23] [A Liouville theorem for the $2$-Hessian equation on the Heisenberg group](https://arxiv.org/abs/2509.08415)
*Wei Zhang,Qi Zhou*

Main category: math.AP

TL;DR: Liouville theorem for 2-Hessian equation on Heisenberg group proved using test functions and integration by parts


<details>
  <summary>Details</summary>
Motivation: To establish a Liouville-type result for the 2-Hessian equation on the Heisenberg group, extending classical Liouville theorems to sub-Riemannian settings

Method: Selection of suitable test functions combined with integration by parts techniques to derive integral estimates

Result: Successful proof of the Liouville theorem for the 2-Hessian equation on the Heisenberg group

Conclusion: The approach using test functions and integration by parts is effective for proving Liouville theorems in sub-Riemannian geometry contexts

Abstract: In this paper, we prove a Liouville theorem for the $2$-Hessian equation on
the Heisenberg group $\mathbb{H}^n$. The result is obtained by choosing a
suitable test function and using integration by parts to derive the necessary
integral estimates.

</details>


### [24] [One-dimensional symmetry results for semilinear equations and inequalities on half-spaces](https://arxiv.org/abs/2509.08431)
*Nicolas Beuvin,Alberto Farina*

Main category: math.AP

TL;DR: New one-dimensional symmetry results for non-negative solutions to semilinear equations in the upper half-space, with Liouville-type theorems for differential inequalities without boundary conditions.


<details>
  <summary>Details</summary>
Motivation: To establish symmetry properties and classification results for solutions to semilinear elliptic equations in unbounded domains, particularly the upper half-space, which has applications in various PDE problems.

Method: Analytical proof techniques for semilinear elliptic equations, focusing on one-dimensional symmetry and Liouville-type theorems for non-negative solutions, possibly unbounded, without requiring boundary conditions.

Result: Proved new one-dimensional symmetry results for non-negative solutions to -Δu = f(u) in ℝⁿ₊, and established Liouville-type theorems for differential inequalities in ℝⁿ₊ even without boundary conditions, applicable to broad families of functions f including those with at least linear growth at infinity.

Conclusion: The paper provides significant advances in understanding symmetry properties of solutions to semilinear elliptic equations in half-spaces, with results that apply to a wide range of nonlinearities and work under minimal assumptions, including cases without boundary conditions.

Abstract: We prove new one-dimensional symmetry results for non-negative solutions,
possibly unbounded, to the semilinear equation $ -\Delta u= f(u)$ in the upper
half-space $\mathbb{R}^{N}_{+}$. Some Liouville-type theorems are also proven
in the case of differential inequalities in $\mathbb{R}^{N}_{+}$, even without
imposing any boundary condition. Although subject to dimensional restrictions,
our results apply to a broad family of functions $f$. In particular, they apply
to all non-negative $f$ that behaves at least linearly at infinity.

</details>


### [25] [Global behavior of the energy to the hyperbolic equation of viscoelasticity with combined power-type nonlinearities](https://arxiv.org/abs/2509.08462)
*Haiyang Lin,Jinqi Yan,Bo You*

Main category: math.AP

TL;DR: Analysis of global behavior and blow-up for viscoelastic wave equations with memory, nonlinear damping, and supercritical source terms


<details>
  <summary>Details</summary>
Motivation: To investigate the global existence, energy decay, and blow-up phenomena in viscoelastic wave equations with complex nonlinear interactions between memory effects, damping, and source terms

Method: Mathematical analysis of PDEs with Boltzmann-type memory term, nonlinear friction damping, and combined power-type nonlinear source terms using energy methods and potential well theory

Result: Global existence proven when energy sink dominates source or initial data is in suitable potential well subset; energy decay rates derived; blow-up shown when source dominates dissipation

Conclusion: The interplay between memory effects, nonlinear damping, and source terms determines solution behavior - either global existence with decay or finite-time blow-up depending on energy balance

Abstract: The main objective of this manuscript is to investigate the global behavior
of the solutions to the viscoelastic wave equation with a linear memory term of
Boltzmann type, and a nonlinear damping modeling friction, as well as a
supercritical source term which is a combined power-type nonlinearities. The
global existence of the solutions is obtained provided that the energy sink
dominates the energy source in an appropriate sense. In more general scenarios,
we prove the global existence of the solutions if the initial history value
$u_0$ is taken from a subset of a suitable potential well. Based on global
existence results, the energy decay rate is derived which depends on the
relaxation kernel as well as the growth rate of the damping term. In addition,
we study blow-up of solutions when the source is stronger than dissipation.

</details>


### [26] [Maximal regularity of Dirichlet problem for the Laplacian in Lipschitz domains](https://arxiv.org/abs/2509.08543)
*Chérif Amrouche,Mohand Moussaoui*

Main category: math.AP

TL;DR: Revisits fundamental questions about Laplace equation in bounded Lipschitz domains, challenges established results from 1980s-1990s, introduces new trace definitions, proves maximal H^{3/2} regularity, provides uniqueness criteria for harmonic functions, and refutes classical Area Integral Estimate.


<details>
  <summary>Details</summary>
Motivation: To address fundamental gaps and contradictions in the literature regarding the Dirichlet problem for Laplacian in bounded Lipschitz domains, particularly concerning trace definitions, solution regularity, and established estimates that have been widely accepted since the 1980s-1990s.

Method: Introduces new functional space E for rigorous trace definition, uses equivalent norms in fractional Sobolev spaces, applies Grisvard's results for polygons/polyhedra, provides counterexamples using explicit functions, and develops uniqueness criteria for harmonic functions.

Result: Proves maximal H^{3/2} regularity holds for all right-hand sides in dual of H^{1/2}_{00}(Ω), contradicts prevailing claims since 1990s, establishes new uniqueness results for harmonic functions, and demonstrates that classical Area Integral Estimate cannot hold as stated.

Conclusion: The paper fundamentally challenges and corrects long-standing misconceptions in the field, providing rigorous mathematical foundations for trace definitions, proving optimal regularity results, and clarifying the limitations of previously accepted estimates that have influenced the literature for decades.

Abstract: The focus of this work is on the homogeneous and non-homogeneous Dirichlet
problem for the Laplacian in bounded Lipschitz domains (BLD). Although it has
been extensively studied by many authors, we would like to return to a number
of fundamental questions and known results, such as the traces and the maximal
regularity of solutions. First, to treat non-homogeneous boundary conditions,
we rigorously define the notion of traces for non regular functions. This
approach replaces the non-tangential trace notion that has dominated the
literature since the 1980s. We identify a functional space E = \{v\in
H^{1/2}(\Omega);\nabla v\in [H^1/2(\Omega)]'\} for which the trace operator is
continuous from $E$ into $L^2(\Gamma)$. Second, we address the regularity of
solutions to the Laplace equation with homogeneous Dirichlet conditions. Using
specific equivalent norms in fractional Sobolev spaces and Grisvard's results
for polygons and polyhedral domains, we prove that maximal regularity $H^{3/2}$
holds in any BLD $\Omega$, for all right-hand sides in the dual of
$H^{1/2}_{00}(\Omega)$. This conclusion contradicts the prevailing claims in
the literature since the 1990s. Third, we describe some criteria which
establish new uniqueness results for harmonic functions in Lipschitz domains.
In particular, we show that if $u\in H^{1/2}(\Omega)$ or $u\in W^{1,
2N/(N+1)}(\Omega)$, is harmonic in $\Omega$ and vanishes on $\Gamma$, then $u=
0$. These criteria play a central role in deriving regularity properties.
Finally, we revisit the classical Area Integral Estimate. Using Grisvard's work
and an explicit function given by Necas, we show that this inequality cannot
hold in its stated form. Since this estimate has been widely used to argue that
$H^{3/2}$-regularity is unattainable for data in the dual of
$H^{1/2}_{00}(\Omega)$, our counterexample provides a decisive clarification.

</details>


### [27] [Lipschitz regularity for $p$-harmonic interface transmission problems](https://arxiv.org/abs/2509.08735)
*Marius Müller*

Main category: math.AP

TL;DR: Optimal Lipschitz regularity proved for weak solutions of measure-valued p-Poisson equation with interface transmission, extending linear case results.


<details>
  <summary>Details</summary>
Motivation: To understand nonlinear interface transmission problems and provide insights on a delicate limit case in potential theory, building on previous linear case studies.

Method: Analysis of weak solutions for the measure-valued p-Poisson equation with p in (1,2), using a compact connected C²-hypersurface without boundary and positive W²,∞-density.

Result: Proved optimal Lipschitz regularity for weak solutions of the nonlinear interface transmission problem.

Conclusion: The study successfully extends previous linear case results and provides valuable insights into a delicate limit case of potential theory, both linear and nonlinear.

Abstract: We prove optimal Lipschitz regularity for weak solutions of the
measure-valued $p$-Poisson equation $-\Delta_p u = Q \; \mathcal{H}^{n-1}
\llcorner \Gamma$. Here $p \in (1,2)$, $\Gamma$ is a compact and connected
$C^2$-hypersurface without boundary, and $Q$ is a positive
$W^{2,\infty}$-density. This equation can be understood as a nonlinear
interface transmission problem. Our main result extends previous studies of the
linear case and provides further insights on a delicate limit case of (linear
and nonlinear) potential theory.

</details>


### [28] [Existence of minimizers for interaction energies with external potentials](https://arxiv.org/abs/2509.08761)
*Ruiwen Shu*

Main category: math.AP

TL;DR: Existence and uniqueness of minimizers for interaction energies with external potentials, focusing on subharmonic interaction potentials including Riesz potentials and anisotropic versions in various domains.


<details>
  <summary>Details</summary>
Motivation: To establish sufficient and almost necessary conditions for the existence and uniqueness of minimizers in interaction energy problems with external potentials, extending previous results and providing practical criteria.

Method: Based on the observation that Euler-Lagrange conditions for energy minimizers are nearly identical to those for maximizers of the height functional (essential infimum of generated potential), with analysis of subharmonic interaction potentials.

Result: Provides sufficient and almost necessary conditions for minimizer existence and uniqueness, along with a simple sufficient condition for general interaction/external potentials and an improvement to existing results without external potentials.

Conclusion: The paper establishes comprehensive conditions for minimizer existence and uniqueness in interaction energy problems with external potentials, offering both theoretical insights and practical criteria for a broad class of subharmonic interaction potentials.

Abstract: In this paper we study the existence of minimizers for interaction energies
with the presence of external potentials. We consider a class of subharmonic
interaction potentials, which include the Riesz potentials $|{\bf
x}|^{-s},\,\max\{0,d-2\}<s<d$ and its anisotropic counterparts. The underlying
space is taken as $\mathbb{R}^d$ or a half-space with possibly curved boundary.
We give a sufficient and almost necessary condition for the existence of
minimizers, as well as the uniqueness of minimizers. The proof is based on the
observation that the Euler-Lagrange condition for the energy minimizer is
almost the same as that for the maximizer of the height functional, defined as
the essential infimum of the generated potential. We also give two
complimentary results: a simple sufficient condition for the existence of
minimizers for general interaction/external potentials, and a slight
improvement to the known result on the existence of minimizers without external
potentials.

</details>


### [29] [Sharp power concavity of two relevant free boundary problems of reaction-diffusion type](https://arxiv.org/abs/2509.08768)
*Qingyou He*

Main category: math.AP

TL;DR: Comparison of power concavity properties between porous medium reaction-diffusion equation and Hele-Shaw problem pressures, showing different concavity preservation patterns and establishing sharp spatial Lipschitz regularity.


<details>
  <summary>Details</summary>
Motivation: To investigate and compare the sharp power concavity properties of pressures in two related free boundary problems - the porous medium type reaction-diffusion equation and the Hele-Shaw problem - which are connected through the incompressible limit.

Method: Analyzing the preservation of α-concavity for different α values in both problems, comparing their mathematical properties, and establishing non-degenerate estimates using derived concave properties.

Result: For porous medium equation, only 1/2-concavity persists over time while other α-concavities (α∈[0,1/2)∪(1/2,1]) do not. For Hele-Shaw problem, α-concavity with α∈[0,1/2] is maintained, with 1/2 as the largest index. Sharp spatial Lipschitz regularity is established.

Conclusion: The Hele-Shaw problem, despite being the incompressible limit of the porous medium equation, behaves differently due to not being a degenerate parabolic equation. The 1/2-concavity represents a fundamental mathematical property distinguishing these two related free boundary problems.

Abstract: The porous medium type reaction-diffusion equation and the Hele-Shaw problem
are two free boundary problems linked through the incompressible (Hele-Shaw)
limit. We investigate and compare the sharp power concavities of the pressures
on their respective supports for the two free boundary problems. For the
pressure of the porous medium type reaction-diffusion equation, the
$\frac{1}{2}$-concavity preserves all the time, while $\alpha$-concavity for
$\alpha\in[0,\frac{1}{2})\cup(\frac{1}{2},1]$ does not persist in time. In
contrast, in the case of the pressure for the Hele-Shaw problem,
$\alpha$-concavity with $\alpha\in[0,\frac{1}{2}]$ is maintained all the while
and $\frac{1}{2}$ acts as the largest index. The intuitive explanation for the
difference between the two free boundary problems is that, although the
Hele-Shaw problem is the incompressible limit of the porous medium-type
reaction-diffusion equation, it is no longer a degenerate parabolic equation.
Furthermore, for the pressure of the porous medium type reaction-diffusion
equation, the non-degenerate estimate is established by means of the derived
concave properties, indicating that the spatial Lipschitz regularity in the
whole space is sharp.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [30] [Residence-time theory applied to circulating-fuel reactors: zero-power analysis](https://arxiv.org/abs/2509.07989)
*Lubomír Bureš*

Main category: physics.comp-ph

TL;DR: This paper develops a residence-time theory framework to model delayed-neutron precursor drift and recirculation effects in circulating-fuel reactors, providing closed-form expressions for reactivity loss and transfer functions that work across different mixing regimes.


<details>
  <summary>Details</summary>
Motivation: Circulating-fuel reactors experience reactivity changes when delayed-neutron precursors drift out of and re-enter the core, requiring a physics-based model to understand and predict these effects for reactor design and safety analysis.

Method: Combines DNP transport with residence-time theory, treating core and ex-core regions as two mixing volumes in series to derive closed-form expressions for static reactivity loss and zero-power transfer function.

Result: The model reproduces MSRE measurements (0.32 $ reactivity loss) and frequency response, shows 20% of DNP worth from recirculation, and agrees with high-fidelity Serpent-2/CFD calculations for EVOL MSFR. Most impact occurs when core/ex-core residence times are comparable.

Conclusion: The residence-time approach provides a computationally efficient yet versatile tool for sensitivity studies and developing physical intuition about CFR behavior, with potential for extensions to importance weighting and time-domain analysis.

Abstract: Circulating-fuel reactors (CFRs) lose reactivity when delayed-neutron
precursors (DNPs) drift out of the core and may regain part of it when the fuel
re-enters the core. This paper formulates a physics-based description of both
effects by combining DNP transport with residence-time theory. Then, treating
the core and ex-core regions as two mixing volumes in series, closed-form
expressions for (i) the static reactivity loss due to precursor drift and (ii)
the zero-power transfer function that governs linearised dynamics are derived.
When the gamma residence-time distributions are used, the new framework is
shown to reduce to the plug-flow and Continuous-Stirred-Tank-Reactor limits as
special cases, while generalising to intermediate mixing regimes via a single
parameter: the degree of mixing. Performed parameter studies show that DNP
recirculation has the highest impact when core and ex-core residence times are
comparable and the product of the DNP decay constant and the in-core residence
time is small. Benchmarks against the Molten-Salt Reactor Experiment are able
to reproduce the measured static loss ($k_0 \approx 0.32$ \$) and its frequency
response, with $\approx$20% of the steady-state DNP worth arising from
recirculation. Additionally, for the EVOL reference Molten-Salt Fast Reactor
the model is shown to agree well with the results of high-fidelity Serpent-2
calculations coupled with Computational Fluid Dynamics. Overall, the
residence-time approach offers a computationally light yet versatile tool for
sensitivity studies and generation of physical intuition for the behaviour of
CFRs. Foundation for extensions to importance weighting of DNPs and application
of the framework to time-domain analysis is also briefly sketched.

</details>


### [31] [DDNet: A Unified Physics-Informed Deep Learning Framework for Semiconductor Device Modeling](https://arxiv.org/abs/2509.08073)
*Roberto Riganti,Matteo G. C. Alasio,Enrico Bellotti,Luca Dal Negro*

Main category: physics.comp-ph

TL;DR: DDNet is a physics-informed neural network that solves both forward and inverse drift-diffusion equations for semiconductor device modeling with high accuracy and minimal computational overhead.


<details>
  <summary>Details</summary>
Motivation: Traditional semiconductor simulation tools are good for forward modeling but unsuitable for inverse problems like determining doping profiles and material parameters from desired device performance.

Method: Developed Drift-Diffusion Network (DDNet) - a unified physics-informed deep learning solver for mesh-free solutions of drift-diffusion equations in 1D and 2D device configurations.

Result: DDNet achieves low absolute and relative error compared to traditional simulation software while solving inverse problems with minimal computational overhead.

Conclusion: DDNet will benefit semiconductor device modeling by enabling automated exploration and discovery of novel device structures across comprehensive parameter sets.

Abstract: The accurate modeling of semiconductor devices plays a critical role in the
development of new technology nodes and next-generation devices. Semiconductor
device designers largely rely on advanced simulation software to solve the
drift-diffusion equations, a coupled system of nonlinear partial differential
equations that describe carrier transport in semiconductor devices. While these
tools perform well for forward modeling, they are not suitable to address
inverse problems, for example, determining doping profiles, material, and
geometrical parameters given a desired device performance. Meanwhile,
physics-informed neural networks (PINNs) have grown in popularity in recent
years thanks to their ability to efficiently and accurately solve inverse
problems at minimal computational cost compared to forward problems. In this
study, we introduce the Drift-Diffusion Network (DDNet), a unified
physics-informed deep learning solver for the forward and inverse mesh-free
solutions of the drift-diffusion equations of semiconductor device modeling.
Using prototypical device configurations in one- and two spatial dimensions, we
show that DDNet achieves low absolute and relative error compared to
traditional simulation software while additionally solving user-defined inverse
problems with minimal computational overhead. We expect that DDNet will benefit
semiconductor device modeling by facilitating exploration and discovery of
novel device structures across comprehensive parameter sets in a fully
automated way.

</details>


### [32] [Generative Quasi-Continuum Modeling of Confined Fluids at the Nanoscale](https://arxiv.org/abs/2509.08223)
*Bugra Yalcin,Ishan Nadkarni,Jinu Jeong,Chenxing Liang,Narayana R. Aluru*

Main category: physics.comp-ph

TL;DR: A multiscale framework using conditional diffusion models to predict nanoscale confined fluid density profiles with ab initio accuracy but orders-of-magnitude faster than traditional methods.


<details>
  <summary>Details</summary>
Motivation: Accurate density estimation for confined fluids requires prohibitively long AIMD simulations, and while MLMD offers better scalability, it's still constrained by femtosecond timesteps limiting practical long-time averaging.

Method: Conditional denoising diffusion probabilistic model (DDPM) that predicts long-time force profiles from noisy AIMD data, coupled with continuum theory via Nernst-Planck equation to derive density behavior.

Result: Successfully predicts density profiles for water confined in graphene slits outside training domain with ab initio accuracy, achieving orders-of-magnitude speed-up over AIMD/MLMD with significantly less training data.

Conclusion: The proposed quasi-continuum approach provides an efficient and data-effective solution for predicting nanoscale confined fluid behavior, bridging molecular dynamics with continuum theory for practical applications.

Abstract: We present a data-efficient, multiscale framework for predicting the density
profiles of confined fluids at the nanoscale. While accurate density estimates
require prohibitively long timescales that are inaccessible by ab initio
molecular dynamics (AIMD) simulations, machine-learned molecular dynamics
(MLMD) offers a scalable alternative, enabling the generation of force
predictions at ab initio accuracy with reduced computational cost. However,
despite their efficiency, MLMD simulations remain constrained by femtosecond
timesteps, which limit their practicality for computing long-time averages
needed for accurate density estimation. To address this, we propose a
conditional denoising diffusion probabilistic model (DDPM) based
quasi-continuum approach that predicts the long-time behavior of force profiles
along the confinement direction, conditioned on noisy forces extracted from a
limited AIMD dataset. The predicted smooth forces are then linked to continuum
theory via the Nernst-Planck equation to reveal the underlying density
behavior. We test the framework on water confined between two graphene
nanoscale slits and demonstrate that density profiles for channel widths
outside of the training domain can be recovered with ab initio accuracy.
Compared to AIMD and MLMD simulations, our method achieves orders-of-magnitude
speed-up in runtime and requires significantly less training data than prior
works.

</details>


### [33] [PCGBandit: One-shot acceleration of transient PDE solvers via online-learned preconditioners](https://arxiv.org/abs/2509.08765)
*Mikhail Khodak,Min Ki Jung,Brian Wynne,Edmond chow,Egemen Kolemen*

Main category: physics.comp-ph

TL;DR: PCGBandit - a bandit algorithm that learns adaptive solver configurations from linear solver feedback to accelerate PDE simulations in one-shot without requiring classical simulation training data.


<details>
  <summary>Details</summary>
Motivation: Traditional ML approaches for accelerating scientific computing require classical simulations for training and face data-hungriness issues. The paper proposes an alternative paradigm where the solver's own data is used for real-time acceleration.

Method: Uses a bandit algorithm to online-learn adaptive solver configurations (preconditioners) from feedback during repeated calls to preconditioned conjugate gradient (PCG) solver when solving sequences of related linear systems in transient PDEs.

Result: Implemented on OpenFOAM and demonstrated effectiveness on fluid and magnetohydrodynamics (MHD) problems, showing one-shot speedup of simulations.

Conclusion: PCGBandit provides a practical alternative to data-hungry ML methods by enabling real-time acceleration of PDE simulations using the solver's own feedback data, with successful implementation on industry-standard OpenFOAM software.

Abstract: Data-driven acceleration of scientific computing workflows has been a
high-profile aim of machine learning (ML) for science, with numerical
simulation of transient partial differential equations (PDEs) being one of the
main applications. The focus thus far has been on methods that require
classical simulations to train, which when combined with the data-hungriness
and optimization challenges of neural networks has caused difficulties in
demonstrating a convincing advantage against strong classical baselines. We
consider an alternative paradigm in which the learner uses a classical solver's
own data to accelerate it, enabling a one-shot speedup of the simulation.
Concretely, since transient PDEs often require solving a sequence of related
linear systems, the feedback from repeated calls to a linear solver such as
preconditioned conjugate gradient (PCG) can be used by a bandit algorithm to
online-learn an adaptive sequence of solver configurations (e.g.
preconditioners). The method we develop, PCGBandit, is implemented directly on
top of the popular open source software OpenFOAM, which we use to show its
effectiveness on a set of fluid and magnetohydrodynamics (MHD) problems.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [34] [Control of a Uniformly Magnetized Plasma with External Electric Fields](https://arxiv.org/abs/2509.07988)
*Peiyi Chen,Rogerio Jorge,Qin Li,Yukun Yue*

Main category: physics.plasm-ph

TL;DR: Analysis of plasma stability control using linear analysis and Penrose condition, with numerical verification and control strategy examples.


<details>
  <summary>Details</summary>
Motivation: Plasma stability is challenging, especially in uniform external magnetic fields, requiring effective control strategies for instabilities.

Method: Linear analysis via Laplace-Fourier transform, derivation of Penrose condition for Bernstein modes, numerical verification, and development of general control strategy.

Result: Successfully characterized plasma stability conditions, verified numerically, and demonstrated control of instabilities including Gaussian equilibria and Dory-Guest-Harris instability.

Conclusion: The developed linear analysis and control strategy provide effective tools for managing plasma instabilities in magnetic fields, with practical applications demonstrated through various examples.

Abstract: The stability of plasmas is a challenging topic. We study a control problem
for plasma living in a uniform external magnetic field. Linear analysis for any
equilibrium $\mu$ is investigated through the Laplace-Fourier transform
approach. Moreover, the Penrose condition, which characterizes the stability of
equilibrium, is derived for Bernstein modes and verified numerically. Based on
the linear analysis, a general control strategy is introduced, with recovering
the free-streaming solution as a specific example. We finally present several
examples to control instabilities, including Gaussian equilibria and
Dory-Guest-Harris instability.

</details>


### [35] [Modeling Low-Temperature Plasmas Simulating Titan's Atmosphere](https://arxiv.org/abs/2509.08063)
*David Dubois,Alexander W. Raymond,Ella Sciamma-O'Brien,Farid Salama*

Main category: physics.plasm-ph

TL;DR: Modeling plasma-induced gas phase chemistry in Titan-like N2-CH4-C2H2 mixtures using CO-PRISM, showing cationic pathways drive tholin production with C2H2 enhancing C/N ratios.


<details>
  <summary>Details</summary>
Motivation: To understand gas phase chemistry in cold planetary environments like Titan using plasma discharge simulations, particularly investigating how C2H2 influences polymeric growth and elemental composition.

Method: Used 1D multi-fluid plasma model (CO-PRISM) with extensive chemical reaction network including neutral-neutral, ion-neutral reactions, excited nitrogen states, and updated electron collision cross-sections for N2-CH4-C2H2 mixtures.

Result: Model consistent with COSmIC experiments: produced C6Hx intermediates, methanimine, and showed cationic pathways enable efficient nitrogen-rich tholin production. C2H2 presence increases C/N ratio in both gas and solid phases.

Conclusion: Synergistic low-temperature plasma experiments combined with modeling are crucial for understanding cold planetary chemistry, particularly Titan's atmospheric processes and tholin formation mechanisms.

Abstract: In the study presented here, we model the gas phase chemistry induced by
plasma discharge at low temperature (150 K) in the NASA Ames COSmIC Simulation
Chamber (COSmIC) using a 1-dimensional multi-fluid plasma model named CO-PRISM
(COSmIC Plasma Reactivity and Ionization Simulation Model). Our model
incorporates an extensive chemical reaction network to simulate the
neutral-neutral and ion-neutral reactions occurring in the COSmIC experiments
when using N2-CH4-based gas mixtures relevant to Titan's atmosphere. Our
reaction network now includes crucial reactions involving the first
electronically-excited state of atomic nitrogen, recent electron collision
cross-sections, and radical chemistry. In particular, we have investigated the
influence of C2H2 on the gas phase polymeric growth and the elemental
composition of the chemical products, and we have compared our findings to
recently published solid phase analyses. The modeling results are consistent
with experimental measurements of N2-CH4-C2H2 plasmas on COSmIC, showing the
production of C6Hx intermediates and precursors of larger organics, as well as
methanimine in small concentration. Our numerical results point to cationic
pathways enabling efficient intermediate-sized and nitrogen-rich
\ce{C2H2}-driven chemistry driving tholin production. Comparison of the modeled
gas phase elemental composition with elemental composition of the solid phase
samples produced in COSmIC reveal similar trends, with C/N increasing when C2H2
is present in the gas mixture. Finally, our results demonstrate the importance
of such synergistic studies using low-temperature plasma chemistry experiments
combined with modeling efforts to improve our understanding of cold planetary
environments.

</details>


### [36] [Particle-In-Cell Informed Kinetic Modeling of Nonlinear Skin Effects in Low-Frequency Inductively Coupled Plasmas](https://arxiv.org/abs/2509.08081)
*Haomin Sun,Jian Chen,Alexander Khrabrov,Igor D. Kaganovich,Wei Yang,Dmytro Sydorenko,Stephan Brunner*

Main category: physics.plasm-ph

TL;DR: New kinetic theory for low-frequency ICP discharges reveals electron trapping in RF magnetic fields, predicting jet-like currents and periodic energy bursts, validated by 2D PIC simulations.


<details>
  <summary>Details</summary>
Motivation: To understand electron behavior in low-frequency inductively coupled plasma discharges and develop a predictive theory for electron currents and energy deposition.

Method: Extensive 2D Particle-In-Cell electromagnetic simulations combined with development of new kinetic theory integrating Vlasov equation along unperturbed particle trajectories, coupled with global modeling.

Result: Discovery of electron trapping in combined vector/electrostatic potential wells, jet-like currents, periodic energy bursts, and successful prediction of electron current evolution and nonlinear current-field relationships.

Conclusion: The new kinetic theory provides accurate predictions for low-frequency ICP behavior and offers straightforward methods for estimating key plasma parameters, with good agreement to simulations and potential for experimental validation.

Abstract: We perform extensive 2D Particle-In-Cell (PIC) electromagnetic simulations of
low pressure Inductively Coupled Plasma (ICP) discharges with various coil
current and driving frequencies. Our simulations show that in low-frequency
cases, electrons in the skin region near the coil can be predominantly
magnetized by the Radio Frequency (RF) magnetic field. More specifically, the
electrons are trapped in the combined potential well formed by the vector and
electrostatic potentials, where they oscillate for most of the RF period while
drifting perpendicular to the RF magnetic field. When the magnetic field
weakens, electrons shortly demagnetize, leading to jet-like currents and
periodic bursts of energy deposition. Based on the newly discovered electron
trajectories, we develop a new kinetic theory for the plasma skin effect in
low-frequency Inductively Coupled Plasma (ICP) discharges, incorporating
nonlinear electron motion in an RF magnetic field by integrating Vlasov
equation along the unperturbed particle trajectory. This theory successfully
predicts the time evolution of electron currents in low-frequency ICP plasmas,
as well as a nonlinear relation between electron current and the RF inductive
electric field in the new regime we found. Furthermore, by coupling this new
kinetic theory with a global model, we provide a straightforward method for
estimating equilibrium electron temperature, plasma density and electron
current. These analytical predictions match well with our 2D PIC simulations
and can be validated through future experimental studies.

</details>


### [37] [Geometrical optics in phase space](https://arxiv.org/abs/2509.08098)
*I. Y. Dodin,N. A. Lopez,Tingjing Xing,Rune Højlund Marholt,Valerian H. Hall-Chen*

Main category: physics.plasm-ph

TL;DR: MGO (generalised metaplectic GO) overcomes the amplitude singularity issue in traditional geometrical optics near reflection points by using ray time and energy as canonical coordinates instead of physical space and wavevector.


<details>
  <summary>Details</summary>
Motivation: Traditional geometrical optics fails near reflection points where it predicts spurious singularities in wave amplitude, limiting its practical applications in plasma wave modeling.

Method: Uses ray time τ as canonical coordinate and ray energy h as canonical momentum. Constructs Weyl symbol calculus on (τ, h) space with Airy transform to relate to (x, k) counterparts. Derives envelope equation and maps solutions using generalized metaplectic transform.

Result: Successfully reproduces standard Airy patterns in regions where conventional GO fails. Eliminates amplitude singularity problem at reflection points.

Conclusion: MGO offers a promising tool for reduced modeling of wave phenomena in plasmas, particularly for O-X conversion near critical density, and can replace traditional GO for practical purposes with better reflection handling.

Abstract: Geometrical optics (GO) is widely used for reduced modeling of waves in
plasmas but fails near reflection points, where it predicts a spurious
singularity of the wave amplitude. We show how to avoid this singularity by
adopting a different representation of the wave equation. Instead of the
physical space $x$ and the wavevector $k$, we use the ray time $\tau$ as the
new canonical coordinate and the ray energy $h$ as the associated canonical
momentum. To derive the envelope equation in the $\tau$-representation, we
construct the Weyl symbol calculus on the $(\tau, h)$ space and show that the
corresponding Weyl symbols are related to their $(x, k)$ counterparts by the
Airy transform. This allows us to express the coefficients in the envelope
equation through the known properties of the original dispersion operator. When
necessary, solutions of this equation can be mapped to the $x$-space using a
generalised metaplectic transform. But the field per se might not even be
needed in practice. Instead, knowing the corresponding Wigner function usually
suffices for linear and quasilinear calculations. As a Weyl symbol itself, the
Wigner function can be mapped analytically, using the aforementioned Airy
transform. We show that the standard Airy patterns that form in regions where
conventional GO fails are successfully reproduced within MGO simply by
remapping the field from the $\tau$-space to the $x$-space. An extension to
mode-converting waves is also presented. This formulation, which we call
generalised metaplectic GO (MGO) offers a promising tool, for example, for
reduced modeling of the O--X conversion in inhomogeneous plasma near the
critical density, an effect that is important for fusion applications and also
occurs in the ionosphere. Aside from better handling reflection, MGO is similar
to GO and can replace it for any practical purposes.

</details>


### [38] [Proton-Acoustic Wave Effects on the Relaxation of Proton Transverse Heating in Magnetized Plasmas](https://arxiv.org/abs/2509.08106)
*Martín A. Quijada,Pablo S. Moya,Roberto E. Navarro*

Main category: physics.plasm-ph

TL;DR: Quasilinear analysis shows Alfvén-cyclotron waves can drive proton heating and trigger cyclotron instabilities, but ion-acoustic waves regulate this process by absorbing energy through Landau damping, especially at higher electron-to-proton temperature ratios.


<details>
  <summary>Details</summary>
Motivation: To understand how resonant wave-particle interactions break the linear decoupling between transverse electromagnetic and electrostatic plasma waves, and to track the coupled evolution of Alfvén-cyclotron and ion-acoustic waves in collisionless plasmas.

Method: Solving moment-based quasilinear equations for collisionless plasma with bi-Maxwellian protons and Maxwellian electrons, retaining full kinetic response of both species and treating electrons as thermal reservoir to isolate proton heating. Parameter survey over β∥p (0.01-10) and Te/Tp (1-10).

Result: ACWs drive significant perpendicular proton heating and raise temperature anisotropy at low β∥p, triggering cyclotron instabilities. System self-regulates toward quasi-stationary state. As Te/Tp increases, IAWs absorb more energy through Landau resonance, reducing ACW-driven proton heating efficiency. For large β∥p or Te/Tp ≥ 5, IAWs become dominant dissipation channel.

Conclusion: Modest electrostatic activity in low-β environments regulates but cannot indefinitely sustain cyclotron instabilities, explaining wave behavior in inner heliosphere and planetary magnetosheaths.

Abstract: Transverse electromagnetic and electrostatic plasma wave modes propagating
along a background magnetic field $\vec{B}_0$ are independent according to
linear kinetic theory. However, resonant interactions and energy exchange
between waves and particles break this linear decoupling. This work tracks the
coupled evolution of Alfv\'en-cyclotron (ACWs) and Ion-acoustic waves (IAWs) by
solving moment-based quasilinear equations for a collisionless plasma of
bi-Maxwellian protons and Maxwellian electrons. Unlike earlier quasilinear
studies that adopt the cold-electron limit, our formulation retains the full
kinetic response of both species, treating the electrons as a thermal reservoir
to isolate proton heating. A parameter survey over $0.01\leq\beta_{\parallel
p}\leq10$ and $1\le T_e/T_p\le10$ shows that an ambient spectrum of ACWs can
drive significant perpendicular proton heating and raise the temperature
anisotropy from initially isotropic conditions at low $\beta_{\parallel
p}\lesssim0.1$, thereby triggering cyclotron instabilities. The quasilinear
evolution self-regulates the ACW, driving the system toward a quasi-stationary
state with $\gamma/\Omega_p<10^{-1}$ and reduced anisotropy. As $T_e/T_p$
increases, IAWs become less damped and absorb a larger share of the fluctuation
energy through Landau resonance, reducing the efficiency of ACW-driven proton
heating and thus regulating the instability. For sufficiently large
$\beta_{\parallel p}$ or $T_e/T_p\gtrsim5$, ACWs become inefficient drivers of
perpendicular heating, leaving IAWs as the dominant dissipation channel. These
results explain how modest electrostatic activity in low-$\beta$ environments
such as the inner heliosphere and planetary magnetosheaths can regulate, but
not indefinitely sustain, cyclotron instabilities.

</details>


### [39] [Modeling of convective cells, turbulence, and transport induced by a radio-frequency antenna in the tokamak boundary plasma](https://arxiv.org/abs/2509.08178)
*M. V. Umansky,B. D. Dudson,T. G. Jenkins,J. R. Myra,D. N. Smithe*

Main category: physics.plasm-ph

TL;DR: Simulation of RF antenna effects on tokamak edge plasma using Hermes turbulence model with DC biasing representing RF sheath rectification, showing convective cell formation and Bohm-like impurity transport.


<details>
  <summary>Details</summary>
Motivation: To understand how ICRF antenna-induced sheath rectification affects tokamak boundary plasma turbulence and transport phenomena.

Method: Used Hermes edge turbulence model with Cartesian slab geometry, thin plate limiters representing ICRF antenna side-walls, and ad-hoc DC electric biasing based on VSim calculations to represent RF sheath rectified potential.

Result: RF sheath voltage forms convective cells, flattens SOL density profile, allows fluctuations into limiter shadow regions, and produces Bohm-like effective diffusion rates for impurity ions.

Conclusion: The model successfully captures key physical phenomena of ICRF-induced antenna biasing effects on tokamak boundary plasma transport and turbulence.

Abstract: The edge turbulence model Hermes (Dudson et al., 2017 Plasma Phys. Control.
Fusion 59 05401) is set up for plasma boundary simulations with an RF antenna,
using parameters characteristic of a tokamak edge. Cartesian slab geometry is
used with thin plate limiters representing the ICRF antenna side-wall limiters.
Ad-hoc DC electric biasing of the limiters, motivated by calculations with VSim
(Nieter et al., J. Comput. Phys. 196, 448 (2004)), represents an induced RF
sheath rectified potential in the plasma turbulence model. Flux-driven
turbulence simulations demonstrate a realistic distribution of plasma profiles
and fluctuations. There is a clear effect of the antenna sheath voltage leading
to formation of convective cells; bias-induced convective transport flattens
the SOL density profile and fluctuations penetrate into the shadow region of
the limiters as the bias voltage increases. Turbulent transport for impurity
ions is inferred by following ion trajectories in the simulated plasma
turbulence fields, showing Bohm-like effective diffusion rates. All in all, the
model elucidates the key physical phenomena governing the effects of
ICRF-induced antenna biasing on the tokamak boundary plasma.

</details>


### [40] [Optimal Landau-type closure parameters for two-fluid simulations of plasma turbulence at kinetic scales](https://arxiv.org/abs/2509.08525)
*Simon Lautenbach,Jeremiah Lübke,Maria Elena Innocenti,Katharina Kormann,Rainer Grauer*

Main category: physics.plasm-ph

TL;DR: Two-fluid simulations with Landau-fluid closures can effectively reproduce kinetic energy spectra when closure parameters are properly chosen, validating their use as efficient alternatives to fully kinetic simulations for large-scale turbulence studies.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between computationally intensive kinetic simulations and fluid descriptions while maintaining accuracy in representing kinetic effects, particularly for plasma turbulence and instabilities that operate far from local thermodynamic equilibrium.

Method: Using two-fluid simulations with local Landau-fluid closures derived from linear theory, comparing results against fully kinetic Vlasov simulations as reference benchmarks, with careful selection of local closure parameters.

Result: The simulations successfully reproduce the energy spectra obtained from fully kinetic Vlasov simulations when appropriate closure parameters are chosen, demonstrating good capture of kinetic scale processes even outside the theoretical regime of applicability.

Conclusion: Two-fluid simulations with Landau-fluid closures provide a validated efficient alternative to fully kinetic simulations for turbulence studies, particularly beneficial when simulating extremely large domains where computational efficiency is crucial.

Abstract: Two fluid simulations using local Landau-fluid closures derived from linear
theory provide an efficient computational framework for plasma modelling, since
they bridge the gap between computationally intensive kinetic simulations and
fluid descriptions. Their accuracy in representing kinetic effects depends
critically on the validity of the linear approximation used in the derivation:
the plasma should not be too far from local thermodynamic equilibrium, LTE.
However, many of the problems where these models are of particular interest
(such as plasma turbulence and instabilities) are in fact quite far from LTE.
The question then arises, if kinetic scale processes are still sufficiently
well captured outside of the theoretical regime of applicability of the
closure. In this paper, we show that two fluid simulations with Landau fluid
closures can effectively reproduce the energy spectra obtained with fully
kinetic Vlasov simulations, used as references, as long as the local closure
parameter is appropriately chosen. Our findings validate the usage of two fluid
simulations with Landau-fluid closure as a possible alternative to fully
kinetic simulations of turbulence, in cases where being able to simulate
extremely large domains is of particular interest.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [41] [Contributions to Robust and Efficient Methods for Analysis of High Dimensional Data](https://arxiv.org/abs/2509.08155)
*Kai Yang*

Main category: math.ST

TL;DR: Robust computational methods for high-dimensional data analysis, including nonlinear variable screening, nonconvex sparse estimation, and robust mixed-effects modeling with Tsallis entropy.


<details>
  <summary>Details</summary>
Motivation: High-dimensional data with feature dimensions much larger than sample sizes pose significant analytical challenges that require robust and computationally efficient methods beyond traditional linear assumptions.

Method: Three main approaches: 1) Mutual information-based variable screening for nonlinear associations, 2) Nonconvex penalty optimization for sparse estimation, 3) Tsallis entropy-based mixed-effects modeling with proximal nonlinear conjugate gradient algorithm.

Result: Developed methods that accommodate nonlinear relationships, provide computational efficiency, enhance robustness to outliers, and maintain numerical stability while handling high-dimensional correlated observations.

Conclusion: The proposed coherent framework addresses key challenges in high-dimensional data analysis through robust, computationally efficient methods that transcend traditional linear and Gaussian assumptions, with applications in neuroimaging and complex datasets.

Abstract: A ubiquitous feature of data of our era is their extra-large sizes and
dimensions. Analyzing such high-dimensional data poses significant challenges,
since the feature dimension is often much larger than the sample size. This
thesis introduces robust and computationally efficient methods to address
several common challenges associated with high-dimensional data. In my first
manuscript, I propose a coherent approach to variable screening that
accommodates nonlinear associations. I develop a novel variable screening
method that transcends traditional linear assumptions by leveraging mutual
information, with an intended application in neuroimaging data. This approach
allows for accurate identification of important variables by capturing
nonlinear as well as linear relationships between the outcome and covariates.
Building on this foundation, I develop new optimization methods for sparse
estimation using nonconvex penalties in my second manuscript. These methods
address notable challenges in current statistical computing practices,
facilitating computationally efficient and robust analyses of complex datasets.
The proposed method can be applied to a general class of optimization problems.
In my third manuscript, I contribute to robust modeling of high-dimensional
correlated observations by developing a mixed-effects model based on Tsallis
power-law entropy maximization and discussed the theoretical properties of such
distribution. This model surpasses the constraints of conventional Gaussian
models by accommodating a broader class of distributions with enhanced
robustness to outliers. Additionally, I develop a proximal nonlinear conjugate
gradient algorithm that accelerates convergence while maintaining numerical
stability, along with rigorous statistical properties for the proposed
framework.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [42] [Uniqueness of $S_2$-isotropic solutions to the isotropic $L_p$ Minkowski problem](https://arxiv.org/abs/2509.08588)
*Yao Wan*

Main category: math.DG

TL;DR: Spectral analysis of Hilbert-Brunn-Minkowski operator provides stability estimates for geometric inequalities and establishes uniqueness of S₂-isotropic solutions to L_p Minkowski problem in specific parameter ranges.


<details>
  <summary>Details</summary>
Motivation: To derive stability estimates for geometric inequalities and establish uniqueness results for solutions to the isotropic L_p Minkowski problem through spectral analysis of the Hilbert-Brunn-Minkowski operator.

Method: Analyzing eigenvalues of the Hilbert-Brunn-Minkowski operator L_K to establish spectral properties and derive stability estimates for geometric inequalities.

Result: Proved uniqueness of S₂-isotropic solutions to isotropic L_p Minkowski problem for (1-3n²)/2n ≤ p < -n with λ₂(-L_K) ≥ (n-1)/(2n-1+p), and extended to -2n-1 ≤ p < -n with λ₂(-L_K) ≥ (-p-1)/(n-1) under origin-centered condition.

Conclusion: Spectral analysis of the Hilbert-Brunn-Minkowski operator provides powerful tools for establishing stability of geometric inequalities and uniqueness results for Minkowski problems across extended parameter ranges.

Abstract: This paper investigates the spectral properties of the
Hilbert-Brunn-Minkowski operator $L_K$ to derive stability estimates for
geometric inequalities, including the local Brunn-Minkowski inequality. By
analyzing the eigenvalues of $L_K$, we establish the uniqueness of
$S_2$-isotropic solutions to the isotropic $L_p$ Minkowski problem in
$\mathbb{R}^{n}$ for $\frac{1-3n^2}{2n}\leq p<-n$ with $\lambda_2(-L_K)\geq
\frac{n-1}{2n-1+p}$. Furthermore, we extend this uniqueness result to the range
$-2n-1 \leq p<-n$ with $\lambda_2(-L_K)\geq \frac{-p-1}{n-1}$, assuming the
origin-centred condition.

</details>


<div id='math.CA'></div>

# math.CA [[Back]](#toc)

### [43] [Tensor Forms of Derivatives of Matrices and their applications in the Solutions to Differential Equations](https://arxiv.org/abs/2509.08429)
*Yiran Xu,Guangbin Wang,Changqing Xu*

Main category: math.CA

TL;DR: The paper introduces extended tensor products (outer and contractive) and uses them to unify ODEs and PDEs through tensor calculus, developing a partial Tucker decomposition-based algorithm for solving PDEs.


<details>
  <summary>Details</summary>
Motivation: To develop tensor calculus tools that can unify ordinary and partial differential equations and facilitate their solution through tensor-based methods.

Method: Extends outer and contractive tensor products, defines partial Tucker decompositions, derives tensor expressions for matrix derivatives, and develops an algorithm using partial TuckD for PDE solutions.

Result: Establishes tensor identities for derivatives, provides a unified framework for ODEs and PDEs, presents tensor form for Lyapunov function, and demonstrates efficient PDE solution algorithm with numerical example.

Conclusion: The extended tensor products and partial Tucker decompositions provide powerful tools for unifying differential equations and developing efficient numerical solutions for PDEs.

Abstract: We introduce and extend the outer product and contractive product of tensors
and matrices, and present some identities in terms of these products. We offer
tensor expressions of derivatives of tensors, focus on the tensor forms of
derivatives of a matrix w.r.t. another matrix. This tensor form makes possible
for us to unify ordinary differential equations (ODEs) with partial
differential equations (PDEs), and facilitates solution to them in some cases.
For our purpose, we also extend the outer product and contractive product of
tensors (matrices) to a more general case through any partition of the modes,
present some identities in terms of these products, initialize the definition
of partial Tucker decompositions (TuckD) of a tensor, and use the partial TuckD
to simplify the PDEs. We also present a tensor form for the Lyapunov function.
Our results in the products of tensors and matrices help us to establish some
important equalities on the derivatives of matrices and tensors. An algorithm
based on the partial Tucker decompositions (TuckD) to solve the PDEs is given,
and a numerical example is presented to illustrate the efficiency of the
algorithm.

</details>


<div id='math.FA'></div>

# math.FA [[Back]](#toc)

### [44] [A weak type $(p,a)$ criterion for operators, and applications](https://arxiv.org/abs/2509.08334)
*Bernhard Haak,El-Maati Ouhabaz*

Main category: math.FA

TL;DR: A criterion for weak type (p0,a) estimates of bounded operators on homogeneous type spaces, with applications to Riesz potentials, Riesz transforms, and spectral multipliers.


<details>
  <summary>Details</summary>
Motivation: To establish a general framework for proving weak type estimates for operators acting on function spaces over homogeneous type spaces, particularly for operators arising from spectral theory and potential theory.

Method: Develops a criterion for weak type (p0,a) estimates for bounded operators T: L^p(Ω) → L^q(Ω) where 1/p0 - 1/a = 1/p - 1/q. Applies this to Riesz potentials, Riesz transform type operators, and spectral multipliers under Gaussian or off-diagonal heat kernel bounds.

Result: Proves weak type estimates for various operators including L^{-α/2}, ∇Δ^{-α/2}, and spectral multipliers F(L). Also establishes boundedness from Hardy space H^1_L to L^a(X) and from L^{a'}(X) to BMO_L by duality.

Conclusion: The paper provides a unified approach to obtaining weak type estimates for a broad class of operators on homogeneous type spaces, with significant applications in harmonic analysis and spectral theory.

Abstract: Let $(X, d, \mu)$ be a space of homogeneous type and $\Omega$ an open subset
of $X$. Given a bounded operator $T: L^p(\Omega) \to L^q(\Omega)$ for some $1
\le p \le q < \infty$, we give a criterion for $T$ to be of weak type $(p_0,
a)$ for $p_0$ and $a$ such that $\frac{1}{p_0} - \frac{1}{a} =
\frac{1}{p}-\frac{1}{q}$. These results are illustrated by several applications
including estimates of weak type $(p_0, a)$ for Riesz potentials
$L^{-\frac{\alpha}{2}}$ or for Riesz transform type operators $\nabla
\Delta^{-\frac{\alpha}{2}}$ as well as $L^p-L^q$ boundedness of spectral
multipliers $F(L)$ when the heat kernel of $L$ satisfies a Gaussian upper bound
or an off-diagonal bound. We also prove boundedness of these operators from the
Hardy space $H^1_L$ associated with $L$ into $L^a(X)$. By duality this gives
boundedness from $L^{a'}(X)$ into $\text{BMO}_L$.

</details>


### [45] [On the dichotomy of $p$-walk dimensions on metric measure spaces](https://arxiv.org/abs/2509.08641)
*Meng Yang*

Main category: math.FA

TL;DR: The paper establishes a dichotomy for p-energy functionals on metric measure spaces: either the p-walk dimension β_p equals p for all p in an interval I, or β_p is strictly greater than p for all p in I.


<details>
  <summary>Details</summary>
Motivation: To understand the relationship between p-energy functionals and geometric properties of metric measure spaces, particularly how the p-walk dimension β_p behaves across different values of p.

Method: The analysis is conducted on volume doubling metric measure spaces with p-energies satisfying Poincaré inequality and cutoff Sobolev inequality with p-walk dimension β_p.

Result: A clear dichotomy is proven: for p in an open interval I ⊆ (1,∞), either β_p = p for all p ∈ I, or β_p > p for all p ∈ I.

Conclusion: The p-walk dimension β_p cannot exhibit mixed behavior within an interval - it must either consistently equal p or consistently exceed p throughout the entire interval.

Abstract: On a volume doubling metric measure space endowed with a family of
$p$-energies such that the Poincar\'e inequality and the cutoff Sobolev
inequality with $p$-walk dimension $\beta_p$ hold, for $p$ in an open interval
$I\subseteq (1,+\infty)$, we prove the following dichotomy: either $\beta_p=p$
for all $p\in I$, or $\beta_p>p$ for all $p\in I$.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [46] [Insertion space in repulsive active matter](https://arxiv.org/abs/2509.08131)
*Luke K. Davis,Karel Proesmans*

Main category: cond-mat.soft

TL;DR: Active matter insertion space analysis shows activity increases total insertion volume and connectivity, providing foundation for active matter thermodynamics.


<details>
  <summary>Details</summary>
Motivation: Extend the stochastic geometry concept of insertion space from equilibrium hard spheres to active matter systems to understand how activity affects spatial organization.

Method: Analyzed insertion space for repulsive active particles in 1D and 2D using both on- and off-lattice models. Derived closed-form expressions for mean insertion cavity size, cavity number, and total insertion volume.

Result: Excellent agreement between derived expressions and simulations. Activity increases total insertion volume and tends to keep insertion space more connected compared to equilibrium systems.

Conclusion: Provides first quantitative foundation for stochastic geometry of active matter and opens new route to building thermodynamics of active systems.

Abstract: For equilibrium hard spheres the stochastic geometry of the insertion space,
the room to accommodate another sphere, relates exactly to the equation of
state. We begin to extend this idea to active matter, analyzing insertion space
for repulsive active particles in one and two dimensions using both on- and
off-lattice models. In 1D we derive closed-form expressions for the mean
insertion cavity size, cavity number, and total insertion volume, all in
excellent agreement with simulations. Strikingly, activity increases the total
insertion volume and tends to keep the insertion space more connected. These
results provide the first quantitative foundation for the stochastic geometry
of active matter, and opens up a new route to building a thermodynamics of
active systems.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [47] [Model-Driven Subspaces for Large-Scale Optimization with Local Approximation Strategy](https://arxiv.org/abs/2509.08256)
*Yitong He,Pengcheng Xie*

Main category: math.OC

TL;DR: MD-LAMBO: A new subspace optimization algorithm with advanced model-driven subspaces for large-scale problems, featuring theoretical convergence guarantees and strong numerical performance.


<details>
  <summary>Details</summary>
Motivation: Large-scale optimization is critical for machine learning and scientific problems, but remains computationally challenging. Subspace methods using local approximation are important but need improved subspace constructions.

Method: Proposes MD-LAMBO algorithm with novel model-driven subspaces. Provides theoretical analysis of subspace properties, sufficient function decrease, and global convergence. Includes derivative-free model construction on subspaces.

Result: Numerical results show subspace-dependent performance differences and advantages of the proposed methods. Performance profiles and truncated Newton step errors demonstrate effectiveness.

Conclusion: The new MD-LAMBO algorithm with advanced subspaces provides theoretical guarantees and practical advantages for large-scale optimization problems, outperforming existing subspace methods.

Abstract: Solving large-scale optimization problems is a bottleneck and is very
important for machine learning and multiple kinds of scientific problems.
Subspace-based methods using the local approximation strategy are one of the
most important methods. This paper discusses different and novel kinds of
advanced subspaces for such methods and presents a new algorithm with such
subspaces, called MD-LAMBO. Theoretical analysis including the subspaces'
properties, sufficient function value decrease, and global convergence is given
for the new algorithm. The related model construction on the subspaces is given
under derivative-free settings. In numerical results, performance profiles, and
truncated Newton step errors of MD-LAMBO using different model-driven subspaces
are provided, which show subspace-dependent numerical differences and
advantages of our methods and subspaces.

</details>


### [48] [Hierarchical exact controllability for a parabolic equation with Hardy potential](https://arxiv.org/abs/2509.08471)
*Haiyang Lin,Bo You*

Main category: math.OC

TL;DR: Hierarchical exact controllability study for parabolic equations with Hardy potential using Stackelberg-Nash strategy, covering both linear and semilinear cases with different mathematical approaches.


<details>
  <summary>Details</summary>
Motivation: To investigate exact controllability for parabolic equations with Hardy potential through a hierarchical control framework using Stackelberg-Nash strategy, addressing both linear and nonlinear cases.

Method: For linear case: Lax-Milgram theorem for Nash equilibrium existence, global Carleman inequalities for observability. For semilinear case: well-posedness analysis of coupled systems, Leray-Schauder fixed point theorem for exact controllability.

Result: Proved existence of Nash equilibrium pairs for bi-objective optimal control, established observability inequalities, and demonstrated exact controllability to arbitrary prescribed trajectories in both linear and semilinear cases.

Conclusion: The Stackelberg-Nash strategy successfully achieves hierarchical exact controllability for parabolic equations with Hardy potential, with mathematical foundations established for both linear and semilinear formulations.

Abstract: The main objective of this paper is to study the hierarchical exact
controllability for a parabolic equation with Hardy potential by
Stackelberg-Nash strategy. In linear case, we employ Lax-Milgram theorem to
prove the existence of an associated Nash equilibrium pair corresponding to a
bi-objective optimal control problem for each leader, which is responsible for
an exact controllability property. Then the observability inequality of a
coupled parabolic system is established by using global Carleman inequalities,
which results in the existence of a leader that drives the controlled system
exactly to any prescribed trajectory. In semilinear case, we first prove the
well-posedness of the coupled parabolic system to obtain the existence of Nash
quasi-equilibrium pair and show that Nash quasi-equilibrium is equivalent to
Nash equilibrium. Based on these results, we establish the existence of a
leader that drives the controlled system exactly to a prescribed (but
arbitrary) trajectory by Leray-Schauder fixed point theorem.

</details>


### [49] [Linear Convergence of Gradient Descent for Quadratically Regularized Optimal Transport](https://arxiv.org/abs/2509.08547)
*Alberto González-Sanz,Marcel Nutz,Andrés Riveros Valdevenito*

Main category: math.OC

TL;DR: Quadratic regularization in optimal transport enables sparse couplings and small regularization parameters. Gradient descent for the dual problem converges linearly with exponential error reduction, unlike entropic regularization approaches.


<details>
  <summary>Details</summary>
Motivation: Quadratic regularization is preferred over entropic regularization when sparse transport couplings or small regularization parameters are needed, but it lacks theoretical convergence analysis despite practical computational approaches.

Method: Gradient descent algorithm applied to the dual transport problem in continuous and semi-discrete settings. The method involves analyzing the linearization of the gradient descent operator at the optimum using functional-analytic arguments to bound its spectrum.

Result: Gradient descent converges linearly - the L² distance between iterates and limiting potentials decreases exponentially fast. The algorithm is stable for small regularization parameters and straightforward to implement.

Conclusion: Quadratic regularization with gradient descent provides an analytically tractable alternative to entropic regularization, offering linear convergence and stability for small regularization parameters, with novel functional-analytic techniques for convergence analysis.

Abstract: In optimal transport, quadratic regularization is an alternative to entropic
regularization when sparse couplings or small regularization parameters are
desired. Here quadratic regularization means that transport couplings are
penalized by the squared $L^2$ norm, or equivalently the $\chi^2$ divergence.
While a number of computational approaches have been shown to work in practice,
quadratic regularization is analytically less tractable than entropic, and we
are not aware of a previous theoretical convergence rate analysis. We focus on
the gradient descent algorithm for the dual transport problem in continuous and
semi-discrete settings. This problem is convex but not strongly convex; its
solutions are the potential functions that approximate the Kantorovich
potentials of unregularized optimal transport. The gradient descent steps are
straightforward to implement, and stable for small regularization parameter --
in contrast to Sinkhorn's algorithm in the entropic setting. Our main result is
that gradient descent converges linearly; that is, the $L^2$ distance between
the iterates and the limiting potentials decreases exponentially fast. Our
analysis centers on the linearization of the gradient descent operator at the
optimum and uses functional-analytic arguments to bound its spectrum. These
techniques seem to be novel in this area and are substantially different from
the approaches familiar in entropic optimal transport.

</details>


### [50] [Unidimensional semi-discrete partial optimal transport](https://arxiv.org/abs/2509.08799)
*Adrien Cances,Hugo Leclerc*

Main category: math.OC

TL;DR: This paper studies 1D partial optimal transport with quadratic cost, introducing a regularization method using auxiliary dimension thickening to handle reduced dual functional regularity, proving quadratic convergence and demonstrating improved stability and efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing the reduced regularity of dual functionals in unidimensional partial optimal transport problems that arise in risk management, crowd motion modeling, and point cloud registration applications.

Method: Introduce a regularization procedure based on thickening the probability density along an auxiliary dimension, prove convergence of maximizers with quadratic rate, and develop a numerical scheme leveraging the regularized functional.

Result: The maximizers of the regularized dual problem converge to those of the original dual problem with quadratic rate in thickness, and simulations confirm this quadratic convergence rate.

Conclusion: The proposed approach offers both improved stability and computational efficiency for unidimensional partial transport problems compared to fully discrete settings.

Abstract: We study the semi-discrete formulation of one-dimensional partial optimal
transport with quadratic cost, where a probability density is partially
transported to a finite sum of Dirac masses of smaller total mass. This problem
arises naturally in applications such as risk management, the modeling of crowd
motion, and sliced partial transport algorithms for point cloud registration.
Unlike higher-dimensional settings, the dual functional in the unidimensional
case exhibits reduced regularity. To overcome this difficulty, we introduce a
regularization procedure based on thickening the density along an auxiliary
dimension. We prove that the maximizers of the regularized dual problem
converge to those of the original dual problem, with quadratic rate in the
introduced thickness. We further provide a numerical scheme that leverages the
regularized functional, and we validate our analysis with simulations that
confirm the quadratic convergence rate. Finally, we compare the semi-discrete
and fully discrete settings, demonstrating that our approach offers both
improved stability and computational efficiency for unidimensional partial
transport problems.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [51] [Diameter-Controlled High-Order Vortex States and Magnon Hybridization in VSe2 Nanotubes](https://arxiv.org/abs/2509.08368)
*Jia-Wen Li,Xin-Wei Yi,Jin Zhang,Gang Su,Bo Gu*

Main category: cond-mat.mtrl-sci

TL;DR: High-order vortex states in VSe2 nanotubes enable unique magnonic functionalities through orbital angular momentum selection rules, generating complex high-OAM magnons crucial for spin-wave information transport.


<details>
  <summary>Details</summary>
Motivation: Curved magnets offer rich phase diagrams and promise for next-generation spintronic technologies, but the significance of high-order vortex states with winding numbers >1 remains unexplored for enabling fundamentally inaccessible magnonic functionalities.

Method: Used density-functional theory calculations and Heisenberg modeling to study phase diagrams, and Landau-Lifshitz-Gilbert equation to analyze hybridization mechanisms governed by orbital angular momentum selection rules in VSe2 nanotubes.

Result: Discovered that high-order vortex configurations unlock intrinsic hybridization with strict OAM selection rules (Δl = ±2(n-1)), generating complex high-OAM magnons with measurable topological charge, demonstrated by eight-petal magnon density patterns in 3φ state.

Conclusion: VSe2 nanotubes provide a platform-free solution for generating high-OAM magnons, establishing a predictive framework for controlling high-order vortex states and highlighting their promise for future magnonic and spintronic devices.

Abstract: Curved magnets offer a rich phase diagram and hold great promise for
next-generation spintronic technologies. This study establishes the paramount
significance of high-order vortex states (e.g., 3$\varphi$ with winding number
$n$ > 1) in VSe2 nanotubes, which uniquely enable magnonic functionalities
fundamentally inaccessible to conventional magnetic systems. These states arise
from diameter-dependent competition between the nearest-neighbor ferromagnetic
($J_1$) and longer-range antiferromagnetic ($J_2$/$J_3$) couplings, as
rigorously validated through density-functional theory calculations and
Heisenberg modeling of phase diagrams. Critically, by the
Landau-Lifshitz-Gilbert equation, we find that high-order vortex configurations
unlock an intrinsic hybridization mechanism governed by strict orbital angular
momentum (OAM) selection rules ($\Delta l = \pm 2(n-1)$) -- a process strictly
forbidden in fundamental vortices ($n$ = 1) -- generating complex high-OAM
magnons with measurable topological charge. This is vividly demonstrated in the
3$\varphi$ state, where hybridization between $l$ = -4, 0 and 4 modes produces
eight-petal magnon density patterns. Such states provide an essential
platform-free solution for generating high-OAM magnons, wchich is crucial for
spin-wave-based information transport. These findings establish a predictive
theoretical framework for controlling high-order vortex states in curved
magnets and highlight VSe2 nanotubes as a promising platform for exploring
complex magnetism and developing future magnonic and spintronic devices.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [52] [A transport approach to the cutoff phenomenon](https://arxiv.org/abs/2509.08560)
*Francesco Pedrotti,Justin Salez*

Main category: math.PR

TL;DR: New approach for cutoff phenomenon analysis using W-TV transport inequality instead of varentropy, applicable to non-negatively curved processes on smooth spaces.


<details>
  <summary>Details</summary>
Motivation: Recent progress in understanding cutoff phenomenon for Markov processes relied on varentropy statistics, but this approach requires chain rule approximations. The authors seek an alternative method that bypasses these limitations.

Method: Proposes a new approach using W-TV transport inequality combined with classical parabolic regularization estimates. This method eliminates the need for chain rule or approximate versions thereof, though currently restricted to non-negatively curved processes on smooth spaces.

Result: Successfully recovers the main result establishing cutoff for log-concave Langevin dynamics from previous work, and extends the conclusion to the Proximal Sampler discrete-time sampling algorithm.

Conclusion: The new W-TV transport inequality approach provides an effective alternative to varentropy-based methods for analyzing cutoff phenomena, with broader applicability to both continuous and discrete-time sampling algorithms.

Abstract: Substantial progress has recently been made in the understanding of the
cutoff phenomenon for Markov processes, using an information-theoretic
statistics known as varentropy [Sal23; Sal24; Sal25a; PS25]. In the present
paper, we propose an alternative approach which bypasses the use of varentropy
and exploits instead a new W-TV transport inequality, combined with a classical
parabolic regularization estimate [BGL01; OV01]. While currently restricted to
non-negatively curved processes on smooth spaces, our argument no longer
requires the chain rule, nor any approximate version thereof. As applications,
we recover the main result of [Sal25a] establishing cutoff for the log-concave
Langevin dynamics, and extend the conclusion to a widely-used discrete-time
sampling algorithm known as the Proximal Sampler.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [53] [A Pathway to Practical Quantum Advantage in Solving Navier-Stokes Equations](https://arxiv.org/abs/2509.08807)
*Xi-Ning Zhuang,Zhao-Yun Chen,Ming-Yang Tan,Jiaxuan Zhang,Chuang-Chao Ye,Tian-Hao Wei,Teng-Yang Ma,Cheng Xue,Huan-Yu Liu,Qing-Song Li,Tai-Ping Sun,Xiao-Fan Xu,Yun-Jie Wang,Yu-Chun Wu,Guo-Ping Guo*

Main category: quant-ph

TL;DR: Quantum framework achieves exponential speedup for Navier-Stokes equations, reducing resource requirements by 100x and enabling 2^80-grid simulation in 42.6 days vs. century on supercomputers.


<details>
  <summary>Details</summary>
Motivation: Navier-Stokes equations have remained formidable for quantum algorithms due to high input-output overhead and nonlinearity, despite the promise of fault-tolerant quantum computing for solving classically intractable problems.

Method: Full-stack framework integrating spectral-based input/output algorithm, explicit synthesized quantum circuit using symmetry-based synthesis, and refined error-correction protocol to optimize resource requirements.

Result: Achieves end-to-end exponential speedup meeting lower bound for quantum linear system solvers, reduces logical and physical resources by two orders of magnitude. 2^80-grid simulation feasible with 8.71M physical qubits in 42.6 days.

Conclusion: Bridges gap between theoretical quantum speedup and practical deployment of high-performance scientific computing, demonstrating quantum advantage for large-scale NSE simulation that outperforms state-of-the-art supercomputers by orders of magnitude.

Abstract: The advent of fault-tolerant quantum computing (FTQC) promises to tackle
classically intractable problems. A key milestone is solving the Navier-Stokes
equations (NSE), which has remained formidable for quantum algorithms due to
their high input-output overhead and nonlinearity. Here, we establish a
full-stack framework that charts a practical pathway to a quantum advantage for
large-scale NSE simulation. Our approach integrates a spectral-based
input/output algorithm, an explicit and synthesized quantum circuit, and a
refined error-correction protocol. The algorithm achieves an end-to-end
exponential speedup in asymptotic complexity, meeting the lower bound for
general quantum linear system solvers. Through symmetry-based circuit synthesis
and optimized error correction, we reduce the required logical and physical
resources by two orders of magnitude. Our concrete resource analysis
demonstrates that solving NSE on a $2^{80}$-grid is feasible with 8.71 million
physical qubits (at an error rate of $5 \times 10^{-4}$) in 42.6 days --
outperforming a state-of-the-art supercomputer, which would require over a
century. This work bridges the gap between theoretical quantum speedup and the
practical deployment of high-performance scientific computing.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [54] [Quantifying model prediction sensitivity to model-form uncertainty](https://arxiv.org/abs/2509.08708)
*Teresa Portone,Rebekah D. White,Joseph L. Hart*

Main category: cs.CE

TL;DR: Novel method to quantify importance of model-form uncertainty in physics-based models using parameterized assumption modifications and grouped variance-based sensitivity analysis.


<details>
  <summary>Details</summary>
Motivation: Model-form uncertainty is a significant but hard-to-quantify source of error in predictions beyond available data, making it difficult to prioritize resources for error reduction.

Method: Combines parameterized modifications to assumptions (MFU representations) with grouped variance-based sensitivity analysis to measure assumption importance, applicable with or without calibration data.

Result: The approach can quantify assumption importance without calibration data, and when data is available, it can inform MFU representations and handle parameter dependence from calibration.

Conclusion: Provides a practical framework for quantifying model-form uncertainty importance relative to other uncertainty sources, enabling better resource allocation for model improvement.

Abstract: Model-form uncertainty (MFU) in assumptions made during physics-based model
development is widely considered a significant source of uncertainty; however,
there are limited approaches that can quantify MFU in predictions extrapolating
beyond available data. As a result, it is challenging to know how important MFU
is in practice, especially relative to other sources of uncertainty in a model,
making it difficult to prioritize resources and efforts to drive down error in
model predictions. To address these challenges, we present a novel method to
quantify the importance of uncertainties associated with model assumptions. We
combine parameterized modifications to assumptions (called MFU representations)
with grouped variance-based sensitivity analysis to measure the importance of
assumptions. We demonstrate how, in contrast to existing methods addressing
MFU, our approach can be applied without access to calibration data. However,
if calibration data is available, we demonstrate how it can be used to inform
the MFU representation, and how variance-based sensitivity analysis can be
meaningfully applied even in the presence of dependence between parameters (a
common byproduct of calibration).

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [55] [Acceleration of Heavy Ions at Non-Relativistic Collisionless Shocks](https://arxiv.org/abs/2509.08061)
*Damiano Caprioli,Luca Orusa,Miha Cernetic,Colby C. Haggerty,Bricker Ostler*

Main category: astro-ph.HE

TL;DR: Heavy ions like helium and carbon are preferentially accelerated in collisionless shocks compared to hydrogen, enhancing overall acceleration efficiency, magnetic field amplification, and production of gamma rays and neutrinos.


<details>
  <summary>Details</summary>
Motivation: To investigate how partially-ionized heavy ions with A/Q > 1 undergo Diffusive Shock Acceleration (DSA) and understand their impact on shock acceleration processes in astrophysical contexts.

Method: Two-dimensional hybrid simulations (kinetic ions, fluid electrons) of non-relativistic collisionless shocks with helium- and carbon-like ions introduced at solar abundances.

Result: 1) Heavy ions are preferentially accelerated over hydrogen, with helium energy comparable/exceeding hydrogen; 2) Helium ions amplify magnetic fields, increasing maximum particle energy and steepening spectra; 3) Efficient helium acceleration significantly enhances hadronic gamma ray and neutrino production.

Conclusion: Heavy ion acceleration effects must be considered when modeling strong space and astrophysical shocks due to their substantial impact on acceleration efficiency, magnetic field dynamics, and high-energy particle production.

Abstract: We investigate the process of Diffusive Shock Acceleration (DSA) of particles
with mass number to charge number ratios $A/Q > 1$, e.g., partially-ionized
heavy ions. To this end, we introduce helium- and carbon-like ions at solar
abundances into two-dimensional hybrid (kinetic ions--fluid electrons)
simulations of non-relativistic collisionless shocks. This study yields three
main results: 1) Heavy ions are preferentially accelerated compared to
hydrogen. For typical solar abundances, the energy transferred to accelerated
helium ions is comparable to, or even exceeds, that of hydrogen, thereby
enhancing the overall shock acceleration efficiency. 2) Accelerated helium ions
contribute to magnetic field amplification, which increases the maximum
attainable particle energy and steepen the spectra of accelerated particles. 3)
The efficient acceleration of helium significantly enhances the production of
hadronic gamma rays and neutrinos, likely dominating the one due to hydrogen.
These effects should be taken into account, especially when modeling strong
space and astrophysical shocks.

</details>


<div id='physics.acc-ph'></div>

# physics.acc-ph [[Back]](#toc)

### [56] [Slice Emittance Preservation and Focus Control in a Passive Plasma Lens](https://arxiv.org/abs/2509.08420)
*J. Björklund Svensson,J. Beinortaitė,L. Boulton,B. Foster,J. M. Garland,P. González Caminal,M. Huck,H. Jones,A. Kanekar,G. Loisch,J. Osterhoff,F. Peña,S. Schröder,M. Thévenet,S. Wesch,M. Wing,J. C. Wood,R. D'Arcy*

Main category: physics.acc-ph

TL;DR: Passive plasma lenses can strongly focus high-brightness FEL-quality beams while preserving slice emittance, outperforming quadrupole magnets by two orders of magnitude.


<details>
  <summary>Details</summary>
Motivation: Strong focusing plasma lenses are needed for plasma-based accelerators and collider final foci, but compatibility with high-brightness beams hasn't been demonstrated.

Method: Experimental demonstration of passive plasma lenses focusing capabilities with free-electron-laser-quality beams.

Result: Passive plasma lenses preserved slice emittance while focusing two orders of magnitude more strongly than quadrupole magnets, with controllable focal parameters.

Conclusion: Passive plasma lenses are compatible with high-brightness beams and offer superior focusing performance compared to traditional quadrupole magnets.

Abstract: Strong, symmetrically focusing plasma lenses are promising for accommodating
the small beams associated with plasma-based accelerators and collider final
foci. However, while focusing with active and passive plasma lenses has been
experimentally demonstrated, compatibility with high-brightness beams relevant
for applications has not. In this Letter, we show experimentally that passive
plasma lenses can preserve free-electron-laser-quality slice emittance while
focusing two orders of magnitude more strongly than quadrupole magnets, and
that the focal parameters can be controlled.

</details>


<div id='math.SP'></div>

# math.SP [[Back]](#toc)

### [57] [On shape optimization with large magnetic fields in two dimensions](https://arxiv.org/abs/2509.08412)
*Vladimir Lotoreichik,Léo Morin*

Main category: math.SP

TL;DR: In strong magnetic fields, optimal domains for magnetic Laplacian eigenvalues become symmetric. If a domain's nth eigenvalue is smaller than a disk's, its asymmetry tends to zero in strong field limit.


<details>
  <summary>Details</summary>
Motivation: To demonstrate that symmetry emerges as the optimal configuration for magnetic Laplacian eigenvalues in the limit of strong magnetic fields.

Method: Established asymptotic bounds on magnetic eigenvalues, analyzed magnetic Dirichlet Laplacian on bounded simply-connected planar domains and rectangles, and studied magnetic Dirac operator with infinite mass boundary conditions.

Result: Main result shows that when a domain's nth magnetic eigenvalue is smaller than a disk's equivalent, the domain's Fraenkel asymmetry approaches zero in strong magnetic field limit. Also provided new torsion function estimate for rectangles.

Conclusion: Strong magnetic fields drive optimal domains toward symmetry, with domain asymmetry vanishing when eigenvalues outperform those of symmetric reference domains.

Abstract: This paper aims to show that, in the limit of strong magnetic fields, the
optimal domains for eigenvalues of magnetic Laplacians tend to exhibit
symmetry. We establish several asymptotic bounds on magnetic eigenvalues to
support this conclusion. Our main result implies that if, for a bounded
simply-connected planar domain, the n-th eigenvalue of the magnetic Dirichlet
Laplacian with uniform magnetic field is smaller than the corresponding
eigenvalue for a disk of the same area, then the Fraenkel asymmetry of that
domain tends to zero in the strong magnetic field limit. Comparable results are
also derived for the magnetic Dirichlet Laplacian on rectangles, as well as the
magnetic Dirac operator with infinite mass boundary conditions on smooth
domains. As part of our analysis, we additionally provide a new estimate for
the torsion function on rectangles.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [58] [Modified Loss of Momentum Gradient Descent: Fine-Grained Analysis](https://arxiv.org/abs/2509.08483)
*Matias D. Cattaneo,Boris Shigida*

Main category: cs.LG

TL;DR: Analysis of Polyak heavy-ball momentum gradient descent showing it's equivalent to plain gradient descent with a modified loss on attractive manifolds, with high-order approximation bounds and combinatorial insights.


<details>
  <summary>Details</summary>
Motivation: To understand the mathematical structure and behavior of gradient descent with heavy-ball momentum, particularly its equivalence to plain gradient descent with modified loss and the combinatorial properties underlying its memoryless approximations.

Method: Proved that on exponentially attractive invariant manifolds, HB is exactly plain GD with modified loss for small step sizes. Derived global approximation bounds O(h^R) for any finite order R. Conducted combinatorial analysis of memoryless approximations and derived continuous modified equations.

Result: Established rigorous approximation bounds, revealed rich polynomial family containing Eulerian and Narayana polynomials, derived continuous modified equations of arbitrary order, and generalized principal flow approximation for HB dynamics.

Conclusion: The analysis provides new insights into heavy-ball momentum features and establishes a framework for similar analysis of other optimization algorithms, with applications covering both full-batch and mini-batch settings.

Abstract: We analyze gradient descent with Polyak heavy-ball momentum (HB) whose fixed
momentum parameter $\beta \in (0, 1)$ provides exponential decay of memory.
Building on Kovachki and Stuart (2021), we prove that on an exponentially
attractive invariant manifold the algorithm is exactly plain gradient descent
with a modified loss, provided that the step size $h$ is small enough. Although
the modified loss does not admit a closed-form expression, we describe it with
arbitrary precision and prove global (finite "time" horizon) approximation
bounds $O(h^{R})$ for any finite order $R \geq 2$. We then conduct a
fine-grained analysis of the combinatorics underlying the memoryless
approximations of HB, in particular, finding a rich family of polynomials in
$\beta$ hidden inside which contains Eulerian and Narayana polynomials. We
derive continuous modified equations of arbitrary approximation order (with
rigorous bounds) and the principal flow that approximates the HB dynamics,
generalizing Rosca et al. (2023). Approximation theorems cover both full-batch
and mini-batch HB. Our theoretical results shed new light on the main features
of gradient descent with heavy-ball momentum, and outline a road-map for
similar analysis of other optimization algorithms.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [59] [A Unified Symmetry-Constrained Framework for Band Inversions in Photonic Crystals with $C_n$ Symmetry](https://arxiv.org/abs/2509.08620)
*Ze Tao,Fujun Liu*

Main category: physics.optics

TL;DR: A unified symmetry-constrained k·p framework for characterizing band inversions across C₆, C₄, C₃, and C₂ photonic crystals, enabling coefficient-free topological diagnosis and systematic band engineering.


<details>
  <summary>Details</summary>
Motivation: The lack of a unified theoretical framework for characterizing band inversions across different crystal symmetries hinders the development of topological photonic band engineering.

Method: Constructed a symmetry-constrained k·p framework that universally models bands near high-symmetry points for symmetric photonic crystals with C₆, C₄, C₃, and C₂ symmetries.

Result: Demonstrated systematic band inversion engineering: reopening linear gap at Γ in C₆ crystals, quadratic coupling in C₄ systems, prevention of inversion at Γ in C₃ symmetry, and unique anisotropic gap inversion in C₂ symmetry.

Conclusion: This symmetry-first, fit-free approach establishes a direct link between experimental band maps and topological parameter extraction, providing a universal tool for inversion and coupling-order identification.

Abstract: The lack of a unified theoretical framework for characterizing band
inversions across different crystal symmetries hinders the rapid development of
topological photonic band engineering. To address this issue, we have
constructed a framework constrained by symmetry $k \cdot p$ that universally
models bands near high-symmetry points for symmetric photonic crystals $C_6$,
$C_4$, $C_3$, and $C_2$. This framework enables a coefficient-free quantitative
diagnosis of band topology. We have demonstrated the power of this framework by
systematically engineering band inversions. In $C_6$ crystals, we induce a
reopening of the linear gap at $\Gamma$. In $C_4$ systems, mirror symmetry
enforces a characteristic quadratic coupling leading to distinct spectral
features. Our analysis further reveals that a lone $E$ doublet prevents
inversion at the $\Gamma$ point in $C_3$ symmetry, while $C_2$ symmetry
facilitates a unique inversion of $Y$ pointsints with anisotropic gap. This
symmetry-first, fit-free approach establishes a direct link between
experimental band maps and the extraction of fundamental topological
parameters. It offers a universal tool for inversion and coupling-order
identification.

</details>
