{"id": "2509.08971", "pdf": "https://arxiv.org/pdf/2509.08971", "abs": "https://arxiv.org/abs/2509.08971", "authors": ["Julien Loiseau", "Hyun Lim", "Andr\u00e9s Yag\u00fce L\u00f3pez", "Mammadbaghir Baghirzade", "Shihab Shahriar Khan", "Yoonsoo Kim", "Sudarshan Neopane", "Alexander Strack", "Farhana Taiyebah", "Benjamin K. Bergen"], "title": "HARD: A Performance Portable Radiation Hydrodynamics Code based on FleCSI Framework", "categories": ["physics.comp-ph", "astro-ph.IM", "cs.DC"], "comment": "15 pages, 8 figures", "summary": "Hydrodynamics And Radiation Diffusion} (HARD) is an open-source application\nfor high-performance simulations of compressible hydrodynamics with\nradiation-diffusion coupling. Built on the FleCSI (Flexible Computational\nScience Infrastructure) framework, HARD expresses its computational units as\ntasks whose execution can be orchestrated by multiple back-end runtimes,\nincluding Legion, MPI, and HPX. Node-level parallelism is delegated to Kokkos,\nproviding a single, portable code base that runs efficiently on laptops, small\nhomogeneous clusters, and the largest heterogeneous supercomputers currently\navailable. To ensure scientific reliability, HARD includes a regression-test\nsuite that automatically reproduces canonical verification problems such as the\nSod and LeBlanc shock tubes and the Sedov blast wave, comparing numerical\nsolutions against known analytical results. The project is distributed under an\nOSI-approved license, hosted on GitHub, and accompanied by reproducible build\nscripts and continuous integration workflows. This combination of performance\nportability, verification infrastructure, and community-focused development\nmakes HARD a sustainable platform for advancing radiation hydrodynamics\nresearch across multiple domains.", "AI": {"tldr": "HARD is an open-source high-performance simulation tool for compressible hydrodynamics with radiation-diffusion coupling, built on FleCSI framework with portable performance across devices from laptops to supercomputers.", "motivation": "To create a sustainable, portable platform for radiation hydrodynamics research that can run efficiently across diverse computing environments while ensuring scientific reliability through verification testing.", "method": "Built on FleCSI framework using task-based computational units orchestrated by multiple back-end runtimes (Legion, MPI, HPX) with node-level parallelism handled by Kokkos for portability. Includes regression-test suite for verification against analytical solutions.", "result": "A single portable code base that runs efficiently on various hardware from laptops to heterogeneous supercomputers, with verified scientific reliability through canonical test problems.", "conclusion": "HARD provides a sustainable platform combining performance portability, verification infrastructure, and community-focused development for advancing radiation hydrodynamics research across multiple domains."}}
{"id": "2509.09051", "pdf": "https://arxiv.org/pdf/2509.09051", "abs": "https://arxiv.org/abs/2509.09051", "authors": ["Joseph L. Hesse-Withbroe", "Katya S. Arquilla"], "title": "An Improved Rapid Performance Analysis Model for Solenoidal Magnetic Radiation Shields", "categories": ["physics.comp-ph", "physics.app-ph"], "comment": "15 pages + appendix, 10 figures", "summary": "Astronauts participating in deep-space exploration missions will be exposed\nto significantly greater amounts of radiation than is typically encountered on\nEarth or in low Earth orbit (LEO), which poses significant risks to crew health\nand mission safety. Active magnetic radiation shields based on the Lorentz\ndeflection of charged particles have the potential to reduce astronaut doses\nwith lower mass costs than passive shielding techniques. Typically, active\nshielding performance is evaluated using high-fidelity Monte Carlo simulations,\nwhich are too computationally expensive to evaluate an entire trade space of\nshield designs. A rapid, semi-analytical model based on the High Charge and\nEnergy Transport code (HZETRN) developed in 2014 provided an alternative method\nby which to evaluate the performance of solenoidal shields. However, various\nsimplifying assumptions made in the original model have limited its accuracy,\nand therefore require evaluation and correction. In this work, a number of\naspects of the original semi-analytical model are updated and validated by\nMonte Carlo simulation, then used to recharacterize the design trade space of\nsolenoidal magnetic shields. The updated model predicts improved performance\nfor weaker shields as compared to the original model, but greatly diminished\nperformance for strong shields with bending powers greater than 20 T-m.\nOverall, the results indicate that magnetic shields enable significant mass\nsavings over passive shields for mission scenarios where the requisite dose\nreduction is greater than about 60% relative to free space, which includes most\nexploration missions longer than one year with significant time spent outside\nLEO.", "AI": {"tldr": "Updated semi-analytical model for evaluating solenoidal magnetic radiation shields shows improved performance for weaker shields but diminished performance for strong shields (>20 T-m), indicating mass savings over passive shielding for missions requiring >60% dose reduction.", "motivation": "Deep-space exploration exposes astronauts to dangerous radiation levels. Active magnetic shielding offers potential mass savings over passive methods, but existing evaluation models have accuracy limitations that need correction.", "method": "Updated and validated the 2014 HZETRN-based semi-analytical model by comparing with Monte Carlo simulations, then used it to recharacterize solenoidal shield design trade space.", "result": "Updated model shows improved performance for weaker shields but greatly diminished performance for strong shields (>20 T-m). Magnetic shields provide significant mass savings when dose reduction >60% is needed.", "conclusion": "Magnetic radiation shields are viable for long-duration deep-space missions (>1 year outside LEO) where substantial dose reduction (>60%) is required, offering mass advantages over passive shielding approaches."}}
{"id": "2509.08834", "pdf": "https://arxiv.org/pdf/2509.08834", "abs": "https://arxiv.org/abs/2509.08834", "authors": ["John T. Rickard", "William A. Dembski", "James Rickards"], "title": "An Interval Type-2 Version of Bayes Theorem Derived from Interval Probability Range Estimates Provided by Subject Matter Experts", "categories": ["cs.AI", "physics.comp-ph", "physics.data-an", "q-fin.CP"], "comment": "13 pages, 12 figures", "summary": "Bayesian inference is widely used in many different fields to test hypotheses\nagainst observations. In most such applications, an assumption is made of\nprecise input values to produce a precise output value. However, this is\nunrealistic for real-world applications. Often the best available information\nfrom subject matter experts (SMEs) in a given field is interval range estimates\nof the input probabilities involved in Bayes Theorem. This paper provides two\nkey contributions to extend Bayes Theorem to an interval type-2 (IT2) version.\nFirst, we develop an IT2 version of Bayes Theorem that uses a novel and\nconservative method to avoid potential inconsistencies in the input IT2 MFs\nthat otherwise might produce invalid output results. We then describe a novel\nand flexible algorithm for encoding SME-provided intervals into IT2 fuzzy\nmembership functions (MFs), which we can use to specify the input probabilities\nin Bayes Theorem. Our algorithm generalizes and extends previous work on this\nproblem that primarily addressed the encoding of intervals into word MFs for\nComputing with Words applications.", "AI": {"tldr": "This paper extends Bayes Theorem to handle interval type-2 fuzzy inputs from subject matter experts, developing a conservative method to avoid inconsistencies and a flexible algorithm for encoding expert intervals into membership functions.", "motivation": "Traditional Bayesian inference assumes precise input values, which is unrealistic for real-world applications where subject matter experts typically provide interval range estimates rather than precise probabilities.", "method": "Developed an interval type-2 version of Bayes Theorem with a novel conservative method to prevent input inconsistencies, and created a flexible algorithm for encoding expert-provided intervals into IT2 fuzzy membership functions.", "result": "The proposed approach enables Bayesian inference to work with interval-based expert knowledge while maintaining mathematical validity and avoiding potential inconsistencies that could produce invalid outputs.", "conclusion": "The IT2 extension of Bayes Theorem provides a more realistic framework for real-world applications where precise probabilities are unavailable, allowing effective use of expert interval estimates in Bayesian analysis."}}
{"id": "2509.08930", "pdf": "https://arxiv.org/pdf/2509.08930", "abs": "https://arxiv.org/abs/2509.08930", "authors": ["Malte Mederacke", "Chengyou Yu", "Roman Vetter", "Dagmar Iber"], "title": "Simulating Organogenesis in COMSOL Multiphysics: Tissue Patterning with Directed Cell Migration", "categories": ["physics.bio-ph", "nlin.PS", "physics.comp-ph"], "comment": "7 pages, 5 figures, 1 table. COMSOL Conference 2025", "summary": "We present a COMSOL Multiphysics implementation of a continuum model for\ndirected cell migration, a key mechanism underlying tissue self-organization\nand morphogenesis. The model is formulated as a partial integro-differential\nequation (PIDE), combining random motility with non-local, density-dependent\nguidance cues to capture phenomena such as cell sorting and aggregation. Our\nframework supports simulations in one, two, and three dimensions, with both\nzero-flux and periodic boundary conditions, and can be reformulated in a\nLagrangian setting to efficiently handle tissue growth and domain deformation.\nWe demonstrate that COMSOL Multiphysics enables a flexible and accessible\nimplementation of PIDEs, providing a generalizable platform for studying\ncollective cell behavior and pattern formation in complex biological contexts.", "AI": {"tldr": "COMSOL Multiphysics implementation of a continuum model for directed cell migration using partial integro-differential equations to study tissue self-organization and pattern formation.", "motivation": "To provide a flexible and accessible computational framework for studying directed cell migration, which is crucial for understanding tissue self-organization and morphogenesis processes.", "method": "Developed a continuum model formulated as a partial integro-differential equation (PIDE) that combines random motility with non-local, density-dependent guidance cues. Implemented in COMSOL Multiphysics with support for 1D, 2D, and 3D simulations using both zero-flux and periodic boundary conditions, with Lagrangian reformulation for tissue growth and domain deformation.", "result": "Successfully implemented a generalizable platform that can capture cell sorting and aggregation phenomena, demonstrating COMSOL's capability for flexible PIDE implementation in complex biological contexts.", "conclusion": "COMSOL Multiphysics provides an effective and accessible environment for implementing continuum models of directed cell migration, offering a powerful tool for studying collective cell behavior and pattern formation in biological systems."}}
{"id": "2509.08957", "pdf": "https://arxiv.org/pdf/2509.08957", "abs": "https://arxiv.org/abs/2509.08957", "authors": ["Gayana Jayasinghe", "Katrina Morgan", "Jacob Shapiro", "Mengxuan Yang"], "title": "Logarithmic wave decay for short range wavespeed perturbations with radial regularity", "categories": ["math.AP"], "comment": "33 pages", "summary": "We establish logarithmic local energy decay for wave equations with a varying\nwavespeed in dimensions two and higher, where the wavespeed is assumed to be a\nshort range perturbation of unity with mild radial regularity. The key\ningredient is H\\\"older continuity of the weighted resolvent for real\nfrequencies $\\lambda$, modulo a logarithmic remainder in dimension two as\n$\\lambda \\to 0$. Our approach relies on a study of the resolvent in two\ndistinct frequency regimes. In the low frequency regime, we derive an expansion\nfor the resolvent using a Neumann series and properties of the free resolvent.\nFor frequencies away from zero, we establish a uniform resolvent estimate by\nway of a Carleman estimate.", "AI": {"tldr": "Logarithmic local energy decay for wave equations with varying wavespeed in 2+ dimensions, using resolvent analysis in different frequency regimes", "motivation": "To establish energy decay properties for wave equations with perturbed wavespeeds, which has applications in wave propagation and scattering theory", "method": "Analysis of weighted resolvent for real frequencies using two approaches: low frequency expansion via Neumann series and free resolvent properties, and uniform resolvent estimate via Carleman estimate for non-zero frequencies", "result": "Proves logarithmic local energy decay for wave equations with short-range perturbations of unity wavespeed, with mild radial regularity assumptions", "conclusion": "Successfully establishes energy decay properties through resolvent analysis, providing important results for wave propagation in perturbed media across dimensions two and higher"}}
{"id": "2509.09104", "pdf": "https://arxiv.org/pdf/2509.09104", "abs": "https://arxiv.org/abs/2509.09104", "authors": ["Ye Tao", "Lei Chang", "Dingzhou Li", "Yingxin Zhao"], "title": "Exploration of novel ICP using helicon antennas with zero magnetic field", "categories": ["physics.plasm-ph"], "comment": null, "summary": "Inductively coupled plasma (ICP) attracts great attention from aspects of\nfundamental research and practical applications, and efficient power coupling\nis highly desirable for both of them. The present study explores a novel\nstrategy for efficient ICP through using helicon antennas with zero external\nmagnetic field. Specific research is devoted to the effects of antenna geometry\n(loop, half-helix, Boswell, Nagoya III), driving frequency (13.56-54.24 MHz)\nand radial density profile (Gaussian and parabolic) on power coupling. Findings\nreveal that: loop antenna yields higher power deposition efficiency than\nhalf-helix, Boswell, and Nagoya III antennas, driving frequency gives\nnegligible effects, and parabolic density profile results in more efficient\npower coupling than Gaussian density profile especially in the radial\ndirection, for the conditions employed here. Therefore, it is suggested that\nfor this novel ICP strategy one should use loop antenna with parabolic density\nprofile, and the industrial frequency of 13.56 MHz can work well. This study\nprovides a valuable reference for the novel design of efficient ICP sources,\nwhich could be used for material processing and space propulsion, etc. Key\nwords: Inductively coupled plasma; Antenna Geometry; Power Deposition; Driving\nFrequency", "AI": {"tldr": "Loop antenna with parabolic density profile at 13.56 MHz provides most efficient power coupling for ICP without external magnetic field.", "motivation": "To develop efficient inductively coupled plasma (ICP) sources for fundamental research and practical applications like material processing and space propulsion without requiring external magnetic fields.", "method": "Investigated effects of antenna geometry (loop, half-helix, Boswell, Nagoya III), driving frequency (13.56-54.24 MHz), and radial density profile (Gaussian vs parabolic) on power coupling efficiency in ICP using helicon antennas with zero external magnetic field.", "result": "Loop antenna showed highest power deposition efficiency; driving frequency had negligible effects; parabolic density profile resulted in more efficient power coupling than Gaussian profile, especially radially.", "conclusion": "For efficient ICP without external magnetic field, use loop antenna with parabolic density profile at standard industrial frequency of 13.56 MHz."}}
{"id": "2509.08996", "pdf": "https://arxiv.org/pdf/2509.08996", "abs": "https://arxiv.org/abs/2509.08996", "authors": ["Sun Wenming"], "title": "Monte Carlo Simulation of Spallation and Fission Fragment Distributions for ADS-Related Nuclear Reactions", "categories": ["physics.gen-ph"], "comment": "A total of 9 pages, 28 images, and 4 tables", "summary": "Monte Carlo simulations with the CRISP code were conducted to study\nspallation and fission fragment distributions induced by intermediate- and\nhigh-energy protons and photons on actinide and pre-actinide nuclei. The model\naccounts for intranuclear cascade, pre-equilibrium, and evaporation-fission\ncompetition, enabling consistent treatment of both residues and fission\nproducts. Comparisons with experimental data show good agreement in mass and\ncharge distributions, with minor deviations for light fragments. The results\nhighlight the reliability of Monte Carlo approaches for predicting residual\nnuclei and fragment yields under accelerator-driven system (ADS) conditions.\nThis work provides nuclear data relevant to ADS design, safety, and\ntransmutation analysis", "AI": {"tldr": "Monte Carlo simulations study spallation and fission fragment distributions from proton and photon interactions with actinide nuclei, showing good agreement with experimental data.", "motivation": "To provide nuclear data relevant to accelerator-driven system (ADS) design, safety, and transmutation analysis by studying fragment distributions from nuclear reactions.", "method": "Used CRISP code Monte Carlo simulations incorporating intranuclear cascade, pre-equilibrium, and evaporation-fission competition models for consistent treatment of residues and fission products.", "result": "Comparisons with experimental data show good agreement in mass and charge distributions, with minor deviations for light fragments.", "conclusion": "Monte Carlo approaches are reliable for predicting residual nuclei and fragment yields under ADS conditions, providing valuable nuclear data for ADS applications."}}
{"id": "2509.09098", "pdf": "https://arxiv.org/pdf/2509.09098", "abs": "https://arxiv.org/abs/2509.09098", "authors": ["Gregory R. Chambers", "Jared Marx-Kuo"], "title": "Mountain Pass Critical Points of the Liquid Drop Model", "categories": ["math.AP", "math-ph", "math.DG", "math.MP"], "comment": "17 pages, 1 figure, comments welcome!", "summary": "We consider Gamow's liquid drop functional, $\\mathcal{E}$, on $\\mathbb{R}^3$\nand construct non-minimizing, volume constrained, critical points for volumes\n$3.512 \\cong \\alpha_0 < V < 10$. In this range, we establish a mountain pass\nset up between a ball of volume $V$ and two balls of volume $V/2$ infinitely\nfar apart. Intuitively, our critical point corresponds to the maximal energy\nconfiguration of an atom of volume $V$ as it undergoes fission into two atoms\nof volume $V/2$. Our proof relies on geometric measure theoretical methods from\nthe min-max construction of minimal surfaces, and along the way, we address\nissues of non-compactness, ``pull tight\" with a volume constraint, and\nmultiplicity.", "AI": {"tldr": "Construction of non-minimizing critical points for Gamow's liquid drop functional in volume range 3.512 < V < 10, representing maximal energy configuration during nuclear fission.", "motivation": "To understand the energy barrier and critical configurations during nuclear fission processes, specifically the transition from a single atom to two separated atoms.", "method": "Geometric measure theoretical methods from min-max construction of minimal surfaces, addressing non-compactness, volume constraint \"pull tight\", and multiplicity issues.", "result": "Successfully constructed volume-constrained critical points that represent mountain pass configurations between a single ball and two infinitely separated balls.", "conclusion": "The study provides mathematical insight into fission energy barriers and establishes critical configurations that correspond to maximal energy states during nuclear fission transitions."}}
{"id": "2509.09126", "pdf": "https://arxiv.org/pdf/2509.09126", "abs": "https://arxiv.org/abs/2509.09126", "authors": ["Jikai Sun", "Lei Chang", "Yu Liu", "Guojun Wang", "Zichen Kan", "Shijie Zhang", "Jingjing Ma", "Dingzhou Li", "Yingxin Zhao"], "title": "Exploration on the Two-stream Instability in the Polar Cusp Under Solar Storm Disturbances and its Potential Impacts on Spacecraft", "categories": ["physics.plasm-ph"], "comment": null, "summary": "During solar storms, the polar cusp often exhibits electron populations with\ndistinct velocity distributions, which may be associated with the two-stream\ninstability. This study reveals the evolution of the two-stream instability\nassociated with electron velocities and the interaction between the growth\nphase of the two-stream instability and the electrostatic solitary waves\n(ESWs). The results from particle-in-cell (PIC) simulations are compared with\nsatellite observational data and computational outcomes. The potential risks\nassociated with two-stream instability, including surface charge accumulation\nand communication system interference on spacecraft, are also explored. The\nfindings show that, in the high-latitude polar cusp region, the interaction\nbetween the solar wind plasma propagating along magnetic field lines and the\nupward-moving ionospheric plasma could drive two-stream instability, leading to\nthe formation of electron hole structures in phase space and triggering a\nbipolar distribution of ESWs. When the spatial magnetic field and wave vector\nmeet specific conditions, the enhanced electron cyclotron motion could suppress\nthe formation of two-stream instability and electron hole structures, leading\nto a reduction in the amplitude of the ESWs. The results offer valuable\ninsights for a deeper understanding of the impact of solar storms on the polar\ncusp environment, as well as for monitoring electromagnetic environment and\nensuring the stable operation of spacecraft.", "AI": {"tldr": "Study reveals how two-stream instability evolves in polar cusp during solar storms, showing interaction with electrostatic solitary waves and electron cyclotron motion effects.", "motivation": "Understand electron velocity distributions in polar cusp during solar storms and their association with two-stream instability, which poses risks to spacecraft operations.", "method": "Particle-in-cell (PIC) simulations compared with satellite observational data and computational outcomes to analyze instability evolution.", "result": "Solar wind plasma interacting with upward-moving ionospheric plasma drives two-stream instability, forming electron hole structures and bipolar ESWs. Electron cyclotron motion can suppress instability under specific magnetic field conditions.", "conclusion": "Findings provide insights into solar storm impacts on polar cusp environment, valuable for spacecraft electromagnetic monitoring and ensuring stable operation."}}
{"id": "2509.09017", "pdf": "https://arxiv.org/pdf/2509.09017", "abs": "https://arxiv.org/abs/2509.09017", "authors": ["Katerina Beklemysheva", "Egor Michel", "Andrey Ovsiannikov"], "title": "Numerical modeling of elastic waves in thin shells with grid-characteristic method", "categories": ["math.NA", "cs.NA", "physics.comp-ph"], "comment": null, "summary": "Numerical modeling of strength and non-destructive testing of complex\nstructures such as buildings, space rockets or oil reservoirs often involves\ncalculations on extremely large grids. The modeling of elastic wave processes\nin solids places limitations on the grid element size because resolving\ndifferent elastic waves requires at least several grid elements for the\ncharacteristic size of the modeled object. For a thin plate, the defining size\nis its thickness, and a complex structure that contains large-scale thin\nobjects requires a large-scale grid to preserve its uniformity. One way to\nbypass this problem is the theory of thin plates and shells that replaces a\nsimple material model on a fine three-dimensional mesh with a more complex\nmaterial model on a coarser mesh. This approach loses certain fine effects\ninside the thin plate, but allows us to model large and complex thin objects\nwith a reasonable size calculation grid and resolve all the significant wave\ntypes. In this research, we take the Kirchhoff-Love material model and derive a\nhyperbolic dynamic system of equations that allows for a physical\ninterpretation of eigenvalues and eigenvectors. The system is solved\nnumerically with a grid-characteristic method. Numerical results for several\nmodel statements are compared with three-dimensional calculations based on\ngrid-characteristic method for a three dimensional elasticity.", "AI": {"tldr": "Developed a hyperbolic dynamic system based on Kirchhoff-Love theory for modeling thin plates/shells, enabling efficient wave process simulation on coarser grids compared to 3D elasticity calculations.", "motivation": "Numerical modeling of complex structures with thin components requires extremely large grids due to element size constraints for resolving elastic waves. Thin plate theory offers a way to bypass this problem by using complex material models on coarser meshes.", "method": "Derived a hyperbolic dynamic system from Kirchhoff-Love material model that allows physical interpretation of eigenvalues/eigenvectors. Solved numerically using grid-characteristic method.", "result": "Numerical results for model statements show comparable performance to 3D calculations based on grid-characteristic method for 3D elasticity, but with significantly reduced computational requirements.", "conclusion": "The proposed approach enables efficient modeling of large complex thin structures with reasonable grid sizes while resolving significant wave types, making it suitable for non-destructive testing applications."}}
{"id": "2509.09231", "pdf": "https://arxiv.org/pdf/2509.09231", "abs": "https://arxiv.org/abs/2509.09231", "authors": ["Rejeb Hadiji", "Jongmin Han"], "title": "On the Convergence of Solutions for the Ginzburg-Landau Equation and System", "categories": ["math.AP", "35B40, 35J60, 35Q60"], "comment": null, "summary": "Let $(u_\\varepsilon)$ be a family of solutions of the Ginzburg--Landau\nequation with boundary condition $u_\\varepsilon = g$ on $\\partial \\Omega$ and\nof degree $0$. Let $u_0$ denote the harmonic map satisfying $u_0 = g$ on\n$\\partial \\Omega$. We show that, if there exists a constant $C_1 > 0$ such that\nfor $\\varepsilon$ sufficiently small we have $\\frac{1}{2} \\int_\\Omega |\\nabla\nu_\\ve|^2 dx \\leq C_1 \\leq \\frac{1}{2} \\int_\\Omega |\\nabla u_0|^2 dx,$ then $C_1\n= \\frac{1}{2} \\int_\\Omega |\\nabla u_0|^2 dx$ and\n  $u_\\ve ~\\to ~ u_0 \\qin H^1(\\Om)$.\n  We also prove that if there is a constant $C_2$ such that for $\\ve$ small\nenough we have $ \\frac12 \\int_\\Om |\\nabla u_\\ve|^2 dx \\geq C_2 > \\frac12\n\\int_\\Om |\\nabla u_0|^2 dx,$ then $|u_{\\ve}|$ does not converge uniformly to\n$1$ on $\\overline{\\Om} $. We obtain analogous results for both symmetric and\nnon-symmetric two-component Ginzburg--Landau systems.", "AI": {"tldr": "The paper analyzes Ginzburg-Landau equation solutions with boundary conditions, showing that if energy is bounded above by harmonic map energy, solutions converge to harmonic map; if energy exceeds harmonic map energy, uniform convergence to 1 fails.", "motivation": "To understand the convergence behavior of Ginzburg-Landau equation solutions to harmonic maps and characterize when uniform convergence to the vacuum state occurs based on energy bounds.", "method": "Mathematical analysis of solutions to Ginzburg-Landau equations with boundary conditions, comparing energy functionals and establishing convergence properties in Sobolev spaces.", "result": "Proved that if solution energy is bounded above by harmonic map energy, then solutions converge to harmonic map in H^1; if energy exceeds harmonic map energy, |u_\u03b5| does not converge uniformly to 1.", "conclusion": "The energy threshold of the harmonic map determines convergence behavior - solutions either converge to the harmonic map or fail to uniformly approach the vacuum state, with analogous results for symmetric and non-symmetric two-component systems."}}
{"id": "2509.09040", "pdf": "https://arxiv.org/pdf/2509.09040", "abs": "https://arxiv.org/abs/2509.09040", "authors": ["Charles D. Arrowsmith", "Francesco Miniati", "Pablo J. Bilbao", "Pascal Simon", "Archie F. A. Bott", "Stephane Burger", "Hui Chen", "Filipe D. Cruz", "Tristan Davenne", "Anthony Dyson", "Ilias Efthymiopoulos", "Dustin H. Froula", "Alice Goillot", "Jon T. Gudmundsson", "Dan Haberberger", "Jack W. D. Halliday", "Tom Hodge", "Brian T. Huffman", "Sam Iaquinta", "Graham Marshall", "Brian Reville", "Subir Sarkar", "Alexander A. Schekochihin", "Luis O. Silva", "Raspberry Simpson", "Vasiliki Stergiou", "Raoul M. G. M. Trines", "Thibault Vieu", "Nikolaos Charitonidis", "Robert Bingham", "Gianluca Gregori"], "title": "Suppression of pair beam instabilities in a laboratory analogue of blazar pair cascades", "categories": ["astro-ph.HE", "physics.plasm-ph"], "comment": null, "summary": "The generation of dense electron-positron pair beams in the laboratory can\nenable direct tests of theoretical models of $\\gamma$-ray bursts and active\ngalactic nuclei. We have successfully achieved this using ultra-relativistic\nprotons accelerated by the Super Proton Synchrotron at CERN. In the first\napplication of this experimental platform, the stability of the pair beam is\nstudied as it propagates through a metre-length plasma, analogous to TeV\n$\\gamma$-ray induced pair cascades in the intergalactic medium. It has been\nargued that pair beam instabilities disrupt the cascade, thus accounting for\nthe observed lack of reprocessed GeV emission from TeV blazars. If true this\nwould remove the need for a moderate strength intergalactic magnetic field to\nexplain the observations. We find that the pair beam instability is suppressed\nif the beam is not perfectly collimated or monochromatic, hence the lower limit\nto the intergalactic magnetic field inferred from $\\gamma$-ray observations of\nblazars is robust.", "AI": {"tldr": "Laboratory generation of electron-positron pair beams using CERN's Super Proton Synchrotron enables testing of astrophysical models, showing that pair beam instability is suppressed when beams are not perfectly collimated/monochromatic, validating intergalactic magnetic field estimates from blazar observations.", "motivation": "To enable direct laboratory testing of theoretical models for gamma-ray bursts and active galactic nuclei, specifically to study pair beam stability and its implications for explaining the lack of reprocessed GeV emission from TeV blazars.", "method": "Used ultra-relativistic protons accelerated by CERN's Super Proton Synchrotron to generate dense electron-positron pair beams, then studied beam stability as it propagates through a metre-length plasma to simulate TeV gamma-ray induced pair cascades in the intergalactic medium.", "result": "The pair beam instability is suppressed when the beam is not perfectly collimated or monochromatic, indicating that the instability does not disrupt the cascade as previously argued.", "conclusion": "The lower limit to the intergalactic magnetic field inferred from gamma-ray observations of blazars remains robust, as the pair beam instability does not provide an alternative explanation for the observed lack of reprocessed GeV emission."}}
{"id": "2509.08902", "pdf": "https://arxiv.org/pdf/2509.08902", "abs": "https://arxiv.org/abs/2509.08902", "authors": ["Qiumei Huang", "Alexander Ostermann", "Gangfan Zhong"], "title": "Exponential Runge-Kutta methods for parabolic equations with state-dependent delay", "categories": ["math.NA", "cs.NA", "65M12, 65L06"], "comment": null, "summary": "The aim of this paper is to construct and analyze exponential Runge-Kutta\nmethods for the temporal discretization of a class of semilinear parabolic\nproblems with arbitrary state-dependent delay. First, the well-posedness of the\nproblem is established. Subsequently, first and second order schemes are\nconstructed. They are based on the explicit exponential Runge-Kutta methods,\nwhere the delayed solution is approximated by a continuous extension of the\ntime discrete solution. Schemes of arbitrary order can be constructed using the\nmethods of collocation type. The unique solvability and convergence of the\nproposed schemes are established. Finally, we discuss implementation issues and\npresent some numerical experiments to illustrate our theoretical results.", "AI": {"tldr": "Construction and analysis of exponential Runge-Kutta methods for semilinear parabolic problems with arbitrary state-dependent delay, including well-posedness, scheme development, convergence analysis, and numerical validation.", "motivation": "To develop efficient temporal discretization methods for semilinear parabolic problems with arbitrary state-dependent delay, which are challenging due to the delay dependency on the solution state.", "method": "Construct first and second order explicit exponential Runge-Kutta methods using continuous extensions for delayed solution approximation, with collocation-type methods for arbitrary orders. Establish unique solvability and convergence properties.", "result": "Successfully developed exponential Runge-Kutta schemes that handle state-dependent delays, proved well-posedness of the problem, established unique solvability and convergence of the proposed methods.", "conclusion": "The proposed exponential Runge-Kutta methods provide effective numerical solutions for semilinear parabolic problems with arbitrary state-dependent delay, with theoretical convergence guarantees and practical implementation demonstrated through numerical experiments."}}
{"id": "2509.09235", "pdf": "https://arxiv.org/pdf/2509.09235", "abs": "https://arxiv.org/abs/2509.09235", "authors": ["Sarah C. Irvine", "Christian Lucas", "Diana Kr\u00fcger", "Bianca Guedert", "Julian Moosmann", "Berit Zeller-Plumhoff"], "title": "Virtual staining for 3D X-ray histology of bone implants", "categories": ["eess.IV", "cs.AI", "cs.CV", "physics.comp-ph", "q-bio.QM"], "comment": null, "summary": "Three-dimensional X-ray histology techniques offer a non-invasive alternative\nto conventional 2D histology, enabling volumetric imaging of biological tissues\nwithout the need for physical sectioning or chemical staining. However, the\ninherent greyscale image contrast of X-ray tomography limits its biochemical\nspecificity compared to traditional histological stains. Within digital\npathology, deep learning-based virtual staining has demonstrated utility in\nsimulating stained appearances from label-free optical images. In this study,\nwe extend virtual staining to the X-ray domain by applying cross-modality image\ntranslation to generate artificially stained slices from\nsynchrotron-radiation-based micro-CT scans. Using over 50 co-registered image\npairs of micro-CT and toluidine blue-stained histology from bone-implant\nsamples, we trained a modified CycleGAN network tailored for limited paired\ndata. Whole slide histology images were downsampled to match the voxel size of\nthe CT data, with on-the-fly data augmentation for patch-based training. The\nmodel incorporates pixelwise supervision and greyscale consistency terms,\nproducing histologically realistic colour outputs while preserving\nhigh-resolution structural detail. Our method outperformed Pix2Pix and standard\nCycleGAN baselines across SSIM, PSNR, and LPIPS metrics. Once trained, the\nmodel can be applied to full CT volumes to generate virtually stained 3D\ndatasets, enhancing interpretability without additional sample preparation.\nWhile features such as new bone formation were able to be reproduced, some\nvariability in the depiction of implant degradation layers highlights the need\nfor further training data and refinement. This work introduces virtual staining\nto 3D X-ray imaging and offers a scalable route for chemically informative,\nlabel-free tissue characterisation in biomedical research.", "AI": {"tldr": "This paper introduces virtual staining for 3D X-ray imaging, using deep learning to generate artificially stained histology images from micro-CT scans without physical staining or sectioning.", "motivation": "3D X-ray histology provides non-invasive volumetric imaging but lacks biochemical specificity compared to traditional stained histology. The researchers aim to extend virtual staining techniques from optical to X-ray domain to enhance interpretability without additional sample preparation.", "method": "Used over 50 co-registered micro-CT and toluidine blue-stained histology image pairs. Trained a modified CycleGAN network tailored for limited paired data, incorporating pixelwise supervision and greyscale consistency terms. Applied on-the-fly data augmentation for patch-based training with downsampled whole slide histology images.", "result": "The modified CycleGAN outperformed Pix2Pix and standard CycleGAN baselines across SSIM, PSNR, and LPIPS metrics. The model produces histologically realistic color outputs while preserving high-resolution structural detail. Features like new bone formation were reproduced, though some variability in implant degradation layers was noted.", "conclusion": "This work successfully introduces virtual staining to 3D X-ray imaging, offering a scalable route for chemically informative, label-free tissue characterization in biomedical research. The approach enhances interpretability without additional sample preparation, though further training data and refinement are needed for certain features."}}
{"id": "2509.09237", "pdf": "https://arxiv.org/pdf/2509.09237", "abs": "https://arxiv.org/abs/2509.09237", "authors": ["Giacomo Bertazzoni", "Elisa Davoli", "Samuele Ricco`", "Elvira Zappale"], "title": "Functions of bounded Musielak-Orlicz-type deformation and anisotropic Total Generalized Variation for image-denoising problems", "categories": ["math.AP"], "comment": null, "summary": "In the first part of this paper we introduce the space of bounded deformation\nfields with generalized Orlicz growth. We establish their main properties,\nprovide a modular representation, and characterize a decomposition of the\nmodular into an absolutely continuous part and a singular part weighted via a\nrecession function. A further analysis in the variable exponent case is also\nprovided. The second part of the paper contains a notion of Musielak-Orlicz\nanisotropic Total Generalized Variation. We establish a duality representation,\nand show well-posedness of the corresponding image reconstruction problem.", "AI": {"tldr": "The paper introduces bounded deformation fields with generalized Orlicz growth and establishes their properties, then defines Musielak-Orlicz anisotropic Total Generalized Variation for image reconstruction problems.", "motivation": "To develop a mathematical framework for analyzing deformation fields with generalized growth conditions and apply it to image reconstruction problems using anisotropic variational methods.", "method": "Introduces space of bounded deformation fields with generalized Orlicz growth, establishes modular representation and decomposition. Defines Musielak-Orlicz anisotropic Total Generalized Variation with duality representation.", "result": "Established main properties of bounded deformation fields, provided modular representation and decomposition. Showed well-posedness of image reconstruction problems using the proposed anisotropic Total Generalized Variation framework.", "conclusion": "The paper provides a comprehensive mathematical framework for deformation analysis with generalized growth conditions and demonstrates its applicability to image reconstruction through anisotropic variational methods with proven well-posedness."}}
{"id": "2509.09057", "pdf": "https://arxiv.org/pdf/2509.09057", "abs": "https://arxiv.org/abs/2509.09057", "authors": ["Yici Zhong", "Elias R. Most"], "title": "Unraveling the emission mechanism powering long period radio transients from interacting white dwarf binaries via kinetic plasma simulations", "categories": ["astro-ph.HE", "astro-ph.SR", "physics.plasm-ph"], "comment": "13 pages, 5 figures", "summary": "Recent observations of long period radio transients, such as GLEAM-X J0704-37\nand ILTJ1101 + 5521, have revealed a previously unrecognized population of\ngalactic radio transient sources associated with white dwarf - M dwarf\nbinaries. It is an open question how to produce coherent radio emission in\nthese systems, though a model driven by binary interaction seems likely given\nthe nature and correlation of the emission with the binaries' orbital period.\nUsing kinetic plasma simulations, we demonstrate that the relativistic electron\ncyclotron maser instability (ECMI) is a viable mechanism for generating radio\npulses in white dwarf - M dwarf systems, akin to planetary radio emission, such\nas that from the Jupiter-Io system. We quantify the relativistic ECMI in the\nnonlinear regime under conditions relevant for white dwarf radio emission for\nthe first time. Our simulations demonstrate that the ECMI can intrinsically\nproduce partially linearly polarized emission relevant to explaining the\nobserved emission spectrum of the two galactic sources, though the precise\ndetails will depend on the plasma composition. Our work paves the way for a\nsystematic and fully nonlinear computational modeling of radio emission from\ninteracting white dwarf sources.", "AI": {"tldr": "The paper demonstrates that the relativistic electron cyclotron maser instability (ECMI) is a viable mechanism for generating radio pulses in white dwarf-M dwarf binary systems, similar to planetary radio emissions.", "motivation": "Recent observations of long period radio transients from white dwarf-M dwarf binaries require explanation of how coherent radio emission is produced in these systems, suggesting binary interaction-driven models.", "method": "The researchers used kinetic plasma simulations to quantify the relativistic ECMI in the nonlinear regime under conditions relevant for white dwarf radio emission.", "result": "Simulations show ECMI can intrinsically produce partially linearly polarized emission that explains the observed emission spectrum of galactic sources, though details depend on plasma composition.", "conclusion": "This work enables systematic and fully nonlinear computational modeling of radio emission from interacting white dwarf sources, establishing ECMI as a viable emission mechanism."}}
{"id": "2509.08936", "pdf": "https://arxiv.org/pdf/2509.08936", "abs": "https://arxiv.org/abs/2509.08936", "authors": ["Lise-Marie Imbert-G\u00e9rard", "Andr\u00e9a Lagard\u00e8re", "Guillaume Sylvand", "S\u00e9bastien Tordeux"], "title": "Quasi-Trefftz spaces for a first-order formulation of the Helmholtz equation", "categories": ["math.NA", "cs.NA"], "comment": null, "summary": "This work is the first step in the development of quasi-Trefftz methods for\nfirst-order differential systems. It focuses on discrete quasi-Trefftz spaces,\nstarting from their definition and including construction of corresponding\nbases together with their computational aspect.", "AI": {"tldr": "Development of quasi-Trefftz methods for first-order differential systems, focusing on discrete quasi-Trefftz spaces including definition, basis construction, and computational aspects.", "motivation": "To establish the foundation for quasi-Trefftz methods applied to first-order differential systems, which represents a novel approach in numerical methods for differential equations.", "method": "Defines discrete quasi-Trefftz spaces, constructs corresponding bases, and addresses computational implementation aspects for these methods.", "result": "This work provides the initial framework and computational tools for quasi-Trefftz methods in first-order differential systems.", "conclusion": "This foundational work enables further development and application of quasi-Trefftz methods to solve first-order differential systems numerically."}}
{"id": "2509.09253", "pdf": "https://arxiv.org/pdf/2509.09253", "abs": "https://arxiv.org/abs/2509.09253", "authors": ["Shuai Zhang", "Mengqi Wang", "Tiantian Zhang"], "title": "Electronic order induced symmetry breaking in lattice dynamics of Co$_3$Sn$_2$S$_2$", "categories": ["cond-mat.mtrl-sci", "physics.comp-ph"], "comment": null, "summary": "Based on the molecular Berry curvature (MBC) framework, we develop an\n\\textit{ab initio} algorithm to capture the quantitative effects of magnetic\norder on lattice dynamics. Using the ferromagnetic Weyl semimetal\nCo$_3$Sn$_2$S$_2$ as a prototype, we show that electronic-order-driven phonon\nsymmetry breaking requires spin-orbit coupling (SOC) and leads to an MBC term\nthat breaks both time-reversal ($\\mathcal{T}$) and mirror symmetries. We\ndemonstrate that mirror-symmetry breaking is essential to account for the\nexperimentally observed phonon splitting, $\\mathcal{T}$-breaking alone is\ninsufficient. The MBC is widely distributed across the Brillouin zone, giving\nrise to significant off-$\\Gamma$ effects. Our results agree well with\nexperiments and establish a framework for predicting large phonon magnetism in\nmagnetic materials with strong spin-orbit coupling and electron-phonon\ncoupling. This work also suggests new avenues for controlling non-reciprocal\nphonon transport.", "AI": {"tldr": "Developed ab initio algorithm using molecular Berry curvature to quantify magnetic order effects on lattice dynamics in ferromagnetic Weyl semimetals like Co3Sn2S2, showing spin-orbit coupling and mirror symmetry breaking are essential for observed phonon splitting.", "motivation": "To understand and predict how magnetic order quantitatively affects lattice dynamics in magnetic materials, particularly the role of spin-orbit coupling and symmetry breaking in phonon behavior.", "method": "Developed an ab initio algorithm based on molecular Berry curvature framework, applied to ferromagnetic Weyl semimetal Co3Sn2S2 to analyze electronic-order-driven phonon symmetry breaking.", "result": "Found that spin-orbit coupling is required for phonon symmetry breaking, leading to molecular Berry curvature terms that break time-reversal and mirror symmetries. Mirror symmetry breaking is essential for observed phonon splitting, with significant off-Gamma effects across Brillouin zone. Results agree well with experiments.", "conclusion": "Established framework for predicting large phonon magnetism in magnetic materials with strong spin-orbit and electron-phonon coupling, suggesting new avenues for controlling non-reciprocal phonon transport."}}
{"id": "2509.09276", "pdf": "https://arxiv.org/pdf/2509.09276", "abs": "https://arxiv.org/abs/2509.09276", "authors": ["Francis Filbet", "Yanzhi Gui", "Ling-Bing He"], "title": "Numerical analysis of the homogeneous Landau equation: approximation, error estimates and simulation", "categories": ["math.AP", "cs.NA", "math.NA"], "comment": null, "summary": "We construct a numerical solution to the spatially homogeneous Landau\nequation with Coulomb potential on a domain $D_L$ with N retained Fourier\nmodes. By deriving an explicit error estimate in terms of $L$ and $N$, we\ndemonstrate that for any prescribed error tolerance and fixed time interval\n$[0, T ]$, there exist choices of $D_L$ and $N$ satisfying explicit conditions\nsuch that the error between the numerical and exact solutions is below the\ntolerance. Specifically, the estimate shows that sufficiently large $L$ and $N$\n(depending on initial data parameters and $T$) can reduce the error to any\ndesired level. Numerical simulations based on this construction are also\npresented. The results in particular demonstrate the mathematical validity of\nthe spectral method proposed in the referenced literature.", "AI": {"tldr": "Numerical solution for Landau equation with Coulomb potential using spectral method with error control", "motivation": "To provide rigorous error estimates for numerical solutions of the spatially homogeneous Landau equation with Coulomb potential, ensuring computational reliability", "method": "Construct numerical solution using spectral method with N Fourier modes on domain D_L, derive explicit error estimates in terms of L (domain size) and N (number of modes)", "result": "Proved that for any error tolerance and fixed time interval, explicit conditions exist for L and N to achieve desired accuracy. Numerical simulations confirm theoretical estimates", "conclusion": "The spectral method is mathematically valid and provides controllable error bounds for solving Landau equation with Coulomb potential"}}
{"id": "2509.09487", "pdf": "https://arxiv.org/pdf/2509.09487", "abs": "https://arxiv.org/abs/2509.09487", "authors": ["Snehanshu Maiti", "Shishir Biswas", "Rajaraman Ganesh"], "title": "Vorticity Packing Effects on Turbulent Transport in Decaying 2D Incompressible Navier-Stokes Fluids", "categories": ["physics.flu-dyn", "nlin.CD", "physics.comp-ph", "physics.plasm-ph"], "comment": null, "summary": "This paper investigates the role of initial vorticity packing fractions on\nthe transport properties of decaying incompressible two-dimensional\nNavier-Stokes turbulence at very high Reynolds numbers and spatial resolutions.\nTurbulence is initiated via the Kelvin-Helmholtz instability and evolves\nthrough nonlinear inverse energy cascades, forming large-scale coherent\nstructures that dominate the flow over long eddy turnover times. The initial\nvorticity packing fraction and circulation direction lead to qualitatively\ndistinct turbulence dynamics and transport behaviors. Tracer particle\ntrajectories are computed in the fluid field obtained using the Eulerian\nframework, with transport and mixing quantified using statistical measures such\nas absolute dispersion, position probability distribution functions (PDFs), and\nvelocity PDFs. In the early stages, the onset of turbulence is primarily\ngoverned by the instability growth rate, which increases with vorticity packing\nfraction. As the flow evolves, transport exhibits a range of\nbehaviors-subdiffusive, diffusive, or superdiffusive-and transitions between\nanisotropic and isotropic regimes, depending on the initial vorticity packing,\nflow structure, and stage of evolution. At later times, transport is dominated\nby the motion of large-scale coherent vortices, whose dynamics are also\ninfluenced by the initial vorticity packing ranging from subdiffusive trapping\nrotational motion and random walks, and L\\'evy flight-like events. These\nfindings offer insights into transport in quasi-2D systems-ranging from\nlaboratory-scale flows to geophysical phenomena and astrophysical\nstructures-through analogies with 2D Navier-Stokes turbulence.", "AI": {"tldr": "Study examines how initial vorticity packing fractions affect transport in 2D turbulence, showing distinct transport behaviors from subdiffusive to superdiffusive regimes influenced by initial conditions and large-scale vortex dynamics.", "motivation": "To understand how initial vorticity conditions influence transport properties in 2D turbulence, which has applications ranging from laboratory flows to geophysical and astrophysical systems.", "method": "Simulated decaying incompressible 2D Navier-Stokes turbulence initiated via Kelvin-Helmholtz instability, computed tracer particle trajectories, and analyzed transport using statistical measures like absolute dispersion, position PDFs, and velocity PDFs.", "result": "Initial vorticity packing fraction significantly affects turbulence dynamics - higher packing increases instability growth rate early on, while later transport shows range of behaviors (subdiffusive, diffusive, superdiffusive) and transitions between anisotropic/isotropic regimes dominated by large-scale vortices.", "conclusion": "Transport in 2D turbulence is strongly influenced by initial vorticity conditions, with coherent vortex dynamics driving various transport regimes, providing insights for understanding transport in quasi-2D systems across different scales."}}
{"id": "2509.08990", "pdf": "https://arxiv.org/pdf/2509.08990", "abs": "https://arxiv.org/abs/2509.08990", "authors": ["Shalmali Bandyopadhyay", "Thomas Lewis", "Dustin Nichols"], "title": "Numerical Approximation and Bifurcation Results for an Elliptic Problem with Superlinear Subcritical Nonlinearity on the Boundary", "categories": ["math.NA", "cs.NA", "math.AP"], "comment": null, "summary": "We develop numerical algorithms to approximate positive solutions of elliptic\nboundary value problems with superlinear subcritical nonlinearity on the\nboundary of the form $-\\Delta u + u = 0$ in $\\Omega$ with $\\frac{\\partial\nu}{\\partial \\eta} = \\lambda f(u)$ on $\\partial\\Omega$ as well as an extension\nto a corresponding system of equations. While existence, uniqueness,\nnonexistence, and multiplicity results for such problems are well-established,\ntheir numerical treatment presents computational challenges due to the absence\nof comparison principles and complex bifurcation phenomena. We present finite\ndifference formulations for both single equations and coupled systems with\ncross-coupling boundary conditions, establishing admissibility results for the\nfinite difference method. We derive principal eigenvalue analysis for the\nlinearized problems to determine unique bifurcation points from trivial\nsolutions. The eigenvalue analysis provides additional insight into the\ntheoretical properties of the problem while also providing intuition for\ncomputing approximate solutions based on the proposed finite difference\nformulation. We combine our finite difference methods with continuation methods\nto trace complete bifurcation curves, validating established existence and\nuniqueness results and consistent with the results of the principle eigenvalue\nanalysis.", "AI": {"tldr": "Numerical algorithms for approximating positive solutions of elliptic boundary value problems with superlinear subcritical nonlinearity on the boundary, including extension to coupled systems with cross-coupling boundary conditions.", "motivation": "While existence, uniqueness, nonexistence, and multiplicity results for such problems are well-established theoretically, their numerical treatment presents computational challenges due to the absence of comparison principles and complex bifurcation phenomena.", "method": "Finite difference formulations for both single equations and coupled systems, combined with continuation methods to trace complete bifurcation curves. Principal eigenvalue analysis for linearized problems to determine unique bifurcation points from trivial solutions.", "result": "The finite difference methods successfully approximate solutions and trace bifurcation curves, validating established theoretical results and providing computational tools for these challenging problems.", "conclusion": "The proposed numerical approach provides effective computational methods for handling elliptic boundary value problems with complex nonlinear boundary conditions, offering both theoretical insight through eigenvalue analysis and practical solution approximation capabilities."}}
{"id": "2509.09275", "pdf": "https://arxiv.org/pdf/2509.09275", "abs": "https://arxiv.org/abs/2509.09275", "authors": ["Lixing Zhang", "Di luo"], "title": "Neural Transformer Backflow for Solving Momentum-Resolved Ground States of Strongly Correlated Materials", "categories": ["cond-mat.str-el", "physics.comp-ph"], "comment": "11 pages, 6 figures", "summary": "Strongly correlated materials, such as twisted transition-metal\ndichalcogenide homobilayers, host a variety of exotic quantum phases but remain\nnotoriously difficult to solve due to strong interactions. We introduce a\npowerful neural network ansatz, Neural Transformer Backflow (NTB), formulated\nwithin a multi-band projection framework. It naturally enforces momentum\nconservation and enables efficient calculations of momentum-resolved ground\nstates. NTB attains high accuracy on small systems and scales to higher bands\nand larger system sizes far beyond the reach of exact diagonalization. By\nevaluating observables such as the structure factor and momentum distribution,\nwe show that NTB captures diverse correlated states in tMoTe$_2$, including\ncharge density waves, fractional Chern insulators, and anomalous Hall Fermi\nliquids, within a unified framework. Our approach paves the way for\nunderstanding and discovering novel phases of matter in strongly correlated\nmaterials.", "AI": {"tldr": "Neural Transformer Backflow (NTB) - a neural network ansatz for solving strongly correlated materials like twisted MoTe2 bilayers, enabling accurate calculation of momentum-resolved ground states and capturing diverse quantum phases.", "motivation": "Strongly correlated materials host exotic quantum phases but are notoriously difficult to solve due to strong interactions, requiring new computational approaches beyond exact diagonalization.", "method": "Developed Neural Transformer Backflow (NTB) ansatz within multi-band projection framework that enforces momentum conservation and enables efficient momentum-resolved ground state calculations.", "result": "NTB achieves high accuracy on small systems and scales to larger systems beyond exact diagonalization reach, capturing charge density waves, fractional Chern insulators, and anomalous Hall Fermi liquids in tMoTe2.", "conclusion": "NTB provides a unified framework for understanding and discovering novel phases in strongly correlated materials, paving the way for computational studies of complex quantum systems."}}
{"id": "2509.09335", "pdf": "https://arxiv.org/pdf/2509.09335", "abs": "https://arxiv.org/abs/2509.09335", "authors": ["Manil T. Mohan"], "title": "Well-posedness of stationary 2D and 3D convective Brinkman-Forchheimer extended Darcy Hemivariational inequalities", "categories": ["math.AP"], "comment": null, "summary": "This study addresses the well-posedness of a hemivariational inequality\nderived from the convective Brinkman-Forchheimer extended Darcy (CBFeD) model\nin both two and three dimensions. The CBFeD model describes the behavior of\nincompressible viscous fluid flow through a porous medium, incorporating the\neffects of convection, damping, and nonlinear resistance. The mathematical\nframework captures steady-state flow conditions under a no-slip boundary\nassumption, with a non-monotone boundary condition that links the total fluid\npressure and the velocity's normal component through a Clarke subdifferential\nformulation. To facilitate the analysis, we introduce an auxiliary\nhemivariational inequality resembling a nonlinear Stokes-type problem with\ndamping and pumping terms, which serves as a foundational tool in establishing\nthe existence and uniqueness of weak solutions for the CBFeD model. The\nanalytical strategy integrates techniques from convex minimization theory with\nfixed-point methods, specifically employing either the Banach contraction\nmapping principle or Schauder's fixed point theorem. The Banach-based approach,\nin particular, leads to a practical iterative algorithm that solves the\noriginal nonlinear hemivariational inequality by sequentially solving\nStokes-type problems, ensuring convergence of the solution sequence.\nAdditionally, we derive equivalent variational formulations in terms of\nminimization problems. These formulations lay the groundwork for the design of\nefficient and stable numerical schemes tailored to simulate flows governed by\nthe CBFeD model.", "AI": {"tldr": "Analysis of well-posedness for convective Brinkman-Forchheimer extended Darcy model hemivariational inequality in 2D/3D, with existence/uniqueness proofs and numerical scheme foundations.", "motivation": "To establish mathematical foundations for the CBFeD model describing incompressible viscous fluid flow through porous media with convection, damping, and nonlinear resistance effects under steady-state conditions.", "method": "Introduces auxiliary hemivariational inequality resembling nonlinear Stokes-type problem, integrates convex minimization theory with fixed-point methods (Banach contraction mapping or Schauder's theorem), and develops iterative algorithm solving Stokes-type problems sequentially.", "result": "Establishes existence and uniqueness of weak solutions for CBFeD model, provides convergent iterative algorithm, and derives equivalent variational formulations for numerical scheme design.", "conclusion": "Successfully proves well-posedness of CBFeD hemivariational inequality, provides mathematical framework for analyzing porous media flows, and lays foundation for efficient numerical simulations of such flows."}}
{"id": "2509.09409", "pdf": "https://arxiv.org/pdf/2509.09409", "abs": "https://arxiv.org/abs/2509.09409", "authors": ["Coleman Hines", "James Kolesar", "Peter McGrath"], "title": "New Homogeneous Solutions for the One-Phase Free Boundary Problem", "categories": ["math.AP", "math.DG", "35R35"], "comment": null, "summary": "For each sufficiently large integer $k$, we construct a domain in the round\n$2$-sphere with $k$ boundary components which is the link of a cone in\n$\\mathbb{R}^3$ admitting a homogeneous solution to the one-phase free boundary\nproblem. This answers a question of Jerison-Kamburov, and also disproves a\nconjecture of Souam left open in earlier work. The method exploits a new\nconnection with minimal surfaces, which we also use to construct an infinite\nfamily of homogeneous solutions in dimension four.", "AI": {"tldr": "Construction of domains in 2-sphere with k boundary components that are links of cones in R^3 admitting homogeneous solutions to one-phase free boundary problem, answering Jerison-Kamburov's question and disproving Souam's conjecture.", "motivation": "To answer a question by Jerison-Kamburov regarding the existence of domains with multiple boundary components that serve as links of cones admitting homogeneous solutions to the one-phase free boundary problem, and to test Souam's conjecture.", "method": "Exploits a new connection with minimal surfaces and constructs domains in the round 2-sphere with k boundary components for sufficiently large integers k.", "result": "Successfully constructed such domains for each sufficiently large k, providing counterexamples that disprove Souam's conjecture and answering Jerison-Kamburov's question. Also constructed an infinite family of homogeneous solutions in dimension four.", "conclusion": "The paper establishes a novel connection between free boundary problems and minimal surfaces, demonstrates the existence of multiple boundary component domains serving as cone links with homogeneous solutions, and extends results to higher dimensions."}}
{"id": "2509.09023", "pdf": "https://arxiv.org/pdf/2509.09023", "abs": "https://arxiv.org/abs/2509.09023", "authors": ["Austen J. Nelson", "Panayot S. Vassilevski"], "title": "Characterization of the near-null error components utilized in composite adaptive AMG solvers", "categories": ["math.NA", "cs.NA", "15 (Primary), 65 (Secondary)"], "comment": "16 pages, 6 figures, presented at 22nd Copper Mountain Conference on\n  Multigrid Methods", "summary": "We provide a theoretical justification for the construction of adaptive\ncomposite solvers based on a sequence of AMG (algebraic multigrid) $\\mu$-cycle\nmethods that exploit error components that the current solver cannot damp\nefficiently. Each solver component is an aggregation based AMG where its\naggregates are constructed using the popular in graph community detection\nmodularity matrix. The latter utilizes the given matrix and the error component\nvector the current solver cannot handle. The performance of the resulting\nadaptive composite solver is illustrated on a variety of sparse matrices both\narising from discretized PDEs and ones with more general nature.", "AI": {"tldr": "Theoretical justification for adaptive composite solvers using AMG \u03bc-cycle methods with modularity-based aggregation to handle error components that standard solvers cannot damp efficiently.", "motivation": "To improve solver performance by addressing error components that current AMG methods cannot efficiently handle, particularly for sparse matrices from PDE discretizations and general problems.", "method": "Construct adaptive composite solvers using a sequence of AMG \u03bc-cycle methods with aggregation based on modularity matrix from graph community detection, utilizing both the matrix and error component vectors.", "result": "The adaptive composite solver demonstrates improved performance across various sparse matrices, including those from discretized PDEs and more general problems.", "conclusion": "The proposed adaptive composite AMG approach with modularity-based aggregation effectively handles challenging error components and shows promising performance on diverse sparse matrix problems."}}
{"id": "2509.09531", "pdf": "https://arxiv.org/pdf/2509.09531", "abs": "https://arxiv.org/abs/2509.09531", "authors": ["Fatemeh Haddadi", "Davide Campi", "Flaviano dos Santos", "Nicolas Mounet", "Louis Ponet", "Nicola Marzari", "Marco Gibertini"], "title": "Exploring the magnetic landscape of easily-exfoliable two-dimensional materials", "categories": ["cond-mat.mtrl-sci", "physics.comp-ph"], "comment": "29 pages including references, 8 figures", "summary": "Magnetic materials often exhibit complex energy landscapes with multiple\nlocal minima, each corresponding to a self-consistent electronic structure\nsolution. Finding the global minimum is challenging, and heuristic methods are\nnot always guaranteed to succeed. Here, we apply a recently developed automated\nworkflow to systematically explore the energy landscape of 194 magnetic\nmonolayers obtained from the Materials Cloud 2D crystals database and determine\ntheir ground-state magnetic order. Our approach enables effective control and\nsampling of orbital occupation matrices, allowing rapid identification of local\nminima. We find a diverse set of self-consistent collinear metastable states,\nfurther enriched by Hubbard-corrected energy functionals, when the $U$\nparameters have been computed from first principles using linear-response\ntheory. We categorise the monolayers by their magnetic ordering and highlight\npromising candidates. Our results include 109 ferromagnetic, 83\nantiferromagnetic, and 2 altermagnetic monolayers, along with 12 novel\nferromagnetic half-metals with potential for spintronics technologies.", "AI": {"tldr": "Automated workflow applied to explore energy landscapes of 194 magnetic monolayers from Materials Cloud database, identifying ground-state magnetic orders and discovering novel ferromagnetic half-metals for spintronics.", "motivation": "Magnetic materials have complex energy landscapes with multiple local minima, making global minimum identification challenging with heuristic methods that aren't guaranteed to succeed.", "method": "Applied automated workflow to systematically explore energy landscapes, enabling effective control and sampling of orbital occupation matrices for rapid identification of local minima. Used Hubbard-corrected energy functionals with first-principles computed U parameters from linear-response theory.", "result": "Found diverse collinear metastable states: 109 ferromagnetic, 83 antiferromagnetic, and 2 altermagnetic monolayers. Discovered 12 novel ferromagnetic half-metals with spintronics potential.", "conclusion": "The automated workflow successfully identified ground-state magnetic orders and revealed promising spintronic candidates, demonstrating the effectiveness of systematic exploration over heuristic methods."}}
{"id": "2509.09410", "pdf": "https://arxiv.org/pdf/2509.09410", "abs": "https://arxiv.org/abs/2509.09410", "authors": ["Weisheng Niu", "Yao Xu", "Jinping Zhuge"], "title": "Optimal convergence rates in multiscale elliptic homogenization", "categories": ["math.AP", "35B27"], "comment": "71 pages", "summary": "This paper is devoted to the quantitative homogenization of multiscale\nelliptic operator $-\\nabla\\cdot A_\\varepsilon \\nabla$, where $A_\\varepsilon(x)\n= A(x/\\varepsilon_1, x/\\varepsilon_2,\\cdots, x/\\varepsilon_n)$, $\\varepsilon =\n(\\varepsilon_1, \\varepsilon_2,\\cdots, \\varepsilon_n) \\in (0,1]^n$ and\n$\\varepsilon_i > \\varepsilon_{i+1}$. We assume that $A(y_1,y_2,\\cdots, y_n)$ is\n1-periodic in each $y_i \\in \\mathbb{R}^d$ and real analytic. Classically, the\nmethod of reiterated homogenization has been applied to study this multiscale\nelliptic operator, which leads to a convergence rate limited by the ratios\n$\\max \\{ \\varepsilon_{i+1}/\\varepsilon_i: 1\\le i\\le n-1\\}$. In the present\npaper, under the assumption of real analytic coefficients, we introduce the\nso-called multiscale correctors and more accurate effective operators, and\nimprove the ratio part of the convergence rate to $\\max \\{\ne^{-c\\varepsilon_{i}/\\varepsilon_{i+1}}: 1\\le i\\le n-1 \\}$. This convergence\nrate is optimal in the sense that $c>0$ cannot be replaced by a larger\nconstant. As a byproduct, the uniform Lipschitz estimate is established under a\nmild double-log scale-separation condition.", "AI": {"tldr": "Improved homogenization convergence rates for multiscale elliptic operators with analytic coefficients using multiscale correctors and effective operators", "motivation": "Classical reiterated homogenization methods for multiscale elliptic operators have convergence rates limited by the ratios between successive scales, which can be suboptimal", "method": "Introduced multiscale correctors and more accurate effective operators under the assumption of real analytic coefficients to improve convergence rates", "result": "Achieved optimal convergence rate improvement from max{\u03b5_{i+1}/\u03b5_i} to max{e^{-c\u03b5_i/\u03b5_{i+1}}} and established uniform Lipschitz estimates under mild scale-separation conditions", "conclusion": "The proposed method provides optimal convergence rates for multiscale homogenization problems with analytic coefficients, significantly improving upon classical approaches"}}
{"id": "2509.09032", "pdf": "https://arxiv.org/pdf/2509.09032", "abs": "https://arxiv.org/abs/2509.09032", "authors": ["Guy Tsafack", "Antoine Tambue"], "title": "Strong convergence of a semi tamed scheme for stochastic differential algebraic equation under non-global Lipschitz coefficients", "categories": ["math.NA", "cs.NA"], "comment": null, "summary": "We are investigating the first strong convergence analysis of a numerical\nmethod for stochastic differential algebraic equations (SDAEs) under a\nnon-global Lipschitz setting. It is well known that the explicit Euler scheme\nfails to converge strongly to the exact solution of a stochastic differential\nequation (SDEs) when at least one of the coefficients grows superlinearly. The\nproblem becomes more challenging in the case of stochastic\ndifferential-algebraic equations (SDAEs) due to the singularity of the matrix.\nTo address this, we build a new scheme called the semi-implicit tamed method\nfor SDAEs and provide its strong convergence result under non-global Lipschitz\nsetting. In other words, the linear component of the drift term is approximated\nimplicitly, whereas its nonlinear component is tamed and approximated\nexplicitly. We show that this method strongly converges with order\n$\\frac{1}{2}$ to the exact solution. To prove this strong convergence result,\nwe first derive an equivalent scheme, that we call the dual tamed scheme, which\nis more suitable for mathematical analysis and is associated with the inherent\nstochastic differential equation obtained by eliminating the constraints from\nthe original SDAEs. To demonstrate the effectiveness of the proposed scheme,\nnumerical simulations are performed, confirming that the theoretical findings\nare consistent with the numerical results.", "AI": {"tldr": "First strong convergence analysis of a semi-implicit tamed method for stochastic differential algebraic equations (SDAEs) under non-global Lipschitz conditions, achieving order 1/2 convergence.", "motivation": "Explicit Euler scheme fails for SDEs with superlinear growth coefficients, and the problem is more challenging for SDAEs due to matrix singularity. Need for a method that handles non-global Lipschitz settings.", "method": "Developed a semi-implicit tamed method where linear drift components are approximated implicitly and nonlinear components are tamed and approximated explicitly. Also created an equivalent dual tamed scheme for mathematical analysis.", "result": "The proposed method strongly converges with order 1/2 to the exact solution. Numerical simulations confirm theoretical findings.", "conclusion": "The semi-implicit tamed method effectively addresses the convergence challenges of SDAEs under non-global Lipschitz conditions, providing a reliable numerical solution approach."}}
{"id": "2509.09591", "pdf": "https://arxiv.org/pdf/2509.09591", "abs": "https://arxiv.org/abs/2509.09591", "authors": ["Zhen Liu", "David Soper", "Hassan Hemida", "Boyang Chen"], "title": "Numerical modelling of a partially loaded intermodal container freight train passing through a tunnel", "categories": ["physics.flu-dyn", "physics.comp-ph"], "comment": null, "summary": "The bluff nature of a freight train locomotive, coupled with large gaps\ncreated between different wagon formations and loaded goods, influence the\noverall pressure wave pattern generated as the train passes through a tunnel.\nTypically, 1D models are used to predict the patterns and properties of tunnel\npressure wave formations. However, accurate modelling of regions of separation\nat the head of the blunted containers and at unloaded gap sections is essential\nfor precise predictions of pressure magnitudes. This has traditionally been\ndifficult to capture with 1D models. Furthermore, achieving this accuracy\nthrough 3D computational methods demands exceptional mesh quality, significant\ncomputational resources, and the careful selection of numerical models. This\npaper evaluates various numerical models to capture these complexities within\nregions of flow separation. Findings have supported the development of a new 1D\nprogramme to calculate the pressure wave generated by a freight locomotive\nentering a tunnel, and is here further extended to consider the discontinuities\nof the train body created by intermodal container loading patterns, by\nimplementing new mesh system and boundary conditions into the 1D programme. A\nparameterisation study for different loading configurations is also presented\nto improve the overall programme adaptability, and the relationship between\npredetermined parameters and gap length is investigated. We validate the\neffectiveness of the improved 1D model through comprehensive Large Eddy\nSimulation (LES) results and conduct an extensive parameterisation study to\nenhance its applicability across various loading configurations. Consequently,\nthis research bridges the gap in freight train tunnel aerodynamics, offering a\nversatile 1D numerical tool for accurate pressure wave prediction.", "AI": {"tldr": "Developed improved 1D model for freight train tunnel pressure waves that captures flow separation effects at blunted containers and gaps, validated with LES simulations and parameterization studies.", "motivation": "Traditional 1D models struggle to capture flow separation at blunted containers and gaps in freight trains, while 3D methods require excessive computational resources and mesh quality.", "method": "Developed new 1D program with improved mesh system and boundary conditions to handle discontinuities from container loading patterns, validated with Large Eddy Simulation (LES) results and parameterization studies.", "result": "Created a versatile 1D numerical tool that accurately predicts pressure wave patterns for various freight train loading configurations, bridging the gap in freight train tunnel aerodynamics.", "conclusion": "The research successfully developed an efficient 1D model that provides accurate pressure wave predictions for freight trains in tunnels, overcoming limitations of traditional 1D models while avoiding computational costs of 3D methods."}}
{"id": "2509.09518", "pdf": "https://arxiv.org/pdf/2509.09518", "abs": "https://arxiv.org/abs/2509.09518", "authors": ["Andrew Hassell", "Qiuye Jia", "Ethan Sussman", "Andras Vasy"], "title": "Microlocal analysis of the non-relativistic limit of the Klein--Gordon equation: Estimates", "categories": ["math.AP", "math-ph", "math.MP", "Primary 35L05, 35L15. Secondary 35B25, 35Q40, 58J47, 58J50"], "comment": "99 pages, 14 figures", "summary": "This is the more technical half of a two-part work in which we introduce a\nrobust microlocal framework for analyzing the non-relativistic limit of\nrelativistic wave equations with time-dependent coefficients, focusing on the\nKlein--Gordon equation. Two asymptotic regimes in phase space are relevant to\nthe non-relativistic limit: one corresponding to what physicists call\n``natural'' units, in which the PDE is approximable by the free Klein--Gordon\nequation, and a low-frequency regime in which the equation is approximable by\nthe usual Schrodinger equation. Combining the analyses in the two regimes gives\nglobal estimates which are uniform as the speed of light goes to infinity. The\ncompanion paper gives applications. Our main technical tools are three new\npseudodifferential calculi, $\\Psi_{\\natural}$ (a variant of the semiclassical\nscattering calculus), $\\Psi_{\\natural\\mathrm{res}}$, and\n$\\Psi_{\\natural2\\mathrm{res}}$, the latter two of which are created by ``second\nmicrolocalizing'' the first at certain locations. This paper and the companion\npaper can be read in either order, since the latter treats the former as a\nblack box.", "AI": {"tldr": "Robust microlocal framework for analyzing non-relativistic limit of relativistic wave equations with time-dependent coefficients, focusing on Klein-Gordon equation.", "motivation": "To develop a rigorous mathematical framework for studying the non-relativistic limit of relativistic wave equations as the speed of light goes to infinity, addressing two distinct asymptotic regimes in phase space.", "method": "Uses three new pseudodifferential calculi: \u03a8\u2099 (semiclassical scattering variant), \u03a8\u2099\u1d63\u2091\u209b, and \u03a8\u2099\u2082\u1d63\u2091\u209b, with the latter two created by second microlocalizing the first at specific locations. Combines analyses from two asymptotic regimes.", "result": "Develops global estimates that are uniform as the speed of light approaches infinity, providing a robust technical foundation for analyzing non-relativistic limits.", "conclusion": "Establishes a comprehensive microlocal framework for studying non-relativistic limits, with applications detailed in a companion paper. The technical tools enable rigorous analysis of wave equation behavior across different asymptotic regimes."}}
{"id": "2509.09132", "pdf": "https://arxiv.org/pdf/2509.09132", "abs": "https://arxiv.org/abs/2509.09132", "authors": ["Jingyu Yang", "Shingyu Leung", "Jianliang Qian", "Hao Liu"], "title": "Fast Operator-Splitting Methods for Nonlinear Elliptic Equations", "categories": ["math.NA", "cs.NA", "65N30, 65M60"], "comment": null, "summary": "Nonlinear elliptic problems arise in many fields, including plasma physics,\nastrophysics, and optimal transport. In this article, we propose a novel\noperator-splitting/finite element method for solving such problems. We begin by\nintroducing an auxiliary function in a new way for a semilinear elliptic\npartial differential equation, leading to the development of a convergent\noperator-splitting/finite element scheme for this equation. The algorithm is\nthen extended to fully nonlinear elliptic equations of the Monge-Amp\\`ere type,\nincluding the Dirichlet Monge-Amp\\`ere equation and Pucci's equation. This is\nachieved by reformulating the fully nonlinear equations into forms analogous to\nthe semilinear case, enabling the application of the proposed splitting\nalgorithm. In our implementation, a mixed finite element method is used to\napproximate both the solution and its Hessian matrix. Numerical experiments\nshow that the proposed method outperforms existing approaches in efficiency and\naccuracy, and can be readily applied to problems defined on domains with curved\nboundaries.", "AI": {"tldr": "Novel operator-splitting/finite element method for nonlinear elliptic problems, extended from semilinear to fully nonlinear equations including Monge-Amp\u00e8re and Pucci's equations.", "motivation": "Nonlinear elliptic problems are important in plasma physics, astrophysics, and optimal transport, requiring efficient and accurate numerical methods.", "method": "Introduce auxiliary function for semilinear elliptic PDEs, develop convergent operator-splitting/finite element scheme, extend to fully nonlinear equations via reformulation, use mixed finite element method for solution and Hessian approximation.", "result": "Numerical experiments show the method outperforms existing approaches in efficiency and accuracy, and works well on domains with curved boundaries.", "conclusion": "The proposed method provides an effective approach for solving various nonlinear elliptic problems with improved performance over existing methods."}}
{"id": "2509.09601", "pdf": "https://arxiv.org/pdf/2509.09601", "abs": "https://arxiv.org/abs/2509.09601", "authors": ["St\u00e9phane Delorme", "Leon Mach", "Hubert Paszkiewicz", "Richard Ruiz"], "title": "Are arXiv submissions on Wednesday better cited? Introducing Big Data methods in undergraduate courses on scientific computing", "categories": ["physics.ed-ph", "hep-ex", "physics.comp-ph"], "comment": "10 pages, 8 figures, 2 tables, 1 listing, project available at\n  https://gitlab.cern.ch/riruiz/public-projects/-/tree/master/BibAPI/", "summary": "Extracting information from big data sets, both real and simulated, is a\nmodern hallmark of the physical sciences. In practice, students face barriers\nto learning ``Big Data'' methods in undergraduate physics and astronomy\ncurricula. As an attempt to alleviate some of these challenges, we present a\nsimple, farm-to-table data analysis pipeline that can collect, process, and\nplot data from the 800k entries common to the arXiv preprint repository and the\nbibliographical database inSpireHEP. The pipeline employs contemporary research\npractices and can be implemented using open-sourced Python libraries common to\nundergraduate courses on Scientific Computing. To support the use such\npipelines in classroom contexts, we make public an example implementation,\nauthored by two undergraduate physics students, that runs on off-the-shelf\nlaptops. For advanced students, we discuss applications of the pipeline,\nincluding for online DAQ monitoring and commercialization.", "AI": {"tldr": "A simple data analysis pipeline for teaching big data methods using arXiv and inSpireHEP data, implemented with open-source Python libraries for undergraduate physics education.", "motivation": "Address barriers to learning big data methods in undergraduate physics curricula by providing accessible, practical tools that use real scientific data sources.", "method": "Develop a farm-to-table pipeline that collects, processes, and plots data from 800k entries common to arXiv preprint repository and inSpireHEP bibliographical database using open-source Python libraries.", "result": "Created a publicly available implementation that runs on standard laptops, authored by undergraduate students, demonstrating practical application of contemporary research practices.", "conclusion": "The pipeline successfully provides an accessible entry point for teaching big data analysis methods and has potential applications for advanced uses like online DAQ monitoring and commercialization."}}
{"id": "2509.09565", "pdf": "https://arxiv.org/pdf/2509.09565", "abs": "https://arxiv.org/abs/2509.09565", "authors": ["Yangkendi Deng", "Yunfeng Zhang", "Zehua Zhao"], "title": "Sharp bilinear eigenfunction estimate, $L^\\infty_{x_2}L^p_{t,x_1}$-type Strichartz estimate, and energy-critical NLS", "categories": ["math.AP"], "comment": "31 pages. Comments are welcome!", "summary": "We establish sharp bilinear and multilinear eigenfunction estimates for the\nLaplace-Beltrami operator on the standard three-sphere $\\mathbb{S}^3$,\neliminating the logarithmic loss that has persisted in the literature since the\npioneering work of Burq, G\\'erard, and Tzvetkov over twenty years ago. This\ncompletes the theory of multilinear eigenfunction estimates on the standard\nspheres. Our approach relies on viewing $\\mathbb{S}^3$ as the compact Lie group\n$\\mathrm{SU}(2)$ and exploiting its representation theory, especially the\nproperties of Clebsch-Gordan coefficients. Motivated by application to the\nenergy-critical nonlinear Schr\\\"odinger equation (NLS) on $\\mathbb{R} \\times\n\\mathbb{S}^3$, we also prove a refined Strichartz estimate of mixed-norm type\n$L^\\infty_{x_2}L^4_{t,x_1}$ on the cylindrical space $\\mathbb{R}_{x_1} \\times\n\\mathbb{T}_{x_2}$, adapted to certain spectrally localized functions. Combining\nthese two ingredients, we derive a refined bilinear Strichartz estimate on\n$\\mathbb{R} \\times \\mathbb{S}^3$, which in turn yields small data global\nwell-posedness for the above mentioned NLS in the energy space.", "AI": {"tldr": "Sharp bilinear and multilinear eigenfunction estimates on S\u00b3 without logarithmic loss, completing the theory on standard spheres. Application to energy-critical NLS yields small data global well-posedness.", "motivation": "Eliminate persistent logarithmic loss in eigenfunction estimates that has existed for 20+ years since Burq, G\u00e9rard, and Tzvetkov's work, and apply to energy-critical nonlinear Schr\u00f6dinger equation on S\u00b3.", "method": "View S\u00b3 as compact Lie group SU(2) and exploit representation theory, especially Clebsch-Gordan coefficients. Prove refined Strichartz estimates on cylindrical space and combine with eigenfunction estimates.", "result": "Sharp bilinear/multilinear eigenfunction estimates without logarithmic loss, refined bilinear Strichartz estimate on R\u00d7S\u00b3, and small data global well-posedness for energy-critical NLS in energy space.", "conclusion": "Completes theory of multilinear eigenfunction estimates on standard spheres and provides important tools for studying nonlinear PDEs on curved geometries like S\u00b3."}}
{"id": "2509.09139", "pdf": "https://arxiv.org/pdf/2509.09139", "abs": "https://arxiv.org/abs/2509.09139", "authors": ["Zijian Zhang", "Rui Hong", "Xuesong Chen", "Shuting Cai"], "title": "Hybrid-Precision Block-Jacobi Preconditioned GMRES Solver for Linear System in Circuit Simulation", "categories": ["math.NA", "cs.NA"], "comment": "18 pages, 8 figures", "summary": "As integrated circuits become increasingly complex, the demand for efficient\nand accurate simulation solvers continues to rise. Traditional solvers often\nstruggle with large-scale sparse systems, leading to prolonged simulation times\nand reduced accuracy. In this paper, a hybrid-precision block-Jacobi\npreconditioned GMRES solver is proposed to solve the large sparse system in\ncircuit simulation. The proposed method capitalizes on the structural sparsity\nand block properties of circuit matrices, employing a novel hybrid-precision\nstrategy that applies single-precision arithmetic for computationally intensive\ntasks and double-precision arithmetic for critical accuracy-sensitive\ncomputations. Additionally, we use the graph partitioning tools to assist in\ngenerating preconditioners, ensuring an optimized preconditioning process. For\nlarge-scale problems, we adopt the restart strategy to increase the\ncomputational efficiency. Through rigorous mathematical reasoning, the\nconvergence and error analysis of the proposed method are carried out.\nNumerical experiments on various benchmark matrices demonstrate that our\napproach significantly outperforms existing solvers, including SuperLU, KLU,\nand SFLU, in terms of both preconditioning and GMRES runtime. The proposed\nhybrid-precision preconditioner effectively improves spectral clustering,\nleading to faster solutions.", "AI": {"tldr": "Hybrid-precision block-Jacobi preconditioned GMRES solver for circuit simulation that uses single-precision for intensive tasks and double-precision for accuracy-critical computations, with graph partitioning for preconditioner optimization.", "motivation": "Traditional solvers struggle with large-scale sparse systems in circuit simulation, leading to prolonged simulation times and reduced accuracy as integrated circuits become increasingly complex.", "method": "Proposes a hybrid-precision block-Jacobi preconditioned GMRES solver that leverages structural sparsity and block properties of circuit matrices. Uses single-precision for computationally intensive tasks and double-precision for accuracy-sensitive computations. Employs graph partitioning tools for preconditioner generation and restart strategy for large-scale problems.", "result": "Numerical experiments show the approach significantly outperforms existing solvers (SuperLU, KLU, SFLU) in both preconditioning and GMRES runtime. The hybrid-precision preconditioner effectively improves spectral clustering for faster solutions.", "conclusion": "The proposed method provides an efficient and accurate solution for large sparse systems in circuit simulation, with rigorous mathematical convergence and error analysis supporting its effectiveness."}}
{"id": "2509.09648", "pdf": "https://arxiv.org/pdf/2509.09648", "abs": "https://arxiv.org/abs/2509.09648", "authors": ["Francesca De Marchis", "Lisa Mazzuoli", "Filomena Pacella"], "title": "Stability and asymptotic behaviour of one-dimensional solutions in cylinders", "categories": ["math.AP"], "comment": null, "summary": "We consider positive one-dimensional solutions of a Lane-Emden relative\nDirichlet problem in a cylinder and study their stability/instability\nproperties as the energy varies with respect to domain perturbations. This\ndepends on the exponent $p >1$ of the nonlinearity and we obtain results for\n$p$ close to 1 and for $p$ large. This is achieved by a careful asymptotic\nanalysis of the one-dimensional solution as $p \\to 1$ or $p \\to \\infty$, which\nis of independent interest. It allows to detect the limit profile and other\nqualitative properties of these solutions.", "AI": {"tldr": "Analysis of stability/instability properties of positive one-dimensional solutions to Lane-Emden Dirichlet problems in cylinders as energy varies with domain perturbations, focusing on asymptotic behavior near p=1 and large p.", "motivation": "To understand how the stability properties of one-dimensional Lane-Emden solutions change with respect to domain perturbations as the nonlinearity exponent p varies, particularly for extreme values close to 1 and approaching infinity.", "method": "Careful asymptotic analysis of one-dimensional solutions as p approaches 1 and infinity, examining limit profiles and qualitative properties through mathematical analysis of the Lane-Emden relative Dirichlet problem in cylindrical domains.", "result": "The study reveals how stability/instability properties depend on the exponent p, with specific results obtained for p close to 1 and for large p values through asymptotic analysis of solution behavior.", "conclusion": "The asymptotic analysis provides insights into the limit profiles and qualitative properties of one-dimensional Lane-Emden solutions, enabling detection of stability transitions as the nonlinearity exponent p varies between extreme values."}}
{"id": "2509.09236", "pdf": "https://arxiv.org/pdf/2509.09236", "abs": "https://arxiv.org/abs/2509.09236", "authors": ["Guilherme Henrique Teixeira", "Nepomuk Krenn", "Peter Gangl", "Benjamin Marussig"], "title": "Isogeometric Topology Optimization Based on Topological Derivatives", "categories": ["math.NA", "cs.CE", "cs.NA", "math.OC"], "comment": "19 pages, 11 figures, pre-print,", "summary": "Topology optimization is a valuable tool in engineering, facilitating the\ndesign of optimized structures. However, topological changes often require a\nremeshing step, which can become challenging. In this work, we propose an\nisogeometric approach to topology optimization driven by topological\nderivatives. The combination of a level-set method together with an immersed\nisogeometric framework allows seamless geometry updates without the necessity\nof remeshing. At the same time, topological derivatives provide topological\nmodifications without the need to define initial holes [7]. We investigate the\ninfluence of higher-degree basis functions in both the level-set representation\nand the approximation of the solution. Two numerical examples demonstrate the\nproposed approach, showing that employing higher-degree basis functions for\napproximating the solution improves accuracy, while linear basis functions\nremain sufficient for the level-set function representation.", "AI": {"tldr": "Isogeometric topology optimization using topological derivatives and level-set method that eliminates remeshing requirements, with investigation of higher-degree basis functions' impact.", "motivation": "Topology optimization requires remeshing for topological changes, which can be challenging. The paper aims to develop an approach that enables seamless geometry updates without remeshing.", "method": "Combines level-set method with immersed isogeometric framework driven by topological derivatives. Investigates higher-degree basis functions for both level-set representation and solution approximation.", "result": "Two numerical examples show that higher-degree basis functions improve solution accuracy, while linear basis functions remain sufficient for level-set representation.", "conclusion": "The proposed isogeometric approach successfully eliminates remeshing requirements and demonstrates that higher-degree basis functions benefit solution accuracy but linear functions are adequate for level-set representation."}}
{"id": "2509.09274", "pdf": "https://arxiv.org/pdf/2509.09274", "abs": "https://arxiv.org/abs/2509.09274", "authors": ["Taiyuan Liu", "Yaozhong Hu", "Siqing Gan"], "title": "Long time strong convergence analysis of one-step methods for McKean-Vlasov SDEs with superlinear growth coefficients", "categories": ["math.NA", "cs.NA"], "comment": null, "summary": "This paper presents a strong convergence rate analysis of general\ndiscretization approximations for McKean-Vlasov SDEs with super-linear growth\ncoefficients over infinite time horizon. Under some specified non-globally\nLipschitz conditions, we derive the propagation of chaos, and the mean-square\nconvergence rate over infinite time horizon for general one-step time\ndiscretization schemes for the underlying Mckean-Vlasov SDEs. As an application\nof the general result it is obtained the mean-square convergence rate over\ninfinite time horizon for two numerical schemes: the projected Euler scheme and\nthe backward Euler scheme for Mckean-Vlasov SDEs in non-globally Lipschitz\nsettings. Numerical experiments are provided to validate the theoretical\nfindings.", "AI": {"tldr": "Strong convergence rate analysis for discretization approximations of McKean-Vlasov SDEs with super-linear growth coefficients over infinite time horizon under non-globally Lipschitz conditions.", "motivation": "To analyze the convergence properties of numerical schemes for McKean-Vlasov SDEs with super-linear growth coefficients, which are challenging due to non-globally Lipschitz conditions and infinite time horizon considerations.", "method": "Derived propagation of chaos and mean-square convergence rate for general one-step time discretization schemes. Applied the general framework to analyze two specific schemes: projected Euler scheme and backward Euler scheme.", "result": "Obtained mean-square convergence rate over infinite time horizon for the numerical schemes. Conducted numerical experiments that validate the theoretical findings.", "conclusion": "The paper provides rigorous convergence analysis for discretization methods of McKean-Vlasov SDEs with super-linear growth, demonstrating effectiveness of projected Euler and backward Euler schemes in non-globally Lipschitz settings."}}
{"id": "2509.09000", "pdf": "https://arxiv.org/pdf/2509.09000", "abs": "https://arxiv.org/abs/2509.09000", "authors": ["Wael El Khateeb", "Chanaka Kottegoda", "Chunhua Shan"], "title": "Complex dynamics and pattern formation in a diffusive epidemic model with an infection-dependent recovery rate", "categories": ["math.DS", "math.AP", "35K57, 92D30, 35B32, 35B36, 92C60"], "comment": null, "summary": "A diffusive epidemic model with an infection-dependent recovery rate is\nformulated in this paper. Multiple constant steady states and spatially\nhomogeneous periodic solutions are first proven by bifurcation analysis of the\nreaction kinetics. It is shown that the model exhibits diffusion-driven\ninstability, where the infected population acts as an activator and the\nsusceptible population functions as an in hibitor. The faster movement of the\nsusceptible class will induce the spatial and spatiotemporal patterns, which\nare characterized by k-mode Turing instability and (k1,k2)-mode Turing-Hopf\nbifurcation. The transient dynamics from a purely temporal oscillatory regime\nto a spatial periodic pattern are discovered. The model reveals key\ntransmission dynamics, including asynchronous disease recurrence, spatially\npatterned waves, and the formation of localized hotspots. The study suggests\nthat spatially targeted strategies are necessary to contain disease waves that\nvary regionally and cyclically.", "AI": {"tldr": "A diffusive epidemic model with infection-dependent recovery rate shows diffusion-driven instability, Turing patterns, and spatiotemporal dynamics including disease recurrence and localized hotspots.", "motivation": "To understand how infection-dependent recovery rates and population movement affect disease transmission dynamics and pattern formation in epidemics.", "method": "Bifurcation analysis of reaction kinetics to identify steady states and periodic solutions, then analysis of diffusion-driven instability through Turing patterns and Turing-Hopf bifurcation.", "result": "Model exhibits multiple constant steady states, homogeneous periodic solutions, spatial patterns from faster susceptible movement, and transient dynamics from temporal oscillations to spatial patterns.", "conclusion": "Spatially targeted strategies are necessary to contain regionally varying and cyclical disease waves, as revealed by the model's transmission dynamics including asynchronous recurrence and localized hotspots."}}
{"id": "2509.09287", "pdf": "https://arxiv.org/pdf/2509.09287", "abs": "https://arxiv.org/abs/2509.09287", "authors": ["Wasim Akram", "Manil T. Mohan"], "title": "Optimal Control of a Hemivariational Inequality of Stationary Convective Brinkman-Forchheimer Extended Darcy equations with Numerical Approximation", "categories": ["math.NA", "cs.NA"], "comment": null, "summary": "This paper studies an optimal control problem for a stationary convective\nBrinkman-Forchheimer extended Darcy (CBFeD) hemivariational inequality in two\nand three dimensions, subject to control constraints, and develops its\nnumerical approximation. The hemivariational inequality provides the weak\nformulation of a stationary incompressible fluid flow through a porous medium,\ngoverned by the CBFeD equations, which account for convection, damping, and\nnonlinear resistance effects. The problem incorporates a non-leak boundary\ncondition and a subdifferential friction-type condition. We first analyze the\nstability of solutions with respect to perturbations in the external force\ndensity and the superpotential. Next, we prove the existence of a solution to\nthe optimal control problem, where the external force density acts as the\ncontrol variable. We then propose a numerical scheme for solving the optimal\ncontrol problem and establish its convergence. For concreteness, the numerical\nmethod is implemented using finite element discretization. Finally, we provide\nsome numerical examples to validate the theory developed.", "AI": {"tldr": "Analysis of optimal control for convective Brinkman-Forchheimer extended Darcy hemivariational inequality with control constraints, including numerical approximation and convergence proofs.", "motivation": "To develop optimal control framework for incompressible fluid flow through porous media governed by CBFeD equations, accounting for convection, damping, and nonlinear resistance effects with non-leak boundary conditions.", "method": "Analyze solution stability under perturbations, prove existence of optimal control solutions, develop numerical scheme with finite element discretization, and establish convergence properties.", "result": "Successfully developed theoretical framework for optimal control problem, proved solution existence, implemented numerical method with finite elements, and validated through numerical examples.", "conclusion": "The paper provides a comprehensive analysis and numerical solution approach for optimal control of CBFeD hemivariational inequalities, with proven convergence and practical validation."}}
{"id": "2509.09269", "pdf": "https://arxiv.org/pdf/2509.09269", "abs": "https://arxiv.org/abs/2509.09269", "authors": ["Luca Ballotta", "Juncal Arbelaiz", "Vijay Gupta", "Luca Schenato", "Mihailo R. Jovanovi\u0107"], "title": "The role of communication delays in the optimal control of spatially invariant systems", "categories": ["math.OC", "cs.SY", "eess.SY", "math.AP", "93C43 (Primary) 49N10 (Secondary)"], "comment": "{\\copyright} 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "We study optimal proportional feedback controllers for spatially invariant\nsystems when the controller has access to delayed state measurements received\nfrom different spatial locations. We analyze how delays affect the spatial\nlocality of the optimal feedback gain leveraging the problem decoupling in the\nspatial frequency domain. For the cases of expensive control and small delay,\nwe provide exact expressions of the optimal controllers in the limit for\ninfinite control weight and vanishing delay, respectively. In the expensive\ncontrol regime, the optimal feedback control law decomposes into a delay-aware\nfiltering of the delayed state and the optimal controller in the delay-free\nsetting. Under small delays, the optimal controller is a perturbation of the\ndelay-free one which depends linearly on the delay. We illustrate our\nanalytical findings with a reaction-diffusion process over the real line and a\nmulti-agent system coupled through circulant matrices, showing that delays\nreduce the effectiveness of optimal feedback control and may require each\nsubsystem within a distributed implementation to communicate with farther-away\nlocations.", "AI": {"tldr": "Analysis of optimal proportional feedback controllers for spatially invariant systems with delayed state measurements, showing how delays impact spatial locality and control effectiveness.", "motivation": "To understand how communication delays affect the performance and spatial locality of optimal feedback control in distributed systems, particularly in spatially invariant systems where delays can degrade control effectiveness.", "method": "Leveraging problem decoupling in the spatial frequency domain, analyzing optimal controllers for expensive control and small delay regimes, providing exact expressions for infinite control weight and vanishing delay limits.", "result": "Delays reduce control effectiveness and may require subsystems to communicate with farther locations. Optimal controller decomposes into delay-aware filtering plus delay-free controller in expensive control regime, and is a linear perturbation of delay-free controller under small delays.", "conclusion": "Communication delays significantly impact the spatial locality and performance of optimal feedback control in distributed systems, necessitating adjustments in controller design and potentially increased communication range between subsystems."}}
{"id": "2509.09302", "pdf": "https://arxiv.org/pdf/2509.09302", "abs": "https://arxiv.org/abs/2509.09302", "authors": ["Jingtao Zhu", "Yuying Zhao", "Siqing Gan"], "title": "Euler-type methods for Levy-driven McKean-Vlasov SDEs with super-linear coefficients: mean-square error analysis", "categories": ["math.NA", "cs.NA"], "comment": null, "summary": "We develop and analyze a general class of Euler-type numerical schemes for\nLevy-driven McKean-Vlasov stochastic differential equations (SDEs), where the\ndrift, diffusion and jump coefficients grow super-linearly in the state\nvariable. These numerical schemes are derived by incorporating projections or\nnonlinear transformations into the classical Euler method, with the primary\nobjective of establishing moment bounds for the numerical solutions. This class\nof schemes includes the tanh-Euler, tamed-Euler and sine-Euler schemes as\nspecial cases. In contrast to existing approaches that rely on a coercivity\ncondition (e.g., Assumption B-1 in Kumar et al., arXiv:2010.08585), the\nproposed schemes remove such a restrictive assumption. We provide a rigorous\nmean-square convergence analysis and establish that the proposed schemes\nachieve convergence rates arbitrarily close to 1/2 for the interacting particle\nsystems associated with Levy-driven McKean-Vlasov SDEs. Several numerical\nexamples are presented to illustrate the convergence behavior and validate the\ntheoretical results.", "AI": {"tldr": "Developed Euler-type numerical schemes for Levy-driven McKean-Vlasov SDEs with super-linear growth coefficients, removing restrictive coercivity assumptions and achieving near 1/2 convergence rates.", "motivation": "To address Levy-driven McKean-Vlasov SDEs with super-linear growth coefficients where existing methods rely on restrictive coercivity conditions that limit applicability.", "method": "Developed a general class of Euler-type schemes incorporating projections or nonlinear transformations (including tanh-Euler, tamed-Euler, sine-Euler) to establish moment bounds for numerical solutions without coercivity assumptions.", "result": "The proposed schemes achieve mean-square convergence with rates arbitrarily close to 1/2 for interacting particle systems associated with Levy-driven McKean-Vlasov SDEs, validated through numerical examples.", "conclusion": "The new class of Euler-type schemes successfully removes the restrictive coercivity condition requirement while maintaining strong convergence properties for Levy-driven McKean-Vlasov SDEs with super-linear growth."}}
{"id": "2509.09362", "pdf": "https://arxiv.org/pdf/2509.09362", "abs": "https://arxiv.org/abs/2509.09362", "authors": ["Hanfei Zhou", "Lei Shi"], "title": "Expressive Power of Deep Networks on Manifolds: Simultaneous Approximation", "categories": ["math.NA", "cs.LG", "cs.NA", "stat.ML"], "comment": null, "summary": "A key challenge in scientific machine learning is solving partial\ndifferential equations (PDEs) on complex domains, where the curved geometry\ncomplicates the approximation of functions and their derivatives required by\ndifferential operators. This paper establishes the first simultaneous\napproximation theory for deep neural networks on manifolds. We prove that a\nconstant-depth $\\mathrm{ReLU}^{k-1}$ network with bounded weights--a property\nthat plays a crucial role in controlling generalization error--can approximate\nany function in the Sobolev space $\\mathcal{W}_p^{k}(\\mathcal{M}^d)$ to an\nerror of $\\varepsilon$ in the $\\mathcal{W}_p^{s}(\\mathcal{M}^d)$ norm, for\n$k\\geq 3$ and $s<k$, using $\\mathcal{O}(\\varepsilon^{-d/(k-s)})$ nonzero\nparameters, a rate that overcomes the curse of dimensionality by depending only\non the intrinsic dimension $d$. These results readily extend to functions in\nH\\\"older-Zygmund spaces. We complement this result with a matching lower bound,\nproving our construction is nearly optimal by showing the required number of\nparameters matches up to a logarithmic factor. Our proof of the lower bound\nintroduces novel estimates for the Vapnik-Chervonenkis dimension and\npseudo-dimension of the network's high-order derivative classes. These\ncomplexity bounds provide a theoretical cornerstone for learning PDEs on\nmanifolds involving derivatives. Our analysis reveals that the network\narchitecture leverages a sparse structure to efficiently exploit the manifold's\nlow-dimensional geometry.", "AI": {"tldr": "First simultaneous approximation theory for deep neural networks on manifolds, showing ReLU networks can approximate Sobolev space functions with optimal parameter efficiency that overcomes curse of dimensionality.", "motivation": "Solving PDEs on complex domains with curved geometry is challenging for scientific machine learning, requiring efficient approximation of functions and derivatives on manifolds.", "method": "Prove that constant-depth ReLU networks with bounded weights can approximate Sobolev space functions with O(\u03b5^{-d/(k-s)}) parameters, and provide matching lower bound using novel VC dimension estimates.", "result": "Achieves approximation error \u03b5 in W_p^s norm using optimal number of parameters that depends only on intrinsic dimension d, overcoming curse of dimensionality. Construction is nearly optimal.", "conclusion": "Provides theoretical foundation for learning PDEs on manifolds, showing neural networks leverage sparse structure to exploit low-dimensional geometry efficiently."}}
{"id": "2509.09434", "pdf": "https://arxiv.org/pdf/2509.09434", "abs": "https://arxiv.org/abs/2509.09434", "authors": ["Tom-Christian Riemer", "Martin Stoll"], "title": "A Low-Rank tensor framework for THB-Splines", "categories": ["math.NA", "cs.NA"], "comment": null, "summary": "We introduce a low-rank framework for adaptive isogeometric analysis with\ntruncated hierarchical B-splines (THB-splines) that targets the main bottleneck\nof local refinement: memory- and time-intensive matrix assembly once the global\ntensor-product structure is lost. The method interpolates geometry-induced\nweight and source terms in separable spline spaces and computes their\ntensor-train (TT) representations via the alternating minimal energy (AMEn)\nsolver, enabling level-wise assembly of system operators using univariate\nquadrature. To recover separability in the adaptive setting, we reduce the\nactive basis to tensor-product domains and partition active/non-active cells\ninto a small number of Cartesian cuboids, so each contributes a Kronecker\nfactor that is accumulated and rounded in TT. We realize the two-scale relation\nwith truncation in low rank and assemble the global hierarchical operators in a\nblock TT format suitable for iterative solvers. A prototype MATLAB\nimplementation built on the GeoPDEs package and the TT-Toolbox demonstrates\nthat, for model problems with moderately complex refinement regions, the\napproach reduces memory footprint and assembly time while maintaining accuracy;\nwe also discuss limitations when ranks grow with geometric or refinement\ncomplexity. This framework advances scalable adaptive IgA with THB-splines,\nparticularly in three dimensions.", "AI": {"tldr": "Low-rank framework for adaptive isogeometric analysis using tensor-train representations to reduce memory and time costs of matrix assembly with hierarchical B-splines.", "motivation": "Address the memory- and time-intensive matrix assembly bottleneck in adaptive isogeometric analysis when global tensor-product structure is lost due to local refinement.", "method": "Interpolates geometry-induced terms in separable spline spaces, computes tensor-train representations via AMEn solver, partitions cells into Cartesian cuboids for Kronecker factor accumulation, and assembles hierarchical operators in block TT format.", "result": "Reduces memory footprint and assembly time while maintaining accuracy for model problems with moderately complex refinement regions, though limitations exist when ranks grow with geometric complexity.", "conclusion": "This framework advances scalable adaptive IgA with THB-splines, particularly beneficial for three-dimensional problems, by leveraging low-rank tensor representations to overcome assembly bottlenecks."}}
{"id": "2509.09460", "pdf": "https://arxiv.org/pdf/2509.09460", "abs": "https://arxiv.org/abs/2509.09460", "authors": ["Federico Gatti", "Giuseppe Orlando"], "title": "Second-order Optimally Stable IMEX (pseudo-)staggered Galerkin discretization: application to lava flow modeling", "categories": ["math.NA", "cs.NA"], "comment": null, "summary": "We present second-order optimally stable Implicit-Explicit (IMEX) Runge-Kutta\n(RK) schemes with application to a modified set of shallow water equations that\ncan be used to model the dynamics of lava flows. The schemes are optimally\nstable in the sense that they satisfy, at the space-time discretization level,\na condition analogous to the \\texttt{L}-stability of Runge-Kutta methods for\nordinary differential equations. A novel (pseudo-)staggered Galerkin scheme is\nintroduced, which can be interpreted as an extension of the classical two-step\nTaylor-Galerkin (TG2) scheme. The method is derived by combining a von Neumann\nstability analysis with a Lax-Wendroff procedure. For the discretization of the\nnon-conservative terms that characterize the lava flow model, we employ the\nPath-Conservative (PC) method. The proposed scheme is evaluated on a number of\nrelevant test cases, demonstrating accuracy, robustness, and well-balancing\nproperties for the lava flow model.", "AI": {"tldr": "Second-order optimally stable IMEX Runge-Kutta schemes for lava flow modeling using shallow water equations, featuring a novel pseudo-staggered Galerkin scheme and path-conservative method for non-conservative terms.", "motivation": "To develop stable and accurate numerical schemes for modeling lava flow dynamics using modified shallow water equations, addressing the need for robust discretization methods that handle non-conservative terms effectively.", "method": "Combines IMEX Runge-Kutta schemes with a novel pseudo-staggered Galerkin approach (extension of Taylor-Galerkin TG2), using von Neumann stability analysis and Lax-Wendroff procedure. Employs Path-Conservative method for non-conservative terms.", "result": "The proposed scheme demonstrates accuracy, robustness, and well-balancing properties across multiple relevant test cases for the lava flow model.", "conclusion": "The second-order optimally stable IMEX schemes with pseudo-staggered Galerkin discretization provide an effective framework for simulating lava flow dynamics with good stability and accuracy properties."}}
{"id": "2509.09463", "pdf": "https://arxiv.org/pdf/2509.09463", "abs": "https://arxiv.org/abs/2509.09463", "authors": ["Jana Jovcheva", "Tim Seynnaeve", "Nick Vannieuwenhoven"], "title": "Minimality of Tree Tensor Network Ranks", "categories": ["math.NA", "cs.NA", "15A69 (Primary) 65F99 (Secondary)"], "comment": "12 pages, 3 figures", "summary": "For a given tree tensor network $G$, we call a tuple of bond dimensions\nminimal if there exists a tensor $T$ that can be represented by this network\nbut not on the same tree topology with strictly smaller bond dimensions. We\nestablish necessary and sufficient conditions on the bond dimensions of a tree\ntensor network to be minimal, generalizing a characterization of Carlini and\nKleppe about existence of tensors with a given multilinear rank. We also show\nthat in a minimal tree tensor network, the non-minimal tensors form a Zariski\nclosed subset, so minimality is a generic property in this sense.", "AI": {"tldr": "Characterization of minimal bond dimensions in tree tensor networks, generalizing multilinear rank conditions and showing minimality is a generic property.", "motivation": "To establish necessary and sufficient conditions for bond dimensions in tree tensor networks to be minimal, extending previous work on multilinear ranks and understanding when tensors cannot be represented with smaller bond dimensions.", "method": "Mathematical analysis of tree tensor network structures, generalizing Carlini and Kleppe's characterization of multilinear ranks, and using Zariski topology to study generic properties.", "result": "Established complete characterization of minimal bond dimensions for tree tensor networks, showing that non-minimal tensors form a Zariski closed subset, making minimality a generic property.", "conclusion": "The paper provides a comprehensive theoretical framework for determining minimal bond dimensions in tree tensor networks, with implications for tensor decomposition theory and practical applications in numerical computations."}}
{"id": "2509.09533", "pdf": "https://arxiv.org/pdf/2509.09533", "abs": "https://arxiv.org/abs/2509.09533", "authors": ["Qianqian Wu", "Rongfang Gong", "Wei Gong", "Ziyi Zhang", "Shengfeng Zhu"], "title": "Bioluminescence tomography: A new regularized shape optimization method", "categories": ["math.NA", "cs.NA", "math.OC"], "comment": null, "summary": "In this paper, we investigate an inverse source problem arising in\nbioluminescence tomography (BLT), where the objective is to recover both the\nsupport and intensity of the light source from boundary measurements. A shape\noptimization framework is developed, in which the source strength and its\nsupport are decoupled through first-order optimality conditions. To enhance the\nstability of the reconstruction, we incorporate a parameter-dependent coupled\ncomplex boundary method(CCBM) scheme together with perimeter and volume\nregularizations. The level-set representation naturally accommodates\ntopological changes, enabling the reconstruction of multiple, closely located,\nor nested sources. Theoretical justifications are provided, and a series of\nnumerical experiments are conducted to validate the proposed method. The\nresults demonstrate the robustness, accuracy, and noise-resistance of the\nalgorithm, as well as its advantages over existing approaches.", "AI": {"tldr": "Shape optimization framework for bioluminescence tomography inverse source problem using level-set representation with perimeter/volume regularization and coupled complex boundary method for stable reconstruction of source support and intensity.", "motivation": "Solve the inverse source problem in bioluminescence tomography to accurately recover both the support and intensity of light sources from boundary measurements, which is challenging due to ill-posedness and potential complex source geometries.", "method": "Developed a shape optimization framework that decouples source strength and support through first-order optimality conditions, incorporates parameter-dependent coupled complex boundary method (CCBM) scheme with perimeter and volume regularizations, and uses level-set representation to handle topological changes.", "result": "Numerical experiments demonstrate the algorithm's robustness, accuracy, and noise-resistance, showing advantages over existing approaches in reconstructing multiple, closely located, or nested sources.", "conclusion": "The proposed method provides an effective and stable approach for bioluminescence tomography inverse source problems, with theoretical justification and practical validation through comprehensive numerical testing."}}
{"id": "2509.09600", "pdf": "https://arxiv.org/pdf/2509.09600", "abs": "https://arxiv.org/abs/2509.09600", "authors": ["Pascal Heid", "Thomas P. Wihler"], "title": "Iterative energy reduction Galerkin methods and variational adaptivity", "categories": ["math.NA", "cs.NA", "35A15, 35B38, 65J15, 65M50, 65N30"], "comment": null, "summary": "Critical points of energy functionals, which are of broad interest, for\ninstance, in physics and chemistry, in solid and quantum mechanics, in material\nscience, or in general diffusion-reaction models arise as solutions to the\nassociated Euler-Lagrange equations. While classical computational solution\nmethods for such models typically focus solely on the underlying partial\ndifferential equations, we propose an approach that also incorporates the\nenergy structure itself. Specifically, we examine (linearized) iterative\nGalerkin discretization schemes that ensure energy reduction at each step.\nAdditionally, we provide necessary conditions, which are applicable to a wide\nclass of problems, that guarantee convergence to critical points of the PDE.\nMoreover, in the specific context of finite element discretizations, we present\na very generally applicable adaptive mesh refinement strategy - the so-called\nvariational adaptivity approach - which, rather than using classical a\nposteriori estimates, is based on exploiting local energy reductions. The\ntheoretical results are validated for several computational experiments in the\ncontext of nonlinear diffusion-reaction models, thereby demonstrating the\neffectiveness of the proposed scheme.", "AI": {"tldr": "Novel computational approach that incorporates energy structure alongside PDEs for solving critical point problems, featuring energy-reducing iterative schemes and variational mesh adaptivity.", "motivation": "Classical methods focus only on PDEs, but energy functionals are fundamental in physics, chemistry, mechanics, and material science. The paper aims to leverage the energy structure itself for more effective computational solutions.", "method": "Proposes (linearized) iterative Galerkin discretization schemes that ensure energy reduction at each step. Introduces variational adaptivity - an adaptive mesh refinement strategy based on local energy reductions rather than classical a posteriori estimates.", "result": "Provides necessary conditions for convergence to critical points applicable to a wide class of problems. Validates theoretical results through computational experiments with nonlinear diffusion-reaction models, demonstrating effectiveness.", "conclusion": "The proposed approach successfully incorporates energy structure into computational methods, offering energy-reducing iterative schemes and variational mesh adaptivity that outperform classical PDE-only methods for critical point problems."}}
{"id": "2509.08939", "pdf": "https://arxiv.org/pdf/2509.08939", "abs": "https://arxiv.org/abs/2509.08939", "authors": ["M. Castill\u00f3n", "I. Romero", "J. Segurado"], "title": "A Phase-Field Approach to Fracture and Fatigue Analysis: Bridging Theory and Simulation", "categories": ["cond-mat.mtrl-sci", "cs.NA", "math.NA"], "comment": null, "summary": "This article presents a novel, robust and efficient framework for fatigue\ncrack-propagation that combines the principles of Linear Elastic Fracture\nMechanics (LEFM) with phase-field fracture (PFF). Contrary to cycle-by-cycle\nPFF approaches, this work relies on a single simulation and uses standard crack\npropagation models such as Paris' law for the material response, simplifying\nits parametrization.\n  The core of the methodology is the numerical evaluation of the derivative of\na specimen's compliance with respect to the crack area. To retrieve this\ncompliance the framework relies on a PFF-FEM simulation, controlled imposing a\nmonotonic crack growth. This control of the loading process is done by a new\ncrack-control scheme which allows to robustly trace the complete equilibrium\npath of a crack, capturing complex instabilities. The specimen's compliance\nobtained from the PFF simulation enables the integration of Paris' law to\npredict fatigue life.\n  The proposed methodology is first validated through a series of benchmarks\nwith analytical solutions to demonstrate its accuracy. The framework is then\napplied to more complex geometries where the crack path is unknown, showing a\nvery good agreement with experimental results of both crack paths and fatigue\nlife.", "AI": {"tldr": "A novel framework combining phase-field fracture with linear elastic fracture mechanics for fatigue crack propagation analysis using a single simulation and Paris' law.", "motivation": "Traditional cycle-by-cycle phase-field fracture approaches are computationally expensive and complex to parameterize. This work aims to simplify fatigue crack propagation analysis while maintaining accuracy.", "method": "Uses phase-field fracture FEM simulation with a new crack-control scheme to trace complete equilibrium paths. Computes specimen compliance derivative numerically and integrates Paris' law for fatigue life prediction.", "result": "Validated through benchmarks with analytical solutions showing high accuracy. Applied to complex geometries with unknown crack paths, achieving very good agreement with experimental results for both crack paths and fatigue life.", "conclusion": "The proposed framework provides an efficient, robust, and accurate method for fatigue crack propagation analysis that simplifies parametrization while capturing complex instabilities and matching experimental data."}}
{"id": "2509.08968", "pdf": "https://arxiv.org/pdf/2509.08968", "abs": "https://arxiv.org/abs/2509.08968", "authors": ["Mahsa Sajjadi", "Kaiyang Huang", "Kai Sun"], "title": "Efficient High-Order Participation Factor Computation via Batch-Structured Tensor Contraction", "categories": ["eess.SY", "cs.NA", "cs.SY", "math.NA"], "comment": null, "summary": "Participation factors (PFs) quantify the interaction between system modes and\nstate variables, and they play a crucial role in various applications such as\nmodal analysis, model reduction, and control design. With increasing system\ncomplexity, especially due to power electronic devices and renewable\nintegration, the need for scalable and high-order nonlinear PF (NPF)\ncomputation has become more critical. This paper presents an efficient\ntensor-based method for calculating NPFs up to an arbitrary order. Traditional\ncomputation of PFs directly from normal form theory is computationally\nexpensive -- even for second-order PFs -- and becomes infeasible for higher\norders due to memory constraints. To address this, a tensor contraction-based\napproach is introduced that enables the calculation of high-order PFs using a\nbatching strategy. The batch sizes are dynamically determined based on the\navailable computational resources, allowing scalable and memory-efficient\ncomputation.", "AI": {"tldr": "Efficient tensor-based method for computing high-order nonlinear participation factors using dynamic batching strategy to overcome memory constraints.", "motivation": "Increasing system complexity from power electronics and renewable integration requires scalable computation of nonlinear participation factors for modal analysis, model reduction, and control design.", "method": "Tensor contraction-based approach with dynamic batch sizing determined by available computational resources, enabling memory-efficient calculation of arbitrary-order participation factors.", "result": "The method overcomes computational infeasibility of traditional normal form theory approaches, making high-order nonlinear participation factor computation scalable and practical.", "conclusion": "The proposed tensor-based approach provides an efficient and memory-conscious solution for computing high-order nonlinear participation factors in complex modern power systems."}}
{"id": "2509.08977", "pdf": "https://arxiv.org/pdf/2509.08977", "abs": "https://arxiv.org/abs/2509.08977", "authors": ["Binh Huy Nguyen", "Matti Schneider"], "title": "Symmetries in stochastic homogenization and acclimatizations for the RVE method", "categories": ["cs.CE", "cs.NA", "math.NA"], "comment": "47 pages, 19 figures", "summary": "We investigate the implications of a given symmetry of a random\nmicrostructure on the obtained effective tensor and its fluctuation in the\ncontext of thermal conductivity, and study strategies for enforcing these\nsymmetries in postprocessing via orthogonal projectors. Within the framework of\nthe representative volume element (RVE) method, we establish the invariance\nconditions for the effective tensor and its fluctuation under different\nsymmetry groups of the microstructure. Interestingly, the symmetry of the\nconsidered cell type in the RVE method may break the ensemble symmetry and\ncompromise the approximation of the effective properties. To rectify this\nissue, we introduce dedicated techniques which permit to enforce the expected\nsymmetries in postprocessing and study the implications on the bounds for the\neffective properties as well as the total, the random and the systematic\nerrors. We provide theoretical arguments that suitable projections lead to\nunbiased variance-reduction strategies which furthermore enforce the expected\nsymmetries exactly. Through large-scale FFT-based homogenization simulations,\nwe study the symmetry structure of the estimated effective conductivities and\ntheir fluctuations. Moreover, we demonstrate the power of the\nsymmetry-projection techniques for fiber-reinforced composite microstructures\nof industrial scale.", "AI": {"tldr": "Study on enforcing microstructure symmetries in effective thermal conductivity tensor estimation using orthogonal projectors to reduce errors and maintain symmetry properties.", "motivation": "To address how microstructure symmetries affect effective tensor properties and their fluctuations, and to develop methods for enforcing these symmetries in postprocessing to improve accuracy.", "method": "Use orthogonal projectors to enforce symmetry conditions in postprocessing, analyze invariance under different symmetry groups, and validate with large-scale FFT-based homogenization simulations on fiber-reinforced composites.", "result": "Suitable projections provide unbiased variance-reduction strategies that exactly enforce expected symmetries, improving bounds for effective properties and reducing total, random, and systematic errors.", "conclusion": "Symmetry-projection techniques effectively rectify symmetry breaking in RVE methods, enhancing the accuracy and reliability of effective property estimations in composite microstructures."}}
{"id": "2509.09380", "pdf": "https://arxiv.org/pdf/2509.09380", "abs": "https://arxiv.org/abs/2509.09380", "authors": ["Luca Giuliani", "Michele Lombardi"], "title": "Robust Non-Linear Correlations via Polynomial Regression", "categories": ["cs.LG", "cs.AI", "cs.NA", "math.NA"], "comment": null, "summary": "The Hirschfeld-Gebelein-R\\'enyi (HGR) correlation coefficient is an extension\nof Pearson's correlation that is not limited to linear correlations, with\npotential applications in algorithmic fairness, scientific analysis, and causal\ndiscovery. Recently, novel algorithms to estimate HGR in a differentiable\nmanner have been proposed to facilitate its use as a loss regularizer in\nconstrained machine learning applications. However, the inherent\nuncomputability of HGR requires a bias-variance trade-off, which can possibly\ncompromise the robustness of the proposed methods, hence raising technical\nconcerns if applied in real-world scenarios. We introduce a novel computational\napproach for HGR that relies on user-configurable polynomial kernels, offering\ngreater robustness compared to previous methods and featuring a faster yet\nalmost equally effective restriction. Our approach provides significant\nadvantages in terms of robustness and determinism, making it a more reliable\noption for real-world applications. Moreover, we present a brief experimental\nanalysis to validate the applicability of our approach within a constrained\nmachine learning framework, showing that its computation yields an insightful\nsubgradient that can serve as a loss regularizer.", "AI": {"tldr": "Novel robust computational approach for HGR correlation coefficient using configurable polynomial kernels, offering improved reliability for real-world applications like machine learning regularization.", "motivation": "Existing HGR estimation methods suffer from bias-variance trade-offs due to inherent uncomputability, compromising robustness in real-world scenarios like algorithmic fairness and constrained ML applications.", "method": "Proposed computational approach based on user-configurable polynomial kernels, providing greater robustness and faster yet effective restriction compared to previous methods.", "result": "The method offers significant advantages in robustness and determinism, with experimental validation showing it produces insightful subgradients suitable as loss regularizers in constrained ML frameworks.", "conclusion": "This approach provides a more reliable and robust computational method for HGR correlation, making it suitable for practical real-world applications in machine learning and fairness contexts."}}
{"id": "2509.09391", "pdf": "https://arxiv.org/pdf/2509.09391", "abs": "https://arxiv.org/abs/2509.09391", "authors": ["Kelin Wu", "Hongpeng Sun"], "title": "A preconditioned third-order implicit-explicit algorithm with a difference of varying convex functions and extrapolation", "categories": ["math.OC", "cs.NA", "math.NA"], "comment": null, "summary": "This paper proposes a novel preconditioned implicit-explicit algorithm\nenhanced with the extrapolation technique for non-convex optimization problems.\nThe algorithm employs a third-order Adams-Bashforth scheme for the nonlinear\nand explicit parts and a third-order backward differentiation formula for the\nimplicit part of the gradient flow in variational functions. The proposed\nalgorithm, akin to a generalized difference-of-convex (DC) approach, employs a\nchanging set of convex functions in each iteration. Under the Kurdyka-\\L\nojasiewicz (KL) properties, the global convergence of the algorithm is\nguaranteed, ensuring that it converges within a finite number of preconditioned\niterations. Our numerical experiments, including least squares problems with\nSCAD regularization and the graphical Ginzburg-Landau model, demonstrate the\nproposed algorithm's highly efficient performance compared to conventional DC\nalgorithms.", "AI": {"tldr": "Novel preconditioned IMEX algorithm with extrapolation for non-convex optimization, combining third-order Adams-Bashforth and backward differentiation methods with DC approach for global convergence under KL properties.", "motivation": "To develop an efficient algorithm for non-convex optimization problems that overcomes limitations of conventional difference-of-convex (DC) algorithms by leveraging gradient flow techniques and ensuring global convergence.", "method": "Preconditioned implicit-explicit algorithm using third-order Adams-Bashforth scheme for nonlinear/explicit parts and third-order backward differentiation formula for implicit parts of gradient flow, combined with extrapolation technique and changing convex functions in each iteration.", "result": "Global convergence guaranteed under Kurdyka-\u0141ojasiewicz properties with finite preconditioned iterations. Numerical experiments show highly efficient performance on least squares with SCAD regularization and graphical Ginzburg-Landau model compared to conventional DC algorithms.", "conclusion": "The proposed algorithm provides an effective and efficient approach for non-convex optimization with proven convergence properties and superior performance over existing DC methods."}}
{"id": "2509.09611", "pdf": "https://arxiv.org/pdf/2509.09611", "abs": "https://arxiv.org/abs/2509.09611", "authors": ["Haolan Zheng", "Yanlai Chen", "Jiequn Han", "Yue Yu"], "title": "ReBaNO: Reduced Basis Neural Operator Mitigating Generalization Gaps and Achieving Discretization Invariance", "categories": ["cs.LG", "cs.NA", "math.NA"], "comment": null, "summary": "We propose a novel data-lean operator learning algorithm, the Reduced Basis\nNeural Operator (ReBaNO), to solve a group of PDEs with multiple distinct\ninputs. Inspired by the Reduced Basis Method and the recently introduced\nGenerative Pre-Trained Physics-Informed Neural Networks, ReBaNO relies on a\nmathematically rigorous greedy algorithm to build its network structure offline\nadaptively from the ground up. Knowledge distillation via task-specific\nactivation function allows ReBaNO to have a compact architecture requiring\nminimal computational cost online while embedding physics. In comparison to\nstate-of-the-art operator learning algorithms such as PCA-Net, DeepONet, FNO,\nand CNO, numerical results demonstrate that ReBaNO significantly outperforms\nthem in terms of eliminating/shrinking the generalization gap for both in- and\nout-of-distribution tests and being the only operator learning algorithm\nachieving strict discretization invariance.", "AI": {"tldr": "ReBaNO is a novel data-lean operator learning algorithm that combines reduced basis methods with neural networks to solve PDEs with multiple inputs, achieving superior generalization and strict discretization invariance compared to state-of-the-art methods.", "motivation": "To address the generalization gap and lack of discretization invariance in existing operator learning algorithms for solving PDEs with multiple distinct inputs, while maintaining computational efficiency.", "method": "Combines Reduced Basis Method with Generative Pre-Trained Physics-Informed Neural Networks using a greedy algorithm to build network structure offline, with knowledge distillation via task-specific activation functions for compact architecture.", "result": "Significantly outperforms PCA-Net, DeepONet, FNO, and CNO in eliminating/shrinking generalization gap for both in- and out-of-distribution tests, and is the only operator learning algorithm achieving strict discretization invariance.", "conclusion": "ReBaNO provides a mathematically rigorous, computationally efficient approach for operator learning that overcomes key limitations of existing methods, particularly in generalization performance and discretization invariance."}}
