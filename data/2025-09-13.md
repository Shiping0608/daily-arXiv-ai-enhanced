<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 18]
- [math.AP](#math.AP) [Total: 11]
- [physics.comp-ph](#physics.comp-ph) [Total: 2]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 2]
- [eess.SY](#eess.SY) [Total: 1]
- [math.DS](#math.DS) [Total: 1]
- [astro-ph.HE](#astro-ph.HE) [Total: 2]
- [cs.AI](#cs.AI) [Total: 1]
- [physics.bio-ph](#physics.bio-ph) [Total: 1]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 1]
- [eess.IV](#eess.IV) [Total: 1]
- [math.OC](#math.OC) [Total: 2]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 3]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 2]
- [physics.gen-ph](#physics.gen-ph) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [physics.ed-ph](#physics.ed-ph) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Exponential Runge-Kutta methods for parabolic equations with state-dependent delay](https://arxiv.org/abs/2509.08902)
*Qiumei Huang,Alexander Ostermann,Gangfan Zhong*

Main category: math.NA

TL;DR: Construction and analysis of exponential Runge-Kutta methods for semilinear parabolic problems with state-dependent delay, including well-posedness, scheme development, and convergence analysis.


<details>
  <summary>Details</summary>
Motivation: To develop efficient temporal discretization methods for semilinear parabolic problems with arbitrary state-dependent delay, which are challenging due to the delay dependency on the solution state.

Method: Explicit exponential Runge-Kutta methods with continuous extension for delay approximation, including first and second order schemes and collocation-type methods for arbitrary order.

Result: Established well-posedness of the problem, unique solvability of the schemes, and proved convergence of the proposed numerical methods.

Conclusion: The developed exponential Runge-Kutta methods are effective for solving semilinear parabolic problems with state-dependent delay, with theoretical convergence supported by numerical experiments.

Abstract: The aim of this paper is to construct and analyze exponential Runge-Kutta
methods for the temporal discretization of a class of semilinear parabolic
problems with arbitrary state-dependent delay. First, the well-posedness of the
problem is established. Subsequently, first and second order schemes are
constructed. They are based on the explicit exponential Runge-Kutta methods,
where the delayed solution is approximated by a continuous extension of the
time discrete solution. Schemes of arbitrary order can be constructed using the
methods of collocation type. The unique solvability and convergence of the
proposed schemes are established. Finally, we discuss implementation issues and
present some numerical experiments to illustrate our theoretical results.

</details>


### [2] [Quasi-Trefftz spaces for a first-order formulation of the Helmholtz equation](https://arxiv.org/abs/2509.08936)
*Lise-Marie Imbert-Gérard,Andréa Lagardère,Guillaume Sylvand,Sébastien Tordeux*

Main category: math.NA

TL;DR: Development of quasi-Trefftz methods for first-order differential systems, focusing on discrete quasi-Trefftz spaces including definition, basis construction, and computational aspects.


<details>
  <summary>Details</summary>
Motivation: To establish the foundation for quasi-Trefftz methods applied to first-order differential systems, which represents a novel approach in numerical methods for differential equations.

Method: Defines discrete quasi-Trefftz spaces, constructs corresponding basis functions, and addresses computational implementation aspects of these methods.

Result: Presents the first systematic framework for quasi-Trefftz methods in first-order differential systems, providing definitions and computational foundations.

Conclusion: This work lays the groundwork for future development and application of quasi-Trefftz methods to solve first-order differential systems numerically.

Abstract: This work is the first step in the development of quasi-Trefftz methods for
first-order differential systems. It focuses on discrete quasi-Trefftz spaces,
starting from their definition and including construction of corresponding
bases together with their computational aspect.

</details>


### [3] [Numerical Approximation and Bifurcation Results for an Elliptic Problem with Superlinear Subcritical Nonlinearity on the Boundary](https://arxiv.org/abs/2509.08990)
*Shalmali Bandyopadhyay,Thomas Lewis,Dustin Nichols*

Main category: math.NA

TL;DR: Numerical algorithms for approximating positive solutions of elliptic boundary value problems with superlinear subcritical nonlinearity on the boundary, including extension to coupled systems with cross-coupling boundary conditions.


<details>
  <summary>Details</summary>
Motivation: While theoretical results (existence, uniqueness, nonexistence, multiplicity) for such problems are well-established, their numerical treatment presents computational challenges due to absence of comparison principles and complex bifurcation phenomena.

Method: Finite difference formulations for single equations and coupled systems, combined with continuation methods to trace complete bifurcation curves. Principal eigenvalue analysis for linearized problems to determine unique bifurcation points.

Result: Established admissibility results for finite difference method. Validated existing existence and uniqueness results through numerical computations. Eigenvalue analysis provided additional theoretical insights and computational intuition.

Conclusion: The proposed finite difference methods combined with continuation techniques successfully address computational challenges and provide effective numerical treatment for these complex boundary value problems with nonlinear boundary conditions.

Abstract: We develop numerical algorithms to approximate positive solutions of elliptic
boundary value problems with superlinear subcritical nonlinearity on the
boundary of the form $-\Delta u + u = 0$ in $\Omega$ with $\frac{\partial
u}{\partial \eta} = \lambda f(u)$ on $\partial\Omega$ as well as an extension
to a corresponding system of equations. While existence, uniqueness,
nonexistence, and multiplicity results for such problems are well-established,
their numerical treatment presents computational challenges due to the absence
of comparison principles and complex bifurcation phenomena. We present finite
difference formulations for both single equations and coupled systems with
cross-coupling boundary conditions, establishing admissibility results for the
finite difference method. We derive principal eigenvalue analysis for the
linearized problems to determine unique bifurcation points from trivial
solutions. The eigenvalue analysis provides additional insight into the
theoretical properties of the problem while also providing intuition for
computing approximate solutions based on the proposed finite difference
formulation. We combine our finite difference methods with continuation methods
to trace complete bifurcation curves, validating established existence and
uniqueness results and consistent with the results of the principle eigenvalue
analysis.

</details>


### [4] [Numerical modeling of elastic waves in thin shells with grid-characteristic method](https://arxiv.org/abs/2509.09017)
*Katerina Beklemysheva,Egor Michel,Andrey Ovsiannikov*

Main category: math.NA

TL;DR: A method for modeling elastic waves in thin structures using Kirchhoff-Love theory with grid-characteristic numerical solution, enabling efficient simulation of large complex structures while maintaining wave resolution.


<details>
  <summary>Details</summary>
Motivation: To overcome computational limitations when modeling elastic wave processes in large complex structures with thin components, which require extremely fine grids for accurate 3D modeling.

Method: Derived hyperbolic dynamic system from Kirchhoff-Love material model, solved numerically using grid-characteristic method, replacing 3D fine mesh with complex material model on coarser mesh.

Result: Developed efficient numerical approach that allows modeling of large thin structures with reasonable grid sizes while resolving significant wave types, validated against 3D elasticity calculations.

Conclusion: The Kirchhoff-Love based approach provides a practical solution for modeling wave processes in complex thin structures, balancing computational efficiency with adequate physical accuracy for engineering applications.

Abstract: Numerical modeling of strength and non-destructive testing of complex
structures such as buildings, space rockets or oil reservoirs often involves
calculations on extremely large grids. The modeling of elastic wave processes
in solids places limitations on the grid element size because resolving
different elastic waves requires at least several grid elements for the
characteristic size of the modeled object. For a thin plate, the defining size
is its thickness, and a complex structure that contains large-scale thin
objects requires a large-scale grid to preserve its uniformity. One way to
bypass this problem is the theory of thin plates and shells that replaces a
simple material model on a fine three-dimensional mesh with a more complex
material model on a coarser mesh. This approach loses certain fine effects
inside the thin plate, but allows us to model large and complex thin objects
with a reasonable size calculation grid and resolve all the significant wave
types. In this research, we take the Kirchhoff-Love material model and derive a
hyperbolic dynamic system of equations that allows for a physical
interpretation of eigenvalues and eigenvectors. The system is solved
numerically with a grid-characteristic method. Numerical results for several
model statements are compared with three-dimensional calculations based on
grid-characteristic method for a three dimensional elasticity.

</details>


### [5] [Characterization of the near-null error components utilized in composite adaptive AMG solvers](https://arxiv.org/abs/2509.09023)
*Austen J. Nelson,Panayot S. Vassilevski*

Main category: math.NA

TL;DR: Theoretical justification for adaptive composite solvers using AMG μ-cycle methods that target error components current solvers can't handle efficiently, with aggregates constructed via modularity matrix from graph community detection.


<details>
  <summary>Details</summary>
Motivation: To improve solver efficiency by addressing error components that existing AMG methods struggle to damp effectively, creating more robust composite solvers for various matrix types.

Method: Construct adaptive composite solvers using sequence of AMG μ-cycle methods, where each component uses aggregation-based AMG with aggregates built via modularity matrix (from graph community detection) applied to the matrix and current error vector.

Result: The performance is demonstrated on various sparse matrices including those from discretized PDEs and more general matrices, showing effectiveness of the approach.

Conclusion: The adaptive composite solver framework provides an effective approach for handling challenging error components in AMG methods, with broad applicability across different matrix types including PDE-derived and general sparse matrices.

Abstract: We provide a theoretical justification for the construction of adaptive
composite solvers based on a sequence of AMG (algebraic multigrid) $\mu$-cycle
methods that exploit error components that the current solver cannot damp
efficiently. Each solver component is an aggregation based AMG where its
aggregates are constructed using the popular in graph community detection
modularity matrix. The latter utilizes the given matrix and the error component
vector the current solver cannot handle. The performance of the resulting
adaptive composite solver is illustrated on a variety of sparse matrices both
arising from discretized PDEs and ones with more general nature.

</details>


### [6] [Strong convergence of a semi tamed scheme for stochastic differential algebraic equation under non-global Lipschitz coefficients](https://arxiv.org/abs/2509.09032)
*Guy Tsafack,Antoine Tambue*

Main category: math.NA

TL;DR: First strong convergence analysis of a semi-implicit tamed method for stochastic differential algebraic equations (SDAEs) under non-global Lipschitz conditions, achieving order 1/2 convergence.


<details>
  <summary>Details</summary>
Motivation: Explicit Euler scheme fails for SDEs with superlinear growth coefficients, and the problem is more challenging for SDAEs due to matrix singularity. Need for a robust numerical method that can handle non-global Lipschitz settings.

Method: Developed a semi-implicit tamed method where linear drift components are approximated implicitly and nonlinear components are tamed and approximated explicitly. Created an equivalent dual tamed scheme for mathematical analysis by eliminating constraints from original SDAEs.

Result: The proposed method strongly converges with order 1/2 to the exact solution under non-global Lipschitz conditions. Numerical simulations confirm theoretical findings.

Conclusion: The semi-implicit tamed method effectively addresses the convergence challenges of SDAEs in non-global Lipschitz settings, providing a reliable numerical approach with proven strong convergence properties.

Abstract: We are investigating the first strong convergence analysis of a numerical
method for stochastic differential algebraic equations (SDAEs) under a
non-global Lipschitz setting. It is well known that the explicit Euler scheme
fails to converge strongly to the exact solution of a stochastic differential
equation (SDEs) when at least one of the coefficients grows superlinearly. The
problem becomes more challenging in the case of stochastic
differential-algebraic equations (SDAEs) due to the singularity of the matrix.
To address this, we build a new scheme called the semi-implicit tamed method
for SDAEs and provide its strong convergence result under non-global Lipschitz
setting. In other words, the linear component of the drift term is approximated
implicitly, whereas its nonlinear component is tamed and approximated
explicitly. We show that this method strongly converges with order
$\frac{1}{2}$ to the exact solution. To prove this strong convergence result,
we first derive an equivalent scheme, that we call the dual tamed scheme, which
is more suitable for mathematical analysis and is associated with the inherent
stochastic differential equation obtained by eliminating the constraints from
the original SDAEs. To demonstrate the effectiveness of the proposed scheme,
numerical simulations are performed, confirming that the theoretical findings
are consistent with the numerical results.

</details>


### [7] [Fast Operator-Splitting Methods for Nonlinear Elliptic Equations](https://arxiv.org/abs/2509.09132)
*Jingyu Yang,Shingyu Leung,Jianliang Qian,Hao Liu*

Main category: math.NA

TL;DR: Novel operator-splitting/finite element method for solving nonlinear elliptic problems, including semilinear and fully nonlinear equations like Monge-Ampère and Pucci's equations, with improved efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Nonlinear elliptic problems are important in fields like plasma physics, astrophysics, and optimal transport, but existing solution methods need improvement in efficiency and accuracy.

Method: Introduces auxiliary function for semilinear elliptic PDEs, develops convergent operator-splitting/finite element scheme, extends to fully nonlinear equations via reformulation, uses mixed finite element method to approximate solution and Hessian matrix.

Result: Numerical experiments show the proposed method outperforms existing approaches in both efficiency and accuracy, and works well on domains with curved boundaries.

Conclusion: The developed operator-splitting/finite element method provides an effective and accurate approach for solving various nonlinear elliptic problems, with practical advantages for complex domain geometries.

Abstract: Nonlinear elliptic problems arise in many fields, including plasma physics,
astrophysics, and optimal transport. In this article, we propose a novel
operator-splitting/finite element method for solving such problems. We begin by
introducing an auxiliary function in a new way for a semilinear elliptic
partial differential equation, leading to the development of a convergent
operator-splitting/finite element scheme for this equation. The algorithm is
then extended to fully nonlinear elliptic equations of the Monge-Amp\`ere type,
including the Dirichlet Monge-Amp\`ere equation and Pucci's equation. This is
achieved by reformulating the fully nonlinear equations into forms analogous to
the semilinear case, enabling the application of the proposed splitting
algorithm. In our implementation, a mixed finite element method is used to
approximate both the solution and its Hessian matrix. Numerical experiments
show that the proposed method outperforms existing approaches in efficiency and
accuracy, and can be readily applied to problems defined on domains with curved
boundaries.

</details>


### [8] [Hybrid-Precision Block-Jacobi Preconditioned GMRES Solver for Linear System in Circuit Simulation](https://arxiv.org/abs/2509.09139)
*Zijian Zhang,Rui Hong,Xuesong Chen,Shuting Cai*

Main category: math.NA

TL;DR: Hybrid-precision block-Jacobi preconditioned GMRES solver for efficient circuit simulation of large sparse systems


<details>
  <summary>Details</summary>
Motivation: Traditional solvers struggle with large-scale sparse systems in circuit simulation, leading to long simulation times and reduced accuracy as integrated circuits become more complex

Method: Uses hybrid-precision strategy (single-precision for intensive tasks, double-precision for accuracy-sensitive computations), graph partitioning for preconditioner generation, and restart strategy for large-scale problems

Result: Significantly outperforms existing solvers (SuperLU, KLU, SFLU) in both preconditioning and GMRES runtime, with improved spectral clustering for faster solutions

Conclusion: The proposed hybrid-precision preconditioner effectively addresses large sparse system challenges in circuit simulation with proven convergence and superior performance

Abstract: As integrated circuits become increasingly complex, the demand for efficient
and accurate simulation solvers continues to rise. Traditional solvers often
struggle with large-scale sparse systems, leading to prolonged simulation times
and reduced accuracy. In this paper, a hybrid-precision block-Jacobi
preconditioned GMRES solver is proposed to solve the large sparse system in
circuit simulation. The proposed method capitalizes on the structural sparsity
and block properties of circuit matrices, employing a novel hybrid-precision
strategy that applies single-precision arithmetic for computationally intensive
tasks and double-precision arithmetic for critical accuracy-sensitive
computations. Additionally, we use the graph partitioning tools to assist in
generating preconditioners, ensuring an optimized preconditioning process. For
large-scale problems, we adopt the restart strategy to increase the
computational efficiency. Through rigorous mathematical reasoning, the
convergence and error analysis of the proposed method are carried out.
Numerical experiments on various benchmark matrices demonstrate that our
approach significantly outperforms existing solvers, including SuperLU, KLU,
and SFLU, in terms of both preconditioning and GMRES runtime. The proposed
hybrid-precision preconditioner effectively improves spectral clustering,
leading to faster solutions.

</details>


### [9] [Isogeometric Topology Optimization Based on Topological Derivatives](https://arxiv.org/abs/2509.09236)
*Guilherme Henrique Teixeira,Nepomuk Krenn,Peter Gangl,Benjamin Marussig*

Main category: math.NA

TL;DR: Isogeometric topology optimization using topological derivatives and level-set method that eliminates remeshing requirements and initial hole definitions, with investigation of higher-degree basis functions.


<details>
  <summary>Details</summary>
Motivation: Traditional topology optimization often requires challenging remeshing steps during topological changes, and typically needs initial holes defined for optimization.

Method: Combines level-set method with immersed isogeometric framework driven by topological derivatives, allowing geometry updates without remeshing and topological modifications without initial holes.

Result: Higher-degree basis functions improve solution accuracy, while linear basis functions remain sufficient for level-set representation. Two numerical examples demonstrate the approach.

Conclusion: The proposed isogeometric approach successfully eliminates remeshing requirements and initial hole definitions, with optimal performance achieved using higher-degree basis for solution approximation and linear basis for level-set representation.

Abstract: Topology optimization is a valuable tool in engineering, facilitating the
design of optimized structures. However, topological changes often require a
remeshing step, which can become challenging. In this work, we propose an
isogeometric approach to topology optimization driven by topological
derivatives. The combination of a level-set method together with an immersed
isogeometric framework allows seamless geometry updates without the necessity
of remeshing. At the same time, topological derivatives provide topological
modifications without the need to define initial holes [7]. We investigate the
influence of higher-degree basis functions in both the level-set representation
and the approximation of the solution. Two numerical examples demonstrate the
proposed approach, showing that employing higher-degree basis functions for
approximating the solution improves accuracy, while linear basis functions
remain sufficient for the level-set function representation.

</details>


### [10] [Long time strong convergence analysis of one-step methods for McKean-Vlasov SDEs with superlinear growth coefficients](https://arxiv.org/abs/2509.09274)
*Taiyuan Liu,Yaozhong Hu,Siqing Gan*

Main category: math.NA

TL;DR: Strong convergence rate analysis for discretization approximations of McKean-Vlasov SDEs with super-linear growth coefficients over infinite time horizon under non-globally Lipschitz conditions.


<details>
  <summary>Details</summary>
Motivation: To analyze the convergence properties of numerical schemes for McKean-Vlasov SDEs with super-linear growth coefficients, which are challenging due to the non-globally Lipschitz conditions and infinite time horizon.

Method: Derived propagation of chaos and mean-square convergence rate for general one-step time discretization schemes. Applied the general result to two specific schemes: projected Euler scheme and backward Euler scheme.

Result: Obtained mean-square convergence rate over infinite time horizon for the numerical schemes. Numerical experiments validated the theoretical findings.

Conclusion: The paper provides rigorous convergence analysis for discretization approximations of McKean-Vlasov SDEs with super-linear growth, establishing theoretical foundations and practical validation through numerical experiments.

Abstract: This paper presents a strong convergence rate analysis of general
discretization approximations for McKean-Vlasov SDEs with super-linear growth
coefficients over infinite time horizon. Under some specified non-globally
Lipschitz conditions, we derive the propagation of chaos, and the mean-square
convergence rate over infinite time horizon for general one-step time
discretization schemes for the underlying Mckean-Vlasov SDEs. As an application
of the general result it is obtained the mean-square convergence rate over
infinite time horizon for two numerical schemes: the projected Euler scheme and
the backward Euler scheme for Mckean-Vlasov SDEs in non-globally Lipschitz
settings. Numerical experiments are provided to validate the theoretical
findings.

</details>


### [11] [Optimal Control of a Hemivariational Inequality of Stationary Convective Brinkman-Forchheimer Extended Darcy equations with Numerical Approximation](https://arxiv.org/abs/2509.09287)
*Wasim Akram,Manil T. Mohan*

Main category: math.NA

TL;DR: Optimal control analysis for stationary convective Brinkman-Forchheimer extended Darcy hemivariational inequality in 2D/3D with control constraints and numerical approximation.


<details>
  <summary>Details</summary>
Motivation: Study fluid flow through porous media governed by CBFeD equations accounting for convection, damping, and nonlinear resistance effects with non-leak boundary and friction conditions.

Method: Analyze solution stability, prove existence of optimal control solutions, develop numerical scheme with finite element discretization, and validate with numerical examples.

Result: Established stability of solutions under perturbations, proved existence of optimal control solutions, developed convergent numerical scheme, and validated theory through numerical implementation.

Conclusion: Successfully developed and validated a comprehensive framework for optimal control of CBFeD hemivariational inequalities with practical numerical implementation for porous media flow problems.

Abstract: This paper studies an optimal control problem for a stationary convective
Brinkman-Forchheimer extended Darcy (CBFeD) hemivariational inequality in two
and three dimensions, subject to control constraints, and develops its
numerical approximation. The hemivariational inequality provides the weak
formulation of a stationary incompressible fluid flow through a porous medium,
governed by the CBFeD equations, which account for convection, damping, and
nonlinear resistance effects. The problem incorporates a non-leak boundary
condition and a subdifferential friction-type condition. We first analyze the
stability of solutions with respect to perturbations in the external force
density and the superpotential. Next, we prove the existence of a solution to
the optimal control problem, where the external force density acts as the
control variable. We then propose a numerical scheme for solving the optimal
control problem and establish its convergence. For concreteness, the numerical
method is implemented using finite element discretization. Finally, we provide
some numerical examples to validate the theory developed.

</details>


### [12] [Euler-type methods for Levy-driven McKean-Vlasov SDEs with super-linear coefficients: mean-square error analysis](https://arxiv.org/abs/2509.09302)
*Jingtao Zhu,Yuying Zhao,Siqing Gan*

Main category: math.NA

TL;DR: Euler-type numerical schemes for Levy-driven McKean-Vlasov SDEs with super-linear growth coefficients, removing restrictive coercivity assumptions and achieving near 1/2 convergence rates.


<details>
  <summary>Details</summary>
Motivation: To develop numerical methods for Levy-driven McKean-Vlasov SDEs with super-linear growth coefficients without relying on restrictive coercivity conditions that limit applicability.

Method: General class of Euler-type schemes incorporating projections or nonlinear transformations (tanh-Euler, tamed-Euler, sine-Euler) to establish moment bounds for numerical solutions.

Result: Proposed schemes achieve convergence rates arbitrarily close to 1/2 for interacting particle systems, validated through numerical examples.

Conclusion: The developed Euler-type schemes successfully handle super-linear growth in Levy-driven McKean-Vlasov SDEs without coercivity assumptions, providing rigorous convergence analysis and practical numerical performance.

Abstract: We develop and analyze a general class of Euler-type numerical schemes for
Levy-driven McKean-Vlasov stochastic differential equations (SDEs), where the
drift, diffusion and jump coefficients grow super-linearly in the state
variable. These numerical schemes are derived by incorporating projections or
nonlinear transformations into the classical Euler method, with the primary
objective of establishing moment bounds for the numerical solutions. This class
of schemes includes the tanh-Euler, tamed-Euler and sine-Euler schemes as
special cases. In contrast to existing approaches that rely on a coercivity
condition (e.g., Assumption B-1 in Kumar et al., arXiv:2010.08585), the
proposed schemes remove such a restrictive assumption. We provide a rigorous
mean-square convergence analysis and establish that the proposed schemes
achieve convergence rates arbitrarily close to 1/2 for the interacting particle
systems associated with Levy-driven McKean-Vlasov SDEs. Several numerical
examples are presented to illustrate the convergence behavior and validate the
theoretical results.

</details>


### [13] [Expressive Power of Deep Networks on Manifolds: Simultaneous Approximation](https://arxiv.org/abs/2509.09362)
*Hanfei Zhou,Lei Shi*

Main category: math.NA

TL;DR: First simultaneous approximation theory for deep neural networks on manifolds, showing constant-depth ReLU networks with bounded weights can approximate Sobolev space functions with optimal parameter efficiency that overcomes curse of dimensionality.


<details>
  <summary>Details</summary>
Motivation: Solving PDEs on complex curved domains is challenging due to geometry complicating function and derivative approximations required by differential operators.

Method: Prove approximation theory using constant-depth ReLU^{k-1} networks with bounded weights to approximate functions in Sobolev spaces W_p^k(M^d) to error ε in W_p^s(M^d) norm.

Result: Achieves approximation with O(ε^{-d/(k-s)}) parameters, overcoming curse of dimensionality by depending only on intrinsic dimension d. Matching lower bound proves near optimality.

Conclusion: Network architecture leverages sparse structure to exploit manifold's low-dimensional geometry, providing theoretical foundation for learning PDEs on manifolds involving derivatives.

Abstract: A key challenge in scientific machine learning is solving partial
differential equations (PDEs) on complex domains, where the curved geometry
complicates the approximation of functions and their derivatives required by
differential operators. This paper establishes the first simultaneous
approximation theory for deep neural networks on manifolds. We prove that a
constant-depth $\mathrm{ReLU}^{k-1}$ network with bounded weights--a property
that plays a crucial role in controlling generalization error--can approximate
any function in the Sobolev space $\mathcal{W}_p^{k}(\mathcal{M}^d)$ to an
error of $\varepsilon$ in the $\mathcal{W}_p^{s}(\mathcal{M}^d)$ norm, for
$k\geq 3$ and $s<k$, using $\mathcal{O}(\varepsilon^{-d/(k-s)})$ nonzero
parameters, a rate that overcomes the curse of dimensionality by depending only
on the intrinsic dimension $d$. These results readily extend to functions in
H\"older-Zygmund spaces. We complement this result with a matching lower bound,
proving our construction is nearly optimal by showing the required number of
parameters matches up to a logarithmic factor. Our proof of the lower bound
introduces novel estimates for the Vapnik-Chervonenkis dimension and
pseudo-dimension of the network's high-order derivative classes. These
complexity bounds provide a theoretical cornerstone for learning PDEs on
manifolds involving derivatives. Our analysis reveals that the network
architecture leverages a sparse structure to efficiently exploit the manifold's
low-dimensional geometry.

</details>


### [14] [A Low-Rank tensor framework for THB-Splines](https://arxiv.org/abs/2509.09434)
*Tom-Christian Riemer,Martin Stoll*

Main category: math.NA

TL;DR: A low-rank framework for adaptive isogeometric analysis with THB-splines that reduces memory and time costs of matrix assembly through tensor-train representations and level-wise assembly.


<details>
  <summary>Details</summary>
Motivation: To address the memory- and time-intensive matrix assembly bottleneck in adaptive isogeometric analysis when the global tensor-product structure is lost due to local refinement.

Method: Uses tensor-train (TT) representations via AMEn solver, interpolates geometry-induced terms in separable spline spaces, partitions cells into Cartesian cuboids, and assembles hierarchical operators in block TT format.

Result: For model problems with moderately complex refinement regions, the approach reduces memory footprint and assembly time while maintaining accuracy, though limitations exist when ranks grow with complexity.

Conclusion: This framework advances scalable adaptive IgA with THB-splines, particularly benefiting three-dimensional applications, despite some limitations with increasing geometric or refinement complexity.

Abstract: We introduce a low-rank framework for adaptive isogeometric analysis with
truncated hierarchical B-splines (THB-splines) that targets the main bottleneck
of local refinement: memory- and time-intensive matrix assembly once the global
tensor-product structure is lost. The method interpolates geometry-induced
weight and source terms in separable spline spaces and computes their
tensor-train (TT) representations via the alternating minimal energy (AMEn)
solver, enabling level-wise assembly of system operators using univariate
quadrature. To recover separability in the adaptive setting, we reduce the
active basis to tensor-product domains and partition active/non-active cells
into a small number of Cartesian cuboids, so each contributes a Kronecker
factor that is accumulated and rounded in TT. We realize the two-scale relation
with truncation in low rank and assemble the global hierarchical operators in a
block TT format suitable for iterative solvers. A prototype MATLAB
implementation built on the GeoPDEs package and the TT-Toolbox demonstrates
that, for model problems with moderately complex refinement regions, the
approach reduces memory footprint and assembly time while maintaining accuracy;
we also discuss limitations when ranks grow with geometric or refinement
complexity. This framework advances scalable adaptive IgA with THB-splines,
particularly in three dimensions.

</details>


### [15] [Second-order Optimally Stable IMEX (pseudo-)staggered Galerkin discretization: application to lava flow modeling](https://arxiv.org/abs/2509.09460)
*Federico Gatti,Giuseppe Orlando*

Main category: math.NA

TL;DR: Second-order optimally stable IMEX Runge-Kutta schemes for lava flow modeling using shallow water equations, featuring a novel pseudo-staggered Galerkin scheme and path-conservative method for non-conservative terms.


<details>
  <summary>Details</summary>
Motivation: To develop stable and accurate numerical schemes for modeling lava flow dynamics using modified shallow water equations, addressing stability challenges in space-time discretization.

Method: Combines IMEX Runge-Kutta schemes with a novel pseudo-staggered Galerkin approach (extension of Taylor-Galerkin TG2), using von Neumann stability analysis, Lax-Wendroff procedure, and Path-Conservative method for non-conservative terms.

Result: The proposed scheme demonstrates accuracy, robustness, and well-balancing properties across multiple test cases for the lava flow model.

Conclusion: The developed second-order optimally stable IMEX schemes with pseudo-staggered Galerkin discretization provide an effective numerical framework for simulating lava flow dynamics with improved stability and accuracy.

Abstract: We present second-order optimally stable Implicit-Explicit (IMEX) Runge-Kutta
(RK) schemes with application to a modified set of shallow water equations that
can be used to model the dynamics of lava flows. The schemes are optimally
stable in the sense that they satisfy, at the space-time discretization level,
a condition analogous to the \texttt{L}-stability of Runge-Kutta methods for
ordinary differential equations. A novel (pseudo-)staggered Galerkin scheme is
introduced, which can be interpreted as an extension of the classical two-step
Taylor-Galerkin (TG2) scheme. The method is derived by combining a von Neumann
stability analysis with a Lax-Wendroff procedure. For the discretization of the
non-conservative terms that characterize the lava flow model, we employ the
Path-Conservative (PC) method. The proposed scheme is evaluated on a number of
relevant test cases, demonstrating accuracy, robustness, and well-balancing
properties for the lava flow model.

</details>


### [16] [Minimality of Tree Tensor Network Ranks](https://arxiv.org/abs/2509.09463)
*Jana Jovcheva,Tim Seynnaeve,Nick Vannieuwenhoven*

Main category: math.NA

TL;DR: Characterization of minimal bond dimensions in tree tensor networks, generalizing multilinear rank theory and showing minimality is a generic property.


<details>
  <summary>Details</summary>
Motivation: To establish necessary and sufficient conditions for bond dimensions in tree tensor networks to be minimal, extending previous work on multilinear ranks and understanding when tensors cannot be represented with smaller bond dimensions.

Method: Mathematical analysis of tree tensor network structures, generalizing Carlini and Kleppe's characterization of tensors with given multilinear rank, and employing Zariski topology concepts to study generic properties.

Result: Established complete characterization of minimal bond dimensions for tree tensor networks, and proved that non-minimal tensors form a Zariski closed subset, making minimality a generic property in these networks.

Conclusion: The paper provides fundamental theoretical results about minimal representations in tree tensor networks, showing that minimal bond dimensions can be precisely characterized and that most tensors in such networks exhibit this minimality property.

Abstract: For a given tree tensor network $G$, we call a tuple of bond dimensions
minimal if there exists a tensor $T$ that can be represented by this network
but not on the same tree topology with strictly smaller bond dimensions. We
establish necessary and sufficient conditions on the bond dimensions of a tree
tensor network to be minimal, generalizing a characterization of Carlini and
Kleppe about existence of tensors with a given multilinear rank. We also show
that in a minimal tree tensor network, the non-minimal tensors form a Zariski
closed subset, so minimality is a generic property in this sense.

</details>


### [17] [Bioluminescence tomography: A new regularized shape optimization method](https://arxiv.org/abs/2509.09533)
*Qianqian Wu,Rongfang Gong,Wei Gong,Ziyi Zhang,Shengfeng Zhu*

Main category: math.NA

TL;DR: A shape optimization framework for bioluminescence tomography that recovers light source support and intensity using level-set representation with regularization for stable reconstruction.


<details>
  <summary>Details</summary>
Motivation: To solve the inverse source problem in bioluminescence tomography by recovering both the spatial support and intensity of light sources from boundary measurements, addressing challenges with closely located or nested sources.

Method: Developed a shape optimization framework that decouples source strength and support through first-order optimality conditions, incorporating parameter-dependent coupled complex boundary method (CCBM) with perimeter and volume regularizations, using level-set representation to handle topological changes.

Result: The method demonstrates robustness, accuracy, and noise-resistance in numerical experiments, showing advantages over existing approaches for reconstructing multiple, closely located, or nested sources.

Conclusion: The proposed shape optimization framework with level-set representation and regularization techniques provides an effective solution for stable and accurate bioluminescence tomography reconstruction, capable of handling complex source configurations.

Abstract: In this paper, we investigate an inverse source problem arising in
bioluminescence tomography (BLT), where the objective is to recover both the
support and intensity of the light source from boundary measurements. A shape
optimization framework is developed, in which the source strength and its
support are decoupled through first-order optimality conditions. To enhance the
stability of the reconstruction, we incorporate a parameter-dependent coupled
complex boundary method(CCBM) scheme together with perimeter and volume
regularizations. The level-set representation naturally accommodates
topological changes, enabling the reconstruction of multiple, closely located,
or nested sources. Theoretical justifications are provided, and a series of
numerical experiments are conducted to validate the proposed method. The
results demonstrate the robustness, accuracy, and noise-resistance of the
algorithm, as well as its advantages over existing approaches.

</details>


### [18] [Iterative energy reduction Galerkin methods and variational adaptivity](https://arxiv.org/abs/2509.09600)
*Pascal Heid,Thomas P. Wihler*

Main category: math.NA

TL;DR: Proposes energy-based computational methods for solving Euler-Lagrange equations, featuring energy-reducing iterative schemes and variational mesh refinement without classical error estimates.


<details>
  <summary>Details</summary>
Motivation: Classical PDE solvers ignore the energy structure of variational problems. The authors aim to incorporate energy reduction principles directly into computational methods for better convergence and efficiency.

Method: Develops (linearized) iterative Galerkin discretization schemes that ensure energy reduction at each step. Introduces variational adaptivity - an adaptive mesh refinement strategy based on local energy reductions rather than traditional a posteriori error estimates.

Result: Provides necessary conditions for convergence to critical points applicable to a wide class of problems. Validates theoretical results through computational experiments on nonlinear diffusion-reaction models, demonstrating scheme effectiveness.

Conclusion: Energy-based computational approaches that incorporate the variational structure outperform classical PDE-only methods, with energy-reducing iterative schemes and variational mesh refinement proving effective for solving Euler-Lagrange equations.

Abstract: Critical points of energy functionals, which are of broad interest, for
instance, in physics and chemistry, in solid and quantum mechanics, in material
science, or in general diffusion-reaction models arise as solutions to the
associated Euler-Lagrange equations. While classical computational solution
methods for such models typically focus solely on the underlying partial
differential equations, we propose an approach that also incorporates the
energy structure itself. Specifically, we examine (linearized) iterative
Galerkin discretization schemes that ensure energy reduction at each step.
Additionally, we provide necessary conditions, which are applicable to a wide
class of problems, that guarantee convergence to critical points of the PDE.
Moreover, in the specific context of finite element discretizations, we present
a very generally applicable adaptive mesh refinement strategy - the so-called
variational adaptivity approach - which, rather than using classical a
posteriori estimates, is based on exploiting local energy reductions. The
theoretical results are validated for several computational experiments in the
context of nonlinear diffusion-reaction models, thereby demonstrating the
effectiveness of the proposed scheme.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [19] [Logarithmic wave decay for short range wavespeed perturbations with radial regularity](https://arxiv.org/abs/2509.08957)
*Gayana Jayasinghe,Katrina Morgan,Jacob Shapiro,Mengxuan Yang*

Main category: math.AP

TL;DR: Logarithmic local energy decay for wave equations with varying wavespeed in 2+ dimensions, using resolvent analysis in different frequency regimes.


<details>
  <summary>Details</summary>
Motivation: To establish energy decay properties for wave equations with perturbed wavespeeds, which is important for understanding wave propagation and scattering in non-uniform media.

Method: Analyze weighted resolvent for real frequencies using two approaches: low-frequency regime (Neumann series expansion of free resolvent) and non-zero frequencies (uniform resolvent estimate via Carleman estimate).

Result: Proves logarithmic local energy decay for wavespeed perturbations of unity with mild radial regularity, with Hölder continuity of weighted resolvent modulo logarithmic terms in 2D.

Conclusion: The paper successfully establishes energy decay properties through resolvent analysis, providing important results for wave equations with varying wavespeeds in multiple dimensions.

Abstract: We establish logarithmic local energy decay for wave equations with a varying
wavespeed in dimensions two and higher, where the wavespeed is assumed to be a
short range perturbation of unity with mild radial regularity. The key
ingredient is H\"older continuity of the weighted resolvent for real
frequencies $\lambda$, modulo a logarithmic remainder in dimension two as
$\lambda \to 0$. Our approach relies on a study of the resolvent in two
distinct frequency regimes. In the low frequency regime, we derive an expansion
for the resolvent using a Neumann series and properties of the free resolvent.
For frequencies away from zero, we establish a uniform resolvent estimate by
way of a Carleman estimate.

</details>


### [20] [Mountain Pass Critical Points of the Liquid Drop Model](https://arxiv.org/abs/2509.09098)
*Gregory R. Chambers,Jared Marx-Kuo*

Main category: math.AP

TL;DR: Construction of non-minimizing critical points for Gamow's liquid drop model representing maximal energy configurations during nuclear fission from volume V to two V/2 volumes.


<details>
  <summary>Details</summary>
Motivation: To understand the energy barrier and critical configurations in nuclear fission processes using Gamow's liquid drop model, particularly for volumes between 3.512 and 10.

Method: Geometric measure theoretical methods from min-max construction of minimal surfaces, addressing non-compactness, volume constraint "pull tight", and multiplicity issues.

Result: Successfully constructed volume-constrained critical points representing maximal energy configurations during fission from single atom to two separated atoms.

Conclusion: The study provides mathematical insight into nuclear fission energy barriers using geometric analysis techniques, establishing a mountain pass structure between different equilibrium states.

Abstract: We consider Gamow's liquid drop functional, $\mathcal{E}$, on $\mathbb{R}^3$
and construct non-minimizing, volume constrained, critical points for volumes
$3.512 \cong \alpha_0 < V < 10$. In this range, we establish a mountain pass
set up between a ball of volume $V$ and two balls of volume $V/2$ infinitely
far apart. Intuitively, our critical point corresponds to the maximal energy
configuration of an atom of volume $V$ as it undergoes fission into two atoms
of volume $V/2$. Our proof relies on geometric measure theoretical methods from
the min-max construction of minimal surfaces, and along the way, we address
issues of non-compactness, ``pull tight" with a volume constraint, and
multiplicity.

</details>


### [21] [On the Convergence of Solutions for the Ginzburg-Landau Equation and System](https://arxiv.org/abs/2509.09231)
*Rejeb Hadiji,Jongmin Han*

Main category: math.AP

TL;DR: The paper analyzes convergence properties of Ginzburg-Landau equation solutions. If energy is bounded above by harmonic map energy, solutions converge to harmonic map. If energy exceeds harmonic map energy, uniform convergence to 1 fails.


<details>
  <summary>Details</summary>
Motivation: To understand the convergence behavior of Ginzburg-Landau equation solutions to harmonic maps and establish energy threshold conditions for different convergence patterns.

Method: Mathematical analysis of Ginzburg-Landau equation solutions with boundary conditions, comparing their energy with harmonic map energy, and proving convergence results using energy estimates.

Result: Proved that if solution energy is bounded above by harmonic map energy, convergence to harmonic map occurs; if energy exceeds harmonic map energy, uniform convergence to 1 fails. Results extended to symmetric and non-symmetric two-component systems.

Conclusion: The energy threshold of the harmonic map serves as a critical value determining convergence behavior in Ginzburg-Landau systems, with implications for both single and multi-component cases.

Abstract: Let $(u_\varepsilon)$ be a family of solutions of the Ginzburg--Landau
equation with boundary condition $u_\varepsilon = g$ on $\partial \Omega$ and
of degree $0$. Let $u_0$ denote the harmonic map satisfying $u_0 = g$ on
$\partial \Omega$. We show that, if there exists a constant $C_1 > 0$ such that
for $\varepsilon$ sufficiently small we have $\frac{1}{2} \int_\Omega |\nabla
u_\ve|^2 dx \leq C_1 \leq \frac{1}{2} \int_\Omega |\nabla u_0|^2 dx,$ then $C_1
= \frac{1}{2} \int_\Omega |\nabla u_0|^2 dx$ and
  $u_\ve ~\to ~ u_0 \qin H^1(\Om)$.
  We also prove that if there is a constant $C_2$ such that for $\ve$ small
enough we have $ \frac12 \int_\Om |\nabla u_\ve|^2 dx \geq C_2 > \frac12
\int_\Om |\nabla u_0|^2 dx,$ then $|u_{\ve}|$ does not converge uniformly to
$1$ on $\overline{\Om} $. We obtain analogous results for both symmetric and
non-symmetric two-component Ginzburg--Landau systems.

</details>


### [22] [Functions of bounded Musielak-Orlicz-type deformation and anisotropic Total Generalized Variation for image-denoising problems](https://arxiv.org/abs/2509.09237)
*Giacomo Bertazzoni,Elisa Davoli,Samuele Ricco`,Elvira Zappale*

Main category: math.AP

TL;DR: This paper introduces bounded deformation fields with generalized Orlicz growth, analyzes their properties and decomposition, and develops Musielak-Orlicz anisotropic Total Generalized Variation for image reconstruction.


<details>
  <summary>Details</summary>
Motivation: To extend deformation field analysis to generalized Orlicz growth spaces and develop advanced variational methods for image reconstruction problems.

Method: Introduces bounded deformation fields with generalized Orlicz growth, establishes modular representation and decomposition, then develops Musielak-Orlicz anisotropic Total Generalized Variation with duality representation.

Result: Established main properties of bounded deformation fields, provided modular decomposition, and showed well-posedness of the image reconstruction problem using the new variational formulation.

Conclusion: The framework successfully extends deformation analysis to generalized Orlicz spaces and provides effective tools for anisotropic image reconstruction with theoretical guarantees.

Abstract: In the first part of this paper we introduce the space of bounded deformation
fields with generalized Orlicz growth. We establish their main properties,
provide a modular representation, and characterize a decomposition of the
modular into an absolutely continuous part and a singular part weighted via a
recession function. A further analysis in the variable exponent case is also
provided. The second part of the paper contains a notion of Musielak-Orlicz
anisotropic Total Generalized Variation. We establish a duality representation,
and show well-posedness of the corresponding image reconstruction problem.

</details>


### [23] [Numerical analysis of the homogeneous Landau equation: approximation, error estimates and simulation](https://arxiv.org/abs/2509.09276)
*Francis Filbet,Yanzhi Gui,Ling-Bing He*

Main category: math.AP

TL;DR: Numerical solution for Landau equation with Coulomb potential using spectral method, with explicit error estimates showing convergence to exact solution.


<details>
  <summary>Details</summary>
Motivation: To provide rigorous mathematical validation for spectral methods in solving the spatially homogeneous Landau equation with Coulomb potential, ensuring numerical solutions can achieve arbitrary precision.

Method: Construct numerical solution on domain D_L with N Fourier modes, derive explicit error estimates in terms of L (domain size) and N (number of modes), and perform numerical simulations to validate the approach.

Result: Proved that for any error tolerance and fixed time interval, specific choices of D_L and N exist to keep error below tolerance. Numerical simulations confirm the theoretical error estimates and validate the spectral method.

Conclusion: The spectral method is mathematically valid for solving Landau equation with Coulomb potential, with explicit conditions provided for achieving desired accuracy through appropriate domain size and mode selection.

Abstract: We construct a numerical solution to the spatially homogeneous Landau
equation with Coulomb potential on a domain $D_L$ with N retained Fourier
modes. By deriving an explicit error estimate in terms of $L$ and $N$, we
demonstrate that for any prescribed error tolerance and fixed time interval
$[0, T ]$, there exist choices of $D_L$ and $N$ satisfying explicit conditions
such that the error between the numerical and exact solutions is below the
tolerance. Specifically, the estimate shows that sufficiently large $L$ and $N$
(depending on initial data parameters and $T$) can reduce the error to any
desired level. Numerical simulations based on this construction are also
presented. The results in particular demonstrate the mathematical validity of
the spectral method proposed in the referenced literature.

</details>


### [24] [Well-posedness of stationary 2D and 3D convective Brinkman-Forchheimer extended Darcy Hemivariational inequalities](https://arxiv.org/abs/2509.09335)
*Manil T. Mohan*

Main category: math.AP

TL;DR: Analysis of well-posedness for convective Brinkman-Forchheimer extended Darcy model hemivariational inequality in 2D/3D, with existence/uniqueness proofs and numerical scheme foundations.


<details>
  <summary>Details</summary>
Motivation: To establish mathematical foundations for the CBFeD model describing incompressible viscous fluid flow through porous media with convection, damping, and nonlinear resistance effects under steady-state conditions.

Method: Introduces auxiliary hemivariational inequality resembling nonlinear Stokes-type problem; integrates convex minimization theory with fixed-point methods (Banach contraction or Schauder's theorem); develops iterative algorithm solving Stokes-type problems sequentially.

Result: Establishes existence and uniqueness of weak solutions for CBFeD model; provides practical iterative algorithm with guaranteed convergence; derives equivalent variational formulations for numerical schemes.

Conclusion: Successfully proves well-posedness of CBFeD hemivariational inequality and provides mathematical framework for developing efficient numerical simulation schemes for porous media flows.

Abstract: This study addresses the well-posedness of a hemivariational inequality
derived from the convective Brinkman-Forchheimer extended Darcy (CBFeD) model
in both two and three dimensions. The CBFeD model describes the behavior of
incompressible viscous fluid flow through a porous medium, incorporating the
effects of convection, damping, and nonlinear resistance. The mathematical
framework captures steady-state flow conditions under a no-slip boundary
assumption, with a non-monotone boundary condition that links the total fluid
pressure and the velocity's normal component through a Clarke subdifferential
formulation. To facilitate the analysis, we introduce an auxiliary
hemivariational inequality resembling a nonlinear Stokes-type problem with
damping and pumping terms, which serves as a foundational tool in establishing
the existence and uniqueness of weak solutions for the CBFeD model. The
analytical strategy integrates techniques from convex minimization theory with
fixed-point methods, specifically employing either the Banach contraction
mapping principle or Schauder's fixed point theorem. The Banach-based approach,
in particular, leads to a practical iterative algorithm that solves the
original nonlinear hemivariational inequality by sequentially solving
Stokes-type problems, ensuring convergence of the solution sequence.
Additionally, we derive equivalent variational formulations in terms of
minimization problems. These formulations lay the groundwork for the design of
efficient and stable numerical schemes tailored to simulate flows governed by
the CBFeD model.

</details>


### [25] [New Homogeneous Solutions for the One-Phase Free Boundary Problem](https://arxiv.org/abs/2509.09409)
*Coleman Hines,James Kolesar,Peter McGrath*

Main category: math.AP

TL;DR: Constructed domains in 2-sphere with k boundary components that serve as links of cones in R^3 admitting homogeneous solutions to one-phase free boundary problem, answering Jerison-Kamburov's question and disproving Souam's conjecture.


<details>
  <summary>Details</summary>
Motivation: To answer Jerison-Kamburov's question about existence of domains with multiple boundary components admitting homogeneous solutions, and to test Souam's conjecture about such solutions.

Method: Exploited new connection with minimal surfaces to construct the domains and solutions.

Result: Successfully constructed domains with k boundary components for sufficiently large k, and also constructed infinite family of homogeneous solutions in dimension four.

Conclusion: The work provides counterexamples to Souam's conjecture and answers Jerison-Kamburov's question, demonstrating the power of connecting free boundary problems with minimal surface theory.

Abstract: For each sufficiently large integer $k$, we construct a domain in the round
$2$-sphere with $k$ boundary components which is the link of a cone in
$\mathbb{R}^3$ admitting a homogeneous solution to the one-phase free boundary
problem. This answers a question of Jerison-Kamburov, and also disproves a
conjecture of Souam left open in earlier work. The method exploits a new
connection with minimal surfaces, which we also use to construct an infinite
family of homogeneous solutions in dimension four.

</details>


### [26] [Optimal convergence rates in multiscale elliptic homogenization](https://arxiv.org/abs/2509.09410)
*Weisheng Niu,Yao Xu,Jinping Zhuge*

Main category: math.AP

TL;DR: Improved convergence rates for multiscale elliptic operators with analytic coefficients using novel multiscale correctors and effective operators


<details>
  <summary>Details</summary>
Motivation: Classical reiterated homogenization methods for multiscale elliptic operators have convergence rates limited by scale separation ratios, which this paper aims to improve

Method: Introduce multiscale correctors and more accurate effective operators under the assumption of real analytic coefficients in the multiscale elliptic operator

Result: Achieved improved convergence rate from max{ε_{i+1}/ε_i} to max{e^{-cε_i/ε_{i+1}}}, which is optimal, and established uniform Lipschitz estimates under mild scale-separation conditions

Conclusion: The proposed method with multiscale correctors provides optimal convergence rates for multiscale homogenization problems with analytic coefficients, significantly improving upon classical approaches

Abstract: This paper is devoted to the quantitative homogenization of multiscale
elliptic operator $-\nabla\cdot A_\varepsilon \nabla$, where $A_\varepsilon(x)
= A(x/\varepsilon_1, x/\varepsilon_2,\cdots, x/\varepsilon_n)$, $\varepsilon =
(\varepsilon_1, \varepsilon_2,\cdots, \varepsilon_n) \in (0,1]^n$ and
$\varepsilon_i > \varepsilon_{i+1}$. We assume that $A(y_1,y_2,\cdots, y_n)$ is
1-periodic in each $y_i \in \mathbb{R}^d$ and real analytic. Classically, the
method of reiterated homogenization has been applied to study this multiscale
elliptic operator, which leads to a convergence rate limited by the ratios
$\max \{ \varepsilon_{i+1}/\varepsilon_i: 1\le i\le n-1\}$. In the present
paper, under the assumption of real analytic coefficients, we introduce the
so-called multiscale correctors and more accurate effective operators, and
improve the ratio part of the convergence rate to $\max \{
e^{-c\varepsilon_{i}/\varepsilon_{i+1}}: 1\le i\le n-1 \}$. This convergence
rate is optimal in the sense that $c>0$ cannot be replaced by a larger
constant. As a byproduct, the uniform Lipschitz estimate is established under a
mild double-log scale-separation condition.

</details>


### [27] [Microlocal analysis of the non-relativistic limit of the Klein--Gordon equation: Estimates](https://arxiv.org/abs/2509.09518)
*Andrew Hassell,Qiuye Jia,Ethan Sussman,Andras Vasy*

Main category: math.AP

TL;DR: Robust microlocal framework for non-relativistic limit of relativistic wave equations with time-dependent coefficients, focusing on Klein-Gordon equation.


<details>
  <summary>Details</summary>
Motivation: To analyze the non-relativistic limit of relativistic wave equations and provide global estimates uniform as speed of light approaches infinity.

Method: Uses three new pseudodifferential calculi: Ψₙ (semiclassical scattering variant), Ψₙres, and Ψₙ2res, with second microlocalization techniques to handle two asymptotic regimes in phase space.

Result: Developed framework that combines analyses of two regimes (natural units and low-frequency) to obtain uniform global estimates.

Conclusion: Provides technical foundation for analyzing non-relativistic limits, with applications detailed in companion paper; both papers can be read independently.

Abstract: This is the more technical half of a two-part work in which we introduce a
robust microlocal framework for analyzing the non-relativistic limit of
relativistic wave equations with time-dependent coefficients, focusing on the
Klein--Gordon equation. Two asymptotic regimes in phase space are relevant to
the non-relativistic limit: one corresponding to what physicists call
``natural'' units, in which the PDE is approximable by the free Klein--Gordon
equation, and a low-frequency regime in which the equation is approximable by
the usual Schrodinger equation. Combining the analyses in the two regimes gives
global estimates which are uniform as the speed of light goes to infinity. The
companion paper gives applications. Our main technical tools are three new
pseudodifferential calculi, $\Psi_{\natural}$ (a variant of the semiclassical
scattering calculus), $\Psi_{\natural\mathrm{res}}$, and
$\Psi_{\natural2\mathrm{res}}$, the latter two of which are created by ``second
microlocalizing'' the first at certain locations. This paper and the companion
paper can be read in either order, since the latter treats the former as a
black box.

</details>


### [28] [Sharp bilinear eigenfunction estimate, $L^\infty_{x_2}L^p_{t,x_1}$-type Strichartz estimate, and energy-critical NLS](https://arxiv.org/abs/2509.09565)
*Yangkendi Deng,Yunfeng Zhang,Zehua Zhao*

Main category: math.AP

TL;DR: Eliminates logarithmic loss in bilinear/multilinear eigenfunction estimates on S^3 using SU(2) representation theory, proves refined Strichartz estimates, and achieves small data global well-posedness for energy-critical NLS.


<details>
  <summary>Details</summary>
Motivation: To complete the theory of multilinear eigenfunction estimates on standard spheres by removing the persistent logarithmic loss that has existed for over 20 years since Burq, Gérard, and Tzvetkov's work, with applications to the energy-critical nonlinear Schrödinger equation on S^3.

Method: Views S^3 as the compact Lie group SU(2) and exploits its representation theory, particularly properties of Clebsch-Gordan coefficients. Also proves refined mixed-norm Strichartz estimates on cylindrical space adapted to spectrally localized functions.

Result: Establishes sharp bilinear and multilinear eigenfunction estimates without logarithmic loss, completes the theory on standard spheres, derives refined bilinear Strichartz estimates, and obtains small data global well-posedness for energy-critical NLS in the energy space.

Conclusion: The approach successfully eliminates the long-standing logarithmic loss in eigenfunction estimates on S^3 using group representation theory, leading to complete multilinear theory and applications to nonlinear PDE well-posedness.

Abstract: We establish sharp bilinear and multilinear eigenfunction estimates for the
Laplace-Beltrami operator on the standard three-sphere $\mathbb{S}^3$,
eliminating the logarithmic loss that has persisted in the literature since the
pioneering work of Burq, G\'erard, and Tzvetkov over twenty years ago. This
completes the theory of multilinear eigenfunction estimates on the standard
spheres. Our approach relies on viewing $\mathbb{S}^3$ as the compact Lie group
$\mathrm{SU}(2)$ and exploiting its representation theory, especially the
properties of Clebsch-Gordan coefficients. Motivated by application to the
energy-critical nonlinear Schr\"odinger equation (NLS) on $\mathbb{R} \times
\mathbb{S}^3$, we also prove a refined Strichartz estimate of mixed-norm type
$L^\infty_{x_2}L^4_{t,x_1}$ on the cylindrical space $\mathbb{R}_{x_1} \times
\mathbb{T}_{x_2}$, adapted to certain spectrally localized functions. Combining
these two ingredients, we derive a refined bilinear Strichartz estimate on
$\mathbb{R} \times \mathbb{S}^3$, which in turn yields small data global
well-posedness for the above mentioned NLS in the energy space.

</details>


### [29] [Stability and asymptotic behaviour of one-dimensional solutions in cylinders](https://arxiv.org/abs/2509.09648)
*Francesca De Marchis,Lisa Mazzuoli,Filomena Pacella*

Main category: math.AP

TL;DR: Analysis of stability/instability properties of positive one-dimensional solutions to Lane-Emden Dirichlet problems in cylinders as energy varies with domain perturbations, focusing on asymptotic behavior near p=1 and large p.


<details>
  <summary>Details</summary>
Motivation: To understand how the stability properties of one-dimensional solutions to Lane-Emden relative Dirichlet problems depend on the nonlinearity exponent p, particularly for extreme values close to 1 and approaching infinity.

Method: Careful asymptotic analysis of one-dimensional solutions as p approaches 1 and infinity, examining limit profiles and qualitative properties through mathematical analysis of the Lane-Emden equation.

Result: The study reveals how stability/instability properties vary with the exponent p, with specific results obtained for p close to 1 and for large p values through asymptotic analysis.

Conclusion: The asymptotic analysis provides insights into the limit behavior and qualitative properties of solutions, enabling detection of stability transitions as the nonlinearity exponent p varies in extreme ranges.

Abstract: We consider positive one-dimensional solutions of a Lane-Emden relative
Dirichlet problem in a cylinder and study their stability/instability
properties as the energy varies with respect to domain perturbations. This
depends on the exponent $p >1$ of the nonlinearity and we obtain results for
$p$ close to 1 and for $p$ large. This is achieved by a careful asymptotic
analysis of the one-dimensional solution as $p \to 1$ or $p \to \infty$, which
is of independent interest. It allows to detect the limit profile and other
qualitative properties of these solutions.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [30] [HARD: A Performance Portable Radiation Hydrodynamics Code based on FleCSI Framework](https://arxiv.org/abs/2509.08971)
*Julien Loiseau,Hyun Lim,Andrés Yagüe López,Mammadbaghir Baghirzade,Shihab Shahriar Khan,Yoonsoo Kim,Sudarshan Neopane,Alexander Strack,Farhana Taiyebah,Benjamin K. Bergen*

Main category: physics.comp-ph

TL;DR: HARD is an open-source radiation hydrodynamics simulation tool built on FleCSI framework with multiple runtime backends (Legion, MPI, HPX) and Kokkos for node-level parallelism, featuring verification tests and cross-platform performance.


<details>
  <summary>Details</summary>
Motivation: To create a sustainable, high-performance simulation platform for radiation hydrodynamics research that works efficiently across different computing environments from laptops to supercomputers.

Method: Built on FleCSI framework with task-based computational units, multiple runtime orchestrators (Legion, MPI, HPX), and Kokkos for portable node-level parallelism. Includes regression-test suite for verification against analytical solutions.

Result: A single portable code base that runs efficiently on various computing platforms with automated verification through canonical test problems (Sod shock tube, LeBlanc shock tube, Sedov blast wave).

Conclusion: HARD provides a sustainable platform for radiation hydrodynamics research with performance portability, verification infrastructure, and community-focused development under an OSI-approved license.

Abstract: Hydrodynamics And Radiation Diffusion} (HARD) is an open-source application
for high-performance simulations of compressible hydrodynamics with
radiation-diffusion coupling. Built on the FleCSI (Flexible Computational
Science Infrastructure) framework, HARD expresses its computational units as
tasks whose execution can be orchestrated by multiple back-end runtimes,
including Legion, MPI, and HPX. Node-level parallelism is delegated to Kokkos,
providing a single, portable code base that runs efficiently on laptops, small
homogeneous clusters, and the largest heterogeneous supercomputers currently
available. To ensure scientific reliability, HARD includes a regression-test
suite that automatically reproduces canonical verification problems such as the
Sod and LeBlanc shock tubes and the Sedov blast wave, comparing numerical
solutions against known analytical results. The project is distributed under an
OSI-approved license, hosted on GitHub, and accompanied by reproducible build
scripts and continuous integration workflows. This combination of performance
portability, verification infrastructure, and community-focused development
makes HARD a sustainable platform for advancing radiation hydrodynamics
research across multiple domains.

</details>


### [31] [An Improved Rapid Performance Analysis Model for Solenoidal Magnetic Radiation Shields](https://arxiv.org/abs/2509.09051)
*Joseph L. Hesse-Withbroe,Katya S. Arquilla*

Main category: physics.comp-ph

TL;DR: Updated semi-analytical model for evaluating solenoidal magnetic radiation shields shows improved performance for weaker shields but diminished performance for strong shields (>20 T-m), indicating mass savings over passive shielding for missions requiring >60% dose reduction.


<details>
  <summary>Details</summary>
Motivation: Deep-space radiation exposure poses significant health risks to astronauts, and active magnetic shielding offers potential mass savings over passive methods, but existing evaluation models have accuracy limitations.

Method: Updated and validated a semi-analytical model based on HZETRN code, correcting simplifying assumptions from the original 2014 model, then used Monte Carlo simulation for validation and recharacterized the shield design trade space.

Result: The updated model predicts improved performance for weaker shields but greatly diminished performance for strong shields with bending powers >20 T-m. Magnetic shields enable significant mass savings for missions requiring >60% dose reduction.

Conclusion: Active magnetic radiation shields are viable for long-duration exploration missions (>1 year) outside LEO where substantial dose reduction (>60%) is needed, offering mass advantages over passive shielding techniques.

Abstract: Astronauts participating in deep-space exploration missions will be exposed
to significantly greater amounts of radiation than is typically encountered on
Earth or in low Earth orbit (LEO), which poses significant risks to crew health
and mission safety. Active magnetic radiation shields based on the Lorentz
deflection of charged particles have the potential to reduce astronaut doses
with lower mass costs than passive shielding techniques. Typically, active
shielding performance is evaluated using high-fidelity Monte Carlo simulations,
which are too computationally expensive to evaluate an entire trade space of
shield designs. A rapid, semi-analytical model based on the High Charge and
Energy Transport code (HZETRN) developed in 2014 provided an alternative method
by which to evaluate the performance of solenoidal shields. However, various
simplifying assumptions made in the original model have limited its accuracy,
and therefore require evaluation and correction. In this work, a number of
aspects of the original semi-analytical model are updated and validated by
Monte Carlo simulation, then used to recharacterize the design trade space of
solenoidal magnetic shields. The updated model predicts improved performance
for weaker shields as compared to the original model, but greatly diminished
performance for strong shields with bending powers greater than 20 T-m.
Overall, the results indicate that magnetic shields enable significant mass
savings over passive shields for mission scenarios where the requisite dose
reduction is greater than about 60% relative to free space, which includes most
exploration missions longer than one year with significant time spent outside
LEO.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [32] [Exploration of novel ICP using helicon antennas with zero magnetic field](https://arxiv.org/abs/2509.09104)
*Ye Tao,Lei Chang,Dingzhou Li,Yingxin Zhao*

Main category: physics.plasm-ph

TL;DR: Loop antenna with parabolic density profile at 13.56 MHz provides most efficient ICP power coupling without external magnetic field.


<details>
  <summary>Details</summary>
Motivation: Efficient power coupling in inductively coupled plasma is crucial for both fundamental research and practical applications like material processing and space propulsion.

Method: Investigated effects of antenna geometry (loop, half-helix, Boswell, Nagoya III), driving frequency (13.56-54.24 MHz), and radial density profile (Gaussian vs parabolic) on power coupling efficiency in ICP without external magnetic field.

Result: Loop antenna showed highest power deposition efficiency; driving frequency had negligible effects; parabolic density profile resulted in more efficient power coupling than Gaussian profile, especially radially.

Conclusion: For efficient ICP without external magnetic field, use loop antenna with parabolic density profile at industrial frequency 13.56 MHz.

Abstract: Inductively coupled plasma (ICP) attracts great attention from aspects of
fundamental research and practical applications, and efficient power coupling
is highly desirable for both of them. The present study explores a novel
strategy for efficient ICP through using helicon antennas with zero external
magnetic field. Specific research is devoted to the effects of antenna geometry
(loop, half-helix, Boswell, Nagoya III), driving frequency (13.56-54.24 MHz)
and radial density profile (Gaussian and parabolic) on power coupling. Findings
reveal that: loop antenna yields higher power deposition efficiency than
half-helix, Boswell, and Nagoya III antennas, driving frequency gives
negligible effects, and parabolic density profile results in more efficient
power coupling than Gaussian density profile especially in the radial
direction, for the conditions employed here. Therefore, it is suggested that
for this novel ICP strategy one should use loop antenna with parabolic density
profile, and the industrial frequency of 13.56 MHz can work well. This study
provides a valuable reference for the novel design of efficient ICP sources,
which could be used for material processing and space propulsion, etc. Key
words: Inductively coupled plasma; Antenna Geometry; Power Deposition; Driving
Frequency

</details>


### [33] [Exploration on the Two-stream Instability in the Polar Cusp Under Solar Storm Disturbances and its Potential Impacts on Spacecraft](https://arxiv.org/abs/2509.09126)
*Jikai Sun,Lei Chang,Yu Liu,Guojun Wang,Zichen Kan,Shijie Zhang,Jingjing Ma,Dingzhou Li,Yingxin Zhao*

Main category: physics.plasm-ph

TL;DR: Study examines two-stream instability evolution in polar cusp during solar storms, showing how electron-cyclotron motion can suppress instability and reduce electrostatic wave amplitudes, with implications for spacecraft safety.


<details>
  <summary>Details</summary>
Motivation: To understand the evolution of two-stream instability associated with electron velocities during solar storms and its interaction with electrostatic solitary waves, particularly in relation to spacecraft risks like surface charge accumulation and communication interference.

Method: Used particle-in-cell (PIC) simulations and compared results with satellite observational data and computational outcomes to analyze the two-stream instability mechanisms.

Result: Found that solar wind plasma interacting with upward-moving ionospheric plasma drives two-stream instability, forming electron hole structures and triggering bipolar ESW distribution. Enhanced electron cyclotron motion under specific magnetic field conditions can suppress instability and reduce ESW amplitude.

Conclusion: The study provides valuable insights for understanding solar storm impacts on polar cusp environment and offers guidance for monitoring electromagnetic environment and ensuring spacecraft operational stability.

Abstract: During solar storms, the polar cusp often exhibits electron populations with
distinct velocity distributions, which may be associated with the two-stream
instability. This study reveals the evolution of the two-stream instability
associated with electron velocities and the interaction between the growth
phase of the two-stream instability and the electrostatic solitary waves
(ESWs). The results from particle-in-cell (PIC) simulations are compared with
satellite observational data and computational outcomes. The potential risks
associated with two-stream instability, including surface charge accumulation
and communication system interference on spacecraft, are also explored. The
findings show that, in the high-latitude polar cusp region, the interaction
between the solar wind plasma propagating along magnetic field lines and the
upward-moving ionospheric plasma could drive two-stream instability, leading to
the formation of electron hole structures in phase space and triggering a
bipolar distribution of ESWs. When the spatial magnetic field and wave vector
meet specific conditions, the enhanced electron cyclotron motion could suppress
the formation of two-stream instability and electron hole structures, leading
to a reduction in the amplitude of the ESWs. The results offer valuable
insights for a deeper understanding of the impact of solar storms on the polar
cusp environment, as well as for monitoring electromagnetic environment and
ensuring the stable operation of spacecraft.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [34] [Efficient High-Order Participation Factor Computation via Batch-Structured Tensor Contraction](https://arxiv.org/abs/2509.08968)
*Mahsa Sajjadi,Kaiyang Huang,Kai Sun*

Main category: eess.SY

TL;DR: Efficient tensor-based method for computing high-order nonlinear participation factors using dynamic batching to overcome memory constraints in complex power systems.


<details>
  <summary>Details</summary>
Motivation: Increasing system complexity from power electronics and renewable integration requires scalable computation of nonlinear participation factors for modal analysis and control design.

Method: Tensor contraction-based approach with dynamic batching strategy that adjusts batch sizes based on available computational resources.

Result: Enables calculation of high-order nonlinear participation factors that were previously infeasible due to memory constraints.

Conclusion: Provides a scalable and memory-efficient solution for computing arbitrary-order nonlinear participation factors in complex power systems.

Abstract: Participation factors (PFs) quantify the interaction between system modes and
state variables, and they play a crucial role in various applications such as
modal analysis, model reduction, and control design. With increasing system
complexity, especially due to power electronic devices and renewable
integration, the need for scalable and high-order nonlinear PF (NPF)
computation has become more critical. This paper presents an efficient
tensor-based method for calculating NPFs up to an arbitrary order. Traditional
computation of PFs directly from normal form theory is computationally
expensive -- even for second-order PFs -- and becomes infeasible for higher
orders due to memory constraints. To address this, a tensor contraction-based
approach is introduced that enables the calculation of high-order PFs using a
batching strategy. The batch sizes are dynamically determined based on the
available computational resources, allowing scalable and memory-efficient
computation.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [35] [Complex dynamics and pattern formation in a diffusive epidemic model with an infection-dependent recovery rate](https://arxiv.org/abs/2509.09000)
*Wael El Khateeb,Chanaka Kottegoda,Chunhua Shan*

Main category: math.DS

TL;DR: A diffusive epidemic model with infection-dependent recovery rate shows diffusion-driven instability, Turing patterns, and complex spatiotemporal dynamics including disease recurrence and localized hotspots.


<details>
  <summary>Details</summary>
Motivation: To understand how infection-dependent recovery rates and population movement affect disease transmission patterns and spatial spread in epidemic models.

Method: Bifurcation analysis of reaction kinetics to identify steady states and periodic solutions, followed by analysis of diffusion-driven instability and Turing-Hopf bifurcation patterns.

Result: The model exhibits multiple constant steady states, spatially homogeneous periodic solutions, diffusion-driven instability, and complex spatiotemporal patterns including k-mode Turing instability and (k1,k2)-mode Turing-Hopf bifurcation.

Conclusion: Spatially targeted strategies are necessary to contain regionally and cyclically varying disease waves, as the model reveals asynchronous recurrence, patterned waves, and localized hotspots.

Abstract: A diffusive epidemic model with an infection-dependent recovery rate is
formulated in this paper. Multiple constant steady states and spatially
homogeneous periodic solutions are first proven by bifurcation analysis of the
reaction kinetics. It is shown that the model exhibits diffusion-driven
instability, where the infected population acts as an activator and the
susceptible population functions as an in hibitor. The faster movement of the
susceptible class will induce the spatial and spatiotemporal patterns, which
are characterized by k-mode Turing instability and (k1,k2)-mode Turing-Hopf
bifurcation. The transient dynamics from a purely temporal oscillatory regime
to a spatial periodic pattern are discovered. The model reveals key
transmission dynamics, including asynchronous disease recurrence, spatially
patterned waves, and the formation of localized hotspots. The study suggests
that spatially targeted strategies are necessary to contain disease waves that
vary regionally and cyclically.

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [36] [Suppression of pair beam instabilities in a laboratory analogue of blazar pair cascades](https://arxiv.org/abs/2509.09040)
*Charles D. Arrowsmith,Francesco Miniati,Pablo J. Bilbao,Pascal Simon,Archie F. A. Bott,Stephane Burger,Hui Chen,Filipe D. Cruz,Tristan Davenne,Anthony Dyson,Ilias Efthymiopoulos,Dustin H. Froula,Alice Goillot,Jon T. Gudmundsson,Dan Haberberger,Jack W. D. Halliday,Tom Hodge,Brian T. Huffman,Sam Iaquinta,Graham Marshall,Brian Reville,Subir Sarkar,Alexander A. Schekochihin,Luis O. Silva,Raspberry Simpson,Vasiliki Stergiou,Raoul M. G. M. Trines,Thibault Vieu,Nikolaos Charitonidis,Robert Bingham,Gianluca Gregori*

Main category: astro-ph.HE

TL;DR: Laboratory generation of dense electron-positron pair beams using CERN's Super Proton Synchrotron enables direct testing of astrophysical models, showing that pair beam instabilities are suppressed when beams are not perfectly collimated/monochromatic, supporting the need for intergalactic magnetic fields.


<details>
  <summary>Details</summary>
Motivation: To enable direct laboratory testing of theoretical models for gamma-ray bursts and active galactic nuclei, particularly to study pair beam stability and its implications for astrophysical observations of blazars.

Method: Used ultra-relativistic protons accelerated by CERN's Super Proton Synchrotron to generate dense electron-positron pair beams, then studied beam stability as it propagates through a meter-length plasma to simulate TeV gamma-ray induced pair cascades in intergalactic medium.

Result: Found that pair beam instability is suppressed when the beam is not perfectly collimated or monochromatic, indicating that the instability does not disrupt the cascade as previously argued.

Conclusion: The lower limit to intergalactic magnetic field inferred from gamma-ray observations of blazars remains robust, as the pair beam instability does not eliminate the need for moderate strength magnetic fields to explain observational data.

Abstract: The generation of dense electron-positron pair beams in the laboratory can
enable direct tests of theoretical models of $\gamma$-ray bursts and active
galactic nuclei. We have successfully achieved this using ultra-relativistic
protons accelerated by the Super Proton Synchrotron at CERN. In the first
application of this experimental platform, the stability of the pair beam is
studied as it propagates through a metre-length plasma, analogous to TeV
$\gamma$-ray induced pair cascades in the intergalactic medium. It has been
argued that pair beam instabilities disrupt the cascade, thus accounting for
the observed lack of reprocessed GeV emission from TeV blazars. If true this
would remove the need for a moderate strength intergalactic magnetic field to
explain the observations. We find that the pair beam instability is suppressed
if the beam is not perfectly collimated or monochromatic, hence the lower limit
to the intergalactic magnetic field inferred from $\gamma$-ray observations of
blazars is robust.

</details>


### [37] [Unraveling the emission mechanism powering long period radio transients from interacting white dwarf binaries via kinetic plasma simulations](https://arxiv.org/abs/2509.09057)
*Yici Zhong,Elias R. Most*

Main category: astro-ph.HE

TL;DR: The paper demonstrates that the relativistic electron cyclotron maser instability (ECMI) can produce coherent radio emission in white dwarf-M dwarf binary systems, explaining recent observations of long-period radio transients.


<details>
  <summary>Details</summary>
Motivation: Recent discoveries of long-period radio transients from white dwarf-M dwarf binaries require explanation of how coherent radio emission is produced in these systems, similar to planetary radio emission mechanisms.

Method: Using kinetic plasma simulations, the authors quantified the relativistic ECMI in the nonlinear regime under conditions relevant for white dwarf radio emission for the first time.

Result: Simulations show ECMI can intrinsically produce partially linearly polarized emission that explains the observed emission spectrum of galactic sources like GLEAM-X J0704-37 and ILTJ1101+5521.

Conclusion: This work establishes ECMI as a viable mechanism for radio emission in white dwarf binary systems and enables systematic nonlinear computational modeling of radio emission from interacting white dwarf sources.

Abstract: Recent observations of long period radio transients, such as GLEAM-X J0704-37
and ILTJ1101 + 5521, have revealed a previously unrecognized population of
galactic radio transient sources associated with white dwarf - M dwarf
binaries. It is an open question how to produce coherent radio emission in
these systems, though a model driven by binary interaction seems likely given
the nature and correlation of the emission with the binaries' orbital period.
Using kinetic plasma simulations, we demonstrate that the relativistic electron
cyclotron maser instability (ECMI) is a viable mechanism for generating radio
pulses in white dwarf - M dwarf systems, akin to planetary radio emission, such
as that from the Jupiter-Io system. We quantify the relativistic ECMI in the
nonlinear regime under conditions relevant for white dwarf radio emission for
the first time. Our simulations demonstrate that the ECMI can intrinsically
produce partially linearly polarized emission relevant to explaining the
observed emission spectrum of the two galactic sources, though the precise
details will depend on the plasma composition. Our work paves the way for a
systematic and fully nonlinear computational modeling of radio emission from
interacting white dwarf sources.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [38] [An Interval Type-2 Version of Bayes Theorem Derived from Interval Probability Range Estimates Provided by Subject Matter Experts](https://arxiv.org/abs/2509.08834)
*John T. Rickard,William A. Dembski,James Rickards*

Main category: cs.AI

TL;DR: Extends Bayes Theorem to interval type-2 fuzzy logic to handle uncertain input probabilities from experts, developing a conservative method to avoid inconsistencies and a flexible algorithm for encoding interval estimates.


<details>
  <summary>Details</summary>
Motivation: Traditional Bayesian inference assumes precise input values, but real-world applications often rely on interval range estimates from subject matter experts, making precise inputs unrealistic.

Method: Develops an IT2 version of Bayes Theorem with a conservative method to avoid input inconsistencies, and creates a novel algorithm for encoding SME-provided intervals into IT2 fuzzy membership functions.

Result: Provides a framework that can handle uncertain interval inputs in Bayesian analysis while maintaining validity of output results.

Conclusion: The proposed IT2 extension of Bayes Theorem enables more realistic Bayesian inference using uncertain expert estimates while preventing invalid outputs through conservative consistency maintenance.

Abstract: Bayesian inference is widely used in many different fields to test hypotheses
against observations. In most such applications, an assumption is made of
precise input values to produce a precise output value. However, this is
unrealistic for real-world applications. Often the best available information
from subject matter experts (SMEs) in a given field is interval range estimates
of the input probabilities involved in Bayes Theorem. This paper provides two
key contributions to extend Bayes Theorem to an interval type-2 (IT2) version.
First, we develop an IT2 version of Bayes Theorem that uses a novel and
conservative method to avoid potential inconsistencies in the input IT2 MFs
that otherwise might produce invalid output results. We then describe a novel
and flexible algorithm for encoding SME-provided intervals into IT2 fuzzy
membership functions (MFs), which we can use to specify the input probabilities
in Bayes Theorem. Our algorithm generalizes and extends previous work on this
problem that primarily addressed the encoding of intervals into word MFs for
Computing with Words applications.

</details>


<div id='physics.bio-ph'></div>

# physics.bio-ph [[Back]](#toc)

### [39] [Simulating Organogenesis in COMSOL Multiphysics: Tissue Patterning with Directed Cell Migration](https://arxiv.org/abs/2509.08930)
*Malte Mederacke,Chengyou Yu,Roman Vetter,Dagmar Iber*

Main category: physics.bio-ph

TL;DR: COMSOL Multiphysics implementation of a continuum model for directed cell migration using partial integro-differential equations to study tissue self-organization and pattern formation.


<details>
  <summary>Details</summary>
Motivation: To provide a flexible and accessible computational framework for studying directed cell migration, which is crucial for understanding tissue self-organization and morphogenesis processes.

Method: Developed a continuum model formulated as a partial integro-differential equation (PIDE) combining random motility with non-local, density-dependent guidance cues. Implemented in COMSOL Multiphysics with support for 1D, 2D, and 3D simulations, various boundary conditions, and Lagrangian formulation for tissue growth.

Result: Successfully implemented a generalizable platform that can capture phenomena like cell sorting and aggregation, demonstrating COMSOL's capability for flexible PIDE implementation.

Conclusion: COMSOL Multiphysics provides an effective and accessible environment for implementing complex PIDEs, offering a powerful tool for studying collective cell behavior and pattern formation in biological systems.

Abstract: We present a COMSOL Multiphysics implementation of a continuum model for
directed cell migration, a key mechanism underlying tissue self-organization
and morphogenesis. The model is formulated as a partial integro-differential
equation (PIDE), combining random motility with non-local, density-dependent
guidance cues to capture phenomena such as cell sorting and aggregation. Our
framework supports simulations in one, two, and three dimensions, with both
zero-flux and periodic boundary conditions, and can be reformulated in a
Lagrangian setting to efficiently handle tissue growth and domain deformation.
We demonstrate that COMSOL Multiphysics enables a flexible and accessible
implementation of PIDEs, providing a generalizable platform for studying
collective cell behavior and pattern formation in complex biological contexts.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [40] [Neural Transformer Backflow for Solving Momentum-Resolved Ground States of Strongly Correlated Materials](https://arxiv.org/abs/2509.09275)
*Lixing Zhang,Di luo*

Main category: cond-mat.str-el

TL;DR: Neural Transformer Backflow (NTB) enables accurate simulation of strongly correlated materials like twisted MoTe2, capturing diverse quantum phases including charge density waves and fractional Chern insulators.


<details>
  <summary>Details</summary>
Motivation: Strongly correlated materials host exotic quantum phases but are notoriously difficult to solve due to strong interactions, requiring new computational approaches.

Method: A neural network ansatz called Neural Transformer Backflow (NTB) formulated within a multi-band projection framework that enforces momentum conservation and enables efficient momentum-resolved ground state calculations.

Result: NTB achieves high accuracy on small systems and scales to larger systems beyond exact diagonalization capabilities, successfully capturing charge density waves, fractional Chern insulators, and anomalous Hall Fermi liquids in twisted MoTe2.

Conclusion: The NTB approach provides a unified framework for understanding and discovering novel phases of matter in strongly correlated materials, paving the way for future research in this field.

Abstract: Strongly correlated materials, such as twisted transition-metal
dichalcogenide homobilayers, host a variety of exotic quantum phases but remain
notoriously difficult to solve due to strong interactions. We introduce a
powerful neural network ansatz, Neural Transformer Backflow (NTB), formulated
within a multi-band projection framework. It naturally enforces momentum
conservation and enables efficient calculations of momentum-resolved ground
states. NTB attains high accuracy on small systems and scales to higher bands
and larger system sizes far beyond the reach of exact diagonalization. By
evaluating observables such as the structure factor and momentum distribution,
we show that NTB captures diverse correlated states in tMoTe$_2$, including
charge density waves, fractional Chern insulators, and anomalous Hall Fermi
liquids, within a unified framework. Our approach paves the way for
understanding and discovering novel phases of matter in strongly correlated
materials.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [41] [Virtual staining for 3D X-ray histology of bone implants](https://arxiv.org/abs/2509.09235)
*Sarah C. Irvine,Christian Lucas,Diana Krüger,Bianca Guedert,Julian Moosmann,Berit Zeller-Plumhoff*

Main category: eess.IV

TL;DR: This paper introduces virtual staining for 3D X-ray histology using deep learning to convert greyscale micro-CT scans into artificially stained histological images, enabling volumetric tissue analysis without physical sectioning or chemical staining.


<details>
  <summary>Details</summary>
Motivation: 3D X-ray histology provides non-invasive volumetric imaging but lacks biochemical specificity compared to traditional histological stains. The researchers aim to extend virtual staining techniques from optical to X-ray domain to enhance interpretability of micro-CT data.

Method: Used over 50 co-registered micro-CT and toluidine blue-stained histology image pairs to train a modified CycleGAN network with pixelwise supervision and greyscale consistency terms. Applied on-the-fly data augmentation for patch-based training and downsampled histology images to match CT voxel size.

Result: The modified CycleGAN outperformed Pix2Pix and standard CycleGAN baselines across SSIM, PSNR, and LPIPS metrics. The model produces histologically realistic color outputs while preserving high-resolution structural detail and can generate virtually stained 3D datasets from full CT volumes.

Conclusion: This work successfully introduces virtual staining to 3D X-ray imaging, providing a scalable route for chemically informative, label-free tissue characterization. While features like new bone formation were reproduced, some variability in implant degradation depiction indicates need for more training data and refinement.

Abstract: Three-dimensional X-ray histology techniques offer a non-invasive alternative
to conventional 2D histology, enabling volumetric imaging of biological tissues
without the need for physical sectioning or chemical staining. However, the
inherent greyscale image contrast of X-ray tomography limits its biochemical
specificity compared to traditional histological stains. Within digital
pathology, deep learning-based virtual staining has demonstrated utility in
simulating stained appearances from label-free optical images. In this study,
we extend virtual staining to the X-ray domain by applying cross-modality image
translation to generate artificially stained slices from
synchrotron-radiation-based micro-CT scans. Using over 50 co-registered image
pairs of micro-CT and toluidine blue-stained histology from bone-implant
samples, we trained a modified CycleGAN network tailored for limited paired
data. Whole slide histology images were downsampled to match the voxel size of
the CT data, with on-the-fly data augmentation for patch-based training. The
model incorporates pixelwise supervision and greyscale consistency terms,
producing histologically realistic colour outputs while preserving
high-resolution structural detail. Our method outperformed Pix2Pix and standard
CycleGAN baselines across SSIM, PSNR, and LPIPS metrics. Once trained, the
model can be applied to full CT volumes to generate virtually stained 3D
datasets, enhancing interpretability without additional sample preparation.
While features such as new bone formation were able to be reproduced, some
variability in the depiction of implant degradation layers highlights the need
for further training data and refinement. This work introduces virtual staining
to 3D X-ray imaging and offers a scalable route for chemically informative,
label-free tissue characterisation in biomedical research.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [42] [The role of communication delays in the optimal control of spatially invariant systems](https://arxiv.org/abs/2509.09269)
*Luca Ballotta,Juncal Arbelaiz,Vijay Gupta,Luca Schenato,Mihailo R. Jovanović*

Main category: math.OC

TL;DR: Analysis of optimal proportional feedback controllers for spatially invariant systems with delayed state measurements, showing how delays impact spatial locality and control effectiveness.


<details>
  <summary>Details</summary>
Motivation: To understand how time delays in state measurements affect the spatial locality and performance of optimal feedback controllers in spatially invariant systems.

Method: Leveraging problem decoupling in spatial frequency domain, analyzing expensive control and small delay regimes, providing exact expressions for optimal controllers in limiting cases.

Result: Delays reduce control effectiveness and may require increased communication range in distributed implementations; optimal controller decomposes into delay-aware filtering and delay-free components.

Conclusion: Time delays significantly impact spatial control strategies, necessitating delay-aware filtering and potentially broader communication networks for optimal performance in spatially distributed systems.

Abstract: We study optimal proportional feedback controllers for spatially invariant
systems when the controller has access to delayed state measurements received
from different spatial locations. We analyze how delays affect the spatial
locality of the optimal feedback gain leveraging the problem decoupling in the
spatial frequency domain. For the cases of expensive control and small delay,
we provide exact expressions of the optimal controllers in the limit for
infinite control weight and vanishing delay, respectively. In the expensive
control regime, the optimal feedback control law decomposes into a delay-aware
filtering of the delayed state and the optimal controller in the delay-free
setting. Under small delays, the optimal controller is a perturbation of the
delay-free one which depends linearly on the delay. We illustrate our
analytical findings with a reaction-diffusion process over the real line and a
multi-agent system coupled through circulant matrices, showing that delays
reduce the effectiveness of optimal feedback control and may require each
subsystem within a distributed implementation to communicate with farther-away
locations.

</details>


### [43] [A preconditioned third-order implicit-explicit algorithm with a difference of varying convex functions and extrapolation](https://arxiv.org/abs/2509.09391)
*Kelin Wu,Hongpeng Sun*

Main category: math.OC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper proposes a novel preconditioned implicit-explicit algorithm
enhanced with the extrapolation technique for non-convex optimization problems.
The algorithm employs a third-order Adams-Bashforth scheme for the nonlinear
and explicit parts and a third-order backward differentiation formula for the
implicit part of the gradient flow in variational functions. The proposed
algorithm, akin to a generalized difference-of-convex (DC) approach, employs a
changing set of convex functions in each iteration. Under the Kurdyka-\L
ojasiewicz (KL) properties, the global convergence of the algorithm is
guaranteed, ensuring that it converges within a finite number of preconditioned
iterations. Our numerical experiments, including least squares problems with
SCAD regularization and the graphical Ginzburg-Landau model, demonstrate the
proposed algorithm's highly efficient performance compared to conventional DC
algorithms.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [44] [Electronic order induced symmetry breaking in lattice dynamics of Co$_3$Sn$_2$S$_2$](https://arxiv.org/abs/2509.09253)
*Shuai Zhang,Mengqi Wang,Tiantian Zhang*

Main category: cond-mat.mtrl-sci

TL;DR: Ab initio algorithm using molecular Berry curvature framework to quantify magnetic order effects on lattice dynamics in ferromagnetic Weyl semimetals like Co3Sn2S2, showing spin-orbit coupling is essential for phonon symmetry breaking.


<details>
  <summary>Details</summary>
Motivation: To understand and quantify how magnetic order affects lattice dynamics in magnetic materials, particularly focusing on the role of spin-orbit coupling and symmetry breaking in phonon behavior.

Method: Developed an ab initio algorithm based on molecular Berry curvature framework, applied to ferromagnetic Weyl semimetal Co3Sn2S2, analyzing effects of spin-orbit coupling and symmetry breaking on phonon dynamics.

Result: Electronic-order-driven phonon symmetry breaking requires SOC and creates MBC term breaking time-reversal and mirror symmetries. Mirror-symmetry breaking is essential for observed phonon splitting. MBC is widely distributed across Brillouin zone with significant off-Γ effects. Results match experiments well.

Conclusion: Establishes framework for predicting large phonon magnetism in magnetic materials with strong SOC and electron-phonon coupling, suggesting new avenues for controlling non-reciprocal phonon transport.

Abstract: Based on the molecular Berry curvature (MBC) framework, we develop an
\textit{ab initio} algorithm to capture the quantitative effects of magnetic
order on lattice dynamics. Using the ferromagnetic Weyl semimetal
Co$_3$Sn$_2$S$_2$ as a prototype, we show that electronic-order-driven phonon
symmetry breaking requires spin-orbit coupling (SOC) and leads to an MBC term
that breaks both time-reversal ($\mathcal{T}$) and mirror symmetries. We
demonstrate that mirror-symmetry breaking is essential to account for the
experimentally observed phonon splitting, $\mathcal{T}$-breaking alone is
insufficient. The MBC is widely distributed across the Brillouin zone, giving
rise to significant off-$\Gamma$ effects. Our results agree well with
experiments and establish a framework for predicting large phonon magnetism in
magnetic materials with strong spin-orbit coupling and electron-phonon
coupling. This work also suggests new avenues for controlling non-reciprocal
phonon transport.

</details>


### [45] [Exploring the magnetic landscape of easily-exfoliable two-dimensional materials](https://arxiv.org/abs/2509.09531)
*Fatemeh Haddadi,Davide Campi,Flaviano dos Santos,Nicolas Mounet,Louis Ponet,Nicola Marzari,Marco Gibertini*

Main category: cond-mat.mtrl-sci

TL;DR: Automated workflow applied to explore energy landscapes of 194 magnetic monolayers from Materials Cloud database, identifying ground-state magnetic orders and discovering novel ferromagnetic half-metals for spintronics.


<details>
  <summary>Details</summary>
Motivation: Magnetic materials have complex energy landscapes with multiple local minima, making global minimum identification challenging with heuristic methods that may not guarantee success.

Method: Applied automated workflow to systematically explore energy landscapes, enabling effective control and sampling of orbital occupation matrices for rapid identification of local minima. Used Hubbard-corrected energy functionals with first-principles computed U parameters from linear-response theory.

Result: Found diverse collinear metastable states: 109 ferromagnetic, 83 antiferromagnetic, and 2 altermagnetic monolayers. Identified 12 novel ferromagnetic half-metals with spintronics potential.

Conclusion: The automated workflow successfully determined ground-state magnetic orders and revealed promising magnetic monolayer candidates, particularly novel ferromagnetic half-metals suitable for spintronics applications.

Abstract: Magnetic materials often exhibit complex energy landscapes with multiple
local minima, each corresponding to a self-consistent electronic structure
solution. Finding the global minimum is challenging, and heuristic methods are
not always guaranteed to succeed. Here, we apply a recently developed automated
workflow to systematically explore the energy landscape of 194 magnetic
monolayers obtained from the Materials Cloud 2D crystals database and determine
their ground-state magnetic order. Our approach enables effective control and
sampling of orbital occupation matrices, allowing rapid identification of local
minima. We find a diverse set of self-consistent collinear metastable states,
further enriched by Hubbard-corrected energy functionals, when the $U$
parameters have been computed from first principles using linear-response
theory. We categorise the monolayers by their magnetic ordering and highlight
promising candidates. Our results include 109 ferromagnetic, 83
antiferromagnetic, and 2 altermagnetic monolayers, along with 12 novel
ferromagnetic half-metals with potential for spintronics technologies.

</details>


### [46] [A Phase-Field Approach to Fracture and Fatigue Analysis: Bridging Theory and Simulation](https://arxiv.org/abs/2509.08939)
*M. Castillón,I. Romero,J. Segurado*

Main category: cond-mat.mtrl-sci

TL;DR: Novel framework combining LEFM with phase-field fracture for fatigue crack propagation using single simulation and Paris' law, validated against analytical solutions and experimental results.


<details>
  <summary>Details</summary>
Motivation: To develop a more efficient and robust approach for fatigue crack propagation analysis that avoids cycle-by-cycle simulations and simplifies parametrization while capturing complex crack instabilities.

Method: Combines Linear Elastic Fracture Mechanics with phase-field fracture using FEM simulation with monotonic crack growth control. Uses numerical evaluation of compliance derivative with respect to crack area and integrates Paris' law for fatigue life prediction.

Result: Validated through benchmarks with analytical solutions showing high accuracy. Applied to complex geometries with unknown crack paths, demonstrating very good agreement with experimental results for both crack paths and fatigue life.

Conclusion: The proposed framework provides an efficient, robust, and accurate method for fatigue crack propagation analysis that simplifies parametrization and captures complex crack instabilities while maintaining agreement with experimental data.

Abstract: This article presents a novel, robust and efficient framework for fatigue
crack-propagation that combines the principles of Linear Elastic Fracture
Mechanics (LEFM) with phase-field fracture (PFF). Contrary to cycle-by-cycle
PFF approaches, this work relies on a single simulation and uses standard crack
propagation models such as Paris' law for the material response, simplifying
its parametrization.
  The core of the methodology is the numerical evaluation of the derivative of
a specimen's compliance with respect to the crack area. To retrieve this
compliance the framework relies on a PFF-FEM simulation, controlled imposing a
monotonic crack growth. This control of the loading process is done by a new
crack-control scheme which allows to robustly trace the complete equilibrium
path of a crack, capturing complex instabilities. The specimen's compliance
obtained from the PFF simulation enables the integration of Paris' law to
predict fatigue life.
  The proposed methodology is first validated through a series of benchmarks
with analytical solutions to demonstrate its accuracy. The framework is then
applied to more complex geometries where the crack path is unknown, showing a
very good agreement with experimental results of both crack paths and fatigue
life.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [47] [Vorticity Packing Effects on Turbulent Transport in Decaying 2D Incompressible Navier-Stokes Fluids](https://arxiv.org/abs/2509.09487)
*Snehanshu Maiti,Shishir Biswas,Rajaraman Ganesh*

Main category: physics.flu-dyn

TL;DR: Study examines how initial vorticity packing fractions affect transport in 2D turbulence, showing distinct transport behaviors (subdiffusive, diffusive, superdiffusive) depending on initial conditions and flow evolution stages.


<details>
  <summary>Details</summary>
Motivation: To understand how initial vorticity conditions influence transport properties in high-Reynolds-number 2D turbulence, with applications to laboratory flows, geophysical phenomena, and astrophysical structures.

Method: Used Kelvin-Helmholtz instability to initiate turbulence, computed tracer particle trajectories in Eulerian fluid fields, and quantified transport using statistical measures (absolute dispersion, position PDFs, velocity PDFs).

Result: Initial vorticity packing fraction governs instability growth rate early on, while later transport exhibits various behaviors (subdiffusive to superdiffusive) and transitions between anisotropic/isotropic regimes, dominated by large-scale coherent vortices.

Conclusion: Initial vorticity conditions significantly impact turbulence dynamics and transport, with coherent vortex motion driving transport at late times, providing insights for quasi-2D systems across multiple physical scales.

Abstract: This paper investigates the role of initial vorticity packing fractions on
the transport properties of decaying incompressible two-dimensional
Navier-Stokes turbulence at very high Reynolds numbers and spatial resolutions.
Turbulence is initiated via the Kelvin-Helmholtz instability and evolves
through nonlinear inverse energy cascades, forming large-scale coherent
structures that dominate the flow over long eddy turnover times. The initial
vorticity packing fraction and circulation direction lead to qualitatively
distinct turbulence dynamics and transport behaviors. Tracer particle
trajectories are computed in the fluid field obtained using the Eulerian
framework, with transport and mixing quantified using statistical measures such
as absolute dispersion, position probability distribution functions (PDFs), and
velocity PDFs. In the early stages, the onset of turbulence is primarily
governed by the instability growth rate, which increases with vorticity packing
fraction. As the flow evolves, transport exhibits a range of
behaviors-subdiffusive, diffusive, or superdiffusive-and transitions between
anisotropic and isotropic regimes, depending on the initial vorticity packing,
flow structure, and stage of evolution. At later times, transport is dominated
by the motion of large-scale coherent vortices, whose dynamics are also
influenced by the initial vorticity packing ranging from subdiffusive trapping
rotational motion and random walks, and L\'evy flight-like events. These
findings offer insights into transport in quasi-2D systems-ranging from
laboratory-scale flows to geophysical phenomena and astrophysical
structures-through analogies with 2D Navier-Stokes turbulence.

</details>


### [48] [Numerical modelling of a partially loaded intermodal container freight train passing through a tunnel](https://arxiv.org/abs/2509.09591)
*Zhen Liu,David Soper,Hassan Hemida,Boyang Chen*

Main category: physics.flu-dyn

TL;DR: Improved 1D model for predicting pressure waves when freight trains enter tunnels, addressing flow separation issues at blunt containers and gaps between wagons through new mesh systems and boundary conditions.


<details>
  <summary>Details</summary>
Motivation: Traditional 1D models struggle to capture flow separation at blunt container heads and unloaded gap sections, while 3D methods require excessive computational resources and mesh quality.

Method: Developed a new 1D programme with enhanced mesh system and boundary conditions to handle discontinuities from container loading patterns, validated against Large Eddy Simulation results and parameterization studies.

Result: The improved 1D model effectively captures pressure wave complexities, bridges the gap in freight train tunnel aerodynamics, and provides accurate predictions across various loading configurations.

Conclusion: This research offers a versatile and computationally efficient 1D numerical tool for accurate pressure wave prediction in freight train tunnel entry scenarios, overcoming limitations of traditional approaches.

Abstract: The bluff nature of a freight train locomotive, coupled with large gaps
created between different wagon formations and loaded goods, influence the
overall pressure wave pattern generated as the train passes through a tunnel.
Typically, 1D models are used to predict the patterns and properties of tunnel
pressure wave formations. However, accurate modelling of regions of separation
at the head of the blunted containers and at unloaded gap sections is essential
for precise predictions of pressure magnitudes. This has traditionally been
difficult to capture with 1D models. Furthermore, achieving this accuracy
through 3D computational methods demands exceptional mesh quality, significant
computational resources, and the careful selection of numerical models. This
paper evaluates various numerical models to capture these complexities within
regions of flow separation. Findings have supported the development of a new 1D
programme to calculate the pressure wave generated by a freight locomotive
entering a tunnel, and is here further extended to consider the discontinuities
of the train body created by intermodal container loading patterns, by
implementing new mesh system and boundary conditions into the 1D programme. A
parameterisation study for different loading configurations is also presented
to improve the overall programme adaptability, and the relationship between
predetermined parameters and gap length is investigated. We validate the
effectiveness of the improved 1D model through comprehensive Large Eddy
Simulation (LES) results and conduct an extensive parameterisation study to
enhance its applicability across various loading configurations. Consequently,
this research bridges the gap in freight train tunnel aerodynamics, offering a
versatile 1D numerical tool for accurate pressure wave prediction.

</details>


<div id='physics.gen-ph'></div>

# physics.gen-ph [[Back]](#toc)

### [49] [Monte Carlo Simulation of Spallation and Fission Fragment Distributions for ADS-Related Nuclear Reactions](https://arxiv.org/abs/2509.08996)
*Sun Wenming*

Main category: physics.gen-ph

TL;DR: Monte Carlo simulations using CRISP code successfully model spallation and fission fragment distributions from proton/photon interactions with actinide nuclei, showing good agreement with experimental data for ADS applications.


<details>
  <summary>Details</summary>
Motivation: To provide reliable nuclear data for accelerator-driven system (ADS) design, safety analysis, and transmutation studies by accurately predicting residual nuclei and fission fragment yields.

Method: Used CRISP code Monte Carlo simulations incorporating intranuclear cascade, pre-equilibrium, and evaporation-fission competition models to study spallation and fission from intermediate/high-energy protons and photons on actinide/pre-actinide nuclei.

Result: Good agreement with experimental data for mass and charge distributions, with minor deviations only for light fragments. Demonstrates reliability of Monte Carlo approaches for predicting fragment yields under ADS conditions.

Conclusion: The Monte Carlo approach with CRISP code provides accurate nuclear data relevant to ADS applications, supporting design, safety, and transmutation analysis with consistent treatment of both residues and fission products.

Abstract: Monte Carlo simulations with the CRISP code were conducted to study
spallation and fission fragment distributions induced by intermediate- and
high-energy protons and photons on actinide and pre-actinide nuclei. The model
accounts for intranuclear cascade, pre-equilibrium, and evaporation-fission
competition, enabling consistent treatment of both residues and fission
products. Comparisons with experimental data show good agreement in mass and
charge distributions, with minor deviations for light fragments. The results
highlight the reliability of Monte Carlo approaches for predicting residual
nuclei and fragment yields under accelerator-driven system (ADS) conditions.
This work provides nuclear data relevant to ADS design, safety, and
transmutation analysis

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [50] [Robust Non-Linear Correlations via Polynomial Regression](https://arxiv.org/abs/2509.09380)
*Luca Giuliani,Michele Lombardi*

Main category: cs.LG

TL;DR: Novel computational approach for HGR correlation coefficient using polynomial kernels, offering improved robustness and determinism for real-world applications.


<details>
  <summary>Details</summary>
Motivation: Existing HGR estimation methods suffer from bias-variance trade-offs due to inherent uncomputability, compromising robustness in real-world scenarios like algorithmic fairness and constrained ML.

Method: User-configurable polynomial kernels approach for HGR computation, providing faster yet effective restriction compared to previous methods.

Result: The method demonstrates significant advantages in robustness and determinism, with experimental validation showing it produces insightful subgradients suitable as loss regularizers in constrained ML frameworks.

Conclusion: The proposed polynomial kernel approach offers a more reliable and robust computational method for HGR correlation, making it suitable for practical applications in fairness, scientific analysis, and causal discovery.

Abstract: The Hirschfeld-Gebelein-R\'enyi (HGR) correlation coefficient is an extension
of Pearson's correlation that is not limited to linear correlations, with
potential applications in algorithmic fairness, scientific analysis, and causal
discovery. Recently, novel algorithms to estimate HGR in a differentiable
manner have been proposed to facilitate its use as a loss regularizer in
constrained machine learning applications. However, the inherent
uncomputability of HGR requires a bias-variance trade-off, which can possibly
compromise the robustness of the proposed methods, hence raising technical
concerns if applied in real-world scenarios. We introduce a novel computational
approach for HGR that relies on user-configurable polynomial kernels, offering
greater robustness compared to previous methods and featuring a faster yet
almost equally effective restriction. Our approach provides significant
advantages in terms of robustness and determinism, making it a more reliable
option for real-world applications. Moreover, we present a brief experimental
analysis to validate the applicability of our approach within a constrained
machine learning framework, showing that its computation yields an insightful
subgradient that can serve as a loss regularizer.

</details>


### [51] [ReBaNO: Reduced Basis Neural Operator Mitigating Generalization Gaps and Achieving Discretization Invariance](https://arxiv.org/abs/2509.09611)
*Haolan Zheng,Yanlai Chen,Jiequn Han,Yue Yu*

Main category: cs.LG

TL;DR: ReBaNO is a novel data-lean operator learning algorithm that combines reduced basis methods with neural networks to solve PDEs with multiple inputs, achieving superior generalization and strict discretization invariance compared to state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: To address the generalization gap and lack of discretization invariance in existing operator learning algorithms for solving PDEs with multiple distinct inputs, while maintaining computational efficiency.

Method: Combines Reduced Basis Method with Generative Pre-Trained Physics-Informed Neural Networks using a greedy algorithm to build network structure offline, and employs knowledge distillation via task-specific activation functions for compact architecture.

Result: Significantly outperforms PCA-Net, DeepONet, FNO, and CNO in eliminating/shrinking generalization gap for both in- and out-of-distribution tests, and is the only operator learning algorithm achieving strict discretization invariance.

Conclusion: ReBaNO provides a mathematically rigorous, computationally efficient approach for operator learning that overcomes key limitations of existing methods while maintaining physics embedding capabilities.

Abstract: We propose a novel data-lean operator learning algorithm, the Reduced Basis
Neural Operator (ReBaNO), to solve a group of PDEs with multiple distinct
inputs. Inspired by the Reduced Basis Method and the recently introduced
Generative Pre-Trained Physics-Informed Neural Networks, ReBaNO relies on a
mathematically rigorous greedy algorithm to build its network structure offline
adaptively from the ground up. Knowledge distillation via task-specific
activation function allows ReBaNO to have a compact architecture requiring
minimal computational cost online while embedding physics. In comparison to
state-of-the-art operator learning algorithms such as PCA-Net, DeepONet, FNO,
and CNO, numerical results demonstrate that ReBaNO significantly outperforms
them in terms of eliminating/shrinking the generalization gap for both in- and
out-of-distribution tests and being the only operator learning algorithm
achieving strict discretization invariance.

</details>


<div id='physics.ed-ph'></div>

# physics.ed-ph [[Back]](#toc)

### [52] [Are arXiv submissions on Wednesday better cited? Introducing Big Data methods in undergraduate courses on scientific computing](https://arxiv.org/abs/2509.09601)
*Stéphane Delorme,Leon Mach,Hubert Paszkiewicz,Richard Ruiz*

Main category: physics.ed-ph

TL;DR: A simple data analysis pipeline for teaching big data methods using arXiv and inSpireHEP databases, implemented with open-source Python libraries for undergraduate physics education.


<details>
  <summary>Details</summary>
Motivation: Address barriers to learning big data methods in undergraduate physics curricula by providing accessible, practical tools that use real scientific data sources.

Method: Developed a farm-to-table data analysis pipeline that collects, processes, and plots data from 800k entries common to arXiv preprint repository and inSpireHEP bibliographical database using open-source Python libraries.

Result: Created an example implementation that runs on standard laptops, authored by undergraduate students, demonstrating practical application of contemporary research practices in educational settings.

Conclusion: The pipeline successfully provides an accessible entry point for teaching big data analysis methods and has potential applications for advanced uses like online DAQ monitoring and commercialization.

Abstract: Extracting information from big data sets, both real and simulated, is a
modern hallmark of the physical sciences. In practice, students face barriers
to learning ``Big Data'' methods in undergraduate physics and astronomy
curricula. As an attempt to alleviate some of these challenges, we present a
simple, farm-to-table data analysis pipeline that can collect, process, and
plot data from the 800k entries common to the arXiv preprint repository and the
bibliographical database inSpireHEP. The pipeline employs contemporary research
practices and can be implemented using open-sourced Python libraries common to
undergraduate courses on Scientific Computing. To support the use such
pipelines in classroom contexts, we make public an example implementation,
authored by two undergraduate physics students, that runs on off-the-shelf
laptops. For advanced students, we discuss applications of the pipeline,
including for online DAQ monitoring and commercialization.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [53] [Symmetries in stochastic homogenization and acclimatizations for the RVE method](https://arxiv.org/abs/2509.08977)
*Binh Huy Nguyen,Matti Schneider*

Main category: cs.CE

TL;DR: Study on enforcing microstructure symmetries in effective thermal conductivity tensors using orthogonal projectors to maintain symmetry properties and reduce errors in RVE homogenization.


<details>
  <summary>Details</summary>
Motivation: To address how microstructure symmetries affect effective tensor properties and their fluctuations, and to develop methods for enforcing these symmetries when they are broken by RVE cell types.

Method: Use orthogonal projectors in postprocessing to enforce expected symmetries, analyze implications on error bounds, and validate through large-scale FFT-based homogenization simulations on fiber-reinforced composites.

Result: Suitable projections provide unbiased variance-reduction strategies that exactly enforce expected symmetries, improving accuracy in effective property estimation.

Conclusion: Symmetry-projection techniques effectively rectify symmetry breaking in RVE methods, enhancing the reliability of effective property predictions for composite microstructures.

Abstract: We investigate the implications of a given symmetry of a random
microstructure on the obtained effective tensor and its fluctuation in the
context of thermal conductivity, and study strategies for enforcing these
symmetries in postprocessing via orthogonal projectors. Within the framework of
the representative volume element (RVE) method, we establish the invariance
conditions for the effective tensor and its fluctuation under different
symmetry groups of the microstructure. Interestingly, the symmetry of the
considered cell type in the RVE method may break the ensemble symmetry and
compromise the approximation of the effective properties. To rectify this
issue, we introduce dedicated techniques which permit to enforce the expected
symmetries in postprocessing and study the implications on the bounds for the
effective properties as well as the total, the random and the systematic
errors. We provide theoretical arguments that suitable projections lead to
unbiased variance-reduction strategies which furthermore enforce the expected
symmetries exactly. Through large-scale FFT-based homogenization simulations,
we study the symmetry structure of the estimated effective conductivities and
their fluctuations. Moreover, we demonstrate the power of the
symmetry-projection techniques for fiber-reinforced composite microstructures
of industrial scale.

</details>
