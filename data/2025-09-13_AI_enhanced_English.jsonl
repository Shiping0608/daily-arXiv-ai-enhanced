{"id": "2509.08902", "pdf": "https://arxiv.org/pdf/2509.08902", "abs": "https://arxiv.org/abs/2509.08902", "authors": ["Qiumei Huang", "Alexander Ostermann", "Gangfan Zhong"], "title": "Exponential Runge-Kutta methods for parabolic equations with state-dependent delay", "categories": ["math.NA", "cs.NA", "65M12, 65L06"], "comment": null, "summary": "The aim of this paper is to construct and analyze exponential Runge-Kutta\nmethods for the temporal discretization of a class of semilinear parabolic\nproblems with arbitrary state-dependent delay. First, the well-posedness of the\nproblem is established. Subsequently, first and second order schemes are\nconstructed. They are based on the explicit exponential Runge-Kutta methods,\nwhere the delayed solution is approximated by a continuous extension of the\ntime discrete solution. Schemes of arbitrary order can be constructed using the\nmethods of collocation type. The unique solvability and convergence of the\nproposed schemes are established. Finally, we discuss implementation issues and\npresent some numerical experiments to illustrate our theoretical results.", "AI": {"tldr": "Construction and analysis of exponential Runge-Kutta methods for semilinear parabolic problems with state-dependent delay, including well-posedness, scheme development, and convergence analysis.", "motivation": "To develop efficient temporal discretization methods for semilinear parabolic problems with arbitrary state-dependent delay, which are challenging due to the delay dependency on the solution state.", "method": "Explicit exponential Runge-Kutta methods with continuous extension for delay approximation, including first and second order schemes and collocation-type methods for arbitrary order.", "result": "Established well-posedness of the problem, unique solvability of the schemes, and proved convergence of the proposed numerical methods.", "conclusion": "The developed exponential Runge-Kutta methods are effective for solving semilinear parabolic problems with state-dependent delay, with theoretical convergence supported by numerical experiments."}}
{"id": "2509.08936", "pdf": "https://arxiv.org/pdf/2509.08936", "abs": "https://arxiv.org/abs/2509.08936", "authors": ["Lise-Marie Imbert-G\u00e9rard", "Andr\u00e9a Lagard\u00e8re", "Guillaume Sylvand", "S\u00e9bastien Tordeux"], "title": "Quasi-Trefftz spaces for a first-order formulation of the Helmholtz equation", "categories": ["math.NA", "cs.NA"], "comment": null, "summary": "This work is the first step in the development of quasi-Trefftz methods for\nfirst-order differential systems. It focuses on discrete quasi-Trefftz spaces,\nstarting from their definition and including construction of corresponding\nbases together with their computational aspect.", "AI": {"tldr": "Development of quasi-Trefftz methods for first-order differential systems, focusing on discrete quasi-Trefftz spaces including definition, basis construction, and computational aspects.", "motivation": "To establish the foundation for quasi-Trefftz methods applied to first-order differential systems, which represents a novel approach in numerical methods for differential equations.", "method": "Defines discrete quasi-Trefftz spaces, constructs corresponding basis functions, and addresses computational implementation aspects of these methods.", "result": "Presents the first systematic framework for quasi-Trefftz methods in first-order differential systems, providing definitions and computational foundations.", "conclusion": "This work lays the groundwork for future development and application of quasi-Trefftz methods to solve first-order differential systems numerically."}}
{"id": "2509.08990", "pdf": "https://arxiv.org/pdf/2509.08990", "abs": "https://arxiv.org/abs/2509.08990", "authors": ["Shalmali Bandyopadhyay", "Thomas Lewis", "Dustin Nichols"], "title": "Numerical Approximation and Bifurcation Results for an Elliptic Problem with Superlinear Subcritical Nonlinearity on the Boundary", "categories": ["math.NA", "cs.NA", "math.AP"], "comment": null, "summary": "We develop numerical algorithms to approximate positive solutions of elliptic\nboundary value problems with superlinear subcritical nonlinearity on the\nboundary of the form $-\\Delta u + u = 0$ in $\\Omega$ with $\\frac{\\partial\nu}{\\partial \\eta} = \\lambda f(u)$ on $\\partial\\Omega$ as well as an extension\nto a corresponding system of equations. While existence, uniqueness,\nnonexistence, and multiplicity results for such problems are well-established,\ntheir numerical treatment presents computational challenges due to the absence\nof comparison principles and complex bifurcation phenomena. We present finite\ndifference formulations for both single equations and coupled systems with\ncross-coupling boundary conditions, establishing admissibility results for the\nfinite difference method. We derive principal eigenvalue analysis for the\nlinearized problems to determine unique bifurcation points from trivial\nsolutions. The eigenvalue analysis provides additional insight into the\ntheoretical properties of the problem while also providing intuition for\ncomputing approximate solutions based on the proposed finite difference\nformulation. We combine our finite difference methods with continuation methods\nto trace complete bifurcation curves, validating established existence and\nuniqueness results and consistent with the results of the principle eigenvalue\nanalysis.", "AI": {"tldr": "Numerical algorithms for approximating positive solutions of elliptic boundary value problems with superlinear subcritical nonlinearity on the boundary, including extension to coupled systems with cross-coupling boundary conditions.", "motivation": "While theoretical results (existence, uniqueness, nonexistence, multiplicity) for such problems are well-established, their numerical treatment presents computational challenges due to absence of comparison principles and complex bifurcation phenomena.", "method": "Finite difference formulations for single equations and coupled systems, combined with continuation methods to trace complete bifurcation curves. Principal eigenvalue analysis for linearized problems to determine unique bifurcation points.", "result": "Established admissibility results for finite difference method. Validated existing existence and uniqueness results through numerical computations. Eigenvalue analysis provided additional theoretical insights and computational intuition.", "conclusion": "The proposed finite difference methods combined with continuation techniques successfully address computational challenges and provide effective numerical treatment for these complex boundary value problems with nonlinear boundary conditions."}}
{"id": "2509.09017", "pdf": "https://arxiv.org/pdf/2509.09017", "abs": "https://arxiv.org/abs/2509.09017", "authors": ["Katerina Beklemysheva", "Egor Michel", "Andrey Ovsiannikov"], "title": "Numerical modeling of elastic waves in thin shells with grid-characteristic method", "categories": ["math.NA", "cs.NA", "physics.comp-ph"], "comment": null, "summary": "Numerical modeling of strength and non-destructive testing of complex\nstructures such as buildings, space rockets or oil reservoirs often involves\ncalculations on extremely large grids. The modeling of elastic wave processes\nin solids places limitations on the grid element size because resolving\ndifferent elastic waves requires at least several grid elements for the\ncharacteristic size of the modeled object. For a thin plate, the defining size\nis its thickness, and a complex structure that contains large-scale thin\nobjects requires a large-scale grid to preserve its uniformity. One way to\nbypass this problem is the theory of thin plates and shells that replaces a\nsimple material model on a fine three-dimensional mesh with a more complex\nmaterial model on a coarser mesh. This approach loses certain fine effects\ninside the thin plate, but allows us to model large and complex thin objects\nwith a reasonable size calculation grid and resolve all the significant wave\ntypes. In this research, we take the Kirchhoff-Love material model and derive a\nhyperbolic dynamic system of equations that allows for a physical\ninterpretation of eigenvalues and eigenvectors. The system is solved\nnumerically with a grid-characteristic method. Numerical results for several\nmodel statements are compared with three-dimensional calculations based on\ngrid-characteristic method for a three dimensional elasticity.", "AI": {"tldr": "A method for modeling elastic waves in thin structures using Kirchhoff-Love theory with grid-characteristic numerical solution, enabling efficient simulation of large complex structures while maintaining wave resolution.", "motivation": "To overcome computational limitations when modeling elastic wave processes in large complex structures with thin components, which require extremely fine grids for accurate 3D modeling.", "method": "Derived hyperbolic dynamic system from Kirchhoff-Love material model, solved numerically using grid-characteristic method, replacing 3D fine mesh with complex material model on coarser mesh.", "result": "Developed efficient numerical approach that allows modeling of large thin structures with reasonable grid sizes while resolving significant wave types, validated against 3D elasticity calculations.", "conclusion": "The Kirchhoff-Love based approach provides a practical solution for modeling wave processes in complex thin structures, balancing computational efficiency with adequate physical accuracy for engineering applications."}}
{"id": "2509.09104", "pdf": "https://arxiv.org/pdf/2509.09104", "abs": "https://arxiv.org/abs/2509.09104", "authors": ["Ye Tao", "Lei Chang", "Dingzhou Li", "Yingxin Zhao"], "title": "Exploration of novel ICP using helicon antennas with zero magnetic field", "categories": ["physics.plasm-ph"], "comment": null, "summary": "Inductively coupled plasma (ICP) attracts great attention from aspects of\nfundamental research and practical applications, and efficient power coupling\nis highly desirable for both of them. The present study explores a novel\nstrategy for efficient ICP through using helicon antennas with zero external\nmagnetic field. Specific research is devoted to the effects of antenna geometry\n(loop, half-helix, Boswell, Nagoya III), driving frequency (13.56-54.24 MHz)\nand radial density profile (Gaussian and parabolic) on power coupling. Findings\nreveal that: loop antenna yields higher power deposition efficiency than\nhalf-helix, Boswell, and Nagoya III antennas, driving frequency gives\nnegligible effects, and parabolic density profile results in more efficient\npower coupling than Gaussian density profile especially in the radial\ndirection, for the conditions employed here. Therefore, it is suggested that\nfor this novel ICP strategy one should use loop antenna with parabolic density\nprofile, and the industrial frequency of 13.56 MHz can work well. This study\nprovides a valuable reference for the novel design of efficient ICP sources,\nwhich could be used for material processing and space propulsion, etc. Key\nwords: Inductively coupled plasma; Antenna Geometry; Power Deposition; Driving\nFrequency", "AI": {"tldr": "Loop antenna with parabolic density profile at 13.56 MHz provides most efficient ICP power coupling without external magnetic field.", "motivation": "Efficient power coupling in inductively coupled plasma is crucial for both fundamental research and practical applications like material processing and space propulsion.", "method": "Investigated effects of antenna geometry (loop, half-helix, Boswell, Nagoya III), driving frequency (13.56-54.24 MHz), and radial density profile (Gaussian vs parabolic) on power coupling efficiency in ICP without external magnetic field.", "result": "Loop antenna showed highest power deposition efficiency; driving frequency had negligible effects; parabolic density profile resulted in more efficient power coupling than Gaussian profile, especially radially.", "conclusion": "For efficient ICP without external magnetic field, use loop antenna with parabolic density profile at industrial frequency 13.56 MHz."}}
{"id": "2509.08971", "pdf": "https://arxiv.org/pdf/2509.08971", "abs": "https://arxiv.org/abs/2509.08971", "authors": ["Julien Loiseau", "Hyun Lim", "Andr\u00e9s Yag\u00fce L\u00f3pez", "Mammadbaghir Baghirzade", "Shihab Shahriar Khan", "Yoonsoo Kim", "Sudarshan Neopane", "Alexander Strack", "Farhana Taiyebah", "Benjamin K. Bergen"], "title": "HARD: A Performance Portable Radiation Hydrodynamics Code based on FleCSI Framework", "categories": ["physics.comp-ph", "astro-ph.IM", "cs.DC"], "comment": "15 pages, 8 figures", "summary": "Hydrodynamics And Radiation Diffusion} (HARD) is an open-source application\nfor high-performance simulations of compressible hydrodynamics with\nradiation-diffusion coupling. Built on the FleCSI (Flexible Computational\nScience Infrastructure) framework, HARD expresses its computational units as\ntasks whose execution can be orchestrated by multiple back-end runtimes,\nincluding Legion, MPI, and HPX. Node-level parallelism is delegated to Kokkos,\nproviding a single, portable code base that runs efficiently on laptops, small\nhomogeneous clusters, and the largest heterogeneous supercomputers currently\navailable. To ensure scientific reliability, HARD includes a regression-test\nsuite that automatically reproduces canonical verification problems such as the\nSod and LeBlanc shock tubes and the Sedov blast wave, comparing numerical\nsolutions against known analytical results. The project is distributed under an\nOSI-approved license, hosted on GitHub, and accompanied by reproducible build\nscripts and continuous integration workflows. This combination of performance\nportability, verification infrastructure, and community-focused development\nmakes HARD a sustainable platform for advancing radiation hydrodynamics\nresearch across multiple domains.", "AI": {"tldr": "HARD is an open-source radiation hydrodynamics simulation tool built on FleCSI framework with multiple runtime backends (Legion, MPI, HPX) and Kokkos for node-level parallelism, featuring verification tests and cross-platform performance.", "motivation": "To create a sustainable, high-performance simulation platform for radiation hydrodynamics research that works efficiently across different computing environments from laptops to supercomputers.", "method": "Built on FleCSI framework with task-based computational units, multiple runtime orchestrators (Legion, MPI, HPX), and Kokkos for portable node-level parallelism. Includes regression-test suite for verification against analytical solutions.", "result": "A single portable code base that runs efficiently on various computing platforms with automated verification through canonical test problems (Sod shock tube, LeBlanc shock tube, Sedov blast wave).", "conclusion": "HARD provides a sustainable platform for radiation hydrodynamics research with performance portability, verification infrastructure, and community-focused development under an OSI-approved license."}}
{"id": "2509.08957", "pdf": "https://arxiv.org/pdf/2509.08957", "abs": "https://arxiv.org/abs/2509.08957", "authors": ["Gayana Jayasinghe", "Katrina Morgan", "Jacob Shapiro", "Mengxuan Yang"], "title": "Logarithmic wave decay for short range wavespeed perturbations with radial regularity", "categories": ["math.AP"], "comment": "33 pages", "summary": "We establish logarithmic local energy decay for wave equations with a varying\nwavespeed in dimensions two and higher, where the wavespeed is assumed to be a\nshort range perturbation of unity with mild radial regularity. The key\ningredient is H\\\"older continuity of the weighted resolvent for real\nfrequencies $\\lambda$, modulo a logarithmic remainder in dimension two as\n$\\lambda \\to 0$. Our approach relies on a study of the resolvent in two\ndistinct frequency regimes. In the low frequency regime, we derive an expansion\nfor the resolvent using a Neumann series and properties of the free resolvent.\nFor frequencies away from zero, we establish a uniform resolvent estimate by\nway of a Carleman estimate.", "AI": {"tldr": "Logarithmic local energy decay for wave equations with varying wavespeed in 2+ dimensions, using resolvent analysis in different frequency regimes.", "motivation": "To establish energy decay properties for wave equations with perturbed wavespeeds, which is important for understanding wave propagation and scattering in non-uniform media.", "method": "Analyze weighted resolvent for real frequencies using two approaches: low-frequency regime (Neumann series expansion of free resolvent) and non-zero frequencies (uniform resolvent estimate via Carleman estimate).", "result": "Proves logarithmic local energy decay for wavespeed perturbations of unity with mild radial regularity, with H\u00f6lder continuity of weighted resolvent modulo logarithmic terms in 2D.", "conclusion": "The paper successfully establishes energy decay properties through resolvent analysis, providing important results for wave equations with varying wavespeeds in multiple dimensions."}}
{"id": "2509.09023", "pdf": "https://arxiv.org/pdf/2509.09023", "abs": "https://arxiv.org/abs/2509.09023", "authors": ["Austen J. Nelson", "Panayot S. Vassilevski"], "title": "Characterization of the near-null error components utilized in composite adaptive AMG solvers", "categories": ["math.NA", "cs.NA", "15 (Primary), 65 (Secondary)"], "comment": "16 pages, 6 figures, presented at 22nd Copper Mountain Conference on\n  Multigrid Methods", "summary": "We provide a theoretical justification for the construction of adaptive\ncomposite solvers based on a sequence of AMG (algebraic multigrid) $\\mu$-cycle\nmethods that exploit error components that the current solver cannot damp\nefficiently. Each solver component is an aggregation based AMG where its\naggregates are constructed using the popular in graph community detection\nmodularity matrix. The latter utilizes the given matrix and the error component\nvector the current solver cannot handle. The performance of the resulting\nadaptive composite solver is illustrated on a variety of sparse matrices both\narising from discretized PDEs and ones with more general nature.", "AI": {"tldr": "Theoretical justification for adaptive composite solvers using AMG \u03bc-cycle methods that target error components current solvers can't handle efficiently, with aggregates constructed via modularity matrix from graph community detection.", "motivation": "To improve solver efficiency by addressing error components that existing AMG methods struggle to damp effectively, creating more robust composite solvers for various matrix types.", "method": "Construct adaptive composite solvers using sequence of AMG \u03bc-cycle methods, where each component uses aggregation-based AMG with aggregates built via modularity matrix (from graph community detection) applied to the matrix and current error vector.", "result": "The performance is demonstrated on various sparse matrices including those from discretized PDEs and more general matrices, showing effectiveness of the approach.", "conclusion": "The adaptive composite solver framework provides an effective approach for handling challenging error components in AMG methods, with broad applicability across different matrix types including PDE-derived and general sparse matrices."}}
{"id": "2509.09126", "pdf": "https://arxiv.org/pdf/2509.09126", "abs": "https://arxiv.org/abs/2509.09126", "authors": ["Jikai Sun", "Lei Chang", "Yu Liu", "Guojun Wang", "Zichen Kan", "Shijie Zhang", "Jingjing Ma", "Dingzhou Li", "Yingxin Zhao"], "title": "Exploration on the Two-stream Instability in the Polar Cusp Under Solar Storm Disturbances and its Potential Impacts on Spacecraft", "categories": ["physics.plasm-ph"], "comment": null, "summary": "During solar storms, the polar cusp often exhibits electron populations with\ndistinct velocity distributions, which may be associated with the two-stream\ninstability. This study reveals the evolution of the two-stream instability\nassociated with electron velocities and the interaction between the growth\nphase of the two-stream instability and the electrostatic solitary waves\n(ESWs). The results from particle-in-cell (PIC) simulations are compared with\nsatellite observational data and computational outcomes. The potential risks\nassociated with two-stream instability, including surface charge accumulation\nand communication system interference on spacecraft, are also explored. The\nfindings show that, in the high-latitude polar cusp region, the interaction\nbetween the solar wind plasma propagating along magnetic field lines and the\nupward-moving ionospheric plasma could drive two-stream instability, leading to\nthe formation of electron hole structures in phase space and triggering a\nbipolar distribution of ESWs. When the spatial magnetic field and wave vector\nmeet specific conditions, the enhanced electron cyclotron motion could suppress\nthe formation of two-stream instability and electron hole structures, leading\nto a reduction in the amplitude of the ESWs. The results offer valuable\ninsights for a deeper understanding of the impact of solar storms on the polar\ncusp environment, as well as for monitoring electromagnetic environment and\nensuring the stable operation of spacecraft.", "AI": {"tldr": "Study examines two-stream instability evolution in polar cusp during solar storms, showing how electron-cyclotron motion can suppress instability and reduce electrostatic wave amplitudes, with implications for spacecraft safety.", "motivation": "To understand the evolution of two-stream instability associated with electron velocities during solar storms and its interaction with electrostatic solitary waves, particularly in relation to spacecraft risks like surface charge accumulation and communication interference.", "method": "Used particle-in-cell (PIC) simulations and compared results with satellite observational data and computational outcomes to analyze the two-stream instability mechanisms.", "result": "Found that solar wind plasma interacting with upward-moving ionospheric plasma drives two-stream instability, forming electron hole structures and triggering bipolar ESW distribution. Enhanced electron cyclotron motion under specific magnetic field conditions can suppress instability and reduce ESW amplitude.", "conclusion": "The study provides valuable insights for understanding solar storm impacts on polar cusp environment and offers guidance for monitoring electromagnetic environment and ensuring spacecraft operational stability."}}
{"id": "2509.09051", "pdf": "https://arxiv.org/pdf/2509.09051", "abs": "https://arxiv.org/abs/2509.09051", "authors": ["Joseph L. Hesse-Withbroe", "Katya S. Arquilla"], "title": "An Improved Rapid Performance Analysis Model for Solenoidal Magnetic Radiation Shields", "categories": ["physics.comp-ph", "physics.app-ph"], "comment": "15 pages + appendix, 10 figures", "summary": "Astronauts participating in deep-space exploration missions will be exposed\nto significantly greater amounts of radiation than is typically encountered on\nEarth or in low Earth orbit (LEO), which poses significant risks to crew health\nand mission safety. Active magnetic radiation shields based on the Lorentz\ndeflection of charged particles have the potential to reduce astronaut doses\nwith lower mass costs than passive shielding techniques. Typically, active\nshielding performance is evaluated using high-fidelity Monte Carlo simulations,\nwhich are too computationally expensive to evaluate an entire trade space of\nshield designs. A rapid, semi-analytical model based on the High Charge and\nEnergy Transport code (HZETRN) developed in 2014 provided an alternative method\nby which to evaluate the performance of solenoidal shields. However, various\nsimplifying assumptions made in the original model have limited its accuracy,\nand therefore require evaluation and correction. In this work, a number of\naspects of the original semi-analytical model are updated and validated by\nMonte Carlo simulation, then used to recharacterize the design trade space of\nsolenoidal magnetic shields. The updated model predicts improved performance\nfor weaker shields as compared to the original model, but greatly diminished\nperformance for strong shields with bending powers greater than 20 T-m.\nOverall, the results indicate that magnetic shields enable significant mass\nsavings over passive shields for mission scenarios where the requisite dose\nreduction is greater than about 60% relative to free space, which includes most\nexploration missions longer than one year with significant time spent outside\nLEO.", "AI": {"tldr": "Updated semi-analytical model for evaluating solenoidal magnetic radiation shields shows improved performance for weaker shields but diminished performance for strong shields (>20 T-m), indicating mass savings over passive shielding for missions requiring >60% dose reduction.", "motivation": "Deep-space radiation exposure poses significant health risks to astronauts, and active magnetic shielding offers potential mass savings over passive methods, but existing evaluation models have accuracy limitations.", "method": "Updated and validated a semi-analytical model based on HZETRN code, correcting simplifying assumptions from the original 2014 model, then used Monte Carlo simulation for validation and recharacterized the shield design trade space.", "result": "The updated model predicts improved performance for weaker shields but greatly diminished performance for strong shields with bending powers >20 T-m. Magnetic shields enable significant mass savings for missions requiring >60% dose reduction.", "conclusion": "Active magnetic radiation shields are viable for long-duration exploration missions (>1 year) outside LEO where substantial dose reduction (>60%) is needed, offering mass advantages over passive shielding techniques."}}
{"id": "2509.09098", "pdf": "https://arxiv.org/pdf/2509.09098", "abs": "https://arxiv.org/abs/2509.09098", "authors": ["Gregory R. Chambers", "Jared Marx-Kuo"], "title": "Mountain Pass Critical Points of the Liquid Drop Model", "categories": ["math.AP", "math-ph", "math.DG", "math.MP"], "comment": "17 pages, 1 figure, comments welcome!", "summary": "We consider Gamow's liquid drop functional, $\\mathcal{E}$, on $\\mathbb{R}^3$\nand construct non-minimizing, volume constrained, critical points for volumes\n$3.512 \\cong \\alpha_0 < V < 10$. In this range, we establish a mountain pass\nset up between a ball of volume $V$ and two balls of volume $V/2$ infinitely\nfar apart. Intuitively, our critical point corresponds to the maximal energy\nconfiguration of an atom of volume $V$ as it undergoes fission into two atoms\nof volume $V/2$. Our proof relies on geometric measure theoretical methods from\nthe min-max construction of minimal surfaces, and along the way, we address\nissues of non-compactness, ``pull tight\" with a volume constraint, and\nmultiplicity.", "AI": {"tldr": "Construction of non-minimizing critical points for Gamow's liquid drop model representing maximal energy configurations during nuclear fission from volume V to two V/2 volumes.", "motivation": "To understand the energy barrier and critical configurations in nuclear fission processes using Gamow's liquid drop model, particularly for volumes between 3.512 and 10.", "method": "Geometric measure theoretical methods from min-max construction of minimal surfaces, addressing non-compactness, volume constraint \"pull tight\", and multiplicity issues.", "result": "Successfully constructed volume-constrained critical points representing maximal energy configurations during fission from single atom to two separated atoms.", "conclusion": "The study provides mathematical insight into nuclear fission energy barriers using geometric analysis techniques, establishing a mountain pass structure between different equilibrium states."}}
{"id": "2509.09032", "pdf": "https://arxiv.org/pdf/2509.09032", "abs": "https://arxiv.org/abs/2509.09032", "authors": ["Guy Tsafack", "Antoine Tambue"], "title": "Strong convergence of a semi tamed scheme for stochastic differential algebraic equation under non-global Lipschitz coefficients", "categories": ["math.NA", "cs.NA"], "comment": null, "summary": "We are investigating the first strong convergence analysis of a numerical\nmethod for stochastic differential algebraic equations (SDAEs) under a\nnon-global Lipschitz setting. It is well known that the explicit Euler scheme\nfails to converge strongly to the exact solution of a stochastic differential\nequation (SDEs) when at least one of the coefficients grows superlinearly. The\nproblem becomes more challenging in the case of stochastic\ndifferential-algebraic equations (SDAEs) due to the singularity of the matrix.\nTo address this, we build a new scheme called the semi-implicit tamed method\nfor SDAEs and provide its strong convergence result under non-global Lipschitz\nsetting. In other words, the linear component of the drift term is approximated\nimplicitly, whereas its nonlinear component is tamed and approximated\nexplicitly. We show that this method strongly converges with order\n$\\frac{1}{2}$ to the exact solution. To prove this strong convergence result,\nwe first derive an equivalent scheme, that we call the dual tamed scheme, which\nis more suitable for mathematical analysis and is associated with the inherent\nstochastic differential equation obtained by eliminating the constraints from\nthe original SDAEs. To demonstrate the effectiveness of the proposed scheme,\nnumerical simulations are performed, confirming that the theoretical findings\nare consistent with the numerical results.", "AI": {"tldr": "First strong convergence analysis of a semi-implicit tamed method for stochastic differential algebraic equations (SDAEs) under non-global Lipschitz conditions, achieving order 1/2 convergence.", "motivation": "Explicit Euler scheme fails for SDEs with superlinear growth coefficients, and the problem is more challenging for SDAEs due to matrix singularity. Need for a robust numerical method that can handle non-global Lipschitz settings.", "method": "Developed a semi-implicit tamed method where linear drift components are approximated implicitly and nonlinear components are tamed and approximated explicitly. Created an equivalent dual tamed scheme for mathematical analysis by eliminating constraints from original SDAEs.", "result": "The proposed method strongly converges with order 1/2 to the exact solution under non-global Lipschitz conditions. Numerical simulations confirm theoretical findings.", "conclusion": "The semi-implicit tamed method effectively addresses the convergence challenges of SDAEs in non-global Lipschitz settings, providing a reliable numerical approach with proven strong convergence properties."}}
{"id": "2509.09040", "pdf": "https://arxiv.org/pdf/2509.09040", "abs": "https://arxiv.org/abs/2509.09040", "authors": ["Charles D. Arrowsmith", "Francesco Miniati", "Pablo J. Bilbao", "Pascal Simon", "Archie F. A. Bott", "Stephane Burger", "Hui Chen", "Filipe D. Cruz", "Tristan Davenne", "Anthony Dyson", "Ilias Efthymiopoulos", "Dustin H. Froula", "Alice Goillot", "Jon T. Gudmundsson", "Dan Haberberger", "Jack W. D. Halliday", "Tom Hodge", "Brian T. Huffman", "Sam Iaquinta", "Graham Marshall", "Brian Reville", "Subir Sarkar", "Alexander A. Schekochihin", "Luis O. Silva", "Raspberry Simpson", "Vasiliki Stergiou", "Raoul M. G. M. Trines", "Thibault Vieu", "Nikolaos Charitonidis", "Robert Bingham", "Gianluca Gregori"], "title": "Suppression of pair beam instabilities in a laboratory analogue of blazar pair cascades", "categories": ["astro-ph.HE", "physics.plasm-ph"], "comment": null, "summary": "The generation of dense electron-positron pair beams in the laboratory can\nenable direct tests of theoretical models of $\\gamma$-ray bursts and active\ngalactic nuclei. We have successfully achieved this using ultra-relativistic\nprotons accelerated by the Super Proton Synchrotron at CERN. In the first\napplication of this experimental platform, the stability of the pair beam is\nstudied as it propagates through a metre-length plasma, analogous to TeV\n$\\gamma$-ray induced pair cascades in the intergalactic medium. It has been\nargued that pair beam instabilities disrupt the cascade, thus accounting for\nthe observed lack of reprocessed GeV emission from TeV blazars. If true this\nwould remove the need for a moderate strength intergalactic magnetic field to\nexplain the observations. We find that the pair beam instability is suppressed\nif the beam is not perfectly collimated or monochromatic, hence the lower limit\nto the intergalactic magnetic field inferred from $\\gamma$-ray observations of\nblazars is robust.", "AI": {"tldr": "Laboratory generation of dense electron-positron pair beams using CERN's Super Proton Synchrotron enables direct testing of astrophysical models, showing that pair beam instabilities are suppressed when beams are not perfectly collimated/monochromatic, supporting the need for intergalactic magnetic fields.", "motivation": "To enable direct laboratory testing of theoretical models for gamma-ray bursts and active galactic nuclei, particularly to study pair beam stability and its implications for astrophysical observations of blazars.", "method": "Used ultra-relativistic protons accelerated by CERN's Super Proton Synchrotron to generate dense electron-positron pair beams, then studied beam stability as it propagates through a meter-length plasma to simulate TeV gamma-ray induced pair cascades in intergalactic medium.", "result": "Found that pair beam instability is suppressed when the beam is not perfectly collimated or monochromatic, indicating that the instability does not disrupt the cascade as previously argued.", "conclusion": "The lower limit to intergalactic magnetic field inferred from gamma-ray observations of blazars remains robust, as the pair beam instability does not eliminate the need for moderate strength magnetic fields to explain observational data."}}
{"id": "2509.08834", "pdf": "https://arxiv.org/pdf/2509.08834", "abs": "https://arxiv.org/abs/2509.08834", "authors": ["John T. Rickard", "William A. Dembski", "James Rickards"], "title": "An Interval Type-2 Version of Bayes Theorem Derived from Interval Probability Range Estimates Provided by Subject Matter Experts", "categories": ["cs.AI", "physics.comp-ph", "physics.data-an", "q-fin.CP"], "comment": "13 pages, 12 figures", "summary": "Bayesian inference is widely used in many different fields to test hypotheses\nagainst observations. In most such applications, an assumption is made of\nprecise input values to produce a precise output value. However, this is\nunrealistic for real-world applications. Often the best available information\nfrom subject matter experts (SMEs) in a given field is interval range estimates\nof the input probabilities involved in Bayes Theorem. This paper provides two\nkey contributions to extend Bayes Theorem to an interval type-2 (IT2) version.\nFirst, we develop an IT2 version of Bayes Theorem that uses a novel and\nconservative method to avoid potential inconsistencies in the input IT2 MFs\nthat otherwise might produce invalid output results. We then describe a novel\nand flexible algorithm for encoding SME-provided intervals into IT2 fuzzy\nmembership functions (MFs), which we can use to specify the input probabilities\nin Bayes Theorem. Our algorithm generalizes and extends previous work on this\nproblem that primarily addressed the encoding of intervals into word MFs for\nComputing with Words applications.", "AI": {"tldr": "Extends Bayes Theorem to interval type-2 fuzzy logic to handle uncertain input probabilities from experts, developing a conservative method to avoid inconsistencies and a flexible algorithm for encoding interval estimates.", "motivation": "Traditional Bayesian inference assumes precise input values, but real-world applications often rely on interval range estimates from subject matter experts, making precise inputs unrealistic.", "method": "Develops an IT2 version of Bayes Theorem with a conservative method to avoid input inconsistencies, and creates a novel algorithm for encoding SME-provided intervals into IT2 fuzzy membership functions.", "result": "Provides a framework that can handle uncertain interval inputs in Bayesian analysis while maintaining validity of output results.", "conclusion": "The proposed IT2 extension of Bayes Theorem enables more realistic Bayesian inference using uncertain expert estimates while preventing invalid outputs through conservative consistency maintenance."}}
{"id": "2509.09231", "pdf": "https://arxiv.org/pdf/2509.09231", "abs": "https://arxiv.org/abs/2509.09231", "authors": ["Rejeb Hadiji", "Jongmin Han"], "title": "On the Convergence of Solutions for the Ginzburg-Landau Equation and System", "categories": ["math.AP", "35B40, 35J60, 35Q60"], "comment": null, "summary": "Let $(u_\\varepsilon)$ be a family of solutions of the Ginzburg--Landau\nequation with boundary condition $u_\\varepsilon = g$ on $\\partial \\Omega$ and\nof degree $0$. Let $u_0$ denote the harmonic map satisfying $u_0 = g$ on\n$\\partial \\Omega$. We show that, if there exists a constant $C_1 > 0$ such that\nfor $\\varepsilon$ sufficiently small we have $\\frac{1}{2} \\int_\\Omega |\\nabla\nu_\\ve|^2 dx \\leq C_1 \\leq \\frac{1}{2} \\int_\\Omega |\\nabla u_0|^2 dx,$ then $C_1\n= \\frac{1}{2} \\int_\\Omega |\\nabla u_0|^2 dx$ and\n  $u_\\ve ~\\to ~ u_0 \\qin H^1(\\Om)$.\n  We also prove that if there is a constant $C_2$ such that for $\\ve$ small\nenough we have $ \\frac12 \\int_\\Om |\\nabla u_\\ve|^2 dx \\geq C_2 > \\frac12\n\\int_\\Om |\\nabla u_0|^2 dx,$ then $|u_{\\ve}|$ does not converge uniformly to\n$1$ on $\\overline{\\Om} $. We obtain analogous results for both symmetric and\nnon-symmetric two-component Ginzburg--Landau systems.", "AI": {"tldr": "The paper analyzes convergence properties of Ginzburg-Landau equation solutions. If energy is bounded above by harmonic map energy, solutions converge to harmonic map. If energy exceeds harmonic map energy, uniform convergence to 1 fails.", "motivation": "To understand the convergence behavior of Ginzburg-Landau equation solutions to harmonic maps and establish energy threshold conditions for different convergence patterns.", "method": "Mathematical analysis of Ginzburg-Landau equation solutions with boundary conditions, comparing their energy with harmonic map energy, and proving convergence results using energy estimates.", "result": "Proved that if solution energy is bounded above by harmonic map energy, convergence to harmonic map occurs; if energy exceeds harmonic map energy, uniform convergence to 1 fails. Results extended to symmetric and non-symmetric two-component systems.", "conclusion": "The energy threshold of the harmonic map serves as a critical value determining convergence behavior in Ginzburg-Landau systems, with implications for both single and multi-component cases."}}
{"id": "2509.09132", "pdf": "https://arxiv.org/pdf/2509.09132", "abs": "https://arxiv.org/abs/2509.09132", "authors": ["Jingyu Yang", "Shingyu Leung", "Jianliang Qian", "Hao Liu"], "title": "Fast Operator-Splitting Methods for Nonlinear Elliptic Equations", "categories": ["math.NA", "cs.NA", "65N30, 65M60"], "comment": null, "summary": "Nonlinear elliptic problems arise in many fields, including plasma physics,\nastrophysics, and optimal transport. In this article, we propose a novel\noperator-splitting/finite element method for solving such problems. We begin by\nintroducing an auxiliary function in a new way for a semilinear elliptic\npartial differential equation, leading to the development of a convergent\noperator-splitting/finite element scheme for this equation. The algorithm is\nthen extended to fully nonlinear elliptic equations of the Monge-Amp\\`ere type,\nincluding the Dirichlet Monge-Amp\\`ere equation and Pucci's equation. This is\nachieved by reformulating the fully nonlinear equations into forms analogous to\nthe semilinear case, enabling the application of the proposed splitting\nalgorithm. In our implementation, a mixed finite element method is used to\napproximate both the solution and its Hessian matrix. Numerical experiments\nshow that the proposed method outperforms existing approaches in efficiency and\naccuracy, and can be readily applied to problems defined on domains with curved\nboundaries.", "AI": {"tldr": "Novel operator-splitting/finite element method for solving nonlinear elliptic problems, including semilinear and fully nonlinear equations like Monge-Amp\u00e8re and Pucci's equations, with improved efficiency and accuracy.", "motivation": "Nonlinear elliptic problems are important in fields like plasma physics, astrophysics, and optimal transport, but existing solution methods need improvement in efficiency and accuracy.", "method": "Introduces auxiliary function for semilinear elliptic PDEs, develops convergent operator-splitting/finite element scheme, extends to fully nonlinear equations via reformulation, uses mixed finite element method to approximate solution and Hessian matrix.", "result": "Numerical experiments show the proposed method outperforms existing approaches in both efficiency and accuracy, and works well on domains with curved boundaries.", "conclusion": "The developed operator-splitting/finite element method provides an effective and accurate approach for solving various nonlinear elliptic problems, with practical advantages for complex domain geometries."}}
{"id": "2509.09057", "pdf": "https://arxiv.org/pdf/2509.09057", "abs": "https://arxiv.org/abs/2509.09057", "authors": ["Yici Zhong", "Elias R. Most"], "title": "Unraveling the emission mechanism powering long period radio transients from interacting white dwarf binaries via kinetic plasma simulations", "categories": ["astro-ph.HE", "astro-ph.SR", "physics.plasm-ph"], "comment": "13 pages, 5 figures", "summary": "Recent observations of long period radio transients, such as GLEAM-X J0704-37\nand ILTJ1101 + 5521, have revealed a previously unrecognized population of\ngalactic radio transient sources associated with white dwarf - M dwarf\nbinaries. It is an open question how to produce coherent radio emission in\nthese systems, though a model driven by binary interaction seems likely given\nthe nature and correlation of the emission with the binaries' orbital period.\nUsing kinetic plasma simulations, we demonstrate that the relativistic electron\ncyclotron maser instability (ECMI) is a viable mechanism for generating radio\npulses in white dwarf - M dwarf systems, akin to planetary radio emission, such\nas that from the Jupiter-Io system. We quantify the relativistic ECMI in the\nnonlinear regime under conditions relevant for white dwarf radio emission for\nthe first time. Our simulations demonstrate that the ECMI can intrinsically\nproduce partially linearly polarized emission relevant to explaining the\nobserved emission spectrum of the two galactic sources, though the precise\ndetails will depend on the plasma composition. Our work paves the way for a\nsystematic and fully nonlinear computational modeling of radio emission from\ninteracting white dwarf sources.", "AI": {"tldr": "The paper demonstrates that the relativistic electron cyclotron maser instability (ECMI) can produce coherent radio emission in white dwarf-M dwarf binary systems, explaining recent observations of long-period radio transients.", "motivation": "Recent discoveries of long-period radio transients from white dwarf-M dwarf binaries require explanation of how coherent radio emission is produced in these systems, similar to planetary radio emission mechanisms.", "method": "Using kinetic plasma simulations, the authors quantified the relativistic ECMI in the nonlinear regime under conditions relevant for white dwarf radio emission for the first time.", "result": "Simulations show ECMI can intrinsically produce partially linearly polarized emission that explains the observed emission spectrum of galactic sources like GLEAM-X J0704-37 and ILTJ1101+5521.", "conclusion": "This work establishes ECMI as a viable mechanism for radio emission in white dwarf binary systems and enables systematic nonlinear computational modeling of radio emission from interacting white dwarf sources."}}
{"id": "2509.08930", "pdf": "https://arxiv.org/pdf/2509.08930", "abs": "https://arxiv.org/abs/2509.08930", "authors": ["Malte Mederacke", "Chengyou Yu", "Roman Vetter", "Dagmar Iber"], "title": "Simulating Organogenesis in COMSOL Multiphysics: Tissue Patterning with Directed Cell Migration", "categories": ["physics.bio-ph", "nlin.PS", "physics.comp-ph"], "comment": "7 pages, 5 figures, 1 table. COMSOL Conference 2025", "summary": "We present a COMSOL Multiphysics implementation of a continuum model for\ndirected cell migration, a key mechanism underlying tissue self-organization\nand morphogenesis. The model is formulated as a partial integro-differential\nequation (PIDE), combining random motility with non-local, density-dependent\nguidance cues to capture phenomena such as cell sorting and aggregation. Our\nframework supports simulations in one, two, and three dimensions, with both\nzero-flux and periodic boundary conditions, and can be reformulated in a\nLagrangian setting to efficiently handle tissue growth and domain deformation.\nWe demonstrate that COMSOL Multiphysics enables a flexible and accessible\nimplementation of PIDEs, providing a generalizable platform for studying\ncollective cell behavior and pattern formation in complex biological contexts.", "AI": {"tldr": "COMSOL Multiphysics implementation of a continuum model for directed cell migration using partial integro-differential equations to study tissue self-organization and pattern formation.", "motivation": "To provide a flexible and accessible computational framework for studying directed cell migration, which is crucial for understanding tissue self-organization and morphogenesis processes.", "method": "Developed a continuum model formulated as a partial integro-differential equation (PIDE) combining random motility with non-local, density-dependent guidance cues. Implemented in COMSOL Multiphysics with support for 1D, 2D, and 3D simulations, various boundary conditions, and Lagrangian formulation for tissue growth.", "result": "Successfully implemented a generalizable platform that can capture phenomena like cell sorting and aggregation, demonstrating COMSOL's capability for flexible PIDE implementation.", "conclusion": "COMSOL Multiphysics provides an effective and accessible environment for implementing complex PIDEs, offering a powerful tool for studying collective cell behavior and pattern formation in biological systems."}}
{"id": "2509.09237", "pdf": "https://arxiv.org/pdf/2509.09237", "abs": "https://arxiv.org/abs/2509.09237", "authors": ["Giacomo Bertazzoni", "Elisa Davoli", "Samuele Ricco`", "Elvira Zappale"], "title": "Functions of bounded Musielak-Orlicz-type deformation and anisotropic Total Generalized Variation for image-denoising problems", "categories": ["math.AP"], "comment": null, "summary": "In the first part of this paper we introduce the space of bounded deformation\nfields with generalized Orlicz growth. We establish their main properties,\nprovide a modular representation, and characterize a decomposition of the\nmodular into an absolutely continuous part and a singular part weighted via a\nrecession function. A further analysis in the variable exponent case is also\nprovided. The second part of the paper contains a notion of Musielak-Orlicz\nanisotropic Total Generalized Variation. We establish a duality representation,\nand show well-posedness of the corresponding image reconstruction problem.", "AI": {"tldr": "This paper introduces bounded deformation fields with generalized Orlicz growth, analyzes their properties and decomposition, and develops Musielak-Orlicz anisotropic Total Generalized Variation for image reconstruction.", "motivation": "To extend deformation field analysis to generalized Orlicz growth spaces and develop advanced variational methods for image reconstruction problems.", "method": "Introduces bounded deformation fields with generalized Orlicz growth, establishes modular representation and decomposition, then develops Musielak-Orlicz anisotropic Total Generalized Variation with duality representation.", "result": "Established main properties of bounded deformation fields, provided modular decomposition, and showed well-posedness of the image reconstruction problem using the new variational formulation.", "conclusion": "The framework successfully extends deformation analysis to generalized Orlicz spaces and provides effective tools for anisotropic image reconstruction with theoretical guarantees."}}
{"id": "2509.09139", "pdf": "https://arxiv.org/pdf/2509.09139", "abs": "https://arxiv.org/abs/2509.09139", "authors": ["Zijian Zhang", "Rui Hong", "Xuesong Chen", "Shuting Cai"], "title": "Hybrid-Precision Block-Jacobi Preconditioned GMRES Solver for Linear System in Circuit Simulation", "categories": ["math.NA", "cs.NA"], "comment": "18 pages, 8 figures", "summary": "As integrated circuits become increasingly complex, the demand for efficient\nand accurate simulation solvers continues to rise. Traditional solvers often\nstruggle with large-scale sparse systems, leading to prolonged simulation times\nand reduced accuracy. In this paper, a hybrid-precision block-Jacobi\npreconditioned GMRES solver is proposed to solve the large sparse system in\ncircuit simulation. The proposed method capitalizes on the structural sparsity\nand block properties of circuit matrices, employing a novel hybrid-precision\nstrategy that applies single-precision arithmetic for computationally intensive\ntasks and double-precision arithmetic for critical accuracy-sensitive\ncomputations. Additionally, we use the graph partitioning tools to assist in\ngenerating preconditioners, ensuring an optimized preconditioning process. For\nlarge-scale problems, we adopt the restart strategy to increase the\ncomputational efficiency. Through rigorous mathematical reasoning, the\nconvergence and error analysis of the proposed method are carried out.\nNumerical experiments on various benchmark matrices demonstrate that our\napproach significantly outperforms existing solvers, including SuperLU, KLU,\nand SFLU, in terms of both preconditioning and GMRES runtime. The proposed\nhybrid-precision preconditioner effectively improves spectral clustering,\nleading to faster solutions.", "AI": {"tldr": "Hybrid-precision block-Jacobi preconditioned GMRES solver for efficient circuit simulation of large sparse systems", "motivation": "Traditional solvers struggle with large-scale sparse systems in circuit simulation, leading to long simulation times and reduced accuracy as integrated circuits become more complex", "method": "Uses hybrid-precision strategy (single-precision for intensive tasks, double-precision for accuracy-sensitive computations), graph partitioning for preconditioner generation, and restart strategy for large-scale problems", "result": "Significantly outperforms existing solvers (SuperLU, KLU, SFLU) in both preconditioning and GMRES runtime, with improved spectral clustering for faster solutions", "conclusion": "The proposed hybrid-precision preconditioner effectively addresses large sparse system challenges in circuit simulation with proven convergence and superior performance"}}
{"id": "2509.09487", "pdf": "https://arxiv.org/pdf/2509.09487", "abs": "https://arxiv.org/abs/2509.09487", "authors": ["Snehanshu Maiti", "Shishir Biswas", "Rajaraman Ganesh"], "title": "Vorticity Packing Effects on Turbulent Transport in Decaying 2D Incompressible Navier-Stokes Fluids", "categories": ["physics.flu-dyn", "nlin.CD", "physics.comp-ph", "physics.plasm-ph"], "comment": null, "summary": "This paper investigates the role of initial vorticity packing fractions on\nthe transport properties of decaying incompressible two-dimensional\nNavier-Stokes turbulence at very high Reynolds numbers and spatial resolutions.\nTurbulence is initiated via the Kelvin-Helmholtz instability and evolves\nthrough nonlinear inverse energy cascades, forming large-scale coherent\nstructures that dominate the flow over long eddy turnover times. The initial\nvorticity packing fraction and circulation direction lead to qualitatively\ndistinct turbulence dynamics and transport behaviors. Tracer particle\ntrajectories are computed in the fluid field obtained using the Eulerian\nframework, with transport and mixing quantified using statistical measures such\nas absolute dispersion, position probability distribution functions (PDFs), and\nvelocity PDFs. In the early stages, the onset of turbulence is primarily\ngoverned by the instability growth rate, which increases with vorticity packing\nfraction. As the flow evolves, transport exhibits a range of\nbehaviors-subdiffusive, diffusive, or superdiffusive-and transitions between\nanisotropic and isotropic regimes, depending on the initial vorticity packing,\nflow structure, and stage of evolution. At later times, transport is dominated\nby the motion of large-scale coherent vortices, whose dynamics are also\ninfluenced by the initial vorticity packing ranging from subdiffusive trapping\nrotational motion and random walks, and L\\'evy flight-like events. These\nfindings offer insights into transport in quasi-2D systems-ranging from\nlaboratory-scale flows to geophysical phenomena and astrophysical\nstructures-through analogies with 2D Navier-Stokes turbulence.", "AI": {"tldr": "Study examines how initial vorticity packing fractions affect transport in 2D turbulence, showing distinct transport behaviors (subdiffusive, diffusive, superdiffusive) depending on initial conditions and flow evolution stages.", "motivation": "To understand how initial vorticity conditions influence transport properties in high-Reynolds-number 2D turbulence, with applications to laboratory flows, geophysical phenomena, and astrophysical structures.", "method": "Used Kelvin-Helmholtz instability to initiate turbulence, computed tracer particle trajectories in Eulerian fluid fields, and quantified transport using statistical measures (absolute dispersion, position PDFs, velocity PDFs).", "result": "Initial vorticity packing fraction governs instability growth rate early on, while later transport exhibits various behaviors (subdiffusive to superdiffusive) and transitions between anisotropic/isotropic regimes, dominated by large-scale coherent vortices.", "conclusion": "Initial vorticity conditions significantly impact turbulence dynamics and transport, with coherent vortex motion driving transport at late times, providing insights for quasi-2D systems across multiple physical scales."}}
{"id": "2509.08996", "pdf": "https://arxiv.org/pdf/2509.08996", "abs": "https://arxiv.org/abs/2509.08996", "authors": ["Sun Wenming"], "title": "Monte Carlo Simulation of Spallation and Fission Fragment Distributions for ADS-Related Nuclear Reactions", "categories": ["physics.gen-ph"], "comment": "A total of 9 pages, 28 images, and 4 tables", "summary": "Monte Carlo simulations with the CRISP code were conducted to study\nspallation and fission fragment distributions induced by intermediate- and\nhigh-energy protons and photons on actinide and pre-actinide nuclei. The model\naccounts for intranuclear cascade, pre-equilibrium, and evaporation-fission\ncompetition, enabling consistent treatment of both residues and fission\nproducts. Comparisons with experimental data show good agreement in mass and\ncharge distributions, with minor deviations for light fragments. The results\nhighlight the reliability of Monte Carlo approaches for predicting residual\nnuclei and fragment yields under accelerator-driven system (ADS) conditions.\nThis work provides nuclear data relevant to ADS design, safety, and\ntransmutation analysis", "AI": {"tldr": "Monte Carlo simulations using CRISP code successfully model spallation and fission fragment distributions from proton/photon interactions with actinide nuclei, showing good agreement with experimental data for ADS applications.", "motivation": "To provide reliable nuclear data for accelerator-driven system (ADS) design, safety analysis, and transmutation studies by accurately predicting residual nuclei and fission fragment yields.", "method": "Used CRISP code Monte Carlo simulations incorporating intranuclear cascade, pre-equilibrium, and evaporation-fission competition models to study spallation and fission from intermediate/high-energy protons and photons on actinide/pre-actinide nuclei.", "result": "Good agreement with experimental data for mass and charge distributions, with minor deviations only for light fragments. Demonstrates reliability of Monte Carlo approaches for predicting fragment yields under ADS conditions.", "conclusion": "The Monte Carlo approach with CRISP code provides accurate nuclear data relevant to ADS applications, supporting design, safety, and transmutation analysis with consistent treatment of both residues and fission products."}}
{"id": "2509.09276", "pdf": "https://arxiv.org/pdf/2509.09276", "abs": "https://arxiv.org/abs/2509.09276", "authors": ["Francis Filbet", "Yanzhi Gui", "Ling-Bing He"], "title": "Numerical analysis of the homogeneous Landau equation: approximation, error estimates and simulation", "categories": ["math.AP", "cs.NA", "math.NA"], "comment": null, "summary": "We construct a numerical solution to the spatially homogeneous Landau\nequation with Coulomb potential on a domain $D_L$ with N retained Fourier\nmodes. By deriving an explicit error estimate in terms of $L$ and $N$, we\ndemonstrate that for any prescribed error tolerance and fixed time interval\n$[0, T ]$, there exist choices of $D_L$ and $N$ satisfying explicit conditions\nsuch that the error between the numerical and exact solutions is below the\ntolerance. Specifically, the estimate shows that sufficiently large $L$ and $N$\n(depending on initial data parameters and $T$) can reduce the error to any\ndesired level. Numerical simulations based on this construction are also\npresented. The results in particular demonstrate the mathematical validity of\nthe spectral method proposed in the referenced literature.", "AI": {"tldr": "Numerical solution for Landau equation with Coulomb potential using spectral method, with explicit error estimates showing convergence to exact solution.", "motivation": "To provide rigorous mathematical validation for spectral methods in solving the spatially homogeneous Landau equation with Coulomb potential, ensuring numerical solutions can achieve arbitrary precision.", "method": "Construct numerical solution on domain D_L with N Fourier modes, derive explicit error estimates in terms of L (domain size) and N (number of modes), and perform numerical simulations to validate the approach.", "result": "Proved that for any error tolerance and fixed time interval, specific choices of D_L and N exist to keep error below tolerance. Numerical simulations confirm the theoretical error estimates and validate the spectral method.", "conclusion": "The spectral method is mathematically valid for solving Landau equation with Coulomb potential, with explicit conditions provided for achieving desired accuracy through appropriate domain size and mode selection."}}
{"id": "2509.09236", "pdf": "https://arxiv.org/pdf/2509.09236", "abs": "https://arxiv.org/abs/2509.09236", "authors": ["Guilherme Henrique Teixeira", "Nepomuk Krenn", "Peter Gangl", "Benjamin Marussig"], "title": "Isogeometric Topology Optimization Based on Topological Derivatives", "categories": ["math.NA", "cs.CE", "cs.NA", "math.OC"], "comment": "19 pages, 11 figures, pre-print,", "summary": "Topology optimization is a valuable tool in engineering, facilitating the\ndesign of optimized structures. However, topological changes often require a\nremeshing step, which can become challenging. In this work, we propose an\nisogeometric approach to topology optimization driven by topological\nderivatives. The combination of a level-set method together with an immersed\nisogeometric framework allows seamless geometry updates without the necessity\nof remeshing. At the same time, topological derivatives provide topological\nmodifications without the need to define initial holes [7]. We investigate the\ninfluence of higher-degree basis functions in both the level-set representation\nand the approximation of the solution. Two numerical examples demonstrate the\nproposed approach, showing that employing higher-degree basis functions for\napproximating the solution improves accuracy, while linear basis functions\nremain sufficient for the level-set function representation.", "AI": {"tldr": "Isogeometric topology optimization using topological derivatives and level-set method that eliminates remeshing requirements and initial hole definitions, with investigation of higher-degree basis functions.", "motivation": "Traditional topology optimization often requires challenging remeshing steps during topological changes, and typically needs initial holes defined for optimization.", "method": "Combines level-set method with immersed isogeometric framework driven by topological derivatives, allowing geometry updates without remeshing and topological modifications without initial holes.", "result": "Higher-degree basis functions improve solution accuracy, while linear basis functions remain sufficient for level-set representation. Two numerical examples demonstrate the approach.", "conclusion": "The proposed isogeometric approach successfully eliminates remeshing requirements and initial hole definitions, with optimal performance achieved using higher-degree basis for solution approximation and linear basis for level-set representation."}}
{"id": "2509.09335", "pdf": "https://arxiv.org/pdf/2509.09335", "abs": "https://arxiv.org/abs/2509.09335", "authors": ["Manil T. Mohan"], "title": "Well-posedness of stationary 2D and 3D convective Brinkman-Forchheimer extended Darcy Hemivariational inequalities", "categories": ["math.AP"], "comment": null, "summary": "This study addresses the well-posedness of a hemivariational inequality\nderived from the convective Brinkman-Forchheimer extended Darcy (CBFeD) model\nin both two and three dimensions. The CBFeD model describes the behavior of\nincompressible viscous fluid flow through a porous medium, incorporating the\neffects of convection, damping, and nonlinear resistance. The mathematical\nframework captures steady-state flow conditions under a no-slip boundary\nassumption, with a non-monotone boundary condition that links the total fluid\npressure and the velocity's normal component through a Clarke subdifferential\nformulation. To facilitate the analysis, we introduce an auxiliary\nhemivariational inequality resembling a nonlinear Stokes-type problem with\ndamping and pumping terms, which serves as a foundational tool in establishing\nthe existence and uniqueness of weak solutions for the CBFeD model. The\nanalytical strategy integrates techniques from convex minimization theory with\nfixed-point methods, specifically employing either the Banach contraction\nmapping principle or Schauder's fixed point theorem. The Banach-based approach,\nin particular, leads to a practical iterative algorithm that solves the\noriginal nonlinear hemivariational inequality by sequentially solving\nStokes-type problems, ensuring convergence of the solution sequence.\nAdditionally, we derive equivalent variational formulations in terms of\nminimization problems. These formulations lay the groundwork for the design of\nefficient and stable numerical schemes tailored to simulate flows governed by\nthe CBFeD model.", "AI": {"tldr": "Analysis of well-posedness for convective Brinkman-Forchheimer extended Darcy model hemivariational inequality in 2D/3D, with existence/uniqueness proofs and numerical scheme foundations.", "motivation": "To establish mathematical foundations for the CBFeD model describing incompressible viscous fluid flow through porous media with convection, damping, and nonlinear resistance effects under steady-state conditions.", "method": "Introduces auxiliary hemivariational inequality resembling nonlinear Stokes-type problem; integrates convex minimization theory with fixed-point methods (Banach contraction or Schauder's theorem); develops iterative algorithm solving Stokes-type problems sequentially.", "result": "Establishes existence and uniqueness of weak solutions for CBFeD model; provides practical iterative algorithm with guaranteed convergence; derives equivalent variational formulations for numerical schemes.", "conclusion": "Successfully proves well-posedness of CBFeD hemivariational inequality and provides mathematical framework for developing efficient numerical simulation schemes for porous media flows."}}
{"id": "2509.09274", "pdf": "https://arxiv.org/pdf/2509.09274", "abs": "https://arxiv.org/abs/2509.09274", "authors": ["Taiyuan Liu", "Yaozhong Hu", "Siqing Gan"], "title": "Long time strong convergence analysis of one-step methods for McKean-Vlasov SDEs with superlinear growth coefficients", "categories": ["math.NA", "cs.NA"], "comment": null, "summary": "This paper presents a strong convergence rate analysis of general\ndiscretization approximations for McKean-Vlasov SDEs with super-linear growth\ncoefficients over infinite time horizon. Under some specified non-globally\nLipschitz conditions, we derive the propagation of chaos, and the mean-square\nconvergence rate over infinite time horizon for general one-step time\ndiscretization schemes for the underlying Mckean-Vlasov SDEs. As an application\nof the general result it is obtained the mean-square convergence rate over\ninfinite time horizon for two numerical schemes: the projected Euler scheme and\nthe backward Euler scheme for Mckean-Vlasov SDEs in non-globally Lipschitz\nsettings. Numerical experiments are provided to validate the theoretical\nfindings.", "AI": {"tldr": "Strong convergence rate analysis for discretization approximations of McKean-Vlasov SDEs with super-linear growth coefficients over infinite time horizon under non-globally Lipschitz conditions.", "motivation": "To analyze the convergence properties of numerical schemes for McKean-Vlasov SDEs with super-linear growth coefficients, which are challenging due to the non-globally Lipschitz conditions and infinite time horizon.", "method": "Derived propagation of chaos and mean-square convergence rate for general one-step time discretization schemes. Applied the general result to two specific schemes: projected Euler scheme and backward Euler scheme.", "result": "Obtained mean-square convergence rate over infinite time horizon for the numerical schemes. Numerical experiments validated the theoretical findings.", "conclusion": "The paper provides rigorous convergence analysis for discretization approximations of McKean-Vlasov SDEs with super-linear growth, establishing theoretical foundations and practical validation through numerical experiments."}}
{"id": "2509.09235", "pdf": "https://arxiv.org/pdf/2509.09235", "abs": "https://arxiv.org/abs/2509.09235", "authors": ["Sarah C. Irvine", "Christian Lucas", "Diana Kr\u00fcger", "Bianca Guedert", "Julian Moosmann", "Berit Zeller-Plumhoff"], "title": "Virtual staining for 3D X-ray histology of bone implants", "categories": ["eess.IV", "cs.AI", "cs.CV", "physics.comp-ph", "q-bio.QM"], "comment": null, "summary": "Three-dimensional X-ray histology techniques offer a non-invasive alternative\nto conventional 2D histology, enabling volumetric imaging of biological tissues\nwithout the need for physical sectioning or chemical staining. However, the\ninherent greyscale image contrast of X-ray tomography limits its biochemical\nspecificity compared to traditional histological stains. Within digital\npathology, deep learning-based virtual staining has demonstrated utility in\nsimulating stained appearances from label-free optical images. In this study,\nwe extend virtual staining to the X-ray domain by applying cross-modality image\ntranslation to generate artificially stained slices from\nsynchrotron-radiation-based micro-CT scans. Using over 50 co-registered image\npairs of micro-CT and toluidine blue-stained histology from bone-implant\nsamples, we trained a modified CycleGAN network tailored for limited paired\ndata. Whole slide histology images were downsampled to match the voxel size of\nthe CT data, with on-the-fly data augmentation for patch-based training. The\nmodel incorporates pixelwise supervision and greyscale consistency terms,\nproducing histologically realistic colour outputs while preserving\nhigh-resolution structural detail. Our method outperformed Pix2Pix and standard\nCycleGAN baselines across SSIM, PSNR, and LPIPS metrics. Once trained, the\nmodel can be applied to full CT volumes to generate virtually stained 3D\ndatasets, enhancing interpretability without additional sample preparation.\nWhile features such as new bone formation were able to be reproduced, some\nvariability in the depiction of implant degradation layers highlights the need\nfor further training data and refinement. This work introduces virtual staining\nto 3D X-ray imaging and offers a scalable route for chemically informative,\nlabel-free tissue characterisation in biomedical research.", "AI": {"tldr": "This paper introduces virtual staining for 3D X-ray histology using deep learning to convert greyscale micro-CT scans into artificially stained histological images, enabling volumetric tissue analysis without physical sectioning or chemical staining.", "motivation": "3D X-ray histology provides non-invasive volumetric imaging but lacks biochemical specificity compared to traditional histological stains. The researchers aim to extend virtual staining techniques from optical to X-ray domain to enhance interpretability of micro-CT data.", "method": "Used over 50 co-registered micro-CT and toluidine blue-stained histology image pairs to train a modified CycleGAN network with pixelwise supervision and greyscale consistency terms. Applied on-the-fly data augmentation for patch-based training and downsampled histology images to match CT voxel size.", "result": "The modified CycleGAN outperformed Pix2Pix and standard CycleGAN baselines across SSIM, PSNR, and LPIPS metrics. The model produces histologically realistic color outputs while preserving high-resolution structural detail and can generate virtually stained 3D datasets from full CT volumes.", "conclusion": "This work successfully introduces virtual staining to 3D X-ray imaging, providing a scalable route for chemically informative, label-free tissue characterization. While features like new bone formation were reproduced, some variability in implant degradation depiction indicates need for more training data and refinement."}}
{"id": "2509.09409", "pdf": "https://arxiv.org/pdf/2509.09409", "abs": "https://arxiv.org/abs/2509.09409", "authors": ["Coleman Hines", "James Kolesar", "Peter McGrath"], "title": "New Homogeneous Solutions for the One-Phase Free Boundary Problem", "categories": ["math.AP", "math.DG", "35R35"], "comment": null, "summary": "For each sufficiently large integer $k$, we construct a domain in the round\n$2$-sphere with $k$ boundary components which is the link of a cone in\n$\\mathbb{R}^3$ admitting a homogeneous solution to the one-phase free boundary\nproblem. This answers a question of Jerison-Kamburov, and also disproves a\nconjecture of Souam left open in earlier work. The method exploits a new\nconnection with minimal surfaces, which we also use to construct an infinite\nfamily of homogeneous solutions in dimension four.", "AI": {"tldr": "Constructed domains in 2-sphere with k boundary components that serve as links of cones in R^3 admitting homogeneous solutions to one-phase free boundary problem, answering Jerison-Kamburov's question and disproving Souam's conjecture.", "motivation": "To answer Jerison-Kamburov's question about existence of domains with multiple boundary components admitting homogeneous solutions, and to test Souam's conjecture about such solutions.", "method": "Exploited new connection with minimal surfaces to construct the domains and solutions.", "result": "Successfully constructed domains with k boundary components for sufficiently large k, and also constructed infinite family of homogeneous solutions in dimension four.", "conclusion": "The work provides counterexamples to Souam's conjecture and answers Jerison-Kamburov's question, demonstrating the power of connecting free boundary problems with minimal surface theory."}}
{"id": "2509.09287", "pdf": "https://arxiv.org/pdf/2509.09287", "abs": "https://arxiv.org/abs/2509.09287", "authors": ["Wasim Akram", "Manil T. Mohan"], "title": "Optimal Control of a Hemivariational Inequality of Stationary Convective Brinkman-Forchheimer Extended Darcy equations with Numerical Approximation", "categories": ["math.NA", "cs.NA"], "comment": null, "summary": "This paper studies an optimal control problem for a stationary convective\nBrinkman-Forchheimer extended Darcy (CBFeD) hemivariational inequality in two\nand three dimensions, subject to control constraints, and develops its\nnumerical approximation. The hemivariational inequality provides the weak\nformulation of a stationary incompressible fluid flow through a porous medium,\ngoverned by the CBFeD equations, which account for convection, damping, and\nnonlinear resistance effects. The problem incorporates a non-leak boundary\ncondition and a subdifferential friction-type condition. We first analyze the\nstability of solutions with respect to perturbations in the external force\ndensity and the superpotential. Next, we prove the existence of a solution to\nthe optimal control problem, where the external force density acts as the\ncontrol variable. We then propose a numerical scheme for solving the optimal\ncontrol problem and establish its convergence. For concreteness, the numerical\nmethod is implemented using finite element discretization. Finally, we provide\nsome numerical examples to validate the theory developed.", "AI": {"tldr": "Optimal control analysis for stationary convective Brinkman-Forchheimer extended Darcy hemivariational inequality in 2D/3D with control constraints and numerical approximation.", "motivation": "Study fluid flow through porous media governed by CBFeD equations accounting for convection, damping, and nonlinear resistance effects with non-leak boundary and friction conditions.", "method": "Analyze solution stability, prove existence of optimal control solutions, develop numerical scheme with finite element discretization, and validate with numerical examples.", "result": "Established stability of solutions under perturbations, proved existence of optimal control solutions, developed convergent numerical scheme, and validated theory through numerical implementation.", "conclusion": "Successfully developed and validated a comprehensive framework for optimal control of CBFeD hemivariational inequalities with practical numerical implementation for porous media flow problems."}}
{"id": "2509.09253", "pdf": "https://arxiv.org/pdf/2509.09253", "abs": "https://arxiv.org/abs/2509.09253", "authors": ["Shuai Zhang", "Mengqi Wang", "Tiantian Zhang"], "title": "Electronic order induced symmetry breaking in lattice dynamics of Co$_3$Sn$_2$S$_2$", "categories": ["cond-mat.mtrl-sci", "physics.comp-ph"], "comment": null, "summary": "Based on the molecular Berry curvature (MBC) framework, we develop an\n\\textit{ab initio} algorithm to capture the quantitative effects of magnetic\norder on lattice dynamics. Using the ferromagnetic Weyl semimetal\nCo$_3$Sn$_2$S$_2$ as a prototype, we show that electronic-order-driven phonon\nsymmetry breaking requires spin-orbit coupling (SOC) and leads to an MBC term\nthat breaks both time-reversal ($\\mathcal{T}$) and mirror symmetries. We\ndemonstrate that mirror-symmetry breaking is essential to account for the\nexperimentally observed phonon splitting, $\\mathcal{T}$-breaking alone is\ninsufficient. The MBC is widely distributed across the Brillouin zone, giving\nrise to significant off-$\\Gamma$ effects. Our results agree well with\nexperiments and establish a framework for predicting large phonon magnetism in\nmagnetic materials with strong spin-orbit coupling and electron-phonon\ncoupling. This work also suggests new avenues for controlling non-reciprocal\nphonon transport.", "AI": {"tldr": "Ab initio algorithm using molecular Berry curvature framework to quantify magnetic order effects on lattice dynamics in ferromagnetic Weyl semimetals like Co3Sn2S2, showing spin-orbit coupling is essential for phonon symmetry breaking.", "motivation": "To understand and quantify how magnetic order affects lattice dynamics in magnetic materials, particularly focusing on the role of spin-orbit coupling and symmetry breaking in phonon behavior.", "method": "Developed an ab initio algorithm based on molecular Berry curvature framework, applied to ferromagnetic Weyl semimetal Co3Sn2S2, analyzing effects of spin-orbit coupling and symmetry breaking on phonon dynamics.", "result": "Electronic-order-driven phonon symmetry breaking requires SOC and creates MBC term breaking time-reversal and mirror symmetries. Mirror-symmetry breaking is essential for observed phonon splitting. MBC is widely distributed across Brillouin zone with significant off-\u0393 effects. Results match experiments well.", "conclusion": "Establishes framework for predicting large phonon magnetism in magnetic materials with strong SOC and electron-phonon coupling, suggesting new avenues for controlling non-reciprocal phonon transport."}}
{"id": "2509.09410", "pdf": "https://arxiv.org/pdf/2509.09410", "abs": "https://arxiv.org/abs/2509.09410", "authors": ["Weisheng Niu", "Yao Xu", "Jinping Zhuge"], "title": "Optimal convergence rates in multiscale elliptic homogenization", "categories": ["math.AP", "35B27"], "comment": "71 pages", "summary": "This paper is devoted to the quantitative homogenization of multiscale\nelliptic operator $-\\nabla\\cdot A_\\varepsilon \\nabla$, where $A_\\varepsilon(x)\n= A(x/\\varepsilon_1, x/\\varepsilon_2,\\cdots, x/\\varepsilon_n)$, $\\varepsilon =\n(\\varepsilon_1, \\varepsilon_2,\\cdots, \\varepsilon_n) \\in (0,1]^n$ and\n$\\varepsilon_i > \\varepsilon_{i+1}$. We assume that $A(y_1,y_2,\\cdots, y_n)$ is\n1-periodic in each $y_i \\in \\mathbb{R}^d$ and real analytic. Classically, the\nmethod of reiterated homogenization has been applied to study this multiscale\nelliptic operator, which leads to a convergence rate limited by the ratios\n$\\max \\{ \\varepsilon_{i+1}/\\varepsilon_i: 1\\le i\\le n-1\\}$. In the present\npaper, under the assumption of real analytic coefficients, we introduce the\nso-called multiscale correctors and more accurate effective operators, and\nimprove the ratio part of the convergence rate to $\\max \\{\ne^{-c\\varepsilon_{i}/\\varepsilon_{i+1}}: 1\\le i\\le n-1 \\}$. This convergence\nrate is optimal in the sense that $c>0$ cannot be replaced by a larger\nconstant. As a byproduct, the uniform Lipschitz estimate is established under a\nmild double-log scale-separation condition.", "AI": {"tldr": "Improved convergence rates for multiscale elliptic operators with analytic coefficients using novel multiscale correctors and effective operators", "motivation": "Classical reiterated homogenization methods for multiscale elliptic operators have convergence rates limited by scale separation ratios, which this paper aims to improve", "method": "Introduce multiscale correctors and more accurate effective operators under the assumption of real analytic coefficients in the multiscale elliptic operator", "result": "Achieved improved convergence rate from max{\u03b5_{i+1}/\u03b5_i} to max{e^{-c\u03b5_i/\u03b5_{i+1}}}, which is optimal, and established uniform Lipschitz estimates under mild scale-separation conditions", "conclusion": "The proposed method with multiscale correctors provides optimal convergence rates for multiscale homogenization problems with analytic coefficients, significantly improving upon classical approaches"}}
{"id": "2509.09302", "pdf": "https://arxiv.org/pdf/2509.09302", "abs": "https://arxiv.org/abs/2509.09302", "authors": ["Jingtao Zhu", "Yuying Zhao", "Siqing Gan"], "title": "Euler-type methods for Levy-driven McKean-Vlasov SDEs with super-linear coefficients: mean-square error analysis", "categories": ["math.NA", "cs.NA"], "comment": null, "summary": "We develop and analyze a general class of Euler-type numerical schemes for\nLevy-driven McKean-Vlasov stochastic differential equations (SDEs), where the\ndrift, diffusion and jump coefficients grow super-linearly in the state\nvariable. These numerical schemes are derived by incorporating projections or\nnonlinear transformations into the classical Euler method, with the primary\nobjective of establishing moment bounds for the numerical solutions. This class\nof schemes includes the tanh-Euler, tamed-Euler and sine-Euler schemes as\nspecial cases. In contrast to existing approaches that rely on a coercivity\ncondition (e.g., Assumption B-1 in Kumar et al., arXiv:2010.08585), the\nproposed schemes remove such a restrictive assumption. We provide a rigorous\nmean-square convergence analysis and establish that the proposed schemes\nachieve convergence rates arbitrarily close to 1/2 for the interacting particle\nsystems associated with Levy-driven McKean-Vlasov SDEs. Several numerical\nexamples are presented to illustrate the convergence behavior and validate the\ntheoretical results.", "AI": {"tldr": "Euler-type numerical schemes for Levy-driven McKean-Vlasov SDEs with super-linear growth coefficients, removing restrictive coercivity assumptions and achieving near 1/2 convergence rates.", "motivation": "To develop numerical methods for Levy-driven McKean-Vlasov SDEs with super-linear growth coefficients without relying on restrictive coercivity conditions that limit applicability.", "method": "General class of Euler-type schemes incorporating projections or nonlinear transformations (tanh-Euler, tamed-Euler, sine-Euler) to establish moment bounds for numerical solutions.", "result": "Proposed schemes achieve convergence rates arbitrarily close to 1/2 for interacting particle systems, validated through numerical examples.", "conclusion": "The developed Euler-type schemes successfully handle super-linear growth in Levy-driven McKean-Vlasov SDEs without coercivity assumptions, providing rigorous convergence analysis and practical numerical performance."}}
{"id": "2509.09275", "pdf": "https://arxiv.org/pdf/2509.09275", "abs": "https://arxiv.org/abs/2509.09275", "authors": ["Lixing Zhang", "Di luo"], "title": "Neural Transformer Backflow for Solving Momentum-Resolved Ground States of Strongly Correlated Materials", "categories": ["cond-mat.str-el", "physics.comp-ph"], "comment": "11 pages, 6 figures", "summary": "Strongly correlated materials, such as twisted transition-metal\ndichalcogenide homobilayers, host a variety of exotic quantum phases but remain\nnotoriously difficult to solve due to strong interactions. We introduce a\npowerful neural network ansatz, Neural Transformer Backflow (NTB), formulated\nwithin a multi-band projection framework. It naturally enforces momentum\nconservation and enables efficient calculations of momentum-resolved ground\nstates. NTB attains high accuracy on small systems and scales to higher bands\nand larger system sizes far beyond the reach of exact diagonalization. By\nevaluating observables such as the structure factor and momentum distribution,\nwe show that NTB captures diverse correlated states in tMoTe$_2$, including\ncharge density waves, fractional Chern insulators, and anomalous Hall Fermi\nliquids, within a unified framework. Our approach paves the way for\nunderstanding and discovering novel phases of matter in strongly correlated\nmaterials.", "AI": {"tldr": "Neural Transformer Backflow (NTB) enables accurate simulation of strongly correlated materials like twisted MoTe2, capturing diverse quantum phases including charge density waves and fractional Chern insulators.", "motivation": "Strongly correlated materials host exotic quantum phases but are notoriously difficult to solve due to strong interactions, requiring new computational approaches.", "method": "A neural network ansatz called Neural Transformer Backflow (NTB) formulated within a multi-band projection framework that enforces momentum conservation and enables efficient momentum-resolved ground state calculations.", "result": "NTB achieves high accuracy on small systems and scales to larger systems beyond exact diagonalization capabilities, successfully capturing charge density waves, fractional Chern insulators, and anomalous Hall Fermi liquids in twisted MoTe2.", "conclusion": "The NTB approach provides a unified framework for understanding and discovering novel phases of matter in strongly correlated materials, paving the way for future research in this field."}}
{"id": "2509.09518", "pdf": "https://arxiv.org/pdf/2509.09518", "abs": "https://arxiv.org/abs/2509.09518", "authors": ["Andrew Hassell", "Qiuye Jia", "Ethan Sussman", "Andras Vasy"], "title": "Microlocal analysis of the non-relativistic limit of the Klein--Gordon equation: Estimates", "categories": ["math.AP", "math-ph", "math.MP", "Primary 35L05, 35L15. Secondary 35B25, 35Q40, 58J47, 58J50"], "comment": "99 pages, 14 figures", "summary": "This is the more technical half of a two-part work in which we introduce a\nrobust microlocal framework for analyzing the non-relativistic limit of\nrelativistic wave equations with time-dependent coefficients, focusing on the\nKlein--Gordon equation. Two asymptotic regimes in phase space are relevant to\nthe non-relativistic limit: one corresponding to what physicists call\n``natural'' units, in which the PDE is approximable by the free Klein--Gordon\nequation, and a low-frequency regime in which the equation is approximable by\nthe usual Schrodinger equation. Combining the analyses in the two regimes gives\nglobal estimates which are uniform as the speed of light goes to infinity. The\ncompanion paper gives applications. Our main technical tools are three new\npseudodifferential calculi, $\\Psi_{\\natural}$ (a variant of the semiclassical\nscattering calculus), $\\Psi_{\\natural\\mathrm{res}}$, and\n$\\Psi_{\\natural2\\mathrm{res}}$, the latter two of which are created by ``second\nmicrolocalizing'' the first at certain locations. This paper and the companion\npaper can be read in either order, since the latter treats the former as a\nblack box.", "AI": {"tldr": "Robust microlocal framework for non-relativistic limit of relativistic wave equations with time-dependent coefficients, focusing on Klein-Gordon equation.", "motivation": "To analyze the non-relativistic limit of relativistic wave equations and provide global estimates uniform as speed of light approaches infinity.", "method": "Uses three new pseudodifferential calculi: \u03a8\u2099 (semiclassical scattering variant), \u03a8\u2099res, and \u03a8\u20992res, with second microlocalization techniques to handle two asymptotic regimes in phase space.", "result": "Developed framework that combines analyses of two regimes (natural units and low-frequency) to obtain uniform global estimates.", "conclusion": "Provides technical foundation for analyzing non-relativistic limits, with applications detailed in companion paper; both papers can be read independently."}}
{"id": "2509.09362", "pdf": "https://arxiv.org/pdf/2509.09362", "abs": "https://arxiv.org/abs/2509.09362", "authors": ["Hanfei Zhou", "Lei Shi"], "title": "Expressive Power of Deep Networks on Manifolds: Simultaneous Approximation", "categories": ["math.NA", "cs.LG", "cs.NA", "stat.ML"], "comment": null, "summary": "A key challenge in scientific machine learning is solving partial\ndifferential equations (PDEs) on complex domains, where the curved geometry\ncomplicates the approximation of functions and their derivatives required by\ndifferential operators. This paper establishes the first simultaneous\napproximation theory for deep neural networks on manifolds. We prove that a\nconstant-depth $\\mathrm{ReLU}^{k-1}$ network with bounded weights--a property\nthat plays a crucial role in controlling generalization error--can approximate\nany function in the Sobolev space $\\mathcal{W}_p^{k}(\\mathcal{M}^d)$ to an\nerror of $\\varepsilon$ in the $\\mathcal{W}_p^{s}(\\mathcal{M}^d)$ norm, for\n$k\\geq 3$ and $s<k$, using $\\mathcal{O}(\\varepsilon^{-d/(k-s)})$ nonzero\nparameters, a rate that overcomes the curse of dimensionality by depending only\non the intrinsic dimension $d$. These results readily extend to functions in\nH\\\"older-Zygmund spaces. We complement this result with a matching lower bound,\nproving our construction is nearly optimal by showing the required number of\nparameters matches up to a logarithmic factor. Our proof of the lower bound\nintroduces novel estimates for the Vapnik-Chervonenkis dimension and\npseudo-dimension of the network's high-order derivative classes. These\ncomplexity bounds provide a theoretical cornerstone for learning PDEs on\nmanifolds involving derivatives. Our analysis reveals that the network\narchitecture leverages a sparse structure to efficiently exploit the manifold's\nlow-dimensional geometry.", "AI": {"tldr": "First simultaneous approximation theory for deep neural networks on manifolds, showing constant-depth ReLU networks with bounded weights can approximate Sobolev space functions with optimal parameter efficiency that overcomes curse of dimensionality.", "motivation": "Solving PDEs on complex curved domains is challenging due to geometry complicating function and derivative approximations required by differential operators.", "method": "Prove approximation theory using constant-depth ReLU^{k-1} networks with bounded weights to approximate functions in Sobolev spaces W_p^k(M^d) to error \u03b5 in W_p^s(M^d) norm.", "result": "Achieves approximation with O(\u03b5^{-d/(k-s)}) parameters, overcoming curse of dimensionality by depending only on intrinsic dimension d. Matching lower bound proves near optimality.", "conclusion": "Network architecture leverages sparse structure to exploit manifold's low-dimensional geometry, providing theoretical foundation for learning PDEs on manifolds involving derivatives."}}
{"id": "2509.09565", "pdf": "https://arxiv.org/pdf/2509.09565", "abs": "https://arxiv.org/abs/2509.09565", "authors": ["Yangkendi Deng", "Yunfeng Zhang", "Zehua Zhao"], "title": "Sharp bilinear eigenfunction estimate, $L^\\infty_{x_2}L^p_{t,x_1}$-type Strichartz estimate, and energy-critical NLS", "categories": ["math.AP"], "comment": "31 pages. Comments are welcome!", "summary": "We establish sharp bilinear and multilinear eigenfunction estimates for the\nLaplace-Beltrami operator on the standard three-sphere $\\mathbb{S}^3$,\neliminating the logarithmic loss that has persisted in the literature since the\npioneering work of Burq, G\\'erard, and Tzvetkov over twenty years ago. This\ncompletes the theory of multilinear eigenfunction estimates on the standard\nspheres. Our approach relies on viewing $\\mathbb{S}^3$ as the compact Lie group\n$\\mathrm{SU}(2)$ and exploiting its representation theory, especially the\nproperties of Clebsch-Gordan coefficients. Motivated by application to the\nenergy-critical nonlinear Schr\\\"odinger equation (NLS) on $\\mathbb{R} \\times\n\\mathbb{S}^3$, we also prove a refined Strichartz estimate of mixed-norm type\n$L^\\infty_{x_2}L^4_{t,x_1}$ on the cylindrical space $\\mathbb{R}_{x_1} \\times\n\\mathbb{T}_{x_2}$, adapted to certain spectrally localized functions. Combining\nthese two ingredients, we derive a refined bilinear Strichartz estimate on\n$\\mathbb{R} \\times \\mathbb{S}^3$, which in turn yields small data global\nwell-posedness for the above mentioned NLS in the energy space.", "AI": {"tldr": "Eliminates logarithmic loss in bilinear/multilinear eigenfunction estimates on S^3 using SU(2) representation theory, proves refined Strichartz estimates, and achieves small data global well-posedness for energy-critical NLS.", "motivation": "To complete the theory of multilinear eigenfunction estimates on standard spheres by removing the persistent logarithmic loss that has existed for over 20 years since Burq, G\u00e9rard, and Tzvetkov's work, with applications to the energy-critical nonlinear Schr\u00f6dinger equation on S^3.", "method": "Views S^3 as the compact Lie group SU(2) and exploits its representation theory, particularly properties of Clebsch-Gordan coefficients. Also proves refined mixed-norm Strichartz estimates on cylindrical space adapted to spectrally localized functions.", "result": "Establishes sharp bilinear and multilinear eigenfunction estimates without logarithmic loss, completes the theory on standard spheres, derives refined bilinear Strichartz estimates, and obtains small data global well-posedness for energy-critical NLS in the energy space.", "conclusion": "The approach successfully eliminates the long-standing logarithmic loss in eigenfunction estimates on S^3 using group representation theory, leading to complete multilinear theory and applications to nonlinear PDE well-posedness."}}
{"id": "2509.09434", "pdf": "https://arxiv.org/pdf/2509.09434", "abs": "https://arxiv.org/abs/2509.09434", "authors": ["Tom-Christian Riemer", "Martin Stoll"], "title": "A Low-Rank tensor framework for THB-Splines", "categories": ["math.NA", "cs.NA"], "comment": null, "summary": "We introduce a low-rank framework for adaptive isogeometric analysis with\ntruncated hierarchical B-splines (THB-splines) that targets the main bottleneck\nof local refinement: memory- and time-intensive matrix assembly once the global\ntensor-product structure is lost. The method interpolates geometry-induced\nweight and source terms in separable spline spaces and computes their\ntensor-train (TT) representations via the alternating minimal energy (AMEn)\nsolver, enabling level-wise assembly of system operators using univariate\nquadrature. To recover separability in the adaptive setting, we reduce the\nactive basis to tensor-product domains and partition active/non-active cells\ninto a small number of Cartesian cuboids, so each contributes a Kronecker\nfactor that is accumulated and rounded in TT. We realize the two-scale relation\nwith truncation in low rank and assemble the global hierarchical operators in a\nblock TT format suitable for iterative solvers. A prototype MATLAB\nimplementation built on the GeoPDEs package and the TT-Toolbox demonstrates\nthat, for model problems with moderately complex refinement regions, the\napproach reduces memory footprint and assembly time while maintaining accuracy;\nwe also discuss limitations when ranks grow with geometric or refinement\ncomplexity. This framework advances scalable adaptive IgA with THB-splines,\nparticularly in three dimensions.", "AI": {"tldr": "A low-rank framework for adaptive isogeometric analysis with THB-splines that reduces memory and time costs of matrix assembly through tensor-train representations and level-wise assembly.", "motivation": "To address the memory- and time-intensive matrix assembly bottleneck in adaptive isogeometric analysis when the global tensor-product structure is lost due to local refinement.", "method": "Uses tensor-train (TT) representations via AMEn solver, interpolates geometry-induced terms in separable spline spaces, partitions cells into Cartesian cuboids, and assembles hierarchical operators in block TT format.", "result": "For model problems with moderately complex refinement regions, the approach reduces memory footprint and assembly time while maintaining accuracy, though limitations exist when ranks grow with complexity.", "conclusion": "This framework advances scalable adaptive IgA with THB-splines, particularly benefiting three-dimensional applications, despite some limitations with increasing geometric or refinement complexity."}}
{"id": "2509.09531", "pdf": "https://arxiv.org/pdf/2509.09531", "abs": "https://arxiv.org/abs/2509.09531", "authors": ["Fatemeh Haddadi", "Davide Campi", "Flaviano dos Santos", "Nicolas Mounet", "Louis Ponet", "Nicola Marzari", "Marco Gibertini"], "title": "Exploring the magnetic landscape of easily-exfoliable two-dimensional materials", "categories": ["cond-mat.mtrl-sci", "physics.comp-ph"], "comment": "29 pages including references, 8 figures", "summary": "Magnetic materials often exhibit complex energy landscapes with multiple\nlocal minima, each corresponding to a self-consistent electronic structure\nsolution. Finding the global minimum is challenging, and heuristic methods are\nnot always guaranteed to succeed. Here, we apply a recently developed automated\nworkflow to systematically explore the energy landscape of 194 magnetic\nmonolayers obtained from the Materials Cloud 2D crystals database and determine\ntheir ground-state magnetic order. Our approach enables effective control and\nsampling of orbital occupation matrices, allowing rapid identification of local\nminima. We find a diverse set of self-consistent collinear metastable states,\nfurther enriched by Hubbard-corrected energy functionals, when the $U$\nparameters have been computed from first principles using linear-response\ntheory. We categorise the monolayers by their magnetic ordering and highlight\npromising candidates. Our results include 109 ferromagnetic, 83\nantiferromagnetic, and 2 altermagnetic monolayers, along with 12 novel\nferromagnetic half-metals with potential for spintronics technologies.", "AI": {"tldr": "Automated workflow applied to explore energy landscapes of 194 magnetic monolayers from Materials Cloud database, identifying ground-state magnetic orders and discovering novel ferromagnetic half-metals for spintronics.", "motivation": "Magnetic materials have complex energy landscapes with multiple local minima, making global minimum identification challenging with heuristic methods that may not guarantee success.", "method": "Applied automated workflow to systematically explore energy landscapes, enabling effective control and sampling of orbital occupation matrices for rapid identification of local minima. Used Hubbard-corrected energy functionals with first-principles computed U parameters from linear-response theory.", "result": "Found diverse collinear metastable states: 109 ferromagnetic, 83 antiferromagnetic, and 2 altermagnetic monolayers. Identified 12 novel ferromagnetic half-metals with spintronics potential.", "conclusion": "The automated workflow successfully determined ground-state magnetic orders and revealed promising magnetic monolayer candidates, particularly novel ferromagnetic half-metals suitable for spintronics applications."}}
{"id": "2509.09648", "pdf": "https://arxiv.org/pdf/2509.09648", "abs": "https://arxiv.org/abs/2509.09648", "authors": ["Francesca De Marchis", "Lisa Mazzuoli", "Filomena Pacella"], "title": "Stability and asymptotic behaviour of one-dimensional solutions in cylinders", "categories": ["math.AP"], "comment": null, "summary": "We consider positive one-dimensional solutions of a Lane-Emden relative\nDirichlet problem in a cylinder and study their stability/instability\nproperties as the energy varies with respect to domain perturbations. This\ndepends on the exponent $p >1$ of the nonlinearity and we obtain results for\n$p$ close to 1 and for $p$ large. This is achieved by a careful asymptotic\nanalysis of the one-dimensional solution as $p \\to 1$ or $p \\to \\infty$, which\nis of independent interest. It allows to detect the limit profile and other\nqualitative properties of these solutions.", "AI": {"tldr": "Analysis of stability/instability properties of positive one-dimensional solutions to Lane-Emden Dirichlet problems in cylinders as energy varies with domain perturbations, focusing on asymptotic behavior near p=1 and large p.", "motivation": "To understand how the stability properties of one-dimensional solutions to Lane-Emden relative Dirichlet problems depend on the nonlinearity exponent p, particularly for extreme values close to 1 and approaching infinity.", "method": "Careful asymptotic analysis of one-dimensional solutions as p approaches 1 and infinity, examining limit profiles and qualitative properties through mathematical analysis of the Lane-Emden equation.", "result": "The study reveals how stability/instability properties vary with the exponent p, with specific results obtained for p close to 1 and for large p values through asymptotic analysis.", "conclusion": "The asymptotic analysis provides insights into the limit behavior and qualitative properties of solutions, enabling detection of stability transitions as the nonlinearity exponent p varies in extreme ranges."}}
{"id": "2509.09460", "pdf": "https://arxiv.org/pdf/2509.09460", "abs": "https://arxiv.org/abs/2509.09460", "authors": ["Federico Gatti", "Giuseppe Orlando"], "title": "Second-order Optimally Stable IMEX (pseudo-)staggered Galerkin discretization: application to lava flow modeling", "categories": ["math.NA", "cs.NA"], "comment": null, "summary": "We present second-order optimally stable Implicit-Explicit (IMEX) Runge-Kutta\n(RK) schemes with application to a modified set of shallow water equations that\ncan be used to model the dynamics of lava flows. The schemes are optimally\nstable in the sense that they satisfy, at the space-time discretization level,\na condition analogous to the \\texttt{L}-stability of Runge-Kutta methods for\nordinary differential equations. A novel (pseudo-)staggered Galerkin scheme is\nintroduced, which can be interpreted as an extension of the classical two-step\nTaylor-Galerkin (TG2) scheme. The method is derived by combining a von Neumann\nstability analysis with a Lax-Wendroff procedure. For the discretization of the\nnon-conservative terms that characterize the lava flow model, we employ the\nPath-Conservative (PC) method. The proposed scheme is evaluated on a number of\nrelevant test cases, demonstrating accuracy, robustness, and well-balancing\nproperties for the lava flow model.", "AI": {"tldr": "Second-order optimally stable IMEX Runge-Kutta schemes for lava flow modeling using shallow water equations, featuring a novel pseudo-staggered Galerkin scheme and path-conservative method for non-conservative terms.", "motivation": "To develop stable and accurate numerical schemes for modeling lava flow dynamics using modified shallow water equations, addressing stability challenges in space-time discretization.", "method": "Combines IMEX Runge-Kutta schemes with a novel pseudo-staggered Galerkin approach (extension of Taylor-Galerkin TG2), using von Neumann stability analysis, Lax-Wendroff procedure, and Path-Conservative method for non-conservative terms.", "result": "The proposed scheme demonstrates accuracy, robustness, and well-balancing properties across multiple test cases for the lava flow model.", "conclusion": "The developed second-order optimally stable IMEX schemes with pseudo-staggered Galerkin discretization provide an effective numerical framework for simulating lava flow dynamics with improved stability and accuracy."}}
{"id": "2509.09591", "pdf": "https://arxiv.org/pdf/2509.09591", "abs": "https://arxiv.org/abs/2509.09591", "authors": ["Zhen Liu", "David Soper", "Hassan Hemida", "Boyang Chen"], "title": "Numerical modelling of a partially loaded intermodal container freight train passing through a tunnel", "categories": ["physics.flu-dyn", "physics.comp-ph"], "comment": null, "summary": "The bluff nature of a freight train locomotive, coupled with large gaps\ncreated between different wagon formations and loaded goods, influence the\noverall pressure wave pattern generated as the train passes through a tunnel.\nTypically, 1D models are used to predict the patterns and properties of tunnel\npressure wave formations. However, accurate modelling of regions of separation\nat the head of the blunted containers and at unloaded gap sections is essential\nfor precise predictions of pressure magnitudes. This has traditionally been\ndifficult to capture with 1D models. Furthermore, achieving this accuracy\nthrough 3D computational methods demands exceptional mesh quality, significant\ncomputational resources, and the careful selection of numerical models. This\npaper evaluates various numerical models to capture these complexities within\nregions of flow separation. Findings have supported the development of a new 1D\nprogramme to calculate the pressure wave generated by a freight locomotive\nentering a tunnel, and is here further extended to consider the discontinuities\nof the train body created by intermodal container loading patterns, by\nimplementing new mesh system and boundary conditions into the 1D programme. A\nparameterisation study for different loading configurations is also presented\nto improve the overall programme adaptability, and the relationship between\npredetermined parameters and gap length is investigated. We validate the\neffectiveness of the improved 1D model through comprehensive Large Eddy\nSimulation (LES) results and conduct an extensive parameterisation study to\nenhance its applicability across various loading configurations. Consequently,\nthis research bridges the gap in freight train tunnel aerodynamics, offering a\nversatile 1D numerical tool for accurate pressure wave prediction.", "AI": {"tldr": "Improved 1D model for predicting pressure waves when freight trains enter tunnels, addressing flow separation issues at blunt containers and gaps between wagons through new mesh systems and boundary conditions.", "motivation": "Traditional 1D models struggle to capture flow separation at blunt container heads and unloaded gap sections, while 3D methods require excessive computational resources and mesh quality.", "method": "Developed a new 1D programme with enhanced mesh system and boundary conditions to handle discontinuities from container loading patterns, validated against Large Eddy Simulation results and parameterization studies.", "result": "The improved 1D model effectively captures pressure wave complexities, bridges the gap in freight train tunnel aerodynamics, and provides accurate predictions across various loading configurations.", "conclusion": "This research offers a versatile and computationally efficient 1D numerical tool for accurate pressure wave prediction in freight train tunnel entry scenarios, overcoming limitations of traditional approaches."}}
{"id": "2509.09463", "pdf": "https://arxiv.org/pdf/2509.09463", "abs": "https://arxiv.org/abs/2509.09463", "authors": ["Jana Jovcheva", "Tim Seynnaeve", "Nick Vannieuwenhoven"], "title": "Minimality of Tree Tensor Network Ranks", "categories": ["math.NA", "cs.NA", "15A69 (Primary) 65F99 (Secondary)"], "comment": "12 pages, 3 figures", "summary": "For a given tree tensor network $G$, we call a tuple of bond dimensions\nminimal if there exists a tensor $T$ that can be represented by this network\nbut not on the same tree topology with strictly smaller bond dimensions. We\nestablish necessary and sufficient conditions on the bond dimensions of a tree\ntensor network to be minimal, generalizing a characterization of Carlini and\nKleppe about existence of tensors with a given multilinear rank. We also show\nthat in a minimal tree tensor network, the non-minimal tensors form a Zariski\nclosed subset, so minimality is a generic property in this sense.", "AI": {"tldr": "Characterization of minimal bond dimensions in tree tensor networks, generalizing multilinear rank theory and showing minimality is a generic property.", "motivation": "To establish necessary and sufficient conditions for bond dimensions in tree tensor networks to be minimal, extending previous work on multilinear ranks and understanding when tensors cannot be represented with smaller bond dimensions.", "method": "Mathematical analysis of tree tensor network structures, generalizing Carlini and Kleppe's characterization of tensors with given multilinear rank, and employing Zariski topology concepts to study generic properties.", "result": "Established complete characterization of minimal bond dimensions for tree tensor networks, and proved that non-minimal tensors form a Zariski closed subset, making minimality a generic property in these networks.", "conclusion": "The paper provides fundamental theoretical results about minimal representations in tree tensor networks, showing that minimal bond dimensions can be precisely characterized and that most tensors in such networks exhibit this minimality property."}}
{"id": "2509.09601", "pdf": "https://arxiv.org/pdf/2509.09601", "abs": "https://arxiv.org/abs/2509.09601", "authors": ["St\u00e9phane Delorme", "Leon Mach", "Hubert Paszkiewicz", "Richard Ruiz"], "title": "Are arXiv submissions on Wednesday better cited? Introducing Big Data methods in undergraduate courses on scientific computing", "categories": ["physics.ed-ph", "hep-ex", "physics.comp-ph"], "comment": "10 pages, 8 figures, 2 tables, 1 listing, project available at\n  https://gitlab.cern.ch/riruiz/public-projects/-/tree/master/BibAPI/", "summary": "Extracting information from big data sets, both real and simulated, is a\nmodern hallmark of the physical sciences. In practice, students face barriers\nto learning ``Big Data'' methods in undergraduate physics and astronomy\ncurricula. As an attempt to alleviate some of these challenges, we present a\nsimple, farm-to-table data analysis pipeline that can collect, process, and\nplot data from the 800k entries common to the arXiv preprint repository and the\nbibliographical database inSpireHEP. The pipeline employs contemporary research\npractices and can be implemented using open-sourced Python libraries common to\nundergraduate courses on Scientific Computing. To support the use such\npipelines in classroom contexts, we make public an example implementation,\nauthored by two undergraduate physics students, that runs on off-the-shelf\nlaptops. For advanced students, we discuss applications of the pipeline,\nincluding for online DAQ monitoring and commercialization.", "AI": {"tldr": "A simple data analysis pipeline for teaching big data methods using arXiv and inSpireHEP databases, implemented with open-source Python libraries for undergraduate physics education.", "motivation": "Address barriers to learning big data methods in undergraduate physics curricula by providing accessible, practical tools that use real scientific data sources.", "method": "Developed a farm-to-table data analysis pipeline that collects, processes, and plots data from 800k entries common to arXiv preprint repository and inSpireHEP bibliographical database using open-source Python libraries.", "result": "Created an example implementation that runs on standard laptops, authored by undergraduate students, demonstrating practical application of contemporary research practices in educational settings.", "conclusion": "The pipeline successfully provides an accessible entry point for teaching big data analysis methods and has potential applications for advanced uses like online DAQ monitoring and commercialization."}}
{"id": "2509.09000", "pdf": "https://arxiv.org/pdf/2509.09000", "abs": "https://arxiv.org/abs/2509.09000", "authors": ["Wael El Khateeb", "Chanaka Kottegoda", "Chunhua Shan"], "title": "Complex dynamics and pattern formation in a diffusive epidemic model with an infection-dependent recovery rate", "categories": ["math.DS", "math.AP", "35K57, 92D30, 35B32, 35B36, 92C60"], "comment": null, "summary": "A diffusive epidemic model with an infection-dependent recovery rate is\nformulated in this paper. Multiple constant steady states and spatially\nhomogeneous periodic solutions are first proven by bifurcation analysis of the\nreaction kinetics. It is shown that the model exhibits diffusion-driven\ninstability, where the infected population acts as an activator and the\nsusceptible population functions as an in hibitor. The faster movement of the\nsusceptible class will induce the spatial and spatiotemporal patterns, which\nare characterized by k-mode Turing instability and (k1,k2)-mode Turing-Hopf\nbifurcation. The transient dynamics from a purely temporal oscillatory regime\nto a spatial periodic pattern are discovered. The model reveals key\ntransmission dynamics, including asynchronous disease recurrence, spatially\npatterned waves, and the formation of localized hotspots. The study suggests\nthat spatially targeted strategies are necessary to contain disease waves that\nvary regionally and cyclically.", "AI": {"tldr": "A diffusive epidemic model with infection-dependent recovery rate shows diffusion-driven instability, Turing patterns, and complex spatiotemporal dynamics including disease recurrence and localized hotspots.", "motivation": "To understand how infection-dependent recovery rates and population movement affect disease transmission patterns and spatial spread in epidemic models.", "method": "Bifurcation analysis of reaction kinetics to identify steady states and periodic solutions, followed by analysis of diffusion-driven instability and Turing-Hopf bifurcation patterns.", "result": "The model exhibits multiple constant steady states, spatially homogeneous periodic solutions, diffusion-driven instability, and complex spatiotemporal patterns including k-mode Turing instability and (k1,k2)-mode Turing-Hopf bifurcation.", "conclusion": "Spatially targeted strategies are necessary to contain regionally and cyclically varying disease waves, as the model reveals asynchronous recurrence, patterned waves, and localized hotspots."}}
{"id": "2509.09533", "pdf": "https://arxiv.org/pdf/2509.09533", "abs": "https://arxiv.org/abs/2509.09533", "authors": ["Qianqian Wu", "Rongfang Gong", "Wei Gong", "Ziyi Zhang", "Shengfeng Zhu"], "title": "Bioluminescence tomography: A new regularized shape optimization method", "categories": ["math.NA", "cs.NA", "math.OC"], "comment": null, "summary": "In this paper, we investigate an inverse source problem arising in\nbioluminescence tomography (BLT), where the objective is to recover both the\nsupport and intensity of the light source from boundary measurements. A shape\noptimization framework is developed, in which the source strength and its\nsupport are decoupled through first-order optimality conditions. To enhance the\nstability of the reconstruction, we incorporate a parameter-dependent coupled\ncomplex boundary method(CCBM) scheme together with perimeter and volume\nregularizations. The level-set representation naturally accommodates\ntopological changes, enabling the reconstruction of multiple, closely located,\nor nested sources. Theoretical justifications are provided, and a series of\nnumerical experiments are conducted to validate the proposed method. The\nresults demonstrate the robustness, accuracy, and noise-resistance of the\nalgorithm, as well as its advantages over existing approaches.", "AI": {"tldr": "A shape optimization framework for bioluminescence tomography that recovers light source support and intensity using level-set representation with regularization for stable reconstruction.", "motivation": "To solve the inverse source problem in bioluminescence tomography by recovering both the spatial support and intensity of light sources from boundary measurements, addressing challenges with closely located or nested sources.", "method": "Developed a shape optimization framework that decouples source strength and support through first-order optimality conditions, incorporating parameter-dependent coupled complex boundary method (CCBM) with perimeter and volume regularizations, using level-set representation to handle topological changes.", "result": "The method demonstrates robustness, accuracy, and noise-resistance in numerical experiments, showing advantages over existing approaches for reconstructing multiple, closely located, or nested sources.", "conclusion": "The proposed shape optimization framework with level-set representation and regularization techniques provides an effective solution for stable and accurate bioluminescence tomography reconstruction, capable of handling complex source configurations."}}
{"id": "2509.09269", "pdf": "https://arxiv.org/pdf/2509.09269", "abs": "https://arxiv.org/abs/2509.09269", "authors": ["Luca Ballotta", "Juncal Arbelaiz", "Vijay Gupta", "Luca Schenato", "Mihailo R. Jovanovi\u0107"], "title": "The role of communication delays in the optimal control of spatially invariant systems", "categories": ["math.OC", "cs.SY", "eess.SY", "math.AP", "93C43 (Primary) 49N10 (Secondary)"], "comment": "{\\copyright} 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "We study optimal proportional feedback controllers for spatially invariant\nsystems when the controller has access to delayed state measurements received\nfrom different spatial locations. We analyze how delays affect the spatial\nlocality of the optimal feedback gain leveraging the problem decoupling in the\nspatial frequency domain. For the cases of expensive control and small delay,\nwe provide exact expressions of the optimal controllers in the limit for\ninfinite control weight and vanishing delay, respectively. In the expensive\ncontrol regime, the optimal feedback control law decomposes into a delay-aware\nfiltering of the delayed state and the optimal controller in the delay-free\nsetting. Under small delays, the optimal controller is a perturbation of the\ndelay-free one which depends linearly on the delay. We illustrate our\nanalytical findings with a reaction-diffusion process over the real line and a\nmulti-agent system coupled through circulant matrices, showing that delays\nreduce the effectiveness of optimal feedback control and may require each\nsubsystem within a distributed implementation to communicate with farther-away\nlocations.", "AI": {"tldr": "Analysis of optimal proportional feedback controllers for spatially invariant systems with delayed state measurements, showing how delays impact spatial locality and control effectiveness.", "motivation": "To understand how time delays in state measurements affect the spatial locality and performance of optimal feedback controllers in spatially invariant systems.", "method": "Leveraging problem decoupling in spatial frequency domain, analyzing expensive control and small delay regimes, providing exact expressions for optimal controllers in limiting cases.", "result": "Delays reduce control effectiveness and may require increased communication range in distributed implementations; optimal controller decomposes into delay-aware filtering and delay-free components.", "conclusion": "Time delays significantly impact spatial control strategies, necessitating delay-aware filtering and potentially broader communication networks for optimal performance in spatially distributed systems."}}
{"id": "2509.09600", "pdf": "https://arxiv.org/pdf/2509.09600", "abs": "https://arxiv.org/abs/2509.09600", "authors": ["Pascal Heid", "Thomas P. Wihler"], "title": "Iterative energy reduction Galerkin methods and variational adaptivity", "categories": ["math.NA", "cs.NA", "35A15, 35B38, 65J15, 65M50, 65N30"], "comment": null, "summary": "Critical points of energy functionals, which are of broad interest, for\ninstance, in physics and chemistry, in solid and quantum mechanics, in material\nscience, or in general diffusion-reaction models arise as solutions to the\nassociated Euler-Lagrange equations. While classical computational solution\nmethods for such models typically focus solely on the underlying partial\ndifferential equations, we propose an approach that also incorporates the\nenergy structure itself. Specifically, we examine (linearized) iterative\nGalerkin discretization schemes that ensure energy reduction at each step.\nAdditionally, we provide necessary conditions, which are applicable to a wide\nclass of problems, that guarantee convergence to critical points of the PDE.\nMoreover, in the specific context of finite element discretizations, we present\na very generally applicable adaptive mesh refinement strategy - the so-called\nvariational adaptivity approach - which, rather than using classical a\nposteriori estimates, is based on exploiting local energy reductions. The\ntheoretical results are validated for several computational experiments in the\ncontext of nonlinear diffusion-reaction models, thereby demonstrating the\neffectiveness of the proposed scheme.", "AI": {"tldr": "Proposes energy-based computational methods for solving Euler-Lagrange equations, featuring energy-reducing iterative schemes and variational mesh refinement without classical error estimates.", "motivation": "Classical PDE solvers ignore the energy structure of variational problems. The authors aim to incorporate energy reduction principles directly into computational methods for better convergence and efficiency.", "method": "Develops (linearized) iterative Galerkin discretization schemes that ensure energy reduction at each step. Introduces variational adaptivity - an adaptive mesh refinement strategy based on local energy reductions rather than traditional a posteriori error estimates.", "result": "Provides necessary conditions for convergence to critical points applicable to a wide class of problems. Validates theoretical results through computational experiments on nonlinear diffusion-reaction models, demonstrating scheme effectiveness.", "conclusion": "Energy-based computational approaches that incorporate the variational structure outperform classical PDE-only methods, with energy-reducing iterative schemes and variational mesh refinement proving effective for solving Euler-Lagrange equations."}}
{"id": "2509.08939", "pdf": "https://arxiv.org/pdf/2509.08939", "abs": "https://arxiv.org/abs/2509.08939", "authors": ["M. Castill\u00f3n", "I. Romero", "J. Segurado"], "title": "A Phase-Field Approach to Fracture and Fatigue Analysis: Bridging Theory and Simulation", "categories": ["cond-mat.mtrl-sci", "cs.NA", "math.NA"], "comment": null, "summary": "This article presents a novel, robust and efficient framework for fatigue\ncrack-propagation that combines the principles of Linear Elastic Fracture\nMechanics (LEFM) with phase-field fracture (PFF). Contrary to cycle-by-cycle\nPFF approaches, this work relies on a single simulation and uses standard crack\npropagation models such as Paris' law for the material response, simplifying\nits parametrization.\n  The core of the methodology is the numerical evaluation of the derivative of\na specimen's compliance with respect to the crack area. To retrieve this\ncompliance the framework relies on a PFF-FEM simulation, controlled imposing a\nmonotonic crack growth. This control of the loading process is done by a new\ncrack-control scheme which allows to robustly trace the complete equilibrium\npath of a crack, capturing complex instabilities. The specimen's compliance\nobtained from the PFF simulation enables the integration of Paris' law to\npredict fatigue life.\n  The proposed methodology is first validated through a series of benchmarks\nwith analytical solutions to demonstrate its accuracy. The framework is then\napplied to more complex geometries where the crack path is unknown, showing a\nvery good agreement with experimental results of both crack paths and fatigue\nlife.", "AI": {"tldr": "Novel framework combining LEFM with phase-field fracture for fatigue crack propagation using single simulation and Paris' law, validated against analytical solutions and experimental results.", "motivation": "To develop a more efficient and robust approach for fatigue crack propagation analysis that avoids cycle-by-cycle simulations and simplifies parametrization while capturing complex crack instabilities.", "method": "Combines Linear Elastic Fracture Mechanics with phase-field fracture using FEM simulation with monotonic crack growth control. Uses numerical evaluation of compliance derivative with respect to crack area and integrates Paris' law for fatigue life prediction.", "result": "Validated through benchmarks with analytical solutions showing high accuracy. Applied to complex geometries with unknown crack paths, demonstrating very good agreement with experimental results for both crack paths and fatigue life.", "conclusion": "The proposed framework provides an efficient, robust, and accurate method for fatigue crack propagation analysis that simplifies parametrization and captures complex crack instabilities while maintaining agreement with experimental data."}}
{"id": "2509.08968", "pdf": "https://arxiv.org/pdf/2509.08968", "abs": "https://arxiv.org/abs/2509.08968", "authors": ["Mahsa Sajjadi", "Kaiyang Huang", "Kai Sun"], "title": "Efficient High-Order Participation Factor Computation via Batch-Structured Tensor Contraction", "categories": ["eess.SY", "cs.NA", "cs.SY", "math.NA"], "comment": null, "summary": "Participation factors (PFs) quantify the interaction between system modes and\nstate variables, and they play a crucial role in various applications such as\nmodal analysis, model reduction, and control design. With increasing system\ncomplexity, especially due to power electronic devices and renewable\nintegration, the need for scalable and high-order nonlinear PF (NPF)\ncomputation has become more critical. This paper presents an efficient\ntensor-based method for calculating NPFs up to an arbitrary order. Traditional\ncomputation of PFs directly from normal form theory is computationally\nexpensive -- even for second-order PFs -- and becomes infeasible for higher\norders due to memory constraints. To address this, a tensor contraction-based\napproach is introduced that enables the calculation of high-order PFs using a\nbatching strategy. The batch sizes are dynamically determined based on the\navailable computational resources, allowing scalable and memory-efficient\ncomputation.", "AI": {"tldr": "Efficient tensor-based method for computing high-order nonlinear participation factors using dynamic batching to overcome memory constraints in complex power systems.", "motivation": "Increasing system complexity from power electronics and renewable integration requires scalable computation of nonlinear participation factors for modal analysis and control design.", "method": "Tensor contraction-based approach with dynamic batching strategy that adjusts batch sizes based on available computational resources.", "result": "Enables calculation of high-order nonlinear participation factors that were previously infeasible due to memory constraints.", "conclusion": "Provides a scalable and memory-efficient solution for computing arbitrary-order nonlinear participation factors in complex power systems."}}
{"id": "2509.08977", "pdf": "https://arxiv.org/pdf/2509.08977", "abs": "https://arxiv.org/abs/2509.08977", "authors": ["Binh Huy Nguyen", "Matti Schneider"], "title": "Symmetries in stochastic homogenization and acclimatizations for the RVE method", "categories": ["cs.CE", "cs.NA", "math.NA"], "comment": "47 pages, 19 figures", "summary": "We investigate the implications of a given symmetry of a random\nmicrostructure on the obtained effective tensor and its fluctuation in the\ncontext of thermal conductivity, and study strategies for enforcing these\nsymmetries in postprocessing via orthogonal projectors. Within the framework of\nthe representative volume element (RVE) method, we establish the invariance\nconditions for the effective tensor and its fluctuation under different\nsymmetry groups of the microstructure. Interestingly, the symmetry of the\nconsidered cell type in the RVE method may break the ensemble symmetry and\ncompromise the approximation of the effective properties. To rectify this\nissue, we introduce dedicated techniques which permit to enforce the expected\nsymmetries in postprocessing and study the implications on the bounds for the\neffective properties as well as the total, the random and the systematic\nerrors. We provide theoretical arguments that suitable projections lead to\nunbiased variance-reduction strategies which furthermore enforce the expected\nsymmetries exactly. Through large-scale FFT-based homogenization simulations,\nwe study the symmetry structure of the estimated effective conductivities and\ntheir fluctuations. Moreover, we demonstrate the power of the\nsymmetry-projection techniques for fiber-reinforced composite microstructures\nof industrial scale.", "AI": {"tldr": "Study on enforcing microstructure symmetries in effective thermal conductivity tensors using orthogonal projectors to maintain symmetry properties and reduce errors in RVE homogenization.", "motivation": "To address how microstructure symmetries affect effective tensor properties and their fluctuations, and to develop methods for enforcing these symmetries when they are broken by RVE cell types.", "method": "Use orthogonal projectors in postprocessing to enforce expected symmetries, analyze implications on error bounds, and validate through large-scale FFT-based homogenization simulations on fiber-reinforced composites.", "result": "Suitable projections provide unbiased variance-reduction strategies that exactly enforce expected symmetries, improving accuracy in effective property estimation.", "conclusion": "Symmetry-projection techniques effectively rectify symmetry breaking in RVE methods, enhancing the reliability of effective property predictions for composite microstructures."}}
{"id": "2509.09380", "pdf": "https://arxiv.org/pdf/2509.09380", "abs": "https://arxiv.org/abs/2509.09380", "authors": ["Luca Giuliani", "Michele Lombardi"], "title": "Robust Non-Linear Correlations via Polynomial Regression", "categories": ["cs.LG", "cs.AI", "cs.NA", "math.NA"], "comment": null, "summary": "The Hirschfeld-Gebelein-R\\'enyi (HGR) correlation coefficient is an extension\nof Pearson's correlation that is not limited to linear correlations, with\npotential applications in algorithmic fairness, scientific analysis, and causal\ndiscovery. Recently, novel algorithms to estimate HGR in a differentiable\nmanner have been proposed to facilitate its use as a loss regularizer in\nconstrained machine learning applications. However, the inherent\nuncomputability of HGR requires a bias-variance trade-off, which can possibly\ncompromise the robustness of the proposed methods, hence raising technical\nconcerns if applied in real-world scenarios. We introduce a novel computational\napproach for HGR that relies on user-configurable polynomial kernels, offering\ngreater robustness compared to previous methods and featuring a faster yet\nalmost equally effective restriction. Our approach provides significant\nadvantages in terms of robustness and determinism, making it a more reliable\noption for real-world applications. Moreover, we present a brief experimental\nanalysis to validate the applicability of our approach within a constrained\nmachine learning framework, showing that its computation yields an insightful\nsubgradient that can serve as a loss regularizer.", "AI": {"tldr": "Novel computational approach for HGR correlation coefficient using polynomial kernels, offering improved robustness and determinism for real-world applications.", "motivation": "Existing HGR estimation methods suffer from bias-variance trade-offs due to inherent uncomputability, compromising robustness in real-world scenarios like algorithmic fairness and constrained ML.", "method": "User-configurable polynomial kernels approach for HGR computation, providing faster yet effective restriction compared to previous methods.", "result": "The method demonstrates significant advantages in robustness and determinism, with experimental validation showing it produces insightful subgradients suitable as loss regularizers in constrained ML frameworks.", "conclusion": "The proposed polynomial kernel approach offers a more reliable and robust computational method for HGR correlation, making it suitable for practical applications in fairness, scientific analysis, and causal discovery."}}
{"id": "2509.09391", "pdf": "https://arxiv.org/pdf/2509.09391", "abs": "https://arxiv.org/abs/2509.09391", "authors": ["Kelin Wu", "Hongpeng Sun"], "title": "A preconditioned third-order implicit-explicit algorithm with a difference of varying convex functions and extrapolation", "categories": ["math.OC", "cs.NA", "math.NA"], "comment": null, "summary": "This paper proposes a novel preconditioned implicit-explicit algorithm\nenhanced with the extrapolation technique for non-convex optimization problems.\nThe algorithm employs a third-order Adams-Bashforth scheme for the nonlinear\nand explicit parts and a third-order backward differentiation formula for the\nimplicit part of the gradient flow in variational functions. The proposed\nalgorithm, akin to a generalized difference-of-convex (DC) approach, employs a\nchanging set of convex functions in each iteration. Under the Kurdyka-\\L\nojasiewicz (KL) properties, the global convergence of the algorithm is\nguaranteed, ensuring that it converges within a finite number of preconditioned\niterations. Our numerical experiments, including least squares problems with\nSCAD regularization and the graphical Ginzburg-Landau model, demonstrate the\nproposed algorithm's highly efficient performance compared to conventional DC\nalgorithms.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.09611", "pdf": "https://arxiv.org/pdf/2509.09611", "abs": "https://arxiv.org/abs/2509.09611", "authors": ["Haolan Zheng", "Yanlai Chen", "Jiequn Han", "Yue Yu"], "title": "ReBaNO: Reduced Basis Neural Operator Mitigating Generalization Gaps and Achieving Discretization Invariance", "categories": ["cs.LG", "cs.NA", "math.NA"], "comment": null, "summary": "We propose a novel data-lean operator learning algorithm, the Reduced Basis\nNeural Operator (ReBaNO), to solve a group of PDEs with multiple distinct\ninputs. Inspired by the Reduced Basis Method and the recently introduced\nGenerative Pre-Trained Physics-Informed Neural Networks, ReBaNO relies on a\nmathematically rigorous greedy algorithm to build its network structure offline\nadaptively from the ground up. Knowledge distillation via task-specific\nactivation function allows ReBaNO to have a compact architecture requiring\nminimal computational cost online while embedding physics. In comparison to\nstate-of-the-art operator learning algorithms such as PCA-Net, DeepONet, FNO,\nand CNO, numerical results demonstrate that ReBaNO significantly outperforms\nthem in terms of eliminating/shrinking the generalization gap for both in- and\nout-of-distribution tests and being the only operator learning algorithm\nachieving strict discretization invariance.", "AI": {"tldr": "ReBaNO is a novel data-lean operator learning algorithm that combines reduced basis methods with neural networks to solve PDEs with multiple inputs, achieving superior generalization and strict discretization invariance compared to state-of-the-art methods.", "motivation": "To address the generalization gap and lack of discretization invariance in existing operator learning algorithms for solving PDEs with multiple distinct inputs, while maintaining computational efficiency.", "method": "Combines Reduced Basis Method with Generative Pre-Trained Physics-Informed Neural Networks using a greedy algorithm to build network structure offline, and employs knowledge distillation via task-specific activation functions for compact architecture.", "result": "Significantly outperforms PCA-Net, DeepONet, FNO, and CNO in eliminating/shrinking generalization gap for both in- and out-of-distribution tests, and is the only operator learning algorithm achieving strict discretization invariance.", "conclusion": "ReBaNO provides a mathematically rigorous, computationally efficient approach for operator learning that overcomes key limitations of existing methods while maintaining physics embedding capabilities."}}
