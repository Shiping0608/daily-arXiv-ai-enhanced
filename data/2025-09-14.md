<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 18]
- [math.AP](#math.AP) [Total: 11]
- [physics.comp-ph](#physics.comp-ph) [Total: 2]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 2]
- [physics.gen-ph](#physics.gen-ph) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 3]
- [physics.ed-ph](#physics.ed-ph) [Total: 1]
- [math.DS](#math.DS) [Total: 1]
- [math.OC](#math.OC) [Total: 2]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.AI](#cs.AI) [Total: 1]
- [astro-ph.HE](#astro-ph.HE) [Total: 2]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [eess.SY](#eess.SY) [Total: 1]
- [physics.bio-ph](#physics.bio-ph) [Total: 1]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 2]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Exponential Runge-Kutta methods for parabolic equations with state-dependent delay](https://arxiv.org/abs/2509.08902)
*Qiumei Huang,Alexander Ostermann,Gangfan Zhong*

Main category: math.NA

TL;DR: Construction and analysis of exponential Runge-Kutta methods for semilinear parabolic problems with arbitrary state-dependent delay, including well-posedness, scheme development, and numerical validation.


<details>
  <summary>Details</summary>
Motivation: To develop efficient temporal discretization methods for semilinear parabolic problems with arbitrary state-dependent delay, which are challenging due to the delay dependency on the solution state.

Method: Construct first and second order explicit exponential Runge-Kutta schemes using continuous extension of time discrete solution for delay approximation, with collocation-type methods for arbitrary order schemes.

Result: Established well-posedness of the problem, unique solvability of proposed schemes, and proved convergence of the methods with numerical experiments validating theoretical results.

Conclusion: The exponential Runge-Kutta methods provide effective numerical solutions for semilinear parabolic problems with state-dependent delays, with proven convergence properties and practical implementation feasibility.

Abstract: The aim of this paper is to construct and analyze exponential Runge-Kutta
methods for the temporal discretization of a class of semilinear parabolic
problems with arbitrary state-dependent delay. First, the well-posedness of the
problem is established. Subsequently, first and second order schemes are
constructed. They are based on the explicit exponential Runge-Kutta methods,
where the delayed solution is approximated by a continuous extension of the
time discrete solution. Schemes of arbitrary order can be constructed using the
methods of collocation type. The unique solvability and convergence of the
proposed schemes are established. Finally, we discuss implementation issues and
present some numerical experiments to illustrate our theoretical results.

</details>


### [2] [Quasi-Trefftz spaces for a first-order formulation of the Helmholtz equation](https://arxiv.org/abs/2509.08936)
*Lise-Marie Imbert-Gérard,Andréa Lagardère,Guillaume Sylvand,Sébastien Tordeux*

Main category: math.NA

TL;DR: Development of quasi-Trefftz methods for first-order differential systems, focusing on discrete quasi-Trefftz spaces including their definition, basis construction, and computational aspects.


<details>
  <summary>Details</summary>
Motivation: To establish the foundation for quasi-Trefftz methods applied to first-order differential systems, which represents a novel approach in numerical methods for differential equations.

Method: Development of discrete quasi-Trefftz spaces through definition, construction of corresponding bases, and analysis of computational aspects.

Result: First systematic framework for quasi-Trefftz methods in first-order differential systems, providing the fundamental building blocks for this numerical approach.

Conclusion: This work lays the groundwork for future development of quasi-Trefftz methods, establishing the essential components needed for practical implementation in solving first-order differential systems.

Abstract: This work is the first step in the development of quasi-Trefftz methods for
first-order differential systems. It focuses on discrete quasi-Trefftz spaces,
starting from their definition and including construction of corresponding
bases together with their computational aspect.

</details>


### [3] [Numerical Approximation and Bifurcation Results for an Elliptic Problem with Superlinear Subcritical Nonlinearity on the Boundary](https://arxiv.org/abs/2509.08990)
*Shalmali Bandyopadhyay,Thomas Lewis,Dustin Nichols*

Main category: math.NA

TL;DR: Numerical algorithms for approximating positive solutions of elliptic boundary value problems with superlinear subcritical nonlinearity on the boundary, including coupled systems with cross-coupling boundary conditions.


<details>
  <summary>Details</summary>
Motivation: While existence, uniqueness, nonexistence, and multiplicity results for such problems are well-established, their numerical treatment presents computational challenges due to the absence of comparison principles and complex bifurcation phenomena.

Method: Finite difference formulations for both single equations and coupled systems, combined with principal eigenvalue analysis for linearized problems and continuation methods to trace complete bifurcation curves.

Result: The methods establish admissibility results for finite difference method, determine unique bifurcation points from trivial solutions, and provide insight into theoretical properties while enabling computation of approximate solutions.

Conclusion: The proposed numerical approach successfully validates established existence and uniqueness results and is consistent with principle eigenvalue analysis, providing effective computational tools for these challenging boundary value problems.

Abstract: We develop numerical algorithms to approximate positive solutions of elliptic
boundary value problems with superlinear subcritical nonlinearity on the
boundary of the form $-\Delta u + u = 0$ in $\Omega$ with $\frac{\partial
u}{\partial \eta} = \lambda f(u)$ on $\partial\Omega$ as well as an extension
to a corresponding system of equations. While existence, uniqueness,
nonexistence, and multiplicity results for such problems are well-established,
their numerical treatment presents computational challenges due to the absence
of comparison principles and complex bifurcation phenomena. We present finite
difference formulations for both single equations and coupled systems with
cross-coupling boundary conditions, establishing admissibility results for the
finite difference method. We derive principal eigenvalue analysis for the
linearized problems to determine unique bifurcation points from trivial
solutions. The eigenvalue analysis provides additional insight into the
theoretical properties of the problem while also providing intuition for
computing approximate solutions based on the proposed finite difference
formulation. We combine our finite difference methods with continuation methods
to trace complete bifurcation curves, validating established existence and
uniqueness results and consistent with the results of the principle eigenvalue
analysis.

</details>


### [4] [Numerical modeling of elastic waves in thin shells with grid-characteristic method](https://arxiv.org/abs/2509.09017)
*Katerina Beklemysheva,Egor Michel,Andrey Ovsiannikov*

Main category: math.NA

TL;DR: A method for modeling elastic waves in thin structures using Kirchhoff-Love theory to reduce computational grid size while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Numerical modeling of complex structures with thin components requires extremely large grids due to size constraints, making 3D calculations computationally expensive.

Method: Derived hyperbolic dynamic system from Kirchhoff-Love material model, solved numerically with grid-characteristic method to replace fine 3D meshes with coarser meshes.

Result: The approach allows modeling large complex thin objects with reasonable grid sizes while resolving significant wave types, with numerical results validated against 3D calculations.

Conclusion: The thin plate/shell theory approach provides computationally efficient modeling of elastic wave processes in complex structures containing thin components, balancing accuracy with computational feasibility.

Abstract: Numerical modeling of strength and non-destructive testing of complex
structures such as buildings, space rockets or oil reservoirs often involves
calculations on extremely large grids. The modeling of elastic wave processes
in solids places limitations on the grid element size because resolving
different elastic waves requires at least several grid elements for the
characteristic size of the modeled object. For a thin plate, the defining size
is its thickness, and a complex structure that contains large-scale thin
objects requires a large-scale grid to preserve its uniformity. One way to
bypass this problem is the theory of thin plates and shells that replaces a
simple material model on a fine three-dimensional mesh with a more complex
material model on a coarser mesh. This approach loses certain fine effects
inside the thin plate, but allows us to model large and complex thin objects
with a reasonable size calculation grid and resolve all the significant wave
types. In this research, we take the Kirchhoff-Love material model and derive a
hyperbolic dynamic system of equations that allows for a physical
interpretation of eigenvalues and eigenvectors. The system is solved
numerically with a grid-characteristic method. Numerical results for several
model statements are compared with three-dimensional calculations based on
grid-characteristic method for a three dimensional elasticity.

</details>


### [5] [Characterization of the near-null error components utilized in composite adaptive AMG solvers](https://arxiv.org/abs/2509.09023)
*Austen J. Nelson,Panayot S. Vassilevski*

Main category: math.NA

TL;DR: Theoretical justification for adaptive composite AMG solvers using modularity-based aggregation to handle error components that standard solvers cannot damp efficiently.


<details>
  <summary>Details</summary>
Motivation: To improve algebraic multigrid (AMG) performance by adaptively constructing solver components that target specific error components that current solvers handle poorly.

Method: Uses a sequence of AMG μ-cycle methods with aggregates constructed via modularity matrix from graph community detection, utilizing both the matrix and current error component vectors.

Result: Demonstrated performance on various sparse matrices from PDE discretizations and general problems, showing improved solver efficiency.

Conclusion: Adaptive composite solvers with modularity-based aggregation provide effective approach for handling challenging error components in AMG methods across diverse matrix types.

Abstract: We provide a theoretical justification for the construction of adaptive
composite solvers based on a sequence of AMG (algebraic multigrid) $\mu$-cycle
methods that exploit error components that the current solver cannot damp
efficiently. Each solver component is an aggregation based AMG where its
aggregates are constructed using the popular in graph community detection
modularity matrix. The latter utilizes the given matrix and the error component
vector the current solver cannot handle. The performance of the resulting
adaptive composite solver is illustrated on a variety of sparse matrices both
arising from discretized PDEs and ones with more general nature.

</details>


### [6] [Strong convergence of a semi tamed scheme for stochastic differential algebraic equation under non-global Lipschitz coefficients](https://arxiv.org/abs/2509.09032)
*Guy Tsafack,Antoine Tambue*

Main category: math.NA

TL;DR: First strong convergence analysis of a semi-implicit tamed method for stochastic differential algebraic equations (SDAEs) under non-global Lipschitz conditions, achieving order 1/2 convergence.


<details>
  <summary>Details</summary>
Motivation: Explicit Euler scheme fails for SDEs with superlinear growth coefficients, and the problem is more challenging for SDAEs due to matrix singularity. Need for a robust numerical method that can handle non-global Lipschitz settings.

Method: Developed a semi-implicit tamed method where linear drift components are approximated implicitly and nonlinear components are tamed and approximated explicitly. Created an equivalent dual tamed scheme for mathematical analysis by eliminating constraints from original SDAEs.

Result: The proposed method strongly converges with order 1/2 to the exact solution under non-global Lipschitz conditions. Numerical simulations confirm theoretical findings.

Conclusion: The semi-implicit tamed method provides an effective numerical solution for SDAEs in challenging non-global Lipschitz settings, overcoming limitations of traditional explicit schemes.

Abstract: We are investigating the first strong convergence analysis of a numerical
method for stochastic differential algebraic equations (SDAEs) under a
non-global Lipschitz setting. It is well known that the explicit Euler scheme
fails to converge strongly to the exact solution of a stochastic differential
equation (SDEs) when at least one of the coefficients grows superlinearly. The
problem becomes more challenging in the case of stochastic
differential-algebraic equations (SDAEs) due to the singularity of the matrix.
To address this, we build a new scheme called the semi-implicit tamed method
for SDAEs and provide its strong convergence result under non-global Lipschitz
setting. In other words, the linear component of the drift term is approximated
implicitly, whereas its nonlinear component is tamed and approximated
explicitly. We show that this method strongly converges with order
$\frac{1}{2}$ to the exact solution. To prove this strong convergence result,
we first derive an equivalent scheme, that we call the dual tamed scheme, which
is more suitable for mathematical analysis and is associated with the inherent
stochastic differential equation obtained by eliminating the constraints from
the original SDAEs. To demonstrate the effectiveness of the proposed scheme,
numerical simulations are performed, confirming that the theoretical findings
are consistent with the numerical results.

</details>


### [7] [Fast Operator-Splitting Methods for Nonlinear Elliptic Equations](https://arxiv.org/abs/2509.09132)
*Jingyu Yang,Shingyu Leung,Jianliang Qian,Hao Liu*

Main category: math.NA

TL;DR: Novel operator-splitting/finite element method for solving nonlinear elliptic problems, including semilinear and fully nonlinear Monge-Ampère type equations, with improved efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Nonlinear elliptic problems are important in plasma physics, astrophysics, and optimal transport, but existing methods may lack efficiency or struggle with complex geometries.

Method: Introduces auxiliary function for semilinear elliptic PDEs, develops convergent operator-splitting/finite element scheme, extends to fully nonlinear equations via reformulation, uses mixed finite element method for solution and Hessian approximation.

Result: Numerical experiments demonstrate superior efficiency and accuracy compared to existing approaches, with capability to handle domains with curved boundaries.

Conclusion: The proposed method provides an effective and accurate computational framework for solving various nonlinear elliptic problems, particularly those of Monge-Ampère type, with practical applicability to complex geometries.

Abstract: Nonlinear elliptic problems arise in many fields, including plasma physics,
astrophysics, and optimal transport. In this article, we propose a novel
operator-splitting/finite element method for solving such problems. We begin by
introducing an auxiliary function in a new way for a semilinear elliptic
partial differential equation, leading to the development of a convergent
operator-splitting/finite element scheme for this equation. The algorithm is
then extended to fully nonlinear elliptic equations of the Monge-Amp\`ere type,
including the Dirichlet Monge-Amp\`ere equation and Pucci's equation. This is
achieved by reformulating the fully nonlinear equations into forms analogous to
the semilinear case, enabling the application of the proposed splitting
algorithm. In our implementation, a mixed finite element method is used to
approximate both the solution and its Hessian matrix. Numerical experiments
show that the proposed method outperforms existing approaches in efficiency and
accuracy, and can be readily applied to problems defined on domains with curved
boundaries.

</details>


### [8] [Hybrid-Precision Block-Jacobi Preconditioned GMRES Solver for Linear System in Circuit Simulation](https://arxiv.org/abs/2509.09139)
*Zijian Zhang,Rui Hong,Xuesong Chen,Shuting Cai*

Main category: math.NA

TL;DR: Hybrid-precision block-Jacobi preconditioned GMRES solver for circuit simulation that uses single-precision for intensive tasks and double-precision for accuracy-critical computations, with graph partitioning for optimized preconditioning.


<details>
  <summary>Details</summary>
Motivation: Traditional solvers struggle with large-scale sparse systems in circuit simulation, leading to prolonged simulation times and reduced accuracy as integrated circuits become more complex.

Method: Proposes a hybrid-precision block-Jacobi preconditioned GMRES solver that leverages structural sparsity and block properties of circuit matrices. Uses graph partitioning tools for preconditioner generation and restart strategy for large-scale problems. Includes mathematical convergence and error analysis.

Result: Numerical experiments show the approach significantly outperforms existing solvers (SuperLU, KLU, SFLU) in both preconditioning and GMRES runtime. The hybrid-precision preconditioner improves spectral clustering for faster solutions.

Conclusion: The proposed method provides an efficient and accurate solution for large sparse systems in circuit simulation, demonstrating superior performance over state-of-the-art solvers through optimized hybrid-precision computation and preconditioning strategies.

Abstract: As integrated circuits become increasingly complex, the demand for efficient
and accurate simulation solvers continues to rise. Traditional solvers often
struggle with large-scale sparse systems, leading to prolonged simulation times
and reduced accuracy. In this paper, a hybrid-precision block-Jacobi
preconditioned GMRES solver is proposed to solve the large sparse system in
circuit simulation. The proposed method capitalizes on the structural sparsity
and block properties of circuit matrices, employing a novel hybrid-precision
strategy that applies single-precision arithmetic for computationally intensive
tasks and double-precision arithmetic for critical accuracy-sensitive
computations. Additionally, we use the graph partitioning tools to assist in
generating preconditioners, ensuring an optimized preconditioning process. For
large-scale problems, we adopt the restart strategy to increase the
computational efficiency. Through rigorous mathematical reasoning, the
convergence and error analysis of the proposed method are carried out.
Numerical experiments on various benchmark matrices demonstrate that our
approach significantly outperforms existing solvers, including SuperLU, KLU,
and SFLU, in terms of both preconditioning and GMRES runtime. The proposed
hybrid-precision preconditioner effectively improves spectral clustering,
leading to faster solutions.

</details>


### [9] [Isogeometric Topology Optimization Based on Topological Derivatives](https://arxiv.org/abs/2509.09236)
*Guilherme Henrique Teixeira,Nepomuk Krenn,Peter Gangl,Benjamin Marussig*

Main category: math.NA

TL;DR: Isogeometric topology optimization using topological derivatives and level-set method that eliminates remeshing requirements, with investigation of higher-degree basis functions.


<details>
  <summary>Details</summary>
Motivation: Traditional topology optimization requires remeshing for topological changes, which can be challenging. The paper aims to develop an approach that enables seamless geometry updates without remeshing.

Method: Combines level-set method with immersed isogeometric framework driven by topological derivatives. Uses higher-degree basis functions for solution approximation while linear basis functions for level-set representation.

Result: Two numerical examples demonstrate that higher-degree basis functions improve solution accuracy, while linear basis functions remain sufficient for level-set function representation.

Conclusion: The proposed isogeometric approach successfully eliminates remeshing requirements and topological derivatives enable modifications without initial holes. Higher-degree basis functions benefit solution accuracy but linear ones suffice for level-set representation.

Abstract: Topology optimization is a valuable tool in engineering, facilitating the
design of optimized structures. However, topological changes often require a
remeshing step, which can become challenging. In this work, we propose an
isogeometric approach to topology optimization driven by topological
derivatives. The combination of a level-set method together with an immersed
isogeometric framework allows seamless geometry updates without the necessity
of remeshing. At the same time, topological derivatives provide topological
modifications without the need to define initial holes [7]. We investigate the
influence of higher-degree basis functions in both the level-set representation
and the approximation of the solution. Two numerical examples demonstrate the
proposed approach, showing that employing higher-degree basis functions for
approximating the solution improves accuracy, while linear basis functions
remain sufficient for the level-set function representation.

</details>


### [10] [Long time strong convergence analysis of one-step methods for McKean-Vlasov SDEs with superlinear growth coefficients](https://arxiv.org/abs/2509.09274)
*Taiyuan Liu,Yaozhong Hu,Siqing Gan*

Main category: math.NA

TL;DR: Strong convergence rate analysis for discretization approximations of McKean-Vlasov SDEs with super-linear growth coefficients over infinite time horizon.


<details>
  <summary>Details</summary>
Motivation: To analyze the convergence properties of numerical schemes for McKean-Vlasov SDEs with non-globally Lipschitz coefficients over infinite time periods, which is challenging due to super-linear growth.

Method: Derived propagation of chaos and mean-square convergence rate for general one-step time discretization schemes under specified non-globally Lipschitz conditions. Applied to projected Euler and backward Euler schemes.

Result: Obtained mean-square convergence rate over infinite time horizon for numerical schemes in non-globally Lipschitz settings. Numerical experiments validated theoretical findings.

Conclusion: The paper provides rigorous convergence analysis for discretization methods of McKean-Vlasov SDEs with super-linear growth coefficients, establishing theoretical foundations for numerical approximations over infinite time horizons.

Abstract: This paper presents a strong convergence rate analysis of general
discretization approximations for McKean-Vlasov SDEs with super-linear growth
coefficients over infinite time horizon. Under some specified non-globally
Lipschitz conditions, we derive the propagation of chaos, and the mean-square
convergence rate over infinite time horizon for general one-step time
discretization schemes for the underlying Mckean-Vlasov SDEs. As an application
of the general result it is obtained the mean-square convergence rate over
infinite time horizon for two numerical schemes: the projected Euler scheme and
the backward Euler scheme for Mckean-Vlasov SDEs in non-globally Lipschitz
settings. Numerical experiments are provided to validate the theoretical
findings.

</details>


### [11] [Optimal Control of a Hemivariational Inequality of Stationary Convective Brinkman-Forchheimer Extended Darcy equations with Numerical Approximation](https://arxiv.org/abs/2509.09287)
*Wasim Akram,Manil T. Mohan*

Main category: math.NA

TL;DR: Analysis of optimal control for convective Brinkman-Forchheimer extended Darcy hemivariational inequality with control constraints and numerical implementation using finite elements.


<details>
  <summary>Details</summary>
Motivation: To develop optimal control framework for incompressible fluid flow through porous media governed by CBFeD equations, accounting for convection, damping, and nonlinear resistance effects with non-leak boundary and friction conditions.

Method: Analyze solution stability to perturbations, prove existence of optimal control solutions, develop numerical scheme with finite element discretization, and validate with numerical examples.

Result: Established stability analysis, proved existence of optimal control solutions, developed convergent numerical scheme, and provided validation through numerical implementation.

Conclusion: Successfully developed comprehensive framework for optimal control of CBFeD hemivariational inequalities with theoretical guarantees and practical numerical implementation.

Abstract: This paper studies an optimal control problem for a stationary convective
Brinkman-Forchheimer extended Darcy (CBFeD) hemivariational inequality in two
and three dimensions, subject to control constraints, and develops its
numerical approximation. The hemivariational inequality provides the weak
formulation of a stationary incompressible fluid flow through a porous medium,
governed by the CBFeD equations, which account for convection, damping, and
nonlinear resistance effects. The problem incorporates a non-leak boundary
condition and a subdifferential friction-type condition. We first analyze the
stability of solutions with respect to perturbations in the external force
density and the superpotential. Next, we prove the existence of a solution to
the optimal control problem, where the external force density acts as the
control variable. We then propose a numerical scheme for solving the optimal
control problem and establish its convergence. For concreteness, the numerical
method is implemented using finite element discretization. Finally, we provide
some numerical examples to validate the theory developed.

</details>


### [12] [Euler-type methods for Levy-driven McKean-Vlasov SDEs with super-linear coefficients: mean-square error analysis](https://arxiv.org/abs/2509.09302)
*Jingtao Zhu,Yuying Zhao,Siqing Gan*

Main category: math.NA

TL;DR: Developed Euler-type numerical schemes for Levy-driven McKean-Vlasov SDEs with super-linear growth coefficients, removing restrictive coercivity assumptions and achieving near 1/2 convergence rates.


<details>
  <summary>Details</summary>
Motivation: To address Levy-driven McKean-Vlasov SDEs with super-linear growth coefficients where classical Euler methods fail and existing approaches rely on restrictive coercivity conditions.

Method: Developed a class of Euler-type schemes incorporating projections or nonlinear transformations (tanh-Euler, tamed-Euler, sine-Euler) to establish moment bounds for numerical solutions without coercivity assumptions.

Result: Proposed schemes achieve convergence rates arbitrarily close to 1/2 for interacting particle systems associated with Levy-driven McKean-Vlasov SDEs, validated through numerical examples.

Conclusion: The new class of Euler-type schemes successfully handles super-linear growth in Levy-driven McKean-Vlasov SDEs without coercivity conditions, providing rigorous convergence analysis and practical numerical performance.

Abstract: We develop and analyze a general class of Euler-type numerical schemes for
Levy-driven McKean-Vlasov stochastic differential equations (SDEs), where the
drift, diffusion and jump coefficients grow super-linearly in the state
variable. These numerical schemes are derived by incorporating projections or
nonlinear transformations into the classical Euler method, with the primary
objective of establishing moment bounds for the numerical solutions. This class
of schemes includes the tanh-Euler, tamed-Euler and sine-Euler schemes as
special cases. In contrast to existing approaches that rely on a coercivity
condition (e.g., Assumption B-1 in Kumar et al., arXiv:2010.08585), the
proposed schemes remove such a restrictive assumption. We provide a rigorous
mean-square convergence analysis and establish that the proposed schemes
achieve convergence rates arbitrarily close to 1/2 for the interacting particle
systems associated with Levy-driven McKean-Vlasov SDEs. Several numerical
examples are presented to illustrate the convergence behavior and validate the
theoretical results.

</details>


### [13] [Expressive Power of Deep Networks on Manifolds: Simultaneous Approximation](https://arxiv.org/abs/2509.09362)
*Hanfei Zhou,Lei Shi*

Main category: math.NA

TL;DR: First simultaneous approximation theory for deep neural networks on manifolds, showing ReLU networks can approximate Sobolev functions with optimal parameter efficiency that overcomes curse of dimensionality.


<details>
  <summary>Details</summary>
Motivation: Solving PDEs on complex curved domains is challenging due to geometry complicating function and derivative approximations required by differential operators.

Method: Prove that constant-depth ReLU networks with bounded weights can approximate Sobolev space functions on manifolds with error ε using O(ε^{-d/(k-s)}) parameters, and provide matching lower bounds with novel VC dimension estimates.

Result: Achieves approximation rate that depends only on intrinsic dimension d, overcoming curse of dimensionality. Construction is nearly optimal with matching lower bounds up to logarithmic factors.

Conclusion: Provides theoretical foundation for learning PDEs on manifolds, revealing networks leverage sparse structure to exploit low-dimensional geometry efficiently.

Abstract: A key challenge in scientific machine learning is solving partial
differential equations (PDEs) on complex domains, where the curved geometry
complicates the approximation of functions and their derivatives required by
differential operators. This paper establishes the first simultaneous
approximation theory for deep neural networks on manifolds. We prove that a
constant-depth $\mathrm{ReLU}^{k-1}$ network with bounded weights--a property
that plays a crucial role in controlling generalization error--can approximate
any function in the Sobolev space $\mathcal{W}_p^{k}(\mathcal{M}^d)$ to an
error of $\varepsilon$ in the $\mathcal{W}_p^{s}(\mathcal{M}^d)$ norm, for
$k\geq 3$ and $s<k$, using $\mathcal{O}(\varepsilon^{-d/(k-s)})$ nonzero
parameters, a rate that overcomes the curse of dimensionality by depending only
on the intrinsic dimension $d$. These results readily extend to functions in
H\"older-Zygmund spaces. We complement this result with a matching lower bound,
proving our construction is nearly optimal by showing the required number of
parameters matches up to a logarithmic factor. Our proof of the lower bound
introduces novel estimates for the Vapnik-Chervonenkis dimension and
pseudo-dimension of the network's high-order derivative classes. These
complexity bounds provide a theoretical cornerstone for learning PDEs on
manifolds involving derivatives. Our analysis reveals that the network
architecture leverages a sparse structure to efficiently exploit the manifold's
low-dimensional geometry.

</details>


### [14] [A Low-Rank tensor framework for THB-Splines](https://arxiv.org/abs/2509.09434)
*Tom-Christian Riemer,Martin Stoll*

Main category: math.NA

TL;DR: A low-rank framework for adaptive isogeometric analysis with THB-splines that reduces memory and time requirements for matrix assembly through tensor-train representations and level-wise assembly.


<details>
  <summary>Details</summary>
Motivation: To address the memory- and time-intensive matrix assembly bottleneck in adaptive isogeometric analysis when the global tensor-product structure is lost during local refinement.

Method: Interpolates geometry-induced weight and source terms in separable spline spaces, computes tensor-train representations via AMEn solver, reduces active basis to tensor-product domains, partitions cells into Cartesian cuboids, and assembles hierarchical operators in block TT format.

Result: For model problems with moderately complex refinement regions, the approach reduces memory footprint and assembly time while maintaining accuracy, though limitations exist when ranks grow with geometric or refinement complexity.

Conclusion: This framework advances scalable adaptive IgA with THB-splines, particularly in three dimensions, by providing efficient low-rank assembly methods.

Abstract: We introduce a low-rank framework for adaptive isogeometric analysis with
truncated hierarchical B-splines (THB-splines) that targets the main bottleneck
of local refinement: memory- and time-intensive matrix assembly once the global
tensor-product structure is lost. The method interpolates geometry-induced
weight and source terms in separable spline spaces and computes their
tensor-train (TT) representations via the alternating minimal energy (AMEn)
solver, enabling level-wise assembly of system operators using univariate
quadrature. To recover separability in the adaptive setting, we reduce the
active basis to tensor-product domains and partition active/non-active cells
into a small number of Cartesian cuboids, so each contributes a Kronecker
factor that is accumulated and rounded in TT. We realize the two-scale relation
with truncation in low rank and assemble the global hierarchical operators in a
block TT format suitable for iterative solvers. A prototype MATLAB
implementation built on the GeoPDEs package and the TT-Toolbox demonstrates
that, for model problems with moderately complex refinement regions, the
approach reduces memory footprint and assembly time while maintaining accuracy;
we also discuss limitations when ranks grow with geometric or refinement
complexity. This framework advances scalable adaptive IgA with THB-splines,
particularly in three dimensions.

</details>


### [15] [Second-order Optimally Stable IMEX (pseudo-)staggered Galerkin discretization: application to lava flow modeling](https://arxiv.org/abs/2509.09460)
*Federico Gatti,Giuseppe Orlando*

Main category: math.NA

TL;DR: Second-order optimally stable IMEX Runge-Kutta schemes for lava flow modeling using shallow water equations, featuring a novel pseudo-staggered Galerkin scheme and path-conservative method for non-conservative terms.


<details>
  <summary>Details</summary>
Motivation: To develop stable and accurate numerical schemes for modeling lava flow dynamics using modified shallow water equations, addressing the need for robust discretization methods that handle non-conservative terms effectively.

Method: Combines second-order optimally stable IMEX Runge-Kutta schemes with a novel pseudo-staggered Galerkin scheme (extension of Taylor-Galerkin TG2), using von Neumann stability analysis and Lax-Wendroff procedure. Employs Path-Conservative method for non-conservative terms.

Result: The proposed scheme demonstrates accuracy, robustness, and well-balancing properties across multiple relevant test cases for the lava flow model.

Conclusion: The developed IMEX Runge-Kutta schemes with pseudo-staggered Galerkin discretization provide an effective and stable numerical framework for simulating lava flow dynamics using modified shallow water equations.

Abstract: We present second-order optimally stable Implicit-Explicit (IMEX) Runge-Kutta
(RK) schemes with application to a modified set of shallow water equations that
can be used to model the dynamics of lava flows. The schemes are optimally
stable in the sense that they satisfy, at the space-time discretization level,
a condition analogous to the \texttt{L}-stability of Runge-Kutta methods for
ordinary differential equations. A novel (pseudo-)staggered Galerkin scheme is
introduced, which can be interpreted as an extension of the classical two-step
Taylor-Galerkin (TG2) scheme. The method is derived by combining a von Neumann
stability analysis with a Lax-Wendroff procedure. For the discretization of the
non-conservative terms that characterize the lava flow model, we employ the
Path-Conservative (PC) method. The proposed scheme is evaluated on a number of
relevant test cases, demonstrating accuracy, robustness, and well-balancing
properties for the lava flow model.

</details>


### [16] [Minimality of Tree Tensor Network Ranks](https://arxiv.org/abs/2509.09463)
*Jana Jovcheva,Tim Seynnaeve,Nick Vannieuwenhoven*

Main category: math.NA

TL;DR: Characterization of minimal bond dimensions in tree tensor networks, generalizing multilinear rank results and showing minimality is generic.


<details>
  <summary>Details</summary>
Motivation: To establish necessary and sufficient conditions for bond dimensions to be minimal in tree tensor networks, extending previous work on multilinear rank characterization.

Method: Mathematical analysis of tree tensor network structures, generalizing Carlini and Kleppe's characterization of tensors with given multilinear rank to tree topologies.

Result: Established necessary and sufficient conditions for bond dimensions to be minimal, and proved that non-minimal tensors form a Zariski closed subset, making minimality a generic property.

Conclusion: The paper provides a complete characterization of minimal bond dimensions in tree tensor networks and demonstrates that minimality is a generic property within these networks.

Abstract: For a given tree tensor network $G$, we call a tuple of bond dimensions
minimal if there exists a tensor $T$ that can be represented by this network
but not on the same tree topology with strictly smaller bond dimensions. We
establish necessary and sufficient conditions on the bond dimensions of a tree
tensor network to be minimal, generalizing a characterization of Carlini and
Kleppe about existence of tensors with a given multilinear rank. We also show
that in a minimal tree tensor network, the non-minimal tensors form a Zariski
closed subset, so minimality is a generic property in this sense.

</details>


### [17] [Bioluminescence tomography: A new regularized shape optimization method](https://arxiv.org/abs/2509.09533)
*Qianqian Wu,Rongfang Gong,Wei Gong,Ziyi Zhang,Shengfeng Zhu*

Main category: math.NA

TL;DR: A shape optimization framework for bioluminescence tomography that recovers light source support and intensity using level-set representation with regularization techniques for stable reconstruction.


<details>
  <summary>Details</summary>
Motivation: To solve the inverse source problem in bioluminescence tomography by developing a robust method that can accurately recover both the spatial support and intensity of light sources from boundary measurements.

Method: Developed a shape optimization framework that decouples source strength and support through first-order optimality conditions, incorporating parameter-dependent coupled complex boundary method (CCBM) with perimeter and volume regularizations, using level-set representation to handle topological changes.

Result: The algorithm demonstrates robustness, accuracy, and noise-resistance in numerical experiments, successfully reconstructing multiple, closely located, or nested sources with advantages over existing approaches.

Conclusion: The proposed method provides an effective solution for bioluminescence tomography inverse problems, offering theoretical justification and practical validation through comprehensive numerical experiments.

Abstract: In this paper, we investigate an inverse source problem arising in
bioluminescence tomography (BLT), where the objective is to recover both the
support and intensity of the light source from boundary measurements. A shape
optimization framework is developed, in which the source strength and its
support are decoupled through first-order optimality conditions. To enhance the
stability of the reconstruction, we incorporate a parameter-dependent coupled
complex boundary method(CCBM) scheme together with perimeter and volume
regularizations. The level-set representation naturally accommodates
topological changes, enabling the reconstruction of multiple, closely located,
or nested sources. Theoretical justifications are provided, and a series of
numerical experiments are conducted to validate the proposed method. The
results demonstrate the robustness, accuracy, and noise-resistance of the
algorithm, as well as its advantages over existing approaches.

</details>


### [18] [Iterative energy reduction Galerkin methods and variational adaptivity](https://arxiv.org/abs/2509.09600)
*Pascal Heid,Thomas P. Wihler*

Main category: math.NA

TL;DR: Novel computational approach that incorporates energy structure alongside PDEs for solving critical point problems, featuring energy-reducing iterative Galerkin schemes and variational mesh refinement.


<details>
  <summary>Details</summary>
Motivation: Classical methods focus only on PDEs, but energy functionals are crucial in physics, chemistry, mechanics, and material science. The paper aims to leverage the energy structure itself for more effective computational solutions.

Method: Proposes (linearized) iterative Galerkin discretization schemes that ensure energy reduction at each step. Introduces variational adaptivity - an adaptive mesh refinement strategy based on local energy reductions rather than classical a posteriori estimates.

Result: Provides necessary conditions for convergence to critical points applicable to a wide class of problems. Validates theoretical results through computational experiments on nonlinear diffusion-reaction models.

Conclusion: The proposed scheme effectively combines energy structure with PDE solution methods, demonstrating improved computational performance through energy-reducing iterations and variational mesh refinement strategies.

Abstract: Critical points of energy functionals, which are of broad interest, for
instance, in physics and chemistry, in solid and quantum mechanics, in material
science, or in general diffusion-reaction models arise as solutions to the
associated Euler-Lagrange equations. While classical computational solution
methods for such models typically focus solely on the underlying partial
differential equations, we propose an approach that also incorporates the
energy structure itself. Specifically, we examine (linearized) iterative
Galerkin discretization schemes that ensure energy reduction at each step.
Additionally, we provide necessary conditions, which are applicable to a wide
class of problems, that guarantee convergence to critical points of the PDE.
Moreover, in the specific context of finite element discretizations, we present
a very generally applicable adaptive mesh refinement strategy - the so-called
variational adaptivity approach - which, rather than using classical a
posteriori estimates, is based on exploiting local energy reductions. The
theoretical results are validated for several computational experiments in the
context of nonlinear diffusion-reaction models, thereby demonstrating the
effectiveness of the proposed scheme.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [19] [Logarithmic wave decay for short range wavespeed perturbations with radial regularity](https://arxiv.org/abs/2509.08957)
*Gayana Jayasinghe,Katrina Morgan,Jacob Shapiro,Mengxuan Yang*

Main category: math.AP

TL;DR: Logarithmic local energy decay for wave equations with varying wavespeed in dimensions ≥2, with short-range perturbations of unity and mild radial regularity.


<details>
  <summary>Details</summary>
Motivation: To establish energy decay properties for wave equations with variable wavespeed, which is important for understanding wave propagation in inhomogeneous media.

Method: Uses Hölder continuity of weighted resolvent for real frequencies with logarithmic remainder in 2D. Approach involves studying resolvent in two frequency regimes: low frequencies (Neumann series expansion) and frequencies away from zero (Carleman estimate).

Result: Successfully establishes logarithmic local energy decay for wave equations with varying wavespeed in dimensions two and higher.

Conclusion: The method provides logarithmic energy decay for wave equations with short-range perturbations of unity wavespeed, with different behavior in dimension two due to logarithmic remainder as frequency approaches zero.

Abstract: We establish logarithmic local energy decay for wave equations with a varying
wavespeed in dimensions two and higher, where the wavespeed is assumed to be a
short range perturbation of unity with mild radial regularity. The key
ingredient is H\"older continuity of the weighted resolvent for real
frequencies $\lambda$, modulo a logarithmic remainder in dimension two as
$\lambda \to 0$. Our approach relies on a study of the resolvent in two
distinct frequency regimes. In the low frequency regime, we derive an expansion
for the resolvent using a Neumann series and properties of the free resolvent.
For frequencies away from zero, we establish a uniform resolvent estimate by
way of a Carleman estimate.

</details>


### [20] [Mountain Pass Critical Points of the Liquid Drop Model](https://arxiv.org/abs/2509.09098)
*Gregory R. Chambers,Jared Marx-Kuo*

Main category: math.AP

TL;DR: Construction of non-minimizing critical points for Gamow's liquid drop functional in volume range 3.512 < V < 10, representing maximal energy configuration during nuclear fission.


<details>
  <summary>Details</summary>
Motivation: To understand the energy landscape of Gamow's liquid drop model during nuclear fission, specifically the transition state between a single atom and two fission fragments.

Method: Geometric measure theoretical methods from min-max construction of minimal surfaces, addressing non-compactness, volume constraint "pull tight", and multiplicity issues.

Result: Established mountain pass setup between ball of volume V and two balls of volume V/2 infinitely far apart, constructing critical points representing maximal energy fission configurations.

Conclusion: Successfully constructed non-minimizing critical points that represent the energy barrier in nuclear fission processes within the specified volume range.

Abstract: We consider Gamow's liquid drop functional, $\mathcal{E}$, on $\mathbb{R}^3$
and construct non-minimizing, volume constrained, critical points for volumes
$3.512 \cong \alpha_0 < V < 10$. In this range, we establish a mountain pass
set up between a ball of volume $V$ and two balls of volume $V/2$ infinitely
far apart. Intuitively, our critical point corresponds to the maximal energy
configuration of an atom of volume $V$ as it undergoes fission into two atoms
of volume $V/2$. Our proof relies on geometric measure theoretical methods from
the min-max construction of minimal surfaces, and along the way, we address
issues of non-compactness, ``pull tight" with a volume constraint, and
multiplicity.

</details>


### [21] [On the Convergence of Solutions for the Ginzburg-Landau Equation and System](https://arxiv.org/abs/2509.09231)
*Rejeb Hadiji,Jongmin Han*

Main category: math.AP

TL;DR: The paper shows that if Ginzburg-Landau solutions have energy bounded above by the harmonic map energy, they must converge to the harmonic map. If energy exceeds the harmonic map energy, uniform convergence to 1 fails. Results extend to symmetric and non-symmetric two-component systems.


<details>
  <summary>Details</summary>
Motivation: To establish precise energy bounds and convergence properties for Ginzburg-Landau equation solutions compared to harmonic maps, particularly understanding when solutions converge to harmonic maps versus when they exhibit different behavior.

Method: Mathematical analysis of Ginzburg-Landau equation solutions with boundary conditions, comparing their energy with that of harmonic maps. Uses energy estimates and convergence arguments in Sobolev spaces.

Result: Proves that if solutions' energy is bounded above by harmonic map energy, they converge to harmonic map in H^1. If energy exceeds harmonic map energy, |u_ε| does not converge uniformly to 1. Results hold for both single and two-component systems.

Conclusion: The energy threshold of the harmonic map determines convergence behavior: sub-threshold energy implies convergence to harmonic map, while super-threshold energy prevents uniform convergence to 1, with extensions to multi-component systems.

Abstract: Let $(u_\varepsilon)$ be a family of solutions of the Ginzburg--Landau
equation with boundary condition $u_\varepsilon = g$ on $\partial \Omega$ and
of degree $0$. Let $u_0$ denote the harmonic map satisfying $u_0 = g$ on
$\partial \Omega$. We show that, if there exists a constant $C_1 > 0$ such that
for $\varepsilon$ sufficiently small we have $\frac{1}{2} \int_\Omega |\nabla
u_\ve|^2 dx \leq C_1 \leq \frac{1}{2} \int_\Omega |\nabla u_0|^2 dx,$ then $C_1
= \frac{1}{2} \int_\Omega |\nabla u_0|^2 dx$ and
  $u_\ve ~\to ~ u_0 \qin H^1(\Om)$.
  We also prove that if there is a constant $C_2$ such that for $\ve$ small
enough we have $ \frac12 \int_\Om |\nabla u_\ve|^2 dx \geq C_2 > \frac12
\int_\Om |\nabla u_0|^2 dx,$ then $|u_{\ve}|$ does not converge uniformly to
$1$ on $\overline{\Om} $. We obtain analogous results for both symmetric and
non-symmetric two-component Ginzburg--Landau systems.

</details>


### [22] [Functions of bounded Musielak-Orlicz-type deformation and anisotropic Total Generalized Variation for image-denoising problems](https://arxiv.org/abs/2509.09237)
*Giacomo Bertazzoni,Elisa Davoli,Samuele Ricco`,Elvira Zappale*

Main category: math.AP

TL;DR: This paper introduces bounded deformation fields with generalized Orlicz growth and establishes their properties, then defines Musielak-Orlicz anisotropic Total Generalized Variation for image reconstruction.


<details>
  <summary>Details</summary>
Motivation: To develop a mathematical framework for analyzing deformation fields with generalized growth conditions and apply it to image reconstruction problems using advanced variational methods.

Method: Introduces space of bounded deformation fields with generalized Orlicz growth, establishes modular representation and decomposition. Defines Musielak-Orlicz anisotropic Total Generalized Variation with duality representation.

Result: Established main properties of bounded deformation fields, provided modular decomposition, and showed well-posedness of image reconstruction using the proposed anisotropic Total Generalized Variation framework.

Conclusion: The paper provides a comprehensive mathematical framework for deformation analysis with generalized growth conditions and demonstrates its applicability to image reconstruction through well-posed variational formulations.

Abstract: In the first part of this paper we introduce the space of bounded deformation
fields with generalized Orlicz growth. We establish their main properties,
provide a modular representation, and characterize a decomposition of the
modular into an absolutely continuous part and a singular part weighted via a
recession function. A further analysis in the variable exponent case is also
provided. The second part of the paper contains a notion of Musielak-Orlicz
anisotropic Total Generalized Variation. We establish a duality representation,
and show well-posedness of the corresponding image reconstruction problem.

</details>


### [23] [Numerical analysis of the homogeneous Landau equation: approximation, error estimates and simulation](https://arxiv.org/abs/2509.09276)
*Francis Filbet,Yanzhi Gui,Ling-Bing He*

Main category: math.AP

TL;DR: Numerical solution for Landau equation with Coulomb potential using spectral method, with explicit error bounds showing convergence for sufficiently large domain and Fourier modes.


<details>
  <summary>Details</summary>
Motivation: To provide rigorous mathematical validation for spectral methods in solving the spatially homogeneous Landau equation with Coulomb potential, ensuring numerical solutions meet prescribed error tolerances.

Method: Construct numerical solution using spectral method with N Fourier modes on domain D_L, derive explicit error estimates in terms of L (domain size) and N (number of modes), and perform numerical simulations to validate the approach.

Result: Demonstrated that for any error tolerance and fixed time interval, there exist explicit conditions for L and N such that numerical error falls below the tolerance. Numerical simulations confirm the theoretical error estimates.

Conclusion: The spectral method is mathematically valid for solving Landau equation with Coulomb potential, with explicit convergence guarantees through proper selection of domain size and Fourier mode count.

Abstract: We construct a numerical solution to the spatially homogeneous Landau
equation with Coulomb potential on a domain $D_L$ with N retained Fourier
modes. By deriving an explicit error estimate in terms of $L$ and $N$, we
demonstrate that for any prescribed error tolerance and fixed time interval
$[0, T ]$, there exist choices of $D_L$ and $N$ satisfying explicit conditions
such that the error between the numerical and exact solutions is below the
tolerance. Specifically, the estimate shows that sufficiently large $L$ and $N$
(depending on initial data parameters and $T$) can reduce the error to any
desired level. Numerical simulations based on this construction are also
presented. The results in particular demonstrate the mathematical validity of
the spectral method proposed in the referenced literature.

</details>


### [24] [Well-posedness of stationary 2D and 3D convective Brinkman-Forchheimer extended Darcy Hemivariational inequalities](https://arxiv.org/abs/2509.09335)
*Manil T. Mohan*

Main category: math.AP

TL;DR: Analysis of well-posedness for convective Brinkman-Forchheimer extended Darcy model hemivariational inequalities in 2D/3D, establishing existence and uniqueness of weak solutions through convex minimization and fixed-point methods.


<details>
  <summary>Details</summary>
Motivation: To address the mathematical well-posedness of the CBFeD model describing incompressible viscous fluid flow through porous media with convection, damping, and nonlinear resistance effects under non-monotone boundary conditions.

Method: Introduces auxiliary hemivariational inequality resembling nonlinear Stokes-type problems, integrates convex minimization theory with fixed-point methods (Banach contraction or Schauder's theorem), and develops iterative algorithms solving Stokes-type problems sequentially.

Result: Establishes existence and uniqueness of weak solutions for the CBFeD model, provides practical iterative algorithms with convergence guarantees, and derives equivalent variational formulations for numerical scheme design.

Conclusion: The study successfully demonstrates well-posedness of the CBFeD hemivariational inequality and provides mathematical foundations for developing efficient numerical methods to simulate porous media flows governed by this model.

Abstract: This study addresses the well-posedness of a hemivariational inequality
derived from the convective Brinkman-Forchheimer extended Darcy (CBFeD) model
in both two and three dimensions. The CBFeD model describes the behavior of
incompressible viscous fluid flow through a porous medium, incorporating the
effects of convection, damping, and nonlinear resistance. The mathematical
framework captures steady-state flow conditions under a no-slip boundary
assumption, with a non-monotone boundary condition that links the total fluid
pressure and the velocity's normal component through a Clarke subdifferential
formulation. To facilitate the analysis, we introduce an auxiliary
hemivariational inequality resembling a nonlinear Stokes-type problem with
damping and pumping terms, which serves as a foundational tool in establishing
the existence and uniqueness of weak solutions for the CBFeD model. The
analytical strategy integrates techniques from convex minimization theory with
fixed-point methods, specifically employing either the Banach contraction
mapping principle or Schauder's fixed point theorem. The Banach-based approach,
in particular, leads to a practical iterative algorithm that solves the
original nonlinear hemivariational inequality by sequentially solving
Stokes-type problems, ensuring convergence of the solution sequence.
Additionally, we derive equivalent variational formulations in terms of
minimization problems. These formulations lay the groundwork for the design of
efficient and stable numerical schemes tailored to simulate flows governed by
the CBFeD model.

</details>


### [25] [New Homogeneous Solutions for the One-Phase Free Boundary Problem](https://arxiv.org/abs/2509.09409)
*Coleman Hines,James Kolesar,Peter McGrath*

Main category: math.AP

TL;DR: Construction of domains in 2-sphere with k boundary components as links of cones in R^3 admitting homogeneous solutions to one-phase free boundary problem, answering Jerison-Kamburov's question and disproving Souam's conjecture.


<details>
  <summary>Details</summary>
Motivation: To answer Jerison-Kamburov's question about existence of domains with multiple boundary components serving as links of cones with homogeneous solutions, and to test Souam's conjecture.

Method: Exploits a new connection with minimal surfaces, using geometric construction techniques to build domains with specified boundary components.

Result: Successfully constructed domains with k boundary components for sufficiently large integers k, and also constructed an infinite family of homogeneous solutions in dimension four.

Conclusion: The paper provides affirmative answer to Jerison-Kamburov's question, disproves Souam's conjecture, and demonstrates the power of connecting free boundary problems with minimal surface theory.

Abstract: For each sufficiently large integer $k$, we construct a domain in the round
$2$-sphere with $k$ boundary components which is the link of a cone in
$\mathbb{R}^3$ admitting a homogeneous solution to the one-phase free boundary
problem. This answers a question of Jerison-Kamburov, and also disproves a
conjecture of Souam left open in earlier work. The method exploits a new
connection with minimal surfaces, which we also use to construct an infinite
family of homogeneous solutions in dimension four.

</details>


### [26] [Optimal convergence rates in multiscale elliptic homogenization](https://arxiv.org/abs/2509.09410)
*Weisheng Niu,Yao Xu,Jinping Zhuge*

Main category: math.AP

TL;DR: Improved homogenization convergence rates for multiscale elliptic operators with analytic coefficients using multiscale correctors, achieving optimal exponential decay rates instead of polynomial ratios.


<details>
  <summary>Details</summary>
Motivation: Classical reiterated homogenization methods for multiscale elliptic operators with multiple scales produce convergence rates limited by polynomial ratios of scale separations, which is suboptimal for analytic coefficients.

Method: Introduces multiscale correctors and more accurate effective operators for elliptic operators with real analytic coefficients, leveraging the analyticity to construct improved approximations.

Result: Achieves optimal convergence rate with exponential decay (max{e^{-cε_i/ε_{i+1}}}) instead of polynomial ratios (max{ε_{i+1}/ε_i}), and establishes uniform Lipschitz estimates under mild double-log scale-separation conditions.

Conclusion: The method provides optimal convergence rates for multiscale homogenization with analytic coefficients, demonstrating that exponential improvement is possible and establishing fundamental regularity results.

Abstract: This paper is devoted to the quantitative homogenization of multiscale
elliptic operator $-\nabla\cdot A_\varepsilon \nabla$, where $A_\varepsilon(x)
= A(x/\varepsilon_1, x/\varepsilon_2,\cdots, x/\varepsilon_n)$, $\varepsilon =
(\varepsilon_1, \varepsilon_2,\cdots, \varepsilon_n) \in (0,1]^n$ and
$\varepsilon_i > \varepsilon_{i+1}$. We assume that $A(y_1,y_2,\cdots, y_n)$ is
1-periodic in each $y_i \in \mathbb{R}^d$ and real analytic. Classically, the
method of reiterated homogenization has been applied to study this multiscale
elliptic operator, which leads to a convergence rate limited by the ratios
$\max \{ \varepsilon_{i+1}/\varepsilon_i: 1\le i\le n-1\}$. In the present
paper, under the assumption of real analytic coefficients, we introduce the
so-called multiscale correctors and more accurate effective operators, and
improve the ratio part of the convergence rate to $\max \{
e^{-c\varepsilon_{i}/\varepsilon_{i+1}}: 1\le i\le n-1 \}$. This convergence
rate is optimal in the sense that $c>0$ cannot be replaced by a larger
constant. As a byproduct, the uniform Lipschitz estimate is established under a
mild double-log scale-separation condition.

</details>


### [27] [Microlocal analysis of the non-relativistic limit of the Klein--Gordon equation: Estimates](https://arxiv.org/abs/2509.09518)
*Andrew Hassell,Qiuye Jia,Ethan Sussman,Andras Vasy*

Main category: math.AP

TL;DR: Robust microlocal framework for analyzing non-relativistic limit of relativistic wave equations with time-dependent coefficients, focusing on Klein-Gordon equation.


<details>
  <summary>Details</summary>
Motivation: To develop a rigorous mathematical framework for studying the non-relativistic limit of relativistic wave equations as the speed of light goes to infinity, addressing two different asymptotic regimes in phase space.

Method: Uses three new pseudodifferential calculi: Ψₙ (semiclassical scattering calculus variant), Ψₙᵣₑₛ, and Ψₙ₂ᵣₑₛ, with the latter two created by second microlocalizing the first at certain locations. Combines analyses from two asymptotic regimes.

Result: Provides global estimates that are uniform as the speed of light goes to infinity, enabling rigorous treatment of the non-relativistic limit.

Conclusion: Establishes a robust microlocal framework for analyzing the non-relativistic limit, with applications presented in a companion paper. The technical approach uses novel pseudodifferential calculi to handle different phase space regimes.

Abstract: This is the more technical half of a two-part work in which we introduce a
robust microlocal framework for analyzing the non-relativistic limit of
relativistic wave equations with time-dependent coefficients, focusing on the
Klein--Gordon equation. Two asymptotic regimes in phase space are relevant to
the non-relativistic limit: one corresponding to what physicists call
``natural'' units, in which the PDE is approximable by the free Klein--Gordon
equation, and a low-frequency regime in which the equation is approximable by
the usual Schrodinger equation. Combining the analyses in the two regimes gives
global estimates which are uniform as the speed of light goes to infinity. The
companion paper gives applications. Our main technical tools are three new
pseudodifferential calculi, $\Psi_{\natural}$ (a variant of the semiclassical
scattering calculus), $\Psi_{\natural\mathrm{res}}$, and
$\Psi_{\natural2\mathrm{res}}$, the latter two of which are created by ``second
microlocalizing'' the first at certain locations. This paper and the companion
paper can be read in either order, since the latter treats the former as a
black box.

</details>


### [28] [Sharp bilinear eigenfunction estimate, $L^\infty_{x_2}L^p_{t,x_1}$-type Strichartz estimate, and energy-critical NLS](https://arxiv.org/abs/2509.09565)
*Yangkendi Deng,Yunfeng Zhang,Zehua Zhao*

Main category: math.AP

TL;DR: Sharp bilinear and multilinear eigenfunction estimates for Laplace-Beltrami operator on S³, eliminating logarithmic loss from previous work. Complete theory for standard spheres using SU(2) representation theory and Clebsch-Gordan coefficients.


<details>
  <summary>Details</summary>
Motivation: Application to energy-critical nonlinear Schrödinger equation (NLS) on ℝ × S³, seeking small data global well-posedness in energy space.

Method: View S³ as compact Lie group SU(2) and exploit representation theory, particularly Clebsch-Gordan coefficients. Prove refined Strichartz estimate of mixed-norm type on cylindrical space for spectrally localized functions.

Result: Derived refined bilinear Strichartz estimate on ℝ × S³, leading to small data global well-posedness for energy-critical NLS in energy space.

Conclusion: Complete elimination of logarithmic loss in eigenfunction estimates on S³, establishing complete theory for standard spheres and enabling energy-critical NLS well-posedness results.

Abstract: We establish sharp bilinear and multilinear eigenfunction estimates for the
Laplace-Beltrami operator on the standard three-sphere $\mathbb{S}^3$,
eliminating the logarithmic loss that has persisted in the literature since the
pioneering work of Burq, G\'erard, and Tzvetkov over twenty years ago. This
completes the theory of multilinear eigenfunction estimates on the standard
spheres. Our approach relies on viewing $\mathbb{S}^3$ as the compact Lie group
$\mathrm{SU}(2)$ and exploiting its representation theory, especially the
properties of Clebsch-Gordan coefficients. Motivated by application to the
energy-critical nonlinear Schr\"odinger equation (NLS) on $\mathbb{R} \times
\mathbb{S}^3$, we also prove a refined Strichartz estimate of mixed-norm type
$L^\infty_{x_2}L^4_{t,x_1}$ on the cylindrical space $\mathbb{R}_{x_1} \times
\mathbb{T}_{x_2}$, adapted to certain spectrally localized functions. Combining
these two ingredients, we derive a refined bilinear Strichartz estimate on
$\mathbb{R} \times \mathbb{S}^3$, which in turn yields small data global
well-posedness for the above mentioned NLS in the energy space.

</details>


### [29] [Stability and asymptotic behaviour of one-dimensional solutions in cylinders](https://arxiv.org/abs/2509.09648)
*Francesca De Marchis,Lisa Mazzuoli,Filomena Pacella*

Main category: math.AP

TL;DR: Analysis of stability/instability properties of positive 1D solutions to Lane-Emden Dirichlet problems in cylinders as energy varies with domain perturbations, focusing on asymptotic behavior near p=1 and large p.


<details>
  <summary>Details</summary>
Motivation: Study how the stability properties of one-dimensional solutions to Lane-Emden relative Dirichlet problems depend on the nonlinearity exponent p, particularly for extreme values (p close to 1 and large p).

Method: Careful asymptotic analysis of one-dimensional solutions as p approaches 1 and infinity, examining limit profiles and qualitative properties through mathematical analysis of the Lane-Emden equation.

Result: Obtained stability/instability results that depend on the exponent p, with specific findings for p close to 1 and for large p values through asymptotic analysis.

Conclusion: The asymptotic analysis reveals important qualitative properties and limit profiles of solutions, providing insights into stability behavior under domain perturbations for different nonlinearity exponents in the Lane-Emden problem.

Abstract: We consider positive one-dimensional solutions of a Lane-Emden relative
Dirichlet problem in a cylinder and study their stability/instability
properties as the energy varies with respect to domain perturbations. This
depends on the exponent $p >1$ of the nonlinearity and we obtain results for
$p$ close to 1 and for $p$ large. This is achieved by a careful asymptotic
analysis of the one-dimensional solution as $p \to 1$ or $p \to \infty$, which
is of independent interest. It allows to detect the limit profile and other
qualitative properties of these solutions.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [30] [HARD: A Performance Portable Radiation Hydrodynamics Code based on FleCSI Framework](https://arxiv.org/abs/2509.08971)
*Julien Loiseau,Hyun Lim,Andrés Yagüe López,Mammadbaghir Baghirzade,Shihab Shahriar Khan,Yoonsoo Kim,Sudarshan Neopane,Alexander Strack,Farhana Taiyebah,Benjamin K. Bergen*

Main category: physics.comp-ph

TL;DR: HARD is an open-source radiation hydrodynamics simulation tool built on FleCSI with Kokkos for performance portability, featuring verification tests and community development.


<details>
  <summary>Details</summary>
Motivation: To create a sustainable, high-performance platform for radiation hydrodynamics research that works across diverse computing environments from laptops to supercomputers.

Method: Built on FleCSI framework with task-based computational units, multiple runtime backends (Legion, MPI, HPX), and Kokkos for node-level parallelism. Includes regression-test suite for verification against analytical solutions.

Result: A single portable code base that runs efficiently across different computing platforms with automatic verification through canonical test problems like Sod shock tube and Sedov blast wave.

Conclusion: HARD provides a sustainable platform combining performance portability, verification infrastructure, and community-focused development for advancing radiation hydrodynamics research.

Abstract: Hydrodynamics And Radiation Diffusion} (HARD) is an open-source application
for high-performance simulations of compressible hydrodynamics with
radiation-diffusion coupling. Built on the FleCSI (Flexible Computational
Science Infrastructure) framework, HARD expresses its computational units as
tasks whose execution can be orchestrated by multiple back-end runtimes,
including Legion, MPI, and HPX. Node-level parallelism is delegated to Kokkos,
providing a single, portable code base that runs efficiently on laptops, small
homogeneous clusters, and the largest heterogeneous supercomputers currently
available. To ensure scientific reliability, HARD includes a regression-test
suite that automatically reproduces canonical verification problems such as the
Sod and LeBlanc shock tubes and the Sedov blast wave, comparing numerical
solutions against known analytical results. The project is distributed under an
OSI-approved license, hosted on GitHub, and accompanied by reproducible build
scripts and continuous integration workflows. This combination of performance
portability, verification infrastructure, and community-focused development
makes HARD a sustainable platform for advancing radiation hydrodynamics
research across multiple domains.

</details>


### [31] [An Improved Rapid Performance Analysis Model for Solenoidal Magnetic Radiation Shields](https://arxiv.org/abs/2509.09051)
*Joseph L. Hesse-Withbroe,Katya S. Arquilla*

Main category: physics.comp-ph

TL;DR: Updated semi-analytical model for magnetic radiation shielding shows improved performance for weak shields but diminished performance for strong shields (>20 T-m), enabling mass savings over passive shielding for missions requiring >60% dose reduction.


<details>
  <summary>Details</summary>
Motivation: Deep-space radiation exposure poses significant health risks to astronauts, and active magnetic shielding offers potential mass savings over passive shielding but requires accurate performance evaluation.

Method: Updated and validated a semi-analytical model based on HZETRN code, correcting simplifying assumptions from the original 2014 model, then used Monte Carlo simulations for validation and recharacterized solenoidal shield design trade space.

Result: The updated model predicts improved performance for weaker shields but greatly diminished performance for strong shields with bending powers >20 T-m. Magnetic shields enable significant mass savings over passive shields when dose reduction >60% is required.

Conclusion: Magnetic radiation shields are viable for long-duration exploration missions (>1 year) outside LEO where substantial dose reduction (>60%) is needed, offering mass advantages over traditional passive shielding approaches.

Abstract: Astronauts participating in deep-space exploration missions will be exposed
to significantly greater amounts of radiation than is typically encountered on
Earth or in low Earth orbit (LEO), which poses significant risks to crew health
and mission safety. Active magnetic radiation shields based on the Lorentz
deflection of charged particles have the potential to reduce astronaut doses
with lower mass costs than passive shielding techniques. Typically, active
shielding performance is evaluated using high-fidelity Monte Carlo simulations,
which are too computationally expensive to evaluate an entire trade space of
shield designs. A rapid, semi-analytical model based on the High Charge and
Energy Transport code (HZETRN) developed in 2014 provided an alternative method
by which to evaluate the performance of solenoidal shields. However, various
simplifying assumptions made in the original model have limited its accuracy,
and therefore require evaluation and correction. In this work, a number of
aspects of the original semi-analytical model are updated and validated by
Monte Carlo simulation, then used to recharacterize the design trade space of
solenoidal magnetic shields. The updated model predicts improved performance
for weaker shields as compared to the original model, but greatly diminished
performance for strong shields with bending powers greater than 20 T-m.
Overall, the results indicate that magnetic shields enable significant mass
savings over passive shields for mission scenarios where the requisite dose
reduction is greater than about 60% relative to free space, which includes most
exploration missions longer than one year with significant time spent outside
LEO.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [32] [Exploration of novel ICP using helicon antennas with zero magnetic field](https://arxiv.org/abs/2509.09104)
*Ye Tao,Lei Chang,Dingzhou Li,Yingxin Zhao*

Main category: physics.plasm-ph

TL;DR: Loop antenna with parabolic density profile at 13.56 MHz provides most efficient power coupling for ICP without external magnetic field


<details>
  <summary>Details</summary>
Motivation: Efficient power coupling is crucial for both fundamental research and practical applications of inductively coupled plasma (ICP), particularly for material processing and space propulsion

Method: Investigated effects of antenna geometry (loop, half-helix, Boswell, Nagoya III), driving frequency (13.56-54.24 MHz), and radial density profile (Gaussian vs parabolic) on power coupling efficiency in ICP without external magnetic field

Result: Loop antenna showed highest power deposition efficiency; driving frequency had negligible effects; parabolic density profile provided more efficient power coupling than Gaussian profile, especially radially

Conclusion: For efficient ICP without external magnetic field, use loop antenna with parabolic density profile at standard industrial frequency of 13.56 MHz

Abstract: Inductively coupled plasma (ICP) attracts great attention from aspects of
fundamental research and practical applications, and efficient power coupling
is highly desirable for both of them. The present study explores a novel
strategy for efficient ICP through using helicon antennas with zero external
magnetic field. Specific research is devoted to the effects of antenna geometry
(loop, half-helix, Boswell, Nagoya III), driving frequency (13.56-54.24 MHz)
and radial density profile (Gaussian and parabolic) on power coupling. Findings
reveal that: loop antenna yields higher power deposition efficiency than
half-helix, Boswell, and Nagoya III antennas, driving frequency gives
negligible effects, and parabolic density profile results in more efficient
power coupling than Gaussian density profile especially in the radial
direction, for the conditions employed here. Therefore, it is suggested that
for this novel ICP strategy one should use loop antenna with parabolic density
profile, and the industrial frequency of 13.56 MHz can work well. This study
provides a valuable reference for the novel design of efficient ICP sources,
which could be used for material processing and space propulsion, etc. Key
words: Inductively coupled plasma; Antenna Geometry; Power Deposition; Driving
Frequency

</details>


### [33] [Exploration on the Two-stream Instability in the Polar Cusp Under Solar Storm Disturbances and its Potential Impacts on Spacecraft](https://arxiv.org/abs/2509.09126)
*Jikai Sun,Lei Chang,Yu Liu,Guojun Wang,Zichen Kan,Shijie Zhang,Jingjing Ma,Dingzhou Li,Yingxin Zhao*

Main category: physics.plasm-ph

TL;DR: Study examines two-stream instability evolution in polar cusp during solar storms, showing how electron-cyclotron motion can suppress instability and reduce electrostatic wave amplitudes.


<details>
  <summary>Details</summary>
Motivation: Understand electron velocity distributions in polar cusp during solar storms and their association with two-stream instability, which poses risks to spacecraft operations.

Method: Particle-in-cell (PIC) simulations compared with satellite observational data and computational outcomes to analyze instability evolution.

Result: Solar wind and ionospheric plasma interaction drives two-stream instability, forming electron hole structures and bipolar ESWs. Enhanced electron cyclotron motion under specific conditions suppresses instability and reduces ESW amplitude.

Conclusion: Findings provide insights into solar storm impacts on polar cusp environment, valuable for spacecraft electromagnetic monitoring and operational stability.

Abstract: During solar storms, the polar cusp often exhibits electron populations with
distinct velocity distributions, which may be associated with the two-stream
instability. This study reveals the evolution of the two-stream instability
associated with electron velocities and the interaction between the growth
phase of the two-stream instability and the electrostatic solitary waves
(ESWs). The results from particle-in-cell (PIC) simulations are compared with
satellite observational data and computational outcomes. The potential risks
associated with two-stream instability, including surface charge accumulation
and communication system interference on spacecraft, are also explored. The
findings show that, in the high-latitude polar cusp region, the interaction
between the solar wind plasma propagating along magnetic field lines and the
upward-moving ionospheric plasma could drive two-stream instability, leading to
the formation of electron hole structures in phase space and triggering a
bipolar distribution of ESWs. When the spatial magnetic field and wave vector
meet specific conditions, the enhanced electron cyclotron motion could suppress
the formation of two-stream instability and electron hole structures, leading
to a reduction in the amplitude of the ESWs. The results offer valuable
insights for a deeper understanding of the impact of solar storms on the polar
cusp environment, as well as for monitoring electromagnetic environment and
ensuring the stable operation of spacecraft.

</details>


<div id='physics.gen-ph'></div>

# physics.gen-ph [[Back]](#toc)

### [34] [Monte Carlo Simulation of Spallation and Fission Fragment Distributions for ADS-Related Nuclear Reactions](https://arxiv.org/abs/2509.08996)
*Sun Wenming*

Main category: physics.gen-ph

TL;DR: Monte Carlo simulations study spallation and fission fragment distributions from proton/photon interactions with actinide nuclei, showing good agreement with experimental data for ADS applications.


<details>
  <summary>Details</summary>
Motivation: To provide nuclear data for accelerator-driven system (ADS) design, safety, and transmutation analysis by studying spallation and fission fragment distributions.

Method: Used CRISP code Monte Carlo simulations incorporating intranuclear cascade, pre-equilibrium, and evaporation-fission competition models for consistent treatment of residues and fission products.

Result: Comparisons with experimental data show good agreement in mass and charge distributions, with minor deviations for light fragments.

Conclusion: Monte Carlo approaches are reliable for predicting residual nuclei and fragment yields under ADS conditions, providing relevant nuclear data for ADS applications.

Abstract: Monte Carlo simulations with the CRISP code were conducted to study
spallation and fission fragment distributions induced by intermediate- and
high-energy protons and photons on actinide and pre-actinide nuclei. The model
accounts for intranuclear cascade, pre-equilibrium, and evaporation-fission
competition, enabling consistent treatment of both residues and fission
products. Comparisons with experimental data show good agreement in mass and
charge distributions, with minor deviations for light fragments. The results
highlight the reliability of Monte Carlo approaches for predicting residual
nuclei and fragment yields under accelerator-driven system (ADS) conditions.
This work provides nuclear data relevant to ADS design, safety, and
transmutation analysis

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [35] [Electronic order induced symmetry breaking in lattice dynamics of Co$_3$Sn$_2$S$_2$](https://arxiv.org/abs/2509.09253)
*Shuai Zhang,Mengqi Wang,Tiantian Zhang*

Main category: cond-mat.mtrl-sci

TL;DR: Developed ab initio algorithm using molecular Berry curvature to quantify magnetic order effects on lattice dynamics. Found that spin-orbit coupling and mirror symmetry breaking are essential for phonon splitting in ferromagnetic Weyl semimetal Co3Sn2S2.


<details>
  <summary>Details</summary>
Motivation: To understand and quantify how magnetic order affects lattice dynamics in magnetic materials, particularly focusing on the role of spin-orbit coupling and symmetry breaking in phonon behavior.

Method: Developed an ab initio algorithm based on molecular Berry curvature framework, applied to ferromagnetic Weyl semimetal Co3Sn2S2 as a prototype system to study electronic-order-driven phonon symmetry breaking.

Result: Demonstrated that mirror-symmetry breaking (not just time-reversal breaking) is essential for observed phonon splitting. MBC is widely distributed across Brillouin zone, causing significant off-Gamma effects. Results agree well with experiments.

Conclusion: Established framework for predicting large phonon magnetism in magnetic materials with strong spin-orbit and electron-phonon coupling. Suggests new avenues for controlling non-reciprocal phonon transport.

Abstract: Based on the molecular Berry curvature (MBC) framework, we develop an
\textit{ab initio} algorithm to capture the quantitative effects of magnetic
order on lattice dynamics. Using the ferromagnetic Weyl semimetal
Co$_3$Sn$_2$S$_2$ as a prototype, we show that electronic-order-driven phonon
symmetry breaking requires spin-orbit coupling (SOC) and leads to an MBC term
that breaks both time-reversal ($\mathcal{T}$) and mirror symmetries. We
demonstrate that mirror-symmetry breaking is essential to account for the
experimentally observed phonon splitting, $\mathcal{T}$-breaking alone is
insufficient. The MBC is widely distributed across the Brillouin zone, giving
rise to significant off-$\Gamma$ effects. Our results agree well with
experiments and establish a framework for predicting large phonon magnetism in
magnetic materials with strong spin-orbit coupling and electron-phonon
coupling. This work also suggests new avenues for controlling non-reciprocal
phonon transport.

</details>


### [36] [Exploring the magnetic landscape of easily-exfoliable two-dimensional materials](https://arxiv.org/abs/2509.09531)
*Fatemeh Haddadi,Davide Campi,Flaviano dos Santos,Nicolas Mounet,Louis Ponet,Nicola Marzari,Marco Gibertini*

Main category: cond-mat.mtrl-sci

TL;DR: Automated workflow applied to explore energy landscapes of 194 magnetic monolayers to determine ground-state magnetic order, identifying 109 ferromagnetic, 83 antiferromagnetic, and 2 altermagnetic materials plus 12 novel ferromagnetic half-metals.


<details>
  <summary>Details</summary>
Motivation: Magnetic materials have complex energy landscapes with multiple local minima, making global minimum finding challenging with heuristic methods that aren't guaranteed to succeed.

Method: Applied automated workflow to systematically explore energy landscapes, enabling effective control and sampling of orbital occupation matrices for rapid identification of local minima. Used Hubbard-corrected energy functionals with first-principles computed U parameters from linear-response theory.

Result: Found diverse collinear metastable states: 109 ferromagnetic, 83 antiferromagnetic, and 2 altermagnetic monolayers. Identified 12 novel ferromagnetic half-metals with spintronics potential.

Conclusion: The automated workflow successfully determined ground-state magnetic orders and revealed promising spintronic candidates, demonstrating the importance of systematic exploration of magnetic energy landscapes.

Abstract: Magnetic materials often exhibit complex energy landscapes with multiple
local minima, each corresponding to a self-consistent electronic structure
solution. Finding the global minimum is challenging, and heuristic methods are
not always guaranteed to succeed. Here, we apply a recently developed automated
workflow to systematically explore the energy landscape of 194 magnetic
monolayers obtained from the Materials Cloud 2D crystals database and determine
their ground-state magnetic order. Our approach enables effective control and
sampling of orbital occupation matrices, allowing rapid identification of local
minima. We find a diverse set of self-consistent collinear metastable states,
further enriched by Hubbard-corrected energy functionals, when the $U$
parameters have been computed from first principles using linear-response
theory. We categorise the monolayers by their magnetic ordering and highlight
promising candidates. Our results include 109 ferromagnetic, 83
antiferromagnetic, and 2 altermagnetic monolayers, along with 12 novel
ferromagnetic half-metals with potential for spintronics technologies.

</details>


### [37] [A Phase-Field Approach to Fracture and Fatigue Analysis: Bridging Theory and Simulation](https://arxiv.org/abs/2509.08939)
*M. Castillón,I. Romero,J. Segurado*

Main category: cond-mat.mtrl-sci

TL;DR: A novel framework combining Linear Elastic Fracture Mechanics with phase-field fracture for fatigue crack propagation analysis using single simulation and Paris' law integration.


<details>
  <summary>Details</summary>
Motivation: To develop a more efficient and robust approach for fatigue crack propagation analysis that avoids cycle-by-cycle simulations and simplifies parametrization while capturing complex crack instabilities.

Method: Uses phase-field fracture FEM simulation with a new crack-control scheme to trace complete equilibrium paths, numerically evaluates compliance derivative with respect to crack area, and integrates Paris' law for fatigue life prediction.

Result: Validated through benchmarks with analytical solutions showing accuracy, and applied to complex geometries with unknown crack paths demonstrating good agreement with experimental results for both crack paths and fatigue life.

Conclusion: The proposed framework provides an efficient and robust methodology for fatigue crack propagation analysis that successfully combines LEFM principles with phase-field fracture, enabling accurate predictions through single simulations rather than cycle-by-cycle approaches.

Abstract: This article presents a novel, robust and efficient framework for fatigue
crack-propagation that combines the principles of Linear Elastic Fracture
Mechanics (LEFM) with phase-field fracture (PFF). Contrary to cycle-by-cycle
PFF approaches, this work relies on a single simulation and uses standard crack
propagation models such as Paris' law for the material response, simplifying
its parametrization.
  The core of the methodology is the numerical evaluation of the derivative of
a specimen's compliance with respect to the crack area. To retrieve this
compliance the framework relies on a PFF-FEM simulation, controlled imposing a
monotonic crack growth. This control of the loading process is done by a new
crack-control scheme which allows to robustly trace the complete equilibrium
path of a crack, capturing complex instabilities. The specimen's compliance
obtained from the PFF simulation enables the integration of Paris' law to
predict fatigue life.
  The proposed methodology is first validated through a series of benchmarks
with analytical solutions to demonstrate its accuracy. The framework is then
applied to more complex geometries where the crack path is unknown, showing a
very good agreement with experimental results of both crack paths and fatigue
life.

</details>


<div id='physics.ed-ph'></div>

# physics.ed-ph [[Back]](#toc)

### [38] [Are arXiv submissions on Wednesday better cited? Introducing Big Data methods in undergraduate courses on scientific computing](https://arxiv.org/abs/2509.09601)
*Stéphane Delorme,Leon Mach,Hubert Paszkiewicz,Richard Ruiz*

Main category: physics.ed-ph

TL;DR: A simple data analysis pipeline for teaching big data methods using arXiv and inSpireHEP databases, implemented with Python libraries for undergraduate physics education.


<details>
  <summary>Details</summary>
Motivation: Address barriers to learning big data methods in undergraduate physics curricula by providing accessible, practical tools for data collection, processing, and visualization.

Method: Developed a farm-to-table data analysis pipeline that collects, processes, and plots data from 800k entries in arXiv and inSpireHEP databases using open-source Python libraries.

Result: Created a publicly available implementation that runs on standard laptops, authored by undergraduate students, demonstrating practical application of big data methods in physics education.

Conclusion: The pipeline successfully bridges the gap in big data education for physics students and has potential applications in online DAQ monitoring and commercialization for advanced learners.

Abstract: Extracting information from big data sets, both real and simulated, is a
modern hallmark of the physical sciences. In practice, students face barriers
to learning ``Big Data'' methods in undergraduate physics and astronomy
curricula. As an attempt to alleviate some of these challenges, we present a
simple, farm-to-table data analysis pipeline that can collect, process, and
plot data from the 800k entries common to the arXiv preprint repository and the
bibliographical database inSpireHEP. The pipeline employs contemporary research
practices and can be implemented using open-sourced Python libraries common to
undergraduate courses on Scientific Computing. To support the use such
pipelines in classroom contexts, we make public an example implementation,
authored by two undergraduate physics students, that runs on off-the-shelf
laptops. For advanced students, we discuss applications of the pipeline,
including for online DAQ monitoring and commercialization.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [39] [Complex dynamics and pattern formation in a diffusive epidemic model with an infection-dependent recovery rate](https://arxiv.org/abs/2509.09000)
*Wael El Khateeb,Chanaka Kottegoda,Chunhua Shan*

Main category: math.DS

TL;DR: A diffusive epidemic model with infection-dependent recovery rate shows diffusion-driven instability, Turing patterns, and spatiotemporal dynamics including disease recurrence and localized hotspots.


<details>
  <summary>Details</summary>
Motivation: To understand how infection-dependent recovery rates and population movement affect disease transmission dynamics and pattern formation in epidemics.

Method: Bifurcation analysis of reaction kinetics to identify steady states and periodic solutions, followed by analysis of diffusion-driven instability and Turing-Hopf bifurcation.

Result: The model exhibits multiple constant steady states, spatially homogeneous periodic solutions, diffusion-driven instability, and spatiotemporal patterns including k-mode Turing instability and (k1,k2)-mode Turing-Hopf bifurcation.

Conclusion: Faster movement of susceptible populations induces spatial patterns, revealing asynchronous disease recurrence and localized hotspots, suggesting spatially targeted strategies are needed for containment.

Abstract: A diffusive epidemic model with an infection-dependent recovery rate is
formulated in this paper. Multiple constant steady states and spatially
homogeneous periodic solutions are first proven by bifurcation analysis of the
reaction kinetics. It is shown that the model exhibits diffusion-driven
instability, where the infected population acts as an activator and the
susceptible population functions as an in hibitor. The faster movement of the
susceptible class will induce the spatial and spatiotemporal patterns, which
are characterized by k-mode Turing instability and (k1,k2)-mode Turing-Hopf
bifurcation. The transient dynamics from a purely temporal oscillatory regime
to a spatial periodic pattern are discovered. The model reveals key
transmission dynamics, including asynchronous disease recurrence, spatially
patterned waves, and the formation of localized hotspots. The study suggests
that spatially targeted strategies are necessary to contain disease waves that
vary regionally and cyclically.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [40] [The role of communication delays in the optimal control of spatially invariant systems](https://arxiv.org/abs/2509.09269)
*Luca Ballotta,Juncal Arbelaiz,Vijay Gupta,Luca Schenato,Mihailo R. Jovanović*

Main category: math.OC

TL;DR: Analysis of optimal proportional feedback controllers for spatially invariant systems with delayed state measurements, showing how delays affect spatial locality and require extended communication in distributed implementations.


<details>
  <summary>Details</summary>
Motivation: To understand how communication delays impact the performance and spatial locality of optimal feedback control in spatially distributed systems, which is crucial for designing effective distributed control strategies.

Method: Leveraging problem decoupling in the spatial frequency domain, analyzing optimal controllers for expensive control and small delay regimes, and providing exact expressions for infinite control weight and vanishing delay limits.

Result: Delays reduce control effectiveness and increase spatial communication requirements. In expensive control regime, optimal law combines delay-aware filtering with delay-free controller. For small delays, optimal controller is a linear perturbation of delay-free version.

Conclusion: Communication delays fundamentally alter optimal control strategies, requiring extended spatial communication and delay-compensating filters, which has significant implications for distributed control system design and implementation.

Abstract: We study optimal proportional feedback controllers for spatially invariant
systems when the controller has access to delayed state measurements received
from different spatial locations. We analyze how delays affect the spatial
locality of the optimal feedback gain leveraging the problem decoupling in the
spatial frequency domain. For the cases of expensive control and small delay,
we provide exact expressions of the optimal controllers in the limit for
infinite control weight and vanishing delay, respectively. In the expensive
control regime, the optimal feedback control law decomposes into a delay-aware
filtering of the delayed state and the optimal controller in the delay-free
setting. Under small delays, the optimal controller is a perturbation of the
delay-free one which depends linearly on the delay. We illustrate our
analytical findings with a reaction-diffusion process over the real line and a
multi-agent system coupled through circulant matrices, showing that delays
reduce the effectiveness of optimal feedback control and may require each
subsystem within a distributed implementation to communicate with farther-away
locations.

</details>


### [41] [A preconditioned third-order implicit-explicit algorithm with a difference of varying convex functions and extrapolation](https://arxiv.org/abs/2509.09391)
*Kelin Wu,Hongpeng Sun*

Main category: math.OC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper proposes a novel preconditioned implicit-explicit algorithm
enhanced with the extrapolation technique for non-convex optimization problems.
The algorithm employs a third-order Adams-Bashforth scheme for the nonlinear
and explicit parts and a third-order backward differentiation formula for the
implicit part of the gradient flow in variational functions. The proposed
algorithm, akin to a generalized difference-of-convex (DC) approach, employs a
changing set of convex functions in each iteration. Under the Kurdyka-\L
ojasiewicz (KL) properties, the global convergence of the algorithm is
guaranteed, ensuring that it converges within a finite number of preconditioned
iterations. Our numerical experiments, including least squares problems with
SCAD regularization and the graphical Ginzburg-Landau model, demonstrate the
proposed algorithm's highly efficient performance compared to conventional DC
algorithms.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [42] [Virtual staining for 3D X-ray histology of bone implants](https://arxiv.org/abs/2509.09235)
*Sarah C. Irvine,Christian Lucas,Diana Krüger,Bianca Guedert,Julian Moosmann,Berit Zeller-Plumhoff*

Main category: eess.IV

TL;DR: This paper introduces virtual staining for 3D X-ray histology using deep learning to generate artificially stained tissue images from micro-CT scans, enabling label-free 3D tissue characterization without physical sectioning.


<details>
  <summary>Details</summary>
Motivation: 3D X-ray histology provides non-invasive volumetric imaging but lacks biochemical specificity compared to traditional stained histology. The researchers aim to extend virtual staining techniques from optical to X-ray domain to enhance interpretability without additional sample preparation.

Method: Used over 50 co-registered micro-CT and toluidine blue-stained histology image pairs to train a modified CycleGAN network tailored for limited paired data. Incorporated pixelwise supervision and greyscale consistency terms, with on-the-fly data augmentation for patch-based training.

Result: The modified CycleGAN outperformed Pix2Pix and standard CycleGAN baselines across SSIM, PSNR, and LPIPS metrics. The model produces histologically realistic color outputs while preserving high-resolution structural detail, enabling generation of virtually stained 3D datasets from full CT volumes.

Conclusion: This work successfully introduces virtual staining to 3D X-ray imaging, offering a scalable route for chemically informative, label-free tissue characterization. While features like new bone formation were reproduced, some variability in implant degradation depiction indicates need for more training data and refinement.

Abstract: Three-dimensional X-ray histology techniques offer a non-invasive alternative
to conventional 2D histology, enabling volumetric imaging of biological tissues
without the need for physical sectioning or chemical staining. However, the
inherent greyscale image contrast of X-ray tomography limits its biochemical
specificity compared to traditional histological stains. Within digital
pathology, deep learning-based virtual staining has demonstrated utility in
simulating stained appearances from label-free optical images. In this study,
we extend virtual staining to the X-ray domain by applying cross-modality image
translation to generate artificially stained slices from
synchrotron-radiation-based micro-CT scans. Using over 50 co-registered image
pairs of micro-CT and toluidine blue-stained histology from bone-implant
samples, we trained a modified CycleGAN network tailored for limited paired
data. Whole slide histology images were downsampled to match the voxel size of
the CT data, with on-the-fly data augmentation for patch-based training. The
model incorporates pixelwise supervision and greyscale consistency terms,
producing histologically realistic colour outputs while preserving
high-resolution structural detail. Our method outperformed Pix2Pix and standard
CycleGAN baselines across SSIM, PSNR, and LPIPS metrics. Once trained, the
model can be applied to full CT volumes to generate virtually stained 3D
datasets, enhancing interpretability without additional sample preparation.
While features such as new bone formation were able to be reproduced, some
variability in the depiction of implant degradation layers highlights the need
for further training data and refinement. This work introduces virtual staining
to 3D X-ray imaging and offers a scalable route for chemically informative,
label-free tissue characterisation in biomedical research.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [43] [An Interval Type-2 Version of Bayes Theorem Derived from Interval Probability Range Estimates Provided by Subject Matter Experts](https://arxiv.org/abs/2509.08834)
*John T. Rickard,William A. Dembski,James Rickards*

Main category: cs.AI

TL;DR: Extends Bayes Theorem to interval type-2 fuzzy logic to handle imprecise input probabilities from subject matter experts, using a conservative method to avoid inconsistencies and a novel algorithm for encoding interval estimates into membership functions.


<details>
  <summary>Details</summary>
Motivation: Traditional Bayesian inference assumes precise input values, but real-world applications often rely on interval range estimates from experts, requiring a more robust approach that handles uncertainty and imprecision.

Method: Develops an interval type-2 version of Bayes Theorem with a conservative method to prevent input inconsistencies, and creates a novel algorithm for encoding expert-provided intervals into type-2 fuzzy membership functions.

Result: The paper provides a framework that can process imprecise probability inputs while maintaining validity of output results, extending Bayesian inference to handle real-world uncertainty.

Conclusion: The proposed interval type-2 Bayesian approach effectively addresses the limitation of precise input assumptions in traditional Bayes Theorem, enabling more realistic applications with expert-provided interval estimates.

Abstract: Bayesian inference is widely used in many different fields to test hypotheses
against observations. In most such applications, an assumption is made of
precise input values to produce a precise output value. However, this is
unrealistic for real-world applications. Often the best available information
from subject matter experts (SMEs) in a given field is interval range estimates
of the input probabilities involved in Bayes Theorem. This paper provides two
key contributions to extend Bayes Theorem to an interval type-2 (IT2) version.
First, we develop an IT2 version of Bayes Theorem that uses a novel and
conservative method to avoid potential inconsistencies in the input IT2 MFs
that otherwise might produce invalid output results. We then describe a novel
and flexible algorithm for encoding SME-provided intervals into IT2 fuzzy
membership functions (MFs), which we can use to specify the input probabilities
in Bayes Theorem. Our algorithm generalizes and extends previous work on this
problem that primarily addressed the encoding of intervals into word MFs for
Computing with Words applications.

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [44] [Suppression of pair beam instabilities in a laboratory analogue of blazar pair cascades](https://arxiv.org/abs/2509.09040)
*Charles D. Arrowsmith,Francesco Miniati,Pablo J. Bilbao,Pascal Simon,Archie F. A. Bott,Stephane Burger,Hui Chen,Filipe D. Cruz,Tristan Davenne,Anthony Dyson,Ilias Efthymiopoulos,Dustin H. Froula,Alice Goillot,Jon T. Gudmundsson,Dan Haberberger,Jack W. D. Halliday,Tom Hodge,Brian T. Huffman,Sam Iaquinta,Graham Marshall,Brian Reville,Subir Sarkar,Alexander A. Schekochihin,Luis O. Silva,Raspberry Simpson,Vasiliki Stergiou,Raoul M. G. M. Trines,Thibault Vieu,Nikolaos Charitonidis,Robert Bingham,Gianluca Gregori*

Main category: astro-ph.HE

TL;DR: Laboratory generation of electron-positron pair beams using CERN's Super Proton Synchrotron enables direct testing of astrophysical models for gamma-ray bursts and active galactic nuclei.


<details>
  <summary>Details</summary>
Motivation: To enable direct laboratory tests of theoretical models for gamma-ray bursts and active galactic nuclei, and to study pair beam stability which has implications for understanding TeV gamma-ray observations from blazars.

Method: Used ultra-relativistic protons accelerated by CERN's Super Proton Synchrotron to generate dense electron-positron pair beams, then studied beam stability as it propagates through a metre-length plasma to simulate astrophysical conditions.

Result: Found that pair beam instability is suppressed if the beam is not perfectly collimated or monochromatic, indicating that the lower limit to intergalactic magnetic field inferred from gamma-ray observations of blazars remains robust.

Conclusion: The experimental platform successfully enables laboratory testing of astrophysical phenomena, and the findings support the existing lower limit estimates for intergalactic magnetic fields based on gamma-ray observations.

Abstract: The generation of dense electron-positron pair beams in the laboratory can
enable direct tests of theoretical models of $\gamma$-ray bursts and active
galactic nuclei. We have successfully achieved this using ultra-relativistic
protons accelerated by the Super Proton Synchrotron at CERN. In the first
application of this experimental platform, the stability of the pair beam is
studied as it propagates through a metre-length plasma, analogous to TeV
$\gamma$-ray induced pair cascades in the intergalactic medium. It has been
argued that pair beam instabilities disrupt the cascade, thus accounting for
the observed lack of reprocessed GeV emission from TeV blazars. If true this
would remove the need for a moderate strength intergalactic magnetic field to
explain the observations. We find that the pair beam instability is suppressed
if the beam is not perfectly collimated or monochromatic, hence the lower limit
to the intergalactic magnetic field inferred from $\gamma$-ray observations of
blazars is robust.

</details>


### [45] [Unraveling the emission mechanism powering long period radio transients from interacting white dwarf binaries via kinetic plasma simulations](https://arxiv.org/abs/2509.09057)
*Yici Zhong,Elias R. Most*

Main category: astro-ph.HE

TL;DR: The paper demonstrates that the relativistic electron cyclotron maser instability (ECMI) is a viable mechanism for generating radio pulses in white dwarf-M dwarf binary systems, similar to planetary radio emissions like Jupiter-Io.


<details>
  <summary>Details</summary>
Motivation: Recent observations of long period radio transients from white dwarf-M dwarf binaries raise questions about how coherent radio emission is produced in these systems, with binary interaction being a likely driver given emission correlation with orbital periods.

Method: Using kinetic plasma simulations, the authors quantify the relativistic ECMI in the nonlinear regime under conditions relevant for white dwarf radio emission for the first time.

Result: Simulations show ECMI can intrinsically produce partially linearly polarized emission that explains the observed emission spectrum of galactic sources, though precise details depend on plasma composition.

Conclusion: This work enables systematic and fully nonlinear computational modeling of radio emission from interacting white dwarf sources, establishing ECMI as a viable emission mechanism.

Abstract: Recent observations of long period radio transients, such as GLEAM-X J0704-37
and ILTJ1101 + 5521, have revealed a previously unrecognized population of
galactic radio transient sources associated with white dwarf - M dwarf
binaries. It is an open question how to produce coherent radio emission in
these systems, though a model driven by binary interaction seems likely given
the nature and correlation of the emission with the binaries' orbital period.
Using kinetic plasma simulations, we demonstrate that the relativistic electron
cyclotron maser instability (ECMI) is a viable mechanism for generating radio
pulses in white dwarf - M dwarf systems, akin to planetary radio emission, such
as that from the Jupiter-Io system. We quantify the relativistic ECMI in the
nonlinear regime under conditions relevant for white dwarf radio emission for
the first time. Our simulations demonstrate that the ECMI can intrinsically
produce partially linearly polarized emission relevant to explaining the
observed emission spectrum of the two galactic sources, though the precise
details will depend on the plasma composition. Our work paves the way for a
systematic and fully nonlinear computational modeling of radio emission from
interacting white dwarf sources.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [46] [Symmetries in stochastic homogenization and acclimatizations for the RVE method](https://arxiv.org/abs/2509.08977)
*Binh Huy Nguyen,Matti Schneider*

Main category: cs.CE

TL;DR: Study on enforcing microstructure symmetries in effective tensor calculations using orthogonal projectors to maintain symmetry properties and reduce errors in RVE homogenization.


<details>
  <summary>Details</summary>
Motivation: To address how microstructure symmetries affect effective properties and their fluctuations, and to develop methods for enforcing these symmetries when they are broken by RVE cell types.

Method: Use orthogonal projectors in postprocessing to enforce expected symmetries, analyze implications on error bounds, and conduct large-scale FFT-based homogenization simulations on fiber-reinforced composites.

Result: Suitable projections provide unbiased variance-reduction strategies that exactly enforce expected symmetries, improving accuracy in effective property estimation.

Conclusion: Symmetry-projection techniques effectively rectify symmetry breaking in RVE methods, ensuring proper symmetry enforcement and enhanced accuracy in homogenization simulations.

Abstract: We investigate the implications of a given symmetry of a random
microstructure on the obtained effective tensor and its fluctuation in the
context of thermal conductivity, and study strategies for enforcing these
symmetries in postprocessing via orthogonal projectors. Within the framework of
the representative volume element (RVE) method, we establish the invariance
conditions for the effective tensor and its fluctuation under different
symmetry groups of the microstructure. Interestingly, the symmetry of the
considered cell type in the RVE method may break the ensemble symmetry and
compromise the approximation of the effective properties. To rectify this
issue, we introduce dedicated techniques which permit to enforce the expected
symmetries in postprocessing and study the implications on the bounds for the
effective properties as well as the total, the random and the systematic
errors. We provide theoretical arguments that suitable projections lead to
unbiased variance-reduction strategies which furthermore enforce the expected
symmetries exactly. Through large-scale FFT-based homogenization simulations,
we study the symmetry structure of the estimated effective conductivities and
their fluctuations. Moreover, we demonstrate the power of the
symmetry-projection techniques for fiber-reinforced composite microstructures
of industrial scale.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [47] [Robust Non-Linear Correlations via Polynomial Regression](https://arxiv.org/abs/2509.09380)
*Luca Giuliani,Michele Lombardi*

Main category: cs.LG

TL;DR: A novel computational approach for HGR correlation coefficient using configurable polynomial kernels, offering improved robustness and determinism for real-world applications.


<details>
  <summary>Details</summary>
Motivation: Existing HGR estimation methods suffer from bias-variance trade-offs due to inherent uncomputability, compromising robustness in real-world scenarios like algorithmic fairness and constrained ML.

Method: Proposes a computational approach based on user-configurable polynomial kernels that provides greater robustness and faster yet effective restriction compared to previous methods.

Result: The method offers significant advantages in robustness and determinism, with experimental validation showing it produces insightful subgradients suitable as loss regularizers in constrained ML frameworks.

Conclusion: This approach provides a more reliable and robust computational method for HGR correlation estimation, making it suitable for practical real-world applications in fairness, scientific analysis, and causal discovery.

Abstract: The Hirschfeld-Gebelein-R\'enyi (HGR) correlation coefficient is an extension
of Pearson's correlation that is not limited to linear correlations, with
potential applications in algorithmic fairness, scientific analysis, and causal
discovery. Recently, novel algorithms to estimate HGR in a differentiable
manner have been proposed to facilitate its use as a loss regularizer in
constrained machine learning applications. However, the inherent
uncomputability of HGR requires a bias-variance trade-off, which can possibly
compromise the robustness of the proposed methods, hence raising technical
concerns if applied in real-world scenarios. We introduce a novel computational
approach for HGR that relies on user-configurable polynomial kernels, offering
greater robustness compared to previous methods and featuring a faster yet
almost equally effective restriction. Our approach provides significant
advantages in terms of robustness and determinism, making it a more reliable
option for real-world applications. Moreover, we present a brief experimental
analysis to validate the applicability of our approach within a constrained
machine learning framework, showing that its computation yields an insightful
subgradient that can serve as a loss regularizer.

</details>


### [48] [ReBaNO: Reduced Basis Neural Operator Mitigating Generalization Gaps and Achieving Discretization Invariance](https://arxiv.org/abs/2509.09611)
*Haolan Zheng,Yanlai Chen,Jiequn Han,Yue Yu*

Main category: cs.LG

TL;DR: ReBaNO is a novel data-lean operator learning algorithm that combines reduced basis methods with neural networks to solve PDEs with multiple inputs, achieving superior generalization and strict discretization invariance compared to state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: To address the generalization gap and lack of strict discretization invariance in existing operator learning algorithms for solving PDEs with multiple distinct inputs, while minimizing computational costs.

Method: Combines Reduced Basis Method with Generative Pre-Trained Physics-Informed Neural Networks using a greedy algorithm to build network structure offline, with knowledge distillation via task-specific activation functions for compact architecture.

Result: Significantly outperforms PCA-Net, DeepONet, FNO, and CNO in eliminating/shrinking generalization gap for both in- and out-of-distribution tests, and is the only operator learning algorithm achieving strict discretization invariance.

Conclusion: ReBaNO provides a mathematically rigorous, computationally efficient approach for operator learning that demonstrates superior generalization performance and strict discretization invariance compared to existing state-of-the-art methods.

Abstract: We propose a novel data-lean operator learning algorithm, the Reduced Basis
Neural Operator (ReBaNO), to solve a group of PDEs with multiple distinct
inputs. Inspired by the Reduced Basis Method and the recently introduced
Generative Pre-Trained Physics-Informed Neural Networks, ReBaNO relies on a
mathematically rigorous greedy algorithm to build its network structure offline
adaptively from the ground up. Knowledge distillation via task-specific
activation function allows ReBaNO to have a compact architecture requiring
minimal computational cost online while embedding physics. In comparison to
state-of-the-art operator learning algorithms such as PCA-Net, DeepONet, FNO,
and CNO, numerical results demonstrate that ReBaNO significantly outperforms
them in terms of eliminating/shrinking the generalization gap for both in- and
out-of-distribution tests and being the only operator learning algorithm
achieving strict discretization invariance.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [49] [Efficient High-Order Participation Factor Computation via Batch-Structured Tensor Contraction](https://arxiv.org/abs/2509.08968)
*Mahsa Sajjadi,Kaiyang Huang,Kai Sun*

Main category: eess.SY

TL;DR: Efficient tensor-based method for computing high-order nonlinear participation factors using dynamic batching to overcome memory constraints.


<details>
  <summary>Details</summary>
Motivation: Increasing system complexity from power electronics and renewable integration requires scalable computation of nonlinear participation factors for modal analysis and control design.

Method: Tensor contraction-based approach with dynamic batching strategy that adjusts batch sizes based on available computational resources.

Result: Enables calculation of high-order nonlinear participation factors that were previously infeasible due to memory constraints.

Conclusion: Provides a scalable and memory-efficient solution for computing arbitrary-order nonlinear participation factors in complex power systems.

Abstract: Participation factors (PFs) quantify the interaction between system modes and
state variables, and they play a crucial role in various applications such as
modal analysis, model reduction, and control design. With increasing system
complexity, especially due to power electronic devices and renewable
integration, the need for scalable and high-order nonlinear PF (NPF)
computation has become more critical. This paper presents an efficient
tensor-based method for calculating NPFs up to an arbitrary order. Traditional
computation of PFs directly from normal form theory is computationally
expensive -- even for second-order PFs -- and becomes infeasible for higher
orders due to memory constraints. To address this, a tensor contraction-based
approach is introduced that enables the calculation of high-order PFs using a
batching strategy. The batch sizes are dynamically determined based on the
available computational resources, allowing scalable and memory-efficient
computation.

</details>


<div id='physics.bio-ph'></div>

# physics.bio-ph [[Back]](#toc)

### [50] [Simulating Organogenesis in COMSOL Multiphysics: Tissue Patterning with Directed Cell Migration](https://arxiv.org/abs/2509.08930)
*Malte Mederacke,Chengyou Yu,Roman Vetter,Dagmar Iber*

Main category: physics.bio-ph

TL;DR: COMSOL Multiphysics implementation of a continuum model for directed cell migration using partial integro-differential equations to study cell sorting, aggregation, and tissue self-organization.


<details>
  <summary>Details</summary>
Motivation: To provide a flexible and accessible computational framework for studying directed cell migration, which is crucial for understanding tissue self-organization and morphogenesis processes.

Method: Developed a continuum model formulated as a partial integro-differential equation (PIDE) that combines random motility with non-local, density-dependent guidance cues. Implemented in COMSOL Multiphysics with support for 1D, 2D, and 3D simulations, various boundary conditions, and Lagrangian formulation for tissue growth.

Result: Successfully created a generalizable platform that enables simulations of collective cell behavior and pattern formation in complex biological contexts.

Conclusion: COMSOL Multiphysics proves to be an effective tool for implementing PIDEs, offering a versatile approach for studying directed cell migration and related biological phenomena across multiple dimensions and boundary conditions.

Abstract: We present a COMSOL Multiphysics implementation of a continuum model for
directed cell migration, a key mechanism underlying tissue self-organization
and morphogenesis. The model is formulated as a partial integro-differential
equation (PIDE), combining random motility with non-local, density-dependent
guidance cues to capture phenomena such as cell sorting and aggregation. Our
framework supports simulations in one, two, and three dimensions, with both
zero-flux and periodic boundary conditions, and can be reformulated in a
Lagrangian setting to efficiently handle tissue growth and domain deformation.
We demonstrate that COMSOL Multiphysics enables a flexible and accessible
implementation of PIDEs, providing a generalizable platform for studying
collective cell behavior and pattern formation in complex biological contexts.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [51] [Neural Transformer Backflow for Solving Momentum-Resolved Ground States of Strongly Correlated Materials](https://arxiv.org/abs/2509.09275)
*Lixing Zhang,Di luo*

Main category: cond-mat.str-el

TL;DR: Neural Transformer Backflow (NTB) is a neural network ansatz for solving strongly correlated materials like twisted MoTe2 bilayers, enabling efficient calculation of momentum-resolved ground states beyond exact diagonalization limits.


<details>
  <summary>Details</summary>
Motivation: Strongly correlated materials host exotic quantum phases but are notoriously difficult to solve due to strong interactions, requiring new computational approaches.

Method: A neural network ansatz called Neural Transformer Backflow (NTB) formulated within a multi-band projection framework that naturally enforces momentum conservation.

Result: NTB achieves high accuracy on small systems and scales to larger systems, capturing diverse correlated states including charge density waves, fractional Chern insulators, and anomalous Hall Fermi liquids.

Conclusion: The approach provides a unified framework for understanding and discovering novel phases of matter in strongly correlated materials.

Abstract: Strongly correlated materials, such as twisted transition-metal
dichalcogenide homobilayers, host a variety of exotic quantum phases but remain
notoriously difficult to solve due to strong interactions. We introduce a
powerful neural network ansatz, Neural Transformer Backflow (NTB), formulated
within a multi-band projection framework. It naturally enforces momentum
conservation and enables efficient calculations of momentum-resolved ground
states. NTB attains high accuracy on small systems and scales to higher bands
and larger system sizes far beyond the reach of exact diagonalization. By
evaluating observables such as the structure factor and momentum distribution,
we show that NTB captures diverse correlated states in tMoTe$_2$, including
charge density waves, fractional Chern insulators, and anomalous Hall Fermi
liquids, within a unified framework. Our approach paves the way for
understanding and discovering novel phases of matter in strongly correlated
materials.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [52] [Vorticity Packing Effects on Turbulent Transport in Decaying 2D Incompressible Navier-Stokes Fluids](https://arxiv.org/abs/2509.09487)
*Snehanshu Maiti,Shishir Biswas,Rajaraman Ganesh*

Main category: physics.flu-dyn

TL;DR: Study examines how initial vorticity packing fractions affect transport in 2D Navier-Stokes turbulence, showing distinct transport behaviors from subdiffusive to superdiffusive regimes depending on initial conditions and flow evolution stages.


<details>
  <summary>Details</summary>
Motivation: To understand how initial vorticity conditions influence transport properties in high Reynolds number 2D turbulence, with applications to laboratory flows, geophysical phenomena, and astrophysical structures.

Method: Used Kelvin-Helmholtz instability to initiate turbulence, computed tracer particle trajectories in Eulerian fluid fields, and quantified transport using statistical measures like absolute dispersion, position PDFs, and velocity PDFs.

Result: Initial vorticity packing fraction controls turbulence onset rate and leads to distinct transport behaviors (subdiffusive, diffusive, superdiffusive) with transitions between anisotropic and isotropic regimes. Late-stage transport dominated by large-scale coherent vortices influenced by initial conditions.

Conclusion: Initial vorticity packing significantly impacts 2D turbulence dynamics and transport, with implications for understanding transport in various quasi-2D systems through analogies with 2D Navier-Stokes turbulence.

Abstract: This paper investigates the role of initial vorticity packing fractions on
the transport properties of decaying incompressible two-dimensional
Navier-Stokes turbulence at very high Reynolds numbers and spatial resolutions.
Turbulence is initiated via the Kelvin-Helmholtz instability and evolves
through nonlinear inverse energy cascades, forming large-scale coherent
structures that dominate the flow over long eddy turnover times. The initial
vorticity packing fraction and circulation direction lead to qualitatively
distinct turbulence dynamics and transport behaviors. Tracer particle
trajectories are computed in the fluid field obtained using the Eulerian
framework, with transport and mixing quantified using statistical measures such
as absolute dispersion, position probability distribution functions (PDFs), and
velocity PDFs. In the early stages, the onset of turbulence is primarily
governed by the instability growth rate, which increases with vorticity packing
fraction. As the flow evolves, transport exhibits a range of
behaviors-subdiffusive, diffusive, or superdiffusive-and transitions between
anisotropic and isotropic regimes, depending on the initial vorticity packing,
flow structure, and stage of evolution. At later times, transport is dominated
by the motion of large-scale coherent vortices, whose dynamics are also
influenced by the initial vorticity packing ranging from subdiffusive trapping
rotational motion and random walks, and L\'evy flight-like events. These
findings offer insights into transport in quasi-2D systems-ranging from
laboratory-scale flows to geophysical phenomena and astrophysical
structures-through analogies with 2D Navier-Stokes turbulence.

</details>


### [53] [Numerical modelling of a partially loaded intermodal container freight train passing through a tunnel](https://arxiv.org/abs/2509.09591)
*Zhen Liu,David Soper,Hassan Hemida,Boyang Chen*

Main category: physics.flu-dyn

TL;DR: This paper develops an improved 1D model for predicting pressure waves when freight trains enter tunnels, addressing limitations of traditional 1D models in capturing flow separation at blunted containers and gap sections, and validating against 3D LES simulations.


<details>
  <summary>Details</summary>
Motivation: Traditional 1D models struggle to accurately capture flow separation regions at blunted freight train containers and unloaded gap sections, while 3D computational methods require excessive resources. There's a need for an accurate yet efficient predictive tool for freight train tunnel aerodynamics.

Method: The researchers evaluated various numerical models to capture flow separation complexities, developed a new 1D program with improved mesh system and boundary conditions to handle train body discontinuities from container loading patterns, and conducted parameterization studies for different loading configurations.

Result: The improved 1D model was validated against comprehensive Large Eddy Simulation (LES) results and showed effectiveness in predicting pressure wave patterns. Parameterization studies established relationships between predetermined parameters and gap length, enhancing program adaptability across various loading configurations.

Conclusion: This research successfully bridges the gap in freight train tunnel aerodynamics by providing a versatile 1D numerical tool that offers accurate pressure wave prediction while being computationally efficient compared to 3D methods.

Abstract: The bluff nature of a freight train locomotive, coupled with large gaps
created between different wagon formations and loaded goods, influence the
overall pressure wave pattern generated as the train passes through a tunnel.
Typically, 1D models are used to predict the patterns and properties of tunnel
pressure wave formations. However, accurate modelling of regions of separation
at the head of the blunted containers and at unloaded gap sections is essential
for precise predictions of pressure magnitudes. This has traditionally been
difficult to capture with 1D models. Furthermore, achieving this accuracy
through 3D computational methods demands exceptional mesh quality, significant
computational resources, and the careful selection of numerical models. This
paper evaluates various numerical models to capture these complexities within
regions of flow separation. Findings have supported the development of a new 1D
programme to calculate the pressure wave generated by a freight locomotive
entering a tunnel, and is here further extended to consider the discontinuities
of the train body created by intermodal container loading patterns, by
implementing new mesh system and boundary conditions into the 1D programme. A
parameterisation study for different loading configurations is also presented
to improve the overall programme adaptability, and the relationship between
predetermined parameters and gap length is investigated. We validate the
effectiveness of the improved 1D model through comprehensive Large Eddy
Simulation (LES) results and conduct an extensive parameterisation study to
enhance its applicability across various loading configurations. Consequently,
this research bridges the gap in freight train tunnel aerodynamics, offering a
versatile 1D numerical tool for accurate pressure wave prediction.

</details>
