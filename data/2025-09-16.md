<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 25]
- [math.AP](#math.AP) [Total: 25]
- [physics.comp-ph](#physics.comp-ph) [Total: 3]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 10]
- [physics.space-ph](#physics.space-ph) [Total: 1]
- [stat.ML](#stat.ML) [Total: 1]
- [physics.ins-det](#physics.ins-det) [Total: 1]
- [math.PR](#math.PR) [Total: 4]
- [math.CA](#math.CA) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [cs.CE](#cs.CE) [Total: 2]
- [cs.LG](#cs.LG) [Total: 4]
- [math.DG](#math.DG) [Total: 2]
- [math-ph](#math-ph) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 2]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [quant-ph](#quant-ph) [Total: 3]
- [astro-ph.SR](#astro-ph.SR) [Total: 1]
- [math.DS](#math.DS) [Total: 1]
- [math.CV](#math.CV) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Invariant subspace perturbations related to defective eigenvalues of $Δ$-Hermitian and Hamiltonian matrices](https://arxiv.org/abs/2509.10643)
*Hongguo Xu*

Main category: math.NA

TL;DR: Perturbation analysis for invariant subspaces of Δ-Hermitian and Hamiltonian matrices, focusing on defective eigenvalues and their generalized eigenvectors.


<details>
  <summary>Details</summary>
Motivation: To understand how invariant subspaces associated with defective eigenvalues behave under perturbations in structured matrices like Δ-Hermitian and Hamiltonian matrices.

Method: Developed structured perturbation theory to analyze how original eigenvectors and generalized eigenvectors contribute to perturbed invariant subspaces.

Result: Provides explicit results showing the composition of perturbed invariant subspaces in terms of the original eigenvectors and generalized eigenvectors.

Conclusion: The analysis offers insights into the structure of perturbed invariant subspaces for defective eigenvalues in these special matrix classes.

Abstract: Structured perturbation results for invariant subspaces of $\Delta$-Hermitian
and Hamiltonian matrices are provided. The invariant subspaces under
consideration are associated with the eigenvalues perturbed from a single
defective eigenvalue. The results show how the original eigenvectors and
generalized eigenvectors are involved in composing such perturbed invariant
subspaces and eigenvectors.

</details>


### [2] [Combined perturbation bounds for eigenstructure of Hermitian matrices and singular structure of general matrices](https://arxiv.org/abs/2509.10688)
*Xiao Shan Chen,Hongguo Xu*

Main category: math.NA

TL;DR: Perturbation bounds for eigenvalues/eigenspaces of Hermitian matrices and singular values/subspaces of general matrices using smooth decompositions and calculus techniques


<details>
  <summary>Details</summary>
Motivation: To provide rigorous mathematical bounds for how eigenvalues, eigenspaces, singular values, and singular subspaces change under matrix perturbations

Method: Uses smooth decompositions and elementary calculus techniques to derive perturbation bounds

Result: Developed combined perturbation bounds for spectral properties of Hermitian and general matrices

Conclusion: The paper establishes mathematical foundations for understanding sensitivity of matrix spectral properties to perturbations

Abstract: Combined perturbation bounds are presented for eigenvalues and eigenspaces of
Hermitian matrices or singular values and singular subspaces of general
matrices. The bounds are derived based on the smooth decompositions and
elementary calculus techniques.

</details>


### [3] [Mixed regularity and sparse grid approximations of $N$-body Schrödinger evolution equation](https://arxiv.org/abs/2509.10805)
*Long Meng,Dexuan Zhou*

Main category: math.NA

TL;DR: Mathematical analysis of time-dependent N-body electronic systems with sparse grid approximations to reduce computational complexity, validated on Helium atom and Hydrogen molecule.


<details>
  <summary>Details</summary>
Motivation: To address the computational complexity of time-dependent N-body electronic systems by establishing mixed regularity and developing efficient approximation methods.

Method: Developed mathematical analysis of mixed regularity for wavefunctions and created sparse grid approximations including a sparse grid Gaussian-type orbital (GTO) scheme.

Result: Validated approach on Helium atom and Hydrogen molecule, demonstrating that sparse grid GTOs offer efficient alternatives to full grid discretizations.

Conclusion: Sparse grid GTOs provide an effective computational framework for time-dependent N-body electronic systems with reduced complexity compared to traditional full grid methods.

Abstract: In this paper, we present a mathematical analysis of time-dependent $N$-body
electronic systems and establish mixed regularity for the corresponding
wavefunctions. Based on this, we develop sparse grid approximations to reduce
computational complexity, including a sparse grid Gaussian-type orbital (GTO)
scheme. We validate the approach on the Helium atom (${\rm He}$) and Hydrogen
molecule (${\rm H}_2$), showing that sparse grid GTOs offer an efficient
alternative to full grid discretizations.

</details>


### [4] [The coupling of mixed and primal finite element methods for the coupled body-plate problem](https://arxiv.org/abs/2509.10827)
*Jun Hu,Zhen Liu,Rui Ma*

Main category: math.NA

TL;DR: Coupled 3D elastic body and 2D plate analysis using Hellinger-Reissner formulation for body and primal formulation for plate, enabling non-matching meshes and direct stress approximation.


<details>
  <summary>Details</summary>
Motivation: To develop a robust computational framework for analyzing coupled 3D elastic bodies and 2D plates with rigid interface connections, allowing for flexible mesh generation and accurate stress approximation.

Method: Hellinger-Reissner formulation for 3D body with stress as auxiliary variable, primal formulation for 2D plate, domain decomposition to interface problem solved by conjugate gradient iteration, with both conforming and nonconforming finite element methods.

Result: Well-posed weak formulation established, discrete stability and error estimates derived, two specific finite element pairs shown to satisfy assumptions, numerical experiments validate theoretical results.

Conclusion: The proposed approach successfully enables non-matching meshes at interface, provides direct stress approximations, and offers effective computational solution through domain decomposition and conjugate gradient methods.

Abstract: This paper considers the coupled problem of a three-dimensional elastic body
and a two-dimensional plate, which are rigidly connected at their interface.
The plate consists of a plane elasticity model along the longitudinal direction
and a plate bending model with Kirchhoff assumptions along the transverse
direction. The Hellinger-Reissner formulation is adopted for the body by
introducing the stress as an auxiliary variable, while the primal formulation
is employed for the plate. The well-posedness of the weak formulation is
established. This approach enables direct stress approximations and allows for
non-matching meshes at the interface since the continuity condition of
displacements acts as a natural boundary condition for the body. Under certain
assumptions, discrete stability and error estimates are derived for both
conforming and nonconforming finite element methods. Two specific pairs of
conforming and nonconforming finite elements are shown to satisfy the required
assumptions, respectively. Furthermore, the problem is reduced to an interface
problem based on the domain decomposition, which can be solved effectively by a
conjugate gradient iteration. Numerical experiments are conducted to validate
the theoretical results.

</details>


### [5] [Design and accuracy trade-offs in Computational Statistics](https://arxiv.org/abs/2509.10934)
*Tiancheng Xu,Alan L. Cox,Scott Rixner*

Main category: math.NA

TL;DR: Posit floating-point format outperforms log-space representations for statistical computations with extremely small numbers, offering higher accuracy, lower resource utilization, and better performance in FPGA accelerators.


<details>
  <summary>Details</summary>
Motivation: Statistical computations often require log-space operations to prevent numerical underflow from repeated multiplications of small probabilities, but this approach has high performance, resource, and accuracy costs.

Method: Comprehensive analysis comparing posit, binary64, and logarithm representations through individual arithmetic operations, statistical bioinformatics applications, and FPGA accelerator implementations.

Result: Posit-based accelerators achieved up to 100x higher accuracy, 60% lower resource utilization, and 1.3x speedup compared to log-space accelerators, translating to 2x performance per unit resource on FPGA.

Conclusion: Posit format is a superior alternative to log-space representations for statistical computations with extremely small numbers, providing significant improvements in accuracy, efficiency, and performance.

Abstract: Statistical computations are becoming increasingly important. These
computations often need to be performed in log-space because probabilities
become extremely small due to repeated multiplications. While using logarithms
effectively prevents numerical underflow, this paper shows that its cost is
high in performance, resource utilization, and, notably, numerical accuracy.
This paper then argues that using posit, a recently proposed floating-point
format, is a better strategy for statistical computations operating on
extremely small numbers because of its unique encoding mechanism. To that end,
this paper performs a comprehensive analysis comparing posit, binary64, and
logarithm representations, examining both individual arithmetic operations,
statistical bioinformatics applications, and their accelerators. FPGA
implementation results highlight that posit-based accelerators can achieve up
to two orders of magnitude higher accuracy, up to 60\% lower resource
utilization, and up to $1.3\times$ speedup, compared to log-space accelerators.
Such improvement translates to $2\times$ performance per unit resource on the
FPGA.

</details>


### [6] [Development and Analysis of Chien-Physics-Informed Neural Networks for Singular Perturbation Problems](https://arxiv.org/abs/2509.10945)
*Gautam Singh,Sofia Haider*

Main category: math.NA

TL;DR: C-PINNs effectively solve singular perturbation problems in convection-diffusion and reaction-diffusion equations, outperforming standard PINNs and conventional methods.


<details>
  <summary>Details</summary>
Motivation: Standard PINNs struggle with singular perturbation problems due to sharp gradient changes near boundary layers caused by small perturbation parameters multiplying highest-order derivatives.

Method: Employed Chien-Physics Informed Neural Networks (C-PINNs), a modified version of standard PINNs specifically designed for singular perturbation problems, applied to 1D and 2D convection-diffusion and reaction-diffusion equations.

Result: C-PINNs provide more accurate solutions for singular perturbation problems compared to conventional methods, demonstrating better performance in handling sharp gradient changes.

Conclusion: C-PINNs are an effective modified framework that successfully addresses the challenges of singular perturbation problems where standard PINNs face difficulties.

Abstract: In this article, we employ Chien-Physics Informed Neural Networks (C-PINNs)
to obtain solutions for singularly perturbed convection-diffusion equations,
reaction-diffusion equations, and their coupled forms in both one and
two-dimensional settings. While PINNs have emerged as a powerful tool for
solving various types of differential equations, their application to singular
perturbation problems (SPPs) presents significant challenges. These challenges
arise because a small perturbation parameter multiplies the highest-order
derivatives, leading to sharp gradient changes near the boundary layer. To
overcome these difficulties, we apply C-PINNs, a modified version of the
standard PINNs framework, which is specifically designed to address singular
perturbation problems. Our study shows that C-PINNs provide a more accurate
solution for SPPs, demonstrating better performance than conventional methods.

</details>


### [7] [Vectorized 3D mesh refinement and implementation of primal hybrid FEM in MATLAB](https://arxiv.org/abs/2509.11133)
*Harish Nagula Mallesham,Sharat Gaddam,Jan Valdman,Sanjib Kumar Acharya*

Main category: math.NA

TL;DR: MATLAB implementation of 3D hybrid finite element method with vectorized mesh refinement, matrix assembly, and parallel solvers for elliptic problems.


<details>
  <summary>Details</summary>
Motivation: To develop efficient computational tools for 3D finite element analysis with improved performance through vectorization and parallel processing.

Method: Face-to-Tetrahedron connectivity mapping, vectorized 3D uniform mesh refinement, vectorized assembly of lowest-order primal hybrid finite element matrices, parallel solver and vectorized Schur complement solver.

Result: The software demonstrates good runtime performance as shown by numerical results.

Conclusion: The developed MATLAB tools provide efficient computational methods for 3D finite element analysis with vectorized and parallelized approaches that enhance performance.

Abstract: In this article, we introduce a Face-to-Tetrahedron connectivity in MATLAB
together with a vectorized 3D uniform mesh refinement technique. We introduce a
MATLAB vectorized assembly of 3D lowest-order primal hybrid finite element
matrices for a second-order elliptic problem. We introduce a parallel solver
and a vectorized Schur complement solver to solve the associated linear
problem. The numerical results illustrate the software's runtime performance.

</details>


### [8] [A time-splitting Fourier pseudospectral method for the Wigner(-Poisson)-Fokker-Planck equations](https://arxiv.org/abs/2509.11153)
*Qian Yi,Limin Xu*

Main category: math.NA

TL;DR: Proposed efficient time-splitting Fourier pseudospectral method for Wigner(-Poisson)-Fokker-Planck equations with second-order time accuracy and spectral space accuracy, validated numerically and used to study long-time dynamics and steady states.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient numerical method for solving Wigner(-Poisson)-Fokker-Planck equations and investigate the existence of steady states in these quantum systems, particularly when external potentials deviate significantly from harmonic forms.

Method: Time-splitting Fourier pseudospectral method achieving second-order accuracy in time and spectral accuracy in phase space, rigorously validated through numerical experiments.

Result: Numerical evidence demonstrates existence of steady states for Wigner-Fokker-Planck equations even with non-harmonic external potentials, a phenomenon not thoroughly established theoretically.

Conclusion: The proposed method is effective for studying long-time dynamics and provides important numerical evidence for steady state existence in quantum systems with complex external potentials.

Abstract: In this article, we propose an efficient time-splitting Fourier
pseudospectral method for the Wigner(-Poisson)-Fokker-Planck equations. The
method achieves second-order accuracy in time and spectral accuracy in phase
space, both of which are rigorously verified by numerical experiments. The
validated scheme is then employed to study the long-time dynamics of these
systems. We investigate the existence of steady states for both the
Wigner-Fokker-Planck and Wigner-Poisson-Fokker-Planck equations. Notably, for
the Wigner-Fokker-Planck system, our results provide numerical evidence for the
existence of a steady state even when the external potential is far from
harmonic. This is an important discovery, since this phenomenon has not been
thoroughly established in theory.

</details>


### [9] [Improvements on uncertainty quantification with variational autoencoders](https://arxiv.org/abs/2509.11174)
*Andrea Tonini,Tan Bui-Thanh,Francesco Regazzoni,Luca Dede',Alfio Quarteroni*

Main category: math.NA

TL;DR: Proposes a novel loss function for UQ-VAEs that removes sample mean computation, improving accuracy and reducing training time in high-dimensional Bayesian inverse problems.


<details>
  <summary>Details</summary>
Motivation: Existing UQ-VAE methods suffer from the curse of dimensionality when computing sample means, requiring prohibitively large sample sizes and increasing training time in high-dimensional parameter spaces.

Method: Developed a modified loss function that eliminates the sample mean term, establishing new theoretical results for posterior mean and covariance approximation in general mathematical problems.

Result: The new approach significantly reduces training time while improving accuracy. Validated through three benchmark tests including Poisson inverse problem, non-affine inverse problem, and cardiocirculatory model with clinical scenarios.

Conclusion: The proposed UQ-VAE with novel loss function provides efficient uncertainty quantification for Bayesian inverse problems, overcoming dimensionality challenges and demonstrating practical effectiveness in medical applications.

Abstract: Inverse problems aim to determine model parameters of a mathematical problem
from given observational data. Neural networks can provide an efficient tool to
solve these problems. In the context of Bayesian inverse problems, Uncertainty
Quantification Variational AutoEncoders (UQ-VAE), a class of neural networks,
approximate the posterior distribution mean and covariance of model parameters.
This allows for both the estimation of the parameters and their uncertainty in
relation to the observational data. In this work, we propose a novel loss
function for training UQ-VAEs, which includes, among other modifications, the
removal of a sample mean term from an already existing one. This modification
improves the accuracy of UQ-VAEs, as the original theoretical result relies on
the convergence of the sample mean to the expected value (a condition that, in
high dimensional parameter spaces, requires a prohibitively large number of
samples due to the curse of dimensionality). Avoiding the computation of the
sample mean significantly reduces the training time in high dimensional
parameter spaces compared to previous literature results. Under this new
formulation, we establish a new theoretical result for the approximation of the
posterior mean and covariance for general mathematical problems. We validate
the effectiveness of UQ-VAEs through three benchmark numerical tests: a Poisson
inverse problem, a non affine inverse problem and a 0D cardiocirculatory model,
under the two clinical scenarios of systemic hypertension and ventricular
septal defect. For the latter case, we perform forward uncertainty
quantification.

</details>


### [10] [Mechanical Proving the Symplecticity of Partitioned Runge--Kutta Methods for Determinate and Stochastic Hamiltonian Systems](https://arxiv.org/abs/2509.11188)
*Xiaojing Zhang*

Main category: math.NA

TL;DR: A new method using Gröbner basis technology to prove symplecticity of partitioned Runge-Kutta methods for deterministic and stochastic Hamiltonian systems.


<details>
  <summary>Details</summary>
Motivation: To develop a systematic algebraic approach for proving structure-preservation properties of numerical methods in Hamiltonian systems using symbolic computation.

Method: Uses Gröbner basis technology to treat partial differential relations as polynomials, compute normal forms of symplectic expressions, and verify that normal forms vanish to prove symplecticity.

Result: Successfully proves symplecticity for partitioned Runge-Kutta methods in both deterministic and stochastic Hamiltonian systems.

Conclusion: The approach provides a new framework for proving various structure-preservation laws (energy conservation, momentum conservation) for numerical methods beyond symplecticity.

Abstract: We propose a new method to prove the partitioned Runge--Kutta methods with
symplectic conditions for determinate and stochastic Hamiltonian systems are
symplectic. We utilize Gr\"obner basis technology which is the one of symbolic
computation method based on computer algebra theory and geometrical mechanical
proving theory. In this approach, from determinate Hamilton's equations, we get
the relations of partial differentials which are regarded as polynomials of
plenty variables marked indeterminates. Then, we compute the Gr\"obner basis of
above polynomials, and the normal form of symplectic expression, which is as
the middle expression, with respect to the Gr\"obner basis. Then, we compute
the Gr\"obner basis of symplectic conditions and the normal form of the middle
expression with respect to above Gr\"obner basis, and get that the normal form
is zero, which complete the proof. We also develop this procedure to the
stochastic Hamiltonian systems case and get similar result. In this paper, the
new try provide us a new idea to prove the structure-preservation laws of
another numerical methods, including the energy conservation law, the momentum
conservation law and so on.

</details>


### [11] [Dynamical Low-Rank Approximations for Kalman Filtering](https://arxiv.org/abs/2509.11210)
*Fabio Nobile,Thomas Trigo Trindade*

Main category: math.NA

TL;DR: Dynamical low rank approximation for Kalman-Bucy filtering and ensemble Kalman filter with reduced computational cost and improved accuracy.


<details>
  <summary>Details</summary>
Motivation: To reduce computational cost in filtering problems for partially observed linear SDEs while maintaining accuracy, particularly effective when the filtering distribution concentrates around low-dimensional subspaces and in small noise scenarios.

Method: Proposes DLR-KBP (dynamical low rank approximation of Kalman-Bucy process) that evolves filtering distribution on time-varying low-dimensional subspaces. Extends to DLR-ENKF where particles are evolved in low-dimensional subspace, enabling larger ensemble sizes at same computational cost.

Result: Theoretical analysis shows propagation of chaos property. Numerical experiments demonstrate effectiveness - significantly larger ensemble sizes possible compared to standard EnKF at equivalent cost, reducing Monte Carlo error and improving filter accuracy.

Conclusion: DLR approximation provides efficient computational framework for filtering problems, particularly beneficial when working with small noise and concentrated distributions, offering improved accuracy through larger ensemble capabilities.

Abstract: We propose a dynamical low rank approximation of the Kalman-Bucy process
(DLR-KBP), which evolves the filtering distribution of a partially continuously
observed linear SDE on a small time-varying subspace at reduced computational
cost. This reduction is valid in presence of small noise and when the filtering
distribution concentrates around a low dimensional subspace. We further extend
this approach to a DLR-ENKF process, where particles are evolved in a low
dimensional time-varying subspace at reduced cost. This allows for a
significantly larger ensemble size compared to standard EnKF at equivalent
cost, thereby lowering the Monte Carlo error and improving filter accuracy.
Theoretical properties of the DLR-KBP and DLR-ENKF are investigated, including
a propagation of chaos property. Numerical experiments demonstrate the
effectiveness of the technique.

</details>


### [12] [GP-CMRH: An inner product free iterative method for block two-by-two nonsymmetric linear systems](https://arxiv.org/abs/2509.11272)
*Kui Du,Jia-Jun Fan*

Main category: math.NA

TL;DR: GP-CMRH is an inner product free iterative method for solving block two-by-two nonsymmetric linear systems using a simultaneous Hessenberg process, offering better computational efficiency than existing methods.


<details>
  <summary>Details</summary>
Motivation: To develop a more computationally efficient iterative method for solving block two-by-two nonsymmetric linear systems that avoids inner products, making it suitable for high performance computing and mixed precision arithmetic.

Method: GP-CMRH uses a new simultaneous Hessenberg process that reduces two rectangular matrices to upper Hessenberg form simultaneously without employing inner products, requiring less computational cost per iteration than GPMR.

Result: Numerical experiments show GP-CMRH and GPMR have comparable convergence (GP-CMRH needs slightly more iterations), but GP-CMRH consumes less computational time in most cases and significantly outperforms GMRES and CMRH in convergence rate and runtime efficiency.

Conclusion: GP-CMRH is an effective inner product free method that offers computational advantages over existing approaches, particularly for high performance computing environments and mixed precision applications.

Abstract: We propose an inner product free iterative method called GP-CMRH for solving
block two-by-two nonsymmetric linear systems. GP-CMRH relies on a new
simultaneous Hessenberg process that reduces two rectangular matrices to upper
Hessenberg form simultaneously, without employing inner products. Compared with
GPMR [SIAM J. Matrix Anal. Appl., 44 (2023), pp. 293--311], GP-CMRH requires
less computational cost per iteration and may be more suitable for high
performance computing and low or mixed precision arithmetic due to its inner
product free property. Our numerical experiments demonstrate that GP-CMRH and
GPMR exhibit comparable convergence behavior (with GP-CMRH requiring slightly
more iterations), yet GP-CMRH consumes less computational time in most cases.
GP-CMRH significantly outperforms GMRES and CMRH in terms of convergence rate
and runtime efficiency.

</details>


### [13] [Derivative-informed Graph Convolutional Autoencoder with Phase Classification for the Lifshitz-Petrich Model](https://arxiv.org/abs/2509.11293)
*Yanlai Chen,Yajie Ji,Zhenli Xu*

Main category: math.NA

TL;DR: A Derivative-informed Graph Convolutional Autoencoder (DiGCA) is proposed to classify complex solutions of the Lifshitz-Petrich model, achieving high accuracy and efficiency in generating phase diagrams.


<details>
  <summary>Details</summary>
Motivation: The Lifshitz-Petrich model describes complex spatial patterns like quasicrystals, but solving and classifying its solutions is challenging due to high-order gradient terms and long-range orientational order.

Method: A two-stage DiGCA classifier: offline stage trains a graph convolutional autoencoder using both solutions and their derivatives to capture spatial dependencies and reduce dimensionality; online stage uses a neural network to categorize encoded solutions into phase diagrams.

Result: The DiGCA phase classifier accurately solves the LP model, classifies its solutions, and rapidly generates detailed phase diagrams with significant improvements in both efficiency and accuracy over traditional methods.

Conclusion: The proposed DiGCA framework offers a robust and efficient approach for classifying complex multi-component multi-state solutions in the Lifshitz-Petrich model, outperforming conventional methods.

Abstract: The Lifshitz-Petrich (LP) model is a classical model for describing complex
spatial patterns such as quasicrystals and multiphase structures. Solving and
classifying the solutions of the LP model is challenging due to the presence of
high-order gradient terms and the long-range orientational order characteristic
of the quasicrystals. To address these challenges, we propose a
Derivative-informed Graph Convolutional Autoencoder (DiGCA) to classify the
multi-component multi-state solutions of the LP model. The classifier consists
of two stages. In the offline stage, the DiGCA phase classifier innovatively
incorporates both solutions and their derivatives for training a graph
convolutional autoencoder which effectively captures intricate spatial
dependencies while significantly reducing the dimensionality of the solution
space. In the online phase, the framework employs a neural network classifier
to efficiently categorize encoded solutions into distinct phase diagrams. The
numerical results demonstrate that the DiGCA phase classifier accurately solves
the LP model, classifies its solutions, and rapidly generates detailed phase
diagrams in a robust manner, offering significant improvements in both
efficiency and accuracy over traditional methods.

</details>


### [14] [IGA-LBM: Isogeometric lattice Boltzmann method](https://arxiv.org/abs/2509.11427)
*Ye Ji,Monica Lacatus,Matthias Möller*

Main category: math.NA

TL;DR: The paper proposes IGA-LBM, a novel method that integrates Isogeometric Analysis with Lattice Boltzmann Method to overcome geometric limitations of traditional Cartesian-based LBM, enabling accurate simulations of flows with curved boundaries while maintaining computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Conventional LBM relies on Cartesian grids which introduce stair-step artifacts and inaccuracies in flows with curved boundaries, limiting geometric fidelity and causing spurious forces and boundary-layer errors.

Method: The proposed IGA-LBM method integrates Isogeometric Analysis with LBM, using non-uniform rational B-Splines (NURBS) to construct body-fitted computational grids that provide sub-element geometric accuracy and higher-order continuity.

Result: Benchmark simulations demonstrate that IGA-LBM delivers significantly more accurate boundary-layer predictions and pressure/force estimates than standard Cartesian LBM while preserving computational efficiency and scalability.

Conclusion: IGA-LBM combines geometric exactness with LBM's algorithmic simplicity, offering a practical route to high-fidelity simulations in engineering and scientific applications involving complex curved geometries.

Abstract: The lattice Boltzmann method has become a widely adopted approach in
computational fluid dynamics, offering unique advantages in mesoscopic kinetic
modeling, intrinsic parallelism, and simple treatment of boundary conditions.
However, its conventional reliance on Cartesian grids fundamentally limits
geometric fidelity in flows involving curved boundaries, introducing stair-step
artifacts that propagate as spurious forces and boundary-layer inaccuracies.
  To address these challenges, we propose the isogeometric lattice Boltzmann
method, which seamlessly integrates Isogeometric Analysis with LBM, leveraging
the geometric precision of non-uniform rational B-Splines to construct
body-fitted computational grids. Unlike conventional Cartesian-based LBM, the
proposed approach eliminates stair-step boundary artifacts by providing
sub-element geometric accuracy while maintaining the efficiency of LBM.
Furthermore, the higher-order continuity of NURBS improves gradient resolution,
reducing numerical diffusion in high-Reynold's-number flows. The parametric
grid adaptation of IGA enables $h$-, $p$-, and $k$-refinement strategies,
allowing for localized resolution enhancement in boundary layers and regions
with high solution gradients. Additionally, the diffeomorphic mapping
properties of IGA ensure intrinsic conservation, preserving advection
invariants and suppressing numerical oscillations, leading to enhanced
stability.
  Benchmark simulations on flows with curved and complex geometries demonstrate
that IGA-LBM delivers significantly more accurate boundary-layer predictions
and pressure/force estimates than standard Cartesian LBM, while preserving its
computational efficiency and scalability. By combining geometric exactness with
the algorithmic simplicity of LBM, IGA-LBM offers a practical route to
high-fidelity simulations in engineering and scientific applications.

</details>


### [15] [Unified analysis of saddle point problems via auxiliary space theory](https://arxiv.org/abs/2509.11434)
*Jongho Park*

Main category: math.NA

TL;DR: Sharp eigenvalue estimates for Schur complements in saddle point problems using auxiliary space theory, improving existing results with applications in augmented Lagrangian, mixed FEM, and domain decomposition methods.


<details>
  <summary>Details</summary>
Motivation: To develop a unified framework for obtaining sharp convergence estimates for iterative methods in saddle point problems by interpreting them through auxiliary space theory.

Method: Uses auxiliary space theory to reinterpret iterative methods as equivalent elementary methods on auxiliary spaces, enabling derivation of sharp extremal eigenvalue estimates for Schur complements.

Result: Derived sharp estimates for extremal eigenvalues of Schur complements, improving and refining existing results which can be recovered as corollaries of the proposed framework.

Conclusion: The auxiliary space framework provides a versatile approach for straightforward condition number estimation of Schur complements in various scientific computing applications including augmented Lagrangian, mixed FEM, and domain decomposition methods.

Abstract: We present sharp estimates for the extremal eigenvalues of the Schur
complements arising in saddle point problems. These estimates are derived using
the auxiliary space theory, in which a given iterative method is interpreted as
an equivalent but more elementary iterative method on an auxiliary space,
enabling us to obtain sharp convergence estimates. The proposed framework
improves or refines several existing results, which can be recovered as
corollaries of our results. To demonstrate the versatility of the framework, we
present various applications from scientific computing: the augmented
Lagrangian method, mixed finite element methods, and nonoverlapping domain
decomposition methods. In all these applications, the condition numbers of the
corresponding Schur complements can be estimated in a straightforward manner
using the proposed framework.

</details>


### [16] [Convergence of a Second-Order Projection Method to Leray-Hopf Solutions of the Incompressible Navier-Stokes Equations](https://arxiv.org/abs/2509.11483)
*Franziska Weber*

Main category: math.NA

TL;DR: First rigorous convergence proof for second-order projection method for incompressible Navier-Stokes equations to Leray-Hopf weak solutions under minimal data assumptions.


<details>
  <summary>Details</summary>
Motivation: Projection methods are widely used for incompressibility enforcement, but rigorous convergence results for higher-order schemes (beyond first-order) for non-smooth solutions have been lacking.

Method: BDF2 time discretization combined with conforming finite elements in space, using discrete energy inequality and compactness argument with Simon's theorem and refined time-continuity estimates.

Result: Established convergence (up to subsequence) of second-order projection method to Leray-Hopf weak solutions with minimal assumptions: u0 ∈ L²_div(Ω) and f ∈ L²(0,T;L²_div(Ω)).

Conclusion: This provides the first rigorous convergence proof for higher-order projection methods without additional solution assumptions beyond standard a priori energy estimates.

Abstract: We analyze a second-order projection method for the incompressible
Navier-Stokes equations on bounded Lipschitz domains. The scheme employs a
Backward Differentiation Formula of order two (BDF2) for the time
discretization, combined with conforming finite elements in space. Projection
methods are widely used to enforce incompressibility, yet rigorous convergence
results for possibly non-smooth solutions have so far been restricted to
first-order schemes. We establish, for the first time, convergence (up to
subsequence) of a second-order projection method to Leray-Hopf weak solutions
under minimal assumptions on the data, namely $u_0 \in
L^2_{\text{div}}(\Omega)$ and $f \in L^2(0,T;L^2_{\text{div}}(\Omega))$. Our
analysis relies on two ingredients: A discrete energy inequality providing
uniform $L^{\infty}(0,T;L^2(\Omega))$ and $L^2(0,T;H^1_0(\Omega))$ bounds for
suitable interpolants of the discrete velocities, and a compactness argument
combining Simon's theorem with refined time-continuity estimates. These tools
overcome the difficulty that only the projected velocity satisfies an
approximate divergence-free condition, while the intermediate velocity is
controlled in space. We conclude that a subsequence of the approximations
converges to a Leray-Hopf weak solution. This result provides the first
rigorous convergence proof for a higher-order projection method under no
additional assumptions on the solution beyond those following from the standard
a priori energy estimate.

</details>


### [17] [Neural solver for sixth-order ordinary differential equations](https://arxiv.org/abs/2509.11541)
*Janavi Bhalala,B. Veena S. N. Rao*

Main category: math.NA

TL;DR: A deep learning neural solver method for approximating sixth-order ODEs using unsupervised learning with a two-term loss function that handles both the differential equation and boundary conditions.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient method for solving complex sixth-order ordinary differential equations using neural networks, reducing the need for extensive hyperparameter tuning while maintaining high accuracy.

Method: Uses a feedforward neural network with a mean squared loss function composed of two terms - one for satisfying the differential equation and another for boundary/initial conditions. Minimized using quasi-Newton optimization.

Result: The method shows strong agreement with exact solutions in point-wise comparisons, achieves high accuracy with low epochs, and minimizes learnable hyperparameters in boundary value problems.

Conclusion: The proposed neural solver framework provides an attractive and effective approach for solving sixth-order ODEs, offering high accuracy with computational efficiency for the mathematics community.

Abstract: A method for approximating sixth-order ordinary differential equations is
proposed, which utilizes a deep learning feedforward artificial neural network,
referred to as a neural solver. The efficacy of this unsupervised machine
learning method is demonstrated through the solution of two distinct boundary
value problems (BVPs), with the method being extended to include the solution
of a sixth-order ordinary differential equation (ODE). The proposed mean
squared loss function is comprised of two terms: the differential equation is
satisfied by the first term, while the initial or boundary conditions are
satisfied by the second. The total loss function is minimized using a
quasi-Newton optimization method to obtain the desired network output. The
approximation capability of the proposed method is verified for sixth-order
ODEs. Point-wise comparisons of the approximations show strong agreement with
available exact solutions. The proposed algorithm minimizes the overall
learnable network hyperparameters in a given BVP. Simple minimization of the
total loss function yields highly accurate results even with a low number of
epochs. Therefore, the proposed framework offers an attractive setting for the
computational mathematics community.

</details>


### [18] [Learning Singularity-Encoded Green's Functions with Application to Iterative Methods](https://arxiv.org/abs/2509.11580)
*Qi Sun,Shengyan Li,Bowen Zheng,Lili Ju,Xuejun Xu*

Main category: math.NA

TL;DR: A novel singularity-encoded learning approach for Green's function surrogate modeling that embeds the function in higher-dimensional space using neural networks to handle singularities and accelerate iterative solvers.


<details>
  <summary>Details</summary>
Motivation: Green's function is crucial for elliptic PDEs but lacks closed-form expressions and is challenging to compute numerically due to doubled dimensionality and intrinsic singularities, requiring effective surrogate modeling.

Method: Embed Green's function in one-order higher-dimensional space by encoding prior estimates as augmented variables, use neural network parametrization to manage dimensionality, then project back to original domain to exploit spectral bias.

Result: Empirical verification through 2D and 4D Green's function experiments shows satisfactory singularity resolution and acceleration of iterative solvers, serving as effective preconditioners or hybrid solver components.

Conclusion: The singularity-encoded learning approach successfully addresses Green's function computation challenges, providing an unsupervised method that handles singularities and accelerates conventional iterative schemes through deep surrogate modeling.

Abstract: Green's function provides an inherent connection between theoretical analysis
and numerical methods for elliptic partial differential equations, and general
absence of its closed-form expression necessitates surrogate modeling to guide
the design of effective solvers. Unfortunately, numerical computation of
Green's function remains challenging due to its doubled dimensionality and
intrinsic singularity. In this paper, we present a novel singularity-encoded
learning approach to resolve these problems in an unsupervised fashion. Our
method embeds the Green's function within a one-order higher-dimensional space
by encoding its prior estimate as an augmented variable, followed by a neural
network parametrization to manage the increased dimensionality. By projecting
the trained neural network solution back onto the original domain, our deep
surrogate model exploits its spectral bias to accelerate conventional iterative
schemes, serving either as a preconditioner or as part of a hybrid solver. The
effectiveness of our proposed method is empirically verified through numerical
experiments with two and four dimensional Green's functions, achieving
satisfactory resolution of singularities and acceleration of iterative solvers.

</details>


### [19] [Strong convergence rates of stochastic theta methods for index 1 stochastic differential algebraic equations under non-globally Lipschitz conditions](https://arxiv.org/abs/2509.11618)
*Lin Chen,Ziheng Chen,Jing Zhao*

Main category: math.NA

TL;DR: Analysis of numerical methods for stochastic differential algebraic equations with singular matrices under non-global Lipschitz conditions, proving mean square convergence rates.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of analyzing strong convergence rates for SDAEs with singular constraint matrices and superlinear growth coefficients, which is highly nontrivial due to these complexities.

Method: Developed an approach for establishing mean square convergence rates under global monotonicity conditions. Specifically analyzed stochastic theta methods with θ ∈ [1/2,1].

Result: Proved that each stochastic theta method with θ ∈ [1/2,1] achieves a mean square convergence rate of order 1/2. Theoretical findings were validated through numerical experiments.

Conclusion: The proposed approach successfully establishes convergence rates for numerical methods solving index 1 SDAEs with non-constant singular matrices under challenging non-global Lipschitz conditions.

Abstract: This work investigates numerical approximations of index 1 stochastic
differential algebraic equations (SDAEs) with non-constant singular matrices
under non-global Lipschitz conditions. Analyzing the strong convergence rates
of numerical solutions in this setting is highly nontrivial, due to both the
singularity of the constraint matrix and the superlinear growth of the
coefficients. To address these challenges, we develop an approach for
establishing mean square convergence rates of numerical methods for SDAEs under
global monotonicity conditions. Specifically, we prove that each stochastic
theta method with $\theta \in [\frac{1}{2},1]$ achieves a mean square
convergence rate of order $\frac{1}{2}$. Theoretical findings are further
validated through a series of numerical experiments.

</details>


### [20] [Linear and Nonlinear Boundary Conditions: What's the difference?](https://arxiv.org/abs/2509.11651)
*Jan Nordström*

Main category: math.NA

TL;DR: New energy and entropy stable open boundary conditions for linear/nonlinear problems that generalize classical characteristic boundary conditions to nonlinear settings


<details>
  <summary>Details</summary>
Motivation: To develop boundary conditions that provide stability estimates bounded only by external data for both linear and nonlinear initial boundary value problems

Method: Derived new boundary procedures that can be implemented both strongly and weakly, generalizing classical characteristic boundary conditions to nonlinear problems

Result: Successfully developed boundary conditions that bound both linear and nonlinear problems with stability estimates dependent only on external data

Conclusion: The new boundary procedures offer advantages over classical methods by providing stability for nonlinear problems and flexible implementation options

Abstract: In previous work, we derived new energy and entropy stable open boundary
conditions and implementation procedures for linear and nonlinear initial
boundary value problems. These boundary procedures results in estimates bounded
by external data only. Interestingly, these new boundary conditions generalize
the well known classical characteristic boundary conditions for linear problems
to the nonlinear setting. We discuss the similarities and differences between
these two boundary procedures and point out the advantages with the new
procedures. In particular we show that the new boundary conditions bound both
linear and nonlinear initial boundary value problems and can be implemented
both strongly and weakly.

</details>


### [21] [Numerical Approximation of the logarithmic Laplacian via sinc-basis](https://arxiv.org/abs/2509.11693)
*Patrick Dondl,Ludwig Striet*

Main category: math.NA

TL;DR: Using sinc-function basis to solve fractional PDEs and extend to nonlocal operators like logarithmic Laplacian, with eigenvalue computations on 2D disks.


<details>
  <summary>Details</summary>
Motivation: To extend previous work on solving fractional partial differential equations using sinc-function basis to more general nonlocal operators, specifically demonstrating applicability to the logarithmic Laplacian operator.

Method: Utilize a basis of dilated and shifted sinc-functions to numerically solve nonlocal equations, applying the approach to the Dirichlet problem for the logarithmic Laplacian operator with Fourier symbol log(|ω|²).

Result: Successfully computed eigenvalues of the logarithmic Laplacian on disks with different radii in ℝ² using the developed numerical algorithms.

Conclusion: The sinc-function basis approach previously developed for fractional PDEs can be effectively extended to solve various nonlocal equations, including those involving the logarithmic Laplacian operator.

Abstract: In recent works, the authors of this chapter have shown with co-authors how a
basis consisting of dilated and shifted $\text{sinc}$-functions can be used to
solve fractional partial differential equations. As a model problem, the
fractional Dirichlet problem with homogeneous exterior value conditions was
solved. In this work, we briefly recap the algorithms developed there and that
-- from a computational point of view -- they can be used to solve nonlocal
equations given through different operators as well. As an example, we
numerically solve the Dirichlet problem for the logarithmic Laplacian
$\log(-\Delta)$ which has the Fourier symbol $\log(\left|\omega\right|^2)$ and
compute its Eigenvalues on disks with different radii in $\mathbb R^2$.

</details>


### [22] [A Chebyshev--Ritz Spectral Framework for Nonlinear Vibration of CNT-Reinforced Composite Beams](https://arxiv.org/abs/2509.11946)
*Maryam Jalili,Samad Noeiaghdam*

Main category: math.NA

TL;DR: Spectral Ritz method for nonlinear vibration analysis of CNT-reinforced composite beams using Chebyshev basis functions with exact boundary conditions, showing exponential convergence and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient and accurate computational method for analyzing nonlinear free vibrations of carbon nanotube-reinforced composite beams, addressing the need for reliable analysis tools for advanced composite materials with complex CNT distributions.

Method: Spectral Ritz formulation with boundary-adapted Chebyshev basis functions that exactly satisfy clamped and simply supported boundary conditions. Incorporates von Kármán geometric nonlinearity and uses modified rule of mixtures for effective material properties of uniform and functionally graded CNT distributions.

Result: Exponential convergence achieved with basis sizes N ≥ 12 (fundamental frequency error < 0.1%). Significant computational efficiency gains compared to finite element methods. Fundamental frequency increases with CNT volume fraction and is sensitive to load-transfer efficiency. Functionally graded patterns enhance stiffness over uniform distributions. Validation shows only a few percent differences from benchmarks.

Conclusion: The spectral Ritz method provides an efficient and accurate approach for nonlinear vibration analysis of CNTRC beams. Current limitation is reliance on Euler-Bernoulli beam assumption (neglecting shear deformation and damping), which should be addressed in future work. All data and scripts provided for reproducibility.

Abstract: This study develops a spectral Ritz formulation for the nonlinear free
vibration analysis of carbon nanotube-reinforced composite (CNTRC) beams.
Boundary-adapted Chebyshev basis functions are constructed to exactly satisfy
clamped and simply supported boundary conditions. The governing equations
incorporate von~K\'{a}rm\'{a}n geometric nonlinearity, while the effective
material properties for both uniform and functionally graded (FG) CNT
distributions are evaluated using a modified rule of mixtures. Discretization
via the Chebyshev-Ritz approach produces a reduced-order model exhibiting
exponential convergence; for basis sizes $N \geq 12$, the fundamental frequency
error remains below $0.1\%$ relative to published benchmarks.
  Computational results demonstrate substantial efficiency gains, with the
spectral approach requiring significantly less time than high-fidelity finite
element discretizations of comparable accuracy. Parametric studies reveal that
the fundamental frequency increases with CNT volume fraction and is sensitive
to the interfacial load-transfer efficiency parameter $\eta_E$. Selected FG
patterns are shown to enhance stiffness relative to uniformly distributed CNTs.
  Validation against established numerical benchmarks yields relative
differences of only a few percent. The current limitation of the method is its
reliance on the Euler-Bernoulli beam assumption, which neglects transverse
shear deformation and damping; addressing these effects is proposed for future
work. All numerical data and scripts are provided as supplementary material to
ensure reproducibility.

</details>


### [23] [X-ray imaging from nonlinear waves: numerical reconstruction of a cubic nonlinearity](https://arxiv.org/abs/2509.11951)
*Markus Harju,Suvi Takalahti,Teemu Tyni*

Main category: math.NA

TL;DR: Direct numerical method to recover unknown potential q(x,t) from boundary measurements of nonlinear waves using Radon transform and spectral regularization for noise stability.


<details>
  <summary>Details</summary>
Motivation: Solve inverse boundary value problem for nonlinear wave equation to reconstruct unknown potential q(x,t) from Dirichlet-to-Neumann map using real-valued waves.

Method: Propose Radon transform-based reconstruction with spectral regularization to stabilize numerical differentiation, plus direct pointwise reconstruction for comparison.

Result: Numerical experiments demonstrate feasibility of recovering potentials from boundary measurements, showing advantages of Radon-based approach.

Conclusion: The method successfully reconstructs unknown potentials from nonlinear wave boundary data with improved robustness to noise through spectral regularization.

Abstract: We study an inverse boundary value problem for the nonlinear wave equation in
$2 + 1$ dimensions. The objective is to recover an unknown potential $q(x, t)$
from the associated Dirichlet-to-Neumann map using real-valued waves. We
propose a direct numerical reconstruction method for the Radon transform of
$q$, which can then be inverted using standard X-ray tomography techniques to
determine $q$. Our implementation introduces a spectral regularization
procedure to stabilize the numerical differentiation step required in the
reconstruction, improving robustness with respect to noise in the boundary
data. We also give rigorous justification and stability estimates for the
regularized spectral differentiation of noisy measurements. A direct pointwise
reconstruction method for $q$ is also implemented for comparison. Numerical
experiments demonstrate the feasibility of recovering potentials from boundary
measurements of nonlinear waves and illustrate the advantages of the
Radon-based reconstruction.

</details>


### [24] [Adaptive least-squares space-time finite element methods for convection-diffusion problems](https://arxiv.org/abs/2509.11955)
*Christian Köthe,Olaf Steinbach*

Main category: math.NA

TL;DR: Adaptive space-time least-squares finite element methods for convection-diffusion equations with stability analysis and error indicators for adaptive refinement.


<details>
  <summary>Details</summary>
Motivation: To develop stable numerical methods for convection-diffusion problems, especially convection-dominated cases with small diffusion coefficients, without needing additional stabilization terms.

Method: Formulate space-time least-squares finite element methods treating convective derivative as part of total time derivative. Use discrete adjoint to define local a posteriori error indicators for adaptive refinement.

Result: Discrete finite element schemes are uniquely solvable. Numerical examples demonstrate the effectiveness of the adaptive approach in resolving convection-dominated problems.

Conclusion: The adaptive space-time least-squares method provides a stable framework for convection-diffusion equations, effectively handling both bounded and unbounded velocities through adaptive refinement driven by error indicators.

Abstract: In this paper we formulate and analyse adaptive (space-time) least-squares
finite element methods for the solution of convection-diffusion equations. The
convective derivative $\mathbf{v} \cdot \nabla u$ is considered as part of the
total time derivative $\frac{d}{dt}u = \partial_t u + \mathbf{v} \cdot \nabla
u$, and therefore we can use a rather standard stability and error analysis for
related space-time finite element methods. For stationary problems we restrict
the ansatz space $H^1_0(\Omega)$ such that the convective derivative is
considered as an element of the dual $H^{-1}(\Omega)$ of the test space
$H^1_0(\Omega)$, which also allows unbounded velocities $\mathbf{v}$. While the
discrete finite element schemes are always unique solvable, the numerical
solutions may suffer from a bad approximation property of the finite element
space when considering convection dominated problems, i.e., small diffusion
coefficients. Instead of adding suitable stabilization terms, we aim to resolve
the solutions by using adaptive (space-time) finite element methods. For this
we introduce a least-squares approach where the discrete adjoint defines local
a posteriori error indicators to drive an adaptive scheme. Numerical examples
illustrate the theoretical considerations.

</details>


### [25] [RJD-BASE: Multi-Modal Spectral Clustering via Randomized Joint Diagonalization](https://arxiv.org/abs/2509.11981)
*Haoze He,Artemis Pados,Daniel Kressner*

Main category: math.NA

TL;DR: RJD-BASE: Randomized joint diagonalization with BASE selection for efficient multimodal spectral clustering using random Laplacian combinations and principled embedding selection.


<details>
  <summary>Details</summary>
Motivation: Classical multimodal clustering methods rely on costly iterative refinement and may fail to directly target the spectral subspace relevant for clustering, requiring more efficient and objective-aligned approaches.

Method: Two innovations: 1) Sampling random convex combinations of Laplacians as scalable alternative to explicit eigenspace alignment, 2) BASE selection rule (Bottom-k Aggregated Spectral Energy) applied as selection mechanism rather than optimization target.

Result: RJD-BASE reliably selects high-quality embeddings, outperforming classical multimodal clustering methods at low computational cost in experiments on synthetic and real-world datasets.

Conclusion: RJD-BASE provides an easily implementable, computationally efficient method that aligns with clustering objectives and leverages standard eigensolver progress, offering superior performance over traditional approaches.

Abstract: We revisit the problem of spectral clustering in multimodal settings, where
each data modality is encoded as a graph Laplacian. While classical
approaches--including joint diagonalization, spectral co-regularization, and
multiview clustering--attempt to align embeddings across modalities, they often
rely on costly iterative refinement and may fail to directly target the
spectral subspace relevant for clustering. In this work, we introduce two key
innovations. First, we bring the power of randomization to this setting by
sampling random convex combinations of Laplacians as a simple and scalable
alternative to explicit eigenspace alignment. Second, we propose a principled
selection rule based on Bottom-$k$ Aggregated Spectral Energy (BASE)--a
$k$-dimensional extension of the directional smoothness objective from recent
minimax formulations--which we uniquely apply as a selection mechanism rather
than an optimization target. The result is Randomized Joint Diagonalization
with BASE Selection (RJD-BASE), a method that is easily implementable,
computationally efficient, aligned with the clustering objective, and grounded
in decades of progress in standard eigensolvers. Through experiments on
synthetic and real-world datasets, we show that RJD-BASE reliably selects
high-quality embeddings, outperforming classical multimodal clustering methods
at low computational cost.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [26] [Simultaneous determination of wave speed, diffusivity and nonlinearity in the Westervelt equation using complex time-periodic solutions](https://arxiv.org/abs/2509.10718)
*Sebastian Acosta,Benjamin Palacios*

Main category: math.AP

TL;DR: Recovering coefficients of Westervelt equation using complex-valued time-periodic solutions and harmonic boundary data.


<details>
  <summary>Details</summary>
Motivation: Solve inverse problem for Westervelt equation to determine wave speed, diffusivity, and nonlinearity coefficients simultaneously.

Method: Construct complex-valued time-periodic solutions excited at high frequency from boundary, use first- and second-harmonic Cauchy data.

Result: Boundary harmonic data is sufficient to determine all three coefficients (wave speed, diffusivity, nonlinearity) in the interior domain.

Conclusion: High-frequency time-harmonic excitation with harmonic boundary measurements enables complete coefficient recovery for Westervelt equation.

Abstract: We consider an inverse problem governed by the Westervelt equation with
linear diffusivity and quadratic-type nonlinearity. The objective of this
problem is to recover all the coefficients of this nonlinear partial
differential equation. We show that, by constructing complex-valued
time-periodic solutions excited from the boundary time-harmonically at a
sufficiently high frequency, knowledge of the first- and second-harmonic Cauchy
data at the boundary is sufficient to simultaneously determine the wave speed,
diffusivity and nonlinearity in the interior of the domain of interest.

</details>


### [27] [Alexandrov estimates for polynomial operators by determinant majorization](https://arxiv.org/abs/2509.10879)
*F. Reese Harvey,Kevin R. Payne*

Main category: math.AP

TL;DR: Estimates for supremum, infimum and oscillation of solutions to inhomogeneous fully nonlinear elliptic equations using I-central Garding-Dirichlet operators, combining Alexandrov estimates and determinant majorization techniques.


<details>
  <summary>Details</summary>
Motivation: To extend classical Alexandrov-Bakelman-Pucci estimates from linear operators to a wide class of inhomogeneous fully nonlinear elliptic equations, leveraging recent advances in geometric analysis and potential theory.

Method: Combines two recent results: an Alexandrov estimate for locally semiconvex functions based on the area formula (Payne-Redaelli 2025) and a determinant majorization estimate (Harvey-Lawson 2024) that generalizes the arithmetic-geometric mean inequality. Uses potential theoretic approach with subequation subharmonics and their duals, with semiconvex approximation playing a crucial role.

Result: Obtains estimates on the supremum, infimum and oscillation of solutions for inhomogeneous fully nonlinear elliptic equations, extending classical results to this broader class of operators.

Conclusion: The approach successfully generalizes classical Alexandrov-Bakelman-Pucci estimates to fully nonlinear elliptic equations through the combination of modern geometric analysis techniques and potential theory, with semiconvex approximation as a key methodological component.

Abstract: We obtain estimates on the supremum, infimum and oscillation of solutions for
a wide class of inhomogeneous fully nonlinear elliptic equations on Euclidean
domains where the differential operator is an I-central Garding-Dirichlet
operator in the sense of Harvey-Lawson (2024). The argument combines two recent
results: an Alexandrov estimate of Payne-Redaelli (2025) for locally semiconvex
functions based on the area formula and a determinant majorization estimate of
Harvey-Lawson (2024).
  The determinant majorization estimate has as a special case the arithmetic -
geometric mean inequality, so the result includes the classical
Alexandrov-Bakelman-Pucci estimate for linear operators. A potential theoretic
approach is used involving subequation subharmonics and their dual
subharmonics. Semiconvex approximation plays a crucial role.

</details>


### [28] [Direct reconstruction of anisotropic self-adjoint inclusions in the Calderón problem](https://arxiv.org/abs/2509.10994)
*Henrik Garde,David Johansson,Thanasis Zacharopoulos*

Main category: math.AP

TL;DR: Extension of monotonicity method for exact reconstruction of inclusions in anisotropic Calderón problem from partial data, handling unknown anisotropic perturbations to known conductivity.


<details>
  <summary>Details</summary>
Motivation: To address the direct exact reconstruction of inclusions in the anisotropic Calderón problem with partial data, extending previous methods that were limited to isotropic cases or full data scenarios.

Method: Monotonicity method applied to local Neumann-to-Dirichlet map for reconstructing inclusions based on unknown anisotropic self-adjoint perturbations to a known anisotropic conductivity coefficient, with definiteness condition near boundaries.

Result: Successfully extends the reconstruction method to anisotropic conductivities in any spatial dimension d≥2, providing exact reconstruction capabilities from partial data.

Conclusion: The method enables direct exact reconstruction of inclusions in anisotropic Calderón problems and offers new insights into non-uniqueness issues in anisotropic conductivity reconstruction.

Abstract: We extend the monotonicity method for direct exact reconstruction of
inclusions in the partial data Calder\'on problem, to the case of anisotropic
conductivities in any spatial dimension $d\geq 2$. Specifically, from a local
Neumann-to-Dirichlet map, we give reconstruction methods of inclusions based on
unknown anisotropic self-adjoint perturbations to a known anisotropic
conductivity coefficient. A main assumption is a definiteness condition for the
perturbations near the outer inclusion boundaries. This additionally provides
new insights into the non-uniqueness issues of the anisotropic Calder\'on
problem.

</details>


### [29] [The concentration-compactness principle for Musielak-Orlicz spaces and applications](https://arxiv.org/abs/2509.11008)
*Ala Eddine Bahrouni,Anouar Bahrouni*

Main category: math.AP

TL;DR: Extension of Concentration-Compactness Principle to Musielak-Orlicz spaces with applications to critical quasilinear equations


<details>
  <summary>Details</summary>
Motivation: To generalize the Concentration-Compactness Principle beyond classical function spaces to more flexible Musielak-Orlicz spaces, enabling analysis of a wider range of nonlinear problems including variable exponent and double phase spaces

Method: Extends the Concentration-Compactness Principle to Musielak-Orlicz spaces in both bounded and unbounded domains, then applies these results with variational methods

Result: The generalized principle covers important special cases (classical Orlicz, variable exponent, double phase spaces) and enables proof of existence of solutions for quasilinear equations with critical nonlinear terms

Conclusion: The extension provides a unified framework for analyzing critical nonlinear problems in various generalized function spaces, with applications to new types of double phase problems where exponents depend on the solution

Abstract: This paper extends the Concentration-Compactness Principle to Musielak-Orlicz
spaces, working in both bounded and unbounded domains. We show that our results
include important special cases like classical Orlicz spaces, variable exponent
spaces, double phase spaces, and a new type of double phase problem where the
exponents depend on the solution. Using these general results with variational
methods, we prove that certain quasilinear equations with critical nonlinear
terms have solutions.

</details>


### [30] [Approximation in an optimal design problem governed by the heat equation](https://arxiv.org/abs/2509.11011)
*Kei Matsushima,Tomoyuki Oka*

Main category: math.AP

TL;DR: Analysis of optimal two-material design for heat equations with time-dependent sources, using level set methods and perimeter regularization to ensure existence and convergence properties.


<details>
  <summary>Details</summary>
Motivation: To address the lack of guaranteed existence in optimal two-material designs for heat equations with time constraints, and to develop approximation methods with geometric constraints like perimeter regularization.

Method: Uses level set function representation with Dirichlet energy perturbation, analyzes existence of optimal level set functions, and constructs two-material domains via nonlinear diffusion-based level set method.

Result: Shows optimal level set functions exist for perturbation problem, minimum value converges to elliptic case, and establishes validity of approximation with clarified asymptotic behavior.

Conclusion: The level set approach with perimeter regularization provides effective approximation for two-material optimal design problems in heat equations, with proven convergence properties and asymptotic behavior.

Abstract: This paper studies a two-material optimal design problem for the
time-averaged duality pairing between a (possibly time-dependent) heat source
and the weak solution of an initial-boundary value problem for the heat
equation with a two-material diffusion coefficient, under a volume constraint.
In general, such optimal designs are not guaranteed to exist, and geometric
constraints such as the perimeter are required. As an approximation of the
problem with an additional perimeter constraint, a material representation
based on a level set function, together with a perturbation of the Dirichlet
energy, is employed. It is then shown that optimal level set functions exist
for the perturbation problem, and the corresponding minimum value converges to
that of the elliptic case, thereby elucidating the long-time behavior.
Furthermore, two-material domains satisfying this property are also constructed
via the nonlinear diffusion-based level set method. In particular, the
asymptotic behavior with respect to the perturbation parameter is clarified,
and the validity of the approximation is established.

</details>


### [31] [Long-Time Dynamics of the 3D Vlasov-Maxwell System with Boundaries](https://arxiv.org/abs/2509.11064)
*Jin Woo Jang,Chanwoo Kim*

Main category: math.AP

TL;DR: First construction of asymptotically stable non-vacuum steady states for 3D nonlinear Vlasov-Maxwell system in half-space beyond vacuum scattering regime


<details>
  <summary>Details</summary>
Motivation: Understanding long-time wave-particle interactions in presence of boundaries and interacting magnetic fields

Method: Combines construction of stationary solutions to boundary-value problem with proof of asymptotic dynamical stability in L^∞ under small perturbations

Result: Global-in-time classical solutions constructed with asymptotic stability of non-vacuum steady states

Conclusion: Provides new framework for analyzing boundary effects and magnetic field interactions in Vlasov-Maxwell systems

Abstract: We construct global-in-time classical solutions to the nonlinear
Vlasov-Maxwell system in a three-dimensional half-space beyond the vacuum
scattering regime. Our approach combines the construction of stationary
solutions to the associated boundary-value problem with a proof of their
asymptotic dynamical stability in $L^\infty$ under small perturbations,
providing a new framework for understanding long-time wave-particle
interactions in the presence of boundaries and interacting magnetic fields. To
the best of our knowledge, this work presents the first construction of
asymptotically stable non-vacuum steady states under general perturbations in
the full three-dimensional nonlinear Vlasov-Maxwell system.

</details>


### [32] [A new proof on quasilinear Schrödinger equations with prescribed mass and combined nonlinearity](https://arxiv.org/abs/2509.11073)
*Jianhua Chen,Jijiang Sun,Chenggui Yuan,Jian Zhang*

Main category: math.AP

TL;DR: Study of quasilinear Schrödinger equation with mass constraint, establishing existence of normalized solutions using new methods that resolve previous open problems.


<details>
  <summary>Details</summary>
Motivation: This paper continues previous work on quasilinear Schrödinger equations with mass constraints, aiming to resolve open problems in the existence and characterization of normalized solutions, particularly addressing limitations of previous approaches.

Method: Uses a suitable change of variables and develops new methods including monotonicity tricks to study constrained minimization problems and obtain mountain pass type solutions, avoiding reliance on Palais-Smale-Pohozaev sequences.

Result: Provides qualitative analysis of constrained minimization for certain parameter ranges and proves existence of two distinct radial normalized solutions - a local minimizer and a mountain pass type solution distinct from the minimizer.

Conclusion: The developed methods successfully resolve the open problem (OP1) from previous work and can be extended to study mountain pass-type normalized solutions for other classes of quasilinear Schrödinger equations, offering a new approach beyond traditional techniques.

Abstract: In this work, we study the quasilinear Schr\"{o}dinger equation
\begin{equation*} \aligned -\Delta u-\Delta(u^2)u=|u|^{p-2}u+|u|^{q-2}u+\lambda
u,\,\, x\in\R^N, \endaligned \end{equation*} under the mass constraint
\begin{equation*} \int_{\R^N}|u|^2\text{d}x=a, \end{equation*} where $N\geq2$,
$2<p<2+\frac{4}{N}<4+\frac{4}{N}<q<22^*$, $a>0$ is a given mass and $\lambda$
is a Lagrange multiplier. As a continuation of our previous work (Chen et al.,
2025, arXiv:2506.07346v1), we establish some results by means of a suitable
change of variables as follows:
  \begin{itemize} \item[{\bf(i) }] {\bf qualitative analysis of the constrained
minimization}\\ For $2<p<4+\frac{4}{N}\leq q<22^*$, we provide a detailed study
of the minimization problem under some appropriate conditions on $a>0$;
  \end{itemize}
  \begin{itemize} \item[{\bf(ii)}]{\bf existence of two radial distinct
normalized solutions}\\ For $2<p<2+\frac{4}{N}<4+\frac{4}{N}<q<22^*$, we obtain
a local minimizer under the normalized constraint;\\ For
$2<p<2+\frac{4}{N}<4+\frac{4}{N}<q\leq2^*$, we obtain a mountain pass type
normalized solution distinct from the local minimizer.
  \end{itemize} Notably, the second result {\bf (ii)} resolves the open problem
{\bf(OP1)} posed by (Chen et al., 2025, arXiv:2506.07346v1). Unlike previous
approaches that rely on constructing Palais-Smale-Pohozaev sequences by
[Jeanjean, 1997, Nonlinear Anal. {\bf 28}, 1633-1659], we obtain the mountain
pass solution employing a new method, which lean upon the monotonicity trick
developed by (Chang et al., 2024, Ann. Inst. H. Poincar\'{e} C Anal. Non
Lin\'{e}aire, {\bf 41}, 933-959).
  We emphasize that the methods developed in this work can be extended to
investigate the existence of mountain pass-type normalized solutions for other
classes of quasilinear Schr\"{o}dinger equations.

</details>


### [33] [Sketches of Nonuniformly Elliptic Schauder Theory](https://arxiv.org/abs/2509.11205)
*Cristiana De Filippis*

Main category: math.AP

TL;DR: Recent progress in extending Schauder theory to nonlinear, nonuniformly elliptic PDEs, solving a longstanding problem where classical perturbative methods failed.


<details>
  <summary>Details</summary>
Motivation: Classical Schauder theory only works for uniformly elliptic operators using perturbative techniques, but these methods break down in nonuniformly elliptic settings where homogeneous estimates fail.

Method: Developed new approaches beyond classical perturbative techniques (freezing coefficients and comparison with constant-coefficient problems) that work for nonuniformly elliptic operators.

Result: Successfully established Schauder estimates for nonlinear, nonuniformly elliptic PDEs, overcoming the limitations of classical methods.

Conclusion: The solution to this longstanding problem extends Schauder theory's applicability to a broader class of elliptic problems, including those with nonuniform ellipticity and free boundaries.

Abstract: Schauder theory is a basic tool in the study of elliptic and parabolic PDEs,
asserting that solutions inherit the regularity of the coefficients. It plays a
central role in establishing higher regularity for solutions to a broad class
of elliptic problems exhibiting ellipticity, including those involving free
boundaries. In the linear setting, Schauder theory dates back to the 1920-30s
and is now considered classical. Nonlinear extensions were developed in the
1980s. All these classical results are restricted to uniformly elliptic
operators and heavily rely on perturbative techniques - freezing the
coefficients and comparing the solution to that of a constant-coefficient
problem. However, such methods fail in the nonuniformly elliptic setting, where
homogeneous a priori estimates break down and standard iteration arguments no
longer apply. Here we give a brief survey on recent progresses including the
solution to the longstanding problem of proving the validity of Schauder
estimates in the nonlinear, nonuniformly elliptic setting.

</details>


### [34] [Construction of solutions for a critical elliptic system of Hamiltonian type](https://arxiv.org/abs/2509.11251)
*Congzheng Xuanyuan,Tingfeng Yuan,Yuxia Guo*

Main category: math.AP

TL;DR: Study of nonlinear elliptic Hamiltonian system with critical exponents in R^N. Proves existence of infinitely many solutions with arbitrarily large energy using finite reduction and Pohozaev identities.


<details>
  <summary>Details</summary>
Motivation: To investigate Hamiltonian-type elliptic systems with critical exponents and non-trivial potential, addressing the existence of multiple solutions in higher dimensions.

Method: Finite reduction argument combined with local Pohozaev identities. Assumes N≥5, specific range for (p,q), and stable critical point condition for r²V(r,y'').

Result: Proves the existence of infinitely many solutions to the nonlinear elliptic system, with solutions having energy that can be made arbitrarily large.

Conclusion: The Hamiltonian system with critical exponents and non-zero bounded potential admits an infinite family of solutions with unbounded energy under the specified conditions.

Abstract: We consider the following nonlinear elliptic system of Hamiltonian type with
critical exponents: \begin{equation*}\left\{\begin{aligned} &-\Delta u +
V(|y'|,y'') u = v^p, \;\; \text{in} \;\; \mathbb{R}^N,\\ &-\Delta v +
V(|y'|,y'') v = u^q, \;\; \text{in} \;\; \mathbb{R}^N,\\ &u, v > 0 , (u,v) \in
(\dot{W}^{2,\frac{p+1}{p}} (\mathbb{R}^N) \cap L^2(\mathbb{R}^N)) \times
(\dot{W}^{2,\frac{q+1}{q}} (\mathbb{R}^N) \cap L^2(\mathbb{R}^N))
,\end{aligned}\right. \end{equation*} where $(y', y'') \in \mathbb{R}^2 \times
\mathbb{R}^{N-2}$ and $V(|y'|, y'')\not\equiv 0$ is a bounded non-negative
function in $\mathbb{R}_+\times \mathbb{R}^{N-2}$, $p,q>1$ satisfying
$$\frac{1}{p+1}+\frac{1}{q+1}=\frac{N-2}{N}.$$ By using a finite reduction
argument and local Pohozaev identities, under the assumption that $N\geq 5$,
$(p,q)$ lies in the certain range and $r^2V(r,y'')$ has a stable critical
point, we prove that the above problem has infinitely many solutions whose
energy can be made arbitrarily large.

</details>


### [35] [Global Strong Solutions to the Three-Dimensional Axisymmetric Compressible Navier-Stokes Equations with Large Initial Data and Vacuum](https://arxiv.org/abs/2509.11260)
*Qinghao Lei*

Main category: math.AP

TL;DR: Global existence and asymptotic behavior of strong/weak solutions for 3D axisymmetric compressible Navier-Stokes equations with slip boundary conditions, allowing vanishing initial density and no size restrictions on initial data.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous mathematical foundations for compressible fluid flows in cylindrical domains excluding the axis, particularly addressing challenges with vanishing density and slip boundary conditions.

Method: Analysis of three-dimensional axisymmetric compressible Navier-Stokes equations with constant shear viscosity and bulk viscosity as a power function of density (power > 4/3), using slip boundary conditions in cylindrical domain excluding axis.

Result: Proved global existence and large time asymptotic behavior of both strong and weak solutions without requiring any restrictions on the size of initial data, even when initial density is allowed to vanish.

Conclusion: The study successfully establishes comprehensive existence and asymptotic results for compressible Navier-Stokes equations with physically relevant boundary conditions and viscosity assumptions, providing important mathematical guarantees for such fluid flow problems.

Abstract: This paper investigates the three-dimensional axisymmetric compressible
Navier-Stokes equations with slip boundary conditions in a cylindrical domain
that excludes the axis. For initial density allowed to vanish, the global
existence and large time asymptotic behavior of strong and weak solutions are
established, provided the shear viscosity is a positive constant and the bulk
one is a power function of density with the power bigger than four-thirds. It
should be noted that this result is obtained without any restrictions on the
size of initial data.

</details>


### [36] [Infinitely many solutions to a conformally invariant elliptic equation with Choquard-type nonlinearity](https://arxiv.org/abs/2509.11263)
*Mona Almutairi,Mathew Gluck*

Main category: math.AP

TL;DR: Existence of infinitely many solutions to a conformally invariant elliptic equation with nonlocal critical-power nonlinearity, overcoming compactness issues via symmetry exploitation on the sphere.


<details>
  <summary>Details</summary>
Motivation: To address the failure of compactness in Sobolev embedding for conformally invariant elliptic equations with nonlocal critical-power nonlinearity, which prevents standard existence proofs.

Method: Lift the problem to an equivalent formulation on the standard sphere to leverage its symmetries. Consider two classes of symmetries and construct unbounded sequences of solutions with prescribed symmetries for each class.

Result: For the first symmetry class (always exists), solutions are sign-changing when dimension and nonlocality parameter satisfy a specific relationship. For the second symmetry class (may not always exist), solutions are always sign-changing when the symmetry exists.

Conclusion: The symmetry-based approach successfully overcomes compactness issues and establishes existence of infinitely many sign-changing solutions for conformally invariant elliptic equations with nonlocal critical nonlinearity.

Abstract: The existence of an unbounded sequence of solutions to a conformally
invariant elliptic equation having nonlocal critical-power nonlinearity is
established. The primary obstacle to establishing existence of solutions is the
failure of compactness in the Sobolev embedding. To overcome this obstacle, the
problem under consideration is lifted to an equivalent problem on the standard
sphere so that the symmetries of the sphere can be leveraged. Two classes of
symmetries are considered and for each class of symmetries, an unbounded
sequence of solutions to the lifted problem with the prescribed symmetries is
produced. One class of symmetries always exists and the corresponding solutions
are guaranteed to be sign-changing whenever a suitable relationship between the
dimension and the nonlocality parameter holds. The other class of symmetries
need not always exist but when it exists, the corresponding solutions are
guaranteed to be sign-changing.

</details>


### [37] [Liouville type theorem for a class of quasilinear p-Laplace type equations in the half space](https://arxiv.org/abs/2509.11283)
*Bao Yu,Yang Zhou*

Main category: math.AP

TL;DR: Liouville-type theorem for quasilinear p-Laplace equations with conormal boundary conditions in half space using vector field methods


<details>
  <summary>Details</summary>
Motivation: To establish nonexistence results for subcritical p-Laplace type equations that arise as Euler-Lagrange equations of Sobolev trace inequalities in half space

Method: Vector field method applied to quasilinear p-Laplace type equations with conormal boundary conditions

Result: Obtained a Liouville-type theorem (nonexistence result) for this class of equations in the half space setting

Conclusion: The vector field method successfully provides Liouville-type results for subcritical p-Laplace equations with conormal boundary conditions, extending previous work on Sobolev trace inequality Euler-Lagrange equations

Abstract: We use the method of vector fields to obtain a Liouville-type theorem for a
class of quasilinear p-Laplace type equations with conormal boundary condition
in the half space. These p-Laplace type equations are the subcritical case of
the Euler-Lagrange equation of the Sobolev trace inequality in the half space.

</details>


### [38] [An inductive approach to stochastic estimates for the $\varphi^{4}_2$-equation with correlated coefficient field](https://arxiv.org/abs/2509.11309)
*Nicolas Clozeau*

Main category: math.AP

TL;DR: Inductive approach for stochastic estimates of φ⁴₂-equation with correlated coefficient field and noise


<details>
  <summary>Details</summary>
Motivation: To develop analytical tools for the φ⁴₂-equation when the coefficient field is correlated with the driving noise, which presents mathematical challenges in stochastic PDE analysis

Method: Uses infinite-dimensional Gaussian integration by parts with respect to Wick products of Gaussian random variables, specifically mollifications of space-time white noise

Result: Develops an inductive methodology for obtaining stochastic estimates in this correlated setting

Conclusion: The approach provides a systematic framework for analyzing the φ⁴₂-equation with correlated coefficient and noise fields using Gaussian integration techniques

Abstract: We develop an inductive approach to obtaining stochastic estimates for the
$\varphi^{4}_2$-equation when the coefficient field is correlated with the
driving noise. Our method is based on (infinite-dimensional) Gaussian
integration by parts with respect to Wick products of Gaussian random variables
(more precisely, mollifications of space-time white noise)

</details>


### [39] [On a class of thin obstacle-type problems for the bi-Laplacian operator](https://arxiv.org/abs/2509.11372)
*Donatella Danielli,Giovanni Gravina*

Main category: math.AP

TL;DR: Analysis of regularity and free boundary structure for fourth-order elliptic problems with Neumann boundary conditions, including blow-up analysis and stratification results.


<details>
  <summary>Details</summary>
Motivation: To study solutions and free boundary properties of fourth-order elliptic problems that arise from higher-order fractional Laplacian extensions and encompass two-phase boundary obstacle problems.

Method: Establish local regularity properties, derive Almgren- and Monneau-type monotonicity formulas, perform blow-up analysis, and prove stratification results for the free boundary.

Result: Successfully established regularity properties and derived monotonicity formulas that enable comprehensive blow-up analysis and stratification of the free boundary structure.

Conclusion: The paper provides a complete analysis framework for fourth-order elliptic problems with Neumann conditions, including regularity results and detailed free boundary characterization through monotonicity formulas and blow-up techniques.

Abstract: This paper investigates the regularity of solutions and structural properties
of the free boundary for a class of fourth-order elliptic problems with
Neumann-type boundary conditions. The singular and degenerate elliptic
operators studied naturally emerge from the extension procedure for
higher-order fractional powers of the Laplacian, while the choice of
non-linearity considered encompasses two-phase boundary obstacle problems as a
special case. After establishing local regularity properties of solutions,
Almgren- and Monneau-type monotonicity formulas are derived and utilized to
carry out a blow-up analysis and prove a stratification result for the free
boundary.

</details>


### [40] [Solvability of some integro-differential equations with the bi-Laplacian and transport](https://arxiv.org/abs/2509.11515)
*Vitali Vougalter*

Main category: math.AP

TL;DR: Existence of solutions for integro-differential problems with drift terms and bi-Laplacian operators using fixed point techniques and Fredholm theory.


<details>
  <summary>Details</summary>
Motivation: To establish solution existence for fourth-order integro-differential problems on real line and periodic domains, addressing challenges with non-Fredholm operators.

Method: Fixed point technique applied to elliptic equations containing fourth-order differential operators, analyzing both Fredholm and non-Fredholm cases. Convergence in L^1 of integral kernels is used.

Result: Under reasonable technical conditions, convergence in L^1 of integral kernels yields existence and convergence in H^4 of solutions.

Conclusion: The fixed point approach successfully establishes solution existence and convergence for fourth-order integro-differential problems with drift terms in H^4 spaces.

Abstract: We demonstrate the existence in the sense of sequences of solutions for some
integro-differential type problems involving the drift term and the square of
the Laplace operator, on the whole real line or on a finite interval with
periodic boundary conditions in the corresponding H^4 spaces. Our argument is
based on the fixed point technique when the elliptic equations contain fourth
order differential operators with and without the Fredholm property. It is
established that, under the reasonable technical conditions, the convergence in
L^1 of the integral kernels yields the existence and convergence in H^4 of the
solutions.

</details>


### [41] [On the logarithmic correction of transition fronts in shifting environments](https://arxiv.org/abs/2509.11521)
*King-Yeung Lam,Chang-Hong Wu*

Main category: math.AP

TL;DR: Analysis of Fisher-KPP equation front location and traveling wave convergence in expanding domains and discontinuous environments


<details>
  <summary>Details</summary>
Motivation: To understand how spreading fronts behave and converge to traveling wave profiles in two challenging scenarios: unbounded domains with expanding boundaries and environments with shifting jump discontinuities

Method: Extends Bramson's 1983 seminal work, employs gluing technique to construct super/subsolutions for rigorous analysis

Result: Developed analytical framework for studying front propagation in complex domain geometries and discontinuous media

Conclusion: The approach successfully addresses front location and convergence problems in non-standard Fisher-KPP settings through careful construction of comparison functions

Abstract: In this paper, we investigate the location of the spreading front and
convergence to traveling wave profile of solutions to the Fisher-KPP equation
in the following two cases: (i) in unbounded domains with an expanding
boundary; (ii) on the real line where the environment function has a shifting
jump discontinuity. Our approach is based on extending ideas in Bramson's
seminal work in 1983, and applying gluing technique to construct
super/subsolutions.

</details>


### [42] [Random data Cauchy theory for fully nonlocal telegraph equations](https://arxiv.org/abs/2509.11564)
*Xi Huang,Li Peng,Juan Carlos Pozo,Yong Zhou*

Main category: math.AP

TL;DR: Analysis of random Cauchy problem for nonlocal telegraph equation with novel solution operators and probabilistic randomization methods for initial data.


<details>
  <summary>Details</summary>
Motivation: To characterize high-frequency signal transmission in small-scale systems using fully nonlocal telegraph equations and establish probabilistic estimates for solutions.

Method: Established new completely positive kernel, derived novel solution operators related to fractional Laplacian operators, used probabilistic randomization methods for initial data analysis.

Result: Obtained mixed-norm estimates for solution operators, established average effects, local existence and uniqueness for initial data in L^2(Ω,H^{s,p}(R^3)), and revealed critical temporal regularity phenomenon.

Conclusion: The study provides new mathematical framework for analyzing nonlocal telegraph equations with probabilistic methods, revealing important regularity properties for signal transmission applications.

Abstract: We consider the random Cauchy problem for the fully nonlocal telegraph
equation of power type with the general $(\mathcal{PC}^{\ast})$ type kernel
$(a,b)$. This equation can effectively characterize high-frequency signal
transmission in small-scale systems. We establish a new completely positive
kernel induced by $b$ (see Appendix \refeq{app b}) and derive two novel
solution operators by using the relaxation functions associated with the new
kernel,which are closely related to the operators
$\cos(\theta(-\Delta)^{\frac{\beta}{4}} )$ and $(-\Delta)^{-\frac{\beta}{4}
}\sin(\theta(-\Delta)^{\frac{\beta}{4}} )$ for $\beta\in(1,2]$. These operators
enable, for the first time, the derivation of mixed-norm $L_t^qL_x^{p'}$
estimates for the novel solution operators. Next, utilizing probabilistic
randomization methods, we establish the average effects, the local existence
and uniqueness for a large set of initial data $u^\omega \in L^{2}(\Omega,
H^{s,p}(\mathbb R^3))$ ($p\in (1,2)$) while also obtaining probabilistic
estimates for local existence under randomized initial conditions. The results
reveal a critical phenomenon in the temporal regularity of the solution
regarding the regularity index $s$ of the initial data $u^\omega$.

</details>


### [43] [A Survey on the Div-Curl Lemma and Some Extensions to Fractional Sobolev Spaces](https://arxiv.org/abs/2509.11734)
*Maicol Caponi*

Main category: math.AP

TL;DR: Survey on Div-Curl lemma generalizations to fractional setting using Riesz fractional operators


<details>
  <summary>Details</summary>
Motivation: To extend classical Div-Curl lemma results to fractional calculus framework using recently developed Riesz fractional gradient and divergence operators

Method: Review classical Div-Curl lemma formulation and proof, then present generalizations within fractional setting using Riesz fractional operators from Shieh-Spector (2015) and Comi-Stefani (2019)

Result: Provides systematic overview of how classical Div-Curl lemma can be extended to fractional calculus context

Conclusion: Fractional Div-Curl lemma generalizations are feasible and represent natural extension of classical results using modern fractional operator frameworks

Abstract: This survey is a chapter of a forthcoming book. This chapter recalls the
classical formulation of the Div-Curl lemma along with its proof, and presents
some possible generalizations in the fractional setting, within the framework
of the Riesz fractional gradient and divergence introduced by Shieh and Spector
(2015) and further developed by Comi and Stefani (2019).

</details>


### [44] [A survey on anisotropic integral representation results](https://arxiv.org/abs/2509.11760)
*Simone Verzellesi*

Main category: math.AP

TL;DR: Review of integral representation results for local functionals with Lipschitz continuous anisotropies


<details>
  <summary>Details</summary>
Motivation: To summarize recent developments in the mathematical theory of integral representations for local functionals, particularly those involving Lipschitz continuous anisotropic properties

Method: The paper conducts a review and analysis of existing mathematical results and theorems related to integral representation properties of local functionals driven by Lipschitz continuous anisotropies

Result: The review consolidates and presents recent findings on the representation properties of these functionals, though specific results are not detailed in the abstract

Conclusion: This note provides a comprehensive overview of current results in this specialized area of functional analysis and representation theory

Abstract: In this note we review some recent results concerning integral representation
properties of local functionals driven by Lipschitz continuous anisotropies.

</details>


### [45] [A nonlinear model for long-range segregation](https://arxiv.org/abs/2509.11912)
*Howen Chuah,Stefania Patrizi,Monica Torres*

Main category: math.AP

TL;DR: Analysis of a system of fully nonlinear elliptic equations modeling population segregation, showing convergence to a free boundary problem with finite perimeter and semi-convexity properties.


<details>
  <summary>Details</summary>
Motivation: To study long-range segregation of populations modeled by nonlinear elliptic equations with small parameter ε, extending previous linear case work by Caffarelli et al. in population dynamics.

Method: Investigate a system of fully nonlinear elliptic equations governed by the negative Pucci operator, establish existence of solutions, and prove convergence as ε→0⁺ to a free boundary problem.

Result: Proved convergence to a free boundary problem where populations remain segregated at positive distance, with limiting functions having supports that are sets of finite perimeter and satisfy semi-convexity property.

Conclusion: The nonlinear elliptic system successfully models population segregation, converging to a well-defined free boundary problem with mathematically rigorous geometric properties in the limit.

Abstract: We study a system of fully nonlinear elliptic equations, depending on a small
parameter $\eps$, that models long-range segregation of populations. The
diffusion is governed by the negative Pucci operator. In the linear case, this
system was previously investigated by Caffarelli, the second author, and
Quitalo in \cite{CL2} as a model in population dynamics. We establish the
existence of solutions and prove convergence as $\eps\to0^+$ to a free boundary
problem in which populations remain segregated at a positive distance. In
addition, we show that the supports of the limiting functions are sets of
finite perimeter and satisfy a semi-convexity property.

</details>


### [46] [A thermodynamically consistent model for bulk-surface viscous fluid mixtures: Model derivation and mathematical analysis](https://arxiv.org/abs/2509.11925)
*Patrik Knopf,Jonas Stange*

Main category: math.AP

TL;DR: A new diffuse interface model for incompressible viscous fluid mixtures with bulk-surface interaction, combining Navier-Stokes-Cahn-Hilliard in bulk and surface with mathematical analysis of weak solutions.


<details>
  <summary>Details</summary>
Motivation: Biological applications like the fluid mosaic model where cell surfaces are treated as thin viscous fluid layers, requiring coupled bulk-surface fluid interaction modeling.

Method: Derived through local mass balance laws, energy dissipation laws, and Lagrange multiplier approach. Mathematical analysis via semi-Galerkin discretization using eigenfunctions of novel bulk-surface Stokes operator.

Result: Development of a coupled bulk-surface Navier-Stokes-Cahn-Hilliard system and proof of global weak solution existence.

Conclusion: The model provides a mathematically rigorous framework for studying fluid mixtures with bulk-surface interactions, particularly relevant for biological membrane applications.

Abstract: We derive and analyze a new diffuse interface model for incompressible,
viscous fluid mixtures with bulk-surface interaction. Our system consists of a
Navier--Stokes--Cahn--Hilliard model in the bulk that is coupled to a surface
Navier--Stokes--Cahn--Hilliard model on the boundary. Compared with previous
models, the inclusion of an additional surface Navier--Stokes equation is
motivated, for example, by biological applications such as the seminal
\textit{fluid mosaic model} (Singer \& Nicolson, \textit{Science}, 1972) in
which the surface of biological cells is interpreted as a thin layer of viscous
fluids. We derive our new model by means of local mass balance laws, local
energy dissipation laws, and the Lagrange multiplier approach. Moreover, we
prove the existence of global weak solutions via a semi-Galerkin
discretization. The core part of the mathematical analysis is the study of a
novel bulk-surface Stokes system and its corresponding bulk-surface Stokes
operator. Its eigenfunctions are used as the Galerkin basis to discretize the
bulk-surface Navier--Stokes subsystem.

</details>


### [47] [Entire Large Solutions for Competitive Semilinear Elliptic Systems with General Nonlinearities Satisfying Keller--Osserman Conditions](https://arxiv.org/abs/2509.11933)
*Dragos-Patru Covei*

Main category: math.AP

TL;DR: Extends Lair's theorem on positive entire large solutions from power-type to general nonlinearities satisfying Keller-Osserman growth conditions


<details>
  <summary>Details</summary>
Motivation: To generalize the existence theory for competitive semilinear elliptic systems beyond power-type nonlinearities to a broader class of functions

Method: Uses monotone iteration to construct solutions, reduces to scalar inequality, applies Keller-Osserman transform, and employs two-step radial integration with general monotone envelope functions

Result: Develops unified criterion for existence of large solutions using Keller-Osserman integral, covering both critical and supercritical growth regimes

Conclusion: Successfully extends the theoretical framework to handle general nonlinearities while maintaining the conceptual proof structure of the original power-type case

Abstract: We generalize a theorem of Lair concerning the existence of positive entire
large solutions to competitive semilinear elliptic systems. While Lair's
original result \cite{Lair2025} was established for power-type nonlinearities,
our work extends the theory to a broad class of general nonlinearities
satisfying a Keller--Osserman-type growth condition. The proof follows the same
conceptual framework monotone iteration to construct global positive solutions,
reduction to a scalar inequality for the sum of the components, application of
a Keller--Osserman transform, and a two-step radial integration argument but
replaces the explicit power-law growth with a general monotone envelope
function. This approach yields a unified and verifiable criterion for the
existence of large solutions in terms of the Keller--Osserman integral, thereby
encompassing both critical and supercritical growth regimes within a single
analytical setting.

</details>


### [48] [The Polya-Szego principle in the fractional setting: a glimpse on nonlocal functional inequalities](https://arxiv.org/abs/2509.12023)
*Alessandro Carbotti*

Main category: math.AP

TL;DR: Survey on fractional Polya Szego principle and its applications in nonlocal functional inequalities, showing symmetrization methods work in fractional setting to yield sharp isoperimetric inequalities.


<details>
  <summary>Details</summary>
Motivation: To extend symmetrization methods and functional inequalities from classical to fractional settings, establishing sharp nonlocal isoperimetric inequalities.

Method: Uses fractional Polya Szego principle and symmetrization techniques adapted to fractional calculus framework.

Result: Demonstrates that symmetrization methods work effectively in fractional setting, producing sharp isoperimetric type inequalities.

Conclusion: Fractional Polya Szego principle provides powerful tool for nonlocal functional inequalities, with extensions possible to anisotropic and Gaussian settings including stability analysis.

Abstract: In this survey we present the fractional Polya Szego principle and its main
consequences in the study of nonlocal functional inequalities. In particular,
we show how symmetrization methods work also in the fractional setting and
yield sharp results such as isoperimetric type inequalities. Further
developments including stability issues and generalizations in the anisotropic
and the Gaussian setting are also discussed.

</details>


### [49] [Multidimensional Scalar Conservation Laws with Non-Aligned Discontinuous Flux and Singularity of Solutions](https://arxiv.org/abs/2509.12099)
*Ajlan Zajmović*

Main category: math.AP

TL;DR: Vanishing viscosity approximation for multidimensional scalar conservation laws with discontinuous non-aligned flux and zero initial data produces singular measures along discontinuity surfaces


<details>
  <summary>Details</summary>
Motivation: To understand the limiting behavior of solutions to vanishing viscosity approximations for conservation laws with discontinuous flux functions and zero initial conditions

Method: Analysis of vanishing viscosity approximation for multidimensional scalar conservation laws with discontinuous non-aligned flux and zero initial data

Result: The family of solutions generates a singular measure supported along the discontinuity surface in the limit

Conclusion: The vanishing viscosity limit produces concentration phenomena along discontinuity surfaces rather than classical solutions

Abstract: We prove that the family of solutions to vanishing viscosity approximation
for multidimensional scalar conservation laws with discontinuous non-aligned
flux and zero initial data in the limit generates a singular measure supported
along the discontinuity surface.

</details>


### [50] [Convergence rates for the vanishing viscosity approximation of fully nonlinear, non-convex, second-order Hamilton-Jacobi equations](https://arxiv.org/abs/2509.12144)
*Alekos Cecchin,Alessandro Goffi*

Main category: math.AP

TL;DR: New convergence rates for vanishing viscosity approximation of degenerate Hamilton-Jacobi equations with non-convex/non-concave structure, using sup/inf-convolutions and comparison principle.


<details>
  <summary>Details</summary>
Motivation: To provide quantitative estimates for vanishing viscosity approximations of time-dependent degenerate Hamilton-Jacobi equations that lack convexity/concavity properties, which are common in many applications but challenging to analyze.

Method: Uses regularizing properties of sup/inf-convolutions for viscosity solutions and the comparison principle. The approach approximates PDE with fully nonlinear degenerate elliptic operator and establishes convergence rates without requiring differentiability of solutions or Hamiltonian.

Result: Establishes convergence rate of order ε^min{η/2, (β+γ(α-1))/(β+γ(α-1)+2-α)} under assumptions of u∈C^α_x, u_0∈C^η, H∈C^β_x with power growth γ in gradient. Method also provides rates for stationary equations and transport equations with Hölder coefficients.

Conclusion: The novel approach using sup/inf-convolutions successfully provides quantitative convergence rates for vanishing viscosity approximations of degenerate Hamilton-Jacobi equations without convexity assumptions, with explicit constants and no differentiability requirements.

Abstract: We obtain new quantitative estimates of the vanishing viscosity approximation
for time-dependent, degenerate, Hamilton-Jacobi equations that are neither
concave nor convex in the gradient and Hessian entries of the form $\partial_t
u+H(x,t,Du,D^2u)=0$ in the whole space. We approximate the PDE with a fully
nonlinear, possibly degenerate, elliptic operator $\varepsilon F(x,t,D^2u)$.
Assuming that $u\in C^\alpha_x$, $u_0\in C^\eta$, $H\in C^\beta_x$ and having
power growth $\gamma$ in the gradient entry, we establish a convergence rate of
order
$\varepsilon^{\min\left\{\frac{\eta}{2},\frac{\beta+\gamma(\alpha-1)}{\beta+\gamma(\alpha-1)+2-\alpha}\right\}}$.
Our novel approach exploits the regularizing properties of sup/inf-convolutions
for viscosity solutions and the comparison principle. We also obtain explicit
constants and do not assume differentiability properties neither on solutions
nor on $H$. The same method provides new convergence rates for the vanishing
viscosity approximation of the stationary counterpart of the equation and for
transport equations with H\"older coefficients.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [51] [Calibrating a Finite-strain Phase-field Model of Fracture for Bonded Granular Materials with Uncertainty Quantification](https://arxiv.org/abs/2509.10484)
*Abigail C. Schmid,Erik Jensen,Fabio Di Gioacchino,Pooyan B. Javadzadeh,Nate E. Peterson,C. Gus Becker,Hongbing Lu,Fatemeh Pourahmadian,Amy J. Clarke,Alireza Doostan,Richard A. Regueiro*

Main category: physics.comp-ph

TL;DR: A framework for calibrating material models with uncertainty quantification was developed and tested on mock high explosives, showing that manufacturing temperature affects elastic response while initial grain configuration has negligible impact.


<details>
  <summary>Details</summary>
Motivation: To develop a generalizable methodology for calibrating material models of bonded granular materials (mock high explosives) with quantified uncertainty against experimental data.

Method: Used hundreds of high-fidelity direct numerical simulations with GPU-enabled finite element software (Ratel) to calibrate a finite-strain phase-field fracture model against experimental data. Studied two manufacturing temperatures and three initial granular configurations.

Result: Manufacturing temperature influenced elastic response (higher temperatures yielded stiffer response), while initial grain configuration had negligible impact on overall behavior (though local damage accumulation might be affected).

Conclusion: The calibration framework successfully created well-calibrated models, demonstrating its usefulness as an engineering and scientific tool for material model calibration with uncertainty quantification.

Abstract: To study the mechanical behavior of mock high explosives, an experimental and
simulation program was developed to calibrate, with quantified uncertainty, a
material model of the bonded granular material Idoxuridine and nitroplasticized
Estane-5703. This paper reports on the efficacy of such a framework as a
generalizable methodology for calibrating material models against experimental
data with uncertainty quantification. Additionally, this paper studies the
effect of two manufacturing temperatures and three initial granular
configurations on the unconfined compressive behavior of the resulting bonded
granular materials. In each of these cases, the same calibration framework was
used; in that, hundreds of high-fidelity direct numerical simulations using a
new, GPU-enabled, high-performance finite element method software, Ratel, were
run to calibrate a finite-strain phase-field fracture model against
experimental data. It was found that manufacturing temperature influenced the
elastic response of the mock high explosives, with higher temperatures yielding
a stiffer response. By contrast, it was found that the initial configuration of
the grains had a negligible impact on the overall behavior of the mock high
explosives, though it remains possible that local damage accumulation within
the specimens could be altered by the initial configurations. Overall, the
calibration framework was successful at creating well-calibrated models,
showing its usefulness as an engineering and scientific tool.

</details>


### [52] [A Phase Field Formulation of Frictional Sliding Contact for 3D Fully Eulerian Fluid Structure Interactions](https://arxiv.org/abs/2509.11611)
*Biswajeet Rath,Rajeev K. Jaiman*

Main category: physics.comp-ph

TL;DR: A phase field-based formulation for 3D dynamic frictional contact in hydrodynamic environments, integrating normal and tangential forces within a single momentum equation using diffuse interface overlap detection and Coulomb friction.


<details>
  <summary>Details</summary>
Motivation: Accurate modeling of frictional sliding contact in hydrodynamic environments requires integrated numerical framework capable of handling large motions, multiphase interactions, and nonlinear contact responses.

Method: Phase field-based formulation using diffuse interface overlap for contact detection, volumetric body force for normal response, Coulomb friction model for tangential response, with friction forces derived from phase-averaged relative velocities projected onto tangent planes.

Result: Excellent agreement with Hertzian contact analytical solution (<3% error), accurate displacement profiles in sliding block benchmark across multiple friction coefficients, stable force predictions under finite deformation in ironing problem, and demonstrated robustness in ship-ice interaction scenarios.

Conclusion: The proposed unified treatment successfully computes both normal and frictional forces within a single momentum balance equation, showing high accuracy, stability, and scalability for complex hydrodynamic frictional contact problems.

Abstract: Frictional sliding contact in hydrodynamic environments can be found in a
range of engineering applications. Accurate modeling requires an integrated
numerical framework capable of resolving large relative motions, multiphase
interactions, and nonlinear contact responses. Building on our previously
developed fully Eulerian fluid structure formulation, we introduce a phase
field based formulation for dynamic frictional contact in 3D. Contact detection
is achieved via the overlap of diffuse interfaces of colliding solids. The
normal contact response is defined as a volumetric body force proportional to
the overlap parameter, while the tangential response is computed using the
Coulomb friction model. The direction of the friction forces are derived by
projecting phase-averaged relative velocities onto the local tangent plane of
colliding bodies. This proposed unified treatment enables the computation of
both normal and frictional forces within a single momentum balance equation,
avoiding separate velocity fields for individual solids. We present several
test cases with increasing complexity to verify and demonstrate our proposed
frictional contact model. Verification against the Hertzian contact problem
shows excellent agreement with the analytical solution, with errors below $3\%$
in the traction profile. In the sliding block benchmark, the computed
displacement profiles closely follow the analytical solution for point-mass
systems across multiple friction coefficients. The ironing problem demonstrates
stable force predictions under finite deformation, with normal and tangential
forces matching kinetic friction laws. The robustness and scalability of the
proposed formulation are further demonstrated through a representative ship ice
interaction scenario with free surface and frictional sliding effects.

</details>


### [53] [Automated training of neural-network interatomic potentials](https://arxiv.org/abs/2509.11703)
*Davide Bidoggia,Nataliia Manko,Maria Peressi,Antimo Marrazzo*

Main category: physics.comp-ph

TL;DR: Automated workflow for creating neural-network interatomic potentials (NNIPs) that simplifies the complex process of developing accurate atomistic simulation models with minimal human expertise required.


<details>
  <summary>Details</summary>
Motivation: NNIPs enable near ab initio accuracy molecular dynamics simulations but require specialized expertise in both machine learning and electronic-structure calculations, making them difficult to develop for non-experts.

Method: Integrates density-functional theory, data augmentation, classical molecular dynamics, and active learning with on-the-fly calibration of committee disagreement against true errors. Uses electronic-structure descriptors and dimensionality reduction for efficient active learning.

Result: Validated on automated training of NNIP for diverse carbon allotropes, achieving state-of-the-art accuracy and data efficiency while minimizing false positives/negatives in relabeling decisions.

Conclusion: The platform democratizes NNIP development, enabling high-precision simulations with minimal human intervention through an automated, open-source, user-friendly workflow.

Abstract: Neural-network interatomic potentials (NNIPs) have transformed atomistic
simulations, by enabling molecular dynamics simulations with near ab initio
accuracy at reduced computational costs and improved scalability. Despite these
advances, crafting NNIPs remains complex, demanding specialized expertise in
both machine learning and electronic-structure calculations. Here, we introduce
an automated, open-source, and user-friendly workflow that streamlines the
creation of accurate NNIPs. Our approach integrates density-functional theory,
data augmentation strategies and classical molecular dynamics to systematically
explore the potential energy landscape. Our active-learning strategy leverages
on-the-fly calibration of committee disagreement against true errors to ensure
reliable uncertainty estimates. We use electronic-structure descriptors and
dimensionality reduction to analyze the efficiency of our active learning
strategy, which is shown to minimize both false positives and false negatives
when deciding what to relabel with ab initio calculations. The method is
validated on the fully automated training of a NNIP for a diverse set of carbon
allotropes, reaching state-of-the-art accuracy and data efficiency. This
platform democratizes NNIP development, empowering users to achieve
high-precision simulations with minimal human intervention.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [54] [Temperature and mean axial momentum vs. laser intensity of electrons released from O$_2$ by an 800 nm ultrashort pulsed laser](https://arxiv.org/abs/2509.10986)
*Edward L. Ruden*

Main category: physics.plasm-ph

TL;DR: Semi-empirical model for electron temperature and momentum from O2 ionization by 800nm laser pulses, using rescatter effects to provide initial conditions for fluid simulations.


<details>
  <summary>Details</summary>
Motivation: To provide accurate initial conditions for electrodynamic fluid simulations of laser-matter interactions by modeling electron thermalization and momentum after ionization.

Method: Modified theoretical kinetic energy spectra with two adjustable parameters representing electron rescatter effects during subsequent optical cycles, using classical kinematics to estimate mean momentum.

Result: Developed a model that estimates thermalized temperature T and mean momentum <p_fz> of electrons released from O2 vs. laser intensity I0.

Conclusion: The semi-empirical approach incorporating rescatter effects provides practical initial conditions for electrodynamic simulations of laser-induced ionization processes.

Abstract: A semi-empirical model is presented for the thermalized temperature $T$ and
mean momentum in the direction of laser propagation <$p_{fz}$> of electrons
released from O$_{2}$ after the passage of a focused $800$ nm ultrashort pulsed
laser pulse vs. peak laser intensity $I_{0}$ to provide initial conditions for
electrodynamic fluid simulations. For this, theoretical kinetic energy spectra
in different directions are modified with two adjustable parameters
representing the effects of electron rescatter off its parent ion during the
optical cycle subsequent to ionization. The classical kinematics of rescatter,
in conjunction with the spectral fits, is used to estimate <$p_{fz}$>.

</details>


### [55] [Energetic spectra from semi-implicit particle-in-cell simulations of magnetic reconnection](https://arxiv.org/abs/2509.11100)
*K. M. Schoeffler,F. Bacchini,K. Kormann,B. Eichmann,M. E. Innocenti*

Main category: physics.plasm-ph

TL;DR: RelSIM method enables efficient relativistic magnetic reconnection simulations with reduced resolution while maintaining accuracy in particle acceleration modeling


<details>
  <summary>Details</summary>
Motivation: Astrophysical observations show magnetic reconnection in relativistic plasmas is crucial for energetic particle acceleration, requiring numerical methods that bridge large scales and kinetic physics without computational limitations

Method: Relativistic semi-implicit method (RelSIM) with particle-in-cell simulations in 2D and 3D, starting from relativistic Harris equilibrium without guide field, to study tearing instability and nonlinear reconnection development

Result: Simulations produce energetic power-law spectra with cutoff energies consistent with explicit methods, but achieved with significantly reduced computational resolution

Conclusion: RelSIM provides computational advantages over explicit methods by enabling reduced resolution while avoiding numerical instabilities, helping bridge macroscopic and kinetic scales in relativistic plasma simulations

Abstract: Astrophysical observations suggest that magnetic reconnection in relativistic
plasmas plays an important role in the acceleration of energetic particles.
Modeling this accurately requires numerical schemes capable of addressing large
scales and realistic magnetic field configurations without sacrificing the
kinetic description needed to model particle acceleration self-consistently. We
demonstrate the computational advantage of the relativistic semi-implicit
method (RelSIM), which allows for reduced resolution while avoiding the
numerical instabilities typically affecting standard explicit methods, helping
to bridge the gap between macroscopic and kinetic scales. Two- and
three-dimensional semi-implicit particle-in-cell simulations explore the linear
tearing instability and the nonlinear development of reconnection and
subsequent particle acceleration starting from a relativistic Harris
equilibrium with no guide field. The simulations show that particle
acceleration in the context of magnetic reconnection leads to energetic
power-law spectra with cutoff energies, consistent with previous work done
using explicit methods, but are obtained with a considerably reduced
resolution.

</details>


### [56] [Dynamical generation of macroscale magnetic fields and fast flows in a four-component astrophysical plasma](https://arxiv.org/abs/2509.11122)
*Usman Shazad,M. Iqbal*

Main category: physics.plasm-ph

TL;DR: Study explores magnetic field generation in four-component dusty plasma through dynamo mechanisms depending on turbulent energy type


<details>
  <summary>Details</summary>
Motivation: To understand how macroscale magnetic fields and flows can be generated or amplified in complex astrophysical dusty plasmas with multiple particle components

Method: Analysis of four-component plasma system (electrons, positrons, positive ions, static dust particles) examining dynamo mechanisms under different turbulent energy conditions

Result: Straight dynamo mechanism works when turbulent energy is kinetic; unified reverse-dynamo/dynamo mechanism works when turbulent energy is magnetic. Alfvén Mach numbers affected by plasma densities and helicities

Conclusion: Different dynamo mechanisms operate depending on turbulent energy type, with implications for magnetic field generation in astrophysical environments

Abstract: We explore the possibility of the generation or amplification of macroscale
magnetic fields and flows in a four-component astrophysical dusty plasma
composed of mobile massless electrons and positrons, inertial positive ions and
negatively charged static dust particles. The investigation demonstrates that
when microscopic turbulent ambient plasma energy is predominantly kinetic, a
straight dynamo (DY) mechanism is feasible. Conversely, a unified
reverse-dynamo/dynamo (RDY/DY) mechanism is possible when the microscopic
turbulent ambient plasma energy is primarily magnetic. Additionally, the
evolution of Alfv\'{e}n Mach numbers at the macro- and microscale are
significantly affected by plasma species densities and invariant helicities.
The potential implications of the present study for astrophysical settings are
also highlighted.

</details>


### [57] [On the Poisson brackets of hybrid plasma models with kinetic ions and massless electrons](https://arxiv.org/abs/2509.11160)
*Yingzhe Li,Philip J. Morrison,Stefan Possanner,Eric Sonnendrücker*

Main category: physics.plasm-ph

TL;DR: Analysis of Jacobi identity conditions for anti-symmetric brackets in hybrid plasma models with kinetic ions and massless electrons, covering both vector-potential-based and magnetic-field-based formulations.


<details>
  <summary>Details</summary>
Motivation: To establish precise mathematical conditions under which recently introduced anti-symmetric brackets for hybrid plasma models satisfy the Jacobi identity, ensuring proper Hamiltonian structure for these physical systems.

Method: Mathematical analysis of bracket formulations, establishing conditions for Jacobi identity satisfaction, and providing direct proofs for the magnetic-field-based formulation under divergence-free magnetic field conditions.

Result: Found that vector-potential-based formulations satisfy Jacobi identity for all physically relevant functionals, and magnetic-field-based formulations constitute Poisson brackets under divergence-free magnetic field conditions with proven Jacobi identity.

Conclusion: The results establish rigorous mathematical foundations for Hamiltonian structure in hybrid plasma models and can be extended to more complex models including electron entropy and general hybrid kinetic-fluid systems.

Abstract: We investigate the conditions under which the Jacobi identity holds for a
class of recently introduced anti-symmetric brackets for the hybrid plasma
models with kinetic ions and massless electrons. In particular, we establish
the precise conditions under which the brackets for the vector-potential-based
formulations satisfy the Jacobi identity, and demonstrate that these conditions
are fulfilled by all physically relevant functionals. Moreover, for the
magnetic-field-based formulation, we show that the corresponding anti-symmetric
bracket constitutes a Poisson bracket under the divergence-free condition of
the magnetic field, and we provide a direct proof of the Jacobi identity. These
results are further extended to models incorporating electron entropy as well
as more general hybrid kinetic-fluid models.

</details>


### [58] [Glow discharge induced reactions in mixtures of ozone and chlorodifluoromethane with atmospheric gases](https://arxiv.org/abs/2509.11302)
*Alexander Dorn,Haydar Mutaf,Recep Orhan,Wania Wolff,Thomas Pfeifer,Haji Ahmedov*

Main category: physics.plasm-ph

TL;DR: Lab study shows cosmic radiation simulation via glow discharge causes significant ozone depletion, dramatically enhanced by halocarbon R22 presence


<details>
  <summary>Details</summary>
Motivation: Investigate poorly understood effects of cosmic radiation on stratospheric chemistry and ozone depletion, particularly when combined with halocarbons

Method: Used lab apparatus to simulate stratospheric conditions with glow discharge representing cosmic radiation secondary particles. Tested ozone-halocarbon mixtures in atmospheric gases at low pressure, analyzed gas composition with mass spectrometry

Result: Glow discharge caused 4x ozone depletion without R22, but dramatically enhanced to 100x depletion (two orders of magnitude) when R22 was present

Conclusion: Cosmic radiation can significantly impact ozone chemistry, with halocarbons like R22 dramatically amplifying ozone depletion effects - this should be considered in stratospheric models

Abstract: The influence of extraterrestrial particles like cosmic radiation (CR) on the
chemistry and ozone density in the Earth stratosphere is not well investigated
and normally neglected in stratospheric chemistry models. Here we present the
commissioning of a lab-based apparatus which aims at simulating conditions in
the stratosphere in order to get better insight into the reactions induced by
the secondary-particle showers from high-energetic CR which can reach low
altitudes. Admixtures of ozone and the halocarbon CHClF2 (R22,
chlorodifluoromethane) to atmospheric gases (N2, O2, Ar) were exposed to a glow
discharge in the total pressure regime of a few hPa. According to the mass
spectrometric analysis of the gas composition the discharge initiates
significant ozone depletion by a factor four in the absence of R22. This
depletion is strongly enhanced to two orders of magnitude in the presence of
R22. The possible underlying reactions are discussed.

</details>


### [59] [The impact of kinetic and global effects on ballooning 2nd stable pedestals of conventional and high aspect ratio tokamaks](https://arxiv.org/abs/2509.11484)
*M. S. Anastopoulos Tzanis,M. Yang,A. Kleiner,J. F. Parisi,G. M. Staebler,P. B. Snyder*

Main category: physics.plasm-ph

TL;DR: Development of improved EPED model using Gyro-Fluid System (GFS) code for kinetic ballooning mode stability and ELITE for high-n global ballooning modes, showing better agreement with DIII-D pedestal data compared to previous EPED1 model.


<details>
  <summary>Details</summary>
Motivation: The EPED model had success but used ideal ballooning mode approximation for KBM constraint, which shows quantitative differences from gyro-kinetic stability at low aspect ratio. Need for more accurate kinetic ballooning stability calculations.

Method: Applied reduced model using newly developed Gyro-Fluid System (GFS) code to calculate kinetic ballooning stability boundary. Used ELITE to approximate high-n global ballooning stability. Integrated both into EPED model.

Result: GFS captured KBMs in DIII-D and NSTX(-U) pedestals. High-n global ballooning modes limit local 2nd stability access and constrain width evolution. Improved EPED with GFS and ELITE showed better agreement with DIII-D pedestal data than EPED1.

Conclusion: The integration of GFS for kinetic ballooning stability and ELITE for high-n global ballooning modes into EPED provides improved pedestal prediction capability, successfully capturing experimental observations in conventional and spherical tokamaks.

Abstract: The EPED model [P.B. Snyder et al 2011 Nucl. Fusion 51 103016] had success in
describing type-I ELM and QH-mode pedestals in conventional tokamaks, by
combining kinetic ballooning mode (KBM) and peeling-ballooning (PB)
constraints. Within EPED, the KBM constraint is usually approximated by the
ideal ballooning mode (IBM) stability threshold. It has been noted that
quantitative differences between local ideal MHD and gyro-kinetic (GK)
ballooning stability can be larger at low aspect ratio. KBM critical pedestals
are consistent with observation in initial studies on conventional and
spherical tokamaks. In this work, the application of a reduced model for the
calculation of the kinetic ballooning stability boundary is presented based on
a novel and newly developed Gyro-Fluid System (GFS) code [G.M. Staebler et al
2023 Phys. Plasmas 30 102501]. GFS is observed to capture KBMs in DIII-D as
well as the NSTX(-U) pedestals, opening the route for the integration of this
model into EPED. Finally, high-n global ballooning modes are observed to limit
local 2nd stability access and thus provide a transport mechanism that
constrains the width evolution with beta_p,ped. The high-n global ballooning
stability is approximated by its ideal MHD analogue using ELITE. It is shown
that nearly local high-n with k_y*rho_s~1/2 modes can provide a proxy for the
critical beta_p,ped when 2nd stable access exists on DIII-D plasmas. The use of
GFS and ELITE scaling in EPED provided improved agreement in comparison to
EPED1 with DIII-D pedestal data.

</details>


### [60] [Reconstructing High-fidelity Plasma Turbulence with Data-driven Tuning of Diffusion in Low Resolution Grids](https://arxiv.org/abs/2509.11576)
*Kunpeng Li,Youngwoo Cho,Xavier Garbet,Chenguang Wan,Robin Varennes,Kyungtak Lim,Virginie Grandgirard,Zhisong Qu,Ong Yew Soon*

Main category: physics.plasm-ph

TL;DR: Theory-guided machine learning combines DIA theory with PINNs to create an efficient closure model for plasma turbulence that reveals inverse transport phenomena and achieves 10x speed-up.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of developing physically consistent closure models for plasma turbulence simulation, even in minimal systems like the Hasegawa-Wakatani model.

Method: Leverage Direct Interaction Approximation theory to construct a six-term closure structure, then use physics-informed neural networks to learn transport coefficients from data.

Result: The EHW-C model accurately reproduces DNS results with 1/8th spatial resolution (10x speed-up) and reveals negative coefficients indicating inverse transport phenomena.

Conclusion: Theory-guided machine learning enhances computational efficiency and uncovers emergent transport mechanisms in nonlinear plasma systems.

Abstract: Developing physically consistent closure models is a longstanding challenge
in simulating plasma turbulence, even in minimal systems such as the two-field
Hasegawa-Wakatani (HW) model, which captures essential features of drift-wave
turbulence with a reduced set of variables. In this work, we leverage
theoretical insights from Direct Interaction Approximation (DIA) to construct a
six-term closure structure that captures the dominant turbulent transport
processes, including both diffusion and hyper-diffusion. While the mathematical
form of the closure is fully prescribed by DIA, the corresponding transport
coefficients are learned from data using physics-informed neural networks
(PINNs). The resulting Extended HW model with Closure (EHW-C) model reveals
several nontrivial features of plasma turbulence: notably, some inferred
coefficients become negative in certain regimes, indicating inverse transport,
a phenomenon absent in conventional closure models. Moreover, the EHW-C model
accurately reproduces the spectral and flux characteristics of high-resolution
Direct Numerical Simulations (DNS), while requiring only one-eighth the spatial
resolution per direction, yielding a tenfold speed-up. This work demonstrates
how theory-guided machine learning can both enhance computational efficiency
and uncover emergent transport mechanisms in strongly nonlinear plasma systems.

</details>


### [61] [Helical Core Formation and MHD Stability in ITER-Scale Plasmas with Fusion-born Alpha Particles](https://arxiv.org/abs/2509.11749)
*P. Adulsiriswad,A. Bierwage,M. Yagi*

Main category: physics.plasm-ph

TL;DR: Alpha particles enhance helical core displacement in ITER-scale plasmas, with optimal effect below 1% beta_alpha. Beyond 3%, pressure profiles flatten and secondary resistive modes may cause magnetic chaos.


<details>
  <summary>Details</summary>
Motivation: To study how fusion-born alpha particles affect the helical core stability in ITER-scale hybrid scenarios with low magnetic shear, particularly their impact on plasma confinement and omnigenity.

Method: Used 3-D MHD force balance analysis and MHD-PIC simulations to determine helical core states, measuring magnetic axis displacement and analyzing pressure profiles of all particle species including alpha particles.

Result: Alpha particle pressure enhances helical core displacement up to beta_alpha(0)=1% without profile flattening. Beyond 1%, displacement increases until reaching limit at 3% beta_alpha with partial pressure profile flattening. Secondary resistive pressure-driven MHD modes can become unstable after helical core formation.

Conclusion: Alpha particles significantly influence helical core behavior in ITER-scale plasmas. Below 1% beta_alpha preserves omnigenity, while higher concentrations lead to pressure profile degradation and potential magnetic chaos through secondary modes, affecting plasma confinement.

Abstract: The effect of fusion-born alpha particles on the helical core (HC), a
long-lived ideal saturation state of the $m/n=1/1$ kink/quasi-interchange mode,
is studied in the ITER-scale hybrid scenario where a core plasma has a low
magnetic shear $q\gtrsim1$. The HC state is determined by 3-D MHD force balance
and all factors that contribute to it, such as plasma shaping, the safety
factor profile, and the pressure profiles of all particle species. An
incomplete but useful measure of the HC is the displacement of the magnetic
axis, $\delta_\mathrm{HC}$. Using MHD-PIC simulations, we find that
$\delta_\mathrm{HC}$ is enhanced by increasing alpha particle pressure
$\beta_\mathrm{\alpha}$. Within the ITER operating alpha pressure
$\beta_\mathrm{\alpha}(0) \lesssim 1\%$, $\beta_\mathrm{\alpha}$ can be
approximately treated as part of the total MHD pressure. In this regime, there
is no notable flattening of the pressure profile, indicating that the HC
preserves the omnigenity of the plasma. If one increases
$\beta_\mathrm{\alpha}(0)$ beyond $1\%$, $\delta_\mathrm{HC}$ continues to
increase with $\beta_\mathrm{\alpha}$ until it reaches an upper limit at
$\beta_\mathrm{\alpha}(0)=3\%$ for our reference case. At this limit, both the
bulk and alpha pressure profiles are partially flattened, indicating a
reduction in omnigenity. After HC formation, a resistive pressure-driven MHD
mode can become unstable, which is localized along the compressed magnetic flux
region of the HC. This secondary mode consists of a broad spectrum of
short-wavelength Fourier components that grow at same rates and are thus part
of a single coherent entity. Our present simulation model is insufficient to
adequately represent such a secondary mode; however, preliminary results
suggest that it can facilitate magnetic chaos, which affects plasma
confinement.

</details>


### [62] [A high-performance elliptic solver for plasma boundary turbulence codes](https://arxiv.org/abs/2509.11831)
*Andreas Stegmeir,Cristian Lalescu,Mou Lin,Jordy Trilaksono,Nicola Varini,Tilman Dannert*

Main category: physics.plasm-ph

TL;DR: Efficient GPU-accelerated elliptic solver for magnetic confinement fusion turbulence models using fGMRES with multigrid preconditioning


<details>
  <summary>Details</summary>
Motivation: Elliptic equations are fundamental in fusion turbulence modeling but require efficient solvers for boundary region conditions

Method: Finite difference discretization with fGMRES and geometric multigrid preconditioner, implemented with OpenMP and GPU acceleration (CUDA/HIP)

Result: Significant GPU speed-ups exceeding external libraries, linear O(N) scaling with problem size

Conclusion: Solver successfully implemented in PARALLAX/PAccX libraries and integrated into plasma boundary turbulence codes GRILLIX and GENE-X

Abstract: Elliptic equations play a crucial role in turbulence models for magnetic
confinement fusion. Regardless of the chosen modeling approach - whether
gyrokinetic, gyrofluid, or drift-fluid - the Poisson equation and Amp\`{e}re's
law lead to elliptic problems that must be solved on 2D planes perpendicular to
the magnetic field. In this work, we present an efficient solver for such
generalised elliptic problems, especially suited for the conditions in the
boundary region. A finite difference discretisation is employed, and the solver
is based on a flexible generalised minimal residual method (fGMRES) with a
geometric multigrid preconditioner. We present implementations with OpenMP
parallelisation and GPU acceleration, with backends in CUDA and HIP. On the
node level, significant speed-ups are achieved with the GPU implementation,
exceeding external library solutions such as rocALUTION. In accordance with
theoretical scaling laws for multigrid methods, we observe linear scaling of
the solver with problem size, $O(N)$. This solver is implemented in the
PARALLAX/PAccX libraries and serves as a central component of the plasma
boundary turbulence codes GRILLIX and GENE-X.

</details>


### [63] [Anomalous electron heating in laboratory magnetized quasi-perpendicular collisionless shocks](https://arxiv.org/abs/2509.12164)
*V. Valenzuela-Villaseca,S. Totorica,J. Griff-McMahon,L. -J. Chen,S. Malko,P. V. Heuer,P. Pongkitiwanichakul,W. Fox,D. B. Schaeffer*

Main category: physics.plasm-ph

TL;DR: First observation of fully-developed supercritical magnetized collisionless shocks with R=4 compression ratio after 7 ion gyration periods, showing significant collisionless anomalous electron heating and equipartition between electrons and ions downstream.


<details>
  <summary>Details</summary>
Motivation: To study the formation and characteristics of supercritical magnetized collisionless shocks in laboratory experiments, particularly focusing on shock development time, compression ratios, and collisionless heating mechanisms.

Method: Laboratory experiments with supercritical, magnetized collisionless shock conditions (M_A ≲ 10, β∼1), using Thomson scattering for ion-acoustic wave spectral broadening measurements and Rankine-Hugoniot conditions analysis.

Result: Observed fully-developed shocks after 7 upstream ion gyration periods with R=4 compression ratio. Measured upstream T_e=115 eV, T_i=15 eV; downstream T_e=390 eV, inferred T_i=340 eV. Found 30% excess electron heating from adiabatic/collisional expectations and electron-ion equipartition with T_e/T_i=1.2.

Conclusion: The experiments demonstrate significant collisionless anomalous electron heating in magnetized shocks, with electrons and ions reaching near-equipartition downstream, providing insights into collisionless shock physics and energy dissipation mechanisms.

Abstract: We present laboratory results from supercritical, magnetized collisionless
shock experiments ($M_A \lesssim 10$, $\beta\sim 1$). We report the first
observation of fully-developed shocks ($R=4$ compression ratio and a downstream
region decoupled from the piston) after seven upstream ion gyration periods. A
foot ahead of the shock exhibits super-adiabatic electron and ion heating. We
measure the electron temperature $T_e = 115$ eV and ion temperature $T_i = 15$
eV upstream of the shock; whereas, downstream, we measure $T_e=390$ eV and
infer $T_i=340$ eV, consistent with both Thomson scattering ion-acoustic wave
spectral broadening and Rankine-Hugoniot conditions. The downstream electron
temperature has a $30$-percent excess from adiabatic and collisional
electron-ion heating, implying significant collisionless anomalous electron
heating. Furthermore, downstream electrons and ions are in equipartition, with
a unity electron-ion temperature ratio $T_e/T_i = 1.2$.

</details>


<div id='physics.space-ph'></div>

# physics.space-ph [[Back]](#toc)

### [64] [Magnitude of Short-Wavelength Electric Field Fluctuations in Simulations of Collisionless Plasma Shocks](https://arxiv.org/abs/2509.11286)
*Vadim Roytershteyn,Lynn B. Wilson III,Li-Jen Chen,Michael Gedalin,Nicolai Pogorelov*

Main category: physics.space-ph

TL;DR: Large-amplitude electrostatic fluctuations in collisionless shocks are reproduced in 1D PIC simulations with realistic mass ratios, showing fluctuation magnitude increases with electron plasma-to-cyclotron frequency ratio when electron temperature exceeds ion temperature.


<details>
  <summary>Details</summary>
Motivation: Spacecraft observations show large electrostatic fluctuations in collisionless shocks, but kinetic simulations have struggled to reproduce their amplitude, hindering understanding of their role in energy dissipation and shock structure.

Method: 1D particle-in-cell (PIC) simulations with realistic proton-to-electron mass ratio, varying upstream electron-to-ion temperature ratio and electron plasma-to-cyclotron frequency ratio.

Result: Fluctuation magnitude increases with ω_pe/Ω_ce ratio when T_e > T_i, reaching realistic values at ω_pe/Ω_ce ≳ 30. Fluctuations are associated with electrostatic solitary structures like ion phase-space holes. When T_i > T_e, fluctuations remain small.

Conclusion: Upstream temperature conditions (T_e > T_i) combined with high ω_pe/Ω_ce ratio are key factors for reproducing observed large-amplitude electrostatic fluctuations in collisionless shocks.

Abstract: Large-amplitude electrostatic fluctuations are routinely observed by
spacecraft upon traversal of collisionless shocks in the heliosphere. Kinetic
simulations of shocks have struggled to reproduce the amplitude of such
fluctuations, complicating efforts to understand their influence on energy
dissipation and shock structure. In this paper, 1D particle-in-cell simulations
with realistic proton-to-electron mass ratio are used to show that in cases
with upstream electron temperature $T_e$ exceeding the ion temperature $T_i$,
the magnitude of the fluctuations increases with the electron
plasma-to-cyclotron frequency ratio $\omega_{pe}/\Omega_{ce}$, reaching
realistic values at $\omega_{pe}/\Omega_{ce} \gtrsim 30$. The large-amplitude
fluctuations in the simulations are shown to be associated with electrostatic
solitary structures, such as ion phase-space holes. In the cases where upstream
temperature ratio is reversed, the magnitude of the fluctuations remains small.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [65] [Kernel-based Stochastic Approximation Framework for Nonlinear Operator Learning](https://arxiv.org/abs/2509.11070)
*Jia-Qi Yang,Lei Shi*

Main category: stat.ML

TL;DR: A stochastic approximation framework for learning nonlinear operators between infinite-dimensional spaces using Mercer operator-valued kernels, achieving dimension-free polynomial convergence rates and overcoming the curse of dimensionality.


<details>
  <summary>Details</summary>
Motivation: To develop a general framework for nonlinear operator learning that goes beyond the classical K=kI paradigm, enabling rich structural modeling with rigorous theoretical guarantees for various operator learning tasks.

Method: Utilizes general Mercer operator-valued kernels including compact kernels with discrete spectral decompositions and diagonal kernels K(x,x')=k(x,x')T. Introduces vector-valued interpolation spaces to quantify misspecification error and establishes dimension-free polynomial convergence rates.

Result: The framework demonstrates that nonlinear operator learning can overcome the curse of dimensionality and provides rates for intrinsically nonlinear operator learning beyond linear-type behavior. Validated through numerical experiments on 2D Navier-Stokes equations.

Conclusion: The proposed framework provides a comprehensive approach for learning nonlinear operators with expressive vector-valued RKHSs, theoretical guarantees, and practical effectiveness across various operator learning applications.

Abstract: We develop a stochastic approximation framework for learning nonlinear
operators between infinite-dimensional spaces utilizing general Mercer
operator-valued kernels. Our framework encompasses two key classes: (i) compact
kernels, which admit discrete spectral decompositions, and (ii) diagonal
kernels of the form $K(x,x')=k(x,x')T$, where $k$ is a scalar-valued kernel and
$T$ is a positive operator on the output space. This broad setting induces
expressive vector-valued reproducing kernel Hilbert spaces (RKHSs) that
generalize the classical $K=kI$ paradigm, thereby enabling rich structural
modeling with rigorous theoretical guarantees. To address target operators
lying outside the RKHS, we introduce vector-valued interpolation spaces to
precisely quantify misspecification error. Within this framework, we establish
dimension-free polynomial convergence rates, demonstrating that nonlinear
operator learning can overcome the curse of dimensionality. The use of general
operator-valued kernels further allows us to derive rates for intrinsically
nonlinear operator learning, going beyond the linear-type behavior inherent in
diagonal constructions of $K=kI$. Importantly, this framework accommodates a
wide range of operator learning tasks, ranging from integral operators such as
Fredholm operators to architectures based on encoder-decoder representations.
Moreover, we validate its effectiveness through numerical experiments on the
two-dimensional Navier-Stokes equations.

</details>


<div id='physics.ins-det'></div>

# physics.ins-det [[Back]](#toc)

### [66] [Development of a national thrust test facility for electric propulsion at Robinson Research Institute, New Zealand](https://arxiv.org/abs/2509.10558)
*Emile Webster,Ben Mallett*

Main category: physics.ins-det

TL;DR: Development of test facilities for AF-MPD thruster with HTS magnet, including vacuum chamber, thrust stand, and control systems.


<details>
  <summary>Details</summary>
Motivation: To enable development of applied-field magneto-plasma-dynamic thrusters with high-temperature superconducting magnets by establishing comprehensive test infrastructure.

Method: Built 1m diameter vacuum chamber with 4200 L/s pumping speed, purpose-built pendulum thrust stand capable of 200 mN measurements, and developed software tools to coordinate all systems including HTS magnet and cryocooler.

Result: Facilities achieve vacuum of 5E10-4 hPa during operation, thrust measurement precision of ~1 mN with +/-2.3 mN accuracy, and rapid 2-hour pump-down from atmospheric pressure.

Conclusion: Successful establishment of comprehensive test facilities enables AF-MPD thruster development, with various upgrades planned for future enhancements.

Abstract: The Robinson Research Institute is developing a type of electric propulsion
for spacecraft called an applied-field magneto-plasma-dynamic (AF-MPD)
thruster. The applied field module of the thruster features a cryocooler-cooled
high-temperature-superconducting (HTS) magnet generating central fields
exceeding 1 Tesla. This paper reports on the progress to date of the thruster
test facilities established at the Institute to enable development of the
AF-MPD and electric propulsion in general. The vacuum facilities consist
primarily of a 1 m diameter, 3 m3 cylindrical chamber with a high-vacuum
pumping speed, measured to be about 4200 L/s (Ar). The chamber can be reduced
from atmospheric pressure to 1E10-5 hPa in about 2 h, enabling a rapid testing
turn-around time, and maintains 5E10-4 hPa during thruster operation at typical
mass flow rates of 5 mg/s (Ar). A pendulum thrust-stand has been purpose built
to withstands static thruster loads of up to 30 kg and measure thrust forces up
to 200 mN. It is also designed to be inherently insensitive to the unique
challenges of measuring AF-MPD with superconducting magnets; strong magnetic
fields, cryocooler vibration, and the thermal gradients. Initial thrust-stand
testing, whilst implementing both water-cooling and a cryocooler showed it to
be capable of measuring forces to a precision of approximately 1 mN and with an
absolute accuracy of +/-2.3 mN (full scale). An extensive suite of software
tools has also been developed to coordinate the large set of instruments needed
to run the vacuum facilities, the thrust-stand, HTS magnet and cryocooler in
addition to the many other auxiliary- and logging systems. Various upgrades to
the facilities are in progress or planned.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [67] [Contractive kinetic Langevin samplers beyond global Lipschitz continuity](https://arxiv.org/abs/2509.12031)
*Iosif Lytras,Panagiotis Mertikopoulos*

Main category: math.PR

TL;DR: Novel tamed discretizations of kinetic Langevin SDE for sampling from log-concave distributions with superlinear gradient growth, showing contractivity and log-Sobolev properties with non-asymptotic convergence bounds.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of sampling from log-concave distributions that may exhibit superlinear gradient growth, which standard kinetic Langevin algorithms struggle with due to instability issues.

Method: Developed two carefully tailored taming schemes for discretizing the kinetic Langevin SDE, ensuring both contractive properties and satisfaction of log-Sobolev inequalities.

Result: Established non-asymptotic bounds in 2-Wasserstein distance between the algorithm's output distribution and the target measure, demonstrating effective convergence.

Conclusion: The proposed tamed discretizations provide stable and theoretically grounded methods for sampling from challenging log-concave distributions with superlinear gradient characteristics.

Abstract: In this paper, we examine the problem of sampling from log-concave
distributions with (possibly) superlinear gradient growth under kinetic
(underdamped) Langevin algorithms. Using a carefully tailored taming scheme, we
propose two novel discretizations of the kinetic Langevin SDE, and we show that
they are both contractive and satisfy a log-Sobolev inequality. Building on
this, we establish a series of non-asymptotic bounds in $2$-Wasserstein
distance between the law reached by each algorithm and the underlying target
measure.

</details>


### [68] [Navier-Stokes Equations with Fractional Dissipation and Associated Doubly Stochastic Yule Cascades](https://arxiv.org/abs/2509.10806)
*Radu Dascaliuc,Tuan N. Pham,Enrique Thomann,Edward C. Waymire*

Main category: math.PR

TL;DR: This paper analyzes parametric regions for explosive vs non-explosive regimes in doubly stochastic Yule cascades associated with fractional Navier-Stokes equations, establishing non-uniqueness, local existence, and blow-up results.


<details>
  <summary>Details</summary>
Motivation: To understand the explosive behavior and solution properties of fractional Navier-Stokes equations through the analysis of associated stochastic cascades and solution processes.

Method: Uses parametric analysis of spatial dimension d and Laplacian power γ to identify explosive regimes in doubly stochastic Yule cascades, then applies majorization principle and stochastic solution processes to study FNSE properties.

Result: Identified parametric regions separating explosive/non-explosive regimes, established non-uniqueness/local existence/blow-up results, derived closed-form solution for d=2 case, proved finite-time loss of integrability for large initial data.

Conclusion: The analysis reveals complex solution behavior where loss of integrability doesn't necessarily imply blow-up, and solutions can be continued beyond integrability thresholds using modified averaging techniques.

Abstract: Parametric regions are identified in terms of the spatial dimension $d$ and
the power $\gamma$ of the Laplacian that separate explosive from non-explosive
regimes for the self-similar doubly stochastic Yule cascades (DSY) naturally
associated with the deterministic fractional Navier-Stokes equations (FNSE) on
$\mathbb{R}^d$ in the scaling-supercritical setting. Explosion and/or geometric
properties of the DSY, are then used to establish non-uniqueness, local
existence, and finite-time blow-up results for a scalar partial differential
equation associated to the FNSE through a majorization principle at the level
of stochastic solution processes. The solution processes themselves are
constructed from the DSY and yield solutions to the FNSE upon taking
expectations.
  In the special case $d=2$, a closed-form expression for the solution process
of the FNSE is derived. This representation is employed to prove finite-time
loss of integrability of the solution process for sufficiently large initial
data. Notably, this lack of integrability does not necessarily imply blow-up of
the FNSE solutions themselves. In fact, in the radially symmetric case,
solutions can be continued beyond the integrability threshold by employing a
modified notion of averaging.

</details>


### [69] [Weak Existence and Uniqueness for Super-Brownian Motion with Irregular Drift](https://arxiv.org/abs/2509.10984)
*Leonid Mytnik,Johanna Weinberger*

Main category: math.PR

TL;DR: Weak existence and uniqueness for 1D SPDE with bounded drift and square root noise


<details>
  <summary>Details</summary>
Motivation: To establish existence and uniqueness results for stochastic partial differential equations with non-Lipschitz or discontinuous drift functions, extending beyond classical Lipschitz conditions

Method: Extension of the duality relation of super-Brownian motion to handle a broad class of admissible drifts, including non-Lipschitz and discontinuous functions at zero

Result: Successfully proved weak existence and uniqueness for random field solutions of the one-dimensional SPDE with space-time white noise and bounded drift h where h(0) ≥ 0

Conclusion: The duality approach enables treatment of a wider range of drift functions than traditional methods, providing existence and uniqueness results for SPDEs with challenging drift conditions

Abstract: We establish weak existence and uniqueness for random field solutions of the
one-dimensional SPDE \[ d_tX_t = \frac{1}{2}\Delta X_t +h(X_t)+
\sqrt{X_t}\dot{W}, \quad t\geq 0,\] where $\dot{W}$ is space-time white noise
and $h$ is a bounded drift with $h(0)\geq 0$. The proof relies on an extension
of the duality relation of the super-Brownian motion, which allows us to treat
a broad class of admissible drifts, including functions that are non-Lipschitz
or discontinuous at zero.

</details>


### [70] [A note on Tricomi-type partial differential equations with white noise initial condition](https://arxiv.org/abs/2509.11753)
*Enrico Bernardi,Alberto Lanconelli*

Main category: math.PR

TL;DR: Study of Tricomi-type PDEs showing that random field solutions become ill-posed when lower-order terms are added, despite operators being equivalent in hyperbolic theory. Well-posedness is restored with fractional Gaussian noise initial data.


<details>
  <summary>Details</summary>
Motivation: To understand how lower-order terms affect the well-posedness of random field solutions for Tricomi-type partial differential equations, particularly when initial data is Gaussian white noise.

Method: Generalized representation formula for solutions by allowing non-integer power coefficients, then analyzed solution robustness with Gaussian white noise initial data and examined the impact of lower-order terms.

Result: Discovered that introducing lower-order terms causes loss of well-defined random field solutions, showing different well-posedness properties despite operator equivalence. Well-posedness is restored with fractional Gaussian white noise (Hurst parameter H ∈ (1/2,1)).

Conclusion: Lower-order terms significantly impact random field solution existence for Tricomi-type operators, revealing a divergence between deterministic hyperbolic theory and stochastic well-posedness properties.

Abstract: We study a class of Tricomi-type partial differential equations previously
investigated in [28]. Firstly, we generalize the representation formula for the
solution obtained there by allowing the coefficient in front of the
second-order partial derivative with respect to $x$ to be any non integer power
of $t$. Then, we analyze the robustness of that solution by taking the initial
data to be Gaussian white noise and we discover that the existence of a
well-defined random field solution is lost upon the introduction of lower-order
terms in the operator. This phenomenon shows that, even though the Tricomi-type
operators with or without lower-order terms are the same from the point of view
of the theory of hyperbolic operators with double characteristics, their
corresponding random versions exhibit different well posedness properties. We
also prove that for more regular initial data, specifically fractional Gaussian
white noise with Hurst parameter $H\in (1/2,1)$, the well posedness of the
Cauchy problem for the Tricomi-type operator with lower-order term is restored.

</details>


<div id='math.CA'></div>

# math.CA [[Back]](#toc)

### [71] [Conditional existence of maximizers for the Tomas-Stein inequality for the sphere](https://arxiv.org/abs/2509.10754)
*Shuanglin Shao,Ming Wang*

Main category: math.CA

TL;DR: Existence of extremizers for Tomas-Stein inequality on spheres, using refined inequalities and profile decompositions with Tao's bilinear restriction theorem.


<details>
  <summary>Details</summary>
Motivation: To prove the existence of functions that achieve the best constant in the Tomas-Stein inequality for compact subsets of spheres, extending previous results.

Method: Uses refined Tomas-Stein inequality, profile decompositions, and Tao's sharp bilinear restriction theorem for paraboloids to establish orthogonality in decompositions.

Result: Proves existence of extremizers for the Tomas-Stein inequality, showing any extremizing sequence converges to an extremizer (conditional on strict comparison between sphere and Strichartz constants).

Conclusion: The method successfully establishes extremizer existence using advanced restriction theory techniques, building on but differing from Frank-Lieb-Sabin's missing mass approach.

Abstract: The Tomas-Stein inequality for a compact subset $\Gamma$ of the sphere $S^d$
states that the mapping $f\mapsto \widehat{f\sigma}$ is bounded from
$L^2(\Gamma,\sigma)$ to $L^{2+4/d}(\R^{d+1})$. Then conditional on a strict
comparison between the best constants for the sphere and for the Strichartz
inequality for the Schr\"odinger equations, we prove that there exist functions
which extremize this inequality, and any extremising sequence has a subsequence
which converges to an extremizer. The method is based on the refined
Tomas-Stein inequality for the sphere and the profile decompositions. The key
ingredient to establish orthogonality in profile decompositions is that we use
Tao's sharp bilinear restriction theorem for the paraboloids beyond the
Tomas-Stein range. Similar results have been previously established by Frank,
Lieb and Sabin \cite{Frank-Lieb-Sabin:2007:maxi-sphere-2d}, where they used the
method of the missing mass.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [72] [Probing laser-driven surface and subsurface dynamics via grazing-incidence XFEL scattering and diffraction](https://arxiv.org/abs/2509.12015)
*Lisa Randolph,Özgül Öztürk,Dmitriy Ksenzov,Lingen Huang,Thomas Kluge,S. V. Rahul,Victorien Bouffetier,Carsten Baehtz,Mohammadreza Banjafar,Erik Brambrink,Fabien Brieuc,Byoung Ick Cho,Sebastian Göde,Tobias Held,Hauke Höppner,Gerhard Jakob,Mathias Kläui,Zuzana Konôpková,Changhoo Lee,Gyusang Lee,Mikako Makita,Mikhail Mishchenko,Mianzhen Mo,Pascal D. Ndione,Michael Paulus,Alexander Pelka,Franziska Paschke-Bruehl,Thomas R. Preston,Baerbel Rethfeld,Christian Rödel,Michal Šmíd,Ling Wang,Sebastian T. Weber,Lennart Wollenweber,Jan-Patrick Schwinkendorf,Christian Gutt,Motoaki Nakatsutsumi*

Main category: physics.optics

TL;DR: A grazing-incidence x-ray platform combining GISAXS and GID with picosecond resolution at XFEL for studying ultrafast laser-matter interactions and warm dense matter.


<details>
  <summary>Details</summary>
Motivation: To overcome photon-flux limitations of synchrotron grazing-incidence geometries and provide time-resolved benchmarks for complex theoretical models of ultrafast laser-matter interaction.

Method: Simultaneous time-resolved GISAXS and GID measurements on femtosecond laser-irradiated gold films using grazing-incidence x-ray geometry with depth-selectivity by tuning incidence angle.

Result: GISAXS resolves ultrafast changes in surface nanomorphology while GID quantifies subsurface lattice compression, grain orientation, melting, and recrystallization with picosecond resolution.

Conclusion: The approach provides stringent benchmarks for theoretical models and is well-suited for future applications in inertial confinement fusion to visualize buried-interface perturbations and interfacial thermal resistance.

Abstract: We demonstrate a grazing-incidence x-ray platform that simultaneously records
time-resolved grazing-incidence small-angle x-ray scattering (GISAXS) and
grazing-incidence x-ray diffraction (GID) from a femtosecond laser-irradiated
gold film above the melting threshold, with picosecond resolution at an x-ray
free-electron laser (XFEL). By tuning the x-ray incidence angle, the probe
depth is set to tens of nanometers, enabling depth-selective sensitivity to
near-surface dynamics. GISAXS resolves ultrafast changes in surface
nanomorphology (correlation length, roughness), while GID quantifies subsurface
lattice compression, grain orientation, melting, and recrystallization. The
approach overcomes photon-flux limitations of synchrotron grazing-incidence
geometries and provides stringent, time-resolved benchmarks for complex
theoretical models of ultrafast laser-matter interaction and warm dense matter.
Looking ahead, the same depth-selective methodology is well suited to inertial
confinement fusion (ICF): it can visualize buried-interface perturbations and
interfacial thermal resistance on micron to sub-micron scales that affect
instability seeding and burn propagation.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [73] [Oscillating Heat Transfer Prediction in Porous Structures Using Generative AI-Assisted Explainable Machine Learning](https://arxiv.org/abs/2509.11863)
*Lichang Zhu,Laura Schaefer,Leitao Chen,Ben Xu*

Main category: physics.flu-dyn

TL;DR: A data-driven framework combining wGAN-GP, LBM simulations, and XGBoost with SHAP interpretation predicts and explains thermal performance in porous structures under oscillating flow.


<details>
  <summary>Details</summary>
Motivation: Predicting thermal performance in porous structures under oscillating flow is challenging due to complex fluid dynamics and geometric coupling.

Method: Integrated wGAN-GP for structure generation, LBM simulations for thermal data, nested cross-validation with Bayesian optimization for model selection, and SHAP for interpretation.

Result: XGBoost achieved R^2=0.9981 for Nusselt number prediction. Key predictors identified: Reynolds number, Strouhal number, porosity, specific surface area, and pore size dispersion.

Conclusion: The framework provides accurate predictions and physical interpretability, offering practical design guidelines for enhanced thermal performance in porous media.

Abstract: Predicting and interpreting thermal performance under oscillating flow in
porous structures remains a critical challenge due to the complex coupling
between fluid dynamics and geometric features. This study introduces a
data-driven wGAN-LBM-Nested_CV framework that integrates generative deep
learning, numerical simulation based on the lattice Boltzmann method (LBM), and
interpretable machine learning to predict and explain the thermal behavior in
such systems. A wide range of porous structures with diverse topologies were
synthesized using a Wasserstein generative adversarial network with gradient
penalty (wGAN-GP), significantly expanding the design space. High-fidelity
thermal data were then generated through LBM simulations across various
Reynolds (Re) and Strouhal numbers (St). Among several machine learning models
evaluated via nested cross-validation and Bayesian optimization, XGBoost
achieved the best predictive performance for the average Nusselt number (Nu)
(R^2=0.9981). Model interpretation using SHAP identified the Reynolds number,
Strouhal number, porosity, specific surface area, and pore size dispersion as
the most influential predictors, while also revealing synergistic interactions
among them. Threshold-based insights, including Re > 75 and porosity > 0.6256,
provide practical guidance for enhancing convective heat transfer. This
integrated approach delivers both quantitative predictive accuracy and physical
interpretability, offering actionable guidelines for designing porous media
with improved thermal performance under oscillatory flow conditions.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [74] [Large language model-empowered next-generation computer-aided engineering](https://arxiv.org/abs/2509.11447)
*Jiachen Guo,Chanwook Park,Dong Qian,Thomas J. R. Hughes,Wing Kam Liu*

Main category: cs.CE

TL;DR: LLMs enable automated CAE workflows for model order reduction, reducing manual effort in solving large-scale parametric PDEs through natural language interaction and code generation.


<details>
  <summary>Details</summary>
Motivation: Current LLM applications in CAE only automate routine tasks but fail to address computational challenges of large-scale, high-dimensional systems. Intrusive model order reduction methods are powerful but underused due to their labor-intensive nature.

Method: Propose LLM-empowered CAE agents that plan, execute and adapt CAE workflows autonomously. Specifically develop an agent for data-free model order reduction using Tensor-decomposition-based A Priori Surrogates (TAPS), where LLMs automate derivations, code restructuring and implementation.

Result: Natural language prompts describing parametric PDEs can be translated into efficient solver implementations, substantially reducing human effort while producing high-fidelity reduced-order models. LLMs can also synthesize novel MOR solvers for unseen cases like nonlinear and high-dimensional problems.

Conclusion: LLMs have the potential to establish the foundation for next-generation CAE systems by making intrusive model order reduction both practical and broadly accessible through automated workflow management and code generation.

Abstract: Software development has entered a new era where large language models (LLMs)
now serve as general-purpose reasoning engines, enabling natural language
interaction and transformative applications across diverse domains. This
paradigm is now extending into computer-aided engineering (CAE). Recent
applications of LLMs in CAE have successfully automated routine tasks,
including CAD model generation and FEM simulations. Nevertheless, these
contributions, which primarily serve to reduce manual labor, are often
insufficient for addressing the significant computational challenges posed by
large-scale, high-dimensional systems. To this aim, we first introduce the
concept of LLM-empowered CAE agent, where LLMs act as autonomous collaborators
that plan, execute, and adapt CAE workflows. Then, we propose an LLM-empowered
CAE agent for data-free model order reduction (MOR), a powerful yet underused
approach for ultra-fast large-scale parametric analysis due to the intrusive
nature and labor-intensive redevelopment of solvers. LLMs can alleviate this
barrier by automating derivations, code restructuring, and implementation,
making intrusive MOR both practical and broadly accessible. To demonstrate
feasibility, we present an LLM-empowered CAE agent for solving
ultra-large-scale space-parameter-time (S-P-T) physical problems using
Tensor-decomposition-based A Priori Surrogates (TAPS). Our results show that
natural language prompts describing parametric partial differential equations
(PDEs) can be translated into efficient solver implementations, substantially
reducing human effort while producing high-fidelity reduced-order models.
Moreover, LLMs can synthesize novel MOR solvers for unseen cases such as
nonlinear and high-dimensional parametric problems based on their internal
knowledge base. This highlights the potential of LLMs to establish the
foundation for next-generation CAE systems.

</details>


### [75] [Numerical analysis of fluid estimation for source terms in neutral particles simulation](https://arxiv.org/abs/2509.11883)
*Zhirui Tang,Emil Løvbak,Julian Koellermeier,Giovanni Samaey*

Main category: cs.CE

TL;DR: Numerical analysis confirms KDMC with fluid estimation achieves significantly lower errors than fluid methods and outperforms kinetic MC with speed-up in plasma edge simulations.


<details>
  <summary>Details</summary>
Motivation: High computational cost of kinetic Monte Carlo methods for large reactors like ITER and DEMO due to high particle collision rates.

Method: Asymptotic-preserving kinetic-diffusion Monte Carlo (KDMC) simulation with fluid estimation technique, compared against fluid method and kinetic MC reference.

Result: KDMC with fluid estimation achieves at least one order of magnitude lower errors than fluid method in both high- and low-collisional regimes, with clear speed-up over kinetic MC.

Conclusion: The analysis confirms the effectiveness of KDMC with fluid estimation algorithm for plasma edge simulations in large reactors.

Abstract: In plasma edge simulations, kinetic Monte Carlo (MC) is often used to
simulate neutral particles and estimate source terms. For large-sized reactors,
like ITER and DEMO, high particle collision rates lead to a substantial
computational cost for such schemes. To address this challenge, an
asymptotic-preserving kinetic-diffusion Monte Carlo (KDMC) simulation method
and a corresponding fluid estimation technique have been proposed in the
literature. In this work, we perform numerical analysis on the convergence of
KDMC with the fluid estimation. To do so, we compare the accuracy of the
analyzed algorithm with the accuracy of an approximate fluid method using the
kinetic MC method as a reference. In a one-dimensional test case, KDMC with the
fluid estimation achieves at least one order of magnitude lower errors than the
fluid method for both high- and low-collisional regimes. Moreover, KDMC with
the fluid estimation outperforms the kinetic MC method with a clear speed-up.
Overall, our analysis confirms the effectiveness of the discussed algorithm.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [76] [Moment Estimates and DeepRitz Methods on Learning Diffusion Systems with Non-gradient Drifts](https://arxiv.org/abs/2509.10495)
*Fanze Kong,Chen-Chih Lai,Yubin Lu*

Main category: cs.LG

TL;DR: A data-driven method called Moment-DeepRitz Method for learning drift decompositions in conservative-dissipative diffusion systems, robust to noise and adaptable to complex potentials.


<details>
  <summary>Details</summary>
Motivation: Conservative-dissipative dynamics are common in complex open systems, but existing methods struggle with noisy data, rough potentials, and oscillatory rotations.

Method: Two-phase data-driven approach using Moment-DeepRitz Method to learn drift decompositions in generalized diffusion systems with conservative-dissipative dynamics.

Result: The method demonstrates effectiveness through numerical experiments, showing robustness to noisy data and adaptability to challenging scenarios.

Conclusion: The proposed Moment-DeepRitz Method provides an effective solution for learning drift decompositions in complex conservative-dissipative systems with practical advantages over existing approaches.

Abstract: Conservative-dissipative dynamics are ubiquitous across a variety of complex
open systems. We propose a data-driven two-phase method, the Moment-DeepRitz
Method, for learning drift decompositions in generalized diffusion systems
involving conservative-dissipative dynamics. The method is robust to noisy
data, adaptable to rough potentials and oscillatory rotations. We demonstrate
its effectiveness through several numerical experiments.

</details>


### [77] [M4GN: Mesh-based Multi-segment Hierarchical Graph Network for Dynamic Simulations](https://arxiv.org/abs/2509.10659)
*Bo Lei,Victor M. Castillo,Yeping Hu*

Main category: cs.LG

TL;DR: M4GN is a three-tier hierarchical GNN that addresses computational cost and over-smoothing in mesh-based PDE simulations through hybrid segmentation and efficient cross-segment reasoning.


<details>
  <summary>Details</summary>
Motivation: Traditional mesh-based GNNs suffer from high computational costs and over-smoothing on large meshes, while existing hierarchical approaches struggle with building topology-respecting coarse graphs and maintaining fine-scale accuracy.

Method: Uses hybrid segmentation combining fast graph partitioning with superpixel-style refinement guided by modal-decomposition features, followed by permutation-invariant aggregation and a three-tier architecture with micro-level GNN and macro-level transformer.

Result: Achieves up to 56% improvement in prediction accuracy and up to 22% faster inference compared to state-of-the-art baselines on multiple benchmark datasets.

Conclusion: M4GN provides an effective solution for efficient and accurate PDE simulations by balancing local dynamics capture with global segment reasoning through its hierarchical architecture.

Abstract: Mesh-based graph neural networks (GNNs) have become effective surrogates for
PDE simulations, yet their deep message passing incurs high cost and
over-smoothing on large, long-range meshes; hierarchical GNNs shorten
propagation paths but still face two key obstacles: (i) building coarse graphs
that respect mesh topology, geometry, and physical discontinuities, and (ii)
maintaining fine-scale accuracy without sacrificing the speed gained from
coarsening. We tackle these challenges with M4GN, a three-tier, segment-centric
hierarchical network. M4GN begins with a hybrid segmentation strategy that
pairs a fast graph partitioner with a superpixel-style refinement guided by
modal-decomposition features, producing contiguous segments of dynamically
consistent nodes. These segments are encoded by a permutation-invariant
aggregator, avoiding the order sensitivity and quadratic cost of aggregation
approaches used in prior works. The resulting information bridges a micro-level
GNN, which captures local dynamics, and a macro-level transformer that reasons
efficiently across segments, achieving a principled balance between accuracy
and efficiency. Evaluated on multiple representative benchmark datasets, M4GN
improves prediction accuracy by up to 56% while achieving up to 22% faster
inference than state-of-the-art baselines.

</details>


### [78] [PINGS: Physics-Informed Neural Network for Fast Generative Sampling](https://arxiv.org/abs/2509.11284)
*Achmad Ardani Prasha,Clavino Ourizqi Rachmadi,Muhamad Fauzan Ibnu Syahlan,Naufal Rahfi Anugerah,Nanda Garin Raditya,Putri Amelia,Sabrina Laila Mutiara,Hilman Syachr Ramadhan*

Main category: cs.LG

TL;DR: PINGS is a physics-informed neural network framework that enables single-step generative sampling (NFE=1) by approximating reverse-time probability-flow dynamics, achieving constant-time generation while preserving target distribution structure.


<details>
  <summary>Details</summary>
Motivation: To address the computational inefficiency of iterative diffusion sampling methods by developing a fast, single-pass generative sampling approach that maintains distributional accuracy.

Method: Trains a physics-informed neural network to approximate reverse-time probability-flow dynamics, framing generative sampling as a PINN-style residual problem with endpoint anchoring to create a direct map from noise to target distribution.

Result: Achieves constant-time generation (10^4 samples in 16.54ms) with excellent distribution preservation (MMD^2 = 1.88×10^-2) and small errors in statistical moments, significantly outperforming DPM-Solver and DDIM in speed.

Conclusion: PINGS presents a promising white-box, differentiable approach for fast generative sampling with potential applications in scientific simulation, offering NFE=1 generation while maintaining distributional fidelity.

Abstract: We introduce PINGS (Physics-Informed Neural Network for Fast Generative
Sampling), a framework that amortizes diffusion sampling by training a
physics-informed network to approximate reverse-time probability-flow dynamics,
reducing sampling to a single forward pass (NFE = 1). As a proof of concept, we
learn a direct map from a 3D standard normal to a non-Gaussian Gaussian Mixture
Model (GMM). PINGS preserves the target's distributional structure
(multi-bandwidth kernel $MMD^2 = 1.88 \times 10^{-2}$ with small errors in
mean, covariance, skewness, and excess kurtosis) and achieves constant-time
generation: $10^4$ samples in $16.54 \pm 0.56$ millisecond on an RTX 3090,
versus 468-843 millisecond for DPM-Solver (10/20) and 960 millisecond for DDIM
(50) under matched conditions. We also sanity-check the
PINN/automatic-differentiation pipeline on a damped harmonic oscillator,
obtaining MSEs down to $\mathcal{O}(10^{-5})$. Compared to fast but iterative
ODE solvers and direct-map families (Flow, Rectified-Flow, Consistency), PINGS
frames generative sampling as a PINN-style residual problem with endpoint
anchoring, yielding a white-box, differentiable map with NFE = 1. These
proof-of-concept results position PINGS as a promising route to fast,
function-based generative sampling with potential extensions to scientific
simulation (e.g., fast calorimetry).

</details>


### [79] [Designing MacPherson Suspension Architectures using Bayesian Optimization](https://arxiv.org/abs/2206.09022)
*Sinnu Susan Thomas,Jacopo Palandri,Mohsen Lakehal-ayat,Punarjay Chakravarty,Friedrich Wolf-Monheim,Matthew B. Blaschko*

Main category: cs.LG

TL;DR: Bayesian optimization system for automating engineering design process by optimizing compliance with target specifications, featuring novel two-tier convergence criteria and demonstrated on vehicle chassis design.


<details>
  <summary>Details</summary>
Motivation: Traditional engineering design is manual, time-consuming, and costly, requiring expert experience and extensive testing through simulation and physical prototyping.

Method: Bayesian optimization framework for computing generalized inverse of high-dimensional non-linear functions without requiring gradient information, with two-tier convergence criteria for optimal solutions or minimum-norm solutions.

Result: The approach is general, scalable, and efficient, successfully demonstrated on an industry-motivated vehicle chassis design problem using commercial discipline models.

Conclusion: The proposed Bayesian optimization system provides an effective automated solution for engineering design optimization that can be implemented using existing software packages and concepts.

Abstract: Engineering design is traditionally performed by hand: an expert makes design
proposals based on past experience, and these proposals are then tested for
compliance with certain target specifications. Testing for compliance is
performed first by computer simulation using what is called a discipline model.
Such a model can be implemented by a finite element analysis, multibody systems
approach, etc. Designs passing this simulation are then considered for physical
prototyping. The overall process may take months, and is a significant cost in
practice. We have developed a Bayesian optimization system for partially
automating this process by directly optimizing compliance with the target
specification with respect to the design parameters. The proposed method is a
general framework for computing a generalized inverse of a high-dimensional
non-linear function that does not require e.g. gradient information, which is
often unavailable from discipline models. We furthermore develop a two-tier
convergence criterion based on (i) convergence to a solution optimally
satisfying all specified design criteria, or (ii) convergence to a minimum-norm
solution. We demonstrate the proposed approach on a vehicle chassis design
problem motivated by an industry setting using a state-of-the-art commercial
discipline model. We show that the proposed approach is general, scalable, and
efficient, and that the novel convergence criteria can be implemented
straightforwardly based on existing concepts and subroutines in popular
Bayesian optimization software packages.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [80] [On the planar free elastic flow with small oscillation of curvature](https://arxiv.org/abs/2509.11129)
*Ben Andrews,Glen Wheeler*

Main category: math.DG

TL;DR: The paper improves asymptotic stability results for multiply-covered circles under planar free elastic flow by requiring closeness in terms of curvature scalar rather than its derivative, achieving sharp convergence rates.


<details>
  <summary>Details</summary>
Motivation: To strengthen the asymptotic stability results for multiply-covered circles under elastic flow by using a weaker closeness condition (curvature scalar instead of its derivative) while maintaining convergence.

Method: The authors analyze the planar free elastic flow starting from curves close to multiply-covered circles, using closeness measured by the curvature scalar itself rather than its derivative as in previous work.

Result: The improved result shows that topological ω-circles close to multiply-covered round circles in terms of curvature scalar are asymptotically stable under the flow, with the rescaled flow converging smoothly to stationary ω-circles at a sharp convergence rate.

Conclusion: The paper successfully demonstrates that a weaker closeness condition (using curvature scalar instead of its derivative) is sufficient for asymptotic stability of multiply-covered circles under elastic flow, achieving optimal convergence rates.

Abstract: The free elastic flow that begins at any curve exists for all time. If the
initial curve is an $\omega$-fold covered circle (``$\omega$-circle'') the
solution expands self-similarly. Very recently, Miura and the second author
showed that (topological) $\omega$-circles that are close to multiply-covered
round circles are asymptotically stable under the planar free elastic flow,
which means that upon rescaling the rescaled flow converges smoothly to the
stationary (in the rescaled setting) $\omega$-circle. Closeness in that work
was measured via the derivative of the curvature scalar. In the present paper,
we improve this by requiring closeness in terms of the curvature scalar itself.
The convergence rate we obtain is sharp.

</details>


### [81] [Uniqueness of tangent planes and (non-)removable singularities at infinity for collapsed translators](https://arxiv.org/abs/2509.11473)
*Eddygledson Souza Gama,Francisco Martín,Niels Martin Møller*

Main category: math.DG

TL;DR: Mean curvature flow translators can have non-removable singularities at infinity with jump discontinuities and persistent oscillation, but finite entropy, finite genus, embedded collapsed translators in R³ converge to unique plane collections as t→±∞.


<details>
  <summary>Details</summary>
Motivation: To understand the asymptotic behavior and singularity structure of mean curvature flow translators, particularly addressing non-removable singularities at infinity and oscillation phenomena in these geometric flows.

Method: Global analysis of quasilinear soliton equations with non-perturbative drifts using sharp non-standard elliptic decay estimates for drift Laplacian, improvements on Evans-Spruck and Ecker-Huisken estimates, and linking potential theory of Yukawa equation to heat flows with L∞-data on non-compact slice curves.

Result: Proved that finite entropy, finite genus, embedded collapsed translating solitons in R³ converge to uniquely determined plane collections as t→±∞, with structure theorem showing decomposition into standard regions asymptotic to planes or grim reaper cylinders.

Conclusion: The study provides a complete structure theorem for collapsed translators and classifies those of entropy two with empty limits as t→+∞, establishing fundamental convergence properties despite the presence of non-removable singularities and oscillation phenomena.

Abstract: We show that mean curvature flow translators may exhibit non-removable
singularities at infinity, due to jump discontinuities in their asymptotic
profiles, and that oscillation can persist so as to yield a continuum of
subsequential limit tangent planes. Nonetheless, we prove that as time $t\to\pm
\infty$, any finite entropy, finite genus, embedded, collapsed translating
soliton in $\mathbb{R}^3$ converges to a uniquely determined collection of
planes.
  This requires global analysis of quasilinear soliton equations with
non-perturbative drifts, which we analyze via sharp non-standard elliptic decay
estimates for the drift Laplacian, implying improvements on the Evans-Spruck
and Ecker-Huisken estimates in the soliton setting, and exploiting a link from
potential theory of the Yukawa equation to heat flows with $L^\infty$-data on
non-compact slice curves of these solitons.
  The structure theorem follows: such solitons decompose at infinity into
standard regions asymptotic to planes or grim reaper cylinders. As one
application, we classify collapsed translators of entropy two with empty limits
as $t\to +\infty$.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [82] [Lie symmetry analysis and similarity reductions for the tempered-fractional Keller Segel system](https://arxiv.org/abs/2509.11690)
*Ghorbanali Haghighatdoost,Mustafa Bazghandi*

Main category: math-ph

TL;DR: Lie symmetry analysis applied to tempered-fractional Keller-Segel system to handle nonlocal operators, derive symmetries, reduce PDEs to ODEs, and obtain exact solutions for understanding aggregation dynamics.


<details>
  <summary>Details</summary>
Motivation: To analyze the tempered-fractional Keller-Segel chemo-taxis model with anomalous diffusion using Lie symmetry methods to handle the nonlocal nature of tempered fractional operators and understand long-term behavior.

Method: Performed Lie symmetry analysis, derived full set of Lie point symmetries, identified optimal one-dimensional subalgebras, and reduced the tempered-fractional PDEs to ordinary differential equations.

Result: Obtained new exact solutions for the TFKS system, providing insights into aggregation dynamics and long-term behavior of the chemo-taxis model with anomalous diffusion.

Conclusion: The methodology successfully handles nonlocal tempered fractional operators and yields exact solutions, offering an applicable approach for other tempered fractional differential equations.

Abstract: We perform a Lie symmetry analysis on the tempered-fractional Keller Segel
(TFKS) system, a chemo-taxis model incorporating anomalous diffusion. A novel
approach is used to handle the nonlocal nature of tempered fractional
operators. By deriving the full set of Lie point symmetries and identifying the
optimal one-dimensional subalgebras, we reduce the TFKS PDEs to ordinary
differential equations (ODEs), yielding new exact solutions. These results
offer insights into the long-term behavior and aggregation dynamics of the TFKS
model and present a methodology applicable to other tempered fractional
differential equations.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [83] [Dynamic Decision Modeling for Viable Short and Long Term Production Policies: An HJB Approach](https://arxiv.org/abs/2509.12205)
*Achraf Bouhmady,Mustapha Serhani,Nadia Raissi*

Main category: math.OC

TL;DR: Mathematical framework for production system viability under constraints using HJB theory to analyze short-term tactical decisions and long-term strategic outcomes.


<details>
  <summary>Details</summary>
Motivation: To investigate the viability and reachability of production systems under constraints, providing managers with insights for maintaining production and inventory levels while considering quality and market demand.

Method: Developed a model incorporating pricing, quality investment, and advertising decisions. Used Hamilton-Jacobi-Bellman (HJB) theory with viscosity solutions to characterize capture basins and viability kernels without controllability assumptions. Conducted numerical studies and simulations.

Result: Constructed capture basins defining viable initial conditions, explored quality and demand dynamics for sustainable targets, and developed computational methods validated through simulations.

Conclusion: The framework enhances production system performance and resilience by linking rigorous mathematics with actionable solutions, providing practical tools for decision-makers to address operational challenges while achieving long-term sustainability goals.

Abstract: This study introduces a mathematical framework to investigate the viability
and reachability of production systems under constraints. We develop a model
that incorporates key decision variables, such as pricing policy, quality
investment, and advertising, to analyze short-term tactical decisions and
long-term strategic outcomes. In the short term, we constructed a capture basin
that defined the initial conditions under which production viability
constraints were satisfied within the target zone. In the long term, we explore
the dynamics of product quality and market demand to achieve and sustain the
desired target. The Hamilton-Jacobi-Bellman (HJB) theory characterizes the
capture basin and viability kernel using viscosity solutions of the HJB
equation. This approach, which avoids controllability assumptions, is well
suited to viability problems with specified targets. It provides managers with
insights into maintaining production and inventory levels within viable ranges
while considering product quality and evolving market demand. We numerically
studied the HJB equation to design and test computational methods that validate
the theoretical insights. Simulations offer practical tools for decision-makers
to address operational challenges while aligning with the long-term
sustainability goals. This study enhances the production system performance and
resilience by linking rigorous mathematics with actionable solutions.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [84] [Assessing the Limits of Graph Neural Networks for Vapor-Liquid Equilibrium Prediction: A Cryogenic Mixture Case Study](https://arxiv.org/abs/2509.10565)
*Aryan Gupta*

Main category: physics.chem-ph

TL;DR: Graph neural network trained as surrogate for cryogenic VLE calculations fails to produce valid equilibria and offers no speed advantage over traditional equation of state methods.


<details>
  <summary>Details</summary>
Motivation: Need fast and accurate thermophysical models for cryogenic mixture design, optimization, and control loops requiring vapor-liquid equilibrium calculations.

Method: Used structure-aware graph neural network (DimeNet++) trained on GERG-2008/CoolProp data with two-stage training: pretraining on residual Helmholtz energy followed by pressure fine-tuning with stability penalty.

Result: GNN interpolates single-phase properties reasonably but fails completely in VLE calculations - no equilibria accepted, shows jagged pressure paths and thermal instability in dense/cold regions, and has no speed advantage (tens of ms vs sub-ms for CoolProp).

Conclusion: The configured surrogate is not solver-ready for VLE and offers no runtime benefit; value is methodological in identifying failure modes and pointing to needed improvements like physics-informed training and better phase boundary coverage.

Abstract: Accurate and fast thermophysical models are needed to embed vapor-liquid
equilibrium (VLE) calculations in design, optimization, and control loops for
cryogenic mixtures. This study asks whether a structure-aware graph neural
network (GNN; DimeNet++) trained on GERG-2008/CoolProp data can act as a
practical surrogate for an equation of state (EoS). We generate a ternary
dataset over 90-200 K and pressures to 100 bar, curate it with a 15% density
filter (reducing 5,200 states to 1,516), and pair each state with a lightweight
molecular-dynamics snapshot to supply structural features. The model is trained
in two stages; pretraining on residual Helmholtz energy followed by pressure
fine-tuning with a stability penalty; and evaluated via single-phase
interpolation tests, solver-free derivative-quality diagnostics, an audited VLE
driver, and a latency benchmark. Within its regime, the GNN interpolates
single-phase properties reasonably well; however, the VLE driver accepts no GNN
equilibria on tested binaries (all plotted VLE points are CoolProp fallback or
the solver fails), and diagnostic probes reveal jagged P(V|T) paths and
thermal-stability flags concentrated in dense/cold regions, indicating
insufficient derivative smoothness/consistency for robust equilibrium solving.
An end-to-end timing comparison shows no single-phase speed advantage relative
to CoolProp (tens of milliseconds vs sub-millisecond). We conclude that, as
configured, the surrogate in this study is not solver-ready for VLE and offers
no runtime benefit; its value is methodological, delineating failure modes and
pointing to remedies such as physics-informed training signals and targeted
coverage near phase boundaries.

</details>


### [85] [Taylor series perspective on ab initio path integral Monte Carlo simulations with Fermi-Dirac statistics](https://arxiv.org/abs/2509.11317)
*Tobias Dornheim,Alexander Benedix Robles,Paul Hamann,Thomas M. Chuna,Pontus Svensson,Sebastian Schwalbe,Zhandos A. Moldabekov,Panagiotis Tolias,Jan Vorberger*

Main category: physics.chem-ph

TL;DR: The paper presents a new approach to mitigate the fermion sign problem in quantum simulations by reformulating the ξ-extrapolation method as a Taylor series expansion around distinguishable particles (ξ=0), enabling more efficient path integral Monte Carlo calculations.


<details>
  <summary>Details</summary>
Motivation: The fermion sign problem is a fundamental computational bottleneck in physics and quantum chemistry. The authors aim to improve upon existing ξ-extrapolation methods to better handle quantum degeneracy effects in simulations.

Method: The authors express the original ξ-extrapolation method as a truncated Taylor series expansion around ξ=0 (distinguishable particles limit). They derive new PIMC estimators to evaluate Taylor coefficients to arbitrary order and perform extensive PIMC simulations of warm dense electron gas.

Result: The approach provides important insights into the applicability of ξ-extrapolation for different quantum degeneracy levels through analysis of Taylor series radius of convergence. The direct evaluation of ξ-derivatives potentially eliminates the need for simulations at different ξ values.

Conclusion: This new perspective on the ξ-extrapolation method offers more efficient simulations by focusing computational resources on the most relevant regions of permutation space, potentially overcoming limitations of the original approach for strongly quantum-degenerate systems.

Abstract: The fermion sign problem constitutes a fundamental computational bottleneck
across a plethora of research fields in physics, quantum chemistry and related
disciplines. Recently, it has been suggested to alleviate the sign problem in
\emph{ab initio} path integral Molecular Dynamics and path integral Monte Carlo
(PIMC) calculations based on the simulation of fictitious identical particles
that are represented by a continuous quantum statistics variable $\xi$
[\textit{J.~Chem.~Phys.}~\textbf{157}, 094112 (2022)]. This idea facilitated a
host of applications including the interpretation of an x-ray scattering
experiment with strongly compressed beryllium at the National Ignition Facility
[\textit{Nature Commun.}~\textbf{16}, 5103 (2025)]. In the present work, we
express the original isothermal $\xi$-extrapolation method as a special case of
a truncated Taylor series expansion around the $\xi=0$ limit of distinguishable
particles. We derive new PIMC estimators that allow us to evaluate the Taylor
coefficients up to arbitrary order and we carry out extensive new PIMC
simulations of the warm dense electron gas to systematically analyze the sign
problem from this new perspective. This gives us important insights into the
applicability of the $\xi$-extrapolation method for different levels of quantum
degeneracy in terms of the Taylor series radius of convergence. Moreover, the
direct PIMC evaluation of the $\xi$-derivatives, in principle, removes the
necessity for simulations at different values of $\xi$ and can facilitate more
efficient simulations that are designed to maximize compute time in those
regions of the full permutation space that contribute most to the final Taylor
estimate of the fermionic expectation value of interest.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [86] [Predicting Structural Relaxation in Supercooled Small Molecules via Molecular Dynamics Simulations and Microscopic Theory](https://arxiv.org/abs/2509.12092)
*Anh D. Phan,Ngo T. Que,Nguyen T. T. Duyen*

Main category: cond-mat.soft

TL;DR: A theoretical framework combining molecular dynamics simulations and ECNLE theory to predict glass transition temperatures and relaxation dynamics of small organic molecules.


<details>
  <summary>Details</summary>
Motivation: Understanding glassy dynamics is critical for applications in pharmaceuticals, energy, and food preservation, requiring predictive methods for small organic glass-formers.

Method: Combines molecular dynamics simulations with Elastically Collective Nonlinear Langevin Equation (ECNLE) theory, using propanol, glucose, fructose, and trehalose as model systems to estimate Tg from stepwise cooling and volume-temperature analysis.

Result: Computed glass transition temperatures and relaxation times agree well with experimental data from prior works, demonstrating accurate predictions.

Conclusion: Provides a predictive and experimentally-independent route for characterizing glassy dynamics in molecular materials with good agreement to experimental results.

Abstract: Understanding and predicting the glassy dynamics of small organic molecules
is critical for applications ranging from pharmaceuticals to energy and food
preservation. In this work, we present a theoretical framework that combines
molecular dynamics simulations and Elastically Collective Nonlinear Langevin
Equation (ECNLE) theory to predict the structural relaxation behavior of small
organic glass-formers. By using propanol, glucose, fructose, and trehalose as
model systems, we estimate the glass transition temperature (Tg) from stepwise
cooling simulations and volume-temperature analysis. These computed Tg values
are then inserted into the ECNLE theory to calculate temperature-dependent
relaxation times and diffusion coefficients. Numerical results agree well with
experimental data in prior works. This approach provides a predictive and
experimentally-independent route for characterizing glassy dynamics in
molecular materials.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [87] [Practical Routing and Criticality in Large-Scale Quantum Communication Networks](https://arxiv.org/abs/2509.10908)
*Cillian Harney,Stefano Pirandola*

Main category: quant-ph

TL;DR: Quantum networks overcome point-to-point limitations but require effective design. Single-path routing fails for long distances, while multi-path routing boosts performance efficiently.


<details>
  <summary>Details</summary>
Motivation: Quantum networks need to overcome the fundamental rate-loss tradeoff in point-to-point quantum channels where rates decay exponentially with distance, requiring effective network design to guarantee high rates for future quantum internet success.

Method: Using random network models and practical end-to-end routing protocols, the study analyzes quantum networks with realistic descriptions. It compares single-path routing protocols and adapts novel algorithms for efficient multi-path routing to minimize quantum resource usage.

Result: The research reveals critical phenomena in large-scale quantum networks, showing that single-path routing protocols cannot achieve reliable rates over long distances. Multi-path routing algorithms significantly boost performance while efficiently managing quantum resources.

Conclusion: Multi-path routing is essential for effective quantum network design as it overcomes the limitations of single-path approaches, enabling reliable high-rate quantum communication over long distances in complex optical-fiber quantum networks.

Abstract: The efficacy of a communication network hinges upon both its physical
architecture and the protocols that are employed within it. In the context of
quantum communications, there exists a fundamental rate-loss tradeoff for
point-to-point quantum channels such that the rate for distributing
entanglement, secret keys, or quantum states decays exponentially with respect
to transmission distance. Quantum networks are the solution to overcome
point-to-point limitations, but they simultaneously invite a challenging open
question: How should quantum networks be designed to effectively and
efficiently guarantee high rates? Now that performance and physical topology
are inexorably linked, this question is not easy, but the answer is essential
for a future quantum internet to be successful. In this work, we offer crucial
insight into this open question for complex optical-fiber quantum networks.
Using realistic descriptions of quantum networks via random network models and
practical end-to-end routing protocols, we reveal critical phenomena associated
with large-scale quantum networks. Our work reveals the weaknesses of applying
single-path routing protocols within quantum networks, observing an inability
to achieve reliable rates over long distances. Adapting novel algorithms for
multi-path routing, we employ an efficient and practical multi-path routing
algorithm capable of boosting performance while minimizing costly quantum
resources.

</details>


### [88] [Requirements for Early Quantum Advantage and Quantum Utility in the Capacitated Vehicle Routing Problem](https://arxiv.org/abs/2509.11469)
*Chinonso Onah,Kristel Michielsen*

Main category: quant-ph

TL;DR: Framework for determining when CVRP can achieve quantum advantage, showing NISQ hardware unlikely to succeed even with best encodings. Provides decision metrics and reveals large gap between HOBO (7,685 qubits) and QUBO (200,000+ qubits) encodings.


<details>
  <summary>Details</summary>
Motivation: To establish a transparent framework for assessing quantum advantage potential in Capacitated Vehicle Routing Problems (CVRP) and determine feasibility on current NISQ hardware.

Method: Developed closed-form resource counts and three figures of merit (quantum feasibility point, qubit-feasibility line, gate-feasibility line) that place CVRP instances on a decision diagram. Compared direct QUBO mapping with space-efficient higher-order (HOBO) encoding.

Result: HOBO encoding requires only 7,685 qubits for Golden-5 benchmark, while QUBO encodings exceed 200,000 qubits. Framework provides unified go/no-go metric for any CVRP encoding and hardware profile.

Conclusion: Quantum advantage in CVRP unlikely on NISQ hardware even with best encodings. Would require innovative problem decomposition techniques to challenge classical heuristics.

Abstract: We introduce a transparent, encoding-agnostic framework for determining when
the Capacitated Vehicle Routing Problem (CVRP) can achieve early quantum
advantage. Our analysis shows this is unlikely on noisy intermediate scale
quantum (NISQ) hardware even in best case scenarios that use the most
qubit-efficient direct encodings. Closed-form resource counts, combined with
recent device benchmarks, yield three decisive go/no-go figures of merit: the
quantum feasibility point and the qubit- and gate-feasibility lines, which
place any CVRP instance on a single decision diagram. Contrasting a direct QUBO
mapping with a space-efficient higher-order (HOBO) encoding reveals a large
gap. Applied to early-advantage benchmarks such as Golden-5, our diagram shows
that HOBO circuits require only 7,685 qubits, whereas comparable QUBO encodings
still exceed 200,000 qubits. In addition to identifying candidate instances for
early quantum advantage in CVRP, the framework provides a unifying go/no-go
metric that ingests any CVRP encoding together with any hardware profile and
highlights when quantum devices could challenge classical heuristics. Quantum
advantage in CVRP would likely require innovative problem decomposition
techniques.

</details>


### [89] [Quantum Noise Tomography with Physics-Informed Neural Networks](https://arxiv.org/abs/2509.11911)
*Antonin Sulc*

Main category: quant-ph

TL;DR: PINN-based Lindblad tomography for efficient quantum noise characterization from sparse data


<details>
  <summary>Details</summary>
Motivation: Traditional quantum tomography methods are data-intensive and not scalable for characterizing environmental interactions in quantum systems

Method: Physics-Informed Neural Networks (PINNs) with Lindblad master equation embedded in loss function to learn quantum state evolution and infer dissipation parameters from sparse time-series measurements

Result: PINNs can reconstruct system dynamics and functional form of unknown noise parameters, providing sample-efficient and scalable quantum device characterization

Conclusion: The method creates a fully-differentiable digital twin of noisy quantum systems by learning their governing master equations

Abstract: Characterizing the environmental interactions of quantum systems is a
critical bottleneck in the development of robust quantum technologies.
Traditional tomographic methods are often data-intensive and struggle with
scalability. In this work, we introduce a novel framework for performing
Lindblad tomography using Physics-Informed Neural Networks (PINNs). By
embedding the Lindblad master equation directly into the neural network's loss
function, our approach simultaneously learns the quantum state's evolution and
infers the underlying dissipation parameters from sparse, time-series
measurement data. Our results show that PINNs can reconstruct both the system
dynamics and the functional form of unknown noise parameters, presenting a
sample-efficient and scalable solution for quantum device characterization.
Ultimately, our method produces a fully-differentiable digital twin of a noisy
quantum system by learning its governing master equation.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [90] [Role of magnetic shear distribution on the formation of eruptive flux ropes](https://arxiv.org/abs/2509.11416)
*Samrat Sen,Sushree S Nayak,Patrick Antolin*

Main category: astro-ph.SR

TL;DR: Strong magnetic shear in coronal arcades leads to spontaneous flux rope formation and eruption via magnetic reconnection, while weak shear prevents flux rope formation, highlighting the importance of coronal-scale diagnostics over foot point shear alone.


<details>
  <summary>Details</summary>
Motivation: To understand the mechanisms governing flux rope formation and eruption in solar transients, particularly the role of magnetic shear distribution in coronal arcades, which is not fully understood despite its crucial role in powering flares, jets, and coronal mass ejections.

Method: Magnetohydrodynamic simulations incorporating nonadiabatic effects (optically thin radiative losses, magnetic field-aligned thermal conduction, and spatially varying background heating) in a stratified solar atmosphere initialized with non-force-free sheared arcades. Two cases with different initial shear strengths were studied.

Result: Strong initial magnetic shear leads to spontaneous flux rope formation and eruption via magnetic reconnection driven by Lorentz force. Weaker shear does not form any flux ropes. Shear distribution indicates non-potentiality along arcades and identifies eruption-prone sites.

Conclusion: Magnetic shear distribution is critical for flux rope formation and eruption. Findings highlight limitations of relying solely on foot point shear and underscore the need for coronal-scale diagnostics, with implications for understanding eruptive onset conditions and interpreting coronal observations.

Abstract: Erupting flux ropes play crucial role in powering a wide range of solar
transients, including flares, jets, and coronal mass ejections. These events
are driven by the release of stored magnetic energy, facilitated by the shear
in the complex magnetic topologies. However, the mechanisms governing the
formation and eruption of flux ropes, particularly the role of magnetic shear
distribution in coronal arcades are not fully understood. We employ
magnetohydrodynamic simulations incorporating nonadiabatic effects of optically
thin radiative losses, magnetic field-aligned thermal conduction, and spatially
varying (steady) background heating, to realistically model the coronal
environment. A stratified solar atmosphere under gravity is initialized with a
non-force-free field comprising sheared arcades. We study two different cases
by varying the initial shear to analyze their resulting dynamics, and the
possibility of flux rope formation and eruptions. Our results show that strong
initial magnetic shear leads to spontaneous flux rope formation and eruption
via magnetic reconnection, driven by Lorentz force. The shear distribution
infers the non-potentiality distributed along arcades and demonstrates its
relevance in identifying sites prone to eruptive activity. The evolution of
mean shear and the relative strength between guide to reconnection fields
during the pre- and post-eruption phases are explored, with implications of
bulk heating for the ``hot onset'' phenomena in flares, and particle
acceleration. On the other hand, the weaker shear case does not lead to
formation of any flux ropes. Our findings highlight the limitations of relying
solely on foot point shear and underscore the need for coronal scale
diagnostics. These results are relevant for understanding eruptive onset
conditions and can promote a better interpretation of coronal observations from
current and future missions.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [91] [Chow and Rashevskii meet Sobolev](https://arxiv.org/abs/2509.11400)
*Sergey Kryzhevich,Eugene Stepanov,Dario Trevisan*

Main category: math.DS

TL;DR: Weak version of Chow-Rashevskii theorem for Sobolev-regular vector fields generating suitable flows as ODE solution selections for almost every initial datum.


<details>
  <summary>Details</summary>
Motivation: Extend the classical Chow-Rashevskii theorem (which establishes controllability conditions for smooth vector fields) to vector fields with only Sobolev regularity, which is important for applications involving less regular vector fields.

Method: Prove a weak version of the theorem by considering vector fields with Sobolev regularity that generate suitable flows as selections of solutions to the respective ordinary differential equations (ODEs), working with almost every initial datum rather than all initial data.

Result: Establishes controllability conditions for vector fields with Sobolev regularity, showing that under appropriate conditions, these less regular vector fields can still generate flows that satisfy the theorem's conclusions for almost every starting point.

Conclusion: The classical Chow-Rashevskii theorem can be extended to vector fields with Sobolev regularity, providing important theoretical foundations for control theory and geometric analysis involving non-smooth vector fields.

Abstract: We prove a weak version of the Chow-Rashevskii theorem for vector fields
having only Sobolev regularity and generating suitable flows as selections of
solutions to the respective ODEs, for a.e.\ initial datum.

</details>


<div id='math.CV'></div>

# math.CV [[Back]](#toc)

### [92] [Hölder Regularity of Dirichlet Problem For The Complex Monge-Ampère Equation](https://arxiv.org/abs/2509.11329)
*Yuxuan Hu,Bin Zhou*

Main category: math.CV

TL;DR: Global Hölder continuity of solutions to the complex Monge-Ampère equation on strictly pseudo-convex domains with Lp right-hand side and Hölder continuous boundary data


<details>
  <summary>Details</summary>
Motivation: To establish regularity results for the complex Monge-Ampère equation, which is fundamental in complex geometry and Kähler geometry, particularly for understanding the behavior of solutions under specific boundary and right-hand side conditions

Method: Studying the Dirichlet problem for the complex Monge-Ampère equation on strictly pseudo-convex domains in Cn or Hermitian manifolds, using analytical techniques to prove regularity

Result: Proved that when the right-hand side lies in Lp function space and boundary data are Hölder continuous, the solution exhibits global Hölder continuity

Conclusion: The complex Monge-Ampère equation admits globally Hölder continuous solutions under the specified conditions of Lp right-hand side and Hölder continuous boundary data on strictly pseudo-convex domains

Abstract: We study the Dirichlet problem for the complex Monge-Amp\`ere equation on a
strictly pseudo-convex domain in Cn or a Hermitian manifold. Under the
condition that the right-hand side lies in Lp function and the boundary data
are H\"older continuous, we prove the global H\"older continuity of the
solution.

</details>
