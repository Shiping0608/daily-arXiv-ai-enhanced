<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 19]
- [math.AP](#math.AP) [Total: 17]
- [physics.comp-ph](#physics.comp-ph) [Total: 5]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 4]
- [physics.space-ph](#physics.space-ph) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 3]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [stat.ML](#stat.ML) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [physics.atom-ph](#physics.atom-ph) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [cs.LG](#cs.LG) [Total: 2]
- [hep-ex](#hep-ex) [Total: 1]
- [math.SP](#math.SP) [Total: 1]
- [hep-lat](#hep-lat) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Generalized Non-Standard Finite Difference Method for Fractional PDEs on Non-Uniform Grids](https://arxiv.org/abs/2509.12209)
*Devank Mishra,Sheerin Kayenat,Amit K. Verma*

Main category: math.NA

TL;DR: A novel Generalized Non-Standard Finite Difference (GNSFD) scheme for solving fractional partial differential equations using optimization and fractional Taylor series with Caputo derivatives.


<details>
  <summary>Details</summary>
Motivation: To develop an accurate and computationally efficient numerical method for solving fractional partial differential equations, addressing limitations of existing approaches.

Method: Uses optimization and fractional Taylor series expansion with Caputo fractional derivatives, employing non-trivial denominator functions for time derivative discretization.

Result: Theoretical analysis confirms consistency, stability, and convergence. Comparisons show improved accuracy and computational efficiency over existing methods.

Conclusion: The proposed GNSFD scheme provides an effective numerical solution for fractional PDEs with proven theoretical properties and superior performance compared to current methods.

Abstract: This paper proposes a novel Generalized Non-Standard Finite Difference
(GNSFD) scheme for the numerical solution of a class of fractional partial
differential equations (FrPDEs). The formulation of the method is grounded in
optimization and leverages the fractional Taylor series (FrTS) expansion
associated with Caputo fractional derivatives (FrDs). To discretize the time
derivatives, the non-trivial denominator functions are utilized. The
theoretical analysis establishes the consistency, stability, and convergence of
the proposed scheme. Results are compared against existing methods to
substantiate the accuracy and computational efficiency of the approach.

</details>


### [2] [SRaFTE: Super-Resolution and Future Time Extrapolation for Time-Dependent PDEs](https://arxiv.org/abs/2509.12220)
*Hardeep Bassi,Yuanran Zhu,Erika Ye,Pu Ren,Alec Dektor,Harish S. Bhat,Chao Yang*

Main category: math.NA

TL;DR: SRaFTE is a two-phase framework that combines coarse grid solvers with neural operators to super-resolve and forecast fine grid dynamics for time-dependent PDEs, achieving reliable super-resolution and accurate long-term forecasts.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient method for modeling high-resolution spatiotemporal dynamics of PDEs by coupling learned super-resolution operators with coarse grid solvers, particularly when there's scale separation between coarse and fine resolutions.

Method: Two-phase learning: Phase 1 learns super-resolution map from coarse to fine solutions; Phase 2 embeds this operator in predictor-corrector loop with coarse solver to form surrogate fine grid propagator for future-time extrapolation.

Result: Benchmarked on heat equation, wave equation, and Navier-Stokes equations, SRaFTE provides reliable super-resolution and consistently accurate long-term forecasts across all three examples for new test data.

Conclusion: Coupling learned super-resolution operators with coarse grid solvers provides effective and efficient means for modeling high-resolution spatiotemporal dynamics, especially when pronounced scale separation exists between coarse and fine resolutions.

Abstract: We present SRaFTE (Super-Resolution and Future Time Extrapolation), a
two-phase learning framework that couples coarse grid solvers with neural
operators to super-resolve and forecast fine grid dynamics for time-dependent
partial differential equations (PDEs). In our construction, Phase 1 learns a
super-resolution map from coarse to fine solutions, while Phase 2 embeds this
operator in a predictor-corrector loop with the coarse solver, forming an
operator composition that serves as a surrogate fine grid propagator for
future-time extrapolation. We benchmark SRaFTE on three canonical
two-dimensional PDEs of increasing dynamical complexity: the heat equation, the
wave equation, and the incompressible Navier-Stokes equations. Compared to
well-known benchmarks, SRaFTE provides reliable super-resolution in Phase 1 and
delivers consistently accurate long-term forecasts in Phase 2 across all three
examples for new test data. Our results suggest that coupling a learned
super-resolution operator with a coarse grid solver provides an effective and
efficient means of modeling high-resolution spatiotemporal dynamics,
particularly when the dynamics of the PDEs at the coarse and fine resolutions
exhibit pronounced scale separation.

</details>


### [3] [Transmission Conditions for the Non-Overlapping Schwarz Coupling of Full Order and Operator Inference Models](https://arxiv.org/abs/2509.12228)
*Cameron Rodriguez,Irina Tezaur,Alejandro Mota,Anthony Gruber,Eric Parish,Christopher Wentland*

Main category: math.NA

TL;DR: This paper investigates non-overlapping Schwarz alternating method (NO-SAM) for coupling different computational models using Dirichlet-Neumann and Robin-Robin transmission conditions, with focus on coupling full order models with non-intrusive reduced order models via operator inference.


<details>
  <summary>Details</summary>
Motivation: To develop flexible and efficient domain decomposition methods for coupling different computational models (FOMs and ROMs) across independently meshed subdomains using non-overlapping Schwarz methods, building on previous overlapping SAM research.

Method: Formulated and assessed two NO-SAM variants with alternating Dirichlet-Neumann and Robin-Robin transmission conditions. Used operator inference (OpInf) for non-intrusive ROM construction. Tested on 1D linear elastic wave propagation benchmark.

Result: Robin-Robin coupling often yields faster convergence than alternating Dirichlet-Neumann, but improper parameter selection can cause interface oscillations. FOM-OpInf and OpInf-OpInf couplings with sufficient modal content can outperform FOM-FOM references in accuracy and efficiency.

Conclusion: NO-SAM shows potential for flexible, non-intrusive multi-model coupling across independently meshed subdomains, but requires careful interface condition design, especially for higher-dimensional applications.

Abstract: This work investigates transmission conditions for the domain
decomposition-based coupling of subdomain-local models using the
non-overlapping Schwarz alternating method (NO-SAM). Building on prior efforts
involving overlapping SAM (O-SAM), we formulate and assess two NO-SAM variants,
based on alternating Dirichlet-Neumann and Robin-Robin transmission conditions.
For the subdomain-local models, we consider a mix of full order models (FOMs)
and non-intrusive reduced order models (ROMs) constructed via an emerging model
reduction technique known as operator inference (OpInf). Of particular novelty
is the first application of NO-SAM to couple non-intrusive OpInf ROMs with each
other, and with FOMs. Numerical studies on a one-dimensional linear elastic
wave propagation benchmark problem demonstrate that transmission condition
choice and parameter tuning significantly impact convergence rate, accuracy,
and stability. Robin-Robin coupling often yields faster convergence than
alternating Dirichlet-Neumann, though improper parameter selection can induce
spurious oscillations at subdomain interfaces. For FOM-OpInf and OpInf-OpInf
couplings, sufficient modal content in the ROM basis improves accuracy and
mitigates instability, in some cases outperforming the coupled FOM-FOM
reference solutions in both accuracy and efficiency. These findings highlight
NO-SAM's potential for enabling flexible, non-intrusive, and efficient
multi-model coupling across independently meshed subdomains, while emphasizing
the need for careful interface condition design in higher-dimensional and
predictive settings.

</details>


### [4] [Bi-fidelity Interpolative Decomposition for Multimodal Data](https://arxiv.org/abs/2509.12243)
*Murray Cutforth,Tiffany Fan,Tony Zahtila,Alireza Doostan,Eric Darve*

Main category: math.NA

TL;DR: A new multi-modal interpolative decomposition method for bi-fidelity simulations that handles stochastic multi-modal responses, achieving 16% cost reduction while maintaining high correlation with parameter sensitivities.


<details>
  <summary>Details</summary>
Motivation: Standard bi-fidelity interpolative decomposition fails when model response is multi-modal with stochastic mode occupancy, as in laser-ignited methane-oxygen rocket combustor applications with bifurcation phenomena.

Method: Multi-modal interpolative decomposition method using bi-fidelity data, specifically designed to handle stochastic multi-modal responses where traditional approaches break down.

Result: The method approximates high-fidelity simulations for only 16% of the cost while maintaining high correlation (0.70-0.90) with parameter sensitivities.

Conclusion: The proposed multi-modal extension successfully addresses limitations of standard bi-fidelity approaches for complex engineering applications with stochastic multi-modal behavior.

Abstract: Multi-fidelity simulation is a widely used strategy to reduce the
computational cost of many-query numerical simulation tasks such as uncertainty
quantification, design space exploration, and design optimization. The reduced
basis approach based on bi-fidelity interpolative decomposition is one such
approach, which identifies a reduced basis, along with an interpolation rule in
that basis, from low-fidelity samples to approximate the corresponding
high-fidelity samples. However, as illustrated in the present study, when the
model response is multi-modal and mode occupancy is stochastic, the assumptions
underpinning this approach may not hold, thus leading to inaccurate estimates.
We introduce the multi-modal interpolative decomposition method using
bi-fidelity data, an extension tailored for this use case. Our work is
motivated by a complex engineering application: a laser-ignited methane-oxygen
rocket combustor evaluated over uncertain input parameters, exhibiting a
bifurcation-like phenomenon in some regions of parameter space. Unlike the
standard bi-fidelity interpolative decomposition approach, the proposed method
can approximate a dataset of high-fidelity simulations for 16\% of the cost,
while maintaining relatively high correlation (0.70--0.90) with parameter
sensitivities.

</details>


### [5] [A Variational Physics-Informed Neural Network Framework Using Petrov-Galerkin Method for Solving Singularly Perturbed Boundary Value Problems](https://arxiv.org/abs/2509.12271)
*Vijay Kumar,Gautam Singh*

Main category: math.NA

TL;DR: VPINN framework combines Petrov-Galerkin formulation with neural networks for solving 1D singularly perturbed BVPs and parabolic PDEs with small parameters, using neural network trial functions and hat function test spaces with interface penalties for stability.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of solving singularly perturbed boundary value problems and parabolic PDEs with small parameters, which often exhibit boundary layers and require specialized numerical methods for accurate resolution.

Method: Uses variational physics-informed neural networks with Petrov-Galerkin formulation, neural network trial functions, hat function test spaces, localized test functions with interface penalty terms, hard Dirichlet boundary constraints, and automatic differentiation for source terms.

Result: Numerical experiments show significantly improved accuracy in both L2 and maximum norms compared to standard VPINN approach for one-dimensional singularly perturbed differential equations.

Conclusion: The proposed VPINN framework with Petrov-Galerkin formulation and specialized test functions effectively handles singularly perturbed problems with improved accuracy and stability over conventional methods.

Abstract: This work proposes a Variational Physics-Informed Neural Network (VPINN)
framework that integrates the Petrov-Galerkin formulation with deep neural
networks (DNNs) for solving one-dimensional singularly perturbed boundary value
problems (BVPs) and parabolic partial differential equations (PDEs) involving
one or two small parameters. The method adopts a nonlinear approximation in
which the trial space is defined by neural network functions, while the test
space is constructed from hat functions. The weak formulation is constructed
using localized test functions, with interface penalty terms introduced to
enhance numerical stability and accurately capture boundary layers. Dirichlet
boundary conditions are imposed via hard constraints, and source terms are
computed using automatic differentiation. Numerical experiments on benchmark
problems demonstrate the effectiveness of the proposed method, showing
significantly improved accuracy in both the $L_2$ and maximum norms compared to
the standard VPINN approach for one-dimensional singularly perturbed
differential equations (SPDEs).

</details>


### [6] [Dynamical symmetry breaking described by cubic nonlinear Klein-Gordon equations](https://arxiv.org/abs/2509.12272)
*Yasuhiro Takei,Yoritaka Iwata*

Main category: math.NA

TL;DR: Study of dynamical symmetry breaking through breather solutions in nonlinear hyperbolic equations using high-precision numerical methods


<details>
  <summary>Details</summary>
Motivation: To understand dynamical symmetry breaking associated with breather solutions, which requires high-precision numerical schemes due to error accumulation and instability issues in nonlinear hyperbolic equations

Method: Implicit Runge-Kutta method for time integration and Fourier spectral decomposition for spatial discretization to achieve high precision and stability

Result: Clarified the relationship between velocity, mass, and perturbation amplitude for breather solutions, identifying conditions for state transitions

Conclusion: Successfully established transition conditions between different states through high-precision numerical analysis of breather solutions in nonlinear hyperbolic equations

Abstract: The dynamical symmetry breaking associated with the existence and
non-existence of breather solutions is studied. Here, nonlinear hyperbolic
evolution equations are calculated using a high-precision numerical scheme. %%%
First, for clarifying the dynamical symmetry breaking, it is necessary to use a
sufficiently high-precision scheme in the time-dependent framework. Second, the
error of numerical calculations is generally more easily accumulated for
calculating hyperbolic equations rather than parabolic equations. Third,
numerical calculations become easily unstable for nonlinear cases. Our strategy
for the high-precision and stable scheme is to implement the implicit
Runge-Kutta method for time, and the Fourier spectral decomposition for space.
%%% In this paper, focusing on the breather solutions, the relationship between
the velocity, mass, and the amplitude of the perturbation is clarified. As a
result, the conditions for transitioning from one state to another are
clarified.

</details>


### [7] [The Green's Function on Rhombic Flat Tori](https://arxiv.org/abs/2509.12299)
*A. E. D. Castillo,G. A. Lobos,V. Ramos Batista*

Main category: math.NA

TL;DR: Numerical computation of Green's function for flat rhombic tori with 4-decimal precision using elliptic function theory and complex integration.


<details>
  <summary>Details</summary>
Motivation: To accurately compute the Green's function for flat rhombic tori, which is fundamental for solving Poisson equations and studying harmonic analysis on these surfaces.

Method: Uses Legendre Relation, Weierstraß P-function properties, and complex integration routines developed by H. Karcher, including symmetric P-Weierstraß function for simplified elliptic function computation.

Result: Achieves numerical values with precision up to the fourth decimal place for the Green's function on unit area tori with zero mean.

Conclusion: The adopted strategies and mathematical tools enable reliable and precise computation of Green's functions for flat rhombic tori, with error control ensuring numerical accuracy.

Abstract: We obtain the Green's function $G$ for any flat rhombic torus $T$, always
with numerical values of significant digits up to the fourth decimal place
(noting that $G$ is unique for $|T|=1$ and $\int_TGdA=0$). This precision is
guaranteed by the strategies we adopt, which include theorems such as the
Legendre Relation, properties of the Weierstra\ss\,P-Function, and also the
algorithmic control of numerical errors. Our code uses complex integration
routines developed by H. Karcher, who also introduced the symmetric
P-Weierstra\ss\,function, and these resources simplify the computation of
elliptic functions considerably.

</details>


### [8] [Data-driven balanced truncation for linear systems with quadratic outputs](https://arxiv.org/abs/2509.12393)
*Reetish Padhi,Ion Victor Gosea,Igor Pontes Duff,Serkan Gugercin*

Main category: math.NA

TL;DR: Non-intrusive quadrature-based balanced truncation method extended to linear systems with quadratic outputs, using sampled impulse responses and transfer functions to achieve projection-based quality without intrusive methods.


<details>
  <summary>Details</summary>
Motivation: Extend QuadBT framework from standard linear systems with linear outputs to handle quadratic outputs, maintaining non-intrusive approach while preserving approximation quality.

Method: Quadrature-based representation of infinite Gramians using sampled extended impulse responses and their derivatives (or corresponding transfer functions) to construct reduced-order models.

Result: The method achieves approximation quality comparable to intrusive projection-based balanced truncation, validated through numerical examples.

Conclusion: The proposed QuadBT framework successfully extends balanced truncation to quadratic output systems while maintaining non-intrusive, data-driven advantages with quality matching intrusive methods.

Abstract: We develop the framework for a non-intrusive, quadrature-based method for
approximate balanced truncation (QuadBT) of linear systems with quadratic
outputs, thus extending the applicability of QuadBT, which was originally
designed for data-driven balanced truncation of standard linear systems with
linear outputs only. The new approach makes use of the time-domain and
frequency-domain quadrature-based representation of the system's infinite
Gramians, only implicitly. We show that by sampling solely the extended impulse
responses of the original system and their derivatives (or the corresponding
transfer functions), we construct a reduced-order model that mimics the
approximation quality of the intrusive (projection-based) balanced truncation.
We validate the proposed framework on a numerical example.

</details>


### [9] [Scattering theory for Stokes flow in complex branched structures](https://arxiv.org/abs/2509.12500)
*Haiyang Wang,Fredrik Fryklund,Samuel Potter,Leslie Greengard*

Main category: math.NA

TL;DR: A scattering theory framework for efficient simulation of viscous flow in branched structures by decomposing networks into components with precomputed scattering matrices.


<details>
  <summary>Details</summary>
Motivation: Direct numerical simulation of viscous flow in complex multi-scale branched geometries is computationally intensive, requiring a more efficient approach.

Method: Decompose networks into components connected by straight channels, compute high-order accurate scattering matrices for each component using boundary integral equations, then assemble precomputed components into arbitrary branched structures.

Result: The method matches full-domain solver accuracy while requiring only a fraction of computational effort, is modular, and has negligible cost after precomputation.

Conclusion: This scattering theory framework provides the first full-fidelity solver that exploits the return to Poiseuille flow phenomenon, dramatically reducing computational costs for viscous flow simulation in branched structures.

Abstract: Slow, viscous flow in branched structures arises in many biological and
engineering settings. Direct numerical simulation of flow in such complicated
multi-scale geometry, however, is a computationally intensive task. We propose
a scattering theory framework that dramatically reduces this cost by
decomposing networks into components connected by short straight channels.
Exploiting the phenomenon of rapid return to Poiseuille flow (Saint-Venant's
principle in the context of elasticity), we compute a high-order accurate
scattering matrix for each component via boundary integral equations. These
precomputed components can then be assembled into arbitrary branched
structures, and the precomputed local solutions on each component can be
assembled into an accurate global solution. The method is modular, has
negligible cost, and appears to be the first full-fidelity solver that makes
use of the return to Poiseuille flow phenomenon. In our two-dimensional
examples, it matches the accuracy of full-domain solvers while requiring only a
fraction of the computational effort.

</details>


### [10] [An efficient splitting iteration for a CDA-accelerated solver for incompressible flow problems](https://arxiv.org/abs/2509.12547)
*Victoria L. Fisher,Leo G. Rebholz,Duygu Vargun*

Main category: math.NA

TL;DR: Efficient splitting iteration method for solving incompressible steady Navier-Stokes equations using continuous data assimilation, providing faster convergence and enabling solver success in challenging parameter regimes.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient solver for incompressible steady Navier-Stokes equations that can incorporate partial (possibly noisy) solution data to accelerate convergence and enable solution in parameter regimes where standard methods fail.

Method: Proposes a Picard-type solver with continuous data assimilation (CDA) using an algebraic splitting of Yosida-type for easier linear solves, combined with incremental pressure and grad-div stabilization for accuracy and consistency.

Result: CDA scales the Lipschitz constant by H^{1/2} (where H is data spacing), accelerating convergence and enabling solver success in challenging regimes. Numerical tests show nearly identical iteration counts with efficiency gains.

Conclusion: The proposed efficient splitting iteration with CDA provides significant computational efficiency improvements without sacrificing convergence rate, making it particularly valuable for solving Navier-Stokes equations in difficult parameter regimes.

Abstract: We propose, analyze, and test an efficient splitting iteration for solving
the incompressible, steady Navier-Stokes equations in the setting where partial
solution data is known. The (possibly noisy) solution data is incorporated into
a Picard-type solver via continuous data assimilation (CDA). Efficiency is
gained over the usual Picard iteration through an algebraic splitting of
Yosida-type that produces easier linear solves, and accuracy/consistency is
shown to be maintained through the use of an incremental pressure and grad-div
stabilization. We prove that CDA scales the Lipschitz constant of the
associated fixed point operator by $H^{1/2}$, where $H$ is the characteristic
spacing of the known solution data. This implies that CDA accelerates an
already converging solver (and the more data, the more acceleration) and
enables convergence of solvers in parameter regimes where the solver would fail
(and the more data, the larger the parameter regime). Numerical tests
illustrate the theory on several benchmark test problems and show that the
proposed efficient solver gives nearly identical results in terms of number of
iterations to converge; in other words, the proposed solver gives an efficiency
gain with no loss in convergence rate.

</details>


### [11] [An Immersed $C^0$ Interior Penalty Method for Biharmonic Interface Problems](https://arxiv.org/abs/2509.12555)
*Yuan Chen,Xu Zhang*

Main category: math.NA

TL;DR: An immersed C^0 interior penalty method for 2D biharmonic interface problems on unfitted meshes using high-order immersed finite element spaces constructed via least-squares.


<details>
  <summary>Details</summary>
Motivation: To solve two-dimensional biharmonic interface problems efficiently on unfitted meshes, which requires accommodating complex interface conditions and maintaining optimal convergence properties.

Method: Construct high-order immersed finite element spaces in least-squares sense, incorporate into modified C^0 interior penalty scheme with additional penalty terms on interface segments, and prove well-posedness.

Result: Numerical experiments with various interface geometries confirm optimal convergence in L^2, H^1 and H^2 norms, demonstrating the method's effectiveness.

Conclusion: The proposed immersed C^0 interior penalty method with high-order IFE spaces provides an effective approach for solving biharmonic interface problems on unfitted meshes with optimal convergence rates.

Abstract: In this paper, we introduce an immersed $C^0$ interior penalty method for
solving two-dimensional biharmonic interface problems on unfitted meshes. To
accommodate the biharmonic interface conditions, high-order immersed finite
element (IFE) spaces are constructed in the least-squares sense. We establish
key properties of these spaces including unisolvency and partition of unity
are, and verify their optimal approximation capability. These spaces are
further incorporated into a modified $C^0$ interior penalty scheme with
additional penalty terms on interface segments. The well-posedness of the
discrete solution is proved. Numerical experiments with various interface
geometries confirm optimal convergence of the proposed method in $L^2$, $H^1$
and $H^2$ norms.

</details>


### [12] [Neural Network Localized Orthogonal Decomposition for Numerical Homogenization of Diffusion Operators with Random Coefficients](https://arxiv.org/abs/2509.12896)
*Fabian Kröpfl,Daniel Peterseim,Elisabeth Ullmann*

Main category: math.NA

TL;DR: Neural network-enhanced surrogate modeling for diffusion problems with spatially varying random field coefficients, using numerical homogenization and neural networks to map fine-scale coefficients to coarse-scale surrogates efficiently.


<details>
  <summary>Details</summary>
Motivation: To overcome computational bottlenecks in diffusion problems with spatially varying random field coefficients by developing fast and efficient compression methods for large ensembles of random coefficients.

Method: Combines numerical homogenization (compressing fine-scale coefficients into coarse-scale surrogates) with neural network training to map fine-scale coefficient samples to effective coarse-scale information, enabling accurate surrogate construction at target resolution.

Result: The approach enables fast and efficient compression of new coefficient realizations, ensuring reliable coarse models and supporting scalable computations for large ensembles of random coefficients.

Conclusion: The method demonstrates efficacy through systematic numerical experiments for lognormal diffusion coefficients and hierarchical Gaussian random fields with random correlation lengths, emphasizing its ability to handle coefficient contrast effectively.

Abstract: This paper presents a neural network--enhanced surrogate modeling approach
for diffusion problems with spatially varying random field coefficients. The
method builds on numerical homogenization, which compresses fine-scale
coefficients into coarse-scale surrogates without requiring periodicity. To
overcome computational bottlenecks, we train a neural network to map fine-scale
coefficient samples to effective coarse-scale information, enabling the
construction of accurate surrogates at the target resolution. This framework
allows for the fast and efficient compression of new coefficient realizations,
thereby ensuring reliable coarse models and supporting scalable computations
for large ensembles of random coefficients. We demonstrate the efficacy of our
approach through systematic numerical experiments for two classes of
coefficients, emphasizing the influence of coefficient contrast: (i) lognormal
diffusion coefficients, a standard model for uncertain subsurface structures in
geophysics, and (ii) hierarchical Gaussian random fields with random
correlation lengths.

</details>


### [13] [Finite element method for a constant time delay subdiffusion equation with Riemann-Liouville fractional derivative](https://arxiv.org/abs/2509.13052)
*Weiping Bu,Chen Nie,Weizhi Liao*

Main category: math.NA

TL;DR: Numerical solution of subdiffusion equation with time delay using finite element method with L1 formula and fractional right rectangular rule on symmetric graded time mesh, analyzing stability and convergence under different mesh scenarios.


<details>
  <summary>Details</summary>
Motivation: To develop accurate numerical methods for solving subdiffusion equations with time delay and Riemann-Liouville fractional derivatives, addressing challenges posed by solution singularities at t=0 and τ.

Method: Fully discrete finite element scheme using L1 formula for Caputo derivative approximation and fractional right rectangular rule for Riemann-Liouville integral discretization on symmetric graded time mesh.

Result: Established unconditional stability and local time error estimates for uniform mesh, and stability with globally optimal time error estimates for symmetric graded mesh through discrete fractional Gronwall inequality.

Conclusion: The proposed numerical scheme effectively handles solution singularities and provides reliable results for subdiffusion equations with time delay, as validated by numerical tests.

Abstract: This work considers to numerically solve a subdiffusion equation involving
constant time delay $\tau$ and Riemann-Liouville fractional derivative. First,
a fully discrete finite element scheme is developed for the considered problem
under the symmetric graded time mesh, where the Caputo fractional derivative is
approximated via the L1 formula, while the Riemann-Liouville integral is
discretized using the fractional right rectangular rule. Under the assumption
that the exact solution has low regularities at $t=0$ and $\tau$, the local
truncation errors of both the L1 formula and the fractional right rectangular
rule are analyzed. It is worth noting that, by setting the mesh parameter
$r=1$, the symmetric graded time mesh will degenerate to a uniform mesh.
Consequently, we proceed to discuss the stability and convergence of the
proposed numerical scheme under two scenarios. For the uniform time mesh, by
introducing a discrete sequence $\{P_k\}$, the unconditional stability and
local time error estimate for the developed scheme is established. Conversely,
on the symmetric graded time mesh, through the introduction of a discrete
fractional Gronwall inequality, the stability and globally optimal time error
estimate can be obtained. Finally, some numerical tests are presented to
validate the theoretical results.

</details>


### [14] [Variational data assimilation for the wave equation in heterogeneous media: Numerical investigation of stability](https://arxiv.org/abs/2509.13108)
*Erik Burman,Janosch Preuss,Tim van Beeck*

Main category: math.NA

TL;DR: Numerical methods for wave equation unique continuation in homogeneous media extend to discontinuous media with jump discontinuities, though computational demands increase significantly.


<details>
  <summary>Details</summary>
Motivation: To investigate whether existing numerical methods for solving the unique continuation problem in homogeneous wave equations can be extended to handle media with jump discontinuities while maintaining optimal convergence properties.

Method: The authors conducted numerical experiments to test whether the Lipschitz stability and optimal convergence properties observed in homogeneous media under geometric control conditions carry over to discontinuous media cases.

Result: Numerical experiments suggest positive results - the methods appear to work for discontinuous media, but computations become far more demanding compared to the homogeneous case.

Conclusion: While the numerical methods for unique continuation in wave equations can be extended to handle jump discontinuities in the medium, the presence of such discontinuities significantly increases computational complexity and resource requirements.

Abstract: In recent years, several numerical methods for solving the unique
continuation problem for the wave equation in a homogeneous medium with given
data on the lateral boundary of the space-time cylinder have been proposed.
This problem enjoys Lipschitz stability if the geometric control condition is
fulfilled, which allows devising optimally convergent numerical methods. In
this article, we investigate whether these results carry over to the case in
which the medium exhibits a jump discontinuity. Our numerical experiments
suggest a positive answer. However, we also observe that the presence of
discontinuities in the medium renders the computations far more demanding than
in the homogeneous case.

</details>


### [15] [Elementary differentials from multi-indices to rooted trees](https://arxiv.org/abs/2509.13118)
*Yvain Bruned,Paul Laubie*

Main category: math.NA

TL;DR: No intermediate combinatorial sets exist between multi-indices and rooted trees for encoding elementary differentials in dimensions d≠1, showing naive approaches won't work.


<details>
  <summary>Details</summary>
Motivation: To find intermediate combinatorial sets between multi-indices and rooted trees that could more efficiently describe numerical schemes and expand solutions of singular SPDEs.

Method: Investigated the existence of combinatorial sets lying between multi-indices and rooted trees, focusing on encoding elementary differentials while maintaining compatibility with both existing structures.

Result: Negative result - no such intermediate combinatorial sets exist for dimensions d≠1 aside from rooted trees themselves.

Conclusion: While this doesn't completely close the debate, it demonstrates that intermediate combinatorial sets cannot be obtained through naive or natural approaches, suggesting more sophisticated methods would be needed.

Abstract: Rooted trees are essential for describing numerical schemes via the so-called
B-series. They have also been used extensively in rough analysis for expanding
solutions of singular Stochastic Partial Differential Equations (SPDEs). When
one considers scalar-valued equations, the most efficient combinatorial set is
multi-indices. In this paper, we investigate the existence of intermediate
combinatorial sets that will lie between multi-indices and rooted trees. We
provide a negative result stating that there is no combinatorial set encoding
elementary differentials in dimension $d\neq 1$, and compatible with the rooted
trees and the multi-indices aside from the rooted trees. This does not close
the debate of the existence of such combinatorial sets, but it shows that it
cannot be obtained via a naive and natural approach.

</details>


### [16] [Geometry, Energy and Sensitivity in Stochastic Proton Dynamics](https://arxiv.org/abs/2509.13223)
*Veronika Chronholm,Tristan Pryer*

Main category: math.NA

TL;DR: Numerical schemes and sensitivity methods for proton transport models with energy loss, range straggling, and angular diffusion, featuring positivity-preserving logarithmic Milstein scheme and Lie-group integrator.


<details>
  <summary>Details</summary>
Motivation: To develop accurate and stable numerical methods for stochastic proton transport models that maintain geometric invariants and enable reliable dose sensitivity estimation.

Method: Logarithmic Milstein scheme for energy equation (positivity guarantee, strong order one convergence), Lie-group integrator for angular dynamics, combined approach preserving geometric invariants, and regularized path-dependent functional for dose deposition.

Result: Numerical experiments confirm expected convergence rates and provide stable estimates of dose sensitivities.

Conclusion: The proposed schemes successfully achieve the desired convergence properties while maintaining system invariants and enabling consistent, implementable sensitivity estimation for proton transport modeling.

Abstract: We develop numerical schemes and sensitivity methods for stochastic models of
proton transport that couple energy loss, range straggling and angular
diffusion. For the energy equation we introduce a logarithmic Milstein scheme
that guarantees positivity and achieves strong order one convergence. For the
angular dynamics we construct a Lie-group integrator. The combined method
maintains the natural geometric invariants of the system.
  We formulate dose deposition as a regularised path-dependent functional,
obtaining a pathwise sensitivity estimator that is consistent and
implementable. Numerical experiments confirm that the proposed schemes achieve
the expected convergence rates and provide stable estimates of dose
sensitivities.

</details>


### [17] [A Tensor Train-Based Isogeometric Solver for Large-Scale 3D Poisson Problems on Complex Geometries](https://arxiv.org/abs/2509.13224)
*Quoc Thai Tran,Duc P. Truong,Kim Ø. Rasmussen,Boian Alexandrov*

Main category: math.NA

TL;DR: TT-IGA: A 3D tensor train-based isogeometric analysis framework that compresses PDE operators for efficient computation on complex geometries while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: To enable efficient solution of PDEs on complex 3D geometries by combining tensor train compression with isogeometric analysis, overcoming limitations of previous low-rank methods that required structured domains.

Method: Reformulates IGA discrete operators into tensor train (TT) format, creating low-rank TT representations for both geometry mapping and PDE discretization to handle general 3D geometries.

Result: Achieves substantial reductions in memory usage and computational cost for solving 3D Poisson equation without compromising solution quality.

Conclusion: The TT-IGA framework successfully enables efficient PDE solving on complex 3D geometries through tensor train compression while retaining geometric flexibility and accuracy.

Abstract: We introduce a three-dimensional (3D) fully tensor train (TT)-assembled
isogeometric analysis (IGA) framework, TT-IGA, for solving partial differential
equations (PDEs) on complex geometries. Our method reformulates IGA discrete
operators into TT format, enabling efficient compression and computation while
retaining geometric flexibility and accuracy. Unlike previous low-rank
approaches that typically rely on structured domains, our framework
accommodates general 3D geometries through low-rank TT representations of both
the geometry mapping and the PDE discretization. We demonstrate the
effectiveness of the proposed TT-IGA framework on the 3D Poisson equation,
achieving substantial reductions in memory usage and computational cost without
compromising solution quality.

</details>


### [18] [A generalized reduction scheme for the Stochastic Weighted Particle Method](https://arxiv.org/abs/2509.13259)
*Matthew Goeckner,Donovan Harcey,Rainier Q Pederson,Axel Niyonzima,John Zweck*

Main category: math.NA

TL;DR: A general framework for particle reduction in Stochastic Weighted Particle Method that preserves moments up to order 3, with analysis showing trade-offs between preserving higher-order moments and tail functionals.


<details>
  <summary>Details</summary>
Motivation: To reduce computational cost in SWPM simulations by developing particle reduction schemes that preserve key statistical moments of the particle distribution.

Method: Developed a framework using linear equations to preserve prescribed moments during particle reduction, specifically applied to preserve all moments up to third order.

Result: Numerical simulations verified the scheme and revealed an unexpected trade-off between preserving higher-order moments and tail functionals of the distribution.

Conclusion: The framework successfully enables moment-preserving particle reduction but shows limitations in simultaneously preserving both high-order moments and tail characteristics.

Abstract: The Stochastic Weighted Particle Method (SWPM) of Rjasanow and Wagner is a
generalization of the Direct Simulation Monte Carlo method for computing the
probability density function of the velocities of a system of interacting
particles for applications that include rarefied gas dynamics and plasma
processing systems. Key components of a SWPM simulation are a particle grouping
technique and particle reduction scheme. These are periodically applied to
reduce the computational cost of simulations due to the gradual increase in the
number of stochastic particles. A general framework for designing particle
reduction schemes is introduced that enforces the preservation of a prescribed
set of moments of the distribution through the construction and explicit
solution of a system of linear equations for particle weights in terms of
particle velocities and the moments to be preserved. This framework is applied
to preserve all moments of the distribution up to order three. Numerical
simulations are performed to verify the scheme and quantify the degree to which
even higher-order moments and tail functionals are preserved. These results
reveal an unexpected trade off between the preservation of these higher-order
moments and tail functionals.

</details>


### [19] [Forward Euler for Wasserstein Gradient Flows: Breakdown and Regularization](https://arxiv.org/abs/2509.13260)
*Yewei Xu,Qin Li*

Main category: math.NA

TL;DR: Forward-Euler discretization fails for Wasserstein gradient flows with KL divergence due to loss of regularity, but regularization restores convergence.


<details>
  <summary>Details</summary>
Motivation: Wasserstein gradient flows are important for optimization over probability measures, but standard numerical approaches like forward-Euler can fail even in simple cases like KL divergence minimization.

Method: Analysis of forward-Euler time discretization for Wasserstein gradient flows with KL divergence, identification of regularity loss issue, and proposal of functional regularization to restore smoothness.

Result: Forward-Euler discretization does not converge to the gradient flow for KL divergence minimization due to loss of regularity, but with proper regularization, forward-Euler becomes a viable solver that converges to the global minimizer.

Conclusion: Regularization is necessary to make forward-Euler a convergent numerical scheme for Wasserstein gradient flows with KL divergence, addressing the fundamental issue of lost smoothness in discretization.

Abstract: Wasserstein gradient flows have become a central tool for optimization
problems over probability measures. A natural numerical approach is
forward-Euler time discretization. We show, however, that even in the simple
case where the energy functional is the Kullback-Leibler (KL) divergence
against a smooth target density, forward-Euler can fail dramatically: the
scheme does not converge to the gradient flow, despite the fact that the first
variation $\nabla\frac{\delta F}{\delta\rho}$ remains formally well defined at
every step. We identify the root cause as a loss of regularity induced by the
discretization, and prove that a suitable regularization of the functional
restores the necessary smoothness, making forward-Euler a viable solver that
converges in discrete time to the global minimizer.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [20] [Blow-up exponents and a semilinear elliptic equation for the fractional Laplacian on hyperbolic spaces](https://arxiv.org/abs/2509.12349)
*Tommaso Bruno,Effie Papageorgiou*

Main category: math.AP

TL;DR: This paper determines the Fujita exponent for fractional heat equations and proves existence of solutions for semilinear fractional elliptic equations on hyperbolic spaces, using novel fractional Poincaré inequalities and Sobolev space theory.


<details>
  <summary>Details</summary>
Motivation: To establish precise conditions for existence of global solutions to fractional heat equations and semilinear fractional elliptic equations on hyperbolic spaces, addressing gaps in understanding nonlinear fractional PDEs on non-compact manifolds.

Method: Develops a new fractional Poincaré-type inequality using a novel scale of L² fractional Sobolev spaces, proves Rellich-Kondrachov-like compact embedding for radial functions, and connects the elliptic and parabolic problems through analytical techniques.

Result: Determined that nontrivial positive global solutions exist for the fractional heat equation if and only if γ ≥ 1 + β/λ₀^σ. Proved existence of bounded finite-energy solutions for the semilinear elliptic equation for 0 ≤ λ ≤ λ₀ and 1 < γ < (n+2σ)/(n-2σ).

Conclusion: The paper establishes fundamental existence criteria for fractional nonlinear PDEs on hyperbolic spaces through innovative fractional Sobolev space theory and Poincaré inequalities, with applications extending to Riemannian symmetric spaces of non-compact type.

Abstract: Let $\mathbb{H}^n$ be the $n$-dimensional real hyperbolic space, $\Delta$ its
nonnegative Laplace--Beltrami operator whose bottom of the spectrum we denote
by $\lambda_{0}$, and $\sigma \in (0,1)$.
  The aim of this paper is twofold. On the one hand, we determine the Fujita
exponent for the fractional heat equation \[\partial_{t} u + \Delta^{\sigma}u =
e^{\beta t}|u|^{\gamma-1}u,\] by proving that nontrivial positive global
solutions exist if and only if $\gamma\geq 1 + \beta/ \lambda_{0}^{\sigma}$. On
the other hand, we prove the existence of non-negative, bounded and finite
energy solutions of the semilinear fractional elliptic equation \[
  \Delta^{\sigma} v - \lambda^{\sigma} v - v^{\gamma}=0 \] for $0\leq \lambda
\leq \lambda_{0}$ and $1<\gamma< \frac{n+2\sigma}{n-2\sigma}$. The two problems
are known to be connected and the latter, aside from its independent interest,
is actually instrumental to the former.
  \smallskip
  At the core of our results stands a novel fractional Poincar\'e-type
inequality expressed in terms of a new scale of $L^{2}$ fractional Sobolev
spaces, which sharpens those known so far, and which holds more generally on
Riemannian symmetric spaces of non-compact type. We also establish an
associated Rellich--Kondrachov-like compact embedding theorem for radial
functions, along with other related properties.

</details>


### [21] [Multiscale modelling, analysis and simulation of cancer invasion mediated by bound and soluble enzymes](https://arxiv.org/abs/2509.12413)
*Mariya Ptashnyk,Chandrasekhar Venkataraman*

Main category: math.AP

TL;DR: A mathematical model for cancer cell invasion through ECM degradation by membrane-bound and soluble enzymes, with homogenization from microscopic to macroscopic scale, well-posedness proof, and FEM simulations.


<details>
  <summary>Details</summary>
Motivation: To understand how cancer cells invade extracellular matrix through both membrane-bound and soluble matrix degrading enzymes, which is crucial for understanding metastasis mechanisms.

Method: Developed cell-scale microscopic model, used homogenization theory to derive macroscopic model, proved well-posedness for biologically relevant initial data, implemented finite element method for numerical approximation.

Result: Established a well-posed macroscopic model for cancer invasion mediated by bound and soluble enzymes, with simulation results demonstrating the roles of different enzyme types in invasion processes.

Conclusion: The model successfully captures cancer cell invasion dynamics through ECM degradation, providing insights into the distinct contributions of membrane-bound versus soluble enzymes in metastatic processes.

Abstract: We formulate a cell-scale model for the degradation of the extra-cellular
matrix by membrane-bound and soluble matrix degrading enzymes produced by
cancer cells. Based on the microscopic model and using tools from the theory of
homogenisation we propose a macroscopic model for cancer cell invasion into the
extra-cellular matrix mediated by bound and soluble matrix degrading enzymes.
For suitable and biologically relevant initial data we prove the macroscopic
model is well-posed. We propose a finite element method for the numerical
approximation of the macroscopic model and report on simulation results
illustrating the role of the bound and soluble enzymes in cancer invasion
processes.

</details>


### [22] [Quantitative Scattering for the Energy-Critical Wave Equation on Asymptotically Flat Spacetimes](https://arxiv.org/abs/2509.12424)
*Benjamin Dodson,Shi-Zhuo Looi*

Main category: math.AP

TL;DR: Explicit exponential-type bound on Strichartz norm for energy-critical wave equation on asymptotically flat spacetimes, upgrading qualitative scattering to quantitative.


<details>
  <summary>Details</summary>
Motivation: While qualitative scattering theory for energy-critical wave equation was established, explicit bounds on global spacetime norms were unavailable in geometric settings with perturbed metrics.

Method: Develops strategy bypassing vector-field commutators, combining interaction Morawetz estimate adapted to variable coefficients with dispersive analysis based on integrated local energy decay.

Result: Establishes explicit exponential-type global bound on Strichartz norm ||u||_{L^8_{t,x}} depending on solution's energy and a priori regularity bound.

Conclusion: Upgrades existing qualitative scattering theory to fully quantitative statement, providing concrete measure of global solution behavior in geometric setting.

Abstract: The scattering theory for the energy-critical wave equation on asymptotically
flat spacetimes has, to date, been qualitative. While the qualitative
scattering of solutions is well-understood, explicit bounds on the solution's
global spacetime norms have been unavailable in this geometric setting.
  This paper establishes an explicit, exponential-type global bound on the
Strichartz norm $\| u\|_{L^8_{t,x}}$ for solutions to the defocusing equation
$\Box_g u=u^5$, where $\Box_g$ is the d'Alembertian associated with the
perturbed metric. The bound depends on the solution's energy and an \textit{a
priori} $\dot H^5 \times \dot H^4$ regularity bound on the solution.
  The proof develops a strategy that bypasses the need for vector-field
commutators. It combines an interaction Morawetz estimate adapted to variable
coefficients to control the solution's recent history with a dispersive
analysis founded on integrated local energy decay to control the remote past.
This strategy, in turn, necessitates the regularity and specific decay
assumptions on the metric.
  As a result, this work upgrades the existing qualitative scattering theory to
a fully quantitative statement, which provides a concrete measure of the global
behavior of solutions in this geometric setting.

</details>


### [23] [On the absence of anomalous dissipation for the Navier-Stokes equations with Navier boundary conditions: a sufficient condition](https://arxiv.org/abs/2509.12432)
*Claude Bardos,Daniel W. Boutros,Edriss S. Titi*

Main category: math.AP

TL;DR: Sufficient condition for absence of anomalous energy dissipation in 3D Navier-Stokes equations with Navier boundary conditions, without requiring pressure behavior assumptions or Euler solution existence.


<details>
  <summary>Details</summary>
Motivation: To establish conditions preventing anomalous energy dissipation in bounded domains with Navier boundary conditions, avoiding restrictive assumptions about pressure behavior near boundaries or requiring strong Euler solutions.

Method: Uses recent regularity results for pressure corresponding to weak solutions of incompressible Euler equations from previous work [Arch. Ration. Mech. Anal., 249 (2025), 28].

Result: Provides a sufficient condition that guarantees absence of anomalous energy dissipation in the described setting.

Conclusion: The approach successfully establishes conditions to prevent anomalous energy dissipation without relying on traditional restrictive assumptions about boundary pressure behavior or Euler solution requirements.

Abstract: We consider the three-dimensional incompressible Navier-Stokes equations in a
bounded domain with Navier boundary conditions. We provide a sufficient
condition for the absence of anomalous energy dissipation without making
assumptions on the behaviour of the corresponding pressure near the boundary or
the existence of a strong solution to the incompressible Euler equations with
the same initial data. We establish our result by using our recent regularity
results for the pressure corresponding to weak solutions of the incompressible
Euler equations [Arch. Ration. Mech. Anal., 249 (2025), 28].

</details>


### [24] [Nonlinear stability of the Larson-Penston collapse](https://arxiv.org/abs/2509.12435)
*Yan Guo,Mahir Hadzic,Juhi Jang,Matthew Schrecker*

Main category: math.AP

TL;DR: Nonlinear stability proof for Larson-Penston self-similar collapsing solutions in isothermal Euler-Poisson system under radial perturbations


<details>
  <summary>Details</summary>
Motivation: To establish the first full nonlinear stability result for radially imploding compressible flows and prove stability of Larson-Penston collapsing solutions

Method: High-order energy method in frequency regimes, rigorous computer-assisted techniques, two-tier high-order weighted energy method, Duhamel formula, and Brouwer fixed point theorem

Result: Successfully proved nonlinear stability of Larson-Penston solutions against radially symmetric perturbations and identified the exact number of derivatives needed for stability

Conclusion: The Larson-Penston family exhibits ground state character with global monotonicity properties that enable full nonlinear stability analysis and suppression of time-translation symmetry instability

Abstract: We prove nonlinear stability of the Larson-Penston family of self-similarly
collapsing solutions to the isothermal Euler-Poisson system. Our result applies
to radially symmetric perturbations and it is the first full nonlinear
stability result for radially imploding compressible flows. At the heart of the
proof is the ground state character of the Larson-Penston solution, which
exhibits important global monotonicity properties used throughout the proof.
  One of the key challenges is the proof of mode-stability for the non
self-adjoint spectral problem which arises when linearising the dynamics around
the Larson-Penston collapsing solution. To exclude the presence of complex
growing modes other than the trivial one associated with time translation
symmetry, we use a high-order energy method in low and high frequency regimes,
for which the monotonicity properties are crucially exploited, and use rigorous
computer-assisted techniques in the intermediate regime. In addition, the
maximal dissipativity of the linearised operator is proven on arbitrary large
backward light cones emanating from the singular point using the global
monotonicity of the Larson-Penston solutions. Such a flexibility in linear
analysis also facilitates nonlinear analysis and allows us to identify the
exact number of derivatives necessary for the nonlinear stability statement.
The proof is based on a two-tier high-order weighted energy method which ties
bounds derived from the Duhamel formula to quasilinear top order estimates. To
prove global existence we further use the Brouwer fixed point theorem to
identify the final collapse time, which suppresses the trivial instability
caused by the time-translation symmetry of the system.

</details>


### [25] [Effects of temporal variations on wave speeds of bistable traveling waves for Lotka-Volterra competition systems](https://arxiv.org/abs/2509.12472)
*Weiwei Ding,Zhanghua Liang*

Main category: math.AP

TL;DR: Analysis of bistable traveling waves in two-species Lotka-Volterra competition systems with time periodic environments, focusing on period influence, limiting speeds, and sign changes of wave speed.


<details>
  <summary>Details</summary>
Motivation: To understand how temporal periodicity affects competition outcomes in ecological systems, specifically investigating how different time periods can influence which species becomes dominant.

Method: Mathematical analysis of bistable traveling waves, establishing existence results for small and large periods, deriving explicit formulas for limiting speeds, and analyzing wave speed sign through comparison of intrinsic growth rates and interspecific competition strengths.

Result: Found that temporal variations significantly influence competition outcomes, with wave speed sign changing with temporal period, enabling different species to dominate under different periods. Derived explicit formulas for limiting speeds and convergence rates.

Conclusion: Temporal periodicity plays a crucial role in species competition dynamics, as demonstrated by the constructed example showing that different temporal periods can lead to different dominant species, highlighting the importance of environmental timing in ecological systems.

Abstract: This paper investigates the bistable traveling waves for two-species
Lotka-Volterra competition systems in time periodic environments. We focus
especially on the influence of the temporal period, with existence results
established for both small and large periods.We also show the existence of, and
derive explicit formulas for, the limiting speeds as the period tends to zero
or infinity, and provide estimates for the corresponding rates of convergence.
Furthermore, we analyze the sign of wave speed. Assuming that both species
share identical diffusion rates and intraspecific competition rates, we obtain
a criterion for determining the sign of wave speed by comparing the intrinsic
growth rates and interspecific competition strengths. More intriguingly, based
on our explicit formulas for the limiting speeds, we construct an example in
which the sign of wave speed changes with the temporal period. This example
reveals that temporal variations can significantly influence competition
outcomes,enabling different species to become dominant under different periods.

</details>


### [26] [Global Existence and Incompressible Limit for the Three-Dimensional Axisymmetric Compressible Navier-Stokes Equations with Large Bulk Viscosity and Large Initial Data](https://arxiv.org/abs/2509.12614)
*Qinghao Lei*

Main category: math.AP

TL;DR: Global existence and exponential decay of solutions for 3D axisymmetric compressible Navier-Stokes equations with slip boundary conditions and large initial data/vacuum, given sufficiently large bulk viscosity. Solutions converge to inhomogeneous incompressible Navier-Stokes as bulk viscosity approaches infinity.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous mathematical foundations for compressible fluid flows in cylindrical domains with slip boundary conditions, particularly addressing challenges with large initial data, vacuum regions, and understanding the relationship between compressible and incompressible models through viscosity limits.

Method: Mathematical analysis of three-dimensional axisymmetric compressible Navier-Stokes equations in cylindrical domains excluding the axis, using slip boundary conditions and assuming sufficiently large bulk viscosity coefficient.

Result: Proved global existence and exponential decay of weak, strong, and classical solutions even with large initial data and vacuum. Showed convergence to inhomogeneous incompressible Navier-Stokes equations as bulk viscosity coefficient tends to infinity.

Conclusion: The study provides comprehensive existence and decay results for compressible flows with practical boundary conditions, and establishes a rigorous mathematical connection between compressible and incompressible fluid models through the viscosity limit, offering insights into fluid behavior across different viscosity regimes.

Abstract: In this paper, we study the three-dimensional axisymmetric compressible
Navier-Stokes equations with slip boundary conditions in a cylindrical domain
excluding the axis. We establish the global existence and exponential decay of
weak, strong, and classical solutions with large initial data and vacuum, under
the assumption that the bulk viscosity coefficient is sufficiently large.
Moreover, we demonstrate that as the bulk viscosity coefficient tends to
infinity, the solutions of the compressible Navier-Stokes equations converge to
those of the inhomogeneous incompressible Navier-Stokes equations.

</details>


### [27] [Ill-posedness in $B^s_{p,\infty}$ of the Euler equations: Non-continuous dependence](https://arxiv.org/abs/2509.12619)
*Jinlu Li,Yanghai Yu*

Main category: math.AP

TL;DR: This paper solves an open problem from Bahouri-Chemin-Danchin (2011) by proving that the solution map for Euler equations is discontinuous in Besov spaces B^s_{p,∞} and Hölder spaces C^{k,α} for certain parameter ranges.


<details>
  <summary>Details</summary>
Motivation: To address an unresolved problem left in the monographs by Bahouri-Chemin-Danchin (2011) regarding the continuity properties of Euler equation solution maps in specific function spaces.

Method: The authors construct an approximate solution to the Burgers equation and use technical choices of initial data to demonstrate the discontinuity of the solution map.

Result: Proved that the Euler equation solution map is not continuous from B^s_{p,∞} to L_T^∞ B^s_{p,∞} for s>1+d/p, and from C^{k,α} to L_T^∞ C^{k,α} for k∈ℕ⁺ and α∈(0,1).

Conclusion: The paper successfully resolves the open problem and provides novel insights into the ill-posedness properties of Euler equations in these function spaces, with particular emphasis on the construction technique for Burgers equation approximations.

Abstract: In this paper, we solve an open problem left in the monographs
\cite[Bahouri-Chemin-Danchin, (2011)]{BCD}. Precisely speaking, it was obtained
in \cite[Theorem 7.1 on pp293, (2011)]{BCD} the existence and uniqueness of
$B^s_{p,\infty}$ solution for the Euler equations. We furthermore prove that
the solution map of the Euler equation is not continuous in the Besov spaces
from $B^s_{p,\infty}$ to $L_T^\infty B^s_{p,\infty}$ for $s>1+d/p$ with $1\leq
p\leq \infty$ and in the H\"{o}lder spaces from $C^{k,\alpha}$ to $L_T^\infty
C^{k,\alpha}$ with $k\in \mathbb{N}^+$ and $\alpha\in(0,1)$, which later covers
particularly the ill-posedness of $C^{1,\alpha}$ solution in \cite[Trans. Amer.
Math. Soc., (2018)]{MYtams}. Beyond purely technical aspects on the choice of
initial data, a remarkable novelty of the proof is the construction of an
approximate solution to the Burgers equation.

</details>


### [28] [Inverse scattering for the fractional Schrödinger equation](https://arxiv.org/abs/2509.12685)
*Saumyajit Das,Tuhin Ghosh,Shiqi Ma*

Main category: math.AP

TL;DR: Study of inverse scattering for fractional Schrödinger equation, solving Born approximation problem using resolvent estimates to derive scattering amplitude expression and prove potential uniqueness.


<details>
  <summary>Details</summary>
Motivation: To address the inverse scattering problem for fractional Schrödinger equations and establish theoretical foundations for determining potentials from scattering data.

Method: Utilized (p,q)-type resolvent estimates for fractional Laplacian to derive scattering amplitude expressions and prove uniqueness of potential from scattering amplitude data.

Result: Successfully derived an expression for scattering amplitude and proved uniqueness of the potential using scattering amplitude information.

Conclusion: The study provides a mathematical framework for solving inverse scattering problems in fractional quantum systems through resolvent estimates and scattering amplitude analysis.

Abstract: This article is devoted to studying the inverse scattering for the fractional
Schr\"{o}dinger equation, and in particular we solve the Born approximation
problem. Based on the ($p$,$q$)-type resolvent estimate for the fractional
Laplacian, we derive an expression for the scattering amplitude of the
scattered solution of the fractional Schr\"{o}dinger equation. We prove the
uniqueness of the potential using the scattering amplitude data.

</details>


### [29] [Normalized solutions of quasilinear Schrödinger equation with Sobolev critical exponent on star-shaped bounded domains](https://arxiv.org/abs/2509.12707)
*Ru Yan*

Main category: math.AP

TL;DR: Existence of two positive normalized solutions for quasilinear Schrodinger equation with critical exponent on bounded domains


<details>
  <summary>Details</summary>
Motivation: Study quasilinear Schrodinger equations with critical exponent, which are important in mathematical physics but challenging due to critical nonlinearity and normalization constraints

Method: Dual approach to establish existence of solutions, focusing on ground state and mountain pass solutions with normalization constraints

Result: Proved existence of two distinct positive normalized solutions: one ground state solution and one mountain pass solution

Conclusion: Successful demonstration of multiple normalized solutions existence for critical quasilinear Schrodinger equations using dual methods

Abstract: In this paper, we consider a quasilinear Schr\"odinger equation with critical
exponent on bounded domains. Via a dual approach, we establish the existence of
two positive normalized solutions: one is a ground state and the other is a
mountain pass solution.

</details>


### [30] [Wave maps from circle to Riemannian manifold: global controllability is equivalent to homotopy](https://arxiv.org/abs/2509.12779)
*Jean-Michel Coron,Joachim Krieger,Shengquan Xiang*

Main category: math.AP

TL;DR: Global controllability of wave maps from circle to compact Riemannian manifolds is characterized by homotopy class, with uniform-time controllability between steady states and exponential stability around closed geodesics with negative curvature.


<details>
  <summary>Details</summary>
Motivation: To understand the controllability properties of wave maps, a geometric PDE, and address an open problem raised by Dehman, Lebeau and Zuazua (2003) regarding uniform-time global controllability between steady states.

Method: Study wave maps from the circle to compact Riemannian manifolds, analyzing global controllability through homotopy class characterization and establishing uniform-time controllability results.

Result: Proved that global controllability is precisely characterized by homotopy class of data, established uniform-time global controllability between steady states, and obtained quantitative exponential stability around closed geodesics with negative sectional curvature.

Conclusion: This work demonstrates the deep connections between PDEs, differential geometry, and control theory, solving a long-standing open problem and providing new insights into the controllability of geometric wave equations.

Abstract: We study wave maps from the circle to a general compact Riemannian manifold.
We prove that the global controllability of this geometric equation is
characterized precisely by the homotopy class of the data. As a remarkable
intermediate result, we establish uniform-time global controllability between
steady states, providing a partial answer to an open problem raised by Dehman,
Lebeau and Zuazua (2003). Finally, we obtain quantitative exponential stability
around closed geodesics with negative sectional curvature. This work highlights
the rich interplay between partial differential equations, differential
geometry, and control theory.

</details>


### [31] [Degenerate Elliptic PDEs on a Network with Kirchhoff Conditions](https://arxiv.org/abs/2509.12848)
*Guy Barles,Olivier Ley,Erwin Topp*

Main category: math.AP

TL;DR: Strong comparison principle for semilinear degenerate elliptic equations on networks with nonlinear Kirchhoff conditions, enabling existence and uniqueness of continuous viscosity solutions via Perron's method.


<details>
  <summary>Details</summary>
Motivation: To establish comparison principles for general semilinear elliptic equations on networks with varying equation types (from first-order to uniformly elliptic) incident to vertices, particularly with nonlinear Kirchhoff-type interior vertex conditions.

Method: Develops a strong comparison principle between discontinuous viscosity sub- and supersolutions for the network problem, then applies Perron's method to obtain continuous viscosity solutions.

Result: Proves a strong comparison principle that allows concluding existence and uniqueness of continuous viscosity solutions for the general network problem.

Conclusion: The framework successfully handles general semilinear degenerate elliptic equations on networks with mixed equation types and nonlinear vertex conditions, providing fundamental comparison tools and solution existence/uniqueness results.

Abstract: In this article, we are interested in semilinear, possibly degenerate
elliptic equations posed on a general network, with nonlinear Kirchhoff-type
conditions for its interior vertices and Dirichlet boundary conditions for the
boundary ones. The novelty here is the generality of the equations posed on
each edge that is incident to a particular vertex, ranging from first-order
equations to uniformly elliptic ones. Our main result is a strong comparison
principle, i.e., a comparison result between discontinuous viscosity sub and
supersolutions of such problems, from which we conclude the existence and
uniqueness of a continuous viscosity by Perron's method. Further extensions are
also discussed.

</details>


### [32] [Mathematical Study of Reaction-Diffusion in Congested Crowd Motion](https://arxiv.org/abs/2509.12935)
*Noureddine Igbida,Fahd Karami,Driss Meskine*

Main category: math.AP

TL;DR: Existence, uniqueness and L^1-comparison principle for weak solutions of PDE system modeling phase transition reaction-diffusion in congested crowd motion with general reaction terms and mixed boundary conditions.


<details>
  <summary>Details</summary>
Motivation: To develop a mathematical framework for modeling phase transition reaction-diffusion in congested crowd motion, applicable to multi-species diffusion-segregation and pedestrian dynamics with congestion constraints.

Method: Analysis of PDE system with general reaction term and mixed homogeneous (Dirichlet and Neumann) boundary conditions, establishing existence and uniqueness of weak solutions through L^1-comparison principle.

Result: Established existence, uniqueness, and L^1-comparison principle for weak solutions. Derived sufficient conditions combining drift with reaction that guarantee absence of congestion, reducing dynamics to constrained linear reaction-transport equation.

Conclusion: The paper provides rigorous mathematical foundations for congested crowd motion models with phase transitions, offering both general solutions and specific conditions where congestion constraints can be simplified to linear transport equations.

Abstract: This paper establishes existence, uniqueness, and an L^1-comparison principle
for weak solutions of a PDE system modeling phase transition reaction-diffusion
in congested crowd motion. We consider a general reaction term and mixed
homogeneous (Dirichlet and Neumann) boundary conditions. This model is
applicable to various problems, including multi-species diffusion-segregation
and pedestrian dynamics with congestion. Furthermore, our analysis of the
reaction term yields sufficient conditions combining the drift with the
reaction that guarantee the absence of congestion, reducing the dynamics to a
constrained linear reaction-transport equation.

</details>


### [33] [Low-complexity approximations with least-squares formulation of the time-dependent Schr{ö}dinger equation](https://arxiv.org/abs/2509.13005)
*Mi-Song Dupuy,Virginie Ehrlacher,Clément Guillot*

Main category: math.AP

TL;DR: New numerical methods for solving time-dependent Schrödinger equation using tensor and gaussian wavepacket approximations with optimization algorithms.


<details>
  <summary>Details</summary>
Motivation: To develop efficient numerical approximation methods for solving the time-dependent Schrödinger equation, which is fundamental in quantum mechanics.

Method: Two approaches: 1) Tensor-based approximation solved with Alternating Least Square algorithm, 2) Linear combination of gaussian wave packets solved with greedy algorithms.

Result: Proposed methods provide numerical approximation solutions to the time-dependent Schrödinger equation through restricted optimization problems.

Conclusion: Both tensor and gaussian wavepacket approaches offer viable methods for approximating solutions to the Schrödinger equation, with efficiency considerations discussed for each approach.

Abstract: We propose new methods designed to numerically approximate the solution to
the time dependent Schr{\"o}dinger equation, based on two types of ansatz:
tensors, and approximation by a linear combination of gaussian wave packets. In
both cases, the method can be seen as a restricted optimization problem, which
can be solved by adapting either the Alternating Least Square algorithm in the
tensor case, or some greedy algorithm in the gaussian wavepacket case. We also
discuss the efficiency of both approaches.

</details>


### [34] [Perturbation theory of the compressible Navier-Stokes equations and its application](https://arxiv.org/abs/2509.13012)
*Kazuyuki Tsuda*

Main category: math.AP

TL;DR: Perturbation theory for compressible Navier-Stokes equations in R^n (n≥3) showing decay estimates around non-constant states, with stability analysis from weak L^n stationary solutions.


<details>
  <summary>Details</summary>
Motivation: To investigate decay properties and stability of solutions around non-constant stationary states in compressible Navier-Stokes equations, extending beyond constant state analysis.

Method: Deriving resolvent estimates with perturbation terms in low frequency part having parabolic spectral curve, applicable to dispersive hyperbolic systems like damped wave equations.

Result: Obtained decay rates for perturbations (including L∞ norm) that coincide with heat kernel decay rates, with method applicable to damped wave equations with variable coefficients.

Conclusion: The perturbation theory successfully establishes decay estimates around non-constant states, with methodology extendable to other dispersive hyperbolic systems, demonstrating parabolic-type decay rates.

Abstract: In this article, a perturbation theory of the compressible Navier-Stokes
equations in $\mathbb{R}^n$ $(n \geq 3)$ is studied to investigate decay
estimate of solutions around a non-constant state. As a concrete problem,
stability is considered for a perturbation system from a stationary solution
$u_\omega$ belonging to the weak $L^n$ space. Decay rates of the perturbation
including $L^\infty$ norm are obtained which coincide with those of the heat
kernel. The proof is based on deriving suitable resolvent estimates with
perturbation terms in the low frequency part having a parabolic spectral curve.
Our method can be applicable to dispersive hyperbolic systems like wave
equations with strong damping. Indeed, a parabolic type decay rate of a
solution is obtained for a damped wave equation including variable coefficients
which satisfy spatial decay conditions.

</details>


### [35] [Global existence and decay of small solutions in a viscous half Klein-Gordon equation](https://arxiv.org/abs/2509.13188)
*Louis Garénaux,Björn de Rijk*

Main category: math.AP

TL;DR: Global existence and decay for viscous half Klein-Gordon equation with quadratic nonlinearity using normal form transformation to handle nonlinear terms


<details>
  <summary>Details</summary>
Motivation: To establish global existence and decay properties for solutions of viscous half Klein-Gordon equations with quadratic nonlinearities, particularly for initial data with small Fourier transforms in L1∩L∞ spaces

Method: Apply normal form method (Shatah) or integration by parts in time in Duhamel formula to transform quadratic nonlinearity into subcritical nonlocal quartic nonlinearity, then use standard L1-L∞ argument to control it via linear diffusive dynamics

Result: Successfully established global existence and decay of solutions by leveraging nonresonant dispersive effects to transform the problematic quadratic nonlinearity into a more manageable quartic form

Conclusion: The transformation approach through normal form methods effectively handles quadratic nonlinearities in viscous half Klein-Gordon equations, enabling global existence and decay results for small initial data

Abstract: We establish global existence and decay of solutions of a viscous half
Klein-Gordon equation with a quadratic nonlinearity considering initial data,
whose Fourier transform is small in L1 cap Linfty. Our analysis relies on the
observation that nonresonant dispersive effects yield a transformation of the
quadratic nonlinearity into a subcritical nonlocal quartic one, which can be
controlled by the linear diffusive dynamics through a standard L1 - Linfty
argument. This transformation can be realized by applying the normal form
method of Shatah or, equivalently, through integration by parts in time in the
associated Duhamel formula.

</details>


### [36] [Vorticity blow-up for the 3D incompressible Euler equations](https://arxiv.org/abs/2509.13226)
*Wenjie Deng,Song Jiang,Minling Li,Zhaonan Lou*

Main category: math.AP

TL;DR: Finite-time blow-up analysis for 3D Euler equations with low-regularity initial vorticity using self-similar methods and stability analysis.


<details>
  <summary>Details</summary>
Motivation: To understand the formation of finite-time singularities in 3D incompressible Euler equations, particularly for axi-symmetric cases with low-regularity initial conditions.

Method: Applied self-similar method and stability analysis in critical Sobolev space, investigated null structure of transport term, and analyzed parameter stability of fundamental self-similar models.

Result: Proved that vorticity of axi-symmetric 3D Euler equations develops finite-time singularity with specific scaling indices, and examined time integrability of solutions.

Conclusion: The study provides rigorous mathematical proof of finite-time blow-up in 3D Euler equations using novel analytical techniques focused on self-similar structures and stability properties.

Abstract: In this paper, we study the finite-time blow-up for classical solutions of
the 3D incompressible Euler equations with low-regularity initial vorticity.
Applying the self-similar method and stability analysis of the self-similar
system in critical Sobolev space, we prove that the vorticity of the
axi-symmetric 3D Euler equations develops a finite-time singularity with
certain scaling indices. Furthermore, we investigate the time integrability of
the solutions. The proof is based on the new observations for the null
structure of the transport term, and the parameter stability of the fundamental
self-similar models.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [37] [Quantifying Local Point-Group-Symmetry Order in Complex Particle Systems](https://arxiv.org/abs/2509.12665)
*Domagoj Fijan,Maria R. Ward Rashidi,Sharon C. Glotzer*

Main category: physics.comp-ph

TL;DR: New Point Group Order Parameters (PGOPs) quantify point group symmetry order in crystals, addressing limitations of traditional bond-orientational order metrics that don't directly measure symmetry.


<details>
  <summary>Details</summary>
Motivation: Traditional local order parameters used in crystallization studies don't directly quantify symmetry, which is crucial for understanding order development during crystallization.

Method: Developed Point Group Order Parameters (PGOPs) to continuously quantify point group symmetry order, implemented in open-source SPATULA package (parallelized C++ with Python interface).

Result: PGOPs demonstrate strength in detecting order across different crystalline systems and outperform commonly used bond-orientational order metrics.

Conclusion: PGOPs provide a more direct and effective way to quantify symmetry in crystallization studies, with publicly available implementation for all non-infinite point groups.

Abstract: Crystals and other condensed phases are defined primarily by their inherent
symmetries, which play a crucial role in dictating their structural properties.
In crystallization studies, local order parameters (OPs) that describe bond
orientational order are widely employed to investigate crystal formation.
Despite their utility, these traditional metrics do not directly quantify
symmetry, an important aspect for understanding the development of order during
crystallization. To address this gap, we introduce a new set of OPs, called
Point Group Order Parameters (PGOPs), designed to continuously quantify point
group symmetry order. We demonstrate the strength and utility of PGOP in
detecting order across different crystalline systems and compare its
performance to commonly used bond-orientational order metrics. PGOP
calculations for all non-infinite point groups are implemented in the
open-source package SPATULA (Symmetry Pattern Analysis Toolkit for
Understanding Local Arrangements), written in parallelized C++ with a Python
interface. The code is publicly available on GitHub.

</details>


### [38] [Benchmarking thermostat algorithms in molecular dynamics simulations of a binary Lennard-Jones glass-former model](https://arxiv.org/abs/2509.12837)
*Kumpei Shiraishi,Emi Minamitani,Kang Kim*

Main category: physics.comp-ph

TL;DR: Systematic comparison of thermostat methods in molecular dynamics simulations shows Nosé-Hoover chain and Bussi thermostats provide reliable temperature control but exhibit time-step dependence in potential energy. Langevin methods, particularly Grønbech-Jensen-Farago scheme, offer consistent sampling but at higher computational cost.


<details>
  <summary>Details</summary>
Motivation: To assess the influence of different thermostat methods in constant-temperature molecular dynamics simulations and provide practical guidance for thermostat selection in classical MD simulations.

Method: Used a binary Lennard-Jones liquid as a model glass former to investigate how sampling of physical observables (particle velocities and potential energy) responds to time step changes across various thermostat schemes including Nosé-Hoover, Bussi velocity rescaling, and Langevin dynamics implementations.

Result: Nosé-Hoover chain and Bussi thermostats provide reliable temperature control but show pronounced time-step dependence in potential energy. Langevin methods, especially Grønbech-Jensen-Farago scheme, provide most consistent sampling but incur ~2x computational cost and show systematic decrease in diffusion coefficients with increasing friction.

Conclusion: The study offers practical guidance for thermostat choice in molecular dynamics simulations, with findings applicable to diverse areas including glass transition, phase separation, and nucleation processes.

Abstract: A systematic comparison was carried out to assess the influence of
representative thermostat methods in constant-temperature molecular dynamics
simulations. The thermostat schemes considered include the Nos\'e--Hoover
thermostat and its chain generalisation, the Bussi velocity rescaling method,
and several implementations of the Langevin dynamics. Using a binary
Lennard-Jones liquid as a model glass former, we investigated how the sampling
of physical observables, such as particle velocities and potential energy,
responds to changes in time step across these thermostats. While the
Nos\'e--Hoover chain and Bussi thermostats provide reliable temperature
control, a pronounced time-step dependence was observed in the potential
energy. Amongst the Langevin methods, the Gr{\o}nbech-Jensen--Farago scheme
provided the most consistent sampling of both temperature and potential energy.
Nonetheless, Langevin dynamics typically incurs approximately twice the
computational cost due to the overhead of random number generation, and
exhibits a systematic decrease in diffusion coefficients with increasing
friction. This study presents a broad comparison of thermostat methods using a
binary Lennard-Jones glass-former model, offering practical guidance for the
choice of thermostats in classical molecular dynamics simulations. These
findings provide useful insights for diverse applications, including glass
transition, phase separation, and nucleation.

</details>


### [39] [Weak Generative Sampler for Stationary Distributions of McKean-Vlasov System](https://arxiv.org/abs/2509.12841)
*Zhiqiang Cai,Chengyu Liu,Xiang Zhou*

Main category: physics.comp-ph

TL;DR: Proposes a Weak Generative Sampler (WGS) method using normalizing flows to compute stationary distributions of McKean-Vlasov processes, addressing limitations of finite-particle simulations in capturing phase transitions.


<details>
  <summary>Details</summary>
Motivation: Finite-particle simulations often fail to accurately capture stationary distributions of mean-field systems, especially when multiple metastable states cause phase transitions, due to the non-interchangeability of infinite-time and infinite-particle limits.

Method: Adapts the Weak Generative Sampler framework based on normalizing flows and a weak formulation of the nonlinear Fokker-Planck equation to generate i.i.d. samples from stationary distributions.

Result: Extensive numerical experiments validate the method's efficacy in accurately approximating stationary distributions and capturing phase transitions in complex systems.

Conclusion: The proposed WGS method provides an effective generative technique for computing stationary distributions of McKean-Vlasov processes, overcoming limitations of traditional finite-particle simulation approaches.

Abstract: Stochastic interacting particle systems are widely used to model collective
phenomena across diverse fields, including statistical physics, biology, and
social dynamics. The McKean-Vlasov equation arises as the mean-field limit of
such systems as the number of particles tends to infinity, while its long-time
behaviour is characterized by stationary distributions as time tends to
infinity. However, the validity of interchanging the infinite-time and
infinite-particle limits is not guaranteed. Consequently, simulation methods
that rely on a finite-particle truncation may fail to accurately capture the
mean-field system's stationary distributions, particularly when the coexistence
of multiple metastable states leads to phase transitions. In this paper, we
adapt the framework of the Weak Generative Sampler (WGS) -- a generative
technique based on normalizing flows and a weak formulation of the nonlinear
Fokker-Planck equation -- to compute and generate i.i.d. samples satisfying the
stationary distributions of McKean-Vlasov processes. Extensive numerical
experiments validate the efficacy of the proposed methods, showcasing their
ability to accurately approximate stationary distributions and capture phase
transitions in complex systems.

</details>


### [40] [From higher-order moments to time correlation functions in strongly correlated systems: A DMRG-based memory kernel coupling theory](https://arxiv.org/abs/2509.13140)
*Yunhao Liu,Wenjie Dou*

Main category: physics.comp-ph

TL;DR: Hybrid approach combining memory kernel coupling theory (MKCT) with DMRG for efficient computation of dynamical observables in strongly correlated systems, avoiding expensive real-time propagation.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient method for computing dynamical observables in strongly correlated quantum systems that avoids the computational expense of real-time propagation required in time-dependent DMRG.

Method: Integrates MKCT with DMRG using matrix product operators (MPOs) and matrix product states (MPSs). Repeated application of Liouville operator achieved through iterative DMRG-like procedure. Computes correlation functions from higher-order moments.

Result: Successfully computed spectral function of Hubbard model and electronic friction in Hubbard-Holstein model. Results show excellent agreement with TD-DMRG benchmarks while being computationally more efficient.

Conclusion: MKCT-DMRG establishes a promising and accurate framework for simulating challenging dynamical properties in strongly correlated quantum systems with superior computational efficiency compared to TD-DMRG.

Abstract: We introduce a hybrid approach for computing dynamical observables in
strongly correlated systems using higher-order moments. This method integrates
memory kernel coupling theory (MKCT) with the density matrix renormalization
group (DMRG), extending our recent work on MKCT to strongly correlated systems.
The method establishes that correlation functions can be derived from the
moments. Within our framework, operators and wavefunctions are represented as
matrix product operators (MPOs) and matrix product states (MPSs), respectively.
Crucially, the repeated application of the Liouville operator is achieved
through an iterative procedure analogous to the DMRG algorithm itself. We
demonstrate the effectiveness and efficiency of MKCT-DMRG by computing the
spectral function of the Hubbard model. Furthermore, we successfully apply the
method to compute the electronic friction in the Hubbard-Holstein model. In all
cases, the results show excellent agreement with time-dependent DMRG (TD-DMRG)
benchmarks. The advantage of MKCT-DMRG over TD-DMRG is the computational
efficiency, which avoids expensive real-time propagation in TD-DMRG. These
findings establish MKCT-DMRG as a promising and accurate framework for
simulating challenging dynamical properties in strongly correlated quantum
systems.

</details>


### [41] [Enhancement of torque transmission capability in Magneto-Rheological fluid-based Clutch using novel hybrid corrugated plane transmission surface strategy](https://arxiv.org/abs/2509.13187)
*Jithin Vijaykumar,Loyad Joseph Losan,Saddala Reddy Tharun,Murthi Ram Chandra Reddy,Mood Rahul,Jagadeesha T*

Main category: physics.comp-ph

TL;DR: A novel corrugated surface design for MR clutches improves torque transmission by 39.37% through increased surface area and optimized magnetic field alignment.


<details>
  <summary>Details</summary>
Motivation: Miniaturization requires MR clutches to transmit higher torque within limited radial space constraints, necessitating innovative designs that enhance torque transmissibility with reduced transmission surface area.

Method: Magnetic analysis using COMSOL Multiphysics software to study dimensional parameters of a hybrid corrugated plane type MR clutch design, including annular/radial gaps, disc width, corrugation dimensions, and bobbin thickness.

Result: The corrugated design significantly improves torque transmission capability, with optimization achieving a 39.37% increase compared to non-optimized configurations. Various dimensional parameters were found to critically influence performance.

Conclusion: The corrugated transmissible surface design effectively addresses spatial constraints in MR clutches by maximizing transmission area and optimizing magnetic field alignment, enabling higher torque transmission in miniaturized applications.

Abstract: In an increased automated world, miniaturization is the key to widespread
deployment of advanced technologies. Enhancing the torque transmissibility by
abiding to the spatial constraints imposed by radial space availability has
consistently remained a hurdle in the implementation of Magneto-Rheological
(MR) clutches that use shear mode of MR fluid (MRF). This proves the necessity
of a novel design capable of providing required transmission capability with a
reduced transmission surface area. The present study analyzes a corrugated
transmissible surface design which improves torque transmissibility with the
help of increased transmission area and proper alignment of field lines passing
through the MRF gap. In this paper, the impact of various dimensional
parameters of a hybrid corrugated plane type MR clutch (MRC) design was studied
with the aid of magnetic analysis performed on COMSOL Multiphysics software.
The results obtained shows that various parameters in the design of MR
clutches, such as annular and radial MR gaps, disc width, individual
corrugation heights, corrugation width, bobbin thickness and radii of plane
surface influences the torque transmission capability of MR clutches. Also, an
optimization of the hybrid corrugated plane MR Clutch of the chosen geometry
has been conducted with the transmission capability increasing by 39.37%
compared with the non-optimized geometrical configuration.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [42] [Shock wave bending around a dusty plasma void](https://arxiv.org/abs/2509.12397)
*Sachin Sharma,Rauoof Wani,Prabhakar Srivastav,Meenakshee Sharma,Sayak Bose,Sanat Tiwari,Abhijit Sen*

Main category: physics.plasm-ph

TL;DR: Experimental observation of dust acoustic shock wave bending around a dust void region, showing wavefront deformation, particle trapping, and subsequent Coulomb explosion with bow shock formation.


<details>
  <summary>Details</summary>
Motivation: To study the interaction between planar shock waves and compressible obstacles (voids) in plasma environments, particularly how wavefronts behave when encountering obstacles larger than their wavelength.

Method: Experiments conducted in a DC glow discharge plasma where shock waves and voids are created as self-excited modes of a three-dimensional dust cloud, complemented by molecular dynamics simulations to reproduce and analyze the phenomenon.

Result: Observed inward bending of shock wave when encountering void, transient trapping of dust particles in the void, Coulomb explosion of trapped particles, and generation of a bow shock. Molecular dynamics simulations successfully reproduced the key features.

Conclusion: The study demonstrates complex wave-obstacle interactions in plasma environments, revealing mechanisms of wave bending, particle trapping, and subsequent explosive phenomena that provide insights into fundamental plasma dynamics.

Abstract: We report on experimental observations of the bending of a dust acoustic
shock wave around a dust void region. This phenomenon occurs as a planar shock
wavefront encounters a compressible obstacle in the form of a void whose size
is larger than the wavelength of the wave. As they collide, the central portion
of the wavefront, that is the first to touch the void, is blocked while the
rest of the front continues to propagate, resulting in an inward bending of the
shock wave. The bent shock wave eventually collapses, leading to the transient
trapping of dust particles in the void. Subsequently, a Coulomb explosion of
the trapped particles generates a bow shock. The experiments have been carried
out in a DC glow discharge plasma, where the shock wave and the void are
simultaneously created as self-excited modes of a three-dimensional dust cloud.
The salient features of this phenomenon are reproduced in molecular dynamics
simulations, which provide valuable insights into the underlying dynamics of
this interaction.

</details>


### [43] [Validation of the GFS model for Gyrokinetic Stability of NSTX Pedestal Data](https://arxiv.org/abs/2509.12599)
*M. Yang,J. F. Parisi,M. S. Anastopoulos Tzanis,G. M. Staebler*

Main category: physics.plasm-ph

TL;DR: Validation of Gyro Fluid System (GFS) model for linear gyrokinetic stability in NSTX H-mode edge conditions using Bayesian optimization for resolution parameters.


<details>
  <summary>Details</summary>
Motivation: To validate the GFS model for accurate linear stability analysis in spherical tokamak pedestal conditions with reduced computational cost compared to full gyrokinetic codes like CGYRO.

Method: Used Bayesian optimization to determine optimal resolution parameters for GFS, validated against a database of CGYRO gyrokinetic calculations using NSTX plasma profile measurements to identify various instability modes.

Result: GFS with optimized resolution achieves accurate linear stability analysis for KBM, TEM, and MTM instability branches in NSTX pedestal conditions, though accuracy degrades for low magnetic shear and near separatrix conditions.

Conclusion: GFS is established as a fast linear eigenmode solver for spherical tokamak pedestal gyrokinetic stability with a systematic methodology for determining optimal resolution settings.

Abstract: This study presents a large database validation of the Gyro Fluid System
(GFS) model for linear gyrokinetic stability for high-mode (H-mode) edge
transport barrier conditions in the National Spherical Torus Experiment (NSTX)
tokamak. The database of linear stability calculations with the CGYRO
gyrokinetic code was produced using plasma profile measurements from NSTX
discharges to identify kinetic ballooning modes (KBM), trapped electron modes
(TEM), and micro-tearing modes (MTM) that limit the pressure profile gradient
in the H-mode barrier. A novel Bayesian optimization approach determines
optimal resolution parameters for GFS specifically for spherical tokamak
pedestal conditions. Our results demonstrate that GFS, with optimized
resolution, can achieve accurate linear stability analysis in NSTX pedestal
conditions for reduced resolution compared to CGYRO. GFS can accurately find
the KBM, TEM, and MTM instability branches. Parametric analysis reveals that
GFS accuracy in this extreme pedestal parameter range is degraded for low
magnetic shear and near the separatrix conditions. These findings establish GFS
as a fast linear eigenmode solver for spherical tokamak pedestal gyrokinetic
stability and demonstrate a systematic methodology for determining the optimum
resolution settings.

</details>


### [44] [FusionMAE: large-scale pretrained model to optimize and simplify diagnostic and control of fusion plasma](https://arxiv.org/abs/2509.12945)
*Zongyu Yang,Zhenghao Yang,Wenjing Tian,Jiyuan Li,Xiang Sun,Guohui Zheng,Songfen Liu,Niannian Wu,Rongpeng Li,Zhaohe Xu,Bo Li,Zhongbing Shi,Zhe Gao,Wei Chen,Xiaoquan Ji,Min Xu,Wulyu Zhong*

Main category: physics.plasm-ph

TL;DR: FusionMAE model compresses 88 diagnostic signals into unified embeddings, enabling virtual backup diagnosis with 96.7% reliability and emergent capabilities for fusion energy systems.


<details>
  <summary>Details</summary>
Motivation: Complex multiscale plasma dynamics in fusion devices require extensive diagnostic systems, but their complexity and uncertainty hinder fusion energy development acceleration.

Method: Large-scale masked autoencoder (FusionMAE) pre-trained with compression-reduction and missing-signal reconstruction mechanisms to create unified diagnostic embeddings.

Result: Achieves 96.7% reliability in virtual backup diagnosis, demonstrates emergent capabilities including automatic data analysis, universal interface, and enhanced control performance.

Conclusion: Pioneers large-scale AI integration in fusion energy, showing pre-trained embeddings can simplify system interfaces, reduce diagnostic needs, and optimize reactor operations.

Abstract: In magnetically confined fusion device, the complex, multiscale, and
nonlinear dynamics of plasmas necessitate the integration of extensive
diagnostic systems to effectively monitor and control plasma behaviour. The
complexity and uncertainty arising from these extensive systems and their
tangled interrelations has long posed a significant obstacle to the
acceleration of fusion energy development. In this work, a large-scale model,
fusion masked auto-encoder (FusionMAE) is pre-trained to compress the
information from 88 diagnostic signals into a concrete embedding, to provide a
unified interface between diagnostic systems and control actuators. Two
mechanisms are proposed to ensure a meaningful embedding: compression-reduction
and missing-signal reconstruction. Upon completion of pre-training, the model
acquires the capability for 'virtual backup diagnosis', enabling the inference
of missing diagnostic data with 96.7% reliability. Furthermore, the model
demonstrates three emergent capabilities: automatic data analysis, universal
control-diagnosis interface, and enhancement of control performance on multiple
tasks. This work pioneers large-scale AI model integration in fusion energy,
demonstrating how pre-trained embeddings can simplify the system interface,
reducing necessary diagnostic systems and optimize operation performance for
future fusion reactors.

</details>


### [45] [Runaway electron interactions with whistler waves in tokamak plasmas: energy-dependent transport scaling](https://arxiv.org/abs/2509.13271)
*Yashika Ghai,D. Del-Castillo-Negrete,D. A. Spong,M. T. Beidler*

Main category: physics.plasm-ph

TL;DR: First-principles modeling of runaway electron-whistler wave interactions in 3D tokamak geometry reveals anomalous diffusion and transport regime transitions not captured by quasi-linear models.


<details>
  <summary>Details</summary>
Motivation: Resonant interactions between runaway electrons and whistler waves are promising for runaway electron mitigation in tokamaks, but prior studies relied on simplified quasi-linear models in simplified geometries.

Method: Coupled AORSA (computes whistler eigenmodes) with KORC (kinetic orbit code) to model full-orbit runaway electron trajectories in 3D tokamak equilibrium wave fields.

Result: Runaway electrons undergo scattering to large pitch angles and exhibit anomalous diffusion in both pitch-angle and kinetic energy space, with transitions between diffusive, sub-diffusive, and super-diffusive transport regimes based on initial energy.

Conclusion: This anomalous transport behavior advances understanding of runaway electron dynamics and provides theoretical foundation for designing targeted wave-based mitigation strategies in future tokamak experiments.

Abstract: Resonant interactions between high energy runaway electrons (REs) and
whistler waves are a promising mechanism for RE mitigation in tokamak plasmas.
While prior studies have largely relied on quasi-linear diffusion models in
simplified geometries, we present a first-principles-informed framework that
models RE-whistler interactions in a 3D tokamak equilibrium. This is achieved
by coupling AORSA, which computes whistler eigenmodes for a given tokamak
plasma equilibrium, and KORC, a kinetic orbit code that tracks full orbit RE
trajectories in prescribed wave fields. Our results demonstrate that REs
undergo scattering to large pitch angles and exhibit anomalous diffusion in
both pitch-angle and kinetic energy space. Crucially, we observe a transition
between diffusive, sub-diffusive, and super-diffusive transport regimes as a
function of initial RE energy - an effect not captured by existing quasi-linear
models. This anomalous transport behavior represents a significant advancement
in understanding RE dynamics in the presence of wave - particle interactions.
By identifying the conditions under which anomalous diffusion arises, this work
lays the theoretical foundation for designing targeted, wave-based mitigation
strategies in future tokamak experiments.

</details>


<div id='physics.space-ph'></div>

# physics.space-ph [[Back]](#toc)

### [46] [The impact of Alfvenic shear flow on magnetic reconnection and turbulence](https://arxiv.org/abs/2509.12608)
*Tamar Ervin,Alfred Mallet,Stefan Eriksson,Marc Swisdak,James Juno,Orlando M. Romeo,Tai Phan,Trevor A. Bowen,Roberto Livi,Phyllis L. Whittlesey,Davin E. Larson,Stuart D. Bake*

Main category: physics.space-ph

TL;DR: PSP observations show magnetic reconnection absence in near-Sun solar wind current sheets, with velocity shear identified as a key suppression mechanism through tearing mode growth rate analysis.


<details>
  <summary>Details</summary>
Motivation: To understand why Parker Solar Probe observations show frequent absence of magnetic reconnection in near-Sun solar wind current sheets, and investigate whether velocity shear suppresses reconnection onset.

Method: Analyzed PSP Encounters 4-11 data (Jan 2020-March 2022), compared tearing mode growth rates with/without shear flow for reconnecting vs non-reconnecting periods, and examined wind stream types and magnetic field spectra.

Result: 85% of reconnection events occur in slow, non-Alfvenic wind; growth rates systematically larger during reconnection; reconnection suppressed in highly Alfvenic fast/slow wind with strong flow shear; magnetic field spectra steepen during reconnection periods.

Conclusion: Velocity shear from Alfvenic flows suppresses magnetic reconnection onset, explaining PSP's frequent observation of reconnection absence in near-Sun solar wind, with implications for turbulence dynamics and compressible fluctuation generation.

Abstract: Magnetic reconnection is a fundamental and omnipresent energy conversion
process in plasma physics. Novel observations of fields and particles from
Parker Solar Probe (PSP) have shown the absence of reconnection in a large
number of current sheets in the near-Sun solar wind. Using near-Sun
observations from PSP Encounters 4 to 11 (Jan 2020 to March 2022), we
investigate whether reconnection onset might be suppressed by velocity shear.
We compare estimates of the tearing mode growth rate in the presence of shear
flow for time periods identified as containing reconnecting current sheets
versus non-reconnecting times, finding systematically larger growth rates for
reconnection periods. Upon examination of the parameters associated with
reconnection onset, we find that 85% of the reconnection events are embedded in
slow, non-Alfvenic wind streams. We compare with fast, slow non-Alfvenic, and
slow Alfvenic streams, finding that the growth rate is suppressed in highly
Alfvenic fast and slow wind and reconnection is not seen in these wind types,
as would be expected from our theoretical expressions. These wind streams have
strong Alfvenic} flow shear, consistent with the idea of reconnection
suppression by such flows. This could help explain the frequent absence of
reconnection events in the highly Alfvenic, near-Sun solar wind observed by
PSP. Finally, we find a steepening of both the trace and magnitude magnetic
field spectra within reconnection periods in comparison to ambient wind. We tie
this to the dynamics of relatively balanced turbulence within these
reconnection periods and the potential generation of compressible fluctuations.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [47] [Virtual states and exponential decay in small-scale dynamo](https://arxiv.org/abs/2509.13206)
*A. V. Kopyev,A. S. Il'yn,V. A. Sirota,K. P. Zybin*

Main category: physics.flu-dyn

TL;DR: Kazantsev theory of small-scale dynamo generation at low Prandtl numbers near threshold shows power-law decay, but simulations show exponential decay. This discrepancy is resolved by showing exponential decay is temporary due to velocity correlator flattening at large scales.


<details>
  <summary>Details</summary>
Motivation: To reconcile the discrepancy between theoretical predictions (power-law decay below threshold) and numerical simulations (exponential decay) in small-scale dynamo generation at low Prandtl numbers.

Method: Developed Kazantsev theory near generation threshold, analyzed velocity correlator flattening effects, and solved corresponding Schrodinger-type equation to identify virtual level effects.

Result: Found that exponential decay is temporary and caused by flattening of velocity correlator at large scales, corresponding to a long-living virtual level. Determined critical Reynolds number and growth/decay increments in terms of velocity correlator properties.

Conclusion: The study restores concordance between theory and simulations by explaining the apparent exponential decay as a temporary effect, providing quantitative expressions for critical parameters that enable comparison across different simulation data.

Abstract: We develop the Kazantsev theory of small-scale dynamo generation at small
Prandtl numbers near the generation threshold and restore the concordance
between the theory and numerical simulations: the theory predicted a power-law
decay below the threshold, while simulations demonstrate exponential decay. We
show that the exponential decay is temporary and owes its existence to the
flattening of the velocity correlator at large scales. This effect corresponds
to the existence of a long-living virtual level in the corresponding
Schrodinger type equation. We also find the critical Reynolds number and the
increment of growth/decay above and under the threshold; we express them in
terms of the quantitative characteristic properties of the velocity correlator,
which makes it possible to compare the results with the data of different
simulations.

</details>


### [48] [Generalization of the viscous stress tensor to the case of non-small gradients of hydrodynamic velocity: a path to numerical modeling of turbulence non-locality](https://arxiv.org/abs/2509.12885)
*A. B. Kukushkin*

Main category: physics.flu-dyn

TL;DR: Extended Chapman-Enskog method for large velocity gradients to derive nonlocal viscous stress tensor, enabling better modeling of turbulence nonlocality and handling tangential discontinuities.


<details>
  <summary>Details</summary>
Motivation: Standard viscous stress tensor formulations struggle with describing tangential discontinuities and separated flows, particularly in turbulent media where nonlocal effects are important.

Method: Generalized the Chapman-Enskog method to accommodate large hydrodynamic velocity gradients, deriving an integral representation of the viscous stress tensor over spatial coordinates.

Result: Obtained a nonlocal viscous stress tensor that reduces to standard form for small disturbance path lengths, but can better handle complex flow phenomena and turbulence nonlocality.

Conclusion: The derived integral representation enables numerical modeling of turbulence nonlocality and can describe empirical Richardson t^3 law for pair correlations in turbulent media.

Abstract: Generalization of the Chapman-Enskog method to the case of large gradients of
hydrodynamic velocity allowed us to obtain an integral (over spatial
coordinates) representation of the viscous stress tensor in the Navier-Stokes
equation. In the case of small path lengths of the medium disturbance, the
tensor goes over to the standard form, which, as is known, is difficult to
apply to the description of tangential discontinuities and separated flows. The
obtained expression can allow numerical modeling of the nonlocality of
turbulence, expressed by the empirical Richardson t^3 law for pair correlations
in a turbulent medium.

</details>


### [49] [Attenuation of long waves through regions of irregular floating ice and bathymetry](https://arxiv.org/abs/2309.05581)
*Lloyd Dafydd,Richard Porter*

Main category: physics.flu-dyn

TL;DR: Revised theory for surface wave attenuation on random bathymetry that conserves energy, with applications to wave propagation through fragmented ice showing good agreement with field data.


<details>
  <summary>Details</summary>
Motivation: Existing theoretical models overpredict wave attenuation rates due to improper ensemble averaging methods in random depth fluctuations.

Method: Developed a revised approach using transfer matrix eigenvalues to accurately measure decay through finite sections of random bathymetry, adapting linearized long wavelength assumptions for both water depth variations and fragmented ice cover.

Result: New theory conserves energy and numerical simulations support predictions. For broken ice, theory reproduces key field data features including frequency-dependent attenuation (power 2-4) and high-frequency roll-over effect.

Conclusion: The corrected approach provides more accurate attenuation predictions for both random bathymetry and fragmented ice environments, successfully capturing observed physical phenomena in marginal ice zones.

Abstract: Existing theoretical results for attenuation of surface waves propagating on
water of random fluctuating depth are shown to over predict the rate of decay
due to the way in which ensemble averaging is performed. A revised approach is
presented which corrects this and is shown to conserve energy. New theoretical
predictions are supported by numerical results which use averaging of
simulations of wave scattering over finite sections of random bathymetry for
which transfer matrix eigenvalues are used to accurately measure decay. The
model of wave propagation used in this paper is derived from a linearised long
wavelength assumption whereby depth averaging leads to time harmonic waves
being represented as solutions to a simple ordinary differential equation. In
this paper it is shown how this can be adapted to incorporate a model of a
continuous covering of the surface by fragmented floating ice. Attenuation of
waves through broken ice of random thickness is then analysed in a similar
manner as bed variations previously and some comparisons are made with
published field data for attenuation of waves in the marginal ice zones. Key
features of the data are reproduced by theory including the attenuation being
proportional to a power of frequency between 2 and 4 as well as capturing the
"roll-over effect" at high frequencies.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [50] [On the attenuation of waves through broken ice of randomly-varying thickness on water of finite depth](https://arxiv.org/abs/2508.20099)
*Lloyd Dafydd,Richard Porter*

Main category: physics.ao-ph

TL;DR: Extension of wave attenuation theory in floating broken ice to non-shallow water depths, showing good agreement between theoretical predictions and numerical simulations with frequency-dependent attenuation patterns.


<details>
  <summary>Details</summary>
Motivation: To extend previous work on wave attenuation in floating broken ice with random thickness to account for non-shallow water depths, providing more realistic modeling of wave propagation in ice-covered waters.

Method: Multiple scales analysis of a theoretical model for broken floating ice, comparing theoretical predictions with numerical simulations from an approximate depth-averaged model derived under mild-slope assumption.

Result: Theoretical predictions show good agreement with numerical simulations. The theory predicts low-frequency attenuation proportional to the eighth power of frequency and a roll-over effect at higher frequencies.

Conclusion: The extended model successfully captures wave attenuation in non-shallow water conditions, with frequency-dependent behavior that aligns with numerical simulations and has implications for interpreting field measurements.

Abstract: The recent work of Dafydd and Porter [2024] on the attenuation of waves
propagating through floating broken ice of random thickness is extended to
consider water of non-shallow depth. A theoretical model of broken floating ice
is analysed using a multiple scales analysis to provide an explicit expression
for the attenuation of waves as they propagate from a region of constant
thickness ice into a semi-infinite region of ice whose thickness is a
slowly-varying random function of distance. Theoretical predictions are shown
to compare well to numerical simulations of scattering over long finite regions
of ice of randomly-varying thickness computed from an approximate
depth-averaged model derived under a mild-slope assumption. The theory predicts
a low-frequency attenuation proportional to the eighth power of frequency and a
roll-over effect at higher frequencies. The relationship between the results
and field measurements are discussed.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [51] [PBPK-iPINNs : Inverse Physics-Informed Neural Networks for Physiologically Based Pharmacokinetic Brain Models](https://arxiv.org/abs/2509.12666)
*Charuka D. Wickramasinghe,Krishanthi C. Weerasinghe,Pradeep K. Ranaweera*

Main category: stat.ML

TL;DR: PBPK-iPINN uses inverse Physics-Informed Neural Networks to estimate drug/patient parameters and concentration profiles in brain PBPK models, requiring careful loss weighting and parameter tuning for convergence.


<details>
  <summary>Details</summary>
Motivation: To advance PBPK modeling beyond classical compartmental approaches by leveraging PINNs for parameter estimation while ensuring predictions follow physical laws described by ODEs.

Method: Inverse PINNs approach with weighted loss function components (data loss, initial conditions loss, residual loss) and careful tuning of neural network parameters and collocation points.

Result: Demonstrates that appropriate loss weighting and parameter tuning are crucial for convergence to correct solutions in inverse PBPK problems.

Conclusion: PBPK-iPINN provides an effective framework for parameter estimation in PBPK brain compartment models, comparable to traditional numerical and statistical methods when properly configured.

Abstract: Physics-Informed Neural Networks (PINNs) leverage machine learning with
differential equations to solve direct and inverse problems, ensuring
predictions follow physical laws. Physiologically based pharmacokinetic (PBPK)
modeling advances beyond classical compartmental approaches by using a
mechanistic, physiology focused framework. A PBPK model is based on a system of
ODEs, with each equation representing the mass balance of a drug in a
compartment, such as an organ or tissue. These ODEs include parameters that
reflect physiological, biochemical, and drug-specific characteristics to
simulate how the drug moves through the body. In this paper, we introduce
PBPK-iPINN, a method to estimate drug-specific or patient-specific parameters
and drug concentration profiles in PBPK brain compartment models using inverse
PINNs. We demonstrate that, for the inverse problem to converge to the correct
solution, the loss function components (data loss, initial conditions loss, and
residual loss) must be appropriately weighted, and parameters (including number
of layers, number of neurons, activation functions, learning rate, optimizer,
and collocation points) must be carefully tuned. The performance of the
PBPK-iPINN approach is then compared with established traditional numerical and
statistical methods.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [52] [Emergent complexity and rhythms in evoked and spontaneous dynamics of human whole-brain models after tuning through analysis tools](https://arxiv.org/abs/2509.12873)
*Gianluca Gaglioti,Alessandra Cardinale,Cosimo Lupo,Thierry Nieus,Federico Marmoreo,Robin Gutzen,Michael Denker,Andrea Pigorini,Marcello Massimini,Simone Sarasso,Pier Stanislao Paolucci,Giulia De Bonis*

Main category: q-bio.NC

TL;DR: A framework combining TVB simulation platform and Cobrawap analysis pipeline enables effective parameter tuning for whole-brain models, producing more biologically realistic neural activity compared to default configurations.


<details>
  <summary>Details</summary>
Motivation: To develop a systematic approach for parameter tuning in whole-brain models that can reproduce realistic spontaneous and evoked neural activity across different scales, addressing the challenge of proper model configuration.

Method: Integrated The Virtual Brain (TVB) platform for whole-brain simulation with Collaborative Brain Wave Analysis Pipeline (Cobrawap) for standardized metric analysis. Applied to 998-node human connectome using Larter-Breakspear neural mass model with both default and Cobrawap-tuned parameters.

Result: The tuned configuration showed biologically relevant features absent in default model: non-stereotyped complex spatio-temporal activity in response to perturbations, robust alpha-band oscillations, infra-slow rhythms, scale-free characteristics, greater spatio-temporal heterogeneity, and asymmetric functional connectivity in spontaneous activity.

Conclusion: The TVB-Cobrawap combination demonstrates strong potential for guiding parameter tuning and provides groundwork for data-driven calibration and validation of accurate whole-brain models.

Abstract: The simulation of whole-brain dynamics should reproduce realistic spontaneous
and evoked neural activity across different scales, including emergent rhythms,
spatio-temporal activation patterns, and macroscale complexity. Once a
mathematical model is selected, its configuration must be determined by
properly setting its parameters. A critical preliminary step in this process is
defining an appropriate set of observables to guide the selection of model
configurations (parameter tuning), laying the groundwork for quantitative
calibration of accurate whole-brain models. Here, we address this challenge by
presenting a framework that integrates two complementary tools: The Virtual
Brain (TVB) platform for simulating whole-brain dynamics, and the Collaborative
Brain Wave Analysis Pipeline (Cobrawap) for analyzing the simulations using a
set of standardized metrics. We apply this framework to a 998-node human
connectome, using two configurations of the Larter-Breakspear neural mass
model: one with the TVB default parameters, the other tuned using Cobrawap. The
results reveal that the tuned configuration exhibits several biologically
relevant features, absent in the default model for both spontaneous and evoked
dynamics. In response to external perturbations, the tuned model generates
non-stereotyped, complex spatio-temporal activity, as measured by the
perturbational complexity index. In spontaneous activity, it displays robust
alpha-band oscillations, infra-slow rhythms, scale-free characteristics,
greater spatio-temporal heterogeneity, and asymmetric functional connectivity.
This work demonstrates the potential of combining TVB and Cobrawap to guide
parameter tuning and lays the groundwork for data-driven calibration and
validation of accurate whole-brain models.

</details>


<div id='physics.atom-ph'></div>

# physics.atom-ph [[Back]](#toc)

### [53] [GPU-Accelerated MATLAB Software for Atom-Ion Dynamics](https://arxiv.org/abs/2509.12381)
*Saajid Chowdhury,Jesús Pérez-Ríos*

Main category: physics.atom-ph

TL;DR: A MATLAB script called atomiongpu.m that uses GPU acceleration to simulate trapped ion-atom interactions, achieving up to 22x speedup over standard MATLAB ode45 solver.


<details>
  <summary>Details</summary>
Motivation: To enable efficient and massively parallel molecular dynamics simulations of trapped ion interactions with low-density atom clouds, which require solving complex trajectory calculations.

Method: Developed a specialized GPU-accelerated implementation (ode45gpu) of the Runge-Kutta algorithm optimized for MATLAB, packaged in atomiongpu.m script that handles various physical parameters and initial conditions.

Result: Achieved up to 22x faster simulation speeds compared to MATLAB's standard ode45 solver while maintaining flexibility for different atomic systems, interactions, and computational hardware.

Conclusion: The atomiongpu.m script provides an efficient, customizable, and high-performance solution for simulating trapped ion-atom interactions with GPU acceleration, enabling faster computation of complex formation probabilities and trajectory analysis.

Abstract: We present a MATLAB script which can use GPU acceleration to simulate a
trapped ion interacting with a low-density cloud of atoms. This script, called
atomiongpu.m, can massively parallelize MD simulations of trajectories of a
trapped ion and an atom starting far away. The script uses ode45gpu, which is
our optimized and specialized implementation of the Runge-Kutta algorithm used
in MATLAB's ODE solver ode45. We first discuss the physical system and show how
ode45gpu can solve it up to 22x faster than MATLAB's ode45. Then, we show how
to easily modify the inputs to atomiongpu.m to account for different kinds of
atoms, ions, atom-ion interactions, trap potentials, simulation parameters,
initial conditions, and computational hardware, so that atomiongpu.m
automatically finds the probability of complex formation, the distribution of
observables such as the scattering angle and complex lifetime, and plots of
specific trajectories.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [54] [Leveraging Machine Learning Force Fields (MLFFs) to Simulate Large Atomistic Systems for Fidelity Improvement of Superconducting Qubits and Sensors](https://arxiv.org/abs/2509.12509)
*Søren Smidstrup,Shela Aboud,Ricardo Borges,Anders Blom,Pankaj Aggarwal,Robert Freeman,Jamil Kawa*

Main category: quant-ph

TL;DR: QuantumATK tool combines DFT with machine learning and many-body physics to address limitations in modeling quantum materials for qubits and sensors, enabling large-scale simulations of interfaces, surface states, and thermal properties.


<details>
  <summary>Details</summary>
Motivation: Traditional DFT fails to capture complete physics for quantum materials including superconductivity dynamics, surface states, and has limitations in system size for thermal property simulations needed in quantum computing applications.

Method: Combines DFT with LCAO basis sets, non-equilibrium Green's functions, machine-learned force fields, and adds electron-electron interactions to single-particle tight-binding models to create realistic double-quantum dot systems.

Result: Enables computation of superconductor-insulator interface characteristics, topological insulator surface states, large-scale thermal property simulations, and realistic modeling of superconducting qubits as two-level systems.

Conclusion: The integrated approach provides comprehensive atomistic modeling capabilities for quantum material development, overcoming traditional DFT limitations and enabling accurate simulation of key quantum computing components.

Abstract: Materials engineering using atomistic modeling is an essential tool for the
development of qubits and quantum sensors. Traditional density-functional
theory (DFT) does however not adequately capture the complete physics involved,
including key aspects and dynamics of superconductivity, surface states, etc.
There are also significant challenges regarding the system sizes that can be
simulated, not least for thermal properties which are key in quantum-computing
applications. The QuantumATK tool combines DFT, based on LCAO basis sets, with
non-equilibrium Green's functions, to compute the characteristics of interfaces
between superconductors and insulators, as well as the surface states of
topological insulators. Additionally, the software leverages machine-learned
force-fields to simulate thermal properties and to generate realistic amorphous
geometries in large-scale systems. Finally, the description of superconducting
qubits and sensors as two-level systems modeled with a double-well potential
requires many-body physics, and this paper demonstrates how electron-electron
interaction can be added to the single-particle energy levels from an atomistic
tight-binding model to describe a realistic double-quantum dot system.

</details>


### [55] [Mitigating the sign problem by quantum computing](https://arxiv.org/abs/2509.13017)
*Kwai-Kong Ng,Min-Fong Yang*

Main category: quant-ph

TL;DR: The paper critically examines the qc-SSE method's claim of resolving the sign problem in quantum Monte Carlo simulations, showing it doesn't strictly solve the issue but provides mitigation through energy shifts that suppress negative weights.


<details>
  <summary>Details</summary>
Motivation: To address the severe limitations of quantum Monte Carlo simulations caused by the sign problem, which makes statistical errors grow exponentially with system size and inverse temperature.

Method: Analyzed the qc-SSE framework using antiferromagnetic anisotropic XY chain as test case, examined dependence of average sign on various parameters, and introduced operator contraction method to improve efficiency.

Result: Moderate energy shifts optimally balance sign mitigation and statistical accuracy, while large shifts amplify errors. The method alleviates but does not resolve the sign problem for Hamiltonians with non-commuting terms.

Conclusion: The qc-SSE method provides practical mitigation rather than strict resolution of the sign problem, offering a strategy to suppress negative weights through carefully tuned energy shifts.

Abstract: The notorious sign problem severely limits the applicability of quantum Monte
Carlo (QMC) simulations, as statistical errors grow exponentially with system
size and inverse temperature. A recent proposal of a quantum-computing
stochastic series expansion (qc-SSE) method suggested that the problem could be
avoided by introducing constant energy shifts into the Hamiltonian. Here we
critically examine this framework and show that it does not strictly resolve
the sign problem for Hamiltonians with non-commuting terms. Instead, it
provides a practical mitigation strategy that suppresses the occurrence of
negative weights. Using the antiferromagnetic anisotropic XY chain as a test
case, we analyze the dependence of the average sign on system size,
temperature, anisotropy, and shift parameters. An operator contraction method
is introduced to improve efficiency. Our results demonstrate that moderate
shifts optimally balance sign mitigation and statistical accuracy, while large
shifts amplify errors, leaving the sign problem unresolved but alleviated.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [56] [Curriculum Learning for Mesh-based simulations](https://arxiv.org/abs/2509.13138)
*Paul Garnier,Vincent Lannelongue,Elie Hachem*

Main category: cs.LG

TL;DR: Coarse-to-fine curriculum learning accelerates GNN training for CFD by starting with coarse meshes and progressively increasing resolution, reducing training time by 50% while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Training graph neural networks on high-resolution unstructured meshes for computational fluid dynamics is computationally expensive and time-consuming.

Method: A coarse-to-fine curriculum approach where the model trains first on very coarse meshes, then progressively introduces medium and high resolutions (up to 300,000 nodes) without changing the model architecture.

Result: Achieved comparable generalization accuracy while reducing total wall-clock training time by up to 50%. Also enabled models to break through plateaus when lacking capacity to learn underlying physics.

Conclusion: Curriculum learning with progressive mesh resolution provides an effective way to accelerate GNN training for CFD applications without architectural changes, offering significant time savings and improved learning capability.

Abstract: Graph neural networks (GNNs) have emerged as powerful surrogates for
mesh-based computational fluid dynamics (CFD), but training them on
high-resolution unstructured meshes with hundreds of thousands of nodes remains
prohibitively expensive. We study a \emph{coarse-to-fine curriculum} that
accelerates convergence by first training on very coarse meshes and then
progressively introducing medium and high resolutions (up to \(3\times10^5\)
nodes). Unlike multiscale GNN architectures, the model itself is unchanged;
only the fidelity of the training data varies over time. We achieve comparable
generalization accuracy while reducing total wall-clock time by up to 50\%.
Furthermore, on datasets where our model lacks the capacity to learn the
underlying physics, using curriculum learning enables it to break through
plateaus.

</details>


### [57] [Nonlocal Neural Tangent Kernels via Parameter-Space Interactions](https://arxiv.org/abs/2509.12467)
*Sriram Nagaraj,Vishakh Hari*

Main category: cs.LG

TL;DR: Proposes Nonlocal Neural Tangent Kernel (NNTK) to extend NTK theory to non-smooth functions and broader model classes by replacing local gradients with nonlocal approximations.


<details>
  <summary>Details</summary>
Motivation: The standard Neural Tangent Kernel framework assumes differentiability, which breaks down for non-smooth target functions and models with non-differentiable behavior, limiting its applicability.

Method: Introduces a Nonlocal Neural Tangent Kernel that uses nonlocal interaction-based approximations instead of local gradients, enabling analysis of nonsmooth functions and stochastic estimators through fixed-kernel and attention-based formulations.

Result: The NNTK framework extends NTK theory to handle non-smooth functions and broader model families, with numerical studies demonstrating its effectiveness.

Conclusion: The proposed Nonlocal Neural Tangent Kernel successfully generalizes the NTK framework to non-smooth settings, expanding its theoretical reach and practical applicability to a wider range of neural network behaviors.

Abstract: The Neural Tangent Kernel (NTK) framework has provided deep insights into the
training dynamics of neural networks under gradient flow. However, it relies on
the assumption that the network is differentiable with respect to its
parameters, an assumption that breaks down when considering non-smooth target
functions or parameterized models exhibiting non-differentiable behavior. In
this work, we propose a Nonlocal Neural Tangent Kernel (NNTK) that replaces the
local gradient with a nonlocal interaction-based approximation in parameter
space. Nonlocal gradients are known to exist for a wider class of functions
than the standard gradient. This allows NTK theory to be extended to nonsmooth
functions, stochastic estimators, and broader families of models. We explore
both fixed-kernel and attention-based formulations of this nonlocal operator.
We illustrate the new formulation with numerical studies.

</details>


<div id='hep-ex'></div>

# hep-ex [[Back]](#toc)

### [58] [Ionization and temperature measurements in warm dense copper using x-ray absorption spectroscopy](https://arxiv.org/abs/2509.13272)
*T. Cordova,E. V. Marley,D. A. Chin,R. A. London,H. A. Scott,T. Döppner,F. N. Beg,F. Coppari,M. Millot,J. Emig,S. B. Hansen,P. M. Nilson,P. Sterne,M. J. MacDonald*

Main category: hep-ex

TL;DR: Experimental study of warm dense copper plasmas at 15-25 g/cm³ density and 10-21 eV temperatures, measuring ionization states and K-shell absorption shifts using x-ray spectroscopy.


<details>
  <summary>Details</summary>
Motivation: To provide experimental data for improving ionization and opacity models in the warm dense matter regime, which is crucial for understanding high-energy density physics and astrophysical phenomena.

Method: Symmetric shock compression of buried copper layers at OMEGA Laser Facility to generate uniform warm dense matter conditions, followed by probing with laser-generated x-ray source to collect K-shell absorption spectra and fitting bound-bound absorption contributions.

Result: Estimated average ionization state (Z̄) of 4-7, K-edge shifts of 12-30 eV, and bound-bound resonance absorption shifts of 4-26 eV relative to cold copper K-edge.

Conclusion: The study provides essential experimental validation data that will help refine theoretical models of ionization and opacity in warm dense matter systems.

Abstract: We detail experimental results inferring ionization and temperature for warm
dense copper plasmas at several times solid density (15 to 25 g/cm$^3$) and
temperatures of 10 to 21 eV. Experiments performed at the OMEGA Laser Facility
generate uniform warm dense matter conditions via symmetric shock compression
of a buried copper layer. The plasma is probed with a laser-generated x-ray
source to collect the K-shell x-ray absorption spectrum. Fitting bound-bound
absorption contributions from constituent charge states of copper provides an
estimated $\overline{Z}$ of approximately 4 to 7 for these warm dense copper
plasmas. We find that these partially ionized plasmas have K-edge shifts of 12
to 30 eV and bound-bound resonance 1s$\rightarrow$3p absorption shifts of 4 to
26 eV with respect to the cold K-edge. This study provides necessary
experimental data to improve ionization and opacity models in the warm dense
matter regime.

</details>


<div id='math.SP'></div>

# math.SP [[Back]](#toc)

### [59] [On Courant-type bounds and spectral partitioning via Neumann domains on quantum graphs](https://arxiv.org/abs/2509.13228)
*Luís Baptista,Matthias Hofmann*

Main category: math.SP

TL;DR: Analysis of eigenfunctions on quantum graphs, focusing on Morse eigenfunctions and their nodal/Neumann domains. Establishes upper bounds for nodal points and connects Neumann domains to spectral minimal partitions, particularly on tree graphs.


<details>
  <summary>Details</summary>
Motivation: To understand the structure of Laplacian eigenfunctions on quantum graphs and explore the relationship between Neumann domains of eigenfunctions and spectral minimal partition problems.

Method: Uses Courant-type arguments to establish bounds on nodal points, analyzes conditions where Neumann domains correspond to minimizers of spectral partition problems, and focuses on tree graphs with genericity assumptions.

Result: Characterizes spectral energies of partitions on tree graphs and relates them to Laplacian eigenvalues. Introduces a Courant-sharpness analog for Neumann counts and shows when spectral minimal partitions coincide with Neumann domain partitions.

Conclusion: The paper provides new insights into the connection between eigenfunction structure and spectral partition problems on quantum graphs, particularly establishing relationships between Neumann domains and optimal partitions on tree graphs.

Abstract: We study the structure of eigenfunctions of the Laplacian on quantum graphs,
with a particular focus on Morse eigenfunctions via nodal and Neumann domains.
Building on Courant-type arguments, we establish upper bounds for the number of
nodal points and explore conditions under which Neumann domains of
eigenfunctions correspond to minimizers to a class of spectral partition
problems often known as spectral minimal partitions. The main focus will be the
analysis on tree graphs, where we characterize the spectral energies of such
partitions and relate them to the eigenvalues of the Laplacian under genericity
assumptions. Notably, we introduce a notion analogous to Courant-sharpness for
Neumann counts and demonstrate when spectral minimal partitions coincide with
partitions formed by Neumann domains of eigenfunctions.

</details>


<div id='hep-lat'></div>

# hep-lat [[Back]](#toc)

### [60] [Accurate ground states of $SU(2)$ lattice gauge theory in 2+1D and 3+1D](https://arxiv.org/abs/2509.12323)
*Thomas Spriggs,Eliska Greplova,Juan Carrasquilla,Jannes Nys*

Main category: hep-lat

TL;DR: Neural network wavefunction framework for solving non-Abelian lattice gauge theories using SU(2) equivariant neural networks and physics-inspired ansatz to parameterize ground state wavefunctions in 2+1D and 3+1D.


<details>
  <summary>Details</summary>
Motivation: To develop a method for studying lattice gauge theories beyond one dimension that avoids the sign problem and gauge group discretization, while enabling efficient scaling to larger systems.

Method: Combination of SU(2) equivariant neural networks with SU(2) invariant physics-inspired ansatz to learn ground state wavefunction parameterization in Hamiltonian formulation, benchmarked against invariant-only ansatz.

Result: Achieved strong agreement with perturbative expansions for ground state energy, Creutz ratio, and average Wilson loop. Demonstrated superiority of gauge equivariant transformations over invariant-only approaches.

Conclusion: The framework successfully opens new avenues for studying higher-dimensional lattice gauge theories with efficient scaling, while avoiding sign problems and gauge group discretization issues.

Abstract: We present a neural network wavefunction framework for solving non-Abelian
lattice gauge theories in a continuous group representation. Using a
combination of $SU(2)$ equivariant neural networks alongside an $SU(2)$
invariant, physics-inspired ansatz, we learn a parameterization of the ground
state wavefunction of $SU(2)$ lattice gauge theory in 2+1 and 3+1 dimensions.
Our method, performed in the Hamiltonian formulation, has a straightforward
generalization to $SU(N)$. We benchmark our approach against a solely invariant
ansatz by computing the ground state energy, demonstrating the need for bespoke
gauge equivariant transformations. We evaluate the Creutz ratio and average
Wilson loop, and obtain results in strong agreement with perturbative
expansions. Our method opens up an avenue for studying lattice gauge theories
beyond one dimension, with efficient scaling to larger systems, and in a way
that avoids both the sign problem and any discretization of the gauge group.

</details>
