<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 7]
- [math.AP](#math.AP) [Total: 20]
- [physics.comp-ph](#physics.comp-ph) [Total: 3]
- [physics.acc-ph](#physics.acc-ph) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 3]
- [cs.CE](#cs.CE) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 2]
- [nlin.CD](#nlin.CD) [Total: 1]
- [cond-mat.mes-hall](#cond-mat.mes-hall) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [cs.LG](#cs.LG) [Total: 5]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Convergence of Pivoted Cholesky Algorithm for Lipschitz Kernels](https://arxiv.org/abs/2509.13582)
*Sungwoo Jeong,Alex Townsend*

Main category: math.NA

TL;DR: Analysis of pivoted Cholesky algorithm for kernel matrices shows uniform residual bounds proportional to fill distance, with convergence rates O(n^{-1/d}) for Lipschitz kernels and O(n^{-2/d}) for differentiable kernels with Lipschitz derivatives.


<details>
  <summary>Details</summary>
Motivation: Bridge the gap between theory and practice where previous analyses required C^2-regularity of kernels for convergence proofs, while empirical evidence showed robust performance even for non-differentiable kernels.

Method: Theoretical analysis of the continuous analogue of Cholesky factorization, establishing quantitative convergence guarantees for kernels with minimal smoothness conditions (Lipschitz continuity and differentiability with Lipschitz derivatives).

Result: Proved that for symmetric positive definite Lipschitz continuous kernels, the residual is uniformly bounded by a constant multiple of the fill distance of pivots. Under complete pivoting: ||R_n||_∞ = O(n^{-1/d}) for Lipschitz kernels, improving to O(n^{-2/d}) for differentiable kernels with Lipschitz derivatives.

Conclusion: The analysis closes the theory-practice gap, providing rigorous convergence guarantees for pivoted Cholesky algorithm under minimal smoothness assumptions, with implications for discrete analogues, Gaussian process regression, and P-greedy interpolation methods.

Abstract: We investigate the continuous analogue of the Cholesky factorization, namely
the pivoted Cholesky algorithm. Our analysis establishes quantitative
convergence guarantees for kernels of minimal smoothness. We prove that for a
symmetric positive definite Lipschitz continuous kernel $K:\Omega\times \Omega
\rightarrow \mathbb{R}$ on a compact domain $\Omega\subset\mathbb{R}^d$, the
residual of the Cholesky algorithm with any pivoting strategy is uniformly
bounded above by a constant multiple of the fill distance of pivots. In
particular, our result implies that under complete pivoting (where the maximum
value of the diagonal of the residual is selected as the next pivot):
\begin{equation*}
  \|R_n\|_{\infty} = \mathcal{O}(n^{-1/d}), \end{equation*} where $R_n$ is the
residual after $n$ Cholesky steps and $\|\cdot\|_\infty$ is the absolute
maximum value of $R_n$. Moreover, if $K$ is differentiable in both variables
with a Lipschitz derivative, our convergence rate improves to
$\mathcal{O}(n^{-2/d})$. Our result closes a gap between theory and practice as
previous analyses required $C^2$-regularity of $K$ to establish convergence,
whereas empirical evidence indicated robust performance even for
non-differentiable kernels. We further detail how our convergence results
propagate to downstream applications, including discrete analogues, Gaussian
process regression, and the P-greedy interpolation method.

</details>


### [2] [p-multigrid method for the discontinuous Galerkin discretization of elliptic problems](https://arxiv.org/abs/2509.13669)
*Nuo Lei,Donghang Zhang,Weiying Zheng*

Main category: math.NA

TL;DR: Proposed W-cycle p-multigrid method for SIPDG discretization of elliptic problems with improved convergence analysis reducing smoothing steps from O(p²) to O(p) while maintaining mesh size independence.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient multigrid method for solving symmetric interior penalty discontinuous Galerkin discretizations of elliptic problems, improving upon previous theoretical results that required more smoothing steps.

Method: W-cycle p-multigrid method using hierarchical Legendre polynomial basis functions, with rigorous convergence analysis for both inherited and non-inherited bilinear forms of SIPDG discretization.

Result: Theoretical analysis shows significant improvement over previous work, reducing required smoothing steps from O(p²) to O(p) while maintaining mesh size independence. Numerical experiments verify the theoretical findings and demonstrate effectiveness for unfitted finite element discretization of elliptic interface problems.

Conclusion: The proposed p-multigrid method provides an efficient solver for SIPDG discretizations with improved convergence properties and practical applicability to complex interface problems.

Abstract: In this paper, we propose a $W$-cycle $p$-multigrid method for solving the
$p$-version symmetric interior penalty discontinuous Galerkin (SIPDG)
discretization of elliptic problems. This SIPDG discretization employs
hierarchical Legendre polynomial basis functions. Inspired by the uniform
convergence theory of the $W$-cycle $hp$-multigrid method in [P. F. Antonietti,
et al., SIAM J. Numer. Anal., 53 (2015)], we provide a rigorous convergence
analysis for the proposed $p$-multigrid method, considering both inherited and
non-inherited bilinear forms of SIPDG discretization. Our theoretical results
show significant improvement over [P. F. Antonietti, et al., SIAM J. Numer.
Anal., 53 (2015)], reducing the required number of smoothing steps from
$O(p^2)$ to $O(p)$, where $p$ is the polynomial degree of the discrete broken
polynomial space. Moreover, the convergence rate remains independent of the
mesh size. Several numerical experiments are presented to verify our
theoretical findings. Finally, we numerically verify the effectiveness of the
$p$-multigrid method for unfitted finite element discretization in solving
elliptic interface problems on general $C^{2} $-smooth interfaces.

</details>


### [3] [Semi-Discrete in Time Method for Time-Dependent Equations by Random Neural Basis](https://arxiv.org/abs/2509.13751)
*Guihong Wang,Zheng-An Chen,Tao Luo*

Main category: math.NA

TL;DR: Proposes a semi-discrete time method combining classical numerical time integrators with random neural basis functions to improve accuracy and efficiency for time-dependent PDEs.


<details>
  <summary>Details</summary>
Motivation: Neural network-based PDE solvers face challenges in accuracy and computational efficiency, particularly when coupling space and time in a single network increases approximation difficulty.

Method: Semi-discrete in time method (SDTM) that leverages classical numerical time integrators and random neural basis (RNB), with adaptive operations to capture multi-scale features and ensure uniform approximation accuracy.

Result: Numerical experiments demonstrate the framework's effectiveness and confirm convergence of the temporal integrator as well as the network's approximation performance.

Conclusion: The proposed SDTM approach successfully addresses accuracy and efficiency challenges in neural network-based PDE solvers for time-dependent problems, particularly for multi-scale PDEs.

Abstract: Neural network-based solvers for partial differential equations (PDEs) have
attracted considerable attention, yet they often face challenges in accuracy
and computational efficiency. In this work, we focus on time-dependent PDEs and
observe that coupling space and time in a single network can increase the
difficulty of approximation. To address this, we propose a semi-discrete in
time method (SDTM) which leverages classical numerical time integrators and
random neural basis (RNB). Additional adaptive operations are introduced to
enhance the network's ability to capture features across scales to ensure
uniform approximation accuracy for multi-scale PDEs. Numerical experiments
demonstrate the framework's effectiveness and confirm the convergence of the
temporal integrator as well as the network's approximation performance.

</details>


### [4] [Hierarchical Importance Sampling for Estimating Occupation Time for SDE Solutions](https://arxiv.org/abs/2509.13950)
*Eya Ben Amar,Nadhir Ben Rached,Raul Tempone*

Main category: math.NA

TL;DR: Optimal importance sampling methods for rare event estimation of occupation time distributions using stochastic optimal control theory, with single-level and multilevel approaches that achieve better than O(2) complexity.


<details>
  <summary>Details</summary>
Motivation: Efficient estimation of rare events in occupation time distributions for stochastic processes is computationally challenging due to the rarity of events, requiring variance reduction techniques for practical computation.

Method: Develops optimal single-level IS estimator using HJB-PDE solution, extends to multilevel IS with common likelihood formulation, and establishes conditions for MLIS superiority over SLIS with complexity analysis.

Result: Theoretical framework shows MLIS can achieve better than order two complexity, with numerical experiments demonstrating benefits for fade duration estimation problems.

Conclusion: The proposed multilevel importance sampling approach provides computationally efficient estimation of rare events in occupation time distributions, with proven theoretical advantages and practical validation.

Abstract: This study considers the estimation of the complementary cumulative
distribution function of the occupation time (i.e., the time spent below a
threshold) for a process governed by a stochastic differential equation. The
focus is on the right tail, where the underlying event becomes rare, and using
variance reduction techniques is essential to obtain computationally efficient
estimates. Building on recent developments that relate importance sampling (IS)
to stochastic optimal control, this work develops an optimal single level IS
(SLIS) estimator based on the solution of an auxiliary Hamilton Jacobi Bellman
(HJB) partial differential equation (PDE). The cost of solving the HJB-PDE is
incorporated into the total computational work, and an optimized trade off
between preprocessing and sampling is proposed to minimize the overall cost.
The SLIS approach is extended to the multilevel setting to enhance efficiency,
yielding a multilevel IS (MLIS) estimator. A necessary and sufficient condition
under which the MLIS method outperforms the SLIS method is established, and a
common likelihood MLIS formulation is introduced that satisfies this condition
under appropriate regularity assumptions. The classical multilevel Monte Carlo
complexity theory can be extended to accommodate settings where the
single-level variance varies with the discretization level. As a special case,
the variance-decay behavior observed in the IS framework stems from the zero
variance property of the optimal control. Notably, the total work complexity of
MLIS can be better than an order of two. Numerical experiments in the context
of fade duration estimation demonstrate the benefits of the proposed approach
and validate these theoretical results.

</details>


### [5] [Low-rank approximation of analytic kernels](https://arxiv.org/abs/2509.14017)
*Marcus Webb*

Main category: math.NA

TL;DR: Framework for bounding low-rank approximation error of matrices from analytically continuable kernels, using Zolotarev rational functions for computable approximations.


<details>
  <summary>Details</summary>
Motivation: Understanding why nearly-low-rank structure occurs in matrices from kernel samples is essential for analyzing and developing algorithms in scientific computing and data science.

Method: Uses rational interpolation with roots and poles of Zolotarev rational functions to construct computable low-rank approximations for matrices from analytically continuable kernels.

Result: Provides a framework for bounding best low-rank approximation error and leads to a fast algorithm for constructing these approximations.

Conclusion: The approach elegantly connects analytical continuation properties with computable rational approximations, enabling efficient low-rank matrix approximations for kernels with complex analytic properties.

Abstract: Many algorithms in scientific computing and data science take advantage of
low-rank approximation of matrices and kernels, and understanding why
nearly-low-rank structure occurs is essential for their analysis and further
development. This paper provides a framework for bounding the best low-rank
approximation error of matrices arising from samples of a kernel that is
analytically continuable in one of its variables to an open region of the
complex plane. Elegantly, the low-rank approximations used in the proof are
computable by rational interpolation using the roots and poles of Zolotarev
rational functions, leading to a fast algorithm for their construction.

</details>


### [6] [Mixed finite element projection methods for the unsteady Brinkman equations](https://arxiv.org/abs/2509.14059)
*Costanza Aricò,Rainer Helmig,Ivan Yotov*

Main category: math.NA

TL;DR: H(div)-conforming mixed finite element methods for unsteady Brinkman equations using projection scheme with incremental pressure correction, achieving pointwise divergence-free velocity and robustness in both Stokes and Darcy regimes.


<details>
  <summary>Details</summary>
Motivation: To develop efficient numerical methods for incompressible single-phase flow with porous solid inclusions that are robust across different flow regimes (Stokes and Darcy) and provide divergence-free velocity solutions.

Method: Projection scheme with incremental pressure correction using Raviart-Thomas or Brezzi-Douglas-Marini mixed finite element spaces on simplicial grids. Predictor problem handles viscous effects (stress-velocity formulation), projection problem handles incompressibility (velocity-pressure formulation).

Result: Unconditionally stable fully-discrete scheme with first-order time accuracy. Velocity is pointwise divergence-free at each time step. Methods are robust in both Stokes and Darcy regimes. Efficient implementation with local elimination of variables reduces computational complexity.

Conclusion: The proposed H(div)-conforming mixed finite element methods provide an effective and robust approach for solving unsteady Brinkman equations, offering divergence-free solutions and good performance across different flow regimes with efficient computational implementation.

Abstract: We present $H(\text{div})$-conforming mixed finite element methods for the
unsteady Brinkman equations for incompressible single-phase flow with fixed in
space porous solid inclusions. We employ a projection scheme with incremental
pressure correction, which requires at each time step the solution of a
predictor and a projection problem. The predictor problem, which uses a
stress-velocity mixed formulation, accounts for the viscous effects, while the
projection problem, which is based on a velocity-pressure mixed formulation,
accounts for the incompressibility. The spatial discretization is based on the
Raviart-Thomas or Brezzi-Douglas-Marini mixed finite element spaces on
simplicial grids. The velocity computed at the end of each time step is
pointwise divergence-free. Unconditional stability of the fully-discrete scheme
and first order in time accuracy are established. Due the $H$(div)-conformity
of the formulation, the methods are robust in both the Stokes and the Darcy
regimes. In the specific code implementation, we discretize the computational
domain using generally unstructured triangular (in 2D) and tetrahedral (in 3D)
grids, and we use the Raviart--Thomas space $RT_1$, applying a second order
multipoint flux mixed finite element scheme with a quadrature rule that samples
the flux degrees of freedom. In the predictor problem this allows for a local
elimination of the viscous stress and results in element-based symmetric and
positive definite systems for each velocity component with $\left(d+1\right)$
degrees of freedom per simplex (where $d$ is the dimension of the problem). In
a similar way, we locally eliminate the corrected velocity in the projection
problem, and solve an element-based system for the pressure. A series of
challenging numerical experiments is presented to verify the convergence and
performance of the proposed scheme

</details>


### [7] [A numerical scheme for a fully nonlinear free boundary problem](https://arxiv.org/abs/2509.14150)
*Edgard A. Pimentel,Ercília Sousa*

Main category: math.NA

TL;DR: A numerical method for approximating viscosity solutions of fully nonlinear free transmission problems using a two-layer regularization approach with fixed-point arguments and vanishing parameters.


<details>
  <summary>Details</summary>
Motivation: To develop a computational strategy for solving fully nonlinear free boundary models, which are challenging due to their complex nature involving free transmission problems and viscosity solutions.

Method: Discretizes a two-layer regularization of the PDE involving a functional and vanishing parameter, handled via fixed-point arguments. Proves convergence to a one-parameter regularization and uses regularity estimates to recover viscosity solutions.

Result: The numerical method converges to regularized solutions and successfully recovers viscosity solutions of free transmission problems. Two numerical examples validate the method's effectiveness.

Conclusion: The paper presents a successful computational strategy based on fixed-point arguments and approximated problems for solving fully nonlinear free boundary models, with numerical validation demonstrating its practical utility.

Abstract: We propose a numerical method to approximate viscosity solutions of fully
nonlinear free transmission problems. The method discretises a two-layer
regularisation of a PDE, involving a functional and a vanishing parameter. The
former is handled via a fixed-point argument. We then prove that the numerical
method converges to a one-parameter regularisation of the free boundary
problem. Regularity estimates enable us to take the vanishing limit of such a
parameter and recover a viscosity solution of the free transmission problem.
Our main contribution is the design of a computational strategy, based on
fixed-point arguments and approximated problems, to solve fully nonlinear free
boundary models. We finish the paper with two numerical examples to validate
our method.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [8] [Global propagation of singularities for magnetic mechanical systems](https://arxiv.org/abs/2509.13433)
*Piermarco Cannarsa,Wei Cheng,Jiahui Hong,Wenxue Wei*

Main category: math.AP

TL;DR: Singularities propagate globally for viscosity solutions of magnetic Hamilton-Jacobi equations on closed Riemannian manifolds, with singular sets remaining invariant under gradient flow dynamics.


<details>
  <summary>Details</summary>
Motivation: To understand how singularities behave and propagate in magnetic mechanical systems described by Hamilton-Jacobi equations on Riemannian manifolds, particularly investigating the invariance of singular sets under flow dynamics.

Method: Combines three key techniques: reduction from magnetic to Riemannian systems, analysis of reparameterized flows, and regularization methods. Uses geometric approach rather than analytic methods to better understand the underlying Riemannian structure.

Result: Proves that for any weak KAM solution, the singular set remains invariant under generalized gradient flow dynamics. Establishes necessary conditions for singularity existence, particularly when Euler characteristic is nonzero and magnetic form is non-exact.

Conclusion: The geometric method provides clearer insights into Riemannian structure compared to previous analytic approaches. The approach is specific to Riemannian metrics and does not extend to Finsler metrics due to structural differences.

Abstract: We prove that singularities propagate globally for viscosity solutions of
Hamilton-Jacobi equations related to magnetic mechanical systems on closed
Riemannian manifolds. Our main result shows that for any weak KAM solution $u$,
the singular set $\text{Sing}\,(u)$ remains invariant under the generalized
gradient flow dynamics. The proof combines three key elements: (1) reduction
from magnetic to Riemannian systems, (2) analysis of reparameterized flows, and
(3) regularization techniques. Compared to previous analytic approaches, our
geometric method provides clearer insights into the underlying Riemannian
structure. We also establish necessary conditions for singularity existence,
particularly when the Euler characteristic is nonzero and the magnetic form is
non-exact. This approach does not extend directly to Finsler metrics due to
structural differences.

</details>


### [9] [Scattering for the $1d$ NLS with inhomogeneous nonlinearities](https://arxiv.org/abs/2509.13438)
*Luke Baker,Jason Murphy*

Main category: math.AP

TL;DR: Large-data scattering in H^1 for 1D inhomogeneous nonlinear Schrödinger equations with p>2, using concentration-compactness method and Morawetz estimates to preclude compact solutions.


<details>
  <summary>Details</summary>
Motivation: To establish scattering properties for inhomogeneous nonlinear Schrödinger equations in one dimension, particularly for powers greater than 2, which extends previous results and addresses cases with nonnegative repulsive inhomogeneities.

Method: Uses the concentration-compactness method combined with contradiction approach. Employs Morawetz estimates in the style of Nakanishi to rule out the existence of compact solutions.

Result: Proves large-data scattering in H^1 space for one-dimensional inhomogeneous nonlinear Schrödinger equations with powers p>2, under the conditions that the inhomogeneity is nonnegative and repulsive, with additional decay requirements at infinity for 2<p≤4.

Conclusion: The method successfully establishes scattering results for a broader class of nonlinear Schrödinger equations by combining concentration-compactness techniques with specialized Morawetz estimates, providing important insights into the long-time behavior of solutions to these equations.

Abstract: We prove large-data scattering in $H^1$ for inhomogeneous nonlinear
Schr\"odinger equations in one space dimension for powers $p>2$. We assume the
inhomogeneity is nonnegative and repulsive; we additionally require decay at
infinity in the case $2<p\leq 4$. We use the method of
concentration-compactness and contradiction, utilizing a Morawetz estimate in
the style of Nakanishi in order to preclude the existence of compact solutions.

</details>


### [10] [Critical p-biharmonic problems and applications to Hamiltonian systems](https://arxiv.org/abs/2509.13596)
*Kanishka Perera,Bruno Ribeiro*

Main category: math.AP

TL;DR: Study of fourth-order quasilinear elliptic problems with p-biharmonic operator and critical Sobolev growth. Uses Hamiltonian system reduction and handles both resonant/non-resonant cases to show existence of non-trivial solutions.


<details>
  <summary>Details</summary>
Motivation: Extend classical results for Laplacian problems to p-biharmonic equations and critical Hamiltonian systems, improving earlier work and covering both homogeneous and nonhomogeneous settings.

Method: Reduction from Hamiltonian system to single p-biharmonic equation via inversion step. Combines concentration-compactness with abstract critical point method based on cohomological index.

Result: Proves existence of non-trivial solutions when forcing term and superscaled perturbation are sufficiently small, for both resonant and non-resonant cases.

Conclusion: The approach successfully extends Tarantello's classical results to p-biharmonic problems and provides new existence theorems for critical Hamiltonian systems.

Abstract: We study fourth-order quasilinear elliptic problems that involve the
p-biharmonic operator and Navier boundary conditions. The nonlinear term grows
at the critical Sobolev rate. Starting from a Hamiltonian system of two
second-order equations, we use an inversion step to reduce it to a single
p-biharmonic equation with a lower-order perturbation. We handle both
non-resonant and resonant cases and show that the problem admits non-trivial
solutions when the forcing term and the superscaled perturbation are small
enough. The proof combines concentration-compactness with an abstract critical
point method based on the cohomological index. Our theorems cover both
homogeneous and nonhomogeneous settings and extend Tarantello's classical
results for the Laplacian, improving earlier work on p-biharmonic equations
(including the case p = 2) and on critical Hamiltonian systems.

</details>


### [11] [Asymptotic Behavior of Homogeneous Complex Monge-Ampere Equations on ALE Kahler manifolds](https://arxiv.org/abs/2509.13609)
*Qi Yao*

Main category: math.AP

TL;DR: Analysis of homogeneous complex Monge-Ampère equation on ALE Kähler manifold product space, establishing precise asymptotic behavior and uniform control in weighted Hölder norms.


<details>
  <summary>Details</summary>
Motivation: Extend previous work on HCMA equation to noncompact product spaces, specifically ALE Kähler manifolds times unit disc, to understand asymptotic behavior and regularity properties.

Method: Combines redevelopment of pluripotential theory on noncompact space X×D with PDE-based construction of holomorphic disc foliations inspired by Semmes and Donaldson's work.

Result: Established that solution decay rate matches boundary data decay rate, achieved uniform control in weighted Hölder norms, and obtained local regularity result for HCMA in general Kähler manifolds.

Conclusion: Developed analytical framework for HCMA equation on noncompact product spaces with precise asymptotic control, providing new tools for studying complex Monge-Ampère equations in noncompact settings.

Abstract: This paper is a sequel to the author's earlier work and investigates the
homogeneous complex Monge--Ampere equation (HCMA) on the product space $X
\times D$, where $X$ is an asymptotically locally Euclidean (ALE) Kahler
manifold and $D subset C$ is the unit disc. We establish precise asymptotic
behavior of the solution to the HCMA equation, showing that the decay rate of
the solution matches that of the prescribed boundary data and that uniform
control in weighted Holder norms can be achieved.
  The analysis combines two main ingredients: a redevelopment of pluripotential
theory on the noncompact space $X \times D$ and a PDE-based construction of
holomorphic disc foliations on the end of $X$, inspired by the works of Semmes
and Donaldson. As an application in general Kahler manifolds, the techniques
developed in this paper also imply a local regularity result for the HCMA
equation.

</details>


### [12] [Local energy decay for 2-D wave equations with variable coefficients](https://arxiv.org/abs/2509.13640)
*Ryo Ikehata*

Main category: math.AP

TL;DR: Analysis of 2D wave equation with varying coefficients showing local energy decay of O(t^{-1}√log t) using multiplier methods


<details>
  <summary>Details</summary>
Motivation: To study the two-dimensional initial value problem for wave equations with varying spatial coefficients and establish optimal energy decay rates

Method: Multiplier method to establish optimal L^2-estimates for solutions, skillfully avoiding limitations of Hardy-type inequalities in 2D

Result: Local energy decays to order O(t^{-1}√log t) after sufficiently large time, with extensions to slightly generalized variable coefficients

Conclusion: Successfully established optimal decay rates for 2D wave equations with varying coefficients using innovative multiplier techniques

Abstract: This paper addresses the two-dimensional initial value problem in ${\bf
R}^{2}$ for the wave equation with varying spatial coefficients in the main
part. Assuming compactness in the support of the initial value, we report that
the corresponding local energy decays to an order of magnitude of, for example,
$O(t^{-1}\sqrt{\log t})$ after sufficiently large time. For the two-dimensional
whole space case, it is crucial to establish the optimal $L^2$-estimate for the
solution itself, skillfully avoiding the difficulty of not being able to use
useful inequalities such as Hardy-type inequalities in higher dimensional case.
We also consider cases where the variable coefficients are slightly
generalized. These proofs are developed using the multiplier method.

</details>


### [13] [Fast energy decay for 2-D wave equation with localized damping near spatial infinity](https://arxiv.org/abs/2509.13645)
*Ryo Ikehata*

Main category: math.AP

TL;DR: Wave equations with localized damping in 2D space achieve O(t^{-2}log t) energy decay, overcoming Hardy inequality limitations with Poincaré-type inequalities and finite propagation properties.


<details>
  <summary>Details</summary>
Motivation: To study the Cauchy problem for wave equations with localized damping in 2D space, particularly when damping is only effective near spatial infinity, and understand the energy decay properties compared to exterior domain problems.

Method: Used Poincaré-type inequalities in all spaces and leveraged the finite propagation property of solutions to construct estimation formulas, overcoming the inability to use Hardy-type inequalities in 2D.

Result: Obtained fast energy decay estimate of O(t^{-2}log t) as t → ∞, demonstrating that the whole space problem requires logarithmic correction compared to exterior domain cases.

Conclusion: There is a significant difference between 2D whole space and exterior domain problems, with the former requiring logarithmic correction to energy decay rates, and the methodology successfully addresses the Hardy inequality limitation through Poincaré-type inequalities.

Abstract: We consider the Cauchy problem for wave equations with localized damping in
${\bf R}^{2}$. The damping is effective only near spatial infinity. We obtain
fast energy decay estimate such that $O(t^{-2}\log t)$ as $t \to \infty$.
Unlike the results for the two-dimensional exterior mixed problem case, the
difficulty of not being able to use Hardy-type inequalities is overcome by
using Poincar\'e-type inequalities in all spaces and the finite propagation
property of the solution to construct an estimate formula. In the
two-dimensional case, when comparing the problem in the whole space with that
in the exterior domain, we find that there is a significant difference in the
sense that the former requires a logarithmic correction to the energy decay
rate.

</details>


### [14] [Normalized solutions to Kirchhoff equation with the Sobolev critical exponent in high dimensional spaces](https://arxiv.org/abs/2509.13663)
*Ruikang Lu,Qilin Xie,Jianshe Yu*

Main category: math.AP

TL;DR: Analysis of normalized solutions for Kirchhoff equations with Sobolev critical exponent in high dimensions, covering existence, nonexistence, and multiplicity results for different parameter cases.


<details>
  <summary>Details</summary>
Motivation: To study the well-known Kirchhoff equation with Sobolev critical exponent and prescribed mass constraint, addressing gaps in understanding normalized solutions for this complex problem in high-dimensional spaces.

Method: Variational methods, including establishing Palais-Smale conditions, analyzing constraint functionals with concave-convex structure, and introducing new functionals to establish mountain pass thresholds.

Result: Complete conclusions for pure critical case (μ=0), existence results for μ>0 in N≥5, and local minimizer/mountain pass solutions for N=4 under explicit parameter conditions.

Conclusion: Provides refinement and extension of normalized solution results for Kirchhoff type problems in high-dimensional spaces, with novel approaches for handling complex cases and establishing multiple solution types.

Abstract: The following well-known Kirchhoff equation with the Sobolev critical
exponent has been extensively studied,
  \begin{equation*}
  -\Big(a+b\int_{\mathbb R^N} | \nabla u|^2dx\Big) \Delta u+\lambda u=\mu
|u|^{q-2}u+|u|^{2^*-2}u \ \ {\rm in}\ \ \mathbb{R}^N, \ \ N\geq4,
  \end{equation*} having prescribed mass $\int_{\mathbb R^N}|u|^2dx=c$, where
$a$, $c$ are two positive constants, $b,\mu$ are two parameters, $\lambda$
appears as a real Lagrange multiplier and $2<q<2^*$, $2^*$ is the Sobolev
critical exponent. Firstly, for the special case $\mu=0$ and $N\geq4$, the
above equation reduces to a pure critical Kirchhoff equation, we obtain a
complete conclusion including the existence, nonexistence and multiplicity of
the normalized solutions by the variational methods. Secondly, when $\mu>0$,
$N\geq5$ and $2<q<2+\frac{4}{N}$, we investigate the existence of the positive
normalize solution under suitable assumptions on parameter $b$ and mass $c$. To
the best of our knowledge, it is the first time to consider the above case,
which is a more complicated case not only the difficulties on checking the
Palais-Smale condition, but also the constraint functional requesting the
intricate concave-convex structure. Lastly, when $\mu>0$ and $N=4$, we obtain a
local minimizer solution and a mountain pass solution under explicit conditions
on $b$ and $c$. It is worth noting that the second solution is obtained by
introducing a new functional to establish a threshold for the mountain pass
level, which is the key step for the fulfillment of the Palais-Smale condition.
This paper provides a refinement and extension of the results of the normalized
solutions for Kirchhoff type problem in high-dimensional spaces.

</details>


### [15] [The Correspondence Principle: A bridge between general potential theories and nonlinear elliptic differential operators](https://arxiv.org/abs/2509.13709)
*F. Reese Harvey,Kevin R. Payne*

Main category: math.AP

TL;DR: General potential theories study subharmonic functions relative to constraint sets (subequations) in 2-jet space, with applications to fully nonlinear PDEs. The correspondence principle establishes equivalence between subharmonics and viscosity solutions for compatible operators.


<details>
  <summary>Details</summary>
Motivation: To develop a framework for studying fully nonlinear PDEs determined by degenerate elliptic operators through general potential theories and constraint sets in 2-jet space.

Method: Uses the correspondence principle to show equivalence between subequation subharmonics/superharmonics and admissible viscosity subsolutions/supersolutions for operators compatible with given subequations.

Result: Establishes a powerful tool connecting potential theory with PDE theory, enabling the study of degenerate elliptic operators through constraint sets and viscosity solution methods.

Conclusion: The correspondence principle provides an effective approach for analyzing fully nonlinear PDEs, with applications to comparison principles via duality monotonicity and fiber regularity methods.

Abstract: General potential theories concern the study of functions which are
subharmonic with respect to a suitable constraint set (called a subequation) in
the space of 2-jets. While interesting in their own right, general potential
theories are being widely used to study fully nonlinear PDEs determined by
degenerate elliptic operators acting on the space of 2-jets. We will discuss a
powerful tool, the correspondence principle, which establishes the equivalence
between subequation subharmonics (superharmonics) and admissible subsolutions
(supersolutions) in the viscosity sense of the PDE determined by every operator
which is compatible with a given subequation. The crucial degenerate
ellipticity often requires the operator to be restricted to a suitable
constraint set, which determines the admissibility. Applications to comparison
principles by way of the duality monotonicity fiberegularity method will also
be discussed.

</details>


### [16] [MUSIC algorithm for locating point-like scatterers with multiple interactions](https://arxiv.org/abs/2509.13738)
*Nana Meng*

Main category: math.AP

TL;DR: A direct computational method for locating point sources from far-field data using multiple scattering theory and a novel indicator function, avoiding iterative solving.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient and noise-stable approach for locating point sources from far-field data under plane wave incidence, particularly for resolving closely spaced or multiple sources.

Method: Based on multiple scattering theory with a novel indicator function that avoids the need for iterative solving, providing a direct computational approach.

Result: The method demonstrates efficiency, noise stability, and capability to resolve closely spaced or multiple sources. Numerical results validate its accuracy and robustness in various configurations.

Conclusion: This work provides a practical computational tool for wave-based imaging and non-destructive evaluation applications.

Abstract: We study the inverse problem of locating point sources from far-field data
under plane wave incidence. A direct computational method is developed based on
multiple scattering theory, using a novel indicator function to avoid iterative
solving. The approach is efficient, noise-stable, and capable of resolving
closely spaced or multiple sources. Numerical results validate its accuracy and
robustness in various configurations. This work provides a practical
computational tool for wave-based imaging and non-destructive evaluation.

</details>


### [17] [Gamma-convergence as $s\to1^-$ of anisotropic nonlocal fractional perimeter functionals](https://arxiv.org/abs/2509.13823)
*Alberto Fanizza*

Main category: math.AP

TL;DR: Analysis of Γ-convergence of anisotropic nonlocal fractional perimeters as s→1⁻, showing convergence to anisotropic perimeter in ℝⁿ and bounded domains.


<details>
  <summary>Details</summary>
Motivation: To understand the asymptotic behavior of anisotropic nonlocal fractional perimeters and establish their convergence to classical anisotropic perimeters as the fractional parameter approaches 1.

Method: Using Γ(L¹_loc)-convergence framework with general anisotropic integration kernels k_s(·) under pointwise convergence hypotheses, analyzing both ℝⁿ and bounded Lipschitz domains.

Result: Proves Γ-convergence of rescaled anisotropic nonlocal s-fractional perimeters to suitable anisotropic perimeter as s→1⁻.

Conclusion: Anisotropic nonlocal fractional perimeters converge to classical anisotropic perimeters in the limit s→1⁻, providing a bridge between nonlocal and local geometric measures.

Abstract: We investigate the asymptotic behavior in the sense of
$\Gamma(L^1_{loc})$-convergence as $s\to1^-$ of anisotropic non local
$s$-fractional perimeters defined with respect to general anisotropic
integration kernels $k_s(\cdot)$, under the hypothesis of pointwise convergence
of such kernels. In particular, we prove the $\Gamma(L^1_{loc})$-convergence as
$s\to1^-$ of the rescaled anisotropic nonlocal $s$-fractional perimeters
defined with respect to the kernels $k_s(\cdot)$ to a suitable anisotropic
perimeter. We do so both in $\mathbb{R}^n$ and on a bounded domain
$\Omega\subset\mathbb{R}^n$ with Lipschitz boundary.

</details>


### [18] [Global stability of the inhomogeneous sheared Boltzmann equation in torus](https://arxiv.org/abs/2509.13921)
*Renjun Duan,Shuangqian Liu,Shunlin Shen*

Main category: math.AP

TL;DR: Global stability analysis for inhomogeneous Boltzmann equation solutions with growing energy on periodic domains using Maxwell molecule model-specific energy estimates.


<details>
  <summary>Details</summary>
Motivation: Homo-energetic solutions to the spatially homogeneous Boltzmann equation have been studied, but global stability in inhomogeneous settings remains challenging due to unbounded energy growth and complex spatial-collision dynamics interplay.

Method: Introduce approach for periodic spatial domains to construct global-in-time inhomogeneous solutions using non-conservative perturbation framework. Derive new energy estimates specific to Maxwell molecule model combining low-high frequency decomposition, spectral analysis of second-order moment matrix, and cancellation property in zero-frequency mode of nonlinear collision term.

Result: Growth of energy is governed by a long-time limit state exhibiting features not captured in homogeneous case or classical Boltzmann theory.

Conclusion: The cancellation property in zero-frequency mode of nonlinear collision term shows close analogy to null condition in nonlinear wave equations, providing key insight for global stability analysis.

Abstract: Homo-energetic solutions to the spatially homogeneous Boltzmann equation have
been extensively studied, but their global stability in the inhomogeneous
setting remains challenging due to unbounded energy growth under self-similar
scaling and the intricate interplay between spatial dependence and nonlinear
collision dynamics. In this paper, we introduce an approach for periodic
spatial domains to construct global-in-time inhomogeneous solutions in a
non-conservative perturbation framework, characterizing the global dynamics of
growing energy. The growth of energy is shown to be governed by a long-time
limit state that exhibits features not captured in either the homogeneous case
or the classical Boltzmann theory. The core of our proof is the derivation of
new energy estimates specific to the Maxwell molecule model. These estimates
combine three key ingredients: a low-high frequency decomposition, a spectral
analysis of the matrix associated with the second-order moment equation, and a
crucial cancellation property in the zero-frequency mode of the nonlinear
collision term. This last property bears a close analogy to the null condition
in nonlinear wave equations.

</details>


### [19] [Saddle solution to the complex Ginzburg-Landau system in three dimensions](https://arxiv.org/abs/2509.13935)
*Michele Caselli,Nicola Picenni*

Main category: math.AP

TL;DR: Construction of a 3D saddle solution to complex Ginzburg-Landau system with intersecting line singularities


<details>
  <summary>Details</summary>
Motivation: To study non-magnetic complex Ginzburg-Landau systems and construct solutions with specific geometric singularities in three dimensions

Method: Constructed an entire saddle solution whose zero set forms two perpendicular intersecting lines

Result: Successfully built a solution where blow-downs concentrate on these intersecting lines with multiplicity one

Conclusion: Demonstrates existence of complex Ginzburg-Landau solutions with prescribed geometric singular structure in 3D

Abstract: We construct an entire saddle solution to the non-magnetic complex
Ginzburg-Landau system in three dimensions, whose zero set is the union of two
perpendicular, intersecting lines, and whose blow-downs concentrate on these
lines with multiplicity one.

</details>


### [20] [Reconstruction of strong degeneracy region for parabolic equations and systems](https://arxiv.org/abs/2509.13962)
*Piermarco Cannarsa,Veronica Danesi,Anna Doubova*

Main category: math.AP

TL;DR: Recovering degeneracy points in diffusion coefficients from boundary measurements in 1D degenerate parabolic equations, with stability and uniqueness guarantees.


<details>
  <summary>Details</summary>
Motivation: Addressing the inverse problem of identifying degeneracy points in diffusion coefficients using limited boundary measurements, which is important for practical applications where only partial data is available.

Method: Spectral analysis approach with explicit solution representation using Bessel functions, analyzing the degenerate parabolic equation and coupled systems with specific structure.

Result: Derived sufficient conditions for stability and uniqueness from one-point measurements, and more general uniqueness theorems for identifying initial data and zero-order coefficients using time measurements.

Conclusion: The method provides effective recovery of degeneracy points with theoretical guarantees, supported by numerical simulations, and extends to coupled degenerate parabolic systems.

Abstract: We address the inverse problem of recovering a degeneracy point within the
diffusion coefficient of a one-dimensional complex parabolic equation by
observing the normal derivative at one point of the boundary. The strongly
degenerate case is analyzed. In particular, we derive sufficient conditions on
the initial data that guarantee the stability and uniqueness of the solution
obtained from a one-point measurement. Moreover, we present more general
uniqueness theorems, which also cover the identification of the initial data
and the coefficient of the zero order term, using measurements taken over time.
Our method is based on a careful analysis of the spectral problem and relies on
an explicit form of the solution in terms of Bessel functions. Our
investigation also covers the case of real 1-D degenerate parabolic systems of
equations coupled with a specific structure. Theoretical results are also
supported by numerical simulations.

</details>


### [21] [Steady Prandtl Layer for Compressible Fluids](https://arxiv.org/abs/2509.13988)
*Yan Guo,Yong Wang*

Main category: math.AP

TL;DR: Local well-posedness and expansion construction for Prandtl layers in steady compressible fluids with strong Sobolev norms


<details>
  <summary>Details</summary>
Motivation: Few PDE results exist for compressible Prandtl layers where thermal boundary layers interact strongly with velocity boundary layers in nonlinear fashion

Method: Combination of recent quotient estimate for velocity field and introduction of pseudo entropy estimate for temperature field

Result: Established local-in-x well-posedness and constructed Prandtl layer expansion in strong Sobolev norms

Conclusion: Provides foundation for validity of Prandtl layer theory in compressible fluids

Abstract: Despite its importance, there have been few PDE results to investigate
Prandtl layers for compressible fluids, in which the thermal boundary layer for
the temperature field interacts with the classical velocity Prandtl boundary
layer in a strong nonlinear fashion. We establish local-in-$x$ well-posedness
of Prandtl layers for steady compressible fluids and construct Prandtl layer
expansion in strong Sobolev norms, a foundation for the validity of Prandtl
layer theory \cite{Guo-Wang}. Our method is based on a combination of the
recent quotient estimate for the velocity field and an introduction of a pseudo
entropy estimate for the temperature field.

</details>


### [22] [Propagation of chaos for first-order mean-field systems with non-attractive moderately singular interaction](https://arxiv.org/abs/2509.14022)
*Richard M. Höfer,Richard Schubert*

Main category: math.AP

TL;DR: Quantitative mean-field limit for non-attractive particle systems with singular interactions, extending propagation of chaos results to stronger singularities by leveraging non-attraction property.


<details>
  <summary>Details</summary>
Motivation: To extend existing results on mean-field limits and propagation of chaos for particle systems with singular interaction kernels beyond the attractive case, particularly for stronger singularities where previous methods fail.

Method: Prove quantitative mean-field limit using Wasserstein distances under specific initial conditions, while controlling particle configurations through minimal distances and singular sums of particle distances. Key insight: non-attraction property allows control via distance to next-to-nearest neighbor rather than nearest neighbor.

Result: Established propagation of chaos for α < (d-1)/2 for d≥3 and α < 1/3 for d=2, extending previous results that only worked for α < (d-2)/2 without sign assumptions on interaction.

Conclusion: Non-attractive interactions enable better control of particle configurations, allowing extension of propagation of chaos results to more singular kernels than previously possible, with the novel approach of controlling next-to-nearest neighbor distances being crucial.

Abstract: We consider particle systems that evolve by inertialess binary interaction
through general non-attractive kernels of singularity $|x|^{-\alpha}$ with
$\alpha<d-1$. We prove a quantitative mean-field limit in terms of Wasserstein
distances under certain conditions on the initial configuration while
maintaining control of the particle configuration in the form of the minimal
distance and certain singular sums of the particle distances. As a corollary,
we show propagation of chaos for $\alpha<\frac{d-1}{2}$ for $d\ge 3$ and
$\alpha<\frac 13=\frac{2d-3}{3}$ for $d=2$. This extends the results of Hauray
(https://doi.org/10.1142/S0218202509003814), which yield propagation of chaos
for $\alpha < \frac{d-2}{2}$ without an assumption on the sign of the
interaction. The main novel ingredient is that due to the non-attraction
property it is enough to control the distance to the next-to-nearest neighbour
particle.

</details>


### [23] [Cross-diffusion limits in multispecies kinetic models](https://arxiv.org/abs/2509.14046)
*Ansgar Jüngel,Annamaria Pollino,Satoshi Taguchi*

Main category: math.AP

TL;DR: Cross-diffusion systems derived from multispecies kinetic models via BGK limits, producing Maxwell-Stefan and Busenberg-Travis type equations with entropy analysis.


<details>
  <summary>Details</summary>
Motivation: To formally derive cross-diffusion systems from multispecies kinetic models and establish their mathematical foundations through entropy analysis.

Method: Using multispecies BGK models with two different limits: first limit produces non-isothermal Maxwell-Stefan equations, second limit with Brinkman-type force term yields generalized Busenberg-Travis equations.

Result: Successfully derived cross-diffusion systems including entropy equalities for both kinetic and cross-diffusion equations, with special case reducing to classical Busenberg-Travis system for constant temperature.

Conclusion: The paper provides a rigorous kinetic foundation for cross-diffusion systems and establishes their thermodynamic consistency through entropy analysis.

Abstract: Cross-diffusion systems are formally derived from multispecies kinetic models
in the diffusion limit. The first limit in the multispecies BGK model of Gross
and Krook leads to a variant of the non-isothermal Maxwell-Stefan equations.
The second limit in a BGK model with Brinkman-type force term yields
generalized Busenberg-Travis equations, which reduce for constant temperature
to the classical Busenberg-Travis system for segregating population species.
Entropy equalities are derived for the kinetic and cross-diffusion equations.

</details>


### [24] [Nonuniform ellipticity in variational problems and regularity](https://arxiv.org/abs/2509.14114)
*Cristiana De Filippis,Giuseppe Mingione*

Main category: math.AP

TL;DR: Recent developments in regularity theory for nonuniformly elliptic variational problems


<details>
  <summary>Details</summary>
Motivation: To provide an overview and outlook on recent advances in regularity theory, particularly focusing on nonuniformly elliptic problems of variational nature

Method: Review and analysis of recent developments in the field, with special emphasis on variational nonuniformly elliptic problems

Result: A comprehensive outlook on current progress and trends in regularity theory for this class of problems

Conclusion: The paper serves as a brief survey highlighting important recent developments in the regularity theory of nonuniformly elliptic variational problems

Abstract: We provide a brief outlook on recent developments in regularity theory for
nonuniformly elliptic problems, with special emphasis on those of variational
nature.

</details>


### [25] [The zero-dispersion limit for the Benjamin--Ono equation on the circle](https://arxiv.org/abs/2509.14134)
*Ola Mæhlen*

Main category: math.AP

TL;DR: Characterizes zero-dispersion limit for Benjamin-Ono equation on circle with bounded initial data, generalizing previous work and showing agreement with Miller-Xu's alternating sum formula from Burgers' equation characteristics.


<details>
  <summary>Details</summary>
Motivation: To extend the understanding of zero-dispersion limits beyond previous restricted cases (bell-shaped periodic data and L^2∩L^∞ data on real line) to general bounded initial data on the circle.

Method: Uses explicit formula by P. Gérard to characterize zero-dispersion limit for Benjamin-Ono equation with bounded initial data on the circle.

Result: Shows the zero-dispersion limit agrees with Miller-Xu's alternating sum formula from Burgers' equation characteristics, and derives regularity properties including maximum principles and Oleinik estimate.

Conclusion: The work generalizes previous results and provides a unified characterization of zero-dispersion limits for Benjamin-Ono equation across different domains and initial data types, with important regularity properties.

Abstract: Using the explicit formula of P. G\'erard, we characterize the
zero-dispersion limit for solutions of the Benjamin--Ono equation on the circle
$\mathbb{T}= \mathbb{R}/2\pi\mathbb{Z}$ with bounded initial data $u_0\in
L^\infty(\mathbb{T},\mathbb{R})$. The result generalizes the work of L. Gassot,
who focused on periodic bell-shaped data, and complements the work of G\'erard
and X. Chen who identified the zero-dispersion limit on the line with $u_0\in
L^2\cap L^\infty(\mathbb{R})$. Here, as well as in the mentioned cases, the
characterization agrees with the one first obtained by Miller--Xu for
bell-shaped data on the line: The zero-dispersion limit is given as an
alternating sum of the characteristics appearing in the (multivalued) solution
of Burgers' equation. From this characterization, we compute regularity
properties of the zero-dispersion limit, including maximum principles and an
Oleinik estimate.

</details>


### [26] [Orbital stability of Benjamin--Ono multisolitons](https://arxiv.org/abs/2509.14153)
*Rana Badreddine,Rowan Killip,Monica Visan*

Main category: math.AP

TL;DR: Multisoliton solutions to Benjamin-Ono equation are orbitally stable in H^s(R) for -1/2 < s ≤ 1/2, improving previous s≥1/2 requirement to sharp well-posedness threshold.


<details>
  <summary>Details</summary>
Motivation: Previous work on stability of solitons (even single solitons) required higher regularity (s≥1/2), leaving a gap between stability results and the sharp well-posedness threshold.

Method: Developed new variational characterization of multisolitons and extended Wu identity on Lax operator eigenfunctions to low-regularity slowly-decaying solutions.

Result: Achieved uniform orbital stability for multisolitons at the optimal regularity level (-1/2 < s ≤ 1/2) and clarified spectral type of Lax operator by precluding embedded eigenvalues.

Conclusion: The paper establishes optimal stability results for Benjamin-Ono multisolitons and provides new analytical tools (variational characterization and extended Wu identity) that have broader implications for spectral analysis of Lax operators.

Abstract: We show that multisoliton solutions to the Benjamin--Ono equation are
uniformly orbitally stable in $H^s(\mathbb{R})$ for every $-\tfrac12<s\leq
\frac12$. This improves the regularity required for stability up to the sharp
well-posedness threshold; previous work (even on single solitons) had required
$s\geq \frac12$.
  One key ingredient in our argument is a new variational characterization of
multisolitons. A second ingredient is the extension to low-regularity
slowly-decaying solutions of the Wu identity on eigenfunctions of the Lax
operator. This extension also allows us to clarify the spectral type of the Lax
operator for such potentials by precluding embedded eigenvalues.

</details>


### [27] [Discovery of Unstable Singularities](https://arxiv.org/abs/2509.14185)
*Yongji Wang,Mehdi Bennani,James Martens,Sébastien Racanière,Sam Blackwell,Alex Matthews,Stanislav Nikolov,Gonzalo Cao-Labora,Daniel S. Park,Martin Arjovsky,Daniel Worrall,Chongli Qin,Ferran Alet,Borislav Kozlovskii,Nenad Tomašev,Alex Davies,Pushmeet Kohli,Tristan Buckmaster,Bogdan Georgiev,Javier Gómez-Serrano,Ray Jiang,Ching-Yao Lai*

Main category: math.AP

TL;DR: First systematic discovery of unstable singularities in fluids using machine learning and high-precision optimization, revealing new families of solutions with near-machine precision accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the foundational question of whether singularities can form in fluids, particularly unstable singularities that are hypothesized to play crucial roles in key open problems like the 3D Euler and Navier-Stokes equations, but have been exceptionally elusive due to their instability.

Method: Combines curated machine learning architectures and training schemes with a high-precision Gauss-Newton optimizer to achieve unprecedented accuracy in identifying unstable self-similar solutions.

Result: Discovered multiple new unstable self-similar solutions for incompressible porous media equation and 3D Euler equation with boundary, revealing an empirical asymptotic formula relating blow-up rate to instability order, with accuracies surpassing previous work and reaching near double-float machine precision.

Conclusion: Provides a new methodology for exploring nonlinear PDEs and tackling long-standing mathematical physics challenges, with precision levels suitable for rigorous computer-assisted proofs.

Abstract: Whether singularities can form in fluids remains a foundational unanswered
question in mathematics. This phenomenon occurs when solutions to governing
equations, such as the 3D Euler equations, develop infinite gradients from
smooth initial conditions. Historically, numerical approaches have primarily
identified stable singularities. However, these are not expected to exist for
key open problems, such as the boundary-free Euler and Navier-Stokes cases,
where unstable singularities are hypothesized to play a crucial role. Here, we
present the first systematic discovery of new families of unstable
singularities. A stable singularity is a robust outcome, forming even if the
initial state is slightly perturbed. In contrast, unstable singularities are
exceptionally elusive; they require initial conditions tuned with infinite
precision, being in a state of instability whereby infinitesimal perturbations
immediately divert the solution from its blow-up trajectory. In particular, we
present multiple new, unstable self-similar solutions for the incompressible
porous media equation and the 3D Euler equation with boundary, revealing a
simple empirical asymptotic formula relating the blow-up rate to the order of
instability. Our approach combines curated machine learning architectures and
training schemes with a high-precision Gauss-Newton optimizer, achieving
accuracies that significantly surpass previous work across all discovered
solutions. For specific solutions, we reach near double-float machine
precision, attaining a level of accuracy constrained only by the round-off
errors of the GPU hardware. This level of precision meets the requirements for
rigorous mathematical validation via computer-assisted proofs. This work
provides a new playbook for exploring the complex landscape of nonlinear
partial differential equations (PDEs) and tackling long-standing challenges in
mathematical physics.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [28] [A comparative analysis of the effect of bobbin topography in the transmission performance capability of Hybrid corrugated plane type transmission surface for MR Clutches](https://arxiv.org/abs/2509.13611)
*Loyad Joseph Losan,Saddala Reddy Tharun,Jithin Vijaykumar,Murthi Ram Chandra Reddy,Mood Rahul,Jagadeesha T*

Main category: physics.comp-ph

TL;DR: Study investigates how different bobbin configurations affect torque transmission in MR clutches, finding H-sectioned bobbins provide maximum torque while conical shapes perform worst.


<details>
  <summary>Details</summary>
Motivation: To understand how bobbin topography influences magnetic field characteristics and torque transmissibility in magneto-rheological fluid clutches, as bobbin design significantly affects electromagnetic performance.

Method: Used COMSOL Multiphysics software for comparative analysis of five bobbin configurations (rectangular, semi-circular, conical, I-sectioned, H-sectioned) on a hybrid corrugated-plane type MR clutch.

Result: H-sectioned bobbin provided maximum torque transmission capability, while conical shaped bobbin topography proved least effective for torque transmission.

Conclusion: Bobbin design must consider transmission surface geometry, and H-sectioned configurations optimize magnetic field characteristics for enhanced torque transmission in MR clutches.

Abstract: Magneto-Rheological (MR) fluid based devices work on the principle of
changing the rheological properties of MR fluid (MRF) using magnetic field
excitation generated from an electromagnet. The electromagnet is usually
created with the aid of copper coils wound on a low magnetic permeable spindle
structure referred to as bobbin. In this paper, an attempt has been made to
investigate the different bobbin configurations and its effect on the torque
transmissibility of a MR clutch (MRC). A hybrid corrugated-plane type
transmission surface MRC is chosen for the study, due to the advantage of
enhanced transmission capability due to the simultaneous existence of plane and
corrugated extensions on the disc surface. This enhanced transmission
capability, resulting from the hybrid corrugated plane type transmission
surface facilitates mama,. BB vg BBC gg of the influence of bobbin
configuration on the torque transmission capability in an MRC. A comparative
analysis using COMSOL Multiphysics software is carried out between five
different innovative bobbin configurations such as rectangular, semi-circular,
conical, I-sectioned and H-sectioned. This study aims to simulate and reason
the variations in the magnetic field line characteristics upon variations in
bobbin topography. The results obtained testify for the need of a bobbin design
taking into account the transmission surface geometry. For the specific design
analyzed, it was found that the H-sectioned bobbin provided the maximum torque
transmission capability when compared with other topographies, whereas the
conical shaped bobbin topography proved to be least facilitating for torque
transmission.

</details>


### [29] [A reduced-order derivative-informed neural operator for subsurface fluid-flow](https://arxiv.org/abs/2509.13620)
*Jeongjin,Park,Grant Bruer,Huseyin Tuna Erdinc,Abhinav Prakash Gahlot,Felix J. Herrmann*

Main category: physics.comp-ph

TL;DR: DeFINO is a derivative-informed neural operator framework that uses Fisher Information Matrix to reduce computational cost while maintaining gradient accuracy for fluid-flow applications.


<details>
  <summary>Details</summary>
Motivation: Neural operators need accurate gradients for downstream tasks like optimization and Bayesian inference, but current methods have quadratic computational complexity with input parameters.

Method: Combines Fourier neural operators with derivative-based training using Fisher Information Matrix to project Jacobians onto dominant eigen-directions, reducing computational expense.

Result: Demonstrated improved gradient accuracy while maintaining robust forward predictions in subsurface multi-phase fluid-flow experiments.

Conclusion: DeFINO provides practical, scalable solutions for inversion problems with substantially reduced computational cost in complex real-world scenarios.

Abstract: Neural operators have emerged as cost-effective surrogates for expensive
fluid-flow simulators, particularly in computationally intensive tasks such as
permeability inversion from time-lapse seismic data, and uncertainty
quantification. In these applications, the fidelity of the surrogate's
gradients with respect to system parameters is crucial, as the accuracy of
downstream tasks, such as optimization and Bayesian inference, relies directly
on the quality of the derivative information. Recent advances in
physics-informed methods have leveraged derivative information to improve
surrogate accuracy. However, incorporating explicit Jacobians can become
computationally prohibitive, as the complexity typically scales quadratically
with the number of input parameters. To address this limitation, we propose
DeFINO (Derivative-based Fisher-score Informed Neural Operator), a
reduced-order, derivative-informed training framework. DeFINO integrates
Fourier neural operators (FNOs) with a novel derivative-based training strategy
guided by the Fisher Information Matrix (FIM). By projecting Jacobians onto
dominant eigen-directions identified by the FIM, DeFINO captures critical
sensitivity information directly informed by observational data, significantly
reducing computational expense. We validate DeFINO through synthetic
experiments in the context of subsurface multi-phase fluid-flow, demonstrating
improvements in gradient accuracy while maintaining robust forward predictions
of underlying fluid dynamics. These results highlight DeFINO's potential to
offer practical, scalable solutions for inversion problems in complex
real-world scenarios, all at substantially reduced computational cost.

</details>


### [30] [A molecular rotor driven by an electric field on graphene](https://arxiv.org/abs/2509.13995)
*Wanxing Lin,Xiaobo Li,Yan-Ling Zhao*

Main category: physics.comp-ph

TL;DR: A molecular rotor system driven by electric fields achieves continuous rotation with 2.96 ps period using specific field patterns, confirmed by computational modeling and machine learning simulations.


<details>
  <summary>Details</summary>
Motivation: To develop a method for driving dipolar molecular rotors using external electric fields, enabling continuous rotation for molecular machine applications.

Method: Computational modeling with density functional theory and Newtonian mechanics (similar to molecular dynamics), testing different electric field patterns (rectangular, cosine wave time-dependent, and cosine wave angle-dependent fields).

Result: The dipolar rotor rotates with 2.96 ps period using 0.5 V/Å alternating rectangular field. Cosine wave time-dependent fields fail to produce regular rotation, but angle-dependent cosine fields successfully drive rotation and generate additional power. MLMD confirms thermodynamic stability.

Conclusion: The work reveals the rotation mechanism of dipolar molecular rotors in transverse electric fields and provides a new approach for designing diverse molecular machines.

Abstract: We propose a scheme for driving a dipolar molecular rotor to rotate
continuously by applying an external electric field: the dipolar rotor is fixed
on a graphene sheet via a metal atom to facilitate the free rotation; it is in
the meantime subjected to an electric field oriented parallel to the graphene
sheet. We use computational modeling with density functional theory and
Newtonian mechanics, similar to molecular dynamics simulations, to obtain the
torque, angular velocity, and rotation period of the rotor. Our results show
that the dipolar rotor designed here can rotate with a period of 2.96 ps by an
alternating rectangular electric field with a strength of 0.5 V/{\AA}. However,
a cosine wave alternating electric field depending on time cannot drive the
dipolar rotor to rotate regularly. Therefore, a cosine wave electric field
depending on the rotation angle is suggested, as it can not only drive the
rotor but also produce additional power. Machine learning molecular dynamics
(MLMD) simulations further confirm that the rotor remains thermodynamically
stable under an electric field. This work reveals the rotation mechanism of a
dipolar molecular rotor in a transverse electric field, and we hope this work
can open a new path for designing more diverse molecular machines in
experiments.

</details>


<div id='physics.acc-ph'></div>

# physics.acc-ph [[Back]](#toc)

### [31] [Generating 10 GW-class isolated zeptosecond x-ray bursts through bootstrapping two plasma-based accelerator stages](https://arxiv.org/abs/2509.13937)
*Qianyi Ma,Yuhui Xia,Zhenan Wang,Yuekai Chen,Letian Liu,Zhiyan Yang,Chao Feng,Xinlu Xu,Xueqing Yan,Chan Joshi,Warren B. Mori*

Main category: physics.acc-ph

TL;DR: Method to generate intense zeptosecond x-ray pulses using laser wakefield accelerator electron beams to drive plasma wakefield acceleration, producing ultra-short pre-bunched electron beams that emit 700 zs pulses when collided with optical undulator.


<details>
  <summary>Details</summary>
Motivation: To create an essential probe for nuclear physics and quantum electrodynamics processes by generating intense zeptosecond x-ray pulses, which are currently challenging to produce.

Method: Uses a 100 TW laser-driven LWFA stage to produce GeV electron beams, which then drive a PWFA stage with solid-density plasma and modulated density downramp. The pre-bunched electron beam collides with an optical undulator to emit zeptosecond pulses.

Result: Multi-dimensional PIC simulations demonstrate generation of isolated 10 GW-class zeptosecond pulses with 700 zs FWHM duration in a tapered laser pulse, achieving unprecedented electron beam density (10²⁶ cm⁻³) and brightness (10²⁴ A/m²/rad²).

Conclusion: The proposed scheme successfully generates intense zeptosecond x-ray pulses that could serve as essential probes for advanced physics research in nuclear physics and quantum electrodynamics.

Abstract: A method is proposed to generate coherent, intense zeptosecond x-ray pulses
by using the electron beam produced by a laser wakefield accelerator (LWFA)
stage as the driver for a beam driven plasma wakefield accelerator (PWFA)
stage. The LWFA injector stage requires a readily available 100 TW laser driver
to produce a GeV class self-injected electron beam. This beam is focused and
used as the driver in a PWFA stage that utilizes a solid-density plasma with a
modulated density downramp. The concept is shown to be capable of producing an
ultra-short electron beam with unprecedented density
($10^{26}~\mathrm{cm^{-3}}$) and brightness ($ 10^{24}~\ampere/\meter^2/\radian
^2$), that is also pre-bunched on 0.1 Angstrom scales. By colliding this
pre-bunched extreme beam with an optical undulator, an intense zeptosecond
pulse can be emitted if the spatially focusing region is matched with the
lasing region of the beam. Multi-dimensional particle-in-cell simulations are
performed of the entire concept to demonstrate that an isolated 10 GW-class
zeptosecond pulse with a full-width-half-maximum duration of 700 zs can be
generated in a tapered laser pulse. Such an intense zeptosecond pulse
generation scheme may provide an essential probe for nuclear physics and
quantum electrodynamics processes.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [32] [Numerical Optimization Methods in the environment with Quantum Noise](https://arxiv.org/abs/2509.13367)
*Tomáš Bezděk*

Main category: quant-ph

TL;DR: SA-OO-VQE hybrid quantum-classical algorithm for accurate electronic potential energy surfaces, with optimizer comparison showing gradient-based methods outperform Differential Evolution for ground and excited states.


<details>
  <summary>Details</summary>
Motivation: Accurate calculation of electronic potential energy surfaces for ground and excited states near conical intersections is challenging with classical methods due to scaling issues and with quantum algorithms due to hardware limitations.

Method: State-Averaged Orbital-Optimized Variational Quantum Eigensolver (SA-OO-VQE) combining quantum state preparation with classical state-averaged orbital optimization, implemented with Differential Evolution, BFGS, and SLSQP optimizers.

Result: Orbital optimization is essential for correct potential energy surface topology near conical intersections. Gradient-based methods (BFGS, SLSQP) show superior performance over Differential Evolution. Demonstrated on H2, H4, LiH, and formaldimine.

Conclusion: SA-OO-VQE is crucial for treating complex electronic structures, particularly near conical intersections, with gradient-based optimization methods providing the best performance efficiency.

Abstract: The accurate calculation of electronic potential energy surfaces for ground
and excited states is crucial for understanding photochemical processes,
particularly near conical intersections. While classical methods are limited by
scaling and quantum algorithms by hardware, this thesis focuses on the
State-Averaged Orbital-Optimized Variational Quantum Eigensolver (SA-OO-VQE).
This hybrid quantum-classical algorithm provides a balanced description of
multiple electronic states by combining quantum state preparation with
classical state-averaged orbital optimization.
  A key contribution is the implementation and evaluation of the Differential
Evolution algorithm within the SA-OO-VQE framework, with a comparative study
against classical optimizers like the Broyden-Fletcher-Goldfarb-Shanno (BFGS)
and Sequential Least Squares Programming (SLSQP) algorithms. The performance of
these optimizers is assessed by calculating ground and first excited state
energies for H$_2$, H$_4$, and LiH.
  The thesis also demonstrates SA-OO-VQE's capability to accurately model
potential energy surfaces near conical intersections, using formaldimine as a
case study. The results show that orbital optimization is essential for
correctly capturing the potential energy surface topology, a task where
standard methods with fixed orbitals fail. Our findings indicate that while
Differential Evolution presents efficiency challenges, gradient-based methods
like BFGS and SLSQP offer superior performance, confirming that the SA-OO-VQE
approach is crucial for treating complex electronic structures.

</details>


### [33] [Rare Event Simulation of Quantum Error-Correcting Circuits](https://arxiv.org/abs/2509.13678)
*Carolyn Mayer,Anand Ganti,Uzoma Onunkwo,Tzvetan Metodi,Benjamin Anker,Jacek Skryzalin*

Main category: quant-ph

TL;DR: A practical approach for estimating logical failure rates of quantum error-correcting circuits under low physical failure rate regimes, overcoming limitations of standard Monte Carlo methods.


<details>
  <summary>Details</summary>
Motivation: Standard Monte Carlo methods struggle to study fault-tolerant error-correcting circuits at low physical failure rates, particularly for large circuits or those with high correcting power, limiting simulations to failure rates around 10^{-6}.

Method: An approach based on earlier work by Bravyi and Vargo, adapted to the more complex circuit noise model using rare event simulation by splitting technique specifically for circuit-based noise.

Result: The method successfully accesses circuit-model quantum error-correcting code failure rates below 10^{-20}, with promising results confirmed by standard Monte Carlo simulation in accessible regimes.

Conclusion: This work provides the first full prescription of rare event simulation by splitting technique for circuit-based noise models, enabling access to extremely low failure rate regimes previously inaccessible to standard simulation methods.

Abstract: We describe a practical approach for accessing the logical failure rates of
quantum error-correcting (QEC) circuits under low physical (component) failure
rate regimes. Standard Monte Carlo is often the de facto approach for studying
the failure rates of quantum circuits. However, in the study of fault-tolerant
error-correcting circuits, the ability to extend this approach to low physical
failure rates is limited. In particular, the use of Monte Carlo to access
circuits that are relatively large or have high correcting power becomes more
difficult as we lower the input failure rates of the individual components
(gates) in the circuit. For these reasons, many simulations studying the
circuit model go no lower than end-to-end logical failure rates in the 10^{-6}
regime. In this report, we outline an approach that borrows from earlier work
by Bravyi and Vargo to the more complex circuit noise model. Earlier works
studied both the capacity and phenomenological noise models, but the work is
insufficient for generating similar simulations in the circuit-noise model. To
the best of our knowledge, our team is the first to develop a full prescription
of the rare event simulation by splitting technique for the circuit-based noise
model. We have also generated promising results that are confirmed by standard
Monte Carlo simulation under an accessible regime. This work shows that we can
access noise in the circuit-model prescription of quantum error-correcting code
to failure rates below 10^{-20} regime.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [34] [From Data to Alloys Predicting and Screening High Entropy Alloys for High Hardness Using Machine Learning](https://arxiv.org/abs/2509.13479)
*Rahul Bouri,Manikantan R. Nair,Tribeni Roy*

Main category: cond-mat.mtrl-sci

TL;DR: Machine learning framework predicts high entropy alloy hardness using elemental descriptors, with LightGBM showing best accuracy among tested models.


<details>
  <summary>Details</summary>
Motivation: High entropy alloys offer superior mechanical properties but traditional experimental exploration is time-consuming and expensive, requiring machine learning to accelerate discovery.

Method: Used LightGBM, Gradient Boosting Regressor, and Transformer encoder models trained on experimental data with elemental descriptors; also fine-tuned language model for hardness prediction from descriptor strings.

Result: LightGBM demonstrated superior accuracy in predicting hardness; generated over 9 million virtual alloy candidates and predicted their hardness using trained models.

Conclusion: Machine learning-driven high throughput screening and language modeling can significantly accelerate the development of next-generation high entropy alloys.

Abstract: The growing need for structural materials with strength, mechanical
stability, and durability in extreme environments is driving the development of
high entropy alloys. These are materials with near equiatomic mixing of five or
more principal elements, and such compositional complexity often leads to
improvements in mechanical properties and high thermal stability, etc. Thus,
high-entropy alloys have found their applications in domains like aerospace,
biomedical, energy storage, catalysis, electronics, etc. However, the vast
compositional design and experimental exploration of high-entropy alloys are
both time consuming and expensive and require a large number of resources.
Machine learning techniques have thus become essential for accelerating high
entropy alloys discovery using data driven predictions of promising alloy
combinations and their properties. Hence, this work employs a machine learning
framework that predicts high entropy alloy hardness from elemental descriptors
such as atomic radius, valence electron count, bond strength, etc. Machine
learning regression models, like LightGBM, Gradient Boosting Regressor, and
Transformer encoder, were trained on experimental data. Additionally, a
language model was also fine tuned to predict hardness from elemental
descriptor strings. The results indicate that LightGBM has better accuracy in
predicting the hardness of high entropy alloys compared to other models used in
this study. Further, a combinatorial technique was used to generate over 9
million virtual high entropy alloy candidates, and the trained machine learning
models were used to predict their hardness. This study shows how machine
learning-driven high throughput screening and language modelling approaches can
accelerate the development of next generation high entropy alloys.

</details>


### [35] [Valley-Selective Linear Dichroism and Excitonic Effects in Lieb-Lattice Altermagnets](https://arxiv.org/abs/2509.13551)
*Haonan Wang,Xilong Xu,Du Li,Li Yang*

Main category: cond-mat.mtrl-sci

TL;DR: Study reveals spin valley-dependent excitonic selection rules in 2D altermagnetic Lieb lattices using monolayer Mn2WS4, showing strain-tunable selective excitation of spin-polarized excitons not possible in conventional materials.


<details>
  <summary>Details</summary>
Motivation: Altermagnets represent a new class of magnetic materials with alternative spin-split electronic structures but zero net magnetization. While single-particle properties have been studied, many-electron interactions and optical responses in altermagnets remain largely unexplored.

Method: Employed many-body perturbation theory to investigate excited states and their strain tunability using monolayer Mn2WS4 as a representative 2D altermagnetic Lieb lattice material.

Result: Discovered novel spin valley-dependent excitonic selection rules, strongly bound excitons, and linearly polarized light selectively excites valley spin-polarized excitons. Uniaxial strain can lift valley degeneracy and enable selective excitation of spin-polarized excitons.

Conclusion: The spin-valley-locked excitonic states and their strain tunability provide a robust mechanism for four-fold symmetric altermagnets to encode, store, and read valley/spin information, offering unique capabilities not achievable in traditional transition-metal dichalcogenides.

Abstract: Altermagnets have recently been recognized as a distinct class of magnetic
materials characterized by alternative spin-split electronic structures without
net magnetization. Despite intensive studies on their single-particle
spintronic and valleytronic properties, many-electron interactions and optical
responses of altermagnets remain less explored. In this work, we employ
many-body perturbation theory to investigate excited states and their strain
tunability. Using monolayer Mn2WS4 as a representative candidate, we uncover a
novel spin valley-dependent excitonic selection rule in two-dimensional
altermagnetic Lieb lattices. In addition to strongly bound excitons, we find
that linearly polarized light selectively excites valley spin-polarized
excitons. Moreover, due to the interplay between altermagnetic spin symmetry
and electronic orbital character, we predict that applying uniaxial strain can
lift valley degeneracy and enable the selective excitation of spin-polarized
excitons, an effect not achievable in previously studied transition-metal
dichalcogenides. These spin-valley-locked excitonic states and their strain
tunability offer a robust mechanism for four-fold symmetric altermagnets to
encode, store, and read valley/spin information.

</details>


### [36] [Thermal Conductivity Limits of MoS$_2$ and MoSe$_2$: Revisiting High-Order Anharmonic Lattice Dynamics with Machine Learning Potentials](https://arxiv.org/abs/2509.13798)
*Tugbey Kocabas,Murat Keceli,Tanju Gurel,Milorad Milosevic,Cem Sevik*

Main category: cond-mat.mtrl-sci

TL;DR: This study resolves inconsistencies in reported thermal conductivity values for 2D TMDs MoS2 and MoSe2 by using machine learning force fields to demonstrate that fourth-order phonon scattering has negligible impact on intrinsic thermal conductivity.


<details>
  <summary>Details</summary>
Motivation: There are significant discrepancies (over an order of magnitude) in reported lattice thermal conductivity values for 2D TMD materials MoS2 and MoSe2, stemming from measurement uncertainties, computational variations, and treatment of higher-order anharmonic processes.

Method: Combined first-principles calculations, molecular dynamics simulations, and state-of-the-art machine learning force fields (GAP, MACE, NEP, HIPHIVE) benchmarked against DFT. Rigorously evaluated third- and fourth-order phonon scattering processes and extended convergence tests beyond conventional limits using MLFF computational efficiency.

Result: Fully converged four-phonon processes contribute negligibly to the intrinsic thermal conductivity of both MoS2 and MoSe2, contrary to some recent claims.

Conclusion: The findings refine intrinsic transport limits of 2D TMDs and establish MLFF-based approaches as a robust and scalable framework for predictive modeling of phonon-mediated thermal transport in low-dimensional materials.

Abstract: Group-VI transition metal dichalcogenides (TMDs), MoS$_2$ and MoSe$_2$, have
emerged as prototypical low-dimensional systems with distinctive phononic and
electronic properties, making them attractive for applications in
nanoelectronics, optoelectronics, and thermoelectrics. Yet, their reported
lattice thermal conductivities ($\kappa$) remain highly inconsistent, with
experimental values and theoretical predictions differing by more than an order
of magnitude. These discrepancies stem from uncertainties in measurement
techniques, variations in computational protocols, and ambiguities in the
treatment of higher-order anharmonic processes. In this study, we critically
review these inconsistencies, first by mapping the spread of experimental and
modeling results, and then by identifying the methodological origins of
divergence. To this end, we bridge first-principles calculations, molecular
dynamics simulations, and state-of-the-art machine learning force fields
(MLFFs) including recently developed foundation models. %MACE-OMAT-0, UMA, and
NEP89. We train and benchmark GAP, MACE, NEP, and \textsc{HIPHIVE} against
density functional theory (DFT) and rigorously evaluate the impact of third-
and fourth-order phonon scattering processes on $\kappa$. The computational
efficiency of MLFFs enables us to extend convergence tests beyond conventional
limits and to validate predictions through homogeneous nonequilibrium molecular
dynamics as well. Our analysis demonstrates that, contrary to some recent
claims, fully converged four-phonon processes contribute negligibly to the
intrinsic thermal conductivity of both MoS$_2$ and MoSe$_2$. These findings not
only refine the intrinsic transport limits of 2D TMDs but also establish
MLFF-based approaches as a robust and scalable framework for predictive
modeling of phonon-mediated thermal transport in low-dimensional materials.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [37] [Physics-based deep kernel learning for parameter estimation in high dimensional PDEs](https://arxiv.org/abs/2509.14054)
*Weihao Yan,Christoph Brune,Mengwu Guo*

Main category: cs.CE

TL;DR: Novel two-stage Bayesian framework combining physics-based deep kernel learning with Hamiltonian Monte Carlo for robust parameter inference and uncertainty quantification in high-dimensional PDEs from sparse observations.


<details>
  <summary>Details</summary>
Motivation: Address computational and inferential challenges in high-dimensional PDE parameter inference, overcoming curse of dimensionality and limitations of traditional numerical methods.

Method: Two-stage approach: 1) Physics-based deep kernel learning trains surrogate model and provides initial parameter estimates, 2) Hamiltonian Monte Carlo samples joint posterior distribution with fixed neural network weights.

Result: Framework accurately estimates parameters, provides reliable uncertainty estimates, and effectively handles data sparsity and model complexity in canonical and high-dimensional inverse PDE problems.

Conclusion: Provides robust and scalable tool for scientific and engineering applications, successfully addressing challenges in high-dimensional PDE parameter inference with uncertainty quantification.

Abstract: Inferring parameters of high-dimensional partial differential equations
(PDEs) poses significant computational and inferential challenges, primarily
due to the curse of dimensionality and the inherent limitations of traditional
numerical methods. This paper introduces a novel two-stage Bayesian framework
that synergistically integrates training, physics-based deep kernel learning
(DKL) with Hamiltonian Monte Carlo (HMC) to robustly infer unknown PDE
parameters and quantify their uncertainties from sparse, exact observations.
The first stage leverages physics-based DKL to train a surrogate model, which
jointly yields an optimized neural network feature extractor and robust initial
estimates for the PDE parameters. In the second stage, with the neural network
weights fixed, HMC is employed within a full Bayesian framework to efficiently
sample the joint posterior distribution of the kernel hyperparameters and the
PDE parameters. Numerical experiments on canonical and high-dimensional inverse
PDE problems demonstrate that our framework accurately estimates parameters,
provides reliable uncertainty estimates, and effectively addresses challenges
of data sparsity and model complexity, offering a robust and scalable tool for
diverse scientific and engineering applications.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [38] [Ion-modulated structure, proton transfer, and capacitance in the Pt(111)/water electric double layer](https://arxiv.org/abs/2509.13727)
*Xiaoyu Wang,Junmin Chen,Zezhu Zeng,Frederick Stein,Junho Lim,Bingqing Cheng*

Main category: physics.chem-ph

TL;DR: Machine learning interatomic potential framework enables nanosecond simulations of metal-electrolyte interfaces with quantum accuracy, revealing ion-specific effects on capacitance and reactivity that challenge mean-field models.


<details>
  <summary>Details</summary>
Motivation: The electric double layer (EDL) is fundamental to electrocatalysis and energy technologies, but its atomic structure, capacitance, and reactivity remain poorly understood at the molecular level.

Method: Developed a machine learning interatomic potential framework incorporating long-range electrostatics to simulate metal-electrolyte interfaces under applied electric bias with near-quantum-mechanical accuracy.

Result: Resolved molecular EDL structure, revealed proton-transfer mechanisms in water dissociation, showed that K+ and F- ions screen fields, slow proton transfer, and create capacitance peaks near zero charge potential.

Conclusion: Ion-specific interactions, ignored in mean-field models, are crucial for capacitance and reactivity, providing molecular insights for experimental interpretation and electrolyte design.

Abstract: The electric double layer (EDL) governs electrocatalysis, energy conversion,
and storage, yet its atomic structure, capacitance, and reactivity remain
elusive. Here we introduce a machine learning interatomic potential framework
that incorporates long-range electrostatics, enabling nanosecond simulations of
metal-electrolyte interfaces under applied electric bias with
near-quantum-mechanical accuracy. At the benchmark Pt(111)/water and
Pt(111)/aqueous KF electrolyte interfaces, we resolve the molecular structure
of the EDL, reveal proton-transfer mechanisms underlying anodic water
dissociation and the diffusion of ionic water species, and compute differential
capacitance. We find that the nominally inert K+ and F- ions, while leaving
interfacial water structure largely unchanged, screen bulk fields, slow proton
transfer, and generate a prominent capacitance peak near the potential of zero
charge. These results show that ion-specific interactions, which are ignored in
mean-field models, are central to capacitance and reactivity, providing a
molecular basis for interpreting experiments and designing electrolytes.

</details>


### [39] [Frozen Natural Orbitals based Equation-of-motion coupled-cluster singles, doubles and triples for Ionized, Double-Ionized, Electron Attached and Two-Electron Attached states](https://arxiv.org/abs/2509.14157)
*Manisha,Prashant Uday Manohar*

Main category: physics.chem-ph

TL;DR: Frozen natural orbital (FNO) implementation of EOM-CCSDT for ionization potential, double ionization potential, electron attachment, and double electron attachment variants, with performance analysis on total energies and energy gaps.


<details>
  <summary>Details</summary>
Motivation: To extend the FNO approach from EOM-CCSD to EOM-CCSDT for various electronic structure properties including ionization potentials and electron attachments, building on previous work by Krylov and the authors.

Method: Frozen natural orbital (FNO) based equation-of-motion coupled-cluster with singles, doubles, and triples (EOM-CCSDT) for IP, DIP, EA, and DEA variants, including XFNO-EOM-CCSDT approach.

Result: Performance evaluation of FNO-EOM-CCSDT for computing total energies, target-reference energy gaps, and target-target energy gaps across multiple electronic structure variants.

Conclusion: The study demonstrates the extension and performance of FNO-based EOM-CCSDT methods for various electronic excitation and ionization processes, continuing the development of efficient computational approaches for accurate electronic structure calculations.

Abstract: In this work, we present frozen natural orbital (FNO) based implementations
of equation-of-motion (EOM) coupled-cluster (CC) with singles, doubles, and
triples (SDT) for ionization potential (IP), double ionization potential (DIP),
electron attachment (EA), and double electron attachment (DEA) variants. For
EOM-CC with singles and doubles (SD), the FNO approach has already been studied
by Krylov and co-workers for IP variant and for spin-flipping and
spin-conserving excited states (respectively, the SF and EE variants) for both
total energies and energy-gaps. Recently, we presented FNO-CCSDT performance
for ground state energies of molecules, triplet-singlet gaps and for numerical
estimation force constants of some diatomic molecules. Now we present our study
on performance of IP, DIP, EA and DEA variants of FNO-EOM-CCSDT in computing
total-energies, and for target-reference and target-target energy-gaps.
Following earlier studies by us and by Krylov and co-workers, we also present
the XFNO-EOM-CCSDT approach for these variants and examine its performance for
total energies and energy-gaps.

</details>


<div id='nlin.CD'></div>

# nlin.CD [[Back]](#toc)

### [40] [The effect of parameter drift in the transport of magnetized plasma particles](https://arxiv.org/abs/2509.13472)
*P. Haerter,R. L. Viana*

Main category: nlin.CD

TL;DR: Time-dependent modulation of drift wave amplitudes affects particle transport and chaos in magnetized plasma. Rapid sawtooth ramping produces efficient superdiffusive transport, while slower ramping increases chaos but suppresses transport by destroying coherent structures.


<details>
  <summary>Details</summary>
Motivation: To understand how time-dependent modulations of drift wave amplitudes influence particle transport efficiency and chaos in magnetized plasmas, particularly examining the relationship between chaotic behavior and transport properties.

Method: Used the Horton model with sawtooth ramp applied to primary wave amplitude and periodic rectangular kicks to secondary waves. Quantified transport using Mean Square Displacement exponent (α) and chaos using Maximum Lyapunov Exponent (MLE).

Result: Found strong negative correlation between system's average chaoticity and transport efficiency. Rapid sawtooth ramping (short period τ) produces superdiffusive transport (α > 1), while slower ramping increases chaos but suppresses transport toward normal diffusion (α → 1).

Conclusion: Heightened chaos destroys coherent streamer-like structures needed for superdiffusive flights. The coherence of turbulent field, not raw chaoticity, is key determinant of transport efficiency, offering new plasma control perspectives.

Abstract: We investigate how time dependent modulations of drift wave amplitudes affect
particle transport and chaos in a magnetized plasma. Using the Horton model, we
apply a sawtooth ramp to a primary wave's amplitude and periodic rectangular
kicks to secondary waves, simulating a driven system. Particle transport is
quantified by the Mean Square Displacement (MSD) exponent, $\alpha$, and chaos
by the Maximum Lyapunov Exponent (MLE). Our primary finding is a strong
negative correlation between the system's average chaoticity and its transport
efficiency. We show that rapid sawtooth ramping (short period $\tau$) produces
highly efficient, superdiffusive transport ($\alpha > 1$). In contrast, slower
ramping increases the system's chaos but suppresses transport, driving it
towards normal diffusion ($\alpha \to 1$). This counter intuitive result
demonstrates that heightened chaos destroys the coherent, streamer like
structures necessary for superdiffusive flights. Our findings indicate that the
coherence of the turbulent field, rather than its raw chaoticity, is the key
determinant of transport efficiency, offering a new perspective on plasma
control.

</details>


<div id='cond-mat.mes-hall'></div>

# cond-mat.mes-hall [[Back]](#toc)

### [41] [Axial Hall Effect in Altermagnetic Lieb Lattices](https://arxiv.org/abs/2509.13554)
*Xilong Xu,Haonan Wang,Li Yang*

Main category: cond-mat.mes-hall

TL;DR: Discovery of axial Hall effect in Lieb-lattice altermagnets driven by Berry curvature, with significant anomalous Hall conductivity that remains strain-independent due to topological nature.


<details>
  <summary>Details</summary>
Motivation: To explore the role of spin-orbit coupling and noncollinear spin textures in altermagnets, an area that has received limited attention, and to identify new intrinsic Hall phenomena in topological magnetic systems.

Method: Constructed tight-binding model to identify axial direction as hidden topological degree of freedom, performed first-principles calculations on strained altermagnets (particularly ternary transition-metal dichalcogenides like Mn2WS4), and analyzed multilayer systems for thickness-dependent effects.

Result: Found pronounced anomalous Hall conductivity originating from interplay between Dresselhaus spin-orbit coupling and intrinsic piezomagnetic response, leading to highly localized and enhanced Berry curvature. The effect is significant and strain-independent, with distinctive thickness-dependent modulation in multilayer systems.

Conclusion: The axial Hall effect represents a new pathway for exploring intrinsic Hall phenomena in topological magnetic systems, emphasizing the critical role of spin-orbit coupling and noncollinear spin textures in altermagnets.

Abstract: We predict a so-called axial Hall effect, a Berry-curvature-driven anomalous
Hall response, in Lieb-lattice altermagnets. By constructing a tight-binding
model, we identify the axial direction as a hidden topological degree of
freedom. Breaking the double degeneracy of axial symmetry generates substantial
Berry curvature and induces a pronounced anomalous Hall conductivity.
First-principles calculations further confirm the emergence of this effect in
strained altermagnets, particularly in ternary transition-metal
dichalcogenides. We take Mn2WS4 as an example to reveal that the axial Hall
effect originates from the interplay between Dresselhaus spin-orbit coupling
and the intrinsic piezomagnetic response of Lieb-lattice altermagnets, leading
to highly localized and enhanced Berry curvature. Remarkably, the magnitude of
the axial Hall effect is significant and remains unchanged when varying the
strain, highlighting the topological nature of the axial degree of freedom.
Finally, in multilayer systems, the effect manifests as a distinctive
thickness-dependent modulation of both anomalous and spin Hall responses. These
findings emphasize the critical role of spin-orbit coupling and noncollinear
spin textures in altermagnets, an area that has received limited attention, and
open new pathways for exploring intrinsic Hall phenomena in topological
magnetic systems.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [42] [HYCO: Hybrid-Cooperative Learning for Data-Driven PDE Modeling](https://arxiv.org/abs/2509.14123)
*Lorenzo Liverani,Matthys Steynberg,Enrique Zuazua*

Main category: math.OC

TL;DR: HYCO is a hybrid modeling framework that integrates physics-based and data-driven models through mutual regularization, treating them as co-trained agents to improve accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Traditional approaches impose physical constraints directly on synthetic models, but HYCO aims to create a cooperative learning scheme that handles noise, sparse data, and heterogeneous data better.

Method: Iterative integration of physics-based and data-driven models through mutual regularization, where both models are nudged toward agreement while the synthetic model is enhanced to fit available data.

Result: Extensive numerical experiments show HYCO outperforms classical physics-based and data-driven methods, recovering accurate solutions and model parameters even under ill-posed conditions.

Conclusion: HYCO provides a parallelizable, robust framework with natural game-theoretic interpretation that enables alternating optimization and paves the way for future theoretical developments.

Abstract: We present Hybrid-Cooperative Learning (HYCO), a hybrid modeling framework
that iteratively integrates physics-based and data-driven models through a
mutual regularization mechanism. Unlike traditional approaches that impose
physical constraints directly on synthetic models, HYCO treats the physical and
synthetic components as co-trained agents: the physical and synthetic models
are nudged toward agreement, while the synthetic model is enhanced to better
fit the available data. This cooperative learning scheme is naturally
parallelizable and improves robustness to noise as well as to sparse or
heterogeneous data. Extensive numerical experiments on both static and
time-dependent problems demonstrate that HYCO outperforms classical
physics-based and data-driven methods, recovering accurate solutions and model
parameters even under ill-posed conditions. The method also admits a natural
game-theoretic interpretation, enabling alternating optimization and paving the
way for future theoretical developments.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [43] [A Conformal Prediction Framework for Uncertainty Quantification in Physics-Informed Neural Networks](https://arxiv.org/abs/2509.13717)
*Yifan Yu,Cheuk Hin Ho,Yangshuai Wang*

Main category: cs.LG

TL;DR: Introduces a conformal prediction framework for Physics-Informed Neural Networks that provides rigorous statistical guarantees for uncertainty quantification, handling spatial heteroskedasticity through local calibration.


<details>
  <summary>Details</summary>
Motivation: Existing uncertainty quantification approaches for PINNs lack rigorous statistical guarantees, creating a need for distribution-free methods with finite-sample coverage guarantees.

Method: Distribution-free conformal prediction framework that calibrates prediction intervals using nonconformity scores on a calibration set, with local conformal quantile estimation to handle spatial heteroskedasticity.

Result: Achieves reliable calibration and locally adaptive uncertainty intervals, consistently outperforming heuristic UQ approaches across multiple PDE systems and uncertainty metrics.

Conclusion: Bridges PINNs with distribution-free UQ, enhancing calibration and reliability while opening new avenues for uncertainty-aware modeling of complex PDE systems.

Abstract: Physics-Informed Neural Networks (PINNs) have emerged as a powerful framework
for solving PDEs, yet existing uncertainty quantification (UQ) approaches for
PINNs generally lack rigorous statistical guarantees. In this work, we bridge
this gap by introducing a distribution-free conformal prediction (CP) framework
for UQ in PINNs. This framework calibrates prediction intervals by constructing
nonconformity scores on a calibration set, thereby yielding distribution-free
uncertainty estimates with rigorous finite-sample coverage guarantees for
PINNs. To handle spatial heteroskedasticity, we further introduce local
conformal quantile estimation, enabling spatially adaptive uncertainty bands
while preserving theoretical guarantee. Through systematic evaluations on
typical PDEs (damped harmonic oscillator, Poisson, Allen-Cahn, and Helmholtz
equations) and comprehensive testing across multiple uncertainty metrics, our
results demonstrate that the proposed framework achieves reliable calibration
and locally adaptive uncertainty intervals, consistently outperforming
heuristic UQ approaches. By bridging PINNs with distribution-free UQ, this work
introduces a general framework that not only enhances calibration and
reliability, but also opens new avenues for uncertainty-aware modeling of
complex PDE systems.

</details>


### [44] [eXtended Physics Informed Neural Network Method for Fracture Mechanics Problems](https://arxiv.org/abs/2509.13952)
*Amin Lotfalian,Mohammad Reza Banan,Pooyan Broumand*

Main category: cs.LG

TL;DR: X-PINN is a novel framework combining physics-informed neural networks with extended finite element method concepts to handle multiple cracks in fracture mechanics problems using domain decomposition and specialized enrichment functions.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of simulating fracture mechanics problems with multiple cracks in complex media, where traditional methods struggle with discontinuities and singularities at crack tips.

Method: Combines PINNs with XFEM concepts using energy-based loss functions, customized integration schemes, domain decomposition, and neural network enrichment with specialized functions to capture crack discontinuities and singularities.

Result: The method enables flexible and effective simulations of complex multiple-crack problems in 1D and 2D domains, with numerical experiments validating its effectiveness and robustness.

Conclusion: X-PINN provides a robust framework for fracture mechanics with convenient extensibility to 3D problems, successfully addressing multiple crack simulations through neural network enrichment and domain decomposition techniques.

Abstract: This paper presents eXtended Physics-Informed Neural Network (X-PINN), a
novel and robust framework for addressing fracture mechanics problems involving
multiple cracks in fractured media. To address this, an energy-based loss
function, customized integration schemes, and domain decomposition procedures
are proposed. Inspired by the Extended Finite Element Method (XFEM), the neural
network solution space is enriched with specialized functions that allow crack
body discontinuities and singularities at crack tips to be explicitly captured.
Furthermore, a structured framework is introduced in which standard and
enriched solution components are modeled using distinct neural networks,
enabling flexible and effective simulations of complex multiple-crack problems
in 1D and 2D domains, with convenient extensibility to 3D problems. Numerical
experiments are conducted to validate the effectiveness and robustness of the
proposed method.

</details>


### [45] [Deep Learning-Driven Peptide Classification in Biological Nanopores](https://arxiv.org/abs/2509.14029)
*Samuel Tovey,Julian Hoßbach,Sandro Kuppel,Tobias Ensslen,Jan C. Behrends,Christian Holm*

Main category: cs.LG

TL;DR: A new method using wavelet transforms to convert nanopore current signals into scaleogram images achieves 81% accuracy in protein classification, enabling real-time disease diagnosis.


<details>
  <summary>Details</summary>
Motivation: To enable inexpensive and rapid disease diagnosis through real-time protein classification using nanopore devices, overcoming current signal complexity limitations.

Method: Convert nanopore current signals into scaleogram images using wavelet transforms to capture amplitude, frequency, and time information for machine learning classification.

Result: Achieved ~81% classification accuracy on 42 peptides, setting a new state-of-the-art in the field.

Conclusion: The method demonstrates practical potential for peptide/protein diagnostics at point-of-care and includes model transfer techniques for real hardware deployment.

Abstract: A device capable of performing real time classification of proteins in a
clinical setting would allow for inexpensive and rapid disease diagnosis. One
such candidate for this technology are nanopore devices. These devices work by
measuring a current signal that arises when a protein or peptide enters a
nanometer-length-scale pore. Should this current be uniquely related to the
structure of the peptide and its interactions with the pore, the signals can be
used to perform identification. While such a method would allow for real time
identification of peptides and proteins in a clinical setting, to date, the
complexities of these signals limit their accuracy. In this work, we tackle the
issue of classification by converting the current signals into scaleogram
images via wavelet transforms, capturing amplitude, frequency, and time
information in a modality well-suited to machine learning algorithms. When
tested on 42 peptides, our method achieved a classification accuracy of
~$81\,\%$, setting a new state-of-the-art in the field and taking a step toward
practical peptide/protein diagnostics at the point of care. In addition, we
demonstrate model transfer techniques that will be critical when deploying
these models into real hardware, paving the way to a new method for real-time
disease diagnosis.

</details>


### [46] [A Variational Framework for Residual-Based Adaptivity in Neural PDE Solvers and Operator Learning](https://arxiv.org/abs/2509.14198)
*Juan Diego Toscano,Daniel T. Chen,Vivek Oommen,George Em Karniadakis*

Main category: cs.LG

TL;DR: A variational framework formalizes residual-based adaptive strategies in scientific ML by integrating convex residual transformations, linking discretization choices to error metrics and enabling systematic adaptive scheme design.


<details>
  <summary>Details</summary>
Motivation: Residual-based adaptive strategies are widely used but remain largely heuristic, lacking theoretical foundation and principled design approaches.

Method: Introduces a unifying variational framework that integrates convex transformations of the residual, where different transformations correspond to distinct objective functionals (exponential for uniform error, linear for quadratic error).

Result: The framework enables systematic design of adaptive schemes across norms, reduces discretization error through variance reduction, and enhances learning dynamics by improving gradient signal-to-noise ratio. Substantial performance gains demonstrated across optimizers and architectures.

Conclusion: Provides theoretical justification for residual-based adaptivity and establishes a foundation for principled discretization and training strategies in scientific machine learning.

Abstract: Residual-based adaptive strategies are widely used in scientific machine
learning but remain largely heuristic. We introduce a unifying variational
framework that formalizes these methods by integrating convex transformations
of the residual. Different transformations correspond to distinct objective
functionals: exponential weights target the minimization of uniform error,
while linear weights recover the minimization of quadratic error. Within this
perspective, adaptive weighting is equivalent to selecting sampling
distributions that optimize the primal objective, thereby linking
discretization choices directly to error metrics. This principled approach
yields three benefits: (1) it enables systematic design of adaptive schemes
across norms, (2) reduces discretization error through variance reduction of
the loss estimator, and (3) enhances learning dynamics by improving the
gradient signal-to-noise ratio. Extending the framework to operator learning,
we demonstrate substantial performance gains across optimizers and
architectures. Our results provide a theoretical justification of
residual-based adaptivity and establish a foundation for principled
discretization and training strategies.

</details>


### [47] [Data Denoising and Derivative Estimation for Data-Driven Modeling of Nonlinear Dynamical Systems](https://arxiv.org/abs/2509.14219)
*Jiaqi Yao,Lewis Mitchell,John Maclean,Hemanth Saratchandran*

Main category: cs.LG

TL;DR: RKTV-INR is a denoising framework that uses implicit neural representations with Runge-Kutta and total variation constraints to clean noisy dynamical system data, enabling accurate derivative estimation and system identification via SINDy.


<details>
  <summary>Details</summary>
Motivation: Measurement noise hampers data-driven modeling of nonlinear dynamical systems, making it difficult to accurately identify governing equations from noisy observations.

Method: Uses implicit neural representation (INR) with Runge-Kutta integration and total variation constraints to represent state trajectories directly from noisy data, ensuring the reconstructed state follows dynamical system principles while staying close to original data.

Result: Effective noise suppression, precise derivative estimation via automatic differentiation, and reliable system identification when combined with SINDy for recovering governing equations.

Conclusion: RKTV-INR provides an effective framework for denoising dynamical system data, enabling accurate derivative computation and successful identification of underlying governing equations from noisy measurements.

Abstract: Data-driven modeling of nonlinear dynamical systems is often hampered by
measurement noise. We propose a denoising framework, called Runge-Kutta and
Total Variation Based Implicit Neural Representation (RKTV-INR), that
represents the state trajectory with an implicit neural representation (INR)
fitted directly to noisy observations. Runge-Kutta integration and total
variation are imposed as constraints to ensure that the reconstructed state is
a trajectory of a dynamical system that remains close to the original data. The
trained INR yields a clean, continuous trajectory and provides accurate
first-order derivatives via automatic differentiation. These denoised states
and derivatives are then supplied to Sparse Identification of Nonlinear
Dynamics (SINDy) to recover the governing equations. Experiments demonstrate
effective noise suppression, precise derivative estimation, and reliable system
identification.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [48] [Flow-driven hysteresis in the transition boiling regime](https://arxiv.org/abs/2509.14042)
*Alessandro Gabbana,Xander M. de Wit,Linlin Fei,Ziqi Wang,Daniel Livescu,Federico Toschi*

Main category: physics.flu-dyn

TL;DR: Transition boiling hysteresis occurs due to flow dynamics in liquid-vapor systems, not surface defects, with asymmetric heating/cooling transitions.


<details>
  <summary>Details</summary>
Motivation: Transition boiling hysteresis is a critical unsolved problem in phase-change heat transfer with major implications for industrial cooling and nuclear reactor safety, as it can lead to overheating and component damage.

Method: Large-scale three-dimensional numerical simulations of pool boiling on flat surfaces at constant temperature under idealized conditions.

Result: Hysteresis occurs even under idealized conditions, demonstrating it arises purely from flow dynamics rather than surface properties. Strong asymmetries exist: heating transitions are abrupt and memory-less, while cooling transitions show metastable coexisting states that delay transition.

Conclusion: Transition boiling hysteresis is an intrinsic property of liquid-vapor flow dynamics, not dependent on surface defects, with fundamentally different mechanisms governing heating vs cooling transitions.

Abstract: Transition boiling is an intermediate regime occurring between nucleate
boiling, where bubbles at the surface efficiently carry heat away, and film
boiling, where a layer of vapor formed over the surface insulates the system
reducing heat transfer. This regime is inherently unstable and typically occurs
near the boiling crisis, where the system approaches the maximum heat flux.
Transition boiling hysteresis remains a central open problem in phase-change
heat transfer, with critical implications for industrial cooling systems and
nuclear reactor safety, since entering this regime sharply reduces heat removal
potentially leading to overheating or component damage. We investigate the
mechanisms driving hysteresis in the transition boiling regime through
large-scale three-dimensional numerical simulations, providing clearcut
evidence that hysteresis occurs even under idealized conditions of pool boiling
on flat surfaces at constant temperature. This demonstrates that hysteresis
arises purely from the flow dynamics of the liquid-vapor system, rather than
from surface properties or defects. Moreover, we disclose strong asymmetries in
the transition dynamics between nucleate and film boiling. During heating, the
transition is abrupt and memory-less, whereas, upon decreasing the surface
temperature, it is more complex, with the emergence of metastable coexisting
states that can delay the transition.

</details>
