<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 15]
- [math.AP](#math.AP) [Total: 13]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 1]
- [math.SP](#math.SP) [Total: 1]
- [physics.app-ph](#physics.app-ph) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [stat.CO](#stat.CO) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 5]
- [cs.LG](#cs.LG) [Total: 2]
- [cs.CV](#cs.CV) [Total: 1]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [math.FA](#math.FA) [Total: 1]
- [cs.RO](#cs.RO) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [On the extension of a class of Hermite bivariate interpolation problems](https://arxiv.org/abs/2509.14359)
*Hakop Hakopian,Anush Khachatryan*

Main category: math.NA

TL;DR: Characterization of solvability sets for Hermite bivariate interpolation problems when sum of multiplicities ≤ 2n+2, extending previous work on 2n+1 case.


<details>
  <summary>Details</summary>
Motivation: Extend previous theorem (2000) on Hermite bivariate interpolation solvability from sum of multiplicities ≤ 2n+1 to ≤ 2n+2, continuing the generalization of Severi's classical theorem (1921).

Method: Mathematical analysis and characterization of solvability conditions for Hermite bivariate interpolation problems in polynomial spaces of degree n.

Result: Complete characterization of solvability sets for the case when sum of multiplicities is at most 2n+2.

Conclusion: Successfully extended the solvability theory for Hermite bivariate interpolation problems, providing a natural progression from Severi's classical result through the previous 2n+1 theorem to the new 2n+2 characterization.

Abstract: We characterize the sets of solvability for Hermite bivariate interpolation
problems when the sum of multiplicities is at most $2n + 2$, with $n$ the
degree of the polynomial space. This result extends an earlier theorem (2000)
by one of the authors concerning the case $2n+1$. The latter theorem, in turn,
can be regarded as a natural generalization of a classical theorem of Severi
(1921).

</details>


### [2] [Error analysis of a fully discrete structure-preserving finite element scheme for a diffuse-interface model of tumour growth](https://arxiv.org/abs/2509.14486)
*Agus L. Soenjaya,Ping Lin,Thanh Tran*

Main category: math.NA

TL;DR: A structure-preserving finite element method for tumor growth modeling using Cahn-Hilliard and reaction-diffusion equations with SAV formulation, ensuring energy stability and mass conservation.


<details>
  <summary>Details</summary>
Motivation: To develop a numerical method that preserves the dissipative energy law and physical properties of the continuous tumor growth model while maintaining computational efficiency.

Method: Combines scalar auxiliary variable (SAV) formulation with mixed finite elements for Cahn-Hilliard and standard conforming elements for reaction-diffusion, using first-order Euler time-stepping.

Result: The method is linear, unconditionally energy-stable, mass-preserving, and requires solving only linear systems. Achieves first-order time accuracy and optimal-order space accuracy with proven error estimates.

Conclusion: The proposed method effectively captures tumor growth phenomena while maintaining mathematical properties and computational efficiency, validated by numerical experiments.

Abstract: We develop a fully discrete structure-preserving finite element method for a
diffuse-interface model of tumour growth. The system couples a Cahn--Hilliard
type equation with a nonlinear reaction-diffusion equation for nutrient
concentration and admits a dissipative energy law at the continuous level. For
the discretisation, we employ a scalar auxiliary variable (SAV) formulation
together with a mixed finite element method for the Cahn--Hilliard part and
standard conforming finite elements for the reaction-diffusion equation in
space, combined with a first-order Euler time-stepping scheme. The resulting
method is linear, unconditionally energy-stable, mass-preserving, and inherits
a discrete energy dissipation law associated with the SAV-based approximate
energy functional, while requiring the solution of only linear systems at each
time step. Under suitable regularity assumptions on the exact solution, we
derive rigorous error estimates in $L^2$, $H^1$, and $L^\infty$ norms,
establishing first-order accuracy in time and optimal-order accuracy in space.
A key step in this analysis is the proof of boundedness of the numerical
solutions in $L^\infty$. Numerical experiments validate the theoretical
convergence rates and demonstrate the robustness of the method in capturing
characteristic phenomena such as aggregation and chemotactic tumour growth.

</details>


### [3] [The extended horizontal linear complementarity problem: iterative methods and error analysis](https://arxiv.org/abs/2509.14491)
*Shi-Liang Wu,Cui-Xia Li*

Main category: math.NA

TL;DR: This paper addresses the lack of iterative methods and error analysis for the extended horizontal linear complementarity problem (EHLCP) by proposing a fixed-point formulation, developing efficient iterative methods with convergence analysis, and deriving global error bounds with numerical validation.


<details>
  <summary>Details</summary>
Motivation: Since its introduction in 1977, the EHLCP has lacked iterative solution methods and error analysis due to its complex 'chain-like' structure with interdependent multiple unknowns, creating a significant research gap.

Method: The authors use a variable transformation technique with the max-min function to create an equivalent fixed-point formulation of EHLCP, then develop iterative methods based on this formulation with convergence analysis, and derive global error bounds and computable estimates.

Result: The paper provides efficient iterative methods for solving EHLCP with proven convergence properties, along with global error bounds and computable estimates that are validated through numerical examples from multicommodity market equilibrium and bilateral obstacle problems.

Conclusion: This work successfully fills the longstanding gap in EHLCP research by providing the first iterative methods, convergence analysis, and error bounds for this complex problem, demonstrating practical effectiveness through real-world applications.

Abstract: To the best of our knowledge, since the extended horizontal linear
complementarity problem (EHLCP) was first introduced and studied by Kaneko in
1977, no iterative methods or error analysis have been developed for it due to
the interdependence of its multiple unknowns in a 'chain-like' structure. This
paper aims to address these gaps by:
  (1) proposing an equivalent fixed-point formulation of the EHLCP by using a
variable transformation technique with the max-min function;
  (2) developing efficient iterative methods for solving the EHLCP based on
this fixed-point form, along with their convergence analysis;
  (3) deriving global error bounds and computable estimates for the EHLCP.
  Several numerical examples from applications such as multicommodity market
equilibrium and bilateral obstacle problems are given to demonstrate the
effectiveness of the proposed methods and bounds.

</details>


### [4] [The whys and hows of conditioning of DG plane wave Trefftz methods: a single element](https://arxiv.org/abs/2509.14500)
*Joseph Coyle,Nilima Nigam*

Main category: math.NA

TL;DR: Analysis of conditioning issues in plane-wave Trefftz methods for Helmholtz equation, focusing on disk-shaped elements and evaluating preconditioning strategies using conditioning metrics, GMRES residuals, and L²-error impact.


<details>
  <summary>Details</summary>
Motivation: Plane-wave Trefftz methods offer advantages over standard polynomial-based discretizations for Helmholtz equations but suffer from poor conditioning of system matrices, which needs to be addressed for practical implementation.

Method: Study conditioning of mass and stiffness matrices on a single disk-shaped element, examining preconditioning strategies and evaluating them through three criteria: matrix conditioning, GMRES residual behavior, and L²-error impact.

Result: The paper provides analysis of how element size and geometry affect matrix properties in plane-wave discontinuous Galerkin methods, and presents performance results of different preconditioning approaches.

Conclusion: Preconditioning strategies can effectively address the conditioning issues in plane-wave Trefftz methods, with evaluation metrics showing improved performance in terms of conditioning, solver convergence, and solution accuracy.

Abstract: Plane-wave Trefftz methods (PWB) for the Helmholtz equation offer significant
advantages over standard discretization approaches whose implementation employs
more general polynomial basis functions. A disadvantage of these methods is the
poor conditioning of the system matrices. In the present paper, we carefully
examine the conditioning of the plane-wave discontinuous Galerkin method with
reference to a single element. The properties of the mass and stiffness
matrices depend on the size and geometry of the element. We study the mass and
system matrices arising from a PWB on a single disk-shaped element. We then
examine some preconditioning strategies, and present results showing their
behaviour with three different criteria: conditioning, the behaviour of GMRES
residuals, and impact on the $L^2$-error.

</details>


### [5] [Weak Adversarial Neural Pushforward Mappings for Fokker-Planck Equations](https://arxiv.org/abs/2509.14575)
*Andrew Qing He,Wei Cai*

Main category: math.NA

TL;DR: Neural sampler method for Fokker-Planck equations using weak adversarial framework with plane-wave test functions


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of density-based methods and handle distributions without densities while ensuring probability conservation

Method: Learning neural samplers via weak adversarial framework with neural pushforward map representation and computationally efficient plane-wave test functions

Result: Novel approach that bypasses density-based limitations and naturally enforces probability conservation

Conclusion: The method provides an effective solution for Fokker-Planck equations with improved computational efficiency and broader applicability

Abstract: This paper presents a novel method for solving Fokker-Planck equations by
learning neural samplers via a weak adversarial framework. We represent the
solution distribution through a neural pushforward map, bypassing the
limitations of density-based methods. A key innovation is our use of
computationally efficient plane-wave test functions, whose derivatives are
explicitly computed -- a treatment distinct from prior work. This approach
handles distributions without densities and naturally enforces probability
conservation.

</details>


### [6] [A Weighted Sampling Method for Inverse Medium Problem with Limited Aperture](https://arxiv.org/abs/2509.14580)
*Fuqun Han,Kazufumi Ito*

Main category: math.NA

TL;DR: A new sampling method for inverse medium scattering with limited-aperture data that improves reconstruction accuracy and stability compared to classical methods like MUSIC and linear sampling.


<details>
  <summary>Details</summary>
Motivation: Classical sampling methods perform poorly with limited-aperture data, especially in noisy conditions, due to physical or experimental constraints that restrict measurement data to limited apertures.

Method: Proposes a sampling method based on linear sampling framework that incorporates a weight function into the index function. The weight is designed to reproduce full-aperture behavior using limited data, localizing oscillations and improving conditioning of the far-field system.

Result: Numerical experiments in 2D and 3D demonstrate the method achieves greater accuracy and robustness than existing sampling-type methods, particularly for noisy, limited-aperture data.

Conclusion: The proposed method provides more accurate and stable reconstructions for inverse medium scattering problems with limited-aperture measurements, with theoretical justification under Born approximation and efficient computational algorithm.

Abstract: Inverse medium scattering problems arise in many applications, but in
practice, the measurement data are often restricted to a limited aperture by
physical or experimental constraints. Classical sampling methods, such as MUSIC
and the linear sampling method, are well understood for full-aperture data, yet
their performance deteriorates severely under limited-aperture conditions,
especially in the presence of noise. We propose a new sampling method tailored
to the inverse medium problem with limited-aperture data. The method is
motivated by the linear sampling framework and incorporates a weight function
into the index function. The weight is designed so that the modified kernel
reproduces the full-aperture behavior using only limited data, which both
localizes oscillations and improves the conditioning of the far-field system,
thereby yielding more accurate and stable reconstructions. We provide a
theoretical justification of the method under the Born approximation and an
efficient algorithm for computing the weight. Numerical experiments in two and
three dimensions demonstrate that the proposed method achieves greater accuracy
and robustness than existing sampling-type methods, particularly for noisy,
limited-aperture data.

</details>


### [7] [Decay of Chebyshev coefficients and error estimates of associated quadrature on shrinking intervals](https://arxiv.org/abs/2509.14602)
*Krishna Yamanappa Poojara,Sabhrant Sachan,Ambuj Pandey*

Main category: math.NA

TL;DR: Analysis of Chebyshev coefficient decay and local approximations for functions on shrinking intervals, with error estimates for coefficient decay, discrete-continuous coefficient approximation, and quadrature convergence.


<details>
  <summary>Details</summary>
Motivation: To fill a gap in existing theory by providing a unified and rigorous description of how approximation accuracy scales on shrinking intervals, offering theoretical insight and practical guidance for high-order numerical methods on decomposed domains.

Method: Derive error estimates for all four families of Chebyshev polynomials, quantifying dependence on interval length for coefficient decay, approximation error between continuous/discrete coefficients, and quadrature convergence.

Result: Numerical experiments corroborate theoretical results, confirming decay rates and illustrating error behavior in practice.

Conclusion: The results provide comprehensive error estimates that fill theoretical gaps and offer practical guidance for high-order numerical methods working with decomposed domains and shrinking intervals.

Abstract: We analyze decay of Chebyshev coefficients and local Chebyshev approximations
for functions of finite regularity on finite intervals, focusing on the
framework where the interval length tends to zero while the number of
approximation nodes remains fixed. For all four families of Chebyshev
polynomials, we derive error estimates that quantify the dependence on the
interval length for (i) the decay of Chebyshev coefficients, (ii) the
approximation error between continuous and discrete Chebyshev coefficients, and
(iii) the convergence of Chebyshev-based quadrature rules. These results fill a
gap in the existing theory and provide a unified and rigorous description of
how approximation accuracy scales on shrinking intervals, offering new
theoretical insight and practical guidance for high-order numerical methods on
decomposed domains. Numerical experiments corroborate the theoretical results,
confirming the decay rates and illustrating the error behavior in practice.

</details>


### [8] [Unconditional and optimal error analysis of two linearized finite difference schemes for the logarithmic Schrödinger equation](https://arxiv.org/abs/2509.14736)
*Tingchun Wang,Jingye Yan*

Main category: math.NA

TL;DR: Two linearized finite difference schemes for solving logarithmic Schrödinger equation without regularization, with improved convergence rates and no time step restrictions.


<details>
  <summary>Details</summary>
Motivation: To develop efficient numerical methods for the logarithmic Schrödinger equation that overcome the singularity of the logarithmic nonlinearity and provide better error estimates without restrictive conditions.

Method: First-order and second-order backward difference formulas for temporal discretization combined with second-order central finite difference for spatial discretization, with novel techniques to handle the logarithmic singularity.

Result: Optimal l²-error estimates for first-order scheme and almost optimal for second-order scheme, plus discrete H¹ error estimates without grid ratio restrictions, outperforming existing methods.

Conclusion: The proposed schemes successfully handle the logarithmic singularity, provide superior error estimates without time step or grid ratio restrictions, and demonstrate effectiveness through numerical validation.

Abstract: In this paper, we propose two linearized finite difference schemes for
solving the logarithmic Schr\"odinger equation (LogSE) without the need for
regularization of the logarithmic term. These two schemes employ the
first-order and the second-order backward difference formula, respectively, for
temporal discretization of the LogSE, while using the second-order central
finite difference method for spatial discretization. We overcome the
singularity posed by the logarithmic nonlinearity $f(u)=u\ln|u|$ in
establishing optimal $l^{2}$-error estimates for the first-order scheme, and an
almost optimal $l^{2}$-error estimate for the second-order scheme. Compared to
the error estimates of the LogSE in the literature, our error bounds not only
greatly improve the convergence rate but also get rid of the time step
restriction. Furthermore, without enhancing the regularity of the exact
solution or imposing any requirements on the grid ratio, we establish error
estimates of the two proposed schemes in the discrete $H^{1}$ norm. However,
the existing results available in the literature either fail to provide $H^{1}$
error estimates or require certain restrictions on the grid ratio. Numerical
results are reported to confirm our error estimates and demonstrate rich
dynamics of the LogSE.

</details>


### [9] [The Bayesian SIAC filter](https://arxiv.org/abs/2509.14771)
*Jan Glaubitz,Tongtong Li,Jennifer Ryan,Roman Stuhlmacher*

Main category: math.NA

TL;DR: Bayesian extension of SIAC filter for noise removal with uncertainty quantification and support for general data models


<details>
  <summary>Details</summary>
Motivation: Current SIAC filter is limited to nodal data and deterministic estimates without uncertainty propagation

Method: Hierarchical Bayesian extension with structure-exploiting algorithms for MAP estimation and MCMC sampling, focusing on linear data models with additive Gaussian noise

Result: Produces point estimates with comparable or better accuracy than deterministic SIAC filter, while providing built-in uncertainty quantification

Conclusion: Bayesian SIAC filter overcomes limitations of deterministic version, broadening applicability with support for general data models and rigorous uncertainty quantification

Abstract: We propose the Bayesian smoothness-increasing accuracy-conserving (SIAC)
filter -- a hierarchical Bayesian extension of the existing deterministic SIAC
filter. The SIAC filter is a powerful numerical tool for removing
high-frequency noise from data or numerical solutions without degrading
accuracy. However, current SIAC methodology is limited to (i) nodal data
(direct, typically noisy function values) and (ii) deterministic point
estimates that do not account for uncertainty propagation from input data to
the SIAC reconstruction. The proposed Bayesian SIAC filter overcomes these
limitations by (i) supporting general (non-nodal) data models and (ii) enabling
rigorous uncertainty quantification (UQ), thereby broadening the applicability
of SIAC filtering. We also develop structure-exploiting algorithms for
efficient maximum a posteriori (MAP) estimation and Markov chain Monte Carlo
(MCMC) sampling, with a focus on linear data models with additive Gaussian
noise. Computational experiments demonstrate the effectiveness of the Bayesian
SIAC filter across several applications, including signal denoising, image
deblurring, and post-processing of numerical solutions to hyperbolic
conservation laws. The results show that the Bayesian approach produces point
estimates with accuracy comparable to, and in some cases exceeding, that of the
deterministic SIAC filter. In addition, it extends naturally to general data
models and provides built-in UQ.

</details>


### [10] [A class of flexible and efficient partitioned Runge-Kutta-Chebyshev methods for some time-dependent partial differential equations](https://arxiv.org/abs/2509.14847)
*Xiao Tang,Junwei Huang*

Main category: math.NA

TL;DR: New second-order partitioned explicit stabilized methods combining RKC for moderately stiff terms and flexible-stage RK for non-stiff terms in PDE-derived ODEs.


<details>
  <summary>Details</summary>
Motivation: Many time-dependent PDEs become ODEs with mixed stiff and non-stiff terms after spatial discretization, requiring flexible methods that can handle both components efficiently.

Method: Construct partitioned methods using s-stage Runge-Kutta-Chebyshev for moderately stiff terms and 4m-stage explicit Runge-Kutta for non-stiff terms, with both s and m parameters adjustable.

Result: The methods demonstrate superior flexibility and applicability compared to existing fixed-stage approaches, validated through numerical examples including advection-diffusion, Burgers, Brusselator, and damped wave equations.

Conclusion: The proposed flexible parameter adjustment provides enhanced adaptability for solving PDE-derived ODE systems with mixed stiffness characteristics.

Abstract: Many time-dependent partial differential equations (PDEs) can be transformed
into an ordinary differential equations (ODEs) containing moderately stiff and
non-stiff terms after spatial semi-discretization. In the present paper, we
construct a new class of second-order partitioned explicit stabilized methods
for the above ODEs. We treat the moderately stiff term with an s-stage
Runge-Kutta-Chebyshev (RKC) method and treat the non-stiff term with a 4m-stage
explicit Runge-Kutta (RK) method. Different from several existing partitioned
explicit stabilized methods that employ fixed-stage RK methods to handle the
non-stiff term, both the parameters $s$ and $m$ in our methods can be flexibly
adjusted as needed for the problems. This feature endows our methods with
superior flexibility and applicability compared to several existing partitioned
explicit stabilized methods, as demonstrated in several specific numerical
examples (including the advection-diffusion equations, the Burgers equations,
the Brusselator equations and the damped wave equations).

</details>


### [11] [A cell centered Galerkin method for miscible displacement in heterogeneous porous media](https://arxiv.org/abs/2509.14864)
*Maurice S. Fabien*

Main category: math.NA

TL;DR: A cell-centered Galerkin method combining finite volume and discontinuous Galerkin concepts for efficient miscible displacement modeling in porous media with one unknown per cell.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient computational method for miscible displacement problems in heterogeneous porous media that maintains accuracy while reducing computational complexity compared to traditional higher-order methods.

Method: Cell-centered Galerkin (CCG) approach that combines finite volume and discontinuous Galerkin methods, using classical DG weak formulations with only one unknown per cell.

Result: CCG delivers comparable accuracy and improved efficiency over traditional higher-order interior penalty DG methods, with proven inverse-positive matrix properties in 1D for Poisson problems.

Conclusion: The CCG method is effective for highly heterogeneous flow and transport problems in porous media, offering a computationally efficient alternative to classical DG methods while maintaining accuracy.

Abstract: In this paper we present a cell centered Galerkin (CCG) method applied to
miscible displacement problems in heterogeneous porous media. The CCG approach
combines concepts from finite volume and discontinuous Galerkin (DG) methods to
arrive at an efficient lowest-order approximation (one unknown per cell). We
demonstrate that the CCG method can be defined using classical DG weak
formulations, only requires one unknown per cell, and is able to deliver
comparable accuracy and improved efficiency over traditional higher-order
interior penalty DG methods. In addition, we prove that the CCG method for a
model Poisson problem gives rise to a inverse-positive matrix in 1D. A plethora
of computational experiments in 2D and 3D showcase the effectiveness of the CCG
method for highly heterogeneous flow and transport problems in porous media.
Comparisons between CCG and classical DG methods are included.

</details>


### [12] [An Adaptive Sampling Algorithm for Level-set Approximation](https://arxiv.org/abs/2509.14896)
*Matteo Croci,Abdul-Lateef Haji-Ali,Ian C. J. Powell*

Main category: math.NA

TL;DR: Adaptive grid-based stochastic approximation scheme for level-set approximation of Lipschitz multivariate functions with noise robustness and improved computational efficiency.


<details>
  <summary>Details</summary>
Motivation: To develop a robust numerical method for approximating level-sets of Lipschitz multivariate functions that can handle stochastic noise effectively, addressing the challenge of accurate level-set estimation in noisy environments.

Method: Combines adaptive grid-based stochastic approximation with local function approximation and noise reduction. The algorithm automatically refines approximations near the level set using a strategy that controls noise in L^p spaces.

Result: Achieves ε-accurate approximations with expected cost complexity reduction of ε^{-−(−(p+1)/(αp)) compared to non-adaptive schemes, where α is the convergence rate of function approximation. Validated with 2D/3D functions and hyperelasticity PDE problems.

Conclusion: The proposed adaptive scheme provides significant computational efficiency gains for level-set approximation in noisy settings, demonstrating robustness across complex geometries and real-world applications like failure region estimation.

Abstract: We propose a new numerical scheme for approximating level-sets of Lipschitz
multivariate functions which is robust to stochastic noise. The algorithm's
main feature is an adaptive grid-based stochastic approximation strategy which
automatically refines the approximation over regions close to the level set.
This strategy combines a local function approximation method with a noise
reduction scheme and produces $\varepsilon$-accurate approximations with an
expected cost complexity reduction of $\varepsilon^{-\left(\frac{p+1}{\alpha
p}\right)}$ compared to a non-adaptive scheme, where $\alpha$ is the
convergence rate of the function approximation method and we assume that the
noise can be controlled in $L^p$. We provide numerical experiments in support
of our theoretical findings. These include 2- and 3-dimensional functions with
a complex level set structure, as well as a failure region estimation problem
described by a hyperelasticity partial differential equation with random field
coefficients.

</details>


### [13] [Finite Volumes for a dissipative free boundary problem](https://arxiv.org/abs/2509.14908)
*Clément Cancès,Claire Chainais-Hillairet,Amélie Dupouy*

Main category: math.NA

TL;DR: Study of oxygen concentration evolution in oxide layer using convection-diffusion model with moving boundaries, showing existence of traveling-wave solutions and universal entropy structure, with development of unconditionally convergent numerical scheme.


<details>
  <summary>Details</summary>
Motivation: To understand the evolution of oxygen concentration in oxide layers through mathematical modeling and develop accurate numerical methods that preserve key physical properties like traveling waves and entropy dissipation.

Method: Transient convection-diffusion equation in 1D variable width domain with boundary motions governed by concentration traces. Analysis of traveling-wave existence conditions and universal entropy structure. Development of implicit ALE finite volume scheme with Scharfetter-Gummel fluxes.

Result: Established necessary and sufficient conditions for unique traveling-wave solution existence. Demonstrated universal entropy dissipation property. Developed numerical scheme that preserves traveling waves exactly and dissipates free energies unconditionally, with first-order time and second-order space accuracy.

Conclusion: The model successfully captures oxide layer evolution physics with mathematically rigorous properties. The proposed numerical scheme is robust, accurate, and preserves key physical characteristics, making it suitable for long-time simulations converging to traveling-wave solutions.

Abstract: We study a toy model for the evolution of the oxygen concentration in an
oxide layer. It consists in a transient convection diffusion equation in a
one-dimensional domain of variable width. The motions of the boundaries are
governed by the traces of the concentration. We exhibit a necessary and
sufficient condition on the parameters involved in the model for the existence
of a unique traveling-wave solution. Moreover, we show that the model admits
some universal entropy structure, in the sense that any convex function of the
concentration
  yields a dissipated free energy (up to exchanges with the outer environment
at the boundaries). We propose then an implicit in time arbitrary
Lagrangian-Eulerian finite volume scheme based on Scharfetter-Gummel fluxes. It
is shown to be unconditionally convergent, to preserve exactly the travelling
wave, and to dissipate all the aforementioned free energies. Numerical
experiments show that our scheme is first order accurate in time and second
order in space, and that the transient solution converges in the long-time
limit towards the traveling-wave solution.

</details>


### [14] [On the Late-Time Instability of MOT solution to the Time-Domain PMCHWT Equation](https://arxiv.org/abs/2509.14995)
*Van Chien Le,Viviana Giunzioni,Pierrick Cordel,Francesco P. Andriulli,Kristof Cools*

Main category: math.NA

TL;DR: The paper analyzes late-time instability in marching-on-in-time solutions for the time-domain PMCHWT equation, identifying the static solenoidal nullspace as the primary cause and showing fundamental differences from EFIE instability mechanisms.


<details>
  <summary>Details</summary>
Motivation: To understand and address the late-time instability issues in time-domain PMCHWT equation solutions, which are critical for accurate electromagnetic scattering simulations.

Method: Stability analysis of marching-on-in-time solutions, examining the static solenoidal nullspace of the time-domain electric field integral operator and comparing with EFIE instability mechanisms.

Result: Identified that PMCHWT instability is more sensitive to numerical quadrature errors and its spectral characteristics depend heavily on scatterer surface topology and smoothness.

Conclusion: The instability mechanisms of time-domain PMCHWT equation are fundamentally different from EFIE, requiring specialized treatment due to quadrature sensitivity and geometric dependence.

Abstract: This paper investigates the late-time instability of marching-on-in-time
solution to the time-domain PMCHWT equation. The stability analysis identifies
the static solenoidal nullspace of the time-domain electric field integral
operator as the primary cause of instability. Furthermore, it reveals that the
instability mechanisms of the time-domain PMCHWT equation are fundamentally
different from those of the time-domain electric field integral equation. In
particular, the PMCHWT's instability is much more sensitive to numerical
quadrature errors, and its spectral characteristics are strongly influenced by
the topology and smoothness of the scatterer surface.

</details>


### [15] [Fourier heuristic PINNs to solve the biharmonic equations based on its coupled scheme](https://arxiv.org/abs/2509.15004)
*Yujia Huang,Xi'an Li ansd Jinran Wu*

Main category: math.NA

TL;DR: FCPINN is a Fourier-enhanced physics-informed neural network that decomposes high-order biharmonic equations into Poisson equations, improving accuracy and reducing computational complexity for complex boundary problems.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of solving high-order biharmonic equations with complex boundary conditions using traditional PINNs, which often struggle with accuracy and computational efficiency.

Method: Decomposes high-order biharmonic equations into two Poisson equations, integrates Fourier spectral theory with reduced-order formulation, and uses neural networks to handle intricate boundary constraints.

Result: FCPINN achieves optimal trade-off between speed and accuracy, outperforming conventional PINN and deep mixed residual methods, while maintaining stability across different hidden layer configurations.

Conclusion: The Fourier heuristic-enhanced approach provides an effective solution for high-order PDEs with complex boundary conditions, offering improved performance and robustness compared to existing methods.

Abstract: Physics-informed neural networks (PINNs) have been widely utilized for
solving a range of partial differential equations (PDEs) in various scientific
and engineering disciplines. This paper presents a Fourier heuristic-enhanced
PINN (termed FCPINN) designed to address a specific class of biharmonic
equations with Dirichlet and Navier boundary conditions. The method achieves
this by decomposing the high-order equations into two Poisson equations. FCPINN
integrates Fourier spectral theory with a reduced-order formulation for
high-order PDEs, significantly improving approximation accuracy and reducing
computational complexity. This approach is especially beneficial for problems
with intricate boundary constraints and high-dimensional inputs. To assess the
effectiveness and robustness of the FCPINN algorithm, we conducted several
numerical experiments on both linear and nonlinear biharmonic problems across
different Euclidean spaces. The results show that FCPINN provides an optimal
trade-off between speed and accuracy for high-order PDEs, surpassing the
performance of conventional PINN and deep mixed residual method (MIM)
approaches, while also maintaining stability and robustness with varying
numbers of hidden layer nodes.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [16] [Linear non-divergence elliptic equations in a bounded, infinitely winding planar domain](https://arxiv.org/abs/2509.14352)
*Luan Hoang,Akif Ibragimov*

Main category: math.AP

TL;DR: Analysis of second order elliptic equations in complex planar domains that wind infinitely around a circle, examining solution behavior with bounded/unbounded drifts and Dirichlet boundary conditions.


<details>
  <summary>Details</summary>
Motivation: To understand how solutions to non-divergence form elliptic equations behave in domains with intricate geometry that winds infinitely around a fixed circle, particularly examining growth/decay patterns and stability properties.

Method: Studied second order elliptic equations of non-divergence form in planar domains with complex winding geometry, analyzing homogeneous and inhomogeneous cases with bounded/unbounded drifts and Dirichlet boundary conditions using mathematical analysis techniques.

Result: Proved that solutions exhibit exponential growth or decay at infinity for bounded drifts; obtained oscillation estimates for inhomogeneous data; showed similar behavior for unbounded drifts under growth constraints; established uniqueness and continuous dependence on boundary data and forcing functions.

Conclusion: The geometry of infinitely winding domains significantly impacts solution behavior of elliptic equations, with exponential growth/decay patterns emerging and stability properties maintained under various drift and boundary conditions.

Abstract: We study the second order elliptic equations of non-divergence form in a
planar domain with complicated geometry. In this case the domain winds around a
fixed circle infinitely many times and converges to it when the rotating angle
goes to infinity. For the homogeneous equation and the homogeneous Dirichlet
boundary condition, in the case of bounded drifts, we prove that the maximum of
the solution on the cross-section corresponding to a given rotating angle
either grows or decays exponentially as the angle goes to infinity. Results for
the oscillation and its asymptotic estimates are also obtained for
inhomogeneous Dirichlet data. If the drift is unbounded but does not grow to
infinity too fast, then the above maximum also goes to either zero or infinity.
For the inhomogeneous equation, we obtain the estimates in the case of bounded
forcing functions. Moreover, we establish the uniqueness of the solution and
its continuous dependence on the boundary data and the forcing function.

</details>


### [17] [Strong solutions for a class of stochastic thermo-magneto-hydrodynamic-type systems with multiplicative noise](https://arxiv.org/abs/2509.14490)
*Agus L. Soenjaya,Thanh Tran*

Main category: math.AP

TL;DR: Existence and uniqueness of strong solutions for nonlinear stochastic PDEs with multiplicative noise on bounded domains, applicable to various physical systems including fluid dynamics and magnetohydrodynamics.


<details>
  <summary>Details</summary>
Motivation: To develop a unified framework for analyzing a broad class of nonlinear stochastic partial differential equations that model physically relevant thermo-magneto-fluid systems perturbed by spatially correlated multiplicative noise.

Method: Uses Galerkin approximations and compactness arguments, derives strong moment bounds and verifies Cauchy property for approximate solutions up to a stopping time, applies Gronwall-type lemma for stochastic processes.

Result: Establishes existence and uniqueness of maximal strong pathwise solutions, which are global in two spatial dimensions, for a wide range of stochastic thermo-magneto-fluid models.

Conclusion: Provides a unified treatment and rigorous mathematical foundation for nonlinear stochastic PDEs in fluid dynamics and magnetohydrodynamics, with global results in 2D spatial dimensions.

Abstract: We study the existence and uniqueness of strong solutions, in the sense of
PDEs and probability, for a broad class of nonlinear stochastic partial
differential equations (SPDEs) on a bounded domain $\mathscr{O}\subset
\mathbb{R}^d$ ($d\leq 3$), perturbed by spatially correlated multiplicative
noise. Our framework applies to many physically relevant systems, including
stochastic convective Brinkman--Forchheimer equations, stochastic
magnetohydrodynamics (MHD), stochastic B\'enard convection in porous media,
stochastic convective dynamo models, and stochastic magneto-micropolar fluids,
among others. The analysis relies on Galerkin approximations and compactness
arguments. Up to a suitable stopping time, we derive strong moment bounds and
verify a Cauchy property for the approximate solutions, in the absence of any
inherent cancellation structure. By applying a Gronwall-type lemma for
stochastic processes, we establish the existence and uniqueness of maximal
strong pathwise solutions, which are global in two spatial dimensions. These
results provide a unified treatment of a wide class of nonlinear stochastic
thermo-magneto-fluid models in the literature.

</details>


### [18] [Existence, asymptotic behaviors, and high-dimensional uniqueness of topological solutions to the skew-symmetric Chern-Simons system on lattice graphs](https://arxiv.org/abs/2509.14538)
*Honggang Liu*

Main category: math.AP

TL;DR: Existence, asymptotic behavior, and uniqueness of topological solutions to skew-symmetric Chern-Simons system on lattice graphs with Dirac mass sources


<details>
  <summary>Details</summary>
Motivation: To study topological solutions of Chern-Simons systems on lattice graphs, which are important in theoretical physics and mathematical physics for understanding topological phenomena in discrete settings

Method: Mathematical analysis of the skew-symmetric Chern-Simons system with Dirac delta sources, proving existence theorems, analyzing asymptotic behavior as λ→0+ and λ→∞, and establishing uniqueness conditions

Result: Proved existence of topological solutions for fixed g and h, obtained asymptotic behaviors of solutions in both small and large λ limits, and demonstrated uniqueness when lattice dimension is large or λ is sufficiently large

Conclusion: The paper establishes comprehensive mathematical foundations for topological solutions in discrete Chern-Simons systems, providing existence, asymptotic analysis, and uniqueness results under specific conditions

Abstract: In this paper, we consider the topological solutions to the skew-symmetric
Chern-Simons system on lattice graphs: $$\left\{\begin{aligned} \Delta u
&=\lambda\mathrm{e}^{\upsilon}(\mathrm{e}^{u}-1)+4\pi\sum\limits_{j=1}^{k_1}m_j\delta_{p_j},
\Delta
\upsilon&=\lambda\mathrm{e}^{u}(\mathrm{e}^{\upsilon}-1)+4\pi\sum\limits_{j=1}^{k_2}n_j\delta_{q_j},
\end{aligned} \right. $$ here, $\lambda\in\mathbb{R}_+$, $k_1$ and $k_2$ are
two positive integers, $m_j\in\mathbb{N}\, (j=1,2,\cdot\cdot\cdot,k_1)$,
$n_j\in\mathbb{N}\,(j=1,2,\cdot\cdot\cdot,k_2)$, and $\delta_{p}$ denotes the
Dirac mass at vertex $p$. Write $$g=4\pi\sum_{j=1}^{k_1}m_j\delta_{p_j},\
h=4\pi\sum_{j=1}^{k_2}n_j\delta_{q_j},\ B = 4\pi\sum_{j=1}^{k_1}m_j +
4\pi\sum_{j=1}^{k_2}n_j.$$ For any fixed $g,h$, we prove the existence of the
topological solutions to the systems, then obtain the asymptotic behaviors of
topological solutions as $\lambda \rightarrow 0_+$ and $\lambda \rightarrow
+\infty$, and finally prove the uniqueness of the topological solutions when
the dimension of lattice graph $\mathbb{Z}^n$ is large enough or $\lambda$ is
large enough.

</details>


### [19] [Discretization and Vanishing Discount Problems for First-order Mean Field Games](https://arxiv.org/abs/2509.14541)
*Renato Iturriaga,Cristian Mendico,Kaizhi Wang,Yuchen Xu*

Main category: math.AP

TL;DR: Analysis of time discretization and vanishing discount problems for first-order discounted mean field games systems using weak KAM theory, with existence proofs and demonstration of solution non-uniqueness.


<details>
  <summary>Details</summary>
Motivation: To address two key issues in first-order discounted mean field games: time discretization for proving solution existence, and vanishing discount problems for both the original and discretized systems.

Method: Time discretization approach combined with weak KAM theory to prove existence of solutions (u,m) where u is a viscosity solution of the discounted Hamilton-Jacobi equation and m is a projected minimizing measure satisfying continuity equation.

Result: Proved existence of solutions to the discounted mean field games system and provided analysis of vanishing discount problems. Demonstrated non-uniqueness of solutions through an example.

Conclusion: The time discretization method successfully establishes solution existence for discounted mean field games systems, while weak KAM theory provides effective tools for analyzing both discretized and vanishing discount problems, with important implications regarding solution non-uniqueness.

Abstract: This article focuses two issues related to the first-order discounted mean
field games system. The first is the time discretization problem. The time
discretization approach enables us to prove the existence of solutions (u,m) of
the system, where u is a viscosity solution of the discounted Hamilton-Jacobi
equation and m is a projected minimizing measure satisfying the continuity
equation in the sense of distributions. The second is the vanishing discount
problems for both the discounted mean field games system and its discretized
system. The methods we use primarily derive from weak KAM theory. Moreover, we
provide an example demonstrating the non-uniqueness of solutions to the
discounted mean field games system.

</details>


### [20] [Monotonicity properties of the Robin torsion function in a class of symmetric planar domains](https://arxiv.org/abs/2509.14648)
*Qinfeng Li,Juncheng Wei,Ruofei Yao*

Main category: math.AP

TL;DR: Monotonicity of Robin torsion function in symmetric planar domains when Robin coefficient β ≥ -κ (boundary curvature), with counterexample showing condition is sharp.


<details>
  <summary>Details</summary>
Motivation: To establish conditions under which the Robin torsion function exhibits monotonicity properties in symmetric planar domains, which has implications for understanding geometric properties of solutions to Robin boundary value problems.

Method: Mathematical proof using analysis of the Robin torsion function in smooth planar domains with symmetry lines, and construction of a counterexample to demonstrate sharpness.

Result: Proved that monotonicity holds when β ≥ -κ on the boundary, and showed this condition is sharp through counterexample construction.

Conclusion: The condition β ≥ -κ is both sufficient and essentially necessary for monotonicity of the Robin torsion function in symmetric planar domains.

Abstract: We prove the monotonicity property of the Robin torsion function in a smooth
planar domain $\Omega$ with a line of symmetry, provided that the Robin
coefficient $\beta$ is greater than or equal to the negative of the boundary
curvature $\kappa$ (i.e., $\beta \geq -\kappa$ on $\partial\Omega$). We also
show that this condition is, in a certain sense, sharp by constructing a
counterexample.

</details>


### [21] [Normalized solution for Kirchhoff equation with upper critical exponent and mixed Choquard type nonlinearities](https://arxiv.org/abs/2509.14681)
*Jinyuan Shang,Wenting Zhao,Xianjiu Huang*

Main category: math.AP

TL;DR: Existence of normalized solutions for Kirchhoff equation with mixed Choquard nonlinearities in R³, extending previous results to higher exponent range 14/3 < q < 6.


<details>
  <summary>Details</summary>
Motivation: Generalize previous work by Wang et al. (2025) on Kirchhoff equations with combined nonlinearities from 2 < q < 10/3 to the broader range 14/3 < q < 6, addressing challenges from L²-constraint and two nonlocal terms.

Method: Employ subtle energy estimates under L²-constraint to achieve compactness recovery, while handling difficulties from both nonlocal terms in the equation (Kirchhoff term and Choquard nonlinearity).

Result: Established existence of normalized solutions for the Kirchhoff-Choquard equation with mixed nonlinearities in the specified parameter range.

Conclusion: The paper successfully extends previous results to a wider range of exponents and overcomes challenges posed by the L²-constraint and dual nonlocal structure, contributing to the theory of normalized solutions for Kirchhoff-type equations with critical exponents.

Abstract: In this paper, we consider the existence of normalized solution to the
following Kirchhoff equation with mixed Choquard type nonlinearities:
\begin{equation*} \begin{cases} -\left(a + b \int_{\mathbb{R}^3} |\nabla u|^2
\, dx\right) \Delta u - \lambda u = \mu |u|^{q-2} u + (I_\alpha * |u|^{\alpha +
3}) |u|^{\alpha +1} u, \quad x \in \mathbb{R}^3, \\ \int_{\mathbb{R}^3} u^2 \,
dx = \rho^2, \end{cases} \end{equation*} where $a,b,\rho >0$, $\alpha \in
\left(0, 3\right)$, $\frac{14}{3} < q < 6$ and $\lambda \in \mathbb{R}$ will
arise as a Lagrange multiplier. The quantity $\alpha + 3$ here represents the
upper critical exponent relevant to the Hardy-Littlewood-Sobolev inequality,
and this exponent can be regarded as equivalent to the Sobolev critical
exponent $2^*$. We generalize the results by Wang et al.(Discrete and
Continuous Dynamical Systems, 2025), which focused on nonlinear Kirchhoff
equations with combined nonlinearities when $2< q< \frac{10}{3}$. The primary
challenge lies in the necessity for subtle energy estimates under the
\(L^2\)-constraint to achieve compactness recovery. Meanwhile, we need to deal
with the difficulties created by the two nonlocal terms appearing in the
equation.

</details>


### [22] [Blow up results and lifespan estimates for nonlinear damped wave equations on weighted graphs](https://arxiv.org/abs/2509.14767)
*Tuan Anh Dao,Anh Tuan Duong*

Main category: math.AP

TL;DR: Study of nonlinear damped wave equations on weighted graphs, focusing on nonexistence of global solutions and lifespan estimates for blow-up phenomena, with applications to integer lattice graphs.


<details>
  <summary>Details</summary>
Motivation: To extend the analysis of nonlinear damped wave equations from Euclidean settings to weighted graphs, particularly examining blow-up behavior and optimality of results.

Method: Analysis of Cauchy problems for nonlinear damped wave equations on weighted graphs under specific volume growth conditions and initial data requirements.

Result: Proves nonexistence of global weak solutions and provides lifespan estimates for local weak solutions when finite-time blow-up occurs.

Conclusion: The results are partially validated as optimal through application to n-dimensional integer lattice graphs, recovering known Euclidean results.

Abstract: In this article, we are interested in studying the Cauchy problems for
nonlinear damped wave equations and their systems on a weighted graph. Our main
purpose is two-fold, namely, under certain conditions for volume growth of a
ball and the initial data we would like to not only prove nonexistence of
global (in time) weak solutions but also indicate lifespan estimates for local
(in time) weak solutions when a blow-up phenomenon in finite time occurs.
Throughout the present paper, we will partially give a positive answer for the
optimality of our results by an application to the $n$-dimensional integer
lattice graph $\Z^n$ to recover the well-known results in the Euclidean
setting.

</details>


### [23] [Existence and summability of solutions to nonlinear X-elliptic equations with measurable coefficients](https://arxiv.org/abs/2509.14811)
*Marco Picerni*

Main category: math.AP

TL;DR: Existence and regularity results for nonlinear degenerate-elliptic equations with measurable coefficients and zero Dirichlet boundary conditions


<details>
  <summary>Details</summary>
Motivation: To establish existence and regularity properties for solutions to a class of nonlinear degenerate-elliptic equations with measurable coefficients, extending results from Leray-Lions type equations to more general settings involving vector fields satisfying Poincaré inequality and doubling conditions

Method: Proving existence of solutions through analytical methods for nonlinear degenerate-elliptic equations in divergence form, with operators associated to vector fields that satisfy Poincaré inequality and doubling condition

Result: Successfully proved existence of solutions and established that these solutions satisfy a generalization of L^p-regularity results that are known for Leray-Lions type equations

Conclusion: The paper provides important existence and regularity results for a broad class of nonlinear degenerate-elliptic equations with measurable coefficients, extending classical regularity theory to more general geometric settings defined by vector fields with appropriate structural conditions

Abstract: We prove an existence result for solutions to a class of nonlinear
degenerate-elliptic equations with measurable coefficients and zero Dirichlet
boundary condition. The main term is given by a nonlinear operator in
divergence form associated to a family of vector fields which satisfy a
Poincar\'e inequality and the doubling condition.
  Furthermore, we prove that the solutions satisfy a generalization of the
$L^p$-regularity results which hold for the solutions to Leray-Lions type
equations.

</details>


### [24] [Well-posedness of the Boltzmann and Landau Equations in Critical Spaces](https://arxiv.org/abs/2509.14845)
*Ke Chen,Quoc-Hung Nguyen,Tong Yang*

Main category: math.AP

TL;DR: Establishes local and global well-posedness for inhomogeneous Boltzmann and Landau equations in critical function spaces using novel anisotropic norms adapted to scaling invariance.


<details>
  <summary>Details</summary>
Motivation: To resolve the fundamental open problem of well-posedness for kinetic equations in critical function spaces, which is crucial for understanding the mathematical foundations of kinetic theory.

Method: Develops a new analytical framework with novel anisotropic norms adapted to scaling invariance, leverages regularizing effects, decomposes linearized collision operator, and establishes pointwise decay estimates.

Result: Successfully proves local well-posedness near global Maxwellian for small initial perturbations in critical norm, and extends solutions globally with decay estimates.

Conclusion: Resolves a fundamental issue in kinetic theory and provides a new approach applicable to broader class of kinetic models, advancing the mathematical understanding of kinetic equations.

Abstract: This paper investigates the well-posedness of the inhomogeneous Boltzmann and
Landau equations in critical function spaces, a fundamental open problem in
kinetic theory. We develop a new analytical framework to establish local
well-posedness near a global Maxwellian for both equations, under the
assumption that the initial perturbation is small in a critical norm. A major
contribution lies in the introduction of a novel anisotropic norm adapted to
the intrinsic scaling invariance of the equations, which provides precise
control over the high-frequency behavior of solutions. By leveraging the
regularizing effect and a decomposition of the linearized collision operator,
we further extend the local solution globally in time and establish pointwise
decay estimates. Our work not only resolves a fundamental issue in the theory
of kinetic equations in critical spaces, but also provides a new approach
applicable to a broader class of kinetic models.

</details>


### [25] [Invariant Gibbs dynamics for the nonlinear Schrödinger equations on the disc](https://arxiv.org/abs/2509.14861)
*Justin Forlano,Yuzhao Wang*

Main category: math.AP

TL;DR: Construction of invariant Gibbs dynamics for 2D defocusing NLS on unit disc with Gibbs initial data using random averaging operator ansatz


<details>
  <summary>Details</summary>
Motivation: Complete Tzvetkov's program (2006, 2008) on constructing invariant Gibbs dynamics for NLS on the disc with strong solutions

Method: Random averaging operator ansatz to build strong local-in-time solutions, then apply Bourgain's invariant measure argument

Result: Proved almost sure global well-posedness and invariance of the Gibbs measure

Conclusion: Successfully established invariant Gibbs dynamics for 2D defocusing NLS on unit disc under radial symmetry

Abstract: We consider the two-dimensional defocusing nonlinear Schr\"odinger equation
(NLS) on the unit disc in the plane with the Gibbs initial data under radial
symmetry. By using a type of random averaging operator ansatz, we build a
strong local-in-time solution theory, and thus prove almost sure global
well-posedness and invariance of the Gibbs measure via Bourgain's invariant
measure argument. This work completes the program initiated by Tzvetkov (2006,
2008) on the construction of invariant Gibbs dynamics (of strong solutions) for
NLS on the disc.

</details>


### [26] [A monotonicity formula for the fractional Laplacian and instability results for the Shrira equation](https://arxiv.org/abs/2509.14870)
*Argenis J. Méndez,Oscar Riaño*

Main category: math.AP

TL;DR: New monotonicity formula for fractional Laplacian enables localized analysis of L^2-mass preservation in dispersive equations, applied to prove conditional instability of Shrira equation traveling waves without pointwise decay estimates.


<details>
  <summary>Details</summary>
Motivation: To develop a robust framework for analyzing monotonicity properties in higher dimensions and understand the interplay between nonlocal dispersion and spatial localization in nonlinear dispersive models.

Method: Employed a new class of pseudo-differential operators to establish a novel monotonicity formula for the fractional Laplacian |∇_x|^α in R^n, enabling localized analysis of L^2-mass preservation in specific regions.

Result: Proved conditional instability results for traveling wave solutions of the Shrira equation in the critical regime, derived virial-type estimates for long-time behavior, and established instability without requiring pointwise decay estimates.

Conclusion: The approach provides a flexible method for extending monotonicity techniques to higher dimensions and reveals the delicate relationship between nonlocal dispersion and spatial localization in nonlinear dispersive models.

Abstract: By employing a new class of pseudo-differential operators introduced in a
previous work, we establish a novel monotonicity formula for the fractional
Laplacian $|\nabla_x|^\alpha$ in $\mathbb{R}^n$, with $n \geq 2$ and $\alpha
\in [1,2)$, This framework enables us to localize our analysis to specific
regions of Euclidean space where monotonicity properties of the $L^2$-mass of
solutions to dispersive equations with fractional dispersion are preserved.
  As an application, we focus on the Shrira equation, proving conditional
instability results for its traveling wave solutions in the critical regime. We
deduce key virial-type estimates that govern the long-time behavior of the
$L^2$-mass. As a consequence, we establish instability results without
requiring pointwise decay estimates employed in previous works. Our approach
provides a robust and flexible method for monotonicity techniques to higher
dimensions, shedding light on the delicate interplay between nonlocal
dispersion and spatial localization in nonlinear dispersive models.

</details>


### [27] [Lagrangian controllability in perforated domains](https://arxiv.org/abs/2509.14913)
*Mitsuo Higaki,Jiajiang Liao,Franck Sueur*

Main category: math.AP

TL;DR: Study examines Lagrangian controllability of viscous incompressible fluids in perforated domains with small holes, addressing boundary layer challenges through homogenization and stability estimates.


<details>
  <summary>Details</summary>
Motivation: Address the gap in Lagrangian controllability for Navier-Stokes equations with no-slip boundary conditions on solid boundaries, particularly the difficulties caused by viscous boundary layers in bounded domains.

Method: Uses homogenization techniques for evolutionary problems, weak-strong stability estimates in measure of flows, Runge-type approximations for elliptic equations, and Cauchy-Kowalevsky-type theorems for equations with analytic coefficients.

Result: Establishes Lagrangian controllability in perforated domains when the volume fraction occupied by holes is sufficiently small, with quantitative distinctions based on hole parameters (diameter and distance) and fluid initial data size.

Conclusion: The approach successfully addresses viscous boundary layer challenges through homogenization, connecting Navier-Stokes equations to Euler/Darcy equations in the vanishing viscosity limit outside porous media.

Abstract: The question at stake in Lagrangian controllability is whether one can move a
patch of fluid particles to a target location by means of remote action in a
given time interval. In the last two decades, positive results have been
obtained both for the incompressible Euler and Navier-Stokes equations.
However, for the latter, the case where the fluid is contained within domains
bounded by solid boundaries with the no-slip condition has not been addressed,
with respect to the difficulty caused by viscous boundary layers. In this
paper, we investigate the Lagrangian controllability of viscous incompressible
fluid in perforated domains for which the fraction of volume occupied by the
holes is sufficiently small. Moreover, we quantitatively distinguish situations
depending on the parameters for holes (diameter and distance) and for fluid
(size of the initial data). Our approach relies on recent results on
homogenization for evolutionary problems and on weak-strong stability estimates
in measure of flows, alongside classical results on Runge-type approximations
for elliptic equations and on Cauchy-Kowalevsky-type theorems for equations
with analytic coefficients. Here, homogenization refers to the vanishing
viscosity limit outside a porous medium, where (after scaling in time) the
Navier-Stokes equations are homogenized to the Euler or Darcy equations.

</details>


### [28] [Recovering elastic subdomains with strain-gradient elastic interfaces from force measurements: the antiplane shear setting](https://arxiv.org/abs/2509.15171)
*Govanni Granados,Jeremy L. Marzuola,Casey Rodriguez*

Main category: math.AP

TL;DR: Inverse problem for antiplane shear in elastic bodies with strain-gradient interfaces, using boundary measurements to recover inclusion shape and material parameters.


<details>
  <summary>Details</summary>
Motivation: To develop a practical tool for nondestructive detection of interior inhomogeneities and damaged subvolumes in elastic materials with complex interfaces.

Method: Adapted the factorization method to handle higher-order boundary operators and nontrivial null spaces, using displacement-stress measurements on exterior boundary through Dirichlet-to-Neumann map.

Result: Showed uniqueness in recovering shear parameters, interface parameters, and inclusion shape. Numerical experiments demonstrated feasibility of the approach.

Conclusion: The framework provides a viable method for inverse shape problems in elastic bodies with strain-gradient interfaces, potentially enabling practical nondestructive testing applications.

Abstract: We introduce and study a new inverse problem for antiplane shear in elastic
bodies with strain-gradient interfaces. The setting is a homogeneous isotropic
elastic body containing an inclusion separated by a thin interface endowed with
higher-order surface energy. Using displacement-stress measurements on the
exterior boundary, expressed through a certain Dirichlet-to-Neumann map, we
show uniqueness in recovering both the shear and interface parameters, as well
as the shape of the inclusion. To address the inverse shape problem, we adapt
the factorization method to account for the complications introduced by the
higher-order boundary operator and its nontrivial null space. Numerical
experiments illustrate the feasibility of the approach, indicating that the
framework potentially provides a practical tool for nondestructive detection of
interior inhomogeneities, including damaged subvolumes.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [29] [Electron Inertia and Magnetic Reconnection](https://arxiv.org/abs/2509.14400)
*Allen H Boozer*

Main category: physics.plasm-ph

TL;DR: Electron inertia enables magnetic reconnection even without other non-ideal effects, with a modified field evolving ideally while particle trajectories remain largely unaffected except in thin current sheets.


<details>
  <summary>Details</summary>
Motivation: To understand how finite electron mass alone can drive magnetic reconnection in plasma physics, independent of other non-ideal effects.

Method: Introduces a modified magnetic field ε = B + ∇×((m_e/n e²)j) that evolves ideally under electron inertia, and analyzes its relationship to Voigt normalized field and relativistic invariants.

Result: Electron inertia causes magnetic reconnection without other non-ideal effects, but particle trajectories are minimally affected except in regions with extremely large currents in thin sheets.

Conclusion: The modified field ε provides an ideal evolution framework, with electron inertia effects becoming significant only in localized high-current regions, offering insights into reconnection mechanisms.

Abstract: The finite electron mass can cause magnetic reconnection even in the absence
of any other non-ideal effect in a magnetic evolution. It will be shown that
when electron inertia is the only non-ideal effect in the evolution of the
magnetic field $\vec{B}$, there is a related field that evolves ideally. This
field is $\vec{\mathcal{B}} \equiv \vec{B} + \vec{\nabla}\times \left( (m_e/n
e^2) \vec{j} \right)$ with $m_e$ the electron mass, $n$ the electron number
density, and $\vec{j}$ the current density. Although the magnetic field is
modified from its ideal evolution form by the electron inertia, the effect on
particle trajectories, even electron trajectories, is small unless the current
lies in thin sheets, which make $\vec{j}$ extremely large. The field
$\vec{\mathcal{B}}$ is closely related to Voigt normalized magnetic field,
which is defined by a Laplacian smoothing of $\vec{B}$. The difference between
$\vec{\mathcal{B}}$ and $\vec{B}$ involves the relativistically invariant
four-space Laplacian acting on $\vec{B}$ with a $c/\omega_{pe}$ smoothing
distance; $\omega_{pe}$ is the plasma frequency.

</details>


<div id='math.SP'></div>

# math.SP [[Back]](#toc)

### [30] [Sharp Fundamental Gap Estimate on Convex Domains of Gaussian Spaces](https://arxiv.org/abs/2509.14743)
*Jin Sun,Kui Wang*

Main category: math.SP

TL;DR: Sharp lower bound for fundamental gap of convex domains in Gaussian space, confirming Gaussian analogue of fundamental gap conjecture


<details>
  <summary>Details</summary>
Motivation: Extend fundamental gap results from Euclidean and spherical domains to Gaussian spaces, which are fundamentally important settings

Method: Prove lower bound by comparing to one-dimensional Schrödinger operator gap, show monotonicity of normalized gap with diameter, and demonstrate sharpness of estimate

Result: Established that gap is bounded below by corresponding 1D Schrödinger operator gap, confirmed Gaussian analogue of fundamental gap conjecture

Conclusion: Successfully extends Andrews-Clutterbuck (Euclidean) and Seto-Wang-Wei (spherical) results to Gaussian space setting with sharp estimates

Abstract: We prove a sharp lower bound for the fundamental gap of convex domains in
Gaussian space, the difference between the first two eigenvalues of the
Ornstein-Uhlenbeck operator with Dirichlet boundary conditions. Our main result
establishes that the gap is bounded below by the gap of a corresponding
one-dimensional Schr\"odinger operator, confirming the Gaussian analogue of the
fundamental gap conjecture. Furthermore, we demonstrate that the normalized gap
of the one dimensional model is monotonically increasing with the diameter and
prove the sharpness of our estimate. This work extends the seminal results of
Andrews and Clutterbuck for Euclidean domains and Seto, Wang and Wei for
spherical domains to the fundamentally important setting of Gaussian spaces.

</details>


<div id='physics.app-ph'></div>

# physics.app-ph [[Back]](#toc)

### [31] [Electromagnetics of deeply subwavelength metamaterial particles](https://arxiv.org/abs/2509.14690)
*Aleksander O. Makarenko,Maxim A. Yurkin,Alexey A. Shcherbakov,Mikhail Lapine*

Main category: physics.app-ph

TL;DR: Numerical analysis shows million-atom metamaterial samples with sharp edges differ significantly from homogenized materials and are critically sensitive to shape and boundary structure.


<details>
  <summary>Details</summary>
Motivation: To challenge the common belief that large metamaterial samples behave as uniform homogenized materials and to understand the electromagnetic properties of discrete metamaterial structures.

Method: Developed an efficient numerical procedure for calculating quasi-static electromagnetic response of samples containing several million meta-atoms, comparing results with discrete dipole approximation and continuous particle integral models.

Result: Even million-atom samples with sharp edges remain distinct from uniform materials and show critical sensitivity to shape and boundary structure. Discrete metamaterials provide a stringent test platform for continuous models.

Conclusion: The findings are important for understanding mesoscopic systems with strongly interacting elements and demonstrate that discrete structure effects persist even in large metamaterial samples.

Abstract: This article discusses electromagnetic properties of volumetric metamaterial
samples with essentially discrete structure, that is, assembled as a periodic
array of electromagnetic resonators. We develop an efficient numerical
procedure for calculating quasi-static electromagnetic response precisely to
analyse samples containing several million meta-atoms. We demonstrate that,
contrary to a common belief, even million-``atoms'' samples with sharp edges
are still quite different from uniform (``homogenised'') materials, and their
properties are critically sensitive to their shape and boundary structure. We
also compare our results with calculations based on the discrete dipole
approximation as well as with an integral model for continuous particles, and
analyse distinctions and similarities between the different approaches. In
particular, discrete metamaterials present themselves as a stringent platform
for assessing continuous models developed for finite objects with sharp edges.
Overall, the reported results should be important for understanding mesoscopic
systems with strongly interacting elements.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [32] [Immersed Boundary Projection Method for Navier Slip Boundaries](https://arxiv.org/abs/2509.14713)
*Takehiro Fujii,Takeshi Omori*

Main category: physics.flu-dyn

TL;DR: A new immersed boundary method for incompressible flow with surface slip using Navier boundary conditions, featuring implicit determination of slip velocity and boundary forces through projection to satisfy constraints.


<details>
  <summary>Details</summary>
Motivation: To overcome the difficulty of conventional continuous forcing approaches in accurately evaluating velocity gradients on boundaries for flows with surface slip conditions.

Method: Implicit formulation using projection to determine wall slip velocity and boundary force, satisfying both boundary conditions and divergence-free velocity constraint. First-order spatial accuracy, fourth-order temporal accuracy.

Result: Simulations of flow past stationary and moving circular cylinders show good agreement with previous experimental and numerical results across a wide range of slip lengths, including no-slip cases.

Conclusion: The method successfully handles flows with surface slip conditions and provides accurate results comparable to established experimental and numerical data.

Abstract: A formulation of the immersed boundary method for incompressible flow over
bodies with surface slip described by the Navier boundary condition is
presented. In the present method, the wall slip velocity and the boundary force
are determined implicitly through a projection to satisfy the boundary
conditions and the divergence-free condition of the velocity field as
constraints. The present method is first-order accurate in space and
fourth-order accurate in time, overcoming the difficulty of the conventional
continuous forcing approaches to accurately evaluate the velocity gradient on
the boundary. Results from the simulation of the flow past stationary and
moving circular cylinders are in good agreement with previous experimental and
numerical results for a wide range of slip length on the surface, including the
no-slip case.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [33] [Non-Intrusive Parametrized-Background Data-Weak Reconstruction of Cardiac Displacement Fields from Sparse MRI-like Observations](https://arxiv.org/abs/2509.14844)
*Francesco C. Mantegazza,Federica Caforio,Christoph Augustin,Matthias A. F. Gsell,Gundolf Haase,Elias Karabelas*

Main category: physics.med-ph

TL;DR: Non-intrusive 3D cardiac displacement field reconstruction from sparse MRI data using PBDW method with computational efficiency improvements and robust performance under noise.


<details>
  <summary>Details</summary>
Motivation: Personalized cardiac diagnostics need accurate myocardial displacement reconstruction from sparse clinical imaging, but current methods require intrusive access to computational models.

Method: Applied Parametrized-Background Data-Weak (PBDW) approach for 3D cardiac displacement reconstruction using only solution snapshots. Enhanced with H-size minibatch worst-case Orthogonal Matching Pursuit for efficient sensor selection and memory optimization techniques for vectorial problems.

Result: Exceptional accuracy in noise-free conditions (relative L2 error ~1e-5), robust performance with 10% noise (error ~1e-2), effective reconstruction from sparse measurements (error ~1e-2). Achieved 4-order-of-magnitude speed-up compared to full FE simulations with reconstruction times under 0.1 seconds.

Conclusion: The method demonstrates significant potential for clinical integration, enabling immediate deployment across commercial/research codes without requiring governing equations or solver access, making it suitable for real-time cardiac modeling workflows.

Abstract: Personalized cardiac diagnostics require accurate reconstruction of
myocardial displacement fields from sparse clinical imaging data, yet current
methods often demand intrusive access to computational models. In this work, we
apply the non-intrusive Parametrized-Background Data-Weak (PBDW) approach to
three-dimensional (3D) cardiac displacement field reconstruction from limited
Magnetic Resonance Image (MRI)-like observations. Our implementation requires
only solution snapshots -- no governing equations, assembly routines, or solver
access -- enabling immediate deployment across commercial and research codes
using different constitutive models. Additionally, we introduce two
enhancements: an H-size minibatch worst-case Orthogonal Matching Pursuit (wOMP)
algorithm that improves Sensor Selection (SS) computational efficiency while
maintaining reconstruction accuracy, and memory optimization techniques
exploiting block matrix structures in vectorial problems. We demonstrate the
effectiveness of the method through validation on a 3D left ventricular model
with simulated scar tissue. Starting with noise-free reconstruction, we
systematically incorporate Gaussian noise and spatial sparsity mimicking
realistic MRI acquisition protocols. Results show exceptional accuracy in
noise-free conditions (relative L2 error of order O(1e-5)), robust performance
with 10% noise (relative L2 error of order O(1e-2)), and effective
reconstruction from sparse measurements (relative L2 error of order O(1e-2)).
The online reconstruction achieves four-order-of-magnitude computational
speed-up compared to full Finite Element (FE) simulations, with reconstruction
times under one tenth of second for sparse scenarios, demonstrating significant
potential for integration into clinical cardiac modeling workflows.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [34] [A Bayesian thinning algorithm for the point source identification of heat equation](https://arxiv.org/abs/2509.14245)
*Zhiliang Deng,Chen Li,Xiaomei Yang*

Main category: stat.CO

TL;DR: Bayesian thinning algorithm for recovering weighted point sources in heat equation from boundary flux observations using level set representation and Poisson point process thinning


<details>
  <summary>Details</summary>
Motivation: The classical Bayesian framework struggles with constructing suitable priors for highly structured unknowns like point source functions in heat equations

Method: Combines level set representation on discretized mesh with marked Poisson point process modeling and thinning mechanism to generate and refine candidate point sources

Result: Numerical experiments validate the framework, demonstrating accurate reconstruction of point sources

Conclusion: The proposed Bayesian thinning algorithm effectively addresses the challenge of recovering weighted point source functions through complementary level set sampling and thinning processes

Abstract: In this work, we propose a Bayesian thinning algorithm for recovering
weighted point source functions in the heat equation from boundary flux
observations. The major challenge in the classical Bayesian framework lies in
constructing suitable priors for such highly structured unknowns. To address
this, we introduce a level set representation on a discretized mesh for the
unknown, which enables the infinite-dimensional Bayesian framework to the
reconstruction. From another perspective, the point source configuration can be
modeled as a marked Poisson point process (PPP), then a thinning mechanism is
employed to selectively retain points. These two proposals are complementary
with the Bayesian level set sampling generating candidate point sources and the
thinning process acting as a filter to refine them. This combined framework is
validated through numerical experiments, which demonstrate its accuracy in
reconstructing point sources.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [35] [Efficient Computation of Time-Index Powered Weighted Sums Using Cascaded Accumulators](https://arxiv.org/abs/2509.15069)
*Deijany Rodriguez Linares,Oksana Moryakova,Håkan Johansson*

Main category: eess.SP

TL;DR: Novel method for efficiently computing time-index powered weighted sums using cascaded accumulators, reducing computational cost from K×N to K+1 multiplications without requiring data storage.


<details>
  <summary>Details</summary>
Motivation: Traditional computation of time-index powered weighted sums requires K×N multiplications which becomes prohibitive for large N, and alternative methods require storing entire data blocks, making real-time implementation inefficient.

Method: Uses cascaded accumulators to exploit accumulator properties, eliminating the need for data storage and reducing multiplicative operations to only K+1 constant multiplications.

Result: Achieves significant computational efficiency improvement by reducing the number of multiplications from O(K×N) to O(K+1), enabling real-time sample-by-sample processing without storage requirements.

Conclusion: The proposed cascaded accumulator approach provides an efficient solution for computing time-index powered weighted sums in real-time systems, particularly beneficial for sample-by-sample processing where storage and computational efficiency are critical.

Abstract: This letter presents a novel approach for \mbox{efficiently} computing
time-index powered weighted sums of the form $\sum_{n=0}^{N-1} n^{K} v[n]$
using cascaded accumulators. Traditional direct computation requires
$K{\times}N$ general multiplications, which become prohibitive for large $N$,
while alternative strategies based on lookup tables or signal reversal require
storing entire data blocks. By exploiting accumulator properties, the proposed
method eliminates the need for such storage and reduces the multiplicative cost
to only $K{+}1$ constant multiplications, enabling efficient real-time
implementation. The approach is particularly useful when such sums need to be
efficiently computed in sample-by-sample processing systems.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [36] [Computational uncertainties in lattice thermal conductivity prediction of crystalline solids](https://arxiv.org/abs/2509.14702)
*Yagyank Srivastava,Amey G. Gokhale,Ankit Jain*

Main category: cond-mat.mtrl-sci

TL;DR: Study compares computational uncertainties in lattice thermal conductivity predictions from different Boltzmann Transport Equation solvers, DFT packages, exchange-correlation functionals, and machine learning force fields across 50 semiconductors.


<details>
  <summary>Details</summary>
Motivation: To quantify and understand the sources of uncertainty in computational predictions of lattice thermal conductivity using various BTE solvers, DFT implementations, exchange-correlation functionals, and emerging machine learning approaches.

Method: Used three different BTE solvers (ShengBTE, Phono3Py, in-house code) with interatomic forces from DFT packages (Quantum Espresso, VASP) using PBE, LDA, PBEsol, rSCAN functionals, and pre-trained ML force fields on two material datasets for 50 diverse semiconductors.

Result: BTE solvers show minimal uncertainty (1% MAPE), DFT packages show moderate differences (~10% MAPE), exchange-correlation functionals cause large variations (>20% MAPE), and pre-trained ML models predict trends but with high errors limiting screening applications.

Conclusion: Exchange-correlation functional choice introduces the largest uncertainty in thermal conductivity predictions, while BTE solver selection has negligible impact. ML force fields show promise but require further development for accurate quantitative predictions.

Abstract: We report computational uncertainties in Boltzmann Transport Equation
(BTE)-based lattice thermal conductivity prediction of 50 diverse
semiconductors from the use of different BTE solvers (ShengBTE, Phono3Py, and
in-house code) and interatomic forces. The interatomic forces are obtained
either using the density functional theory (DFT) as implemented in packages
Quantum Espresso and VASP employing commonly used exchange correlation
functionals (PBE, LDA, PBEsol, and rSCAN) or using the pre-trained foundational
machine learning forcefields trained on two different material datasets.
  We find that the considered BTE solvers introduce minimal uncertainties and,
using the same interatomic force constants, all solvers result in an excellent
agreement with each other, with a mean absolute percentage error (MAPE) of only
1%. While this error increases to around 10% with the use of different DFT
packages, the error is still small and can be reduced further with the use of
stringent planewave energy cutoffs. On the other hand, the differences in
thermal conductivity due to the use of different exchange correlation
functionals are large, with a MAPE of more than 20%. The currently available
pre-trained foundational ML models predict the right trend for thermal
conductivity, but the associated errors are high, limiting their applications
for coarse screening of materials.

</details>


### [37] [Thermoelectric properties of defective scandium nitride nanostructures](https://arxiv.org/abs/2509.14762)
*Luigi Cigarini,Urszula Danuta Wdowik,Dominik Legut*

Main category: cond-mat.mtrl-sci

TL;DR: Theoretical study using Landauer approach to analyze how oxygen impurities and vacancies affect thermoelectric properties of scandium nitride nanostructures, explaining experimental results and highlighting defect management strategies.


<details>
  <summary>Details</summary>
Motivation: To understand the microscopic origins of thermoelectric efficiency in scandium nitride and explain experimental observations of fabrication-dependent thermoelectric properties.

Method: Used Landauer approach to model electronic transport in ScN nanostructures, analyzing effects of oxygen impurities and spatial vacancies on electronic and structural modifications.

Result: Found that thermoelectric properties of ScN are strongly influenced by structural and electronic factors from defects/impurities, providing theoretical interpretation of experimental fabrication-dependent results.

Conclusion: The theoretical approach shows potential for studying thermoelectricity and developing strategies to improve thermoelectric efficiency through defect management.

Abstract: Transition-metal nitrides (TMNs) are currently being studied for potential
applications in energy conversion. In this work, we used the Landauer approach
to relate the various effects contributing to the thermoelectric efficiency of
scandium nitride (ScN) to their microscopic origins. We model the impact of
electronic and structural modifications induced by oxygen impurities and
spatial vacancies on electronic transport in ScN nanostructures. Taking
advantage of the results of our calculations, we propose a theoretical
interpretation of recent experimental results revealing a strong dependence of
the thermoelectric properties of ScN thin films on procedural variations during
fabrication. The thermoelectric properties of ScN are decisively influenced by
structural and electronic factors arising from defects or impurities. Our
findings highlight the potential of this theoretical approach in studying
thermoelectricity and uncovering future strategies to improve thermoelectric
efficiency.

</details>


### [38] [Ultrafast controlling net magnetization in g-wave altermagnets via laser fields](https://arxiv.org/abs/2509.14991)
*Zhaobo Zhou,Sangeeta Sharma,Junjie He*

Main category: cond-mat.mtrl-sci

TL;DR: Laser-induced demagnetization in g-wave altermagnet CrSb shows direction-dependent behavior, with normal incidence preserving zero net magnetization and off-normal incidence creating transient ferrimagnetic-like states due to anisotropic optical intersite spin transfer.


<details>
  <summary>Details</summary>
Motivation: To understand how diverse nodal spin structures in d/g/i-wave altermagnets affect light-induced spin responses, which remain poorly understood despite their potential for spintronic applications.

Method: Used time-dependent density functional theory (TDDFT) to study laser-induced ultrafast demagnetization dynamics in g-wave altermagnet CrSb, analyzing different laser incidence directions and comparing with d-wave altermagnets.

Result: Normal incidence along [0001] axis causes symmetric demagnetization with different amplitudes but preserves net-zero magnetization. Off-normal incidence induces asymmetric demagnetization, creating transient ferrimagnetic-like states with sizable net magnetization. The response depends on laser polarization alignment with spin-uncompensated regions.

Conclusion: Light-induced magnetization occurs when laser polarization aligns with spin-uncompensated regions in electronic structures, which can be determined from local spin density of states. This provides fundamental understanding of ultrafast dynamics in altermagnets.

Abstract: The diverse nodal spin structures in d/g/i-wave altermagnets (AM) may cause
distinct light-induced spin responses yet remain poorly understood. Using
time-dependent density functional theory (TDDFT), we reveal that laser induced
ultrafast demagnetization dynamics in the g-wave AM CrSb are strongly governed
by the laser incidence direction. Under normal incidence along the [0001] axis,
two Cr sublattices exhibit symmetric temporal demagnetization but with
different amplitudes, preserving the net-zero magnetization, unlike the
behavior in d-wave AM. Off-normal incidence, however, induces pronounced
asymmetric demagnetization between sublattices, transiently driving the system
into a ferrimagnetic-like state with a sizable net magnetization. This
direction-dependent response arises from the characteristic nodal structures in
bulk g-wave AM electronic structure, which enable anisotropic optical intersite
spin transfer (OISTR). By comparing g-wave and d-wave AMs, we propose that
light-induced magnetization arises when laser polarization aligns with
spin-uncompensated regions in electronic structures. This can be readily
determined from the local spin density of states along specific band paths. Our
results provide a fundamental understanding for laser-induced ultrafast
dynamics in AM.

</details>


### [39] [Mapping Microstructure: Manifold Construction for Accelerated Materials Exploration](https://arxiv.org/abs/2509.15022)
*Simon A. Mason,Megna N. Shah,Jeffrey P. Simmons,Dennis M. Dimiduk,Stephen R. Niezgoda*

Main category: cond-mat.mtrl-sci

TL;DR: Framework maps microstructure to low-dimensional manifold parametrized by processing conditions, treating microstructure as stochastic distribution. Distribution-based descriptors enable invertible mapping between processing and microstructure.


<details>
  <summary>Details</summary>
Motivation: Accelerate materials development by establishing quantitative linkages between processing, microstructure, and properties through low-dimensional material manifolds.

Method: Uses phase-field simulations of spinodal decomposition to compare microstructure descriptors (two-point statistics, chord-length distributions, persistent homology) based on dimensionality and invertibility criteria.

Result: Distribution-based descriptors recover two-dimensional latent structure aligned with true processing parameters, creating invertible and interpretable mapping. Non-distribution descriptors overestimate dimensionality or lose fidelity.

Conclusion: Material manifold is locally continuous, enabling microstructure-informed process design and paving way for closed-loop optimization of processing-structure-property relationships.

Abstract: Accelerating materials development requires quantitative linkages between
processing, microstructure, and properties. In this work, we introduce a
framework for mapping microstructure onto a low-dimensional material manifold
that is parametrized by processing conditions. A key innovation is treating
microstructure as a stochastic process, defined as a distribution of
microstructural instances rather than a single image, enabling the extraction
of material state descriptors that capture the essential process-dependent
features. We leverage the manifold hypothesis to assert that microstructural
outcomes lie on a low-dimensional latent space controlled by only a few
parameters. Using phase-field simulations of spinodal decomposition as a model
material system, we compare multiple microstructure descriptors (two-point
statistics, chord-length distributions, and persistent homology) in terms of
two criteria: (1) intrinsic dimensionality of the latent space, and (2)
invertibility of the processing-to-structure mapping. The results demonstrate
that distribution-based descriptors can recover a two-dimensional latent
structure aligned with the true processing parameters, yielding an invertible
and physically interpretable mapping between processing and microstructure. In
contrast, descriptors that do not account for microstructure variability either
overestimate dimensionality or lose predictive fidelity. The constructed
material manifold is shown to be locally continuous, wherein small changes in
process variables correspond to smooth changes in microstructure descriptors.
This data-driven manifold mapping approach provides a quantitative foundation
for microstructure-informed process design and paves the way toward closed-loop
optimization of processing--structure--property relationships in an integrated
materials engineering context.

</details>


### [40] [Physics-Informed GCN-LSTM Framework for Long-Term Forecasting of 2D and 3D Microstructure Evolution](https://arxiv.org/abs/2509.15029)
*Hamidreza Razavi,Nele Moelans*

Main category: cond-mat.mtrl-sci

TL;DR: Physics-informed GCN-LSTM framework for efficient long-term microstructure evolution forecasting in 2D/3D with composition awareness


<details>
  <summary>Details</summary>
Motivation: To develop a computationally efficient method for forecasting microstructure evolution over long time horizons while capturing compositional and morphological dynamics across different material compositions

Method: Integrates graph convolutional networks (GCN) with LSTM architecture, uses convolutional autoencoders to compress phase-field simulation data, operates in latent graph space, and is trained jointly on datasets with different compositions

Result: Remarkable performance across varied metrics, enables long-range forecasting at reduced computational cost while capturing spatial and temporal patterns of evolving microstructures

Conclusion: The framework successfully enables efficient modeling of microstructural evolution across composition, dimensions, and long-term horizons while maintaining computational efficiency

Abstract: This paper presents a physics-informed framework that integrates graph
convolutional networks (GCN) with long short-term memory (LSTM) architecture to
forecast microstructure evolution over long time horizons in both 2D and 3D
with remarkable performance across varied metrics. The proposed framework is
composition-aware, trained jointly on datasets with different compositions, and
operates in latent graph space, which enables the model to capture compositions
and morphological dynamics while remaining computationally efficient.
Compressing and encoding phase-field simulation data with convolutional
autoencoders and operating in Latent graph space facilitates efficient modeling
of microstructural evolution across composition, dimensions, and long-term
horizons. The framework captures the spatial and temporal patterns of evolving
microstructures while enabling long-range forecasting at reduced computational
cost after training.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [41] [Evidential Physics-Informed Neural Networks for Scientific Discovery](https://arxiv.org/abs/2509.14568)
*Hai Siong Tan,Kuancheng Wang,Rafe McBeth*

Main category: cs.LG

TL;DR: E-PINN is a novel uncertainty-aware Physics-Informed Neural Network that uses evidential deep learning for uncertainty estimation and parameter inference, outperforming Bayesian PINN and Deep Ensemble methods in calibration.


<details>
  <summary>Details</summary>
Motivation: To develop a more reliable uncertainty-aware PINN framework that can better quantify uncertainty in PDE solutions and parameter estimation, particularly for real-world applications like medical data analysis.

Method: Leverages marginal distribution loss function from evidential deep learning to estimate output uncertainty, infers unknown PDE parameters through learned posterior distribution, and validates on 1D Poisson and 2D Fisher-KPP equations.

Result: E-PINN generated significantly better calibrated empirical coverage probabilities compared to Bayesian PINN and Deep Ensemble methods, and demonstrated real-world applicability in clinical glucose-insulin dataset analysis.

Conclusion: E-PINN provides a superior uncertainty quantification framework for PINNs with better calibration performance and practical applicability to complex real-world problems like medical research.

Abstract: We present the fundamental theory and implementation guidelines underlying
Evidential Physics-Informed Neural Network (E-PINN) -- a novel class of
uncertainty-aware PINN. It leverages the marginal distribution loss function of
evidential deep learning for estimating uncertainty of outputs, and infers
unknown parameters of the PDE via a learned posterior distribution. Validating
our model on two illustrative case studies -- the 1D Poisson equation with a
Gaussian source and the 2D Fisher-KPP equation, we found that E-PINN generated
empirical coverage probabilities that were calibrated significantly better than
Bayesian PINN and Deep Ensemble methods. To demonstrate real-world
applicability, we also present a brief case study on applying E-PINN to analyze
clinical glucose-insulin datasets that have featured in medical research on
diabetes pathophysiology.

</details>


### [42] [A Neural Network for the Identical Kuramoto Equation: Architectural Considerations and Performance Evaluation](https://arxiv.org/abs/2509.14384)
*Nishantak Panigrahi,Mayank Patwal*

Main category: cs.LG

TL;DR: DNNs for approximating nonlocal conservation law solutions from Kuramoto model, focusing on architecture impact on accuracy and computation time.


<details>
  <summary>Details</summary>
Motivation: Investigate efficiency of Deep Neural Networks for solving nonlocal conservation laws derived from identical-oscillator Kuramoto model, evaluating architectural choices and their impact on solution accuracy.

Method: Systematic experimentation with different network configurations: activation functions (tanh, sin, ReLU), network depth (4-8 hidden layers), width (64-256 neurons), and training methodology (collocation points, epoch count). Comparative analysis with traditional numerical methods.

Result: Tanh activation provides stable convergence, sine activation achieves marginally lower errors and training times in some cases but produces nonphysical artefacts. Optimally configured DNNs offer competitive accuracy with different computational trade-offs. Standard feed-forward architectures have limitations with singular or piecewise-constant solutions, oversmoothing sharp features.

Conclusion: Provides empirical guidelines for DNN implementation in scientific computing while identifying fundamental theoretical constraints that need to be overcome for handling discontinuous physical systems.

Abstract: In this paper, we investigate the efficiency of Deep Neural Networks (DNNs)
to approximate the solution of a nonlocal conservation law derived from the
identical-oscillator Kuramoto model, focusing on the evaluation of an
architectural choice and its impact on solution accuracy based on the energy
norm and computation time. Through systematic experimentation, we demonstrate
that network configuration parameters-specifically, activation function
selection (tanh vs. sin vs. ReLU), network depth (4-8 hidden layers), width
(64-256 neurons), and training methodology (collocation points, epoch
count)-significantly influence convergence characteristics. We observe that
tanh activation yields stable convergence across configurations, whereas sine
activation can attain marginally lower errors and training times in isolated
cases, but occasionally produce nonphysical artefacts. Our comparative analysis
with traditional numerical methods shows that optimally configured DNNs offer
competitive accuracy with notably different computational trade-offs.
Furthermore, we identify fundamental limitations of standard feed-forward
architectures when handling singular or piecewise-constant solutions, providing
empirical evidence that such networks inherently oversmooth sharp features due
to the natural function space limitations of standard activation functions.
This work contributes to the growing body of research on neural network-based
scientific computing by providing practitioners with empirical guidelines for
DNN implementation while illuminating fundamental theoretical constraints that
must be overcome to expand their applicability to more challenging physical
systems with discontinuities.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [43] [Fracture interactive geodesic active contours for bone segmentation](https://arxiv.org/abs/2509.14817)
*Liheng Wang,Licheng Zhang,Hailin Xu,Jingxin Zhao,Xiuyun Su,Jiantao Li,Miutian Tang,Weilu Gao,Chong Chen*

Main category: cs.CV

TL;DR: A fracture interactive geodesic active contour algorithm for bone segmentation that addresses edge obstruction, leakage, and fracture issues by combining intensity and gradient features with distance information.


<details>
  <summary>Details</summary>
Motivation: Classical geodesic active contour models struggle with indiscriminate feature extraction, edge obstruction, edge leakage, and bone fracture handling in bone segmentation tasks.

Method: Proposes a novel edge-detector function combining intensity and gradient norm to guide contour evolution, and introduces distance information with fracture prompts as adaptive step size to stabilize evolution and improve fracture region accuracy.

Result: Experiments on pelvic and ankle segmentation demonstrate effective handling of edge obstruction, leakage, and fracture problems with accurate, stable, and consistent performance.

Conclusion: The algorithm provides robust bone segmentation with fracture interaction capabilities and offers insights for combining domain knowledge with deep neural networks for broader bone anatomy applications.

Abstract: For bone segmentation, the classical geodesic active contour model is usually
limited by its indiscriminate feature extraction, and then struggles to handle
the phenomena of edge obstruction, edge leakage and bone fracture. Thus, we
propose a fracture interactive geodesic active contour algorithm tailored for
bone segmentation, which can better capture bone features and perform robustly
to the presence of bone fractures and soft tissues. Inspired by orthopedic
knowledge, we construct a novel edge-detector function that combines the
intensity and gradient norm, which guides the contour towards bone edges
without being obstructed by other soft tissues and therefore reduces
mis-segmentation. Furthermore, distance information, where fracture prompts can
be embedded, is introduced into the contour evolution as an adaptive step size
to stabilize the evolution and help the contour stop at bone edges and
fractures. This embedding provides a way to interact with bone fractures and
improves the accuracy in the fracture regions. Experiments in pelvic and ankle
segmentation demonstrate the effectiveness on addressing the aforementioned
problems and show an accurate, stable and consistent performance, indicating a
broader application in other bone anatomies. Our algorithm also provides
insights into combining the domain knowledge and deep neural networks.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [44] [The Varieties of Schelling Model Experience](https://arxiv.org/abs/2509.14462)
*Marlyn Boke,Timothy Sorochkin,Jesse Anttila-Hughes,Alan O. Jamison*

Main category: cond-mat.stat-mech

TL;DR: Comprehensive analysis of 54 Schelling model rule variants reveals only 3 distinct phase diagram classes based on number of phase transitions, with robust classification across different segregation measures.


<details>
  <summary>Details</summary>
Motivation: To systematically analyze and classify the diverse rule variants of the Schelling model, which serves as a prototype for agent-based social modeling, and understand the drivers of segregation transitions.

Method: Classification of macroscopic outcomes through phase diagrams for 54 rule variants, analyzing statistical and dynamic drivers including vision, movement criteria, vacancies, initial state, and rivalry using both sociological and percolation-inspired segregation measures.

Result: Only 3 distinct phase diagram classes were found among all variants, characterized by the number of phase transitions. Schelling's original step function was identified as pathological at high thresholds, causing coordination failures. The classification proved robust across different segregation measurement approaches.

Conclusion: This comprehensive classification provides new insights into transition drivers in the Schelling model and establishes a foundation for studying more complex Schelling-like models, revealing the model's fundamental behavioral patterns despite rule variations.

Abstract: The Schelling model is a prototype for agent-based modeling in social
systems. A comprehensive analysis of Schelling model rule variants is achieved
by classification of the space of macroscopic outcomes via phase diagrams.
Among 54 rule variants, only 3 phase diagram classes are found, characterized
by the number of phase transitions. This classification scheme is found to be
robust to the use of sociological and percolation-inspired measures of
segregation. The statistical and dynamic drivers of these transitions are
elucidated by analyzing the roles of vision, movement criteria, vacancies, the
initial state, and rivalry. Schelling's original step function dictating
satisfaction is found to be pathological at high thresholds, producing
coordination failures as satisfactory sites become increasingly rare. This
comprehensive classification gives new insight into the drivers of transitions
in the Schelling model and creates a basis for studying more complex
Schelling-like models.

</details>


<div id='math.FA'></div>

# math.FA [[Back]](#toc)

### [45] [Fractional Sobolev spaces via interpolation, and applications to mixed local-nonlocal operators](https://arxiv.org/abs/2509.14703)
*Alberto Maione*

Main category: math.FA

TL;DR: Connection between Sobolev-Slobodeckij spaces and interpolation theory, showing Sobolev spaces as interpolation spaces between Lebesgue and integer-order Sobolev spaces, with application to mixed local-nonlocal operators.


<details>
  <summary>Details</summary>
Motivation: To provide early-career researchers with a concise and accessible introduction to the connection between fractional Sobolev spaces and interpolation theory.

Method: Presents the well-known connection showing Sobolev spaces can be equivalently characterized as real and complex interpolation spaces between Lebesgue spaces and integer-order Sobolev spaces.

Result: States a spectral theorem for mixed local-nonlocal operators and demonstrates how interpolation theory leads to its proof.

Conclusion: This note establishes the fundamental relationship between Sobolev-Slobodeckij spaces and interpolation theory, providing a foundation for understanding fractional Sobolev spaces through interpolation methods.

Abstract: In this note, we present a well-known connection between the
Sobolev-Slobodeckij spaces, also known as Fractional Sobolev spaces, and
interpolation theory. We show how Sobolev spaces can be equivalently
characterized as real and complex interpolation spaces between Lebesgue spaces
and integer-order Sobolev spaces. We also state a spectral theorem for the
so-called mixed local-nonlocal operators, and show how interpolation theory
leads to its proof. This note is intended for early-career researchers, and
aims to provide a concise and accessible introduction to the subject.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [46] [Wohlhart's Three-Loop Mechanism: An Overconstrained and Shaky Linkage](https://arxiv.org/abs/2509.14698)
*Andreas Mueller*

Main category: cs.RO

TL;DR: Reanalysis of a three-loop spatial linkage shows it has finite DOF 3 (overconstrained) but differential DOF 5 in reference configuration, with smooth manifold c-space and constant differential DOF indicating shakiness.


<details>
  <summary>Details</summary>
Motivation: To revisit and provide deeper analysis of a previously studied three-loop spatial linkage to understand its kinematic properties, particularly the discrepancy between finite and differential degrees of freedom.

Method: Local kinematic analysis including computation of kinematic tangent cone and local approximation of configuration space to examine the linkage's behavior around reference configuration.

Result: The linkage has finite DOF 3 (overconstrained) but differential DOF 5 in reference configuration. Configuration space is locally smooth manifold, reference configuration is not a singularity, and constant differential DOF indicates the linkage is shaky.

Conclusion: The three-loop spatial linkage exhibits shakiness with constant differential DOF, providing new insights into its kinematic behavior that were not fully captured in previous analyses.

Abstract: This paper revisits a three-loop spatial linkage that was proposed in an ARK
2004 paper by Karl Wohlhart (as extension of a two-loop linkage proposed by
Eddie Baker in 1980) and later analyzed in an ARK 2006 paper by Diez-Martinez
et. al. A local analysis shows that this linkage has a finite degree of freedom
(DOF) 3 (and is thus overconstrained) while in its reference configuration the
differential DOF is 5. It is shown that its configuration space is locally a
smooth manifold so that the reference configuration is not a c-space
singularity. It is shown that the differential DOF is locally constant, which
makes this linkage shaky (so that the reference configuration is not a
singularity). The higher-order local analysis is facilitated by the computation
of the kinematic tangent cone as well as a local approximation of the c-space.

</details>
