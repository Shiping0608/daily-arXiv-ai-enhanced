<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 14]
- [math.AP](#math.AP) [Total: 20]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 8]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [math.SP](#math.SP) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.CV](#cs.CV) [Total: 1]
- [astro-ph.HE](#astro-ph.HE) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [stat.ME](#stat.ME) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Variable-preconditioned transformed primal-dual method for generalized Wasserstein Gradient Flows](https://arxiv.org/abs/2509.15385)
*Jin Zeng,Dawei Zhan,Ruchi Guo,Chaozhen Wei*

Main category: math.NA

TL;DR: VPTPD method for solving generalized Wasserstein gradient flows using structure-preserving JKO scheme with variable preconditioners and proximal splitting techniques.


<details>
  <summary>Details</summary>
Motivation: To address challenges from nonsmoothness in objective functions for Wasserstein gradient flows, extending previous TPD method with improved computational efficiency.

Method: Semi-implicit-explicit iteration combining proximal steps for nonsmooth parts with explicit gradient steps for smooth parts, using variable preconditioners from Hessian of regularized objective, adaptive step-size strategy, and convergent Newton solver.

Result: Method achieves superior computational efficiency compared to existing methods, demonstrated through comprehensive numerical experiments from 1D to 3D settings.

Conclusion: VPTPD method provides an effective solution for generalized Wasserstein gradient flows with improved robustness and convergence under poor Lipschitz conditions.

Abstract: We propose a Variable-Preconditioned Transformed Primal-Dual (VPTPD) method
for solving generalized Wasserstein gradient flows via a structure-preserving
JKO scheme. This is a nontrivial extension of the TPD method [Chen et al.
(2023) arXiv:2312.12355] incorporating proximal splitting techniques to address
the challenges arising from the nonsmoothness of the objective function. Our
key contributions include: (i) a semi-implicit-explicit iteration that combines
proximal steps for the nonsmooth part with explicit gradient steps for the
smooth part, and variable preconditioners constructed from the Hessian of a
regularized objective to balance iteration count and per-iteration cost; (ii) a
proof of existence and uniqueness of bounded solutions for the resulting
generalized proximal operator, along with a convergent and bound-preserving
Newton solver; and (iii) an adaptive step-size strategy to improve robustness
and accelerate convergence under poor Lipschitz conditions of the energy
derivative. Comprehensive numerical experiments spanning from 1D to 3D settings
demonstrate that our method achieves superior computational efficiency compared
to existing methods, highlighting its broad applicability through several
challenging simulations.

</details>


### [2] [Numerical Discretization Methods for Seismic Response Analysis of SDOF Systems: A Unified Perspective](https://arxiv.org/abs/2509.15474)
*Farid Ghahari*

Main category: math.NA

TL;DR: This paper reviews numerical methods for solving differential equations governing linear elastic Single-Degree-of-Freedom systems, focusing on response spectrum generation for seismic design applications.


<details>
  <summary>Details</summary>
Motivation: To educate the earthquake engineering community, particularly students, about various numerical methods for solving SDOF system equations, as many may not be fully familiar with methods widely used in other engineering fields.

Method: The author reviews and compares multiple time-discretization methods for Linear Time-Invariant systems, evaluates their accuracy through examples with analytical solutions, and provides MATLAB code implementations.

Result: The paper presents a comprehensive comparison of numerical methods for solving SDOF system equations, focusing on accuracy assessment and practical implementation for real-time response prediction.

Conclusion: This review provides accessible implementation of various numerical methods for earthquake engineering applications, particularly useful for earthquake early warning and post-earthquake assessment systems.

Abstract: This paper reviews the most commonly used numerical methods for solving the
differential equation governing the dynamic response of linear elastic
Single-Degree-of-Freedom (SDOF) systems. For more than 80 years since its
introduction, the response spectrum has remained the cornerstone of every
seismic design code. The second-order differential equation that governs the
dynamic response of a linear elastic SDOF system must be solved numerically to
generate such response spectra. Although only one or two well-accepted
time-discretization methods have been predominantly used by the earthquake
engineering community over the past decades, these methods are directly or
indirectly related to a broader family of methods for solving Linear
Time-Invariant (LTI) systems, which have been extensively applied in other
branches of engineering, particularly electrical engineering. It has recently
come to my attention that a portion of our community, particularly students,
may not be fully familiar with these methods. In this paper, I review these
methods and describe their mathematical background, with a focus on the
relative displacement of the SDOF system under ground acceleration-an essential
quantity for various types of response spectra. I also briefly review some of
the numerical methods traditionally used within our community, highlighting
their similarities and differences. I evaluate the accuracy of all numerical
methods introduced in this paper through several examples with available
analytical solutions. This study focuses on time-domain solutions that can be
employed for real- or near-real-time response prediction, which is particularly
important for applications such as earthquake early warning and post-earthquake
assessment. The paper is written to enable readers to implement these methods
with minimal effort; however, MATLAB codes for all methods discussed are also
provided.

</details>


### [3] [Spotlight inversion by orthogonal projections](https://arxiv.org/abs/2509.15512)
*Daniela Calvetti,Nuutti Hyv√∂nen,Ville Kolehmainen,Erkki Somersalo*

Main category: math.NA

TL;DR: A novel technique called spotlight inversion is proposed for linear inverse problems to eliminate or mitigate the effects of nuisance parameters using linear algebra and orthogonal projections.


<details>
  <summary>Details</summary>
Motivation: In inverse problems, forward models often depend on nuisance parameters that are not of primary interest, complicating the estimation of the desired parameters from noisy observations.

Method: The approach uses linear algebra and orthogonal projections to focus on the region of interest while leaving nuisance parameters in the shadow, demonstrated with local fanbeam X-ray tomography.

Result: The viability of the approach is shown through a computed example, effectively isolating the region of interest from the full target.

Conclusion: Spotlight inversion provides a viable method to handle nuisance parameters in linear inverse problems, enhancing focus on the primary parameters of interest.

Abstract: In inverse problems, the goal is to estimate unknown parameters from indirect
noisy observations. It is not uncommon that the forward model assigning the
observed variables to given values of the unknowns depend on variables that are
not of primary interest, often referred to as nuisance parameters. In this
article, we consider linear inverse problems, and propose a novel technique,
based on linear algebra and orthogonal projections, to eliminate, or at least
mitigate, the contribution of the nuisance parameters on the data. The approach
is referred to as spotlight inversion, as it allows to focus on the part of
primary interest of the unknown parameter, leaving the uninteresting part in
the shadow. The viability of the approach is demonstrated by a computed example
of local fanbeam X-ray tomography: the spotlight is on the region of interest
that is part of the full target.

</details>


### [4] [Numerical Analysis of Simultaneous Reconstruction of Initial Condition and Potential in Subdiffusion](https://arxiv.org/abs/2509.15633)
*Xu Wu,Jiang Yang,Zhi Zhou*

Main category: math.NA

TL;DR: This paper develops a method to simultaneously identify a spatially dependent potential and initial condition in subdiffusion models using terminal observations, with theoretical guarantees and numerical validation.


<details>
  <summary>Details</summary>
Motivation: To solve inverse problems in subdiffusion models where both the potential function and initial condition need to be recovered from limited terminal observations, addressing the ill-posed nature of such problems.

Method: Uses a constructive fixed-point iteration approach for theoretical analysis, then implements a fully discrete scheme combining finite element method, convolution quadrature, and quasi-boundary value method with regularization.

Result: Establishes existence, uniqueness, and conditional stability of the inverse problem; demonstrates linear convergence of the iterative algorithm; provides error analysis for reconstruction.

Conclusion: The proposed method effectively solves the simultaneous identification problem with theoretical guarantees, and numerical experiments validate the approach and error estimates.

Abstract: This paper investigates the simultaneous identification of a spatially
dependent potential and the initial condition in a subdiffusion model based on
two terminal observations. The existence, uniqueness, and conditional stability
of the inverse problem are established under weak regularity assumptions
through a constructive fixed-point iteration approach. The theoretical analysis
further inspires the development of an easy-to-implement iterative algorithm. A
fully discrete scheme is then proposed, combining the finite element method for
spatial discretization, convolution quadrature for temporal discretization, and
the quasi-boundary value method to handle the ill-posedness of recovering the
initial condition. Inspired by the conditional stability estimate, we
demonstrate the linear convergence of the iterative algorithm and provide a
detailed error analysis for the reconstructed initial condition and potential.
The derived \textsl{a priori} error estimate offers a practical guide for
selecting regularization parameters and discretization mesh sizes based on the
noise level. Numerical experiments are provided to illustrate and support our
theoretical findings.

</details>


### [5] [Weak Error Estimates of Ergodic Approximations for Monotone Jump-diffusion SODEs](https://arxiv.org/abs/2509.15698)
*Zhihui Liu,Xiaoming Wu*

Main category: math.NA

TL;DR: This paper analyzes the stochastic theta method (STM) for monotone jump-diffusion SODEs, proving exponential ergodicity for Œ∏ ‚àà (1/2,1] and establishing weak error estimates for the backward Euler method (BEM), with applications to invariant measure convergence.


<details>
  <summary>Details</summary>
Motivation: To address theoretical gaps in numerical methods for stochastic differential equations, particularly proving ergodicity properties and error estimates for the stochastic theta method and backward Euler method in jump-diffusion settings.

Method: Theoretical analysis using mathematical proofs to derive exponential ergodicity of STM and weak error estimates for BEM under dissipative conditions for monotone jump-diffusion SODEs.

Result: Proved exponential ergodicity for STM with Œ∏ ‚àà (1/2,1], established weak error estimates for BEM, and obtained time-independent estimate showing one-order convergence rate between exact and numerical invariant measures in jump-free case.

Conclusion: The results provide rigorous theoretical foundations for numerical methods in stochastic differential equations, answering an open question about convergence rates for invariant measures and extending the understanding of STM and BEM performance in jump-diffusion settings.

Abstract: We first derive the exponential ergodicity of the stochastic theta method
(STM) with $\theta \in (1/2,1]$ for monotone jump-diffusion stochastic ordinary
differential equations (SODEs) under a dissipative condition. Then we establish
the weak error estimates of the backward Euler method (BEM), corresponding to
the STM with $\theta=1$. In particular, the time-independent estimate for the
BEM in the jump-free case yields a one-order convergence rate between the exact
and numerical invariant measures, answering a question left in {\it Z. Liu and
Z. Liu, J. Sci. Comput. (2025) 103:87}.

</details>


### [6] [Polynomial approximation from diffused data: unisolvence and stability](https://arxiv.org/abs/2509.15813)
*Ludovico Bruni Bruno,Stefano De Marchi,Giacomo Elefante*

Main category: math.NA

TL;DR: This paper addresses polynomial interpolation of non-pointwise data from diffuse compact domains, developing a framework to recover unisolvence from nodal results and analyzing stability through a Lebesgue constant-like quantity.


<details>
  <summary>Details</summary>
Motivation: Traditional polynomial interpolation methods assume pointwise data, but many real-world applications involve measurements obtained on diffuse compact domains. The mean value theorem connection between nodal and diffused problems doesn't provide sufficient insights into well-posedness and stability.

Method: The authors develop a framework that recovers unisolvence from nodal interpolation results. They characterize the norm of the interpolation operator using a Lebesgue constant-like quantity, analyze its properties (invariance, sensitivity to support overlapping), and numerically verify theoretical findings.

Result: The proposed framework successfully enables polynomial interpolation for non-pointwise data from diffuse domains, with stability analysis provided through the developed Lebesgue constant characterization.

Conclusion: The work provides a mathematically rigorous approach to polynomial interpolation of diffuse data, establishing connections to classical nodal interpolation while addressing the unique challenges of non-pointwise measurements through stability analysis and numerical validation.

Abstract: In this work, we address the problem of polynomial interpolation of
non-pointwise data. More specifically, we assume that our input information
comes from measurements obtained on diffuse compact domains. Although the nodal
and the diffused problems are related by the mean value theorem, such an
approach does not provide any concrete insights in terms of well-posedness and
stability. We hence develop a different framework in which {\it unisolvence}
can be again recovered from nodal results, for which a wide literature is
available. To analyze the stability of the so-obtained diffused interpolation
procedure, we characterize the norm of the interpolation operator in terms of a
Lebesgue constant-like quantity. After analyzing some of its features, such as
invariance properties and sensitivity to support overlapping, we numerically
verify the theoretical findings.

</details>


### [7] [The inverse of the star discrepancy of a union of randomly digitally shifted Korobov polynomial lattice point sets depends polynomially on the dimension](https://arxiv.org/abs/2509.15877)
*Josef Dick,Friedrich Pillichshammer*

Main category: math.NA

TL;DR: This paper makes progress on finding explicit point set constructions that achieve optimal linear dependence on dimension for star discrepancy, using unions of digitally shifted Korobov polynomial lattice point sets.


<details>
  <summary>Details</summary>
Motivation: The star discrepancy measures uniformity of point sets, and finding explicit constructions that achieve the optimal linear dependence on dimension (N(Œµ,s)) has been a major open problem, as all known proofs are non-constructive.

Method: Analyze point sets constructed from multiset unions of digitally shifted Korobov polynomial lattice point sets, using randomly generated Korobov polynomial lattice point sets shifted by random digital shifts of depth m.

Result: Shows that both a union of randomly generated Korobov polynomial lattice point sets with random digital shifts, and a union of all Korobov polynomial lattice point sets with different random digital shifts, can achieve star discrepancy with inverse depending only linearly on dimension s.

Conclusion: While the proof relies on Bennett's inequality and remains non-constructive, it significantly reduces the search space from a continuum to a finite set of candidates, marking progress towards fully explicit constructions.

Abstract: The star discrepancy is a quantitative measure of the uniformity of a point
set in the unit cube. A central quantity of interest is the inverse of the star
discrepancy, $N(\varepsilon, s)$, defined as the minimum number of points
required to achieve a star discrepancy of at most~$\varepsilon$ in
dimension~$s$. It is known that $N(\varepsilon, s)$ depends only linearly on
the dimension~$s$. All known proofs of this result are non-constructive.
Finding explicit point set constructions that achieve this optimal linear
dependence on the dimension remains a major open problem.
  In this paper, we make progress on this question by analyzing point sets
constructed from a multiset union of digitally shifted Korobov polynomial
lattice point sets. Specifically, we show the following two results. A union of
randomly generated Korobov polynomial lattice point sets shifted by a random
digital shift of depth $m$ can achieve a star discrepancy whose inverse depends
only linearly on $s$. The second result shows that a union of all Korobov
polynomial lattice point sets, each shifted by a different random digital
shift, achieves the same star discrepancy bound. While our proof relies on a
concentration result (Bennett's inequality) and is therefore non-constructive,
it significantly reduces the search space for such point sets from a continuum
of possibilities to a finite set of candidates, marking a step towards a fully
explicit construction.

</details>


### [8] [A Multidimensional Self-Adaptive Numerical Simulation Framework for Semiconductor Boltzmann Transport Equation](https://arxiv.org/abs/2509.15879)
*Zeyu Zhang,Xiaoyu Zhang,Zhigang Song,Qing Fang*

Main category: math.NA

TL;DR: A multidimensional self-adaptive numerical simulation framework for Boltzmann transport equation in semiconductor devices, using polar coordinate transformation and adaptive mesh partitioning to handle singularities and improve accuracy.


<details>
  <summary>Details</summary>
Motivation: To enhance the alignment of numerical simulations with physical characteristics in semiconductor device modeling, particularly for handling singular properties in generalized forms of the Boltzmann transport equation.

Method: Polar coordinate transformation with variable drift-diffusion coefficients, multidimensional adaptive mesh partitioning (radius-angle-time), Swartztrauber-Sweet method and control volume method to eliminate origin singularity, parallelized MATLAB algorithm for efficiency.

Result: Adaptive method improves accuracy: parabolic equation by 1-7 times, continuity equation by 10-70% while maintaining computational efficiency. High stability and systematic verification of convergence, stability, and parameter sensitivity.

Conclusion: Provides theoretical support for high-precision adaptive methods in semiconductor simulation, demonstrating outstanding advantages in handling singular regions with established tuning optimization criteria.

Abstract: This research addresses the numerical simulation of the Boltzmann transport
equation for semiconductor devices by proposing a multidimensional
self-adaptive numerical simulation framework. This framework is applied to two
important generalized forms of the equation: a parabolic equation with singular
properties on the unit disk and a continuity equation. The study enhances the
alignment of numerical simulations with physical characteristics through polar
coordinate transformation and variable drift-diffusion coefficients.
Innovatively, a multidimensional adaptive mesh partitioning strategy for
radius-angle-time is designed and combined with an adjustable finite difference
scheme to construct a highly adaptive numerical simulation method. In the
construction of discrete schemes, the Swartztrauber-Sweet method and the
control volume method are employed to effectively eliminate the origin
singularity caused by polar coordinate transformation. On the programming
front, a parallelized MATLAB algorithm is developed to optimize code execution
efficiency. Numerical comparative experiments demonstrate that the adaptive
method improves the accuracy of the parabolic equation by 1 to 7 times and that
of the continuity equation by 10% to 70% while maintaining computational
efficiency, significantly enhancing numerical simulation accuracy with high
stability. Furthermore, this study systematically verifies the algorithm's
convergence, stability, and parameter sensitivity using error visualization and
other means. It also explores optimal parameters and establishes tuning
optimization criteria. The research provides theoretical support for
high-precision and highly adaptive methods in semiconductor device simulation,
demonstrating outstanding advantages in handling singular regions.

</details>


### [9] [A Flow-rate-conserving CNN-based Domain Decomposition Method for Blood Flow Simulations](https://arxiv.org/abs/2509.15900)
*Simon Klaes,Axel Klawonn,Natalie Kubicki,Martin Lanser,Kengo Nakajima,Takashi Shimokawabe,Janine Weber*

Main category: math.NA

TL;DR: This paper develops CNN-based surrogate models using an alternating Schwarz domain decomposition method to predict blood flow with non-Newtonian viscosity in stenosed arteries, showing that physics-aware approaches outperform purely data-driven methods.


<details>
  <summary>Details</summary>
Motivation: To efficiently predict complex blood flow patterns in stenotic arteries with non-Newtonian viscosity using machine learning surrogates that can handle varying geometries and inflow conditions.

Method: Proposes an alternating Schwarz domain decomposition method with CNN-based subdomain solvers, training a universal subdomain solver (USDS) on fixed geometry and applying it across subdomains, with physics-aware constraints like flow rate conservation.

Result: Physics-aware USDS outperforms purely data-driven approaches, delivering improved subdomain solutions and preventing overshooting/undershooting during Schwarz iterations, leading to more reliable convergence with limited training data.

Conclusion: Incorporating physical constraints like flow rate conservation is crucial for successful CNN-based surrogate modeling in blood flow prediction, especially when training data is limited, as physics-aware approaches ensure more reliable convergence and better performance.

Abstract: This work aims to predict blood flow with non-Newtonian viscosity in stenosed
arteries using convolutional neural network (CNN) surrogate models. An
alternating Schwarz domain decomposition method is proposed which uses
CNN-based subdomain solvers. A universal subdomain solver (USDS) is trained on
a single, fixed geometry and then applied for each subdomain solve in the
Schwarz method. Results for two-dimensional stenotic arteries of varying shape
and length for different inflow conditions are presented and statistically
evaluated. One key finding, when using a limited amount of training data, is
the need to implement a USDS which preserves some of the physics, as, in our
case, flow rate conservation. A physics-aware approach outperforms purely
data-driven USDS, delivering improved subdomain solutions and preventing
overshooting or undershooting of the global solution during the Schwarz
iterations, thereby leading to more reliable convergence.

</details>


### [10] [Computing the Zeros of a Holomorphic Function Using Quadrature-Based Subdivision and Rational Approximation of the Logarithmic Derivative](https://arxiv.org/abs/2509.15936)
*Jake Bowhay,Yuji Nakatsukasa,Irwin Zaid*

Main category: math.NA

TL;DR: A new method using AAA approximation to reliably compute all zeros of holomorphic functions in specified complex regions, based on rational approximation of logarithmic derivative and subdivision using Cauchy's argument principle.


<details>
  <summary>Details</summary>
Motivation: While computing zeros of AAA rational approximations is straightforward, there's no guarantee all zeros in the approximated region will be found. The method addresses this reliability issue.

Method: Combines rational approximation of the logarithmic derivative with subdivision of search regions based on Cauchy's argument principle. Implemented in Python package skzeros.

Result: Provides reliable computation of all zeros of holomorphic functions in specified complex regions. Also applicable to computing zeros and poles of meromorphic functions.

Conclusion: The method offers a reliable approach for finding all zeros in complex regions, addressing limitations of standard AAA approximation methods.

Abstract: We introduce a new method that uses AAA approximation to reliably compute all
the zeros of a holomorphic function in a specified search region in the complex
plane. Specifically, the method is based on rational approximation of the
logarithmic derivative in combination with subdivision of the search region
based on Cauchy's argument principle. This is motivated by the fact that, while
it is straightforward to compute the zeros of a AAA rational approximation,
there is no guarantee that all of the zeros of the function being approximated
in the region being approximated will be found. Many of the ideas presented are
also applicable to computing both the zeros and the poles of a meromorphic
function. A implementation of the method is provided by the Python package
skzeros.

</details>


### [11] [In Ratio Section Method and Algorithms for Minimizing Unimodal Functions](https://arxiv.org/abs/2509.15972)
*Vladimir Kodnyanko*

Main category: math.NA

TL;DR: A new ratio section search method for minimizing unimodal functions that outperforms classical methods like bisection and golden section search, with passive algorithm being 2.26x faster than bisection and 1.72x faster than golden section, and active algorithm being even faster at 3.31x and 2.52x respectively.


<details>
  <summary>Details</summary>
Motivation: To develop a more efficient method for minimizing unimodal functions that can quickly recognize monotone functions and functions with flat bottoms, thereby reducing the number of function evaluations required.

Method: Proposes ratio section search implemented as passive and active algorithms, which involves dividing intervals in a given ratio. The method was also used to modernize the Brent method by replacing its golden section procedure.

Result: The passive algorithm is 2.26x faster than bisection and 1.72x faster than golden section. The active algorithm is 3.31x and 2.52x faster respectively. The modernized Brent method is 1.69x faster than original Brent and 4x faster than bisection.

Conclusion: The proposed ratio section search method is the fastest known method for minimizing unimodal functions, with significant performance improvements over classical methods and even outperforming the modernized Brent method.

Abstract: This paper proposes a new method for section an interval in a given ratio
intended for minimizing unimodal functions. The ratio section search is capable
of quickly recognizing monotone functions and functions with a flat bottom,
which contributes to increasing its performance, as measured by the number of
minimized function evaluations. The method is implemented as passive and active
algorithms. A comparison of the performance of the developed method with that
of the classical methods of bisection search and the golden section search was
performed on the basis of the data used to minimize twenty unimodal functions
of various types. For all types of functions, the passive algorithm is 2.26
times faster than the bisection search and 1.72 times faster than the golden
section method. Thus, the proposed method turned out to be the fastest of the
known methods of cutting off segments intended for minimizing unimodal
functions. The active algorithm is faster: for all types of functions, these
indicators are 3.31 and 2.52, respectively. The fastest combined Brent method
was also modernized. After the golden section procedure is replaced with a
procedure for dividing a segment in a given ratio, a numerical experiment is
conducted. The modernized method is 1.69 times faster than its prototype.
Moreover, the performance of the active algorithm for dividing a segment at a
given ratio exceeds that of the Brent method by 1.48 times for all types of
functions. The modernized Brent method is approximately 4 times faster than the
bisection search and 3 times faster than the golden section method.

</details>


### [12] [An Isogeometric Tearing and Interconnecting (IETI) method for solving high order partial differential equations over planar multi-patch geometries](https://arxiv.org/abs/2509.15997)
*Mario Kapl,Alja≈æ Kosmaƒç,Vito Vitrih*

Main category: math.NA

TL;DR: A novel IETI method for solving high-order PDEs on multi-patch geometries using bilinear-like parameterizations and Lagrange multipliers for smooth coupling.


<details>
  <summary>Details</summary>
Motivation: To efficiently solve high-order partial differential equations (like polyharmonic equations) over complex multi-patch domains while maintaining high-order smoothness across patch boundaries.

Method: Uses Isogeometric Tearing and Interconnecting (IETI) with bilinear-like G^s multi-patch parameterizations, coupling patches via Lagrange multipliers to form a saddle point problem solved through a dual problem and parallel local computations.

Result: Successfully demonstrated on biharmonic (m=2) and triharmonic (m=3) equations across various multi-patch geometries, showing efficient solution capabilities.

Conclusion: The IETI method provides an effective approach for high-order PDE problems on multi-patch domains, enabling C^s-smooth solutions with efficient parallelizable computation.

Abstract: We present a novel method for solving high-order partial differential
equations (PDEs) over planar multi-patch geometries demonstrated on the basis
of the polyharmonic equation of order $m$, $m \geq 1$, which is a particular
linear elliptic PDE of order $2m$. Our approach is based on the concept of
Isogeometric Tearing and Interconnecting (IETI) [43] and allows to couple the
numerical solution of the PDE with $C^s$-smoothness, $s \geq m-1$, across the
edges of the multi-patch geometry. The proposed technique relies on the use of
a particular class of multi-patch geometries, called bilinear-like $G^s$
multi-patch parameterizations [37], to represent the multi-patch domain. The
coupling between the neighboring patches is done via the use of Lagrange
multipliers and leads to a saddle point problem, which can be solved
efficiently first by a small dual problem for a subset of the Lagrange
multipliers followed by local, parallelizable problems on the single patches
for the coefficients of the numerical solution. Several numerical examples of
solving the polyharmonic equation of order $m=2$ and $m=3$, i.e. the biharmonic
and triharmonic equation, respectively, over different multi-patch geometries
are shown to demonstrate the potential of our IETI method for high-order
problems.

</details>


### [13] [Discrete Empirical Interpolation Method with Upper and Lower Bound Constraints](https://arxiv.org/abs/2509.16018)
*Louisa B. Ebby,Mohammad Farazmand*

Main category: math.NA

TL;DR: Constrained DEIM (C-DEIM) extends the Discrete Empirical Interpolation Method to enforce physical constraints on reconstructed functions, ensuring values stay within prescribed bounds through penalty-based optimization.


<details>
  <summary>Details</summary>
Motivation: Standard DEIM can produce reconstructions that violate physical constraints (e.g., negative mass density), making them unusable for downstream applications like forecasting and control.

Method: C-DEIM adds a carefully designed penalty term to the least squares problem to enforce bounds as soft constraints, with theoretical guarantees and an efficient implementation algorithm.

Result: The method produces reconstructions that satisfy physical constraints asymptotically and provides quantitative bounds on observation residuals, demonstrated on heat transfer and wildfire spread models.

Conclusion: C-DEIM successfully addresses DEIM's limitation of violating physical constraints, making it suitable for practical applications requiring physically meaningful reconstructions.

Abstract: Discrete Empirical Interpolation Method (DEIM) is a simple and effective
method for reconstructing a function from its incomplete pointwise
observations. However, applying DEIM to functions with physically constrained
ranges can produce reconstructions with values outside the prescribed physical
bounds. Such physically constrained quantities occur routinely in applications,
e.g., mass density whose range is nonnegative. The DEIM reconstructions which
violate these physical constraints are not usable in downstream tasks such as
forecasting and control. To address this issue, we develop Constrained DEIM
(C-DEIM) whose reconstructions are guaranteed to respect the physical bounds of
the quantity of interest. C-DEIM enforces the bounds as soft constraints, in
the form of a carefully designed penalty term, added to the underlying least
squares problem. We prove that the C-DEIM reconstructions satisfy the physical
constraints asymptotically, i.e., as the penalty parameter increases towards
infinity. We also derive a quantitative upper bound for the observation
residual of C-DEIM. Based on these theoretical results, we devise an efficient
algorithm for practical implementation of C-DEIM. The efficacy of the method
and the accompanying algorithm are demonstrated on several examples, including
a heat transfer problem from fluid dynamics and a cellular automaton model of
wildfire spread.

</details>


### [14] [Sensitivity of Perron and Fiedler eigenpairs to structural perturbations of a network](https://arxiv.org/abs/2509.16024)
*Silvia Noschese,Lothar Reichel*

Main category: math.NA

TL;DR: Analysis of how edge weight perturbations affect Perron and Fiedler eigenvalues and vectors in connected networks, with applications to identifying critical edges for network robustness.


<details>
  <summary>Details</summary>
Motivation: To understand how network connectivity properties change when edge weights are modified, which helps identify edges that are most critical for maintaining network structure and robustness.

Method: Analyzing relevant entries of Perron and Fiedler vectors to estimate changes in eigenvalues when edge weights are perturbed, and investigating the sensitivity of these eigenvectors to perturbations.

Result: Provides a framework to identify edges whose weight perturbation causes the largest change in Perron and Fiedler values, revealing critical edges for network structural integrity.

Conclusion: Perturbation analysis of Perron and Fiedler values/vectors is valuable for identifying edges critical to network robustness, with applications in network design and structural analysis.

Abstract: One can estimate the change of the Perron and Fiedler values for a connected
network when the weight of an edge is perturbed by analyzing relevant entries
of the Perron and Fiedler vectors. This is helpful for identifying edges whose
weight perturbation causes the largest change in the Perron and Fiedler values.
It also is important to investigate the sensitivity of the Perron and Fiedler
vectors to perturbations. Applications of the perturbation analysis include the
identification of edges that are critical for the structural robustness of the
network.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [15] [Construction of multi-solitary waves solution to the focusing nonlinear Schr√∂dinger equation outside an obstacle in the $L^2$-subcritical case](https://arxiv.org/abs/2509.15374)
*Oussama Landoulsi*

Main category: math.AP

TL;DR: Construction of multi-solitary wave solutions for focusing L¬≤-subcritical Schr√∂dinger equation in exterior domains with Dirichlet boundary conditions


<details>
  <summary>Details</summary>
Motivation: To understand how solitary waves behave in exterior domains with obstacles, particularly how multiple solitary waves can coexist and propagate while satisfying boundary conditions

Method: Combines compactness argument (Merle's method for multi-point blow-up), modulation theory, coercivity of linearized operator, and localized energy estimates

Result: Successfully constructed solutions that asymptotically behave as finite sums of solitary waves with distinct large velocities, satisfying Dirichlet boundary conditions

Conclusion: The method provides a framework for constructing multi-solitary wave solutions in exterior domains with obstacles, demonstrating their asymptotic behavior

Abstract: We consider the focusing $L^2$-subcritical Schr\"odinger equation in the
exterior of a smooth, compact, strictly convex obstacle $\Theta \subset
\mathbb{R}^d$. We construct a solution that, for large times, behaves
asymptotically as a finite sum of solitary waves on $\mathbb{R}^d$, each
traveling with sufficiently large and distinct velocities, and satisfying
Dirichlet boundary conditions. The construction is achieved via a compactness
argument similar to that introduced by F.Merle in 1990 for constructing
solutions of the NLS equation that blow up at several points, combined with
modulation theory, the coercivity property of the linearized operator, and
localized energy estimates.

</details>


### [16] [Global Existence and Boundedness of Gray-Scott Model with Local and Nonlocal Diffusion](https://arxiv.org/abs/2509.15535)
*Md Shah Alam*

Main category: math.AP

TL;DR: Global existence of nonnegative solutions for Gray-Scott model with mixed local/nonlocal diffusion using semigroup theory and duality arguments


<details>
  <summary>Details</summary>
Motivation: Study the Gray-Scott model with both local and nonlocal diffusion operators to understand global solution behavior

Method: Use semigroup theory with duality arguments to establish global existence and boundedness

Result: Established global existence and boundedness of component-wise nonnegative solutions

Conclusion: Successfully proved global existence for Gray-Scott model with mixed diffusion operators

Abstract: In this paper, we study the global existence of component-wise nonnegative
solutions of the Gray-Scott model in $\Omega \subset \mathbb{R}^n$, $n \ge 1$,
with a mixture of both local and nonlocal diffusion operators. We use semigroup
theory with duality arguments to establish the global existence and boundedness
of solutions of our model.

</details>


### [17] [Global Existence of Solutions of Nonlocal Geirer-Meinhardt Model and Effect of Nonlocal Operator in Pattern Formation](https://arxiv.org/abs/2509.15598)
*Md Shah Alam*

Main category: math.AP

TL;DR: Global existence analysis of nonlocal Gierer-Meinhardt system using semigroup theory and L^b functional bounds, with diffusive limit results and numerical pattern formation comparisons.


<details>
  <summary>Details</summary>
Motivation: To establish global existence of solutions for nonlocal Gierer-Meinhardt reaction-diffusion systems and compare pattern formation with classical local models.

Method: Used semigroup theory to derive estimates for global existence, constructed L^b functional to bound solutions independent of nonlocal convolution kernel, and performed numerical simulations.

Result: Proved global existence of solutions, obtained diffusive limit similar to previous work, and demonstrated pattern formation through numerical simulations.

Conclusion: The nonlocal Gierer-Meinhardt system exhibits global solutions and pattern formation comparable to classical models, with established mathematical foundations for analysis.

Abstract: We study the global existence of solutions to a class of nonlocal
Geirer-Meinhardt system. This is a two component reaction-diffusion model on a
bounded domain in $\mathbb{R}^n$, $n \ge 1$, with nonlocal diffusion given by a
nonlocal convolution operator. We have used semigroup theory and derive
estimate to guarantee global existence. Then we build an $L^b$ functional to
bound our solution independent of the nonlocal convolution kernel for $2 \le b
< \infty$. Next, we have used this result to obtain a diffusive limit similar
to \cite{laurenccot2023nonlocal} for our model. We also numerically simulate
our model to show the formation of patterns by this model and compare the
results with the patterns with the traditional local/classical Geirer-Meinhardt
model.

</details>


### [18] [Global well-posedness and Gevrey regularity of Navier-Stokes equations in critical Triebel-Lizorkin-Lorentz spaces](https://arxiv.org/abs/2509.15663)
*Qixiang Yang,Hongwei Li*

Main category: math.AP

TL;DR: Global well-posedness and Gevrey regularity of Navier-Stokes equations in critical Triebel-Lizorkin-Lorentz spaces, which are more general than previous spaces studied.


<details>
  <summary>Details</summary>
Motivation: To establish stronger regularity results for Navier-Stokes equations by leveraging the distribution-reflecting properties of Lorentz type spaces, which can handle more general initial value spaces than previous Besov space approaches.

Method: Established global well-posedness in critical Triebel-Lizorkin-Lorentz spaces, then used this foundation to obtain Gevrey regularity of mild solutions.

Result: Proved stronger Gevrey regularity than analyticity, with the solution spaces containing more general initial value spaces including part of Besov spaces and all Triebel-Lizorkin spaces.

Conclusion: The Triebel-Lizorkin-Lorentz space framework provides a more comprehensive approach to Navier-Stokes regularity analysis, yielding stronger results than previous methods focused on Besov spaces.

Abstract: The properties of solutions to Navier-Stokes equations, including
well-posedness and Gevrey regularity, are a class of highly interesting
problems. Inspired by the property of Lorentz type spaces that they reflect the
distribution of large value points, we establish the global well-posedness of
Navier-Stokes equations in critical Triebel-Lizorkin-Lorentz space. Based on
this, we obtained the Gevrey regularity of the mild solution. Compared with
Germain-Pavlovi\'c-Staffilani (2007), the Gevrey regularity we studied is
stronger than analyticity. Furthermore, regarding that previous regularity
studies mostly focused on Besov spaces, such as Liu-Zhang (2024),our
Triebel-Lizorkin-Lorentz spaces contain more general initial value spaces,
including part of Besov spaces and all of Triebel-Lizorkin spaces, etc..

</details>


### [19] [Nonlocal problems with Hardy-Littlewood-Sobolev critical exponent and Hardy potential](https://arxiv.org/abs/2509.15697)
*Guangze Gu,Aleks Jevnikar*

Main category: math.AP

TL;DR: Existence results for a Brezis-Nirenberg type problem involving critical Choquard equations with Hardy potential in bounded domains, obtained via variational methods.


<details>
  <summary>Details</summary>
Motivation: To extend Brezis-Nirenberg type results to critical Choquard equations with Hardy potential, addressing existence problems in bounded domains.

Method: Variational methods are exploited to study the problem, with additional derivation of estimates for a nonlocal minimization problem.

Result: Existence results are obtained for different perturbation terms in the critical Choquard equation with Hardy potential.

Conclusion: The paper successfully extends Brezis-Nirenberg type existence results to critical Choquard equations with Hardy potential, providing new estimates for nonlocal minimization problems.

Abstract: We are concerned with a Brezis-Nirenberg type problem for a critical Choquard
equation, in the sense of Hardy-Littlewood-Sobolev inequality, and with the
Hardy potential in a smooth bounded domain. By exploiting variational methods
we obtain existence results, which extend to different perturbation terms. Some
estimates of independent interest about a nonlocal minimization problem are
also derived.

</details>


### [20] [Phase separation for the 2D Cahn-Hilliard equation with a background shear flow](https://arxiv.org/abs/2509.15773)
*Yu Feng,Yuanyuan Feng,Anna L. Mazzucato,Xiaoqian Xu*

Main category: math.AP

TL;DR: The paper analyzes the Cahn-Hilliard equation with background shear flow, showing that large-amplitude shear causes the 2D solution to converge to a 1D equation, explaining striation patterns in binary fluids.


<details>
  <summary>Details</summary>
Motivation: To rigorously justify the observed phenomenon of striation in concentration fields of binary fluids undergoing phase separation with shear flow.

Method: Using enhanced dissipation properties of the linearized operator with large-amplitude background shear flow satisfying certain conditions, and projecting the full equation orthogonally to the shear direction.

Result: With well-prepared initial data, the solution converges asymptotically to a one-dimensional Cahn-Hilliard equation, demonstrating striation formation.

Conclusion: The study provides mathematical justification for striation patterns in phase-separating binary fluids under shear flow, linking 2D dynamics to reduced 1D behavior.

Abstract: We consider the Cahn-Hilliard equation, which models phase separation in
binary fluids, on the two-dimen\-sional torus in the presence of advection by a
given background shear flow, satisfying certain conditions and of sufficiently
large amplitude. By exploiting the resulting enhanced dissipation for the
linearized operator, we prove that, with well-prepared data, the solution
converges asymptotically at large times to the solution of a one-dimensional
Cahn-Hilliard equation, obtained by projecting the full equation in the
direction orthogonal to the shear in a suitable sense. This result rigorously
justified the observed phenomenon of striation in the concentration field.

</details>


### [21] [On nonlinear elliptic problems with Hardy-Littlewood-Sobolev critical exponent and Sobolev-Hardy critical exponent](https://arxiv.org/abs/2509.15806)
*Guangze Gu,Aleks Jevnikar*

Main category: math.AP

TL;DR: Existence of solutions for nonlinear elliptic problems with convolution-type and Hardy nonlinearity using variational methods.


<details>
  <summary>Details</summary>
Motivation: To address the existence of solutions for a specific class of nonlinear elliptic problems that involve combined convolution-type and Hardy nonlinearities, particularly those with subcritical and critical growth conditions.

Method: Variational methods are employed to establish the existence of solutions for the described nonlinear elliptic problems.

Result: The paper demonstrates the existence of solutions for the class of nonlinear elliptic problems under consideration.

Conclusion: Variational methods are effective in proving the existence of solutions for nonlinear elliptic problems with convolution-type and Hardy nonlinearities, including cases of subcritical and critical growth.

Abstract: In this paper we use variational methods to establish the existence of
solutions for a class of nonlinear elliptic problems involving a combined
convolution-type and Hardy nonlinearity with subcritical and critical growth.

</details>


### [22] [On the simultaneous recovery of boundary impedance and internal conductivity](https://arxiv.org/abs/2509.15878)
*Jinchao Pan,Jijun Liu*

Main category: math.AP

TL;DR: This paper develops a method to simultaneously recover boundary impedance and internal conductivity in electrical impedance tomography using local internal measurements, proving uniqueness and establishing regularized reconstruction schemes with error estimates.


<details>
  <summary>Details</summary>
Motivation: To solve the inverse problem of simultaneously determining boundary impedance and internal conductivity in EIT models, which is challenging due to the coupled nature of these parameters and measurement noise.

Method: Uses volume and surface potentials via Levi function for forward problem. For inverse problem: proves uniqueness through solution extension, applies mollification method for boundary impedance recovery, and Tikhonov regularization for internal conductivity recovery from integral systems.

Result: Establishes rigorous error estimates for noisy data, demonstrates that boundary impedance error propagates to conductivity recovery, and presents numerical implementations validating the proposed approach.

Conclusion: The proposed method successfully recovers both boundary impedance and internal conductivity simultaneously with stable reconstruction schemes and validated numerical performance.

Abstract: Consider an inverse problem of the simultaneous recovery of boundary
impedance and internal conductivity in the electrical impedance tomography
(EIT) model using local internal measurement data, which is governed by a
boundary value problem for an elliptic equation in divergence form with Robin
boundary condition. We firstly express the solution to the forward problem by
volume and surface potentials in terms of the Levi function. Then, for the
inverse problem, we prove the uniqueness of the solution in an admissible set
by unique extension of the solution under some {a-prior} assumption. Finally we
establish the regularizing reconstruction schemes for boundary impedance and
internal conductivity using noisy measurement data with rigorous error
estimates. The mollification method is proposed to recover the boundary
impedance from the boundary condition, and the internal conductivity with known
boundary value is recovered from an integral system, where the Tikhonov
regularization is applied to seek the stable solution, considering that the
error involved in the boundary impedance coefficient reconstruction will
propagate to the recovering process for internal conductivity. Numerical
implementations are presented to illustrate the validity of the proposed
method.

</details>


### [23] [On traveling wave solutions of water-wave equations in curved annular domains](https://arxiv.org/abs/2509.15881)
*Liang Li,Quan Wang*

Main category: math.AP

TL;DR: This paper establishes local and global bifurcation of traveling wave solutions for 2D Euler equations with constant vorticity in curved annular domains with radial gravity, relevant to astrophysical and oceanic flows.


<details>
  <summary>Details</summary>
Motivation: To study traveling waves in systems with curved boundaries and radial gravity, which better model astrophysical flows (planetary rings) and equatorial oceanic currents than traditional flat-bed water wave models.

Method: Local bifurcation analysis near trivial solutions to identify critical parameters and prove existence of small-amplitude solutions, combined with global bifurcation using modified Leray-Schauder degree theory to extend results to large-amplitude waves.

Result: Proved existence of smooth branch of traveling wave solutions with pitchfork-type bifurcation (direction determined by parameter O), and demonstrated that local branch extends globally to large-amplitude waves. Numerical examples confirm both supercritical and subcritical regimes.

Conclusion: The study provides comprehensive understanding of traveling wave behavior in curved domains with radial gravity, offering significant advancement for modeling complex astrophysical and oceanic flow systems.

Abstract: This paper presents a pioneering investigation into the existence of
traveling wave solutions for the two-dimensional Euler equations with constant
vorticity in a curved annular domain, where gravity acts radially inward. This
configuration is highly relevant to astrophysical and equatorial oceanic flows,
such as those found in planetary rings and equatorial currents. Unlike
traditional water wave models that assume flat beds and vertical gravity, our
study more accurately captures the centripetal effects and boundary-driven
vorticity inherent in these complex systems.
  Our main results establish both local and global bifurcation of traveling
waves, marking a significant advancement in the field. First, through a local
bifurcation analysis near a trivial solution, we identify a critical parameter
\(\alpha_c\) and prove the existence of a smooth branch of small-amplitude
solutions. The bifurcation is shown to be pitchfork-type, with its direction
(subcritical or supercritical) determined by the sign of an explicit parameter
\(\mathcal{O}\). This finding provides a nuanced understanding of the wave
behavior under varying conditions. Second, we obtain a global bifurcation
result using a modified Leray-Schauder degree theory. This result demonstrates
that the local branch extends to large-amplitude waves. This comprehensive
analysis offers a holistic view of the traveling wave solutions in this complex
domain. Finally, numerical examples illustrate the theoretical bifurcation
types, confirming both supercritical and subcritical regimes. These examples
highlight the practical applicability of our results.

</details>


### [24] [Infinitely many solutions for $(p,q)$-Schr√∂dinger-Poisson system with concave and convex nonlinearities](https://arxiv.org/abs/2509.15903)
*Yao Du,Jiahao Peng*

Main category: math.AP

TL;DR: Infinitely many solutions for a quasilinear Schr√∂dinger-Poisson system with p-Laplacian Schr√∂dinger equation and q-Laplacian Poisson equation, involving concave/convex nonlinearities and indefinite weighted functions.


<details>
  <summary>Details</summary>
Motivation: To study the existence of multiple solutions for complex quasilinear systems that combine Schr√∂dinger and Poisson equations with nonlinear operators and indefinite weights, which have applications in quantum mechanics and plasma physics.

Method: Analysis of a coupled system consisting of a Schr√∂dinger equation with p-Laplacian operator and a Poisson equation with q-Laplacian operator, incorporating concave and convex nonlinearities with indefinite weighted functions.

Result: The paper proves the existence of infinitely many solutions for this class of quasilinear Schr√∂dinger-Poisson systems.

Conclusion: The established results demonstrate that such complex coupled systems with mixed nonlinearities and indefinite weights admit an infinite number of solutions, extending the understanding of solution multiplicity in nonlinear PDE systems.

Abstract: In this paper, we obtain infinitely many solutions for a class of quasilinear
Schr\"{o}dinger-Poisson system which is coupled by a Schr\"{o}dinger equation
of $p$-Laplacian and a Poisson equation of $q$-Laplacian, involving with
concave and convex nonlinearities and indefinite weighted functions.

</details>


### [25] [On completeness of modified wave operators for defocusing NLS](https://arxiv.org/abs/2509.15921)
*Vladimir Georgiev,Tohru Ozawa*

Main category: math.AP

TL;DR: Study of modified scattering for defocusing nonlinear Schr√∂dinger equation with critical gauge-invariant nonlinearity, focusing on asymptotic behavior and wave operator completeness.


<details>
  <summary>Details</summary>
Motivation: To determine the leading asymptotic term of solutions as time goes to infinity for initial data in weighted Sobolev spaces, and establish completeness of wave operators.

Method: Analysis of nonlinear defocusing Schr√∂dinger equation with critical gauge-invariant nonlinearity of order 1+2/n, using weighted Sobolev spaces and wave operator theory.

Result: Complete answer obtained for n=1 (even for large initial data), and completeness established for n=2 assuming control of solution's sup norm.

Conclusion: The paper provides rigorous asymptotic analysis and wave operator completeness results for critical nonlinear Schr√∂dinger equations in dimensions 1 and 2.

Abstract: In this manuscript, we study modified scattering for the nonlinear defocusing
Schr\"odinger equation with a critical gauge-invariant nonlinearity of order
1+2/n.
  We address the following question: Given initial data in an appropriate
weighted Sobolev space, what is the leading term in the asymptotic behavior of
the solution as times goes to infinity? More precisely, we seek a final state
in a space of type similar to the space of the initial data such that the
leading term is represented by the free propagator and modified phase function.
  The solution to this problem can be reformulated in terms of the completeness
of wave operators. For n=1, we obtain a complete answer, even for large initial
data. For n = 2 completeness is established assuming suitable control of the
sup norm of the solution.

</details>


### [26] [Uniqueness of inverse random source problems for stochastic heat and wave equations](https://arxiv.org/abs/2509.15928)
*Xu Wang,Guanlin Yang,Zhidong Zhang*

Main category: math.AP

TL;DR: This paper studies an inverse random source problem for stochastic evolution equations (heat and wave equations) with unknown source g(x)f(t)·∫Ü(t). It establishes well-posedness of the direct problem, proves existence of strong solutions, and demonstrates unique recovery of |f(t)| from boundary flux measurements.


<details>
  <summary>Details</summary>
Motivation: To address the inverse problem of recovering time-dependent source components in stochastic evolution equations from boundary measurements, which has applications in various fields requiring source identification from limited observational data.

Method: The research first establishes well-posedness of the stochastic direct problem and existence of strong solutions under suitable regularity conditions. For the inverse problem, it proves uniqueness of recovering |f(t)| from boundary flux measurements on nonempty open subsets.

Result: The paper demonstrates the existence of stochastic strong solutions for both heat and wave equations, and proves uniqueness in recovering the time-dependent source strength |f(t)| from boundary flux data. Numerical examples validate the theoretical findings.

Conclusion: The study successfully establishes theoretical foundations for inverse source problems in stochastic evolution equations, providing both existence results for direct problems and uniqueness guarantees for inverse recovery of time-dependent source components from boundary measurements.

Abstract: This paper investigates an inverse random source problem for stochastic
evolution equations, including stochastic heat and wave equations, with the
unknown source modeled as $g(x)f(t)\dot{W}(t)$. The research commences with the
establishment of the well-posedness of the corresponding stochastic direct
problem. Under suitable regularity conditions, the existence of stochastic
strong solutions for both the stochastic heat and wave equations is
demonstrated. For the inverse problem, the objective is to uniquely recover the
strength $|f(t)|$ of the time-dependent component of the source from the
boundary flux on a nonempty open subset. The uniqueness of the recovery for
both the stochastic heat and wave equations is proven, and several numerical
examples are given to verify the theoretical results.

</details>


### [27] [Blow-up analysis for the generalized hyperelastic rod equation](https://arxiv.org/abs/2509.15960)
*Shaojie Yang*

Main category: math.AP

TL;DR: The paper establishes a new local-in-space blow-up criterion for the generalized hyperelastic rod equation, which includes several important equations as special cases.


<details>
  <summary>Details</summary>
Motivation: To understand blow-up phenomena in the generalized hyperelastic rod equation family, which encompasses the Camassa-Holm equation, hyperelastic-rod wave equation, and rotation-Camassa-Holm equation.

Method: Developed a new local-in-space blow-up criterion that is purely local in the space variable under certain initial data conditions.

Result: Established a novel blow-up condition that depends only on local spatial properties of the initial data rather than global characteristics.

Conclusion: The paper provides a significant advancement in understanding blow-up behavior for this class of equations by introducing a purely local spatial criterion.

Abstract: We investigate the blow-up for the generalized hyperelastic rod equation,
which includes the Camassa-Holm equation, the the hyperelastic-rod wave
equation, and the rotation-Camassa-Holm equation as special cases. We establish
a new local-in-space blow-up criterion under certain initial data. Our
local-in-space blow-up condition on the initial data is purely local in the
space variable.

</details>


### [28] [Going with the Flow: Solving for Symmetry-Driven PDE dynamics with Physics-informed Neural Networks](https://arxiv.org/abs/2509.15963)
*Michail Kavousanakis,Gianluca Fabiani,Anastasia Georgiou,Constantinos Siettos,Panagiotis Kevrekidis,Ioannis Kevrekidis*

Main category: math.AP

TL;DR: A computational framework using dynamic symmetry factoring and Physics-Informed Neural Networks (PINNs) to analyze self-similar and traveling wave dynamics in nonlinear PDEs, eliminating the need for large domains or front tracking.


<details>
  <summary>Details</summary>
Motivation: To systematically analyze self-similar and traveling wave dynamics in nonlinear PDEs by dynamically factoring out continuous symmetries like translation and scaling, enabling efficient computation without traditional computational challenges.

Method: Uses time-dependent transformations (dynamic pinning conditions) to render symmetry-invariant solutions stationary, resulting in index-2 DAE systems solved with PINNs that integrate PDE residuals and algebraic constraints into a unified loss function.

Result: Successfully demonstrated on four problems: Nagumo equation (traveling waves), diffusion equation (1D/2D first-kind self-similarity), 2D axisymmetric porous medium equation (second-kind self-similarity), and Burgers equation (both translational and scaling invariance).

Conclusion: PINNs effectively solve complex PDE-DAE systems, providing a promising tool for studying nonlinear wave and scaling phenomena by simultaneously inferring invariant solutions and transformation properties.

Abstract: In the past, we have presented a systematic computational framework for
analyzing self-similar and traveling wave dynamics in nonlinear partial
differential equations (PDEs) by dynamically factoring out continuous
symmetries such as translation and scaling. This is achieved through the use of
time-dependent transformations -- what can be viewed as dynamic pinning
conditions -- that render the symmetry-invariant solution stationary or slowly
varying in rescaled coordinates. The transformation process yields a modified
evolution equation coupled with algebraic constraints on the symmetry
parameters, resulting in index-2 differential-algebraic equation (DAE) systems.
The framework accommodates both first-kind and second-kind self-similarity, and
directly recovers the self-similarity exponents or wave speeds as part of the
solution, upon considering steady-state solutions in the rescaled coordinate
frame. To solve the resulting high-index DAE systems, we employ
Physics-Informed Neural Networks (PINNs), which naturally integrate PDE
residuals and algebraic constraints into a unified loss function. This allows
simultaneous inference of both the invariant solution and the transformation
properties (such as the speed or the scaling rate without the need for large
computational domains, mesh adaptivity, or front tracking. We demonstrate the
effectiveness of the method on four canonical problems: (i) the Nagumo equation
exhibiting traveling waves, (ii) the diffusion equation (1D and 2D) with
first-kind self-similarity, (iii) the 2D axisymmetric porous medium equation
showcasing second-kind self-similarity, and (iv) the Burgers equation, which
involves both translational and scaling invariance. The results demonstrate the
capability of PINNs to effectively solve these complex PDE-DAE systems,
providing a promising tool for studying nonlinear wave and scaling phenomena.

</details>


### [29] [Fundamental solution and Harnack inequality for subelliptic evolution operators on Carnot groups](https://arxiv.org/abs/2509.15982)
*Giulio Pecorella,Annalaura Rebucci*

Main category: math.AP

TL;DR: Study of regularity properties for subelliptic evolution operators using fundamental solutions and geometric methods


<details>
  <summary>Details</summary>
Motivation: To understand and establish regularity properties for a class of subelliptic evolution operators, which are important in partial differential equations and geometric analysis

Method: Use Levi's parametrix method to prove existence of fundamental solution, then apply mean value formulas and Carnot-Caratheodory geometry to derive maximum principle and Harnack inequality

Result: Proved existence of fundamental solution, established key properties, and derived maximum principle and invariant Harnack inequality for classical solutions

Conclusion: The Carnot-Caratheodory geometry induced by the vector fields is crucial for the analysis of these subelliptic operators

Abstract: In this paper we study regularity properties of a class of subelliptic
evolution operators. We first prove the existence of the fundamental solution
by means of Levi's parametrix method, establishing also several key properties.
We then employ these results, together with some mean value formulas, to prove
a maximum principle and an invariant Harnack inequality for the classical
solutions to the equations under study. Our analysis critically relies on the
Carnot Caratheodory geometry naturally induced by the vector fields defining
these operators.

</details>


### [30] [Higher H√∂lder regularity for fractional $(p,q)$-Laplace equations](https://arxiv.org/abs/2509.15988)
*Prashanta Garain,Erik Lindgren*

Main category: math.AP

TL;DR: Study of fractional (p,q)-Laplace equation with H√∂lder estimates and Liouville-type theorem using Moser-type iteration techniques.


<details>
  <summary>Details</summary>
Motivation: To extend results from fractional p-Laplace equations to the more general fractional (p,q)-Laplace equation, establishing regularity properties and qualitative behavior.

Method: Builds on techniques for fractional p-Laplace equation, using Moser-type iteration for difference quotients to prove regularity estimates.

Result: Established H√∂lder estimates with explicit exponent and derived a Liouville-type theorem for the fractional (p,q)-Laplace equation.

Conclusion: Successfully extended regularity results to the fractional (p,q)-Laplace operator, providing important tools for studying this class of nonlocal equations.

Abstract: We study the fractional $(p,q)$-Laplace equation $$ (-\Delta_p)^s u
+(-\Delta_q)^t u= 0 $$ for $s,t\in(0,1)$ and $p,q\in(1,\infty)$. We establish
H\"older estimates with an explicit exponent. As a consequence, we derive a
Liouville-type theorem. Our approach builds on techniques previously developed
for the fractional $p$-Laplace equation, relying on a Moser-type iteration for
difference quotients.

</details>


### [31] [Viscosity and minimax solutions for path-dependent Hamilton-Jacobi equations in infinite dimensions and related differential games](https://arxiv.org/abs/2509.16015)
*Erhan Bayraktar,Mikhail Gomoyunov,Christian Keller*

Main category: math.AP

TL;DR: This paper establishes new results for path-dependent Hamilton-Jacobi equations with nonlinear monotone and coercive operators on Hilbert space, proving uniqueness of minimax solutions, introducing equivalent viscosity solutions, and applying these to zero-sum differential games for time-delay evolution equations.


<details>
  <summary>Details</summary>
Motivation: To extend previous work by Bayraktar and Keller on path-dependent Hamilton-Jacobi equations by proving results under more general assumptions than previously considered, including in finite-dimensional cases, and to develop applications to differential games.

Method: The authors prove uniqueness of minimax solutions for terminal-value problems, introduce a new notion of viscosity solution and show equivalence to minimax solutions, obtain stability results using half-relaxed limits method, and apply these to prove existence and characterization theorems for zero-sum differential games with time-delay evolution equations.

Result: The paper establishes: (1) uniqueness of minimax solutions under more general assumptions, (2) existence of solutions on the whole path space, (3) equivalence between new viscosity solution concept and minimax solutions, (4) stability results for viscosity solutions, and (5) two theorems on existence and characterization of value for zero-sum differential games.

Conclusion: The paper successfully extends the theory of path-dependent Hamilton-Jacobi equations to more general settings, establishes connections between different solution concepts, and provides concrete applications to differential game theory with time-delay systems, bridging theoretical advances with practical applications.

Abstract: We establish new results for path-dependent Hamilton-Jacobi equations with
nonlinear monotone, and coercive operators on Hilbert space, which were
initially studied in Bayraktar and Keller [J. Funct. Anal., 275 (8) (2018), pp.
2096-2161]. Under more general assumptions than in the cited paper (and more
general than in the finite-dimensional case as well), we prove the uniqueness
of a minimax solution of a terminal-value problem for the equation under
consideration and the existence of such a solution on the whole path space. We
introduce a new notion of a viscosity solution for this problem and show the
equivalence of this notion to the notion of a minimax solution, which implies
the corresponding existence and uniqueness theorem for viscosity solutions. In
addition, we obtain a stability result for viscosity solutions using the
half-relaxed limits method. As applications, we prove two theorems on the
existence and characterization of value of a zero-sum differential game for a
time-delay (path-dependent) evolution equation. The first theorem pertains to
the case of non-anticipative (Elliott-Kalton) strategies and is related to the
results on viscosity solutions, and the second theorem deals with the case of
feedback (Krasovskii-Subbotin) strategies and is based on the results on
minimax solutions.

</details>


### [32] [On the Fountain Theorem for Continuous Functionals and Its Application to a Semilinear Elliptic Problem in $\mathbb{R}^2$](https://arxiv.org/abs/2509.16059)
*Ablanvi Songo,Fabrice Colin*

Main category: math.AP

TL;DR: Establishes a continuous version of the Fountain theorem using weak slope framework for continuous functionals, generalizing Willem's Theorem 3.6, with application to semilinear problems.


<details>
  <summary>Details</summary>
Motivation: To generalize and extend the Fountain theorem to continuous functionals using the weak slope framework, building upon existing mathematical foundations.

Method: Uses the framework of weak slope for continuous functionals to establish a continuous version of the Fountain theorem.

Result: Successfully generalizes Theorem 3.6 of Willem and provides a continuous Fountain theorem applicable to semilinear problems.

Conclusion: The continuous Fountain theorem provides a valuable extension to existing mathematical tools with practical applications in semilinear analysis.

Abstract: In this work, we establish a continuous version of the Fountain theorem by
using the framework of the weak slope for continuous functionals, which
generalizes Theorem 3.6 of Willem \cite{Wi}. Then we present an application to
a semilinear problem.

</details>


### [33] [On Tent Spaces for the Gaussian Measure](https://arxiv.org/abs/2509.16148)
*Liliana Forzani,Roberto Scotto,Wilfredo Urbina*

Main category: math.AP

TL;DR: This paper extends tent space theory from classical harmonic analysis to the Gaussian setting, defining Gaussian tent spaces with atomic decompositions and dual characterizations.


<details>
  <summary>Details</summary>
Motivation: To develop a complete tent space theory for Gaussian harmonic analysis, building on previous partial results and using area function methods.

Method: Defines Gaussian tent spaces using a variation of the area function from prior work, then proves atomic decompositions and characterizes dual spaces.

Result: Establishes Gaussian tent space theory with atomic decompositions and dual space characterizations, providing applications.

Conclusion: Successfully extends classical tent space theory to the Gaussian setting, creating a foundation for further applications in Gaussian harmonic analysis.

Abstract: Following the scheme of tent spaces in classical harmonic analysis developed
by R. Coifman, Y. Meyer, and E. Stein in \cite{cms}, we succeed in doing so for
the Gaussian setting. In \cite{MNP}, part of this theory (an atomic
decomposition) is developed for a specific tent space where functions are
defined just in a proper subset of $\mathbb{R}^{n+1}_+,$ and without the use of
an area function. In the present paper, using a variation of the area function
considered in \cite{FSU}, we define the Gaussian area function and Gaussian
tent spaces and prove both their atomic decompositions and the characterization
of their dual spaces. Some applications are also considered.

</details>


### [34] [Quantitative stability of the Rossby--Haurwitz waves of degree two for the Euler equation on $\mathbb{S}^2$](https://arxiv.org/abs/2509.16156)
*Matias G. Delgadino,Luca Melzi*

Main category: math.AP

TL;DR: The paper proves orbital stability of degree-2 Rossby-Haurwitz travelling waves on the Euler equation on the 2-sphere.


<details>
  <summary>Details</summary>
Motivation: To establish stability properties for fundamental wave solutions in geophysical fluid dynamics, specifically for Rossby-Haurwitz waves which are important in atmospheric and oceanic modeling.

Method: The authors provide a short, quantitative proof that is conceptually easy to follow, suggesting they use clear mathematical arguments rather than complex technical machinery.

Result: The main result is the orbital stability of degree-2 Rossby-Haurwitz travelling waves on the Euler equation on the 2-sphere.

Conclusion: The proof successfully demonstrates orbital stability with an approach that is both concise and accessible, contributing to the understanding of wave stability in geophysical fluid dynamics.

Abstract: We show that the degree-2 Rossby--Haurwitz travelling waves on the Euler
equation on $\mathbb{S}^2$ are orbitally stable. Our proof is short,
quantitative, and conceptually easy to follow.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [35] [What does it mean for a system to compute?](https://arxiv.org/abs/2509.15855)
*David H. Wolpert,Jan Korbel*

Main category: physics.comp-ph

TL;DR: The paper introduces a framework for identifying computations in dynamical systems, particularly natural ones like the brain, by mapping their evolution to abstract computational machines.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of identifying computations in naturally occurring dynamical systems (e.g., the human brain) where inputs, outputs, and logical variables are not explicitly defined, unlike in artificial systems.

Method: Surveying computational properties of various dynamical systems and proposing a framework that defines computations via maps between a system's evolution and an abstract computational machine's evolution.

Result: The framework is illustrated with examples from the literature, highlighting cases that do not fully fit the framework, and discusses uncomputability and quantifying computation value.

Conclusion: The framework provides a broadly applicable approach to identify computations in dynamical systems, bridging the gap between artificial and natural computers.

Abstract: Many real-world dynamic systems, both natural and artificial, are understood
to be performing computations. For artificial dynamic systems, explicitly
designed to perform computation - such as digital computers - by construction,
we can identify which aspects of the dynamic system match the input and output
of the computation that it performs, as well as the aspects of the dynamic
system that match the intermediate logical variables of that computation. In
contrast, in many naturally occurring dynamical systems that we understand to
be computers, even though we neither designed nor constructed them - such as
the human brain - it is not a priori clear how to identify the computation we
presume to be encoded in the dynamic system. Regardless of their origin,
dynamical systems capable of computation can, in principle, be mapped onto
corresponding abstract computational machines that perform the same operations.
In this paper, we begin by surveying a wide range of dynamic systems whose
computational properties have been studied. We then introduce a very broadly
applicable framework for identifying what computations(s) are emulated by a
given dynamic system. After an introduction, we summarize key examples of
dynamical systems whose computational properties have been studied. We then
introduce a very broadly applicable framework that defines the computation
performed by a given dynamical system in terms of maps between that system's
evolution and the evolution of an abstract computational machine. We illustrate
this framework with several examples from the literature, in particular
discussing why some of those examples do not fully fall within the remit of our
framework. We also briefly discuss several related issues, such as
uncomputability in dynamical systems, and how to quantify the value of
computation in naturally occurring computers.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [36] [The hot-electron closure of the moment-based gyrokinetic plasma model](https://arxiv.org/abs/2509.15329)
*A. C. D. Hoffmann,P. Giroud-Garampon,P. Ricci*

Main category: physics.plasm-ph

TL;DR: Derivation of hot-electron limit (HEL) closure for gyromoment (GM) equations by expanding gyroaveraging kernels in small Ti/Te limit, obtaining closed system for key plasma moments.


<details>
  <summary>Details</summary>
Motivation: To develop a simplified closure for gyrokinetic moment hierarchy that captures essential physics while reducing computational complexity, particularly in the hot-electron regime.

Method: Expand gyroaveraging kernels in small Ti/Te limit, retain only essential O(Ti/Te) terms to obtain closed system for density, parallel velocity, and parallel/perpendicular temperatures. Validate through analytical equivalence in Z-pinch geometry and numerical benchmarks.

Result: HEL closure accurately reproduces linear growth rates, nonlinear heat transport, and low-collisionality dynamics in Z-pinch geometry. In tokamak geometry, it preserves transport levels and temporal dynamics even at Ti/Te=1, but cannot predict Dimits shift or transport suppression due to absence of higher-order moments.

Conclusion: The HEL closure provides an efficient and accurate reduced model for gyrokinetic simulations in hot-electron regimes, though limited in capturing certain kinetic effects requiring higher-order moments.

Abstract: We derive the hot-electron limit (HEL) closure of the moment hierarchy used
to solve the gyrokinetic equations, denoted as gyromoment (GM). By expanding
gyroaveraging kernels in the small temperature ratio Ti/Te limit and retaining
only essential O(Ti/Te) terms, we obtain a closed system for density, parallel
velocity, and parallel and perpendicular temperatures. In the Z-pinch geometry,
the GM system with HEL closure is analytically equivalent to the one developed
by Ivanov et al. (2022). Numerical benchmarks confirm the closure's accuracy in
Z-pinch geometry, reproducing established linear growth rates, nonlinear heat
transport, and low-collisionality dynamics. Extension to tokamak-relevant
s-alpha geometry, and comparison with gyrokinetic simulations, reveals the HEL
closed GM model capabilities and limitations: while transport levels and
temporal dynamics are qualitatively preserved even at Ti/Te=1, the absence of
higher-order kinetic moments prevents accurate Dimits shift prediction and
transport suppression.

</details>


### [37] [Structure-preserving long-time simulations of turbulence in magnetized ideal fluids](https://arxiv.org/abs/2509.15425)
*Klas Modin,Michael Roop*

Main category: physics.plasm-ph

TL;DR: Analysis of three 2D magnetohydrodynamics models (RMHD, Hazeltine's model, CHM) using structure-preserving discretizations to study long-time statistical behavior, revealing differences in magnetic potential and vorticity dynamics.


<details>
  <summary>Details</summary>
Motivation: To understand the long-time statistical behavior of magnetohydrodynamic turbulence and plasma dynamics by preserving the geometric phase space structure through Hamiltonian formulations, which is crucial for accurate numerical simulations.

Method: Used matrix hydrodynamics approach for structure-preserving discretizations of all three models, conducted long-time simulations with randomized initial data, and compared model behaviors through spectral scaling diagrams.

Result: RMHD and Hazeltine's model produce magnetic dipoles suggesting inverse cascade of magnetic energy, while vorticity dynamics differ: RMHD forms sharp vortex filaments with growing vorticity, whereas Hazeltine's model and CHM show small vorticity variations and indicate inverse cascade of kinetic energy not present in RMHD.

Conclusion: The geometric phase space structure significantly influences long-time statistical behavior, with different models exhibiting distinct cascade properties - magnetic energy inverse cascade in RMHD and Hazeltine's model, and kinetic energy inverse cascade in Hazeltine's model and CHM but not in RMHD.

Abstract: We address three two-dimensional magnetohydrodynamics models: reduced
magnetohydrodynamics (RMHD), Hazeltine's model, and the Charney-Hasegawa-Mima
(CHM) equation. These models are derived to capture the basic features of
magnetohydrodynamic turbulence and plasma behaviour. They all possess
non-canonical Hamiltonian formulations in terms of Lie-Poisson brackets, which
imply an infinite number of conservation laws along with symplecticity of the
phase flow. This geometric structure in phase space affects the statistical
long-time behaviour. Therefore, to capture the qualitative features in
long-time numerical simulations, it is critical to use a discretization that
preserves the rich phase space geometry. Here, we use the matrix hydrodynamics
approach to achieve structure-preserving discretizations for each model. We
furthermore carry out long-time simulations with randomized initial data and a
comparison between the models. The study shows consistent behaviour for the
magnetic potential: both RMHD and Hazeltine's model produce magnetic dipoles
(in CHM, the magnetic potential is prescribed). These results suggest an
inverse cascade of magnetic energy and of the mean-square magnetic potential,
which is empirically verified via spectral scaling diagrams. On the other hand,
the vorticity field dynamics differs between the models: RMHD forms sharp
vortex filaments with rapidly growing vorticity values, whereas Hazeltine's
model and CHM show only small variation in the vorticity values. Related to
this observation, both Hazeltine's model and CHM give spectral scaling diagrams
indicating an inverse cascade of kinetic energy not present in RMHD.

</details>


### [38] [Runaway electrons during a coil quench in stellarators](https://arxiv.org/abs/2509.15721)
*Pavel Aleynikov,Per Helander,H√•kan M Smith*

Main category: physics.plasm-ph

TL;DR: Avalanches of runaway electrons can occur in stellarators without net toroidal current if magnetic-field coil currents vary rapidly, potentially causing wall-damaging runaway currents in reactor-scale devices.


<details>
  <summary>Details</summary>
Motivation: To investigate the potential for runaway electron generation in stellarators, particularly during rapid coil current variations like superconductor quenches, and assess risks for present and future devices.

Method: Theoretical analysis of runaway electron avalanche conditions in stellarators, examining scenarios with rapid coil current variations and low plasma densities.

Result: Runaway generation is possible in present devices like W7-X only at very low densities between discharges, but reactor-scale stellarators face more dangerous runaway generation during both inter-discharge periods and low-density operation.

Conclusion: While runaway electron avalanches pose a risk in stellarators, there is significantly more time to mitigate such events compared to tokamak disruptions, though accidental coil ramp-downs could convert substantial magnetic energy into damaging runaway currents.

Abstract: It is shown that avalanches of runaway electrons can arise in stellarators,
even if there is no net toroidal current in the plasma or the magnetic-field
coils, if the current in the latter varies rapidly enough, e.g. due to a
superconductor quench. In present-day devices such as W7-X, significant runaway
generation is theoretically possible only at very low gas or plasma density in
the vacuum vessel, e.g. between discharges, if a seed population of free
electrons is present. In reactor-scale stellarators, more dangerous runaway
generation may occur both between discharges and during low-density plasma
operation. Since a radiation-induced seed population is necessarily present in
an activated device, an accidental coil ramp-down could convert substantial
magnetic energy into wall-damaging runaway currents. There is however much more
time to mitigate such events than in tokamaks disruptions.

</details>


### [39] [The Role of Phase and Spatial Modes in Wave-Induced Plasma Transport](https://arxiv.org/abs/2509.15729)
*L. F. B. Souza,Y. Elskens,R. Egydio de Carvalho,I. L. Caldas*

Main category: physics.plasm-ph

TL;DR: A two-dimensional symplectic map models particle motion at plasma edge using electrostatic potential harmonics, showing phase and mode selection control transport via interference effects.


<details>
  <summary>Details</summary>
Motivation: To understand how perturbation amplitudes, relative phase, and spatial-mode choices affect particle transport in plasma edge confinement.

Method: Derived a symplectic map from superposition of integer spatial harmonics, reduced to two-wave model, analyzed using particle transmissivity and box-counting dimensions.

Result: Anti-phase identical modes cause destructive interference and strong confinement; in-phase modes drive chaotic transport. Distinct modes create complex fractal-like boundaries with higher-order resonances.

Conclusion: Phase and spectral content of waves jointly determine whether interference suppresses or promotes particle transport in plasma confinement.

Abstract: We derive a two-dimensional symplectic map for particle motion at the plasma
edge by modeling the electrostatic potential as a superposition of integer
spatial harmonics with relative phase shift, then reduce it to a two-wave model
to study the transport dependence on the perturbation amplitudes, relative
phase, and spatial-mode choice. Using particle transmissivity as a confinement
criterion, identical-mode pairs exhibit phase-controlled behavior: anti-phase
waves produce destructive interference and strong confinement while in-phase
waves add constructively and drive chaotic transport. Mode-mismatched pairs
produce richer phase-space structure with higher-order resonances and sticky
regions; the transmissivity boundaries become geometrically complex.
Box-counting dimensions quantify this: integer dimension smooth boundaries for
identical modes versus non-integer fractal-like dimension for distinct modes,
demonstrating that phase and spectral content of waves jointly determine
whether interference suppresses or promotes transport.

</details>


### [40] [Modeling of runaway electron induced damage on boron-nitride tiles in WEST](https://arxiv.org/abs/2509.15821)
*Tommaso Rizzi,Svetlana Ratynskaia,Panagiotis Tolias,Yann Corre,Mathilde Diez,Mehdi Firdaouss,Jonathan Gerardin,Raphael Mitteau,Cedric Reux,Artem Kulachenko,the WEST team,the EUROfusion Tokamak Exploitation Team*

Main category: physics.plasm-ph

TL;DR: Modeling of runaway electron-induced damage on boron nitride tiles in WEST tokamak using validated workflow for brittle failure prediction.


<details>
  <summary>Details</summary>
Motivation: To understand and predict damage caused by runaway electrons on boron nitride tiles in tokamak fusion devices, particularly for WEST disruptions.

Method: Monte Carlo simulations of RE transport combined with finite-element simulations of thermoelastic response, using Rankine criterion for brittle failure prediction.

Result: The model successfully captures thermal stress-driven failure and explosion physics, showing high sensitivity to RE impact parameters and predicting failure consistent with experimental observations.

Conclusion: The developed workflow provides realistic predictions of material failure under runaway electron loading conditions expected in WEST disruptions.

Abstract: The runaway electron (RE) - induced damage on boron nitride (BN) tiles
mounted on the inner bumpers of the WEST tokamak is modeled employing available
empirical input and experimental constraints, concerning the post-mortem
documentation of the damaged material topology and infra-red camera
observations of the long-time decay of the surface temperature. A newly
developed work-flow for the modeling of brittle failure due to RE impacts,
recently validated against a controlled DIII-D experiment, is employed. Monte
Carlo simulations of RE transport into BN provide volumetric heat source maps
for finite-element simulations of the linear thermoelastic material response,
while the brittle failure onset is predicted on the basis of the Rankine
criterion. The physics of thermal stress driven failure and explosion are well
captured by this model, which exhibits high sensitivity to RE impact
parameters. Despite the accidental nature of the damage events, the workflow
predicts failure in accordance with observations for realistic loading
specifications expected in WEST disruptions.

</details>


### [41] [Electron-positron pair generation using a single kJ-class laser pulse in a foam-reflector setup](https://arxiv.org/abs/2509.15853)
*Oliver Mathiak,Lars Reichwein,Alexander Pukhov*

Main category: physics.plasm-ph

TL;DR: Study of electron-positron pair production from laser-matter interaction in pre-ionised foam targets using particle-in-cell simulations


<details>
  <summary>Details</summary>
Motivation: To investigate how plasma-channel shape affects laser propagation and subsequent pair production in laser-matter interactions

Method: Using particle-in-cell simulations to model high-intensity laser pulses driving electrons via direct laser acceleration to a cone-shaped reflector, where high-energy electrons interact with reflected laser pulses to generate pairs

Result: The number of Compton emission and Breit-Wheeler pair creation events is highly sensitive to laser diffraction due to interaction with the foam

Conclusion: Plasma-channel shape significantly influences laser propagation and pair production efficiency in laser-foam interactions

Abstract: We investigate the process of creating electron-positron pairs from
laser-matter interaction in pre-ionised foam targets using particle-in-cell
simulations. A high-intensity laser pulse drives electrons via direct laser
acceleration up to a cone-shaped reflector. The high-energy electrons interact
with the reflected laser pulse, generating abundant pairs. The effects of the
plasma-channel shape on the propagation of the laser pulse and subsequent pair
production is studied. The results show that the number of Compton emission and
Breit-Wheeler pair creation events is highly sensitive to the diffraction of
the laser due to its interaction with the foam.

</details>


### [42] [On the enhanced Balmer emission of hydrogen in helium Capacitively Coupled Radio Frequency (CCRF) plasma](https://arxiv.org/abs/2509.16130)
*Varsha S,Prabhakar Srivastav,Yukti Goel,Milaan Patel,Hem Chandra Joshi,Jinto Thomas*

Main category: physics.plasm-ph

TL;DR: Study investigates enhanced hydrogen Balmer series emission in He CCRF plasma, attributing it to energy transfer from He metastables to water vapor via Penning ionization.


<details>
  <summary>Details</summary>
Motivation: To understand the mechanism behind observed hydrogen Balmer series emission in helium plasma and explain why this phenomenon occurs specifically with helium but not with air or argon.

Method: Used optical emission spectroscopy (OES) to observe emissions in He CCRF plasma, compared with air and Ar discharges, and employed Collisional Radiative (CR) model to analyze metastable population density.

Result: Found that He metastables transfer energy to water vapor, causing Penning ionization and subsequent Balmer emission. Emission intensity depends on electron density, and intensity ratios differ from conventional discharges.

Conclusion: Energy exchange between He metastables and water molecules via Penning ionization is the underlying mechanism, with increased metastable densities and radiative recombination cross-section responsible for emission enhancement.

Abstract: The present study investigates the observation and enhancement in the
intensity of the hydrogen Balmer series emission in a He CCRF plasma using
optical emission spectroscopy (OES). In addition to the characteristic line
emission of He atoms, the Balmer series of hydrogen and the molecular emission
of N2 are also observed in the He discharge. These emissions were primarily
attributed to the presence of water vapor in the chamber. In order to confirm
the role of He, the study is also performed using air and Ar where no such
Balmer series emissions is seen. Experimental evidence suggests that He
metastables transfer energy to trace amount of water content present in the
vacuum chamber. The results point towards the hypothesis that energy exchange
between metastable He and water molecules could be the underlying mechanism.
Since the energy of He metastables exceeds the ionization energy of H or H2O
molecule, Penning ionization is expected to occur upon their interaction. The H
ions formed as a result, consequently recombine with electrons in the plasma,
emitting the Balmer series. Furthermore, the emission intensity of the Balmer
series of H depends on the electron density of the He plasma. Experiments also
show significant deviations in the intensity ratios of the Balmer series from
conventional discharges, indicating difference in the underlying population
mechanism. A Collisional Radiative (CR) model for measured plasma parameters
was used to estimate the metastable population density to understand the
mechanism behind the enhancement of the emission intensity. The increase in the
metastable densities as well as the radiative recombination cross section
appear to be responsible for the observed enhancement. We believe these results
will be significant in terms of applications, in addition to providing a
fundamental understanding of energy transfer between metastables of He and H2O.

</details>


### [43] [A novel agile THz Pulsed Spectropolarimeter measuring 2D distributions of the magnetic field, density and ion temperature of fusion reactor equilibria](https://arxiv.org/abs/2509.16159)
*Roger J Smith*

Main category: physics.plasm-ph

TL;DR: THz Pulsed Spectropolarimetry enables non-perturbative measurements of magnetic fields, density, and temperature in magnetic fusion devices using pulsed THz sources and heterodyne receivers.


<details>
  <summary>Details</summary>
Motivation: To develop advanced diagnostics for magnetic fusion devices that can provide real-time feedback on magnetic fields, density, and temperature distributions without perturbing the plasma, which is crucial for optimizing fusion reactor performance.

Method: Uses intense pulsed THz sources via optical rectification with crystalline lithium niobate. Combines Pulsed Polarimetry with spectroscopy using heterodyne receivers to measure sightline magnetic fields, density, electron temperature, and ion velocity distributions. Enables spatial resolution through electronic filtering and steering capabilities.

Result: The technique can resolve FRC midplane density and axial field profiles, determine azimuthal current density via Ampere's law, measure sightline distributed electron temperature, and obtain confined fast ion velocity distributions. Provides diagnostic agility with temporal and spatial real-time feedback across the poloidal plane.

Conclusion: THz Pulsed Spectropolarimetry represents an advanced diagnostic technique that fulfills the mission of future fusion diagnostics by providing comprehensive measurements to maintain and optimize fusion reactor performance, distinguishing itself through alignment-free steering capabilities compared to crossbeam diagnostics.

Abstract: With the recent inception of intense pulsed THz sources via optical
rectification using crystalline lithium niobate, non-perturbative measurements
of the internal local magnetic field are now possible using Pulsed Polarimetry
on any magnetic fusion device. Pulsed Polarimetry resolves the sightline
magnetic field and density, analogous to RADAR measurements of the spatial
distribution of precipitation. With a suitable sightline, the FRC midplane
density and axial field profiles can be determined with the azimuthal current
density given by Ampere's law. Pulsed Polarimetry can be enhanced to measure
the sightline distributed electron temperature by adding spectroscopy. A THz
Pulsed Spectropolarimeter using a heterodyne receiver is appropriate if
collective Thomson backscatter is induced. In this case, electronic filtering
provides spatial distributions of the ion spectral density function. The
sightline bulk ion temperature as well as confined fast ion velocity
distributions can be measured which is of particular importance to plasmas that
are heated and sustained by NBI. The ability to steer the sightline without
loss of alignment distinguishes this diagnostic from crossbeam diagnostics. The
term, diagnostic agility denotes a diagnostic's ability to provide both
temporal and spatial real-time feedback on demand across the poloidal plane
analogous to 2D imaging Doppler RADAR. THz Pulsed Spectropolarimetry is
extensively detailed along with simulated measurements on an FRC equilibrium.
Comparisons are made with long pulse CTS crossbeam diagnostics. The role this
technique can play within the wider MFE program towards future fusion reactors
as an advanced diagnostic is explored. The nature and breadth of the
measurements fulfills the mission of future diagnostics to maintain and
optimize fusion reactor performance.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [44] [Ring polymers in two-dimensional melts double-fold around randomly branching "primitive shapes"](https://arxiv.org/abs/2509.15757)
*Mattia A. Ubertini,Angelo Rosa*

Main category: cond-mat.soft

TL;DR: The paper introduces a numerical protocol to detect primitive shapes of ring polymers in 2D melts, showing they conform to branched polymer statistics and discussing implications for polymer dynamics.


<details>
  <summary>Details</summary>
Motivation: Drawing inspiration from the concept of primitive paths in linear chain melts, the authors aim to develop an unambiguous method to identify primitive shapes of ring polymers in two-dimensional melt conditions.

Method: The authors introduce a numerical protocol that allows unambiguous detection of primitive shapes of ring polymers in two-dimensional melts.

Result: Analysis of conformational properties shows that the primitive shapes conform to the statistics of two-dimensional branched polymers (trees) in melt conditions, consistent with theoretical work by Khokhlov, Nechaev and Rubinstein.

Conclusion: The branched nature of ring polymers in 2D melts is confirmed, and results for polymer dynamics are presented and discussed in light of this branched structure.

Abstract: Drawing inspiration from the concept of the "primitive path" of a linear
chain in melt conditions, we introduce here a numerical protocol which allows
us to detect, in an unambiguous manner, the "primitive shapes" of ring polymers
in two-dimensional melts. Then, by analysing the conformational properties of
these primitive shapes, we demonstrate that they conform to the statistics of
two-dimensional branched polymers (or, trees) in the same melt conditions, in
agreement with seminal theoretical work by Khokhlov, Nechaev and Rubinstein.
Results for polymer dynamics in light of the branched nature of the rings are
also presented and discussed.

</details>


<div id='math.SP'></div>

# math.SP [[Back]](#toc)

### [45] [Solvability Complexity Index Classification For Koopman Operator Spectra In $L^p$ For $1<p<\infty$](https://arxiv.org/abs/2509.16016)
*Christopher Sorg*

Main category: math.SP

TL;DR: This paper develops computational methods for approximating spectral sets of Koopman operators in L^p spaces using finite sections and residual tests, extending previous Hilbert space results to the more general Banach space setting.


<details>
  <summary>Details</summary>
Motivation: The motivation is to extend spectral computation techniques from the well-understood Hilbert space setting (L^2) to general L^p spaces, which presents significant challenges due to the loss of orthogonality and Hilbertian structure.

Method: The method builds on finite sections in computable unconditional Schauder bases of L^p spaces, designing residual tests that use finite evaluations of the underlying map to produce compact sets that converge to target spectral sets without spectral pollution.

Result: The results include a complete classification of computational complexity in terms of Solvability Complexity Index, and analysis of the sufficiency and existence of a Wold-von Neumann decomposition analog for L^p spaces.

Conclusion: The paper successfully overcomes the obstacles of non-orthogonality in L^p spaces by working with computable unconditional dictionaries adapted to dyadic/Lipschitz filtrations and proving stability of residual tests under non-orthogonal truncations.

Abstract: We study the computation of the approximate point spectrum and the
approximate point $\varepsilon$-pseudospectrum of bounded Koopman operators
acting on $L^p(\mathcal{X},\omega)$ for $1<p<\infty$ and a compact metric space
$(\mathcal{X}, d_{\mathcal{X}})$ with finite Borel measure $\omega$. Building
on finite sections in a computable unconditional Schauder basis of
$L^p(\mathcal{X},\omega)$, we design residual tests that use only finitely many
evaluations of the underlying map and produce compact sets on a planar grid,
that converge in the Hausdorff metric to the target spectral sets, without
spectral pollution. From these constructions we obtain a complete
classification, in the sense of the Solvability Complexity Index, of how many
limiting procedures are inherently necessary. Also we analyze the sufficiency
and existence of a Wold-von Neumann decomposition analog, that was used in the
special $L^2$-case.
  The main difficulty in extending from the already analyzed Hilbert setting
$(p=2)$ to general $L^p$ is the loss of orthogonality and Hilbertian structure:
there is no orthonormal basis with orthogonal coordinate projections in
general, the canonical truncations $E_n$ in a computable Schauder dictionary
need not be contractive (and may oscillate) and the Wold-von Neumann reduction
has no directly computable analog in $L^p$. We overcome these obstacles by
working with computable unconditional dictionaries adapted to dyadic/Lipschitz
filtrations and proving stability of residual tests under non-orthogonal
truncations.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [46] [Photon Blockade Mediated by Two-Photon Absorption in an Optical Parametric Amplifier](https://arxiv.org/abs/2509.15696)
*Weiyi An,Jie Zhu*

Main category: quant-ph

TL;DR: Combining unconventional photon blockade (UPB) and environmentally induced photon blockade (EPB) in optical parametric amplifiers with two-photon absorption (TPA) achieves more stable photon blockade with stronger multi-photon state suppression.


<details>
  <summary>Details</summary>
Motivation: To develop a more stable and effective photon blockade method by combining the advantages of unconventional photon blockade (which uses quantum interference) and environmentally induced photon blockade (which exploits two-photon absorption).

Method: Integrating UPB and EPB approaches by considering the two-photon absorption effect in optical parametric amplifiers, creating a hybrid system that leverages both quantum interference and environmental absorption mechanisms.

Result: The combined approach achieves a more stable photon blockade with enhanced suppression of multi-photon states compared to individual UPB or EPB methods.

Conclusion: The hybrid UPB-EPB system provides superior photon blockade performance, offering stronger and more stable single-photon generation capabilities for quantum information applications.

Abstract: Photon blockade (PB) is a quantum effect in strongly nonlinear systems where
a single photon prevents the system from being excited to a higher level,
generating anti-bunched light fields. It enables the generation of
single-photon sources for quantum information processing. Conventional photon
blockade (CPB) leverages strong nonlinear interactions to generate an
anharmonic energy spectrum. Unconventional photon blockade (UPB) utilizes
destructive quantum interference between excitation pathways. Recently,
environmentally induced photon blockade (EPB) has emerged as a novel approach,
exploiting two-photon absorption (TPA) to realize photon blockade. In this
work, we combine UPB and EPB together, considering the TPA effect in the
optical parametric amplifier (OPA), thereby achieving a more stable PB with
stronger suppression of multi-photon states.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [47] [Fast OTSU Thresholding Using Bisection Method](https://arxiv.org/abs/2509.16179)
*Sai Varun Kodathala*

Main category: cs.CV

TL;DR: This paper presents an optimized Otsu thresholding algorithm that uses the bisection method to reduce computational complexity from O(L) to O(log L) while maintaining segmentation accuracy.


<details>
  <summary>Details</summary>
Motivation: The conventional Otsu thresholding algorithm suffers from computational inefficiency due to exhaustive search requirements across all possible threshold values, creating bottlenecks in large-scale image processing systems.

Method: The proposed approach leverages the bisection method to exploit the unimodal characteristics of the between-class variance function, reducing the number of required evaluations.

Result: Experimental results show 91.63% reduction in variance computations and 97.21% reduction in algorithmic iterations, with exact threshold matches in 66.67% of cases and 95.83% within 5 gray levels deviation.

Conclusion: The optimized algorithm provides deterministic performance guarantees suitable for real-time applications while preserving the theoretical foundations and segmentation quality of the original Otsu method.

Abstract: The Otsu thresholding algorithm represents a fundamental technique in image
segmentation, yet its computational efficiency is severely limited by
exhaustive search requirements across all possible threshold values. This work
presents an optimized implementation that leverages the bisection method to
exploit the unimodal characteristics of the between-class variance function.
Our approach reduces the computational complexity from O(L) to O(log L)
evaluations while preserving segmentation accuracy. Experimental validation on
48 standard test images demonstrates a 91.63% reduction in variance
computations and 97.21% reduction in algorithmic iterations compared to
conventional exhaustive search. The bisection method achieves exact threshold
matches in 66.67% of test cases, with 95.83% exhibiting deviations within 5
gray levels. The algorithm maintains universal convergence within theoretical
logarithmic bounds while providing deterministic performance guarantees
suitable for real-time applications. This optimization addresses critical
computational bottlenecks in large-scale image processing systems without
compromising the theoretical foundations or segmentation quality of the
original Otsu method.

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [48] [Anisotropic Cosmic Ray Transport resulting from Magnetic Mirroring and Resonant Curvature Scattering](https://arxiv.org/abs/2509.15320)
*Jeremiah L√ºbke,Frederic Effenberger,Mike Wilbert,Horst Fichtner,Rainer Grauer*

Main category: astro-ph.HE

TL;DR: Study of cosmic ray transport through turbulent astrophysical plasmas, examining combined effects of magnetic mirroring and resonant curvature scattering on parallel and perpendicular transport using test particle simulations.


<details>
  <summary>Details</summary>
Motivation: Transport of cosmic rays through turbulent astrophysical plasmas remains an open problem, requiring investigation of how magnetic field geometry affects particle transport mechanisms.

Method: Conducted test particle simulations in snapshots of anisotropic magnetohydrodynamics simulation, recording magnetic moment variation and field line curvature around pitch angle reversals.

Result: Found that pitch angle reversals occur via magnetic mirroring in coherent field regions or resonant curvature scattering in chaotic regions; parallel transport follows L√©vy walk with truncated power-law distribution; perpendicular transport enhanced by resonant curvature scattering but diminished by magnetic mirroring.

Conclusion: Magnetic field line geometry plays crucial role in cosmic ray transport, with results consistent with energy-independent diffusion coefficients, while energy-dependent observations may arise from intermittently inhomogeneous interstellar medium.

Abstract: The transport of cosmic rays through turbulent astrophysical plasmas still
constitutes an open problem. Building on recent progress, we study the combined
effect of magnetic mirroring and resonant curvature scattering on parallel and
perpendicular transport. We conduct test particle simulations in snapshots of
an anisotropic magnetohydrodynamics simulation, and record magnetic moment
variation and field line curvature around pitch angle reversals. We find for
strongly magnetized particles that (i) pitch angle reversals may occur either
in coherent regions of the field with small variation of the magnetic moment
via magnetic mirroring, or in chaotic regions of the field with strong
variation of the magnetic moment via resonant curvature scattering; (ii)
parallel transport can be modeled as a L\'evy walk with a truncated power-law
distribution based on pitch angle reversal times; and (iii) perpendicular
transport is enhanced by resonant curvature scattering in synergy with chaotic
field line separation, and diminished by magnetic mirroring due to confinement
in locally ordered field line bundles. While magnetic mirroring constitutes the
bulk of reversal events, resonant curvature scattering additionally acts on
trajectories which fall in the loss cones of typical mirroring structures and
thus provides the cut-off for the reversal time distribution. Our results,
which highlight the role of the magnetic field line geometry in cosmic ray
transport processes, are consistent with energy-independent diffusion
coefficients. We conclude by considering how energy-dependent observations
could arise from an intermittently inhomogeneous interstellar medium.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [49] [kh2d-solver: A Python Library for Idealized Two-Dimensional Incompressible Kelvin-Helmholtz Instability](https://arxiv.org/abs/2509.16080)
*Sandy H. S. Herho,Nurjanna J. Trilaksono,Faiz R. Fajary,Gandhi Napitupulu,Iwan P. Anwar,Faruq Khadami,Dasapta E. Irawan*

Main category: physics.flu-dyn

TL;DR: An open-source Python library for simulating 2D incompressible Kelvin-Helmholtz instabilities in stratified shear flows, using spectral methods with efficient computation.


<details>
  <summary>Details</summary>
Motivation: To study mixing efficiency in stratified shear flows and challenge Richardson number-based parameterizations for climate model improvements.

Method: Fractional-step projection method with spectral Poisson solution via Fast Sine Transform, implemented using NumPy, SciPy, and Numba JIT compilation for second-order spatial accuracy.

Result: Double shear layers achieve 2.8√ó higher mixing rates than forced turbulence despite lower Reynolds numbers, with simulations running efficiently on standard hardware (31 minutes for 384√ó192 grids).

Conclusion: Mixing efficiency depends on instability generation pathways rather than intensity measures alone, suggesting refinements for subgrid-scale representation in climate models.

Abstract: We present an open-source Python library for simulating two-dimensional
incompressible Kelvin-Helmholtz instabilities in stratified shear flows. The
solver employs a fractional-step projection method with spectral Poisson
solution via Fast Sine Transform, achieving second-order spatial accuracy.
Implementation leverages NumPy, SciPy, and Numba JIT compilation for efficient
computation. Four canonical test cases explore Reynolds numbers 1000--5000 and
Richardson numbers 0.1--0.3: classical shear layer, double shear configuration,
rotating flow, and forced turbulence. Statistical analysis using Shannon
entropy and complexity indices reveals that double shear layers achieve
2.8$\times$ higher mixing rates than forced turbulence despite lower Reynolds
numbers. The solver runs efficiently on standard desktop hardware, with
384$\times$192 grid simulations completing in approximately 31 minutes. Results
demonstrate that mixing efficiency depends on instability generation pathways
rather than intensity measures alone, challenging Richardson number-based
parameterizations and suggesting refinements for subgrid-scale representation
in climate models.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [50] [Unsupervised and probabilistic learning with Contrastive Local Learning Networks: The Restricted Kirchhoff Machine](https://arxiv.org/abs/2509.15842)
*Marcelo Guzman,Simone Ciarella,Andrea J. Liu*

Main category: cond-mat.dis-nn

TL;DR: The paper introduces a self-learning resistor network called Restricted Kirchhoff Machine that can solve unsupervised learning tasks similar to Restricted Boltzmann Machines, using physical dynamics for energy-efficient computation.


<details>
  <summary>Details</summary>
Motivation: To develop autonomous physical learning systems that can modify internal parameters without external computation, offering distributed and energy-efficient learning through physical dynamics compared to traditional computers.

Method: The circuit uses Contrastive Local Learning Networks technology with two identical networks comparing different physical states to implement a contrastive local learning rule. Training is simulated on the binarized MNIST dataset.

Result: The paper provides proof of concept for the learning capabilities of the Restricted Kirchhoff Machine and compares its scaling behavior (time, power, energy per operation) against Restricted Boltzmann Machines on CPU and GPU platforms.

Conclusion: The Restricted Kirchhoff Machine demonstrates potential as an energy-efficient physical computing system for unsupervised learning tasks, with favorable scaling properties compared to traditional computational platforms.

Abstract: Autonomous physical learning systems modify their internal parameters and
solve computational tasks without relying on external computation. Compared to
traditional computers, they enjoy distributed and energy-efficient learning due
to their physical dynamics. In this paper, we introduce a self-learning
resistor network, the Restricted Kirchhoff Machine, capable of solving
unsupervised learning tasks akin to the Restricted Boltzmann Machine algorithm.
The circuit relies on existing technology based on Contrastive Local Learning
Networks, in which two identical networks compare different physical states to
implement a contrastive local learning rule. We simulate the training of the
machine on the binarized MNIST dataset, providing a proof of concept of its
learning capabilities. Finally, we compare the scaling behavior of the time,
power, and energy consumed per operation as more nodes are included in the
machine to their Restricted Boltzmann Machine counterpart operated on CPU and
GPU platforms.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [51] [Fast subdivision of B√©zier curves](https://arxiv.org/abs/2509.15691)
*Pawe≈Ç Wo≈∫ny,Filip Chudy*

Main category: cs.GR

TL;DR: A new algorithm using FFT achieves O(dn log n) B√©zier curve subdivision, faster than the traditional O(dn¬≤) de Casteljau method, with good numerical stability and efficient update capabilities.


<details>
  <summary>Details</summary>
Motivation: The de Casteljau algorithm for B√©zier curve subdivision has O(dn¬≤) time complexity, which becomes inefficient for high-degree curves. This paper aims to develop a more efficient subdivision method.

Method: Uses fast Fourier transform (FFT) and inverse FFT to subdivide B√©zier curves in O(dn log n) time. A modified version addresses numerical instability issues while maintaining the same computational complexity.

Result: The new FFT-based method successfully reduces subdivision time complexity from O(dn¬≤) to O(dn log n). The modified algorithm provides good numerical stability, and allows efficient O(d) time updates when adding control points.

Conclusion: FFT-based subdivision offers significant speed improvements for B√©zier curves, with applications extending to rational B√©zier curves, B√©zier surfaces, and derivative computations.

Abstract: It is well-known that a $d$-dimensional polynomial B\'{e}zier curve of degree
$n$ can be subdivided into two segments using the famous de Casteljau algorithm
in $O(dn^2)$ time. Can this problem be solved more efficiently? In this paper,
we show that it is possible to do this in $O(dn\log{n})$ time using the fast
Fourier transform and its inverse. Experiments show that the direct application
of the new method performs well only for small values of $n$, as the algorithm
is numerically unstable. However, a slightly modified version -- which still
has $O(dn\log{n})$ computational complexity -- offers good numerical quality,
which is confirmed by numerical experiments conducted in \textsf{Python}.
Moreover, the new method has a nice property: if a B\'{e}zier curve is extended
by an additional control point, the subdivision can be updated in $O(d)$ time.
  A similar idea can be applied to speed up the subdivision of rational
B\'{e}zier curves and rectangular B\'{e}zier surfaces, as well as to compute
the derivatives of B\'{e}zier curves more efficiently.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [52] [Optimal Experimental Design of a Moving Sensor for Linear Bayesian Inverse Problems](https://arxiv.org/abs/2509.15961)
*Nicole Aretz,Thomas Lynn,Karen Willcox,Sven Leyffer*

Main category: cs.CE

TL;DR: Optimizing mobile sensor path to minimize uncertainty in Bayesian inverse problems for PDE parameter estimation.


<details>
  <summary>Details</summary>
Motivation: To improve parameter estimation accuracy in physical systems modeled by PDEs by strategically planning sensor movement to collect the most informative measurements.

Method: Derive closed-form posterior covariance for linear PDEs, formulate optimal experimental design problem with path constraints, discretize with temporal refinement consistency, and solve using interior-point method.

Result: Computational results demonstrate the approach for a convection-diffusion equation with unknown initial condition.

Conclusion: The method effectively reduces posterior uncertainty in Bayesian inverse problems through optimal sensor path planning while maintaining physical interpretability and obstacle avoidance.

Abstract: We optimize the path of a mobile sensor to minimize the posterior uncertainty
of a Bayesian inverse problem. Along its path, the sensor continuously takes
measurements of the state, which is a physical quantity modeled as the solution
of a partial differential equation (PDE) with uncertain parameters. Considering
linear PDEs specifically, we derive the closed-form expression of the posterior
covariance matrix of the model parameters as a function of the path, and
formulate the optimal experimental design problem for minimizing the
posterior's uncertainty. We discretize the problem such that the cost function
remains consistent under temporal refinement. Additional constraints ensure
that the path avoids obstacles and remains physically interpretable through a
control parameterization. The constrained optimization problem is solved using
an interior-point method. We present computational results for a
convection-diffusion equation with unknown initial condition.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [53] [First-principles calculation of higher-order elastic constants from divided differences](https://arxiv.org/abs/2509.15468)
*Ruvini Attanayake,Umesh C. Roy,Abhiyan Pandit,Angelo Bongiorno*

Main category: cond-mat.mtrl-sci

TL;DR: A first-principles method for calculating higher-order elastic constants using finite strain deformations, density functional theory, and recursive numerical differentiation, applicable to materials of any symmetry.


<details>
  <summary>Details</summary>
Motivation: To develop a general computational framework for calculating elastic constants of any order for various materials, addressing the need for accurate higher-order elastic property predictions.

Method: Combines finite strain deformations, density functional theory for stress calculations, and recursive numerical differentiation similar to divided differences polynomial interpolation.

Result: Successfully calculated elastic constants up to 6th order for cubic materials (silicon, gold), 5th order for trigonal Œ±-quartz, and 2nd-3rd order for anisotropic kevlar, with validation against DFT stress responses.

Conclusion: The method provides a robust, general-purpose approach for computing higher-order elastic constants across diverse material symmetries, validated through continuum approximation comparisons.

Abstract: A method is presented to calculate from first principles the higher-order
elastic constants of a solid material. The method relies on finite strain
deformations, a density functional theory approach to calculate the Cauchy
stress tensor, and a recursive numerical differentiation technique homologous
to the divided differences polynomial interpolation algorithm. The method is
applicable as is to any material, regardless its symmetry, to calculate elastic
constants of, in principle, any order. Here, we introduce conceptual framework
and technical details of our method, we discuss sources of errors, we assess
convergence trends, and we present selected applications. In particular, our
method is used to calculate elastic constants up to the 6$^{th}$ order of two
crystalline materials with the cubic symmetry, silicon and gold. To demonstrate
general applicability, our method is also used to calculate the elastic
constants up to the 5$^{th}$ order of $\alpha$-quartz, a crystalline material
belonging to the trigonal crystal system, and the second- and third-order
elastic constants of kevlar, a material with an anisotropic bonding network.
Higher order elastic constants computed with our method are validated against
density functional theory calculations by comparing stress responses to large
deformations derived within the continuum approximation.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [54] [Estimating systematic errors in Bayesian inversion using transport maps](https://arxiv.org/abs/2509.16116)
*Maren Casfor,Philipp Trunschke,Sebastian Heidenreich,Nando Hegemann*

Main category: stat.ME

TL;DR: A unified framework combining transport maps and model error approach to correct bias in Bayesian inversion for indirect measurements


<details>
  <summary>Details</summary>
Motivation: Indirect measurements require solving inverse problems using approximate models, which introduce systematic errors and bias the posterior distribution in Bayesian inversion

Method: Combines transport maps from a reference distribution to the posterior distribution with the model error approach, creating an adaptive algorithm that jointly estimates the posterior distribution and model error

Result: Demonstrated efficiency and accuracy on two model problems, showing effective bias correction while enabling fast sampling

Conclusion: The proposed framework successfully corrects biases in posterior distributions arising from model approximations in indirect measurements

Abstract: In indirect measurements, the measurand is determined by solving an inverse
problem which requires a model of the measurement process. Such models are
often approximations and introduce systematic errors leading to a bias of the
posterior distribution in Bayesian inversion. We propose a unified framework
that combines transport maps from a reference distribution to the posterior
distribution with the model error approach. This leads to an adaptive algorithm
that jointly estimates the posterior distribution of the measurand and the
model error. The efficiency and accuracy of the method are demonstrated on two
model problems, showing that the approach effectively corrects biases while
enabling fast sampling.

</details>
