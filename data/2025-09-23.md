<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 23]
- [math.AP](#math.AP) [Total: 40]
- [physics.comp-ph](#physics.comp-ph) [Total: 2]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 9]
- [physics.class-ph](#physics.class-ph) [Total: 1]
- [astro-ph.HE](#astro-ph.HE) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.RO](#cs.RO) [Total: 1]
- [math.DG](#math.DG) [Total: 1]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [physics.app-ph](#physics.app-ph) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 4]
- [astro-ph.SR](#astro-ph.SR) [Total: 4]
- [math.ST](#math.ST) [Total: 1]
- [math.DS](#math.DS) [Total: 1]
- [physics.space-ph](#physics.space-ph) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Nonlocal modeling of spatial fractional diffusion with truncated interaction domains and truncated kernel function singularity](https://arxiv.org/abs/2509.16315)
*Shiping Zhou,Yanzhi Zhang,Max Gunzburger*

Main category: math.NA

TL;DR: This paper addresses challenges in modeling diffusion processes using fractional Laplacian operators by proposing strategies to handle infinite domains, unbounded integration, and singular kernels through domain restriction, bounded integration regions, and singularity truncation.


<details>
  <summary>Details</summary>
Motivation: Parabolic PDEs are widely used for diffusion modeling but often fail to match observations in applications like hydrology, biology, and light diffusion. Fractional diffusion models with fractional Laplacian operators provide better accuracy but face computational challenges due to infinite domains, unbounded integration, and singular kernels.

Method: The authors propose three strategies: (a) restrict solutions to a bounded domain Ω, (b) use bounded integration domains like Euclidean balls B_δ(x) with finite radius δ, and (c) truncate the singularity of the kernel φ(y-x) by setting φ(y-x) = φ(ε) for |y-x| ≤ ε. They explore combinations of these strategies and analyze limiting behaviors.

Result: The paper provides extensive illustrations of the combinations of strategies (a), (b), and (c). It demonstrates that as δ → 0, the model recovers the standard PDE, and for other parameter limits, it recovers the fractional Laplacian model, validating the approach.

Conclusion: The proposed strategies effectively address the computational challenges of fractional Laplacian operators, enabling practical implementation of fractional diffusion models while maintaining theoretical consistency with both standard and fractional PDE limits.

Abstract: Parabolic partial differential equations (PDEs) are in ubiquitous, very
effective use to model diffusion processes. However, there are many
applications (e.g., such as in hydrology, animal foraging, biology, and light
diffusion just do name a few) for which results obtained through the use of
parabolic PDEs do not agree with observations. In many situations the use of
fractional diffusion models has been found to be more faithful to that which is
observed. Specifically, we replace the Laplacian operator in the PDE by a
fractional Laplacian operator ${\mathcal L}$ which is an integral operator for
which solutions are sought for on all of space, has an unbounded domain of
integration, and for a given point $x$ the integrand contains a kernel function
$\phi(y-x)$ that is infinite whenever $y=x$.
  These three features pose impediments not only for the construction of
efficient discretization methods but also because all three involve one or more
sort of "infinity''. To overcome these impediments we choose to invoke one or
more of the following strategies. (a) We seek solutions only within a chosen
bounded domain $\Omega$. (b) For every $x\in\Omega$, we choose a bounded domain
of integration such as, e.g., an Euclidean ball $B_\delta(x)$ having finite
radius $\delta$. (c) We truncate the singularity of $\phi(y-x)$ by setting, for
a given constant $\varepsilon>0$, $\phi(y-x)= \phi(\varepsilon)$ whenever
$|y-x|\le\varepsilon$.
  We then provide extensive illustrations of the possible combinations chosen
from among (a), (b), and (c). We also illustrate, for the various models
defined for each of these combinations, their limiting behavior of solutions
such as showing that as $\delta\to0$ we recover the PDE model and also showing
that in the limit of some other parameters we recover the fractional Laplacian
model.

</details>


### [2] [Finite element approximation of the stationary Navier-Stokes problem with non-smooth data](https://arxiv.org/abs/2509.16461)
*María Gabriela Armentano,Mauricio Mendiluce*

Main category: math.NA

TL;DR: Analysis of finite element approximation for stationary Navier-Stokes equations with non-smooth Dirichlet boundary data using regularization approach


<details>
  <summary>Details</summary>
Motivation: To develop accurate numerical methods for Navier-Stokes equations with non-smooth boundary data, which is common in practical applications but challenging for standard finite element approaches

Method: Regularize the boundary solution and use a priori error estimates between the approximation of Navier-Stokes with non-smooth data and finite element solution of the regularized problem

Result: Obtained convergence with optimal order for the finite element approximation

Conclusion: The regularization approach provides an effective method for handling non-smooth boundary data in Navier-Stokes equations with optimal convergence rates

Abstract: The aim of this work is to analyze the finite element approximation of the
stationary Navier-Stokes equations with non-smooth Dirichlet boundary data. The
discrete approximation is obtained by considering the Navier-Stokes system with
a regularized boundary solution. Based on the existence of the very weak
solution for the Navier-Stokes system with L2 boundary data, and a suitable
decomposition of this solution, we obtain a priori error estimates between the
approximation of the Navier-Stokes system with non-smooth data and the finite
element solution of the associated regularized problem. These estimates allow
us to conclude that our approach converges with optimal order.

</details>


### [3] [Computational Modeling of Selective Capture Mechanisms in Conduction System Pacing](https://arxiv.org/abs/2509.16631)
*Mohammadreza Kariman,Matthias A. F. Gsell,Edward J. Vigmond,Aurel Neic,Christoph M. Augustin,Gernot Plank*

Main category: math.NA

TL;DR: This paper develops a detailed computer model of the interventricular septum and left bundle branch to understand the physical mechanisms behind selective left bundle branch area pacing (LBBAP), analyzing how lead position, orientation, and polarity affect optimal pacing thresholds.


<details>
  <summary>Details</summary>
Motivation: Conduction system pacing (CSP) is clinically significant for restoring physiological ventricular activation, but the physical mechanisms enabling selective LBB activation remain poorly understood despite LBBAP being a simpler alternative to His bundle pacing.

Method: The researchers created a structurally and biophysically detailed computer model of the IVS and LBB, incorporating a geometrically detailed model of a clinically used CSP lead to quantitatively analyze lead position, orientation, and polarity effects on s-LBBP thresholds.

Result: Deep implantation within the LV sub-endocardium with direct electrode-LBB contact is crucial for effective s-LBBP. Low strength pacing can achieve s-LBBP, but capturing the entire LBB requires higher strengths leading to non-selective pacing. Anodal tip polarity requires higher activation strengths, and lead orientation affects capture thresholds and bundle synchronization.

Conclusion: The model explains clinically observed impedance trends and complications, with quantitative consistency supporting model credibility. Simulations may guide improved CSP lead designs for selective and synchronous LBB activation.

Abstract: CSP is gaining clinical significance owing to its ability to restore a
physiological activation sequence in the ventricles. While His bundle pacing
(HBP) producing the most physiological activation is preferable, due to implant
complications the selective activation of the LBB by left bundle branch area
pacing (LBBAP) is considered an alternative, offering both a simpler implant
and a physiological activation sequence. However, the physical mechanisms
facilitating selective activation of the LBB remain poorly understood. We
developed a structurally and biophysically detailed computer model of the IVS
and LBB to quantitatively elucidate the role of lead position, orientation and
polarity in achieving optimal s-LBBP thresholds, using a geometrically detailed
model of a clinically widely used CSP lead. A deep implant within the LV
sub-endocardium ensuring a direct contact between electrode and LBB is key for
effective s-LBBP. For low strength s-LBBP is feasible, but capturing the LBB in
its entirety could only be achieved using higher strengths that led to
non-selective left bundle branch pacing (ns-LBBP). Switching the tip polarity
to anodal was not beneficial, requiring higher strengths to activate the LBB.
Lead orientation relative to the LBB bundles was found to influence the s-LBBP
capture threshold and the number of synchronously activating bundles. The model
explains the impedance trends that are clinically observed when advancing the
tip through the IVS into the LBB region, as well as sudden impedance drops
associated with implant complications such as septal perforation or lead
dislodgement. Quantitative consistence with clinically observed trends support
model credibility, and indicate that simulation may offer an effective approach
for guiding the design of improved CSP leads, facilitating a selective and
synchronous activation of the entire LBB.

</details>


### [4] [Fast and accurate computation of classical Gaussian quadratures](https://arxiv.org/abs/2509.16716)
*A. Gil,J. Segura,N. M. Temme*

Main category: math.NA

TL;DR: New algorithms for computing classical Gaussian quadrature rules using globally convergent fourth-order iterative methods and asymptotic approximations, achieving superior speed, accuracy, and computational range compared to previous methods.


<details>
  <summary>Details</summary>
Motivation: To develop more efficient and accurate computational methods for classical Gaussian quadrature rules that overcome limitations of previous algorithms in terms of speed, accuracy, and parameter range restrictions.

Method: Combines globally convergent fourth-order iterative methods with asymptotic approximations applied in complementary regions of the parameter space. Also covers Gauss-Radau and Gauss-Lobatto variants, barycentric weights computation, and provides arbitrary accuracy algorithms for symmetric cases.

Result: The proposed methods surpass previous algorithms in speed, accuracy, and computational range (practically unrestricted). Arbitrary accuracy is achieved for symmetric quadrature cases.

Conclusion: The combination of iterative methods and asymptotic approximations provides a comprehensive solution for computing Gaussian quadrature rules with unprecedented performance across speed, accuracy, and parameter range.

Abstract: Algorithms for computing classical Gaussian quadrature rules (Gauss-Jacobi,
Gauss-Laguerre, and Gauss-Hermite) are presented, based on globally convergent
fourth-order iterative methods and asymptotic approximations, which are applied
in complementary regions of the parameter space. The combination of these
approaches results in methods that surpass previous algorithms in terms of
speed, accuracy, and computational range (practically unrestricted). The
Gauss-Radau and Gauss-Lobatto variants are also considered, along with the
computation of the associated barycentric weights. Arbitrary accuracy
algorithms are also provided for the symmetric cases (Gauss-Gegenbauer and
Gauss-Hermite).

</details>


### [5] [Spectral Analysis of the Weighted Frobenius Objective](https://arxiv.org/abs/2509.16783)
*Vladislav Trifonov,Ivan Oseledets,Ekaterina Muravleva*

Main category: math.NA

TL;DR: Analysis of weighted Frobenius loss for preconditioning shows it penalizes errors in small eigenvalues more strongly, confining error to largest eigenvalue direction and suppressing low-frequency components for conjugate gradient method.


<details>
  <summary>Details</summary>
Motivation: To understand why weighted Frobenius loss is effective for preconditioning iterative solvers, particularly how it naturally suppresses low-frequency components which is beneficial for conjugate gradient method.

Method: Theoretical analysis of weighted Frobenius loss showing each eigenmode is scaled by square of its eigenvalue, with numerical experiments including training sparse factors via direct gradient updates to IC(0) factor entries.

Result: Analysis reveals error is minimized when confined to direction of largest eigenvalue, providing rigorous explanation for low-frequency suppression. Numerical experiments confirm theoretical predictions.

Conclusion: Weighted Frobenius loss provides effective preconditioning strategy by naturally suppressing low-frequency components, with analysis applicable to various approximation schemes including incomplete factorizations and learning-based constructions.

Abstract: We analyze a weighted Frobenius loss for approximating symmetric positive
definite matrices in the context of preconditioning iterative solvers. Unlike
the standard Frobenius norm, the weighted loss penalizes error components
associated with small eigenvalues of the system matrix more strongly. Our
analysis reveals that each eigenmode is scaled by the corresponding square of
its eigenvalue, and that, under a fixed error budget, the loss is minimized
only when the error is confined to the direction of the largest eigenvalue.
This provides a rigorous explanation of why minimizing the weighted loss
naturally suppresses low-frequency components, which can be a desirable
strategy for the conjugate gradient method. The analysis is independent of the
specific approximation scheme or sparsity pattern, and applies equally to
incomplete factorizations, algebraic updates, and learning-based constructions.
Numerical experiments confirm the predictions of the theory, including an
illustration where sparse factors are trained by a direct gradient updates to
IC(0) factor entries, i.e., no trained neural network model is used.

</details>


### [6] [Randomized Space-Time Sampling for Affine Graph Dynamical Systems](https://arxiv.org/abs/2509.16818)
*Le Gong,Longxiu Huang*

Main category: math.NA

TL;DR: This paper studies dynamical sampling of graph signals with constant source terms, introducing random space-time sampling regimes and analyzing stable recovery conditions for joint estimation of initial state and forcing term.


<details>
  <summary>Details</summary>
Motivation: Extend dynamical sampling to non-homogeneous systems with constant source terms, which create non-orthogonal-diagonalizable system matrices that render classical spectral techniques inapplicable.

Method: Introduce spectral graph weighted coherence to characterize sampling-graph interplay, establish RIP-based sampling complexity bounds, and develop robust recovery algorithm with error guarantees.

Result: Prove stable recovery conditions via RIP analysis and validate effectiveness through extensive experiments on synthetic and real-world datasets.

Conclusion: The framework successfully addresses the challenging setting of dynamical sampling with constant source terms, providing theoretical guarantees and practical algorithms for joint recovery of initial state and forcing term.

Abstract: This paper investigates the problem of dynamical sampling for graph signals
influenced by a constant source term. We consider signals evolving over time
according to a linear dynamical system on a graph, where both the initial state
and the source term are bandlimited. We introduce two random space-time
sampling regimes and analyze the conditions under which stable recovery is
achievable. While our framework extends recent work on homogeneous dynamics, it
addresses a fundamentally different setting where the evolution includes a
constant source term. This results in a non-orthogonal-diagonalizable system
matrix, rendering classical spectral techniques inapplicable and introducing
new challenges in sampling design, stability analysis, and joint recovery of
both the initial state and the forcing term. A key component of our analysis is
the spectral graph weighted coherence, which characterizes the interplay
between the sampling distribution and the graph structure. We establish
sampling complexity bounds ensuring stable recovery via the Restricted Isometry
Property (RIP), and develop a robust recovery algorithm with provable error
guarantees. The effectiveness of our method is validated through extensive
experiments on both synthetic and real-world datasets.

</details>


### [7] [A decoupled and structure-preserving direct discontinuous Galerkin method for the Keller-Segel Model](https://arxiv.org/abs/2509.16940)
*X. Yin,X. Lan,Y. Qin*

Main category: math.NA

TL;DR: A novel structure-preserving numerical scheme for the Keller-Segel model that maintains energy stability, mass conservation, and positivity while achieving optimal accuracy through gradient flow reformulation and discontinuous Galerkin discretization.


<details>
  <summary>Details</summary>
Motivation: To develop a numerical method that preserves the intrinsic mathematical structure of the Keller-Segel model while achieving optimal accuracy, addressing the need for reliable simulation of chemotaxis models and gradient flow systems.

Method: Reformulated the KS model into gradient flow structure using energy variational method, then constructed a structure-preserving discretization using semi-implicit time stepping and direct discontinuous Galerkin (DDG) spatial discretization with positivity-preserving limiter.

Result: The scheme achieves energy stability, exact mass conservation, positivity preservation, and optimal convergence rates (first-order in time, (k+1)-th order in space). Numerical experiments validate robustness and accuracy on benchmark problems including pattern formation and near blow-up scenarios.

Conclusion: The proposed method provides a flexible and reliable framework for structure-preserving numerical simulation of chemotaxis models and other gradient flow-type systems, with rigorous theoretical guarantees and demonstrated practical effectiveness.

Abstract: In this work, we develop a novel numerical scheme to solve the classical
Keller--Segel (KS) model which simultaneously preserves its intrinsic
mathematical structure and achieves optimal accuracy. The model is reformulated
into a gradient flow structure using the energy variational method, which
reveals the inherent energy dissipative dynamics of the system. Based on this
reformulation, we construct a structure-preserving discretization by
semi-implicit method in time and the direct discontinuous Galerkin (DDG) method
in space, resulting in a stable and high-order accurate approximation. The
proposed scheme enjoys several desirable properties: (i) energy stability,
ensuring discrete free energy dissipation; (ii) exact conservation of mass for
the cell density; (iii) positivity preservation of the numerical cell density,
enforced via a carefully designed limiter; and (iv) optimal convergence rate,
with first-order accuracy in time and $(k+1)$-th order accuracy in space for
polynomials of degree $k$. We provide rigorous theoretical analysis that
substantiate these properties. In addition, extensive numerical experiments,
including benchmark problems exhibiting pattern formation and near blow-up
behavior, are conducted to validate the theoretical results and demonstrate the
robustness, efficiency, and accuracy of the proposed method. The approach
offers a flexible and reliable framework for structure-preserving numerical
simulation of chemotaxis models and other gradient flow-type systems.

</details>


### [8] [Numerical Reconstruction of Coefficients in Elliptic Equations Using Continuous Data Assimilation](https://arxiv.org/abs/2509.16954)
*Peiran Zhang*

Main category: math.NA

TL;DR: Numerical reconstruction of conductivity coefficient and source term in 2D elliptic PDEs using data assimilation with error estimates for discretized reconstructions.


<details>
  <summary>Details</summary>
Motivation: To develop effective methods for reconstructing spatially dependent conductivity coefficients and source terms in elliptic partial differential equations with given interior solution observations.

Method: Using data assimilation to derive approximated gradients of error functional for updating reconstructed coefficients, with new L² error estimates for spatially discretized reconstructions.

Result: Numerical examples demonstrate method effectiveness and validate error estimates, showing robust reconstruction performance even with errors in certain input coefficients.

Conclusion: The proposed data assimilation approach provides effective and robust reconstruction of conductivity coefficients and source terms in elliptic PDEs with proven error estimates.

Abstract: We consider the numerical reconstruction of the spatially dependent
conductivity coefficient and the source term in elliptic partial differential
equations of in a two-dimensional convex polygonal domain, with the homogeneous
Dirichlet boundary condition and given interior observation of the solution.
Using data assimilation, some approximated gradients of our error functional
are derived to update the reconstructed coefficients, and new $L^2$ error
estimates for such minimization formulations are given for the spatially
discretized reconstructions. Numerical examples are provided to show the
effectiveness of the method and demonstrate the error estimates. The numerical
results also show that the reconstruction is very robust for the error in
certain inputted coefficients.

</details>


### [9] [Neural Network Dual Norms for Minimal Residual Finite Element Methods](https://arxiv.org/abs/2509.16961)
*Hamd Alsobhi,Emin Benny-Chacko,Ignacio Brevis,Kristoffer G. van der Zee*

Main category: math.NA

TL;DR: A hybrid minimal-residual finite element method using neural networks as test functions to improve stability and accuracy for PDEs with dual space residuals.


<details>
  <summary>Details</summary>
Motivation: Minimal-residual methods for PDEs with residuals in dual spaces face stability challenges. The authors aim to develop a stable method by leveraging neural networks to better approximate residual representers.

Method: Hybrid approach combining standard finite element spaces for solutions with neural networks as test functions. Implemented via deep residual Uzawa algorithm alternating finite element updates with neural network training.

Result: Proven consistency, convergence, and a priori error estimates. Numerical experiments on advection-reaction problems with singular/discontinuous data show robust and accurate approximations.

Conclusion: The neural network-enhanced minimal-residual method provides improved stability and accuracy for challenging PDE problems, with theoretical guarantees and practical effectiveness demonstrated through numerical experiments.

Abstract: Minimal-residual methods for PDEs with a residual in a dual space are
non-trivial to guarantee stability. We present a minimal-residual finite
element method in which the solution space is a standard finite element space,
but neural networks are used as test functions for the evaluation of residual
dual norms. The use of a neural network improves the approximation of the
residual representer, and thereby improves the stability of the method. Our
hybrid approach is implemented through a deep residual Uzawa algorithm that
alternates finite element updates with neural network training. We prove
consistency and convergence results for the Uzawa methodology. We also prove an
a priori error estimate that relies on a suitable Fortin compatibility
condition. Numerical experiments on advection-reaction problems with singular
or discontinuous data show that the proposed framework delivers robust and
accurate approximations.

</details>


### [10] [Multiscale solution decomposition of nonlocal-in-time problems with application in numerical computation](https://arxiv.org/abs/2509.17020)
*Mengmeng Liu,Jie Ma,Wenlin Qiu,Xiangcheng Zheng*

Main category: math.NA

TL;DR: The paper develops a multiscale solution decomposition (MSD) method for nonlocal-in-time problems to separate singular terms from the original solution, making the remaining part smoother and enabling better numerical analysis and computation.


<details>
  <summary>Details</summary>
Motivation: Nonlocal-in-time problems often have singular solutions that violate smoothness assumptions required by many numerical methods, limiting their applicability and accuracy.

Method: A multiscale solution decomposition (MSD) approach that extracts known singular terms from the solution, leaving a smoother unknown part that satisfies standard smoothness assumptions for numerical analysis.

Result: MSD makes numerical analysis results applicable to nonlocal problems, significantly reduces computational difficulties by avoiding direct handling of solution singularities, and improves numerical accuracy and stability across various problem types.

Conclusion: The MSD method provides a universal framework for handling nonlocal-in-time problems by transforming singular solutions into smoother forms, enabling more effective numerical methods and analysis across multiple application domains.

Abstract: This work develops a multiscale solution decomposition (MSD) method for
nonlocal-in-time problems to separate a series of known terms with multiscale
singularity from the original singular solution such that the remaining unknown
part becomes smoother. We demonstrate that the MSD provides a scenario where
the smoothness assumption for solutions of weakly singular nonlocal-in-time
problems, a commonly encountered assumption in numerous literature of numerical
methods that is in general not true for original solutions, becomes appropriate
such that abundant numerical analysis results therein become applicable. From
computational aspect, instead of handling solution singularity, the MSD
significantly reduces the numerical difficulties by separating and thus
circumventing the solution singularity. We consider typical problems, including
the fractional relaxation equation, Volterra integral equation, subdiffusion,
integrodifferential equation and diffusion-wave equation, to demonstrate the
universality of MSD and its effectiveness in improving the numerical accuracy
or stability in comparison with classical methods.

</details>


### [11] [Admissible convergence behavior and mirroring of stagnation in restarted (block) GMRES](https://arxiv.org/abs/2509.17077)
*Marie Kubínová,Kirk M. Soodhalter*

Main category: math.NA

TL;DR: Construction of matrices and block right-hand sides with specified restarted block GMRES convergence patterns, where eigenvalues and Ritz values can be chosen independently of convergence behavior.


<details>
  <summary>Details</summary>
Motivation: To generalize previous work on non-block GMRES to the block setting, enabling control over convergence patterns while maintaining independent selection of eigenvalues and Ritz values.

Method: Analyze block GMRES as an iteration over a right vector space with scalars from the *-algebra of matrices, extending previous proofs to facilitate generalization to block setting.

Result: Development of a framework for constructing matrices and block right-hand sides that exhibit predetermined convergence behavior in restarted block GMRES.

Conclusion: Successfully generalized non-block GMRES convergence control methods to block GMRES, providing tools for independent manipulation of convergence patterns and spectral properties.

Abstract: In this work, we describe how to construct matrices and block right-hand
sides the exhibit a specified restarted block GMRES convergence pattern, such
that the eigenvalues and Ritz values at each iteration can be chosen
independent of the specified convergence behavior. This work is a
generalization of the work in [Meurant and Tebbens, Num. Alg. 2019] in which
the authors do the same for restarted non-block GMRES. We use the same tools as
were used in [Kub\'inov\'a and Soodhalter, SIMAX 2020], namely to analyze block
GMRES as an iteration over a right vector space with scalars from the
$^\ast$-algebra of matrices. To facilitate our work, we also extend the work of
Meurant and Tebbens and offer alternative proofs of some of their results, that
can be more easily generalized to the block setting.

</details>


### [12] [Data-efficient Kernel Methods for Learning Hamiltonian Systems](https://arxiv.org/abs/2509.17154)
*Yasamin Jalalian,Mostafa Samir,Boumediene Hamzi,Peyman Tavallali,Houman Owhadi*

Main category: math.NA

TL;DR: Kernel-based methods for data-driven identification and forecasting of Hamiltonian systems, with two approaches: two-step trajectory reconstruction then Hamiltonian learning, and one-step joint inference.


<details>
  <summary>Details</summary>
Motivation: Hamiltonian dynamics describe many physical systems, and data-driven simulations are important for scientific and engineering applications.

Method: Proposed kernel-based methods including two-step (trajectory reconstruction then Hamiltonian learning) and one-step (joint inference) approaches.

Result: Achieves accurate, data-efficient predictions across benchmark systems (mass-spring, nonlinear pendulum, Henon-Heiles), outperforming baselines in scarce-data regimes while preserving conservation properties.

Conclusion: Method provides theoretical error estimates and extends to a general numerical framework for learning arbitrary dynamical systems beyond Hamiltonian systems.

Abstract: Hamiltonian dynamics describe a wide range of physical systems. As such,
data-driven simulations of Hamiltonian systems are important for many
scientific and engineering problems. In this work, we propose kernel-based
methods for identifying and forecasting Hamiltonian systems directly from data.
We present two approaches: a two-step method that reconstructs trajectories
before learning the Hamiltonian, and a one-step method that jointly infers
both. Across several benchmark systems, including mass-spring dynamics, a
nonlinear pendulum, and the Henon-Heiles system, we demonstrate that our
framework achieves accurate, data-efficient predictions and outperforms
two-step kernel-based baselines, particularly in scarce-data regimes, while
preserving the conservation properties of Hamiltonian dynamics. Moreover, our
methodology provides theoretical a priori error estimates, ensuring reliability
of the learned models. We also provide a more general, problem-agnostic
numerical framework that goes beyond Hamiltonian systems and can be used for
data-driven learning of arbitrary dynamical systems.

</details>


### [13] [On efficient block Krylov-solvers for $\mathcal H^2$-matrices](https://arxiv.org/abs/2509.17257)
*Sven Christophersen*

Main category: math.NA

TL;DR: Efficient implementation of H^2-matrix operations for block Krylov subspace methods, optimizing memory and cache usage on modern hardware.


<details>
  <summary>Details</summary>
Motivation: Hierarchical matrices (H^2 format) enable memory-efficient storage of dense operators from boundary element methods. Solving multiple linear systems with the same matrix naturally leads to block methods, which require efficient matrix-matrix multiplications.

Method: Developed an optimized implementation of H^2-matrix-vector and H^2-matrix-matrix multiplication that maximizes memory and cache utilization on modern hardware. This implementation is used to accelerate block Krylov subspace methods.

Result: The paper presents efficient algorithms for H^2-matrix operations that fully exploit modern hardware capabilities, leading to accelerated performance in block iterative solvers.

Conclusion: The optimized H^2-matrix implementation enables efficient block Krylov subspace methods, providing significant performance improvements for solving multiple linear systems with hierarchical matrices.

Abstract: Hierarchical matrices provide a highly memory-efficient way of storing dense
linear operators arising, for example, from boundary element methods,
particularly when stored in the H^2 format. In such data-sparse
representations, iterative solvers are preferred over direct ones due to the
cost-efficient matrix-vector multiplications they enable. Solving multiple
systems of linear equations with the same hierarchical matrix naturally leads
to block methods, which in turn make heavy use of BLAS level-3 functions such
as GEMM. We present an efficient implementation of H^2-matrix-vector and
H^2-matrix-matrix multiplication that fully exploits the potential of modern
hardware in terms of memory and cache utilization. The latter is employed to
accelerate block Krylov subspace methods, which we present later as the main
results of this paper.

</details>


### [14] [A two-grid method with dispersion matching for finite-element Helmholtz problems](https://arxiv.org/abs/2509.17494)
*Christiaan C. Stolk*

Main category: math.NA

TL;DR: A new two-level solver for Helmholtz equations using finite elements, combining finite-difference coarse discretization with domain-decomposition smoothing, achieving fast convergence independent of problem size.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient solver for Helmholtz equations that overcomes the limitations of conventional methods which show dependency on problem size, by ensuring close matching of dispersion relations between fine and coarse levels.

Method: Two-level method with compact-stencil finite-difference for coarse discretization to minimize dispersion errors, and domain-decomposition solver with complex-shifted Helmholtz operator as smoother.

Result: Local Fourier analysis shows convergence when degrees of freedom per wavelength exceed order-dependent bounds (e.g., >8 for order 4). Numerical tests up to 80 wavelengths demonstrate fast convergence nearly independent of problem size.

Conclusion: Close matching of fine- and coarse-level dispersion relations is essential for good convergence in two-grid methods for Helmholtz problems.

Abstract: This work is about a new two-level solver for Helmholtz equations discretized
by finite elements. The method is inspired by two-grid methods for
finite-difference Helmholtz problems as well as by previous work on two-level
domain-decomposition methods. For the coarse-level discretization, a
compact-stencil finite-difference method is used that minimizes dispersion
errors. The smoother involves a domain-decomposition solver applied to a
complex-shifted Helmholtz operator. Local Fourier analysis shows the method is
convergent if the number of degrees of freedom per wavelength is larger than
some lower bound that depends on the order, e.g. more than 8 for order 4. In
numerical tests, with problem sizes up to 80 wavelengths, convergence was fast,
and almost independent of problem size unlike what is observed for conventional
methods. Analysis and comparison with dispersion-error data shows that, for
good convergence of a two-grid method for Helmholtz problems, it is essential
that fine- and coarse-level dispersion relations closely match.

</details>


### [15] [Robust spectral preconditioning for high-Péclet number convection-diffusion](https://arxiv.org/abs/2509.17531)
*Lukas Holbach,Peter Bastian,Robert Scheichl*

Main category: math.NA

TL;DR: A two-level hybrid restricted additive Schwarz preconditioner for high Péclet number convection-diffusion problems, using MS-GFEM coarse spaces with exponential convergence properties.


<details>
  <summary>Details</summary>
Motivation: To develop robust preconditioners for heterogeneous convection-diffusion equations at high Péclet numbers, where traditional methods struggle with high-contrast diffusion and complex velocity fields.

Method: Combines multiscale spectral generalized finite element method (MS-GFEM) with restricted additive Schwarz (RAS) preconditioning. Uses locally optimal basis functions from eigenproblems on operator-harmonic spaces as coarse space.

Result: Demonstrates exponential convergence, robustness to grid Péclet numbers and subdomain counts (up to 10^5), and fast convergence with small coarse spaces. Effective even for indefinite problems and vanishing-diffusion limit.

Conclusion: The method provides arbitrarily fast convergence for preconditioned GMRES with proper coarse space adaptation, showing strong performance across challenging convection-diffusion scenarios.

Abstract: We introduce a two-level hybrid restricted additive Schwarz (RAS)
preconditioner for heterogeneous steady-state convection-diffusion equations at
high P\'{e}clet numbers. Our construction builds on the multiscale spectral
generalized finite element method (MS-GFEM), wherein the coarse space is
spanned by locally optimal basis functions obtained from local generalized
eigenproblems on operator-harmonic spaces. Extending the theory of Ma (2025) to
convection-diffusion problems in conservation form, we establish exponential
convergence of the MS-GFEM approximation. Rewriting MS-GFEM as a RAS-type
iteration, we show for coercive problems that this exponential convergence
property is inherited by the RAS-type iterative method (at least in the
continuous setting). Employed as a preconditioner within the generalized
minimal residual method (GMRES), the resulting method requires only a few
iterations for high accuracy even with low-dimensional coarse spaces.
  Through extensive numerical experiments on problems with high-contrast
diffusion and non-divergence-free, rotating velocity fields, we demonstrate
robustness with respect to the grid P\'{e}clet number and the number of
subdomains (tested up to $10^5$ subdomains), while coarse-space dimensions
remain small as grid P\'{e}clet numbers increase. By adapting the coarse space
and oversampling size, we are able to achieve arbitrarily fast convergence of
preconditioned GMRES. As an extension, for which we do not have theory yet, we
show effectiveness of the method even for indefinite problems and in the
vanishing-diffusion limit.

</details>


### [16] [A Computational Method for the Inverse Robin Problem with Convergence Rate](https://arxiv.org/abs/2509.17571)
*Erik Burman,Marvin Knöller,Lauri Oksanen*

Main category: math.NA

TL;DR: A computational method for reconstructing Robin parameters in elliptic PDEs using first-order Lagrange finite elements, achieving second-order convergence with noise tolerance.


<details>
  <summary>Details</summary>
Motivation: To numerically solve the inverse Robin problem by determining Robin parameters from solution data on subdomains, addressing practical implementation challenges.

Method: Uses a unique continuation-inspired approach with first-order Lagrange finite elements, assuming Robin parameters are in finite-dimensional spaces of continuously differentiable functions.

Result: The method achieves second-order convergence in mesh size, maintains convergence with noisy data until noise dominates, and numerical experiments confirm feasibility.

Conclusion: The proposed scheme provides an effective and implementable solution for Robin parameter reconstruction with proven convergence properties.

Abstract: The inverse Robin problem covers the determination of the Robin parameter in
an elliptic partial differential equation posed on a domain $\Omega$. Given the
solution of the Robin problem on a subdomain $\omega \subset \Omega$ together
with the elliptic problem's right hand sides, the aim is to solve this inverse
Robin problem numerically. In this work, a computational method for the
reconstruction of the Robin parameter inspired by a unique continuation method
is established. The proposed scheme relies solely on first-order Lagrange
finite elements ensuring a straightforward implementation. Under the main
assumption that the Robin parameter is in a finite dimensional space of
continuously differentiable functions it is shown that the numerical method is
second order convergent in the finite element's mesh size. For noisy data this
convergence rate is shown to hold true until the noise term dominates the error
estimate. Numerical experiments are presented that highlight the feasibility of
the Robin parameter reconstruction and that confirm the theoretical convergence
results numerically.

</details>


### [17] [A posteriori existence for the Keller-Segel model via a finite volume scheme](https://arxiv.org/abs/2509.17710)
*Marc Hoffmann,Jan Giesselmann*

Main category: math.NA

TL;DR: Conditional a posteriori error estimates for finite volume scheme approximating Keller-Segel system with linear convergence in mesh size


<details>
  <summary>Details</summary>
Motivation: To derive rigorous error estimates for finite volume approximations of the parabolic-elliptic Keller-Segel system and establish conditions under which numerical solutions guarantee existence of exact weak solutions

Method: Derivation of two forms of conditional a posteriori error estimates controlling error in L∞(0,T,L²(Ω))- and L²(0,T;H¹(Ω))-norms for finite volume scheme

Result: Error estimates exhibit linear convergence in mesh size as observed numerically, and show that satisfying the error estimate conditions implies existence of weak solutions

Conclusion: Numerical solutions with good properties can rigorously infer existence of exact solutions to the Keller-Segel system through conditional error estimates

Abstract: We derive two forms of conditional a posteriori error estimates for a finite
volume scheme approximating the parabolic-elliptic Keller-Segel system. The
estimates control the error in the $L^\infty(0,T, L^2(\Omega))$- and
$L^2(0,T;H^1(\Omega))$-norm and exhibit linear convergence in the mesh size, as
observed in numerical experiments. Crucially, we show that as long as the
condition of the error estimate is satisfied a weak solution exits. This means,
as long as the numerical solution has good properties, we can rigorously infer
existence of an exact solution.

</details>


### [18] [Schrodingerization based quantum algorithms for the time-fractional heat equation](https://arxiv.org/abs/2509.17713)
*Shi Jin,Nana Liu,Yue Yu*

Main category: math.NA

TL;DR: A quantum algorithm for solving high-dimensional time-fractional heat equations using dimension extension and Schrodingerization techniques, achieving exponential advantage over classical methods in high dimensions.


<details>
  <summary>Details</summary>
Motivation: To develop efficient quantum algorithms for solving computationally expensive high-dimensional time-fractional heat equations, which are challenging for classical computers due to the curse of dimensionality.

Method: Combines dimension extension technique to reformulate d+1-dimensional equations as d+2-dimensional local PDEs, followed by discretization and Schrodingerization approach using warped phase transformation to convert the system into unitary Schrodinger-type systems suitable for quantum simulation.

Result: The quantum algorithm achieves up to exponential advantage in high dimensions, requiring only O(T²d⁴h⁻⁸) queries compared to classical methods that need at least O(N_t d h⁻⁽ᵈ⁺⁰·⁵⁾) matrix-vector multiplications, with quantum complexity being independent of dimension d in terms of h⁻¹.

Conclusion: The proposed quantum algorithm provides an efficient solution for high-dimensional time-fractional heat equations, demonstrating significant computational advantages over classical counterparts through theoretical analysis and numerical validation.

Abstract: We develop a quantum algorithm for solving high-dimensional time-fractional
heat equations. By applying the dimension extension technique from [CS07], the
$d+1$-dimensional time-fractional equation is reformulated as a local partial
differential equation in $d+2$ dimensions. Through discretization along both
the extended and spatial domains, a stable system of ordinary differential
equations is obtained by a simple change of variables. We propose a quantum
algorithm for the resulting semi-discrete problem using the Schrodingerization
approach from [JLY24a,JLY23,JL24a]. The Schrodingerization technique transforms
general linear partial and ordinary differential equations into
Schrodinger-type systems--with unitary evolution, making them suitable for
quantum simulation. This is accomplished via the warped phase transformation,
which maps the equation into a higher-dimensional space. We provide detailed
implementations of this method and conduct a comprehensive complexity analysis,
demonstrating up to exponential advantage--with respect to the inverse of the
mesh size in high dimensions~--~compared to its classical counterparts.
Specifically, to compute the solution to time $T$, while the classical method
requires at least $\mathcal{O}(N_t d h^{-(d+0.5)})$ matrix-vector
multiplications, where $N_t $ is the number of time steps (which is, for
example, $\mathcal{O}(Tdh^{-2})$ for the forward Euler method), our quantum
algorithms requires $\widetilde{\mathcal{O}}(T^2d^4 h^{-8})$ queries to the
block-encoding input models, with the quantum complexity being independent of
the dimension $d$ in terms of the inverse mesh size $h^{-1}$. Numerical
experiments are performed to validate our formulation.

</details>


### [19] [Solving time-fractional diffusion equations with Robin boundary conditions via fractional Hamiltonian boundary value methods](https://arxiv.org/abs/2509.17793)
*Qian Luo,Aiguo Xiao,Xiaoqiang Yan,Jingmin Xia*

Main category: math.NA

TL;DR: A novel numerical scheme for solving time-fractional reaction-diffusion problems with Robin boundary conditions using spectral collocation in space and Fractional Hamiltonian boundary value methods in time, achieving spectral accuracy in both dimensions.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient numerical method for time-fractional reaction-diffusion problems with Robin boundary conditions that can achieve high accuracy in both space and time dimensions.

Method: Combines spectral collocation method in space (using basis functions adapted to Robin boundary conditions) with Fractional Hamiltonian boundary value methods in time. The time derivative is in Caputo sense of order α∈(0,1).

Result: The method achieves spectral accuracy in space and is capable of spectral accuracy in time. Existence and uniqueness of solution are proved. Numerical examples support theoretical results and demonstrate exponential convergence.

Conclusion: The proposed hybrid method effectively solves time-fractional reaction-diffusion problems with Robin boundary conditions, providing high accuracy and theoretical guarantees of solution existence and uniqueness.

Abstract: In this paper, we propose a novel numerical scheme for solving
time-fractional reaction-diffusion problems with Robin boundary conditions,
where the time derivative is in the Caputo sense of order $\alpha\in(0,1)$. The
existence and uniqueness of the solution is proved. Our proposed method is
based on the spectral collocation method in space and Fractional Hamiltonian
boundary value methods in time. For the considered spectral collocation method,
the basis functions used are not the standard polynomial basis functions, but
rather adapt to Robin boundary conditions, and the exponential convergence
property is provided. The proposed procedure achieves spectral accuracy in
space and is also capable of getting spectral accuracy in time. Some numerical
examples are provided to support the theoretical results.

</details>


### [20] [Scott-Vogelius element and iterated penalty method for inhomogeneous Dirichlet boundary conditions](https://arxiv.org/abs/2509.17899)
*Franziska Eickmann,Ridgway L. Scott,Tabea Tscherpel*

Main category: math.NA

TL;DR: This paper presents quasi-optimal a priori error estimates for mixed finite element methods applied to Stokes problems with inhomogeneous Dirichlet boundary conditions, with specific focus on pressure-robust estimates for Scott-Vogelius elements and analysis of iterative methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop accurate error estimates for Stokes problems with complex boundary conditions, particularly addressing the challenges of pressure robustness and compatibility conditions for boundary data in mixed finite element methods.

Method: The authors use a modified Fortin operator to preserve compatibility conditions for boundary data, and analyze both the iterated penalty method (a Uzawa-type algorithm) and Scott-Vogelius finite elements for solving Stokes problems with inhomogeneous Dirichlet boundary conditions.

Result: The paper establishes quasi-optimal a priori error estimates, demonstrates convergence of the iterative methods, and shows asymptotic pressure robustness. Numerical experiments validate the theoretical findings.

Conclusion: The study successfully provides rigorous error estimates for mixed finite element methods in Stokes problems, highlighting the critical importance of compatibility conditions for boundary data and proper treatment of nearly singular vertices in practical implementations.

Abstract: We present quasi-optimal a priori error estimates for general mixed finite
element methods to approximate solutions of the Stokes problem subject to
inhomogeneous Dirichlet boundary conditions. For the Scott-Vogelius element
this yields pressure-robust a priori error estimates. Due to the exact
divergence constraint, this requires a compatibility condition for the boundary
data to hold. A key tool is a modified Fortin operator, capable of preserving
this compatibility condition. Furthermore, we analyse the iterated penalty
method, a Uzawa-type algorithm and we show its convergence and asymptotic
pressure robustness. Numerical experiments support the theory and highlight the
importance of the compatibility condition and the appropriate treatment of
nearly singular vertices.

</details>


### [21] [Local Characterization of Noise in Iterative Reconstruction of the Generalized Radon Transform](https://arxiv.org/abs/2509.17944)
*Alexander Katsevich*

Main category: math.NA

TL;DR: Analysis of noise in iterative reconstruction from discrete noisy data of generalized Radon transforms, showing Gaussian convergence of reconstruction error with explicit covariance structure.


<details>
  <summary>Details</summary>
Motivation: To complete the analysis of iterative reconstruction at native scale by characterizing noise effects, complementing earlier deterministic results on data discreteness limitations.

Method: Builds on Local Reconstruction Analysis (LRA) framework to analyze reconstructions at native scale, establishing distributional convergence of rescaled reconstruction error.

Result: Rescaled reconstruction error converges to zero-mean Gaussian random field with explicitly computable covariance, with numerical experiments confirming theoretical predictions.

Conclusion: Combined with prior deterministic results, this work completes the analysis of iterative reconstruction at native scale for both fundamental limitations: data discreteness and noise presence.

Abstract: We study noise in iterative reconstruction from discrete noisy data of a
generalized Radon transform in the plane. Our approach builds on Local
Reconstruction Analysis (LRA), a framework for analyzing reconstructions at the
native scale. We establish that the rescaled reconstruction error converges in
distribution to a zero-mean Gaussian random field with explicitly computable
covariance, providing a complete local characterization of noise in iterative
reconstruction. Numerical experiments show strong agreement with the
theoretical predictions. Combined with earlier deterministic results, our
findings complete the analysis of iterative reconstruction at the native scale
with respect to the two most fundamental limitations: the discreteness of the
data and the presence of noise.

</details>


### [22] [An adaptive Hermite spectral method for the Boltzmann equation](https://arxiv.org/abs/2509.17981)
*Sihong Shao,Yanli Wang,Jie Wu*

Main category: math.NA

TL;DR: An adaptive Hermite spectral method for 3D Boltzmann equation using frequency indicators to dynamically adjust scaling and expansion order, achieving significant error reduction and computational savings.


<details>
  <summary>Details</summary>
Motivation: To efficiently solve the three-dimensional Boltzmann equation by developing an adaptive spectral method that can dynamically optimize computational resources while maintaining accuracy.

Method: Uses frequency indicators to guide adaptive Hermite spectral method: for homogeneous problems, indicator based on high-order coefficients; for non-homogeneous problems, Fourier-Hermite scheme with spatial distribution indicators. Implements scaling and p-adaptive techniques.

Result: Scaling adaptive method substantially reduces L2 errors with negligible computational cost; p-adaptive method achieves up to 74% time savings in numerical experiments covering 3D problems.

Conclusion: The adaptive Hermite spectral method with frequency indicators is highly effective for solving Boltzmann equations, providing both accuracy improvements and significant computational efficiency gains.

Abstract: We propose an adaptive Hermite spectral method for the three-dimensional
velocity space of the Boltzmann equation guided by a newly developed frequency
indicator. For the homogeneous problem, the indicator is defined by the
contribution of high-order coefficients in the spectral expansion. For the
non-homogeneous problem, a Fourier-Hermite scheme is employed, with the
corresponding frequency indicator formulated based on distributions across the
entire spatial domain. The adaptive Hermite method includes scaling and
p-adaptive techniques to dynamically adjust the scaling factor and expansion
order according to the indicator. Numerical experiments cover both homogeneous
and non-homogeneous problems in up to three spatial dimensions. Results
demonstrate that the scaling adaptive method substantially reduces L2 errors at
negligible computational cost, and the p-adaptive method achieves time savings
of up to 74%.

</details>


### [23] [Rational methods for abstract semilinear problems without order reduction](https://arxiv.org/abs/2509.17984)
*Carlos Arranz-Simón,Begoña Cano,César Palencia*

Main category: math.NA

TL;DR: Extension of rational methods to integrate semilinear problems with same computational cost as Runge-Kutta methods but avoiding order reduction.


<details>
  <summary>Details</summary>
Motivation: To extend the scope of rational methods from linear homogeneous problems to cover linear nonhomogeneous and semilinear problems while maintaining computational efficiency.

Method: Developed a procedure that integrates semilinear problems using rational methods, requiring the same computational cost as linked Runge-Kutta methods.

Result: The proposed methods successfully avoid the order reduction phenomenon that typically affects other integration methods for semilinear problems.

Conclusion: Numerical illustrations confirm the predicted behavior of the proposed rational methods, demonstrating their effectiveness for semilinear problem integration.

Abstract: Rational methods are intended to time integrate linear homogeneous problems.
However, their scope can be extended so as to cover linear nonhomogeneous
problems. In this paper the integration of semilinear problems is considered.
The resulting procedure requires the same computational cost than the one of a
linked Runge--Kutta method, with the advantage that the order reduction
phenomenon is avoided. Some numerical illustrations are included showing the
predicted behaviour of the proposed methods.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [24] [Large-time Behavior for a Cutoff Level-Set Mean Curvature G-equation with Source term](https://arxiv.org/abs/2509.16341)
*Adrian D. Calderon*

Main category: math.AP

TL;DR: Analysis of cutoff level-set mean curvature G-equation with non-negative source term, focusing on large-time behavior in periodic and radially symmetric settings, using monotonicity properties due to non-coercive and non-convex Hamiltonian.


<details>
  <summary>Details</summary>
Motivation: To understand the large-time behavior of a fully nonlinear degenerate parabolic PDE (cutoff level-set mean curvature G-equation) with non-negative source term, particularly in periodic and radially symmetric settings where standard Hamilton-Jacobi theory fails due to non-coercivity and non-convexity.

Method: Relies on inherent structure of the equation to find monotonicity property and corresponding uniqueness set, rather than standard Hamilton-Jacobi theory. Also discusses local regularity of solutions and provides representation formula in radial symmetric setting.

Result: Developed approach to analyze large-time behavior despite non-coercive and non-convex Hamiltonian through structural monotonicity properties.

Conclusion: The paper establishes methods for studying large-time behavior of this class of PDEs in specific geometric settings, providing tools for analysis when standard theory is inapplicable.

Abstract: We consider a cutoff level-set mean curvature G-equation with a non-negative
source term. In particular, we study the large-time behavior of this fully
nonlinear degenerate parabolic partial differential equation in two settings:
periodic and radially symmetric. Due to the non-coercivity and non-convexity of
the Hamiltonian, we are not able to use the standard Hamilton-Jacobi theory and
instead rely on the inherent structure to find a monotonicity property and
corresponding uniqueness set. Lastly, we discuss the local regularity of the
solutions, as well as a representation formula in the radial symmetric setting.

</details>


### [25] [A Class of De Giorgi Type and Hölder Continuity for Some Problems in Musielak-Orlicz-Sobolev Spaces](https://arxiv.org/abs/2509.16392)
*Hlel Missaoui,Anouar Bahrouni,Hichem Ounaies*

Main category: math.AP

TL;DR: The paper introduces a new class of De Giorgi type functions and proves their Hölder continuity under suitable conditions on generalized N-functions, with applications to quasilinear equations with G(x,t)-growth conditions.


<details>
  <summary>Details</summary>
Motivation: To generalize previous Hölder continuity results from variable exponent and Orlicz problems to a broader class of quasilinear equations with G(x,t)-growth conditions, encompassing both critical and standard growth cases.

Method: Introduces a new class of De Giorgi type functions (denoted by B_G(x,t)) and establishes their Hölder continuity properties under appropriate assumptions on the generalized N-function G(x,t).

Result: Proves Hölder continuity of solutions to quasilinear equations with divergence-form principal part and G(x,t)-growth conditions, generalizing previous results for variable exponent and Orlicz problems.

Conclusion: The work provides a unified framework that covers a wide variety of quasilinear equations and extends known Hölder continuity results to more general growth conditions.

Abstract: In this paper, we introduce a new class of De Giorgi type functions, denoted
by \(\mathcal{B}_{G(x,t)}\), and establish the H\"older continuity of its
elements under suitable additional assumptions on the generalized
\textnormal{N}-function \(G(x,t)\). As an application, we prove the H\"older
continuity of solutions to quasilinear equations whose principal part is in
divergence form with \(G(x,t)\)-growth conditions, including both critical and
standard growth cases. The novelty of our work lies in the generalization of
the H\"older continuity results previously known for variable exponent \cite[X,
Fan and D. Zhao]{Fan1999} and Orlicz \cite[G. M. Lieberman]{Li1991} problems.
Moreover, our results encompass a wide variety of quasilinear equations.

</details>


### [26] [A special concavity property for positive Hessian quotient operators](https://arxiv.org/abs/2509.16406)
*Pengfei Guan,Marcin Sroka*

Main category: math.AP

TL;DR: The paper establishes a special concavity property for positive Hessian quotient operators and proves a Jacobi inequality for symmetric tensors satisfying positive Hessian quotient equations on Riemannian manifolds.


<details>
  <summary>Details</summary>
Motivation: To understand the concavity properties of Hessian quotient operators and extend Jacobi inequality results to more general settings involving symmetric tensors on Riemannian manifolds.

Method: The authors establish a special concavity property for positive Hessian quotient operators of the form σₙ(W)/σₙ₋ₖ(W) where 1≤k≤n-1, and use this property to prove a Jacobi inequality.

Result: A Jacobi inequality is proven for general symmetric tensors that satisfy positive Hessian quotient equations on Riemannian manifolds.

Conclusion: The established concavity property enables the derivation of Jacobi inequalities in broader geometric contexts, extending previous results to more general symmetric tensor equations.

Abstract: We establish a special concavity property for positive Hessian quotient
operators $\frac{\sigma_n(W)}{\sigma_{n-k}(W)}, \ 1\le k\le n-1$. As a
consequence, we prove a Jacobi inequality for general symmetric tensor
satisfying positive Hessian quotient equation on Riemannian manifolds.

</details>


### [27] [Joint inviscid, incompressible, and continuous phenotype limit in nonlocal models of tissue growth](https://arxiv.org/abs/2509.16416)
*Chen-Chih Lai,Hongbo Lu*

Main category: math.AP

TL;DR: Derivation of joint limit for vanishing viscosity, singular pressure, and discrete-to-continuous phenotype structure in tissue growth models, converging to an incompressible Darcy-type model with continuous phenotypic structure.


<details>
  <summary>Details</summary>
Motivation: To provide a unified framework for modeling constrained tissue growth with mechanical feedback and phenotypic plasticity by combining three singular limits simultaneously.

Method: Builds on recent advances in Brinkman-to-Darcy limit and hydrodynamic limits for phenotype-structured populations, using uniform a priori estimates, compactness in space-phenotype-time, and generalized entropy-dissipation structure.

Result: Established convergence from a viscoelastic (Brinkman-type) system with multiple interacting phenotypes to an incompressible Darcy-type model with continuous phenotypic structure.

Conclusion: The analysis successfully combines three singular limits (vanishing viscosity, singular pressure, discrete-to-continuous phenotype) under a unified framework, providing rigorous mathematical foundation for tissue growth modeling with mechanical constraints and phenotypic plasticity.

Abstract: We rigorously derive the joint limit of vanishing viscosity, singular
pressure, and discrete-to-continuous phenotype structure in a class of tissue
growth models. Starting from a viscoelastic (Brinkman-type) system describing
multiple interacting phenotypes, where pressure depends nonlinearly on total
cell density, we establish convergence to an incompressible Darcy-type model
with a continuous phenotypic structure. Our analysis builds upon recent
advances on the Brinkman-to-Darcy limit and incompressible transitions by
David, Jacob, and Kim [arXiv:2503.18870], and on hydrodynamic limits for
phenotype-structured populations by Debiec, Mandal, and Schmidtchen [J.
Differential Equations 2025]. A key novelty lies in combining all three
singular limits simultaneously, under uniform a priori estimates, compactness
in space-phenotype-time, and a generalized entropy-dissipation structure. This
provides a unified framework for modeling constrained tissue growth with
mechanical feedback and phenotypic plasticity.

</details>


### [28] [Quantitative weak-BV stability of "wild'' solutions to compressible Euler equations, with a view towards higher systems](https://arxiv.org/abs/2509.16432)
*Geng Chen,Cooper Faile,Sam G. Krupa*

Main category: math.AP

TL;DR: This paper extends the weak-BV stability framework from 2D to 3D systems, specifically applying it to the full Euler system with three conserved quantities, and discusses applications to higher-dimensional systems.


<details>
  <summary>Details</summary>
Motivation: The stability of large data solutions for hyperbolic conservation laws remains an open problem, and the authors aim to extend their previous weak-BV stability framework from systems with two conserved quantities to more complex systems like the full Euler system.

Method: The authors apply their previously developed weak-BV stability framework to the full Euler system with three conserved quantities, building on their work from arXiv:2507.23645.

Result: The paper demonstrates how the weak-BV stability methods can be extended to handle systems with three conserved quantities, specifically showing Hölder stability for potentially "wild" large data solutions relative to BV solutions.

Conclusion: The successful application to the full Euler system opens pathways for future work on higher-dimensional systems with additional conserved quantities, advancing the understanding of stability for large data solutions in hyperbolic conservation laws.

Abstract: For hyperbolic systems of conservation laws, including important physical
models from continuum mechanics, the question of stability for large data
solutions remains a challenging open problem. In recent work (arXiv:2507.23645)
the authors introduce a framework for showing H\"older stability of potentially
"wild" large data solutions, relative to a class of BV solutions, for systems
with two conserved quantities. This is referred to as "weak-BV" stability. In
this paper, we give a short introduction to the methods while applying them to
the "full" Euler system with three conserved quantities. We discuss
applications to future work for higher systems with additional conserved
quantities.

</details>


### [29] [Non-isentropic cavity flow for the multi-d compressible Euler system](https://arxiv.org/abs/2509.16435)
*Helge Kristian Jenssen,Charis Tsikkou*

Main category: math.AP

TL;DR: Construction of non-isentropic self-similar multi-dimensional Euler flows with collapsing central vacuum cavities, satisfying physical boundary conditions and verified for physically relevant 2D and 3D cases.


<details>
  <summary>Details</summary>
Motivation: Previous analyses focused on isentropic flows, but non-isentropic flows introduce complications at fluid-vacuum interfaces. This work extends the analysis to non-isentropic settings to better model physical scenarios.

Method: Introduce algebraic conditions on parameters (spatial dimension, adiabatic index, similarity parameters) and use trapping regions for associated similarity ODEs to prove existence of non-isentropic cavity flows.

Result: Successfully constructed non-isentropic self-similar Euler flows with collapsing vacuum cavities that satisfy physical boundary conditions (vanishing pressure at interface, finite non-zero acceleration until collapse).

Conclusion: The algebraic conditions are verified for several physically relevant cases in both two and three dimensions, demonstrating the feasibility of non-isentropic cavity collapse flows.

Abstract: We rigorously construct non-isentropic and self-similar multi-d Euler flows
in which a central cavity (vacuum region) collapses. While isentropic flows of
this type have been analyzed earlier by Hunter \cite{hun_60} and others, the
non-isentropic setting introduces additional complications, in particular with
respect to the behavior along the fluid-vacuum interface. The flows we
construct satisfy the physical boundary conditions: the interface is a material
surface along which the pressure vanishes, and it propagates with a
non-vanishing and finite acceleration until collapse.
  We introduce a number of algebraic conditions on the parameters in the
problem (spatial dimension, adiabatic index, similarity parameters). With these
conditions satisfied, a simple argument based on trapping regions for the
associated similarity ODEs yields the existence of non-isentropic cavity flows.
We finally verify that the conditions are all met for several physically
relevant cases in both two and three dimensions.

</details>


### [30] [Self-similar blowup for mass supercritical Schrödinger equations](https://arxiv.org/abs/2509.16600)
*Roland Donninger,Lorenz Lichtnecker*

Main category: math.AP

TL;DR: Existence of self-similar solutions for focusing nonlinear Schrödinger equation in 3D with powers near three, generalizing cubic case results


<details>
  <summary>Details</summary>
Motivation: To demonstrate that self-similar blowup phenomena are stable under perturbations of the equation parameters

Method: Mathematical analysis of the focusing nonlinear Schrödinger equation in three spatial dimensions with powers close to three, extending previous cubic case methods

Result: Proved the existence of self-similar solutions for the equation with powers near three

Conclusion: Self-similar blowup is stable under perturbations of the equation, providing important insights into blowup behavior in nonlinear Schrödinger equations

Abstract: We consider the focusing nonlinear Schr\"odinger equation in three spatial
dimensions with powers close to three and prove the existence of a self-similar
solution. This generalizes a previous result on the cubic case and shows that
self-similar blowup is stable under perturbations of the equation.

</details>


### [31] [High-capillarity limit and smoothing effect of large solutions for a multi-dimensional generic non-conservative compressible two-fluid model](https://arxiv.org/abs/2509.16607)
*Ling-Yun Shou,Jiayan Wu,Lei Yao,Yinghui Zhang*

Main category: math.AP

TL;DR: Global existence and long-time behavior of large solutions for multidimensional compressible two-fluid models in high-capillarity regime, with improved stability conditions and convergence to incompressible Navier-Stokes flows.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous mathematical foundations for multidimensional compressible two-fluid flows, removing previous restrictive assumptions and providing comprehensive analysis of global solutions and their asymptotic behavior.

Method: Analysis in critical Besov spaces, exploiting interplay between dispersion (two-phase Gross-Pitaevskii structure) and parabolic dissipation induced by capillarity effects, with emphasis on large initial perturbations.

Result: Proved existence and uniqueness of global solutions for large initial data under sharp stability conditions, established convergence to incompressible Navier-Stokes flows with explicit rates, and derived optimal decay rates revealing long-term smoothing effect.

Conclusion: First global large-amplitude strong solutions result for multidimensional compressible two-fluid flows, demonstrating that capillarity effects enable global well-posedness and provide comprehensive understanding of long-time behavior.

Abstract: We investigate the global existence and long-time behavior of large
solutions, in the high-capillarity regime, for a general multidimensional
non-conservative compressible two-fluid model with the capillary pressure
relation \(f(\alpha^{-}\rho^{-})=P^{+}-P^{-}\). Our main contributions are
threefold. First, for sufficiently large capillarity coefficients, we prove the
existence and uniqueness of global solutions in critical Besov spaces for large
initial perturbations, under the sharp stability condition
\(-\frac{s_{-}^{2}(1,1)}{\alpha^{-}(1,1)}<f^{\prime}(1)<0\), thereby removing
the additional negativity restriction assumed by Evje--Wang--Wen [Arch. Ration.
Mech. Anal. 221:1285--1316, 2016]. Second, we give a rigorous justification of
the global-in-time convergence to the incompressible Navier-Stokes flows and
obtain explicit convergence rates in critical spaces for ill-prepared data.
Third, if in addition the initial perturbation lies in a lower-regularity Besov
space, we derive optimal decay rates for the solution and for its derivatives
of any order, revealing a long-term smoothing effect. To the best of our
knowledge, this is the first result on global large-amplitude strong solutions
for multidimensional compressible two-fluid flows. Our analysis exploits the
interplay between dispersion (two-phase Gross--Pitaevskii structure) and
parabolic dissipation, both induced by capillarity effects.

</details>


### [32] [Boundary Hölder gradient estimates for fully nonlinear degenerate or singular parabolic equations](https://arxiv.org/abs/2509.16613)
*Hyungsung Yun*

Main category: math.AP

TL;DR: Boundary regularity analysis for viscosity solutions to fully nonlinear degenerate/singular parabolic equations using compactness methods and barrier constructions.


<details>
  <summary>Details</summary>
Motivation: To address the significant challenges in boundary regularity introduced by gradient-dependent degeneracy/singularity and time derivatives in parabolic equations, which go beyond elliptic case difficulties.

Method: Combining compactness methods, barrier constructions, regularization techniques, and boundary regularity analysis of small perturbation solutions.

Result: Established boundary Hölder gradient estimates that unify and extend previous results.

Conclusion: Successfully developed a comprehensive framework for boundary regularity in fully nonlinear degenerate/singular parabolic equations.

Abstract: We study boundary regularity of viscosity solutions to fully nonlinear
degenerate or singular parabolic equations. The gradient-dependent degeneracy
or singularity, along with the time derivative, introduces significant
challenges beyond the elliptic case. By combining compactness methods, barrier
constructions, regularization techniques, and the boundary regularity of small
perturbation solutions, we establish boundary H\"older gradient estimates that
unify and extend previous results.

</details>


### [33] [Liouville theorem for the inequality $Δ_m u+f(u)\leq 0$ on Riemannian manifolds](https://arxiv.org/abs/2509.16659)
*Biqiang Zhao*

Main category: math.AP

TL;DR: The paper proves nonexistence of positive weak solutions to quasilinear inequalities on Riemannian manifolds under specific volume growth conditions.


<details>
  <summary>Details</summary>
Motivation: To generalize previous results on nonexistence of solutions to quasilinear inequalities by establishing conditions under which no positive weak solutions exist on complete Riemannian manifolds.

Method: The authors study the quasilinear inequality Δₘu + f(u) ≤ 0 with specific conditions on m, α, and f, and analyze volume growth conditions of geodesic balls.

Result: If the volume of geodesic balls grows no faster than Crᵖlnᵠr with specific exponents p and q, then the inequality has no positive weak solution.

Conclusion: The paper generalizes previous nonexistence results by establishing precise volume growth conditions that prevent positive weak solutions to quasilinear inequalities on Riemannian manifolds.

Abstract: In this paper, we study the quasilinear inequality $ \Delta_m u+f(u)\leq 0$
on a complete Riemannian manifold, where \begin{align*} m>1,\alpha>m-1 \quad
and \quad f(t)> 0,\alpha f(t)-tf^{'}(t)\geq 0, \forall t>0. \end{align*} If for
some point $x_0$ and large enough $r$, \begin{align*} vol B_r(x_0)\leq C r^p
ln^q r, \end{align*} where
$p=\frac{m\alpha}{\alpha-(m-1)},q=\frac{m-1}{\alpha-(m-1)}$ and $B_r(x_0) $ is
a geodesic ball of radius $r$ centered at $x_0$, then the inequality possesses
no positive weak solution. This generalizes the result in \cite{AS,Sun}.

</details>


### [34] [Rectifiability of entropy productions for weak solutions of the 2D eikonal equation with supercritical regularity](https://arxiv.org/abs/2509.16692)
*Xavier Lamy,Elio Marconi*

Main category: math.AP

TL;DR: The paper establishes that entropy productions for weak solutions of the eikonal equation concentrate on the 1-rectifiable jump set under Besov regularity assumptions beyond the BV setting, leaving only the borderline case p=3 open.


<details>
  <summary>Details</summary>
Motivation: Weak solutions of the eikonal equation arise as sharp interface limits in physical models like the Aviles-Giga energy. Understanding how entropy productions (distributions divΦ(m)) concentrate on singularities is important for analyzing energy cost and singularity structure.

Method: The authors work under Besov regularity assumptions m ∈ B^{1/p}_{p,∞} for 1 ≤ p < 3, which extends beyond the bounded variation (BV) setting (p=1). They analyze how entropy productions behave under these regularity conditions.

Result: For a large class of entropies, the entropy productions concentrate on the 1-rectifiable jump set of m under the Besov regularity assumption for any 1 ≤ p < 3. This resolves a long-standing conjecture for these cases.

Conclusion: The concentration property holds well beyond the BV setting, with only the borderline case p=3 remaining open. This provides significant progress in understanding the relationship between entropy productions and singularity structure in eikonal equations.

Abstract: Weak solutions $m\colon\Omega\subset\mathbb{R}^2\to\mathbb{R}^2$ of the
eikonal equation \begin{align*} |m|=1\text{ a.e. and }\mathrm{div}\: m =0\,,
\end{align*} arise naturally as sharp interface limits of bounded energy
configurations in various physically motivated models, including the
Aviles-Giga energy. The distributions $\mu_\Phi=\mathrm{div}\,\Phi(m)$, defined
for a class of smooth vector fields $\Phi$ called entropies, carry information
about singularities and energy cost. If these entropy productions are Radon
measures, a long-standing conjecture predicts that they must be concentrated on
the 1-rectifiable jump set of $m$, as they do if $m$ has bounded variation (BV)
thanks to the chain rule. We establish this concentration property, for a large
class of entropies, under the Besov regularity assumption \begin{align*} m\in
B^{1/p}_{p,\infty} \quad \Leftrightarrow \quad \sup_{h\in \mathbb
R^2\setminus\lbrace 0\rbrace} \frac{\|m(\cdot +h)-m\|_{L^p }}{|h|^{1/p}}
<\infty\,, \end{align*} for any $1\leq p<3$, thus going well beyond the BV
setting ($p=1$) and leaving only the borderline case $p=3$ open.

</details>


### [35] [Existence proofs of traveling wave solutions on an infinite strip for the suspension bridge equation and proof of orbital stability](https://arxiv.org/abs/2509.16693)
*Lindsey van der Aalst,Jan Bouwe van den Berg,Matthieu Cadiot*

Main category: math.AP

TL;DR: A computer-assisted method using Fourier analysis to prove existence of traveling wave solutions for suspension bridge equations on infinite strips, with rigorous error control and stability analysis.


<details>
  <summary>Details</summary>
Motivation: To develop a constructive approach for proving existence of traveling wave solutions in suspension bridge equations, which have challenging exponential nonlinearities, using rigorous computational methods.

Method: Uses meticulous Fourier analysis to derive quantifiable approximate inverses for Jacobians, controls aliasing errors in exponential nonlinearities, applies Newton-Kantorovich approach, and employs Fourier series approximations for spectrum analysis.

Result: Successfully proves existence of multiple traveling wave solutions for suspension bridge equations and provides orbital stability analysis through eigenvalue computation.

Conclusion: The methodology provides a rigorous computer-assisted framework for proving existence and stability of traveling wave solutions in challenging PDEs with exponential nonlinearities.

Abstract: In this paper, we present a computer-assisted approach for constructively
proving the existence of traveling wave solutions of the suspension bridge
equation on the infinite strip $\Omega = \mathbb{R} \times (-d_2,d_2)$. Using a
meticulous Fourier analysis, we derive a quantifiable approximate inverse
$\mathbb{A}$ for the Jacobian $D\mathbb{F}(\bar{u})$ of the PDE at an
approximate traveling wave solution $\bar{u}$. Such approximate objects are
obtained thanks to Fourier coefficients sequences and operators, arising from
Fourier series expansions on a rectangle $\Omega_0 = (-d_1,d_1) \times
(-d_2,d_2)$. In particular, the challenging exponential nonlinearity of the
equation is tackled using a rigorous control of the aliasing error when
computing related Fourier coefficients. This allows to establish a
Newton-Kantorovich approach, from which the existence of a true traveling wave
solution of the PDE can be proven in a vicinity of $\bar{u}$. We successfully
apply such a methodology in the case of the suspension bridge equation and
prove the existence of multiple traveling wave solutions on $\Omega$. Finally,
given a proven solution $\tilde{u}$, a Fourier series approximation on
$\Omega_0$ allows us to accurately enclose the spectrum of
$D\mathbb{F}(\tilde{u})$. Such a tight control provides the number of negative
eigenvalues, which in turns, allows to conclude about the orbital (in)stability
of $\tilde{u}$.

</details>


### [36] [Compactness and least energy solutions to the Super-Liouville equation on the Sphere](https://arxiv.org/abs/2509.16712)
*Mingyang Han,Chunqin Zhou*

Main category: math.AP

TL;DR: Analysis of the super-Liouville equation on the sphere with positive coefficients, focusing on spinor estimates, compactness properties, and existence of least energy solutions via variational methods.


<details>
  <summary>Details</summary>
Motivation: To study the super-Liouville equation on the sphere with positive coefficient functions, aiming to understand the energy bounds of spinor components and establish existence results for least energy solutions.

Method: Derived estimates for the spinor component to show uniform energy bounds, analyzed compactness of solution space in two aspects (small energy and conformal transformations), introduced the Nehari manifold constraint, and applied variational methods.

Result: Found that the energy of the spinor part is uniformly bounded, established compactness properties, and proved existence of least energy solutions when coefficient functions are even.

Conclusion: The study successfully demonstrates uniform energy bounds for spinors, compactness of solution spaces, and existence of least energy solutions for the super-Liouville equation on the sphere with even coefficient functions using variational approaches.

Abstract: In this work, we study the super-Liouville equation on the sphere with
positive coefficient functions. We begin by deriving estimates for the spinor
component of the equation, thereby finding that the energy of the spinor part
of solutions is uniformly bounded. We then analyze the compactness of the
solution space in two aspects: the compactness for solutions with small energy
and the compactness with respect to the conformal transformation group of the
sphere. Finally, by introducing a new natural constraint, the Nehari manifold,
and employing variational methods, we obtain the existence of the least energy
solutions when the coefficient functions are even.

</details>


### [37] [Clustered eigenvalue problem for glassy state relaxation and its inverse problem](https://arxiv.org/abs/2509.16714)
*Shuli Chen,Maarten V. de Hoop,Youjun Deng,Ching-Lung Lin,Gen Nakamura*

Main category: math.AP

TL;DR: This paper provides theoretical justification and numerical verification for previous spectral analysis of glass relaxation using Prony series approximation, and solves an inverse spectral problem for eigenvalue clusters.


<details>
  <summary>Details</summary>
Motivation: Previous work by Loreti and Sforza (2019) initiated spectral analysis of glass relaxation using Prony series approximation but left theoretical justification and further numerical studies open. This paper aims to complete this work.

Method: The authors provide complete theoretical justification for previous results and perform numerical verification. They also solve an inverse spectral problem for clusters of eigenvalues associated with glass relaxation.

Result: The paper successfully provides theoretical justification for previous spectral analysis results and verifies them numerically. Additionally, it solves the inverse spectral problem for eigenvalue clusters in glass relaxation.

Conclusion: This work completes the theoretical foundation for spectral analysis of glass relaxation using Prony series approximation, providing both justification and numerical verification for previous results while extending the analysis to solve inverse spectral problems.

Abstract: For computational convenience, a Prony series approximation of the stretched
exponential relaxation function of homogeneous glasses has been proposed (J.
Mauro, Y. Mauro, 2018), which is the extended Burgers model known for
viscoelasticity equations. The authors of [P. Loreti and D. Sforza, 2019]
initiated a spectral analysis of glass relaxation along this line, and gave
some numerical results on clusters of eigenvalues. A theoretical justification
of the results and development of further numerical studies were left open. In
this paper, we provide a complete theoretical justification of their results
and their numerical verification. Besides these, we solve an inverse spectral
problem for clusters of eigenvalues associated with the glass relaxation.

</details>


### [38] [Hasegawa-Mima equation in bounded domain with singular density](https://arxiv.org/abs/2509.16754)
*Franco Flandoli,Yassine Tahraoui*

Main category: math.AP

TL;DR: Analysis of Hasegawa-Mima equation well-posedness in bounded domains with Dirichlet boundary conditions and singular density using regularization methods


<details>
  <summary>Details</summary>
Motivation: To establish the well-posedness of the Hasegawa-Mima equation under challenging conditions including bounded domains, Dirichlet boundary conditions, and singular density profiles

Method: Coupling of fourth-order regularization of HME, spectral Galerkin method, and appropriate regularization of singular density

Result: Uniqueness holds for solutions with mild singularities (Yudovich-type) or with bounded densities

Conclusion: The approach successfully establishes well-posedness for the Hasegawa-Mima equation under the specified challenging conditions

Abstract: We investigate the well-posedness of Hasegawa-Mima equation (HME) in bounded
domain with Dirichlet boundary condition and singular density under different
regularity assumptions on the data. Our approach relies on the coupling of a
fourth-order regularization of the HME, the spectral Galerkin method and an
appropriate regularization of the singular density. Uniqueness holds in the
class of solutions with very mild singularities \`a la Yudovich or with bounded
densities.

</details>


### [39] [The slender body free boundary problem](https://arxiv.org/abs/2509.16800)
*Laurel Ohm*

Main category: math.AP

TL;DR: Analysis of the slender body free boundary problem for an inextensible elastic filament in Stokes fluid, developing mathematical foundation for 3D-1D coupling models.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous mathematical foundations for slender body theories that couple 1D filament elasticity with 3D fluid dynamics, providing theoretical justification for computational models.

Method: Uses the slender body Neumann-to-Dirichlet (NtD) map to couple Euler-Bernoulli beam elasticity with Stokes fluid, extracts principal symbol of NtD map, and solves tension determination problem for inextensibility constraint.

Result: Developed a solution theory for filament evolution under 3D-1D coupling, providing mathematical justification for slender body theories.

Conclusion: This work forms a key development in placing slender body theories on firm theoretical footing, enabling rigorous analysis of filament-fluid interactions.

Abstract: We consider the slender body free boundary problem describing the evolution
of an inextensible, closed elastic filament immersed in a Stokes fluid in
$\mathbb{R}^3$. The filament elasticity is governed by Euler-Bernoulli beam
theory, and the coupling between this 1D elasticity law and the surrounding 3D
fluid is governed by the slender body Neumann-to-Dirichlet (NtD) map, which
treats the filament as a 3D object with constant cross-sectional radius
$0<\epsilon\ll1$. This map serves as a mathematical justification for slender
body theories wherein such 3D-1D couplings play a central role. We develop a
solution theory for the filament evolution under this coupling. Our analysis
relies on two main ingredients: (1) an extraction of the principal symbol of
the slender body NtD map, from the author's previous work, and (2) a detailed
treatment of the tension determination problem for enforcing the
inextensibility constraint. Our work provides a mathematical foundation for
various computational models in which a slender filament evolves according to a
1D elasticity law in a 3D fluid. This forms a key development in our broader
program to place slender body theories on firm theoretical footing.

</details>


### [40] [Solvability of fractional semilinear heat equations with distributional forcing terms in Lorentz spaces](https://arxiv.org/abs/2509.16809)
*Yusuke Oka*

Main category: math.AP

TL;DR: Analysis of Cauchy problem for fractional semilinear heat equation with distributional forcing terms using Besov-type spaces based on Lorentz spaces


<details>
  <summary>Details</summary>
Motivation: To establish necessary and sufficient conditions on forcing terms for local-in-time existence of solutions in Lorentz spaces

Method: Using Besov-type spaces based on Lorentz spaces to analyze the fractional semilinear heat equation with distributional forcing terms

Result: Provides both necessary conditions and sufficient conditions for local-in-time existence of solutions belonging to Lorentz spaces

Conclusion: The paper establishes rigorous mathematical conditions for solution existence in fractional semilinear heat equations with distributional forcing terms

Abstract: We study the Cauchy problem for the fractional semilinear heat equation with
distributional forcing terms. By using Besov-type spaces based on Lorentz
spaces, we give necessary conditions and sufficient conditions on forcing terms
for the local-in-time existence of solutions belonging to Lorentz spaces.

</details>


### [41] [A nonlinear homogenization-based perspective on the soft modes and effective energies of some conformal metamaterials](https://arxiv.org/abs/2509.16907)
*Xuenan Li,Robert V. Kohn*

Main category: math.AP

TL;DR: The paper presents a new technique for identifying soft modes in mechanical metamaterials using homogenization methods, specifically applied to 2D examples like Rotating Squares and Kagome metamaterials.


<details>
  <summary>Details</summary>
Motivation: There is growing interest in understanding the macroscopic properties of mechanism-based mechanical metamaterials, which mathematically corresponds to a homogenization problem in nonlinear elasticity. A key challenge is identifying the 'soft modes' of these metamaterials.

Method: The authors develop a new technique for bounding the effective energy from below, leveraging the metamaterial's structure and symmetry. They apply homogenization methods to specific 2D examples including discrete models of Rotating Squares and Kagome metamaterials.

Result: The method successfully identifies soft modes that correspond to compressive conformal maps in the studied metamaterials.

Conclusion: The new bounding technique provides an effective approach for identifying soft modes in mechanical metamaterials, with successful application to 2D examples demonstrating compressive conformal map behavior.

Abstract: There is a growing mechanics literature concerning the macroscopic properties
of mechanism-based mechanical metamaterials. This amounts mathematically to a
homogenization problem involving nonlinear elasticity. A key goal is to
identify the "soft modes" of the metamaterial. We achieve this goal using
methods from homogenization for some specific 2D examples -- including discrete
models of the Rotating Squares metamaterial and the Kagome metamaterial --
whose soft modes are compressive conformal maps. The innovation behind this
achievement is a new technique for bounding the effective energy from below,
which takes advantage of the metamaterial's structure and symmetry.

</details>


### [42] [Global well-posedness for the 3D compressible Navier-Stokes equations in optimal Besov space](https://arxiv.org/abs/2509.17005)
*Zihua Guo,Zihao Song,Minghua Yang*

Main category: math.AP

TL;DR: Global well-posedness for 3D barotropic compressible Navier-Stokes equations with small initial data in critical Besov spaces, extending previous results to optimal range p<6.


<details>
  <summary>Details</summary>
Motivation: To establish global existence and uniqueness of solutions for compressible Navier-Stokes equations under minimal regularity assumptions, improving upon previous restrictions where p<4 was required.

Method: Uses a novel nonlinear transform combining momentum formulation for low-frequency and effective velocity method for high frequency, along with L^q-framework estimates for parabolic-dispersive semigroup.

Result: Proves global well-posedness for initial data with small norms in critical Besov space for 2≤p<6, with additional low frequency condition on (ρ₀-1,ρ₀u₀).

Conclusion: Extends previous results to the optimal range p<6, providing a more complete theory for compressible Navier-Stokes equations in critical Besov spaces.

Abstract: We consider the Cauchy problem to the 3D barotropic compressible
Navier-Stokes equation. We prove global well-posedness, assuming that the
initial data $(\rho_0-1,u_0)$ has small norms in the critical Besov space
$\mathbb{X}_p=\dot{B}_{p,1}^{3/p}(\mathbb{R}^3)\times
\dot{B}_{p,1}^{-1+3/p}(\mathbb{R}^3)$ for $2\leq p<6$ and
$(\rho_0-1,\rho_0u_0)$ satisfies an additional low frequency condition. Our
results extend the previous results in \cite{FD2010, CMZ2010, H20112} where
$p<4$ is needed for high frequency, to the optimal range $p<6$. The main
ingredients of the proof consist of: a novel nonlinear transform that uses
momentum formulation for low-frequency and effective velocity method for high
frequency, and estimate of parabolic-dispersive semigroup that enables a
$L^q$-framework for low frequency.

</details>


### [43] [Sharp Onofri trace inequality on the upper half space and quasi-linear Liouville equation with Neumann boundary](https://arxiv.org/abs/2509.17031)
*Jingbo Dou,Yazhou Han,Shuang Yuan,Yang Zhou*

Main category: math.AP

TL;DR: This paper establishes a sharp Onofri trace inequality on the upper half space and classifies its extremal functions using Sobolev trace inequality limits, Serrin-Zou type identities, and Pohozaev type identities.


<details>
  <summary>Details</summary>
Motivation: To derive a sharp Onofri trace inequality on the upper half space by examining the limiting case of Sobolev trace inequality and to classify extremal functions in a weighted Sobolev space.

Method: Uses Serrin-Zou type identity and Pohozaev type identity to classify solutions for a quasi-linear Liouville equation with Neumann boundary conditions, which is linked to the Euler-Lagrange equation of the Onofri trace inequality.

Result: Establishes a sharp Onofri trace inequality and classifies its extremal functions, with independent research value from the quasi-linear Liouville equation analysis.

Conclusion: The regularity and asymptotic estimates of solutions to the quasi-linear Liouville equation are crucial for the discussion and classification of extremal functions in the Onofri trace inequality.

Abstract: In this paper, we establish a sharp Onofri trace inequality on the upper half
space $\overline{\mathbb R_+^n} (n\geq 2)$ by considering the limiting case of
Sobolev trace inequality and classify its extremal functions on a suitable
weighted Sobolev space. For this aim, by the Serrin-Zou type identity and the
Pohozaev type identity, we show the classification of the solutions for a
quasi-linear Liouville equation with Neumann boundary which is closely related
to the Euler-Lagrange equation of the Onofri trace inequality and it has
independent research value. The regularity and asymptotic estimates of
solutions to the above equation are essential to discuss.

</details>


### [44] [Global classical solutions to a two-dimensional chemotaxis-fluid system involving signal-dependent degenerate diffusion](https://arxiv.org/abs/2509.17073)
*Yansheng Ma,Peter Y. H. Pang,Yifu Wang*

Main category: math.AP

TL;DR: This paper analyzes a 2D chemotaxis-fluid model with signal-dependent motilities, showing global existence of classical solutions with small initial mass conditions and convergence to equilibrium states.


<details>
  <summary>Details</summary>
Motivation: To extend recent results from fluid-free chemotaxis systems to models incorporating Navier-Stokes fluid environments, accounting for microbial population interactions with incompressible liquids through transport and buoyancy effects.

Method: Analysis of a coupled system of PDEs describing chemotaxis-fluid interactions, using mathematical techniques to prove global existence and boundedness of classical solutions under various parameter conditions (μ=0 vs μ>0).

Result: For μ=0, global classical solutions exist with smallness condition on initial cell mass; for μ>0, global bounded classical solutions exist and can converge to (1,0,0) equilibrium under small initial signal mass conditions.

Conclusion: The results successfully extend fluid-free chemotaxis system findings to Navier-Stokes fluid environments, demonstrating robust mathematical properties of the coupled chemotaxis-fluid model under appropriate conditions.

Abstract: This paper is concerned with the two-dimensional chemotaxis-fluid model
\begin{equation*} \begin{cases} n_t+u\cdot\nabla n=\Delta (n\phi(v))+\mu
n(1-n),\\ v_t+u\cdot\nabla v=\Delta v-nv,\\ u_t+ \kappa (u\cdot\nabla) u=\Delta
u+n\nabla\Phi-\nabla P, \quad\nabla\cdot u=0, \end{cases} \end{equation*}
  accounting for signal-dependent motilities of microbial populations
  interacting with an incompressible liquid through transport and buoyancy,
where the suitably smooth function $\phi$ satisfies $\phi>0$ on $(0,\infty)$
with $\phi(0)=0$ and $\phi'(0)>0$, and the parameter $\mu\geq 0$. For all
reasonably regular initial data, if $\mu=0$, the corresponding initial boundary
value problem possesses global classical solutions with a smallness condition
on $\int_\Omega n_0$; whereas if $\mu>0$, this problem possesses global bounded
classical solutions, which can converge toward (1,0,0) as time tends to
infinity when a certain small mass is imposed on the initial data $v_0$. These
results extend recent results for the fluid-free system to one in a
Navier-Stokes fluid environment.

</details>


### [45] [Proving the existence of localized patterns and saddle node bifurcations in 1D activator-inhibitor type models](https://arxiv.org/abs/2509.17099)
*Dominic Blanco,Matthieu Cadiot,Daniel Fassler*

Main category: math.AP

TL;DR: A framework for computer-assisted proofs of existence and stability of stationary localized 1D solutions and saddle-node bifurcations in activator-inhibitor systems using Newton-Kantorovich approach.


<details>
  <summary>Details</summary>
Motivation: To develop rigorous computational methods for proving existence, stability, and bifurcations of patterns in biological systems where analytical proofs are challenging.

Method: Constructive proof framework using Newton-Kantorovich approach with approximate inverses of linearization, rigorous error bounds, and spectral control for bifurcation analysis.

Result: Successfully proved existence and stability of multiple steady-state patterns in various activator-inhibitor systems and a saddle-node bifurcation in the Glycolysis model.

Conclusion: The framework provides effective computer-assisted proof methodology for rigorous analysis of pattern formation and bifurcations in complex biological systems.

Abstract: In this paper, we present a general framework for constructively proving the
existence and stability of stationary localized 1D solutions and saddle-node
bifurcations in activator--inhibitor systems using computer-assisted proofs.
Specifically, we develop the necessary analysis to compute explicit upper
bounds required in a Newton--Kantorovich approach. Given an approximate
solution $\bar{\mathbf{u}}$, this approach relies on establishing that a
well-chosen fixed point map is contracting on a neighborhood
$\bar{\mathbf{u}}$. For this matter, we construct an approximate inverse of the
linearization around $\bar{\mathbf{u}}$, and establish sufficient conditions
under which the contraction is achieved. This provides a framework for which
computer-assisted analysis can be applied to verify the existence and local
uniqueness of solutions in a vicinity of $\bar{\mathbf{u}}$, and control the
linearization around $\bar{\mathbf{u}}$. Furthermore, we extend the method to
rigorously establish saddle-node bifurcations of localized solutions for the
same type of models, by considering a well--chosen zero--finding problem. This
depends on the rigorous control of the spectrum of the linearization around the
bifurcation point. Finally, we demonstrate the effectiveness of the framework
by proving the existence and stability of multiple steady-state patterns in
various activator--inhibitor systems, as well as a saddle--node bifurcation in
the Glycolysis model.

</details>


### [46] [Almost sure global weak solutions and optimal decay for the incompressible generalized Navier-Stokes equations](https://arxiv.org/abs/2509.17171)
*Y. -X. Lin,Y. -G. Wang*

Main category: math.AP

TL;DR: Analysis of global weak solutions for incompressible generalized Navier-Stokes equations with fractional Laplacian and randomized initial data in negative Sobolev spaces.


<details>
  <summary>Details</summary>
Motivation: To study the existence and properties of solutions for generalized Navier-Stokes equations with fractional dissipation when initial data is in negative order Sobolev spaces, which extends classical results to more singular initial conditions.

Method: Randomization of initial data in negative Sobolev spaces and analysis of generalized Navier-Stokes equations with fractional Laplacian operator -(-Δ)^α, where α ∈ (1/2, (d+2)/4] in ℝ^d with d ≥ 2.

Result: Almost sure existence and optimal decay rate of global weak solutions for initial data in Ḣ^s(ℝ^d) with s ∈ (-α+(1-α)_+,0). Uniqueness of weak solutions when α=(d+2)/4 with d ≥ 2.

Conclusion: The randomization technique enables the construction of global weak solutions with optimal decay properties for generalized Navier-Stokes equations with fractional dissipation, even for initial data in negative Sobolev spaces, with uniqueness established in the critical case α=(d+2)/4.

Abstract: In this paper, we consider the initial value problem of the incompressible
generalized Navier-Stokes equations with initial data being in negative order
Sobolev spaces, in the whole space $\mathbb{R}^d$ with $d \geq 2$. The
generalized Navier-Stokes equations studied here is obtained by replacing the
standard Laplacian in the classical Navier-Stokes equations by the fractional
order Laplacian $-(-\Delta)^\al$ with $\al \in \left( \frac{1}{2},\frac{d+2}{4}
\right]$. After an appropriate randomization on the initial data, we obtain the
almost sure existence and optimal decay rate of global weak solutions when the
initial data belongs to $\Dot{H}^s(\mathbb{R}^d)$ with $s\in
(-\al+(1-\al)_+,0)$. Moreover, we show that the weak solutions are unique when
$\al=\frac{d+2}{4}$ with $d \geq 2$.

</details>


### [47] [Optimizers of the Finite-Rank Hardy-Lieb-Thirring Inequality for Hardy-Schrödinger Operator](https://arxiv.org/abs/2509.17307)
*Bin Chen,Yujin Guo,Shuang Wu*

Main category: math.AP

TL;DR: Analysis of finite-rank Hardy-Lieb-Thirring inequality for Hardy-Schrödinger operators, focusing on existence and properties of optimizers.


<details>
  <summary>Details</summary>
Motivation: To study the Hardy-Lieb-Thirring inequality for Hardy-Schrödinger operators with finite-rank potentials, particularly examining when optimizers exist and their analytical characteristics.

Method: Mathematical analysis of the inequality involving Hardy-Schrödinger operators, using min-max level theory and eigenvalue analysis in ℝ^d with d≥3.

Result: The paper analyzes the conditions under which optimizers exist for the finite-rank Hardy-Lieb-Thirring inequality and studies their analytical properties.

Conclusion: Provides insights into the existence and properties of optimizers for Hardy-Lieb-Thirring inequalities, contributing to the understanding of spectral properties of Hardy-Schrödinger operators.

Abstract: We study the following finite-rank Hardy-Lieb-Thirring inequality of
Hardy-Schr\"odinger operator: \begin{equation*}
  \sum_{i=1}^N\left|\lambda_i\Big(-\Delta-\frac{c}{|x|^2}-V\Big)\right|^s\leq
C_{s,d}^{(N)}\int_{\mathbb R^d}V_+^{s+\frac d2}dx, \end{equation*} where
$N\in\mathbb N^+$, $d\geq3$, $0<c\leq c_*:=\frac{(d-2)^2}{4}$, $c_*>0$ is the
best constant of Hardy's inequality, and $V\in L^{s+\frac d2}(\mathbb R^d)$
holds for $s>0$. Here $\lambda_i\big(-\Delta-{c}{|x|^{-2}}-V\big)$ denotes the
$i$-th min-max level of Hardy-Schr\"odinger operator
$H_{c,V}:=-\Delta-{c}{|x|^{-2}}-V $ in $\mathbb R^d$, which equals to the
$i$-th negative eigenvalue (counted with multiplicity) of $H_{c,V}$ in $\mathbb
R^d$ if it exists, and vanishes otherwise. We analyze the existence and
analytical properties of the optimizers for the above inequality.

</details>


### [48] [The Asymptotic Analysis of Some PDE and Steklov Eigenvalue Problems with Partially Reactive Patches in 3-D](https://arxiv.org/abs/2509.17394)
*Denis S. Grebenkov,Michael J. Ward*

Main category: math.AP

TL;DR: This paper analyzes steady-state diffusion in 3D bounded domains with partially reactive patches on reflecting boundaries, using matched asymptotic expansions to study competition between patches and surface reactions.


<details>
  <summary>Details</summary>
Motivation: To understand how small reactive patches on boundaries compete for diffusing particles and the role of surface reactions, extending classical diffusion-reaction problems to more complex scenarios.

Method: Uses matched asymptotic expansions to derive asymptotic expansions for mean first-reaction time and splitting probabilities in the small-patch limit, particularly for spherical domains with various boundary conditions.

Result: Derives three-term asymptotic expansions for MFRT and splitting probabilities valid for arbitrary reactivities, and provides first asymptotic analysis of eigenvalues/eigenfunctions for mixed Steklov-Neumann problems in small-patch limit.

Conclusion: The approach successfully extends classical diffusion-reaction theory to handle complex surface reactions and patch configurations, with potential applications to arbitrary domains and physical systems.

Abstract: We consider steady-state diffusion in a three-dimensional bounded domain with
a smooth reflecting boundary that is partially covered by small partially
reactive patches. By using the method of matched asymptotic expansions, we
investigate the competition of these patches for a diffusing particle and the
crucial role of surface reactions on these targets. After a brief overview of
former contributions to this field, we first illustrate our approach by
considering the classical problems of the mean first-reaction time (MFRT) and
the splitting probability for partially reactive patches characterized by a
Robin boundary condition. For a spherical domain, we derive a three-term
asymptotic expansion for the MFRT and splitting probabilities in the
small-patch limit. This expansion is valid for arbitrary reactivities, and also
accounts for the effect of the spatial configuration of patches on the
boundary. Secondly, we consider more intricate surface reactions modeled by
mixed Steklov-Neumann or Steklov-Neumann-Dirichlet problems. We provide the
first derivation of the asymptotic behavior of the eigenvalues and
eigenfunctions for these spectral problems in the small-patch limit for a
spherical domain. Extensions of these asymptotic results to arbitrary domains
and their physical applications are discussed.

</details>


### [49] [On the vanishing viscosity limit of Hamilton-Jacobi equations with nearly optimal discount](https://arxiv.org/abs/2509.17402)
*Zibo Wang,Jianlu Zhang*

Main category: math.AP

TL;DR: Convergence of solutions to viscous Hamilton-Jacobi equation with Tonelli Hamiltonian as λ→0+ when ε(λ)/λ→0


<details>
  <summary>Details</summary>
Motivation: To establish the convergence conditions for solutions to viscous Hamilton-Jacobi equations as the parameter λ approaches zero, determining the optimal exponent for ε(λ)

Method: Analysis of the viscous Hamilton-Jacobi equation with Tonelli Hamiltonian, studying the limit behavior as λ→0+ under specific conditions on ε(λ)

Result: The convergence holds when the upper limit of ε(λ)/λ as λ→0+ is zero, and this exponent is nearly optimal for convergence

Conclusion: The established condition ε(λ)/λ→0 is nearly optimal for ensuring convergence of solutions to the viscous Hamilton-Jacobi equation as λ approaches zero

Abstract: In this paper, we establish the convergence of solutions to the viscous
Hamilton-Jacobi equation (with a Tonelli Hamiltonian): \[
  \lambda u +H(x, du)=\varepsilon(\lambda)\Delta u,\quad \lambda>0 \]
  as $\lambda\rightarrow 0_+$, once the modulus $\varepsilon(\lambda)$
satisfies $\varlimsup_{\lambda\rightarrow 0_+}\varepsilon(\lambda)/\lambda=0$.
Such an exponent of $\varepsilon(\lambda)$ is nearly optimal in the
convergence.

</details>


### [50] [On the long-time behavior of mean field game systems with a common noise](https://arxiv.org/abs/2509.17443)
*Pierre Cardaliaguet,Raphaël Maillet,Wenbin Yan*

Main category: math.AP

TL;DR: Analysis of long-time behavior of mean field game systems with common noise, proving exponential convergence to stationary regimes using quantitative methods and backward stochastic PDE estimates.


<details>
  <summary>Details</summary>
Motivation: To extend classical deterministic MFG convergence results to stochastic settings with common noise, addressing the complexity introduced by stochastic perturbations in infinite-player games.

Method: Employ quantitative methods replacing classical compactness arguments, use backward stochastic PDE estimates, analyze the ergodic master equation as the long-time limit of the master equation.

Result: Proved exponential convergence of solutions toward a stationary regime, identified deterministic ergodic constant, demonstrated existence of stationary random processes capturing limiting behavior.

Conclusion: Successfully extended deterministic MFG convergence phenomena to stochastic settings, establishing almost sure long-time results through detailed analysis of the ergodic master equation.

Abstract: In this paper, we study the long-time behavior of mean field game (MFG)
systems influenced by a common noise. While classical results establish the
convergence of deterministic MFG towards stationary solutions under suitable
monotonicity conditions, the introduction of a common stochastic perturbation
significantly complicates the analysis. We consider a standard MFG model with
infinitely many players whose dynamics are subject to both idiosyncratic and
common noise. The central goal is to characterize the asymptotic properties as
the horizon goes to infinity. By employing quantitative methods that replace
classical compactness arguments unavailable in the stochastic context, we prove
that solutions exhibit exponential convergence toward a stationary regime.
Specifically, we identify a deterministic ergodic constant and demonstrate the
existence of stationary random processes capturing the limiting behavior.
Further, we establish almost sure long-time results thanks to a detailed
analysis of the ergodic master equation, which is the long-time limit of the
master equation. Our results extend known deterministic convergence phenomena
to the stochastic setting, relying on novel backward stochastic PDE estimates.

</details>


### [51] [On a zero mass Schrödinger-Bopp-Podolsky system: ground states, nonexistence results and asymptotic behaviour](https://arxiv.org/abs/2509.17479)
*Alessio Pomponio,Lianfeng Yang*

Main category: math.AP

TL;DR: This paper studies a zero mass Schrödinger-Bopp-Podolsky system in R³, providing existence of ground state solutions for p ∈ (4,6) using Mountain Pass Theorem, nonexistence results via Pohozaev identity, and asymptotic analysis linking to Schrödinger-Poisson system.


<details>
  <summary>Details</summary>
Motivation: To complete the study initiated in [2] by providing a direct approach for ground state solutions rather than relying on perturbation arguments, and to establish connections with the Schrödinger-Poisson system.

Method: Uses Mountain Pass Theorem and splitting lemma for existence proofs, derives Pohozaev identity for nonexistence results, and employs minimax characterization for asymptotic analysis as a→0 in the radial case.

Result: Establishes existence of ground state solutions for p ∈ (4,6), obtains nonexistence results for certain p values, and shows convergence to solutions of the zero mass Schrödinger-Poisson system as a→0.

Conclusion: The paper provides a comprehensive analysis of the Schrödinger-Bopp-Podolsky system, bridging the gap with previous perturbation-based approaches and connecting it to the Schrödinger-Poisson framework through asymptotic behavior.

Abstract: In this paper, we consider the following zero mass
Schr\"{o}dinger-Bopp-Podolsky system
  \[
  \begin{cases}
  -\Delta u +q^2\phi u=|u|^{p-2}u,
  -\Delta \phi+a^2\Delta^2\phi=4\pi u^2,
  \end{cases}
  \text{ in } \mathbb{R}^3,
  \]
  where $a>0$ and $q\ne 0$. We complete the study initiated in [2], which
relied on a perturbation argument to establish the existence of weak solutions.
Here, in contrast, our approach, based on the Mountain Pass Theorem and the
splitting lemma, directly yields a ground state solution for $p \in (4,6)$.
  Moreover, by deriving a Pohozaev identity, we further obtain some
nonexistence results for suitable $p$. Finally, based on the minimax
characterization, we also analyse, in the radial case, the asymptotic behaviour
of the solutions obtained as $a\to 0$, thereby establishing a link with the
zero mass Schr\"odinger-Poisson system.

</details>


### [52] [Reverse Faber-Krahn inequality for planar doubly connected domains](https://arxiv.org/abs/2509.17480)
*T. V. Anoop,Vladimir Bobkov,Mrityunjoy Ghosh*

Main category: math.AP

TL;DR: This paper proves that concentric annular membranes maximize the fundamental frequency among all doubly connected planar membranes with prescribed area and boundary lengths, under certain elastic support conditions.


<details>
  <summary>Details</summary>
Motivation: The study extends and unifies existing results on membrane vibration optimization, addressing cases with elastic boundary conditions that can include negative values and fixation.

Method: For h_in·h_out = 0, uses Payne & Weinberger's method of interior parallels. For h_in·h_out > 0, develops the 'effectless cut' construction based on gradient flow of the first eigenfunction, building on Weinberger and Hersch's work.

Result: Proves that concentric annular membranes have maximal fundamental frequency under the given constraints and boundary conditions.

Conclusion: The research provides a comprehensive framework for understanding membrane vibration optimization, unifying previous results and handling various elastic boundary conditions including negative values and fixed boundaries.

Abstract: We prove that among all doubly connected and elastically supported planar
membranes $\Omega$ with prescribed values of the area $|\Omega|$ and the
lengths of the inner and outer boundaries $|\partial \Omega_{\rm{in}}|_1$,
$|\partial \Omega_{\rm{out}}|_1$ satisfying $|\partial \Omega_{\rm{out}}|_1^2 -
|\partial \Omega_{\rm{in}}|_1^2 = 4\pi |\Omega|$, the concentric annular
membrane has the maximal fundamental frequency. The elastic constants
$h_{\rm{in}}$, $h_{\rm{out}}$ on $\partial \Omega_{\rm{in}}$, $\partial
\Omega_{\rm{out}}$, respectively, are assumed to satisfy $h_{\rm{in}} \cdot
h_{\rm{out}} \geq 0$ and can admit negative values and $+\infty$, the latter
being understood as a fixation of the membrane on the corresponding part of the
boundary. Our study extends and unifies several existing results in the
literature. The case $h_{\rm{in}} \cdot h_{\rm{out}} = 0$ is proved using the
method of interior parallels \`a la Payne & Weinberger, and it requires less
restrictive assumptions on $\Omega$. For the case $h_{\rm{in}} \cdot
h_{\rm{out}} > 0$, we develop the construction of the so-called ``effectless
cut'' of $\Omega$ described in terms of the gradient flow of the first
eigenfunction. This concept was originally introduced by Weinberger and used by
Hersch in the fixed boundary case, whose arguments we also revise.

</details>


### [53] [On the spectral stability of the periodic capillary-gravity waves](https://arxiv.org/abs/2509.17534)
*Changzhen Sun,Erik Wahlén*

Main category: math.AP

TL;DR: This paper investigates spectral stability of periodic traveling waves in 2D gravity-capillary water waves, deriving a stability criterion based on an index function and showing that surface tension has a stabilizing effect.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous spectral stability criteria for periodic gravity-capillary water waves, building on earlier formal analyses that used nonlinear Schrödinger approximations.

Method: Derived a stability criterion using an index function, analyzed spectral crossings and Krein signatures, and conducted numerical studies to identify stability boundaries.

Result: Found that instability is excluded near spectral crossings when surface tension is positive and α∈(0,1), and identified a decreasing curve β=β(α) where waves are spectrally stable for β>β(α).

Conclusion: Surface tension has a stabilizing effect on periodic capillary-gravity waves, with spectral stability guaranteed above a certain threshold curve in the parameter space.

Abstract: In this paper, we investigate the spectral stability of periodic traveling
waves in the two dimensional gravity-capillary water wave problem. We derive a
stability criterion based on an index function, whose sign determines the
spectral stability of the waves. This result aligns with earlier formal
analyses by Djordjevi\'c \& Redekopp [15] and Ablowitz \& Segur [1], which
employed the nonlinear Schr\"odinger approximation in the modulational regime.
In particular, we show that instability is excluded near spectral crossings
away from the origin when the surface tension is positive and the inverse
square of the Froude number $\alpha\in(0,1),$ which results from the fact that
the corresponding Krein signatures are identical. Numerical studies further
reveal the existence of a decreasing curve $\beta=\beta(\alpha):
(0,1)\rightarrow \mathbb{R}_{+},$ such that the periodic waves are spectrally
stable for all $\beta>\beta(\alpha)$. These findings highlight the stabilizing
effect of surface tension on periodic capillary-gravity waves.

</details>


### [54] [Eyring-Kramers formula for the mean exit time of non-Gibbsian elliptic processes: the non characteristic boundary case](https://arxiv.org/abs/2509.17678)
*Dorian Le Peutrec,Laurent Michel,Boris Nectoux*

Main category: math.AP

TL;DR: Derives a sharp asymptotic equivalent for mean exit time of non-reversible processes in small temperature regime, with a correction term for non-Gibbsian processes.


<details>
  <summary>Details</summary>
Motivation: To analyze mean exit times for non-reversible stochastic processes without assuming Gibbsian behavior, which requires new mathematical approaches.

Method: Uses spectral and semi-classical analysis tools to derive asymptotic equivalents under generic orthogonal decomposition of the drift term and non-characteristic boundary conditions.

Result: Obtains a new sharp asymptotic equivalent for mean exit time that includes a correction term characterizing the non-Gibbsian nature of the process.

Conclusion: The work provides a mathematical framework for analyzing exit times of non-Gibbsian processes, revealing how non-reversibility affects escape dynamics in small temperature regimes.

Abstract: In this work, we derive a new sharp asymptotic equivalent in the small
temperature regime $h\to 0$ for the mean exit time from a bounded domain for
the non-reversible process $dX\_t=b(X\_t)dt + \sqrt h \, dB\_t$ under a generic
orthogonal decomposition of $b$ and when the boundary of $\Omega$ is assumed to
be \textit{non characteristic}. The main contribution of this work lies in the
fact that we do not assume that the process $(X\_t,t\ge 0)$ is
\textit{Gibbsian}. In this case, a new correction term characterizing the
\textit{non-Gibbsianness} of the process appears in the equivalent of the mean
exit time. The proof is mainly based on tools from spectral and semi-classical
analysis.

</details>


### [55] [A survey on the resolvent convergence](https://arxiv.org/abs/2509.17687)
*Joaquim Duran*

Main category: math.AP

TL;DR: Analysis of resolvent convergence for self-adjoint operators and its relationship with spectral convergence and other convergence notions


<details>
  <summary>Details</summary>
Motivation: To understand how different convergence concepts for unbounded self-adjoint operators relate to each other and to spectral convergence

Method: Theoretical mathematical analysis exploring relationships between resolvent convergence, strong graph limit, G-convergence, and Γ-convergence for self-adjoint operators

Result: Establishes connections between various convergence notions and their implications for spectral behavior

Conclusion: Provides a comprehensive framework for understanding operator convergence and its spectral consequences

Abstract: This chapter deals with the notion of the resolvent of a self-adjoint
operator. We pay special attention to the convergence of unbounded self-adjoint
operators in several resolvent senses, and how they are related to the
convergence of their spectra. We also explore the relations that these notions
of convergence have with the so-called strong graph limit, $G$-convergence, and
$\Gamma$-convergence.

</details>


### [56] [Boundary pointwise regularity for the divergence form elliptic boundary problem on uniform domain](https://arxiv.org/abs/2509.17690)
*Tianyu Guan,Lihe Wang,Chunqin Zhou*

Main category: math.AP

TL;DR: This paper studies boundary pointwise regularity for divergence form elliptic boundary problems on rough domains, introducing a new definition of weak solutions that enables regularity analysis.


<details>
  <summary>Details</summary>
Motivation: Traditional weak solution definitions are inconvenient for nonzero boundary data on rough domains like uniform domains. The paper aims to develop a framework for analyzing boundary regularity in such challenging settings.

Method: Introduces a new definition of weak solutions for boundary problems on uniform domains, establishes energy inequality, uses compactness methods under admissible conditions, and analyzes linear properties of solutions relative to harmonic functions.

Result: Proves boundary pointwise C^α regularity using energy inequality and compactness methods. Also establishes boundary pointwise C^{1,α} and C^{2,α} regularities when boundary data and domain boundary are pointwise C^{1,α} and C^{2,α} respectively.

Conclusion: The proposed definition of weak solutions enables systematic analysis of boundary regularity for elliptic boundary problems on rough domains, providing a framework for studying higher-order boundary regularity under appropriate smoothness conditions.

Abstract: In this paper, we study the boundary pointwise regularity for the divergence
form elliptic boundary problem. In generality, it is not convenient to define
weak solutions for nonzero boundary data on domain with the rough boundary,
e.g. uniform domain. However, in this paper, we introduce a definition of weak
solutions for the boundary problem on uniform domain. What is interesting is
that this definition can be considered to analysis the regularity of weak
solutions. In particular, by establishing the energy inequality, we show the
boundary pointwise $C^\alpha$ regularity by using compactness methods under the
admissible condition. Furthermore, by establishing the linear property of
solutions with respective to the harmonic functions, we also prove the boundary
pointwise $C^{1,\alpha}$ and $C^{2,\alpha}$ regularities if the boundary data
and the boundary of domain are pointwise $C^{1,\alpha}$ and $C^{2,\alpha }$
respectively.

</details>


### [57] [Nonlocal Electrostatics and Boundary Charges in Continuum Limits of Two-Dimensional Materials](https://arxiv.org/abs/2509.17744)
*Shoham Sen,Yang Wang,Timothy Breitzman,Kaushik Dayal*

Main category: math.AP

TL;DR: This paper develops a multiscale method to homogenize the electrostatic potential of 2D electronic materials by using polarization density as a mediator, addressing the challenge of atomic-scale charge density oscillations in coarse-graining.


<details>
  <summary>Details</summary>
Motivation: The motivation is to bridge the gap between nanoscale physics and macroscopic engineering applications for 2D materials by developing efficient coarse-graining methods that can handle the rapid spatial oscillations of charge density at atomic scales.

Method: The method involves using polarization density as a multiscale mediator, considering three distinct continuum limits where lattice spacing vanishes: thickness much smaller than, comparable to, and much larger than in-plane lattice spacing. The approach obtains homogenized potentials through limiting procedures.

Result: The results show that homogenized potentials can be expressed in terms of boundary charge and dipole distributions with appropriate boundary conditions. Despite intrinsic non-uniqueness in polarization definition, accounting for boundary charges ensures unique determination of electrostatic potential, electric field, and energy.

Conclusion: The polarization density field serves as an effective multiscale mediator for coarse-graining 2D materials, enabling continuum modeling by capturing electronic properties at larger scales while maintaining physical consistency through proper boundary charge treatment.

Abstract: Two-dimensional (2D) electronic materials are of significant technological
interest due to their exceptional properties and broad applicability in
engineering. The transition from nanoscale physics, that dictates their stable
configurations, to macroscopic engineering applications requires the use of
multiscale methods to systematically capture their electronic properties at
larger scales. A key challenge in coarse-graining is the rapid and
near-periodic variation of the charge density, which exhibits significant
spatial oscillations at the atomic scale. Therefore, the polarization density
field -- the first moment of the charge density over the periodic unit cell --
is used as a multiscale mediator that enables efficient coarse-graining by
exploiting the almost-periodic nature of the variation. Unlike the highly
oscillatory charge density, the polarization varies over lengthscales that are
much larger than the atomic, making it suitable for continuum modeling.
  In this paper, we investigate the electrostatic potential arising from the
charge distribution of 2D materials. Specifically, we consider a sequence of
problems wherein the underlying lattice spacing vanishes and thus obtain the
continuum limit. We consider 3 distinct limits: where the thickness is much
smaller than, comparable to, and much larger than the in-plane lattice spacing.
These limiting procedures provide the homogenized potential expressed in terms
of the boundary charge and dipole distribution, subject to the appropriate
boundary conditions that are also obtained through the limit process. Further,
we demonstrate that despite the intrinsic non-uniqueness in the definition of
polarization, accounting for the boundary charges ensures that the total
electrostatic potential, the associated electric field, and the corresponding
energy of the homogenized system are uniquely determined.

</details>


### [58] [On the mean-field limit for the for the Vlasov-Poisson system in two dimensions](https://arxiv.org/abs/2509.17821)
*Feistl-Held Manuela,Pickl Peter*

Main category: math.AP

TL;DR: Probabilistic proof of mean-field limit and propagation of chaos for N-particle system with Coulomb interaction, showing convergence to Vlasov-Poisson system.


<details>
  <summary>Details</summary>
Motivation: To derive the Vlasov-Poisson equation from microscopic N-particle dynamics with Coulomb-like forces, bridging the gap between particle systems and continuum descriptions.

Method: Uses Gronwall estimate for maximal distance between exact microscopic dynamics and approximate mean-field dynamics, with N-dependent cut-off at |q|>N^{-2}.

Result: Shows convergence of Newtonian trajectories to Vlasov-Poisson characteristics for typical initial data, with force term arbitrarily close to Coulomb force.

Conclusion: Provides rigorous derivation of Vlasov-Poisson equation from microscopic particle dynamics, establishing mean-field limit for Coulomb-interacting systems.

Abstract: We present a probabilistic proof of the mean-field limit and propagation of
chaos of a classical N-particle system in two dimensions with Coulomb
interaction force of the form $f^N(q)=\pm\frac{q}{|q|^2}$ and $N$-dependent
cut-off at $|q|>N^{-2}$. In particular, for typical initial data, we show
convergence of the Newtonian trajectories to the characteristics of the
Vlasov-Poisson system. The proof is based on a Gronwall estimate for the
maximal distance between the exact microscopic dynamics and the approximate
mean-field dynamics. Thus our result leads to a derivation of the
Vlasov-Poisson equation from the microscopic $N$-particle dynamics with force
term arbitrary close to the physically relevant Coulomb force.

</details>


### [59] [Motion of a massive rigid loop in a 3D perfect incompressible flow](https://arxiv.org/abs/2509.17881)
*Olivier Glass,David Meyer,Franck Sueur*

Main category: math.AP

TL;DR: This paper identifies a 3D equivalent of the Kutta-Joukowski effect for rigid bodies in inviscid incompressible fluids, focusing on slender tubular domains with circular cross-sections in the zero-radius limit.


<details>
  <summary>Details</summary>
Motivation: To extend the understanding of fluid-structure interaction from 2D to 3D, particularly the Kutta-Joukowski effect, and to analyze the dynamics of slender rigid bodies in fluids with circulation.

Method: The authors recast Newtonian dynamics as a first-order nonlinear ODE for body velocity, then analyze the limit case of slender tubular domains with fixed inertia and circulation as the radius approaches zero.

Result: The dynamics of the limit massive rigid loop are governed by a first-order nonlinear ODE with coefficients depending on inertia, fluid vorticity, and the limit curve. The fluid dynamics involve vorticity transport with additional contributions from the rigid loop's circulation.

Conclusion: The study reveals a 3D Kutta-Joukowski-like effect and shows that unlike fluid filaments, rigid filaments with fixed density can have finite velocity and displacement in the zero-radius limit, contrasting with infinite velocities in fluid-only cases.

Abstract: We consider the motion of a rigid body immersed in an inviscid incompressible
fluid. In 2D, an important physical effect associated with this system is the
famous Kutta-Joukowski effect. In the present paper, we identify a similar
effect in the 3D case. For this, we first recast the Newtonian dynamics of the
rigid body as a first-order nonlinear ODE for the $6$-component body velocity,
in the body frame. Then, we focus on the particular case where the rigid body
occupies a slender tubular domain with a smooth closed curve as the centerline
and a circular cross-section, in the limit where the radius goes to zero, with
fixed inertia and circulation around the curve. We establish that the dynamics
of the limit massive rigid loop are given by a first-order nonlinear ODE with
coefficients that depend only on the inertia, on the fluid vorticity, and on
the limit curve through two $3$D vectors, which are involved in a skewsymmetric
$6 \times 6$ matrix that appears in the limit force and torque, a structure
which is reminiscent of the 2D Kutta-Joukowski effect. We also identify the
limit fluid dynamics as, where, as in the case of the Euler equation alone, the
vorticity evolves according to the usual transport equation with stretching,
but with a velocity field that is due not only to the fluid vorticity but also
to a vorticity filament associated with the circulation around the limit rigid
loop. This result is in stark contrast with the case where the filament is made
of fluid, with non-zero circulation, since in the latter, the filament velocity
becomes infinite in the zero-radius limit. However, considering the inertia
scaling that corresponds to a fixed density, we prove that there are solutions
for which the solid velocity and its displacement tend to infinity over a time
interval of size $\mathcal{O}(1)$.

</details>


### [60] [The Hénon equation in Orlicz-Sobolev spaces](https://arxiv.org/abs/2509.17923)
*Pablo Ochoa,Ariel Salort*

Main category: math.AP

TL;DR: The paper studies the Hénon problem in Orlicz-Sobolev spaces, showing that the symmetric term |x|^α allows radial solutions even for supercritical H, proves boundedness of radial solutions, and establishes a Pohozaev's identity to determine parameter ranges where no bounded solutions exist.


<details>
  <summary>Details</summary>
Motivation: To generalize classical Hénon equation results to Orlicz-Sobolev spaces and understand how the symmetric term |x|^α affects the existence and properties of solutions in this more general functional setting.

Method: Analysis of the Hénon problem with g-Laplacian operator in Orlicz-Sobolev spaces, employing techniques to prove existence of radial solutions, boundedness properties, and deriving a Pohozaev's identity specific to this functional framework.

Result: Demonstrated that the symmetric term |x|^α enables radial solutions even for supercritical H functions, proved boundedness of these radial solutions, and established parameter ranges for α where no bounded solutions exist using the derived Pohozaev's identity.

Conclusion: The symmetric structure in the Hénon problem plays a crucial role in allowing radial solutions in Orlicz-Sobolev spaces, with the Pohozaev's identity providing important nonexistence criteria for bounded solutions.

Abstract: In this paper, we consider the H\'enon problem in the setting of
Orlicz-Sobolev spaces: \begin{equation*} \begin{cases} -\Delta_g u= |x|^\alpha
h( u) \quad \text{in }B\\ u>0 \quad \text{in }B\\ u= 0 \quad \text{on }\partial
B\\ \end{cases} \end{equation*}where $B$ is the unit ball in $\mathbb{R}^n$,
$g=G'$, $h=H'$ are N-functions and the operator $-\Delta_g$ is the
$g$-Laplacian. We show that the symmetric term $|x|^\alpha$, for $\alpha>0$,
allows to have radial solutions even for supercritical $H$, generalizing
results for the classical H\'enon equation. We also show that radial solutions
are indeed bounded. Finally, we state a Pohozaev's identity in Orlicz-Sobolev
spaces that we apply to get a range in $\alpha$ for which the problem has no
bounded solutions.

</details>


### [61] [Dispersive estimates for fractional order Schrödinger operators](https://arxiv.org/abs/2509.18002)
*M. Burak Erdogan,Michael Goldberg,William Green*

Main category: math.AP

TL;DR: This paper proves dispersive bounds for fractional Schrödinger operators with non-integer exponents and decaying potentials, establishing resolvent bounds, a limiting absorption principle, and global dispersive estimates.


<details>
  <summary>Details</summary>
Motivation: The motivation is to extend dispersive analysis to fractional Schrödinger operators, which have applications in quantum mechanics and mathematical physics, particularly for non-integer exponents where classical methods may not apply.

Method: The authors use mathematical analysis techniques to derive pointwise bounds on resolvent operators, establish a quantitative limiting absorption principle, and prove global dispersive estimates through rigorous mathematical proofs.

Result: The paper successfully proves dispersive bounds for all 0<α<n/2, establishes a quantitative limiting absorption principle for 1/2<α<n/2, and obtains global dispersive estimates in dimension n≥2 for the range (n+1)/4≤α<n/2.

Conclusion: The results provide a comprehensive dispersive analysis framework for fractional Schrödinger operators with decaying potentials, extending classical dispersive estimates to the fractional setting with non-integer exponents.

Abstract: We prove dispersive bounds for fractional Schr\"odinger operators on $\mathbb
R^n$ of the form $H=(-\Delta)^{\alpha}+V$ with $V$ a real-valued, decaying
potential and $\alpha \notin\mathbb N$. We derive pointwise bounds on the
resolvent operators for all $0<\alpha<\frac{n}{2}$, a quantitative limiting
absorption principle for $\frac12<\alpha<\frac{n}{2}$, and establish global
dispersive estimates in dimension $n\geq 2$ for the range $\frac{n+1}{4}\leq
\alpha <\frac{n}2$.

</details>


### [62] [The $L^p$-continuity of wave operators for fractional order Schrödinger operators](https://arxiv.org/abs/2509.18003)
*M. Burak Erdogan,Michael Goldberg,William Green*

Main category: math.AP

TL;DR: The paper analyzes fractional Schrödinger operators with real-valued potentials, proving that wave operators extend to bounded operators on L^p spaces under certain conditions, leading to dispersive and Strichartz estimates.


<details>
  <summary>Details</summary>
Motivation: To extend results for standard Schrödinger operators (when α is a natural number) to fractional Schrödinger operators, particularly when n > 2α and α > 1, by establishing boundedness of wave operators on L^p spaces.

Method: The authors consider fractional Schrödinger operators H = (-Δ)^α + V(x) in n dimensions with real-valued potential V, under the conditions n > 2α and α > 1. They show that wave operators extend to bounded operators on L^p(ℝ^n) for all 1 ≤ p ≤ ∞ under potential conditions analogous to the integer α case.

Result: The main result is the proof that wave operators extend to bounded operators on L^p(ℝ^n) for the full range 1 ≤ p ≤ ∞, which enables the derivation of dispersive and Strichartz estimates for the perturbed fractional Schrödinger operator.

Conclusion: This work successfully generalizes known results for standard Schrödinger operators to fractional cases, providing a framework for analyzing perturbed fractional Schrödinger equations through wave operator theory and establishing important dispersive properties.

Abstract: We consider fractional Schr\"odinger operators $H=(-\Delta)^\alpha+V(x)$ in
$n$ dimensions with real-valued potential $V$ when $n>2\alpha$, $\alpha>1$. We
show that the wave operators extend to bounded operators on $L^p(\mathbb R^n)$
for all $1\leq p\leq\infty$ under conditions on the potential that depend on
$n$ and $\alpha$ analogously to the case when $\alpha\in \mathbb N$. As a
consequence, we deduce a family of dispersive and Strichartz estimates for the
perturbed fractional Schr\"odinger operator.

</details>


### [63] [Long-wave instability of periodic shear flows for the 2D Navier-Stokes equations](https://arxiv.org/abs/2509.18070)
*Maria Colombo,Michele Dolce,Riccardo Montalto,Paolo Ventura*

Main category: math.AP

TL;DR: Extension of Kolmogorov's shear flow instability problem to general shear flows, showing long-wave instability under specific viscosity and wavelength conditions.


<details>
  <summary>Details</summary>
Motivation: To generalize the classical shear flow instability problem from specific sinusoidal flows to arbitrary shear flows, confirming Yudovich's 1966 theoretical predictions with rigorous mathematical proofs.

Method: Two independent approaches: construction of Kato's isomorphism and normal forms method, both handling singular terms in the linearized operator as wavelength approaches zero.

Result: Proves that every shear flow exhibits long-wave instability when the inverse derivative norm exceeds viscosity and wavelength is much smaller than viscosity.

Conclusion: Provides rigorous mathematical confirmation of Yudovich's instability mechanism for general shear flows, overcoming technical challenges with singular linear operators.

Abstract: In 1959, Kolmogorov proposed to study the instability of the shear flow
$(\sin(y),0)$ in the vanishing viscosity regime in tori
$\mathbb{T}_{\alpha}\times \mathbb{T}$. This question was later resolved by
Meshalkin and Sinai. We extend the problem to general shear flows $(U(y),0)$
and show that every $U(y)$ exhibits long-wave instability whenever
$\|\partial_y^{-1} U\|_{L^2} > \nu$ and $\alpha\ll \nu$, with $\nu$ being the
kinematic viscosity. This instability mechanism confirms previous findings by
Yudovich in 1966, supported also by several numerical results, and is
established through two independent approaches: one via the construction of
Kato's isomorphism and one via normal forms. Unlike in many other applications
of the latter methods, both proofs deal with the presence of a delicate term in
the linearized operator that becomes singular as $\alpha$ approaches $0$.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [64] [Topology optimization of cathode gas channel layout in advanced proton exchange membrane fuel cells](https://arxiv.org/abs/2509.16619)
*Zahra Kazemi,Kamran Behdinan*

Main category: physics.comp-ph

TL;DR: Topology optimization is applied to redesign PEMFC cathode gas channels, achieving 20.9% higher current density with 46.7% lower pressure drop compared to serpentine layouts.


<details>
  <summary>Details</summary>
Motivation: Current PEMFC cathode gas channel designs are suboptimal, relying on heuristic modifications rather than systematic optimization approaches.

Method: A topology optimization framework using 3D half-cell and reduced-order 2D models to maximize reactant concentration and minimize power dissipation along flow paths.

Result: Optimized channels show 20.9% higher mean current density, 46.7% lower pressure drop than serpentine layout, and more homogeneous reactant delivery with 4.9% lower standard deviation.

Conclusion: Topology optimization enables discovery of unconventional yet efficient channel layouts, providing design flexibility for improved PEMFC performance.

Abstract: The proton exchange membrane fuel cell (PEMFC) output relies on the transport
behavior within the cathode gas channels. Current designs remain inadequate as
they often rely on heuristic modifications of existing layouts or designer
intuition with suboptimal performance. In this study, topology optimization is
proposed to redesign the PEMFC cathode gas channel layout without a priori
assumptions. The optimization aims to maximize the reactant concentration and
minimize power dissipation along the flow path. The problem is solved within a
three-dimensional half-cell model. For computational tractability, a
reduced-order, depth-averaged two-dimensional model is also implemented. The
optimized topology yields an enhanced current density with lower energy
dissipation over the conventional benchmarks. At an inlet velocity of 0.15 m/s,
the pressure drop is reduced by 46.7% compared to the serpentine layout, though
is 28.2% higher than that of the parallel case. Within the optimized channels,
oxygen flows at higher local velocities, which allows a more homogeneous
reactant delivery across the domain. Relative to the serpentine layout, the
improvement in mean current density reaches 20.9% with 4.9% lower standard
deviation. Placing more emphasis on dissipation minimization during
optimization produces more intricate, tortuous channel topologies. Such design
flexibility enables the discovery of unconventional yet efficient layouts.

</details>


### [65] [Semi-Analytic Solutions to the Noh Problem with a Black Box Equation of State](https://arxiv.org/abs/2509.16766)
*Seth Gerberding,Jeff Peterson,Jim Ferguson,Scott Ramsey*

Main category: physics.comp-ph

TL;DR: This paper develops a method to create semi-analytic solutions for the Noh Problem using black box equations of state, which can be used for verifying hydrodynamics codes.


<details>
  <summary>Details</summary>
Motivation: To enable verification tests for hydrodynamics codes by providing semi-analytic solutions to the Noh Problem when the equation of state is treated as a black box.

Method: Presents the underlying theory and a specific method for finding semi-analytic solutions to the Noh Problem with black box equations of state.

Result: Several examples of derived semi-analytic solutions are presented, and a classic convergence test is performed using a non-trivial semi-analytic solution.

Conclusion: The method successfully produces semi-analytic solutions that can be used for verification purposes in hydrodynamics code testing.

Abstract: The objective of this paper is to derive a method of constructing
semi-analytic solutions to the Noh Problem when the equation of state (EoS) is
a black box. Such solutions can be used for verification tests of hydrodynamics
codes. We present the underlying theory, the method for finding solutions, and
several examples of derived semi-analytic solutions. We end by performing a
classic convergence test using a non-trivial semi-analytic solution.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [66] [Exponential Spectral Scaling: Robust and Efficient Stellarator Boundary Optimization via Mode-Dependent Scaling](https://arxiv.org/abs/2509.16320)
*Byoungchan Jang,Rory Conlin,Matt Landreman*

Main category: physics.plasm-ph

TL;DR: Exponential Spectral Scaling (ESS) is a technique that applies mode-dependent exponential scaling to Fourier modes in stellarator boundary optimization, reducing dynamic range and enabling direct single-step optimization instead of traditional multi-step approaches.


<details>
  <summary>Details</summary>
Motivation: Traditional stellarator boundary optimization faces challenges with extreme disparity between low- and high-mode amplitudes, leading to poor local minima and requiring computationally expensive multi-step Fourier continuation methods.

Method: ESS applies a mode-dependent exponential scaling factor to each Fourier mode using the L∞ norm, creating a square spectral decay profile that reduces dynamic range from 10^6-10^7 to 10^2-10^3, enabling direct full-spectrum optimization.

Result: ESS reduces computation time by a factor of 2 to 5, eliminates arbitrary staging decisions, improves robustness by reducing sensitivity to initial conditions, and works effectively across both QA and QH configurations using SIMSOPT and DESC toolkits.

Conclusion: ESS provides a more efficient and robust alternative to traditional multi-step optimization approaches for stellarator boundary design, enabling direct single-step optimization with improved convergence properties.

Abstract: Stellarator boundary optimization faces a fundamental numerical challenge:
the extreme disparity between low- and high-mode amplitudes creates an
optimization landscape in which direct full-spectrum approaches typically
converge to poor local minima. Traditionally, this challenge has been addressed
through a computationally expensive, multi-step Fourier continuation, in which
low Fourier modes are optimized first, followed by the gradual incorporation of
higher modes. We present Exponential Spectral Scaling (ESS), a technique that
applies a mode-dependent exponential scaling factor to each Fourier mode. Our
primary implementation uses the $L_{\infty}$ norm to determine the scaling
pattern, creating a square spectral decay profile that effectively reduces the
dynamic range of optimization variables from $10^{6}$--$10^{7}$ to
$10^{2}$--$10^{3}$. This scaling aligns with the natural spectral decay of
physically meaningful configurations and enables direct single-step
optimization using the full spectrum of boundary Fourier modes. ESS eliminates
arbitrary staging decisions and reduces computation time by a factor of $2$ to
$5$ in benchmark cases. In addition to accelerating optimization, ESS improves
robustness, reducing sensitivity to initial conditions and increasing
confidence in avoiding local optima. We demonstrate the effectiveness of ESS
across both quasi-axisymmetric (QA) and quasi-helically symmetric (QH)
configurations, using two distinct optimization toolkits: SIMSOPT and DESC.

</details>


### [67] [Data-Driven Reduced-Order Modeling of Phase Mixing Dynamics from Particle Kinetic Simulation](https://arxiv.org/abs/2509.16535)
*Darian Figuera-Michal,Sungpil Yum,Jae-Min Kwon,Eisung Yoon*

Main category: physics.plasm-ph

TL;DR: POD-SINDy framework successfully models phase mixing in collisionless plasmas, achieving high compression (3-5 orders magnitude) with minimal modes (5-10) while maintaining accuracy (4-13% error) across various complexity levels.


<details>
  <summary>Details</summary>
Motivation: Phase mixing's velocity space filamentation poses challenges for simulations and reduced-order modeling, requiring efficient methods to capture essential physics while handling nonlinear interactions and noise.

Method: Joint Proper Orthogonal Decomposition and Sparse Identification of Nonlinear Dynamics (POD-SINDy) applied to particle-in-cell simulations under progressively complex conditions: passive kinetic, self-consistent electrostatic, and noisy datasets.

Result: POD-SINDy achieved near-optimal reconstructions with 5 modes (4% error) in passive cases, and 10 modes (7-13% error) in self-consistent/noisy cases. SINDy provided sparse linear equations, while POD filtered noise effectively.

Conclusion: POD-SINDy offers a compact, interpretable reduced-order modeling approach for phase mixing that preserves essential physics across complexity regimes with significant data compression.

Abstract: Phase mixing is a fundamental kinetic process that governs dissipation and
stability in collisionless plasmas, but its inherent filamentation in velocity
space creates major challenges for both high-fidelity simulations and
reduced-order modeling. This work presents the first exploratory evaluation of
a joint Proper Orthogonal Decomposition and Sparse Identification of Nonlinear
Dynamics (POD-SINDy) framework applied to particle-in-cell simulations of phase
mixing. Simulation datasets were generated under progressively complex
conditions, starting from a passive kinetic case without self-consistent
electric fields, extending to self-consistent simulations with nonlinear
electric field feedback, and finally to a noisy dataset with reduced particle
resolution. In the passive kinetic regime, POD-SINDy achieved near-optimal
reconstructions with only five modes, reproducing filamentation with errors
below four percent. In self-consistent electrostatic cases, variance spread
across more modes due to nonlinear interactions and noise, slowing singular
value decay and making strict low-rank embeddings more demanding. Nevertheless,
retaining ten modes was sufficient to recover the dominant structures, yielding
reconstruction errors of about seven percent for the low-noise case and
thirteen percent for the noisy dataset. Across all scenarios, SINDy provided
sparse and interpretable equations for modal amplitudes that remained
predominantly linear despite the underlying nonlinear data, while POD
truncation effectively filtered particle noise and preserved coherent dynamics.
These findings demonstrate that POD-SINDy constitutes a compact and
interpretable approach to reduced-order modeling of phase mixing, capable of
retaining essential physics across regimes of increasing complexity while
achieving data compression from three to five orders of magnitude depending on
dataset complexity.

</details>


### [68] [Measuring the locations and properties of VHF sources emitted from an aircraft flying through high clouds](https://arxiv.org/abs/2509.16574)
*Olaf Scholten,Marten Lourens,Stijn Buitink,Steve Cummer,Joe Dwyer,Brian M. Hare,Tim Huege,Ningyu Liu,Katie Mulrey,Anna Nelles,Chris Sterpka,T. N. Gia Trinh,Paulina Turekova,Sander ter Veen*

Main category: physics.plasm-ph

TL;DR: Researchers discovered that airplanes emit VHF radio frequency radiation from specific locations (engines and tail) while flying through clouds, using this observation to improve lightning detection techniques with sub-meter precision.


<details>
  <summary>Details</summary>
Motivation: To understand how airplanes shed electrical charge acquired during flight through clouds and to test/improve the precision of lightning observation techniques.

Method: Used the Low-Frequency Array (LOFAR) to image a lightning flash, which serendipitously revealed VHF emissions from specific locations on a Boeing 777-300ER flying through clouds at 8km altitude.

Result: VHF emissions were detected exclusively from the two engines and a specific spot on the tail, with no emissions from electrostatic wicks. The improved procedure achieved location precision better than 50cm and polarization orientation accuracy within 25°.

Conclusion: This discovery provides insights into airplane charge dissipation mechanisms and demonstrates significant improvements in lightning observation precision, with potential applications for aviation safety and atmospheric science.

Abstract: We show that it is possible to locate the few places on the body of an
airplane, while it is flying through high clouds, from which broad-band,
pulsed, radiation is emitted at Very High Frequency (VHF) radio frequencies.
This serendipitous discovery was made whilst imaging a lightning flash using
the Low-Frequency Array (LOFAR). This observation provides insights into the
way the airplane sheds the electrical charge it acquires when flying through
clouds. Furthermore, this observation allowed us to test and improve the
precision and accuracy for our lightning observation techniques.
  Our new results indicate that with the improved procedure the location
precision for strong pulses is better than 50~cm, with the orientation of
linear polarization being accurate to within 25$^\circ$. For the present case
of a Boeing 777-300ER, VHF emissions were observed exclusively associated with
the two engines, as well as a specific spot on the tail. Despite the aircraft
flying through clouds at an altitude of 8~km, we did not detect any emissions
from electrostatic wicks.

</details>


### [69] [Galilean Electromagnetic Particle-in-Cell Code](https://arxiv.org/abs/2509.16793)
*Alexander Pukhov,Nina Elkina*

Main category: physics.plasm-ph

TL;DR: A Galilean electromagnetic particle-in-cell (GEM-PIC) algorithm is introduced that transforms Maxwell and Vlasov equations into boosted coordinates, preserving electromagnetic structure while enabling efficient plasma wakefield acceleration simulations.


<details>
  <summary>Details</summary>
Motivation: To develop a more efficient and accurate method for simulating plasma-based wakefield acceleration that overcomes limitations of quasistatic methods and allows self-consistent treatment of particle trapping.

Method: The GEM-PIC algorithm transforms the full set of Maxwell equations and Vlasov equation into boosted coordinates, preserving electromagnetic structure while exploiting scale separation for computational efficiency.

Result: The method enables highly efficient and accurate simulations of plasma-based wakefield acceleration without needing to distinguish between beam and streaming particles.

Conclusion: GEM-PIC provides a self-consistent approach for plasma wakefield acceleration simulations that maintains electromagnetic structure while offering computational advantages over quasistatic methods.

Abstract: We introduce a Galilean electromagnetic particle-in-cell (GEM-PIC) algorithm,
which transforms the full set of Maxwell equations and the Vlasov equation into
the boosted coordinates. This approach preserves the electromagnetic structure
of the interaction while exploiting scale separation for computational effi
ciency. Unlike quasistatic methods, GEM-PIC does not have to distinguish
between beam and streaming particles, allowing a self-consistent treatment of
particle trapping. The EM-PIC algorithm allows for highly effi cient and
accurate simulations of plasma-based wakefield acceleration.

</details>


### [70] [Nonlinear anisotropic equilibrium reconstruction in axisymmetric magnetic mirrors](https://arxiv.org/abs/2509.17288)
*S. J. Frank,J. K. Anderson,B. Biswas,E. Claveau,D. Endrizzi,C. Everson,R. W. Harvey,I. Kemeny,S. Murdock,Yu. V. Petrov,J. Pizzo,T. Qian,K. Sanwalka,K. Shih,D. A. Sutherland,A. Tran,J. Viola,D. Yakovlev,M. Yu,C. B. Forest*

Main category: physics.plasm-ph

TL;DR: This paper extends nonlinear magnetic equilibrium reconstruction to high-β plasmas with anisotropic pressure, using a novel basis set and machine learning algorithm for faster, more accurate reconstructions with fewer diagnostics.


<details>
  <summary>Details</summary>
Motivation: Traditional equilibrium reconstruction methods are limited to isotropic pressure and low-β plasmas, but high-β plasmas with anisotropic pressure require new approaches for accurate diagnostic interpretation in experimental plasma systems.

Method: Developed a novel basis set for plasma profiles and a machine learning algorithm using scalable constrained Bayesian optimization to enable nonlinear equilibrium reconstruction in high-β anisotropic pressure conditions.

Result: Successfully applied the method to Wisconsin High Temperature Superconducting Axisymmetric Magnetic Mirror experiments to infer sloshing ions, achieving faster and more robust reconstructions with fewer experimental diagnostics.

Conclusion: The reconstruction technique is effective for WHAM and other mirror devices, and shows promise for high-performance fusion devices with limited diagnostic capabilities, such as future fusion power plants.

Abstract: Magnetic equilibrium reconstruction is a crucial simulation capability for
interpreting diagnostic measurements of experimental plasmas. Equilibrium
reconstruction has mostly been applied to systems with isotropic pressure and
relatively low plasma $\beta = 2\mu_0p/B^2$. This work extends nonlinear
equilibrium reconstruction to high-$\beta$ plasmas with anisotropic pressure
and applies it to the Wisconsin High Temperature Superconducting Axisymmetric
Magnetic Mirror experiments to infer the presence of sloshing ions. A novel
basis set for the plasma profiles and machine learning algorithm using scalable
constrained Bayesian optimization allow accurate nonlinear reconstructions to
be made more quickly with fewer experimental diagnostics and improves the
robustness of reconstructions at high $\beta$. In addition to WHAM and other
mirrors, such reconstruction techniques are potentially attractive in
high-performance devices with constrained diagnostic capabilities such as
fusion power plants.

</details>


### [71] [Negative charge acquisition by isolated single microdroplets from plasma exposure at atmospheric pressure](https://arxiv.org/abs/2509.17379)
*Nourhan Hendawy,Harold McQuaid,Somhairle Mag Uidhir,David Rutherford,Declan Diver,Davide Mariotti,Paul Maguire*

Main category: physics.plasm-ph

TL;DR: Measurement of charge on individual microdroplets exposed to plasma, showing charges approaching the Rayleigh limit, with comparison to finite element simulations.


<details>
  <summary>Details</summary>
Motivation: Charged liquid microdroplets show significant chemical reaction rate enhancement, and understanding plasma charging mechanisms is crucial for developing plasma-charged microreactors.

Method: Measured charge on fixed-size (47 μm) individual droplets exposed to low-temperature RF-driven plasma for 4 ms, and compared results with finite element simulations of plasma charging.

Result: Measured charge values (0.8-2.0 × 10^7 electrons) approached the Rayleigh limit for the given droplet diameter. Simulations provided estimates of surface electric fields and charge fluxes.

Conclusion: The study advances the theoretical framework for plasma-charged microreactors by providing experimental measurements and simulation validation of plasma charging mechanisms.

Abstract: Charged liquid microdroplets have generated significant interest recently due
to the observation of chemical reaction rate enhancement by orders of
magnitude. Droplet charging by plasma irradiation has been observed along with
significantly enhanced reactions in liquid. In this paper we measure the charge
on fixed size (47 $\mu$m) individual droplets, exposed for approximately 4 ms
to a low temperature RF-driven plasma operated at atmospheric pressure. The
measured charge values (0.8--2.0 $\times 10^{7}$ electrons) approached the
Rayleigh limit for the given droplet diameter. Results were compared with
finite element simulations of plasma charging which provided estimates of
surface electric fields and charge fluxes to the droplet surface and helps
advance the development of a theoretical framework for plasma-charged
microreactors.

</details>


### [72] [Modeling stopping power of ions in plasmas using parametric potentials](https://arxiv.org/abs/2509.17453)
*Tanguy Barges Delattre,Sébastien Rassou,Jean-Christophe Pain*

Main category: physics.plasm-ph

TL;DR: Study of ion stopping power in warm dense plasma with a method valid for any ionization degree, combining free-electron and bound-electron contributions using parametric potentials.


<details>
  <summary>Details</summary>
Motivation: To develop a comprehensive stopping-power calculation method that works across all ionization degrees, bridging the gap between neutral and fully-ionized matter.

Method: Free-electron contribution uses Maynard-Deutsch-Zimmerman formula, bound-electron contribution uses Bethe formula with corrections for density and shell effects. Three parametric potentials (Green-Sellin-Zachor, Yunta, Klapisch) are tested within Garbet formalism for mean excitation energy.

Result: The approach correctly bridges neutral and fully-ionized matter limits, with results compared to self-consistent average-atom calculations.

Conclusion: Proposed method provides valid stopping-power calculations for any ionization degree in warm dense plasmas, successfully connecting the extreme cases of neutral and fully-ionized matter.

Abstract: We present a study of the ion stopping power due to free and bound electrons
in a warm dense plasma. Our main goal is to propose a method of stopping-power
calculation expected to be valid for any ionization degree. The free-electron
contribution is described by the Maynard-Deutsch-Zimmerman formula and the
bound-electron one relies on the Bethe formula with corrections, in particular
taking into account density and shell effects. The impact of the bound-state
computation by three different parametric potentials is investigated within the
Garbet formalism for the mean excitation energy. The first parametric potential
is due to Green, Sellin and Zachor, the second one was proposed by Yunta, and
the third one was introduced by Klapisch in the framework of atomic-structure
computations. The results are compared to the ones of self-consistent
average-atom calculations. This approach correctly bridges the limits of
neutral and fully-ionized matter.

</details>


### [73] [Numerical Analysis of Ground Testing for the Intake Device of an Atmosphere-Breathing Electric Propulsion](https://arxiv.org/abs/2509.17547)
*Geonwoong Moon,Eunji Jun,Minwoo Yi,Hyunjin Choi,Kangmin Park,Younho Kim,Jaecheong Lee,Jeongjae Lee,Gahee Joo,Seungho Shin,Se Lee,Yunhwang Jeong*

Main category: physics.plasm-ph

TL;DR: This study investigates how electric propulsion plasma plumes interact with intake devices for atmosphere-breathing electric propulsion systems, providing guidelines for measuring capture efficiency in ground testing facilities.


<details>
  <summary>Details</summary>
Motivation: Atmosphere-breathing electric propulsion (ABEP) is promising for long-term orbit maintenance in very-low-Earth orbit, but requires reliable measurement of intake capture efficiency which is challenging to evaluate experimentally.

Method: A hybrid PIC-DSMC method with ion-surface interaction models is used to simulate plasma plume interaction with intake devices, analyzing how beam-ion energy and species mass affect flow composition and capture efficiency.

Result: Lower beam energies and lighter atmospheric constituents increase plume divergence and neutralization, yielding neutral-dominated outlet flow. Sputtering becomes significant at high energies but can be mitigated with appropriate low beam energies.

Conclusion: Simultaneous ion and neutral diagnostics are essential for reliable capture-efficiency evaluation when using EP plasma plumes in ground testing facilities.

Abstract: Atmosphere-breathing electric propulsion (ABEP) is a promising technology for
long-term orbit maintenance in very-low-Earth orbit. The intake device plays a
crucial role in capturing and supplying propellant, and its capture efficiency
is a key indicator of drag-compensation feasibility. For experimental
evaluation, an electric-propulsion (EP) plasma plume can be used as a
particle-flow generator to simulate the VLEO atmosphere in ground facilities.
This study numerically investigates the interaction of an EP plasma plume with
an intake device to establish guidelines for measuring capture efficiency in
conventional vacuum facilities. A hybrid PIC-DSMC method with ion-surface
interaction models is employed to simulate the plasma plume incident on the
intake. The composition of the captured flow is governed by beam-ion energy and
species mass: lowering the energy and using lighter atmospheric constituents
increase plume divergence and promote neutralization, yielding a
neutral-dominated outlet flow. Sputtering of the intake surface becomes
non-negligible at high energies but can be mitigated by operating at
appropriately low beam energies. The results show that simultaneous ion and
neutral diagnostics are required for reliable capture-efficiency evaluation
when using EP plasma plumes in ground facilities.

</details>


### [74] [Validation of the Following Streamer mechanism for SF6 breakdown induced by floating linear metal particles](https://arxiv.org/abs/2509.17716)
*Zihao Feng*

Main category: physics.plasm-ph

TL;DR: Experimental validation of the Following Streamer mechanism for SF6 breakdown induced by floating metal particles, revealing both consistencies with 2D simulations and discrepancies requiring extended 3D modeling.


<details>
  <summary>Details</summary>
Motivation: To validate the recently proposed Following Streamer mechanism for SF6 streamer breakdown induced by floating metal particles, which was derived from limited 2D axisymmetric fluid models that cannot fully describe real-world 3D scenarios.

Method: Experimental investigation of SF6 streamer discharge morphology induced by floating linear metal particles under negative pulsed voltage, with comparison to 2D axisymmetric fluid simulations and proposal of an extended physical model.

Result: Experimental results show consistencies (streamer inception at both particle ends and following streamer formation) supporting the mechanism, but also reveal discrepancies (more following streamers and off-axis propagation paths) not captured by 2D simulations.

Conclusion: The Following Streamer mechanism is generally supported but requires extension to 3D modeling to fully describe experimental observations, leading to proposal of an improved physical model.

Abstract: A recently proposed Following Streamer mechanism (Feng et al. 2025 Phys. Rev.
Applied 23 064039) seeks to explain how floating metal particles induce SF6
streamer breakdown in the combined gap. This mechanism is derived from a 2D
axisymmetric fluid model, which has limitations in describing multiple streamer
events in real-world 3D scenarios. To validate the Following Streamer
mechanism, we experimentally investigate the discharge morphology of SF6
streamers induced by a floating linear metal particle under negative pulsed
voltage. The results are then compared with those from 2D axisymmetric fluid
simulations. The comparison reveals both consistencies and discrepancies.
Regarding consistencies, experimentally observed features-such as streamer
inception at both ends of the metal particle and the formation of subsequent
following streamers-support the general idea of the Following Streamer
mechanism. Regarding discrepancies, the experiments show a larger number of
following streamers and off-axis propagation paths, which cannot be described
in the 2D simulation. For scientific rigor, an extended physical model was
proposed to improve the description of the Following Streamer mechanism.

</details>


<div id='physics.class-ph'></div>

# physics.class-ph [[Back]](#toc)

### [75] [Transition Frequencies and Dynamic Amplification of Buried Lifelines: A Semi-Analytical Timoshenko Beam on Winkler Foundation Model](https://arxiv.org/abs/2509.16906)
*Gersena Banushi,Kenichi Soga*

Main category: physics.class-ph

TL;DR: A semi-analytical model using Timoshenko beam theory for transverse vibration analysis of buried pipelines/tunnels, revealing vibration spectrum with four parts separated by three transition frequencies that affect dynamic amplification.


<details>
  <summary>Details</summary>
Motivation: Underground lifelines are vulnerable to ground vibrations from seismic events and traffic, requiring accurate prediction methods for structural safety and operability.

Method: Formulated using Timoshenko beam theory on elastic foundation, with closed-form analytical solutions validated through case studies of buried steel pipelines and finite element simulations.

Result: The model shows excellent agreement with finite element simulations. Parametric study quantifies influence of soil stiffness and system length on dynamic performance.

Conclusion: The proposed method provides computationally efficient and physically transparent framework for capturing complex vibration phenomena, offering valuable guidance for design and resilience assessment of underground lifelines.

Abstract: Underground lifelines, such as pipelines and tunnels, are susceptible to
ground vibrations from seismic events, traffic, and other dynamic sources.
Accurate prediction of their response is essential for ensuring structural
safety and operability. This study introduces a semi-analytical model for
transverse vibration analysis of buried lifelines, formulated using the
Timoshenko beam theory on elastic foundation. The closed-form analytical
solutions revealed that the vibration spectrum comprises four parts, separated
by three transition frequencies. At each transition, the oscillatory
characteristics of the modes change as a function of the system properties,
leading to marked variations in dynamic amplification. The model's validity is
confirmed through case studies of buried steel pipelines of varying lengths and
operating conditions, showing excellent agreement with finite element
simulations. A subsequent parametric study quantifies the influence of key
factors - including soil stiffness and system length - on dynamic performance.
The proposed method provides a computationally efficient and physically
transparent framework for capturing complex vibration phenomena beyond
simplified travelling-wave approaches, offering valuable guidance for the
design and resilience assessment of underground lifeline systems subjected to
various dynamic loads.

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [76] [Relativistically-strong electromagnetic waves in magnetized plasmas](https://arxiv.org/abs/2509.17245)
*Maxim Lyutikov*

Main category: astro-ph.HE

TL;DR: Analysis of nonlinear electromagnetic waves in magnetized plasmas showing that nonlinear effects reduce cut-off frequencies for superluminal waves and cause current starvation termination for subluminal waves.


<details>
  <summary>Details</summary>
Motivation: To understand how relativistically nonlinear electromagnetic waves behave in magnetized plasmas (electron-ion and pair plasmas) when propagating along magnetic fields, particularly how wave intensity affects dispersion relations.

Method: Using a two-fluid approach to study relativistically nonlinear circularly polarized electromagnetic waves, analyzing dispersion relations as functions of wave intensity scaling with frequency.

Result: For superluminal waves, nonlinear effects reduce cut-off frequency while maintaining similar dispersion relation form. For subluminal waves (whistlers and Alfven), dispersion curves terminate at finite ω*-k* due to current starvation when fluctuating electric field exceeds guide field.

Conclusion: Subluminal modes with fluctuating electric field larger than the guide field cannot propagate, representing a fundamental limitation for wave propagation in magnetized plasmas under nonlinear conditions.

Abstract: Using a two-fluid approach, we consider the properties of relativistically
nonlinear (arbitrary $a_0$), circularly polarized electromagnetic waves
propagating along magnetic field in electron-ion and pair plasmas. Dispersion
relations depend on how wave intensity scales with frequency, $a_0 (\omega)$.
For superluminal branches, the nonlinear effects reduce the cut-off frequency,
while the general form of the dispersion relations $\omega(k)$ remains similar
to the linear case. For subluminal waves, whistlers and Alfven, a new effect
appears: dispersion curves effectively terminate at finite $\omega^\ast -
k^\ast$ due to current starvation. Qualitatively, subluminal modes with
fluctuating electric field larger than the guide field, $E_w (\omega) \geq
B_0$, cannot propagate.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [77] [Delta-Doped Diamond via in-situ Plasma-Distance Control](https://arxiv.org/abs/2509.17521)
*Philip Schätzle,Felix Hoffmann,Sven Mägdefessel,Patrik Straňák,Lutz Kirste,Peter Knittel*

Main category: cond-mat.mtrl-sci

TL;DR: Novel CVD diamond growth approach with controlled sample distance from plasma reveals two new growth regimes enabling precise nitrogen doping for quantum applications.


<details>
  <summary>Details</summary>
Motivation: To develop controlled diamond growth techniques that enable precise defect incorporation for quantum sensing and computing applications, particularly for creating thin delta-doped layers and low-concentration NV centers.

Method: CVD diamond growth with controlled sample positioning relative to plasma: within 3-5mm from plasma for delta-doping, and >10mm away for nitrogen species deposition. Different distances create distinct growth regimes affecting nitrogen incorporation.

Result: Two new growth regimes identified: 3-5mm distance enables thin delta-doped layers (<30nm) with increased nitrogen incorporation; >10mm distance shows no growth but nitrogen species deposition. All layers show NV emission correlating with nitrogen content.

Conclusion: This distance-controlled CVD approach enables fabrication of highly doped thin films for quantum sensing and low-NV-concentration layers for quantum computing, with potential applications for other defects like phosphorus in diamond-based electronics.

Abstract: We present an approach for the CVD growth of diamond, where the sample is
placed in a defined distance from the reactor baseplate, to which the plasma
couples. We observe two previously unknown growth regimes. In the first case,
the sample is positioned within three to five millimeters of the plasma,
leading to a decreased growth rate, compared to a position inside the plasma
and, additionally, to an increased nitrogen incorporation, allowing the
fabrication of delta-doped layers with a thickness below 30 nm. In another
regime, where the sample is more than 10 mm away from the plasma, no growth is
observed. Instead, we assume a deposition of nitrogen-rich species on the
diamond surface, which is incorporated during the growth of the following
layer. All fabricated layers show NV emission, where the intensity correlates
with the nitrogen incorporation. The growth techniques could allow the
fabrication of highly doped thin films for quantum sensing applications, as
well as layers with low NV concentration, for quantum computing. The new
approaches are applicable not only for nitrogen incorporation but also for
other defects, for example, phosphorus, which could open up new avenues for
diamond-based electronics.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [78] [Geometric Interpolation of Rigid Body Motions](https://arxiv.org/abs/2509.16966)
*Andreas Mueller*

Main category: cs.RO

TL;DR: This paper addresses rigid body motion interpolation problems, specifically kth-order initial value trajectory interpolation (k-IV-TIP) and kth-order boundary value trajectory interpolation (k-BV-TIP), providing solutions for various derivative constraints.


<details>
  <summary>Details</summary>
Motivation: To develop systematic methods for interpolating spatial trajectories between prescribed initial and terminal poses of rigid bodies, accounting for constraints on higher-order derivatives of the rigid body twist.

Method: The authors present solutions for k-IV-TIP problems (k=1 to 4) where initial conditions on twist derivatives are prescribed, and a novel solution for 1-BV-TIP where initial and terminal twists are specified. The approach involves deriving higher-order interpolation schemes.

Result: Numerical results demonstrate the effectiveness of the proposed interpolation methods for two examples. The cubic interpolation for 1-BV-TIP automatically reduces to minimum acceleration curves when twists are zero.

Conclusion: The paper provides systematic solutions for rigid body motion interpolation with higher-order derivative constraints, offering a general framework for deriving such interpolations and demonstrating practical applicability through numerical examples.

Abstract: The problem of interpolating a rigid body motion is to find a spatial
trajectory between a prescribed initial and terminal pose. Two variants of this
interpolation problem are addressed. The first is to find a solution that
satisfies initial conditions on the k-1 derivatives of the rigid body twist.
This is called the kth-order initial value trajectory interpolation problem
(k-IV-TIP). The second is to find a solution that satisfies conditions on the
rigid body twist and its k-1 derivatives at the initial and terminal pose. This
is called the kth-order boundary value trajectory interpolation problem
(k-BV-TIP). Solutions to the k-IV-TIP for k=1,...,4, i.e. the initial twist and
up to the 4th time derivative are prescribed. Further, a solution to the
1-IV-TBP is presented, i.e. the initial and terminal twist are prescribed. The
latter is a novel cubic interpolation between two spatial configurations with
given initial and terminal twist. This interpolation is automatically identical
to the minimum acceleration curve when the twists are set to zero. The general
approach to derive higher-order solutions is presented. Numerical results are
shown for two examples.

</details>


<div id='math.DG'></div>

# math.DG [[Back]](#toc)

### [79] [Huisken's Distance Comparison Principle in Higher Codimension](https://arxiv.org/abs/2509.16823)
*Qi Sun*

Main category: math.DG

TL;DR: A variant of Huisken's distance comparison principle is established for reflection symmetric Curve Shortening flow in R^n, showing that certain symmetric flows develop Type I singularities and become asymptotically circular.


<details>
  <summary>Details</summary>
Motivation: To extend Huisken's distance comparison principle to reflection symmetric Curve Shortening flow and analyze the behavior of symmetric flows with convex projections.

Method: Establish a variant of Huisken's distance comparison principle for reflection symmetric immersed Curve Shortening flow in R^n (n≥2).

Result: Certain symmetric Curve Shortening flow with a one-to-one convex projection develops a Type I singularity and becomes asymptotically circular.

Conclusion: The established variant provides insights into the singularity formation and asymptotic behavior of symmetric Curve Shortening flows with convex projections.

Abstract: We establish a variant of Huisken's distance comparison principle for
reflection symmetric immersed Curve Shortening flow in $\mathbb R^n,n\geq2$.
  As an application, we show that certain symmetric Curve Shortening flow with
a one-to-one convex projection develops a Type~I singularity and becomes
asymptotically circular.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [80] [Topological Mechanics of Entangled Networks](https://arxiv.org/abs/2509.17813)
*Juntao Huang,Jiabin Liu,Shaoting Lin*

Main category: cond-mat.soft

TL;DR: A mathematical framework modeling entangled networks as graphs to understand topological constraints, showing entanglements reduce energy and enhance mechanical properties like strength and toughness.


<details>
  <summary>Details</summary>
Motivation: Entangled networks are common in tissues, polymers, and fabrics, but their mechanics are poorly understood due to complex topological constraints at the network level.

Method: Developed a mathematical framework modeling entangled networks as graphs to capture topological constraints, validated by experiments on entangled fabrics and hydrogels.

Result: Entanglements increase strength by enabling stress homogeneity and enhance toughness by mitigating stress concentration around crack tips. Discovered counterintuitive physical laws governing crack-tip stretch during crack opening.

Conclusion: The framework establishes fundamental principles linking topology to mechanics of entangled networks and provides a foundational tool for designing reconfigurable materials.

Abstract: Entangled networks are ubiquitous in tissues, polymers, and fabrics. However,
their mechanics remain insufficiently understood due to the complexity of the
topological constraints at the network level. Here, we develop a mathematical
framework that models entangled networks as graphs, capturing topological
constraints of entanglements. We prove that entanglements reduce system energy
by enabling uniform tension along chains crossing entanglements and by
redistributing stress through sliding. Under this framework, we study
elasticity and fracture, validated by experiments on entangled fabrics and
hydrogels. For elasticity, entanglements increase strength by enabling stress
homogeneity in the network. For fracture, entanglements enhance toughness by
mitigating stress concentration around crack tips. We discover counterintuitive
physical laws governing crack-tip stretch during crack opening: stress
deconcentration at small deformation, constitutive-law independence at
intermediate deformation, and linear scaling at large deformation. This
framework establishes fundamental principles of linking topology to mechanics
of entangled networks and offers a foundational tool for designing
reconfigurable materials.

</details>


<div id='physics.app-ph'></div>

# physics.app-ph [[Back]](#toc)

### [81] [Criticality of a stochastic modern Hopfield network model with exponential interaction function](https://arxiv.org/abs/2509.17152)
*Marco Cafiso,Paolo Paradisi*

Main category: physics.app-ph

TL;DR: The paper studies critical behavior in stochastic exponential Modern Hopfield Networks (SMHNs) with multiplicative noise, finding a critical transition in both overlap parameter Q and diffusion scaling H, with the critical regime showing long-range correlated dynamics and persistent temporal memory.


<details>
  <summary>Details</summary>
Motivation: While criticality of classical Hopfield Networks and p-body Modern Hopfield Networks has been extensively studied, the investigation of critical behavior in exponential MHNs is still in early stages, particularly for stochastic versions with noise.

Method: The authors study a stochastic exponential MHN with multiplicative salt-and-pepper noise, using noise probability p as control parameter, and analyzing average overlap Q and diffusion scaling H as order parameters to characterize critical transitions.

Result: Critical transitions were found in both Q and H parameters, with critical noise level weakly decreasing as network load N increases. The critical regime (p around 0.23-0.3) displays long-range correlated dynamics with highly persistent temporal memory (H around 1.3), while sub- and super-critical regimes show short-range correlations.

Conclusion: Exponential MHNs exhibit rich critical behavior with distinct dynamical regimes, where the critical phase is characterized by long-range temporal correlations and persistent memory, providing insights into the computational properties of these advanced associative memory models.

Abstract: The Hopfield network (HN) is a classical model of associative memory whose
dynamics are closely related to the Ising spin system with 2-body interactions.
Stored patterns are encoded as minima of an energy function shaped by a Hebbian
learning rule, and retrieval corresponds to convergence towards these minima.
Modern Hopfield Networks (MHNs) introduce p-body interactions among neurons
with p greater than 2 and, more recently, also exponential interaction
functions, which significantly improve network's storing and retrieval
capacity. While the criticality of HNs and p-body MHNs were extensively studied
since the 1980s, the investigation of critical behavior in exponential MHNs is
still in its early stages. Here, we study a stochastic exponential MHN (SMHN)
with a multiplicative salt-and-pepper noise. While taking the noise probability
p as control parameter, the average overlap parameter Q and a diffusion scaling
H are taken as order parameters. In particular, H is related to the time
correlation features of the network, with H greater than 0.5 signaling the
emergence of persistent time memory. We found the emergence of a critical
transition in both Q and H, with the critical noise level weakly decreasing as
the load N increases. Notably, for each load N, the diffusion scaling H
highlights a transition between a sub- and a super-critical regime, both with
short-range correlated dynamics. Conversely, the critical regime, which is
found in the range of p around 0.23-0.3, displays a long-range correlated
dynamics with highly persistent temporal memory marked by the high value H
around 1.3.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [82] [Super-resolution reconstruction of turbulent flows from a single Lagrangian trajectory](https://arxiv.org/abs/2509.17109)
*Hua-Lin Wu,Ao Xu,Heng-Dong Xi*

Main category: physics.flu-dyn

TL;DR: Track-to-Flow (T2F) and T2F+PINN models reconstruct turbulent flow fields from single Lagrangian agent trajectories using deep learning with physics-informed constraints.


<details>
  <summary>Details</summary>
Motivation: To accurately reconstruct turbulent flow fields from limited trajectory data recorded by actively migrating Lagrangian agents, enabling better flow analysis with minimal sensor data.

Method: T2F uses Vision Transformer encoder for trajectory features and CNN decoder for flow reconstruction. T2F+PINN adds physics-informed loss function based on PINN framework for enhanced physical consistency.

Result: T2F achieves velocity reconstruction accuracy comparable to existing methods. T2F+PINN reduces normalized vorticity error by up to 60% for gradient-related quantities like temperature gradients and vorticity in turbulent convection.

Conclusion: T2F is better for primitive flow variables, while T2F+PINN excels at gradient-related quantities. Both models enable accurate flow reconstruction from single Lagrangian trajectories.

Abstract: We studied the reconstruction of turbulent flow fields from trajectory data
recorded by actively migrating Lagrangian agents. We propose a deep learning
model, Track-to-Flow (T2F), which employs a Vision Transformer as an encoder to
capture the spatiotemporal features of a single agent trajectory, and a
convolutional neural network as the decoder to reconstruct the flow field. To
enhance the physical consistency of the T2F model, we further incorporate a
physics-informed loss function inspired by the framework of Physics-Informed
Neural Network (PINN), yielding a variant model referred to as T2F+PINN. We
first evaluate both models in a laminar cylinder wake flow at a Reynolds number
of $Re = 800$ as a proof-of-concept. The results show that the T2F model
achieves velocity reconstruction accuracy comparable to existing flow
reconstruction methods, while the T2F+PINN model reduces the normalized error
in vorticity reconstruction relative to the T2F model. We then apply the models
in a turbulent Rayleigh-B\'{e}nard convection at a Rayleigh number of $Ra =
10^{8}$ and a Prandtl number of $Pr = 0.71$. The results show that the T2F
model accurately reconstructs both the velocity and temperature fields, whereas
the T2F+PINN model further improves the reconstruction accuracy of
gradient-related physical quantities, such as temperature gradients, vorticity,
and the $Q$ value, with a maximum improvement of approximately 60\% compared to
the T2F model. Overall, the T2F model is better suited for reconstructing
primitive flow variables, while the T2F+PINN model provides advantages in
reconstructing gradient-related quantities. Our models open a promising avenue
for accurate flow reconstruction from a single Lagrangian trajectory.

</details>


### [83] [Compact representation of transonic airfoil buffet flows with observable-augmented machine learning](https://arxiv.org/abs/2509.17306)
*Kai Fukami,Yuta Iwatani,Soju Maejima,Hiroyuki Asada,Soshi Kawai*

Main category: physics.flu-dyn

TL;DR: This paper presents a machine learning approach to extract low-dimensional representations of transonic airfoil buffet phenomena using lift-augmented autoencoder compression, enabling sensor-based reconstruction and analysis of buffet dynamics at high Reynolds numbers.


<details>
  <summary>Details</summary>
Motivation: Transonic buffet exhibits complex nonlinear aerodynamic characteristics but shows dominant low-dimensional patterns. The study aims to leverage machine learning to capture these low-dimensional representations for real-time analysis of buffet conditions in aircraft operations.

Method: Wall-modeled large-eddy simulations of flow over OAT15A supercritical airfoil at Mach numbers 0.715 (non-buffet) and 0.730 (buffet) at Re=3×10^6. Used lift-augmented autoencoder compression to extract 3D latent representations of buffet dynamics.

Result: Successfully extracted a sole three-dimensional latent representation that captures both shock movement and trailing edge separation. Demonstrated sensor-based reconstruction capability and reasonable phase dynamics estimation at higher Reynolds number (Re=3×10^7) using sparse sensors.

Conclusion: The method provides a foundation for data-driven real-time analysis of transonic buffet conditions, showing potential for practical aircraft operation applications through low-dimensional representation of complex buffet phenomena.

Abstract: Transonic buffet presents time-dependent aerodynamic characteristics
associated with shock, turbulent boundary layer, and their interactions.
Despite strong nonlinearities and a large degree of freedom, there exists a
dominant dynamic pattern of a buffet cycle, suggesting the low dimensionality
of transonic buffet phenomena. This study seeks a low-dimensional
representation of transonic airfoil buffet at a high Reynolds number with
machine learning. Wall-modeled large-eddy simulations of flow over the OAT15A
supercritical airfoil at two Mach numbers, $M_\infty = 0.715$ and 0.730,
respectively producing non-buffet and buffet conditions, at a chord-based
Reynolds number of $Re = 3\times 10^6$ are performed to generate the present
datasets. We find that the low-dimensional nature of transonic airfoil buffet
can be extracted as a sole three-dimensional latent representation through
lift-augmented autoencoder compression. The current low-order representation
not only describes the shock movement but also captures the moment when the
separation occurs near the trailing edge in a low-order manner. We further show
that it is possible to perform sensor-based reconstruction through the present
low-dimensional expression while identifying the sensitivity with respect to
aerodynamic responses. The present model trained at $Re = 3\times 10^6$ is
lastly evaluated at the level of a real aircraft operation of $Re = 3\times
10^7$, exhibiting that the phase dynamics of lift is reasonably estimated from
sparse sensors. The current study may provide a foundation toward data-driven
real-time analysis of transonic buffet conditions under aircraft operation.

</details>


### [84] [A Comparison of Low and high-Order Methods for the Simulation of Supersonic Jet Flows](https://arxiv.org/abs/2509.17725)
*D. F. Abreu,C. Junqueira-Junior,E. T. V. Dauricio,J. L. F. Azevedo*

Main category: physics.flu-dyn

TL;DR: Comparison of numerical methods for supersonic turbulent jet simulations shows that high-order discontinuous Galerkin solver on unstructured meshes provides better correlation with experimental data than traditional second-order finite difference methods.


<details>
  <summary>Details</summary>
Motivation: To find alternatives to improve the quality of large-eddy simulations for supersonic turbulent jet flows, as previous second-order methods showed discrepancies in jet potential core length and velocity fluctuations.

Method: Compared previous second-order finite difference solver (structured meshes) with new high-order discontinuous Galerkin solver (unstructured meshes), keeping total degrees of freedom constant for fair comparison.

Result: High-order method produced very similar mean velocity distributions but slightly smaller velocity fluctuations that correlate better with experimental data.

Conclusion: Future studies should focus on improving jet inlet boundary conditions to better represent the early stages of jet development.

Abstract: The present work compares results for different numerical methods in search
of alternatives to improve the quality of large-eddy simulations for the
problem of supersonic turbulent jet flows. Previous work has analyzed
supersonic jet flows using a second-order, finite difference solver based on
structured meshes, and the results indicated a shorter potential core of the
jet and different levels of velocity fluctuations. In the present work, the
results of previous simulations are compared to new results using a high-order,
discontinuous Galerkin solver for unstructured meshes. All simulations are
performed keeping the total number of degrees of freedom constant. The results
of the current simulations present very similar mean velocity distributions and
slightly smaller velocity fluctuations, and they seem to correlate better with
the experimental data. The present results indicate that additional studies
should focus on the jet inlet boundary conditions in order to improve the
physical representation of the early stages of the jet development.

</details>


### [85] [Characterization of boundary layers on isothermal and adiabatic curved surfaces of a supersonic turbine cascade](https://arxiv.org/abs/2509.17745)
*Gabriel Y. R. Hamada,Hugo F. S. Lui,William R. Wolf,Carlos Junqueira-Junior*

Main category: physics.flu-dyn

TL;DR: This paper investigates how adiabatic vs isothermal boundary conditions affect shock-boundary layer interactions in a supersonic turbine cascade using large eddy simulations at Mach 2.0.


<details>
  <summary>Details</summary>
Motivation: To understand how different thermal boundary conditions (adiabatic vs cooled walls) influence boundary layer characteristics and shock-induced separation in supersonic turbine cascades, which is crucial for turbine design and performance.

Method: Large eddy simulations (LES) were performed for an inlet Mach number of 2.0 and Reynolds number of 200,000 based on axial chord. Two thermal conditions were studied: adiabatic walls and isothermal walls with T_w/T_∞=0.75 (cooled walls).

Result: Adiabatic boundary layers are more prone to separation than isothermal ones on the suction side, forming larger separation bubbles. On the pressure side, separation occurs at the same chord position regardless of thermal condition. Different shock topologies (oblique vs normal shocks) lead to different separation behaviors on suction and pressure sides.

Conclusion: Thermal boundary conditions significantly affect shock-boundary layer interactions, particularly on the suction side where adiabatic walls promote larger separation. The pressure side behavior remains consistent across thermal conditions, suggesting different underlying mechanisms that require further investigation.

Abstract: The effects of adiabatic and isothermal boundary conditions are investigated
on the shock-boundary layer interactions (SBLIs) in a supersonic turbine
cascade. Special attention is given to the characterization of the incoming
boundary layers over the convex and concave walls of the blade and their impact
in the SBLIs. Large eddy simulations (LES) are performed for an inlet Mach
number of $\mathbf{M_\infty = 2.0}$ and Reynolds number based on the axial
chord $\mathbf{Re = 200\,000}$. For the isothermal condition, the wall to inlet
temperature ratio is $\mathbf{T_w/T_{\infty}=0.75}$, representing a cooled
wall. Different incident shock wave topologies occur on the suction and
pressure sides of the airfoil. For the former, an oblique shock impinges on the
boundary layer leading to a larger separation bubble. On the other hand, a
normal shock from a Mach reflection induces a small separation region near the
wall for the pressure side. Results are presented in terms of mean velocity and
temperature contours, and the incoming boundary layers are characterized by
looking at the Clauser parameter, shape factor, dilatation and turbulent
kinetic energy (TKE) profiles. Inspection of the shape factors show that the
adiabatic wall boundary layers are more prone to separate than the isothermal
ones. This is indeed observed in the airfoil suction side, but not on the
pressure side, where the flow separates in the same chord position regardless
of the thermal boundary condition. This is a topic for investigation in the
final version of the paper. An assessment of the dilatation and TKE profiles
explains the disparities of the bubble sizes on the pressure and suction sides
of the airfoil.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [86] [Quantifying the Effects of Parameters in Widespread SEP Events with EPREM](https://arxiv.org/abs/2509.16401)
*Matthew A. Young,Bala Poduval*

Main category: astro-ph.SR

TL;DR: Analysis of how multiple physical parameters affect widespread solar energetic particle (SEP) events using the uncoupled EPREM model with a simple magnetized shock model.


<details>
  <summary>Details</summary>
Motivation: To understand how different physical parameters influence the morphology and spread of SEP events in the heliosphere, providing insights into observed SEP events and solar wind conditions.

Method: Used the uncoupled EPREM model with a simple magnetized shock propagating through inner heliosphere, comparing baseline simulation with seven parameter variations (diffusion, mean free path, shock profile).

Result: All simulations showed complex SEP flux profiles with clear parameter dependencies. While all exhibited significant longitudinal spread, some parameter values caused decreased/absent SEP flux at observers ≥90° from shock origin.

Conclusion: Parameter variations provide insight into SEP event morphology and the state of solar wind through which CMEs propagate, helping understand observed SEP event characteristics.

Abstract: The Energetic Particle Radiation Environment Model (EPREM) solves the focused
transport equation (FTE) on a Lagrangian grid in a frame co-moving with the
solar wind plasma and simulates the acceleration and transport of solar
energetic particles (SEP) in the heliosphere. When not coupled to an external
magnetohydrodynamic model, EPREM functions in an uncoupled mode where an ideal
cone-shock is injected into a homogeneous background solar wind. We carried out
an analysis of the effects of multiple physical parameters in producing
widespread SEP events simulated by the uncoupled EPREM using a relatively
simple model of a strong magnetized shock propagating radially outward through
the inner heliosphere to produce the requisite MHD quantities for EPREM's
sophisticated model of proton acceleration and transport. We compared a
baseline simulation with seven variations in which the value of a single
parameter differed from its baseline value. All simulations exhibit complex
profiles of SEP flux as a function of time and energy, with clear dependence on
parameters related to diffusion, mean free path, and shock profile. Moreover,
while all simulations exhibit significant longitudinal spread in SEP flux, for
certain parameter values there exists a decrease or absence in SEP flux at
observers located >=90 degrees from the shock origin. Relating the differences
in SEP flux to the specific values of each parameter in the simulations
provides insight into the morphology of observed SEP events and the state of
the solar wind through which the driving CME propagates.

</details>


### [87] [Magnetic flux ropes within reconnection exhausts close to the centers of heliospheric current sheets near the Sun](https://arxiv.org/abs/2509.16849)
*Dae-Young Lee,Dooyoung Choi,Kyung-Eun Choi,Sung Jun Noh*

Main category: astro-ph.SR

TL;DR: Analysis of Parker Solar Probe observations reveals small-scale flux ropes embedded within reconnection exhausts near the heliospheric current sheet at 12 solar radii, attributed to secondary reconnection processes.


<details>
  <summary>Details</summary>
Motivation: To understand the relationship between magnetic flux ropes and magnetic reconnection in space and astrophysical plasmas, particularly near the Sun where high ambient magnetic fields can obscure such structures.

Method: Used Parker Solar Probe (PSP) data from two consecutive heliospheric current sheet crossings at ~12 solar radii, identifying flux ropes through magnetic field enhancements, guide field components, travel speeds, and plasma parameters.

Result: Identified flux ropes with durations <20 seconds (spatial scales of thousands of km) embedded in reconnection exhausts, showing enhanced magnetic field strength, faster travel speeds, and often increased density with reduced temperature.

Conclusion: These flux ropes likely form through secondary reconnection within exhausts and subsequent merging, and are most identifiable at the HCS center where weak background fields make their field enhancements prominent, especially closer to the Sun.

Abstract: Understanding the relationship between magnetic flux ropes and magnetic
reconnection is fundamental to both space and astrophysical plasma studies. In
this study, we report on two consecutive heliospheric current sheet (HCS)
crossings by Parker Solar Probe (PSP), separated by ~10.5 hours, at a
heliocentric distance of ~12 solar radii. For each crossing, we identified a
series of flux ropes embedded within reconnection exhausts on the sunward side
of X-line. Their passage durations are <20sec, corresponding to spatial scales
of a few thousands kilometers, still larger by three orders of magnitude than
ion inertial length. This identification was possible particularly during
intervals when PSP was closest to the HCS center. These flux ropes are
distinguishable from the background exhausts by enhancements in magnetic field
strength, significantly in the guide field component, travel speed slightly
faster (typically by <10km/s) than surrounding outflows, and often accompanied
by, though not always, increased density and reduced temperature. We attribute
their origin to secondary reconnection within the exhausts and subsequent
merging of smaller flux ropes into larger structures, consistent with
predictions by various simulations. We suggest that such flux ropes are most
readily identifiable at the HCS center where the background magnetic field is
weakest so that the relative enhancement in flux rope field becomes most
prominent. This observational advantage is particularly notable closer to the
Sun where the high ambient magnetic field strength can otherwise obscure such
structures unless the spacecraft trajectory remains within the HCS central
region for a sufficient duration.

</details>


### [88] [Ion-scale Turbulence and Energy Cascade Rate in the Solar Corona and Inner Heliosphere](https://arxiv.org/abs/2509.17861)
*Eduard P. Kontar,A. Gordon Emslie,Daniel L. Clarkson,Alexander Pitna*

Main category: astro-ph.SR

TL;DR: This paper analyzes plasma turbulence in the heliosphere using radio burst data and in-situ measurements, finding consistency with kinetic Alfvén waves and calculating energy cascade rates from the corona to 1 AU.


<details>
  <summary>Details</summary>
Motivation: To better understand plasma turbulence properties in the heliosphere, which plays a key role in coronal heating and solar wind acceleration but remains poorly constrained by observations.

Method: Compare ion-scale density fluctuations from solar radio bursts with magnetic field fluctuations from in-situ measurements, use kinetic Alfvén wave scenario to deduce magnetic fluctuation amplitudes near the Sun, and calculate energy cascade rate profiles.

Result: Observed magnetic and density fluctuation amplitudes are consistent with kinetic Alfvén waves across broad distances. The calculated cascade rate agrees with in-situ measurements and provides predictions closer to the Sun where direct measurements are unavailable.

Conclusion: The study provides unique diagnostics of ion-scale plasma turbulence amplitude and energy cascade rate spanning three orders of magnitude in solar distance, offering insights into solar wind acceleration mechanisms.

Abstract: Plasma turbulence cascading from MHD to kinetic scales in the heliospheric
plasma is believed to play a key role in coronal heating and fast solar wind
acceleration, but the properties of the turbulence remain poorly constrained by
observations. Here we compare the ion-scale density fluctuation levels inferred
from the properties of solar radio bursts with the magnetic field fluctuation
levels obtained through in-situ measurements in the inner heliosphere. We find
that the observed magnetic and density fluctuation amplitudes are consistent
with excitation by kinetic Alfv\'en waves and/or KAW structures over broad
range of distances from the Sun. We then use the radio diagnostics and the KAW
scenario to deduce the radial variation of magnetic fluctuation amplitudes in
regions close to the Sun where in-situ measurements cannot be obtained.
Further, we calculate the energy cascade rate (plasma heating rate) profile
over a region that extends from the low corona ($\sim 0.1$~R$_\odot$) into the
heliosphere (out to $\sim 1$~au), and compare it to the energy deposition rate
required to drive the solar wind. The cascade rate agrees with the available
in-situ measurements and also provides predictions closer than $\sim
10$~R$_\odot$ where in-situ approaches are not available. The results provide
unique diagnostics of the ion-scale plasma turbulence amplitude and energy
cascade rate spanning over three orders of magnitude in solar distance.

</details>


### [89] [Impact of nonthermal electron distributions on the triggering of the ion-ion acoustic instability near the Sun: Kinetic simulations](https://arxiv.org/abs/2509.18032)
*M. S. Afify,J. Dreher,S. O'Neill,M. E. Innocenti*

Main category: astro-ph.SR

TL;DR: This paper investigates whether non-Maxwellian electron distributions (kappa and core-strahl) can explain observed ion-ion acoustic instability (IIAI) in solar wind conditions measured by Parker Solar Probe, finding that while thresholds change slightly, they don't significantly alter the instability requirements.


<details>
  <summary>Details</summary>
Motivation: Previous research found that observed solar wind parameters are close to IIAI instability threshold but require higher electron temperatures than observed. Since solar wind electron distributions show non-Maxwellian features, the authors investigate if these deviations could explain the observed IIAI.

Method: The study uses analytical methods and kinetic simulations with a Vlasov-Poisson code in parameter regimes relevant to Parker Solar Probe observations. Simulated growth rates are validated against kinetic theory.

Result: The IIAI threshold changes slightly with kappa or core-strahl electron distributions but not significantly. For core-strahl distributions, the effective temperature concept from Jones et al. (1975) is confirmed by simulations.

Conclusion: Non-Maxwellian electron distributions don't significantly alter the IIAI threshold. The validated effective temperature concept could simplify stability assessment for future solar wind observations.

Abstract: Context. In a previous paper (Afify et al. 2024), we have investigated the
stability threshold of the ion-ion acoustic instability (IIAI) in parameter
regimes compatible with recent Parker Solar Probe (PSP, (Fox et al. 2016))
observations, in the presence of a Maxwellian electron distribution. We found
that observed parameters are close to the instability threshold, but IIAI
requires a higher electron temperature than observed. Aims. As electron
distributions in the solar wind present clear non-Maxwellian features, we
investigate here if deviations from the Maxwellian distribution could explain
the observed IIAI. We address specifically the kappa ( $\kappa$ ) and
core-strahl distributions for the electrons. Methods. We perform analytical
studies and kinetic simulations using a Vlasov-Poisson code in a parameter
regime relevant to PSP observations. The simulated growth rates are validated
against kinetic theory. Results. We show that the IIAI threshold changes in the
presence of $\kappa$ or core-strahl electron distributions, but not
significantly. In the latter case, the expression of an effective temperature
for an equivalent Maxwellian electron distribution given in Jones et al. (1975)
is confirmed by simulations. Such an effective temperature could simplify
stability assessment of future observations.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [90] [Monte Carlo on a single sample](https://arxiv.org/abs/2509.17025)
*Nils Detering,Paul Eisenberg,Nicole Hufnagel*

Main category: math.ST

TL;DR: MinMC is a Monte Carlo simulation method that approximates prices and risk measures for multiple model parameters simultaneously using neural network optimization, outperforming traditional Monte Carlo approaches.


<details>
  <summary>Details</summary>
Motivation: Traditional Monte Carlo methods require separate simulations for each parameter value, which is computationally expensive. MinMC aims to efficiently handle parameter-dependent simulations that appear in quantitative finance and scientific computing.

Method: The method approximates expectations across all parameters by minimizing a map V:H→ℝ using a feedforward neural network trained with stochastic gradient descent, requiring only one sample per model parameter.

Result: MinMC significantly outperforms traditional Monte Carlo methods that perform simulations for multiple parameter values and then interpolate, demonstrating superior efficiency and accuracy.

Conclusion: MinMC serves as a new benchmark for parameter-dependent Monte Carlo simulations, offering a more efficient approach for applications in quantitative finance and scientific computing.

Abstract: In this paper, we consider a Monte Carlo simulation method (MinMC) that
approximates prices and risk measures for a range $\Gamma$ of model parameters
at once. The simulation method that we study has recently gained popularity
[HS20, FPP22, BDG24], and we provide a theoretical framework and convergence
rates for it. In particular, we show that sample-based approximations to
$\mathbb{E}_{\theta}[X]$, where $\theta$ denotes the model and
$\mathbb{E}_{\theta}$ the expectation with respect to the distribution
$P_\theta$ of the model $\theta$, can be obtained across all $\theta \in
\Gamma$ by minimizing a map $V:H\rightarrow \mathbb{R}$ with $H$ a suitable
function space. The minimization can be achieved easily by fitting a standard
feedforward neural network with stochastic gradient descent. We show that
MinMC, which uses only one sample for each model, significantly outperforms a
traditional Monte Carlo method performed for multiple values of $\theta$, which
are subsequently interpolated. Our case study suggests that MinMC might serve
as a new benchmark for parameter-dependent Monte Carlo simulations, which
appear not only in quantitative finance but also in many other areas of
scientific computing.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [91] [Generalized comparison principle for contact Hamilton-Jacobi equations](https://arxiv.org/abs/2509.17310)
*Gengyu Liu,Jianlu Zhang*

Main category: math.DS

TL;DR: Analysis of contact Hamilton-Jacobi equations on closed manifolds with convex, coercive Hamiltonians that are non-decreasing in u, focusing on comparison principles using Mather measures and characterizing solvability conditions.


<details>
  <summary>Details</summary>
Motivation: To study the solvability and solution structure of contact Hamilton-Jacobi equations with specific convexity and coercivity conditions, extending previous work on Hamilton-Jacobi equations to the contact case.

Method: Using dynamical information from Mather measures to establish comparison principles for viscosity solutions, then characterizing the set of constants c for which the equation is solvable.

Result: Development of comparison principles and complete characterization of the solvability set 𝔠 for the contact Hamilton-Jacobi equation under the given conditions.

Conclusion: The approach provides optimal conditions for solvability of contact Hamilton-Jacobi equations and demonstrates the effectiveness of using Mather measures in this context.

Abstract: In this paper, we discuss all the possible pairs $(u,c)\in C(M,\mathbb
R)\times\mathbb R$ solving (in the sense of viscosity) the contact
Hamilton-Jacobi equation \[ H (x, d_xu, u) = c,\quad x\in M \] of which $M$ is
a closed manifold and the continuous Hamiltonian $H: (x,p,u)\in
T^*M\times\mathbb R\rightarrow\mathbb R$ is convex, coercive in $p$ but merely
non-decreasing in $u$. Firstly, we propose a comparison principle for solutions
by using the dynamical information of Mather measures. We then describe the
structure of $\mathfrak C$ containing all the $c\in\mathbb R$ makes previous
equation solvable. We also propose examples to verify the optimality of our
approach.

</details>


<div id='physics.space-ph'></div>

# physics.space-ph [[Back]](#toc)

### [92] [The Nature of Turbulence at Sub-Electron Scales in the Solar Wind](https://arxiv.org/abs/2509.17061)
*Shiladittya Mondal,Christopher H. K. Chen,Davide Manzini*

Main category: physics.space-ph

TL;DR: Analysis of Parker Solar Probe data reveals that turbulence becomes electrostatic at sub-electron scales, with density spectra showing -10/3 slopes consistent with electron entropy cascade theory.


<details>
  <summary>Details</summary>
Motivation: To understand how electrons are heated in the solar wind by studying turbulence at sub-electron scales, which was previously limited due to spacecraft measurement constraints.

Method: Derived high-resolution density fluctuations from Parker Solar Probe spacecraft potential measurements, enabling analysis of scales smaller than electron gyro-radius, and systematically compared density and magnetic spectra.

Result: Both density and magnetic spectra steepen near electron scales, with density spectrum showing slopes close to -10/3 and magnetic spectrum becoming steeper than density spectrum at sub-electron scales, indicating electrostatic turbulence.

Conclusion: The results support theoretical predictions of an electron entropy cascade, which may explain irreversible dissipation of turbulent energy at sub-electron scales in the solar wind.

Abstract: The nature of turbulence at sub-electron scales has remained an open
question, central to understanding how electrons are heated in the solar wind.
This is primarily because spacecraft measurements have been limited to magnetic
field fluctuations alone. We resolve this by deriving new high-resolution
density fluctuations from spacecraft potential measurements of Parker Solar
Probe resolving scales smaller than the electron gyro-radius ($\rho_e$). A
systematic comparison of the density and magnetic spectra shows that both
steepen near the electron scales. Notably, the density spectrum exhibits slopes
close to $-10/3$, while the magnetic spectrum becomes consistently steeper than
the density spectrum at scales smaller than $\rho_e$, indicating that the
turbulence becomes electrostatic. These results are consistent with theoretical
predictions of an electron entropy cascade, which may explain the irreversible
dissipation of turbulent energy at sub-$\rho_e$ scales.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [93] [Functional Information in Quantum Darwinism: An Operational Measure of Objectivity](https://arxiv.org/abs/2509.17775)
*Arda Batin Tank*

Main category: quant-ph

TL;DR: This paper introduces Functional Information in Quantum Darwinism (FI_QD) as a measure to quantify classical objectivity in quantum systems, showing how redundant environmental records emerge through numerical simulations of dephasing models.


<details>
  <summary>Details</summary>
Motivation: To establish a practical and quantifiable measure for classical objectivity in quantum systems, moving beyond assumptions to demonstrate how redundant information emerges in environmental fragments about system pointer states.

Method: Uses the Holevo quantity as an upper bound on accessible information and introduces δ-adequacy criterion. Performs numerical simulations of dephasing models with fragment sampling to analyze redundancy growth patterns.

Result: Simulations reveal three robust features: linear early-time growth of redundancy, capacity-limited plateaus determined by fragment size, and stability under different sampling strategies. FI_QD proves to be a conservative and practical measure of operational objectivity.

Conclusion: Classical objectivity emerges as a quantifiable, resource-limited abundance of redundant records rather than an assumption. FI_QD serves as a resource monotone under noisy dynamics and links redundancy growth to thermodynamic costs of record formation.

Abstract: This paper investigates the emergence of classical objectivity in quantum
systems through the measure of Functional Information in Quantum Darwinism
($FI_{QD}$). The goal is to quantify objectivity as the abundance of
environment fragments that independently contain sufficient information about a
system's pointer states. The method relies on the Holevo quantity -- an upper
bound on accessible information -- and introduces a tolerance criterion called
$\delta$-adequacy, where fragments are considered adequate if they retain at
least $(1-\delta)H_S$ bits of pointer information. Numerical simulations of a
dephasing model with fragment sampling reveal three robust features: (i) an
early-time regime where $\log R_\delta(t)$ grows approximately linearly, (ii)
capacity-limited plateaus determined by fragment size and environment
dimension, and (iii) stability of the onset criterion under different sampling
strategies and overlap corrections. These results establish $FI_{QD}$ as a
practical and conservative yardstick for operational objectivity. Beyond
numerical findings, the analysis links redundancy growth to thermodynamic costs
of record formation and interprets $FI_{QD}$ as a resource monotone under noisy
dynamics. The study suggests that classical objectivity emerges not as an
assumption but as a quantifiable, resource-limited abundance of redundant
records.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [94] [Optimal local basis truncation of lattice quantum many-body systems](https://arxiv.org/abs/2509.17975)
*Peter Majcen,Giovanni Cataldi,Pietro Silvi,Simone Montangero*

Main category: cond-mat.str-el

TL;DR: Optimal reduction of local Hilbert basis for lattice quantum many-body Hamiltonians using eigenvalues of single-site reduced density matrix


<details>
  <summary>Details</summary>
Motivation: To reduce computational resources needed for classical and quantum simulations of quantum many-body systems while maintaining accuracy across different model phases

Method: Basis truncation exploiting the most relevant eigenvalues of the estimated single-site reduced density matrix (RDM)

Result: Accurate and numerically stable basis reduction across different model phases, even near quantum phase transitions; applied successfully to Sine-Gordon model, φ⁴ theory, and lattice gauge theories (U(1) and SU(2)) in 1D and 2D

Conclusion: The method significantly reduces state-of-the-art estimates of computational resources required for classical and quantum simulations

Abstract: We show how to optimally reduce the local Hilbert basis of lattice quantum
many-body (QMB) Hamiltonians. The basis truncation exploits the most relevant
eigenvalues of the estimated single-site reduced density matrix (RDM). It is
accurate and numerically stable across different model phases, even close to
quantum phase transitions. We apply this procedure to different models, such as
the Sine-Gordon model, the $\varphi^{4}$ theory, and lattice gauge theories,
namely Abelian $\mathrm{U}(1)$ and non-Abelian $\mathrm{SU}(2)$, in one and two
spatial dimensions. Our results reduce state-of-the-art estimates of
computational resources for classical and quantum simulations.

</details>
