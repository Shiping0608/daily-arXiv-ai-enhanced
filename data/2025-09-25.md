<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 12]
- [math.AP](#math.AP) [Total: 8]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 4]
- [math.CV](#math.CV) [Total: 1]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [astro-ph.SR](#astro-ph.SR) [Total: 3]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 2]
- [math.GR](#math.GR) [Total: 1]
- [math.FA](#math.FA) [Total: 2]
- [stat.ML](#stat.ML) [Total: 1]
- [math-ph](#math-ph) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [physics.space-ph](#physics.space-ph) [Total: 1]
- [math.OC](#math.OC) [Total: 2]
- [cs.DS](#cs.DS) [Total: 1]
- [quant-ph](#quant-ph) [Total: 3]
- [gr-qc](#gr-qc) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [Stability of high-order Scott-Vogelius elements for 2D non-Newtonian incompressible flow](https://arxiv.org/abs/2509.19488)
*Charles Parker,Endre SÃ¼li*

Main category: math.NA

TL;DR: Analysis of high-order Scott-Vogelius elements for 2D non-Newtonian incompressible flow, showing stability results and superior convergence rates for p-version methods.


<details>
  <summary>Details</summary>
Motivation: To establish stability properties of high-order finite elements for non-Newtonian flow problems, particularly investigating how stability constants behave with increasing polynomial degree.

Method: Constructed a right-inverse of the divergence operator stable uniformly in polynomial degree N, analyzed inf-sup constant bounds, and built local Fortin operators with explicit stability constants in N.

Result: Inf-sup constant bounded below by constant decaying like N^{-3|1/2 - 1/p|}, with numerical examples showing p-version methods outperform h-version in non-Newtonian setting.

Conclusion: High-order Scott-Vogelius elements maintain stability for non-Newtonian flows, and p-version methods offer superior convergence rates compared to traditional h-version approaches.

Abstract: We consider the stability of high-order Scott-Vogelius elements for 2D
non-Newtonian incompressible flow problems. For elements of degree 4 or higher,
we construct a right-inverse of the divergence operator that is stable
uniformly in the polynomial degree $N$ from $L^p$ to $\boldsymbol{W}^{1,p}$,
show that the associated inf-sup constant is bounded below by a constant that
decays at worst like $N^{-3\left| \frac{1}{2} - \frac{1}{p}\right|}$, and
construct local Fortin operators with stability constants explicit in the
polynomial degree. We demonstrate these results with several numerical examples
suggesting that the $p$-version method can offer superior convergence rates
over the $h$-version method even in the non-Newtonian setting.

</details>


### [2] [Spectral theory of matrix-sequences: perspectives of the GLT analysis and beyond](https://arxiv.org/abs/2509.19544)
*Stefano Serra-Capizzano*

Main category: math.NA

TL;DR: This paper discusses open problems and future challenges in the analysis of distribution results for eigenvalues and singular values of matrix-sequences, particularly focusing on generalized locally Toeplitz matrix-sequences and their applications to numerical methods for various equations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to build upon recent advances in the analysis of matrix-sequences with hidden asymptotic structure, moving from specific results to identifying key open problems and challenges for future research in this field.

Method: The paper takes a conceptual approach by reviewing recent developments and systematically identifying open problems rather than presenting new mathematical proofs or specific analytical methods.

Result: The main result is a comprehensive identification of open problems and research challenges in the field of matrix-sequence analysis, particularly regarding generalized locally Toeplitz matrix-sequences and their applications to numerical methods for differential and integral equations.

Conclusion: The paper concludes by outlining a roadmap for future research directions in the analysis of matrix-sequences, emphasizing the need to address these open problems to advance the understanding of collective eigenvalue and singular value behavior in various numerical approximation contexts.

Abstract: In recent years there has been a growing attention on distribution results in
the sense of Weyl for the collective behavior of eigenvalues and singular
values of matrix-sequences. Starting from the work of Szeg\"o regarding the
case of Toeplitz matrix-sequences, there has been a wealth of associated
results, which culminated in the works of Tilli and of Tyrtyshnikov,
Zamarashkin, and of the author for preconditioned and non-preconditioned
$r$-block $d$-level Toeplitz matrix-sequences with Lebesgue integrable
generating functions. In the latter the use of matrix-valued linear positive
operators and related Korovkin theories has been crucial. The subsequent steps
induced by the analysis of preconditioning techniques and inspired by the rich
world of the (pseudo) differential operators have been studies from the same
perspective of matrix-sequences with hidden (asymptotic) structure, widely
studied in the literature. The widest generalization is represented by the
notion of generalized locally Toeplitz matrix-sequences, which have inherent
hidden structure and which include virtually any approximation via local
numerical methods of (systems of) integral equations, partial and fractional
differential equations, also with nonsmooth variable coefficients and irregular
bounded/unbounded domains/manifolds.
  In the current work, instead of focusing on a specific type of results,
starting from the most recent advances on the topic, we describe shortly a
series of open problems, challenges to be developed in the future.

</details>


### [3] [HPL-MxP Benchmark: Mixed-Precision Algorithms, Iterative Refinement, and Scalable Data Generation](https://arxiv.org/abs/2509.19618)
*Jack Dongarra,Piotr Luszczek*

Main category: math.NA

TL;DR: HPL-MxP is a mixed-precision benchmark using lower-precision LU factorization with GMRES-based iterative refinement, achieving Exascale-level performance and demonstrating viability for evaluating supercomputers and AI hardware accelerators.


<details>
  <summary>Details</summary>
Motivation: To develop a reliable benchmark for evaluating large-scale supercomputing installations and hardware accelerators for AI workloads, addressing the challenge of reliable performance assessment in mixed-precision computing environments.

Method: Uses mixed-precision approach with lower-precision LU factorization combined with non-stationary iterative refinement based on GMRES. Includes evaluation of numerical stability of input matrix generation methods and analysis of diagonal scaling effects on solution quality through backward-error metrics.

Result: Achieved Exascale-level compute throughput at large-scale supercomputing installations, demonstrating the benchmark's viability. Showed how diagonal scaling affects solution quality and evaluated numerical stability of scalable input matrix generation methods.

Conclusion: HPL-MxP proves to be an effective benchmark for evaluating Exascale computing systems and has strong potential for continued use with the proliferation of AI hardware accelerators, addressing the challenge of reliable performance evaluation in modern computing environments.

Abstract: We present a mixed-precision benchmark called HPL-MxP that uses both a
lower-precision LU factorization with a non-stationary iterative refinement
based on GMRES. We evaluate the numerical stability of one of the methods of
generating the input matrix in a scalable fashion and show how the diagonal
scaling affects the solution quality in terms of the backward-error. Some of
the performance results at large scale supercomputing installations produced
Exascale-level compute throughput numbers thus proving the viability of the
proposed benchmark for evaluating such machines. We also present the potential
of the benchmark to continue increasing its use with proliferation of hardware
accelerators for AI workloads whose reliable evaluation continues to pose a
particular challenge for the users.

</details>


### [4] [Preconditioning via Randomized Range Deflation (RandRAND)](https://arxiv.org/abs/2509.19747)
*Oleg Balabanov,Caleb Ju,Kaiwen He,Aryaman Jeendgar,Michael W. Mahoney*

Main category: math.NA

TL;DR: RandRAND is a new class of randomized preconditioning methods for large-scale linear systems that deflates the spectrum via efficient orthogonal projections onto random subspaces without computing eigenpairs or low-rank approximations.


<details>
  <summary>Details</summary>
Motivation: To develop efficient preconditioning methods for large-scale linear systems that offer advantages in computational cost and numerical stability compared to traditional approaches.

Method: Uses orthogonal projections onto random subspaces for spectrum deflation, employs fast randomized transforms without explicit basis operations, and bypasses costly orthogonalization via fast randomized Q-less QR factorizations or iterative methods.

Result: Establishes rigorous condition number bounds that depend weakly on problem size and reduce to small constants when deflated subspace dimension matches effective spectral dimension.

Conclusion: RandRAND provides robust, computationally efficient preconditioning with balanced construction and application costs, ensuring numerical stability through strategies that mitigate rounding errors.

Abstract: We introduce RandRAND, a new class of randomized preconditioning methods for
large-scale linear systems. RandRAND deflates the spectrum via efficient
orthogonal projections onto random subspaces, without computing eigenpairs or
low-rank approximations. This leads to advantages in computational cost and
numerical stability. We establish rigorous condition number bounds that depend
only weakly on the problem size and that reduce to a small constant when the
dimension of the deflated subspace is comparable to the effective spectral
dimension. RandRAND can be employed without explicit operations with the
deflation basis, enabling the effective use of fast randomized transforms. In
this setting, the costly explicit basis orthogonalization is bypassed by using
fast randomized Q-less QR factorizations or iterative methods for computing
orthogonal projections. These strategies balance the cost of constructing
RandRAND preconditioners and applying them within linear solvers, and can
ensure robustness to rounding errors.

</details>


### [5] [High-order Multiscale Preconditioner for Elasticity of Arbitrary Structures](https://arxiv.org/abs/2509.19777)
*Sabit Mahmood Khan,Yashar Mehmani*

Main category: math.NA

TL;DR: A two-level preconditioner called high-order pore-level multiscale method (hPLMM) for solving linear systems from elliptic, linear-elastic deformation equations on complex heterogeneous domains with fractures.


<details>
  <summary>Details</summary>
Motivation: To efficiently solve deformation equations on geometrically complex domains with material heterogeneities and fractures, overcoming limitations of existing preconditioners like low-order PLMM, Schwarz, and multigrid methods.

Method: Domain decomposition into non-overlapping subdomains with numerically computed local basis functions using mortar spaces that allow interface deformation, capturing bending/twisting moments under challenging loading conditions.

Result: hPLMM demonstrates superior performance in Krylov solvers compared to PLMM, Schwarz, and multigrid preconditioners across various complex porous structures and material heterogeneities.

Conclusion: The high-order approach with deformable interfaces provides a robust preconditioning framework applicable to subsurface CO2/H2 storage risk analysis and optimization of porous materials for batteries, prosthetics, and aircraft.

Abstract: We present a two-level preconditioner for solving linear systems arising from
the discretization of the elliptic, linear-elastic deformation equation, in
displacement unknowns, over domains that have arbitrary geometric and
topological complexity and heterogeneity in material properties (including
fractures). The preconditioner is an algebraic translation of the high-order
pore-level multiscale method (hPLMM) proposed recently by the authors, wherein
a domain is decomposed into non-overlapping subdomains, and local basis
functions are numerically computed over the subdomains to construct a
high-quality coarse space (or prolongation matrix). The term "high-order"
stands in contrast to the recent low-order PLMM preconditioner, where BCs of
local basis problems assume rigidity of all interfaces shared between
subdomains. In hPLMM, interfaces are allowed to deform, through the use of
suitable mortar spaces, thereby capturing local bending/twisting moments under
challenging loading conditions. Benchmarked across a wide range of complex
(porous) structures and material heterogeneities, we find hPLMM exhibits
superior performance in Krylov solvers than PLMM, as well as state-of-the-art
Schwarz and multigrid preconditioners. Applications include risk analysis of
subsurface CO2/H2 storage and optimizing porous materials for batteries,
prosthetics, and aircraft.

</details>


### [6] [A fast direct solver for two-dimensional transmission problems of elastic waves](https://arxiv.org/abs/2509.19986)
*Yasuhiro Matsumoto,Taizo Maruyama*

Main category: math.NA

TL;DR: A fast direct boundary element method for 2D elastodynamic transmission problems that efficiently handles elastic wave scattering by inclusions with linear computational complexity.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of mixed bases strategy where CalderÃ³n preconditioning cannot be applied, preventing the use of iterative solvers for elastodynamic transmission problems.

Method: Developed a fast direct solver using Burton-Miller and PMCHWT boundary integral equations with proxy method for low-rank approximation and separate binary tree partitions for nodes and elements.

Result: The solver achieves linear computational complexity, handles multiple right-sides efficiently, with Burton-Miller formulation being ~20% faster than PMCHWT formulation.

Conclusion: Provides a versatile, fast solver with performance independent of inclusion shape and computational parameters for elastodynamic transmission problems.

Abstract: This paper describes a fast direct boundary element method for elastodynamic
transmission problems in two dimensions, which can be used for analyzing
elastic wave scattering by an inclusion. This paper reports the development of
an efficient solver based on a discretization method that is broadly applicable
regardless of the inclusion shape. From the smoothness of the solutions of the
Navier--Cauchy equation, it is reasonable that the displacement is approximated
by the piecewise linear bases and the traction is approximated by the piecewise
constant bases. However, in this mixed bases strategy, Calder\'on
preconditioning, that is, an analytical preconditioning with excellent
performance, cannot be applied, which means we cannot adopt iterative solvers.
To address this challenge, we developed a fast direct solver formulated using
both Burton--Miller and Poggio--Miller--Chang--Harrington--Wu--Tsai (PMCHWT)
boundary integral equations. Our method uses a technique based on the proxy
method for low-rank approximation of the coefficient matrix's off-diagonal
blocks. To handle transmission problems, the proposed fast direct solver uses
separate binary tree partitions for nodes and elements. Numerical examples
demonstrate that our solver achieves linear computational complexity and can
efficiently handle problems with multiple right-sides. Notably, the solver
based on the Burton--Miller formulation is approximately 20\% faster than the
one using the PMCHWT formulation. Our new method provides a versatile, fast
solver, whose performance is independent of the shape of inclusions and
computational parameters, such as frequency and density, for elastodynamic
transmission problems.

</details>


### [7] [Modelling and Analysis of Non-Contacting Mechanical Face Seals with Axial Disturbances and Misalignment](https://arxiv.org/abs/2509.19993)
*Ben S Ashby,Tristan Pryer,Nicola Y Bailey*

Main category: math.NA

TL;DR: This paper analyzes the dynamic behavior of non-contacting mechanical face seals under misalignment and external disturbances, developing a coupled fluid-structure model to identify safe operating conditions and manufacturing tolerances.


<details>
  <summary>Details</summary>
Motivation: Advancements in industrial applications require improved efficiency and reliability of non-contacting mechanical seals, but external disturbances can cause destabilization and premature failures through unanticipated face contact.

Method: Developed a fully coupled unsteady mathematical model where fluid flow is coupled to structural response of the stator. The stator is modeled as a spring-mass-damper system, and fluid film uses lubrication approximation of Navier-Stokes equations. Solved via finite element and Runge-Kutta methods.

Result: Parameter study reveals impact of misalignment on seal dynamics under external disturbances. Identifies critical misalignment angles and forcing amplitudes where minimum fluid film thickness falls below tolerance, indicating potential failure conditions.

Conclusion: The research provides insights into safe operating conditions and manufacturing tolerances, aiding improved design criteria and reliability of non-contacting mechanical face seals.

Abstract: Advancements in industrial applications are driving developments in
non-contacting
  mechanical seal technology. Key requirements include improvements in
efficiency and
  reliability, which lead to smaller clearances, lower frictional losses, and
  minimisation of wear, during operation. However, a critical consideration is
the
  effect of external disturbances experience by the seal from the local
environment
  which may cause destabilisation, and lead to premature failures through
unanticipated
  face contact.
  This work examines the dynamic behaviour of a non-contacting mechanical face
seals,
  where a thin fluid film separates a pair of coaxial discs; a rotor (rotating
face) and
  stator (stationary face). It is assumed that the rotor-stator have an angular
  misalignment and operation is under conditions involving large axial
disturbances,
  representing external disturbances. A fully coupled unsteady mathematical
  representation is developed, where the fluid flow is coupled to the
structural
  response of the stator, and the rotor motion is prescribed. The stator is
modelled as
  a spring-mass-damper system, and the fluid film model is based on a
lubrication
  approximation of the Navier-Stokes equations. The governing equations are
solved via a
  numerical technique based on finite element and Runge-Kutta methods.
  A parameter study reveals the impact of misalignment on the seal dynamics,
when
  experiencing an external disturbance. The angle of misalignment and
corresponding
  amplitude of forcing can be identified when the minimum fluid film thickness
becomes
  less than a given tolerance. This provides insights into safe operating
conditions and
  manufacturing tolerances, with the research aiding to improve the design
critera and
  reliability of non-contacting mechanical face seals.

</details>


### [8] [A note on the compactness properties of discontinuous Galerkin time discretizations](https://arxiv.org/abs/2509.20039)
*Sergio GÃ³mez*

Main category: math.NA

TL;DR: This paper extends discrete compactness results for high-order discontinuous Galerkin time discretizations of parabolic problems to general Banach space settings, proving a discrete Aubin-Lions-Simon lemma without requiring quasi-uniform time partitions.


<details>
  <summary>Details</summary>
Motivation: To generalize previous discrete compactness results by Walkington (2010) from specific function spaces to more general Banach space settings, removing the restrictive assumption of quasi-uniform time partitions.

Method: The authors develop proofs based on the properties of a time reconstruction operator, which allows them to establish compactness results for high-order discontinuous Galerkin time discretizations in general Banach spaces X, B, Y with X compactly embedded in B and B continuously embedded in Y.

Result: The paper successfully proves a discrete version of the Aubin-Lions-Simon lemma that holds for general Banach spaces, extending previous results and eliminating the need for quasi-uniform time partitions.

Conclusion: This work provides a more general framework for analyzing discrete compactness in high-order discontinuous Galerkin time discretizations of parabolic problems, with broader applicability across various function space settings.

Abstract: This work extends the discrete compactness results of Walkington (SIAM J.
Numer. Anal., 47(6):4680-4710, 2010) for high-order discontinuous Galerkin time
discretizations of parabolic problems to more general function space settings.
In particular, we show a discrete version of the Aubin-Lions-Simon lemma that
holds for general Banach spaces $X$, $B$, and $Y$ satisfying $X \hookrightarrow
B$ compactly and $B \hookrightarrow Y$ continuously. Our proofs rely on the
properties of a time reconstruction operator and remove the need for
quasi-uniform time partitions assumed in previous works.

</details>


### [9] [An Overview of Meshfree Collocation Methods](https://arxiv.org/abs/2509.20056)
*Tomas Halada,Serhii Yaskovets,Abhinav Singh,Ludek Benes,Pratik Suchde,Ivo F. Sbalzarini*

Main category: math.NA

TL;DR: A comprehensive review and classification of meshfree collocation methods for approximating differential operators on unstructured point clouds, with a unifying formulation and generalized derivation approach.


<details>
  <summary>Details</summary>
Motivation: To provide a systematic overview of meshfree collocation methods, highlight subtle but important differences between various approaches, and establish a unifying framework for understanding and developing these methods.

Method: Review and classification of existing meshfree collocation methods from literature, historical analysis of key concepts, and development of a unifying formulation that reveals differences between methods. A generalized derivation approach is proposed.

Result: A comprehensive classification system for meshfree collocation methods based on their derivation principles, and a unifying formulation that makes differences between methods apparent while showing how each can be derived from this common framework.

Conclusion: The paper establishes a systematic foundation for understanding meshfree collocation methods, provides a generalized derivation approach for future development, and highlights the importance of subtle methodological differences that impact practical implementation and performance.

Abstract: We provide a comprehensive overview of meshfree collocation methods for
numerically approximating differential operators on continuously labeled
unstructured point clouds. Meshfree collocation methods do not require a
computational grid or mesh. Instead, they approximate smooth functions and
their derivatives at potentially irregularly distributed collocation points,
often called particles, to a desired order of consistency. We review several
meshfree collocation methods from the literature, trace the historical
development of key concepts, and propose a classification of methods according
to their principle of derivation. Although some of the methods reviewed are
similar or identical, there are subtle yet important differences between many,
which we highlight and discuss. We present a unifying formulation of meshfree
collocation methods that renders these differences apparent and show how each
method can be derived from this formulation. Finally, we propose a generalized
derivation for meshfree collocation methods going forward.

</details>


### [10] [Development of a Model Order Reduced Arbitrary Lagrangian Eulerian (MORALE) formulation for structures subjected to dynamic moving loads](https://arxiv.org/abs/2509.20069)
*Atul Anantheswar,Jannick Kehls,Ines Wollny,Tim Brepols,Stefanie Reese,Michael Kaliske*

Main category: math.NA

TL;DR: A novel MORALE formulation combining Arbitrary Lagrangian Eulerian (ALE) and Model Order Reduction (MOR) techniques for efficient simulation of structures under moving loads.


<details>
  <summary>Details</summary>
Motivation: To enhance computational efficiency for simulating pavement structures subjected to moving loads, which is crucial for digital twin technologies and what-if analyses in roadway infrastructure management.

Method: Combination of ALE formulation and MOR techniques into a novel MORALE formulation, applied to both hyperelastic and viscoelastic material models for pavement simulations.

Result: Significant enhancement in computational speed and efficiency when simulating pavement structures under moving loads.

Conclusion: The MORALE formulation provides an efficient simulation framework vital for digital twins of roadway infrastructure, enabling quick what-if analyses and informed decision-making for structure management.

Abstract: In recent developments, it has been demonstrated that the Arbitrary
Lagrangian Eulerian (ALE) formulation can be utilized to improve computational
efficiency, when simulating the response of structures subjected to moving
loads. It is also well established in literature, that Model Order Reduction
(MOR) techniques significantly enhance calculation speed. This contribution
details the combination of both these tools into a novel Model Order Reduced
Arbitrary Lagrangian Eulerian (MORALE) formulation. Both hyperelastic and
viscoelastic material models are considered. Simulations of pavement structures
subjected to moving loads are then carried out, which show a significant
enhancement in computational speed and efficiency. Such an efficient and fast
simulation framework is of vital importance in technologies such as digital
twins of roadway infrastructure (like pavements), as it enables engineers to
quickly run what-if analyses and make informed decisions about the management
of the structure under consideration.

</details>


### [11] [Efficient Long-Time Simulations of Multiscale Systems via High-Order Numerical Homogenization](https://arxiv.org/abs/2509.20108)
*Bojin Chen,Zeyu Jin,Ruo Li*

Main category: math.NA

TL;DR: Development of an efficient multigrid-in-time algorithm for long-time simulations of differential equations with two time scales, combining high-order coarse-grid approximations with low-order fine-grid evaluations.


<details>
  <summary>Details</summary>
Motivation: To enable long-time simulations of multiscale differential equations that were previously computationally prohibitive, building on previous high-order homogenization methods.

Method: Multigrid-in-time method that combines coarse-grid high-order approximations with fine-grid low-order evaluations to minimize computational cost while maintaining accuracy.

Result: The algorithm achieves high efficiency through optimized computational cost while guaranteeing approximation accuracy, with rigorous a priori error estimates established and validated numerically.

Conclusion: The proposed multigrid-in-time method successfully enables efficient long-time simulations of multiscale differential equations, representing a significant advancement over previous computational limitations.

Abstract: By a high-order numerical homogenization method, a heterogeneous multiscale
scheme was developed in Jin & Li (2022) for evolving differential equations
containing two time scales. In this paper, we further explore the technique to
propose an efficient algorithm able to carry out simulations up to a long time
which was prohibitive before. The new algorithm is a multigrid-in-time method
which combines coarse-grid high-order approximations with fine-grid low-order
evaluations. The high efficiency is attained by minimizing the computational
cost while the approximation accuracy is guaranteed. A priori error estimates
are rigorously established and validated by numerical examples.

</details>


### [12] [A convergent finite element method for two-phase Stokes flow driven by surface tension](https://arxiv.org/abs/2509.20111)
*Genming Bai,Harald Garcke,Shravan Veerapaneni*

Main category: math.NA

TL;DR: First convergence proof for iso-parametric finite element discretization of two-phase Stokes flow with interface dynamics governed by mean curvature


<details>
  <summary>Details</summary>
Motivation: To provide fundamental numerical analysis tools for general curvature-driven free boundary problems by establishing convergence for this complex fluid-structure interaction problem

Method: Uses iso-parametric finite element discretization with a crucial discrete coupled parabolicity structure of the error system and a novel mixing approach that doesn't discriminate between consistency and stability

Result: Successfully proves convergence for the discretization scheme in 2D and 3D domains, with non-trivial construction of bulk mesh in consistency analysis

Conclusion: The techniques and analysis developed provide essential tools for numerical analysis of general curvature-driven free boundary problems

Abstract: We present the first convergence proof for an iso-parametric finite element
discretization of two-phase Stokes flow in $\Omega \subset \mathbb{R}^d$,
$d=2,3$, with interface dynamics governed by mean curvature. The proof relies
on a crucial discrete coupled parabolicity structure of the error system and a
powerful iso-parametric framework of convergence analysis where we do not
really discriminate consistency and stability. This new mixing idea leads to a
non-trivial construction of the bulk mesh in the consistency analysis. The
techniques and analysis developed in this paper provide fundamental numerical
analysis tools for general curvature-driven free boundary problems.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [13] [Existence of flat flows for volume-preserving mean curvature flow with contact angle](https://arxiv.org/abs/2509.19470)
*Jiwoong Jang*

Main category: math.AP

TL;DR: Existence of global-in-time weak solution for droplet motion by mean curvature with volume constraint and contact angle condition on half space.


<details>
  <summary>Details</summary>
Motivation: To study the motion of droplets evolving by mean curvature under volume constraints and contact angle conditions, which is a fundamental problem in fluid dynamics and interfacial phenomena.

Method: Proving existence of flat flow (global-in-time weak solution) using blowup analysis at contact points to handle difficulties in establishing local equi-boundedness of approximate solutions and uniform LÂ²-estimate of multipliers.

Result: Successfully established the existence of a global-in-time weak solution for the droplet motion problem.

Conclusion: The blowup analysis technique effectively resolves the key difficulties in proving the existence of flat flow solutions for mean curvature-driven droplet motion with constraints.

Abstract: We study the motion of a droplet evolving by mean curvature with volume
constraint and contact angle condition on a half space. We prove the existence
of a global-in-time weak solution, called the flat flow. A difficulty arises
when we establish the local-in-time equi-boundedness of approximate solutions
and a uniform $L^2$-estimate of multipliers. The difficulty is handled by
conducting blowup analysis at a point in contact to a spherical cap with sharp
angle.

</details>


### [14] [The preservation of nonnegativity of solutions of a parabolic system with the cubed Laplacian](https://arxiv.org/abs/2509.19520)
*Messoud Efendiev,Vitali Vougalter*

Main category: math.AP

TL;DR: The paper presents a verifiable necessary condition for preserving nonnegativity in solutions of parabolic equations with cubed Laplacian, which is crucial for applied analysis.


<details>
  <summary>Details</summary>
Motivation: To establish necessary conditions that ensure nonnegative solutions in systems of parabolic equations containing cubed Laplacian, which is important for mathematical modeling in applied analysis.

Method: The authors develop and verify a necessary condition through analytical methods for systems of parabolic equations with cubed Laplacian operators.

Result: A verifiable necessary condition is derived that must be satisfied by the system of equations to preserve nonnegativity of solutions.

Conclusion: The established necessary condition provides an essential mathematical constraint that guides the proper formulation and study of parabolic equation systems in applied analysis.

Abstract: The article is devoted to the easily verifiable necessary condition of the
preservation of the nonnegativity of the solutions of a system of parabolic
equations containing the cubed Laplacian. Such necessary condition is extremely
important for the applied analysis community since it imposes the necessary
form of the system of equations that must be studied mathematically.

</details>


### [15] [Scattering of plane waves](https://arxiv.org/abs/2509.19716)
*Narek Hovsepyan,Michael S. Vogelius*

Main category: math.AP

TL;DR: This paper presents a variation of the Pompeiu/Schiffer problem applied to scattering of plane waves for the Linear Helmholtz equation, establishing conditions for non-vanishing scattered fields in 2D.


<details>
  <summary>Details</summary>
Motivation: To extend the classical Pompeiu/Schiffer problem to the context of wave scattering, specifically investigating when scattered fields must be non-zero under certain wave conditions.

Method: The authors formulate a natural variation of the Pompeiu/Schiffer problem for the Linear Helmholtz equation and analyze the two-dimensional case, establishing mathematical conditions on wave numbers and incident directions.

Result: The paper establishes specific conditions on wave numbers and incident directions that guarantee a non-vanishing scattered field in the two-dimensional scattering problem.

Conclusion: The research provides new insights into wave scattering phenomena by connecting classical mathematical problems with practical scattering theory, offering conditions that ensure non-trivial scattering behavior.

Abstract: We formulate a problem that can be viewed as a natural variation of the
so-called Pompeiu or Schiffer problem in the context of scattering of plane
waves for the Linear Helmholtz equation. For the two dimensional version of
this variation, we establish conditions on the wave numbers and incident
directions, that ensure a non-vanishing scattered field.

</details>


### [16] [Boundary effect on asymptotic behaviour of solution to the hyperbolic-parabolic chemotaxis system](https://arxiv.org/abs/2509.19828)
*Nangao Zhang*

Main category: math.AP

TL;DR: Global existence and stability analysis of quasi-linear hyperbolic-parabolic chemotaxis system on half-line describing vascular network formation


<details>
  <summary>Details</summary>
Motivation: To describe the formation of coherent vascular networks observed in vitro experiments

Method: Establish existence and uniqueness of asymptotic state equation (nonlinear diffusion wave) under two different boundary conditions, then prove nonlinear asymptotic stability of original equation solution

Result: Proved global existence and nonlinear asymptotic stability of solutions, obtained convergence rates for both boundary conditions

Conclusion: The chemotaxis system solution is nonlinearly asymptotically stable and converges to the nonlinear diffusion wave with specific rates under different boundary conditions

Abstract: This paper is concerned with the global existence and stability of solution
to the quasi linear hyperbolic-parabolic chemotaxis system on the
half-line,which was proposed in[1] to primarily
  describe the formation of coherent vascular networks observed invitro
experiment. Under two diffrent
  boundary conditions,we first establish the existence and uniqueness of the
solution of the asymptotic
  state equation (the so-called nonlinear diffusion wave),and then prove that
the solution of the original
  equation is nonlinearly asymptotically stable. Additionally,we obtain the
convergence rates for both
  types of boundary conditions.

</details>


### [17] [On the asymptotic profile of solutions to semilinear damped wave equations with critical nonlinearities](https://arxiv.org/abs/2509.19835)
*Dinh Van Duong,Trung Loc Tang*

Main category: math.AP

TL;DR: This paper extends previous critical condition results for semilinear damped wave equations to dimension n=4, proving the same threshold condition applies and providing asymptotic profiles and sharp lifespan estimates.


<details>
  <summary>Details</summary>
Motivation: Previous research established sharp critical conditions for semilinear damped wave equations in low dimensions (n=1,2,3), but it was unknown if these conditions remain valid in higher dimensions like n=4.

Method: The authors analyze the Cauchy problem for semilinear damped wave equations with nonlinear term |u|^{1+2/n}Î¼(|u|), extending the mathematical framework used in prior works to dimension n=4.

Result: The critical condition on Î¼ remains valid in dimension n=4, and global solutions exhibit asymptotic behavior identified by the Gauss kernel. Sharp lifespan estimates are derived for blow-up cases.

Conclusion: The threshold condition between global existence and finite-time blow-up established for low dimensions extends to dimension n=4, with solutions asymptotically approaching the Gauss kernel behavior.

Abstract: In this paper, we consider the Cauchy problem for a semilinear damped wave
equation with the nonlinear term $|u|^{1+2/n} \mu(|u|)$, where $\mu$ is a
modulus of continuity. In recent papers by Ebert,Girardi,Reissig (Math. Ann.
378 (2020)) and Girardi (Nonlinear Differ. Equ. Appl. 32 (2025)), the authors
obtained a sharp critical condition on $\mu$ in low space dimensions $n=1,2,3$,
which determines the threshold between global (in time) existence of small data
solutions and blow-up of solutions in finite time. Our new results are to prove
that this condition remains valid in dimension $n=4$, together with the
asymptotic profiles of global solutions. From this, we see that the behavior of
the solution at $t \to \infty$ is identified by the Gauss kernel. Finally, a
sharp lifespan estimate for local solutions is also derived in the case when
blow-up occurs.

</details>


### [18] [On Brezis-Nirenberg problems: open questions and new results in dimension six](https://arxiv.org/abs/2509.19863)
*Fengliu Li,Giusi Vaira,Juncheng Wei,Yuanze Wu*

Main category: math.AP

TL;DR: This paper analyzes the Brezis-Nirenberg problem, a critical semilinear elliptic equation, providing new results for dimension six and listing open questions.


<details>
  <summary>Details</summary>
Motivation: To study the existence and properties of solutions to the Brezis-Nirenberg problem, particularly focusing on the critical case in dimension six, and to contribute to the understanding of this classical problem in partial differential equations.

Method: The authors first review the historical development of the Brezis-Nirenberg problem, then present new mathematical results specifically for the six-dimensional case, using techniques from variational methods and critical point theory for semilinear elliptic equations.

Result: New findings regarding the existence and behavior of solutions to the Brezis-Nirenberg problem in dimension six are presented, though specific results are not detailed in the abstract.

Conclusion: The paper contributes to the ongoing research on the Brezis-Nirenberg problem by providing new insights for dimension six and identifying important open questions that remain to be solved in this area of mathematical analysis.

Abstract: In this paper, we consider the Brezis-Nirenberg problem \begin{equation*}
\left\{\begin{aligned} &-\Delta u = \lambda u+|u|^{2^*-2}u, \quad
&\mbox{in}\,\Omega,\\ &u=0,\quad &\mbox{on}\, \partial\Omega,
\end{aligned}\right. \end{equation*} where $\Omega $ is a smoothly bounded
domain of $\mathbb R^N$ with $N\geq 3$, $\lambda>0$ is a parameter and
$2^*=\frac{2N}{N-2}$ is the critical Sobolev exponent. We first recall the
history of the Brezis-Nirenberg problem and then provide new results of it in
dimension six. Finally, we also list some open questions on the
Brezis-Nirenberg problem.

</details>


### [19] [On Brinkman flows with curvature-induced phase separation in binary mixtures](https://arxiv.org/abs/2509.20282)
*Pierluigi Colli,Gianni Gilardi,Andrea Signori,JÃ¼rgen Sprekels*

Main category: math.AP

TL;DR: Analysis of a novel Brinkman-Cahn-Hilliard system for multiphase flows with sixth-order phase-field evolution and variable viscosity, establishing existence of weak solutions and uniqueness under constant parameters.


<details>
  <summary>Details</summary>
Motivation: To mathematically analyze diffuse-interface models for multiphase flows that can capture complex interfacial dynamics with energetic consistency.

Method: Study a Brinkman-Cahn-Hilliard system coupling sixth-order phase-field evolution with Brinkman-type momentum equation, using divergence-free variational framework for weak solution analysis.

Result: Established existence of weak solutions, proved uniqueness and continuous dependence on forcing for constant mobility/viscosity, and analyzed Darcy limit with existence results for reduced system.

Conclusion: The mathematical framework successfully analyzes the novel multiphase flow model, providing rigorous existence and uniqueness results under specific conditions.

Abstract: The mathematical analysis of diffuse-interface models for multiphase flows
has attracted significant attention due to their ability to capture complex
interfacial dynamics, including curvature effects, within a unified,
energetically consistent framework. In this work, we study a novel
Brinkman-Cahn-Hilliard system, coupling a sixth-order phase-field evolution
with a Brinkman-type momentum equation featuring variable shear viscosity. The
Cahn-Hilliard equation includes a nonconservative source term accounting for
mass exchange, and the velocity equation contains a non divergence-free forcing
term. We establish the existence of weak solutions in a divergence-free
variational framework, and, in the case of constant mobility and shear
viscosity, prove uniqueness and continuous dependence on the forcing.
Additionally, we analyze the Darcy limit, providing existence results for the
corresponding reduced system.

</details>


### [20] [Evolution of internal waves in a 2D subcritical channel](https://arxiv.org/abs/2509.20327)
*Zhenhao Li,Jian Wang,Jared Wunsch*

Main category: math.AP

TL;DR: Analysis of long-term evolution of internal waves in 2D subcritical channels with flat horizontal ends, showing leading profiles are outgoing solutions to stationary equations.


<details>
  <summary>Details</summary>
Motivation: To understand the long-time behavior of internal waves in specific channel configurations and establish mathematical foundations for wave evolution.

Method: Proving a limiting absorption principle and demonstrating differentiability of the spectral measure for a zeroth order operator.

Result: Established that the leading profiles of solutions correspond to outgoing solutions of stationary equations.

Conclusion: The study provides mathematical validation for the long-term evolution patterns of internal waves in subcritical channels with flat horizontal boundaries.

Abstract: We study the long time evolution of internal waves in two dimensional
subcritical channels with flat horizontal ends. We show the leading profiles of
solutions are the outgoing solutions to the stationary equations. This is done
by showing a limiting absorption principle and the differentiability of the
spectral measure for a zeroth order operator.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [21] [Examining the robustness of Physics-Informed Neural Networks to noise for Inverse Problems](https://arxiv.org/abs/2509.20191)
*Aleksandra Jekic,Afroditi Natsaridou,Signe Riemer-SÃ¸rensen,Helge Langseth,Odd Erik Gundersen*

Main category: physics.comp-ph

TL;DR: PINNs are outperformed by traditional FEM methods in solving inverse PDE problems, though the performance gap decreases with higher dimensions and more data. PINNs require less specialized knowledge but have training failures that need addressing.


<details>
  <summary>Details</summary>
Motivation: To compare PINNs with traditional FEM methods for solving inverse PDE problems, particularly assessing claims that PINNs show promise for handling noisy/incomplete data in inverse problems.

Method: Comparative testing on fluid mechanics problems with increasing difficulty, with and without noise, comparing PINNs against FEM combined with numerical optimization.

Result: PINNs are outperformed by traditional approach in accuracy and computation time, but the performance gap decreases with higher dimensions and more data. PINNs require less human effort.

Conclusion: While PINNs offer advantages in ease of use, they currently underperform traditional methods for inverse problems. Training failures need to be addressed to make PINNs more competitive.

Abstract: Approximating solutions to partial differential equations (PDEs) is
fundamental for the modeling of dynamical systems in science and engineering.
Physics-informed neural networks (PINNs) are a recent machine learning-based
approach, for which many properties and limitations remain unknown. PINNs are
widely accepted as inferior to traditional methods for solving PDEs, such as
the finite element method, both with regard to computation time and accuracy.
However, PINNs are commonly claimed to show promise in solving inverse problems
and handling noisy or incomplete data. We compare the performance of PINNs in
solving inverse problems with that of a traditional approach using the finite
element method combined with a numerical optimizer. The models are tested on a
series of increasingly difficult fluid mechanics problems, with and without
noise. We find that while PINNs may require less human effort and specialized
knowledge, they are outperformed by the traditional approach. However, the
difference appears to decrease with higher dimensions and more data. We
identify common failures during training to be addressed if the performance of
PINNs on noisy inverse problems is to become more competitive.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [22] [Generalized mixed variable-pullback scheme with non-ideal Ohm's law for electromagnetic gyrokinetic simulations](https://arxiv.org/abs/2509.19553)
*Zhixin Lu,Guo Meng,Roman Hatzky,Eric SonnendrÃ¼cker,Alexey Mishchenko,Matthias Hoelzl*

Main category: physics.plasm-ph

TL;DR: Integration of non-ideal Ohm's law in mixed variable-pullback scheme for gyrokinetic particle simulations, providing accurate symplectic solutions in both MHD and electrostatic limits without traditional cancellation problems.


<details>
  <summary>Details</summary>
Motivation: To develop improved gyrokinetic particle simulation schemes that work accurately across different physical limits (MHD and electrostatic) while avoiding traditional cancellation issues in pure Hamiltonian schemes.

Method: Mixed variable-pullback scheme with non-ideal Ohm's law integration; comprehensive analysis of various mixed variable schemes for 1D shear AlfvÃ©n wave problem with kinetic electrons; comparison with traditional pure Hamiltonian and symplectic schemes.

Result: The pure vâ¥ form with non-ideal Ohm's law performs comparably to widely used mixed variable-pullback scheme with ideal Ohm's law; mixed variable-pullback scheme without Ohm's law shows significant performance improvement with minimal modification.

Conclusion: The proposed schemes provide feasible improvements over traditional approaches, offering better accuracy and reduced computational requirements (marker number reduction) while maintaining symplectic properties.

Abstract: In this work, the non-ideal Ohm's law is integrated in the mixed
variable-pullback scheme for the gyrokinetic particle simulations. This scheme
captures the evolution of the symplectic solution of the gyrokinetic model
accurately not only in the MHD limit but also in the electrostatic limit. This
scheme also provides a pure symplectic ($v_\shortparallel $) scheme for
electromagnetic gyrokinetic particle simulations without causing the
traditional cancellation problem in the pure Hamiltonian scheme. Various mixed
variable schemes have been comprehensively analyzed for the 1D shear Alfv\`en
wave problem with kinetic electrons, with the connection to the traditional
pure Hamiltonian scheme and the symplectic scheme. It is demonstrated that the
pure $v_\shortparallel $ form with the non-ideal Ohm's law has comparable
performance to the widely used mixed variable-pullback scheme with the ideal
Ohm's law. The mixed variable-pullback scheme without Ohm's law is also
proposed as a feasible improvement of the traditional pure Hamiltonian scheme
with minimum modification and considerable performance improvement in terms of
a marker number reduction.

</details>


### [23] [Anisotropic momentum distributions due to radiation recoil in relativistic plasmas with electric and magnetic fields](https://arxiv.org/abs/2509.19656)
*Haidar Al-Naseri*

Main category: physics.plasm-ph

TL;DR: Analysis of anisotropic momentum distributions in relativistic plasma under electromagnetic fields, examining three cases: magnetic field effects, electric field radiation reaction, and combined field interactions.


<details>
  <summary>Details</summary>
Motivation: To understand how electromagnetic fields alter plasma dynamics through radiation recoil forces and create anisotropic momentum distributions in thermally relativistic plasma.

Method: Theoretical investigation of three scenarios: pure magnetic field effects on isotropic/anisotropic plasma, electric field-driven radiation reaction, and combined electric-magnetic field interactions with frequency comparisons.

Result: Magnetic fields produce ring-shaped momentum profiles; electric fields create partial anisotropic rings in specific quadrants; combined fields break azimuthal symmetry when cyclotron frequency nears upper hybrid frequency, but preserve symmetry when cyclotron frequency is lower.

Conclusion: Electromagnetic fields induce distinct anisotropic momentum distributions in plasma, with symmetry breaking dependent on the relative frequencies of cyclotron and upper hybrid oscillations in combined field scenarios.

Abstract: The interaction of strong electromagnetic fields with plasma generates
radiation accompanied by a recoil force, which can significantly alter the
plasma dynamics. In this work, we investigate the development of anisotropic
momentum distributions induced by the combined action of electric and magnetic
fields on a thermally relativistic plasma. We consider three distinct types of
anisotropy. The first arises from a pure magnetic field acting on plasma with
either isotropic or anisotropic initial momentum distributions, producing the
characteristic ring-shaped momentum profile. The second is driven by a pure
electric field, where radiation reaction generates a partial, anisotropic ring
distribution in momentum space: significant modifications occur primarily in
the 90$^\circ$--180$^\circ$ and 270$^\circ$--360$^\circ$ sectors of the
$p_x$--$p_y$ plane, while the remaining quadrants remain largely unaffected.
The third case considers the combined effect of electric and magnetic fields.
When the cyclotron frequency is very close to the upper hybrid frequency, the
azimuthal symmetry of the ring-momentum distribution is broken. Conversely, in
the regime where the cyclotron frequency is lower than the upper hybrid
frequency, the rapid oscillations of the electric field dominate and preserve
the symmetry of the ring-momentum distribution.

</details>


### [24] [Energy transfer from MHD-scale slow-mode waves to kinetic-scale ion acoustic waves](https://arxiv.org/abs/2509.19763)
*Xiaofei Shi,Xin An,Vassilis Angelopoulos*

Main category: physics.plasm-ph

TL;DR: Large-amplitude slow-mode waves near Earth's magnetopause generate ion acoustic waves through counter-streaming ion beams, revealing energy transfer from MHD-scale to kinetic-scale waves.


<details>
  <summary>Details</summary>
Motivation: Recent observations show slow-mode waves occurring simultaneously with kinetic-scale ion acoustic waves, with enhanced ion acoustic wave amplitude near magnetic field peaks of slow-mode waves, suggesting a potential driving mechanism.

Method: Conducted hybrid simulation using observation-based parameters to test the hypothesis that slow-mode waves drive ion acoustic wave generation.

Result: Simulation demonstrated that large-amplitude slow-mode waves generate counter-streaming ion beams, which excite ion acoustic waves and relax the ion beams.

Conclusion: The study reveals a clear energy transfer channel from MHD-scale slow-mode waves to kinetic-scale ion acoustic waves.

Abstract: Large-amplitude slow-mode waves are commonly observed near Earth's
magnetopause. Recent observations show that these waves can occur
simultaneously with kinetic-scale ion acoustic waves. The amplitude of the ion
acoustic waves is enhanced near the magnetic field peaks of the slow-mode wave,
suggesting that the slow-mode waves may drive the generation of ion acoustic
waves. To test this hypothesis, we conduct a hybrid simulation using
observation-based parameters. The simulation results demonstrate that
large-amplitude slow-mode waves generate counter-streaming ion beams, which in
turn excite ion acoustic waves and relax the ion beams. Our study reveals a
clear energy transfer channel from MHD-scale slow-mode waves to kinetic-scale
ion acoustic waves.

</details>


### [25] [Thermal resilience of the ITER tungsten first wall to runaway electron impact](https://arxiv.org/abs/2509.20261)
*S. Ratynskaia,K. Paschalidis,T. Rizzi,P. Tolias,R. A. Pitts,F. J. Artola,H. BergstrÃ¶m,V. K. Bandaru,M. Hoelzl,S. Nicolici*

Main category: physics.plasm-ph

TL;DR: The paper analyzes the thermal response of ITER's tungsten first wall to high-energy runaway electron beams, finding that increased tungsten thickness is crucial for protecting against damage and cooling system failure.


<details>
  <summary>Details</summary>
Motivation: To address the critical issue of multi-MeV runaway electron beams causing extreme volumetric power densities that can lead to vaporization, melting, material explosions, and cooling system damage in ITER's tungsten first wall.

Method: Used a three-stage, one-way coupled workflow to model the thermal response of the ITER first wall, assessing wall damage extent and bond interface temperature increases for varying tungsten thicknesses.

Result: Increased tungsten thickness was found to be essential for wall protection against intense runaway electron dissipation events, benefiting both tungsten tile integrity and cooling system safety.

Conclusion: Thicker tungsten armor is necessary to mitigate damage from high-current runaway electron beams and maintain the longevity of ITER's first wall components.

Abstract: The fast volumetric deposition of multi-MeV high current runaway electron
(RE) beams constitutes the most critical issue for the ITER tungsten (W) first
wall (FW) longevity. Such relativistic electron beams could generate extreme
volumetric power densities inside the FW armour which lead to significant
vaporization, deep melting and even material explosions, as well as to elevated
temperatures at the bond interface with the cooling substrate that could cause
rupture and water leaks. Here the thermal response of the ITER FW is modeled
with a three-stage, one-way coupled workflow focusing on assessments of the
extent of the wall damage and the increase of the bond interface temperature
for varying W thickness. Increased W thickness is found to be essential for
wall protection against intense RE dissipation events in terms of both W tile
damage and cooling system integrity.

</details>


<div id='math.CV'></div>

# math.CV [[Back]](#toc)

### [26] [Values of Finite Distortion: Continuity](https://arxiv.org/abs/2509.20326)
*Ilmari Kangasniemi,Jani Onninen*

Main category: math.CV

TL;DR: The paper proves continuity for Sobolev mappings satisfying a distortion inequality with optimal integrability conditions, closing a gap between existing methods and counterexamples.


<details>
  <summary>Details</summary>
Motivation: There was a significant gap between existing continuity results for Sobolev mappings with distortion and known counterexamples. The paper aims to establish sharp conditions for continuity.

Method: The authors introduce a novel Sobolev-type inequality based on measures of superlevel sets as a key part of the proof for establishing continuity under optimal integrability conditions.

Result: The main result proves continuity for mappings in the Sobolev class W^{1,n}_{loc} that satisfy the given distortion inequality when p^{-1} + q^{-1} < 1, which is shown to be sharp.

Conclusion: This work closes an important gap in the theory, provides new results even in the planar case, and opens systematic study of mappings with finite distortion in geometric function theory.

Abstract: We prove continuity for mappings $f \colon \Omega \to \mathbb{R}^n$, $\Omega
\subset \mathbb{R}^n$, in the Sobolev class $W^{1,n}_{\text{loc}}(\Omega,
\mathbb{R}^n)$ that satisfy the inequality \[
  \lvert Df(x) \rvert^n \le K(x) \det Df(x) + \Sigma(x) \] whenever $K \in
L^p_{\text{loc}}(\Omega)$ and $\Sigma \in L^q_{\text{loc}}(\Omega)$ with
$p^{-1} + q^{-1} < 1$. This closes a significant gap between existing methods
and known counterexamples. The result is sharp, new even in the planar case,
and opens a systematic study of mappings with values of finite distortion in
geometric function theory. As a key part of the proof, we introduce an
overlooked Sobolev-type inequality based on measures of superlevel sets.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [27] [Efficient Gillespie algorithms for spreading phenomena in large and heterogeneous higher-order networks](https://arxiv.org/abs/2509.20174)
*Hugo P. Maia,Wesley Cota,Yamir Moreno,Silvio C. Ferreira*

Main category: physics.soc-ph

TL;DR: The paper presents optimized Gillespie algorithms using phantom processes to efficiently simulate higher-order contagion dynamics on hypergraphs, achieving near-linear computational complexity scaling instead of quadratic scaling.


<details>
  <summary>Details</summary>
Motivation: Higher-order dynamics exhibit phenomena like catastrophic activation and hybrid transitions that are absent in pairwise models, but simulating contagion on higher-order structures is computationally challenging due to complex propagation through hyperedges.

Method: The authors construct optimized Gillespie algorithms using phantom processes - events that don't change system state but account for time progression - for simulating SIS epidemic models with critical mass thresholds on hypergraphs.

Result: The phantom process algorithms outperform standard approaches by several orders of magnitude, reducing computational complexity from O(NÂ²) to nearly O(N), enabling simulation of highly heterogeneous networks with millions of nodes.

Conclusion: The optimized methods significantly expand the feasible simulation scale for higher-order interacting systems, allowing study of networks with millions of nodes and high order heterogeneity that were previously computationally prohibitive.

Abstract: Higher-order dynamics refer to mechanisms where collective mutual or
synchronous interactions differ fundamentally from their pairwise counterparts
through the concept of many-body interactions. Phenomena absent in pairwise
models, such as catastrophic activation, hysteresis, and hybrid transitions,
emerge naturally in higher-order interacting systems. Thus, the simulation of
contagion dynamics on higher-order structures is algorithmically and
computationally challenging due to the complexity of propagation through
hyperedges of arbitrary order. To address this issue, optimized Gillespie
algorithms were constructed for higher-order structures by means of phantom
processes: events that do not change the state of the system but still account
for time progression. We investigate the algorithm's performance considering
the susceptible-infected-susceptible (SIS) epidemic model with critical mass
thresholds on hypergraphs. Optimizations were assessed on networks of different
sizes and levels of heterogeneity in both connectivity and order interactions,
in a high epidemic prevalence regime. Algorithms with phantom processes are
shown to outperform standard approaches by several orders of magnitude in the
limit of large sizes. Indeed, a high computational complexity scaling
$\mathcal{O}(N^2)$ with system size $N$ of the standard algorithms is improved
to low complexity scaling nearly as $\mathcal{O}(N)$. The optimized methods
allow for the simulation of highly heterogeneous networks with millions of
nodes within affordable computation costs, significantly surpassing the size
range and order heterogeneity currently considered.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [28] [Broadband High-Temperature Multilayer Pyramid-Shaped Metamaterial Thermal Absorber for Thermophotovoltaic applications](https://arxiv.org/abs/2509.19499)
*Bibekananda Nath,Ahmed Zubair*

Main category: physics.optics

TL;DR: A polarization-independent truncated pyramid-shaped MIMI metamaterial absorber achieves 98.2% average absorption up to 4000 nm and withstands temperatures up to 1700 K, making it ideal for thermophotovoltaic systems.


<details>
  <summary>Details</summary>
Motivation: To develop a broadband, thermally stable absorber for thermophotovoltaic systems that can simultaneously convert solar and industrial waste heat into usable energy to meet growing power demands.

Method: Used finite difference time domain (FDTD) method for optical analysis and finite element method (FEM) for thermal stability evaluation. Designed six MIMI structures with lattice-matched materials to prevent delamination, optimized through brute force design approach.

Result: W/AlN structure achieved best performance with 98.2% average absorption up to 4000 nm, 97.73% up to 5072 nm, and over 96% absorption up to 50Â° incident angles. Only W/AlN could withstand 1700 K temperature and 1500 times incident power before permanent deformation.

Conclusion: The W/AlN metamaterial absorber demonstrates exceptional broadband absorption and thermal stability, making it highly suitable for efficient thermophotovoltaic energy conversion systems operating at high temperatures.

Abstract: A broadband, thermally stable absorber is essential for thermophotovoltaic
(TPV) systems to simultaneously convert solar and industrial waste heat into
usable energy to meet growing power demands. Here, we proposed an ingenious
polarization-independent truncated pyramid-shaped symmetric multilayer
metamaterial absorber in a metal-insulator-metal-insulator (MIMI) architecture
with almost complete absorption over a broad wavelength range. A total of six
structures (W/AlN, Mo/AlN, Ta/AlN, Rh/MgO, Rh/SiO2, Re/BN) were designed, and
the materials were selected based on their lattice matching to prevent
delamination at interfaces between layers. The absorption mechanism was studied
at room temperature using the finite difference time domain (FDTD) method, and
the structure was optimized through a brute force design approach, which
illustrates a best average absorption of 98.2% till 4000 nm and 97.73% till
5072 nm wavelength for the W/AlN structure with metal and dielectric
thicknesses of 60 nm and 17.5 nm, respectively. Moreover, W/AlN structure
exhibits over 96% average absorption up to 50 degree incident angles
irrespective of polarizations. The thermal stability was evaluated using the
finite element method (FEM) by determining von Mises stress at elevated
temperatures. Thermal analysis revealed that only W/AlN can withstand around
1700 K temperature and 1500 times the incident power before permanent
deformation. A temperature-dependent Drude-Lorentz model was used to further
analyze the effect of absorption on the optical performance of the highly
absorptive and thermally stable W/AlN structure. Additionally, we determined
the effect of the concentration factor, and the operating temperature on the
system efficiency by considering the emission loss of the heated absorber.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [29] [Advancing Universal Deep Learning for Electronic-Structure Hamiltonian Prediction of Materials](https://arxiv.org/abs/2509.19877)
*Shi Yin,Zujian Dai,Xinyang Pan,Lixin He*

Main category: cs.LG

TL;DR: NextHAM is a neural E(3)-symmetry and expressive correction method for efficient and generalizable materials electronic-structure Hamiltonian prediction, using zeroth-step Hamiltonians as informative descriptors and initial estimates to simplify learning.


<details>
  <summary>Details</summary>
Motivation: Deep learning methods for Hamiltonian prediction face challenges in generalization due to diverse atomic types, structural patterns, and high-dimensional complexity of Hamiltonians.

Method: Proposes NextHAM with three key components: 1) zeroth-step Hamiltonians as input descriptors and output initial estimates, 2) neural Transformer architecture with strict E(3)-symmetry, 3) novel training objective ensuring accuracy in both real and reciprocal spaces.

Result: NextHAM achieves excellent accuracy and efficiency in predicting Hamiltonians and band structures on the Materials-HAM-SOC benchmark dataset.

Conclusion: The method advances universal deep learning paradigm for Hamiltonian prediction through both methodological innovations and a comprehensive benchmark dataset.

Abstract: Deep learning methods for electronic-structure Hamiltonian prediction has
offered significant computational efficiency advantages over traditional DFT
methods, yet the diversity of atomic types, structural patterns, and the
high-dimensional complexity of Hamiltonians pose substantial challenges to the
generalization performance. In this work, we contribute on both the methodology
and dataset sides to advance universal deep learning paradigm for Hamiltonian
prediction. On the method side, we propose NextHAM, a neural E(3)-symmetry and
expressive correction method for efficient and generalizable materials
electronic-structure Hamiltonian prediction. First, we introduce the
zeroth-step Hamiltonians, which can be efficiently constructed by the initial
charge density of DFT, as informative descriptors of neural regression model in
the input level and initial estimates of the target Hamiltonian in the output
level, so that the regression model directly predicts the correction terms to
the target ground truths, thereby significantly simplifying the input-output
mapping for learning. Second, we present a neural Transformer architecture with
strict E(3)-Symmetry and high non-linear expressiveness for Hamiltonian
prediction. Third, we propose a novel training objective to ensure the accuracy
performance of Hamiltonians in both real space and reciprocal space, preventing
error amplification and the occurrence of "ghost states" caused by the large
condition number of the overlap matrix. On the dataset side, we curate a
high-quality broad-coverage large benchmark, namely Materials-HAM-SOC,
comprising 17,000 material structures spanning 68 elements from six rows of the
periodic table and explicitly incorporating SOC effects. Experimental results
on Materials-HAM-SOC demonstrate that NextHAM achieves excellent accuracy and
efficiency in predicting Hamiltonians and band structures.

</details>


### [30] [THINNs: Thermodynamically Informed Neural Networks](https://arxiv.org/abs/2509.19467)
*Javier Castro,Benjamin Gess*

Main category: cs.LG

TL;DR: The paper proposes THINNs, a thermodynamically consistent extension of PINNs that uses physically informed penalization based on fluctuation structure rather than heuristic penalty terms.


<details>
  <summary>Details</summary>
Motivation: Standard PINNs use heuristic penalty terms for PDE residual minimization. This work aims to develop a physically consistent penalization strategy for non-equilibrium fluctuating systems based on fluctuation structure characterized by large deviations principles.

Method: The authors propose THINNs (Thermodynamically consistent PINNs) where the penalty term is chosen to penalize improbable deviations consistent with the underlying fluctuation structure, rather than using heuristic penalization. The method is analyzed through analytical a posteriori estimates and empirical comparisons.

Result: The paper establishes analytical a posteriori estimates for THINNs and provides empirical comparisons showing the effectiveness of the thermodynamically consistent penalization approach compared to established strategies.

Conclusion: THINNs represent a novel formulation of PINNs with physically informed penalization that is consistent with fluctuation thermodynamics, offering improved theoretical foundation and performance for non-equilibrium fluctuating systems.

Abstract: Physics-Informed Neural Networks (PINNs) are a class of deep learning models
aiming to approximate solutions of PDEs by training neural networks to minimize
the residual of the equation. Focusing on non-equilibrium fluctuating systems,
we propose a physically informed choice of penalization that is consistent with
the underlying fluctuation structure, as characterized by a large deviations
principle. This approach yields a novel formulation of PINNs in which the
penalty term is chosen to penalize improbable deviations, rather than being
selected heuristically. The resulting thermodynamically consistent extension of
PINNs, termed THINNs, is subsequently analyzed by establishing analytical a
posteriori estimates, and providing empirical comparisons to established
penalization strategies.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [31] [Lateral disorder in Langmuir monolayers: theoretical derivations and grazing-incidence X-ray diffraction](https://arxiv.org/abs/2509.20130)
*L. R. Muftakhova,K. V. Nikolaev,A. V. Rogachev,N. N. Novikova,B. I. Ostrovskii,S. N. Yakunin*

Main category: cond-mat.soft

TL;DR: A theoretical framework for simulating complex structural molecular ordering in Langmuir monolayers using diffraction patterns, validated with experimental synchrotron data.


<details>
  <summary>Details</summary>
Motivation: Standard analysis of diffraction patterns is insufficient for complex molecular organization in Langmuir monolayers, requiring physics-based simulation methods.

Method: Developed a versatile theoretical framework for simulating diffraction patterns, starting with simple solid-state monolayers and extending to collapsed state structural organization.

Result: The method successfully simulates complex molecular ordering and is validated through comparison with experimental data from synchrotron beamline measurements.

Conclusion: The presented framework provides an effective approach for analyzing intricate molecular organization in Langmuir monolayers that cannot be adequately described by standard diffraction analysis methods.

Abstract: Recent studies of the self-assembly of Langmuir monolayers have revealed
novel forms of lateral molecular ordering. Such studies typically involve the
use of grazing-incidence synchrotron radiation scattering, and the lateral
order manifests itself as distinct diffraction patterns. The more intricate the
molecular organization, the more complicated the corresponding diffraction
pattern is. To the point where standard analysis, i.e., identifying peak
positions and solving the crystal structure, is insufficient to describe the
system. In such cases, a physics-based simulation of the diffraction is
required. In this article, we present a versatile theoretical framework for
simulating complex structural molecular ordering in Langmuir monolayers. We
begin by applying the formalism to a simple case of solid-state monolayers and
extend the analysis to describe the structural organization in the collapsed
state. The applicability of the method is validated through comparison with
experimental data collected at the bending magnet synchrotron beamline.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [32] [Nature of Transonic Sub-AlfvÃ©nic Turbulence and Density Fluctuations in the Near-Sun Solar Wind: Insights from Magnetohydrodynamic Simulations and Nearly-Incompressible Models](https://arxiv.org/abs/2509.19534)
*Giuseppe ArrÃ²,Hui Li,Gary P. Zank,Lingling Zhao,Laxman Adhikari*

Main category: astro-ph.SR

TL;DR: The paper introduces a new magnetohydrodynamic model called Transonic sub-AlfvÃ©nic Turbulence (TsAT) to explain solar wind turbulence that transitions from subsonic to transonic regimes near the Sun while remaining sub-AlfvÃ©nic.


<details>
  <summary>Details</summary>
Motivation: Recent Parker Solar Probe measurements revealed that solar wind turbulence transitions from subsonic to transonic near the Sun while staying sub-AlfvÃ©nic, requiring revision of existing models that assume turbulence is both subsonic and sub-AlfvÃ©nic.

Method: The authors develop a new magnetohydrodynamic (MHD) model of Transonic sub-AlfvÃ©nic Turbulence (TsAT) and validate it with 3D MHD simulations.

Result: The TsAT model shows turbulence remains effectively nearly-incompressible with 2D + slab geometry in both subsonic and transonic regimes when sub-AlfvÃ©nic. Simulations confirm transonic turbulence is dominated by low-frequency quasi-2D incompressible structures.

Conclusion: The TsAT model extends existing nearly-incompressible turbulence theories and is relevant for modeling space and astrophysical plasmas including near-Sun solar wind, solar corona, and interstellar medium.

Abstract: Recent Parker Solar Probe measurements have revealed that solar wind (SW)
turbulence transits from a subsonic to a transonic regime near the Sun, while
remaining sub-Alfv\'enic. These observations call for a revision of existing SW
models, where turbulence is considered to be both subsonic and sub-Alfv\'enic.
In this Letter, we introduce a new magnetohydrodynamic (MHD) model of Transonic
sub-Alfv\'enic Turbulence (TsAT). Our model shows that turbulence is
effectively nearly-incompressible (NI) and has a 2D + slab geometry not only in
the subsonic limit, but also in the transonic regime, as long as it remains
sub-Alfv\'enic, a condition essentially enforced everywhere in the heliosphere
by the strong local magnetic field. These predictions are consistent with 3D
MHD simulations, showing that transonic turbulence is dominated by low
frequency quasi-2D incompressible structures, while compressible fluctuations
are a minor component corresponding to low frequency slow modes and high
frequency fast modes. Our new TsAT model extends existing NI theories of
turbulence, and is potentially relevant for the theoretical and numerical
modeling of space and astrophysical plasmas, including the near-Sun SW, the
solar corona, and the interstellar medium.

</details>


### [33] [The periodicity of three-dimensional oscillatory reconnection](https://arxiv.org/abs/2509.19603)
*Luiz A. C. A. Schiavo,Gert J. J. Botha,James A. McLaughlin*

Main category: astro-ph.SR

TL;DR: This paper investigates oscillatory reconnection at 3D magnetic null points, finding that the periodicity is constant and independent of driving amplitude, with invariant normalized current density.


<details>
  <summary>Details</summary>
Motivation: To understand the long-term periodic signal generated by 3D magnetic null points when perturbed by non-periodic drivers, and to explore oscillatory reconnection as a magnetic relaxation mechanism.

Method: Solving 3D nonlinear magnetohydrodynamic (MHD) equations using a bespoke numerical boundary condition (sponge region) that dampens wave reflections, allowing investigation of long-term periodic signals at 3D null points for various driving amplitudes.

Result: Multiple cycles of 3D oscillatory reconnection were observed for the first time. The periodicity was found to be constant and independent of driving amplitude, and the normalized time-dependent current density at the null point was invariant.

Conclusion: The study successfully extracted a single period for oscillatory reconnection at 3D null points, suggesting this characteristic period could serve as a diagnostic tool to indirectly reveal fundamental plasma properties of 3D null points.

Abstract: Oscillatory reconnection is a dynamic, magnetic relaxation mechanism in which
a perturbed null point reverts back to equilibrium via time-dependent
reconnection. In this paper, we investigate the long-term periodic signal
generated by a three-dimensional (3D) magnetic null point, when it is perturbed
by a non-periodic driver, for a variety of driving amplitudes. We solve the 3D
nonlinear magnetohydrodynamic (MHD) equations using a bespoke numerical
boundary condition (a sponge region) that damps wave reflections and thus
allows the long-term periodic signal at the 3D null point to be investigated.
We observe multiple cycles of the 3D oscillatory reconnection mechanism for the
first time. We find that the periodicity is both constant and independent of
the choice of driving amplitude. Furthermore, the resultant time-dependent
current density at the null point normalized by the driving amplitude is
invariant. We extract a single period for oscillatory reconnection at a 3D null
point, opening the future possibility of using this characteristic period as a
diagnostic tool to reveal indirectly the fundamental plasma properties of 3D
null points.

</details>


### [34] [Residual energy of magnetohydrodynamic shocks](https://arxiv.org/abs/2509.20096)
*S. W. Good,K. J. Palmunen,C. H. K. Chen,E. K. J. Kilpua,T. V. MÃ¤kelÃ¤,J. Ruohotie,C. P. Sishtla,J. E. Soljento*

Main category: astro-ph.SR

TL;DR: This paper develops a theoretical framework using Rankine-Hugoniot conditions to derive equations for residual energy and cross helicity of MHD shocks, showing that fast-mode shocks have positive residual energy which matches observational data.


<details>
  <summary>Details</summary>
Motivation: Recent observations show fast-mode interplanetary shock waves have positive residual energy, contrasting with the negative residual energy of turbulence and magnetic structures in solar wind. This motivates theoretical understanding of residual energy in shock contexts.

Method: Applied Rankine-Hugoniot conditions to derive equations for residual energy and cross helicity as functions of shock angle, density compression ratio, and upstream AlfvÃ©n Mach number. Verified simplified equation for perpendicular shocks against observations of 141 interplanetary shocks.

Result: The residual energy equation gives only positive values for super-AlfvÃ©nic (fast-mode) shocks. The simplified equation matches well with observations, particularly for shocks with higher density compression ratios and Mach numbers.

Conclusion: Positive residual energy can serve as a signature for fast-mode shock identification in spacecraft data. The work provides insights into compressive fluctuations in solar wind more generally.

Abstract: Residual energy quantifies the difference in energy between velocity and
magnetic field fluctuations in a plasma. Recent observational evidence
highlights that fast-mode interplanetary shock waves have positive residual
energy, in sharp contrast to the negative residual energy of the turbulence and
magnetic structures that constitute the vast majority of fluctuation power in
the solar wind at magnetohydrodynamic (MHD) inertial scales. In this work, we
apply the Rankine-Hugoniot conditions to derive an equation for the residual
energy of an MHD shock jump as a function of the shock angle, density
compression ratio and Alfv\'en Mach number upstream of the shock. An equation
for the cross helicity is similarly derived. The residual energy equation gives
only positive values for super-Alfv\'enic (i.e. fast-mode) shocks. The residual
energy and cross helicity of slow-mode shocks and tangential, contact and
rotational discontinuities are also determined. A simplified form of the
residual energy equation applicable to perpendicular shocks has been verified
against residual energy values directly estimated from observations of 141
interplanetary shocks; the equation is found to match well with observations,
particularly for shocks with higher density compression ratios and Mach
numbers. The use of positive residual energy as a signature for fast-mode shock
identification in spacecraft data is briefly considered, and insights from this
work relating to compressive fluctuations more generally in the solar wind are
discussed.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [35] [Massive Discovery of Low-Dimensional Materials from Universal Computational Strategy](https://arxiv.org/abs/2509.20164)
*Mohammad Bagheri,Ethan Berger,Hannu-Pekka Komsa,Pekka Koskinen*

Main category: cond-mat.mtrl-sci

TL;DR: This paper presents a machine learning approach using universal machine-learning interatomic potentials (UMLIPs) and force constant-based dimensionality classification to discover 9,139 novel low-dimensional materials from the Materials Project database.


<details>
  <summary>Details</summary>
Motivation: Current computational methods for materials discovery are often limited to 2D materials and overlook other low-dimensional materials, while experimental discovery is tedious. The researchers aimed to develop a more comprehensive computational approach for systematic discovery of various low-dimensional materials.

Method: Combined UMLIPs with an interatomic force constant-based dimensionality classification method to analyze 35,689 materials from the Materials Project database. Used UMLIPs to achieve first-principles-level accuracy in quantifying force constants and phonon calculations.

Result: Discovered 9,139 low-dimensional materials including: 1,838 0D clusters, 1,760 1D chains, 3,057 2D sheets/layers, and 2,484 mixed-dimensionality materials. Identified 960 potentially exfoliable 2D materials through binding energy calculations.

Conclusion: The method successfully discovered thousands of novel low-dimensional materials that conventional geometric descriptors missed, demonstrating the effectiveness of combining UMLIPs with force constant-based dimensionality classification for comprehensive materials discovery.

Abstract: Low-dimensional materials have attractive properties that drive intense
efforts for novel materials discovery. However, experiments are tedious for
systematic discovery, and present computational methods are often tuned to
two-dimensional (2D) materials, overlooking other low-dimensional materials.
Here, we combined universal machine-learning interatomic potentials (UMLIPs)
and an advanced, interatomic force constant (FC) -based dimensionality
classification method to make a massive discovery of novel low-dimensional
materials. We first benchmarked UMLIPs' first-principles-level accuracy in
quantifying FCs and calculated phonons for 35,689 materials from the Materials
Project database. We then used the FC-based method for dimensionality
classification to discover 9139 low-dimensional materials, including 1838 0D
clusters, 1760 1D chains, 3057 2D sheets/layers, and 2484 mixed-dimensionality
materials, all of which conventional geometric descriptors have not recognized.
By calculating the binding energies for the discovered 2D materials, we also
identified 960 sheets that could be easily or potentially exfoliated from their
parent bulk structures.

</details>


### [36] [A follow-up on the sulphur atom popping model for MoS$_2$ memristor](https://arxiv.org/abs/2509.20179)
*Sanchali Mitra,Santanu Mahapatra*

Main category: cond-mat.mtrl-sci

TL;DR: This paper provides additional perspective on the "sulphur atom popping model" for resistive switching in 2D molybdenum disulfide-based memristors using density functional theory, and discusses limitations of machine learning interatomic potentials in reproducing ReaxFF simulation results.


<details>
  <summary>Details</summary>
Motivation: The mechanism of resistive switching in 2D semiconductor-based memristors differs from conventional bulk-oxide memristors, and experimental data suggest the switching may be intrinsic to the 2D semiconducting layer or result from electrode atom movement. The authors previously proposed a "sulphur atom popping model" to explain this phenomenon.

Method: The study uses density functional theory to provide additional perspective on their previously proposed "sulphur atom popping model" for resistive switching in 2D molybdenum disulfide memristors. They also evaluate the limitations of universal machine learning interatomic potentials in reproducing reactive-force field (ReaxFF) molecular dynamics simulation results.

Result: The paper provides further theoretical support and analysis for the "sulphur atom popping model" that explains the intrinsic nature of non-volatile resistive switching in 2D molybdenum disulfide-based memristors.

Conclusion: The study reinforces the validity of the "sulphur atom popping model" through density functional theory analysis, while highlighting challenges in using machine learning interatomic potentials to replicate ReaxFF simulation outcomes for these complex 2D semiconductor systems.

Abstract: The mechanism of resistive switching in two-dimensional (2D)
semiconductor-based memristors is intriguing, and our conventional knowledge of
bulk-oxide based memristors does not apply to these devices. Experimental data
indicate that the genesis of resistive switching may be intrinsic to the 2D
semiconducting active layer, as well as resulting from the movement of
electrode atoms. Employing reactive-force field (ReaxFF) molecular dynamics
simulations, we introduced the "sulphur atom popping model" [npj 2D Mater.
Appl. 5, 33 (2021)] to elucidate the intrinsic nature of non-volatile resistive
switching in 2D molybdenum disulfide-based memristors. In this paper we provide
additional perspective to this model using density functional theory. We also
discuss the limitations of universal machine learning interatomic potentials in
reproducing ReaxFF simulation results.

</details>


<div id='math.GR'></div>

# math.GR [[Back]](#toc)

### [37] [PySymmetry: A Sage/Python Framework for the Symmetry Reduction of Linear G-Equivariant Systems](https://arxiv.org/abs/2509.19479)
*Leon D. da Silva,Marcelo P. Santos*

Main category: math.GR

TL;DR: PySymmetry is an open-source Sage/Python framework that uses representation theory to simplify G-equivariant linear systems by generating symmetry-adapted bases and transforming operators into block-diagonal form, achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Symmetry in scientific linear systems is prevalent but often underutilized by standard computational software, creating a need for specialized tools to exploit these structural properties.

Method: PySymmetry implements classical representation theory using projection operators to generate symmetry-adapted bases, transforming equivariant operators into block-diagonal form. It includes functionalities for defining/reducing representations, calculating multiplicities, and obtaining explicit block structure.

Result: The framework demonstrated versatility through three case studies: a chemistry application, a 17x performance improvement on non-Hermitian SchrÃ¶dinger equation benchmarks, and the first complete analytical classification of a celestial mechanics problem.

Conclusion: PySymmetry offers a powerful, user-friendly tool for exploring symmetries in theoretical and applied contexts, with seamless integration with NumPy and SciPy libraries.

Abstract: Despite the prevalence of symmetry in scientific linear systems, these
structural properties are often underutilized by standard computational
software. This paper introduces PySymmetry, an open-source Sage/Python
framework that implements classical representation theory to simplify
G-equivariant linear systems. PySymmetry uses projection operators to generate
symmetry-adapted bases, transforming equivariant operators into a more
efficient block-diagonal form. Its functionalities include defining and
reducing representations, calculating multiplicities, and obtaining the
explicit block structure.
  We demonstrate PySymmetry's versatility through three case studies: a
chemistry application, a numerical benchmark on the non-Hermitian Schr\"odinger
equation that achieved a performance increase of over 17x compared to standard
methods, and a symbolic investigation that enabled the first complete
analytical classification of a challenging problem in celestial mechanics.
Designed for seamless integration with libraries like NumPy and SciPy,
PySymmetry offers a powerful, user-friendly tool for exploring symmetries in
theoretical and applied contexts. ```

</details>


<div id='math.FA'></div>

# math.FA [[Back]](#toc)

### [38] [Quantum Harmonic Analysis and the Structure in Data: Augmentation](https://arxiv.org/abs/2509.19474)
*Monika Doerfler,Franz Luef,Henry McNulty*

Main category: math.FA

TL;DR: Study of data augmentation's impact on principal components smoothness using quantum harmonic analysis, showing eigenfunctions lie in modulation space M^1(R^d) ensuring smoothness and continuity.


<details>
  <summary>Details</summary>
Motivation: To understand how data augmentation affects the smoothness and continuity of principal components in high-dimensional datasets, which could benefit manifold learning and feature extraction algorithms.

Method: Using tools from quantum harmonic analysis to analyze eigenfunctions of operators corresponding to augmented data sets, with numerical validation on synthetic and audio data.

Result: Eigenfunctions of operators for augmented data sets lie in the modulation space M^1(R^d), guaranteeing smoothness and continuity, confirmed by numerical examples.

Conclusion: The findings suggest that manifold learning and feature extraction algorithms can benefit from systematic and informed data augmentation principles.

Abstract: In this short note, we study the impact of data augmentation on the
smoothness of principal components of high-dimensional datasets. Using tools
from quantum harmonic analysis, we show that eigenfunctions of operators
corresponding to augmented data sets lie in the modulation space
$M^1(\mathbb{R}^d)$, guaranteeing smoothness and continuity. Numerical examples
on synthetic and audio data confirm the theoretical findings. While interesting
in itself, the results suggest that manifold learning and feature extraction
algorithms can benefit from systematic and informed augmentation principles.

</details>


### [39] [Numerical Ranges and Spectral Sets: the unbounded case](https://arxiv.org/abs/2509.19792)
*Michel Crouzeix*

Main category: math.FA

TL;DR: Improving the constant C_Î© â¤ 1+â2 for C_Î©-spectral sets when Î© is an unbounded convex set containing the numerical range of an operator A


<details>
  <summary>Details</summary>
Motivation: The known bound C_Î© â¤ 1+â2 for spectral sets containing numerical ranges is not optimal in unbounded cases, motivating the search for better estimates

Method: Mathematical analysis of spectral sets and numerical ranges, likely involving operator theory and convex geometry techniques

Result: Obtained improved estimates for the constant C_Î© in unbounded convex set cases

Conclusion: The paper provides better bounds for spectral set constants when dealing with unbounded convex domains containing numerical ranges

Abstract: It is known that, if $\Omega$ $\subset$ C is a convex set containing the
numerical range of an operator A, then $\Omega$ is a C $\Omega$ -spectral set
for A with C $\Omega$ $\le$ 1+ $\sqrt$ 2. We improve this estimate in unbounded
cases.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [40] [Geometric Autoencoder Priors for Bayesian Inversion: Learn First Observe Later](https://arxiv.org/abs/2509.19929)
*Arnaud Vadeboncoeur,Gregory DuthÃ©,Mark Girolami,Eleni Chatzi*

Main category: stat.ML

TL;DR: GABI is a framework that learns geometry-aware generative models for Bayesian inversion, enabling uncertainty quantification on complex geometries without requiring PDE knowledge, using a learn-first-observe-later paradigm with ABC sampling.


<details>
  <summary>Details</summary>
Motivation: Engineering systems often have complicated and variable geometries that prohibit standard Bayesian UQ methods, and there's a need to recover full-field information from sparse noisy observations of physical systems.

Method: Learn geometry-aware generative models from large datasets of systems with varying geometries, then use Approximate Bayesian Computation sampling to combine the learned prior with observation likelihoods for Bayesian inversion.

Result: GABI achieves predictive accuracy comparable to deterministic supervised learning, provides well-calibrated UQ robust on complex geometry problems, and works across various applications including heat transfer, fluid dynamics, and acoustic problems.

Conclusion: The method provides a flexible geometry-aware foundation model that is independent of observation processes and enables train-once-use-anywhere capability for Bayesian inversion on complex geometries.

Abstract: Uncertainty Quantification (UQ) is paramount for inference in engineering
applications. A common inference task is to recover full-field information of
physical systems from a small number of noisy observations, a usually highly
ill-posed problem. Critically, engineering systems often have complicated and
variable geometries prohibiting the use of standard Bayesian UQ. In this work,
we introduce Geometric Autoencoders for Bayesian Inversion (GABI), a framework
for learning geometry-aware generative models of physical responses that serve
as highly informative geometry-conditioned priors for Bayesian inversion.
Following a ''learn first, observe later'' paradigm, GABI distills information
from large datasets of systems with varying geometries, without requiring
knowledge of governing PDEs, boundary conditions, or observation processes,
into a rich latent prior. At inference time, this prior is seamlessly combined
with the likelihood of the specific observation process, yielding a
geometry-adapted posterior distribution. Our proposed framework is architecture
agnostic. A creative use of Approximate Bayesian Computation (ABC) sampling
yields an efficient implementation that utilizes modern GPU hardware. We test
our method on: steady-state heat over rectangular domains; Reynold-Averaged
Navier-Stokes (RANS) flow around airfoils; Helmholtz resonance and source
localization on 3D car bodies; RANS airflow over terrain. We find: the
predictive accuracy to be comparable to deterministic supervised learning
approaches in the restricted setting where supervised learning is applicable;
UQ to be well calibrated and robust on challenging problems with complex
geometries. The method provides a flexible geometry-aware
train-once-use-anywhere foundation model which is independent of any particular
observation process.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [41] [Wigner measure approach to the derivation of the Landau-Pekar equations in the mean-field limit](https://arxiv.org/abs/2509.20043)
*RaphaÃ«l Gautier*

Main category: math-ph

TL;DR: Rigorous derivation of Landau-Pekar equations from FrÃ¶hlich Hamiltonian using Wigner measures in mean-field limit, with extended classical well-posedness results.


<details>
  <summary>Details</summary>
Motivation: To establish a rigorous mathematical connection between quantum FrÃ¶hlich Hamiltonian and classical Landau-Pekar equations, extending classical well-posedness to broader function spaces.

Method: Uses Wigner measure techniques and Gross transform to study the system on energy space without ultraviolet cutoff, enabling analysis in mean-field limit.

Result: Successfully derived Landau-Pekar equations from FrÃ¶hlich Hamiltonian and extended global well-posedness results up to LÂ² â LÂ² function spaces.

Conclusion: The approach provides a mathematically rigorous foundation for the classical limit of quantum polaron systems, with improved well-posedness properties.

Abstract: We provide a rigorous derivation of the Landau-Pekar equations from the
Fr\"ohlich Hamiltonian in the mean-field limit using Wigner measure techniques.
On the classical side, we extend the global well-posedness results up to $L^2
\oplus L^2$. For obtaining the classical limit, we make a crucial use on the
quantum side of the Gross transform, allowing us to study the system on the
energy space, and with no ultraviolet cutoff.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [42] [GPU-accelerated FREDopt package for simultaneous dose and LETd proton radiotherapy plan optimization via superiorization methods](https://arxiv.org/abs/2509.20012)
*Damian Borys,Jan Gajewski,Tobias Becher,Yair Censor,Renata KopeÄ,Marzena Rydygier,Angelo Schiavi,Tomasz SkÃ³ra,Anna Spaleniak,Niklas Wahl,Agnieszka Wochnik,Antoni RuciÅski*

Main category: physics.med-ph

TL;DR: FREDopt is a GPU-accelerated open-source optimization software for simultaneous proton dose and LETd optimization in IMPT treatment planning, using superiorization of feasibility-seeking algorithms to achieve efficient optimization.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient optimization tool for proton therapy planning that can simultaneously optimize dose and dose-averaged LET (LETd) distributions while reducing computational costs compared to traditional constrained optimization methods.

Method: Implemented in Python with CuPy for GPU acceleration, incorporating fast Monte Carlo simulations from FRED code. Uses a novel superiorization of feasibility-seeking algorithms that interlaces perturbations into iterative steps to find superior feasible points without full constrained optimization.

Result: Simultaneous dose and LETd optimization with FREDopt significantly reduced LETd and doseÃLETd in organs at risk while maintaining target dose conformity. Computational performance was 14-50 minutes, suitable for clinical and research use.

Conclusion: FREDopt provides an effective open-source solution for proton therapy optimization, enabling substantial LETd reduction in OARs with satisfactory computational efficiency for clinical applications.

Abstract: This study presents FREDopt, a newly developed GPU-accelerated open-source
optimization software for simultaneous proton dose and dose-averaged LET (LETd)
optimization in IMPT treatment planning. FREDopt was implemented entirely in
Python, leveraging CuPy for GPU acceleration and incorporating fast Monte Carlo
(MC) simulations from the FRED code. The treatment plan optimization workflow
includes pre-optimization and optimization, the latter equipped with a novel
superiorization of feasibility-seeking algorithms. Feasibility-seeking requires
finding a point that satisfies prescribed constraints. Superiorization
interlaces computational perturbations into iterative feasibility-seeking steps
to steer them toward a superior feasible point, replacing the need for costly
full-fledged constrained optimization. The method was validated on two
treatment plans of patients treated in a clinical proton therapy center, with
dose and LETd distributions compared before and after reoptimization.
Simultaneous dose and LETd optimization using FREDopt led to a substantial
reduction of LETd and (dose)x(LETd) in organs at risk (OARs) while preserving
target dose conformity. Computational performance evaluation showed execution
times of 14-50 minutes, depending on the algorithm and target volume
size-satisfactory for clinical and research applications while enabling further
development of the well-tested, documented open-source software.

</details>


<div id='physics.space-ph'></div>

# physics.space-ph [[Back]](#toc)

### [43] [Graph-based Neural Space Weather Forecasting](https://arxiv.org/abs/2509.19605)
*Daniel Holmberg,Ivan Zaitsev,Markku Alho,Ioanna Bouri,Fanni Franssila,Haewon Jeong,Minna Palmroth,Teemu Roos*

Main category: physics.space-ph

TL;DR: A graph-based neural emulator is introduced to predict near-Earth space conditions using Vlasiator data, enabling fast deterministic forecasts and ensemble generation for uncertainty quantification in space weather forecasting.


<details>
  <summary>Details</summary>
Motivation: Hybrid-Vlasov models like Vlasiator provide high physical realism for space weather forecasting but are computationally too expensive for real-time operational use. Machine learning offers a way to make these simulations tractable while adding uncertainty quantification capabilities.

Method: The authors developed a graph-based neural emulator trained on Vlasiator data that can autoregressively predict near-Earth space conditions driven by upstream solar wind. The approach includes both deterministic forecasting and generative modeling for ensemble production to capture forecast uncertainty.

Result: The work demonstrates that machine learning can successfully emulate hybrid-Vlasov simulations, providing both fast deterministic forecasts and uncertainty quantification through ensemble generation.

Conclusion: Machine learning enables the operational use of hybrid-Vlasov simulations for space weather prediction by making them computationally tractable while adding valuable uncertainty quantification capabilities to existing forecasting systems.

Abstract: Accurate space weather forecasting is crucial for protecting our increasingly
digital infrastructure. Hybrid-Vlasov models, like Vlasiator, offer physical
realism beyond that of current operational systems, but are too computationally
expensive for real-time use. We introduce a graph-based neural emulator trained
on Vlasiator data to autoregressively predict near-Earth space conditions
driven by an upstream solar wind. We show how to achieve both fast
deterministic forecasts and, by using a generative model, produce ensembles to
capture forecast uncertainty. This work demonstrates that machine learning
offers a way to add uncertainty quantification capability to existing space
weather prediction systems, and make hybrid-Vlasov simulation tractable for
operational use.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [44] [Hierarchical null controllability of a degenerate parabolic equation with nonlocal coefficient](https://arxiv.org/abs/2509.19505)
*Juan LÃ­maco,JoÃ£o Carlos Barreira,Suerlan Silva,Luis P. Yapu*

Main category: math.OC

TL;DR: This paper demonstrates local null controllability of a degenerate parabolic equation using Stackelberg-Nash strategy with leader and follower controls, adapting Carleman estimates and applying Liusternik's inverse function theorem.


<details>
  <summary>Details</summary>
Motivation: To address the controllability problem for parabolic equations with degenerate diffusion coefficients involving nonlocal terms, using a hierarchical control strategy.

Method: Uses Stackelberg-Nash strategy with one leader control and two follower controls, adapts Carleman estimates for degenerate systems, and applies Liusternik's inverse function theorem for local controllability.

Result: Achieves local null controllability for the nonlinear degenerate parabolic system with nonlocal diffusion coefficients.

Conclusion: The proposed hierarchical control strategy combined with adapted Carleman estimates and inverse function theorem successfully solves the local null controllability problem for this class of degenerate parabolic equations.

Abstract: In this paper we use a Stackelberg-Nash strategy to show the local null
controllability of a parabolic equation where the diffusion coefficient is the
product of a degenerate function in space and a nonlocal term. We consider one
control called \textit{leader} and two controls called \textit{followers}. To
each leader we associate a Nash equilibrium corresponding to a bi-objective
optimal control problem; then, we find a leader that solves the null
controllability problem. The linearized degenerated system is treated adapting
Carleman estimates for degenerated systems from Demarque, L\'imaco and Viana
\cite{DemarqueLimacoViana_deg_sys2020} and the local controllability of the
non-linear system is obtained using Liusternik's inverse function theorem. The
nonlocal coefficient originates a multiplicative coupling in the optimality
system that gives rise to interesting calculations in the applications of the
inverse function theorem.

</details>


### [45] [An Alternating Direction Method of Multipliers for Topology Optimization](https://arxiv.org/abs/2509.19888)
*Harsh Choudhary,Sven Leyffer,Dominic Yang*

Main category: math.OC

TL;DR: ADMM framework for solving integer-constrained PDE optimization with TV regularization in topology optimization


<details>
  <summary>Details</summary>
Motivation: Address computational challenges from discrete design variables, nonsmooth regularization, and non-convex objectives in PDE-constrained topology optimization

Method: Alternating direction method of multipliers (ADMM) to decompose problem into simpler subproblems, using augmented Lagrangian formulation for consistency and convergence

Result: Efficient solution approach for complex integer-constrained optimization problems with PDE constraints

Conclusion: ADMM provides an effective framework for handling the computational difficulties in topology optimization with discrete variables and nonsmooth regularization

Abstract: We consider a class of integer-constrained optimization problems governed by
partial differential equation (PDE) constraints and regularized via total
variation (TV) in the context of topology optimization. The presence of
discrete design variables, nonsmooth regularization, and non-convex objective
renders the problem computationally challenging. To address this, we adopt the
alternating direction method of multipliers (ADMM) framework, which enables a
decomposition of the original problem into simpler subproblems that can be
solved efficiently. The augmented Lagrangian formulation ensures consistency
across variable updates while facilitating convergence under appropriate
conditions.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [46] [A Note on Fine-Grained Quantum Reductions for Linear Algebraic Problems](https://arxiv.org/abs/2509.19528)
*Kyle Doney,Cameron Musco*

Main category: cs.DS

TL;DR: Quantum algorithms for linear algebraic problems like computing determinant, trace of matrix cubes, or trace of matrix inverses are essentially equivalent in complexity to matrix multiplication on quantum computers.


<details>
  <summary>Details</summary>
Motivation: To understand the fundamental complexity relationship between various linear algebraic problems and matrix multiplication in the quantum computing context, and to establish that improvements beyond current matrix multiplication exponents would require breakthroughs in quantum algorithms.

Method: Using the Bernstein-Vazirani algorithm to create a quantum reduction from matrix multiplication to computing trace(ABC), then reducing trace(ABC) to other linear algebraic problems of interest.

Result: Shows that any T(n) time algorithm for computing det(A), tr(AÂ³), or tr(Aâ»Â¹) yields a O(T(n)) + Ã(nÂ²) time quantum algorithm for matrix multiplication, making these problems essentially equivalent in quantum complexity.

Conclusion: The complexity of these linear algebraic problems on quantum computers is fundamentally tied to matrix multiplication complexity, and any improvements beyond the current Ïâ2.37 barrier would lead to faster quantum matrix multiplication algorithms, complementing existing classical reductions.

Abstract: We observe that any $T(n)$ time algorithm (quantum or classical) for several
central linear algebraic problems, such as computing $\det(A)$, $tr(A^3)$, or
$tr(A^{-1})$ for an $n \times n$ integer matrix $A$, yields a $O(T(n)) + \tilde
O(n^2)$ time \textit{quantum algorithm} for $n \times n$ matrix-matrix
multiplication. That is, on quantum computers, the complexity of these problems
is essentially equivalent to that of matrix multiplication. Our results follow
by first observing that the Bernstein-Vazirani algorithm gives a direct quantum
reduction from matrix multiplication to computing $tr(ABC)$ for $n \times n$
inputs $A,B,C$. We can then reduce $tr(ABC)$ to each of our problems of
interest.
  For the above problems, and many others in linear algebra, their fastest
known algorithms require $\Theta(n^\omega)$ time, where $\omega \approx 2.37$
is the current exponent of fast matrix multiplication. Our finding shows that
any improvements beyond this barrier would lead to faster quantum algorithms
for matrix multiplication. Our results complement existing reductions from
matrix multiplication in algebraic circuits [BCS13], and reductions that work
for standard classical algorithms, but are not tight -- i.e., which roughly
show that an $O(n^{3-\delta})$ time algorithm for the problem yields an
$O(n^{3-\delta/3})$ matrix multiplication algorithm [WW10].

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [47] [Exact Solvability and Integrability Signatures in a Periodically Driven Infinite-Range Spin Chain: The Case of Floquet interval $Ï/2$](https://arxiv.org/abs/2509.19991)
*Harshit Sharma,Sashmita Rout,Avadhut V. Purohit,Udaysinh T. Bhosale*

Main category: quant-ph

TL;DR: The paper extends analysis of quantum integrability signatures in a spin chain model with infinite-range Ising interaction under periodic magnetic field pulses, showing that integrability persists for rational coupling parameters but disappears for irrational ones.


<details>
  <summary>Details</summary>
Motivation: To understand how quantum integrability manifests in spin chain models with long-range interactions and periodic driving, specifically examining the role of rational vs irrational coupling parameters in determining integrability signatures.

Method: Analysis of unitary operator, eigensystem, single-qubit reduced density matrix, and entanglement dynamics for arbitrary initial states and system sizes; spectral statistics including eigenvalue spacing distributions and average adjacent gap ratio; computation of entanglement entropy ratios.

Result: Quantum integrability signatures (periodicity of entanglement dynamics, degenerated spectra, Poisson statistics) persist for rational coupling parameters J, but vanish for irrational J. For irrational J and perturbed rational J, eigenvalue spacing follows Poisson statistics with â¨râ©=0.386. Entanglement entropy ratio remains significantly below 1 in thermodynamic limit.

Conclusion: The study demonstrates that quantum integrability in the spin chain model is sensitive to the rationality of coupling parameters, with rational values preserving integrability signatures while irrational values destroy them, providing insights into quantum chaos and integrability transitions.

Abstract: We study the signatures of quantum integrability (QI) in a spin chain model,
having infinite-range Ising interaction and subjected to a periodic pulse of an
external magnetic field. We analyze the unitary operator, its eigensystem, the
single-qubit reduced density matrix, and the entanglement dynamics for
arbitrary initial state for any $N$. The QI in our model can be identified
through key signatures such as the periodicity of entanglement dynamics and the
time-evolved unitary operator, and highly degenerated spectra or Poisson
statistics. In our previous works, these signatures were observed in the model
for parameters $\tau=\pi/4$ and $J=1,1/2$, where we provided exact analytical
results up to $12$ qubits and numerically for large $N$ [Phys. Rev. B
\textbf{110}, 064313,(2024)}; arXiv:2411.16670 (2024)}]. In this paper, we
extend the analysis to $\tau=m\pi/2$, and arbitrary $J$ and $N$. We show that
the signatures of QI persist for the rational $J$, whereas for irrational $J$,
these signatures are absent for any $N$. Further, we perform spectral
statistics and find that for irrational $J$, as well as for rational $J$ with
perturbations, the spacing distributions of eigenvalues follow Poisson
statistics. The average adjacent gap ratio is obtained as $\langle r
\rangle=0.386$, consistent with Poisson statistics. Additionally, we compute
the ratio of eigenstate entanglement entropy to its maximum value ($\langle S
\rangle /S_{Max}$) and find that it remains significantly below $1$ in the
limit $N\rightarrow \infty$, which further confirms the QI. We discuss some
potential experimental realizations of our model.

</details>


### [48] [Refine coherent control of atomic qubits via wave-function approach conditioned on no-decay](https://arxiv.org/abs/2509.20042)
*Yuan Sun*

Main category: quant-ph

TL;DR: Spontaneous emission introduces errors in quantum logic gates that persist even when no actual decay occurs, requiring modified SchrÃ¶dinger equations and careful design considerations for high-fidelity gates.


<details>
  <summary>Details</summary>
Motivation: Spontaneous emission is an inevitable error source in quantum systems that degrades gate fidelity. Even without actual decay events, the wave function is affected, necessitating proper theoretical description.

Method: The paper proposes using a modified SchrÃ¶dinger equation for dynamics conditioned on no-decay events, applying quantum error correction/mitigation theory with post-selection based on syndrome extraction.

Result: Calculations show that spontaneous emission effects must be seriously considered in high-fidelity quantum gate design, as they impact fidelity even when coherence is preserved.

Conclusion: Improving quantum gate fidelity requires careful consideration of subtle spontaneous emission influences, particularly for atomic qubit platforms in gate and readout processes.

Abstract: As a fundamental phenomenon in quantum systems, spontaneous emission
constitutes an inevitable source of error, which ultimately degrades the
fidelity of quantum logic gates. A successful quantum logic gate needs to
operate on the condition that no decay event, such as spontaneous emission,
occurs. Such successes can be ensured by post-selection based on syndrome
extraction according to the theory of quantum error correction or quantum error
mitigation. In this case, the wave function of qubits remains a pure state but
is subject to additional influences from spontaneous emission, even without
actual decay events. Therefore, such a process must be appropriately described
by a modified version of Schr\"odinger equation for the dynamics conditioned on
no-decay. Calculations reveal that this effect must be seriously taken into
consideration for the design of high-fidelity quantum logic gates. With respect
to realistic experimental conditions, even if the coherence is well preserved,
improving the fidelity of manipulating physical qubits requires careful
consideration of the subtle influences of decay processes such as spontaneous
emission. Specifically, the gate and readout processes in the atomic qubit
platform are discussed.

</details>


### [49] [Dynamically Optimal Unraveling Schemes for Simulating Lindblad Equations](https://arxiv.org/abs/2509.19887)
*Yu Cao,Mingfeng He,Xiantao Li*

Main category: quant-ph

TL;DR: This paper introduces dynamically optimal quantum state diffusion (DO-QSD) and quantum jump process (DO-QJP) schemes that minimize variance growth in stochastic unraveling of Lindblad equations, showing DO-QSD outperforms jump processes with simpler expressions.


<details>
  <summary>Details</summary>
Motivation: Stochastic unraveling schemes reduce memory requirements for simulating Lindblad equations but increase stochastic uncertainty, and the question of optimal unraveling remains unresolved.

Method: The authors investigate Brownian motion and Poisson process-driven unraveling schemes, provide parametric characterization, and analytically derive DO-QSD and DO-QJP that minimize short-time variance growth of observables.

Result: DO-QSD is shown to have lower variance than any jump-process ansatz locally in time, with simpler expressions. Numerical results demonstrate substantial variance reduction in observable simulations.

Conclusion: The proposed DO-QSD scheme achieves significant variance reduction and error improvement in Lindblad equation simulations, offering advantages over traditional jump-process approaches.

Abstract: Stochastic unraveling schemes are powerful computational tools for simulating
Lindblad equations, offering significant reductions in memory requirements.
However, this advantage is accompanied by increased stochastic uncertainty, and
the question of optimal unraveling remains open. In this work, we investigate
unraveling schemes driven by Brownian motion or Poisson processes and present a
comprehensive parametric characterization of these approaches. For the case of
a single Lindblad operator and one noise term, this parametric family provides
a complete description for unraveling scheme with pathwise norm-preservation.
We further analytically derive dynamically optimal quantum state diffusion
(DO-QSD) and dynamically optimal quantum jump process (DO-QJP) that minimize
the short-time growth of the variance of an observable. Compared to jump
process ansatz, DO-QSD offers two notable advantages: firstly, the variance for
DO-QSD can be rigorously shown not to exceed that of any jump-process ansatz
locally in time; secondly, it has very simple expressions. Numerical results
demonstrate that the proposed DO-QSD scheme may achieve substantial reductions
in the variance of observables and the resulting simulation error.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [50] [Toward a Theory of Gravitational Wave Turbulence](https://arxiv.org/abs/2509.19769)
*Holly Krynicki,Jiaxi Wu,Elias R. Most*

Main category: gr-qc

TL;DR: This paper develops a theoretical framework for understanding gravitational wave turbulence by reformulating general relativity using Elsasser equations, linking it to magnetohydrodynamic turbulence concepts.


<details>
  <summary>Details</summary>
Motivation: To establish a mathematical description for gravitational wave turbulence, which currently lacks a complete theoretical framework despite evidence of nonlinear interactions in general relativity.

Method: Leveraging a formulation that recasts general relativity as nonlinear electrodynamics equations, the authors demonstrate that general relativity admits an Elsasser formulation similar to magnetohydrodynamic turbulence equations.

Result: The paper shows that nonlinear interactions in gravitational waves are partly AlfvÃ©nic, establishing a connection between gravitational wave turbulence and AlfvÃ©nic turbulence.

Conclusion: This work provides a foundation for understanding nonlinear gravitational wave dynamics using insights from magnetohydrodynamics, paving the way for new theoretical developments.

Abstract: General relativity describes the dynamics of gravitational waves, which can
feature nonlinear interactions, such as those underlying turbulent processes.
Theoretical and numerical explorations have demonstrated the existence of
gravitational wave turbulence, of which a full and general mathematical
description is currently not known. Here, we take essential steps towards such
a theory. Leveraging a formulation exactly recasting general relativity as a
set of nonlinear electrodynamics equations, we demonstrate that general
relativity admits an Elsasser formulation -- the same type of equation
underpinning magnetohydrodynamic turbulence. We further show that nonlinear
interactions described by this equation are in part Alfv\'enic, linking
gravitational wave turbulence to Alfv\'enic turbulence. Our work paves the way
for a new understanding of nonlinear gravitational wave dynamics through
insights from magnetohydrodynamics.

</details>
