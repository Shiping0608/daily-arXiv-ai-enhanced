<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 8]
- [math.AP](#math.AP) [Total: 13]
- [physics.comp-ph](#physics.comp-ph) [Total: 2]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 2]
- [physics.ins-det](#physics.ins-det) [Total: 1]
- [math.PR](#math.PR) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [cs.CE](#cs.CE) [Total: 4]
- [physics.optics](#physics.optics) [Total: 2]
- [q-bio.TO](#q-bio.TO) [Total: 1]
- [stat.ME](#stat.ME) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 2]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [A robust framework for frictional fault contact in geological formations using a stabilized augmented Lagrangian approach](https://arxiv.org/abs/2509.20528)
*Matteo Frigo,Nicola Castelletto,Matteo Cusini,Randolph R. Settgast,Hamdi A. Tchelepi*

Main category: math.NA

TL;DR: This paper presents a numerical method for modeling frictional contact behavior along fault surfaces in geological systems using an augmented Lagrangian approach with mixed finite element spaces and bubble function enrichment for stability.


<details>
  <summary>Details</summary>
Motivation: Accurate modeling of frictional contact behavior along fault surfaces is crucial for evaluating performance and safety of geological engineered systems like carbon storage sites, geothermal fields, and reservoirs, but existing methods struggle with the nonlinear, path-dependent nature of frictional slip constraints.

Method: Uses augmented Lagrangian method via Uzawa algorithm with mixed finite element spaces combining low-order piecewise linear displacements in 3D domain cells with piecewise constant tractions on fault surfaces. Discrete displacement space is enriched with face bubble functions on both sides of contact interfaces for stability.

Result: The proposed approach provides advantages over other stabilization techniques by not requiring additional parameters for implementation and integrating naturally within the Uzawa framework.

Conclusion: The method offers an effective solution for handling the inequality constraints and nonlinear behavior of frictional contact problems in geological systems through a stable, parameter-free implementation.

Abstract: Numerical simulations are essential for evaluating the performance and safety
of geological engineered systems such as geologic carbon storage sites,
enhanced geothermal fields, and oil and gas reservoirs. A key challenge lies in
accurately modeling the frictional contact behavior along fault surfaces. This
problem involves inequality constraints that arise from the physics of
frictional slip, requiring specialized numerical methods to handle the
resulting highly nonlinear and path-dependent behavior. In this work, we
address this challenge using an augmented Lagrangian method implemented via the
Uzawa algorithm. The formulation employs mixed finite element spaces, combining
low-order piecewise linear displacements within the 3D domain cells with
piecewise constant tractions defined on the fault surfaces. Furthermore, to
ensure stability and satisfy the inf-sup condition, the discrete displacement
space is enriched with face bubble functions on both sides of the contact
interfaces. This approach offers several advantages over other stabilization
techniques that rely on additional terms, as it does not require extra
parameters for implementation, and it integrates naturally in the Uzawa
framework.

</details>


### [2] [Symplectic Isospectral Runge--Kutta Methods as Lie group methods](https://arxiv.org/abs/2509.20620)
*Paolo Cifani,Klas Modin,Cecilia Pagliantini,Milo Viviani*

Main category: math.NA

TL;DR: Comparison of three approaches for conservative time integration of isospectral flows on quadratic Lie algebras, showing that equivalent formulations with symplectic Runge-Kutta methods yield conservative and efficient schemes.


<details>
  <summary>Details</summary>
Motivation: To develop efficient and conservative integration methods for isospectral flows on quadratic Lie algebras, improving upon previously proposed schemes.

Method: Using equivalent formulations of isospectral flows combined with symplectic Runge-Kutta methods to create conservative integration schemes.

Result: Demonstrated that this approach produces conservative and computationally efficient schemes, with arbitrary high-order Lie-Poisson integrators for Hamiltonian systems on quadratic Lie algebras.

Conclusion: The proposed method provides superior conservative integration for isospectral flows, offering both computational efficiency and high-order accuracy for Hamiltonian systems.

Abstract: In this paper, we compare three different approaches for a conservative
integration in time of isospectral flows on quadratic Lie algebras.
  We show that it is possible to choose an equivalent formulation of the
original isospectral flow such that, applying a symplectic Runge--Kutta method,
the resulting scheme is both conservative and computationally more efficient
than other schemes previously proposed.
  In particular, in the case of Hamiltonian systems, we get arbitrarily
high-order Lie--Poisson integrators on quadratic Lie algebras.

</details>


### [3] [A domain decomposition method for computing the scattering matrix of waveguide circuits](https://arxiv.org/abs/2509.20695)
*Tristan Goodwill,Shidong Jiang,Manas Rachh,Kosuke Sugita*

Main category: math.NA

TL;DR: A fast divide-and-conquer solver for time-harmonic wave scattering in metallic waveguide structures using impedance-to-impedance maps and second-kind Fredholm integral equations with weakly singular kernels.


<details>
  <summary>Details</summary>
Motivation: To develop efficient numerical methods for wave scattering in infinite metallic waveguide structures, particularly addressing challenges when structures support trapped modes and require handling of propagating modes uniquely.

Method: Uses radiation boundary conditions via projectors onto outgoing modes, constructs solution operators as impedance-to-impedance maps on subdomains, and couples them through continuity conditions. For Dirichlet waveguides, employs a second-kind Fredholm integral equation with only weakly singular kernels.

Result: Numerical experiments show substantial efficiency gains, outperforming state-of-the-art fast iterative and direct solvers by one to two orders of magnitude on large structures with many circuit elements.

Conclusion: The proposed approach provides an efficient and accurate method for solving wave scattering problems in complex waveguide structures, demonstrating significant computational advantages over existing methods.

Abstract: We analyze and develop numerical methods for time-harmonic wave scattering in
metallic waveguide structures of infinite extent. We show that radiation
boundary conditions formulated via projectors onto outgoing modes determine the
coefficients of propagating modes uniquely, even when the structure supports
trapped modes. Building on this, we introduce a fast divide-and-conquer solver
that constructs solution operators on subdomains as impedance-to-impedance maps
and couples them by enforcing continuity conditions across their interfaces.
For Dirichlet waveguides, the computation of impedance-to-impedance maps
requires the solution of mixed Dirichlet-Impedance boundary value problems. We
construct a second-kind Fredholm integral equation that avoids
near-hypersingular operators, requiring only integral operators whose kernels
are at most weakly singular. Numerical experiments on large structures with
many circuit elements demonstrate substantial efficiency gains: the proposed
approach typically outperforms state-of-the-art fast iterative and fast direct
solvers by one to two orders of magnitude.

</details>


### [4] [A Convergent Structure-Preserving Scheme for Dissipative Solutions of the Rotating Shallow Water System](https://arxiv.org/abs/2509.20764)
*K. R. Arun,A. Krishnamurthy*

Main category: math.NA

TL;DR: A semi-implicit finite volume scheme for 2D rotating shallow water equations that achieves energy stability, well-balancing, consistency, and convergence through carefully designed stabilization terms.


<details>
  <summary>Details</summary>
Motivation: To develop a robust numerical scheme for rotating shallow water equations that can preserve important physical properties like energy stability and geostrophic steady states while maintaining numerical convergence.

Method: Introduces stabilization terms into convective fluxes and source terms of mass and momentum equations. Uses semi-implicit finite volume approach with CFL-type conditions and auxiliary time-step restrictions for Coriolis forces.

Result: The scheme achieves energy stability under CFL conditions, preserves discrete geostrophic steady states (well-balanced), maintains consistency as mesh refines, and converges to dissipative measure-valued solutions.

Conclusion: The proposed scheme successfully combines energy stability, well-balancing, consistency, and convergence properties, with theoretical results validated through extensive numerical experiments.

Abstract: We design and analyse a semi-implicit finite volume scheme for the
two-dimensional rotating shallow water (RSW) equations that is energy stable,
well-balanced (capable of preserving discrete geostrophic steady states),
consistent, and covergent. The key idea is the introduction of carefully chosen
stabilisation terms into the convective fluxes of the mass and momentum
equations, as well as the source terms. Under a CFL-type condition, together
with an auxiliary time-step restriction arising from the Coriolis forces, we
establish the energy stability of the scheme. The stabilisation terms are
constructed to vanish at steady states, thereby ensuring the well-balancing
property under an appropriate advective CFL condition. We derive a sufficient
time-step restriction that guarantees stability, well-balancing, existence of
discrete solutions, and positivity simultaneously. Furthermore, under mild
boundedness assumptions, we obtain a priori estimates showing that the
stabilisation terms converge to zero as the mesh is refined, which establishes
the consistency of the scheme. This in turn enables us to prove that numerical
solutions generate a Young measure, identifiable as a dissipative
measure-valued solution of the RSW system, thereby yielding convergence of the
scheme. Finally, we confirm the theoretical results through extensive numerical
experiments.

</details>


### [5] [Higher-Order Root-Finding Algorithm and its Applications](https://arxiv.org/abs/2509.20897)
*Wei Guo Foo,Chik How Tan*

Main category: math.NA

TL;DR: A new higher-order root-finding method using only Taylor expansion is proposed, offering lower computational complexity and explicit convergence factors compared to Householder's method, with applications in coding theory.


<details>
  <summary>Details</summary>
Motivation: Householder's method requires higher-order derivatives which are computationally expensive (symbolic computations take time, numerical differentiation accumulates errors) and has rough convergence factor estimates.

Method: Proposes a higher-order root-finding method using only Taylor expansion of a function, avoiding the need for higher-order derivatives of the reciprocal function.

Result: The method has lower computational complexity with explicit convergence factor, can numerically implement Householder's method, and successfully computes pre-images of q-ary entropy functions.

Conclusion: The proposed Taylor expansion-based method provides an efficient alternative to Householder's method with better computational properties and explicit convergence analysis, validated through applications in coding theory and basin of attraction studies.

Abstract: Root-finding method is an iterative process that constructs a sequence
converging to a solution of an equation. Householder's method is a higher-order
method that requires higher order derivatives of the reciprocal of a function
and has disadvantages. Firstly, symbolic computations can take a long time, and
numerical methods to differentiate a function can accumulate errors. Secondly,
the convergence factor existing in the literature is a rough estimate. In this
paper, we propose a higher-order root-finding method using only Taylor
expansion of a function. It has lower computational complexity with explicit
convergence factor, and can be used to numerically implement Householder's
method. As an application, we apply the proposed method to compute pre-images
of $q$-ary entropy functions, commonly seen in coding theory. Finally, we study
basins of attraction using the proposed method and compare them with other
root-finding methods.

</details>


### [6] [Multiprecision computations with Schwarz methods](https://arxiv.org/abs/2509.20937)
*Michal Outrata,Daniel B. Szyld*

Main category: math.NA

TL;DR: Using lower precision arithmetic (single precision) for local solves in Schwarz methods and preconditioners can achieve theoretical accuracy requirements while reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: To reduce computational costs in Schwarz methods by using lower precision arithmetic for local problem solves while maintaining sufficient accuracy.

Method: Analyze multiprecision arithmetic for Schwarz methods and preconditioners, using single precision (about 5 digits) for local solves instead of double precision. Present round-off criteria and conduct numerical experiments.

Result: Experimental findings show that about 5 digits of accuracy (single precision) are sufficient to meet theoretical restrictions for model problems.

Conclusion: Single precision arithmetic suffices for local solves in Schwarz methods, providing computational efficiency without compromising accuracy for the tested model problems.

Abstract: We explore and analyze the use of multiprecision arithmetic for several
classes of Schwarz methods and preconditioners, where the approximate solution
of the local problems is performed at a lower precision, i.e., with fewer
digits of accuracy than in the underlying (double precision) computation.
Conditions for the appropriate round-off criteria for the lower precision are
presented. It is found experimentally that for the model problems about 5
digits of accuracy are sufficient to achieve the theoretical restrictions, and
thus, single precision suffices for the local solves. Several numerical
experiments illustrate the obtained results.

</details>


### [7] [Model reduction of parametric ordinary differential equations via autoencoders: structure-preserving latent dynamics and convergence analysis](https://arxiv.org/abs/2509.21280)
*Enrico Ballini,Marco Gambarini,Alessio Fumagalli,Luca Formaggia,Anna Scotti,Paolo Zunino*

Main category: math.NA

TL;DR: A reduced-order modeling approach using autoencoder-based nonlinear maps for parameter-dependent ODEs, achieving dimensionality reduction while maintaining accuracy and stability.


<details>
  <summary>Details</summary>
Motivation: To accelerate complex dynamical simulations of nonlinear, parameter-dependent ODEs without sacrificing accuracy by developing efficient reduced-order models.

Method: Using autoencoders (neural networks) for nonlinear dimensionality reduction, solving the low-dimensional ODE with standard time integration, and reconstructing the high-dimensional solution from the reduced representation.

Result: Numerical experiments demonstrate robustness and accuracy, with the approach showing potential to accelerate simulations while maintaining solution quality and stability properties.

Conclusion: The proposed autoencoder-based reduced-order modeling effectively accelerates complex dynamical simulations while preserving accuracy and stability, with proven convergence to high-fidelity models.

Abstract: We propose a reduced-order modeling approach for nonlinear,
parameter-dependent ordinary differential equations (ODE). Dimensionality
reduction is achieved using nonlinear maps represented by autoencoders. The
resulting low-dimensional ODE is then solved using standard integration in time
schemes, and the high-dimensional solution is reconstructed from the
low-dimensional one. We investigate the applicability of neural networks for
constructing effective autoencoders with the property of reconstructing the
input manifold with null representation error. We study the convergence of the
reduced-order model to the high-fidelity one. Numerical experiments show the
robustness and accuracy of our approach, highlighting its potential to
accelerate complex dynamical simulations without sacrificing accuracy.
Moreover, we examine how the reduction influences the stability properties of
the reconstructed high-dimensional solution.

</details>


### [8] [Two ADI compact difference methods for variable-exponent diffusion wave equations](https://arxiv.org/abs/2509.21316)
*Hao Zhang,Kexin Li,Wenlin Qiu*

Main category: math.NA

TL;DR: This paper develops two efficient numerical schemes for solving 2D diffusion-wave equations with variable exponents, using ADI compact methods that achieve high-order accuracy in space and time.


<details>
  <summary>Details</summary>
Motivation: To model mechanical diffusive wave propagation in viscoelastic media with spatially varying properties, which requires efficient numerical methods due to the complexity of variable exponent diffusion-wave equations.

Method: Transformed the diffusion-wave model via convolution method, applied two time discretization strategies, and developed two ADI compact schemes using alternating direction implicit technique with spatial compact finite difference method.

Result: Both ADI compact schemes are proven unconditionally stable and convergent. First scheme achieves α(0)-order accuracy in time and fourth-order in space; second scheme attains second-order accuracy in time and fourth-order in space. Numerical experiments confirm theoretical error estimates.

Conclusion: The proposed methods are efficient for solving 2D diffusion-wave equations with variable exponents, providing high accuracy and computational efficiency through ADI compact schemes.

Abstract: In this work, we study two-dimensional diffusion-wave equations with variable
exponent, modeling mechanical diffusive wave propagation in viscoelastic media
with spatially varying properties. We first transform the diffusion-wave model
into an equivalent form via the convolution method. Two time discretization
strategies are then applied to approximate each term in the transformed
equation, yielding two fully discrete schemes based on a spatial compact finite
difference method. To reduce computational cost, the alternating direction
implicit (ADI) technique is employed. We prove that both ADI compact schemes
are unconditionally stable and convergent. Under solution regularity, the first
scheme achieves $\alpha(0)$-order accuracy in time and fourth-order accuracy in
space, while the second scheme attains second-order accuracy in time and
fourth-order accuracy in space. Numerical experiments confirm the theoretical
error estimates and demonstrate the efficiency of the proposed methods.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [9] [Increased lifespan for 3D compressible Euler flows with rotation](https://arxiv.org/abs/2509.20505)
*Haram Ko,Benoit Pausader,Ryo Takada,Klaus Widmayer*

Main category: math.AP

TL;DR: This paper analyzes the compressible Euler equation with Coriolis term, proving lower bounds on solution existence time based on rotation speed, sound speed, and initial data size, while obtaining dispersive decay estimates for the linearized equation.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of rotating compressible fluids and improve existing bounds for both compressible and incompressible Euler-Coriolis systems, particularly in the incompressible limit.

Method: The authors prove mathematical bounds on solution existence time using analytical techniques, derive precise dispersive decay estimates for the linearized equation, and perform asymptotic analysis in the incompressible limit.

Result: The paper establishes improved lower bounds on solution existence time in terms of rotation speed, sound speed, and initial data size, and shows that these results lead to better bounds for the incompressible Euler-Coriolis system as well.

Conclusion: The analysis provides enhanced understanding of rotating fluid dynamics, with implications for both compressible and incompressible systems, demonstrating that the compressible framework yields improved bounds even in the incompressible limit.

Abstract: We consider the compressible Euler equation with a Coriolis term and prove a
lower bound on the time of existence of solutions in terms of the speed of
rotation, sound speed and size of the initial data. Along the way, we obtain
precise dispersive decay estimates for the linearized equation. In the
incompressible limit, this improves current bounds for the incompressible
Euler-Coriolis system as well.

</details>


### [10] [On the Landis Conjecture for Positive Quasi-linear Operators on Graphs](https://arxiv.org/abs/2509.20559)
*Ujjal Das,Matthias Keller,Yehuda Pinchover*

Main category: math.AP

TL;DR: This paper proves a Landis-type unique continuation result for positive quasi-linear operators on graphs, showing that harmonic functions for certain operators must be zero under decay conditions.


<details>
  <summary>Details</summary>
Motivation: To establish unique continuation properties for harmonic functions associated with positive quasilinear Schrödinger operators on graphs, extending Landis-type results to graph settings.

Method: The approach uses criticality theory (including the Liouville comparison theorem) and builds on the concept of simplified energy. The analysis is applied to model graphs, particularly regular trees.

Result: The paper provides decay criteria that ensure harmonic functions for positive quasilinear Schrödinger operators with potential less than 1 are trivially zero.

Conclusion: The results demonstrate that positivity assumptions on operators enable the application of criticality theory to prove unique continuation properties on graphs, with specific applications to model graphs and regular trees.

Abstract: We prove a Landis type unique continuation result for positive quasi-linear
operators on graphs. Specifically, we give decay criteria that ensures when a
harmonic function for a positive quasilinear Schr\"odinger operator with
potential less than 1 is trivially zero. The assumption of positivity of the
operator allows the application of criticality theory such as the Liouville
comparison theorem. Furthermore, our results fundamentally build on the so
called simplified energy. As an application we discuss the case of model graphs
and in particular regular trees.

</details>


### [11] [Extended Sobolev Scale on Non-Compact Manifolds](https://arxiv.org/abs/2509.20598)
*Ognjen Milatovic*

Main category: math.AP

TL;DR: This paper extends the 'extended Sobolev scale' to manifolds of bounded geometry, defines interpolation spaces between Sobolev spaces, and studies mapping properties of pseudo-differential operators on these scales.


<details>
  <summary>Details</summary>
Motivation: To generalize the extended Sobolev scale concept from compact manifolds and Euclidean spaces to non-compact manifolds of bounded geometry, enabling interpolation theory and operator mapping properties in this broader setting.

Method: Adapting Mikhailets and Murach's definition to manifolds of bounded geometry, defining H^φ(X) spaces with RO-varying functions φ, establishing interpolation properties, and analyzing mapping properties of proper uniform pseudo-differential operators.

Result: Obtained complete description of interpolation spaces between Sobolev spaces, established mapping properties of PUPDOs on the extended Sobolev scale, and showed equivalence between H^φ(X) and the extended A-scale defined via elliptic operators.

Conclusion: The extended Sobolev scale framework successfully generalizes to manifolds of bounded geometry, preserving key properties from compact settings and enabling interpolation theory and operator analysis in this non-compact context.

Abstract: Adapting the definition of ``extended Sobolev scale" on compact manifolds by
Mikhailets and Murach to the setting of a (generally non-compact) manifold of
bounded geometry $X$, we define the ``extended Sobolev scale" $H^{\varphi}(X)$,
where $\varphi$ is a function which is $RO$-varying at infinity. With the help
of the scale $H^{\varphi}(X)$, we obtain a description of all Hilbert
function-spaces that serve as interpolation spaces with respect to a pair of
Sobolev spaces $[H^{(s_0)}(X), H^{(s_1)}(X)]$, with $s_0<s_1$. We use this
interpolation property to establish a mapping property of proper uniform
pseudo-differential operators (PUPDOs) in the context of the scale
$H^{\varphi}(X)$. Additionally, using a first-order positive-definite PUPDO $A$
of elliptic type we define the ``extended $A$-scale" $H^{\varphi}_{A}(X)$ and
show that it coincides, up to norm equivalence, with the scale
$H^{\varphi}(X)$. Besides the mentioned results, we show that further
properties of the $H^{\varphi}$-scale, originally established by Mikhailets and
Murach on $\mathbb{R}^n$ and on compact manifolds, carry over to manifolds of
bounded geometry.

</details>


### [12] [State-Constrained Chemical Reactions: Discrete-to-Continuous Hamilton--Jacobi Equations and Large Deviations](https://arxiv.org/abs/2509.20747)
*Yuan Gao,Yuxi Han*

Main category: math.AP

TL;DR: The paper establishes the large deviation principle for chemical reaction processes in bounded domains by showing convergence of discrete Hamilton-Jacobi equations to continuous ones with Neumann boundary conditions.


<details>
  <summary>Details</summary>
Motivation: To understand the macroscopic behavior of chemical reactions modeled as random time-changed Poisson processes and establish connections between discrete and continuous formulations in bounded domains.

Method: Using WKB reformulation and discrete Hamilton-Jacobi equations with state constraints, then proving convergence to continuous Hamilton-Jacobi equations with Neumann boundary conditions through suitable reparametrization.

Result: Shows that under proper reparametrization, discrete Hamilton-Jacobi equation solutions converge to continuous ones, enabling establishment of large deviation principle for rescaled chemical reaction processes.

Conclusion: The convergence results and variational representations provide a rigorous foundation for large deviation principles in state-constrained chemical reaction systems in bounded domains.

Abstract: We study the macroscopic behavior of chemical reactions modeled as random
time-changed Poisson processes on discrete state spaces. Using the WKB
reformulation, the backward equation of the rescaled process leads to a
discrete Hamilton--Jacobi equation with state constraints. As the grid size
tends to zero, the limiting solution and its associated variational
representation are closely connected to the good rate function of the large
deviation principle for state-constrained chemical reactions in the
thermodynamic limit. In this work, we focus on the limiting behavior of
discrete Hamilton--Jacobi equations defined on bounded domains with
state-constraint boundary conditions. For a single chemical reaction, we show
that, under a suitable reparametrization, the solution of the discrete
Hamilton--Jacobi equation converges to the solution of a continuous
Hamilton--Jacobi equation with a Neumann boundary condition. Building on this
convergence result and the associated variational representation, we establish
the large deviation principle for the rescaled chemical reaction process in
bounded domains.

</details>


### [13] [MIT bag in non-smooth convex domains](https://arxiv.org/abs/2509.20958)
*Konstantin Pankrashkin*

Main category: math.AP

TL;DR: The paper proves self-adjointness of Dirac operators with MIT bag boundary conditions in convex domains and shows they appear as limits of Dirac operators with large mass outside the domain, extending previous smooth-domain results.


<details>
  <summary>Details</summary>
Motivation: Previous results on Dirac operators with MIT bag boundary conditions were limited to smooth domains. The motivation is to extend these results to more general convex domains, which are physically relevant but mathematically more challenging.

Method: The authors work with the Dirac operator in bounded convex domains using the H¹-setting. They demonstrate self-adjointness properties and establish convergence results by considering Dirac operators with large positive mass outside the domain.

Result: The main results show that Dirac operators with MIT bag boundary conditions are always self-adjoint in convex domains, and they can be obtained as limits of Dirac operators with large mass outside the domain.

Conclusion: The paper successfully extends the theory of Dirac operators with MIT bag boundary conditions from smooth domains to convex domains, providing a more general framework that has applications in mathematical physics.

Abstract: The Dirac operator with MIT bag boundary condition in a bounded convex domain
is shown to be always self-adjoint in the $H^1$-setting. This allows one to
show that such operators appear as limit of Dirac operators with large positive
mass outside the domain. Similar results were previously known for smooth
domains only.

</details>


### [14] [A nonlocal Aw-Rascle-Zhang system with linear pressure term](https://arxiv.org/abs/2509.20973)
*Debora Amadori,Felisia Angela Chiarello,Gianmarco Cipollone*

Main category: math.AP

TL;DR: Study of a nonlocal extension of the Aw-Rascle-Zhang traffic model using convolution-based pressure term to capture driver interactions, with entropy solution construction via sticky particle approximation.


<details>
  <summary>Details</summary>
Motivation: To develop a traffic flow model that incorporates nonlocal driver interactions through convolution-based pressure terms, structurally aligning with Euler-alignment systems for more realistic traffic behavior modeling.

Method: Uses sticky particle approximation to construct entropy solutions for the cumulative density equation, proving convergence to weak solutions of the nonlocal system.

Result: Establishes well-posedness, stability estimates, and an entropic selection principle for the nonlocal traffic model.

Conclusion: The nonlocal extension provides a mathematically rigorous framework for traffic flow modeling with nonlocal interactions, with proven solution properties and convergence guarantees.

Abstract: In this paper, we study a nonlocal extension of the Aw-Rascle-Zhang traffic
model, where the pressure-like term is modeled as a convolution between vehicle
density and a kernel function. This formulation captures nonlocal driver
interactions and aligns structurally with the Euler-alignment system studied in
[23]. Using a sticky particle approximation, we construct entropy solutions to
the equation for the cumulative density and prove convergence of approximate
solutions to weak solutions of the nonlocal system. The analysis includes
well-posedness, stability estimates, and an entropic selection principle.

</details>


### [15] [Graphical Willmore Problems with Low-Regularity Boundary and Dirichlet Data](https://arxiv.org/abs/2509.21018)
*Boris Gulyak*

Main category: math.AP

TL;DR: Existence and regularity results for boundary value problems of Willmore energy variation in graphical setting, with reduced regularity requirements and extensions to Lipschitz boundaries.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous mathematical foundations for boundary value problems arising from Willmore energy variation, particularly for surfaces with clamped boundary conditions and non-smooth boundaries.

Method: Construct solutions through linearization and fixed-point argument using weighted second-order Sobolev spaces, rewriting Willmore equation in divergence form based on Koch and Lamm's approach.

Result: Successfully reduced regularity assumptions to C¹⁺α-class for boundaries and Dirichlet data while maintaining interior smoothness; extended existence theory to Lipschitz boundaries within weighted Sobolev framework.

Conclusion: The approach provides a robust framework for Willmore energy boundary value problems with weakened regularity requirements and is applicable to other higher-order geometric PDEs like Helfrich and surface diffusion equations.

Abstract: We establish existence and regularity results for boundary value problems
arising from the first variation of the Willmore energy in the graphical
setting. Our focus lies on two-dimensional surfaces with fixed clamped boundary
conditions, embedded in three-dimensional Euclidean space, and represented as
graphs of height functions over domains with non-smooth boundaries. Our
approach involves constructing solutions through linearization and a
fixed-point argument, requiring small boundary data in suitable functional
spaces. Building on the results of Koch and Lamm \cite{koch2012geometric}, we
rewrite the Willmore equation for graphs in a divergence form that allows the
application of weighted second-order Sobolev spaces. This reformulation
significantly weakens the regularity assumptions on both the boundary and the
Dirichlet data, reducing them to the $C^{1+\alpha}$-class, while the solution
remains smooth in the interior. Moreover, we extend the existence theory to
domains with merely Lipschitz boundaries within a purely weighted Sobolev
framework. Our approach is also applicable to other higher-order geometric
PDEs, including the graphical Helfrich and surface diffusion equations.

</details>


### [16] [The Incompressible Navier-Stokes-Fourier Limits from Boltzmann-Fermi-Dirac Equation for Low Regularity Data](https://arxiv.org/abs/2509.21030)
*Ning Jiang,Chenchen Wang,Kai Zhou*

Main category: math.AP

TL;DR: This paper establishes the hydrodynamic limits of the quantum Boltzmann equation with Fermi-Dirac statistics for hard sphere and hard potentials, rigorously verifying the incompressible Navier-Stokes-Fourier limits from the BFD equation.


<details>
  <summary>Details</summary>
Motivation: To extend previous results by working with lower regularity initial data and ensuring the lifespan of kinetic solutions matches those of limiting fluid solutions.

Method: Analyzing the spectrum of the linearized collision operator combined with the transport operator and its associated semigroup, using fixed-point arguments with time iteration.

Result: Successfully verified the incompressible Navier-Stokes-Fourier limits from the BFD equation with lower regularity initial data than previous work.

Conclusion: The paper provides a rigorous verification of hydrodynamic limits for quantum Boltzmann equations with improved initial data regularity conditions and lifespan matching between kinetic and fluid solutions.

Abstract: We consider the hydrodynamic limits of the quantum Boltzmann equation with
Fermi-Dirac statistics for hard sphere and hard potentials in the whole space.
By analyzing the spectrum of the linearized collision operator combined with
the transport operator and its associated semigroup, the incompressible
Navier-Stokes-Fourier limits from the BFD equation is verified rigorously.
Compared to the results in [Jiang-Xiong-Zhou,J. Differ. Equ.,2022], this paper
works with a lower regularity for the initial data. In addition, the
fixed-point arguments together with a time iteration ensure us to obtain the
lifespan of kinetic solution coincides with those of limiting fluid solution.

</details>


### [17] [On the radius of spatial analyticity for the Majda-Biello and Hirota-Satsuma systems](https://arxiv.org/abs/2509.21095)
*Seongyeon Kim,Ihyeok Seo*

Main category: math.AP

TL;DR: This paper establishes the persistence of spatial analyticity for solutions to the Majda-Biello and Hirota-Satsuma systems with analytic initial data, marking the first proof of analyticity persistence in coupled KdV systems.


<details>
  <summary>Details</summary>
Motivation: To prove that analyticity persists in time for solutions to coupled KdV systems (Majda-Biello and Hirota-Satsuma) when starting with analytic initial data, which had not been previously established for such systems.

Method: Mathematical analysis of the coupled KdV systems, likely employing techniques from partial differential equations and analytic function theory to demonstrate that the analytic property of initial data is preserved by the evolution equations.

Result: The authors successfully prove that spatial analyticity persists for solutions to both the Majda-Biello and Hirota-Satsuma systems when the initial data is analytic.

Conclusion: This work provides the first rigorous proof of analyticity persistence in coupled KdV systems, establishing an important mathematical property for these nonlinear wave equations.

Abstract: We investigate the persistence of spatial analyticity for solutions to the
Majda-Biello and Hirota-Satsuma systems with analytic initial data. This result
is the first to establish analyticity persistence in such coupled KdV systems.

</details>


### [18] [Lagrangian aspects of Yudovich theory for 2D Euler](https://arxiv.org/abs/2509.21121)
*Theodore D. Drivas,Joonhyun La*

Main category: math.AP

TL;DR: Yudovich's existence and uniqueness result for bounded and mildly unbounded vorticity weak solutions of 2D incompressible Euler equations is established, with additional regularity results for the solution map.


<details>
  <summary>Details</summary>
Motivation: To provide a rigorous mathematical foundation for the existence and uniqueness of weak solutions to the 2D incompressible Euler equations with bounded or mildly unbounded vorticity, extending Yudovich's classical result.

Method: Mathematical analysis establishing Yudovich's theorem through rigorous proofs, with additional investigation into the regularity properties of the solution map as it depends on initial conditions and fluid domain.

Result: Successfully established existence and uniqueness of weak solutions for bounded and mildly unbounded vorticity cases, and derived regularity results for the Yudovich solution map.

Conclusion: The paper provides a complete mathematical treatment of Yudovich's result with enhanced understanding of solution map regularity, contributing to the theoretical foundation of 2D fluid dynamics.

Abstract: In this note, we establish Yudovich's existence and uniqueness result for
bounded (as well as mildly unbounded) vorticity weak solution of the
two-dimensional incompressible Euler equations. As a biproduct of our proof, we
establish some regularity results for the Yudovich solution map as it depends
of the initial conditions and the fluid domain.

</details>


### [19] [Asymptotic instability for the forced Navier--Stokes equations in critical Besov spaces](https://arxiv.org/abs/2509.21272)
*Mikihiro Fujii,Hiroyuki Tsurumi*

Main category: math.AP

TL;DR: This paper demonstrates asymptotic instability in forced Navier-Stokes equations for critical Besov spaces when p ≥ n (n≥3) and for all p in 2D case, showing flows oscillate rather than converge despite decaying external forces.


<details>
  <summary>Details</summary>
Motivation: To investigate the limitations of asymptotic stability in forced Navier-Stokes equations, particularly when standard arguments fail for certain parameter ranges in critical Besov spaces.

Method: Mathematical analysis using critical Besov spaces framework, constructing counterexamples with small external forces that decay over time but produce oscillating Navier-Stokes flows.

Result: Proved asymptotic instability for p ≥ n with n ≥ 3, and complete instability for n=2 in all p ranges. The instability arises from nonlinear interactions rather than linear effects.

Conclusion: The classical asymptotic stability results have significant limitations, and forced Navier-Stokes flows can exhibit oscillatory behavior rather than convergence even with decaying external forces in certain critical Besov spaces.

Abstract: The asymptotic stability is one of the classical problems in the field of
mathematical analysis of fluid mechanics. In $\mathbb{R}^n$ with $n \geq 3$, it
is easily proved by the standard argument that if the given small external
force decays at temporal infinity, then the small forced Navier--Stokes flow
also strongly converges to zero as time tends to infinity in the framework of
the critical Besov spaces $\dot{B}_{p,q}^{n/p-1}(\mathbb{R}^n)$ with $1 \leq p
< n$ and $1 \leq q < \infty$. In the present paper, we show that this
asymptotic stability fails for $p \geq n$ with $n \geq 3$ in the sense that
there exist arbitrary small external forces whose critical Besov norm decays in
large time, whereas the corresponding Navier--Stokes flows oscillate and do not
strongly converge as $t \to \infty$ in the framework of the critical Besov
spaces $\dot{B}_{p,q}^{n/p-1}(\mathbb{R}^n)$. Moreover, we find that the
situation is different in the two-dimensional case $n=2$ and show the forced
Navier--Stokes flow is asymptotically unstable in
$\dot{B}_{p,1}^{2/p-1}(\mathbb{R}^2)$ for all $1 \leq p \leq \infty$. Our
instability does not appear in the linear level but is caused by the nonlinear
interaction from external forces.

</details>


### [20] [On the symmetry group for systems of conservation laws](https://arxiv.org/abs/2509.21283)
*Michael Sever*

Main category: math.AP

TL;DR: The paper argues that quasi-linear conservation laws are mainly useful for Euler fluid flow models in higher dimensions, and attempts to extend this class using symmetry groups.


<details>
  <summary>Details</summary>
Motivation: To investigate whether quasi-linear conservation laws have broader physical applicability beyond Euler systems, particularly by leveraging symmetry groups to identify new viable models.

Method: The study employs symmetry group analysis to explore extensions of quasi-linear conservation law systems that could serve as physical models.

Result: The analysis supports the conjecture that Euler systems are the primary physically meaningful models in higher dimensions, with limited success in finding new viable systems via symmetry groups.

Conclusion: Quasi-linear conservation laws have restricted utility for physical modeling beyond Euler fluid flow systems, even when considering symmetry-based extensions.

Abstract: A case can be made that the utility of quasi-linear systems of conservation
laws as physical models is largely limited to Euler system models of fluid
flow, at least in higher dimensions. Qualified corroboration of this conjecture
is obtained here, by attempt to extend the class of systems attractive as
physical models via the associated symmetry group.

</details>


### [21] [Some shape functionals for the $k$-Hessian equation](https://arxiv.org/abs/2509.21313)
*Alba Lia Masiello,Francesco Salerno*

Main category: math.AP

TL;DR: The paper proves a Pólya type lower bound for k-Torsional Rigidity in any dimension and provides quantitative estimates to investigate optimal sets in the Pólya inequality.


<details>
  <summary>Details</summary>
Motivation: To study the Torsional Rigidity associated with the k-Hessian operator for non-empty, bounded, open, and convex sets of class C², and to establish lower bounds and investigate optimality conditions.

Method: Mathematical analysis using techniques from partial differential equations and convex geometry, specifically working with k-Hessian operators and proving Pólya type inequalities.

Result: A Pólya type lower bound for k-Torsional Rigidity is established in any dimension, along with two quantitative estimates that help characterize optimal sets in the inequality.

Conclusion: The paper successfully provides fundamental bounds for k-Torsional Rigidity and develops quantitative tools to analyze optimal geometric configurations in the context of Pólya type inequalities.

Abstract: For a non-empty, bounded, open, and convex set of class $C^2$, we consider
the Torsional Rigidity associated to the $k$-Hessian operator. We first prove
P\'olya type lower bound for the $k$-Torsional Rigidity in any dimension; then,
in order to investigate optimal sets in the P\'olya type inequality, we provide
two quantitative estimates.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [22] [MolCluster: Integrating Graph Neural Network with Community Detection for Coarse-Grained Mapping](https://arxiv.org/abs/2509.20893)
*Zhixuan Zhong,Linbo Ma,Jian Jiang*

Main category: physics.comp-ph

TL;DR: MolCluster is an unsupervised model that uses graph neural networks and community detection for automated coarse-grained molecular mapping with customizable resolution.


<details>
  <summary>Details</summary>
Motivation: Traditional CG methods rely on fixed mapping rules requiring manual intervention, while supervised learning approaches suffer from limited labeled data and inability to control mapping resolution.

Method: Integrates graph neural network with community detection algorithm, uses predefined group pair loss to preserve target groups, and implements bisection strategy for customizable resolution.

Result: Evaluations on MARTINI2 dataset show MolCluster outperforms both traditional clustering and supervised models due to its label-free pretraining strategy.

Conclusion: MolCluster demonstrates potential as a core model for customizable and chemically consistent coarse-grained mapping across diverse molecular systems.

Abstract: Coarse-grained (CG) modeling simplifies molecular systems by mapping groups
of atoms into representative units. However, traditional CG approaches rely on
fixed mapping rules, which limit their ability to handle diverse chemical
systems and require extensive manual intervention. Thus, supervised
learning-based CG methods have been proposed, enabling more automated and
adaptable mapping. Nevertheless, these methods suffer from limited labeled
datasets and the inability to control mapping resolution, which is essential
for multiscale modeling. To overcome these limitations, we propose MolCluster,
an unsupervised model that integrates a graph neural network and a community
detection algorithm to extract CG representations. Additionally, a predefined
group pair loss ensures the preservation of target groups, and a bisection
strategy enables precise, customizable resolution across different molecular
systems. In the case of the downstream task, evaluations on the MARTINI2
dataset demonstrate that MolCluster, benefiting from its label-free pretraining
strategy, outperforms both traditional clustering and supervised models.
Overall, these results highlight the potential of MolCluster as a core model
for customizable and chemically consistent CG mapping.

</details>


### [23] [A Reformulation of UVN-Flash for Multicomponent Two-Phase Systems with Application to CO2-rich Mixture Transport in Pipelines](https://arxiv.org/abs/2509.20965)
*Pardeep Kumar,Patricio I. Rosen Esquivel*

Main category: physics.comp-ph

TL;DR: A unified framework for two-phase multicomponent transport in CO2 pipelines using HEM with thermodynamic closure via Helmholtz energy EOS and novel UVN-flash routine.


<details>
  <summary>Details</summary>
Motivation: Accurate modeling of dense-phase CO2-rich mixtures in CCS pipelines requires coupling fluid dynamics and thermodynamics, especially during transient events like depressurization.

Method: Homogeneous equilibrium model (HEM) for two-phase transport with Helmholtz energy-based EOS thermodynamic closure, using UVN-flash with stability analysis and a novel tailored UVN-flash routine with better-scaled variables.

Result: The framework is successfully applied to depressurization of tanks and pipelines containing CO2-rich mixtures.

Conclusion: The proposed unified framework demonstrates effectiveness for CCS-relevant applications by integrating fluid dynamics and thermodynamics with improved phase-equilibrium calculations.

Abstract: Pipeline transport of dense-phase CO2-rich mixtures is a crucial component in
carbon capture and storage (CCS). Accurate modeling requires coupling of fluid
dynamics and thermodynamics, especially during transient events such as
depressurization. In this work, we present a unified framework for two-phase
multicomponent transport in pipelines that integrates both aspects.
Specifically, we employ the homogeneous equilibrium model (HEM) for modeling
the transport of two-phase CO2-rich mixture, with thermodynamic closure
provided by a Helmholtz energy-based equation of state. Phase equilibrium
calculations are performed using UVN-flash, supplemented with a stability
analysis procedure to detect phase separation and generate initial guesses for
the phase-equilibrium calculations. Specifically, we introduce a novel tailored
UVN-flash routine that aligns with the fluid dynamics formulation. This is
achieved by introducing an alternative and better-scaled set of variables for
the phase-equilibrium calculations. The proposed framework is applied to the
depressurization of tanks and pipelines containing CO2-rich mixtures,
demonstrating its effectiveness for CCS-relevant applications.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [24] [Anderson self-localization of light in pair plasmas](https://arxiv.org/abs/2509.20594)
*Maxim Lyutikov,Victor Gurarie*

Main category: physics.plasm-ph

TL;DR: Anderson self-localization of electromagnetic waves in pair plasma occurs due to random density fluctuations created by wave interactions, leading to wave reflection and trapping in bright pockets.


<details>
  <summary>Details</summary>
Motivation: To understand how weakly nonlinear electromagnetic waves behave in pair plasma and explore the phenomenon of Anderson self-localization, with potential applications to astrophysical Fast Radio Bursts.

Method: Analysis of wave propagation in quasi-1D pair plasma where the beat between driver and back-scattered waves creates random density fluctuations and dielectric permittivity variations, leading to wave localization.

Result: Electromagnetic waves experience Anderson self-localization, resulting in (i) reflection by under-dense pair plasma, and (ii) separation into bright trapped pockets and dark regions. Thermal spread can restore propagation by suppressing density fluctuations.

Conclusion: Anderson self-localization occurs in pair plasma for weakly nonlinear waves, creating linearly polarized structures with randomly varying position angles, potentially explaining features of astrophysical Fast Radio Bursts.

Abstract: We demonstrate that in pair plasma weakly nonlinear electromagnetic waves,
$a_0 \leq 1$, experience Anderson self-localization. The beat between the
driver and a back-scattered wave creates random, charge-neutral, large density
fluctuations $\delta \rho/\rho \gg 1$, and corresponding random fluctuations of
the dielectric permittivity $\epsilon$. Propagating in quasi-1D, waves in a
medium with spatially random, time-varying, self-created fluctuations of
dielectric permeability experience localization. Anderson self-localization of
light leads to (i) reflection of EM waves by the under-dense pair plasma; (ii)
a wave already present inside the plasma separates into bright trapped pockets
and dark regions. Mild initial thermal spread restores wave propagation by
suppressing the seeds of parametrically unstable density fluctuations. A
circularly polarized driver produces linearly polarized structures, with
position angle varying randomly between the bright pulses. We discuss possible
applications to astrophysical Fast Radio Bursts.

</details>


### [25] [Reply to "Comments to Marvel Fusions Mixed Fuels Reactor Concept"](https://arxiv.org/abs/2509.21048)
*Hartmut Ruhl,Georg Korn*

Main category: physics.plasm-ph

TL;DR: This paper refutes Lackner et al.'s conclusion that mixed fuel ignition is impossible by showing their α-stopping model is incomplete and neglecting ionic α-stopping and neutron stopping effects.


<details>
  <summary>Details</summary>
Motivation: To correct the flawed analysis by Lackner et al. regarding mixed fuel ignition, specifically addressing their incomplete α-stopping model and neglected physical effects.

Method: Extends the α-stopping model to include ionic α-stopping and incorporates neutron stopping effects, analyzing their combined impact on ion temperatures.

Result: Shows that when ionic α-stopping and neutron stopping are properly included, ion temperatures exceed electron temperatures (kT_i > kT_e), making mixed fuel ignition possible.

Conclusion: Contrary to Lackner et al.'s analysis, mixed fuel ignition is indeed possible when all relevant stopping mechanisms are properly accounted for, with significant implications.

Abstract: In "arXiv:2312.13429" Lackner et al. use standard methods to decide if it is
possible to ignite mixed fuels. They correctly identify that the increased
radiation losses make ignition significantly more challenging than for pure DT
fuels, since this leads to higher ignition temperatures. Further, they conclude
that at those temperatures the reduced electronic $\alpha$-stopping makes
ignition impossible. We show that this conclusion is not correct. The model
used for $\alpha$-stopping by Lackner et al. is only approximately correct for
low temperatures and hydrogen isotopes. By extending the $\alpha$-stopping
model to include ionic $\alpha$-stopping we show in \cite{ruhlkornarXiv5} that
the contribution of ionic $\alpha$-particle stopping cannot be neglected. The
ionic $\alpha$-stopping together with the neutron stopping, which is also
neglected by Lackner et al., lead to elevated ion temperatures implying $kT_i >
kT_e$. Those three effects combined lead us to the conclusion, that ignition of
mixed fuels is indeed possible with far reaching implications, contrary to the
analysis by Lackner.

</details>


<div id='physics.ins-det'></div>

# physics.ins-det [[Back]](#toc)

### [26] [Physics Informed Neural Networks for design optimisation of diamond particle detectors for charged particle fast-tracking at high luminosity hadron colliders](https://arxiv.org/abs/2509.21123)
*Alessandro Bombini,Alessandro Rosa,Clarissa Buti,Giovanni Passaleva,Lucio Anderlini*

Main category: physics.ins-det

TL;DR: This paper presents a novel modeling approach for 3D diamond pixel sensors, extending classical Ramo-Shockley theory to account for electrode resistivity effects that degrade timing performance in radiation-hard tracking detectors.


<details>
  <summary>Details</summary>
Motivation: Future high-luminosity hadron colliders require tracking detectors with extreme radiation tolerance, high spatial precision, and sub-nanosecond timing. 3D diamond pixel sensors offer these capabilities, but conductive electrodes produced via femtosecond laser pulses exhibit high resistivity that delays signal propagation.

Method: The authors extend the classical Ramo-Shockley weighting potential formalism by modeling the phenomenon through a 3rd-order, 3+1D PDE derived as a quasi-stationary approximation of Maxwell's equations. They solve this PDE numerically and couple it with charge transport simulations for realistic 3D sensor geometries. A Mixture-of-Experts Physics-Informed Neural Network, trained on Spectral Method data, provides a meshless solver.

Result: The developed framework enables assessment of timing degradation caused by electrode resistance in 3D diamond sensors.

Conclusion: The proposed modeling approach successfully addresses the signal propagation delays in 3D diamond pixel sensors, providing a crucial tool for optimizing detector performance in future high-luminosity collider applications.

Abstract: Future high-luminosity hadron colliders demand tracking detectors with
extreme radiation tolerance, high spatial precision, and sub-nanosecond timing.
3D diamond pixel sensors offer these capabilities due to diamond's radiation
hardness and high carrier mobility. Conductive electrodes, produced via
femtosecond IR laser pulses, exhibit high resistivity that delays signal
propagation. This effect necessitates extending the classical Ramo-Shockley
weighting potential formalism. We model the phenomenon through a 3rd-order,
3+1D PDE derived as a quasi-stationary approximation of Maxwell's equations.
The PDE is solved numerically and coupled with charge transport simulations for
realistic 3D sensor geometries. A Mixture-of-Experts Physics-Informed Neural
Network, trained on Spectral Method data, provides a meshless solver to assess
timing degradation from electrode resistance.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [27] [Relaxation to equilibrium of conservative dynamics II: non-gradient exclusion processes](https://arxiv.org/abs/2509.20797)
*Chenlin Gu,Linzhi Yang*

Main category: math.PR

TL;DR: The paper proves variance decay for the speed-change exclusion process on Z^d, extending previous gradient model results to non-gradient models using regularization and chaos expansion techniques.


<details>
  <summary>Details</summary>
Motivation: To extend the variance decay results from gradient models to non-gradient models in the speed-change exclusion process, building on previous work by Janvresse et al. and addressing limitations in existing theory.

Method: Combines regularization arguments from previous work with chaos expansion techniques by Bertini and Zegarlinski, incorporating new inputs from homogenization theory.

Result: Proves that the semigroup satisfies variance decay Var[P_t u] = C_u t^{-d/2} + o(t^{-(d+δ)/2}) for every local function u, with explicit characterization of constant C_u.

Conclusion: Successfully extends variance decay results to non-gradient speed-change exclusion processes, demonstrating the applicability of combined regularization and chaos expansion methods with homogenization theory.

Abstract: For the speed-change exclusion process on $\mathbb{Z}^d$ reversible with
respect to the product Bernoulli measure, we prove that its semigroup $P_t$
satisfies a variance decay $\operatorname{Var}[P_t u] = C_u t^{-\frac{d}{2}} +
o(t^{-\frac{d+\delta}{2}})$ for every local function $u$, with the constant
$C_u$ explicitly characterized. This extends the result of Janvresse, Landim,
Quastel and Yau in [Ann. Probab. 27(1) 325--360, 1999] to a non-gradient model.
The proof combines the regularization argument in the previous work, and the
chaos expansion in [Markov Process. Related Fields, 5(2) 125--162, 1999] by
Bertini and Zegarlinski, via a new input from the homogenization theory.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [28] [A relativistic coupled-cluster treatment of magnetic hyperfine structure of the $X^2Π$ and $A^2Σ^+$ states of OH isotopologues](https://arxiv.org/abs/2509.20522)
*D. P. Usov,Y. S. Kozhedub,A. V. Stolyarov,L. V. Skripnikov,V. M. Shabaev,I. I. Tupitsyn*

Main category: physics.chem-ph

TL;DR: Ab initio calculations of magnetic dipole hyperfine structure constants for OH radical isotopologues using relativistic coupled-cluster methods, showing excellent agreement with experimental data.


<details>
  <summary>Details</summary>
Motivation: To provide accurate theoretical predictions of hyperfine structure constants for hydroxyl radical isotopologues, extending beyond previous studies and enabling robust treatment of higher rovibrational levels.

Method: Four-component relativistic coupled-cluster method with excitations up to triple level (CCSD(T) and CCSDT), calculating HFS functions over internuclear distance range R ∈ [0.6, 1.8] Å for ground (X²Π) and excited (A²Σ⁺) electronic states.

Result: Most accurate theoretical HFS predictions to date, showing excellent agreement (within 1%) with semiempirical curves from high-resolution spectroscopy for lowest vibrational levels (v∈[0,2]) of X²Π state.

Conclusion: The extended internuclear distance range provides robust foundation for accurate HFS treatments of higher rovibrational levels in both adiabatic and non-adiabatic frameworks, with vibrationally averaged values consistent with experiment within 1%.

Abstract: $\textit{Ab initio}$ calculations of the parallel component of the magnetic
dipole hyperfine structure (HFS) constant have been carried out for hydroxyl
radical isotopologues ($^{16,17}$OH(D)) over the internuclear distance range $R
\in [0.6, 1.8]$ \r{A}. For the ground electronic state $X^2\Pi$, the HFS
functions were evaluated for contributions induced by both oxygen and hydrogen
nuclei. In addition, the hydrogen-induced HFS curve was calculated for the
excited $A^2\Sigma^+$ state. The quantum-chemistry study employs a
four-component relativistic coupled-cluster (CC) method, including excitations
up to the triple level, namely: the contribution of triple-cluster amplitudes
was studied both perturbatively (CCSD(T)) and through fully iterative
calculations (CCSDT). The resulting oxygen- and hydrogen-induced HFS functions
represent the most accurate and reliable theoretical predictions to date
exhibiting excellent agreement with semiempirical curve for hydrogen-induced
HFS derived from high-resolution spectroscopic data for the lowest vibrational
levels ($v\in [0,2]$) of the electronic $X^2\Pi$ state. Vibrationally averaged
$\textit{ab initio}$ values are consistent with experimental values within
$1\%$ for all states considered. Furthermore, the internuclear distance range
over which the HFS curves are defined has been extended beyond that of previous
studies, thereby providing a robust foundation for accurate HFS treatments of
higher-lying rovibrational levels of OH isotopologues within both adiabatic and
non-adiabatic frameworks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [29] [Explicit and Effectively Symmetric Schemes for Neural SDEs](https://arxiv.org/abs/2509.20599)
*Daniil Shmelev,Cristopher Salvi*

Main category: cs.LG

TL;DR: A novel class of stable, near-reversible Runge-Kutta schemes called Explicit and Effectively Symmetric (EES) schemes is introduced to address the limitations of existing neural SDE solvers, offering both memory efficiency and gradient accuracy without stability issues.


<details>
  <summary>Details</summary>
Motivation: Traditional approaches for backpropagation through neural SDE solvers face trade-offs: discretise-then-optimise has accurate gradients but high memory costs, while optimise-then-discretise has constant memory but suffers from gradient approximation errors and slow evaluation. Existing reversible solvers like Reversible Heun are unstable under complex models and large step sizes.

Method: The authors propose Explicit and Effectively Symmetric (EES) schemes, a new class of stable, near-reversible Runge-Kutta methods for neural SDEs. These schemes retain the benefits of reversible solvers while overcoming their instability issues.

Result: Numerical experiments demonstrate that EES schemes provide superior stability and reliability compared to existing methods, enabling memory-efficient training without severe restrictions on step size or model complexity.

Conclusion: EES schemes establish a practical foundation for scalable and accurate training of neural SDEs, offering both memory efficiency and gradient accuracy while maintaining stability under challenging conditions.

Abstract: Backpropagation through (neural) SDE solvers is traditionally approached in
two ways: discretise-then-optimise, which offers accurate gradients but incurs
prohibitive memory costs due to storing the full computational graph (even when
mitigated by checkpointing); and optimise-then-discretise, which achieves
constant memory cost by solving an auxiliary backward SDE, but suffers from
slower evaluation and gradient approximation errors. Algebraically reversible
solvers promise both memory efficiency and gradient accuracy, yet existing
methods such as the Reversible Heun scheme are often unstable under complex
models and large step sizes. We address these limitations by introducing a
novel class of stable, near-reversible Runge--Kutta schemes for neural SDEs.
These Explicit and Effectively Symmetric (EES) schemes retain the benefits of
reversible solvers while overcoming their instability, enabling
memory-efficient training without severe restrictions on step size or model
complexity. Through numerical experiments, we demonstrate the superior
stability and reliability of our schemes, establishing them as a practical
foundation for scalable and accurate training of neural SDEs.

</details>


### [30] [Latent Twins](https://arxiv.org/abs/2509.20615)
*Matthias Chung,Deepanshu Verma,Max Collins,Amit N. Subrahmanya,Varuni Katti Sastry,Vishwas Rao*

Main category: cs.LG

TL;DR: Latent Twins is a unifying mathematical framework that creates hidden surrogates in latent space for underlying equations, bridging representation learning and algorithmic solution methods. It provides compact, interpretable surrogates for solution operators that work across arbitrary time gaps.


<details>
  <summary>Details</summary>
Motivation: Scientific machine learning has advanced various fields but progressed in parallel, with representation learning and solution methods evolving separately. There's a need for a unifying framework that bridges these approaches.

Method: Latent Twins create a hidden surrogate in latent space for underlying equations, governed by operators. The framework establishes approximation properties for ODEs and PDEs, demonstrated through canonical ODEs, shallow-water PDEs, and real geopotential data.

Result: The framework provides compact, interpretable surrogates that evaluate across arbitrary time gaps in single-shot, remaining compatible with scientific pipelines like assimilation, control, and uncertainty quantification.

Conclusion: Latent Twins offer scalable, theory-grounded surrogates that bridge data-driven representation learning and classical scientific modeling across disciplines, unifying classical modeling, inversion, model reduction, and operator approximation under a single principle.

Abstract: Over the past decade, scientific machine learning has transformed the
development of mathematical and computational frameworks for analyzing,
modeling, and predicting complex systems. From inverse problems to numerical
PDEs, dynamical systems, and model reduction, these advances have pushed the
boundaries of what can be simulated. Yet they have often progressed in
parallel, with representation learning and algorithmic solution methods
evolving largely as separate pipelines. With \emph{Latent Twins}, we propose a
unifying mathematical framework that creates a hidden surrogate in latent space
for the underlying equations. Whereas digital twins mirror physical systems in
the digital world, Latent Twins mirror mathematical systems in a learned latent
space governed by operators. Through this lens, classical modeling, inversion,
model reduction, and operator approximation all emerge as special cases of a
single principle. We establish the fundamental approximation properties of
Latent Twins for both ODEs and PDEs and demonstrate the framework across three
representative settings: (i) canonical ODEs, capturing diverse dynamical
regimes; (ii) a PDE benchmark using the shallow-water equations, contrasting
Latent Twin simulations with DeepONet and forecasts with a 4D-Var baseline; and
(iii) a challenging real-data geopotential reanalysis dataset, reconstructing
and forecasting from sparse, noisy observations. Latent Twins provide a
compact, interpretable surrogate for solution operators that evaluate across
arbitrary time gaps in a single-shot, while remaining compatible with
scientific pipelines such as assimilation, control, and uncertainty
quantification. Looking forward, this framework offers scalable,
theory-grounded surrogates that bridge data-driven representation learning and
classical scientific modeling across disciplines.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [31] [Strain localization in reduced order asymptotic homogenization](https://arxiv.org/abs/2509.16210)
*Harpreet Singh,Puneet Mahajan*

Main category: cs.CE

TL;DR: A multiscale technique using reduced order asymptotic homogenization to model damage and inelastic effects in composite materials, incorporating continuum damage mechanics and addressing strain localization issues.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient computational method that can accurately capture damage and inelastic behavior in composite materials while reducing computational costs through order reduction techniques.

Method: Two-scale homogenization with eigen strain representation for inelastic response, using representative volume element (RVE) analysis to derive macroscale stress, and implementing continuum damage mechanics at microscale with crack band energy dissipation to address strain localization.

Result: The formulation successfully predicts macroscale response and captures damage and plasticity-induced inelastic strains, with verification studies demonstrating its effectiveness.

Conclusion: The proposed reduced order asymptotic homogenization technique provides an efficient and accurate framework for modeling damage and inelastic effects in composite materials, addressing key computational challenges like strain localization and artificial stiffness.

Abstract: A reduced order asymptotic homogenization based multiscale technique which
can capture damage and inelastic effects in composite materials is proposed.
This technique is based on two scale homogenization procedure where eigen
strain representation accounts for the inelastic response and the computational
efforts are alleviated by reduction of order technique. Macroscale stress is
derived by calculating the influence tensors from the analysis of
representative volume element (RVE). At microscale, the damage in the material
is modeled using continuum damage mechanics (CDM) based framework. To solve the
problem of strain localization a method of the alteration of stress-strain
relation of micro con- stituents based on the dissipated fracture energy in a
crack band is implemented. The issue of spurious post failure artificial
stiffness at macroscale is discussed and effect of increasing the order to
alleviate this problem is checked. Verification studies demonstrated the
proposed formulation predicts the macroscale response and also captures the
damage and plasticity induced inelastic strains.

</details>


### [32] [E$^2$-TFA based multiscale analysis of failure in elasto-plastic composites](https://arxiv.org/abs/2509.16211)
*Harpreet Singh*

Main category: cs.CE

TL;DR: A novel homogenization method called E²-TFA for analyzing failure of elastoplastic composites using elastic and eigen influence tensors to improve computational efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the drawback of TFA-based methods in post-damage stiffness response and provide more realistic computations for composite material failure analysis.

Method: Uses transformation field analysis with microscopic eigenstrain field accounting for intra-phase damage and inelastic strains, employing reduced order modeling with piecewise constant eigenstrain field throughout subdomains.

Result: The method accurately captures damage and inelastic deformations, validated through RVE simulations, complex load histories, and comparison with experimental fracture parameters for glass fiber composites.

Conclusion: E²-TFA can efficiently and accurately estimate mechanical response of composite materials by better capturing damage and inelastic deformations.

Abstract: This paper describes a novel homogenization methodology for analyzing the
failure of elastoplastic composite materials based on elastic and eigen
influence tensors-driven transformation field analysis ($\mathtt{E}^2$-TFA).
The proposed technique considers the microscopic eigenstrain field accounting
for intra-phase damage and inelastic strains. This results in realistic
computations by alleviating the post-damage stiffness response, which is a
drawback of TFA-based methods. We attain computational efficiency by
identifying the preprocessing data solely from the elastic and eigen
transformation functions and adopting a reduced order modelling technique with
a piecewise constant eigenstrain field throughout the subdomains. The
performance of the model is assessed by simulating the response for (a) the
representative volume element (RVE) as a homogenized continuum and (b) the
various composites under complex load histories with intricate macroscale
morphologies. Furthermore, the nonlinear shear stress-strain response of a
glass fiber composite is calculated and compared to experimentally measured
fracture initiation parameters, failure plane orientation, and strain
histories. Finally, we show that $\mathtt{E}^2$-TFA can accurately and
efficiently capture damage and inelastic deformations in order to estimate the
mechanical response of composite materials in a better way.

</details>


### [33] [Characterizing failure morphologies in fiber-reinforced composites via k-means clustering based multiscale framework](https://arxiv.org/abs/2509.20011)
*Harpreet Singh*

Main category: cs.CE

TL;DR: A novel homogenization methodology using damage informed transformation field analysis (D-TFA) for failure analysis of fiber-reinforced composites, featuring computational efficiency through reduced-order modeling and k-means clustering.


<details>
  <summary>Details</summary>
Motivation: To develop a more realistic and computationally efficient approach for simulating failure in fiber-reinforced composite materials by accurately capturing damage patterns and directional strengths.

Method: Uses elastic and eigen influence tensors within D-TFA framework, employs k-means clustering based on elastic and eigen strain distributions, and implements reduced-order modeling for computational efficiency. Performance assessed through RVE simulations and comparison with finite element method.

Result: D-TFA accurately captures damage patterns and directional strengths, with higher cluster counts providing more accurate stress-strain responses, especially for complex microstructures. Successfully predicts failure paths in open-hole specimen tests with different fiber layups.

Conclusion: The proposed D-TFA framework provides improved predictions of mechanical behavior in composite materials and demonstrates that higher cluster counts are essential for accurate stress-strain response in complex microstructures.

Abstract: A novel homogenization methodology is proposed for analyzing the failure of
fiber-reinforced composite materials, utilizing elastic and eigen influence
tensors within a damage informed transformation field analysis (D-TFA)
framework. This approach includes a technique for calculating macroscopic
damage under uniform stress and strain conditions, offering more realistic
simulations. Computational efficiency is enhanced through a reduced-order
modeling strategy, while elastic and eigen strain distribution driven k-means
clustering methods are employed to partition the microscale domain. The model's
performance is assessed by simulating the response of a representative volume
element (RVE) treated as a homogenized continuum. Subsequently, a comparative
assessment is carried out to check the efficacy of two clustering schemes.
Damage morphologies are calculated using proposed framework and compared with
predictions obtained using finite element method. Furthermore, open-hole
specimen tests are simulated and failure paths are predicted for the domains
with different fiber layups. Ultimately, we show that D-TFA can accurately
capture damage patterns and directional strengths, providing improved
predictions of the mechanical behavior of composite materials. It has been
demonstrated that higher cluster counts are crucial for capturing a more
accurate stress-strain response, especially for complex microstructures.

</details>


### [34] [Extrapolating Phase-Field Simulations in Space and Time with Purely Convolutional Architectures](https://arxiv.org/abs/2509.20770)
*Christophe Bonneville,Nathan Bieberdorf,Pieterjan Robbe,Mark Asta,Habib N. Najm,Laurent Capolungo,Cosmin Safta*

Main category: cs.CE

TL;DR: A U-Net surrogate model for liquid metal dealloying that generalizes beyond training data in space and time, achieving 16,000x speedup while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Phase-field models for liquid metal dealloying become computationally intractable for large domains or long time horizons, requiring more efficient simulation methods.

Method: Conditionally parameterized fully convolutional U-Net with convolutional self-attention and physics-aware padding, trained on short small-scale simulations but capable of temporal and spatial extrapolation.

Result: Surrogate achieves relative errors under 5% within training regime and below 10% when extrapolating, with 16,000x speedup (weeks to seconds).

Conclusion: This represents an early step toward scalable, high-fidelity extrapolation of LMD phase-field models, demonstrating significant computational efficiency gains.

Abstract: Phase-field models of liquid metal dealloying (LMD) can resolve rich
microstructural dynamics but become intractable for large domains or long time
horizons. We present a conditionally parameterized, fully convolutional U-Net
surrogate that generalizes far beyond its training window in both space and
time. The design integrates convolutional self-attention and physics-aware
padding, while parameter conditioning enables variable time-step skipping and
adaptation to diverse alloy systems. Although trained only on short,
small-scale simulations, the surrogate exploits the translational invariance of
convolutions to extend predictions to much longer horizons than traditional
solvers. It accurately reproduces key LMD physics, with relative errors
typically under 5% within the training regime and below 10% when extrapolating
to larger domains and later times. The method accelerates computations by up to
16,000 times, cutting weeks of simulation down to seconds, and marks an early
step toward scalable, high-fidelity extrapolation of LMD phase-field models.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [35] [Gaussian splatting holography](https://arxiv.org/abs/2509.20774)
*Shuhe Zhang,Liangcai Cao*

Main category: physics.optics

TL;DR: GSH uses Gaussian splatting to compress unknown parameters by 15x, transforming ill-posed phase retrieval into well-posed problem for twin-image-suppressed holographic reconstruction.


<details>
  <summary>Details</summary>
Motivation: In-line holography suffers from twin-image disruption due to Hermitian symmetry, causing phase ambiguities in ill-posed phase retrieval problems that current methods struggle to balance between data fidelity and twin-image disturbance.

Method: Gaussian splatting holography (GSH) uses Gaussian splatting for optical field representation, compressing unknown parameters by up to 15 folds to reduce phase ambiguities and form sharp patterns instead of noisy twin-image backgrounds.

Result: GSH achieves constraint-free recovery with accuracy comparable to state-of-the-art constraint-based methods (PSNR=26 dB, SSIM=0.8), and with total variation can reach PSNR=31 dB with 15x compression ability.

Conclusion: GSH effectively transforms ill-posed phase retrieval into well-posed problem, providing twin-image-suppressed holographic reconstruction with high compression capability and improved accuracy.

Abstract: In-line holography offers high space-bandwidth product imaging with a
simplified lens-free optical system. However, in-line holographic
reconstruction is troubled by twin images arising from the Hermitian symmetry
of complex fields. Twin images disrupt the reconstruction in solving the
ill-posed phase retrieval problem. The known parameters are less than the
unknown parameters, causing phase ambiguities. State-of-the-art deep-learning
or non-learning methods face challenges in balancing data fidelity with
twin-image disturbance. We propose the Gaussian splatting holography (GSH) for
twin-image-suppressed holographic reconstruction. GSH uses Gaussian splatting
for optical field representation and compresses the number of unknown
parameters by a maximum of 15 folds, transforming the original ill-posed phase
retrieval into a well-posed one with reduced phase ambiguities. Additionally,
the Gaussian splatting tends to form sharp patterns rather than those with
noisy twin-image backgrounds as each Gaussian has a spatially slow-varying
profile. Experiments show that GSH achieves constraint-free recovery for
in-line holography with accuracy comparable to state-of-the-art
constraint-based methods, with an average peak signal-to-noise ratio equal to
26 dB, and structure similarity equal to 0.8. Combined with total variation,
GSH can be further improved, obtaining a peak signal-to-noise ratio of 31 dB,
and a high compression ability of up to 15 folds.

</details>


### [36] [Fast 3D Nanophotonic Inverse Design using Volume Integral Equations](https://arxiv.org/abs/2509.20809)
*Amirhossein Fallah,Constantine Sideris*

Main category: physics.optics

TL;DR: The paper introduces a volume integral equation (VIE)-based forward modeling approach for nanophotonic inverse design, offering orders of magnitude improvement in computational efficiency over traditional finite-difference methods.


<details>
  <summary>Details</summary>
Motivation: Nanophotonic device design requires highly accelerated simulation methods due to the substantial electrical size and subwavelength characteristics of these structures, but conventional electromagnetic solvers are computationally expensive.

Method: Developed a VIE formulation as an efficient alternative to finite-difference methods, derived an adjoint method specifically for VIE framework, and created a novel unidirectional mode excitation strategy compatible with VIE solvers.

Result: Comparative benchmarks show multiple orders of magnitude improvement in computational efficiency over FD methods. Successfully designed a selective mode reflector and 3 dB power splitter to validate practical utility.

Conclusion: The VIE-based framework offers significant runtime advantages and shows promise for accelerating inverse design workflows in next-generation nanophotonic devices.

Abstract: Designing nanophotonic devices with minimal human intervention has gained
substantial attention due to the complexity and precision required in modern
optical technologies. While inverse design techniques typically rely on
conventional electromagnetic solvers as forward models within optimization
routines, the substantial electrical size and subwavelength characteristics of
nanophotonic structures necessitate significantly accelerated simulation
methods. In this work, we introduce a forward modeling approach based on the
volume integral equation (VIE) formulation as an efficient alternative to
traditional finite-difference (FD)-based methods. We derive the adjoint method
tailored specifically for the VIE framework to efficiently compute optimization
gradients and present a novel unidirectional mode excitation strategy
compatible with VIE solvers. Comparative benchmarks demonstrate that our
VIE-based approach provides multiple orders of magnitude improvement in
computational efficiency over conventional FD methods in both time and
frequency domains. To validate the practical utility of our approach, we
successfully designed two representative nanophotonic components: a selective
mode reflector and a 3 dB power splitter. Our results underscore the
significant runtime advantages offered by the VIE-based framework, highlighting
its promising role in accelerating inverse design workflows for next-generation
nanophotonic devices.

</details>


<div id='q-bio.TO'></div>

# q-bio.TO [[Back]](#toc)

### [37] [Data-driven Neural Networks for Windkessel Parameter Calibration](https://arxiv.org/abs/2509.21206)
*Benedikt Hoock,Tobias Köppl*

Main category: q-bio.TO

TL;DR: Proposes a neural network-based method for calibrating Windkessel parameters in 1D-0D blood flow models using simulated brachial artery pressure data, with extensions for handling unknown measurement locations and noisy data.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient and accurate method for calibrating Windkessel parameters in reduced-dimension blood flow models, particularly addressing challenges of unknown measurement locations and data noise.

Method: Uses a data-driven neural network trained on simulated blood pressures from the left brachial artery to emulate pressure pulse waves. Extends the network with dummy neurons for parameter calibration on measured pulse waves.

Result: The neural network successfully emulates pressure pulse waves across time, space, and varying Windkessel parameters with negligible error and low computational cost.

Conclusion: The proposed method effectively calibrates Windkessel parameters and demonstrates robustness in scenarios with unknown measurement locations and noisy data.

Abstract: In this work, we propose a novel method for calibrating Windkessel (WK)
parameters in a dimensionally reduced 1D-0D coupled blood flow model. To this
end, we design a data-driven neural network (NN)trained on simulated blood
pressures in the left brachial artery. Once trained, the NN emulates the
pressure pulse waves across the entire simulated domain, i.e., over time, space
and varying WK parameters, with negligible error and computational effort. To
calibrate the WK parameters on a measured pulse wave, the NN is extended by
dummy neurons and retrained only on these. The main objective of this work is
to assess the effectiveness of the method in various scenarios -- particularly,
when the exact measurement location is unknown or the data are affected by
noise.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [38] [Unbiased Parameter Estimation of Partially Observed Diffusions using Diffusion Bridges](https://arxiv.org/abs/2509.21015)
*Miguel Alvarez,Ajay Jasra*

Main category: stat.ME

TL;DR: A method for estimating static parameters of partially observed diffusion processes with discrete-time observations, eliminating time-discretization bias even when diffusion coefficient depends on parameters.


<details>
  <summary>Details</summary>
Motivation: To address parameter estimation for diffusion processes with discrete observations, overcoming limitations of existing methods that require diffusion coefficient to be independent of parameters and suffer from time-discretization bias.

Method: Leverages an identity related to the gradient of log-likelihood for diffusion bridges, enabling unbiased estimation without time-discretization bias. Allows parameter-dependent diffusion coefficients and facilitates more efficient Markov chain sampling algorithms.

Result: The estimator is proven to be unbiased with finite variance. Method efficacy is demonstrated through several examples.

Conclusion: The proposed approach provides an effective solution for unbiased parameter estimation in diffusion processes with discrete observations, overcoming key limitations of existing methods.

Abstract: In this article we consider the estimation of static parameters for partially
observed diffusion processes with discrete-time observations over a fixed time
interval. In particular, when one only has access to time-discretized solutions
of the diffusions we build upon the works of \cite{ub_par,ub_grad} to devise a
method that can estimate the parameters without time-discretization bias. We
leverage an identity associated to the gradient of the log-likelihood
associated to diffusion bridges, which has not been used before. Contrary to
the afore mentioned methods, the diffusion coefficient can depend on the
parameters and our approach facilitates the use of more efficient Markov chain
sampling algorithms. We prove that our estimator is unbiased with finite
variance and demonstrate the efficacy of our methodology in several examples.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [39] [Accelerating the Monte Carlo simulation of the Enskog equation for multiscale dense gas flows](https://arxiv.org/abs/2509.20816)
*Bin Hu,Liyan Luo,Lei Wu*

Main category: physics.flu-dyn

TL;DR: A general synthetic iterative scheme using Monte Carlo methods to solve the Enskog equation with rapid convergence and asymptotic-preserving properties for near-continuum flows.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient computational method that can handle near-continuum flows with spatial cell sizes larger than the mean free path, overcoming limitations of traditional Monte Carlo simulations.

Method: Mesoscopic-macroscopic two-way coupling: Monte Carlo simulation provides high-order constitutive relations to close the moment equation, while the macroscopic synthetic equation directs particle evolution in the Monte Carlo method.

Result: The method shows rapid convergence, reduces simulation time by several orders of magnitude in near-continuum flows, and successfully handles various flow problems including shock waves, heat transfer, Poiseuille flow, and porous media flow.

Conclusion: The proposed general synthetic iterative scheme is an effective and efficient approach for solving the Enskog equation, enabling large-scale simulations of near-continuum flows with applications to problems like shale gas extraction.

Abstract: A general synthetic iterative scheme is proposed to solve the Enskog equation
within a Monte Carlo framework. The method demonstrates rapid convergence by
reducing intermediate Monte Carlo evolution and preserves the
asymptotic-preserving property, enabling spatial cell sizes much larger than
the mean free path in near-continuum flows. This is realized through
mesoscopic-macroscopic two-way coupling: the mesoscopic Monte Carlo simulation
provides high-order constitutive relations to close the moment (synthetic)
equation, while the macroscopic synthetic equation, once solved toward steady
state, directs the evolution of simulation particles in the Monte Carlo method.
The accuracy of the proposed general synthetic iterative scheme is verified
through one-dimensional normal shock wave and planar Fourier heat transfer
problems, while its fast-converging and asymptotic-preserving properties are
demonstrated in the force-driven Poiseuille flow and two-dimensional hypersonic
cylinder flow and low-speed porous media flow, where the simulation time is
reduced by several orders of magnitude in near-continuum flows. With the
proposed method, a brief analysis is conducted on the role of the adsorption
layer in porous media flow, mimicking shale gas extraction.

</details>


### [40] [A Fourier/Modal-Spectral-Element Method for the Simulation of High-Reynolds Number Incompressible Stratified Flows in Domains with a Single Non-Periodic Direction](https://arxiv.org/abs/2509.20833)
*Nidia Reyes-Gil,Greg Thomsen,Kristopher Rowe,Peter Diamessis*

Main category: physics.flu-dyn

TL;DR: A high-order accurate Navier-Stokes solver for simulating high-Reynolds-number stratified flows, using Fourier pseudo-spectral and modal spectral element methods with efficient implicit-explicit time discretization.


<details>
  <summary>Details</summary>
Motivation: To address numerical challenges in simulating high-Reynolds-number stratified turbulent flows observed in oceanic and atmospheric environments, including thin shear regions and layered turbulence.

Method: Combines Fourier pseudo-spectral method horizontally with modal spectral element discretization vertically, using implicit-explicit time stepping that solves one-dimensional Helmholtz problems via static condensation and boundary-adapted basis functions.

Result: The solver demonstrates robustness through benchmark studies including 2D/3D problems and turbulent stratified wake simulations behind a sphere in linear stratification.

Conclusion: The proposed numerical model successfully facilitates reproduction of stratified turbulent fluid dynamics typical of geophysical flows with computational efficiency.

Abstract: We present the components of a high-order accurate Navier-Stokes solver
designed to simulate high-Reynolds-number stratified flows. The proposed
numerical model addresses some of the numerical and computational challenges
that high-Reynolds-number simulations pose, facilitating the reproduction of
stratified turbulent fluid dynamics typically observed in oceanic and
atmospheric flows, namely the development of thin regions of high vertical
shear, strongly layered turbulence at high Reynolds numbers and internal wave
radiation. This Navier-Stokes solver utilizes a Fourier pseudo-spectral method
in the horizontal direction and a modal spectral element discretization in the
vertical. We adopt an implicit-explicit time discretization scheme that
involves solving several one-dimensional Helmholtz problems at each time step.
Static condensation and modal boundary-adapted basis functions result in an
inexpensive algorithm based on solving many small tridiagonal systems. A series
of benchmark studies is presented to demonstrate the robustness of the flow
solver. These include two-dimensional and three-dimensional problems,
concluding with a turbulent stratified wake generated by a sphere in linear
stratification.

</details>
