<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 8]
- [math.AP](#math.AP) [Total: 13]
- [physics.comp-ph](#physics.comp-ph) [Total: 2]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 2]
- [cs.LG](#cs.LG) [Total: 2]
- [stat.ME](#stat.ME) [Total: 1]
- [physics.optics](#physics.optics) [Total: 2]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 2]
- [math.PR](#math.PR) [Total: 1]
- [cs.CE](#cs.CE) [Total: 4]
- [physics.ins-det](#physics.ins-det) [Total: 1]
- [q-bio.TO](#q-bio.TO) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [A robust framework for frictional fault contact in geological formations using a stabilized augmented Lagrangian approach](https://arxiv.org/abs/2509.20528)
*Matteo Frigo,Nicola Castelletto,Matteo Cusini,Randolph R. Settgast,Hamdi A. Tchelepi*

Main category: math.NA

TL;DR: This paper presents an augmented Lagrangian method using the Uzawa algorithm for modeling frictional contact behavior along fault surfaces in geological engineered systems, with a stabilized mixed finite element formulation that enriches displacement spaces with bubble functions.


<details>
  <summary>Details</summary>
Motivation: Accurate modeling of frictional contact behavior along fault surfaces is crucial for evaluating performance and safety of geological engineered systems like carbon storage sites, geothermal fields, and reservoirs, but this involves complex inequality constraints and highly nonlinear path-dependent behavior.

Method: Augmented Lagrangian method implemented via Uzawa algorithm with mixed finite element spaces combining low-order piecewise linear displacements in 3D domain cells with piecewise constant tractions on fault surfaces, enriched with face bubble functions for stability.

Result: The proposed approach provides advantages over other stabilization techniques by not requiring extra parameters and integrating naturally within the Uzawa framework.

Conclusion: The method offers a robust numerical approach for handling frictional slip problems in geological systems with improved stability and implementation simplicity compared to alternative stabilization methods.

Abstract: Numerical simulations are essential for evaluating the performance and safety
of geological engineered systems such as geologic carbon storage sites,
enhanced geothermal fields, and oil and gas reservoirs. A key challenge lies in
accurately modeling the frictional contact behavior along fault surfaces. This
problem involves inequality constraints that arise from the physics of
frictional slip, requiring specialized numerical methods to handle the
resulting highly nonlinear and path-dependent behavior. In this work, we
address this challenge using an augmented Lagrangian method implemented via the
Uzawa algorithm. The formulation employs mixed finite element spaces, combining
low-order piecewise linear displacements within the 3D domain cells with
piecewise constant tractions defined on the fault surfaces. Furthermore, to
ensure stability and satisfy the inf-sup condition, the discrete displacement
space is enriched with face bubble functions on both sides of the contact
interfaces. This approach offers several advantages over other stabilization
techniques that rely on additional terms, as it does not require extra
parameters for implementation, and it integrates naturally in the Uzawa
framework.

</details>


### [2] [Symplectic Isospectral Runge--Kutta Methods as Lie group methods](https://arxiv.org/abs/2509.20620)
*Paolo Cifani,Klas Modin,Cecilia Pagliantini,Milo Viviani*

Main category: math.NA

TL;DR: Comparison of three approaches for conservative time integration of isospectral flows on quadratic Lie algebras, showing that choosing an equivalent formulation with symplectic Runge-Kutta methods yields conservative and efficient schemes.


<details>
  <summary>Details</summary>
Motivation: To develop more efficient conservative integration methods for isospectral flows on quadratic Lie algebras, particularly for Hamiltonian systems.

Method: Compare three different approaches, select an equivalent formulation of isospectral flows, and apply symplectic Runge-Kutta methods to achieve conservative integration.

Result: The proposed scheme is both conservative and computationally more efficient than previously proposed methods, enabling arbitrarily high-order Lie-Poisson integrators for Hamiltonian systems.

Conclusion: The study demonstrates that careful formulation selection combined with symplectic Runge-Kutta methods can produce superior conservative integration schemes for isospectral flows on quadratic Lie algebras.

Abstract: In this paper, we compare three different approaches for a conservative
integration in time of isospectral flows on quadratic Lie algebras.
  We show that it is possible to choose an equivalent formulation of the
original isospectral flow such that, applying a symplectic Runge--Kutta method,
the resulting scheme is both conservative and computationally more efficient
than other schemes previously proposed.
  In particular, in the case of Hamiltonian systems, we get arbitrarily
high-order Lie--Poisson integrators on quadratic Lie algebras.

</details>


### [3] [A domain decomposition method for computing the scattering matrix of waveguide circuits](https://arxiv.org/abs/2509.20695)
*Tristan Goodwill,Shidong Jiang,Manas Rachh,Kosuke Sugita*

Main category: math.NA

TL;DR: A fast divide-and-conquer solver for time-harmonic wave scattering in metallic waveguides using impedance-to-impedance maps and second-kind Fredholm integral equations.


<details>
  <summary>Details</summary>
Motivation: To develop efficient numerical methods for wave scattering in infinite metallic waveguide structures, particularly when dealing with trapped modes and large-scale problems.

Method: Uses radiation boundary conditions with projectors onto outgoing modes, constructs solution operators as impedance-to-impedance maps via a divide-and-conquer approach, and employs second-kind Fredholm integral equations with weakly singular kernels.

Result: The proposed method achieves substantial efficiency gains, outperforming state-of-the-art fast iterative and direct solvers by one to two orders of magnitude in numerical experiments on large structures.

Conclusion: The approach provides an efficient framework for solving wave scattering problems in complex waveguide structures, handling trapped modes effectively while maintaining computational efficiency.

Abstract: We analyze and develop numerical methods for time-harmonic wave scattering in
metallic waveguide structures of infinite extent. We show that radiation
boundary conditions formulated via projectors onto outgoing modes determine the
coefficients of propagating modes uniquely, even when the structure supports
trapped modes. Building on this, we introduce a fast divide-and-conquer solver
that constructs solution operators on subdomains as impedance-to-impedance maps
and couples them by enforcing continuity conditions across their interfaces.
For Dirichlet waveguides, the computation of impedance-to-impedance maps
requires the solution of mixed Dirichlet-Impedance boundary value problems. We
construct a second-kind Fredholm integral equation that avoids
near-hypersingular operators, requiring only integral operators whose kernels
are at most weakly singular. Numerical experiments on large structures with
many circuit elements demonstrate substantial efficiency gains: the proposed
approach typically outperforms state-of-the-art fast iterative and fast direct
solvers by one to two orders of magnitude.

</details>


### [4] [A Convergent Structure-Preserving Scheme for Dissipative Solutions of the Rotating Shallow Water System](https://arxiv.org/abs/2509.20764)
*K. R. Arun,A. Krishnamurthy*

Main category: math.NA

TL;DR: A semi-implicit finite volume scheme for 2D rotating shallow water equations that is energy stable, well-balanced, consistent, and convergent through carefully designed stabilization terms.


<details>
  <summary>Details</summary>
Motivation: To develop a robust numerical scheme for rotating shallow water equations that can preserve key physical properties like energy stability and geostrophic steady states while maintaining numerical convergence.

Method: Introduces carefully chosen stabilization terms into convective fluxes and source terms, with specific CFL conditions for stability. The scheme is semi-implicit and uses finite volume discretization.

Result: The scheme achieves energy stability, well-balancing, consistency, and convergence. Numerical experiments confirm theoretical results, showing the scheme generates dissipative measure-valued solutions.

Conclusion: The proposed scheme successfully addresses multiple numerical challenges simultaneously - stability, well-balancing, consistency, and convergence - making it a reliable method for rotating shallow water simulations.

Abstract: We design and analyse a semi-implicit finite volume scheme for the
two-dimensional rotating shallow water (RSW) equations that is energy stable,
well-balanced (capable of preserving discrete geostrophic steady states),
consistent, and covergent. The key idea is the introduction of carefully chosen
stabilisation terms into the convective fluxes of the mass and momentum
equations, as well as the source terms. Under a CFL-type condition, together
with an auxiliary time-step restriction arising from the Coriolis forces, we
establish the energy stability of the scheme. The stabilisation terms are
constructed to vanish at steady states, thereby ensuring the well-balancing
property under an appropriate advective CFL condition. We derive a sufficient
time-step restriction that guarantees stability, well-balancing, existence of
discrete solutions, and positivity simultaneously. Furthermore, under mild
boundedness assumptions, we obtain a priori estimates showing that the
stabilisation terms converge to zero as the mesh is refined, which establishes
the consistency of the scheme. This in turn enables us to prove that numerical
solutions generate a Young measure, identifiable as a dissipative
measure-valued solution of the RSW system, thereby yielding convergence of the
scheme. Finally, we confirm the theoretical results through extensive numerical
experiments.

</details>


### [5] [Higher-Order Root-Finding Algorithm and its Applications](https://arxiv.org/abs/2509.20897)
*Wei Guo Foo,Chik How Tan*

Main category: math.NA

TL;DR: A higher-order root-finding method using only Taylor expansion is proposed as an alternative to Householder's method, with lower computational complexity and explicit convergence factor.


<details>
  <summary>Details</summary>
Motivation: Householder's method requires higher-order derivatives which lead to long symbolic computations and error accumulation in numerical differentiation. Existing convergence factors are rough estimates.

Method: The proposed method uses only Taylor expansion of a function, avoiding higher-order derivatives. It can numerically implement Householder's method with lower computational complexity.

Result: The method is applied to compute pre-images of q-ary entropy functions in coding theory. Basins of attraction are studied and compared with other root-finding methods.

Conclusion: The proposed Taylor expansion-based method provides an efficient alternative to Householder's method with explicit convergence factors and practical applications in coding theory.

Abstract: Root-finding method is an iterative process that constructs a sequence
converging to a solution of an equation. Householder's method is a higher-order
method that requires higher order derivatives of the reciprocal of a function
and has disadvantages. Firstly, symbolic computations can take a long time, and
numerical methods to differentiate a function can accumulate errors. Secondly,
the convergence factor existing in the literature is a rough estimate. In this
paper, we propose a higher-order root-finding method using only Taylor
expansion of a function. It has lower computational complexity with explicit
convergence factor, and can be used to numerically implement Householder's
method. As an application, we apply the proposed method to compute pre-images
of $q$-ary entropy functions, commonly seen in coding theory. Finally, we study
basins of attraction using the proposed method and compare them with other
root-finding methods.

</details>


### [6] [Multiprecision computations with Schwarz methods](https://arxiv.org/abs/2509.20937)
*Michal Outrata,Daniel B. Szyld*

Main category: math.NA

TL;DR: Analysis of using lower precision arithmetic in Schwarz methods and preconditioners for local problem solves, showing single precision suffices for model problems.


<details>
  <summary>Details</summary>
Motivation: To reduce computational costs by using lower precision arithmetic for local solves in Schwarz methods while maintaining overall solution accuracy.

Method: Using multiprecision arithmetic with local problems solved at lower precision (single precision) while main computation remains in double precision, with specific round-off criteria.

Result: Experimental findings show that about 5 digits of accuracy are sufficient to achieve theoretical restrictions, making single precision adequate for local solves in model problems.

Conclusion: Single precision arithmetic can effectively replace double precision for local problem solves in Schwarz methods without compromising solution quality, offering computational efficiency gains.

Abstract: We explore and analyze the use of multiprecision arithmetic for several
classes of Schwarz methods and preconditioners, where the approximate solution
of the local problems is performed at a lower precision, i.e., with fewer
digits of accuracy than in the underlying (double precision) computation.
Conditions for the appropriate round-off criteria for the lower precision are
presented. It is found experimentally that for the model problems about 5
digits of accuracy are sufficient to achieve the theoretical restrictions, and
thus, single precision suffices for the local solves. Several numerical
experiments illustrate the obtained results.

</details>


### [7] [Model reduction of parametric ordinary differential equations via autoencoders: structure-preserving latent dynamics and convergence analysis](https://arxiv.org/abs/2509.21280)
*Enrico Ballini,Marco Gambarini,Alessio Fumagalli,Luca Formaggia,Anna Scotti,Paolo Zunino*

Main category: math.NA

TL;DR: A reduced-order modeling approach using autoencoders for nonlinear parameter-dependent ODEs, achieving dimensionality reduction while maintaining accuracy through neural network-based nonlinear maps.


<details>
  <summary>Details</summary>
Motivation: To accelerate complex dynamical simulations by reducing the dimensionality of nonlinear parameter-dependent ODEs without sacrificing accuracy, enabling faster computations while preserving solution quality.

Method: Using autoencoders (neural networks) to create nonlinear maps for dimensionality reduction, solving the resulting low-dimensional ODE with standard time integration schemes, and reconstructing the high-dimensional solution from the reduced representation.

Result: Numerical experiments demonstrate robustness and accuracy, showing convergence of the reduced-order model to the high-fidelity model, with investigation into how dimensionality reduction affects solution stability properties.

Conclusion: The approach shows potential for accelerating complex dynamical simulations while maintaining accuracy, with successful application to nonlinear parameter-dependent ODEs using autoencoder-based dimensionality reduction.

Abstract: We propose a reduced-order modeling approach for nonlinear,
parameter-dependent ordinary differential equations (ODE). Dimensionality
reduction is achieved using nonlinear maps represented by autoencoders. The
resulting low-dimensional ODE is then solved using standard integration in time
schemes, and the high-dimensional solution is reconstructed from the
low-dimensional one. We investigate the applicability of neural networks for
constructing effective autoencoders with the property of reconstructing the
input manifold with null representation error. We study the convergence of the
reduced-order model to the high-fidelity one. Numerical experiments show the
robustness and accuracy of our approach, highlighting its potential to
accelerate complex dynamical simulations without sacrificing accuracy.
Moreover, we examine how the reduction influences the stability properties of
the reconstructed high-dimensional solution.

</details>


### [8] [Two ADI compact difference methods for variable-exponent diffusion wave equations](https://arxiv.org/abs/2509.21316)
*Hao Zhang,Kexin Li,Wenlin Qiu*

Main category: math.NA

TL;DR: This paper develops two efficient numerical schemes for solving 2D diffusion-wave equations with variable exponents, using ADI compact finite difference methods with proven unconditional stability and high-order accuracy.


<details>
  <summary>Details</summary>
Motivation: To model mechanical diffusive wave propagation in viscoelastic media with spatially varying properties, requiring efficient numerical methods for complex diffusion-wave equations with variable exponents.

Method: Transformed the diffusion-wave model via convolution method, applied two time discretization strategies with spatial compact finite difference method, and employed ADI technique to reduce computational cost.

Result: Both ADI compact schemes are unconditionally stable and convergent. First scheme achieves α(0)-order temporal accuracy and fourth-order spatial accuracy; second scheme attains second-order temporal and fourth-order spatial accuracy.

Conclusion: Numerical experiments confirm theoretical error estimates and demonstrate the efficiency of the proposed methods for solving complex diffusion-wave equations with variable exponents.

Abstract: In this work, we study two-dimensional diffusion-wave equations with variable
exponent, modeling mechanical diffusive wave propagation in viscoelastic media
with spatially varying properties. We first transform the diffusion-wave model
into an equivalent form via the convolution method. Two time discretization
strategies are then applied to approximate each term in the transformed
equation, yielding two fully discrete schemes based on a spatial compact finite
difference method. To reduce computational cost, the alternating direction
implicit (ADI) technique is employed. We prove that both ADI compact schemes
are unconditionally stable and convergent. Under solution regularity, the first
scheme achieves $\alpha(0)$-order accuracy in time and fourth-order accuracy in
space, while the second scheme attains second-order accuracy in time and
fourth-order accuracy in space. Numerical experiments confirm the theoretical
error estimates and demonstrate the efficiency of the proposed methods.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [9] [Increased lifespan for 3D compressible Euler flows with rotation](https://arxiv.org/abs/2509.20505)
*Haram Ko,Benoit Pausader,Ryo Takada,Klaus Widmayer*

Main category: math.AP

TL;DR: This paper analyzes the compressible Euler equation with Coriolis term, proving lower bounds on solution existence time based on rotation speed, sound speed, and initial data size, with applications to incompressible Euler-Coriolis systems.


<details>
  <summary>Details</summary>
Motivation: To establish rigorous mathematical bounds on the existence time of solutions for compressible Euler equations with rotation effects, which has implications for geophysical fluid dynamics and atmospheric modeling.

Method: The authors prove lower bounds on solution existence time by analyzing the compressible Euler equation with Coriolis term, obtaining precise dispersive decay estimates for the linearized equation, and examining the incompressible limit.

Result: The paper provides improved bounds for the time of existence of solutions in terms of rotation speed, sound speed, and initial data size, with enhanced results for the incompressible Euler-Coriolis system.

Conclusion: The analysis yields significant improvements in understanding solution behavior for rotating compressible fluids, with direct applications to geophysical flow modeling and better bounds for incompressible rotating systems.

Abstract: We consider the compressible Euler equation with a Coriolis term and prove a
lower bound on the time of existence of solutions in terms of the speed of
rotation, sound speed and size of the initial data. Along the way, we obtain
precise dispersive decay estimates for the linearized equation. In the
incompressible limit, this improves current bounds for the incompressible
Euler-Coriolis system as well.

</details>


### [10] [On the Landis Conjecture for Positive Quasi-linear Operators on Graphs](https://arxiv.org/abs/2509.20559)
*Ujjal Das,Matthias Keller,Yehuda Pinchover*

Main category: math.AP

TL;DR: This paper proves a Landis-type unique continuation result for positive quasi-linear operators on graphs, showing that harmonic functions for certain operators must be zero under decay conditions.


<details>
  <summary>Details</summary>
Motivation: To establish unique continuation properties for harmonic functions associated with positive quasilinear Schrödinger operators on graphs, leveraging criticality theory and simplified energy concepts.

Method: The authors use criticality theory, particularly the Liouville comparison theorem, and build their results on the concept of simplified energy to analyze harmonic functions on graphs.

Result: They provide decay criteria that ensure harmonic functions for positive quasilinear Schrödinger operators with potential less than 1 are trivially zero, with applications to model graphs and regular trees.

Conclusion: The paper successfully establishes Landis-type unique continuation results for positive quasi-linear operators on graphs, demonstrating the utility of criticality theory and simplified energy approaches in this context.

Abstract: We prove a Landis type unique continuation result for positive quasi-linear
operators on graphs. Specifically, we give decay criteria that ensures when a
harmonic function for a positive quasilinear Schr\"odinger operator with
potential less than 1 is trivially zero. The assumption of positivity of the
operator allows the application of criticality theory such as the Liouville
comparison theorem. Furthermore, our results fundamentally build on the so
called simplified energy. As an application we discuss the case of model graphs
and in particular regular trees.

</details>


### [11] [Extended Sobolev Scale on Non-Compact Manifolds](https://arxiv.org/abs/2509.20598)
*Ognjen Milatovic*

Main category: math.AP

TL;DR: This paper extends the 'extended Sobolev scale' concept from compact manifolds to manifolds of bounded geometry, develops interpolation properties, and establishes mapping properties for pseudo-differential operators within this framework.


<details>
  <summary>Details</summary>
Motivation: To generalize the extended Sobolev scale theory from compact manifolds and Euclidean spaces to non-compact manifolds of bounded geometry, enabling broader applications in analysis and PDE theory.

Method: Adapting Mikhailets and Murach's definition of extended Sobolev scale to manifolds of bounded geometry, using interpolation theory between Sobolev spaces, and analyzing properties of proper uniform pseudo-differential operators (PUPDOs).

Result: Successfully defined the extended Sobolev scale H^φ(X) on manifolds of bounded geometry, obtained complete characterization of interpolation spaces, established mapping properties of PUPDOs, and showed equivalence between extended Sobolev scale and extended A-scale defined via elliptic operators.

Conclusion: The extended Sobolev scale theory can be effectively generalized to manifolds of bounded geometry, preserving key properties and enabling new applications in the analysis of pseudo-differential operators on non-compact manifolds.

Abstract: Adapting the definition of ``extended Sobolev scale" on compact manifolds by
Mikhailets and Murach to the setting of a (generally non-compact) manifold of
bounded geometry $X$, we define the ``extended Sobolev scale" $H^{\varphi}(X)$,
where $\varphi$ is a function which is $RO$-varying at infinity. With the help
of the scale $H^{\varphi}(X)$, we obtain a description of all Hilbert
function-spaces that serve as interpolation spaces with respect to a pair of
Sobolev spaces $[H^{(s_0)}(X), H^{(s_1)}(X)]$, with $s_0<s_1$. We use this
interpolation property to establish a mapping property of proper uniform
pseudo-differential operators (PUPDOs) in the context of the scale
$H^{\varphi}(X)$. Additionally, using a first-order positive-definite PUPDO $A$
of elliptic type we define the ``extended $A$-scale" $H^{\varphi}_{A}(X)$ and
show that it coincides, up to norm equivalence, with the scale
$H^{\varphi}(X)$. Besides the mentioned results, we show that further
properties of the $H^{\varphi}$-scale, originally established by Mikhailets and
Murach on $\mathbb{R}^n$ and on compact manifolds, carry over to manifolds of
bounded geometry.

</details>


### [12] [State-Constrained Chemical Reactions: Discrete-to-Continuous Hamilton--Jacobi Equations and Large Deviations](https://arxiv.org/abs/2509.20747)
*Yuan Gao,Yuxi Han*

Main category: math.AP

TL;DR: This paper establishes the large deviation principle for chemical reactions modeled as random time-changed Poisson processes in bounded domains, showing convergence from discrete to continuous Hamilton-Jacobi equations with Neumann boundary conditions.


<details>
  <summary>Details</summary>
Motivation: To understand the macroscopic behavior of chemical reactions in bounded domains and establish rigorous mathematical foundations for large deviation principles in state-constrained systems.

Method: Using WKB reformulation and discrete Hamilton-Jacobi equations with state constraints, analyzing convergence as grid size tends to zero, and establishing variational representations.

Result: Proved that under suitable reparametrization, discrete Hamilton-Jacobi solutions converge to continuous Hamilton-Jacobi equations with Neumann boundary conditions, enabling establishment of large deviation principles.

Conclusion: The work provides a rigorous mathematical framework connecting discrete stochastic chemical reaction models to continuous large deviation principles in bounded domains with state constraints.

Abstract: We study the macroscopic behavior of chemical reactions modeled as random
time-changed Poisson processes on discrete state spaces. Using the WKB
reformulation, the backward equation of the rescaled process leads to a
discrete Hamilton--Jacobi equation with state constraints. As the grid size
tends to zero, the limiting solution and its associated variational
representation are closely connected to the good rate function of the large
deviation principle for state-constrained chemical reactions in the
thermodynamic limit. In this work, we focus on the limiting behavior of
discrete Hamilton--Jacobi equations defined on bounded domains with
state-constraint boundary conditions. For a single chemical reaction, we show
that, under a suitable reparametrization, the solution of the discrete
Hamilton--Jacobi equation converges to the solution of a continuous
Hamilton--Jacobi equation with a Neumann boundary condition. Building on this
convergence result and the associated variational representation, we establish
the large deviation principle for the rescaled chemical reaction process in
bounded domains.

</details>


### [13] [MIT bag in non-smooth convex domains](https://arxiv.org/abs/2509.20958)
*Konstantin Pankrashkin*

Main category: math.AP

TL;DR: The paper proves that the Dirac operator with MIT bag boundary conditions is self-adjoint in convex domains, extending previous smooth-domain results to the convex case.


<details>
  <summary>Details</summary>
Motivation: Previous results on Dirac operators with MIT bag boundary conditions were limited to smooth domains. The authors aim to extend these results to more general convex domains, which are important in mathematical physics applications.

Method: The authors work in the H^1-setting and show that the Dirac operator with MIT bag boundary condition is self-adjoint for bounded convex domains. They also demonstrate that this operator appears as a limit of Dirac operators with large positive mass outside the domain.

Result: The main result establishes that the Dirac operator with MIT bag boundary condition is self-adjoint in bounded convex domains, generalizing previous smooth-domain results. Additionally, it shows this operator can be obtained as a limit case of Dirac operators with large exterior mass.

Conclusion: The paper successfully extends the theory of Dirac operators with MIT bag boundary conditions from smooth domains to convex domains, providing a more general framework for applications in mathematical physics where convex domains naturally arise.

Abstract: The Dirac operator with MIT bag boundary condition in a bounded convex domain
is shown to be always self-adjoint in the $H^1$-setting. This allows one to
show that such operators appear as limit of Dirac operators with large positive
mass outside the domain. Similar results were previously known for smooth
domains only.

</details>


### [14] [A nonlocal Aw-Rascle-Zhang system with linear pressure term](https://arxiv.org/abs/2509.20973)
*Debora Amadori,Felisia Angela Chiarello,Gianmarco Cipollone*

Main category: math.AP

TL;DR: This paper studies a nonlocal extension of the Aw-Rascle-Zhang traffic model using convolution-based pressure terms to capture driver interactions, and proves solution existence and convergence via sticky particle approximation.


<details>
  <summary>Details</summary>
Motivation: To extend traffic flow modeling by incorporating nonlocal driver interactions through convolution-based pressure terms, aligning with Euler-alignment systems for more realistic traffic behavior representation.

Method: Uses sticky particle approximation to construct entropy solutions for cumulative density, proving convergence to weak solutions of the nonlocal system.

Result: Establishes well-posedness, stability estimates, and an entropic selection principle for the nonlocal traffic model.

Conclusion: The nonlocal extension provides a mathematically rigorous framework for traffic modeling with nonlocal interactions, with proven solution properties and convergence guarantees.

Abstract: In this paper, we study a nonlocal extension of the Aw-Rascle-Zhang traffic
model, where the pressure-like term is modeled as a convolution between vehicle
density and a kernel function. This formulation captures nonlocal driver
interactions and aligns structurally with the Euler-alignment system studied in
[23]. Using a sticky particle approximation, we construct entropy solutions to
the equation for the cumulative density and prove convergence of approximate
solutions to weak solutions of the nonlocal system. The analysis includes
well-posedness, stability estimates, and an entropic selection principle.

</details>


### [15] [Graphical Willmore Problems with Low-Regularity Boundary and Dirichlet Data](https://arxiv.org/abs/2509.21018)
*Boris Gulyak*

Main category: math.AP

TL;DR: Existence and regularity results for boundary value problems of Willmore energy variation in graphical setting, with reduced regularity requirements and extensions to Lipschitz boundaries.


<details>
  <summary>Details</summary>
Motivation: To establish existence and regularity for Willmore energy variation problems with clamped boundary conditions on surfaces represented as graphs, particularly for domains with non-smooth boundaries.

Method: Construct solutions through linearization and fixed-point argument using weighted second-order Sobolev spaces, rewriting Willmore equation in divergence form based on Koch and Lamm's approach.

Result: Successfully weakened regularity assumptions to C¹⁺α-class for boundaries and Dirichlet data while maintaining interior smoothness; extended existence theory to Lipschitz boundaries using weighted Sobolev framework.

Conclusion: The approach provides robust existence and regularity results for Willmore energy problems with minimal regularity requirements and is applicable to other higher-order geometric PDEs like Helfrich and surface diffusion equations.

Abstract: We establish existence and regularity results for boundary value problems
arising from the first variation of the Willmore energy in the graphical
setting. Our focus lies on two-dimensional surfaces with fixed clamped boundary
conditions, embedded in three-dimensional Euclidean space, and represented as
graphs of height functions over domains with non-smooth boundaries. Our
approach involves constructing solutions through linearization and a
fixed-point argument, requiring small boundary data in suitable functional
spaces. Building on the results of Koch and Lamm \cite{koch2012geometric}, we
rewrite the Willmore equation for graphs in a divergence form that allows the
application of weighted second-order Sobolev spaces. This reformulation
significantly weakens the regularity assumptions on both the boundary and the
Dirichlet data, reducing them to the $C^{1+\alpha}$-class, while the solution
remains smooth in the interior. Moreover, we extend the existence theory to
domains with merely Lipschitz boundaries within a purely weighted Sobolev
framework. Our approach is also applicable to other higher-order geometric
PDEs, including the graphical Helfrich and surface diffusion equations.

</details>


### [16] [The Incompressible Navier-Stokes-Fourier Limits from Boltzmann-Fermi-Dirac Equation for Low Regularity Data](https://arxiv.org/abs/2509.21030)
*Ning Jiang,Chenchen Wang,Kai Zhou*

Main category: math.AP

TL;DR: This paper establishes the hydrodynamic limits of the quantum Boltzmann equation with Fermi-Dirac statistics, rigorously verifying the incompressible Navier-Stokes-Fourier limits with lower regularity initial data than previous work.


<details>
  <summary>Details</summary>
Motivation: To extend previous results on hydrodynamic limits of quantum Boltzmann equations by working with lower regularity initial data and establishing the lifespan equivalence between kinetic and fluid solutions.

Method: Analyzing the spectrum of the linearized collision operator combined with the transport operator and its associated semigroup, using fixed-point arguments with time iteration.

Result: Successfully verified the incompressible Navier-Stokes-Fourier limits from the BFD equation with lower regularity initial data, and obtained that the lifespan of kinetic solution coincides with those of limiting fluid solution.

Conclusion: The paper provides a rigorous mathematical foundation for hydrodynamic limits of quantum Boltzmann equations with improved regularity conditions and lifespan analysis.

Abstract: We consider the hydrodynamic limits of the quantum Boltzmann equation with
Fermi-Dirac statistics for hard sphere and hard potentials in the whole space.
By analyzing the spectrum of the linearized collision operator combined with
the transport operator and its associated semigroup, the incompressible
Navier-Stokes-Fourier limits from the BFD equation is verified rigorously.
Compared to the results in [Jiang-Xiong-Zhou,J. Differ. Equ.,2022], this paper
works with a lower regularity for the initial data. In addition, the
fixed-point arguments together with a time iteration ensure us to obtain the
lifespan of kinetic solution coincides with those of limiting fluid solution.

</details>


### [17] [On the radius of spatial analyticity for the Majda-Biello and Hirota-Satsuma systems](https://arxiv.org/abs/2509.21095)
*Seongyeon Kim,Ihyeok Seo*

Main category: math.AP

TL;DR: First proof of analyticity persistence for Majda-Biello and Hirota-Satsuma coupled KdV systems with analytic initial data


<details>
  <summary>Details</summary>
Motivation: To establish whether spatial analyticity persists over time for solutions to coupled KdV systems, which has not been previously proven for these specific systems

Method: Mathematical analysis of the persistence of spatial analyticity for solutions to the Majda-Biello and Hirota-Satsuma systems

Result: Successfully proved that analyticity persists for these coupled KdV systems when starting with analytic initial data

Conclusion: This represents the first established result on analyticity persistence in such coupled KdV systems, providing new mathematical understanding of their behavior

Abstract: We investigate the persistence of spatial analyticity for solutions to the
Majda-Biello and Hirota-Satsuma systems with analytic initial data. This result
is the first to establish analyticity persistence in such coupled KdV systems.

</details>


### [18] [Lagrangian aspects of Yudovich theory for 2D Euler](https://arxiv.org/abs/2509.21121)
*Theodore D. Drivas,Joonhyun La*

Main category: math.AP

TL;DR: Yudovich's existence and uniqueness theorem for bounded and mildly unbounded vorticity weak solutions of 2D incompressible Euler equations


<details>
  <summary>Details</summary>
Motivation: To establish rigorous mathematical foundations for the existence and uniqueness of weak solutions to the 2D Euler equations with bounded vorticity

Method: Mathematical analysis and proof techniques establishing Yudovich's classical result, with additional regularity analysis of the solution map

Result: Successfully proved existence and uniqueness for bounded and mildly unbounded vorticity weak solutions, plus regularity results for the solution map's dependence on initial conditions and domain

Conclusion: The paper provides a comprehensive proof of Yudovich's theorem with additional insights into the regularity properties of the solution mapping

Abstract: In this note, we establish Yudovich's existence and uniqueness result for
bounded (as well as mildly unbounded) vorticity weak solution of the
two-dimensional incompressible Euler equations. As a biproduct of our proof, we
establish some regularity results for the Yudovich solution map as it depends
of the initial conditions and the fluid domain.

</details>


### [19] [Asymptotic instability for the forced Navier--Stokes equations in critical Besov spaces](https://arxiv.org/abs/2509.21272)
*Mikihiro Fujii,Hiroyuki Tsurumi*

Main category: math.AP

TL;DR: This paper demonstrates asymptotic instability in forced Navier-Stokes flows in critical Besov spaces for dimensions n≥3 when p≥n, and for all p in 2D case, showing that small decaying external forces can cause oscillatory behavior rather than convergence to zero.


<details>
  <summary>Details</summary>
Motivation: To investigate the limitations of asymptotic stability in forced Navier-Stokes equations, particularly when the standard stability results break down for certain parameter ranges in critical Besov spaces.

Method: Mathematical analysis using critical Besov spaces framework, constructing counterexamples with small external forces that decay in time but produce oscillatory Navier-Stokes flows that do not converge strongly.

Result: Proved asymptotic instability for n≥3 when p≥n, and complete instability for n=2 in all p ranges. The instability is nonlinear in nature and not present at the linear level.

Conclusion: The classical asymptotic stability results for forced Navier-Stokes flows have limitations in critical Besov spaces, with instability arising from nonlinear interactions when parameters exceed certain thresholds, particularly showing different behavior in 2D versus higher dimensions.

Abstract: The asymptotic stability is one of the classical problems in the field of
mathematical analysis of fluid mechanics. In $\mathbb{R}^n$ with $n \geq 3$, it
is easily proved by the standard argument that if the given small external
force decays at temporal infinity, then the small forced Navier--Stokes flow
also strongly converges to zero as time tends to infinity in the framework of
the critical Besov spaces $\dot{B}_{p,q}^{n/p-1}(\mathbb{R}^n)$ with $1 \leq p
< n$ and $1 \leq q < \infty$. In the present paper, we show that this
asymptotic stability fails for $p \geq n$ with $n \geq 3$ in the sense that
there exist arbitrary small external forces whose critical Besov norm decays in
large time, whereas the corresponding Navier--Stokes flows oscillate and do not
strongly converge as $t \to \infty$ in the framework of the critical Besov
spaces $\dot{B}_{p,q}^{n/p-1}(\mathbb{R}^n)$. Moreover, we find that the
situation is different in the two-dimensional case $n=2$ and show the forced
Navier--Stokes flow is asymptotically unstable in
$\dot{B}_{p,1}^{2/p-1}(\mathbb{R}^2)$ for all $1 \leq p \leq \infty$. Our
instability does not appear in the linear level but is caused by the nonlinear
interaction from external forces.

</details>


### [20] [On the symmetry group for systems of conservation laws](https://arxiv.org/abs/2509.21283)
*Michael Sever*

Main category: math.AP

TL;DR: The paper argues that quasi-linear conservation laws are mainly useful for Euler fluid models in higher dimensions, and attempts to extend physical model classes using symmetry groups.


<details>
  <summary>Details</summary>
Motivation: To explore the limited utility of quasi-linear conservation laws beyond Euler systems and investigate if symmetry groups can expand their applicability as physical models.

Method: Using symmetry group analysis to extend the class of systems that could serve as attractive physical models.

Result: Qualified corroboration that quasi-linear conservation laws' utility is largely confined to Euler system models in higher dimensions.

Conclusion: The study supports the conjecture that quasi-linear conservation laws have limited physical modeling utility beyond Euler systems, even when attempting extensions via symmetry groups.

Abstract: A case can be made that the utility of quasi-linear systems of conservation
laws as physical models is largely limited to Euler system models of fluid
flow, at least in higher dimensions. Qualified corroboration of this conjecture
is obtained here, by attempt to extend the class of systems attractive as
physical models via the associated symmetry group.

</details>


### [21] [Some shape functionals for the $k$-Hessian equation](https://arxiv.org/abs/2509.21313)
*Alba Lia Masiello,Francesco Salerno*

Main category: math.AP

TL;DR: The paper studies Pólya type lower bounds for k-Torsional Rigidity associated with the k-Hessian operator on convex sets, and provides quantitative estimates to investigate optimal sets in the inequality.


<details>
  <summary>Details</summary>
Motivation: To establish lower bounds for k-Torsional Rigidity and understand which sets achieve optimality in Pólya type inequalities for k-Hessian operators.

Method: The authors first prove a Pólya type lower bound for k-Torsional Rigidity in any dimension, then provide two quantitative estimates to study optimal sets in this inequality.

Result: A Pólya type lower bound for k-Torsional Rigidity is established, along with quantitative estimates that help characterize optimal sets in this inequality.

Conclusion: The work provides foundational results on lower bounds for k-Torsional Rigidity and offers tools to investigate optimal geometric configurations for k-Hessian operators on convex domains.

Abstract: For a non-empty, bounded, open, and convex set of class $C^2$, we consider
the Torsional Rigidity associated to the $k$-Hessian operator. We first prove
P\'olya type lower bound for the $k$-Torsional Rigidity in any dimension; then,
in order to investigate optimal sets in the P\'olya type inequality, we provide
two quantitative estimates.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [22] [MolCluster: Integrating Graph Neural Network with Community Detection for Coarse-Grained Mapping](https://arxiv.org/abs/2509.20893)
*Zhixuan Zhong,Linbo Ma,Jian Jiang*

Main category: physics.comp-ph

TL;DR: MolCluster is an unsupervised coarse-grained modeling approach that uses graph neural networks and community detection for automated molecular mapping with customizable resolution, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional CG methods rely on fixed mapping rules that limit adaptability and require manual intervention, while supervised learning approaches suffer from limited labeled data and inability to control mapping resolution.

Method: Integrates graph neural network with community detection algorithm, uses predefined group pair loss to preserve target groups, and implements bisection strategy for customizable resolution control.

Result: Evaluations on MARTINI2 dataset show MolCluster outperforms both traditional clustering and supervised models, benefiting from its label-free pretraining strategy.

Conclusion: MolCluster demonstrates potential as a core model for customizable and chemically consistent coarse-grained mapping, addressing limitations of existing approaches.

Abstract: Coarse-grained (CG) modeling simplifies molecular systems by mapping groups
of atoms into representative units. However, traditional CG approaches rely on
fixed mapping rules, which limit their ability to handle diverse chemical
systems and require extensive manual intervention. Thus, supervised
learning-based CG methods have been proposed, enabling more automated and
adaptable mapping. Nevertheless, these methods suffer from limited labeled
datasets and the inability to control mapping resolution, which is essential
for multiscale modeling. To overcome these limitations, we propose MolCluster,
an unsupervised model that integrates a graph neural network and a community
detection algorithm to extract CG representations. Additionally, a predefined
group pair loss ensures the preservation of target groups, and a bisection
strategy enables precise, customizable resolution across different molecular
systems. In the case of the downstream task, evaluations on the MARTINI2
dataset demonstrate that MolCluster, benefiting from its label-free pretraining
strategy, outperforms both traditional clustering and supervised models.
Overall, these results highlight the potential of MolCluster as a core model
for customizable and chemically consistent CG mapping.

</details>


### [23] [A Reformulation of UVN-Flash for Multicomponent Two-Phase Systems with Application to CO2-rich Mixture Transport in Pipelines](https://arxiv.org/abs/2509.20965)
*Pardeep Kumar,Patricio I. Rosen Esquivel*

Main category: physics.comp-ph

TL;DR: A unified framework for two-phase multicomponent transport in CO2 pipelines using homogeneous equilibrium model with thermodynamic closure via Helmholtz energy-based equation of state and novel UVN-flash routine.


<details>
  <summary>Details</summary>
Motivation: Accurate modeling of dense-phase CO2-rich mixtures in CCS pipelines requires coupling fluid dynamics and thermodynamics, especially during transient events like depressurization.

Method: Homogeneous equilibrium model (HEM) for two-phase transport with thermodynamic closure using Helmholtz energy-based equation of state. Novel tailored UVN-flash routine with better-scaled variables aligned with fluid dynamics formulation.

Result: The framework effectively models depressurization of tanks and pipelines containing CO2-rich mixtures for CCS applications.

Conclusion: The proposed unified framework successfully integrates fluid dynamics and thermodynamics for accurate modeling of two-phase multicomponent transport in CO2 pipelines during transient events.

Abstract: Pipeline transport of dense-phase CO2-rich mixtures is a crucial component in
carbon capture and storage (CCS). Accurate modeling requires coupling of fluid
dynamics and thermodynamics, especially during transient events such as
depressurization. In this work, we present a unified framework for two-phase
multicomponent transport in pipelines that integrates both aspects.
Specifically, we employ the homogeneous equilibrium model (HEM) for modeling
the transport of two-phase CO2-rich mixture, with thermodynamic closure
provided by a Helmholtz energy-based equation of state. Phase equilibrium
calculations are performed using UVN-flash, supplemented with a stability
analysis procedure to detect phase separation and generate initial guesses for
the phase-equilibrium calculations. Specifically, we introduce a novel tailored
UVN-flash routine that aligns with the fluid dynamics formulation. This is
achieved by introducing an alternative and better-scaled set of variables for
the phase-equilibrium calculations. The proposed framework is applied to the
depressurization of tanks and pipelines containing CO2-rich mixtures,
demonstrating its effectiveness for CCS-relevant applications.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [24] [Anderson self-localization of light in pair plasmas](https://arxiv.org/abs/2509.20594)
*Maxim Lyutikov,Victor Gurarie*

Main category: physics.plasm-ph

TL;DR: Anderson self-localization occurs in pair plasma for weakly nonlinear electromagnetic waves, leading to wave reflection and trapped pockets due to self-created dielectric fluctuations.


<details>
  <summary>Details</summary>
Motivation: To explore how electromagnetic waves in pair plasma can experience Anderson localization due to self-generated density and dielectric fluctuations, with potential applications to astrophysical phenomena like Fast Radio Bursts.

Method: Analysis of wave propagation in quasi-1D pair plasma with random, time-varying dielectric permittivity fluctuations created by the beat between driver and back-scattered waves.

Result: Waves become localized, reflecting from under-dense plasma and separating into bright trapped pockets and dark regions. Thermal spread can restore propagation by suppressing density fluctuations.

Conclusion: Anderson self-localization in pair plasma explains wave reflection and polarization structures, offering insights into astrophysical Fast Radio Bursts.

Abstract: We demonstrate that in pair plasma weakly nonlinear electromagnetic waves,
$a_0 \leq 1$, experience Anderson self-localization. The beat between the
driver and a back-scattered wave creates random, charge-neutral, large density
fluctuations $\delta \rho/\rho \gg 1$, and corresponding random fluctuations of
the dielectric permittivity $\epsilon$. Propagating in quasi-1D, waves in a
medium with spatially random, time-varying, self-created fluctuations of
dielectric permeability experience localization. Anderson self-localization of
light leads to (i) reflection of EM waves by the under-dense pair plasma; (ii)
a wave already present inside the plasma separates into bright trapped pockets
and dark regions. Mild initial thermal spread restores wave propagation by
suppressing the seeds of parametrically unstable density fluctuations. A
circularly polarized driver produces linearly polarized structures, with
position angle varying randomly between the bright pulses. We discuss possible
applications to astrophysical Fast Radio Bursts.

</details>


### [25] [Reply to "Comments to Marvel Fusions Mixed Fuels Reactor Concept"](https://arxiv.org/abs/2509.21048)
*Hartmut Ruhl,Georg Korn*

Main category: physics.plasm-ph

TL;DR: This paper refutes Lackner et al.'s conclusion that mixed fuel ignition is impossible, showing that by including ionic α-stopping and neutron stopping effects, ignition becomes possible.


<details>
  <summary>Details</summary>
Motivation: To correct the flawed conclusion by Lackner et al. who claimed mixed fuel ignition is impossible due to increased radiation losses and reduced electronic α-stopping at higher temperatures.

Method: Extended the α-stopping model to include ionic α-stopping and neutron stopping effects, which were neglected by Lackner et al., and analyzed their impact on ion temperatures.

Result: The inclusion of ionic α-stopping and neutron stopping leads to elevated ion temperatures (kT_i > kT_e), making ignition of mixed fuels possible.

Conclusion: Contrary to Lackner et al.'s analysis, ignition of mixed fuels is indeed possible when proper α-stopping models including ionic contributions and neutron stopping are considered.

Abstract: In "arXiv:2312.13429" Lackner et al. use standard methods to decide if it is
possible to ignite mixed fuels. They correctly identify that the increased
radiation losses make ignition significantly more challenging than for pure DT
fuels, since this leads to higher ignition temperatures. Further, they conclude
that at those temperatures the reduced electronic $\alpha$-stopping makes
ignition impossible. We show that this conclusion is not correct. The model
used for $\alpha$-stopping by Lackner et al. is only approximately correct for
low temperatures and hydrogen isotopes. By extending the $\alpha$-stopping
model to include ionic $\alpha$-stopping we show in \cite{ruhlkornarXiv5} that
the contribution of ionic $\alpha$-particle stopping cannot be neglected. The
ionic $\alpha$-stopping together with the neutron stopping, which is also
neglected by Lackner et al., lead to elevated ion temperatures implying $kT_i >
kT_e$. Those three effects combined lead us to the conclusion, that ignition of
mixed fuels is indeed possible with far reaching implications, contrary to the
analysis by Lackner.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [26] [Explicit and Effectively Symmetric Schemes for Neural SDEs](https://arxiv.org/abs/2509.20599)
*Daniil Shmelev,Cristopher Salvi*

Main category: cs.LG

TL;DR: Introduces stable, near-reversible Runge-Kutta schemes (EES) for neural SDEs that overcome instability issues of existing reversible solvers while maintaining memory efficiency and gradient accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional backpropagation methods for neural SDEs face trade-offs: discretise-then-optimise has accurate gradients but high memory costs, while optimise-then-discretise has constant memory but suffers from gradient approximation errors. Existing reversible solvers are unstable with complex models and large step sizes.

Method: Develops Explicit and Effectively Symmetric (EES) schemes - a novel class of stable, near-reversible Runge-Kutta solvers that retain the benefits of reversible solvers while overcoming their instability limitations.

Result: Numerical experiments demonstrate superior stability and reliability compared to existing methods, enabling memory-efficient training without severe restrictions on step size or model complexity.

Conclusion: EES schemes provide a practical foundation for scalable and accurate training of neural SDEs, offering both memory efficiency and gradient accuracy without the stability issues of previous reversible approaches.

Abstract: Backpropagation through (neural) SDE solvers is traditionally approached in
two ways: discretise-then-optimise, which offers accurate gradients but incurs
prohibitive memory costs due to storing the full computational graph (even when
mitigated by checkpointing); and optimise-then-discretise, which achieves
constant memory cost by solving an auxiliary backward SDE, but suffers from
slower evaluation and gradient approximation errors. Algebraically reversible
solvers promise both memory efficiency and gradient accuracy, yet existing
methods such as the Reversible Heun scheme are often unstable under complex
models and large step sizes. We address these limitations by introducing a
novel class of stable, near-reversible Runge--Kutta schemes for neural SDEs.
These Explicit and Effectively Symmetric (EES) schemes retain the benefits of
reversible solvers while overcoming their instability, enabling
memory-efficient training without severe restrictions on step size or model
complexity. Through numerical experiments, we demonstrate the superior
stability and reliability of our schemes, establishing them as a practical
foundation for scalable and accurate training of neural SDEs.

</details>


### [27] [Latent Twins](https://arxiv.org/abs/2509.20615)
*Matthias Chung,Deepanshu Verma,Max Collins,Amit N. Subrahmanya,Varuni Katti Sastry,Vishwas Rao*

Main category: cs.LG

TL;DR: Latent Twins is a unifying mathematical framework that creates hidden surrogate models in latent space for underlying equations, bridging representation learning and algorithmic solution methods across scientific machine learning applications.


<details>
  <summary>Details</summary>
Motivation: Current scientific machine learning advances have progressed in parallel with representation learning and algorithmic solution methods evolving as separate pipelines, lacking a unified framework.

Method: The framework creates a hidden surrogate in latent space for underlying equations, governed by operators. It establishes approximation properties for both ODEs and PDEs and is demonstrated across canonical ODEs, PDE benchmarks, and real-data geopotential reanalysis.

Result: Latent Twins provide compact, interpretable surrogates for solution operators that evaluate across arbitrary time gaps in a single shot while remaining compatible with scientific pipelines like assimilation, control, and uncertainty quantification.

Conclusion: The framework offers scalable, theory-grounded surrogates that bridge data-driven representation learning and classical scientific modeling across disciplines, unifying classical modeling, inversion, model reduction, and operator approximation under a single principle.

Abstract: Over the past decade, scientific machine learning has transformed the
development of mathematical and computational frameworks for analyzing,
modeling, and predicting complex systems. From inverse problems to numerical
PDEs, dynamical systems, and model reduction, these advances have pushed the
boundaries of what can be simulated. Yet they have often progressed in
parallel, with representation learning and algorithmic solution methods
evolving largely as separate pipelines. With \emph{Latent Twins}, we propose a
unifying mathematical framework that creates a hidden surrogate in latent space
for the underlying equations. Whereas digital twins mirror physical systems in
the digital world, Latent Twins mirror mathematical systems in a learned latent
space governed by operators. Through this lens, classical modeling, inversion,
model reduction, and operator approximation all emerge as special cases of a
single principle. We establish the fundamental approximation properties of
Latent Twins for both ODEs and PDEs and demonstrate the framework across three
representative settings: (i) canonical ODEs, capturing diverse dynamical
regimes; (ii) a PDE benchmark using the shallow-water equations, contrasting
Latent Twin simulations with DeepONet and forecasts with a 4D-Var baseline; and
(iii) a challenging real-data geopotential reanalysis dataset, reconstructing
and forecasting from sparse, noisy observations. Latent Twins provide a
compact, interpretable surrogate for solution operators that evaluate across
arbitrary time gaps in a single-shot, while remaining compatible with
scientific pipelines such as assimilation, control, and uncertainty
quantification. Looking forward, this framework offers scalable,
theory-grounded surrogates that bridge data-driven representation learning and
classical scientific modeling across disciplines.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [28] [Unbiased Parameter Estimation of Partially Observed Diffusions using Diffusion Bridges](https://arxiv.org/abs/2509.21015)
*Miguel Alvarez,Ajay Jasra*

Main category: stat.ME

TL;DR: A method for estimating static parameters in partially observed diffusion processes with discrete-time observations, eliminating time-discretization bias even when diffusion coefficients depend on parameters.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of parameter estimation in diffusion processes with discrete observations, particularly when diffusion coefficients depend on parameters and existing methods have limitations in efficiency and applicability.

Method: Leverages an identity related to the gradient of the log-likelihood for diffusion bridges, building on previous works but introducing new mathematical identities to enable unbiased estimation without time-discretization bias.

Result: The estimator is proven to be unbiased with finite variance, and the method demonstrates efficacy across several examples while facilitating more efficient Markov chain sampling algorithms.

Conclusion: The proposed approach successfully eliminates time-discretization bias in parameter estimation for diffusion processes, handles parameter-dependent diffusion coefficients, and improves computational efficiency compared to existing methods.

Abstract: In this article we consider the estimation of static parameters for partially
observed diffusion processes with discrete-time observations over a fixed time
interval. In particular, when one only has access to time-discretized solutions
of the diffusions we build upon the works of \cite{ub_par,ub_grad} to devise a
method that can estimate the parameters without time-discretization bias. We
leverage an identity associated to the gradient of the log-likelihood
associated to diffusion bridges, which has not been used before. Contrary to
the afore mentioned methods, the diffusion coefficient can depend on the
parameters and our approach facilitates the use of more efficient Markov chain
sampling algorithms. We prove that our estimator is unbiased with finite
variance and demonstrate the efficacy of our methodology in several examples.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [29] [Gaussian splatting holography](https://arxiv.org/abs/2509.20774)
*Shuhe Zhang,Liangcai Cao*

Main category: physics.optics

TL;DR: GSH uses Gaussian splatting for twin-image-suppressed holographic reconstruction by compressing unknown parameters up to 15x, transforming ill-posed phase retrieval into well-posed problem with reduced phase ambiguities.


<details>
  <summary>Details</summary>
Motivation: In-line holography faces twin-image problems from Hermitian symmetry, causing phase ambiguities in ill-posed phase retrieval. Existing methods struggle to balance data fidelity with twin-image disturbance.

Method: Gaussian splatting holography (GSH) uses Gaussian splatting for optical field representation, compressing unknown parameters by up to 15 folds. Gaussian splatting forms sharp patterns rather than noisy twin-image backgrounds due to spatially slow-varying Gaussian profiles.

Result: GSH achieves constraint-free recovery with accuracy comparable to state-of-the-art constraint-based methods (PSNR=26 dB, SSIM=0.8). Combined with total variation, PSNR improves to 31 dB with 15x compression ability.

Conclusion: GSH effectively suppresses twin-image disturbance in in-line holography through parameter compression and Gaussian splatting representation, providing well-posed phase retrieval with high accuracy and compression capabilities.

Abstract: In-line holography offers high space-bandwidth product imaging with a
simplified lens-free optical system. However, in-line holographic
reconstruction is troubled by twin images arising from the Hermitian symmetry
of complex fields. Twin images disrupt the reconstruction in solving the
ill-posed phase retrieval problem. The known parameters are less than the
unknown parameters, causing phase ambiguities. State-of-the-art deep-learning
or non-learning methods face challenges in balancing data fidelity with
twin-image disturbance. We propose the Gaussian splatting holography (GSH) for
twin-image-suppressed holographic reconstruction. GSH uses Gaussian splatting
for optical field representation and compresses the number of unknown
parameters by a maximum of 15 folds, transforming the original ill-posed phase
retrieval into a well-posed one with reduced phase ambiguities. Additionally,
the Gaussian splatting tends to form sharp patterns rather than those with
noisy twin-image backgrounds as each Gaussian has a spatially slow-varying
profile. Experiments show that GSH achieves constraint-free recovery for
in-line holography with accuracy comparable to state-of-the-art
constraint-based methods, with an average peak signal-to-noise ratio equal to
26 dB, and structure similarity equal to 0.8. Combined with total variation,
GSH can be further improved, obtaining a peak signal-to-noise ratio of 31 dB,
and a high compression ability of up to 15 folds.

</details>


### [30] [Fast 3D Nanophotonic Inverse Design using Volume Integral Equations](https://arxiv.org/abs/2509.20809)
*Amirhossein Fallah,Constantine Sideris*

Main category: physics.optics

TL;DR: A VIE-based forward modeling approach for nanophotonic inverse design that offers orders of magnitude computational efficiency improvement over traditional finite-difference methods.


<details>
  <summary>Details</summary>
Motivation: The complexity and precision requirements of modern optical technologies demand accelerated simulation methods for inverse design of nanophotonic devices, as conventional electromagnetic solvers are computationally expensive for subwavelength structures.

Method: Introduces a volume integral equation (VIE) formulation as an efficient alternative to finite-difference methods, derives an adjoint method tailored for VIE framework, and presents a novel unidirectional mode excitation strategy compatible with VIE solvers.

Result: Comparative benchmarks show multiple orders of magnitude improvement in computational efficiency over conventional FD methods in both time and frequency domains. Successfully designed a selective mode reflector and a 3 dB power splitter to validate practical utility.

Conclusion: The VIE-based framework offers significant runtime advantages and shows promising potential for accelerating inverse design workflows in next-generation nanophotonic devices.

Abstract: Designing nanophotonic devices with minimal human intervention has gained
substantial attention due to the complexity and precision required in modern
optical technologies. While inverse design techniques typically rely on
conventional electromagnetic solvers as forward models within optimization
routines, the substantial electrical size and subwavelength characteristics of
nanophotonic structures necessitate significantly accelerated simulation
methods. In this work, we introduce a forward modeling approach based on the
volume integral equation (VIE) formulation as an efficient alternative to
traditional finite-difference (FD)-based methods. We derive the adjoint method
tailored specifically for the VIE framework to efficiently compute optimization
gradients and present a novel unidirectional mode excitation strategy
compatible with VIE solvers. Comparative benchmarks demonstrate that our
VIE-based approach provides multiple orders of magnitude improvement in
computational efficiency over conventional FD methods in both time and
frequency domains. To validate the practical utility of our approach, we
successfully designed two representative nanophotonic components: a selective
mode reflector and a 3 dB power splitter. Our results underscore the
significant runtime advantages offered by the VIE-based framework, highlighting
its promising role in accelerating inverse design workflows for next-generation
nanophotonic devices.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [31] [Accelerating the Monte Carlo simulation of the Enskog equation for multiscale dense gas flows](https://arxiv.org/abs/2509.20816)
*Bin Hu,Liyan Luo,Lei Wu*

Main category: physics.flu-dyn

TL;DR: A general synthetic iterative scheme using Monte Carlo methods to solve the Enskog equation with rapid convergence and asymptotic-preserving properties for near-continuum flows.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient computational method that can handle near-continuum flows with spatial cell sizes larger than the mean free path, overcoming limitations of traditional approaches.

Method: Mesoscopic-macroscopic two-way coupling: Monte Carlo simulation provides high-order constitutive relations to close the moment equation, while the macroscopic synthetic equation directs particle evolution in Monte Carlo method.

Result: The method achieves rapid convergence, reduces simulation time by several orders of magnitude in near-continuum flows, and successfully handles various flow problems including shock waves, heat transfer, Poiseuille flow, and porous media flow.

Conclusion: The proposed general synthetic iterative scheme is effective for solving the Enskog equation, demonstrating fast convergence and asymptotic-preserving properties, with potential applications in complex flow problems like shale gas extraction.

Abstract: A general synthetic iterative scheme is proposed to solve the Enskog equation
within a Monte Carlo framework. The method demonstrates rapid convergence by
reducing intermediate Monte Carlo evolution and preserves the
asymptotic-preserving property, enabling spatial cell sizes much larger than
the mean free path in near-continuum flows. This is realized through
mesoscopic-macroscopic two-way coupling: the mesoscopic Monte Carlo simulation
provides high-order constitutive relations to close the moment (synthetic)
equation, while the macroscopic synthetic equation, once solved toward steady
state, directs the evolution of simulation particles in the Monte Carlo method.
The accuracy of the proposed general synthetic iterative scheme is verified
through one-dimensional normal shock wave and planar Fourier heat transfer
problems, while its fast-converging and asymptotic-preserving properties are
demonstrated in the force-driven Poiseuille flow and two-dimensional hypersonic
cylinder flow and low-speed porous media flow, where the simulation time is
reduced by several orders of magnitude in near-continuum flows. With the
proposed method, a brief analysis is conducted on the role of the adsorption
layer in porous media flow, mimicking shale gas extraction.

</details>


### [32] [A Fourier/Modal-Spectral-Element Method for the Simulation of High-Reynolds Number Incompressible Stratified Flows in Domains with a Single Non-Periodic Direction](https://arxiv.org/abs/2509.20833)
*Nidia Reyes-Gil,Greg Thomsen,Kristopher Rowe,Peter Diamessis*

Main category: physics.flu-dyn

TL;DR: Development of a high-order accurate Navier-Stokes solver for simulating high-Reynolds-number stratified flows, addressing numerical challenges in oceanic and atmospheric flow simulations.


<details>
  <summary>Details</summary>
Motivation: To overcome numerical and computational challenges in simulating high-Reynolds-number stratified turbulent flows typically observed in oceanic and atmospheric environments, which involve thin regions of high vertical shear, layered turbulence, and internal wave radiation.

Method: Uses Fourier pseudo-spectral method in horizontal direction and modal spectral element discretization in vertical direction. Implements implicit-explicit time discretization scheme solving one-dimensional Helmholtz problems at each time step, with static condensation and modal boundary-adapted basis functions resulting in efficient algorithm based on solving small tridiagonal systems.

Result: The solver demonstrates robustness through benchmark studies including 2D and 3D problems, successfully simulating turbulent stratified wake generated by a sphere in linear stratification.

Conclusion: The proposed high-order accurate Navier-Stokes solver effectively addresses computational challenges of high-Reynolds-number stratified flow simulations and shows promising results for reproducing complex turbulent fluid dynamics in stratified environments.

Abstract: We present the components of a high-order accurate Navier-Stokes solver
designed to simulate high-Reynolds-number stratified flows. The proposed
numerical model addresses some of the numerical and computational challenges
that high-Reynolds-number simulations pose, facilitating the reproduction of
stratified turbulent fluid dynamics typically observed in oceanic and
atmospheric flows, namely the development of thin regions of high vertical
shear, strongly layered turbulence at high Reynolds numbers and internal wave
radiation. This Navier-Stokes solver utilizes a Fourier pseudo-spectral method
in the horizontal direction and a modal spectral element discretization in the
vertical. We adopt an implicit-explicit time discretization scheme that
involves solving several one-dimensional Helmholtz problems at each time step.
Static condensation and modal boundary-adapted basis functions result in an
inexpensive algorithm based on solving many small tridiagonal systems. A series
of benchmark studies is presented to demonstrate the robustness of the flow
solver. These include two-dimensional and three-dimensional problems,
concluding with a turbulent stratified wake generated by a sphere in linear
stratification.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [33] [Relaxation to equilibrium of conservative dynamics II: non-gradient exclusion processes](https://arxiv.org/abs/2509.20797)
*Chenlin Gu,Linzhi Yang*

Main category: math.PR

TL;DR: The paper proves variance decay for the speed-change exclusion process on Z^d, extending previous gradient model results to non-gradient cases with explicit constant characterization.


<details>
  <summary>Details</summary>
Motivation: To extend variance decay results from gradient models to non-gradient speed-change exclusion processes, addressing limitations in previous work that only covered gradient cases.

Method: Combines regularization arguments from previous work with chaos expansion techniques, incorporating new homogenization theory inputs to handle non-gradient aspects.

Result: Shows that the semigroup satisfies variance decay Var[P_t u] = C_u t^{-d/2} + o(t^{-(d+δ)/2}) for local functions u, with explicit constant C_u characterization.

Conclusion: Successfully extends variance decay results to non-gradient models, demonstrating the applicability of combined regularization and chaos expansion methods with homogenization theory.

Abstract: For the speed-change exclusion process on $\mathbb{Z}^d$ reversible with
respect to the product Bernoulli measure, we prove that its semigroup $P_t$
satisfies a variance decay $\operatorname{Var}[P_t u] = C_u t^{-\frac{d}{2}} +
o(t^{-\frac{d+\delta}{2}})$ for every local function $u$, with the constant
$C_u$ explicitly characterized. This extends the result of Janvresse, Landim,
Quastel and Yau in [Ann. Probab. 27(1) 325--360, 1999] to a non-gradient model.
The proof combines the regularization argument in the previous work, and the
chaos expansion in [Markov Process. Related Fields, 5(2) 125--162, 1999] by
Bertini and Zegarlinski, via a new input from the homogenization theory.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [34] [Strain localization in reduced order asymptotic homogenization](https://arxiv.org/abs/2509.16210)
*Harpreet Singh,Puneet Mahajan*

Main category: cs.CE

TL;DR: A multiscale technique using asymptotic homogenization with reduced order methods to capture damage and inelastic effects in composite materials, addressing strain localization and spurious stiffness issues.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient computational method that can accurately predict damage and inelastic behavior in composite materials at both micro and macro scales, while overcoming challenges like strain localization and artificial stiffness.

Method: Two-scale homogenization procedure with eigen strain representation for inelastic response, using reduced order techniques to reduce computational effort. Damage modeling at microscale with continuum damage mechanics, and stress-strain relation alteration based on dissipated fracture energy in crack bands to address strain localization.

Result: The proposed formulation successfully predicts macroscale response and captures damage and plasticity-induced inelastic strains, with verification studies demonstrating its effectiveness.

Conclusion: The reduced order asymptotic homogenization technique provides an efficient and accurate approach for multiscale analysis of composite materials, effectively handling damage, inelastic effects, and overcoming computational challenges associated with traditional methods.

Abstract: A reduced order asymptotic homogenization based multiscale technique which
can capture damage and inelastic effects in composite materials is proposed.
This technique is based on two scale homogenization procedure where eigen
strain representation accounts for the inelastic response and the computational
efforts are alleviated by reduction of order technique. Macroscale stress is
derived by calculating the influence tensors from the analysis of
representative volume element (RVE). At microscale, the damage in the material
is modeled using continuum damage mechanics (CDM) based framework. To solve the
problem of strain localization a method of the alteration of stress-strain
relation of micro con- stituents based on the dissipated fracture energy in a
crack band is implemented. The issue of spurious post failure artificial
stiffness at macroscale is discussed and effect of increasing the order to
alleviate this problem is checked. Verification studies demonstrated the
proposed formulation predicts the macroscale response and also captures the
damage and plasticity induced inelastic strains.

</details>


### [35] [E$^2$-TFA based multiscale analysis of failure in elasto-plastic composites](https://arxiv.org/abs/2509.16211)
*Harpreet Singh*

Main category: cs.CE

TL;DR: A novel homogenization method called E²-TFA for analyzing elastoplastic composite material failure using elastic and eigen influence tensors, which improves computational efficiency and accuracy in capturing damage and inelastic deformations.


<details>
  <summary>Details</summary>
Motivation: To address the drawback of TFA-based methods that have unrealistic post-damage stiffness response, and to develop a more efficient and accurate method for analyzing composite material failure.

Method: Uses elastic and eigen transformation functions with reduced order modeling and piecewise constant eigenstrain field throughout subdomains. Considers microscopic eigenstrain field accounting for intra-phase damage and inelastic strains.

Result: The method accurately captures damage and inelastic deformations, with validation through RVE simulations, complex load histories, and comparison with experimental fracture parameters for glass fiber composites.

Conclusion: E²-TFA provides an accurate and efficient approach for estimating mechanical response of composite materials by better capturing damage and inelastic deformations compared to traditional TFA methods.

Abstract: This paper describes a novel homogenization methodology for analyzing the
failure of elastoplastic composite materials based on elastic and eigen
influence tensors-driven transformation field analysis ($\mathtt{E}^2$-TFA).
The proposed technique considers the microscopic eigenstrain field accounting
for intra-phase damage and inelastic strains. This results in realistic
computations by alleviating the post-damage stiffness response, which is a
drawback of TFA-based methods. We attain computational efficiency by
identifying the preprocessing data solely from the elastic and eigen
transformation functions and adopting a reduced order modelling technique with
a piecewise constant eigenstrain field throughout the subdomains. The
performance of the model is assessed by simulating the response for (a) the
representative volume element (RVE) as a homogenized continuum and (b) the
various composites under complex load histories with intricate macroscale
morphologies. Furthermore, the nonlinear shear stress-strain response of a
glass fiber composite is calculated and compared to experimentally measured
fracture initiation parameters, failure plane orientation, and strain
histories. Finally, we show that $\mathtt{E}^2$-TFA can accurately and
efficiently capture damage and inelastic deformations in order to estimate the
mechanical response of composite materials in a better way.

</details>


### [36] [Characterizing failure morphologies in fiber-reinforced composites via k-means clustering based multiscale framework](https://arxiv.org/abs/2509.20011)
*Harpreet Singh*

Main category: cs.CE

TL;DR: A novel homogenization methodology using damage informed transformation field analysis (D-TFA) for failure analysis of fiber-reinforced composites, featuring computational efficiency through reduced-order modeling and k-means clustering.


<details>
  <summary>Details</summary>
Motivation: To develop a more realistic and computationally efficient approach for simulating failure in fiber-reinforced composite materials by accurately capturing damage patterns and directional strengths.

Method: Utilizes elastic and eigen influence tensors within D-TFA framework, employs reduced-order modeling with k-means clustering based on elastic and eigen strain distributions to partition microscale domains, and simulates representative volume elements as homogenized continua.

Result: D-TFA accurately captures damage patterns and directional strengths, with higher cluster counts proving crucial for accurate stress-strain response, especially for complex microstructures. The method successfully predicts failure paths in open-hole specimens with different fiber layups.

Conclusion: The proposed D-TFA framework provides improved predictions of mechanical behavior in composite materials and demonstrates that increased cluster counts are essential for accurate simulations of complex microstructures.

Abstract: A novel homogenization methodology is proposed for analyzing the failure of
fiber-reinforced composite materials, utilizing elastic and eigen influence
tensors within a damage informed transformation field analysis (D-TFA)
framework. This approach includes a technique for calculating macroscopic
damage under uniform stress and strain conditions, offering more realistic
simulations. Computational efficiency is enhanced through a reduced-order
modeling strategy, while elastic and eigen strain distribution driven k-means
clustering methods are employed to partition the microscale domain. The model's
performance is assessed by simulating the response of a representative volume
element (RVE) treated as a homogenized continuum. Subsequently, a comparative
assessment is carried out to check the efficacy of two clustering schemes.
Damage morphologies are calculated using proposed framework and compared with
predictions obtained using finite element method. Furthermore, open-hole
specimen tests are simulated and failure paths are predicted for the domains
with different fiber layups. Ultimately, we show that D-TFA can accurately
capture damage patterns and directional strengths, providing improved
predictions of the mechanical behavior of composite materials. It has been
demonstrated that higher cluster counts are crucial for capturing a more
accurate stress-strain response, especially for complex microstructures.

</details>


### [37] [Extrapolating Phase-Field Simulations in Space and Time with Purely Convolutional Architectures](https://arxiv.org/abs/2509.20770)
*Christophe Bonneville,Nathan Bieberdorf,Pieterjan Robbe,Mark Asta,Habib N. Najm,Laurent Capolungo,Cosmin Safta*

Main category: cs.CE

TL;DR: A convolutional U-Net surrogate model that accelerates phase-field simulations of liquid metal dealloying by up to 16,000x, enabling long-term predictions from short training data.


<details>
  <summary>Details</summary>
Motivation: Phase-field models for liquid metal dealloying become computationally intractable for large domains or long time horizons, requiring more efficient simulation methods.

Method: Conditionally parameterized fully convolutional U-Net with convolutional self-attention and physics-aware padding, trained on short small-scale simulations but capable of temporal and spatial extrapolation.

Result: Achieves relative errors under 5% within training regime and below 10% when extrapolating to larger domains and later times, with 16,000x speedup over traditional solvers.

Conclusion: This marks an early step toward scalable, high-fidelity extrapolation of LMD phase-field models, reducing simulation time from weeks to seconds.

Abstract: Phase-field models of liquid metal dealloying (LMD) can resolve rich
microstructural dynamics but become intractable for large domains or long time
horizons. We present a conditionally parameterized, fully convolutional U-Net
surrogate that generalizes far beyond its training window in both space and
time. The design integrates convolutional self-attention and physics-aware
padding, while parameter conditioning enables variable time-step skipping and
adaptation to diverse alloy systems. Although trained only on short,
small-scale simulations, the surrogate exploits the translational invariance of
convolutions to extend predictions to much longer horizons than traditional
solvers. It accurately reproduces key LMD physics, with relative errors
typically under 5% within the training regime and below 10% when extrapolating
to larger domains and later times. The method accelerates computations by up to
16,000 times, cutting weeks of simulation down to seconds, and marks an early
step toward scalable, high-fidelity extrapolation of LMD phase-field models.

</details>


<div id='physics.ins-det'></div>

# physics.ins-det [[Back]](#toc)

### [38] [Physics Informed Neural Networks for design optimisation of diamond particle detectors for charged particle fast-tracking at high luminosity hadron colliders](https://arxiv.org/abs/2509.21123)
*Alessandro Bombini,Alessandro Rosa,Clarissa Buti,Giovanni Passaleva,Lucio Anderlini*

Main category: physics.ins-det

TL;DR: 3D diamond pixel sensors for high-luminosity hadron colliders require modeling signal propagation delays due to high electrode resistivity, addressed via a novel PDE formulation and neural network solver.


<details>
  <summary>Details</summary>
Motivation: Future high-luminosity hadron colliders need tracking detectors with extreme radiation tolerance, high precision, and sub-nanosecond timing, which 3D diamond sensors can provide, but electrode resistivity causes signal delays that must be modeled.

Method: Extend Ramo-Shockley formalism with a 3rd-order, 3+1D PDE derived from Maxwell's equations; solve numerically and couple with charge transport simulations; use Mixture-of-Experts Physics-Informed Neural Network trained on Spectral Method data for meshless timing assessment.

Result: Developed a computational framework to model timing degradation in 3D diamond sensors due to electrode resistance, enabling accurate prediction of signal propagation delays.

Conclusion: The proposed method effectively addresses signal delay issues in radiation-hard 3D diamond pixel sensors, crucial for meeting the timing requirements of future high-energy physics experiments.

Abstract: Future high-luminosity hadron colliders demand tracking detectors with
extreme radiation tolerance, high spatial precision, and sub-nanosecond timing.
3D diamond pixel sensors offer these capabilities due to diamond's radiation
hardness and high carrier mobility. Conductive electrodes, produced via
femtosecond IR laser pulses, exhibit high resistivity that delays signal
propagation. This effect necessitates extending the classical Ramo-Shockley
weighting potential formalism. We model the phenomenon through a 3rd-order,
3+1D PDE derived as a quasi-stationary approximation of Maxwell's equations.
The PDE is solved numerically and coupled with charge transport simulations for
realistic 3D sensor geometries. A Mixture-of-Experts Physics-Informed Neural
Network, trained on Spectral Method data, provides a meshless solver to assess
timing degradation from electrode resistance.

</details>


<div id='q-bio.TO'></div>

# q-bio.TO [[Back]](#toc)

### [39] [Data-driven Neural Networks for Windkessel Parameter Calibration](https://arxiv.org/abs/2509.21206)
*Benedikt Hoock,Tobias Köppl*

Main category: q-bio.TO

TL;DR: A novel neural network-based method for calibrating Windkessel parameters in 1D-0D coupled blood flow models using simulated brachial artery pressure data.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient and accurate method for calibrating Windkessel parameters in blood flow models, especially when measurement locations are uncertain or data contains noise.

Method: Design a data-driven neural network trained on simulated blood pressures in the left brachial artery, then extend it with dummy neurons for parameter calibration on measured pulse waves.

Result: The NN emulates pressure pulse waves across the entire simulated domain with negligible error and minimal computational effort.

Conclusion: The method effectively calibrates Windkessel parameters in various challenging scenarios including unknown measurement locations and noisy data.

Abstract: In this work, we propose a novel method for calibrating Windkessel (WK)
parameters in a dimensionally reduced 1D-0D coupled blood flow model. To this
end, we design a data-driven neural network (NN)trained on simulated blood
pressures in the left brachial artery. Once trained, the NN emulates the
pressure pulse waves across the entire simulated domain, i.e., over time, space
and varying WK parameters, with negligible error and computational effort. To
calibrate the WK parameters on a measured pulse wave, the NN is extended by
dummy neurons and retrained only on these. The main objective of this work is
to assess the effectiveness of the method in various scenarios -- particularly,
when the exact measurement location is unknown or the data are affected by
noise.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [40] [A relativistic coupled-cluster treatment of magnetic hyperfine structure of the $X^2Π$ and $A^2Σ^+$ states of OH isotopologues](https://arxiv.org/abs/2509.20522)
*D. P. Usov,Y. S. Kozhedub,A. V. Stolyarov,L. V. Skripnikov,V. M. Shabaev,I. I. Tupitsyn*

Main category: physics.chem-ph

TL;DR: Ab initio calculations of magnetic dipole hyperfine structure constants for hydroxyl radical isotopologues using relativistic coupled-cluster methods, achieving excellent agreement with experimental data.


<details>
  <summary>Details</summary>
Motivation: To provide accurate theoretical predictions of hyperfine structure constants for OH isotopologues, extending beyond previous studies to support analysis of higher rovibrational levels.

Method: Four-component relativistic coupled-cluster method with excitations up to triple level (CCSD(T) and CCSDT), calculating HFS functions over internuclear distance range R ∈ [0.6, 1.8] Å for ground and excited electronic states.

Result: Most accurate theoretical HFS predictions to date, showing excellent agreement (within 1%) with experimental data for lowest vibrational levels, with extended internuclear distance coverage.

Conclusion: The study provides robust foundation for accurate HFS treatments of higher rovibrational levels in both adiabatic and non-adiabatic frameworks, demonstrating reliability of the quantum-chemistry approach.

Abstract: $\textit{Ab initio}$ calculations of the parallel component of the magnetic
dipole hyperfine structure (HFS) constant have been carried out for hydroxyl
radical isotopologues ($^{16,17}$OH(D)) over the internuclear distance range $R
\in [0.6, 1.8]$ \r{A}. For the ground electronic state $X^2\Pi$, the HFS
functions were evaluated for contributions induced by both oxygen and hydrogen
nuclei. In addition, the hydrogen-induced HFS curve was calculated for the
excited $A^2\Sigma^+$ state. The quantum-chemistry study employs a
four-component relativistic coupled-cluster (CC) method, including excitations
up to the triple level, namely: the contribution of triple-cluster amplitudes
was studied both perturbatively (CCSD(T)) and through fully iterative
calculations (CCSDT). The resulting oxygen- and hydrogen-induced HFS functions
represent the most accurate and reliable theoretical predictions to date
exhibiting excellent agreement with semiempirical curve for hydrogen-induced
HFS derived from high-resolution spectroscopic data for the lowest vibrational
levels ($v\in [0,2]$) of the electronic $X^2\Pi$ state. Vibrationally averaged
$\textit{ab initio}$ values are consistent with experimental values within
$1\%$ for all states considered. Furthermore, the internuclear distance range
over which the HFS curves are defined has been extended beyond that of previous
studies, thereby providing a robust foundation for accurate HFS treatments of
higher-lying rovibrational levels of OH isotopologues within both adiabatic and
non-adiabatic frameworks.

</details>
