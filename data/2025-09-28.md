<div id=toc></div>

# Table of Contents

- [math.NA](#math.NA) [Total: 8]
- [math.AP](#math.AP) [Total: 13]
- [physics.comp-ph](#physics.comp-ph) [Total: 2]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 2]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 2]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [q-bio.TO](#q-bio.TO) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [physics.optics](#physics.optics) [Total: 2]
- [physics.ins-det](#physics.ins-det) [Total: 1]
- [cs.CE](#cs.CE) [Total: 4]
- [stat.ME](#stat.ME) [Total: 1]
- [math.PR](#math.PR) [Total: 1]


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [1] [A robust framework for frictional fault contact in geological formations using a stabilized augmented Lagrangian approach](https://arxiv.org/abs/2509.20528)
*Matteo Frigo,Nicola Castelletto,Matteo Cusini,Randolph R. Settgast,Hamdi A. Tchelepi*

Main category: math.NA

TL;DR: This paper presents a stabilized mixed finite element method using augmented Lagrangian approach to model frictional contact behavior along fault surfaces in geological systems.


<details>
  <summary>Details</summary>
Motivation: Accurate modeling of frictional contact behavior along fault surfaces is crucial for evaluating performance and safety of geological engineered systems like carbon storage sites and geothermal fields, but existing methods struggle with the nonlinear, path-dependent nature of frictional slip constraints.

Method: Uses augmented Lagrangian method via Uzawa algorithm with mixed finite element spaces combining low-order piecewise linear displacements in 3D domain cells with piecewise constant tractions on fault surfaces, enriched with face bubble functions for stability.

Result: The proposed method provides a stable formulation that satisfies the inf-sup condition without requiring additional parameters, offering advantages over other stabilization techniques.

Conclusion: This approach effectively handles the inequality constraints of frictional slip physics and integrates naturally within the Uzawa framework, providing a robust numerical method for geological system simulations.

Abstract: Numerical simulations are essential for evaluating the performance and safety
of geological engineered systems such as geologic carbon storage sites,
enhanced geothermal fields, and oil and gas reservoirs. A key challenge lies in
accurately modeling the frictional contact behavior along fault surfaces. This
problem involves inequality constraints that arise from the physics of
frictional slip, requiring specialized numerical methods to handle the
resulting highly nonlinear and path-dependent behavior. In this work, we
address this challenge using an augmented Lagrangian method implemented via the
Uzawa algorithm. The formulation employs mixed finite element spaces, combining
low-order piecewise linear displacements within the 3D domain cells with
piecewise constant tractions defined on the fault surfaces. Furthermore, to
ensure stability and satisfy the inf-sup condition, the discrete displacement
space is enriched with face bubble functions on both sides of the contact
interfaces. This approach offers several advantages over other stabilization
techniques that rely on additional terms, as it does not require extra
parameters for implementation, and it integrates naturally in the Uzawa
framework.

</details>


### [2] [Symplectic Isospectral Runge--Kutta Methods as Lie group methods](https://arxiv.org/abs/2509.20620)
*Paolo Cifani,Klas Modin,Cecilia Pagliantini,Milo Viviani*

Main category: math.NA

TL;DR: Comparison of three approaches for conservative time integration of isospectral flows on quadratic Lie algebras, showing that reformulating the flow allows symplectic Runge-Kutta methods to produce efficient conservative schemes.


<details>
  <summary>Details</summary>
Motivation: To develop efficient conservative integration methods for isospectral flows on quadratic Lie algebras, particularly for Hamiltonian systems.

Method: Reformulate the original isospectral flow and apply symplectic Runge-Kutta methods to create conservative schemes.

Result: The proposed approach yields computationally more efficient conservative schemes than previous methods, and enables arbitrarily high-order Lie-Poisson integrators for Hamiltonian systems.

Conclusion: By choosing appropriate formulations of isospectral flows, symplectic Runge-Kutta methods can produce efficient conservative integrators for quadratic Lie algebras, with high-order capabilities for Hamiltonian systems.

Abstract: In this paper, we compare three different approaches for a conservative
integration in time of isospectral flows on quadratic Lie algebras.
  We show that it is possible to choose an equivalent formulation of the
original isospectral flow such that, applying a symplectic Runge--Kutta method,
the resulting scheme is both conservative and computationally more efficient
than other schemes previously proposed.
  In particular, in the case of Hamiltonian systems, we get arbitrarily
high-order Lie--Poisson integrators on quadratic Lie algebras.

</details>


### [3] [A domain decomposition method for computing the scattering matrix of waveguide circuits](https://arxiv.org/abs/2509.20695)
*Tristan Goodwill,Shidong Jiang,Manas Rachh,Kosuke Sugita*

Main category: math.NA

TL;DR: A fast divide-and-conquer solver for time-harmonic wave scattering in metallic waveguides using impedance-to-impedance maps and second-kind Fredholm integral equations with weakly singular kernels.


<details>
  <summary>Details</summary>
Motivation: To develop efficient numerical methods for wave scattering in infinite metallic waveguide structures, particularly addressing challenges with trapped modes and computational efficiency.

Method: Uses radiation boundary conditions via projectors onto outgoing modes, a divide-and-conquer solver with impedance-to-impedance maps, and second-kind Fredholm integral equations with weakly singular kernels for Dirichlet waveguides.

Result: The method achieves substantial efficiency gains, outperforming state-of-the-art fast iterative and direct solvers by one to two orders of magnitude in numerical experiments on large structures with many circuit elements.

Conclusion: The proposed approach provides an efficient and robust numerical framework for wave scattering in metallic waveguides, with significant computational advantages over existing methods.

Abstract: We analyze and develop numerical methods for time-harmonic wave scattering in
metallic waveguide structures of infinite extent. We show that radiation
boundary conditions formulated via projectors onto outgoing modes determine the
coefficients of propagating modes uniquely, even when the structure supports
trapped modes. Building on this, we introduce a fast divide-and-conquer solver
that constructs solution operators on subdomains as impedance-to-impedance maps
and couples them by enforcing continuity conditions across their interfaces.
For Dirichlet waveguides, the computation of impedance-to-impedance maps
requires the solution of mixed Dirichlet-Impedance boundary value problems. We
construct a second-kind Fredholm integral equation that avoids
near-hypersingular operators, requiring only integral operators whose kernels
are at most weakly singular. Numerical experiments on large structures with
many circuit elements demonstrate substantial efficiency gains: the proposed
approach typically outperforms state-of-the-art fast iterative and fast direct
solvers by one to two orders of magnitude.

</details>


### [4] [A Convergent Structure-Preserving Scheme for Dissipative Solutions of the Rotating Shallow Water System](https://arxiv.org/abs/2509.20764)
*K. R. Arun,A. Krishnamurthy*

Main category: math.NA

TL;DR: A semi-implicit finite volume scheme for 2D rotating shallow water equations that is energy stable, well-balanced, consistent, and convergent through carefully designed stabilization terms.


<details>
  <summary>Details</summary>
Motivation: To develop a numerical scheme for rotating shallow water equations that simultaneously achieves energy stability, preserves geostrophic steady states (well-balanced), maintains consistency, and ensures convergence.

Method: Introduces carefully chosen stabilization terms into convective fluxes and source terms of mass and momentum equations. Uses semi-implicit finite volume approach with CFL-type conditions and auxiliary time-step restrictions for Coriolis forces.

Result: The scheme achieves energy stability under CFL conditions, preserves discrete geostrophic steady states, maintains positivity, and converges to dissipative measure-valued solutions of the RSW system.

Conclusion: The proposed scheme successfully combines multiple desirable properties (energy stability, well-balancing, consistency, convergence) for rotating shallow water equations, with theoretical results validated through numerical experiments.

Abstract: We design and analyse a semi-implicit finite volume scheme for the
two-dimensional rotating shallow water (RSW) equations that is energy stable,
well-balanced (capable of preserving discrete geostrophic steady states),
consistent, and covergent. The key idea is the introduction of carefully chosen
stabilisation terms into the convective fluxes of the mass and momentum
equations, as well as the source terms. Under a CFL-type condition, together
with an auxiliary time-step restriction arising from the Coriolis forces, we
establish the energy stability of the scheme. The stabilisation terms are
constructed to vanish at steady states, thereby ensuring the well-balancing
property under an appropriate advective CFL condition. We derive a sufficient
time-step restriction that guarantees stability, well-balancing, existence of
discrete solutions, and positivity simultaneously. Furthermore, under mild
boundedness assumptions, we obtain a priori estimates showing that the
stabilisation terms converge to zero as the mesh is refined, which establishes
the consistency of the scheme. This in turn enables us to prove that numerical
solutions generate a Young measure, identifiable as a dissipative
measure-valued solution of the RSW system, thereby yielding convergence of the
scheme. Finally, we confirm the theoretical results through extensive numerical
experiments.

</details>


### [5] [Higher-Order Root-Finding Algorithm and its Applications](https://arxiv.org/abs/2509.20897)
*Wei Guo Foo,Chik How Tan*

Main category: math.NA

TL;DR: A higher-order root-finding method using Taylor expansion that avoids higher-order derivatives, has lower computational complexity, explicit convergence factor, and can implement Householder's method numerically.


<details>
  <summary>Details</summary>
Motivation: Householder's method requires higher-order derivatives of reciprocal functions, which leads to slow symbolic computations and error accumulation in numerical differentiation. Existing convergence factors are rough estimates.

Method: Proposed method uses only Taylor expansion of a function without requiring higher-order derivatives, providing explicit convergence factor and lower computational complexity.

Result: The method successfully computes pre-images of q-ary entropy functions from coding theory and demonstrates different basins of attraction compared to other root-finding methods.

Conclusion: The proposed Taylor expansion-based method offers computational advantages over Householder's method while maintaining high-order convergence, with practical applications in coding theory and numerical analysis.

Abstract: Root-finding method is an iterative process that constructs a sequence
converging to a solution of an equation. Householder's method is a higher-order
method that requires higher order derivatives of the reciprocal of a function
and has disadvantages. Firstly, symbolic computations can take a long time, and
numerical methods to differentiate a function can accumulate errors. Secondly,
the convergence factor existing in the literature is a rough estimate. In this
paper, we propose a higher-order root-finding method using only Taylor
expansion of a function. It has lower computational complexity with explicit
convergence factor, and can be used to numerically implement Householder's
method. As an application, we apply the proposed method to compute pre-images
of $q$-ary entropy functions, commonly seen in coding theory. Finally, we study
basins of attraction using the proposed method and compare them with other
root-finding methods.

</details>


### [6] [Multiprecision computations with Schwarz methods](https://arxiv.org/abs/2509.20937)
*Michal Outrata,Daniel B. Szyld*

Main category: math.NA

TL;DR: The paper analyzes using lower precision arithmetic (single precision) for local solves in Schwarz methods and preconditioners, finding that about 5 digits of accuracy is sufficient to maintain theoretical performance.


<details>
  <summary>Details</summary>
Motivation: To reduce computational cost by using multiprecision arithmetic where local problems can be solved with lower precision than the main computation, while maintaining overall accuracy.

Method: Using multiprecision arithmetic with Schwarz methods and preconditioners, solving local problems at lower precision (single precision) while main computation remains in double precision, with analysis of appropriate round-off criteria.

Result: Experimental findings show that approximately 5 digits of accuracy are sufficient to achieve theoretical restrictions, meaning single precision suffices for local solves in model problems.

Conclusion: Single precision arithmetic can be effectively used for local solves in Schwarz methods without compromising overall accuracy, providing computational efficiency benefits.

Abstract: We explore and analyze the use of multiprecision arithmetic for several
classes of Schwarz methods and preconditioners, where the approximate solution
of the local problems is performed at a lower precision, i.e., with fewer
digits of accuracy than in the underlying (double precision) computation.
Conditions for the appropriate round-off criteria for the lower precision are
presented. It is found experimentally that for the model problems about 5
digits of accuracy are sufficient to achieve the theoretical restrictions, and
thus, single precision suffices for the local solves. Several numerical
experiments illustrate the obtained results.

</details>


### [7] [Model reduction of parametric ordinary differential equations via autoencoders: structure-preserving latent dynamics and convergence analysis](https://arxiv.org/abs/2509.21280)
*Enrico Ballini,Marco Gambarini,Alessio Fumagalli,Luca Formaggia,Anna Scotti,Paolo Zunino*

Main category: math.NA

TL;DR: A reduced-order modeling approach using autoencoder-based nonlinear maps to accelerate complex dynamical simulations while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: To accelerate complex dynamical simulations of nonlinear, parameter-dependent ODEs without sacrificing accuracy through dimensionality reduction.

Method: Uses autoencoders for dimensionality reduction, solves the resulting low-dimensional ODE with standard time integration schemes, and reconstructs high-dimensional solutions from low-dimensional ones.

Result: Numerical experiments demonstrate robustness and accuracy, showing potential to accelerate complex dynamical simulations while maintaining solution quality.

Conclusion: The approach successfully accelerates simulations while preserving accuracy, with investigation showing how dimensionality reduction affects solution stability properties.

Abstract: We propose a reduced-order modeling approach for nonlinear,
parameter-dependent ordinary differential equations (ODE). Dimensionality
reduction is achieved using nonlinear maps represented by autoencoders. The
resulting low-dimensional ODE is then solved using standard integration in time
schemes, and the high-dimensional solution is reconstructed from the
low-dimensional one. We investigate the applicability of neural networks for
constructing effective autoencoders with the property of reconstructing the
input manifold with null representation error. We study the convergence of the
reduced-order model to the high-fidelity one. Numerical experiments show the
robustness and accuracy of our approach, highlighting its potential to
accelerate complex dynamical simulations without sacrificing accuracy.
Moreover, we examine how the reduction influences the stability properties of
the reconstructed high-dimensional solution.

</details>


### [8] [Two ADI compact difference methods for variable-exponent diffusion wave equations](https://arxiv.org/abs/2509.21316)
*Hao Zhang,Kexin Li,Wenlin Qiu*

Main category: math.NA

TL;DR: This paper develops two alternating direction implicit (ADI) compact finite difference schemes for solving 2D diffusion-wave equations with variable exponents, achieving high-order accuracy in space and time with unconditional stability.


<details>
  <summary>Details</summary>
Motivation: To model mechanical diffusive wave propagation in viscoelastic media with spatially varying properties, which requires efficient numerical methods due to the complexity of variable exponent diffusion-wave equations.

Method: Transformed the diffusion-wave model using convolution method, applied two time discretization strategies, and employed ADI technique with spatial compact finite difference method to create two fully discrete schemes.

Result: Both ADI compact schemes are proven unconditionally stable and convergent. First scheme achieves α(0)-order accuracy in time and fourth-order accuracy in space; second scheme attains second-order accuracy in time and fourth-order accuracy in space. Numerical experiments confirm theoretical error estimates.

Conclusion: The proposed ADI compact schemes provide efficient and accurate numerical methods for solving 2D diffusion-wave equations with variable exponents, with demonstrated computational efficiency and theoretical guarantees.

Abstract: In this work, we study two-dimensional diffusion-wave equations with variable
exponent, modeling mechanical diffusive wave propagation in viscoelastic media
with spatially varying properties. We first transform the diffusion-wave model
into an equivalent form via the convolution method. Two time discretization
strategies are then applied to approximate each term in the transformed
equation, yielding two fully discrete schemes based on a spatial compact finite
difference method. To reduce computational cost, the alternating direction
implicit (ADI) technique is employed. We prove that both ADI compact schemes
are unconditionally stable and convergent. Under solution regularity, the first
scheme achieves $\alpha(0)$-order accuracy in time and fourth-order accuracy in
space, while the second scheme attains second-order accuracy in time and
fourth-order accuracy in space. Numerical experiments confirm the theoretical
error estimates and demonstrate the efficiency of the proposed methods.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [9] [Increased lifespan for 3D compressible Euler flows with rotation](https://arxiv.org/abs/2509.20505)
*Haram Ko,Benoit Pausader,Ryo Takada,Klaus Widmayer*

Main category: math.AP

TL;DR: The paper establishes a lower bound on the existence time for solutions to the compressible Euler equation with Coriolis force, relating it to rotation speed, sound speed, and initial data size. It also provides dispersive decay estimates for the linearized equation and improves bounds for the incompressible Euler-Coriolis system in the incompressible limit.


<details>
  <summary>Details</summary>
Motivation: To understand how rotation affects the lifespan of solutions to compressible Euler equations and to obtain better bounds for related incompressible systems.

Method: Mathematical analysis of the compressible Euler equation with Coriolis term, including proving existence time bounds and obtaining dispersive decay estimates for the linearized equation.

Result: A lower bound on solution existence time in terms of rotation speed, sound speed, and initial data size, along with precise dispersive decay estimates for the linearized equation.

Conclusion: The analysis provides improved understanding of solution behavior in rotating compressible fluids and yields better bounds for the incompressible Euler-Coriolis system as a special case.

Abstract: We consider the compressible Euler equation with a Coriolis term and prove a
lower bound on the time of existence of solutions in terms of the speed of
rotation, sound speed and size of the initial data. Along the way, we obtain
precise dispersive decay estimates for the linearized equation. In the
incompressible limit, this improves current bounds for the incompressible
Euler-Coriolis system as well.

</details>


### [10] [On the Landis Conjecture for Positive Quasi-linear Operators on Graphs](https://arxiv.org/abs/2509.20559)
*Ujjal Das,Matthias Keller,Yehuda Pinchover*

Main category: math.AP

TL;DR: This paper proves a Landis-type unique continuation theorem for positive quasi-linear operators on graphs, showing that harmonic functions for certain quasilinear Schrödinger operators must be identically zero under specific decay conditions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to establish unique continuation properties for harmonic functions on graphs, extending classical Landis-type results from continuous settings to discrete graph structures, with applications to model graphs and regular trees.

Method: The method uses criticality theory, particularly the Liouville comparison theorem, and builds on the concept of simplified energy to analyze positive quasilinear Schrödinger operators with potentials less than 1.

Result: The main result provides decay criteria that ensure harmonic functions for positive quasilinear Schrödinger operators are trivially zero, establishing a unique continuation property on graphs.

Conclusion: The paper successfully extends Landis-type unique continuation results to graph settings using positivity assumptions and criticality theory, with applications demonstrating the theory's effectiveness on model graphs and regular trees.

Abstract: We prove a Landis type unique continuation result for positive quasi-linear
operators on graphs. Specifically, we give decay criteria that ensures when a
harmonic function for a positive quasilinear Schr\"odinger operator with
potential less than 1 is trivially zero. The assumption of positivity of the
operator allows the application of criticality theory such as the Liouville
comparison theorem. Furthermore, our results fundamentally build on the so
called simplified energy. As an application we discuss the case of model graphs
and in particular regular trees.

</details>


### [11] [Extended Sobolev Scale on Non-Compact Manifolds](https://arxiv.org/abs/2509.20598)
*Ognjen Milatovic*

Main category: math.AP

TL;DR: This paper extends the extended Sobolev scale to manifolds of bounded geometry, describes interpolation spaces between Sobolev spaces, and establishes mapping properties of pseudo-differential operators on these scales.


<details>
  <summary>Details</summary>
Motivation: To generalize the extended Sobolev scale concept from compact manifolds and Euclidean spaces to non-compact manifolds of bounded geometry, and to study interpolation properties and operator mapping properties in this extended framework.

Method: Define the extended Sobolev scale H^φ(X) on manifolds of bounded geometry using RO-varying functions φ, analyze interpolation spaces between Sobolev spaces, study mapping properties of proper uniform pseudo-differential operators, and define extended A-scales using elliptic operators.

Result: Obtained complete description of Hilbert function spaces as interpolation spaces between Sobolev spaces, established mapping properties of PUPDOs on the extended Sobolev scale, and showed equivalence between extended Sobolev scale and extended A-scale defined via elliptic operators.

Conclusion: The extended Sobolev scale on manifolds of bounded geometry successfully generalizes properties from compact manifolds and Euclidean spaces, providing a unified framework for interpolation theory and operator analysis in non-compact geometric settings.

Abstract: Adapting the definition of ``extended Sobolev scale" on compact manifolds by
Mikhailets and Murach to the setting of a (generally non-compact) manifold of
bounded geometry $X$, we define the ``extended Sobolev scale" $H^{\varphi}(X)$,
where $\varphi$ is a function which is $RO$-varying at infinity. With the help
of the scale $H^{\varphi}(X)$, we obtain a description of all Hilbert
function-spaces that serve as interpolation spaces with respect to a pair of
Sobolev spaces $[H^{(s_0)}(X), H^{(s_1)}(X)]$, with $s_0<s_1$. We use this
interpolation property to establish a mapping property of proper uniform
pseudo-differential operators (PUPDOs) in the context of the scale
$H^{\varphi}(X)$. Additionally, using a first-order positive-definite PUPDO $A$
of elliptic type we define the ``extended $A$-scale" $H^{\varphi}_{A}(X)$ and
show that it coincides, up to norm equivalence, with the scale
$H^{\varphi}(X)$. Besides the mentioned results, we show that further
properties of the $H^{\varphi}$-scale, originally established by Mikhailets and
Murach on $\mathbb{R}^n$ and on compact manifolds, carry over to manifolds of
bounded geometry.

</details>


### [12] [State-Constrained Chemical Reactions: Discrete-to-Continuous Hamilton--Jacobi Equations and Large Deviations](https://arxiv.org/abs/2509.20747)
*Yuan Gao,Yuxi Han*

Main category: math.AP

TL;DR: The paper studies the macroscopic behavior of chemical reactions modeled as random time-changed Poisson processes, establishing convergence from discrete to continuous Hamilton-Jacobi equations and proving large deviation principles for state-constrained chemical reactions in bounded domains.


<details>
  <summary>Details</summary>
Motivation: To understand the limiting behavior of chemical reaction systems in bounded domains and establish rigorous mathematical connections between discrete stochastic models and their continuous macroscopic counterparts.

Method: Using WKB reformulation to transform the backward equation into discrete Hamilton-Jacobi equations with state constraints, then analyzing convergence as grid size tends to zero through reparametrization and variational representations.

Result: Shows that under suitable reparametrization, solutions of discrete Hamilton-Jacobi equations converge to solutions of continuous Hamilton-Jacobi equations with Neumann boundary conditions, and establishes the large deviation principle for rescaled chemical reaction processes.

Conclusion: The work provides a rigorous mathematical framework connecting discrete stochastic chemical reaction models to their continuous macroscopic limits, with applications to large deviation theory for state-constrained systems in bounded domains.

Abstract: We study the macroscopic behavior of chemical reactions modeled as random
time-changed Poisson processes on discrete state spaces. Using the WKB
reformulation, the backward equation of the rescaled process leads to a
discrete Hamilton--Jacobi equation with state constraints. As the grid size
tends to zero, the limiting solution and its associated variational
representation are closely connected to the good rate function of the large
deviation principle for state-constrained chemical reactions in the
thermodynamic limit. In this work, we focus on the limiting behavior of
discrete Hamilton--Jacobi equations defined on bounded domains with
state-constraint boundary conditions. For a single chemical reaction, we show
that, under a suitable reparametrization, the solution of the discrete
Hamilton--Jacobi equation converges to the solution of a continuous
Hamilton--Jacobi equation with a Neumann boundary condition. Building on this
convergence result and the associated variational representation, we establish
the large deviation principle for the rescaled chemical reaction process in
bounded domains.

</details>


### [13] [MIT bag in non-smooth convex domains](https://arxiv.org/abs/2509.20958)
*Konstantin Pankrashkin*

Main category: math.AP

TL;DR: The paper proves that Dirac operators with MIT bag boundary conditions are self-adjoint in convex domains and appear as limits of Dirac operators with large mass outside the domain, extending previous results from smooth to convex domains.


<details>
  <summary>Details</summary>
Motivation: Previous results on Dirac operators with MIT bag boundary conditions were limited to smooth domains. The authors aim to extend these results to more general convex domains.

Method: The authors analyze the Dirac operator with MIT bag boundary conditions in bounded convex domains, working in the H^1-setting to establish self-adjointness.

Result: The Dirac operator with MIT bag boundary condition is shown to be always self-adjoint in bounded convex domains. Additionally, such operators appear as limits of Dirac operators with large positive mass outside the domain.

Conclusion: The results extend the known theory from smooth domains to convex domains, establishing important mathematical properties of Dirac operators with MIT bag boundary conditions in more general geometric settings.

Abstract: The Dirac operator with MIT bag boundary condition in a bounded convex domain
is shown to be always self-adjoint in the $H^1$-setting. This allows one to
show that such operators appear as limit of Dirac operators with large positive
mass outside the domain. Similar results were previously known for smooth
domains only.

</details>


### [14] [A nonlocal Aw-Rascle-Zhang system with linear pressure term](https://arxiv.org/abs/2509.20973)
*Debora Amadori,Felisia Angela Chiarello,Gianmarco Cipollone*

Main category: math.AP

TL;DR: Nonlocal extension of Aw-Rascle-Zhang traffic model with convolution-based pressure term, analyzed using sticky particle approximation to construct entropy solutions.


<details>
  <summary>Details</summary>
Motivation: To capture nonlocal driver interactions in traffic flow modeling and align with Euler-alignment systems, extending classical traffic models to account for more realistic driver behavior.

Method: Sticky particle approximation to construct entropy solutions for cumulative density, with convergence analysis to weak solutions of the nonlocal system.

Result: Established well-posedness, stability estimates, and an entropic selection principle for the nonlocal traffic model.

Conclusion: Successfully developed a mathematical framework for nonlocal traffic flow models with rigorous analysis of solution properties and convergence.

Abstract: In this paper, we study a nonlocal extension of the Aw-Rascle-Zhang traffic
model, where the pressure-like term is modeled as a convolution between vehicle
density and a kernel function. This formulation captures nonlocal driver
interactions and aligns structurally with the Euler-alignment system studied in
[23]. Using a sticky particle approximation, we construct entropy solutions to
the equation for the cumulative density and prove convergence of approximate
solutions to weak solutions of the nonlocal system. The analysis includes
well-posedness, stability estimates, and an entropic selection principle.

</details>


### [15] [Graphical Willmore Problems with Low-Regularity Boundary and Dirichlet Data](https://arxiv.org/abs/2509.21018)
*Boris Gulyak*

Main category: math.AP

TL;DR: Existence and regularity results for boundary value problems of Willmore energy in graphical setting, with reduced regularity assumptions on boundaries and Dirichlet data.


<details>
  <summary>Details</summary>
Motivation: To establish existence and regularity for Willmore energy variation problems with clamped boundary conditions on surfaces with non-smooth boundaries.

Method: Linearization and fixed-point argument using divergence form of Willmore equation, applying weighted second-order Sobolev spaces.

Result: Solutions exist with boundary data in C^{1+α}-class, interior smoothness maintained, extended to Lipschitz boundaries using weighted Sobolev framework.

Conclusion: The approach successfully weakens regularity requirements and extends to other geometric PDEs like Helfrich and surface diffusion equations.

Abstract: We establish existence and regularity results for boundary value problems
arising from the first variation of the Willmore energy in the graphical
setting. Our focus lies on two-dimensional surfaces with fixed clamped boundary
conditions, embedded in three-dimensional Euclidean space, and represented as
graphs of height functions over domains with non-smooth boundaries. Our
approach involves constructing solutions through linearization and a
fixed-point argument, requiring small boundary data in suitable functional
spaces. Building on the results of Koch and Lamm \cite{koch2012geometric}, we
rewrite the Willmore equation for graphs in a divergence form that allows the
application of weighted second-order Sobolev spaces. This reformulation
significantly weakens the regularity assumptions on both the boundary and the
Dirichlet data, reducing them to the $C^{1+\alpha}$-class, while the solution
remains smooth in the interior. Moreover, we extend the existence theory to
domains with merely Lipschitz boundaries within a purely weighted Sobolev
framework. Our approach is also applicable to other higher-order geometric
PDEs, including the graphical Helfrich and surface diffusion equations.

</details>


### [16] [The Incompressible Navier-Stokes-Fourier Limits from Boltzmann-Fermi-Dirac Equation for Low Regularity Data](https://arxiv.org/abs/2509.21030)
*Ning Jiang,Chenchen Wang,Kai Zhou*

Main category: math.AP

TL;DR: This paper establishes the hydrodynamic limits of the quantum Boltzmann equation with Fermi-Dirac statistics, rigorously deriving the incompressible Navier-Stokes-Fourier equations from the BFD equation for hard sphere and hard potentials.


<details>
  <summary>Details</summary>
Motivation: To extend previous results by working with lower regularity initial data and establishing the lifespan equivalence between kinetic and fluid solutions.

Method: Analyzing the spectrum of the linearized collision operator combined with the transport operator and its associated semigroup, using fixed-point arguments with time iteration.

Result: Successfully verified the incompressible Navier-Stokes-Fourier limits from the BFD equation with lower regularity initial data than previous work.

Conclusion: The paper provides rigorous derivation of fluid dynamics limits from quantum kinetic equations and establishes that kinetic and fluid solutions have coinciding lifespans.

Abstract: We consider the hydrodynamic limits of the quantum Boltzmann equation with
Fermi-Dirac statistics for hard sphere and hard potentials in the whole space.
By analyzing the spectrum of the linearized collision operator combined with
the transport operator and its associated semigroup, the incompressible
Navier-Stokes-Fourier limits from the BFD equation is verified rigorously.
Compared to the results in [Jiang-Xiong-Zhou,J. Differ. Equ.,2022], this paper
works with a lower regularity for the initial data. In addition, the
fixed-point arguments together with a time iteration ensure us to obtain the
lifespan of kinetic solution coincides with those of limiting fluid solution.

</details>


### [17] [On the radius of spatial analyticity for the Majda-Biello and Hirota-Satsuma systems](https://arxiv.org/abs/2509.21095)
*Seongyeon Kim,Ihyeok Seo*

Main category: math.AP

TL;DR: First proof of spatial analyticity persistence for coupled KdV systems (Majda-Biello and Hirota-Satsuma) with analytic initial data.


<details>
  <summary>Details</summary>
Motivation: To establish whether spatial analyticity persists in coupled KdV systems, which hadn't been proven before for these specific systems.

Method: Investigation of persistence of spatial analyticity for solutions to Majda-Biello and Hirota-Satsuma systems with analytic initial data.

Result: Successfully proved that spatial analyticity persists in these coupled KdV systems.

Conclusion: This is the first result establishing analyticity persistence in coupled KdV systems, providing new insights into the mathematical properties of these systems.

Abstract: We investigate the persistence of spatial analyticity for solutions to the
Majda-Biello and Hirota-Satsuma systems with analytic initial data. This result
is the first to establish analyticity persistence in such coupled KdV systems.

</details>


### [18] [Lagrangian aspects of Yudovich theory for 2D Euler](https://arxiv.org/abs/2509.21121)
*Theodore D. Drivas,Joonhyun La*

Main category: math.AP

TL;DR: Establishes Yudovich's existence and uniqueness result for bounded and mildly unbounded vorticity weak solutions of 2D incompressible Euler equations, with additional regularity results for the solution map.


<details>
  <summary>Details</summary>
Motivation: To prove and extend Yudovich's classical result for 2D Euler equations, particularly for bounded and mildly unbounded vorticity cases, while also studying the regularity of the solution map.

Method: Mathematical analysis approach establishing existence and uniqueness of weak solutions for 2D incompressible Euler equations with bounded and mildly unbounded vorticity.

Result: Successfully proves Yudovich's existence and uniqueness theorem, and as a byproduct establishes regularity properties of the Yudovich solution map with respect to initial conditions and fluid domain.

Conclusion: The paper provides a comprehensive proof of Yudovich's result and extends understanding of solution map regularity for 2D Euler equations with bounded/mildly unbounded vorticity.

Abstract: In this note, we establish Yudovich's existence and uniqueness result for
bounded (as well as mildly unbounded) vorticity weak solution of the
two-dimensional incompressible Euler equations. As a biproduct of our proof, we
establish some regularity results for the Yudovich solution map as it depends
of the initial conditions and the fluid domain.

</details>


### [19] [Asymptotic instability for the forced Navier--Stokes equations in critical Besov spaces](https://arxiv.org/abs/2509.21272)
*Mikihiro Fujii,Hiroyuki Tsurumi*

Main category: math.AP

TL;DR: The paper shows that asymptotic stability of forced Navier-Stokes flows fails in critical Besov spaces for p ≥ n with n ≥ 3, and is unstable in all dimensions for n=2, due to nonlinear interactions from external forces.


<details>
  <summary>Details</summary>
Motivation: To investigate the asymptotic stability of forced Navier-Stokes flows in critical Besov spaces, particularly when standard stability arguments break down for certain parameter ranges.

Method: Mathematical analysis in critical Besov spaces, constructing counterexamples with small external forces that decay in time but produce oscillating Navier-Stokes flows that do not converge strongly.

Result: Asymptotic stability fails for p ≥ n with n ≥ 3, and forced Navier-Stokes flows are asymptotically unstable in all dimensions for n=2 in the critical Besov space framework.

Conclusion: The instability is caused by nonlinear interactions from external forces rather than linear effects, revealing limitations of classical stability arguments in certain Besov space frameworks.

Abstract: The asymptotic stability is one of the classical problems in the field of
mathematical analysis of fluid mechanics. In $\mathbb{R}^n$ with $n \geq 3$, it
is easily proved by the standard argument that if the given small external
force decays at temporal infinity, then the small forced Navier--Stokes flow
also strongly converges to zero as time tends to infinity in the framework of
the critical Besov spaces $\dot{B}_{p,q}^{n/p-1}(\mathbb{R}^n)$ with $1 \leq p
< n$ and $1 \leq q < \infty$. In the present paper, we show that this
asymptotic stability fails for $p \geq n$ with $n \geq 3$ in the sense that
there exist arbitrary small external forces whose critical Besov norm decays in
large time, whereas the corresponding Navier--Stokes flows oscillate and do not
strongly converge as $t \to \infty$ in the framework of the critical Besov
spaces $\dot{B}_{p,q}^{n/p-1}(\mathbb{R}^n)$. Moreover, we find that the
situation is different in the two-dimensional case $n=2$ and show the forced
Navier--Stokes flow is asymptotically unstable in
$\dot{B}_{p,1}^{2/p-1}(\mathbb{R}^2)$ for all $1 \leq p \leq \infty$. Our
instability does not appear in the linear level but is caused by the nonlinear
interaction from external forces.

</details>


### [20] [On the symmetry group for systems of conservation laws](https://arxiv.org/abs/2509.21283)
*Michael Sever*

Main category: math.AP

TL;DR: The paper argues that quasi-linear conservation laws are mainly useful for Euler fluid flow models in higher dimensions, and attempts to extend this class through symmetry group analysis.


<details>
  <summary>Details</summary>
Motivation: To investigate whether quasi-linear conservation laws have broader physical utility beyond Euler fluid flow systems, particularly in higher dimensions.

Method: Using symmetry group analysis to extend the class of systems that could serve as attractive physical models.

Result: The study provides qualified support for the conjecture that quasi-linear conservation laws are largely limited to Euler system models.

Conclusion: The utility of quasi-linear conservation laws as physical models appears to be primarily confined to Euler fluid flow systems in higher dimensions.

Abstract: A case can be made that the utility of quasi-linear systems of conservation
laws as physical models is largely limited to Euler system models of fluid
flow, at least in higher dimensions. Qualified corroboration of this conjecture
is obtained here, by attempt to extend the class of systems attractive as
physical models via the associated symmetry group.

</details>


### [21] [Some shape functionals for the $k$-Hessian equation](https://arxiv.org/abs/2509.21313)
*Alba Lia Masiello,Francesco Salerno*

Main category: math.AP

TL;DR: The paper studies Pólya type lower bounds for k-Torsional Rigidity associated with k-Hessian operators on convex sets, and provides quantitative estimates to investigate optimal sets.


<details>
  <summary>Details</summary>
Motivation: To establish lower bounds for k-Torsional Rigidity and understand which sets achieve optimality in these inequalities.

Method: Proving Pólya type lower bounds for k-Torsional Rigidity in any dimension, then providing two quantitative estimates to analyze optimal sets.

Result: Established Pólya type lower bounds for k-Torsional Rigidity and derived quantitative estimates for investigating optimal sets.

Conclusion: The paper successfully establishes lower bounds and provides tools to study optimal sets in Pólya type inequalities for k-Torsional Rigidity.

Abstract: For a non-empty, bounded, open, and convex set of class $C^2$, we consider
the Torsional Rigidity associated to the $k$-Hessian operator. We first prove
P\'olya type lower bound for the $k$-Torsional Rigidity in any dimension; then,
in order to investigate optimal sets in the P\'olya type inequality, we provide
two quantitative estimates.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [22] [MolCluster: Integrating Graph Neural Network with Community Detection for Coarse-Grained Mapping](https://arxiv.org/abs/2509.20893)
*Zhixuan Zhong,Linbo Ma,Jian Jiang*

Main category: physics.comp-ph

TL;DR: MolCluster is an unsupervised model that uses graph neural networks and community detection for coarse-grained molecular mapping, enabling customizable resolution without labeled data.


<details>
  <summary>Details</summary>
Motivation: Traditional coarse-grained methods rely on fixed mapping rules that limit adaptability and require manual intervention, while supervised learning approaches suffer from limited labeled datasets and inability to control mapping resolution.

Method: Integrates graph neural network with community detection algorithm, uses predefined group pair loss to preserve target groups, and implements bisection strategy for customizable resolution across molecular systems.

Result: Outperforms both traditional clustering and supervised models on MARTINI2 dataset, benefiting from label-free pretraining strategy.

Conclusion: MolCluster shows potential as a core model for customizable and chemically consistent coarse-grained mapping, overcoming limitations of existing approaches.

Abstract: Coarse-grained (CG) modeling simplifies molecular systems by mapping groups
of atoms into representative units. However, traditional CG approaches rely on
fixed mapping rules, which limit their ability to handle diverse chemical
systems and require extensive manual intervention. Thus, supervised
learning-based CG methods have been proposed, enabling more automated and
adaptable mapping. Nevertheless, these methods suffer from limited labeled
datasets and the inability to control mapping resolution, which is essential
for multiscale modeling. To overcome these limitations, we propose MolCluster,
an unsupervised model that integrates a graph neural network and a community
detection algorithm to extract CG representations. Additionally, a predefined
group pair loss ensures the preservation of target groups, and a bisection
strategy enables precise, customizable resolution across different molecular
systems. In the case of the downstream task, evaluations on the MARTINI2
dataset demonstrate that MolCluster, benefiting from its label-free pretraining
strategy, outperforms both traditional clustering and supervised models.
Overall, these results highlight the potential of MolCluster as a core model
for customizable and chemically consistent CG mapping.

</details>


### [23] [A Reformulation of UVN-Flash for Multicomponent Two-Phase Systems with Application to CO2-rich Mixture Transport in Pipelines](https://arxiv.org/abs/2509.20965)
*Pardeep Kumar,Patricio I. Rosen Esquivel*

Main category: physics.comp-ph

TL;DR: A unified framework for two-phase multicomponent transport in CO2 pipelines using homogeneous equilibrium model with Helmholtz energy-based equation of state and novel UVN-flash routine for phase equilibrium calculations.


<details>
  <summary>Details</summary>
Motivation: Accurate modeling of dense-phase CO2-rich mixtures transport in CCS requires coupling fluid dynamics and thermodynamics, especially during transient events like depressurization.

Method: Homogeneous equilibrium model (HEM) for two-phase transport with thermodynamic closure via Helmholtz energy-based equation of state, using UVN-flash with stability analysis and novel tailored UVN-flash routine with better-scaled variables.

Result: The framework is successfully applied to depressurization of tanks and pipelines containing CO2-rich mixtures.

Conclusion: The proposed framework demonstrates effectiveness for CCS-relevant applications in modeling two-phase multicomponent transport in pipelines.

Abstract: Pipeline transport of dense-phase CO2-rich mixtures is a crucial component in
carbon capture and storage (CCS). Accurate modeling requires coupling of fluid
dynamics and thermodynamics, especially during transient events such as
depressurization. In this work, we present a unified framework for two-phase
multicomponent transport in pipelines that integrates both aspects.
Specifically, we employ the homogeneous equilibrium model (HEM) for modeling
the transport of two-phase CO2-rich mixture, with thermodynamic closure
provided by a Helmholtz energy-based equation of state. Phase equilibrium
calculations are performed using UVN-flash, supplemented with a stability
analysis procedure to detect phase separation and generate initial guesses for
the phase-equilibrium calculations. Specifically, we introduce a novel tailored
UVN-flash routine that aligns with the fluid dynamics formulation. This is
achieved by introducing an alternative and better-scaled set of variables for
the phase-equilibrium calculations. The proposed framework is applied to the
depressurization of tanks and pipelines containing CO2-rich mixtures,
demonstrating its effectiveness for CCS-relevant applications.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [24] [Anderson self-localization of light in pair plasmas](https://arxiv.org/abs/2509.20594)
*Maxim Lyutikov,Victor Gurarie*

Main category: physics.plasm-ph

TL;DR: Anderson self-localization occurs in pair plasma for weakly nonlinear electromagnetic waves due to self-created random density fluctuations, leading to wave reflection and formation of bright trapped pockets.


<details>
  <summary>Details</summary>
Motivation: To understand how electromagnetic waves behave in pair plasma and investigate the phenomenon of Anderson self-localization in this context, with potential applications to astrophysical Fast Radio Bursts.

Method: Study weakly nonlinear electromagnetic waves in pair plasma where the beat between driver and back-scattered waves creates random density fluctuations and dielectric permittivity variations, leading to wave localization in quasi-1D propagation.

Result: Waves experience Anderson self-localization resulting in reflection of EM waves by under-dense pair plasma and separation into bright trapped pockets and dark regions. Mild thermal spread restores propagation by suppressing parametric instabilities. Circular polarization produces linearly polarized structures with random position angles.

Conclusion: Anderson self-localization is demonstrated in pair plasma for weakly nonlinear EM waves, creating localized wave structures with implications for understanding astrophysical phenomena like Fast Radio Bursts.

Abstract: We demonstrate that in pair plasma weakly nonlinear electromagnetic waves,
$a_0 \leq 1$, experience Anderson self-localization. The beat between the
driver and a back-scattered wave creates random, charge-neutral, large density
fluctuations $\delta \rho/\rho \gg 1$, and corresponding random fluctuations of
the dielectric permittivity $\epsilon$. Propagating in quasi-1D, waves in a
medium with spatially random, time-varying, self-created fluctuations of
dielectric permeability experience localization. Anderson self-localization of
light leads to (i) reflection of EM waves by the under-dense pair plasma; (ii)
a wave already present inside the plasma separates into bright trapped pockets
and dark regions. Mild initial thermal spread restores wave propagation by
suppressing the seeds of parametrically unstable density fluctuations. A
circularly polarized driver produces linearly polarized structures, with
position angle varying randomly between the bright pulses. We discuss possible
applications to astrophysical Fast Radio Bursts.

</details>


### [25] [Reply to "Comments to Marvel Fusions Mixed Fuels Reactor Concept"](https://arxiv.org/abs/2509.21048)
*Hartmut Ruhl,Georg Korn*

Main category: physics.plasm-ph

TL;DR: This paper refutes Lackner et al.'s conclusion that mixed fuel ignition is impossible, showing that proper inclusion of ionic α-stopping and neutron stopping makes ignition possible.


<details>
  <summary>Details</summary>
Motivation: To correct the incorrect conclusion by Lackner et al. that mixed fuel ignition is impossible due to radiation losses and reduced electronic α-stopping.

Method: Extended the α-stopping model to include ionic α-stopping and neutron stopping, which were neglected in the previous analysis.

Result: Shows that ionic α-stopping and neutron stopping lead to elevated ion temperatures (kT_i > kT_e), making mixed fuel ignition possible.

Conclusion: Ignition of mixed fuels is indeed possible with far-reaching implications, contrary to Lackner et al.'s analysis.

Abstract: In "arXiv:2312.13429" Lackner et al. use standard methods to decide if it is
possible to ignite mixed fuels. They correctly identify that the increased
radiation losses make ignition significantly more challenging than for pure DT
fuels, since this leads to higher ignition temperatures. Further, they conclude
that at those temperatures the reduced electronic $\alpha$-stopping makes
ignition impossible. We show that this conclusion is not correct. The model
used for $\alpha$-stopping by Lackner et al. is only approximately correct for
low temperatures and hydrogen isotopes. By extending the $\alpha$-stopping
model to include ionic $\alpha$-stopping we show in \cite{ruhlkornarXiv5} that
the contribution of ionic $\alpha$-particle stopping cannot be neglected. The
ionic $\alpha$-stopping together with the neutron stopping, which is also
neglected by Lackner et al., lead to elevated ion temperatures implying $kT_i >
kT_e$. Those three effects combined lead us to the conclusion, that ignition of
mixed fuels is indeed possible with far reaching implications, contrary to the
analysis by Lackner.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [26] [Accelerating the Monte Carlo simulation of the Enskog equation for multiscale dense gas flows](https://arxiv.org/abs/2509.20816)
*Bin Hu,Liyan Luo,Lei Wu*

Main category: physics.flu-dyn

TL;DR: A general synthetic iterative scheme for solving the Enskog equation using Monte Carlo methods, featuring rapid convergence and asymptotic-preserving properties for near-continuum flows.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient method for solving the Enskog equation that can handle near-continuum flows with spatial cell sizes larger than the mean free path, reducing computational costs.

Method: Mesoscopic-macroscopic two-way coupling: Monte Carlo simulation provides high-order constitutive relations for moment equations, while macroscopic synthetic equations guide particle evolution in Monte Carlo.

Result: Verified accuracy in shock wave and heat transfer problems; demonstrated fast convergence and asymptotic-preserving properties in Poiseuille flow, hypersonic cylinder flow, and porous media flow, with simulation time reduced by orders of magnitude.

Conclusion: The proposed scheme enables efficient simulation of near-continuum flows and allows analysis of adsorption layer effects in porous media flow, relevant to shale gas extraction.

Abstract: A general synthetic iterative scheme is proposed to solve the Enskog equation
within a Monte Carlo framework. The method demonstrates rapid convergence by
reducing intermediate Monte Carlo evolution and preserves the
asymptotic-preserving property, enabling spatial cell sizes much larger than
the mean free path in near-continuum flows. This is realized through
mesoscopic-macroscopic two-way coupling: the mesoscopic Monte Carlo simulation
provides high-order constitutive relations to close the moment (synthetic)
equation, while the macroscopic synthetic equation, once solved toward steady
state, directs the evolution of simulation particles in the Monte Carlo method.
The accuracy of the proposed general synthetic iterative scheme is verified
through one-dimensional normal shock wave and planar Fourier heat transfer
problems, while its fast-converging and asymptotic-preserving properties are
demonstrated in the force-driven Poiseuille flow and two-dimensional hypersonic
cylinder flow and low-speed porous media flow, where the simulation time is
reduced by several orders of magnitude in near-continuum flows. With the
proposed method, a brief analysis is conducted on the role of the adsorption
layer in porous media flow, mimicking shale gas extraction.

</details>


### [27] [A Fourier/Modal-Spectral-Element Method for the Simulation of High-Reynolds Number Incompressible Stratified Flows in Domains with a Single Non-Periodic Direction](https://arxiv.org/abs/2509.20833)
*Nidia Reyes-Gil,Greg Thomsen,Kristopher Rowe,Peter Diamessis*

Main category: physics.flu-dyn

TL;DR: A high-order accurate Navier-Stokes solver for simulating high-Reynolds-number stratified flows, using Fourier pseudo-spectral method horizontally and modal spectral element discretization vertically with implicit-explicit time stepping.


<details>
  <summary>Details</summary>
Motivation: To address numerical and computational challenges in simulating high-Reynolds-number stratified turbulent flows typically observed in oceanic and atmospheric environments, including thin regions of high vertical shear, layered turbulence, and internal wave radiation.

Method: Uses Fourier pseudo-spectral method in horizontal direction and modal spectral element discretization in vertical direction. Employs implicit-explicit time discretization scheme solving one-dimensional Helmholtz problems, with static condensation and modal boundary-adapted basis functions resulting in efficient tridiagonal system solutions.

Result: The solver demonstrates robustness through benchmark studies including 2D and 3D problems, successfully simulating a turbulent stratified wake generated by a sphere in linear stratification.

Conclusion: The proposed numerical model effectively facilitates reproduction of stratified turbulent fluid dynamics at high Reynolds numbers, providing an efficient computational approach for simulating complex oceanic and atmospheric flow phenomena.

Abstract: We present the components of a high-order accurate Navier-Stokes solver
designed to simulate high-Reynolds-number stratified flows. The proposed
numerical model addresses some of the numerical and computational challenges
that high-Reynolds-number simulations pose, facilitating the reproduction of
stratified turbulent fluid dynamics typically observed in oceanic and
atmospheric flows, namely the development of thin regions of high vertical
shear, strongly layered turbulence at high Reynolds numbers and internal wave
radiation. This Navier-Stokes solver utilizes a Fourier pseudo-spectral method
in the horizontal direction and a modal spectral element discretization in the
vertical. We adopt an implicit-explicit time discretization scheme that
involves solving several one-dimensional Helmholtz problems at each time step.
Static condensation and modal boundary-adapted basis functions result in an
inexpensive algorithm based on solving many small tridiagonal systems. A series
of benchmark studies is presented to demonstrate the robustness of the flow
solver. These include two-dimensional and three-dimensional problems,
concluding with a turbulent stratified wake generated by a sphere in linear
stratification.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [28] [A relativistic coupled-cluster treatment of magnetic hyperfine structure of the $X^2Π$ and $A^2Σ^+$ states of OH isotopologues](https://arxiv.org/abs/2509.20522)
*D. P. Usov,Y. S. Kozhedub,A. V. Stolyarov,L. V. Skripnikov,V. M. Shabaev,I. I. Tupitsyn*

Main category: physics.chem-ph

TL;DR: Ab initio calculations of magnetic dipole hyperfine structure constants for OH radical isotopologues using relativistic coupled-cluster methods, showing excellent agreement with experimental data.


<details>
  <summary>Details</summary>
Motivation: To provide accurate theoretical predictions of hyperfine structure constants for OH radical isotopologues, extending beyond previous studies and supporting analysis of higher rovibrational levels.

Method: Four-component relativistic coupled-cluster method with excitations up to triple level (CCSD(T) and CCSDT), calculating HFS functions over internuclear distance range R ∈ [0.6, 1.8] Å for ground X²Π and excited A²Σ⁺ states.

Result: Most accurate theoretical HFS predictions to date, with excellent agreement (within 1%) with experimental data for lowest vibrational levels, and extended internuclear distance range coverage.

Conclusion: The study provides robust foundation for accurate HFS treatments of higher rovibrational levels in OH isotopologues, demonstrating reliability of relativistic coupled-cluster methods for hyperfine structure calculations.

Abstract: $\textit{Ab initio}$ calculations of the parallel component of the magnetic
dipole hyperfine structure (HFS) constant have been carried out for hydroxyl
radical isotopologues ($^{16,17}$OH(D)) over the internuclear distance range $R
\in [0.6, 1.8]$ \r{A}. For the ground electronic state $X^2\Pi$, the HFS
functions were evaluated for contributions induced by both oxygen and hydrogen
nuclei. In addition, the hydrogen-induced HFS curve was calculated for the
excited $A^2\Sigma^+$ state. The quantum-chemistry study employs a
four-component relativistic coupled-cluster (CC) method, including excitations
up to the triple level, namely: the contribution of triple-cluster amplitudes
was studied both perturbatively (CCSD(T)) and through fully iterative
calculations (CCSDT). The resulting oxygen- and hydrogen-induced HFS functions
represent the most accurate and reliable theoretical predictions to date
exhibiting excellent agreement with semiempirical curve for hydrogen-induced
HFS derived from high-resolution spectroscopic data for the lowest vibrational
levels ($v\in [0,2]$) of the electronic $X^2\Pi$ state. Vibrationally averaged
$\textit{ab initio}$ values are consistent with experimental values within
$1\%$ for all states considered. Furthermore, the internuclear distance range
over which the HFS curves are defined has been extended beyond that of previous
studies, thereby providing a robust foundation for accurate HFS treatments of
higher-lying rovibrational levels of OH isotopologues within both adiabatic and
non-adiabatic frameworks.

</details>


<div id='q-bio.TO'></div>

# q-bio.TO [[Back]](#toc)

### [29] [Data-driven Neural Networks for Windkessel Parameter Calibration](https://arxiv.org/abs/2509.21206)
*Benedikt Hoock,Tobias Köppl*

Main category: q-bio.TO

TL;DR: A neural network-based method for calibrating Windkessel parameters in 1D-0D blood flow models using simulated brachial artery pressure data, with extensions for handling unknown measurement locations and noisy data.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient and accurate method for calibrating Windkessel parameters in coupled blood flow models that can handle practical challenges like unknown measurement locations and noisy data.

Method: Design a data-driven neural network trained on simulated blood pressures in the left brachial artery, then extend it with dummy neurons and retrain for parameter calibration on measured pulse waves.

Result: The neural network emulates pressure pulse waves across the entire simulated domain with negligible error and computational effort.

Conclusion: The method effectively calibrates Windkessel parameters even when measurement locations are unknown or data are affected by noise.

Abstract: In this work, we propose a novel method for calibrating Windkessel (WK)
parameters in a dimensionally reduced 1D-0D coupled blood flow model. To this
end, we design a data-driven neural network (NN)trained on simulated blood
pressures in the left brachial artery. Once trained, the NN emulates the
pressure pulse waves across the entire simulated domain, i.e., over time, space
and varying WK parameters, with negligible error and computational effort. To
calibrate the WK parameters on a measured pulse wave, the NN is extended by
dummy neurons and retrained only on these. The main objective of this work is
to assess the effectiveness of the method in various scenarios -- particularly,
when the exact measurement location is unknown or the data are affected by
noise.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [30] [Explicit and Effectively Symmetric Schemes for Neural SDEs](https://arxiv.org/abs/2509.20599)
*Daniil Shmelev,Cristopher Salvi*

Main category: cs.LG

TL;DR: Introduces stable, near-reversible Runge-Kutta schemes (EES) for neural SDEs that achieve memory-efficient training with accurate gradients, overcoming instability issues of existing reversible solvers.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for backpropagation through neural SDE solvers face trade-offs: discretise-then-optimise has accurate gradients but high memory costs, while optimise-then-discretise has constant memory but suffers from gradient approximation errors and slower evaluation.

Method: Developed a novel class of Explicit and Effectively Symmetric (EES) Runge-Kutta schemes that are stable and near-reversible, retaining memory efficiency while overcoming the instability issues of existing reversible solvers like the Reversible Heun scheme.

Result: Numerical experiments demonstrate superior stability and reliability of EES schemes compared to existing methods, enabling memory-efficient training without severe restrictions on step size or model complexity.

Conclusion: EES schemes provide a practical foundation for scalable and accurate training of neural SDEs, offering both memory efficiency and gradient accuracy while maintaining stability under complex models and large step sizes.

Abstract: Backpropagation through (neural) SDE solvers is traditionally approached in
two ways: discretise-then-optimise, which offers accurate gradients but incurs
prohibitive memory costs due to storing the full computational graph (even when
mitigated by checkpointing); and optimise-then-discretise, which achieves
constant memory cost by solving an auxiliary backward SDE, but suffers from
slower evaluation and gradient approximation errors. Algebraically reversible
solvers promise both memory efficiency and gradient accuracy, yet existing
methods such as the Reversible Heun scheme are often unstable under complex
models and large step sizes. We address these limitations by introducing a
novel class of stable, near-reversible Runge--Kutta schemes for neural SDEs.
These Explicit and Effectively Symmetric (EES) schemes retain the benefits of
reversible solvers while overcoming their instability, enabling
memory-efficient training without severe restrictions on step size or model
complexity. Through numerical experiments, we demonstrate the superior
stability and reliability of our schemes, establishing them as a practical
foundation for scalable and accurate training of neural SDEs.

</details>


### [31] [Latent Twins](https://arxiv.org/abs/2509.20615)
*Matthias Chung,Deepanshu Verma,Max Collins,Amit N. Subrahmanya,Varuni Katti Sastry,Vishwas Rao*

Main category: cs.LG

TL;DR: Latent Twins is a unifying mathematical framework that creates hidden surrogates in latent space for underlying equations, bridging representation learning and algorithmic solution methods in scientific machine learning.


<details>
  <summary>Details</summary>
Motivation: Current scientific machine learning approaches have progressed in parallel with representation learning and algorithmic solution methods evolving as separate pipelines, lacking a unified framework.

Method: Creates a hidden surrogate in latent space governed by operators, where classical modeling, inversion, model reduction, and operator approximation emerge as special cases of a single principle.

Result: Demonstrated across canonical ODEs, PDE benchmarks with shallow-water equations, and real-data geopotential reanalysis, showing compact, interpretable surrogates that evaluate across arbitrary time gaps in single-shot.

Conclusion: Latent Twins provide scalable, theory-grounded surrogates that bridge data-driven representation learning and classical scientific modeling across disciplines, remaining compatible with scientific pipelines like assimilation and uncertainty quantification.

Abstract: Over the past decade, scientific machine learning has transformed the
development of mathematical and computational frameworks for analyzing,
modeling, and predicting complex systems. From inverse problems to numerical
PDEs, dynamical systems, and model reduction, these advances have pushed the
boundaries of what can be simulated. Yet they have often progressed in
parallel, with representation learning and algorithmic solution methods
evolving largely as separate pipelines. With \emph{Latent Twins}, we propose a
unifying mathematical framework that creates a hidden surrogate in latent space
for the underlying equations. Whereas digital twins mirror physical systems in
the digital world, Latent Twins mirror mathematical systems in a learned latent
space governed by operators. Through this lens, classical modeling, inversion,
model reduction, and operator approximation all emerge as special cases of a
single principle. We establish the fundamental approximation properties of
Latent Twins for both ODEs and PDEs and demonstrate the framework across three
representative settings: (i) canonical ODEs, capturing diverse dynamical
regimes; (ii) a PDE benchmark using the shallow-water equations, contrasting
Latent Twin simulations with DeepONet and forecasts with a 4D-Var baseline; and
(iii) a challenging real-data geopotential reanalysis dataset, reconstructing
and forecasting from sparse, noisy observations. Latent Twins provide a
compact, interpretable surrogate for solution operators that evaluate across
arbitrary time gaps in a single-shot, while remaining compatible with
scientific pipelines such as assimilation, control, and uncertainty
quantification. Looking forward, this framework offers scalable,
theory-grounded surrogates that bridge data-driven representation learning and
classical scientific modeling across disciplines.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [32] [Gaussian splatting holography](https://arxiv.org/abs/2509.20774)
*Shuhe Zhang,Liangcai Cao*

Main category: physics.optics

TL;DR: GSH uses Gaussian splatting to suppress twin-image artifacts in in-line holography by compressing unknown parameters up to 15x, transforming ill-posed phase retrieval into well-posed reconstruction with reduced phase ambiguities.


<details>
  <summary>Details</summary>
Motivation: In-line holography suffers from twin-image artifacts due to Hermitian symmetry, causing phase ambiguities in ill-posed phase retrieval problems where known parameters are fewer than unknowns.

Method: Gaussian splatting holography (GSH) uses Gaussian splatting for optical field representation, compressing unknown parameters by up to 15 folds and leveraging Gaussian spatial profiles to form sharp patterns rather than noisy twin-image backgrounds.

Result: GSH achieves constraint-free recovery with accuracy comparable to state-of-the-art constraint-based methods (PSNR=26 dB, SSIM=0.8). With total variation, it reaches PSNR=31 dB and maintains 15x compression ability.

Conclusion: GSH effectively suppresses twin-image artifacts in in-line holography through parameter compression and Gaussian representation, enabling accurate reconstruction without constraints while maintaining high compression capability.

Abstract: In-line holography offers high space-bandwidth product imaging with a
simplified lens-free optical system. However, in-line holographic
reconstruction is troubled by twin images arising from the Hermitian symmetry
of complex fields. Twin images disrupt the reconstruction in solving the
ill-posed phase retrieval problem. The known parameters are less than the
unknown parameters, causing phase ambiguities. State-of-the-art deep-learning
or non-learning methods face challenges in balancing data fidelity with
twin-image disturbance. We propose the Gaussian splatting holography (GSH) for
twin-image-suppressed holographic reconstruction. GSH uses Gaussian splatting
for optical field representation and compresses the number of unknown
parameters by a maximum of 15 folds, transforming the original ill-posed phase
retrieval into a well-posed one with reduced phase ambiguities. Additionally,
the Gaussian splatting tends to form sharp patterns rather than those with
noisy twin-image backgrounds as each Gaussian has a spatially slow-varying
profile. Experiments show that GSH achieves constraint-free recovery for
in-line holography with accuracy comparable to state-of-the-art
constraint-based methods, with an average peak signal-to-noise ratio equal to
26 dB, and structure similarity equal to 0.8. Combined with total variation,
GSH can be further improved, obtaining a peak signal-to-noise ratio of 31 dB,
and a high compression ability of up to 15 folds.

</details>


### [33] [Fast 3D Nanophotonic Inverse Design using Volume Integral Equations](https://arxiv.org/abs/2509.20809)
*Amirhossein Fallah,Constantine Sideris*

Main category: physics.optics

TL;DR: A volume integral equation (VIE)-based forward modeling approach is introduced for efficient inverse design of nanophotonic devices, offering orders of magnitude computational improvement over traditional finite-difference methods.


<details>
  <summary>Details</summary>
Motivation: The complexity and precision requirements of modern optical technologies demand minimal human intervention in nanophotonic device design. Traditional inverse design methods using conventional electromagnetic solvers are computationally expensive due to the substantial electrical size and subwavelength characteristics of nanophotonic structures.

Method: Developed a forward modeling approach based on volume integral equation (VIE) formulation, derived an adjoint method specifically for VIE framework to compute optimization gradients efficiently, and introduced a novel unidirectional mode excitation strategy compatible with VIE solvers.

Result: Comparative benchmarks show multiple orders of magnitude improvement in computational efficiency over conventional finite-difference methods in both time and frequency domains. Successfully designed two representative nanophotonic components: a selective mode reflector and a 3 dB power splitter.

Conclusion: The VIE-based framework offers significant runtime advantages and shows promising potential for accelerating inverse design workflows for next-generation nanophotonic devices.

Abstract: Designing nanophotonic devices with minimal human intervention has gained
substantial attention due to the complexity and precision required in modern
optical technologies. While inverse design techniques typically rely on
conventional electromagnetic solvers as forward models within optimization
routines, the substantial electrical size and subwavelength characteristics of
nanophotonic structures necessitate significantly accelerated simulation
methods. In this work, we introduce a forward modeling approach based on the
volume integral equation (VIE) formulation as an efficient alternative to
traditional finite-difference (FD)-based methods. We derive the adjoint method
tailored specifically for the VIE framework to efficiently compute optimization
gradients and present a novel unidirectional mode excitation strategy
compatible with VIE solvers. Comparative benchmarks demonstrate that our
VIE-based approach provides multiple orders of magnitude improvement in
computational efficiency over conventional FD methods in both time and
frequency domains. To validate the practical utility of our approach, we
successfully designed two representative nanophotonic components: a selective
mode reflector and a 3 dB power splitter. Our results underscore the
significant runtime advantages offered by the VIE-based framework, highlighting
its promising role in accelerating inverse design workflows for next-generation
nanophotonic devices.

</details>


<div id='physics.ins-det'></div>

# physics.ins-det [[Back]](#toc)

### [34] [Physics Informed Neural Networks for design optimisation of diamond particle detectors for charged particle fast-tracking at high luminosity hadron colliders](https://arxiv.org/abs/2509.21123)
*Alessandro Bombini,Alessandro Rosa,Clarissa Buti,Giovanni Passaleva,Lucio Anderlini*

Main category: physics.ins-det

TL;DR: 3D diamond pixel sensors with laser-processed electrodes suffer from signal delay due to high electrode resistivity, requiring extended Ramo-Shockley formalism and numerical modeling to assess timing performance.


<details>
  <summary>Details</summary>
Motivation: Future high-luminosity hadron colliders need radiation-hard, precise tracking detectors with sub-nanosecond timing, but conductive electrodes in 3D diamond sensors cause signal propagation delays.

Method: Extended Ramo-Shockley formalism via 3rd-order 3+1D PDE derived from Maxwell's equations, solved numerically and coupled with charge transport simulations; used Mixture-of-Experts Physics-Informed Neural Network trained on Spectral Method data.

Result: Developed a meshless solver to evaluate timing degradation caused by electrode resistance in realistic 3D sensor geometries.

Conclusion: The proposed modeling approach enables assessment of timing performance in 3D diamond pixel sensors, addressing signal delay issues from resistive electrodes for future collider applications.

Abstract: Future high-luminosity hadron colliders demand tracking detectors with
extreme radiation tolerance, high spatial precision, and sub-nanosecond timing.
3D diamond pixel sensors offer these capabilities due to diamond's radiation
hardness and high carrier mobility. Conductive electrodes, produced via
femtosecond IR laser pulses, exhibit high resistivity that delays signal
propagation. This effect necessitates extending the classical Ramo-Shockley
weighting potential formalism. We model the phenomenon through a 3rd-order,
3+1D PDE derived as a quasi-stationary approximation of Maxwell's equations.
The PDE is solved numerically and coupled with charge transport simulations for
realistic 3D sensor geometries. A Mixture-of-Experts Physics-Informed Neural
Network, trained on Spectral Method data, provides a meshless solver to assess
timing degradation from electrode resistance.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [35] [Strain localization in reduced order asymptotic homogenization](https://arxiv.org/abs/2509.16210)
*Harpreet Singh,Puneet Mahajan*

Main category: cs.CE

TL;DR: A reduced order asymptotic homogenization multiscale technique for capturing damage and inelastic effects in composites, using eigen strain representation and continuum damage mechanics at microscale with crack band energy dissipation.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient multiscale method that can accurately capture both damage and inelastic effects in composite materials while reducing computational costs.

Method: Two-scale homogenization with eigen strain representation for inelastic response, order reduction techniques, RVE analysis for influence tensors, continuum damage mechanics at microscale, and crack band energy dissipation for strain localization.

Result: The formulation successfully predicts macroscale response and captures damage and plasticity-induced inelastic strains, though spurious post-failure stiffness issues were identified.

Conclusion: The proposed reduced order multiscale technique effectively captures composite material damage and inelastic behavior while addressing computational efficiency, though higher-order solutions may be needed to eliminate artificial stiffness artifacts.

Abstract: A reduced order asymptotic homogenization based multiscale technique which
can capture damage and inelastic effects in composite materials is proposed.
This technique is based on two scale homogenization procedure where eigen
strain representation accounts for the inelastic response and the computational
efforts are alleviated by reduction of order technique. Macroscale stress is
derived by calculating the influence tensors from the analysis of
representative volume element (RVE). At microscale, the damage in the material
is modeled using continuum damage mechanics (CDM) based framework. To solve the
problem of strain localization a method of the alteration of stress-strain
relation of micro con- stituents based on the dissipated fracture energy in a
crack band is implemented. The issue of spurious post failure artificial
stiffness at macroscale is discussed and effect of increasing the order to
alleviate this problem is checked. Verification studies demonstrated the
proposed formulation predicts the macroscale response and also captures the
damage and plasticity induced inelastic strains.

</details>


### [36] [E$^2$-TFA based multiscale analysis of failure in elasto-plastic composites](https://arxiv.org/abs/2509.16211)
*Harpreet Singh*

Main category: cs.CE

TL;DR: A novel homogenization method called E²-TFA for analyzing failure in elastoplastic composites using elastic and eigen influence tensors, which improves computational efficiency and accuracy in capturing damage and inelastic deformations.


<details>
  <summary>Details</summary>
Motivation: To address the drawback of TFA-based methods in post-damage stiffness response and provide more realistic computations of composite material failure by considering microscopic eigenstrain fields.

Method: Uses elastic and eigen transformation functions with reduced order modeling and piecewise constant eigenstrain field throughout subdomains. Assesses performance through RVE simulations and various composites under complex load histories.

Result: The method accurately captures damage and inelastic deformations, shows realistic computations by alleviating post-damage stiffness issues, and successfully calculates nonlinear shear stress-strain response matching experimental fracture parameters.

Conclusion: E²-TFA can accurately and efficiently estimate the mechanical response of composite materials by capturing damage and inelastic deformations better than previous methods.

Abstract: This paper describes a novel homogenization methodology for analyzing the
failure of elastoplastic composite materials based on elastic and eigen
influence tensors-driven transformation field analysis ($\mathtt{E}^2$-TFA).
The proposed technique considers the microscopic eigenstrain field accounting
for intra-phase damage and inelastic strains. This results in realistic
computations by alleviating the post-damage stiffness response, which is a
drawback of TFA-based methods. We attain computational efficiency by
identifying the preprocessing data solely from the elastic and eigen
transformation functions and adopting a reduced order modelling technique with
a piecewise constant eigenstrain field throughout the subdomains. The
performance of the model is assessed by simulating the response for (a) the
representative volume element (RVE) as a homogenized continuum and (b) the
various composites under complex load histories with intricate macroscale
morphologies. Furthermore, the nonlinear shear stress-strain response of a
glass fiber composite is calculated and compared to experimentally measured
fracture initiation parameters, failure plane orientation, and strain
histories. Finally, we show that $\mathtt{E}^2$-TFA can accurately and
efficiently capture damage and inelastic deformations in order to estimate the
mechanical response of composite materials in a better way.

</details>


### [37] [Characterizing failure morphologies in fiber-reinforced composites via k-means clustering based multiscale framework](https://arxiv.org/abs/2509.20011)
*Harpreet Singh*

Main category: cs.CE

TL;DR: A novel homogenization method using damage informed transformation field analysis (D-TFA) for predicting failure in fiber-reinforced composites, featuring computational efficiency through reduced-order modeling and k-means clustering.


<details>
  <summary>Details</summary>
Motivation: To develop a more realistic and computationally efficient approach for analyzing failure in fiber-reinforced composite materials, improving predictions of mechanical behavior and damage patterns.

Method: Uses elastic and eigen influence tensors within D-TFA framework, employs k-means clustering based on strain distributions to partition microscale domain, implements reduced-order modeling for efficiency, and validates through RVE simulations and comparisons with FEM.

Result: D-TFA accurately captures damage patterns and directional strengths, provides improved predictions of mechanical behavior, and shows that higher cluster counts are crucial for accurate stress-strain response in complex microstructures.

Conclusion: The proposed D-TFA framework effectively simulates composite material failure with good accuracy and computational efficiency, demonstrating its potential for practical applications in composite material analysis.

Abstract: A novel homogenization methodology is proposed for analyzing the failure of
fiber-reinforced composite materials, utilizing elastic and eigen influence
tensors within a damage informed transformation field analysis (D-TFA)
framework. This approach includes a technique for calculating macroscopic
damage under uniform stress and strain conditions, offering more realistic
simulations. Computational efficiency is enhanced through a reduced-order
modeling strategy, while elastic and eigen strain distribution driven k-means
clustering methods are employed to partition the microscale domain. The model's
performance is assessed by simulating the response of a representative volume
element (RVE) treated as a homogenized continuum. Subsequently, a comparative
assessment is carried out to check the efficacy of two clustering schemes.
Damage morphologies are calculated using proposed framework and compared with
predictions obtained using finite element method. Furthermore, open-hole
specimen tests are simulated and failure paths are predicted for the domains
with different fiber layups. Ultimately, we show that D-TFA can accurately
capture damage patterns and directional strengths, providing improved
predictions of the mechanical behavior of composite materials. It has been
demonstrated that higher cluster counts are crucial for capturing a more
accurate stress-strain response, especially for complex microstructures.

</details>


### [38] [Extrapolating Phase-Field Simulations in Space and Time with Purely Convolutional Architectures](https://arxiv.org/abs/2509.20770)
*Christophe Bonneville,Nathan Bieberdorf,Pieterjan Robbe,Mark Asta,Habib N. Najm,Laurent Capolungo,Cosmin Safta*

Main category: cs.CE

TL;DR: A U-Net surrogate model for liquid metal dealloying that generalizes beyond training data in space and time, achieving 16,000x speedup while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Phase-field models for liquid metal dealloying become computationally intractable for large domains or long time horizons, requiring efficient surrogate models.

Method: Conditionally parameterized fully convolutional U-Net with convolutional self-attention and physics-aware padding, trained on short small-scale simulations but capable of temporal and spatial extrapolation.

Result: Achieves relative errors under 5% within training regime and below 10% when extrapolating, with up to 16,000x speedup (weeks to seconds).

Conclusion: This marks an early step toward scalable, high-fidelity extrapolation of LMD phase-field models using neural network surrogates.

Abstract: Phase-field models of liquid metal dealloying (LMD) can resolve rich
microstructural dynamics but become intractable for large domains or long time
horizons. We present a conditionally parameterized, fully convolutional U-Net
surrogate that generalizes far beyond its training window in both space and
time. The design integrates convolutional self-attention and physics-aware
padding, while parameter conditioning enables variable time-step skipping and
adaptation to diverse alloy systems. Although trained only on short,
small-scale simulations, the surrogate exploits the translational invariance of
convolutions to extend predictions to much longer horizons than traditional
solvers. It accurately reproduces key LMD physics, with relative errors
typically under 5% within the training regime and below 10% when extrapolating
to larger domains and later times. The method accelerates computations by up to
16,000 times, cutting weeks of simulation down to seconds, and marks an early
step toward scalable, high-fidelity extrapolation of LMD phase-field models.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [39] [Unbiased Parameter Estimation of Partially Observed Diffusions using Diffusion Bridges](https://arxiv.org/abs/2509.21015)
*Miguel Alvarez,Ajay Jasra*

Main category: stat.ME

TL;DR: This paper presents a method for estimating static parameters in partially observed diffusion processes without time-discretization bias, allowing parameter-dependent diffusion coefficients and enabling more efficient MCMC sampling.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of estimating parameters in partially observed diffusion processes when only time-discretized solutions are available, overcoming limitations of previous methods that couldn't handle parameter-dependent diffusion coefficients efficiently.

Method: Leverages an identity related to the gradient of the log-likelihood of diffusion bridges, building on previous work but using a novel approach that hasn't been applied before. The method facilitates more efficient Markov chain sampling algorithms.

Result: The estimator is proven to be unbiased with finite variance. The methodology demonstrates efficacy across several examples, showing improved performance over existing approaches.

Conclusion: The proposed method successfully enables unbiased parameter estimation for partially observed diffusion processes without time-discretization bias, while accommodating parameter-dependent diffusion coefficients and allowing for more efficient computational implementation.

Abstract: In this article we consider the estimation of static parameters for partially
observed diffusion processes with discrete-time observations over a fixed time
interval. In particular, when one only has access to time-discretized solutions
of the diffusions we build upon the works of \cite{ub_par,ub_grad} to devise a
method that can estimate the parameters without time-discretization bias. We
leverage an identity associated to the gradient of the log-likelihood
associated to diffusion bridges, which has not been used before. Contrary to
the afore mentioned methods, the diffusion coefficient can depend on the
parameters and our approach facilitates the use of more efficient Markov chain
sampling algorithms. We prove that our estimator is unbiased with finite
variance and demonstrate the efficacy of our methodology in several examples.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [40] [Relaxation to equilibrium of conservative dynamics II: non-gradient exclusion processes](https://arxiv.org/abs/2509.20797)
*Chenlin Gu,Linzhi Yang*

Main category: math.PR

TL;DR: The paper proves that the semigroup of the speed-change exclusion process on Z^d satisfies a variance decay of order t^{-d/2} for local functions, extending previous gradient model results to non-gradient cases using regularization, chaos expansion, and homogenization theory.


<details>
  <summary>Details</summary>
Motivation: To extend the variance decay results from gradient models to non-gradient models in the speed-change exclusion process, building on previous work by Janvresse et al. and addressing the limitations of gradient assumptions.

Method: Combines regularization arguments from previous work with chaos expansion techniques from Bertini and Zegarlinski, introducing new inputs from homogenization theory to handle the non-gradient nature of the model.

Result: Proves that the semigroup satisfies Var[P_t u] = C_u t^{-d/2} + o(t^{-(d+δ)/2}) for every local function u, with explicitly characterized constant C_u, extending the result to non-gradient models.

Conclusion: Successfully extends variance decay results to non-gradient speed-change exclusion processes through a novel combination of regularization, chaos expansion, and homogenization theory, providing explicit characterization of decay constants.

Abstract: For the speed-change exclusion process on $\mathbb{Z}^d$ reversible with
respect to the product Bernoulli measure, we prove that its semigroup $P_t$
satisfies a variance decay $\operatorname{Var}[P_t u] = C_u t^{-\frac{d}{2}} +
o(t^{-\frac{d+\delta}{2}})$ for every local function $u$, with the constant
$C_u$ explicitly characterized. This extends the result of Janvresse, Landim,
Quastel and Yau in [Ann. Probab. 27(1) 325--360, 1999] to a non-gradient model.
The proof combines the regularization argument in the previous work, and the
chaos expansion in [Markov Process. Related Fields, 5(2) 125--162, 1999] by
Bertini and Zegarlinski, via a new input from the homogenization theory.

</details>
